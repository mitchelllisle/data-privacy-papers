[
  {
    "id" : "http://arxiv.org/abs/2402.00342v1",
    "title" : "Survey of Privacy Threats and Countermeasures in Federated Learning",
    "summary" : "Federated learning is widely considered to be as a privacy-aware learning\nmethod because no training data is exchanged directly between clients.\nNevertheless, there are threats to privacy in federated learning, and privacy\ncountermeasures have been studied. However, we note that common and unique\nprivacy threats among typical types of federated learning have not been\ncategorized and described in a comprehensive and specific way. In this paper,\nwe describe privacy threats and countermeasures for the typical types of\nfederated learning; horizontal federated learning, vertical federated learning,\nand transfer federated learning.",
    "updated" : "2024-02-01T05:13:14Z",
    "published" : "2024-02-01T05:13:14Z",
    "authors" : [
      {
        "name" : "Masahiro Hayashitani"
      },
      {
        "name" : "Junki Mori"
      },
      {
        "name" : "Isamu Teranishi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01546v1",
    "title" : "Privacy-Preserving Distributed Learning for Residential Short-Term Load\n  Forecasting",
    "summary" : "In the realm of power systems, the increasing involvement of residential\nusers in load forecasting applications has heightened concerns about data\nprivacy. Specifically, the load data can inadvertently reveal the daily\nroutines of residential users, thereby posing a risk to their property\nsecurity. While federated learning (FL) has been employed to safeguard user\nprivacy by enabling model training without the exchange of raw data, these FL\nmodels have shown vulnerabilities to emerging attack techniques, such as Deep\nLeakage from Gradients and poisoning attacks. To counteract these, we initially\nemploy a Secure-Aggregation (SecAgg) algorithm that leverages multiparty\ncomputation cryptographic techniques to mitigate the risk of gradient leakage.\nHowever, the introduction of SecAgg necessitates the deployment of additional\nsub-center servers for executing the multiparty computation protocol, thereby\nescalating computational complexity and reducing system robustness, especially\nin scenarios where one or more sub-centers are unavailable. To address these\nchallenges, we introduce a Markovian Switching-based distributed training\nframework, the convergence of which is substantiated through rigorous\ntheoretical analysis. The Distributed Markovian Switching (DMS) topology shows\nstrong robustness towards the poisoning attacks as well. Case studies employing\nreal-world power system load data validate the efficacy of our proposed\nalgorithm. It not only significantly minimizes communication complexity but\nalso maintains accuracy levels comparable to traditional FL methods, thereby\nenhancing the scalability of our load forecasting algorithm.",
    "updated" : "2024-02-02T16:39:08Z",
    "published" : "2024-02-02T16:39:08Z",
    "authors" : [
      {
        "name" : "Yi Dong"
      },
      {
        "name" : "Yingjie Wang"
      },
      {
        "name" : "Mariana Gama"
      },
      {
        "name" : "Mustafa A. Mustafa"
      },
      {
        "name" : "Geert Deconinck"
      },
      {
        "name" : "Xiaowei Huang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01296v1",
    "title" : "Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted\n  Inference",
    "summary" : "Privacy-preserving neural networks have attracted increasing attention in\nrecent years, and various algorithms have been developed to keep the balance\nbetween accuracy, computational complexity and information security from the\ncryptographic view. This work takes a different view from the input data and\nstructure of neural networks. We decompose the input data (e.g., some images)\ninto sensitive and insensitive segments according to importance and privacy.\nThe sensitive segment includes some important and private information such as\nhuman faces and we take strong homomorphic encryption to keep security, whereas\nthe insensitive one contains some background and we add perturbations. We\npropose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal\nwith two segments, respectively, and ciphertext branch could utilize the\ninformation from plaintext branch by unidirectional connections. We adopt\nknowledge distillation for our bi-CryptoNets by transferring representations\nfrom a well-trained teacher neural network. Empirical studies show the\neffectiveness and decrease of inference latency for our bi-CryptoNets.",
    "updated" : "2024-02-02T10:35:05Z",
    "published" : "2024-02-02T10:35:05Z",
    "authors" : [
      {
        "name" : "Man-Jie Yuan"
      },
      {
        "name" : "Zheng Zou"
      },
      {
        "name" : "Wei Gao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01226v1",
    "title" : "HW-SW Optimization of DNNs for Privacy-preserving People Counting on\n  Low-resolution Infrared Arrays",
    "summary" : "Low-resolution infrared (IR) array sensors enable people counting\napplications such as monitoring the occupancy of spaces and people flows while\npreserving privacy and minimizing energy consumption. Deep Neural Networks\n(DNNs) have been shown to be well-suited to process these sensor data in an\naccurate and efficient manner. Nevertheless, the space of DNNs' architectures\nis huge and its manual exploration is burdensome and often leads to sub-optimal\nsolutions. To overcome this problem, in this work, we propose a highly\nautomated full-stack optimization flow for DNNs that goes from neural\narchitecture search, mixed-precision quantization, and post-processing, down to\nthe realization of a new smart sensor prototype, including a Microcontroller\nwith a customized instruction set. Integrating these cross-layer optimizations,\nwe obtain a large set of Pareto-optimal solutions in the 3D-space of energy,\nmemory, and accuracy. Deploying such solutions on our hardware platform, we\nimprove the state-of-the-art achieving up to 4.2x model size reduction, 23.8x\ncode size reduction, and 15.38x energy reduction at iso-accuracy.",
    "updated" : "2024-02-02T08:45:38Z",
    "published" : "2024-02-02T08:45:38Z",
    "authors" : [
      {
        "name" : "Matteo Risso"
      },
      {
        "name" : "Chen Xie"
      },
      {
        "name" : "Francesco Daghero"
      },
      {
        "name" : "Alessio Burrello"
      },
      {
        "name" : "Seyedmorteza Mollaei"
      },
      {
        "name" : "Marco Castellano"
      },
      {
        "name" : "Enrico Macii"
      },
      {
        "name" : "Massimo Poncino"
      },
      {
        "name" : "Daniele Jahier Pagliari"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01198v1",
    "title" : "Physical Layer Location Privacy in SIMO Communication Using Fake Paths\n  Injection",
    "summary" : "Fake path injection is an emerging paradigm for inducing privacy over\nwireless networks. In this paper, fake paths are injected by the transmitter\ninto a SIMO multipath communication channel to preserve her physical location\nfrom an eavesdropper. A novel statistical privacy metric is defined as the\nratio between the largest (resp. smallest) eigenvalues of Bob's (resp. Eve's)\nCram\\'er-Rao lower bound on the SIMO multipath channel parameters to assess the\nprivacy enhancements. Leveraging the spectral properties of generalized\nVandermonde matrices, bounds on the privacy margin of the proposed scheme are\nderived. Specifically, it is shown that the privacy margin increases\nquadratically in the inverse of the separation between the true and the fake\npaths under Eve's perspective. Numerical simulations further showcase the\napproach's benefit.",
    "updated" : "2024-02-02T07:52:32Z",
    "published" : "2024-02-02T07:52:32Z",
    "authors" : [
      {
        "name" : "Trong Duy Tran"
      },
      {
        "name" : "Maxime Ferreira Da Costa"
      },
      {
        "name" : "Linh Trung Nguyen"
      }
    ],
    "categories" : [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01096v1",
    "title" : "Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance",
    "summary" : "Emerging Distributed AI systems are revolutionizing big data computing and\ndata processing capabilities with growing economic and societal impact.\nHowever, recent studies have identified new attack surfaces and risks caused by\nsecurity, privacy, and fairness issues in AI systems. In this paper, we review\nrepresentative techniques, algorithms, and theoretical foundations for\ntrustworthy distributed AI through robustness guarantee, privacy protection,\nand fairness awareness in distributed learning. We first provide a brief\noverview of alternative architectures for distributed learning, discuss\ninherent vulnerabilities for security, privacy, and fairness of AI algorithms\nin distributed learning, and analyze why these problems are present in\ndistributed learning regardless of specific architectures. Then we provide a\nunique taxonomy of countermeasures for trustworthy distributed AI, covering (1)\nrobustness to evasion attacks and irregular queries at inference, and\nrobustness to poisoning attacks, Byzantine attacks, and irregular data\ndistribution during training; (2) privacy protection during distributed\nlearning and model inference at deployment; and (3) AI fairness and governance\nwith respect to both data and models. We conclude with a discussion on open\nchallenges and future research directions toward trustworthy distributed AI,\nsuch as the need for trustworthy AI policy guidelines, the AI\nresponsibility-utility co-design, and incentives and compliance.",
    "updated" : "2024-02-02T01:58:58Z",
    "published" : "2024-02-02T01:58:58Z",
    "authors" : [
      {
        "name" : "Wenqi Wei"
      },
      {
        "name" : "Ling Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01001v1",
    "title" : "Ensuring Data Privacy in AC Optimal Power Flow with a Distributed\n  Co-Simulation Framework",
    "summary" : "During the energy transition, the significance of collaborative management\namong institutions is rising, confronting challenges posed by data privacy\nconcerns. Prevailing research on distributed approaches, as an alternative to\ncentralized management, often lacks numerical convergence guarantees or is\nlimited to single-machine numerical simulation. To address this, we present a\ndistributed approach for solving AC Optimal Power Flow (OPF) problems within a\ngeographically distributed environment. This involves integrating the energy\nsystem Co-Simulation (eCoSim) module in the eASiMOV framework with the\nconvergence-guaranteed distributed optimization algorithm, i.e., the Augmented\nLagrangian based Alternating Direction Inexact Newton method (ALADIN).\nComprehensive evaluations across multiple system scenarios reveal a marginal\nperformance slowdown compared to the centralized approach and the distributed\napproach executed on single machines -- a justified trade-off for enhanced data\nprivacy. This investigation serves as empirical validation of the successful\nexecution of distributed AC OPF within a geographically distributed\nenvironment, highlighting potential directions for future research.",
    "updated" : "2024-02-01T20:31:08Z",
    "published" : "2024-02-01T20:31:08Z",
    "authors" : [
      {
        "name" : "Xinliang Dai"
      },
      {
        "name" : "Alexander Kocher"
      },
      {
        "name" : "Jovana Kovačević"
      },
      {
        "name" : "Burak Dindar"
      },
      {
        "name" : "Yuning Jiang"
      },
      {
        "name" : "Colin N. Jones"
      },
      {
        "name" : "Hüseyin Çakmak"
      },
      {
        "name" : "Veit Hagenmeyer"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.00906v1",
    "title" : "BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic\n  Architectures against Model Inversion Attacks",
    "summary" : "With the mainstream integration of machine learning into security-sensitive\ndomains such as healthcare and finance, concerns about data privacy have\nintensified. Conventional artificial neural networks (ANNs) have been found\nvulnerable to several attacks that can leak sensitive data. Particularly, model\ninversion (MI) attacks enable the reconstruction of data samples that have been\nused to train the model. Neuromorphic architectures have emerged as a paradigm\nshift in neural computing, enabling asynchronous and energy-efficient\ncomputation. However, little to no existing work has investigated the privacy\nof neuromorphic architectures against model inversion. Our study is motivated\nby the intuition that the non-differentiable aspect of spiking neural networks\n(SNNs) might result in inherent privacy-preserving properties, especially\nagainst gradient-based attacks. To investigate this hypothesis, we propose a\nthorough exploration of SNNs' privacy-preserving capabilities. Specifically, we\ndevelop novel inversion attack strategies that are comprehensively designed to\ntarget SNNs, offering a comparative analysis with their conventional ANN\ncounterparts. Our experiments, conducted on diverse event-based and static\ndatasets, demonstrate the effectiveness of the proposed attack strategies and\ntherefore questions the assumption of inherent privacy-preserving in\nneuromorphic architectures.",
    "updated" : "2024-02-01T03:16:40Z",
    "published" : "2024-02-01T03:16:40Z",
    "authors" : [
      {
        "name" : "Hamed Poursiami"
      },
      {
        "name" : "Ihsen Alouani"
      },
      {
        "name" : "Maryam Parsa"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03114v1",
    "title" : "Augmenting Security and Privacy in the Virtual Realm: An Analysis of\n  Extended Reality Devices",
    "summary" : "In this work, we present a device-centric analysis of security and privacy\nattacks and defenses on Extended Reality (XR) devices, highlighting the need\nfor robust and privacy-aware security mechanisms. Based on our analysis, we\npresent future research directions and propose design considerations to help\nensure the security and privacy of XR devices.",
    "updated" : "2024-02-05T15:45:11Z",
    "published" : "2024-02-05T15:45:11Z",
    "authors" : [
      {
        "name" : "Derin Cayir"
      },
      {
        "name" : "Abbas Acar"
      },
      {
        "name" : "Riccardo Lazzeretti"
      },
      {
        "name" : "Marco Angelini"
      },
      {
        "name" : "Mauro Conti"
      },
      {
        "name" : "Selcuk Uluagac"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.02672v1",
    "title" : "Estimation of conditional average treatment effects on distributed data:\n  A privacy-preserving approach",
    "summary" : "Estimation of conditional average treatment effects (CATEs) is an important\ntopic in various fields such as medical and social sciences. CATEs can be\nestimated with high accuracy if distributed data across multiple parties can be\ncentralized. However, it is difficult to aggregate such data if they contain\nprivacy information. To address this issue, we proposed data collaboration\ndouble machine learning (DC-DML), a method that can estimate CATE models with\nprivacy preservation of distributed data, and evaluated the method through\nnumerical experiments. Our contributions are summarized in the following three\npoints. First, our method enables estimation and testing of semi-parametric\nCATE models without iterative communication on distributed data.\nSemi-parametric or non-parametric CATE models enable estimation and testing\nthat is more robust to model mis-specification than parametric models. However,\nto our knowledge, no communication-efficient method has been proposed for\nestimating and testing semi-parametric or non-parametric CATE models on\ndistributed data. Second, our method enables collaborative estimation between\ndifferent parties as well as multiple time points because the\ndimensionality-reduced intermediate representations can be accumulated. Third,\nour method performed as well or better than other methods in evaluation\nexperiments using synthetic, semi-synthetic and real-world datasets.",
    "updated" : "2024-02-05T02:17:21Z",
    "published" : "2024-02-05T02:17:21Z",
    "authors" : [
      {
        "name" : "Yuji Kawamata"
      },
      {
        "name" : "Ryoki Motai"
      },
      {
        "name" : "Yukihiko Okada"
      },
      {
        "name" : "Akira Imakura"
      },
      {
        "name" : "Tetsuya Sakurai"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.02230v1",
    "title" : "Federated Learning with Differential Privacy",
    "summary" : "Federated learning (FL), as a type of distributed machine learning, is\ncapable of significantly preserving client's private data from being shared\namong different parties. Nevertheless, private information can still be\ndivulged by analyzing uploaded parameter weights from clients. In this report,\nwe showcase our empirical benchmark of the effect of the number of clients and\nthe addition of differential privacy (DP) mechanisms on the performance of the\nmodel on different types of data. Our results show that non-i.i.d and small\ndatasets have the highest decrease in performance in a distributed and\ndifferentially private setting.",
    "updated" : "2024-02-03T18:21:38Z",
    "published" : "2024-02-03T18:21:38Z",
    "authors" : [
      {
        "name" : "Adrien Banse"
      },
      {
        "name" : "Jan Kreischer"
      },
      {
        "name" : "Xavier Oliva i Jürgens"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "I.2.11"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01994v1",
    "title" : "Human-Centered Privacy Research in the Age of Large Language Models",
    "summary" : "The emergence of large language models (LLMs), and their increased use in\nuser-facing systems, has led to substantial privacy concerns. To date, research\non these privacy concerns has been model-centered: exploring how LLMs lead to\nprivacy risks like memorization, or can be used to infer personal\ncharacteristics about people from their content. We argue that there is a need\nfor more research focusing on the human aspect of these privacy issues: e.g.,\nresearch on how design paradigms for LLMs affect users' disclosure behaviors,\nusers' mental models and preferences for privacy controls, and the design of\ntools, systems, and artifacts that empower end-users to reclaim ownership over\ntheir personal data. To build usable, efficient, and privacy-friendly systems\npowered by these models with imperfect privacy properties, our goal is to\ninitiate discussions to outline an agenda for conducting human-centered\nresearch on privacy issues in LLM-powered systems. This Special Interest Group\n(SIG) aims to bring together researchers with backgrounds in usable security\nand privacy, human-AI collaboration, NLP, or any other related domains to share\ntheir perspectives and experiences on this problem, to help our community\nestablish a collective understanding of the challenges, research opportunities,\nresearch methods, and strategies to collaborate with researchers outside of\nHCI.",
    "updated" : "2024-02-03T02:32:45Z",
    "published" : "2024-02-03T02:32:45Z",
    "authors" : [
      {
        "name" : "Tianshi Li"
      },
      {
        "name" : "Sauvik Das"
      },
      {
        "name" : "Hao-Ping Lee"
      },
      {
        "name" : "Dakuo Wang"
      },
      {
        "name" : "Bingsheng Yao"
      },
      {
        "name" : "Zhiping Zhang"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01857v1",
    "title" : "Position Paper: Assessing Robustness, Privacy, and Fairness in Federated\n  Learning Integrated with Foundation Models",
    "summary" : "Federated Learning (FL), while a breakthrough in decentralized machine\nlearning, contends with significant challenges such as limited data\navailability and the variability of computational resources, which can stifle\nthe performance and scalability of the models. The integration of Foundation\nModels (FMs) into FL presents a compelling solution to these issues, with the\npotential to enhance data richness and reduce computational demands through\npre-training and data augmentation. However, this incorporation introduces\nnovel issues in terms of robustness, privacy, and fairness, which have not been\nsufficiently addressed in the existing research. We make a preliminary\ninvestigation into this field by systematically evaluating the implications of\nFM-FL integration across these dimensions. We analyze the trade-offs involved,\nuncover the threats and issues introduced by this integration, and propose a\nset of criteria and strategies for navigating these challenges. Furthermore, we\nidentify potential research directions for advancing this field, laying a\nfoundation for future development in creating reliable, secure, and equitable\nFL systems.",
    "updated" : "2024-02-02T19:26:00Z",
    "published" : "2024-02-02T19:26:00Z",
    "authors" : [
      {
        "name" : "Xi Li"
      },
      {
        "name" : "Jiaqi Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  }
]