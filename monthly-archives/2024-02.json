[
  {
    "id" : "http://arxiv.org/abs/2402.00342v1",
    "title" : "Survey of Privacy Threats and Countermeasures in Federated Learning",
    "summary" : "Federated learning is widely considered to be as a privacy-aware learning\nmethod because no training data is exchanged directly between clients.\nNevertheless, there are threats to privacy in federated learning, and privacy\ncountermeasures have been studied. However, we note that common and unique\nprivacy threats among typical types of federated learning have not been\ncategorized and described in a comprehensive and specific way. In this paper,\nwe describe privacy threats and countermeasures for the typical types of\nfederated learning; horizontal federated learning, vertical federated learning,\nand transfer federated learning.",
    "updated" : "2024-02-01T05:13:14Z",
    "published" : "2024-02-01T05:13:14Z",
    "authors" : [
      {
        "name" : "Masahiro Hayashitani"
      },
      {
        "name" : "Junki Mori"
      },
      {
        "name" : "Isamu Teranishi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01546v1",
    "title" : "Privacy-Preserving Distributed Learning for Residential Short-Term Load\n  Forecasting",
    "summary" : "In the realm of power systems, the increasing involvement of residential\nusers in load forecasting applications has heightened concerns about data\nprivacy. Specifically, the load data can inadvertently reveal the daily\nroutines of residential users, thereby posing a risk to their property\nsecurity. While federated learning (FL) has been employed to safeguard user\nprivacy by enabling model training without the exchange of raw data, these FL\nmodels have shown vulnerabilities to emerging attack techniques, such as Deep\nLeakage from Gradients and poisoning attacks. To counteract these, we initially\nemploy a Secure-Aggregation (SecAgg) algorithm that leverages multiparty\ncomputation cryptographic techniques to mitigate the risk of gradient leakage.\nHowever, the introduction of SecAgg necessitates the deployment of additional\nsub-center servers for executing the multiparty computation protocol, thereby\nescalating computational complexity and reducing system robustness, especially\nin scenarios where one or more sub-centers are unavailable. To address these\nchallenges, we introduce a Markovian Switching-based distributed training\nframework, the convergence of which is substantiated through rigorous\ntheoretical analysis. The Distributed Markovian Switching (DMS) topology shows\nstrong robustness towards the poisoning attacks as well. Case studies employing\nreal-world power system load data validate the efficacy of our proposed\nalgorithm. It not only significantly minimizes communication complexity but\nalso maintains accuracy levels comparable to traditional FL methods, thereby\nenhancing the scalability of our load forecasting algorithm.",
    "updated" : "2024-02-02T16:39:08Z",
    "published" : "2024-02-02T16:39:08Z",
    "authors" : [
      {
        "name" : "Yi Dong"
      },
      {
        "name" : "Yingjie Wang"
      },
      {
        "name" : "Mariana Gama"
      },
      {
        "name" : "Mustafa A. Mustafa"
      },
      {
        "name" : "Geert Deconinck"
      },
      {
        "name" : "Xiaowei Huang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01296v1",
    "title" : "Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted\n  Inference",
    "summary" : "Privacy-preserving neural networks have attracted increasing attention in\nrecent years, and various algorithms have been developed to keep the balance\nbetween accuracy, computational complexity and information security from the\ncryptographic view. This work takes a different view from the input data and\nstructure of neural networks. We decompose the input data (e.g., some images)\ninto sensitive and insensitive segments according to importance and privacy.\nThe sensitive segment includes some important and private information such as\nhuman faces and we take strong homomorphic encryption to keep security, whereas\nthe insensitive one contains some background and we add perturbations. We\npropose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal\nwith two segments, respectively, and ciphertext branch could utilize the\ninformation from plaintext branch by unidirectional connections. We adopt\nknowledge distillation for our bi-CryptoNets by transferring representations\nfrom a well-trained teacher neural network. Empirical studies show the\neffectiveness and decrease of inference latency for our bi-CryptoNets.",
    "updated" : "2024-02-02T10:35:05Z",
    "published" : "2024-02-02T10:35:05Z",
    "authors" : [
      {
        "name" : "Man-Jie Yuan"
      },
      {
        "name" : "Zheng Zou"
      },
      {
        "name" : "Wei Gao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01226v1",
    "title" : "HW-SW Optimization of DNNs for Privacy-preserving People Counting on\n  Low-resolution Infrared Arrays",
    "summary" : "Low-resolution infrared (IR) array sensors enable people counting\napplications such as monitoring the occupancy of spaces and people flows while\npreserving privacy and minimizing energy consumption. Deep Neural Networks\n(DNNs) have been shown to be well-suited to process these sensor data in an\naccurate and efficient manner. Nevertheless, the space of DNNs' architectures\nis huge and its manual exploration is burdensome and often leads to sub-optimal\nsolutions. To overcome this problem, in this work, we propose a highly\nautomated full-stack optimization flow for DNNs that goes from neural\narchitecture search, mixed-precision quantization, and post-processing, down to\nthe realization of a new smart sensor prototype, including a Microcontroller\nwith a customized instruction set. Integrating these cross-layer optimizations,\nwe obtain a large set of Pareto-optimal solutions in the 3D-space of energy,\nmemory, and accuracy. Deploying such solutions on our hardware platform, we\nimprove the state-of-the-art achieving up to 4.2x model size reduction, 23.8x\ncode size reduction, and 15.38x energy reduction at iso-accuracy.",
    "updated" : "2024-02-02T08:45:38Z",
    "published" : "2024-02-02T08:45:38Z",
    "authors" : [
      {
        "name" : "Matteo Risso"
      },
      {
        "name" : "Chen Xie"
      },
      {
        "name" : "Francesco Daghero"
      },
      {
        "name" : "Alessio Burrello"
      },
      {
        "name" : "Seyedmorteza Mollaei"
      },
      {
        "name" : "Marco Castellano"
      },
      {
        "name" : "Enrico Macii"
      },
      {
        "name" : "Massimo Poncino"
      },
      {
        "name" : "Daniele Jahier Pagliari"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01198v1",
    "title" : "Physical Layer Location Privacy in SIMO Communication Using Fake Paths\n  Injection",
    "summary" : "Fake path injection is an emerging paradigm for inducing privacy over\nwireless networks. In this paper, fake paths are injected by the transmitter\ninto a SIMO multipath communication channel to preserve her physical location\nfrom an eavesdropper. A novel statistical privacy metric is defined as the\nratio between the largest (resp. smallest) eigenvalues of Bob's (resp. Eve's)\nCram\\'er-Rao lower bound on the SIMO multipath channel parameters to assess the\nprivacy enhancements. Leveraging the spectral properties of generalized\nVandermonde matrices, bounds on the privacy margin of the proposed scheme are\nderived. Specifically, it is shown that the privacy margin increases\nquadratically in the inverse of the separation between the true and the fake\npaths under Eve's perspective. Numerical simulations further showcase the\napproach's benefit.",
    "updated" : "2024-02-02T07:52:32Z",
    "published" : "2024-02-02T07:52:32Z",
    "authors" : [
      {
        "name" : "Trong Duy Tran"
      },
      {
        "name" : "Maxime Ferreira Da Costa"
      },
      {
        "name" : "Linh Trung Nguyen"
      }
    ],
    "categories" : [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01096v1",
    "title" : "Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance",
    "summary" : "Emerging Distributed AI systems are revolutionizing big data computing and\ndata processing capabilities with growing economic and societal impact.\nHowever, recent studies have identified new attack surfaces and risks caused by\nsecurity, privacy, and fairness issues in AI systems. In this paper, we review\nrepresentative techniques, algorithms, and theoretical foundations for\ntrustworthy distributed AI through robustness guarantee, privacy protection,\nand fairness awareness in distributed learning. We first provide a brief\noverview of alternative architectures for distributed learning, discuss\ninherent vulnerabilities for security, privacy, and fairness of AI algorithms\nin distributed learning, and analyze why these problems are present in\ndistributed learning regardless of specific architectures. Then we provide a\nunique taxonomy of countermeasures for trustworthy distributed AI, covering (1)\nrobustness to evasion attacks and irregular queries at inference, and\nrobustness to poisoning attacks, Byzantine attacks, and irregular data\ndistribution during training; (2) privacy protection during distributed\nlearning and model inference at deployment; and (3) AI fairness and governance\nwith respect to both data and models. We conclude with a discussion on open\nchallenges and future research directions toward trustworthy distributed AI,\nsuch as the need for trustworthy AI policy guidelines, the AI\nresponsibility-utility co-design, and incentives and compliance.",
    "updated" : "2024-02-02T01:58:58Z",
    "published" : "2024-02-02T01:58:58Z",
    "authors" : [
      {
        "name" : "Wenqi Wei"
      },
      {
        "name" : "Ling Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01001v1",
    "title" : "Ensuring Data Privacy in AC Optimal Power Flow with a Distributed\n  Co-Simulation Framework",
    "summary" : "During the energy transition, the significance of collaborative management\namong institutions is rising, confronting challenges posed by data privacy\nconcerns. Prevailing research on distributed approaches, as an alternative to\ncentralized management, often lacks numerical convergence guarantees or is\nlimited to single-machine numerical simulation. To address this, we present a\ndistributed approach for solving AC Optimal Power Flow (OPF) problems within a\ngeographically distributed environment. This involves integrating the energy\nsystem Co-Simulation (eCoSim) module in the eASiMOV framework with the\nconvergence-guaranteed distributed optimization algorithm, i.e., the Augmented\nLagrangian based Alternating Direction Inexact Newton method (ALADIN).\nComprehensive evaluations across multiple system scenarios reveal a marginal\nperformance slowdown compared to the centralized approach and the distributed\napproach executed on single machines -- a justified trade-off for enhanced data\nprivacy. This investigation serves as empirical validation of the successful\nexecution of distributed AC OPF within a geographically distributed\nenvironment, highlighting potential directions for future research.",
    "updated" : "2024-02-01T20:31:08Z",
    "published" : "2024-02-01T20:31:08Z",
    "authors" : [
      {
        "name" : "Xinliang Dai"
      },
      {
        "name" : "Alexander Kocher"
      },
      {
        "name" : "Jovana Kovačević"
      },
      {
        "name" : "Burak Dindar"
      },
      {
        "name" : "Yuning Jiang"
      },
      {
        "name" : "Colin N. Jones"
      },
      {
        "name" : "Hüseyin Çakmak"
      },
      {
        "name" : "Veit Hagenmeyer"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.00906v1",
    "title" : "BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic\n  Architectures against Model Inversion Attacks",
    "summary" : "With the mainstream integration of machine learning into security-sensitive\ndomains such as healthcare and finance, concerns about data privacy have\nintensified. Conventional artificial neural networks (ANNs) have been found\nvulnerable to several attacks that can leak sensitive data. Particularly, model\ninversion (MI) attacks enable the reconstruction of data samples that have been\nused to train the model. Neuromorphic architectures have emerged as a paradigm\nshift in neural computing, enabling asynchronous and energy-efficient\ncomputation. However, little to no existing work has investigated the privacy\nof neuromorphic architectures against model inversion. Our study is motivated\nby the intuition that the non-differentiable aspect of spiking neural networks\n(SNNs) might result in inherent privacy-preserving properties, especially\nagainst gradient-based attacks. To investigate this hypothesis, we propose a\nthorough exploration of SNNs' privacy-preserving capabilities. Specifically, we\ndevelop novel inversion attack strategies that are comprehensively designed to\ntarget SNNs, offering a comparative analysis with their conventional ANN\ncounterparts. Our experiments, conducted on diverse event-based and static\ndatasets, demonstrate the effectiveness of the proposed attack strategies and\ntherefore questions the assumption of inherent privacy-preserving in\nneuromorphic architectures.",
    "updated" : "2024-02-01T03:16:40Z",
    "published" : "2024-02-01T03:16:40Z",
    "authors" : [
      {
        "name" : "Hamed Poursiami"
      },
      {
        "name" : "Ihsen Alouani"
      },
      {
        "name" : "Maryam Parsa"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03114v1",
    "title" : "Augmenting Security and Privacy in the Virtual Realm: An Analysis of\n  Extended Reality Devices",
    "summary" : "In this work, we present a device-centric analysis of security and privacy\nattacks and defenses on Extended Reality (XR) devices, highlighting the need\nfor robust and privacy-aware security mechanisms. Based on our analysis, we\npresent future research directions and propose design considerations to help\nensure the security and privacy of XR devices.",
    "updated" : "2024-02-05T15:45:11Z",
    "published" : "2024-02-05T15:45:11Z",
    "authors" : [
      {
        "name" : "Derin Cayir"
      },
      {
        "name" : "Abbas Acar"
      },
      {
        "name" : "Riccardo Lazzeretti"
      },
      {
        "name" : "Marco Angelini"
      },
      {
        "name" : "Mauro Conti"
      },
      {
        "name" : "Selcuk Uluagac"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.02672v1",
    "title" : "Estimation of conditional average treatment effects on distributed data:\n  A privacy-preserving approach",
    "summary" : "Estimation of conditional average treatment effects (CATEs) is an important\ntopic in various fields such as medical and social sciences. CATEs can be\nestimated with high accuracy if distributed data across multiple parties can be\ncentralized. However, it is difficult to aggregate such data if they contain\nprivacy information. To address this issue, we proposed data collaboration\ndouble machine learning (DC-DML), a method that can estimate CATE models with\nprivacy preservation of distributed data, and evaluated the method through\nnumerical experiments. Our contributions are summarized in the following three\npoints. First, our method enables estimation and testing of semi-parametric\nCATE models without iterative communication on distributed data.\nSemi-parametric or non-parametric CATE models enable estimation and testing\nthat is more robust to model mis-specification than parametric models. However,\nto our knowledge, no communication-efficient method has been proposed for\nestimating and testing semi-parametric or non-parametric CATE models on\ndistributed data. Second, our method enables collaborative estimation between\ndifferent parties as well as multiple time points because the\ndimensionality-reduced intermediate representations can be accumulated. Third,\nour method performed as well or better than other methods in evaluation\nexperiments using synthetic, semi-synthetic and real-world datasets.",
    "updated" : "2024-02-05T02:17:21Z",
    "published" : "2024-02-05T02:17:21Z",
    "authors" : [
      {
        "name" : "Yuji Kawamata"
      },
      {
        "name" : "Ryoki Motai"
      },
      {
        "name" : "Yukihiko Okada"
      },
      {
        "name" : "Akira Imakura"
      },
      {
        "name" : "Tetsuya Sakurai"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.02230v1",
    "title" : "Federated Learning with Differential Privacy",
    "summary" : "Federated learning (FL), as a type of distributed machine learning, is\ncapable of significantly preserving client's private data from being shared\namong different parties. Nevertheless, private information can still be\ndivulged by analyzing uploaded parameter weights from clients. In this report,\nwe showcase our empirical benchmark of the effect of the number of clients and\nthe addition of differential privacy (DP) mechanisms on the performance of the\nmodel on different types of data. Our results show that non-i.i.d and small\ndatasets have the highest decrease in performance in a distributed and\ndifferentially private setting.",
    "updated" : "2024-02-03T18:21:38Z",
    "published" : "2024-02-03T18:21:38Z",
    "authors" : [
      {
        "name" : "Adrien Banse"
      },
      {
        "name" : "Jan Kreischer"
      },
      {
        "name" : "Xavier Oliva i Jürgens"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "I.2.11"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01994v1",
    "title" : "Human-Centered Privacy Research in the Age of Large Language Models",
    "summary" : "The emergence of large language models (LLMs), and their increased use in\nuser-facing systems, has led to substantial privacy concerns. To date, research\non these privacy concerns has been model-centered: exploring how LLMs lead to\nprivacy risks like memorization, or can be used to infer personal\ncharacteristics about people from their content. We argue that there is a need\nfor more research focusing on the human aspect of these privacy issues: e.g.,\nresearch on how design paradigms for LLMs affect users' disclosure behaviors,\nusers' mental models and preferences for privacy controls, and the design of\ntools, systems, and artifacts that empower end-users to reclaim ownership over\ntheir personal data. To build usable, efficient, and privacy-friendly systems\npowered by these models with imperfect privacy properties, our goal is to\ninitiate discussions to outline an agenda for conducting human-centered\nresearch on privacy issues in LLM-powered systems. This Special Interest Group\n(SIG) aims to bring together researchers with backgrounds in usable security\nand privacy, human-AI collaboration, NLP, or any other related domains to share\ntheir perspectives and experiences on this problem, to help our community\nestablish a collective understanding of the challenges, research opportunities,\nresearch methods, and strategies to collaborate with researchers outside of\nHCI.",
    "updated" : "2024-02-03T02:32:45Z",
    "published" : "2024-02-03T02:32:45Z",
    "authors" : [
      {
        "name" : "Tianshi Li"
      },
      {
        "name" : "Sauvik Das"
      },
      {
        "name" : "Hao-Ping Lee"
      },
      {
        "name" : "Dakuo Wang"
      },
      {
        "name" : "Bingsheng Yao"
      },
      {
        "name" : "Zhiping Zhang"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01857v1",
    "title" : "Position Paper: Assessing Robustness, Privacy, and Fairness in Federated\n  Learning Integrated with Foundation Models",
    "summary" : "Federated Learning (FL), while a breakthrough in decentralized machine\nlearning, contends with significant challenges such as limited data\navailability and the variability of computational resources, which can stifle\nthe performance and scalability of the models. The integration of Foundation\nModels (FMs) into FL presents a compelling solution to these issues, with the\npotential to enhance data richness and reduce computational demands through\npre-training and data augmentation. However, this incorporation introduces\nnovel issues in terms of robustness, privacy, and fairness, which have not been\nsufficiently addressed in the existing research. We make a preliminary\ninvestigation into this field by systematically evaluating the implications of\nFM-FL integration across these dimensions. We analyze the trade-offs involved,\nuncover the threats and issues introduced by this integration, and propose a\nset of criteria and strategies for navigating these challenges. Furthermore, we\nidentify potential research directions for advancing this field, laying a\nfoundation for future development in creating reliable, secure, and equitable\nFL systems.",
    "updated" : "2024-02-02T19:26:00Z",
    "published" : "2024-02-02T19:26:00Z",
    "authors" : [
      {
        "name" : "Xi Li"
      },
      {
        "name" : "Jiaqi Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.04033v1",
    "title" : "On provable privacy vulnerabilities of graph representations",
    "summary" : "Graph representation learning (GRL) is critical for extracting insights from\ncomplex network structures, but it also raises security concerns due to\npotential privacy vulnerabilities in these representations. This paper\ninvestigates the structural vulnerabilities in graph neural models where\nsensitive topological information can be inferred through edge reconstruction\nattacks. Our research primarily addresses the theoretical underpinnings of\ncosine-similarity-based edge reconstruction attacks (COSERA), providing\ntheoretical and empirical evidence that such attacks can perfectly reconstruct\nsparse Erdos Renyi graphs with independent random features as graph size\nincreases. Conversely, we establish that sparsity is a critical factor for\nCOSERA's effectiveness, as demonstrated through analysis and experiments on\nstochastic block models. Finally, we explore the resilience of (provably)\nprivate graph representations produced via noisy aggregation (NAG) mechanism\nagainst COSERA. We empirically delineate instances wherein COSERA demonstrates\nboth efficacy and deficiency in its capacity to function as an instrument for\nelucidating the trade-off between privacy and utility.",
    "updated" : "2024-02-06T14:26:22Z",
    "published" : "2024-02-06T14:26:22Z",
    "authors" : [
      {
        "name" : "Ruofan Wu"
      },
      {
        "name" : "Guanhua Fang"
      },
      {
        "name" : "Qiying Pan"
      },
      {
        "name" : "Mingyang Zhang"
      },
      {
        "name" : "Tengfei Liu"
      },
      {
        "name" : "Weiqiang Wang"
      },
      {
        "name" : "Wenbiao Zhao"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.04013v1",
    "title" : "Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and\n  Defenses",
    "summary" : "Model Inversion (MI) attacks aim to disclose private information about the\ntraining data by abusing access to the pre-trained models. These attacks enable\nadversaries to reconstruct high-fidelity data that closely aligns with the\nprivate training data, which has raised significant privacy concerns. Despite\nthe rapid advances in the field, we lack a comprehensive overview of existing\nMI attacks and defenses. To fill this gap, this paper thoroughly investigates\nthis field and presents a holistic survey. Firstly, our work briefly reviews\nthe traditional MI on machine learning scenarios. We then elaborately analyze\nand compare numerous recent attacks and defenses on \\textbf{D}eep\n\\textbf{N}eural \\textbf{N}etworks (DNNs) across multiple modalities and\nlearning tasks.",
    "updated" : "2024-02-06T14:06:23Z",
    "published" : "2024-02-06T14:06:23Z",
    "authors" : [
      {
        "name" : "Hao Fang"
      },
      {
        "name" : "Yixiang Qiu"
      },
      {
        "name" : "Hongyao Yu"
      },
      {
        "name" : "Wenbo Yu"
      },
      {
        "name" : "Jiawei Kong"
      },
      {
        "name" : "Baoli Chong"
      },
      {
        "name" : "Bin Chen"
      },
      {
        "name" : "Xuan Wang"
      },
      {
        "name" : "Shu-Tao Xia"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03907v1",
    "title" : "Embedding Large Language Models into Extended Reality: Opportunities and\n  Challenges for Inclusion, Engagement, and Privacy",
    "summary" : "Recent developments in computer graphics, hardware, artificial intelligence\n(AI), and human-computer interaction likely lead to extended reality (XR)\ndevices and setups being more pervasive. While these devices and setups provide\nusers with interactive, engaging, and immersive experiences with different\nsensing modalities, such as eye and hand trackers, many non-player characters\nare utilized in a pre-scripted way or by conventional AI techniques. In this\npaper, we argue for using large language models (LLMs) in XR by embedding them\nin virtual avatars or as narratives to facilitate more inclusive experiences\nthrough prompt engineering according to user profiles and fine-tuning the LLMs\nfor particular purposes. We argue that such inclusion will facilitate diversity\nfor XR use. In addition, we believe that with the versatile conversational\ncapabilities of LLMs, users will engage more with XR environments, which might\nhelp XR be more used in everyday life. Lastly, we speculate that combining the\ninformation provided to LLM-powered environments by the users and the biometric\ndata obtained through the sensors might lead to novel privacy invasions. While\nstudying such possible privacy invasions, user privacy concerns and preferences\nshould also be investigated. In summary, despite some challenges, embedding\nLLMs into XR is a promising and novel research area with several opportunities.",
    "updated" : "2024-02-06T11:19:40Z",
    "published" : "2024-02-06T11:19:40Z",
    "authors" : [
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Süleyman Özdel"
      },
      {
        "name" : "Ka Hei Carrie Lau"
      },
      {
        "name" : "Mengdi Wang"
      },
      {
        "name" : "Hong Gao"
      },
      {
        "name" : "Enkelejda Kasneci"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03702v1",
    "title" : "On Learning Spatial Provenance in Privacy-Constrained Wireless Networks",
    "summary" : "In Vehicle-to-Everything networks that involve multi-hop communication, the\nRoad Side Units (RSUs) typically aim to collect location information from the\nparticipating vehicles to provide security and network diagnostics features.\nWhile the vehicles commonly use the Global Positioning System (GPS) for\nnavigation, they may refrain from sharing their precise GPS coordinates with\nthe RSUs due to privacy concerns. Therefore, to jointly address the high\nlocalization requirements by the RSUs as well as the vehicles' privacy, we\npresent a novel spatial-provenance framework wherein each vehicle uses Bloom\nfilters to embed their partial location information when forwarding the\npackets. In this framework, the RSUs and the vehicles agree upon fragmenting\nthe coverage area into several smaller regions so that the vehicles can embed\nthe identity of their regions through Bloom filters. Given the probabilistic\nnature of Bloom filters, we derive an analytical expression on the error-rates\nin provenance recovery and then pose an optimization problem to choose the\nunderlying parameters. With the help of extensive simulation results, we show\nthat our method offers near-optimal Bloom filter parameters in learning spatial\nprovenance. Some interesting trade-offs between the communication-overhead,\nspatial privacy of the vehicles and the error rates in provenance recovery are\nalso discussed.",
    "updated" : "2024-02-06T04:44:36Z",
    "published" : "2024-02-06T04:44:36Z",
    "authors" : [
      {
        "name" : "Manish Bansal"
      },
      {
        "name" : "Pramsu Srivastava"
      },
      {
        "name" : "J. Harshan"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.NI",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03688v1",
    "title" : "A Survey of Privacy Threats and Defense in Vertical Federated Learning:\n  From Model Life Cycle Perspective",
    "summary" : "Vertical Federated Learning (VFL) is a federated learning paradigm where\nmultiple participants, who share the same set of samples but hold different\nfeatures, jointly train machine learning models. Although VFL enables\ncollaborative machine learning without sharing raw data, it is still\nsusceptible to various privacy threats. In this paper, we conduct the first\ncomprehensive survey of the state-of-the-art in privacy attacks and defenses in\nVFL. We provide taxonomies for both attacks and defenses, based on their\ncharacterizations, and discuss open challenges and future research directions.\nSpecifically, our discussion is structured around the model's life cycle, by\ndelving into the privacy threats encountered during different stages of machine\nlearning and their corresponding countermeasures. This survey not only serves\nas a resource for the research community but also offers clear guidance and\nactionable insights for practitioners to safeguard data privacy throughout the\nmodel's life cycle.",
    "updated" : "2024-02-06T04:22:44Z",
    "published" : "2024-02-06T04:22:44Z",
    "authors" : [
      {
        "name" : "Lei Yu"
      },
      {
        "name" : "Meng Han"
      },
      {
        "name" : "Yiming Li"
      },
      {
        "name" : "Changting Lin"
      },
      {
        "name" : "Yao Zhang"
      },
      {
        "name" : "Mingyang Zhang"
      },
      {
        "name" : "Yan Liu"
      },
      {
        "name" : "Haiqin Weng"
      },
      {
        "name" : "Yuseok Jeon"
      },
      {
        "name" : "Ka-Ho Chow"
      },
      {
        "name" : "Stacy Patterson"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03612v1",
    "title" : "Privacy risk in GeoData: A survey",
    "summary" : "With the ubiquitous use of location-based services, large-scale\nindividual-level location data has been widely collected through\nlocation-awareness devices. The exposure of location data constitutes a\nsignificant privacy risk to users as it can lead to de-anonymisation, the\ninference of sensitive information, and even physical threats. Geoprivacy\nconcerns arise on the issues of user identity de-anonymisation and location\nexposure. In this survey, we analyse different geomasking techniques that have\nbeen proposed to protect the privacy of individuals in geodata. We present a\ntaxonomy to characterise these techniques along different dimensions, and\nconduct a survey of geomasking techniques. We then highlight shortcomings of\ncurrent techniques and discuss avenues for future research.",
    "updated" : "2024-02-06T00:55:06Z",
    "published" : "2024-02-06T00:55:06Z",
    "authors" : [
      {
        "name" : "Mahrokh Abdollahi Lorestani"
      },
      {
        "name" : "Thilina Ranbaduge"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03582v1",
    "title" : "Matcha: An IDE Plugin for Creating Accurate Privacy Nutrition Labels",
    "summary" : "Apple and Google introduced their versions of privacy nutrition labels to the\nmobile app stores to better inform users of the apps' data practices. However,\nthese labels are self-reported by developers and have been found to contain\nmany inaccuracies due to misunderstandings of the label taxonomy. In this work,\nwe present Matcha, an IDE plugin that uses automated code analysis to help\ndevelopers create accurate Google Play data safety labels. Developers can\nbenefit from Matcha's ability to detect user data accesses and transmissions\nwhile staying in control of the generated label by adding custom Java\nannotations and modifying an auto-generated XML specification. Our evaluation\nwith 12 developers showed that Matcha helped our participants improved the\naccuracy of a label they created with Google's official tool for a real-world\napp they developed. We found that participants preferred Matcha for its\naccuracy benefits. Drawing on Matcha, we discuss general design recommendations\nfor developer tools used to create accurate standardized privacy notices.",
    "updated" : "2024-02-05T23:17:08Z",
    "published" : "2024-02-05T23:17:08Z",
    "authors" : [
      {
        "name" : "Tianshi Li"
      },
      {
        "name" : "Lorrie Faith Cranor"
      },
      {
        "name" : "Yuvraj Agarwal"
      },
      {
        "name" : "Jason I. Hong"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03531v1",
    "title" : "Fairness and Privacy Guarantees in Federated Contextual Bandits",
    "summary" : "This paper considers the contextual multi-armed bandit (CMAB) problem with\nfairness and privacy guarantees in a federated environment. We consider\nmerit-based exposure as the desired fair outcome, which provides exposure to\neach action in proportion to the reward associated. We model the algorithm's\neffectiveness using fairness regret, which captures the difference between fair\noptimal policy and the policy output by the algorithm. Applying fair CMAB\nalgorithm to each agent individually leads to fairness regret linear in the\nnumber of agents. We propose that collaborative -- federated learning can be\nmore effective and provide the algorithm Fed-FairX-LinUCB that also ensures\ndifferential privacy. The primary challenge in extending the existing privacy\nframework is designing the communication protocol for communicating required\ninformation across agents. A naive protocol can either lead to weaker privacy\nguarantees or higher regret. We design a novel communication protocol that\nallows for (i) Sub-linear theoretical bounds on fairness regret for\nFed-FairX-LinUCB and comparable bounds for the private counterpart,\nPriv-FairX-LinUCB (relative to single-agent learning), (ii) Effective use of\nprivacy budget in Priv-FairX-LinUCB. We demonstrate the efficacy of our\nproposed algorithm with extensive simulations-based experiments. We show that\nboth Fed-FairX-LinUCB and Priv-FairX-LinUCB achieve near-optimal fairness\nregret.",
    "updated" : "2024-02-05T21:38:23Z",
    "published" : "2024-02-05T21:38:23Z",
    "authors" : [
      {
        "name" : "Sambhav Solanki"
      },
      {
        "name" : "Shweta Jain"
      },
      {
        "name" : "Sujit Gujar"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03435v1",
    "title" : "Psychological Assessments with Large Language Models: A Privacy-Focused\n  and Cost-Effective Approach",
    "summary" : "This study explores the use of Large Language Models (LLMs) to analyze text\ncomments from Reddit users, aiming to achieve two primary objectives: firstly,\nto pinpoint critical excerpts that support a predefined psychological\nassessment of suicidal risk; and secondly, to summarize the material to\nsubstantiate the preassigned suicidal risk level. The work is circumscribed to\nthe use of \"open-source\" LLMs that can be run locally, thereby enhancing data\nprivacy. Furthermore, it prioritizes models with low computational\nrequirements, making it accessible to both individuals and institutions\noperating on limited computing budgets. The implemented strategy only relies on\na carefully crafted prompt and a grammar to guide the LLM's text completion.\nDespite its simplicity, the evaluation metrics show outstanding results, making\nit a valuable privacy-focused and cost-effective approach. This work is part of\nthe Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared\ntask.",
    "updated" : "2024-02-05T19:00:02Z",
    "published" : "2024-02-05T19:00:02Z",
    "authors" : [
      {
        "name" : "Sergi Blanco-Cuaresma"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.02672v1",
    "title" : "Estimation of conditional average treatment effects on distributed data:\n  A privacy-preserving approach",
    "summary" : "Estimation of conditional average treatment effects (CATEs) is an important\ntopic in various fields such as medical and social sciences. CATEs can be\nestimated with high accuracy if distributed data across multiple parties can be\ncentralized. However, it is difficult to aggregate such data if they contain\nprivacy information. To address this issue, we proposed data collaboration\ndouble machine learning (DC-DML), a method that can estimate CATE models with\nprivacy preservation of distributed data, and evaluated the method through\nnumerical experiments. Our contributions are summarized in the following three\npoints. First, our method enables estimation and testing of semi-parametric\nCATE models without iterative communication on distributed data.\nSemi-parametric or non-parametric CATE models enable estimation and testing\nthat is more robust to model mis-specification than parametric models. However,\nto our knowledge, no communication-efficient method has been proposed for\nestimating and testing semi-parametric or non-parametric CATE models on\ndistributed data. Second, our method enables collaborative estimation between\ndifferent parties as well as multiple time points because the\ndimensionality-reduced intermediate representations can be accumulated. Third,\nour method performed as well or better than other methods in evaluation\nexperiments using synthetic, semi-synthetic and real-world datasets.",
    "updated" : "2024-02-05T02:17:21Z",
    "published" : "2024-02-05T02:17:21Z",
    "authors" : [
      {
        "name" : "Yuji Kawamata"
      },
      {
        "name" : "Ryoki Motai"
      },
      {
        "name" : "Yukihiko Okada"
      },
      {
        "name" : "Akira Imakura"
      },
      {
        "name" : "Tetsuya Sakurai"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01857v1",
    "title" : "Position Paper: Assessing Robustness, Privacy, and Fairness in Federated\n  Learning Integrated with Foundation Models",
    "summary" : "Federated Learning (FL), while a breakthrough in decentralized machine\nlearning, contends with significant challenges such as limited data\navailability and the variability of computational resources, which can stifle\nthe performance and scalability of the models. The integration of Foundation\nModels (FMs) into FL presents a compelling solution to these issues, with the\npotential to enhance data richness and reduce computational demands through\npre-training and data augmentation. However, this incorporation introduces\nnovel issues in terms of robustness, privacy, and fairness, which have not been\nsufficiently addressed in the existing research. We make a preliminary\ninvestigation into this field by systematically evaluating the implications of\nFM-FL integration across these dimensions. We analyze the trade-offs involved,\nuncover the threats and issues introduced by this integration, and propose a\nset of criteria and strategies for navigating these challenges. Furthermore, we\nidentify potential research directions for advancing this field, laying a\nfoundation for future development in creating reliable, secure, and equitable\nFL systems.",
    "updated" : "2024-02-02T19:26:00Z",
    "published" : "2024-02-02T19:26:00Z",
    "authors" : [
      {
        "name" : "Xi Li"
      },
      {
        "name" : "Jiaqi Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.04840v1",
    "title" : "Efficient Estimation of a Gaussian Mean with Local Differential Privacy",
    "summary" : "In this paper we study the problem of estimating the unknown mean $\\theta$ of\na unit variance Gaussian distribution in a locally differentially private (LDP)\nway. In the high-privacy regime ($\\epsilon\\le 0.67$), we identify the exact\noptimal privacy mechanism that minimizes the variance of the estimator\nasymptotically. It turns out to be the extraordinarily simple sign mechanism\nthat applies randomized response to the sign of $X_i-\\theta$. However, since\nthis optimal mechanism depends on the unknown mean $\\theta$, we employ a\ntwo-stage LDP parameter estimation procedure which requires splitting agents\ninto two groups. The first $n_1$ observations are used to consistently but not\nnecessarily efficiently estimate the parameter $\\theta$ by\n$\\tilde{\\theta}_{n_1}$. Then this estimate is updated by applying the sign\nmechanism with $\\tilde{\\theta}_{n_1}$ instead of $\\theta$ to the remaining\n$n-n_1$ observations, to obtain an LDP and efficient estimator of the unknown\nmean.",
    "updated" : "2024-02-07T13:41:45Z",
    "published" : "2024-02-07T13:41:45Z",
    "authors" : [
      {
        "name" : "Nikita Kalinin"
      },
      {
        "name" : "Lukas Steinberger"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.04489v1",
    "title" : "De-amplifying Bias from Differential Privacy in Language Model\n  Fine-tuning",
    "summary" : "Fairness and privacy are two important values machine learning (ML)\npractitioners often seek to operationalize in models. Fairness aims to reduce\nmodel bias for social/demographic sub-groups. Privacy via differential privacy\n(DP) mechanisms, on the other hand, limits the impact of any individual's\ntraining data on the resulting model. The trade-offs between privacy and\nfairness goals of trustworthy ML pose a challenge to those wishing to address\nboth. We show that DP amplifies gender, racial, and religious bias when\nfine-tuning large language models (LLMs), producing models more biased than\nones fine-tuned without DP. We find the cause of the amplification to be a\ndisparity in convergence of gradients across sub-groups. Through the case of\nbinary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA),\na known method for addressing bias, also mitigates bias amplification by DP. As\na consequence, DP and CDA together can be used to fine-tune models while\nmaintaining both fairness and privacy.",
    "updated" : "2024-02-07T00:30:58Z",
    "published" : "2024-02-07T00:30:58Z",
    "authors" : [
      {
        "name" : "Sanjari Srivastava"
      },
      {
        "name" : "Piotr Mardziel"
      },
      {
        "name" : "Zhikhun Zhang"
      },
      {
        "name" : "Archana Ahlawat"
      },
      {
        "name" : "Anupam Datta"
      },
      {
        "name" : "John C Mitchell"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY",
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.05860v1",
    "title" : "Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic\n  Surgery",
    "summary" : "Deep Neural Networks (DNNs) based semantic segmentation of the robotic\ninstruments and tissues can enhance the precision of surgical activities in\nrobot-assisted surgery. However, in biological learning, DNNs cannot learn\nincremental tasks over time and exhibit catastrophic forgetting, which refers\nto the sharp decline in performance on previously learned tasks after learning\na new one. Specifically, when data scarcity is the issue, the model shows a\nrapid drop in performance on previously learned instruments after learning new\ndata with new instruments. The problem becomes worse when it limits releasing\nthe dataset of the old instruments for the old model due to privacy concerns\nand the unavailability of the data for the new or updated version of the\ninstruments for the continual learning model. For this purpose, we develop a\nprivacy-preserving synthetic continual semantic segmentation framework by\nblending and harmonizing (i) open-source old instruments foreground to the\nsynthesized background without revealing real patient data in public and (ii)\nnew instruments foreground to extensively augmented real background. To boost\nthe balanced logit distillation from the old model to the continual learning\nmodel, we design overlapping class-aware temperature normalization (CAT) by\ncontrolling model learning utility. We also introduce multi-scale\nshifted-feature distillation (SD) to maintain long and short-range spatial\nrelationships among the semantic objects where conventional short-range spatial\nfeatures with limited information reduce the power of feature distillation. We\ndemonstrate the effectiveness of our framework on the EndoVis 2017 and 2018\ninstrument segmentation dataset with a generalized continual learning setting.\nCode is available at~\\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}.",
    "updated" : "2024-02-08T17:44:06Z",
    "published" : "2024-02-08T17:44:06Z",
    "authors" : [
      {
        "name" : "Mengya Xu"
      },
      {
        "name" : "Mobarakol Islam"
      },
      {
        "name" : "Long Bai"
      },
      {
        "name" : "Hongliang Ren"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.05690v1",
    "title" : "Overcoming Noise Limitations in QKD with Quantum Privacy Amplification",
    "summary" : "High-quality, distributed quantum entanglement is the distinctive resource\nfor quantum communication and forms the foundation for the unequalled level of\nsecurity that can be assured in quantum key distribution. While the\nentanglement provider does not need to be trusted, the secure key rate drops to\nzero if the entanglement used is too noisy. In this paper, we show\nexperimentally that QPA is able to increase the secure key rate achievable with\nQKD by improving the quality of distributed entanglement, thus increasing the\nquantum advantage in QKD. Beyond that, we show that QPA enables key generation\nat noise levels that previously prevented key generation. These remarkable\nresults were only made possible by the efficient implementation exploiting\nhyperentanglement in the polarisation and energy-time degrees of freedom. We\nprovide a detailed characterisation of the gain in secure key rate achieved in\nour proof-of-principle experiment at different noise levels. The results are\nparamount for the implementation of a global quantum network linking quantum\nprocessors and ensuring future-proof data security.",
    "updated" : "2024-02-08T14:07:36Z",
    "published" : "2024-02-08T14:07:36Z",
    "authors" : [
      {
        "name" : "Philipp Sohr"
      },
      {
        "name" : "Sebastian Ecker"
      },
      {
        "name" : "Lukas Bulla"
      },
      {
        "name" : "Martin Bohmann"
      },
      {
        "name" : "Rupert Ursin"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.05453v1",
    "title" : "Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss",
    "summary" : "Machine learning models are susceptible to membership inference attacks\n(MIAs), which aim to infer whether a sample is in the training set. Existing\nwork utilizes gradient ascent to enlarge the loss variance of training data,\nalleviating the privacy risk. However, optimizing toward a reverse direction\nmay cause the model parameters to oscillate near local minima, leading to\ninstability and suboptimal performance. In this work, we propose a novel method\n-- Convex-Concave Loss, which enables a high variance of training loss\ndistribution by gradient descent. Our method is motivated by the theoretical\nanalysis that convex losses tend to decrease the loss variance during training.\nThus, our key idea behind CCL is to reduce the convexity of loss functions with\na concave term. Trained with CCL, neural networks produce losses with high\nvariance for training data, reinforcing the defense against MIAs. Extensive\nexperiments demonstrate the superiority of CCL, achieving state-of-the-art\nbalance in the privacy-utility trade-off.",
    "updated" : "2024-02-08T07:14:17Z",
    "published" : "2024-02-08T07:14:17Z",
    "authors" : [
      {
        "name" : "Zhenlong Liu"
      },
      {
        "name" : "Lei Feng"
      },
      {
        "name" : "Huiping Zhuang"
      },
      {
        "name" : "Xiaofeng Cao"
      },
      {
        "name" : "Hongxin Wei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.06137v1",
    "title" : "On the Privacy of Selection Mechanisms with Gaussian Noise",
    "summary" : "Report Noisy Max and Above Threshold are two classical differentially private\n(DP) selection mechanisms. Their output is obtained by adding noise to a\nsequence of low-sensitivity queries and reporting the identity of the query\nwhose (noisy) answer satisfies a certain condition. Pure DP guarantees for\nthese mechanisms are easy to obtain when Laplace noise is added to the queries.\nOn the other hand, when instantiated using Gaussian noise, standard analyses\nonly yield approximate DP guarantees despite the fact that the outputs of these\nmechanisms lie in a discrete space. In this work, we revisit the analysis of\nReport Noisy Max and Above Threshold with Gaussian noise and show that, under\nthe additional assumption that the underlying queries are bounded, it is\npossible to provide pure ex-ante DP bounds for Report Noisy Max and pure\nex-post DP bounds for Above Threshold. The resulting bounds are tight and\ndepend on closed-form expressions that can be numerically evaluated using\nstandard methods. Empirically we find these lead to tighter privacy accounting\nin the high privacy, low data regime. Further, we propose a simple privacy\nfilter for composing pure ex-post DP guarantees, and use it to derive a fully\nadaptive Gaussian Sparse Vector Technique mechanism. Finally, we provide\nexperiments on mobility and energy consumption datasets demonstrating that our\nSparse Vector Technique is practically competitive with previous approaches and\nrequires less hyper-parameter tuning.",
    "updated" : "2024-02-09T02:11:25Z",
    "published" : "2024-02-09T02:11:25Z",
    "authors" : [
      {
        "name" : "Jonathan Lebensold"
      },
      {
        "name" : "Doina Precup"
      },
      {
        "name" : "Borja Balle"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  }
]