[
  {
    "id" : "http://arxiv.org/abs/2402.00342v1",
    "title" : "Survey of Privacy Threats and Countermeasures in Federated Learning",
    "summary" : "Federated learning is widely considered to be as a privacy-aware learning\nmethod because no training data is exchanged directly between clients.\nNevertheless, there are threats to privacy in federated learning, and privacy\ncountermeasures have been studied. However, we note that common and unique\nprivacy threats among typical types of federated learning have not been\ncategorized and described in a comprehensive and specific way. In this paper,\nwe describe privacy threats and countermeasures for the typical types of\nfederated learning; horizontal federated learning, vertical federated learning,\nand transfer federated learning.",
    "updated" : "2024-02-01T05:13:14Z",
    "published" : "2024-02-01T05:13:14Z",
    "authors" : [
      {
        "name" : "Masahiro Hayashitani"
      },
      {
        "name" : "Junki Mori"
      },
      {
        "name" : "Isamu Teranishi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01546v1",
    "title" : "Privacy-Preserving Distributed Learning for Residential Short-Term Load\n  Forecasting",
    "summary" : "In the realm of power systems, the increasing involvement of residential\nusers in load forecasting applications has heightened concerns about data\nprivacy. Specifically, the load data can inadvertently reveal the daily\nroutines of residential users, thereby posing a risk to their property\nsecurity. While federated learning (FL) has been employed to safeguard user\nprivacy by enabling model training without the exchange of raw data, these FL\nmodels have shown vulnerabilities to emerging attack techniques, such as Deep\nLeakage from Gradients and poisoning attacks. To counteract these, we initially\nemploy a Secure-Aggregation (SecAgg) algorithm that leverages multiparty\ncomputation cryptographic techniques to mitigate the risk of gradient leakage.\nHowever, the introduction of SecAgg necessitates the deployment of additional\nsub-center servers for executing the multiparty computation protocol, thereby\nescalating computational complexity and reducing system robustness, especially\nin scenarios where one or more sub-centers are unavailable. To address these\nchallenges, we introduce a Markovian Switching-based distributed training\nframework, the convergence of which is substantiated through rigorous\ntheoretical analysis. The Distributed Markovian Switching (DMS) topology shows\nstrong robustness towards the poisoning attacks as well. Case studies employing\nreal-world power system load data validate the efficacy of our proposed\nalgorithm. It not only significantly minimizes communication complexity but\nalso maintains accuracy levels comparable to traditional FL methods, thereby\nenhancing the scalability of our load forecasting algorithm.",
    "updated" : "2024-02-02T16:39:08Z",
    "published" : "2024-02-02T16:39:08Z",
    "authors" : [
      {
        "name" : "Yi Dong"
      },
      {
        "name" : "Yingjie Wang"
      },
      {
        "name" : "Mariana Gama"
      },
      {
        "name" : "Mustafa A. Mustafa"
      },
      {
        "name" : "Geert Deconinck"
      },
      {
        "name" : "Xiaowei Huang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01296v1",
    "title" : "Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted\n  Inference",
    "summary" : "Privacy-preserving neural networks have attracted increasing attention in\nrecent years, and various algorithms have been developed to keep the balance\nbetween accuracy, computational complexity and information security from the\ncryptographic view. This work takes a different view from the input data and\nstructure of neural networks. We decompose the input data (e.g., some images)\ninto sensitive and insensitive segments according to importance and privacy.\nThe sensitive segment includes some important and private information such as\nhuman faces and we take strong homomorphic encryption to keep security, whereas\nthe insensitive one contains some background and we add perturbations. We\npropose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal\nwith two segments, respectively, and ciphertext branch could utilize the\ninformation from plaintext branch by unidirectional connections. We adopt\nknowledge distillation for our bi-CryptoNets by transferring representations\nfrom a well-trained teacher neural network. Empirical studies show the\neffectiveness and decrease of inference latency for our bi-CryptoNets.",
    "updated" : "2024-02-02T10:35:05Z",
    "published" : "2024-02-02T10:35:05Z",
    "authors" : [
      {
        "name" : "Man-Jie Yuan"
      },
      {
        "name" : "Zheng Zou"
      },
      {
        "name" : "Wei Gao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01226v1",
    "title" : "HW-SW Optimization of DNNs for Privacy-preserving People Counting on\n  Low-resolution Infrared Arrays",
    "summary" : "Low-resolution infrared (IR) array sensors enable people counting\napplications such as monitoring the occupancy of spaces and people flows while\npreserving privacy and minimizing energy consumption. Deep Neural Networks\n(DNNs) have been shown to be well-suited to process these sensor data in an\naccurate and efficient manner. Nevertheless, the space of DNNs' architectures\nis huge and its manual exploration is burdensome and often leads to sub-optimal\nsolutions. To overcome this problem, in this work, we propose a highly\nautomated full-stack optimization flow for DNNs that goes from neural\narchitecture search, mixed-precision quantization, and post-processing, down to\nthe realization of a new smart sensor prototype, including a Microcontroller\nwith a customized instruction set. Integrating these cross-layer optimizations,\nwe obtain a large set of Pareto-optimal solutions in the 3D-space of energy,\nmemory, and accuracy. Deploying such solutions on our hardware platform, we\nimprove the state-of-the-art achieving up to 4.2x model size reduction, 23.8x\ncode size reduction, and 15.38x energy reduction at iso-accuracy.",
    "updated" : "2024-02-02T08:45:38Z",
    "published" : "2024-02-02T08:45:38Z",
    "authors" : [
      {
        "name" : "Matteo Risso"
      },
      {
        "name" : "Chen Xie"
      },
      {
        "name" : "Francesco Daghero"
      },
      {
        "name" : "Alessio Burrello"
      },
      {
        "name" : "Seyedmorteza Mollaei"
      },
      {
        "name" : "Marco Castellano"
      },
      {
        "name" : "Enrico Macii"
      },
      {
        "name" : "Massimo Poncino"
      },
      {
        "name" : "Daniele Jahier Pagliari"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01198v1",
    "title" : "Physical Layer Location Privacy in SIMO Communication Using Fake Paths\n  Injection",
    "summary" : "Fake path injection is an emerging paradigm for inducing privacy over\nwireless networks. In this paper, fake paths are injected by the transmitter\ninto a SIMO multipath communication channel to preserve her physical location\nfrom an eavesdropper. A novel statistical privacy metric is defined as the\nratio between the largest (resp. smallest) eigenvalues of Bob's (resp. Eve's)\nCram\\'er-Rao lower bound on the SIMO multipath channel parameters to assess the\nprivacy enhancements. Leveraging the spectral properties of generalized\nVandermonde matrices, bounds on the privacy margin of the proposed scheme are\nderived. Specifically, it is shown that the privacy margin increases\nquadratically in the inverse of the separation between the true and the fake\npaths under Eve's perspective. Numerical simulations further showcase the\napproach's benefit.",
    "updated" : "2024-02-02T07:52:32Z",
    "published" : "2024-02-02T07:52:32Z",
    "authors" : [
      {
        "name" : "Trong Duy Tran"
      },
      {
        "name" : "Maxime Ferreira Da Costa"
      },
      {
        "name" : "Linh Trung Nguyen"
      }
    ],
    "categories" : [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01096v1",
    "title" : "Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance",
    "summary" : "Emerging Distributed AI systems are revolutionizing big data computing and\ndata processing capabilities with growing economic and societal impact.\nHowever, recent studies have identified new attack surfaces and risks caused by\nsecurity, privacy, and fairness issues in AI systems. In this paper, we review\nrepresentative techniques, algorithms, and theoretical foundations for\ntrustworthy distributed AI through robustness guarantee, privacy protection,\nand fairness awareness in distributed learning. We first provide a brief\noverview of alternative architectures for distributed learning, discuss\ninherent vulnerabilities for security, privacy, and fairness of AI algorithms\nin distributed learning, and analyze why these problems are present in\ndistributed learning regardless of specific architectures. Then we provide a\nunique taxonomy of countermeasures for trustworthy distributed AI, covering (1)\nrobustness to evasion attacks and irregular queries at inference, and\nrobustness to poisoning attacks, Byzantine attacks, and irregular data\ndistribution during training; (2) privacy protection during distributed\nlearning and model inference at deployment; and (3) AI fairness and governance\nwith respect to both data and models. We conclude with a discussion on open\nchallenges and future research directions toward trustworthy distributed AI,\nsuch as the need for trustworthy AI policy guidelines, the AI\nresponsibility-utility co-design, and incentives and compliance.",
    "updated" : "2024-02-02T01:58:58Z",
    "published" : "2024-02-02T01:58:58Z",
    "authors" : [
      {
        "name" : "Wenqi Wei"
      },
      {
        "name" : "Ling Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01001v1",
    "title" : "Ensuring Data Privacy in AC Optimal Power Flow with a Distributed\n  Co-Simulation Framework",
    "summary" : "During the energy transition, the significance of collaborative management\namong institutions is rising, confronting challenges posed by data privacy\nconcerns. Prevailing research on distributed approaches, as an alternative to\ncentralized management, often lacks numerical convergence guarantees or is\nlimited to single-machine numerical simulation. To address this, we present a\ndistributed approach for solving AC Optimal Power Flow (OPF) problems within a\ngeographically distributed environment. This involves integrating the energy\nsystem Co-Simulation (eCoSim) module in the eASiMOV framework with the\nconvergence-guaranteed distributed optimization algorithm, i.e., the Augmented\nLagrangian based Alternating Direction Inexact Newton method (ALADIN).\nComprehensive evaluations across multiple system scenarios reveal a marginal\nperformance slowdown compared to the centralized approach and the distributed\napproach executed on single machines -- a justified trade-off for enhanced data\nprivacy. This investigation serves as empirical validation of the successful\nexecution of distributed AC OPF within a geographically distributed\nenvironment, highlighting potential directions for future research.",
    "updated" : "2024-02-01T20:31:08Z",
    "published" : "2024-02-01T20:31:08Z",
    "authors" : [
      {
        "name" : "Xinliang Dai"
      },
      {
        "name" : "Alexander Kocher"
      },
      {
        "name" : "Jovana Kovačević"
      },
      {
        "name" : "Burak Dindar"
      },
      {
        "name" : "Yuning Jiang"
      },
      {
        "name" : "Colin N. Jones"
      },
      {
        "name" : "Hüseyin Çakmak"
      },
      {
        "name" : "Veit Hagenmeyer"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.00906v1",
    "title" : "BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic\n  Architectures against Model Inversion Attacks",
    "summary" : "With the mainstream integration of machine learning into security-sensitive\ndomains such as healthcare and finance, concerns about data privacy have\nintensified. Conventional artificial neural networks (ANNs) have been found\nvulnerable to several attacks that can leak sensitive data. Particularly, model\ninversion (MI) attacks enable the reconstruction of data samples that have been\nused to train the model. Neuromorphic architectures have emerged as a paradigm\nshift in neural computing, enabling asynchronous and energy-efficient\ncomputation. However, little to no existing work has investigated the privacy\nof neuromorphic architectures against model inversion. Our study is motivated\nby the intuition that the non-differentiable aspect of spiking neural networks\n(SNNs) might result in inherent privacy-preserving properties, especially\nagainst gradient-based attacks. To investigate this hypothesis, we propose a\nthorough exploration of SNNs' privacy-preserving capabilities. Specifically, we\ndevelop novel inversion attack strategies that are comprehensively designed to\ntarget SNNs, offering a comparative analysis with their conventional ANN\ncounterparts. Our experiments, conducted on diverse event-based and static\ndatasets, demonstrate the effectiveness of the proposed attack strategies and\ntherefore questions the assumption of inherent privacy-preserving in\nneuromorphic architectures.",
    "updated" : "2024-02-01T03:16:40Z",
    "published" : "2024-02-01T03:16:40Z",
    "authors" : [
      {
        "name" : "Hamed Poursiami"
      },
      {
        "name" : "Ihsen Alouani"
      },
      {
        "name" : "Maryam Parsa"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03114v1",
    "title" : "Augmenting Security and Privacy in the Virtual Realm: An Analysis of\n  Extended Reality Devices",
    "summary" : "In this work, we present a device-centric analysis of security and privacy\nattacks and defenses on Extended Reality (XR) devices, highlighting the need\nfor robust and privacy-aware security mechanisms. Based on our analysis, we\npresent future research directions and propose design considerations to help\nensure the security and privacy of XR devices.",
    "updated" : "2024-02-05T15:45:11Z",
    "published" : "2024-02-05T15:45:11Z",
    "authors" : [
      {
        "name" : "Derin Cayir"
      },
      {
        "name" : "Abbas Acar"
      },
      {
        "name" : "Riccardo Lazzeretti"
      },
      {
        "name" : "Marco Angelini"
      },
      {
        "name" : "Mauro Conti"
      },
      {
        "name" : "Selcuk Uluagac"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.02672v1",
    "title" : "Estimation of conditional average treatment effects on distributed data:\n  A privacy-preserving approach",
    "summary" : "Estimation of conditional average treatment effects (CATEs) is an important\ntopic in various fields such as medical and social sciences. CATEs can be\nestimated with high accuracy if distributed data across multiple parties can be\ncentralized. However, it is difficult to aggregate such data if they contain\nprivacy information. To address this issue, we proposed data collaboration\ndouble machine learning (DC-DML), a method that can estimate CATE models with\nprivacy preservation of distributed data, and evaluated the method through\nnumerical experiments. Our contributions are summarized in the following three\npoints. First, our method enables estimation and testing of semi-parametric\nCATE models without iterative communication on distributed data.\nSemi-parametric or non-parametric CATE models enable estimation and testing\nthat is more robust to model mis-specification than parametric models. However,\nto our knowledge, no communication-efficient method has been proposed for\nestimating and testing semi-parametric or non-parametric CATE models on\ndistributed data. Second, our method enables collaborative estimation between\ndifferent parties as well as multiple time points because the\ndimensionality-reduced intermediate representations can be accumulated. Third,\nour method performed as well or better than other methods in evaluation\nexperiments using synthetic, semi-synthetic and real-world datasets.",
    "updated" : "2024-02-05T02:17:21Z",
    "published" : "2024-02-05T02:17:21Z",
    "authors" : [
      {
        "name" : "Yuji Kawamata"
      },
      {
        "name" : "Ryoki Motai"
      },
      {
        "name" : "Yukihiko Okada"
      },
      {
        "name" : "Akira Imakura"
      },
      {
        "name" : "Tetsuya Sakurai"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.02230v1",
    "title" : "Federated Learning with Differential Privacy",
    "summary" : "Federated learning (FL), as a type of distributed machine learning, is\ncapable of significantly preserving client's private data from being shared\namong different parties. Nevertheless, private information can still be\ndivulged by analyzing uploaded parameter weights from clients. In this report,\nwe showcase our empirical benchmark of the effect of the number of clients and\nthe addition of differential privacy (DP) mechanisms on the performance of the\nmodel on different types of data. Our results show that non-i.i.d and small\ndatasets have the highest decrease in performance in a distributed and\ndifferentially private setting.",
    "updated" : "2024-02-03T18:21:38Z",
    "published" : "2024-02-03T18:21:38Z",
    "authors" : [
      {
        "name" : "Adrien Banse"
      },
      {
        "name" : "Jan Kreischer"
      },
      {
        "name" : "Xavier Oliva i Jürgens"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "I.2.11"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01994v1",
    "title" : "Human-Centered Privacy Research in the Age of Large Language Models",
    "summary" : "The emergence of large language models (LLMs), and their increased use in\nuser-facing systems, has led to substantial privacy concerns. To date, research\non these privacy concerns has been model-centered: exploring how LLMs lead to\nprivacy risks like memorization, or can be used to infer personal\ncharacteristics about people from their content. We argue that there is a need\nfor more research focusing on the human aspect of these privacy issues: e.g.,\nresearch on how design paradigms for LLMs affect users' disclosure behaviors,\nusers' mental models and preferences for privacy controls, and the design of\ntools, systems, and artifacts that empower end-users to reclaim ownership over\ntheir personal data. To build usable, efficient, and privacy-friendly systems\npowered by these models with imperfect privacy properties, our goal is to\ninitiate discussions to outline an agenda for conducting human-centered\nresearch on privacy issues in LLM-powered systems. This Special Interest Group\n(SIG) aims to bring together researchers with backgrounds in usable security\nand privacy, human-AI collaboration, NLP, or any other related domains to share\ntheir perspectives and experiences on this problem, to help our community\nestablish a collective understanding of the challenges, research opportunities,\nresearch methods, and strategies to collaborate with researchers outside of\nHCI.",
    "updated" : "2024-02-03T02:32:45Z",
    "published" : "2024-02-03T02:32:45Z",
    "authors" : [
      {
        "name" : "Tianshi Li"
      },
      {
        "name" : "Sauvik Das"
      },
      {
        "name" : "Hao-Ping Lee"
      },
      {
        "name" : "Dakuo Wang"
      },
      {
        "name" : "Bingsheng Yao"
      },
      {
        "name" : "Zhiping Zhang"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01857v1",
    "title" : "Position Paper: Assessing Robustness, Privacy, and Fairness in Federated\n  Learning Integrated with Foundation Models",
    "summary" : "Federated Learning (FL), while a breakthrough in decentralized machine\nlearning, contends with significant challenges such as limited data\navailability and the variability of computational resources, which can stifle\nthe performance and scalability of the models. The integration of Foundation\nModels (FMs) into FL presents a compelling solution to these issues, with the\npotential to enhance data richness and reduce computational demands through\npre-training and data augmentation. However, this incorporation introduces\nnovel issues in terms of robustness, privacy, and fairness, which have not been\nsufficiently addressed in the existing research. We make a preliminary\ninvestigation into this field by systematically evaluating the implications of\nFM-FL integration across these dimensions. We analyze the trade-offs involved,\nuncover the threats and issues introduced by this integration, and propose a\nset of criteria and strategies for navigating these challenges. Furthermore, we\nidentify potential research directions for advancing this field, laying a\nfoundation for future development in creating reliable, secure, and equitable\nFL systems.",
    "updated" : "2024-02-02T19:26:00Z",
    "published" : "2024-02-02T19:26:00Z",
    "authors" : [
      {
        "name" : "Xi Li"
      },
      {
        "name" : "Jiaqi Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.04033v1",
    "title" : "On provable privacy vulnerabilities of graph representations",
    "summary" : "Graph representation learning (GRL) is critical for extracting insights from\ncomplex network structures, but it also raises security concerns due to\npotential privacy vulnerabilities in these representations. This paper\ninvestigates the structural vulnerabilities in graph neural models where\nsensitive topological information can be inferred through edge reconstruction\nattacks. Our research primarily addresses the theoretical underpinnings of\ncosine-similarity-based edge reconstruction attacks (COSERA), providing\ntheoretical and empirical evidence that such attacks can perfectly reconstruct\nsparse Erdos Renyi graphs with independent random features as graph size\nincreases. Conversely, we establish that sparsity is a critical factor for\nCOSERA's effectiveness, as demonstrated through analysis and experiments on\nstochastic block models. Finally, we explore the resilience of (provably)\nprivate graph representations produced via noisy aggregation (NAG) mechanism\nagainst COSERA. We empirically delineate instances wherein COSERA demonstrates\nboth efficacy and deficiency in its capacity to function as an instrument for\nelucidating the trade-off between privacy and utility.",
    "updated" : "2024-02-06T14:26:22Z",
    "published" : "2024-02-06T14:26:22Z",
    "authors" : [
      {
        "name" : "Ruofan Wu"
      },
      {
        "name" : "Guanhua Fang"
      },
      {
        "name" : "Qiying Pan"
      },
      {
        "name" : "Mingyang Zhang"
      },
      {
        "name" : "Tengfei Liu"
      },
      {
        "name" : "Weiqiang Wang"
      },
      {
        "name" : "Wenbiao Zhao"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.04013v1",
    "title" : "Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and\n  Defenses",
    "summary" : "Model Inversion (MI) attacks aim to disclose private information about the\ntraining data by abusing access to the pre-trained models. These attacks enable\nadversaries to reconstruct high-fidelity data that closely aligns with the\nprivate training data, which has raised significant privacy concerns. Despite\nthe rapid advances in the field, we lack a comprehensive overview of existing\nMI attacks and defenses. To fill this gap, this paper thoroughly investigates\nthis field and presents a holistic survey. Firstly, our work briefly reviews\nthe traditional MI on machine learning scenarios. We then elaborately analyze\nand compare numerous recent attacks and defenses on \\textbf{D}eep\n\\textbf{N}eural \\textbf{N}etworks (DNNs) across multiple modalities and\nlearning tasks.",
    "updated" : "2024-02-06T14:06:23Z",
    "published" : "2024-02-06T14:06:23Z",
    "authors" : [
      {
        "name" : "Hao Fang"
      },
      {
        "name" : "Yixiang Qiu"
      },
      {
        "name" : "Hongyao Yu"
      },
      {
        "name" : "Wenbo Yu"
      },
      {
        "name" : "Jiawei Kong"
      },
      {
        "name" : "Baoli Chong"
      },
      {
        "name" : "Bin Chen"
      },
      {
        "name" : "Xuan Wang"
      },
      {
        "name" : "Shu-Tao Xia"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03907v1",
    "title" : "Embedding Large Language Models into Extended Reality: Opportunities and\n  Challenges for Inclusion, Engagement, and Privacy",
    "summary" : "Recent developments in computer graphics, hardware, artificial intelligence\n(AI), and human-computer interaction likely lead to extended reality (XR)\ndevices and setups being more pervasive. While these devices and setups provide\nusers with interactive, engaging, and immersive experiences with different\nsensing modalities, such as eye and hand trackers, many non-player characters\nare utilized in a pre-scripted way or by conventional AI techniques. In this\npaper, we argue for using large language models (LLMs) in XR by embedding them\nin virtual avatars or as narratives to facilitate more inclusive experiences\nthrough prompt engineering according to user profiles and fine-tuning the LLMs\nfor particular purposes. We argue that such inclusion will facilitate diversity\nfor XR use. In addition, we believe that with the versatile conversational\ncapabilities of LLMs, users will engage more with XR environments, which might\nhelp XR be more used in everyday life. Lastly, we speculate that combining the\ninformation provided to LLM-powered environments by the users and the biometric\ndata obtained through the sensors might lead to novel privacy invasions. While\nstudying such possible privacy invasions, user privacy concerns and preferences\nshould also be investigated. In summary, despite some challenges, embedding\nLLMs into XR is a promising and novel research area with several opportunities.",
    "updated" : "2024-02-06T11:19:40Z",
    "published" : "2024-02-06T11:19:40Z",
    "authors" : [
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Süleyman Özdel"
      },
      {
        "name" : "Ka Hei Carrie Lau"
      },
      {
        "name" : "Mengdi Wang"
      },
      {
        "name" : "Hong Gao"
      },
      {
        "name" : "Enkelejda Kasneci"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03702v1",
    "title" : "On Learning Spatial Provenance in Privacy-Constrained Wireless Networks",
    "summary" : "In Vehicle-to-Everything networks that involve multi-hop communication, the\nRoad Side Units (RSUs) typically aim to collect location information from the\nparticipating vehicles to provide security and network diagnostics features.\nWhile the vehicles commonly use the Global Positioning System (GPS) for\nnavigation, they may refrain from sharing their precise GPS coordinates with\nthe RSUs due to privacy concerns. Therefore, to jointly address the high\nlocalization requirements by the RSUs as well as the vehicles' privacy, we\npresent a novel spatial-provenance framework wherein each vehicle uses Bloom\nfilters to embed their partial location information when forwarding the\npackets. In this framework, the RSUs and the vehicles agree upon fragmenting\nthe coverage area into several smaller regions so that the vehicles can embed\nthe identity of their regions through Bloom filters. Given the probabilistic\nnature of Bloom filters, we derive an analytical expression on the error-rates\nin provenance recovery and then pose an optimization problem to choose the\nunderlying parameters. With the help of extensive simulation results, we show\nthat our method offers near-optimal Bloom filter parameters in learning spatial\nprovenance. Some interesting trade-offs between the communication-overhead,\nspatial privacy of the vehicles and the error rates in provenance recovery are\nalso discussed.",
    "updated" : "2024-02-06T04:44:36Z",
    "published" : "2024-02-06T04:44:36Z",
    "authors" : [
      {
        "name" : "Manish Bansal"
      },
      {
        "name" : "Pramsu Srivastava"
      },
      {
        "name" : "J. Harshan"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.NI",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03688v1",
    "title" : "A Survey of Privacy Threats and Defense in Vertical Federated Learning:\n  From Model Life Cycle Perspective",
    "summary" : "Vertical Federated Learning (VFL) is a federated learning paradigm where\nmultiple participants, who share the same set of samples but hold different\nfeatures, jointly train machine learning models. Although VFL enables\ncollaborative machine learning without sharing raw data, it is still\nsusceptible to various privacy threats. In this paper, we conduct the first\ncomprehensive survey of the state-of-the-art in privacy attacks and defenses in\nVFL. We provide taxonomies for both attacks and defenses, based on their\ncharacterizations, and discuss open challenges and future research directions.\nSpecifically, our discussion is structured around the model's life cycle, by\ndelving into the privacy threats encountered during different stages of machine\nlearning and their corresponding countermeasures. This survey not only serves\nas a resource for the research community but also offers clear guidance and\nactionable insights for practitioners to safeguard data privacy throughout the\nmodel's life cycle.",
    "updated" : "2024-02-06T04:22:44Z",
    "published" : "2024-02-06T04:22:44Z",
    "authors" : [
      {
        "name" : "Lei Yu"
      },
      {
        "name" : "Meng Han"
      },
      {
        "name" : "Yiming Li"
      },
      {
        "name" : "Changting Lin"
      },
      {
        "name" : "Yao Zhang"
      },
      {
        "name" : "Mingyang Zhang"
      },
      {
        "name" : "Yan Liu"
      },
      {
        "name" : "Haiqin Weng"
      },
      {
        "name" : "Yuseok Jeon"
      },
      {
        "name" : "Ka-Ho Chow"
      },
      {
        "name" : "Stacy Patterson"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03612v1",
    "title" : "Privacy risk in GeoData: A survey",
    "summary" : "With the ubiquitous use of location-based services, large-scale\nindividual-level location data has been widely collected through\nlocation-awareness devices. The exposure of location data constitutes a\nsignificant privacy risk to users as it can lead to de-anonymisation, the\ninference of sensitive information, and even physical threats. Geoprivacy\nconcerns arise on the issues of user identity de-anonymisation and location\nexposure. In this survey, we analyse different geomasking techniques that have\nbeen proposed to protect the privacy of individuals in geodata. We present a\ntaxonomy to characterise these techniques along different dimensions, and\nconduct a survey of geomasking techniques. We then highlight shortcomings of\ncurrent techniques and discuss avenues for future research.",
    "updated" : "2024-02-06T00:55:06Z",
    "published" : "2024-02-06T00:55:06Z",
    "authors" : [
      {
        "name" : "Mahrokh Abdollahi Lorestani"
      },
      {
        "name" : "Thilina Ranbaduge"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03582v1",
    "title" : "Matcha: An IDE Plugin for Creating Accurate Privacy Nutrition Labels",
    "summary" : "Apple and Google introduced their versions of privacy nutrition labels to the\nmobile app stores to better inform users of the apps' data practices. However,\nthese labels are self-reported by developers and have been found to contain\nmany inaccuracies due to misunderstandings of the label taxonomy. In this work,\nwe present Matcha, an IDE plugin that uses automated code analysis to help\ndevelopers create accurate Google Play data safety labels. Developers can\nbenefit from Matcha's ability to detect user data accesses and transmissions\nwhile staying in control of the generated label by adding custom Java\nannotations and modifying an auto-generated XML specification. Our evaluation\nwith 12 developers showed that Matcha helped our participants improved the\naccuracy of a label they created with Google's official tool for a real-world\napp they developed. We found that participants preferred Matcha for its\naccuracy benefits. Drawing on Matcha, we discuss general design recommendations\nfor developer tools used to create accurate standardized privacy notices.",
    "updated" : "2024-02-05T23:17:08Z",
    "published" : "2024-02-05T23:17:08Z",
    "authors" : [
      {
        "name" : "Tianshi Li"
      },
      {
        "name" : "Lorrie Faith Cranor"
      },
      {
        "name" : "Yuvraj Agarwal"
      },
      {
        "name" : "Jason I. Hong"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03531v1",
    "title" : "Fairness and Privacy Guarantees in Federated Contextual Bandits",
    "summary" : "This paper considers the contextual multi-armed bandit (CMAB) problem with\nfairness and privacy guarantees in a federated environment. We consider\nmerit-based exposure as the desired fair outcome, which provides exposure to\neach action in proportion to the reward associated. We model the algorithm's\neffectiveness using fairness regret, which captures the difference between fair\noptimal policy and the policy output by the algorithm. Applying fair CMAB\nalgorithm to each agent individually leads to fairness regret linear in the\nnumber of agents. We propose that collaborative -- federated learning can be\nmore effective and provide the algorithm Fed-FairX-LinUCB that also ensures\ndifferential privacy. The primary challenge in extending the existing privacy\nframework is designing the communication protocol for communicating required\ninformation across agents. A naive protocol can either lead to weaker privacy\nguarantees or higher regret. We design a novel communication protocol that\nallows for (i) Sub-linear theoretical bounds on fairness regret for\nFed-FairX-LinUCB and comparable bounds for the private counterpart,\nPriv-FairX-LinUCB (relative to single-agent learning), (ii) Effective use of\nprivacy budget in Priv-FairX-LinUCB. We demonstrate the efficacy of our\nproposed algorithm with extensive simulations-based experiments. We show that\nboth Fed-FairX-LinUCB and Priv-FairX-LinUCB achieve near-optimal fairness\nregret.",
    "updated" : "2024-02-05T21:38:23Z",
    "published" : "2024-02-05T21:38:23Z",
    "authors" : [
      {
        "name" : "Sambhav Solanki"
      },
      {
        "name" : "Shweta Jain"
      },
      {
        "name" : "Sujit Gujar"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.03435v1",
    "title" : "Psychological Assessments with Large Language Models: A Privacy-Focused\n  and Cost-Effective Approach",
    "summary" : "This study explores the use of Large Language Models (LLMs) to analyze text\ncomments from Reddit users, aiming to achieve two primary objectives: firstly,\nto pinpoint critical excerpts that support a predefined psychological\nassessment of suicidal risk; and secondly, to summarize the material to\nsubstantiate the preassigned suicidal risk level. The work is circumscribed to\nthe use of \"open-source\" LLMs that can be run locally, thereby enhancing data\nprivacy. Furthermore, it prioritizes models with low computational\nrequirements, making it accessible to both individuals and institutions\noperating on limited computing budgets. The implemented strategy only relies on\na carefully crafted prompt and a grammar to guide the LLM's text completion.\nDespite its simplicity, the evaluation metrics show outstanding results, making\nit a valuable privacy-focused and cost-effective approach. This work is part of\nthe Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared\ntask.",
    "updated" : "2024-02-05T19:00:02Z",
    "published" : "2024-02-05T19:00:02Z",
    "authors" : [
      {
        "name" : "Sergi Blanco-Cuaresma"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.02672v1",
    "title" : "Estimation of conditional average treatment effects on distributed data:\n  A privacy-preserving approach",
    "summary" : "Estimation of conditional average treatment effects (CATEs) is an important\ntopic in various fields such as medical and social sciences. CATEs can be\nestimated with high accuracy if distributed data across multiple parties can be\ncentralized. However, it is difficult to aggregate such data if they contain\nprivacy information. To address this issue, we proposed data collaboration\ndouble machine learning (DC-DML), a method that can estimate CATE models with\nprivacy preservation of distributed data, and evaluated the method through\nnumerical experiments. Our contributions are summarized in the following three\npoints. First, our method enables estimation and testing of semi-parametric\nCATE models without iterative communication on distributed data.\nSemi-parametric or non-parametric CATE models enable estimation and testing\nthat is more robust to model mis-specification than parametric models. However,\nto our knowledge, no communication-efficient method has been proposed for\nestimating and testing semi-parametric or non-parametric CATE models on\ndistributed data. Second, our method enables collaborative estimation between\ndifferent parties as well as multiple time points because the\ndimensionality-reduced intermediate representations can be accumulated. Third,\nour method performed as well or better than other methods in evaluation\nexperiments using synthetic, semi-synthetic and real-world datasets.",
    "updated" : "2024-02-05T02:17:21Z",
    "published" : "2024-02-05T02:17:21Z",
    "authors" : [
      {
        "name" : "Yuji Kawamata"
      },
      {
        "name" : "Ryoki Motai"
      },
      {
        "name" : "Yukihiko Okada"
      },
      {
        "name" : "Akira Imakura"
      },
      {
        "name" : "Tetsuya Sakurai"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.01857v1",
    "title" : "Position Paper: Assessing Robustness, Privacy, and Fairness in Federated\n  Learning Integrated with Foundation Models",
    "summary" : "Federated Learning (FL), while a breakthrough in decentralized machine\nlearning, contends with significant challenges such as limited data\navailability and the variability of computational resources, which can stifle\nthe performance and scalability of the models. The integration of Foundation\nModels (FMs) into FL presents a compelling solution to these issues, with the\npotential to enhance data richness and reduce computational demands through\npre-training and data augmentation. However, this incorporation introduces\nnovel issues in terms of robustness, privacy, and fairness, which have not been\nsufficiently addressed in the existing research. We make a preliminary\ninvestigation into this field by systematically evaluating the implications of\nFM-FL integration across these dimensions. We analyze the trade-offs involved,\nuncover the threats and issues introduced by this integration, and propose a\nset of criteria and strategies for navigating these challenges. Furthermore, we\nidentify potential research directions for advancing this field, laying a\nfoundation for future development in creating reliable, secure, and equitable\nFL systems.",
    "updated" : "2024-02-02T19:26:00Z",
    "published" : "2024-02-02T19:26:00Z",
    "authors" : [
      {
        "name" : "Xi Li"
      },
      {
        "name" : "Jiaqi Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.04840v1",
    "title" : "Efficient Estimation of a Gaussian Mean with Local Differential Privacy",
    "summary" : "In this paper we study the problem of estimating the unknown mean $\\theta$ of\na unit variance Gaussian distribution in a locally differentially private (LDP)\nway. In the high-privacy regime ($\\epsilon\\le 0.67$), we identify the exact\noptimal privacy mechanism that minimizes the variance of the estimator\nasymptotically. It turns out to be the extraordinarily simple sign mechanism\nthat applies randomized response to the sign of $X_i-\\theta$. However, since\nthis optimal mechanism depends on the unknown mean $\\theta$, we employ a\ntwo-stage LDP parameter estimation procedure which requires splitting agents\ninto two groups. The first $n_1$ observations are used to consistently but not\nnecessarily efficiently estimate the parameter $\\theta$ by\n$\\tilde{\\theta}_{n_1}$. Then this estimate is updated by applying the sign\nmechanism with $\\tilde{\\theta}_{n_1}$ instead of $\\theta$ to the remaining\n$n-n_1$ observations, to obtain an LDP and efficient estimator of the unknown\nmean.",
    "updated" : "2024-02-07T13:41:45Z",
    "published" : "2024-02-07T13:41:45Z",
    "authors" : [
      {
        "name" : "Nikita Kalinin"
      },
      {
        "name" : "Lukas Steinberger"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.04489v1",
    "title" : "De-amplifying Bias from Differential Privacy in Language Model\n  Fine-tuning",
    "summary" : "Fairness and privacy are two important values machine learning (ML)\npractitioners often seek to operationalize in models. Fairness aims to reduce\nmodel bias for social/demographic sub-groups. Privacy via differential privacy\n(DP) mechanisms, on the other hand, limits the impact of any individual's\ntraining data on the resulting model. The trade-offs between privacy and\nfairness goals of trustworthy ML pose a challenge to those wishing to address\nboth. We show that DP amplifies gender, racial, and religious bias when\nfine-tuning large language models (LLMs), producing models more biased than\nones fine-tuned without DP. We find the cause of the amplification to be a\ndisparity in convergence of gradients across sub-groups. Through the case of\nbinary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA),\na known method for addressing bias, also mitigates bias amplification by DP. As\na consequence, DP and CDA together can be used to fine-tune models while\nmaintaining both fairness and privacy.",
    "updated" : "2024-02-07T00:30:58Z",
    "published" : "2024-02-07T00:30:58Z",
    "authors" : [
      {
        "name" : "Sanjari Srivastava"
      },
      {
        "name" : "Piotr Mardziel"
      },
      {
        "name" : "Zhikhun Zhang"
      },
      {
        "name" : "Archana Ahlawat"
      },
      {
        "name" : "Anupam Datta"
      },
      {
        "name" : "John C Mitchell"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY",
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.05860v1",
    "title" : "Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic\n  Surgery",
    "summary" : "Deep Neural Networks (DNNs) based semantic segmentation of the robotic\ninstruments and tissues can enhance the precision of surgical activities in\nrobot-assisted surgery. However, in biological learning, DNNs cannot learn\nincremental tasks over time and exhibit catastrophic forgetting, which refers\nto the sharp decline in performance on previously learned tasks after learning\na new one. Specifically, when data scarcity is the issue, the model shows a\nrapid drop in performance on previously learned instruments after learning new\ndata with new instruments. The problem becomes worse when it limits releasing\nthe dataset of the old instruments for the old model due to privacy concerns\nand the unavailability of the data for the new or updated version of the\ninstruments for the continual learning model. For this purpose, we develop a\nprivacy-preserving synthetic continual semantic segmentation framework by\nblending and harmonizing (i) open-source old instruments foreground to the\nsynthesized background without revealing real patient data in public and (ii)\nnew instruments foreground to extensively augmented real background. To boost\nthe balanced logit distillation from the old model to the continual learning\nmodel, we design overlapping class-aware temperature normalization (CAT) by\ncontrolling model learning utility. We also introduce multi-scale\nshifted-feature distillation (SD) to maintain long and short-range spatial\nrelationships among the semantic objects where conventional short-range spatial\nfeatures with limited information reduce the power of feature distillation. We\ndemonstrate the effectiveness of our framework on the EndoVis 2017 and 2018\ninstrument segmentation dataset with a generalized continual learning setting.\nCode is available at~\\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}.",
    "updated" : "2024-02-08T17:44:06Z",
    "published" : "2024-02-08T17:44:06Z",
    "authors" : [
      {
        "name" : "Mengya Xu"
      },
      {
        "name" : "Mobarakol Islam"
      },
      {
        "name" : "Long Bai"
      },
      {
        "name" : "Hongliang Ren"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.05690v1",
    "title" : "Overcoming Noise Limitations in QKD with Quantum Privacy Amplification",
    "summary" : "High-quality, distributed quantum entanglement is the distinctive resource\nfor quantum communication and forms the foundation for the unequalled level of\nsecurity that can be assured in quantum key distribution. While the\nentanglement provider does not need to be trusted, the secure key rate drops to\nzero if the entanglement used is too noisy. In this paper, we show\nexperimentally that QPA is able to increase the secure key rate achievable with\nQKD by improving the quality of distributed entanglement, thus increasing the\nquantum advantage in QKD. Beyond that, we show that QPA enables key generation\nat noise levels that previously prevented key generation. These remarkable\nresults were only made possible by the efficient implementation exploiting\nhyperentanglement in the polarisation and energy-time degrees of freedom. We\nprovide a detailed characterisation of the gain in secure key rate achieved in\nour proof-of-principle experiment at different noise levels. The results are\nparamount for the implementation of a global quantum network linking quantum\nprocessors and ensuring future-proof data security.",
    "updated" : "2024-02-08T14:07:36Z",
    "published" : "2024-02-08T14:07:36Z",
    "authors" : [
      {
        "name" : "Philipp Sohr"
      },
      {
        "name" : "Sebastian Ecker"
      },
      {
        "name" : "Lukas Bulla"
      },
      {
        "name" : "Martin Bohmann"
      },
      {
        "name" : "Rupert Ursin"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.05453v1",
    "title" : "Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss",
    "summary" : "Machine learning models are susceptible to membership inference attacks\n(MIAs), which aim to infer whether a sample is in the training set. Existing\nwork utilizes gradient ascent to enlarge the loss variance of training data,\nalleviating the privacy risk. However, optimizing toward a reverse direction\nmay cause the model parameters to oscillate near local minima, leading to\ninstability and suboptimal performance. In this work, we propose a novel method\n-- Convex-Concave Loss, which enables a high variance of training loss\ndistribution by gradient descent. Our method is motivated by the theoretical\nanalysis that convex losses tend to decrease the loss variance during training.\nThus, our key idea behind CCL is to reduce the convexity of loss functions with\na concave term. Trained with CCL, neural networks produce losses with high\nvariance for training data, reinforcing the defense against MIAs. Extensive\nexperiments demonstrate the superiority of CCL, achieving state-of-the-art\nbalance in the privacy-utility trade-off.",
    "updated" : "2024-02-08T07:14:17Z",
    "published" : "2024-02-08T07:14:17Z",
    "authors" : [
      {
        "name" : "Zhenlong Liu"
      },
      {
        "name" : "Lei Feng"
      },
      {
        "name" : "Huiping Zhuang"
      },
      {
        "name" : "Xiaofeng Cao"
      },
      {
        "name" : "Hongxin Wei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.06137v1",
    "title" : "On the Privacy of Selection Mechanisms with Gaussian Noise",
    "summary" : "Report Noisy Max and Above Threshold are two classical differentially private\n(DP) selection mechanisms. Their output is obtained by adding noise to a\nsequence of low-sensitivity queries and reporting the identity of the query\nwhose (noisy) answer satisfies a certain condition. Pure DP guarantees for\nthese mechanisms are easy to obtain when Laplace noise is added to the queries.\nOn the other hand, when instantiated using Gaussian noise, standard analyses\nonly yield approximate DP guarantees despite the fact that the outputs of these\nmechanisms lie in a discrete space. In this work, we revisit the analysis of\nReport Noisy Max and Above Threshold with Gaussian noise and show that, under\nthe additional assumption that the underlying queries are bounded, it is\npossible to provide pure ex-ante DP bounds for Report Noisy Max and pure\nex-post DP bounds for Above Threshold. The resulting bounds are tight and\ndepend on closed-form expressions that can be numerically evaluated using\nstandard methods. Empirically we find these lead to tighter privacy accounting\nin the high privacy, low data regime. Further, we propose a simple privacy\nfilter for composing pure ex-post DP guarantees, and use it to derive a fully\nadaptive Gaussian Sparse Vector Technique mechanism. Finally, we provide\nexperiments on mobility and energy consumption datasets demonstrating that our\nSparse Vector Technique is practically competitive with previous approaches and\nrequires less hyper-parameter tuning.",
    "updated" : "2024-02-09T02:11:25Z",
    "published" : "2024-02-09T02:11:25Z",
    "authors" : [
      {
        "name" : "Jonathan Lebensold"
      },
      {
        "name" : "Doina Precup"
      },
      {
        "name" : "Borja Balle"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.07687v1",
    "title" : "Privacy-Preserving Gaze Data Streaming in Immersive Interactive Virtual\n  Reality: Robustness and User Experience",
    "summary" : "Eye tracking is routinely being incorporated into virtual reality (VR)\nsystems. Prior research has shown that eye tracking data can be used for\nre-identification attacks. The state of our knowledge about currently existing\nprivacy mechanisms is limited to privacy-utility trade-off curves based on\ndata-centric metrics of utility, such as prediction error, and black-box threat\nmodels. We propose that for interactive VR applications, it is essential to\nconsider user-centric notions of utility and a variety of threat models. We\ndevelop a methodology to evaluate real-time privacy mechanisms for interactive\nVR applications that incorporate subjective user experience and task\nperformance metrics. We evaluate selected privacy mechanisms using this\nmethodology and find that re-identification accuracy can be decreased to as low\nas 14% while maintaining a high usability score and reasonable task\nperformance. Finally, we elucidate three threat scenarios (black-box, black-box\nwith exemplars, and white-box) and assess how well the different privacy\nmechanisms hold up to these adversarial scenarios. This work advances the state\nof the art in VR privacy by providing a methodology for end-to-end assessment\nof the risk of re-identification attacks and potential mitigating solutions.",
    "updated" : "2024-02-12T14:53:12Z",
    "published" : "2024-02-12T14:53:12Z",
    "authors" : [
      {
        "name" : "Ethan Wilson"
      },
      {
        "name" : "Azim Ibragimov"
      },
      {
        "name" : "Michael J. Proulx"
      },
      {
        "name" : "Sai Deep Tetali"
      },
      {
        "name" : "Kevin Butler"
      },
      {
        "name" : "Eakta Jain"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.07584v1",
    "title" : "Privacy-Optimized Randomized Response for Sharing Multi-Attribute Data",
    "summary" : "With the increasing amount of data in society, privacy concerns in data\nsharing have become widely recognized. Particularly, protecting personal\nattribute information is essential for a wide range of aims from crowdsourcing\nto realizing personalized medicine. Although various differentially private\nmethods based on randomized response have been proposed for single attribute\ninformation or specific analysis purposes such as frequency estimation, there\nis a lack of studies on the mechanism for sharing individuals' multiple\ncategorical information itself. The existing randomized response for sharing\nmulti-attribute data uses the Kronecker product to perturb each attribute\ninformation in turn according to the respective privacy level but achieves only\na weak privacy level for the entire dataset. Therefore, in this study, we\npropose a privacy-optimized randomized response that guarantees the strongest\nprivacy in sharing multi-attribute data. Furthermore, we present an efficient\nheuristic algorithm for constructing a near-optimal mechanism. The time\ncomplexity of our algorithm is O(k^2), where k is the number of attributes, and\nit can be performed in about 1 second even for large datasets with k = 1,000.\nThe experimental results demonstrate that both of our methods provide\nsignificantly stronger privacy guarantees for the entire dataset than the\nexisting method. In addition, we show an analysis example using genome\nstatistics to confirm that our methods can achieve less than half the output\nerror compared with that of the existing method. Overall, this study is an\nimportant step toward trustworthy sharing and analysis of multi-attribute data.\nThe Python implementation of our experiments and supplemental results are\navailable at https://github.com/ay0408/Optimized-RR.",
    "updated" : "2024-02-12T11:34:42Z",
    "published" : "2024-02-12T11:34:42Z",
    "authors" : [
      {
        "name" : "Akito Yamamoto"
      },
      {
        "name" : "Tetsuo Shibuya"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.07367v1",
    "title" : "Utilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code",
    "summary" : "Mini-applications, commonly referred to as mini-apps, are compact software\nprograms embedded within larger applications or platforms, offering targeted\nfunctionality without the need for separate installations. Typically web-based\nor cloud-hosted, these mini-apps streamline user experiences by providing\nfocused services accessible through web browsers or mobile apps. Their\nsimplicity, speed, and integration capabilities make them valuable additions to\nmessaging platforms, social media networks, e-commerce sites, and various\ndigital environments. WeChat Mini Programs, a prominent feature of China's\nleading messaging app, exemplify this trend, offering users a seamless array of\nservices without additional downloads. Leveraging WeChat's extensive user base\nand payment infrastructure, Mini Programs facilitate efficient transactions and\nbridge online and offline experiences, shaping China's digital landscape\nsignificantly. This paper investigates the potential of employing Large\nLanguage Models (LLMs) to detect privacy breaches within WeChat Mini Programs.\nGiven the widespread use of Mini Programs and growing concerns about data\nprivacy, this research seeks to determine if LLMs can effectively identify\ninstances of privacy leakage within this ecosystem. Through meticulous analysis\nand experimentation, we aim to highlight the efficacy of LLMs in safeguarding\nuser privacy and security within the WeChat Mini Program environment, thereby\ncontributing to a more secure digital landscape.",
    "updated" : "2024-02-12T01:55:40Z",
    "published" : "2024-02-12T01:55:40Z",
    "authors" : [
      {
        "name" : "Liming Jiang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.07180v1",
    "title" : "MAGNETO: Edge AI for Human Activity Recognition -- Privacy and\n  Personalization",
    "summary" : "Human activity recognition (HAR) is a well-established field, significantly\nadvanced by modern machine learning (ML) techniques. While companies have\nsuccessfully integrated HAR into consumer products, they typically rely on a\npredefined activity set, which limits personalizations at the user level (edge\ndevices). Despite advancements in Incremental Learning for updating models with\nnew data, this often occurs on the Cloud, necessitating regular data transfers\nbetween cloud and edge devices, thus leading to data privacy issues. In this\npaper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the\nCloud to the Edge. MAGNETO allows incremental human activity learning directly\non the Edge devices, without any data exchange with the Cloud. This enables\nstrong privacy guarantees, low processing latency, and a high degree of\npersonalization for users. In particular, we demonstrate MAGNETO in an Android\ndevice, validating the whole pipeline from data collection to result\nvisualization.",
    "updated" : "2024-02-11T12:29:16Z",
    "published" : "2024-02-11T12:29:16Z",
    "authors" : [
      {
        "name" : "Jingwei Zuo"
      },
      {
        "name" : "George Arvanitakis"
      },
      {
        "name" : "Mthandazo Ndhlovu"
      },
      {
        "name" : "Hakim Hacid"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.07002v1",
    "title" : "Clients Collaborate: Flexible Differentially Private Federated Learning\n  with Guaranteed Improvement of Utility-Privacy Trade-off",
    "summary" : "To defend against privacy leakage of user data, differential privacy is\nwidely used in federated learning, but it is not free. The addition of noise\nrandomly disrupts the semantic integrity of the model and this disturbance\naccumulates with increased communication rounds. In this paper, we introduce a\nnovel federated learning framework with rigorous privacy guarantees, named\nFedCEO, designed to strike a trade-off between model utility and user privacy\nby letting clients ''Collaborate with Each Other''. Specifically, we perform\nefficient tensor low-rank proximal optimization on stacked local model\nparameters at the server, demonstrating its capability to flexibly truncate\nhigh-frequency components in spectral space. This implies that our FedCEO can\neffectively recover the disrupted semantic information by smoothing the global\nsemantic space for different privacy settings and continuous training\nprocesses. Moreover, we improve the SOTA utility-privacy trade-off bound by an\norder of $\\sqrt{d}$, where $d$ is the input dimension. We illustrate our\ntheoretical results with experiments on representative image datasets. It\nobserves significant performance improvements and strict privacy guarantees\nunder different privacy settings.",
    "updated" : "2024-02-10T17:39:34Z",
    "published" : "2024-02-10T17:39:34Z",
    "authors" : [
      {
        "name" : "Yuecheng Li"
      },
      {
        "name" : "Tong Wang"
      },
      {
        "name" : "Chuan Chen"
      },
      {
        "name" : "Jian Lou"
      },
      {
        "name" : "Bin Chen"
      },
      {
        "name" : "Lei Yang"
      },
      {
        "name" : "Zibin Zheng"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.06701v1",
    "title" : "Privacy Profiles for Private Selection",
    "summary" : "Private selection mechanisms (e.g., Report Noisy Max, Sparse Vector) are\nfundamental primitives of differentially private (DP) data analysis with wide\napplications to private query release, voting, and hyperparameter tuning.\nRecent work (Liu and Talwar, 2019; Papernot and Steinke, 2022) has made\nsignificant progress in both generalizing private selection mechanisms and\ntightening their privacy analysis using modern numerical privacy accounting\ntools, e.g., R\\'enyi DP. But R\\'enyi DP is known to be lossy when\n$(\\epsilon,\\delta)$-DP is ultimately needed, and there is a trend to close the\ngap by directly handling privacy profiles, i.e., $\\delta$ as a function of\n$\\epsilon$ or its equivalent dual form known as $f$-DPs. In this paper, we work\nout an easy-to-use recipe that bounds the privacy profiles of ReportNoisyMax\nand PrivateTuning using the privacy profiles of the base algorithms they\ncorral. Numerically, our approach improves over the RDP-based accounting in all\nregimes of interest and leads to substantial benefits in end-to-end private\nlearning experiments. Our analysis also suggests new distributions, e.g.,\nbinomial distribution for randomizing the number of rounds that leads to more\nsubstantial improvements in certain regimes.",
    "updated" : "2024-02-09T08:31:46Z",
    "published" : "2024-02-09T08:31:46Z",
    "authors" : [
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "Rachel Redberg"
      },
      {
        "name" : "Yu-Xiang Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.06674v1",
    "title" : "Understanding Practical Membership Privacy of Deep Learning",
    "summary" : "We apply a state-of-the-art membership inference attack (MIA) to\nsystematically test the practical privacy vulnerability of fine-tuning large\nimage classification models.We focus on understanding the properties of data\nsets and samples that make them vulnerable to membership inference. In terms of\ndata set properties, we find a strong power law dependence between the number\nof examples per class in the data and the MIA vulnerability, as measured by\ntrue positive rate of the attack at a low false positive rate. For an\nindividual sample, large gradients at the end of training are strongly\ncorrelated with MIA vulnerability.",
    "updated" : "2024-02-07T14:23:01Z",
    "published" : "2024-02-07T14:23:01Z",
    "authors" : [
      {
        "name" : "Marlon Tobaben"
      },
      {
        "name" : "Gauri Pradhan"
      },
      {
        "name" : "Yuan He"
      },
      {
        "name" : "Joonas Jälkö"
      },
      {
        "name" : "Antti Honkela"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.08655v1",
    "title" : "Assessing the Privacy Risk of Cross-Platform Identity Linkage using Eye\n  Movement Biometrics",
    "summary" : "The recent emergence of ubiquitous, multi-platform eye tracking has raised\nuser privacy concerns over re-identification across platforms, where a person\nis re-identified across multiple eye tracking-enabled platforms using\npersonally identifying information that is implicitly expressed through their\neye movement. We present an empirical investigation quantifying a modern eye\nmovement biometric model's ability to link subject identities across three\ndifferent eye tracking devices using eye movement signals from each device. We\nshow that a state-of-the art eye movement biometrics model demonstrates\nabove-chance levels of biometric performance (34.99% equal error rate, 15%\nrank-1 identification rate) when linking user identities across one pair of\ndevices, but not for the other. Considering these findings, we also discuss the\nimpact that eye tracking signal quality has on the model's ability to\nmeaningfully associate a subject's identity between two substantially different\neye tracking devices. Our investigation advances a fundamental understanding of\nthe privacy risks for identity linkage across platforms by employing both\nquantitative and qualitative measures of biometric performance, including a\nvisualization of the model's ability to distinguish genuine and imposter\nauthentication attempts across platforms.",
    "updated" : "2024-02-13T18:37:23Z",
    "published" : "2024-02-13T18:37:23Z",
    "authors" : [
      {
        "name" : "Samantha Aziz"
      },
      {
        "name" : "Oleg Komogortsev"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.08227v1",
    "title" : "Privacy-Preserving Language Model Inference with Instance Obfuscation",
    "summary" : "Language Models as a Service (LMaaS) offers convenient access for developers\nand researchers to perform inference using pre-trained language models.\nNonetheless, the input data and the inference results containing private\ninformation are exposed as plaintext during the service call, leading to\nprivacy issues. Recent studies have started tackling the privacy issue by\ntransforming input data into privacy-preserving representation from the\nuser-end with the techniques such as noise addition and content perturbation,\nwhile the exploration of inference result protection, namely decision privacy,\nis still a blank page. In order to maintain the black-box manner of LMaaS,\nconducting data privacy protection, especially for the decision, is a\nchallenging task because the process has to be seamless to the models and\naccompanied by limited communication and computation overhead. We thus propose\nInstance-Obfuscated Inference (IOI) method, which focuses on addressing the\ndecision privacy issue of natural language understanding tasks in their\ncomplete life-cycle. Besides, we conduct comprehensive experiments to evaluate\nthe performance as well as the privacy-protection strength of the proposed\nmethod on various benchmarking tasks.",
    "updated" : "2024-02-13T05:36:54Z",
    "published" : "2024-02-13T05:36:54Z",
    "authors" : [
      {
        "name" : "Yixiang Yao"
      },
      {
        "name" : "Fei Wang"
      },
      {
        "name" : "Srivatsan Ravi"
      },
      {
        "name" : "Muhao Chen"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.08223v1",
    "title" : "The Limits of Price Discrimination Under Privacy Constraints",
    "summary" : "We consider a producer's problem of selling a product to a continuum of\nprivacy-conscious consumers, where the producer can implement third-degree\nprice discrimination, offering different prices to different market segments.\nIn the absence of privacy constraints, Bergemann, Brooks, and Morris [2015]\ncharacterize the set of all possible consumer-producer utilities, showing that\nit is a triangle. We consider a privacy mechanism that provides a degree of\nprotection by probabilistically masking each market segment, and we establish\nthat the resultant set of all consumer-producer utilities forms a convex\npolygon, characterized explicitly as a linear mapping of a certain\nhigh-dimensional convex polytope into $\\mathbb{R}^2$. This characterization\nenables us to investigate the impact of the privacy mechanism on both producer\nand consumer utilities. In particular, we establish that the privacy constraint\nalways hurts the producer by reducing both the maximum and minimum utility\nachievable. From the consumer's perspective, although the privacy mechanism\nensures an increase in the minimum utility compared to the non-private\nscenario, interestingly, it may reduce the maximum utility. Finally, we\ndemonstrate that increasing the privacy level does not necessarily intensify\nthese effects. For instance, the maximum utility for the producer or the\nminimum utility for the consumer may exhibit nonmonotonic behavior in response\nto an increase of the privacy level.",
    "updated" : "2024-02-13T05:28:38Z",
    "published" : "2024-02-13T05:28:38Z",
    "authors" : [
      {
        "name" : "Alireza Fallah"
      },
      {
        "name" : "Michael I. Jordan"
      },
      {
        "name" : "Ali Makhdoumi"
      },
      {
        "name" : "Azarakhsh Malekian"
      }
    ],
    "categories" : [
      "econ.TH",
      "cs.GT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.08156v1",
    "title" : "Group Decision-Making among Privacy-Aware Agents",
    "summary" : "How can individuals exchange information to learn from each other despite\ntheir privacy needs and security concerns? For example, consider individuals\ndeliberating a contentious topic and being concerned about divulging their\nprivate experiences. Preserving individual privacy and enabling efficient\nsocial learning are both important desiderata but seem fundamentally at odds\nwith each other and very hard to reconcile. We do so by controlling information\nleakage using rigorous statistical guarantees that are based on differential\nprivacy (DP). Our agents use log-linear rules to update their beliefs after\ncommunicating with their neighbors. Adding DP randomization noise to beliefs\nprovides communicating agents with plausible deniability with regard to their\nprivate information and their network neighborhoods. We consider two learning\nenvironments one for distributed maximum-likelihood estimation given a finite\nnumber of private signals and another for online learning from an infinite,\nintermittent signal stream. Noisy information aggregation in the finite case\nleads to interesting tradeoffs between rejecting low-quality states and making\nsure all high-quality states are accepted in the algorithm output. Our results\nflesh out the nature of the trade-offs in both cases between the quality of the\ngroup decision outcomes, learning accuracy, communication cost, and the level\nof privacy protections that the agents are afforded.",
    "updated" : "2024-02-13T01:38:01Z",
    "published" : "2024-02-13T01:38:01Z",
    "authors" : [
      {
        "name" : "Marios Papachristou"
      },
      {
        "name" : "M. Amin Rahimian"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MA",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.09316v1",
    "title" : "Only My Model On My Data: A Privacy Preserving Approach Protecting one\n  Model and Deceiving Unauthorized Black-Box Models",
    "summary" : "Deep neural networks are extensively applied to real-world tasks, such as\nface recognition and medical image classification, where privacy and data\nprotection are critical. Image data, if not protected, can be exploited to\ninfer personal or contextual information. Existing privacy preservation\nmethods, like encryption, generate perturbed images that are unrecognizable to\neven humans. Adversarial attack approaches prohibit automated inference even\nfor authorized stakeholders, limiting practical incentives for commercial and\nwidespread adaptation. This pioneering study tackles an unexplored practical\nprivacy preservation use case by generating human-perceivable images that\nmaintain accurate inference by an authorized model while evading other\nunauthorized black-box models of similar or dissimilar objectives, and\naddresses the previous research gaps. The datasets employed are ImageNet, for\nimage classification, Celeba-HQ dataset, for identity classification, and\nAffectNet, for emotion classification. Our results show that the generated\nimages can successfully maintain the accuracy of a protected model and degrade\nthe average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and\n55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.",
    "updated" : "2024-02-14T17:11:52Z",
    "published" : "2024-02-14T17:11:52Z",
    "authors" : [
      {
        "name" : "Weiheng Chai"
      },
      {
        "name" : "Brian Testa"
      },
      {
        "name" : "Huantao Ren"
      },
      {
        "name" : "Asif Salekin"
      },
      {
        "name" : "Senem Velipasalar"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.08956v1",
    "title" : "Seagull: Privacy preserving network verification system",
    "summary" : "The current routing protocol used in the internet backbone is based on manual\nconfiguration, making it susceptible to errors. To mitigate these\nconfiguration-related issues, it becomes imperative to validate the accuracy\nand convergence of the algorithm, ensuring a seamless operation devoid of\nproblems. However, the process of network verification faces challenges related\nto privacy and scalability. This paper addresses these challenges by\nintroducing a novel approach: leveraging privacy-preserving computation,\nspecifically multiparty computation (MPC), to verify the correctness of\nconfigurations in the internet backbone, governed by the BGP protocol. Not only\ndoes our proposed solution effectively address scalability concerns, but it\nalso establishes a robust privacy framework. Through rigorous analysis, we\ndemonstrate that our approach maintains privacy by not disclosing any\ninformation beyond the query result, thus providing a comprehensive and secure\nsolution to the intricacies associated with routing protocol verification in\nlarge-scale networks.",
    "updated" : "2024-02-14T05:56:51Z",
    "published" : "2024-02-14T05:56:51Z",
    "authors" : [
      {
        "name" : "Jaber Daneshamooz"
      },
      {
        "name" : "Melody Yu"
      },
      {
        "name" : "Sucheer Maddury"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.08826v1",
    "title" : "Equilibria of Data Marketplaces with Privacy-Aware Sellers under\n  Endogenous Privacy Costs",
    "summary" : "We study a two-sided online data ecosystem comprised of an online platform,\nusers on the platform, and downstream learners or data buyers. The learners can\nbuy user data on the platform (to run a statistic or machine learning task).\nPotential users decide whether to join by looking at the trade-off between i)\ntheir benefit from joining the platform and interacting with other users and\nii) the privacy costs they incur from sharing their data.\n  First, we introduce a novel modeling element for two-sided data platforms:\nthe privacy costs of the users are endogenous and depend on how much of their\ndata is purchased by the downstream learners. Then, we characterize marketplace\nequilibria in certain simple settings. In particular, we provide a full\ncharacterization in two variants of our model that correspond to different\nutility functions for the users: i) when each user gets a constant benefit for\nparticipating in the platform and ii) when each user's benefit is linearly\nincreasing in the number of other users that participate. In both variants,\nequilibria in our setting are significantly different from equilibria when\nprivacy costs are exogenous and fixed, highlighting the importance of taking\nendogeneity in the privacy costs into account. Finally, we provide simulations\nand semi-synthetic experiments to extend our results to more general\nassumptions. We experiment with different distributions of users' privacy costs\nand different functional forms of the users' utilities for joining the\nplatform.",
    "updated" : "2024-02-13T22:10:57Z",
    "published" : "2024-02-13T22:10:57Z",
    "authors" : [
      {
        "name" : "Diptangshu Sen"
      },
      {
        "name" : "Jingyan Wang"
      },
      {
        "name" : "Juba Ziani"
      }
    ],
    "categories" : [
      "cs.GT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.10145v1",
    "title" : "A chaotic maps-based privacy-preserving distributed deep learning for\n  incomplete and Non-IID datasets",
    "summary" : "Federated Learning is a machine learning approach that enables the training\nof a deep learning model among several participants with sensitive data that\nwish to share their own knowledge without compromising the privacy of their\ndata. In this research, the authors employ a secured Federated Learning method\nwith an additional layer of privacy and proposes a method for addressing the\nnon-IID challenge. Moreover, differential privacy is compared with\nchaotic-based encryption as layer of privacy. The experimental approach\nassesses the performance of the federated deep learning model with differential\nprivacy using both IID and non-IID data. In each experiment, the Federated\nLearning process improves the average performance metrics of the deep neural\nnetwork, even in the case of non-IID data.",
    "updated" : "2024-02-15T17:49:50Z",
    "published" : "2024-02-15T17:49:50Z",
    "authors" : [
      {
        "name" : "Irina Arévalo"
      },
      {
        "name" : "Jose L. Salmeron"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.10102v1",
    "title" : "A privacy-preserving, distributed and cooperative FCM-based learning\n  approach for Cancer Research",
    "summary" : "Distributed Artificial Intelligence is attracting interest day by day. In\nthis paper, the authors introduce an innovative methodology for distributed\nlearning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a\nprivacy-preserving way. The authors design a training scheme for collaborative\nFCM learning that offers data privacy compliant with the current regulation.\nThis method is applied to a cancer detection problem, proving that the\nperformance of the model is improved by the Federated Learning process, and\nobtaining similar results to the ones that can be found in the literature.",
    "updated" : "2024-02-15T16:56:25Z",
    "published" : "2024-02-15T16:56:25Z",
    "authors" : [
      {
        "name" : "Jose L. Salmeron"
      },
      {
        "name" : "Irina Arévalo"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.10065v1",
    "title" : "How Much Does Each Datapoint Leak Your Privacy? Quantifying the\n  Per-datum Membership Leakage",
    "summary" : "We study the per-datum Membership Inference Attacks (MIAs), where an attacker\naims to infer whether a fixed target datum has been included in the input\ndataset of an algorithm and thus, violates privacy. First, we define the\nmembership leakage of a datum as the advantage of the optimal adversary\ntargeting to identify it. Then, we quantify the per-datum membership leakage\nfor the empirical mean, and show that it depends on the Mahalanobis distance\nbetween the target datum and the data-generating distribution. We further\nassess the effect of two privacy defences, i.e. adding Gaussian noise and\nsub-sampling. We quantify exactly how both of them decrease the per-datum\nmembership leakage. Our analysis builds on a novel proof technique that\ncombines an Edgeworth expansion of the likelihood ratio test and a\nLindeberg-Feller central limit theorem. Our analysis connects the existing\nlikelihood ratio and scalar product attacks, and also justifies different\ncanary selection strategies used in the privacy auditing literature. Finally,\nour experiments demonstrate the impacts of the leakage score, the sub-sampling\nratio and the noise scale on the per-datum membership leakage as indicated by\nthe theory.",
    "updated" : "2024-02-15T16:30:55Z",
    "published" : "2024-02-15T16:30:55Z",
    "authors" : [
      {
        "name" : "Achraf Azize"
      },
      {
        "name" : "Debabrota Basu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.10001v1",
    "title" : "Privacy Attacks in Decentralized Learning",
    "summary" : "Decentralized Gradient Descent (D-GD) allows a set of users to perform\ncollaborative learning without sharing their data by iteratively averaging\nlocal model updates with their neighbors in a network graph. The absence of\ndirect communication between non-neighbor nodes might lead to the belief that\nusers cannot infer precise information about the data of others. In this work,\nwe demonstrate the opposite, by proposing the first attack against D-GD that\nenables a user (or set of users) to reconstruct the private data of other users\noutside their immediate neighborhood. Our approach is based on a reconstruction\nattack against the gossip averaging protocol, which we then extend to handle\nthe additional challenges raised by D-GD. We validate the effectiveness of our\nattack on real graphs and datasets, showing that the number of users\ncompromised by a single or a handful of attackers is often surprisingly large.\nWe empirically investigate some of the factors that affect the performance of\nthe attack, namely the graph topology, the number of attackers, and their\nposition in the graph.",
    "updated" : "2024-02-15T15:06:33Z",
    "published" : "2024-02-15T15:06:33Z",
    "authors" : [
      {
        "name" : "Abdellah El Mrini"
      },
      {
        "name" : "Edwige Cyffers"
      },
      {
        "name" : "Aurélien Bellet"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.09716v1",
    "title" : "User Privacy Harms and Risks in Conversational AI: A Proposed Framework",
    "summary" : "This study presents a unique framework that applies and extends Solove\n(2006)'s taxonomy to address privacy concerns in interactions with text-based\nAI chatbots. As chatbot prevalence grows, concerns about user privacy have\nheightened. While existing literature highlights design elements compromising\nprivacy, a comprehensive framework is lacking. Through semi-structured\ninterviews with 13 participants interacting with two AI chatbots, this study\nidentifies 9 privacy harms and 9 privacy risks in text-based interactions.\nUsing a grounded theory approach for interview and chatlog analysis, the\nframework examines privacy implications at various interaction stages. The aim\nis to offer developers, policymakers, and researchers a tool for responsible\nand secure implementation of conversational AI, filling the existing gap in\naddressing privacy issues associated with text-based AI chatbots.",
    "updated" : "2024-02-15T05:21:58Z",
    "published" : "2024-02-15T05:21:58Z",
    "authors" : [
      {
        "name" : "Ece Gumusel"
      },
      {
        "name" : "Kyrie Zhixuan Zhou"
      },
      {
        "name" : "Madelyn Rose Sanfilippo"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.09715v1",
    "title" : "DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated\n  Learning as a Service",
    "summary" : "Federated learning (FL) has emerged as a prevalent distributed machine\nlearning scheme that enables collaborative model training without aggregating\nraw data. Cloud service providers further embrace Federated Learning as a\nService (FLaaS), allowing data analysts to execute their FL training pipelines\nover differentially-protected data. Due to the intrinsic properties of\ndifferential privacy, the enforced privacy level on data blocks can be viewed\nas a privacy budget that requires careful scheduling to cater to diverse\ntraining pipelines. Existing privacy budget scheduling studies prioritize\neither efficiency or fairness individually. In this paper, we propose\nDPBalance, a novel privacy budget scheduling mechanism that jointly optimizes\nboth efficiency and fairness. We first develop a comprehensive utility function\nincorporating data analyst-level dominant shares and FL-specific performance\nmetrics. A sequential allocation mechanism is then designed using the Lagrange\nmultiplier method and effective greedy heuristics. We theoretically prove that\nDPBalance satisfies Pareto Efficiency, Sharing Incentive, Envy-Freeness, and\nWeak Strategy Proofness. We also theoretically prove the existence of a\nfairness-efficiency tradeoff in privacy budgeting. Extensive experiments\ndemonstrate that DPBalance outperforms state-of-the-art solutions, achieving an\naverage efficiency improvement of $1.44\\times \\sim 3.49 \\times$, and an average\nfairness improvement of $1.37\\times \\sim 24.32 \\times$.",
    "updated" : "2024-02-15T05:19:53Z",
    "published" : "2024-02-15T05:19:53Z",
    "authors" : [
      {
        "name" : "Yu Liu"
      },
      {
        "name" : "Zibo Wang"
      },
      {
        "name" : "Yifei Zhu"
      },
      {
        "name" : "Chen Chen"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.09710v1",
    "title" : "Preserving Data Privacy for ML-driven Applications in Open Radio Access\n  Networks",
    "summary" : "Deep learning offers a promising solution to improve spectrum access\ntechniques by utilizing data-driven approaches to manage and share limited\nspectrum resources for emerging applications. For several of these\napplications, the sensitive wireless data (such as spectrograms) are stored in\na shared database or multistakeholder cloud environment and are therefore prone\nto privacy leaks. This paper aims to address such privacy concerns by examining\nthe representative case study of shared database scenarios in 5G Open Radio\nAccess Network (O-RAN) networks where we have a shared database within the\nnear-real-time (near-RT) RAN intelligent controller. We focus on securing the\ndata that can be used by machine learning (ML) models for spectrum sharing and\ninterference mitigation applications without compromising the model and network\nperformances. The underlying idea is to leverage a (i) Shuffling-based\nlearnable encryption technique to encrypt the data, following which, (ii)\nemploy a custom Vision transformer (ViT) as the trained ML model that is\ncapable of performing accurate inferences on such encrypted data. The paper\noffers a thorough analysis and comparisons with analogous convolutional neural\nnetworks (CNN) as well as deeper architectures (such as ResNet-50) as\nbaselines. Our experiments showcase that the proposed approach significantly\noutperforms the baseline CNN with an improvement of 24.5% and 23.9% for the\npercent accuracy and F1-Score respectively when operated on encrypted data.\nThough deeper ResNet-50 architecture is obtained as a slightly more accurate\nmodel, with an increase of 4.4%, the proposed approach boasts a reduction of\nparameters by 99.32%, and thus, offers a much-improved prediction time by\nnearly 60%.",
    "updated" : "2024-02-15T05:06:53Z",
    "published" : "2024-02-15T05:06:53Z",
    "authors" : [
      {
        "name" : "Pranshav Gajjar"
      },
      {
        "name" : "Azuka Chiejina"
      },
      {
        "name" : "Vijay K. Shah"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.09621v1",
    "title" : "Schnorr Approval-Based Secure and Privacy-Preserving IoV Data\n  Aggregation",
    "summary" : "Secure and privacy-preserving data aggregation in the Internet of Vehicles\n(IoV) continues to be a focal point of interest in both the industry and\nacademia. Aiming at tackling the challenges and solving the remaining\nlimitations of existing works, this paper introduces a novel Schnorr\napproval-based IoV data aggregation framework based on a two-layered\narchitecture. In this framework, a server can aggregate the IoV data from\nclusters without inferring the raw data, real identity and trajectories of\nvehicles. Notably, we avoid incorporating the widely-accepted techniques such\nas homomorphic encryption and digital pseudonym to avoid introducing high\ncomputation cost to vehicles. We propose a novel concept, data approval, based\non the Schnorr signature scheme. With the approval, the fake data injection\nattack carried out by a cluster head can be defended against. The separation of\nliability is achieved as well. The evaluation shows that the framework is\nsecure and lightweight for vehicles in terms of the computation and\ncommunication costs.",
    "updated" : "2024-02-14T23:40:36Z",
    "published" : "2024-02-14T23:40:36Z",
    "authors" : [
      {
        "name" : "Rui Liu"
      },
      {
        "name" : "Jianping Pan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.09611v1",
    "title" : "Towards Privacy-Aware Sign Language Translation at Scale",
    "summary" : "A major impediment to the advancement of sign language translation (SLT) is\ndata scarcity. Much of the sign language data currently available on the web\ncannot be used for training supervised models due to the lack of aligned\ncaptions. Furthermore, scaling SLT using large-scale web-scraped datasets bears\nprivacy risks due to the presence of biometric information, which the\nresponsible development of SLT technologies should account for. In this work,\nwe propose a two-stage framework for privacy-aware SLT at scale that addresses\nboth of these issues. We introduce SSVP-SLT, which leverages self-supervised\nvideo pretraining on anonymized and unannotated videos, followed by supervised\nSLT finetuning on a curated parallel dataset. SSVP-SLT achieves\nstate-of-the-art finetuned and zero-shot gloss-free SLT performance on the\nHow2Sign dataset, outperforming the strongest respective baselines by over 3\nBLEU-4. Based on controlled experiments, we further discuss the advantages and\nlimitations of self-supervised pretraining and anonymization via facial\nobfuscation for SLT.",
    "updated" : "2024-02-14T22:57:03Z",
    "published" : "2024-02-14T22:57:03Z",
    "authors" : [
      {
        "name" : "Phillip Rust"
      },
      {
        "name" : "Bowen Shi"
      },
      {
        "name" : "Skyler Wang"
      },
      {
        "name" : "Necati Cihan Camgöz"
      },
      {
        "name" : "Jean Maillard"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.09540v1",
    "title" : "Why Does Differential Privacy with Large Epsilon Defend Against\n  Practical Membership Inference Attacks?",
    "summary" : "For small privacy parameter $\\epsilon$, $\\epsilon$-differential privacy (DP)\nprovides a strong worst-case guarantee that no membership inference attack\n(MIA) can succeed at determining whether a person's data was used to train a\nmachine learning model. The guarantee of DP is worst-case because: a) it holds\neven if the attacker already knows the records of all but one person in the\ndata set; and b) it holds uniformly over all data sets. In practical\napplications, such a worst-case guarantee may be overkill: practical attackers\nmay lack exact knowledge of (nearly all of) the private data, and our data set\nmight be easier to defend, in some sense, than the worst-case data set. Such\nconsiderations have motivated the industrial deployment of DP models with large\nprivacy parameter (e.g. $\\epsilon \\geq 7$), and it has been observed\nempirically that DP with large $\\epsilon$ can successfully defend against\nstate-of-the-art MIAs. Existing DP theory cannot explain these empirical\nfindings: e.g., the theoretical privacy guarantees of $\\epsilon \\geq 7$ are\nessentially vacuous. In this paper, we aim to close this gap between theory and\npractice and understand why a large DP parameter can prevent practical MIAs. To\ntackle this problem, we propose a new privacy notion called practical\nmembership privacy (PMP). PMP models a practical attacker's uncertainty about\nthe contents of the private data. The PMP parameter has a natural\ninterpretation in terms of the success rate of a practical MIA on a given data\nset. We quantitatively analyze the PMP parameter of two fundamental DP\nmechanisms: the exponential mechanism and Gaussian mechanism. Our analysis\nreveals that a large DP parameter often translates into a much smaller PMP\nparameter, which guarantees strong privacy against practical MIAs. Using our\nfindings, we offer principled guidance for practitioners in choosing the DP\nparameter.",
    "updated" : "2024-02-14T19:31:45Z",
    "published" : "2024-02-14T19:31:45Z",
    "authors" : [
      {
        "name" : "Andrew Lowy"
      },
      {
        "name" : "Zhuohang Li"
      },
      {
        "name" : "Jing Liu"
      },
      {
        "name" : "Toshiaki Koike-Akino"
      },
      {
        "name" : "Kieran Parsons"
      },
      {
        "name" : "Ye Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "68P27"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.09477v1",
    "title" : "PANORAMIA: Privacy Auditing of Machine Learning Models without\n  Retraining",
    "summary" : "We introduce a privacy auditing scheme for ML models that relies on\nmembership inference attacks using generated data as \"non-members\". This\nscheme, which we call PANORAMIA, quantifies the privacy leakage for large-scale\nML models without control of the training process or model re-training and only\nrequires access to a subset of the training data. To demonstrate its\napplicability, we evaluate our auditing scheme across multiple ML domains,\nranging from image and tabular data classification to large-scale language\nmodels.",
    "updated" : "2024-02-12T22:56:07Z",
    "published" : "2024-02-12T22:56:07Z",
    "authors" : [
      {
        "name" : "Mishaal Kazmi"
      },
      {
        "name" : "Hadrien Lautraite"
      },
      {
        "name" : "Alireza Akbari"
      },
      {
        "name" : "Mauricio Soroco"
      },
      {
        "name" : "Qiaoyue Tang"
      },
      {
        "name" : "Tao Wang"
      },
      {
        "name" : "Sébastien Gambs"
      },
      {
        "name" : "Mathias Lécuyer"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.07180v2",
    "title" : "MAGNETO: Edge AI for Human Activity Recognition -- Privacy and\n  Personalization",
    "summary" : "Human activity recognition (HAR) is a well-established field, significantly\nadvanced by modern machine learning (ML) techniques. While companies have\nsuccessfully integrated HAR into consumer products, they typically rely on a\npredefined activity set, which limits personalizations at the user level (edge\ndevices). Despite advancements in Incremental Learning for updating models with\nnew data, this often occurs on the Cloud, necessitating regular data transfers\nbetween cloud and edge devices, thus leading to data privacy issues. In this\npaper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the\nCloud to the Edge. MAGNETO allows incremental human activity learning directly\non the Edge devices, without any data exchange with the Cloud. This enables\nstrong privacy guarantees, low processing latency, and a high degree of\npersonalization for users. In particular, we demonstrate MAGNETO in an Android\ndevice, validating the whole pipeline from data collection to result\nvisualization.",
    "updated" : "2024-02-14T19:59:13Z",
    "published" : "2024-02-11T12:29:16Z",
    "authors" : [
      {
        "name" : "Jingwei Zuo"
      },
      {
        "name" : "George Arvanitakis"
      },
      {
        "name" : "Mthandazo Ndhlovu"
      },
      {
        "name" : "Hakim Hacid"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.09710v1",
    "title" : "Preserving Data Privacy for ML-driven Applications in Open Radio Access\n  Networks",
    "summary" : "Deep learning offers a promising solution to improve spectrum access\ntechniques by utilizing data-driven approaches to manage and share limited\nspectrum resources for emerging applications. For several of these\napplications, the sensitive wireless data (such as spectrograms) are stored in\na shared database or multistakeholder cloud environment and are therefore prone\nto privacy leaks. This paper aims to address such privacy concerns by examining\nthe representative case study of shared database scenarios in 5G Open Radio\nAccess Network (O-RAN) networks where we have a shared database within the\nnear-real-time (near-RT) RAN intelligent controller. We focus on securing the\ndata that can be used by machine learning (ML) models for spectrum sharing and\ninterference mitigation applications without compromising the model and network\nperformances. The underlying idea is to leverage a (i) Shuffling-based\nlearnable encryption technique to encrypt the data, following which, (ii)\nemploy a custom Vision transformer (ViT) as the trained ML model that is\ncapable of performing accurate inferences on such encrypted data. The paper\noffers a thorough analysis and comparisons with analogous convolutional neural\nnetworks (CNN) as well as deeper architectures (such as ResNet-50) as\nbaselines. Our experiments showcase that the proposed approach significantly\noutperforms the baseline CNN with an improvement of 24.5% and 23.9% for the\npercent accuracy and F1-Score respectively when operated on encrypted data.\nThough deeper ResNet-50 architecture is obtained as a slightly more accurate\nmodel, with an increase of 4.4%, the proposed approach boasts a reduction of\nparameters by 99.32%, and thus, offers a much-improved prediction time by\nnearly 60%.",
    "updated" : "2024-02-15T05:06:53Z",
    "published" : "2024-02-15T05:06:53Z",
    "authors" : [
      {
        "name" : "Pranshav Gajjar"
      },
      {
        "name" : "Azuka Chiejina"
      },
      {
        "name" : "Vijay K. Shah"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.10473v1",
    "title" : "Privacy for Fairness: Information Obfuscation for Fair Representation\n  Learning with Local Differential Privacy",
    "summary" : "As machine learning (ML) becomes more prevalent in human-centric\napplications, there is a growing emphasis on algorithmic fairness and privacy\nprotection. While previous research has explored these areas as separate\nobjectives, there is a growing recognition of the complex relationship between\nprivacy and fairness. However, previous works have primarily focused on\nexamining the interplay between privacy and fairness through empirical\ninvestigations, with limited attention given to theoretical exploration. This\nstudy aims to bridge this gap by introducing a theoretical framework that\nenables a comprehensive examination of their interrelation. We shall develop\nand analyze an information bottleneck (IB) based information obfuscation method\nwith local differential privacy (LDP) for fair representation learning. In\ncontrast to many empirical studies on fairness in ML, we show that the\nincorporation of LDP randomizers during the encoding process can enhance the\nfairness of the learned representation. Our analysis will demonstrate that the\ndisclosure of sensitive information is constrained by the privacy budget of the\nLDP randomizer, thereby enabling the optimization process within the IB\nframework to effectively suppress sensitive information while preserving the\ndesired utility through obfuscation. Based on the proposed method, we further\ndevelop a variational representation encoding approach that simultaneously\nachieves fairness and LDP. Our variational encoding approach offers practical\nadvantages. It is trained using a non-adversarial method and does not require\nthe introduction of any variational prior. Extensive experiments will be\npresented to validate our theoretical results and demonstrate the ability of\nour proposed approach to achieve both LDP and fairness while preserving\nadequate utility.",
    "updated" : "2024-02-16T06:35:10Z",
    "published" : "2024-02-16T06:35:10Z",
    "authors" : [
      {
        "name" : "Songjie Xie"
      },
      {
        "name" : "Youlong Wu"
      },
      {
        "name" : "Jiaxuan Li"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Khaled B. Letaief"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.10423v1",
    "title" : "Connect the dots: Dataset Condensation, Differential Privacy, and\n  Adversarial Uncertainty",
    "summary" : "Our work focuses on understanding the underpinning mechanism of dataset\ncondensation by drawing connections with ($\\epsilon$, $\\delta$)-differential\nprivacy where the optimal noise, $\\epsilon$, is chosen by adversarial\nuncertainty \\cite{Grining2017}. We can answer the question about the inner\nworkings of the dataset condensation procedure. Previous work \\cite{dong2022}\nproved the link between dataset condensation (DC) and ($\\epsilon$,\n$\\delta$)-differential privacy. However, it is unclear from existing works on\nablating DC to obtain a lower-bound estimate of $\\epsilon$ that will suffice\nfor creating high-fidelity synthetic data. We suggest that adversarial\nuncertainty is the most appropriate method to achieve an optimal noise level,\n$\\epsilon$. As part of the internal dynamics of dataset condensation, we adopt\na satisfactory scheme for noise estimation that guarantees high-fidelity data\nwhile providing privacy.",
    "updated" : "2024-02-16T03:12:22Z",
    "published" : "2024-02-16T03:12:22Z",
    "authors" : [
      {
        "name" : "Kenneth Odoh"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.08156v2",
    "title" : "Group Decision-Making among Privacy-Aware Agents",
    "summary" : "How can individuals exchange information to learn from each other despite\ntheir privacy needs and security concerns? For example, consider individuals\ndeliberating a contentious topic and being concerned about divulging their\nprivate experiences. Preserving individual privacy and enabling efficient\nsocial learning are both important desiderata but seem fundamentally at odds\nwith each other and very hard to reconcile. We do so by controlling information\nleakage using rigorous statistical guarantees that are based on differential\nprivacy (DP). Our agents use log-linear rules to update their beliefs after\ncommunicating with their neighbors. Adding DP randomization noise to beliefs\nprovides communicating agents with plausible deniability with regard to their\nprivate information and their network neighborhoods. We consider two learning\nenvironments one for distributed maximum-likelihood estimation given a finite\nnumber of private signals and another for online learning from an infinite,\nintermittent signal stream. Noisy information aggregation in the finite case\nleads to interesting tradeoffs between rejecting low-quality states and making\nsure all high-quality states are accepted in the algorithm output. Our results\nflesh out the nature of the trade-offs in both cases between the quality of the\ngroup decision outcomes, learning accuracy, communication cost, and the level\nof privacy protections that the agents are afforded.",
    "updated" : "2024-02-15T21:50:57Z",
    "published" : "2024-02-13T01:38:01Z",
    "authors" : [
      {
        "name" : "Marios Papachristou"
      },
      {
        "name" : "M. Amin Rahimian"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MA",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.11989v1",
    "title" : "Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models",
    "summary" : "Low-rank adaptation (LoRA) is an efficient strategy for adapting latent\ndiffusion models (LDMs) on a training dataset to generate specific objects by\nminimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable\nto membership inference (MI) attacks that can judge whether a particular data\npoint belongs to private training datasets, thus facing severe risks of privacy\nleakage. To defend against MI attacks, we make the first effort to propose a\nstraightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is\nformulated as a min-max optimization problem where a proxy attack model is\ntrained by maximizing its MI gain while the LDM is adapted by minimizing the\nsum of the adaptation loss and the proxy attack model's MI gain. However, we\nempirically disclose that PrivateLoRA has the issue of unstable optimization\ndue to the large fluctuation of the gradient scale which impedes adaptation. To\nmitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by\nminimizing the ratio of the adaptation loss to the MI gain, which implicitly\nrescales the gradient and thus stabilizes the optimization. Our comprehensive\nempirical results corroborate that adapted LDMs via Stable PrivateLoRA can\neffectively defend against MI attacks while generating high-quality images. Our\ncode is available at https://github.com/WilliamLUO0/StablePrivateLoRA.",
    "updated" : "2024-02-19T09:32:48Z",
    "published" : "2024-02-19T09:32:48Z",
    "authors" : [
      {
        "name" : "Zihao Luo"
      },
      {
        "name" : "Xilie Xu"
      },
      {
        "name" : "Feng Liu"
      },
      {
        "name" : "Yun Sing Koh"
      },
      {
        "name" : "Di Wang"
      },
      {
        "name" : "Jingfeng Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.11582v1",
    "title" : "Publicly auditable privacy-preserving electoral rolls",
    "summary" : "While existing literature on electronic voting has extensively addressed\nverifiability of voting protocols, the vulnerability of electoral rolls in\nlarge public elections remains a critical concern. To ensure integrity of\nelectoral rolls, the current practice is to either make electoral rolls public\nor share them with the political parties. However, this enables construction of\ndetailed voter profiles and selective targeting and manipulation of voters,\nthereby undermining the fundamental principle of free and fair elections. In\nthis paper, we study the problem of designing publicly auditable yet\nprivacy-preserving electoral rolls. We first formulate a threat model and\nprovide formal security definitions. We then present a protocol for creation\nand maintenance of electoral rolls that mitigates the threats. Eligible voters\ncan verify their inclusion, whereas political parties and auditors can\nstatistically audit the electoral roll. The entire electoral roll is never\nrevealed, which prevents any large-scale systematic voter targeting and\nmanipulation.",
    "updated" : "2024-02-18T13:11:48Z",
    "published" : "2024-02-18T13:11:48Z",
    "authors" : [
      {
        "name" : "Prashant Agrawal"
      },
      {
        "name" : "Mahabir Prasad Jhanwar"
      },
      {
        "name" : "Subodh Vishnu Sharma"
      },
      {
        "name" : "Subhashis Banerjee"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.11526v1",
    "title" : "Measuring Privacy Loss in Distributed Spatio-Temporal Data",
    "summary" : "Statistics about traffic flow and people's movement gathered from multiple\ngeographical locations in a distributed manner are the driving force powering\nmany applications, such as traffic prediction, demand prediction, and\nrestaurant occupancy reports. However, these statistics are often based on\nsensitive location data of people, and hence privacy has to be preserved while\nreleasing them. The standard way to do this is via differential privacy, which\nguarantees a form of rigorous, worst-case, person-level privacy. In this work,\nmotivated by several counter-intuitive features of differential privacy in\ndistributed location applications, we propose an alternative privacy loss\nagainst location reconstruction attacks by an informed adversary. Our\nexperiments on real and synthetic data demonstrate that our privacy loss better\nreflects our intuitions on individual privacy violation in the distributed\nspatio-temporal setting.",
    "updated" : "2024-02-18T09:53:14Z",
    "published" : "2024-02-18T09:53:14Z",
    "authors" : [
      {
        "name" : "Tatsuki Koga"
      },
      {
        "name" : "Casey Meehan"
      },
      {
        "name" : "Kamalika Chaudhuri"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.11193v1",
    "title" : "Privacy Impact Assessments in the Wild: A Scoping Review",
    "summary" : "Privacy Impact Assessments (PIAs) offer a systematic process for assessing\nthe privacy impacts of a project or system. As a privacy engineering strategy,\nPIAs are heralded as one of the main approaches to privacy by design,\nsupporting the early identification of threats and controls. However, there is\nstill a shortage of empirical evidence on their uptake and proven effectiveness\nin practice. To better understand the current state of literature and research,\nthis paper provides a comprehensive Scoping Review (ScR) on the topic of PIAs\n\"in the wild\", following the well-established Preferred Reporting Items for\nSystematic reviews and Meta-Analyses (PRISMA) guidelines. As a result, this ScR\nincludes 45 studies, providing an extensive synthesis of the existing body of\nknowledge, classifying types of research and publications, appraising the\nmethodological quality of primary research, and summarising the positive and\nnegative aspects of PIAs in practice, as reported by studies. This ScR also\nidentifies significant research gaps (e.g., evidence gaps from contradictory\nresults and methodological gaps from research design deficiencies), future\nresearch pathways, and implications for researchers, practitioners, and\npolicymakers developing and evaluating PIA frameworks. As we conclude, there is\nstill a significant need for more primary research on the topic, both\nqualitative and quantitative. A critical appraisal of qualitative studies\n(n=28) revealed deficiencies in the methodological quality, and only four\nquantitative studies were identified, suggesting that current primary research\nremains incipient. Nonetheless, PIAs can be regarded as a prominent sub-area in\nthe broader field of Empirical Privacy Engineering, warranting further research\ntoward more evidence-based practices.",
    "updated" : "2024-02-17T05:07:10Z",
    "published" : "2024-02-17T05:07:10Z",
    "authors" : [
      {
        "name" : "Leonardo Horn Iwaya"
      },
      {
        "name" : "Ala Sarah Alaqra"
      },
      {
        "name" : "Marit Hansen"
      },
      {
        "name" : "Simone Fischer-Hübner"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.08223v2",
    "title" : "The Limits of Price Discrimination Under Privacy Constraints",
    "summary" : "We consider a producer's problem of selling a product to a continuum of\nprivacy-conscious consumers, where the producer can implement third-degree\nprice discrimination, offering different prices to different market segments.\nIn the absence of privacy constraints, Bergemann, Brooks, and Morris [2015]\ncharacterize the set of all possible consumer-producer utilities, showing that\nit is a triangle. We consider a privacy mechanism that provides a degree of\nprotection by probabilistically masking each market segment, and we establish\nthat the resultant set of all consumer-producer utilities forms a convex\npolygon, characterized explicitly as a linear mapping of a certain\nhigh-dimensional convex polytope into $\\mathbb{R}^2$. This characterization\nenables us to investigate the impact of the privacy mechanism on both producer\nand consumer utilities. In particular, we establish that the privacy constraint\nalways hurts the producer by reducing both the maximum and minimum utility\nachievable. From the consumer's perspective, although the privacy mechanism\nensures an increase in the minimum utility compared to the non-private\nscenario, interestingly, it may reduce the maximum utility. Finally, we\ndemonstrate that increasing the privacy level does not necessarily intensify\nthese effects. For instance, the maximum utility for the producer or the\nminimum utility for the consumer may exhibit nonmonotonic behavior in response\nto an increase of the privacy level.",
    "updated" : "2024-02-17T00:59:36Z",
    "published" : "2024-02-13T05:28:38Z",
    "authors" : [
      {
        "name" : "Alireza Fallah"
      },
      {
        "name" : "Michael I. Jordan"
      },
      {
        "name" : "Ali Makhdoumi"
      },
      {
        "name" : "Azarakhsh Malekian"
      }
    ],
    "categories" : [
      "econ.TH",
      "cs.GT"
    ]
  }
]