[
  {
    "id" : "http://arxiv.org/abs/2510.01793v1",
    "title" : "Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of\n  Privacy Filters for Synthetic Data Generation",
    "summary" : "The generation of privacy-preserving synthetic datasets is a promising avenue\nfor overcoming data scarcity in medical AI research. Post-hoc privacy filtering\ntechniques, designed to remove samples containing personally identifiable\ninformation, have recently been proposed as a solution. However, their\neffectiveness remains largely unverified. This work presents a rigorous\nevaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary\nto claims from the original publications, our results demonstrate that current\nfilters exhibit limited specificity and consistency, achieving high sensitivity\nonly for real images while failing to reliably detect near-duplicates generated\nfrom training data. These results demonstrate a critical limitation of post-hoc\nfiltering: rather than effectively safeguarding patient privacy, these methods\nmay provide a false sense of security while leaving unacceptable levels of\npatient information exposed. We conclude that substantial advances in filter\ndesign are needed before these methods can be confidently deployed in sensitive\napplications.",
    "updated" : "2025-10-02T08:32:20Z",
    "published" : "2025-10-02T08:32:20Z",
    "authors" : [
      {
        "name" : "Adil Koeken"
      },
      {
        "name" : "Alexander Ziller"
      },
      {
        "name" : "Moritz Knolle"
      },
      {
        "name" : "Daniel Rueckert"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.01645v1",
    "title" : "Position: Privacy Is Not Just Memorization!",
    "summary" : "The discourse on privacy risks in Large Language Models (LLMs) has\ndisproportionately focused on verbatim memorization of training data, while a\nconstellation of more immediate and scalable privacy threats remain\nunderexplored. This position paper argues that the privacy landscape of LLM\nsystems extends far beyond training data extraction, encompassing risks from\ndata collection practices, inference-time context leakage, autonomous agent\ncapabilities, and the democratization of surveillance through deep inference\nattacks. We present a comprehensive taxonomy of privacy risks across the LLM\nlifecycle -- from data collection through deployment -- and demonstrate through\ncase studies how current privacy frameworks fail to address these multifaceted\nthreats. Through a longitudinal analysis of 1,322 AI/ML privacy papers\npublished at leading conferences over the past decade (2016--2025), we reveal\nthat while memorization receives outsized attention in technical research, the\nmost pressing privacy harms lie elsewhere, where current technical approaches\noffer little traction and viable paths forward remain unclear. We call for a\nfundamental shift in how the research community approaches LLM privacy, moving\nbeyond the narrow focus of current technical solutions and embracing\ninterdisciplinary approaches that address the sociotechnical nature of these\nemerging threats.",
    "updated" : "2025-10-02T04:02:06Z",
    "published" : "2025-10-02T04:02:06Z",
    "authors" : [
      {
        "name" : "Niloofar Mireshghallah"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.01113v1",
    "title" : "Privacy Preserved Federated Learning with Attention-Based Aggregation\n  for Biometric Recognition",
    "summary" : "Because biometric data is sensitive, centralized training poses a privacy\nrisk, even though biometric recognition is essential for contemporary\napplications. Federated learning (FL), which permits decentralized training,\nprovides a privacy-preserving substitute. Conventional FL, however, has trouble\nwith interpretability and heterogeneous data (non-IID). In order to handle\nnon-IID biometric data, this framework adds an attention mechanism at the\ncentral server that weights local model updates according to their\nsignificance. Differential privacy and secure update protocols safeguard data\nwhile preserving accuracy. The A3-FL framework is evaluated in this study using\nFVC2004 fingerprint data, with each client's features extracted using a Siamese\nConvolutional Neural Network (Siamese-CNN). By dynamically modifying client\ncontributions, the attention mechanism increases the accuracy of the global\nmodel.The accuracy, convergence speed, and robustness of the A3-FL framework\nare superior to those of standard FL (FedAvg) and static baselines, according\nto experimental evaluations using fingerprint data (FVC2004). The accuracy of\nthe attention-based approach was 0.8413, while FedAvg, Local-only, and\nCentralized approaches were 0.8164, 0.7664, and 0.7997, respectively. Accuracy\nstayed high at 0.8330 even with differential privacy. A scalable and\nprivacy-sensitive biometric system for secure and effective recognition in\ndispersed environments is presented in this work.",
    "updated" : "2025-10-01T16:58:59Z",
    "published" : "2025-10-01T16:58:59Z",
    "authors" : [
      {
        "name" : "Kassahun Azezew"
      },
      {
        "name" : "Minyechil Alehegn"
      },
      {
        "name" : "Tsega Asresa"
      },
      {
        "name" : "Bitew Mekuria"
      },
      {
        "name" : "Tizazu Bayh"
      },
      {
        "name" : "Ayenew Kassie"
      },
      {
        "name" : "Amsalu Tesema"
      },
      {
        "name" : "Animut Embiyale"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.00909v1",
    "title" : "\"We are not Future-ready\": Understanding AI Privacy Risks and Existing\n  Mitigation Strategies from the Perspective of AI Developers in Europe",
    "summary" : "The proliferation of AI has sparked privacy concerns related to training\ndata, model interfaces, downstream applications, and more. We interviewed 25 AI\ndevelopers based in Europe to understand which privacy threats they believe\npose the greatest risk to users, developers, and businesses and what protective\nstrategies, if any, would help to mitigate them. We find that there is little\nconsensus among AI developers on the relative ranking of privacy risks. These\ndifferences stem from salient reasoning patterns that often relate to human\nrather than purely technical factors. Furthermore, while AI developers are\naware of proposed mitigation strategies for addressing these risks, they\nreported minimal real-world adoption. Our findings highlight both gaps and\nopportunities for empowering AI developers to better address privacy risks in\nAI.",
    "updated" : "2025-10-01T13:51:33Z",
    "published" : "2025-10-01T13:51:33Z",
    "authors" : [
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Patrick Gage Kelley"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Kurt Thomas"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.00478v1",
    "title" : "Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving\n  Domain Adaptation",
    "summary" : "Recent work on latent diffusion models (LDMs) has focused almost exclusively\non generative tasks, leaving their potential for discriminative transfer\nlargely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a\nnovel LDM-based framework for a more practical variant of source-free domain\nadaptation (SFDA): the source provider may share not only a pre-trained\nclassifier but also an auxiliary latent diffusion module, trained once on the\nsource data and never exposing raw source samples. DVD encodes each source\nfeature's label information into its latent vicinity by fitting a Gaussian\nprior over its k-nearest neighbors and training the diffusion network to drift\nnoisy samples back to label-consistent representations. During adaptation, we\nsample from each target feature's latent vicinity, apply the frozen diffusion\nmodule to generate source-like cues, and use a simple InfoNCE loss to align the\ntarget encoder to these cues, explicitly transferring decision boundaries\nwithout source access. Across standard SFDA benchmarks, DVD outperforms\nstate-of-the-art methods. We further show that the same latent diffusion module\nenhances the source classifier's accuracy on in-domain data and boosts\nperformance in supervised classification and domain generalization experiments.\nDVD thus reinterprets LDMs as practical, privacy-preserving bridges for\nexplicit knowledge transfer, addressing a core challenge in source-free domain\nadaptation that prior methods have yet to solve.",
    "updated" : "2025-10-01T03:58:26Z",
    "published" : "2025-10-01T03:58:26Z",
    "authors" : [
      {
        "name" : "Jing Wang"
      },
      {
        "name" : "Wonho Bae"
      },
      {
        "name" : "Jiahong Chen"
      },
      {
        "name" : "Wenxu Wang"
      },
      {
        "name" : "Junhyug Noh"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03035v1",
    "title" : "Protecting Persona Biometric Data: The Case of Facial Privacy",
    "summary" : "The proliferation of digital technologies has led to unprecedented data\ncollection, with facial data emerging as a particularly sensitive commodity.\nCompanies are increasingly leveraging advanced facial recognition technologies,\noften without the explicit consent or awareness of individuals, to build\nsophisticated surveillance capabilities. This practice, fueled by weak and\nfragmented laws in many jurisdictions, has created a regulatory vacuum that\nallows for the commercialization of personal identity and poses significant\nthreats to individual privacy and autonomy. This article introduces the concept\nof Facial Privacy. It analyzes the profound challenges posed by unregulated\nfacial recognition by conducting a comprehensive review of existing legal\nframeworks. It examines and compares regulations such as the GDPR, Brazil's\nLGPD, Canada's PIPEDA, and privacy acts in China, Singapore, South Korea, and\nJapan, alongside sector-specific laws in the United States like the Illinois\nBiometric Information Privacy Act (BIPA). The analysis highlights the societal\nimpacts of this technology, including the potential for discriminatory bias and\nthe long-lasting harm that can result from the theft of immutable biometric\ndata. Ultimately, the paper argues that existing legal loopholes and\nambiguities leave individuals vulnerable. It proposes a new policy framework\nthat shifts the paradigm from data as property to a model of inalienable\nrights, ensuring that fundamental human rights are upheld against unchecked\ntechnological expansion.",
    "updated" : "2025-10-03T14:16:33Z",
    "published" : "2025-10-03T14:16:33Z",
    "authors" : [
      {
        "name" : "Lambert Hogenhout"
      },
      {
        "name" : "Rinzin Wangmo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.02487v1",
    "title" : "Interplay between Security, Privacy and Trust in 6G-enabled Intelligent\n  Transportation Systems",
    "summary" : "The advancement of 6G technology has the potential to revolutionize the\ntransportation sector and significantly improve how we travel. 6G-enabled\nIntelligent Transportation Systems (ITS) promise to offer high-speed,\nlow-latency communication and advanced data analytics capabilities, supporting\nthe development of safer, more efficient, and more sustainable transportation\nsolutions. However, various security and privacy challenges were identified in\nthe literature that must be addressed to enable the safe and secure deployment\nof 6G-ITS and ensure people's trust in using these technologies. This paper\nreviews the opportunities and challenges of 6G-ITS, particularly focusing on\ntrust, security, and privacy, with special attention to quantum technologies\nthat both enhance security through quantum key distribution and introduce new\nvulnerabilities. It discusses the potential benefits of 6G technology in the\ntransportation sector, including improved communication, device\ninteroperability support, data analytic capabilities, and increased automation\nfor different components, such as transportation management and communication\nsystems. A taxonomy of different attack models in 6G-ITS is proposed, and a\ncomparison of the security threats in 5G-ITS and 6G-ITS is provided, along with\npotential mitigating solutions. This research highlights the urgent need for a\ncomprehensive, multi-layered security framework spanning physical\ninfrastructure protection, network protocol security, data management\nsafeguards, application security measures, and trust management systems to\neffectively mitigate emerging security and privacy risks and ensure the\nintegrity and resilience of future transportation ecosystems.",
    "updated" : "2025-10-02T18:47:36Z",
    "published" : "2025-10-02T18:47:36Z",
    "authors" : [
      {
        "name" : "Ahmed Danladi Abdullahi"
      },
      {
        "name" : "Erfan Bahrami"
      },
      {
        "name" : "Tooska Dargahi"
      },
      {
        "name" : "Mohammed Al-Khalidi"
      },
      {
        "name" : "Mohammad Hammoudeh"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05068v1",
    "title" : "Multi-Agent Distributed Optimization With Feasible Set Privacy",
    "summary" : "We consider the problem of decentralized constrained optimization with\nmultiple agents $E_1,\\ldots,E_N$ who jointly wish to learn the optimal solution\nset while keeping their feasible sets $\\mathcal{P}_1,\\ldots,\\mathcal{P}_N$\nprivate from each other. We assume that the objective function $f$ is known to\nall agents and each feasible set is a collection of points from a universal\nalphabet $\\mathcal{P}_{alph}$. A designated agent (leader) starts the\ncommunication with the remaining (non-leader) agents, and is the first to\nretrieve the solution set. The leader searches for the solution by sending\nqueries to and receiving answers from the non-leaders, such that the\ninformation on the individual feasible sets revealed to the leader should be no\nmore than nominal, i.e., what is revealed from learning the solution set alone.\nWe develop achievable schemes for obtaining the solution set at nominal\ninformation leakage, and characterize their communication costs under two\ncommunication setups between agents. In this work, we focus on two kinds of\nnetwork setups: i) ring, where each agent communicates with two adjacent\nagents, and ii) star, where only the leader communicates with the remaining\nagents. We show that, if the leader first learns the joint feasible set through\nan existing private set intersection (PSI) protocol and then deduces the\nsolution set, the information leaked to the leader is greater than nominal.\nMoreover, we draw connection of our schemes to threshold PSI (ThPSI), which is\na PSI-variant where the intersection is revealed only when its cardinality is\nlarger than a threshold value. Finally, for various realizations of $f$ mapped\nuniformly at random to a fixed range of values, our schemes are more\ncommunication-efficient with a high probability compared to retrieving the\nentire feasible set through PSI.",
    "updated" : "2025-10-06T17:45:57Z",
    "published" : "2025-10-06T17:45:57Z",
    "authors" : [
      {
        "name" : "Shreya Meel"
      },
      {
        "name" : "Sennur Ulukus"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.DC",
      "cs.NI",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04527v1",
    "title" : "Quantum capacity amplification via privacy",
    "summary" : "We investigate superadditivity of quantum capacity through private channels\nwhose Choi-Jamiolkowski operators are private states. This perspective links\nthe security structure of private states to quantum capacity and clarifies the\nrole of the shield system: information encoded in the shield system that would\notherwise leak to the environment can be recycled when paired with an assisting\nchannel, thereby boosting capacity. Our main contributions are threefold:\nFirstly, we develop a general framework that provides a sufficient condition\nfor capacity amplification, which is formulated in terms of the assisting\nchannel's Holevo information. As examples, we give explicit, dimension and\nparameter dependent amplification thresholds for erasure and depolarizing\nchannels. Secondly, assuming the Spin alignment conjecture, we derive a\nsingle-letter expression for the quantum capacity of a family of private\nchannels that are neither degradable, anti-degradable, nor PPT; as an\napplication, we construct channels with vanishing quantum capacity yet\nunbounded private capacity. Thirdly, we further analyze approximate private\nchannels: we give an alternative proof of superactivation that extends its\nvalidity to a broader parameter regime, and, by combining amplification bounds\nwith continuity estimates, we establish a metric separation showing that\nchannels exhibiting capacity amplification have nonzero diamond distance from\nthe set of anti-degradable channels, indicating that existing approximate\n(anti-)degradability bounds are not tight. We also revisit the computability of\nthe regularized quantum capacity and modestly suggest that this fundamental\nquestion still remains open.",
    "updated" : "2025-10-06T06:35:19Z",
    "published" : "2025-10-06T06:35:19Z",
    "authors" : [
      {
        "name" : "Peixue Wu"
      },
      {
        "name" : "Yunkai Wang"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.IT",
      "math.MP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04465v1",
    "title" : "Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM\n  Agents",
    "summary" : "Large Language Model (LLM) agents require personal information for\npersonalization in order to better act on users' behalf in daily tasks, but\nthis raises privacy concerns and a personalization-privacy dilemma. Agent's\nautonomy introduces both risks and opportunities, yet its effects remain\nunclear. To better understand this, we conducted a 3$\\times$3 between-subjects\nexperiment ($N=450$) to study how agent's autonomy level and personalization\ninfluence users' privacy concerns, trust and willingness to use, as well as the\nunderlying psychological processes. We find that personalization without\nconsidering users' privacy preferences increases privacy concerns and decreases\ntrust and willingness to use. Autonomy moderates these effects: Intermediate\nautonomy flattens the impact of personalization compared to No- and Full\nautonomy conditions. Our results suggest that rather than aiming for perfect\nmodel alignment in output generation, balancing autonomy of agent's action and\nuser control offers a promising path to mitigate the personalization-privacy\ndilemma.",
    "updated" : "2025-10-06T03:38:54Z",
    "published" : "2025-10-06T03:38:54Z",
    "authors" : [
      {
        "name" : "Zhiping Zhang"
      },
      {
        "name" : "Yi Evie Zhang"
      },
      {
        "name" : "Freda Shi"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04261v1",
    "title" : "VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient\n  Extraction of User Privacy",
    "summary" : "Large language models (LLMs) have been widely deployed in Conversational AIs\n(CAIs), while exposing privacy and security threats. Recent research shows that\nLLM-based CAIs can be manipulated to extract private information from human\nusers, posing serious security threats. However, the methods proposed in that\nstudy rely on a white-box setting that adversaries can directly modify the\nsystem prompt. This condition is unlikely to hold in real-world deployments.\nThe limitation raises a critical question: can unprivileged attackers still\ninduce such privacy risks in practical LLM-integrated applications? To address\nthis question, we propose \\textsc{VortexPIA}, a novel indirect prompt injection\nattack that induces privacy extraction in LLM-integrated applications under\nblack-box settings. By injecting token-efficient data containing false\nmemories, \\textsc{VortexPIA} misleads LLMs to actively request private\ninformation in batches. Unlike prior methods, \\textsc{VortexPIA} allows\nattackers to flexibly define multiple categories of sensitive data. We evaluate\n\\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,\nacross four benchmark datasets. The results show that \\textsc{VortexPIA}\nsignificantly outperforms baselines and achieves state-of-the-art (SOTA)\nperformance. It also demonstrates efficient privacy requests, reduced token\nconsumption, and enhanced robustness against defense mechanisms. We further\nvalidate \\textsc{VortexPIA} on multiple realistic open-source LLM-integrated\napplications, demonstrating its practical effectiveness.",
    "updated" : "2025-10-05T15:58:55Z",
    "published" : "2025-10-05T15:58:55Z",
    "authors" : [
      {
        "name" : "Yu Cui"
      },
      {
        "name" : "Sicheng Pan"
      },
      {
        "name" : "Yifei Liu"
      },
      {
        "name" : "Haibin Zhang"
      },
      {
        "name" : "Cong Zuo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04153v1",
    "title" : "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation",
    "summary" : "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost.",
    "updated" : "2025-10-05T11:09:10Z",
    "published" : "2025-10-05T11:09:10Z",
    "authors" : [
      {
        "name" : "Haoqi Wu"
      },
      {
        "name" : "Wei Dai"
      },
      {
        "name" : "Ming Xu"
      },
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Qiang Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04027v1",
    "title" : "Multi-Class Support Vector Machine with Differential Privacy",
    "summary" : "With the increasing need to safeguard data privacy in machine learning\nmodels, differential privacy (DP) is one of the major frameworks to build\nprivacy-preserving models. Support Vector Machines (SVMs) are widely used\ntraditional machine learning models due to their robust margin guarantees and\nstrong empirical performance in binary classification. However, applying DP to\nmulti-class SVMs is inadequate, as the standard one-versus-rest (OvR) and\none-versus-one (OvO) approaches repeatedly query each data sample when building\nmultiple binary classifiers, thus consuming the privacy budget proportionally\nto the number of classes. To overcome this limitation, we explore all-in-one\nSVM approaches for DP, which access each data sample only once to construct\nmulti-class SVM boundaries with margin maximization properties. We propose a\nnovel differentially Private Multi-class SVM (PMSVM) with weight and gradient\nperturbation methods, providing rigorous sensitivity and convergence analyses\nto ensure DP in all-in-one SVMs. Empirical results demonstrate that our\napproach surpasses existing DP-SVM methods in multi-class scenarios.",
    "updated" : "2025-10-05T04:25:16Z",
    "published" : "2025-10-05T04:25:16Z",
    "authors" : [
      {
        "name" : "Jinseong Park"
      },
      {
        "name" : "Yujin Choi"
      },
      {
        "name" : "Jaewook Lee"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03996v1",
    "title" : "FHEON: A Configurable Framework for Developing Privacy-Preserving Neural\n  Networks Using Homomorphic Encryption",
    "summary" : "The widespread adoption of Machine Learning as a Service raises critical\nprivacy and security concerns, particularly about data confidentiality and\ntrust in both cloud providers and the machine learning models. Homomorphic\nEncryption (HE) has emerged as a promising solution to this problems, allowing\ncomputations on encrypted data without decryption. Despite its potential,\nexisting approaches to integrate HE into neural networks are often limited to\nspecific architectures, leaving a wide gap in providing a framework for easy\ndevelopment of HE-friendly privacy-preserving neural network models similar to\nwhat we have in the broader field of machine learning. In this paper, we\npresent FHEON, a configurable framework for developing privacy-preserving\nconvolutional neural network (CNN) models for inference using HE. FHEON\nintroduces optimized and configurable implementations of privacy-preserving CNN\nlayers including convolutional layers, average pooling layers, ReLU activation\nfunctions, and fully connected layers. These layers are configured using\nparameters like input channels, output channels, kernel size, stride, and\npadding to support arbitrary CNN architectures. We assess the performance of\nFHEON using several CNN architectures, including LeNet-5, VGG-11, VGG- 16,\nResNet-20, and ResNet-34. FHEON maintains encrypted-domain accuracies within\n+/- 1% of their plaintext counterparts for ResNet-20 and LeNet-5 models.\nNotably, on a consumer-grade CPU, the models build on FHEON achieved 98.5%\naccuracy with a latency of 13 seconds on MNIST using LeNet-5, and 92.2%\naccuracy with a latency of 403 seconds on CIFAR-10 using ResNet-20.\nAdditionally, FHEON operates within a practical memory budget requiring not\nmore than 42.3 GB for VGG-16.",
    "updated" : "2025-10-05T02:12:44Z",
    "published" : "2025-10-05T02:12:44Z",
    "authors" : [
      {
        "name" : "Nges Brian Njungle"
      },
      {
        "name" : "Eric Jahns"
      },
      {
        "name" : "Michel A. Kinsy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03860v1",
    "title" : "Privacy Enhancement in Over-the-Air Federated Learning via Adaptive\n  Receive Scaling",
    "summary" : "In Federated Learning (FL) with over-the-air aggregation, the quality of the\nsignal received at the server critically depends on the receive scaling\nfactors. While a larger scaling factor can reduce the effective noise power and\nimprove training performance, it also compromises the privacy of devices by\nreducing uncertainty. In this work, we aim to adaptively design the receive\nscaling factors across training rounds to balance the trade-off between\ntraining convergence and privacy in an FL system under dynamic channel\nconditions. We formulate a stochastic optimization problem that minimizes the\noverall R\\'enyi differential privacy (RDP) leakage over the entire training\nprocess, subject to a long-term constraint that ensures convergence of the\nglobal loss function. Our problem depends on unknown future information, and we\nobserve that standard Lyapunov optimization is not applicable. Thus, we develop\na new online algorithm, termed AdaScale, based on a sequence of novel per-round\nproblems that can be solved efficiently. We further derive upper bounds on the\ndynamic regret and constraint violation of AdaSacle, establishing that it\nachieves diminishing dynamic regret in terms of time-averaged RDP leakage while\nensuring convergence of FL training to a stationary point. Numerical\nexperiments on canonical classification tasks show that our approach\neffectively reduces RDP and DP leakages compared with state-of-the-art\nbenchmarks without compromising learning performance.",
    "updated" : "2025-10-04T16:15:19Z",
    "published" : "2025-10-04T16:15:19Z",
    "authors" : [
      {
        "name" : "Faeze Moradi Kalarde"
      },
      {
        "name" : "Ben Liang"
      },
      {
        "name" : "Min Dong"
      },
      {
        "name" : "Yahia A. Eldemerdash Ahmed"
      },
      {
        "name" : "Ho Ting Cheng"
      }
    ],
    "categories" : [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03662v1",
    "title" : "Operationalizing Data Minimization for Privacy-Preserving LLM Prompting",
    "summary" : "The rapid deployment of large language models (LLMs) in consumer applications\nhas led to frequent exchanges of personal information. To obtain useful\nresponses, users often share more than necessary, increasing privacy risks via\nmemorization, context-based personalization, or security breaches. We present a\nframework to formally define and operationalize data minimization: for a given\nuser prompt and response model, quantifying the least privacy-revealing\ndisclosure that maintains utility, and we propose a priority-queue tree search\nto locate this optimal point within a privacy-ordered transformation space. We\nevaluated the framework on four datasets spanning open-ended conversations\n(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth\nanswers (CaseHold, MedQA), quantifying achievable data minimization with nine\nLLMs as the response model. Our results demonstrate that larger frontier LLMs\ncan tolerate stronger data minimization while maintaining task quality than\nsmaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for\nQwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that\nLLMs struggle to predict optimal data minimization directly, showing a bias\ntoward abstraction that leads to oversharing. This suggests not just a privacy\ngap, but a capability gap: models may lack awareness of what information they\nactually need to solve a task.",
    "updated" : "2025-10-04T04:20:18Z",
    "published" : "2025-10-04T04:20:18Z",
    "authors" : [
      {
        "name" : "Jijie Zhou"
      },
      {
        "name" : "Niloofar Mireshghallah"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03559v1",
    "title" : "PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating\n  Privacy Reviews in UX Design",
    "summary" : "UX professionals routinely conduct design reviews, yet privacy concerns are\noften overlooked -- not only due to limited tools, but more critically because\nof low intrinsic motivation. Limited privacy knowledge, weak empathy for\nunexpectedly affected users, and low confidence in identifying harms make it\ndifficult to address risks. We present PrivacyMotiv, an LLM-powered system that\nsupports privacy-oriented design diagnosis by generating speculative personas\nwith UX user journeys centered on individuals vulnerable to privacy risks.\nDrawing on narrative strategies, the system constructs relatable and\nattention-drawing scenarios that show how ordinary design choices may cause\nunintended harms, expanding the scope of privacy reflection in UX. In a\nwithin-subjects study with professional UX practitioners (N=16), we compared\nparticipants' self-proposed methods with PrivacyMotiv across two privacy review\ntasks. Results show significant improvements in empathy, intrinsic motivation,\nand perceived usefulness. This work contributes a promising privacy review\napproach which addresses the motivational barriers in privacy-aware UX.",
    "updated" : "2025-10-03T23:14:22Z",
    "published" : "2025-10-03T23:14:22Z",
    "authors" : [
      {
        "name" : "Zeya Chen"
      },
      {
        "name" : "Jianing Wen"
      },
      {
        "name" : "Ruth Schmidt"
      },
      {
        "name" : "Yaxing Yao"
      },
      {
        "name" : "Toby Jia-Jun Li"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03513v1",
    "title" : "A Lightweight Federated Learning Approach for Privacy-Preserving Botnet\n  Detection in IoT",
    "summary" : "The rapid growth of the Internet of Things (IoT) has expanded opportunities\nfor innovation but also increased exposure to botnet-driven cyberattacks.\nConventional detection methods often struggle with scalability, privacy, and\nadaptability in resource-constrained IoT environments. To address these\nchallenges, we present a lightweight and privacy-preserving botnet detection\nframework based on federated learning. This approach enables distributed\ndevices to collaboratively train models without exchanging raw data, thus\nmaintaining user privacy while preserving detection accuracy. A\ncommunication-efficient aggregation strategy is introduced to reduce overhead,\nensuring suitability for constrained IoT networks. Experiments on benchmark IoT\nbotnet datasets demonstrate that the framework achieves high detection accuracy\nwhile substantially reducing communication costs. These findings highlight\nfederated learning as a practical path toward scalable, secure, and\nprivacy-aware intrusion detection for IoT ecosystems.",
    "updated" : "2025-10-03T20:54:58Z",
    "published" : "2025-10-03T20:54:58Z",
    "authors" : [
      {
        "name" : "Taha M. Mahmoud"
      },
      {
        "name" : "Naima Kaabouch"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05959v1",
    "title" : "Distributed Platoon Control Under Quantization: Stability Analysis and\n  Privacy Preservation",
    "summary" : "Distributed control of connected and automated vehicles has attracted\nconsiderable interest for its potential to improve traffic efficiency and\nsafety. However, such control schemes require sharing privacy-sensitive vehicle\ndata, which introduces risks of information leakage and potential malicious\nactivities. This paper investigates the stability and privacy-preserving\nproperties of distributed platoon control under two types of quantizers:\ndeterministic and probabilistic. For deterministic quantization, we show that\nthe resulting control strategy ensures the system errors remain uniformly\nultimately bounded. Moreover, in the absence of auxiliary information, an\neavesdropper cannot uniquely infer sensitive vehicle states. In contrast, the\nuse of probabilistic quantization enables asymptotic convergence of the vehicle\nplatoon in expectation with bounded variance. Importantly, probabilistic\nquantizers can satisfy differential privacy guarantees, thereby preserving\nprivacy even when the eavesdropper possesses arbitrary auxiliary information.\nWe further analyze the trade-off between control performance and privacy by\nformulating an optimization problem that characterizes the impact of the\nquantization step on both metrics. Numerical simulations are provided to\nillustrate the performance differences between the two quantization strategies.",
    "updated" : "2025-10-07T14:16:59Z",
    "published" : "2025-10-07T14:16:59Z",
    "authors" : [
      {
        "name" : "Kaixiang Zhang"
      },
      {
        "name" : "Zhaojian Li"
      },
      {
        "name" : "Wei Lin"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05860v1",
    "title" : "Automated Boilerplate: Prevalence and Quality of Contract Generators in\n  the Context of Swiss Privacy Policies",
    "summary" : "It has become increasingly challenging for firms to comply with a plethora of\nnovel digital regulations. This is especially true for smaller businesses that\noften lack both the resources and know-how to draft complex legal documents.\nInstead of seeking costly legal advice from attorneys, firms may turn to\ncheaper alternative legal service providers such as automated contract\ngenerators. While these services have a long-standing presence, there is little\nempirical evidence on their prevalence and output quality.\n  We address this gap in the context of a 2023 Swiss privacy law revision. To\nenable a systematic evaluation, we create and annotate a multilingual benchmark\ndataset that captures key compliance obligations under Swiss and EU privacy\nlaw. Using this dataset, we validate a novel GPT-5-based method for large-scale\ncompliance assessment of privacy policies, allowing us to measure the impact of\nthe revision. We observe compliance increases indicating an effect of the\nrevision. Generators, explicitly referenced by 18% of local websites, are\nassociated with substantially higher levels of compliance, with increases of up\nto 15 percentage points compared to privacy policies without generator use.\nThese findings contribute to three debates: the potential of LLMs for\ncross-lingual legal analysis, the Brussels Effect of EU regulations, and,\ncrucially, the role of automated tools in improving compliance and contractual\nquality.",
    "updated" : "2025-10-07T12:30:01Z",
    "published" : "2025-10-07T12:30:01Z",
    "authors" : [
      {
        "name" : "Luka Nenadic"
      },
      {
        "name" : "David Rodriguez"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05807v1",
    "title" : "Privacy-Preserving On-chain Permissioning for KYC-Compliant\n  Decentralized Applications",
    "summary" : "Decentralized applications (dApps) in Decentralized Finance (DeFi) face a\nfundamental tension between regulatory compliance requirements like Know Your\nCustomer (KYC) and maintaining decentralization and privacy. Existing\npermissioned DeFi solutions often fail to adequately protect private attributes\nof dApp users and introduce implicit trust assumptions, undermining the\nblockchain's decentralization. Addressing these limitations, this paper\npresents a novel synthesis of Self-Sovereign Identity (SSI), Zero-Knowledge\nProofs (ZKPs), and Attribute-Based Access Control to enable privacy-preserving\non-chain permissioning based on decentralized policy decisions. We provide a\ncomprehensive framework for permissioned dApps that aligns decentralized trust,\nprivacy, and transparency, harmonizing blockchain principles with regulatory\ncompliance. Our framework supports multiple proof types (equality, range,\nmembership, and time-dependent) with efficient proof generation through a\ncommit-and-prove scheme that moves credential authenticity verification outside\nthe ZKP circuit. Experimental evaluation of our KYC-compliant DeFi\nimplementation shows considerable performance improvement for different proof\ntypes compared to baseline approaches. We advance the state-of-the-art through\na holistic approach, flexible proof mechanisms addressing diverse real-world\nrequirements, and optimized proof generation enabling practical deployment.",
    "updated" : "2025-10-07T11:24:51Z",
    "published" : "2025-10-07T11:24:51Z",
    "authors" : [
      {
        "name" : "Fabian Piper"
      },
      {
        "name" : "Karl Wolf"
      },
      {
        "name" : "Jonathan Heiss"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05803v1",
    "title" : "The Five Safes as a Privacy Context",
    "summary" : "The Five Safes is a framework used by national statistical offices (NSO) for\nassessing and managing the disclosure risk of data sharing. This paper makes\ntwo points: Firstly, the Five Safes can be understood as a specialization of a\nbroader concept $\\unicode{x2013}$ contextual integrity $\\unicode{x2013}$ to the\nsituation of statistical dissemination by an NSO. We demonstrate this by\nmapping the five parameters of contextual integrity onto the five dimensions of\nthe Five Safes. Secondly, the Five Safes contextualizes narrow, technical\nnotions of privacy within a holistic risk assessment. We demonstrate this with\nthe example of differential privacy (DP). This contextualization allows NSOs to\nplace DP within their Five Safes toolkit while also guiding the design of DP\nimplementations within the broader privacy context, as delineated by both their\nregulation and the relevant social norms.",
    "updated" : "2025-10-07T11:19:22Z",
    "published" : "2025-10-07T11:19:22Z",
    "authors" : [
      {
        "name" : "James Bailie"
      },
      {
        "name" : "Ruobin Gong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05535v1",
    "title" : "Permutation-Invariant Representation Learning for Robust and\n  Privacy-Preserving Feature Selection",
    "summary" : "Feature selection eliminates redundancy among features to improve downstream\ntask performance while reducing computational overhead. Existing methods often\nstruggle to capture intricate feature interactions and adapt across diverse\napplication scenarios. Recent advances employ generative intelligence to\nalleviate these drawbacks. However, these methods remain constrained by\npermutation sensitivity in embedding and reliance on convexity assumptions in\ngradient-based search. To address these limitations, our initial work\nintroduces a novel framework that integrates permutation-invariant embedding\nwith policy-guided search. Although effective, it still left opportunities to\nadapt to realistic distributed scenarios. In practice, data across local\nclients is highly imbalanced, heterogeneous and constrained by strict privacy\nregulations, limiting direct sharing. These challenges highlight the need for a\nframework that can integrate feature selection knowledge across clients without\nexposing sensitive information. In this extended journal version, we advance\nthe framework from two perspectives: 1) developing a privacy-preserving\nknowledge fusion strategy to derive a unified representation space without\nsharing sensitive raw data. 2) incorporating a sample-aware weighting strategy\nto address distributional imbalance among heterogeneous local clients.\nExtensive experiments validate the effectiveness, robustness, and efficiency of\nour framework. The results further demonstrate its strong generalization\nability in federated learning scenarios. The code and data are publicly\navailable: https://anonymous.4open.science/r/FedCAPS-08BF.",
    "updated" : "2025-10-07T02:53:32Z",
    "published" : "2025-10-07T02:53:32Z",
    "authors" : [
      {
        "name" : "Rui Liu"
      },
      {
        "name" : "Tao Zhe"
      },
      {
        "name" : "Yanjie Fu"
      },
      {
        "name" : "Feng Xia"
      },
      {
        "name" : "Ted Senator"
      },
      {
        "name" : "Dongjie Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05288v1",
    "title" : "DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language\n  Models Using Adam Optimization with Adaptive Clipping",
    "summary" : "Large language models (LLMs) such as ChatGPT have evolved into powerful and\nubiquitous tools. Fine-tuning on small datasets allows LLMs to acquire\nspecialized skills for specific tasks efficiently. Although LLMs provide great\nutility in both general and task-specific use cases, they are limited by two\nsecurity-related concerns. First, traditional LLM hardware requirements make\nthem infeasible to run locally on consumer-grade devices. A remote network\nconnection with the LLM provider's server is usually required, making the\nsystem vulnerable to network attacks. Second, fine-tuning an LLM for a\nsensitive task may involve sensitive data. Non-private fine-tuning algorithms\nproduce models vulnerable to training data reproduction attacks. Our work\naddresses these security concerns by enhancing differentially private\noptimization algorithms and applying them to fine-tune localizable language\nmodels. We introduce adaptable gradient clipping along with other engineering\nenhancements to the standard DP-Adam optimizer to create DP-Adam-AC. We use our\noptimizer to fine-tune examples of two localizable LLM designs, small language\nmodel (Qwen2.5-0.5B) and 1.58 bit quantization (Bitnet-b1.58-2B). We\ndemonstrate promising improvements in loss through experimentation with two\nsynthetic datasets.",
    "updated" : "2025-10-06T18:56:15Z",
    "published" : "2025-10-06T18:56:15Z",
    "authors" : [
      {
        "name" : "Ruoxing Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05172v1",
    "title" : "Learning More with Less: A Generalizable, Self-Supervised Framework for\n  Privacy-Preserving Capacity Estimation with EV Charging Data",
    "summary" : "Accurate battery capacity estimation is key to alleviating consumer concerns\nabout battery performance and reliability of electric vehicles (EVs). However,\npractical data limitations imposed by stringent privacy regulations and labeled\ndata shortages hamper the development of generalizable capacity estimation\nmodels that remain robust to real-world data distribution shifts. While\nself-supervised learning can leverage unlabeled data, existing techniques are\nnot particularly designed to learn effectively from challenging field data --\nlet alone from privacy-friendly data, which are often less feature-rich and\nnoisier. In this work, we propose a first-of-its-kind capacity estimation model\nbased on self-supervised pre-training, developed on a large-scale dataset of\nprivacy-friendly charging data snippets from real-world EV operations. Our\npre-training framework, snippet similarity-weighted masked input\nreconstruction, is designed to learn rich, generalizable representations even\nfrom less feature-rich and fragmented privacy-friendly data. Our key innovation\nlies in harnessing contrastive learning to first capture high-level\nsimilarities among fragmented snippets that otherwise lack meaningful context.\nWith our snippet-wise contrastive learning and subsequent similarity-weighted\nmasked reconstruction, we are able to learn rich representations of both\ngranular charging patterns within individual snippets and high-level\nassociative relationships across different snippets. Bolstered by this rich\nrepresentation learning, our model consistently outperforms state-of-the-art\nbaselines, achieving 31.9% lower test error than the best-performing benchmark,\neven under challenging domain-shifted settings affected by both manufacturer\nand age-induced distribution shifts.",
    "updated" : "2025-10-05T08:58:35Z",
    "published" : "2025-10-05T08:58:35Z",
    "authors" : [
      {
        "name" : "Anushiya Arunan"
      },
      {
        "name" : "Yan Qin"
      },
      {
        "name" : "Xiaoli Li"
      },
      {
        "name" : "U-Xuan Tan"
      },
      {
        "name" : "H. Vincent Poor"
      },
      {
        "name" : "Chau Yuen"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07176v1",
    "title" : "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of\n  Privacy Risks in LLM Agent Interactions",
    "summary" : "Large Language Models (LLMs) are increasingly deployed as agents that\norchestrate tasks and integrate external tools to execute complex workflows. We\ndemonstrate that these interactive behaviors leave distinctive fingerprints in\nencrypted traffic exchanged between users and LLM agents. By analyzing traffic\npatterns associated with agent workflows and tool invocations, adversaries can\ninfer agent activities, distinguish specific agents, and even profile sensitive\nuser attributes. To highlight this risk, we develop AgentPrint, which achieves\nan F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3\naccuracy in user attribute inference for simulated- and real-user settings,\nrespectively. These results uncover an overlooked risk: the very interactivity\nthat empowers LLM agents also exposes user privacy, underscoring the urgent\nneed for technical countermeasures alongside regulatory and policy safeguards.",
    "updated" : "2025-10-08T16:16:23Z",
    "published" : "2025-10-08T16:16:23Z",
    "authors" : [
      {
        "name" : "Yixiang Zhang"
      },
      {
        "name" : "Xinhao Deng"
      },
      {
        "name" : "Zhongyi Gu"
      },
      {
        "name" : "Yihao Chen"
      },
      {
        "name" : "Ke Xu"
      },
      {
        "name" : "Qi Li"
      },
      {
        "name" : "Jianping Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07136v1",
    "title" : "Spectral Graph Clustering under Differential Privacy: Balancing Privacy,\n  Accuracy, and Efficiency",
    "summary" : "We study the problem of spectral graph clustering under edge differential\nprivacy (DP). Specifically, we develop three mechanisms: (i) graph perturbation\nvia randomized edge flipping combined with adjacency matrix shuffling, which\nenforces edge privacy while preserving key spectral properties of the graph.\nImportantly, shuffling considerably amplifies the guarantees: whereas flipping\nedges with a fixed probability alone provides only a constant epsilon edge DP\nguarantee as the number of nodes grows, the shuffled mechanism achieves\n(epsilon, delta) edge DP with parameters that tend to zero as the number of\nnodes increase; (ii) private graph projection with additive Gaussian noise in a\nlower-dimensional space to reduce dimensionality and computational complexity;\nand (iii) a noisy power iteration method that distributes Gaussian noise across\niterations to ensure edge DP while maintaining convergence. Our analysis\nprovides rigorous privacy guarantees and a precise characterization of the\nmisclassification error rate. Experiments on synthetic and real-world networks\nvalidate our theoretical analysis and illustrate the practical privacy-utility\ntrade-offs.",
    "updated" : "2025-10-08T15:30:27Z",
    "published" : "2025-10-08T15:30:27Z",
    "authors" : [
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "H. Vincent Poor"
      },
      {
        "name" : "Andrea J. Goldsmith"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.LG",
      "cs.SI",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.06326v1",
    "title" : "Composable privacy of networked quantum sensing",
    "summary" : "Networks of sensors are a promising scheme to deliver the benefits of quantum\ntechnologies in coming years, offering enhanced precision and accuracy for\ndistributed metrology through the use of large entangled states. Recent work\nhas additionally explored the privacy of these schemes, meaning that local\nparameters can be kept secret while a joint function of these is estimated by\nthe network. In this work, we use the abstract cryptography framework to relate\nthe two proposed definitions of quasi-privacy, showing that both are\ncomposable, which enables the protocol to be securely included as a sub-routine\nto other schemes. We give an explicit example that estimating the mean of a set\nof parameters using GHZ states is composably fully secure.",
    "updated" : "2025-10-07T18:00:04Z",
    "published" : "2025-10-07T18:00:04Z",
    "authors" : [
      {
        "name" : "Naomi R. Solomons"
      },
      {
        "name" : "Damian Markham"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.06267v1",
    "title" : "RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating\n  Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases",
    "summary" : "We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion\nframework that generates realistic yet privacy-preserving synthetic\nelectronic-health-record (EHR) trajectories for ultra-rare diseases.\nRareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human\nPhenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA\nAdverse Event Reporting System (FAERS) into a heterogeneous knowledge graph\ncomprising approximately 8 M typed edges. Meta-path scores extracted from this\n8-million-edge KG modulate the per-token noise schedule in the forward\nstochastic differential equation, steering generation toward biologically\nplausible lab-medication-adverse-event co-occurrences while retaining\nscore-based diffusion model stability. The reverse denoiser then produces\ntimestamped sequences of lab-code, medication-code, and adverse-event-flag\ntriples that contain no protected health information. On simulated\nultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean\nDiscrepancy by 40 percent relative to an unguided diffusion baseline and by\ngreater than 60 percent versus GAN counterparts, without sacrificing downstream\npredictive utility. A black-box membership-inference evaluation using the\nDOMIAS attacker yields AUROC approximately 0.53, well below the 0.55\nsafe-release threshold and substantially better than the approximately 0.61\nplus or minus 0.03 observed for non-KG baselines, demonstrating strong\nresistance to re-identification. These results suggest that integrating\nbiomedical knowledge graphs directly into diffusion noise schedules can\nsimultaneously enhance fidelity and privacy, enabling safer data sharing for\nrare-disease research.",
    "updated" : "2025-10-06T03:59:09Z",
    "published" : "2025-10-06T03:59:09Z",
    "authors" : [
      {
        "name" : "Khartik Uppalapati"
      },
      {
        "name" : "Shakeel Abdulkareem"
      },
      {
        "name" : "Bora Yimenicioglu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "I.2.6; H.2.8; J.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.08355v1",
    "title" : "ExPrESSO: Zero-Knowledge backed Extensive Privacy Preserving Single\n  Sign-on",
    "summary" : "User authentication is one of the most important aspects for secure\ncommunication between services and end-users over the Internet. Service\nproviders leverage Single-Sign On (SSO) to make it easier for their users to\nauthenticate themselves. However, standardized systems for SSO, such as OIDC,\ndo not guarantee user privacy as identity providers can track user activities.\nWe propose a zero-knowledge-based mechanism that integrates with OIDC to let\nusers authenticate through SSO without revealing information about the service\nprovider. Our system leverages Groth's zk-SNARK to prove membership of\nsubscribed service providers without revealing their identity. We adopt a\ndecentralized and verifiable approach to set up the prerequisites of our\nconstruction that further secures and establishes trust in the system. We set\nup high security targets and achieve them with minimal storage and latency\ncost, proving that our research can be adopted for production.",
    "updated" : "2025-10-09T15:42:01Z",
    "published" : "2025-10-09T15:42:01Z",
    "authors" : [
      {
        "name" : "Kaustabh Barman"
      },
      {
        "name" : "Fabian Piper"
      },
      {
        "name" : "Sanjeet Raj Pandey"
      },
      {
        "name" : "Axel Kuepper"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.08247v1",
    "title" : "The Right to Communications Confidentiality in Europe: Protecting\n  Privacy, Freedom of Expression, and Trust",
    "summary" : "In the European Union, the General Data Protection Regulation (GDPR) provides\ncomprehensive rules for the processing of personal data. In addition, the EU\nlawmaker intends to adopt specific rules to protect confidentiality of\ncommunications, in a separate ePrivacy Regulation. Some have argued that there\nis no need for such additional rules for communications confidentiality. This\nArticle discusses the protection of the right to confidentiality of\ncommunications in Europe. We look at the right's origins to assess the\nrationale for protecting it. We also analyze how the right is currently\nprotected under the European Convention on Human Rights and under EU law. We\nshow that at its core the right to communications confidentiality protects\nthree individual and collective values: privacy, freedom of expression, and\ntrust in communication services. The right aims to ensure that individuals and\norganizations can safely entrust communication to service providers. Initially,\nthe right protected only postal letters, but it has gradually developed into a\nstrong safeguard for the protection of confidentiality of communications,\nregardless of the technology used. Hence, the right does not merely serve\nindividual privacy interests, but also other more collective interests that are\ncrucial for the functioning of our information society. We conclude that\nseparate EU rules to protect communications confidentiality, next to the GDPR,\nare justified and necessary.",
    "updated" : "2025-10-09T14:05:36Z",
    "published" : "2025-10-09T14:05:36Z",
    "authors" : [
      {
        "name" : "Frederik J. Zuiderveen Borgesius"
      },
      {
        "name" : "Wilfred Steenbruggen"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07976v1",
    "title" : "The impact of abstract and object tags on image privacy classification",
    "summary" : "Object tags denote concrete entities and are central to many computer vision\ntasks, whereas abstract tags capture higher-level information, which is\nrelevant for tasks that require a contextual, potentially subjective scene\nunderstanding. Object and abstract tags extracted from images also facilitate\ninterpretability. In this paper, we explore which type of tags is more suitable\nfor the context-dependent and inherently subjective task of image privacy.\nWhile object tags are generally used for privacy classification, we show that\nabstract tags are more effective when the tag budget is limited. Conversely,\nwhen a larger number of tags per image is available, object-related information\nis as useful. We believe that these findings will guide future research in\ndeveloping more accurate image privacy classifiers, informed by the role of tag\ntypes and quantity.",
    "updated" : "2025-10-09T09:09:02Z",
    "published" : "2025-10-09T09:09:02Z",
    "authors" : [
      {
        "name" : "Darya Baranouskaya"
      },
      {
        "name" : "Andrea Cavallaro"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07457v1",
    "title" : "Comparison of Fully Homomorphic Encryption and Garbled Circuit\n  Techniques in Privacy-Preserving Machine Learning Inference",
    "summary" : "Machine Learning (ML) is making its way into fields such as healthcare,\nfinance, and Natural Language Processing (NLP), and concerns over data privacy\nand model confidentiality continue to grow. Privacy-preserving Machine Learning\n(PPML) addresses this challenge by enabling inference on private data without\nrevealing sensitive inputs or proprietary models. Leveraging Secure Computation\ntechniques from Cryptography, two widely studied approaches in this domain are\nFully Homomorphic Encryption (FHE) and Garbled Circuits (GC). This work\npresents a comparative evaluation of FHE and GC for secure neural network\ninference. A two-layer neural network (NN) was implemented using the CKKS\nscheme from the Microsoft SEAL library (FHE) and the TinyGarble2.0 framework\n(GC) by IntelLabs. Both implementations are evaluated under the semi-honest\nthreat model, measuring inference output error, round-trip time, peak memory\nusage, communication overhead, and communication rounds. Results reveal a\ntrade-off: modular GC offers faster execution and lower memory consumption,\nwhile FHE supports non-interactive inference.",
    "updated" : "2025-10-08T19:03:40Z",
    "published" : "2025-10-08T19:03:40Z",
    "authors" : [
      {
        "name" : "Kalyan Cheerla"
      },
      {
        "name" : "Lotfi Ben Othmane"
      },
      {
        "name" : "Kirill Morozov"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07452v1",
    "title" : "PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware\n  Targeted Circuit PatcHing",
    "summary" : "Language models (LMs) may memorize personally identifiable information (PII)\nfrom training data, enabling adversaries to extract it during inference.\nExisting defense mechanisms such as differential privacy (DP) reduce this\nleakage, but incur large drops in utility. Based on a comprehensive study using\ncircuit discovery to identify the computational circuits responsible PII\nleakage in LMs, we hypothesize that specific PII leakage circuits in LMs should\nbe responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware\nTargeted Circuit PatcHing), a novel approach that first identifies and\nsubsequently directly edits PII circuits to reduce leakage. PATCH achieves\nbetter privacy-utility trade-off than existing defenses, e.g., reducing recall\nof PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to\nreduce recall of residual leakage of an LM to as low as 0.01%. Our analysis\nshows that PII leakage circuits persist even after the application of existing\ndefense mechanisms. In contrast, PATCH can effectively mitigate their impact.",
    "updated" : "2025-10-08T18:58:41Z",
    "published" : "2025-10-08T18:58:41Z",
    "authors" : [
      {
        "name" : "Anthony Hughes"
      },
      {
        "name" : "Vasisht Duddu"
      },
      {
        "name" : "N. Asokan"
      },
      {
        "name" : "Nikolaos Aletras"
      },
      {
        "name" : "Ning Ma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  }
]