[
  {
    "id" : "http://arxiv.org/abs/2510.01793v1",
    "title" : "Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of\n  Privacy Filters for Synthetic Data Generation",
    "summary" : "The generation of privacy-preserving synthetic datasets is a promising avenue\nfor overcoming data scarcity in medical AI research. Post-hoc privacy filtering\ntechniques, designed to remove samples containing personally identifiable\ninformation, have recently been proposed as a solution. However, their\neffectiveness remains largely unverified. This work presents a rigorous\nevaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary\nto claims from the original publications, our results demonstrate that current\nfilters exhibit limited specificity and consistency, achieving high sensitivity\nonly for real images while failing to reliably detect near-duplicates generated\nfrom training data. These results demonstrate a critical limitation of post-hoc\nfiltering: rather than effectively safeguarding patient privacy, these methods\nmay provide a false sense of security while leaving unacceptable levels of\npatient information exposed. We conclude that substantial advances in filter\ndesign are needed before these methods can be confidently deployed in sensitive\napplications.",
    "updated" : "2025-10-02T08:32:20Z",
    "published" : "2025-10-02T08:32:20Z",
    "authors" : [
      {
        "name" : "Adil Koeken"
      },
      {
        "name" : "Alexander Ziller"
      },
      {
        "name" : "Moritz Knolle"
      },
      {
        "name" : "Daniel Rueckert"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.01645v1",
    "title" : "Position: Privacy Is Not Just Memorization!",
    "summary" : "The discourse on privacy risks in Large Language Models (LLMs) has\ndisproportionately focused on verbatim memorization of training data, while a\nconstellation of more immediate and scalable privacy threats remain\nunderexplored. This position paper argues that the privacy landscape of LLM\nsystems extends far beyond training data extraction, encompassing risks from\ndata collection practices, inference-time context leakage, autonomous agent\ncapabilities, and the democratization of surveillance through deep inference\nattacks. We present a comprehensive taxonomy of privacy risks across the LLM\nlifecycle -- from data collection through deployment -- and demonstrate through\ncase studies how current privacy frameworks fail to address these multifaceted\nthreats. Through a longitudinal analysis of 1,322 AI/ML privacy papers\npublished at leading conferences over the past decade (2016--2025), we reveal\nthat while memorization receives outsized attention in technical research, the\nmost pressing privacy harms lie elsewhere, where current technical approaches\noffer little traction and viable paths forward remain unclear. We call for a\nfundamental shift in how the research community approaches LLM privacy, moving\nbeyond the narrow focus of current technical solutions and embracing\ninterdisciplinary approaches that address the sociotechnical nature of these\nemerging threats.",
    "updated" : "2025-10-02T04:02:06Z",
    "published" : "2025-10-02T04:02:06Z",
    "authors" : [
      {
        "name" : "Niloofar Mireshghallah"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.01113v1",
    "title" : "Privacy Preserved Federated Learning with Attention-Based Aggregation\n  for Biometric Recognition",
    "summary" : "Because biometric data is sensitive, centralized training poses a privacy\nrisk, even though biometric recognition is essential for contemporary\napplications. Federated learning (FL), which permits decentralized training,\nprovides a privacy-preserving substitute. Conventional FL, however, has trouble\nwith interpretability and heterogeneous data (non-IID). In order to handle\nnon-IID biometric data, this framework adds an attention mechanism at the\ncentral server that weights local model updates according to their\nsignificance. Differential privacy and secure update protocols safeguard data\nwhile preserving accuracy. The A3-FL framework is evaluated in this study using\nFVC2004 fingerprint data, with each client's features extracted using a Siamese\nConvolutional Neural Network (Siamese-CNN). By dynamically modifying client\ncontributions, the attention mechanism increases the accuracy of the global\nmodel.The accuracy, convergence speed, and robustness of the A3-FL framework\nare superior to those of standard FL (FedAvg) and static baselines, according\nto experimental evaluations using fingerprint data (FVC2004). The accuracy of\nthe attention-based approach was 0.8413, while FedAvg, Local-only, and\nCentralized approaches were 0.8164, 0.7664, and 0.7997, respectively. Accuracy\nstayed high at 0.8330 even with differential privacy. A scalable and\nprivacy-sensitive biometric system for secure and effective recognition in\ndispersed environments is presented in this work.",
    "updated" : "2025-10-01T16:58:59Z",
    "published" : "2025-10-01T16:58:59Z",
    "authors" : [
      {
        "name" : "Kassahun Azezew"
      },
      {
        "name" : "Minyechil Alehegn"
      },
      {
        "name" : "Tsega Asresa"
      },
      {
        "name" : "Bitew Mekuria"
      },
      {
        "name" : "Tizazu Bayh"
      },
      {
        "name" : "Ayenew Kassie"
      },
      {
        "name" : "Amsalu Tesema"
      },
      {
        "name" : "Animut Embiyale"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.00909v1",
    "title" : "\"We are not Future-ready\": Understanding AI Privacy Risks and Existing\n  Mitigation Strategies from the Perspective of AI Developers in Europe",
    "summary" : "The proliferation of AI has sparked privacy concerns related to training\ndata, model interfaces, downstream applications, and more. We interviewed 25 AI\ndevelopers based in Europe to understand which privacy threats they believe\npose the greatest risk to users, developers, and businesses and what protective\nstrategies, if any, would help to mitigate them. We find that there is little\nconsensus among AI developers on the relative ranking of privacy risks. These\ndifferences stem from salient reasoning patterns that often relate to human\nrather than purely technical factors. Furthermore, while AI developers are\naware of proposed mitigation strategies for addressing these risks, they\nreported minimal real-world adoption. Our findings highlight both gaps and\nopportunities for empowering AI developers to better address privacy risks in\nAI.",
    "updated" : "2025-10-01T13:51:33Z",
    "published" : "2025-10-01T13:51:33Z",
    "authors" : [
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Patrick Gage Kelley"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Kurt Thomas"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.00478v1",
    "title" : "Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving\n  Domain Adaptation",
    "summary" : "Recent work on latent diffusion models (LDMs) has focused almost exclusively\non generative tasks, leaving their potential for discriminative transfer\nlargely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a\nnovel LDM-based framework for a more practical variant of source-free domain\nadaptation (SFDA): the source provider may share not only a pre-trained\nclassifier but also an auxiliary latent diffusion module, trained once on the\nsource data and never exposing raw source samples. DVD encodes each source\nfeature's label information into its latent vicinity by fitting a Gaussian\nprior over its k-nearest neighbors and training the diffusion network to drift\nnoisy samples back to label-consistent representations. During adaptation, we\nsample from each target feature's latent vicinity, apply the frozen diffusion\nmodule to generate source-like cues, and use a simple InfoNCE loss to align the\ntarget encoder to these cues, explicitly transferring decision boundaries\nwithout source access. Across standard SFDA benchmarks, DVD outperforms\nstate-of-the-art methods. We further show that the same latent diffusion module\nenhances the source classifier's accuracy on in-domain data and boosts\nperformance in supervised classification and domain generalization experiments.\nDVD thus reinterprets LDMs as practical, privacy-preserving bridges for\nexplicit knowledge transfer, addressing a core challenge in source-free domain\nadaptation that prior methods have yet to solve.",
    "updated" : "2025-10-01T03:58:26Z",
    "published" : "2025-10-01T03:58:26Z",
    "authors" : [
      {
        "name" : "Jing Wang"
      },
      {
        "name" : "Wonho Bae"
      },
      {
        "name" : "Jiahong Chen"
      },
      {
        "name" : "Wenxu Wang"
      },
      {
        "name" : "Junhyug Noh"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03035v1",
    "title" : "Protecting Persona Biometric Data: The Case of Facial Privacy",
    "summary" : "The proliferation of digital technologies has led to unprecedented data\ncollection, with facial data emerging as a particularly sensitive commodity.\nCompanies are increasingly leveraging advanced facial recognition technologies,\noften without the explicit consent or awareness of individuals, to build\nsophisticated surveillance capabilities. This practice, fueled by weak and\nfragmented laws in many jurisdictions, has created a regulatory vacuum that\nallows for the commercialization of personal identity and poses significant\nthreats to individual privacy and autonomy. This article introduces the concept\nof Facial Privacy. It analyzes the profound challenges posed by unregulated\nfacial recognition by conducting a comprehensive review of existing legal\nframeworks. It examines and compares regulations such as the GDPR, Brazil's\nLGPD, Canada's PIPEDA, and privacy acts in China, Singapore, South Korea, and\nJapan, alongside sector-specific laws in the United States like the Illinois\nBiometric Information Privacy Act (BIPA). The analysis highlights the societal\nimpacts of this technology, including the potential for discriminatory bias and\nthe long-lasting harm that can result from the theft of immutable biometric\ndata. Ultimately, the paper argues that existing legal loopholes and\nambiguities leave individuals vulnerable. It proposes a new policy framework\nthat shifts the paradigm from data as property to a model of inalienable\nrights, ensuring that fundamental human rights are upheld against unchecked\ntechnological expansion.",
    "updated" : "2025-10-03T14:16:33Z",
    "published" : "2025-10-03T14:16:33Z",
    "authors" : [
      {
        "name" : "Lambert Hogenhout"
      },
      {
        "name" : "Rinzin Wangmo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.02487v1",
    "title" : "Interplay between Security, Privacy and Trust in 6G-enabled Intelligent\n  Transportation Systems",
    "summary" : "The advancement of 6G technology has the potential to revolutionize the\ntransportation sector and significantly improve how we travel. 6G-enabled\nIntelligent Transportation Systems (ITS) promise to offer high-speed,\nlow-latency communication and advanced data analytics capabilities, supporting\nthe development of safer, more efficient, and more sustainable transportation\nsolutions. However, various security and privacy challenges were identified in\nthe literature that must be addressed to enable the safe and secure deployment\nof 6G-ITS and ensure people's trust in using these technologies. This paper\nreviews the opportunities and challenges of 6G-ITS, particularly focusing on\ntrust, security, and privacy, with special attention to quantum technologies\nthat both enhance security through quantum key distribution and introduce new\nvulnerabilities. It discusses the potential benefits of 6G technology in the\ntransportation sector, including improved communication, device\ninteroperability support, data analytic capabilities, and increased automation\nfor different components, such as transportation management and communication\nsystems. A taxonomy of different attack models in 6G-ITS is proposed, and a\ncomparison of the security threats in 5G-ITS and 6G-ITS is provided, along with\npotential mitigating solutions. This research highlights the urgent need for a\ncomprehensive, multi-layered security framework spanning physical\ninfrastructure protection, network protocol security, data management\nsafeguards, application security measures, and trust management systems to\neffectively mitigate emerging security and privacy risks and ensure the\nintegrity and resilience of future transportation ecosystems.",
    "updated" : "2025-10-02T18:47:36Z",
    "published" : "2025-10-02T18:47:36Z",
    "authors" : [
      {
        "name" : "Ahmed Danladi Abdullahi"
      },
      {
        "name" : "Erfan Bahrami"
      },
      {
        "name" : "Tooska Dargahi"
      },
      {
        "name" : "Mohammed Al-Khalidi"
      },
      {
        "name" : "Mohammad Hammoudeh"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05068v1",
    "title" : "Multi-Agent Distributed Optimization With Feasible Set Privacy",
    "summary" : "We consider the problem of decentralized constrained optimization with\nmultiple agents $E_1,\\ldots,E_N$ who jointly wish to learn the optimal solution\nset while keeping their feasible sets $\\mathcal{P}_1,\\ldots,\\mathcal{P}_N$\nprivate from each other. We assume that the objective function $f$ is known to\nall agents and each feasible set is a collection of points from a universal\nalphabet $\\mathcal{P}_{alph}$. A designated agent (leader) starts the\ncommunication with the remaining (non-leader) agents, and is the first to\nretrieve the solution set. The leader searches for the solution by sending\nqueries to and receiving answers from the non-leaders, such that the\ninformation on the individual feasible sets revealed to the leader should be no\nmore than nominal, i.e., what is revealed from learning the solution set alone.\nWe develop achievable schemes for obtaining the solution set at nominal\ninformation leakage, and characterize their communication costs under two\ncommunication setups between agents. In this work, we focus on two kinds of\nnetwork setups: i) ring, where each agent communicates with two adjacent\nagents, and ii) star, where only the leader communicates with the remaining\nagents. We show that, if the leader first learns the joint feasible set through\nan existing private set intersection (PSI) protocol and then deduces the\nsolution set, the information leaked to the leader is greater than nominal.\nMoreover, we draw connection of our schemes to threshold PSI (ThPSI), which is\na PSI-variant where the intersection is revealed only when its cardinality is\nlarger than a threshold value. Finally, for various realizations of $f$ mapped\nuniformly at random to a fixed range of values, our schemes are more\ncommunication-efficient with a high probability compared to retrieving the\nentire feasible set through PSI.",
    "updated" : "2025-10-06T17:45:57Z",
    "published" : "2025-10-06T17:45:57Z",
    "authors" : [
      {
        "name" : "Shreya Meel"
      },
      {
        "name" : "Sennur Ulukus"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.DC",
      "cs.NI",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04527v1",
    "title" : "Quantum capacity amplification via privacy",
    "summary" : "We investigate superadditivity of quantum capacity through private channels\nwhose Choi-Jamiolkowski operators are private states. This perspective links\nthe security structure of private states to quantum capacity and clarifies the\nrole of the shield system: information encoded in the shield system that would\notherwise leak to the environment can be recycled when paired with an assisting\nchannel, thereby boosting capacity. Our main contributions are threefold:\nFirstly, we develop a general framework that provides a sufficient condition\nfor capacity amplification, which is formulated in terms of the assisting\nchannel's Holevo information. As examples, we give explicit, dimension and\nparameter dependent amplification thresholds for erasure and depolarizing\nchannels. Secondly, assuming the Spin alignment conjecture, we derive a\nsingle-letter expression for the quantum capacity of a family of private\nchannels that are neither degradable, anti-degradable, nor PPT; as an\napplication, we construct channels with vanishing quantum capacity yet\nunbounded private capacity. Thirdly, we further analyze approximate private\nchannels: we give an alternative proof of superactivation that extends its\nvalidity to a broader parameter regime, and, by combining amplification bounds\nwith continuity estimates, we establish a metric separation showing that\nchannels exhibiting capacity amplification have nonzero diamond distance from\nthe set of anti-degradable channels, indicating that existing approximate\n(anti-)degradability bounds are not tight. We also revisit the computability of\nthe regularized quantum capacity and modestly suggest that this fundamental\nquestion still remains open.",
    "updated" : "2025-10-06T06:35:19Z",
    "published" : "2025-10-06T06:35:19Z",
    "authors" : [
      {
        "name" : "Peixue Wu"
      },
      {
        "name" : "Yunkai Wang"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.IT",
      "math.MP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04465v1",
    "title" : "Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM\n  Agents",
    "summary" : "Large Language Model (LLM) agents require personal information for\npersonalization in order to better act on users' behalf in daily tasks, but\nthis raises privacy concerns and a personalization-privacy dilemma. Agent's\nautonomy introduces both risks and opportunities, yet its effects remain\nunclear. To better understand this, we conducted a 3$\\times$3 between-subjects\nexperiment ($N=450$) to study how agent's autonomy level and personalization\ninfluence users' privacy concerns, trust and willingness to use, as well as the\nunderlying psychological processes. We find that personalization without\nconsidering users' privacy preferences increases privacy concerns and decreases\ntrust and willingness to use. Autonomy moderates these effects: Intermediate\nautonomy flattens the impact of personalization compared to No- and Full\nautonomy conditions. Our results suggest that rather than aiming for perfect\nmodel alignment in output generation, balancing autonomy of agent's action and\nuser control offers a promising path to mitigate the personalization-privacy\ndilemma.",
    "updated" : "2025-10-06T03:38:54Z",
    "published" : "2025-10-06T03:38:54Z",
    "authors" : [
      {
        "name" : "Zhiping Zhang"
      },
      {
        "name" : "Yi Evie Zhang"
      },
      {
        "name" : "Freda Shi"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04261v1",
    "title" : "VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient\n  Extraction of User Privacy",
    "summary" : "Large language models (LLMs) have been widely deployed in Conversational AIs\n(CAIs), while exposing privacy and security threats. Recent research shows that\nLLM-based CAIs can be manipulated to extract private information from human\nusers, posing serious security threats. However, the methods proposed in that\nstudy rely on a white-box setting that adversaries can directly modify the\nsystem prompt. This condition is unlikely to hold in real-world deployments.\nThe limitation raises a critical question: can unprivileged attackers still\ninduce such privacy risks in practical LLM-integrated applications? To address\nthis question, we propose \\textsc{VortexPIA}, a novel indirect prompt injection\nattack that induces privacy extraction in LLM-integrated applications under\nblack-box settings. By injecting token-efficient data containing false\nmemories, \\textsc{VortexPIA} misleads LLMs to actively request private\ninformation in batches. Unlike prior methods, \\textsc{VortexPIA} allows\nattackers to flexibly define multiple categories of sensitive data. We evaluate\n\\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,\nacross four benchmark datasets. The results show that \\textsc{VortexPIA}\nsignificantly outperforms baselines and achieves state-of-the-art (SOTA)\nperformance. It also demonstrates efficient privacy requests, reduced token\nconsumption, and enhanced robustness against defense mechanisms. We further\nvalidate \\textsc{VortexPIA} on multiple realistic open-source LLM-integrated\napplications, demonstrating its practical effectiveness.",
    "updated" : "2025-10-05T15:58:55Z",
    "published" : "2025-10-05T15:58:55Z",
    "authors" : [
      {
        "name" : "Yu Cui"
      },
      {
        "name" : "Sicheng Pan"
      },
      {
        "name" : "Yifei Liu"
      },
      {
        "name" : "Haibin Zhang"
      },
      {
        "name" : "Cong Zuo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04153v1",
    "title" : "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation",
    "summary" : "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost.",
    "updated" : "2025-10-05T11:09:10Z",
    "published" : "2025-10-05T11:09:10Z",
    "authors" : [
      {
        "name" : "Haoqi Wu"
      },
      {
        "name" : "Wei Dai"
      },
      {
        "name" : "Ming Xu"
      },
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Qiang Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04027v1",
    "title" : "Multi-Class Support Vector Machine with Differential Privacy",
    "summary" : "With the increasing need to safeguard data privacy in machine learning\nmodels, differential privacy (DP) is one of the major frameworks to build\nprivacy-preserving models. Support Vector Machines (SVMs) are widely used\ntraditional machine learning models due to their robust margin guarantees and\nstrong empirical performance in binary classification. However, applying DP to\nmulti-class SVMs is inadequate, as the standard one-versus-rest (OvR) and\none-versus-one (OvO) approaches repeatedly query each data sample when building\nmultiple binary classifiers, thus consuming the privacy budget proportionally\nto the number of classes. To overcome this limitation, we explore all-in-one\nSVM approaches for DP, which access each data sample only once to construct\nmulti-class SVM boundaries with margin maximization properties. We propose a\nnovel differentially Private Multi-class SVM (PMSVM) with weight and gradient\nperturbation methods, providing rigorous sensitivity and convergence analyses\nto ensure DP in all-in-one SVMs. Empirical results demonstrate that our\napproach surpasses existing DP-SVM methods in multi-class scenarios.",
    "updated" : "2025-10-05T04:25:16Z",
    "published" : "2025-10-05T04:25:16Z",
    "authors" : [
      {
        "name" : "Jinseong Park"
      },
      {
        "name" : "Yujin Choi"
      },
      {
        "name" : "Jaewook Lee"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03996v1",
    "title" : "FHEON: A Configurable Framework for Developing Privacy-Preserving Neural\n  Networks Using Homomorphic Encryption",
    "summary" : "The widespread adoption of Machine Learning as a Service raises critical\nprivacy and security concerns, particularly about data confidentiality and\ntrust in both cloud providers and the machine learning models. Homomorphic\nEncryption (HE) has emerged as a promising solution to this problems, allowing\ncomputations on encrypted data without decryption. Despite its potential,\nexisting approaches to integrate HE into neural networks are often limited to\nspecific architectures, leaving a wide gap in providing a framework for easy\ndevelopment of HE-friendly privacy-preserving neural network models similar to\nwhat we have in the broader field of machine learning. In this paper, we\npresent FHEON, a configurable framework for developing privacy-preserving\nconvolutional neural network (CNN) models for inference using HE. FHEON\nintroduces optimized and configurable implementations of privacy-preserving CNN\nlayers including convolutional layers, average pooling layers, ReLU activation\nfunctions, and fully connected layers. These layers are configured using\nparameters like input channels, output channels, kernel size, stride, and\npadding to support arbitrary CNN architectures. We assess the performance of\nFHEON using several CNN architectures, including LeNet-5, VGG-11, VGG- 16,\nResNet-20, and ResNet-34. FHEON maintains encrypted-domain accuracies within\n+/- 1% of their plaintext counterparts for ResNet-20 and LeNet-5 models.\nNotably, on a consumer-grade CPU, the models build on FHEON achieved 98.5%\naccuracy with a latency of 13 seconds on MNIST using LeNet-5, and 92.2%\naccuracy with a latency of 403 seconds on CIFAR-10 using ResNet-20.\nAdditionally, FHEON operates within a practical memory budget requiring not\nmore than 42.3 GB for VGG-16.",
    "updated" : "2025-10-05T02:12:44Z",
    "published" : "2025-10-05T02:12:44Z",
    "authors" : [
      {
        "name" : "Nges Brian Njungle"
      },
      {
        "name" : "Eric Jahns"
      },
      {
        "name" : "Michel A. Kinsy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03860v1",
    "title" : "Privacy Enhancement in Over-the-Air Federated Learning via Adaptive\n  Receive Scaling",
    "summary" : "In Federated Learning (FL) with over-the-air aggregation, the quality of the\nsignal received at the server critically depends on the receive scaling\nfactors. While a larger scaling factor can reduce the effective noise power and\nimprove training performance, it also compromises the privacy of devices by\nreducing uncertainty. In this work, we aim to adaptively design the receive\nscaling factors across training rounds to balance the trade-off between\ntraining convergence and privacy in an FL system under dynamic channel\nconditions. We formulate a stochastic optimization problem that minimizes the\noverall R\\'enyi differential privacy (RDP) leakage over the entire training\nprocess, subject to a long-term constraint that ensures convergence of the\nglobal loss function. Our problem depends on unknown future information, and we\nobserve that standard Lyapunov optimization is not applicable. Thus, we develop\na new online algorithm, termed AdaScale, based on a sequence of novel per-round\nproblems that can be solved efficiently. We further derive upper bounds on the\ndynamic regret and constraint violation of AdaSacle, establishing that it\nachieves diminishing dynamic regret in terms of time-averaged RDP leakage while\nensuring convergence of FL training to a stationary point. Numerical\nexperiments on canonical classification tasks show that our approach\neffectively reduces RDP and DP leakages compared with state-of-the-art\nbenchmarks without compromising learning performance.",
    "updated" : "2025-10-04T16:15:19Z",
    "published" : "2025-10-04T16:15:19Z",
    "authors" : [
      {
        "name" : "Faeze Moradi Kalarde"
      },
      {
        "name" : "Ben Liang"
      },
      {
        "name" : "Min Dong"
      },
      {
        "name" : "Yahia A. Eldemerdash Ahmed"
      },
      {
        "name" : "Ho Ting Cheng"
      }
    ],
    "categories" : [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03662v1",
    "title" : "Operationalizing Data Minimization for Privacy-Preserving LLM Prompting",
    "summary" : "The rapid deployment of large language models (LLMs) in consumer applications\nhas led to frequent exchanges of personal information. To obtain useful\nresponses, users often share more than necessary, increasing privacy risks via\nmemorization, context-based personalization, or security breaches. We present a\nframework to formally define and operationalize data minimization: for a given\nuser prompt and response model, quantifying the least privacy-revealing\ndisclosure that maintains utility, and we propose a priority-queue tree search\nto locate this optimal point within a privacy-ordered transformation space. We\nevaluated the framework on four datasets spanning open-ended conversations\n(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth\nanswers (CaseHold, MedQA), quantifying achievable data minimization with nine\nLLMs as the response model. Our results demonstrate that larger frontier LLMs\ncan tolerate stronger data minimization while maintaining task quality than\nsmaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for\nQwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that\nLLMs struggle to predict optimal data minimization directly, showing a bias\ntoward abstraction that leads to oversharing. This suggests not just a privacy\ngap, but a capability gap: models may lack awareness of what information they\nactually need to solve a task.",
    "updated" : "2025-10-04T04:20:18Z",
    "published" : "2025-10-04T04:20:18Z",
    "authors" : [
      {
        "name" : "Jijie Zhou"
      },
      {
        "name" : "Niloofar Mireshghallah"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03559v1",
    "title" : "PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating\n  Privacy Reviews in UX Design",
    "summary" : "UX professionals routinely conduct design reviews, yet privacy concerns are\noften overlooked -- not only due to limited tools, but more critically because\nof low intrinsic motivation. Limited privacy knowledge, weak empathy for\nunexpectedly affected users, and low confidence in identifying harms make it\ndifficult to address risks. We present PrivacyMotiv, an LLM-powered system that\nsupports privacy-oriented design diagnosis by generating speculative personas\nwith UX user journeys centered on individuals vulnerable to privacy risks.\nDrawing on narrative strategies, the system constructs relatable and\nattention-drawing scenarios that show how ordinary design choices may cause\nunintended harms, expanding the scope of privacy reflection in UX. In a\nwithin-subjects study with professional UX practitioners (N=16), we compared\nparticipants' self-proposed methods with PrivacyMotiv across two privacy review\ntasks. Results show significant improvements in empathy, intrinsic motivation,\nand perceived usefulness. This work contributes a promising privacy review\napproach which addresses the motivational barriers in privacy-aware UX.",
    "updated" : "2025-10-03T23:14:22Z",
    "published" : "2025-10-03T23:14:22Z",
    "authors" : [
      {
        "name" : "Zeya Chen"
      },
      {
        "name" : "Jianing Wen"
      },
      {
        "name" : "Ruth Schmidt"
      },
      {
        "name" : "Yaxing Yao"
      },
      {
        "name" : "Toby Jia-Jun Li"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03513v1",
    "title" : "A Lightweight Federated Learning Approach for Privacy-Preserving Botnet\n  Detection in IoT",
    "summary" : "The rapid growth of the Internet of Things (IoT) has expanded opportunities\nfor innovation but also increased exposure to botnet-driven cyberattacks.\nConventional detection methods often struggle with scalability, privacy, and\nadaptability in resource-constrained IoT environments. To address these\nchallenges, we present a lightweight and privacy-preserving botnet detection\nframework based on federated learning. This approach enables distributed\ndevices to collaboratively train models without exchanging raw data, thus\nmaintaining user privacy while preserving detection accuracy. A\ncommunication-efficient aggregation strategy is introduced to reduce overhead,\nensuring suitability for constrained IoT networks. Experiments on benchmark IoT\nbotnet datasets demonstrate that the framework achieves high detection accuracy\nwhile substantially reducing communication costs. These findings highlight\nfederated learning as a practical path toward scalable, secure, and\nprivacy-aware intrusion detection for IoT ecosystems.",
    "updated" : "2025-10-03T20:54:58Z",
    "published" : "2025-10-03T20:54:58Z",
    "authors" : [
      {
        "name" : "Taha M. Mahmoud"
      },
      {
        "name" : "Naima Kaabouch"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05959v1",
    "title" : "Distributed Platoon Control Under Quantization: Stability Analysis and\n  Privacy Preservation",
    "summary" : "Distributed control of connected and automated vehicles has attracted\nconsiderable interest for its potential to improve traffic efficiency and\nsafety. However, such control schemes require sharing privacy-sensitive vehicle\ndata, which introduces risks of information leakage and potential malicious\nactivities. This paper investigates the stability and privacy-preserving\nproperties of distributed platoon control under two types of quantizers:\ndeterministic and probabilistic. For deterministic quantization, we show that\nthe resulting control strategy ensures the system errors remain uniformly\nultimately bounded. Moreover, in the absence of auxiliary information, an\neavesdropper cannot uniquely infer sensitive vehicle states. In contrast, the\nuse of probabilistic quantization enables asymptotic convergence of the vehicle\nplatoon in expectation with bounded variance. Importantly, probabilistic\nquantizers can satisfy differential privacy guarantees, thereby preserving\nprivacy even when the eavesdropper possesses arbitrary auxiliary information.\nWe further analyze the trade-off between control performance and privacy by\nformulating an optimization problem that characterizes the impact of the\nquantization step on both metrics. Numerical simulations are provided to\nillustrate the performance differences between the two quantization strategies.",
    "updated" : "2025-10-07T14:16:59Z",
    "published" : "2025-10-07T14:16:59Z",
    "authors" : [
      {
        "name" : "Kaixiang Zhang"
      },
      {
        "name" : "Zhaojian Li"
      },
      {
        "name" : "Wei Lin"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05860v1",
    "title" : "Automated Boilerplate: Prevalence and Quality of Contract Generators in\n  the Context of Swiss Privacy Policies",
    "summary" : "It has become increasingly challenging for firms to comply with a plethora of\nnovel digital regulations. This is especially true for smaller businesses that\noften lack both the resources and know-how to draft complex legal documents.\nInstead of seeking costly legal advice from attorneys, firms may turn to\ncheaper alternative legal service providers such as automated contract\ngenerators. While these services have a long-standing presence, there is little\nempirical evidence on their prevalence and output quality.\n  We address this gap in the context of a 2023 Swiss privacy law revision. To\nenable a systematic evaluation, we create and annotate a multilingual benchmark\ndataset that captures key compliance obligations under Swiss and EU privacy\nlaw. Using this dataset, we validate a novel GPT-5-based method for large-scale\ncompliance assessment of privacy policies, allowing us to measure the impact of\nthe revision. We observe compliance increases indicating an effect of the\nrevision. Generators, explicitly referenced by 18% of local websites, are\nassociated with substantially higher levels of compliance, with increases of up\nto 15 percentage points compared to privacy policies without generator use.\nThese findings contribute to three debates: the potential of LLMs for\ncross-lingual legal analysis, the Brussels Effect of EU regulations, and,\ncrucially, the role of automated tools in improving compliance and contractual\nquality.",
    "updated" : "2025-10-07T12:30:01Z",
    "published" : "2025-10-07T12:30:01Z",
    "authors" : [
      {
        "name" : "Luka Nenadic"
      },
      {
        "name" : "David Rodriguez"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05807v1",
    "title" : "Privacy-Preserving On-chain Permissioning for KYC-Compliant\n  Decentralized Applications",
    "summary" : "Decentralized applications (dApps) in Decentralized Finance (DeFi) face a\nfundamental tension between regulatory compliance requirements like Know Your\nCustomer (KYC) and maintaining decentralization and privacy. Existing\npermissioned DeFi solutions often fail to adequately protect private attributes\nof dApp users and introduce implicit trust assumptions, undermining the\nblockchain's decentralization. Addressing these limitations, this paper\npresents a novel synthesis of Self-Sovereign Identity (SSI), Zero-Knowledge\nProofs (ZKPs), and Attribute-Based Access Control to enable privacy-preserving\non-chain permissioning based on decentralized policy decisions. We provide a\ncomprehensive framework for permissioned dApps that aligns decentralized trust,\nprivacy, and transparency, harmonizing blockchain principles with regulatory\ncompliance. Our framework supports multiple proof types (equality, range,\nmembership, and time-dependent) with efficient proof generation through a\ncommit-and-prove scheme that moves credential authenticity verification outside\nthe ZKP circuit. Experimental evaluation of our KYC-compliant DeFi\nimplementation shows considerable performance improvement for different proof\ntypes compared to baseline approaches. We advance the state-of-the-art through\na holistic approach, flexible proof mechanisms addressing diverse real-world\nrequirements, and optimized proof generation enabling practical deployment.",
    "updated" : "2025-10-07T11:24:51Z",
    "published" : "2025-10-07T11:24:51Z",
    "authors" : [
      {
        "name" : "Fabian Piper"
      },
      {
        "name" : "Karl Wolf"
      },
      {
        "name" : "Jonathan Heiss"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05803v1",
    "title" : "The Five Safes as a Privacy Context",
    "summary" : "The Five Safes is a framework used by national statistical offices (NSO) for\nassessing and managing the disclosure risk of data sharing. This paper makes\ntwo points: Firstly, the Five Safes can be understood as a specialization of a\nbroader concept $\\unicode{x2013}$ contextual integrity $\\unicode{x2013}$ to the\nsituation of statistical dissemination by an NSO. We demonstrate this by\nmapping the five parameters of contextual integrity onto the five dimensions of\nthe Five Safes. Secondly, the Five Safes contextualizes narrow, technical\nnotions of privacy within a holistic risk assessment. We demonstrate this with\nthe example of differential privacy (DP). This contextualization allows NSOs to\nplace DP within their Five Safes toolkit while also guiding the design of DP\nimplementations within the broader privacy context, as delineated by both their\nregulation and the relevant social norms.",
    "updated" : "2025-10-07T11:19:22Z",
    "published" : "2025-10-07T11:19:22Z",
    "authors" : [
      {
        "name" : "James Bailie"
      },
      {
        "name" : "Ruobin Gong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05535v1",
    "title" : "Permutation-Invariant Representation Learning for Robust and\n  Privacy-Preserving Feature Selection",
    "summary" : "Feature selection eliminates redundancy among features to improve downstream\ntask performance while reducing computational overhead. Existing methods often\nstruggle to capture intricate feature interactions and adapt across diverse\napplication scenarios. Recent advances employ generative intelligence to\nalleviate these drawbacks. However, these methods remain constrained by\npermutation sensitivity in embedding and reliance on convexity assumptions in\ngradient-based search. To address these limitations, our initial work\nintroduces a novel framework that integrates permutation-invariant embedding\nwith policy-guided search. Although effective, it still left opportunities to\nadapt to realistic distributed scenarios. In practice, data across local\nclients is highly imbalanced, heterogeneous and constrained by strict privacy\nregulations, limiting direct sharing. These challenges highlight the need for a\nframework that can integrate feature selection knowledge across clients without\nexposing sensitive information. In this extended journal version, we advance\nthe framework from two perspectives: 1) developing a privacy-preserving\nknowledge fusion strategy to derive a unified representation space without\nsharing sensitive raw data. 2) incorporating a sample-aware weighting strategy\nto address distributional imbalance among heterogeneous local clients.\nExtensive experiments validate the effectiveness, robustness, and efficiency of\nour framework. The results further demonstrate its strong generalization\nability in federated learning scenarios. The code and data are publicly\navailable: https://anonymous.4open.science/r/FedCAPS-08BF.",
    "updated" : "2025-10-07T02:53:32Z",
    "published" : "2025-10-07T02:53:32Z",
    "authors" : [
      {
        "name" : "Rui Liu"
      },
      {
        "name" : "Tao Zhe"
      },
      {
        "name" : "Yanjie Fu"
      },
      {
        "name" : "Feng Xia"
      },
      {
        "name" : "Ted Senator"
      },
      {
        "name" : "Dongjie Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05288v1",
    "title" : "DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language\n  Models Using Adam Optimization with Adaptive Clipping",
    "summary" : "Large language models (LLMs) such as ChatGPT have evolved into powerful and\nubiquitous tools. Fine-tuning on small datasets allows LLMs to acquire\nspecialized skills for specific tasks efficiently. Although LLMs provide great\nutility in both general and task-specific use cases, they are limited by two\nsecurity-related concerns. First, traditional LLM hardware requirements make\nthem infeasible to run locally on consumer-grade devices. A remote network\nconnection with the LLM provider's server is usually required, making the\nsystem vulnerable to network attacks. Second, fine-tuning an LLM for a\nsensitive task may involve sensitive data. Non-private fine-tuning algorithms\nproduce models vulnerable to training data reproduction attacks. Our work\naddresses these security concerns by enhancing differentially private\noptimization algorithms and applying them to fine-tune localizable language\nmodels. We introduce adaptable gradient clipping along with other engineering\nenhancements to the standard DP-Adam optimizer to create DP-Adam-AC. We use our\noptimizer to fine-tune examples of two localizable LLM designs, small language\nmodel (Qwen2.5-0.5B) and 1.58 bit quantization (Bitnet-b1.58-2B). We\ndemonstrate promising improvements in loss through experimentation with two\nsynthetic datasets.",
    "updated" : "2025-10-06T18:56:15Z",
    "published" : "2025-10-06T18:56:15Z",
    "authors" : [
      {
        "name" : "Ruoxing Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05172v1",
    "title" : "Learning More with Less: A Generalizable, Self-Supervised Framework for\n  Privacy-Preserving Capacity Estimation with EV Charging Data",
    "summary" : "Accurate battery capacity estimation is key to alleviating consumer concerns\nabout battery performance and reliability of electric vehicles (EVs). However,\npractical data limitations imposed by stringent privacy regulations and labeled\ndata shortages hamper the development of generalizable capacity estimation\nmodels that remain robust to real-world data distribution shifts. While\nself-supervised learning can leverage unlabeled data, existing techniques are\nnot particularly designed to learn effectively from challenging field data --\nlet alone from privacy-friendly data, which are often less feature-rich and\nnoisier. In this work, we propose a first-of-its-kind capacity estimation model\nbased on self-supervised pre-training, developed on a large-scale dataset of\nprivacy-friendly charging data snippets from real-world EV operations. Our\npre-training framework, snippet similarity-weighted masked input\nreconstruction, is designed to learn rich, generalizable representations even\nfrom less feature-rich and fragmented privacy-friendly data. Our key innovation\nlies in harnessing contrastive learning to first capture high-level\nsimilarities among fragmented snippets that otherwise lack meaningful context.\nWith our snippet-wise contrastive learning and subsequent similarity-weighted\nmasked reconstruction, we are able to learn rich representations of both\ngranular charging patterns within individual snippets and high-level\nassociative relationships across different snippets. Bolstered by this rich\nrepresentation learning, our model consistently outperforms state-of-the-art\nbaselines, achieving 31.9% lower test error than the best-performing benchmark,\neven under challenging domain-shifted settings affected by both manufacturer\nand age-induced distribution shifts.",
    "updated" : "2025-10-05T08:58:35Z",
    "published" : "2025-10-05T08:58:35Z",
    "authors" : [
      {
        "name" : "Anushiya Arunan"
      },
      {
        "name" : "Yan Qin"
      },
      {
        "name" : "Xiaoli Li"
      },
      {
        "name" : "U-Xuan Tan"
      },
      {
        "name" : "H. Vincent Poor"
      },
      {
        "name" : "Chau Yuen"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07176v1",
    "title" : "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of\n  Privacy Risks in LLM Agent Interactions",
    "summary" : "Large Language Models (LLMs) are increasingly deployed as agents that\norchestrate tasks and integrate external tools to execute complex workflows. We\ndemonstrate that these interactive behaviors leave distinctive fingerprints in\nencrypted traffic exchanged between users and LLM agents. By analyzing traffic\npatterns associated with agent workflows and tool invocations, adversaries can\ninfer agent activities, distinguish specific agents, and even profile sensitive\nuser attributes. To highlight this risk, we develop AgentPrint, which achieves\nan F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3\naccuracy in user attribute inference for simulated- and real-user settings,\nrespectively. These results uncover an overlooked risk: the very interactivity\nthat empowers LLM agents also exposes user privacy, underscoring the urgent\nneed for technical countermeasures alongside regulatory and policy safeguards.",
    "updated" : "2025-10-08T16:16:23Z",
    "published" : "2025-10-08T16:16:23Z",
    "authors" : [
      {
        "name" : "Yixiang Zhang"
      },
      {
        "name" : "Xinhao Deng"
      },
      {
        "name" : "Zhongyi Gu"
      },
      {
        "name" : "Yihao Chen"
      },
      {
        "name" : "Ke Xu"
      },
      {
        "name" : "Qi Li"
      },
      {
        "name" : "Jianping Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07136v1",
    "title" : "Spectral Graph Clustering under Differential Privacy: Balancing Privacy,\n  Accuracy, and Efficiency",
    "summary" : "We study the problem of spectral graph clustering under edge differential\nprivacy (DP). Specifically, we develop three mechanisms: (i) graph perturbation\nvia randomized edge flipping combined with adjacency matrix shuffling, which\nenforces edge privacy while preserving key spectral properties of the graph.\nImportantly, shuffling considerably amplifies the guarantees: whereas flipping\nedges with a fixed probability alone provides only a constant epsilon edge DP\nguarantee as the number of nodes grows, the shuffled mechanism achieves\n(epsilon, delta) edge DP with parameters that tend to zero as the number of\nnodes increase; (ii) private graph projection with additive Gaussian noise in a\nlower-dimensional space to reduce dimensionality and computational complexity;\nand (iii) a noisy power iteration method that distributes Gaussian noise across\niterations to ensure edge DP while maintaining convergence. Our analysis\nprovides rigorous privacy guarantees and a precise characterization of the\nmisclassification error rate. Experiments on synthetic and real-world networks\nvalidate our theoretical analysis and illustrate the practical privacy-utility\ntrade-offs.",
    "updated" : "2025-10-08T15:30:27Z",
    "published" : "2025-10-08T15:30:27Z",
    "authors" : [
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "H. Vincent Poor"
      },
      {
        "name" : "Andrea J. Goldsmith"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.LG",
      "cs.SI",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.06326v1",
    "title" : "Composable privacy of networked quantum sensing",
    "summary" : "Networks of sensors are a promising scheme to deliver the benefits of quantum\ntechnologies in coming years, offering enhanced precision and accuracy for\ndistributed metrology through the use of large entangled states. Recent work\nhas additionally explored the privacy of these schemes, meaning that local\nparameters can be kept secret while a joint function of these is estimated by\nthe network. In this work, we use the abstract cryptography framework to relate\nthe two proposed definitions of quasi-privacy, showing that both are\ncomposable, which enables the protocol to be securely included as a sub-routine\nto other schemes. We give an explicit example that estimating the mean of a set\nof parameters using GHZ states is composably fully secure.",
    "updated" : "2025-10-07T18:00:04Z",
    "published" : "2025-10-07T18:00:04Z",
    "authors" : [
      {
        "name" : "Naomi R. Solomons"
      },
      {
        "name" : "Damian Markham"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.06267v1",
    "title" : "RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating\n  Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases",
    "summary" : "We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion\nframework that generates realistic yet privacy-preserving synthetic\nelectronic-health-record (EHR) trajectories for ultra-rare diseases.\nRareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human\nPhenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA\nAdverse Event Reporting System (FAERS) into a heterogeneous knowledge graph\ncomprising approximately 8 M typed edges. Meta-path scores extracted from this\n8-million-edge KG modulate the per-token noise schedule in the forward\nstochastic differential equation, steering generation toward biologically\nplausible lab-medication-adverse-event co-occurrences while retaining\nscore-based diffusion model stability. The reverse denoiser then produces\ntimestamped sequences of lab-code, medication-code, and adverse-event-flag\ntriples that contain no protected health information. On simulated\nultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean\nDiscrepancy by 40 percent relative to an unguided diffusion baseline and by\ngreater than 60 percent versus GAN counterparts, without sacrificing downstream\npredictive utility. A black-box membership-inference evaluation using the\nDOMIAS attacker yields AUROC approximately 0.53, well below the 0.55\nsafe-release threshold and substantially better than the approximately 0.61\nplus or minus 0.03 observed for non-KG baselines, demonstrating strong\nresistance to re-identification. These results suggest that integrating\nbiomedical knowledge graphs directly into diffusion noise schedules can\nsimultaneously enhance fidelity and privacy, enabling safer data sharing for\nrare-disease research.",
    "updated" : "2025-10-06T03:59:09Z",
    "published" : "2025-10-06T03:59:09Z",
    "authors" : [
      {
        "name" : "Khartik Uppalapati"
      },
      {
        "name" : "Shakeel Abdulkareem"
      },
      {
        "name" : "Bora Yimenicioglu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "I.2.6; H.2.8; J.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.08355v1",
    "title" : "ExPrESSO: Zero-Knowledge backed Extensive Privacy Preserving Single\n  Sign-on",
    "summary" : "User authentication is one of the most important aspects for secure\ncommunication between services and end-users over the Internet. Service\nproviders leverage Single-Sign On (SSO) to make it easier for their users to\nauthenticate themselves. However, standardized systems for SSO, such as OIDC,\ndo not guarantee user privacy as identity providers can track user activities.\nWe propose a zero-knowledge-based mechanism that integrates with OIDC to let\nusers authenticate through SSO without revealing information about the service\nprovider. Our system leverages Groth's zk-SNARK to prove membership of\nsubscribed service providers without revealing their identity. We adopt a\ndecentralized and verifiable approach to set up the prerequisites of our\nconstruction that further secures and establishes trust in the system. We set\nup high security targets and achieve them with minimal storage and latency\ncost, proving that our research can be adopted for production.",
    "updated" : "2025-10-09T15:42:01Z",
    "published" : "2025-10-09T15:42:01Z",
    "authors" : [
      {
        "name" : "Kaustabh Barman"
      },
      {
        "name" : "Fabian Piper"
      },
      {
        "name" : "Sanjeet Raj Pandey"
      },
      {
        "name" : "Axel Kuepper"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.08247v1",
    "title" : "The Right to Communications Confidentiality in Europe: Protecting\n  Privacy, Freedom of Expression, and Trust",
    "summary" : "In the European Union, the General Data Protection Regulation (GDPR) provides\ncomprehensive rules for the processing of personal data. In addition, the EU\nlawmaker intends to adopt specific rules to protect confidentiality of\ncommunications, in a separate ePrivacy Regulation. Some have argued that there\nis no need for such additional rules for communications confidentiality. This\nArticle discusses the protection of the right to confidentiality of\ncommunications in Europe. We look at the right's origins to assess the\nrationale for protecting it. We also analyze how the right is currently\nprotected under the European Convention on Human Rights and under EU law. We\nshow that at its core the right to communications confidentiality protects\nthree individual and collective values: privacy, freedom of expression, and\ntrust in communication services. The right aims to ensure that individuals and\norganizations can safely entrust communication to service providers. Initially,\nthe right protected only postal letters, but it has gradually developed into a\nstrong safeguard for the protection of confidentiality of communications,\nregardless of the technology used. Hence, the right does not merely serve\nindividual privacy interests, but also other more collective interests that are\ncrucial for the functioning of our information society. We conclude that\nseparate EU rules to protect communications confidentiality, next to the GDPR,\nare justified and necessary.",
    "updated" : "2025-10-09T14:05:36Z",
    "published" : "2025-10-09T14:05:36Z",
    "authors" : [
      {
        "name" : "Frederik J. Zuiderveen Borgesius"
      },
      {
        "name" : "Wilfred Steenbruggen"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07976v1",
    "title" : "The impact of abstract and object tags on image privacy classification",
    "summary" : "Object tags denote concrete entities and are central to many computer vision\ntasks, whereas abstract tags capture higher-level information, which is\nrelevant for tasks that require a contextual, potentially subjective scene\nunderstanding. Object and abstract tags extracted from images also facilitate\ninterpretability. In this paper, we explore which type of tags is more suitable\nfor the context-dependent and inherently subjective task of image privacy.\nWhile object tags are generally used for privacy classification, we show that\nabstract tags are more effective when the tag budget is limited. Conversely,\nwhen a larger number of tags per image is available, object-related information\nis as useful. We believe that these findings will guide future research in\ndeveloping more accurate image privacy classifiers, informed by the role of tag\ntypes and quantity.",
    "updated" : "2025-10-09T09:09:02Z",
    "published" : "2025-10-09T09:09:02Z",
    "authors" : [
      {
        "name" : "Darya Baranouskaya"
      },
      {
        "name" : "Andrea Cavallaro"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07457v1",
    "title" : "Comparison of Fully Homomorphic Encryption and Garbled Circuit\n  Techniques in Privacy-Preserving Machine Learning Inference",
    "summary" : "Machine Learning (ML) is making its way into fields such as healthcare,\nfinance, and Natural Language Processing (NLP), and concerns over data privacy\nand model confidentiality continue to grow. Privacy-preserving Machine Learning\n(PPML) addresses this challenge by enabling inference on private data without\nrevealing sensitive inputs or proprietary models. Leveraging Secure Computation\ntechniques from Cryptography, two widely studied approaches in this domain are\nFully Homomorphic Encryption (FHE) and Garbled Circuits (GC). This work\npresents a comparative evaluation of FHE and GC for secure neural network\ninference. A two-layer neural network (NN) was implemented using the CKKS\nscheme from the Microsoft SEAL library (FHE) and the TinyGarble2.0 framework\n(GC) by IntelLabs. Both implementations are evaluated under the semi-honest\nthreat model, measuring inference output error, round-trip time, peak memory\nusage, communication overhead, and communication rounds. Results reveal a\ntrade-off: modular GC offers faster execution and lower memory consumption,\nwhile FHE supports non-interactive inference.",
    "updated" : "2025-10-08T19:03:40Z",
    "published" : "2025-10-08T19:03:40Z",
    "authors" : [
      {
        "name" : "Kalyan Cheerla"
      },
      {
        "name" : "Lotfi Ben Othmane"
      },
      {
        "name" : "Kirill Morozov"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07452v1",
    "title" : "PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware\n  Targeted Circuit PatcHing",
    "summary" : "Language models (LMs) may memorize personally identifiable information (PII)\nfrom training data, enabling adversaries to extract it during inference.\nExisting defense mechanisms such as differential privacy (DP) reduce this\nleakage, but incur large drops in utility. Based on a comprehensive study using\ncircuit discovery to identify the computational circuits responsible PII\nleakage in LMs, we hypothesize that specific PII leakage circuits in LMs should\nbe responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware\nTargeted Circuit PatcHing), a novel approach that first identifies and\nsubsequently directly edits PII circuits to reduce leakage. PATCH achieves\nbetter privacy-utility trade-off than existing defenses, e.g., reducing recall\nof PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to\nreduce recall of residual leakage of an LM to as low as 0.01%. Our analysis\nshows that PII leakage circuits persist even after the application of existing\ndefense mechanisms. In contrast, PATCH can effectively mitigate their impact.",
    "updated" : "2025-10-08T18:58:41Z",
    "published" : "2025-10-08T18:58:41Z",
    "authors" : [
      {
        "name" : "Anthony Hughes"
      },
      {
        "name" : "Vasisht Duddu"
      },
      {
        "name" : "N. Asokan"
      },
      {
        "name" : "Nikolaos Aletras"
      },
      {
        "name" : "Ning Ma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09443v1",
    "title" : "The Impact of Sanctions on decentralised Privacy Tools: A Case Study of\n  Tornado Cash",
    "summary" : "This paper investigates the impact of sanctions on Tornado Cash, a smart\ncontract protocol designed to enhance transaction privacy. Following the U.S.\nDepartment of the Treasury's sanctions against Tornado Cash in August 2022,\nplatform activity declined sharply. We document a significant and sustained\nreduction in transaction volume, user diversity, and overall protocol\nutilization after the sanctions were imposed. Our analysis draws on transaction\ndata from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We\nfurther examine developments following the partial lifting and eventual removal\nof sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025.\nAlthough activity partially recovered, the rebound remained limited. The\nTornado Cash case illustrates how regulatory interventions can affect\ndecentralized protocols, while also highlighting the challenges of fully\nenforcing such measures in decentralized environments.",
    "updated" : "2025-10-10T14:55:32Z",
    "published" : "2025-10-10T14:55:32Z",
    "authors" : [
      {
        "name" : "Raffaele Cristodaro"
      },
      {
        "name" : "Benjamin Kramer"
      },
      {
        "name" : "Claudio J. Tessone"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09253v1",
    "title" : "Zero-shot image privacy classification with Vision-Language Models",
    "summary" : "While specialized learning-based models have historically dominated image\nprivacy prediction, the current literature increasingly favours adopting large\nVision-Language Models (VLMs) designed for generic tasks. This trend risks\noverlooking the performance ceiling set by purpose-built models due to a lack\nof systematic evaluation. To address this problem, we establish a zero-shot\nbenchmark for image privacy classification, enabling a fair comparison. We\nevaluate the top-3 open-source VLMs, according to a privacy benchmark, using\ntask-aligned prompts and we contrast their performance, efficiency, and\nrobustness against established vision-only and multi-modal methods.\nCounter-intuitively, our results show that VLMs, despite their\nresource-intensive nature in terms of high parameter count and slower\ninference, currently lag behind specialized, smaller models in privacy\nprediction accuracy. We also find that VLMs exhibit higher robustness to image\nperturbations.",
    "updated" : "2025-10-10T10:50:16Z",
    "published" : "2025-10-10T10:50:16Z",
    "authors" : [
      {
        "name" : "Alina Elena Baia"
      },
      {
        "name" : "Alessio Xompero"
      },
      {
        "name" : "Andrea Cavallaro"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09155v1",
    "title" : "Federated Data Analytics for Cancer Immunotherapy: A Privacy-Preserving\n  Collaborative Platform for Patient Management",
    "summary" : "Connected health is a multidisciplinary approach focused on health\nmanagement, prioritizing pa-tient needs in the creation of tools, services, and\ntreatments. This paradigm ensures proactive and efficient care by facilitating\nthe timely exchange of accurate patient information among all stake-holders in\nthe care continuum. The rise of digital technologies and process innovations\npromises to enhance connected health by integrating various healthcare data\nsources. This integration aims to personalize care, predict health outcomes,\nand streamline patient management, though challeng-es remain, particularly in\ndata architecture, application interoperability, and security. Data analytics\ncan provide critical insights for informed decision-making and health\nco-creation, but solutions must prioritize end-users, including patients and\nhealthcare professionals. This perspective was explored through an agile System\nDevelopment Lifecycle in an EU-funded project aimed at developing an integrated\nAI-generated solution for managing cancer patients undergoing immunotherapy.\nThis paper contributes with a collaborative digital framework integrating\nstakeholders across the care continuum, leveraging federated big data analytics\nand artificial intelligence for improved decision-making while ensuring\nprivacy. Analytical capabilities, such as treatment recommendations and adverse\nevent predictions, were validated using real-life data, achieving 70%-90%\naccuracy in a pilot study with the medical partners, demonstrating the\nframework's effectiveness.",
    "updated" : "2025-10-10T08:57:41Z",
    "published" : "2025-10-10T08:57:41Z",
    "authors" : [
      {
        "name" : "Mira Raheem"
      },
      {
        "name" : "Michael Papazoglou"
      },
      {
        "name" : "Bernd Krmer"
      },
      {
        "name" : "Neamat El-Tazi"
      },
      {
        "name" : "Amal Elgammal"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09114v1",
    "title" : "On the Fairness of Privacy Protection: Measuring and Mitigating the\n  Disparity of Group Privacy Risks for Differentially Private Machine Learning",
    "summary" : "While significant progress has been made in conventional fairness-aware\nmachine learning (ML) and differentially private ML (DPML), the fairness of\nprivacy protection across groups remains underexplored. Existing studies have\nproposed methods to assess group privacy risks, but these are based on the\naverage-case privacy risks of data records. Such approaches may underestimate\nthe group privacy risks, thereby potentially underestimating the disparity\nacross group privacy risks. Moreover, the current method for assessing the\nworst-case privacy risks of data records is time-consuming, limiting their\npractical applicability. To address these limitations, we introduce a novel\nmembership inference game that can efficiently audit the approximate worst-case\nprivacy risks of data records. Experimental results demonstrate that our method\nprovides a more stringent measurement of group privacy risks, yielding a\nreliable assessment of the disparity in group privacy risks. Furthermore, to\npromote privacy protection fairness in DPML, we enhance the standard DP-SGD\nalgorithm with an adaptive group-specific gradient clipping strategy, inspired\nby the design of canaries in differential privacy auditing studies. Extensive\nexperiments confirm that our algorithm effectively reduces the disparity in\ngroup privacy risks, thereby enhancing the fairness of privacy protection in\nDPML.",
    "updated" : "2025-10-10T08:09:08Z",
    "published" : "2025-10-10T08:09:08Z",
    "authors" : [
      {
        "name" : "Zhi Yang"
      },
      {
        "name" : "Changwu Huang"
      },
      {
        "name" : "Ke Tang"
      },
      {
        "name" : "Xin Yao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.08813v1",
    "title" : "The Model's Language Matters: A Comparative Privacy Analysis of LLMs",
    "summary" : "Large Language Models (LLMs) are increasingly deployed across multilingual\napplications that handle sensitive data, yet their scale and linguistic\nvariability introduce major privacy risks. Mostly evaluated for English, this\npaper investigates how language structure affects privacy leakage in LLMs\ntrained on English, Spanish, French, and Italian medical corpora. We quantify\nsix linguistic indicators and evaluate three attack vectors: extraction,\ncounterfactual memorization, and membership inference. Results show that\nprivacy vulnerability scales with linguistic redundancy and tokenization\ngranularity: Italian exhibits the strongest leakage, while English shows higher\nmembership separability. In contrast, French and Spanish display greater\nresilience due to higher morphological complexity. Overall, our findings\nprovide the first quantitative evidence that language matters in privacy\nleakage, underscoring the need for language-aware privacy-preserving mechanisms\nin LLM deployments.",
    "updated" : "2025-10-09T20:59:42Z",
    "published" : "2025-10-09T20:59:42Z",
    "authors" : [
      {
        "name" : "Abhishek K. Mishra"
      },
      {
        "name" : "Antoine Boutet"
      },
      {
        "name" : "Lucas Magnana"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11640v1",
    "title" : "Continual Release of Densest Subgraphs: Privacy Amplification &\n  Sublinear Space via Subsampling",
    "summary" : "We study the sublinear space continual release model for edge-differentially\nprivate (DP) graph algorithms, with a focus on the densest subgraph problem\n(DSG) in the insertion-only setting. Our main result is the first continual\nrelease DSG algorithm that matches the additive error of the best static DP\nalgorithms and the space complexity of the best non-private streaming\nalgorithms, up to constants. The key idea is a refined use of subsampling that\nsimultaneously achieves privacy amplification and sparsification, a connection\nnot previously formalized in graph DP. Via a simple black-box reduction to the\nstatic setting, we obtain both pure and approximate-DP algorithms with $O(\\log\nn)$ additive error and $O(n\\log n)$ space, improving both accuracy and space\ncomplexity over the previous state of the art. Along the way, we introduce\ngraph densification in the graph DP setting, adding edges to trigger earlier\nsubsampling, which removes the extra logarithmic factors in error and space\nincurred by prior work [ELMZ25]. We believe this simple idea may be of\nindependent interest.",
    "updated" : "2025-10-13T17:20:13Z",
    "published" : "2025-10-13T17:20:13Z",
    "authors" : [
      {
        "name" : "Felix Zhou"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11514v1",
    "title" : "Toward Efficient and Privacy-Aware eHealth Systems: An Integrated\n  Sensing, Computing, and Semantic Communication Approach",
    "summary" : "Real-time and contactless monitoring of vital signs, such as respiration and\nheartbeat, alongside reliable communication, is essential for modern healthcare\nsystems, especially in remote and privacy-sensitive environments. Traditional\nwireless communication and sensing networks fall short in meeting all the\nstringent demands of eHealth, including accurate sensing, high data efficiency,\nand privacy preservation. To overcome the challenges, we propose a novel\nintegrated sensing, computing, and semantic communication (ISCSC) framework. In\nthe proposed system, a service robot utilises radar to detect patient positions\nand monitor their vital signs, while sending updates to the medical devices.\nInstead of transmitting raw physiological information, the robot computes and\ncommunicates semantically extracted health features to medical devices. This\nsemantic processing improves data throughput and preserves the clinical\nrelevance of the messages, while enhancing data privacy by avoiding the\ntransmission of sensitive data. Leveraging the estimated patient locations, the\nrobot employs an interacting multiple model (IMM) filter to actively track\npatient motion, thereby enabling robust beam steering for continuous and\nreliable monitoring. We then propose a joint optimisation of the beamforming\nmatrices and the semantic extraction ratio, subject to computing capability and\npower budget constraints, with the objective of maximising both the semantic\nsecrecy rate and sensing accuracy. Simulation results validate that the ISCSC\nframework achieves superior sensing accuracy, improved semantic transmission\nefficiency, and enhanced privacy preservation compared to conventional joint\nsensing and communication methods.",
    "updated" : "2025-10-13T15:21:32Z",
    "published" : "2025-10-13T15:21:32Z",
    "authors" : [
      {
        "name" : "Yinchao Yang"
      },
      {
        "name" : "Yahao Ding"
      },
      {
        "name" : "Zhaohui Yang"
      },
      {
        "name" : "Chongwen Huang"
      },
      {
        "name" : "Zhaoyang Zhang"
      },
      {
        "name" : "Dusit Niyato"
      },
      {
        "name" : "Mohammad Shikh-Bahaei"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11347v1",
    "title" : "Multi-View Graph Feature Propagation for Privacy Preservation and\n  Feature Sparsity",
    "summary" : "Graph Neural Networks (GNNs) have demonstrated remarkable success in node\nclassification tasks over relational data, yet their effectiveness often\ndepends on the availability of complete node features. In many real-world\nscenarios, however, feature matrices are highly sparse or contain sensitive\ninformation, leading to degraded performance and increased privacy risks.\nFurthermore, direct exposure of information can result in unintended data\nleakage, enabling adversaries to infer sensitive information. To address these\nchallenges, we propose a novel Multi-view Feature Propagation (MFP) framework\nthat enhances node classification under feature sparsity while promoting\nprivacy preservation. MFP extends traditional Feature Propagation (FP) by\ndividing the available features into multiple Gaussian-noised views, each\npropagating information independently through the graph topology. The\naggregated representations yield expressive and robust node embeddings. This\nframework is novel in two respects: it introduces a mechanism that improves\nrobustness under extreme sparsity, and it provides a principled way to balance\nutility with privacy. Extensive experiments conducted on graph datasets\ndemonstrate that MFP outperforms state-of-the-art baselines in node\nclassification while substantially reducing privacy leakage. Moreover, our\nanalysis demonstrates that propagated outputs serve as alternative imputations\nrather than reconstructions of the original features, preserving utility\nwithout compromising privacy. A comprehensive sensitivity analysis further\nconfirms the stability and practical applicability of MFP across diverse\nscenarios. Overall, MFP provides an effective and privacy-aware framework for\ngraph learning in domains characterized by missing or sensitive features.",
    "updated" : "2025-10-13T12:42:00Z",
    "published" : "2025-10-13T12:42:00Z",
    "authors" : [
      {
        "name" : "Etzion Harari"
      },
      {
        "name" : "Moshe Unger"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11299v1",
    "title" : "How to Get Actual Privacy and Utility from Privacy Models: the\n  k-Anonymity and Differential Privacy Families",
    "summary" : "Privacy models were introduced in privacy-preserving data publishing and\nstatistical disclosure control with the promise to end the need for costly\nempirical assessment of disclosure risk. We examine how well this promise is\nkept by the main privacy models. We find they may fail to provide adequate\nprotection guarantees because of problems in their definition or incur\nunacceptable trade-offs between privacy protection and utility preservation.\nSpecifically, k-anonymity may not entirely exclude disclosure if enforced with\ndeterministic mechanisms or without constraints on the confidential values. On\nthe other hand, differential privacy (DP) incurs unacceptable utility loss for\nsmall budgets and its privacy guarantee becomes meaningless for large budgets.\nIn the latter case, an ex post empirical assessment of disclosure risk becomes\nnecessary, undermining the main appeal of privacy models. Whereas the utility\npreservation of DP can only be improved by relaxing its privacy guarantees, we\nargue that a semantic reformulation of k-anonymity can offer more robust\nprivacy without losing utility with respect to traditional syntactic\nk-anonymity.",
    "updated" : "2025-10-13T11:41:12Z",
    "published" : "2025-10-13T11:41:12Z",
    "authors" : [
      {
        "name" : "Josep Domingo-Ferrer"
      },
      {
        "name" : "David Snchez"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "68",
      "K.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11116v1",
    "title" : "N-output Mechanism: Estimating Statistical Information from Numerical\n  Data under Local Differential Privacy",
    "summary" : "Local Differential Privacy (LDP) addresses significant privacy concerns in\nsensitive data collection. In this work, we focus on numerical data collection\nunder LDP, targeting a significant gap in the literature: existing LDP\nmechanisms are optimized for either a very small ($|\\Omega| \\in \\{2, 3\\}$) or\ninfinite output spaces. However, no generalized method for constructing an\noptimal mechanism for an arbitrary output size $N$ exists. To fill this gap, we\npropose the \\textbf{N-output mechanism}, a generalized framework that maps\nnumerical data to one of $N$ discrete outputs.\n  We formulate the mechanism's design as an optimization problem to minimize\nestimation variance for any given $N \\geq 2$ and develop both numerical and\nanalytical solutions. This results in a mechanism that is highly accurate and\nadaptive, as its design is determined by solving an optimization problem for\nany chosen $N$. Furthermore, we extend our framework and existing mechanisms to\nthe task of distribution estimation. Empirical evaluations show that the\nN-output mechanism achieves state-of-the-art accuracy for mean, variance, and\ndistribution estimation with small communication costs.",
    "updated" : "2025-10-13T08:06:59Z",
    "published" : "2025-10-13T08:06:59Z",
    "authors" : [
      {
        "name" : "Incheol Baek"
      },
      {
        "name" : "Yon Dohn Chung"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.10805v1",
    "title" : "Therapeutic AI and the Hidden Risks of Over-Disclosure: An Embedded\n  AI-Literacy Framework for Mental Health Privacy",
    "summary" : "Large Language Models (LLMs) are increasingly deployed in mental health\ncontexts, from structured therapeutic support tools to informal chat-based\nwell-being assistants. While these systems increase accessibility, scalability,\nand personalization, their integration into mental health care brings privacy\nand safety challenges that have not been well-examined. Unlike traditional\nclinical interactions, LLM-mediated therapy often lacks a clear structure for\nwhat information is collected, how it is processed, and how it is stored or\nreused. Users without clinical guidance may over-disclose personal information,\nwhich is sometimes irrelevant to their presenting concern, due to misplaced\ntrust, lack of awareness of data risks, or the conversational design of the\nsystem. This overexposure raises privacy concerns and also increases the\npotential for LLM bias, misinterpretation, and long-term data misuse. We\npropose a framework embedding Artificial Intelligence (AI) literacy\ninterventions directly into mental health conversational systems, and outline a\nstudy plan to evaluate their impact on disclosure safety, trust, and user\nexperience.",
    "updated" : "2025-10-12T20:50:06Z",
    "published" : "2025-10-12T20:50:06Z",
    "authors" : [
      {
        "name" : "Soraya S. Anvari"
      },
      {
        "name" : "Rina R. Wehbe"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.10316v1",
    "title" : "An information theorist's tour of differential privacy",
    "summary" : "Since being proposed in 2006, differential privacy has become a standard\nmethod for quantifying certain risks in publishing or sharing analyses of\nsensitive data. At its heart, differential privacy measures risk in terms of\nthe differences between probability distributions, which is a central topic in\ninformation theory. A differentially private algorithm is a channel between the\nunderlying data and the output of the analysis. Seen in this way, the\nguarantees made by differential privacy can be understood in terms of\nproperties of this channel. In this article we examine a few of the key\nconnections between information theory and the formulation/application of\ndifferential privacy, giving an ``operational significance'' for relevant\ninformation measures.",
    "updated" : "2025-10-11T18:54:05Z",
    "published" : "2025-10-11T18:54:05Z",
    "authors" : [
      {
        "name" : "Anand D. Sarwate"
      },
      {
        "name" : "Flavio P. Calmon"
      },
      {
        "name" : "Oliver Kosut"
      },
      {
        "name" : "Lalitha Sankar"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "math.IT",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09985v1",
    "title" : "Prismo: A Decision Support System for Privacy-Preserving ML Framework\n  Selection",
    "summary" : "Machine learning has become a crucial part of our lives, with applications\nspanning nearly every aspect of our daily activities. However, using personal\ninformation in machine learning applications has sparked significant security\nand privacy concerns about user data. To address these challenges, different\nprivacy-preserving machine learning (PPML) frameworks have been developed to\nprotect sensitive information in machine learning applications. These\nframeworks generally attempt to balance design trade-offs such as computational\nefficiency, communication overhead, security guarantees, and scalability.\nDespite the advancements, selecting the optimal framework and parameters for\nspecific deployment scenarios remains a complex and critical challenge for\nprivacy and security application developers.\n  We present Prismo, an open-source recommendation system designed to aid in\nselecting optimal parameters and frameworks for different PPML application\nscenarios. Prismo enables users to explore a comprehensive space of PPML\nframeworks through various properties based on user-defined objectives. It\nsupports automated filtering of suitable candidate frameworks by considering\nparameters such as the number of parties in multi-party computation or\nfederated learning and computation cost constraints in homomorphic encryption.\nPrismo models every use case into a Linear Integer Programming optimization\nproblem, ensuring tailored solutions are recommended for each scenario. We\nevaluate Prismo's effectiveness through multiple use cases, demonstrating its\nability to deliver best-fit solutions in different deployment scenarios.",
    "updated" : "2025-10-11T03:27:15Z",
    "published" : "2025-10-11T03:27:15Z",
    "authors" : [
      {
        "name" : "Nges Brian Njungle"
      },
      {
        "name" : "Eric Jahns"
      },
      {
        "name" : "Luigi Mastromauro"
      },
      {
        "name" : "Edwin P. Kayang"
      },
      {
        "name" : "Milan Stojkov"
      },
      {
        "name" : "Michel A. Kinsy"
      }
    ],
    "categories" : [
      "cs.CR",
      "I.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09443v2",
    "title" : "The Impact of Sanctions on decentralised Privacy Tools: A Case Study of\n  Tornado Cash",
    "summary" : "This paper investigates the impact of sanctions on Tornado Cash, a smart\ncontract protocol designed to enhance transaction privacy. Following the U.S.\nDepartment of the Treasury's sanctions against Tornado Cash in August 2022,\nplatform activity declined sharply. We document a significant and sustained\nreduction in transaction volume, user diversity, and overall protocol\nutilization after the sanctions were imposed. Our analysis draws on transaction\ndata from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We\nfurther examine developments following the partial lifting and eventual removal\nof sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025.\nAlthough activity partially recovered, the rebound remained limited. The\nTornado Cash case illustrates how regulatory interventions can affect\ndecentralized protocols, while also highlighting the challenges of fully\nenforcing such measures in decentralized environments.",
    "updated" : "2025-10-13T09:46:22Z",
    "published" : "2025-10-10T14:55:32Z",
    "authors" : [
      {
        "name" : "Raffaele Cristodaro"
      },
      {
        "name" : "Benjamin Kraner"
      },
      {
        "name" : "Claudio J. Tessone"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09715v1",
    "title" : "A Scalable, Privacy-Preserving Decentralized Identity and Verifiable\n  Data Sharing Framework based on Zero-Knowledge Proofs",
    "summary" : "With the proliferation of decentralized applications (DApps), the conflict\nbetween the transparency of blockchain technology and user data privacy has\nbecome increasingly prominent. While Decentralized Identity (DID) and\nVerifiable Credentials (VCs) provide a standardized framework for user data\nsovereignty, achieving trusted identity verification and data sharing without\ncompromising privacy remains a significant challenge. This paper proposes a\nnovel, comprehensive framework that integrates DIDs and VCs with efficient\nZero-Knowledge Proof (ZKP) schemes to address this core issue. The key\ncontributions of this framework are threefold: first, it constructs a set of\nstrong privacy-preserving protocols based on zk-STARKs, allowing users to prove\nthat their credentials satisfy specific conditions (e.g., \"age is over 18\")\nwithout revealing any underlying sensitive data. Second, it designs a scalable,\nprivacy-preserving credential revocation mechanism based on cryptographic\naccumulators, effectively solving credential management challenges in\nlarge-scale scenarios. Finally, it integrates a practical social key recovery\nscheme, significantly enhancing system usability and security. Through a\nprototype implementation and performance evaluation, this paper quantitatively\nanalyzes the framework's performance in terms of proof generation time,\nverification overhead, and on-chain costs. Compared to existing\nstate-of-the-art systems based on zk-SNARKs, our framework, at the cost of a\nlarger proof size, significantly improves prover efficiency for complex\ncomputations and provides stronger security guarantees, including no trusted\nsetup and post-quantum security. Finally, a case study in the decentralized\nfinance (DeFi) credit scoring scenario demonstrates the framework's immense\npotential for unlocking capital efficiency and fostering a trusted data\neconomy.",
    "updated" : "2025-10-10T06:06:05Z",
    "published" : "2025-10-10T06:06:05Z",
    "authors" : [
      {
        "name" : "Hui Yuan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09691v1",
    "title" : "Evaluation of Differential Privacy Mechanisms on Federated Learning",
    "summary" : "Federated learning is distributed model training across several clients\nwithout disclosing raw data. Despite advancements in data privacy, risks still\nremain. Differential Privacy (DP) is a technique to protect sensitive data by\nadding noise to model updates, usually controlled by a fixed privacy budget.\nHowever, this approach can introduce excessive noise, particularly when the\nmodel converges, which compromises performance. To address this problem,\nadaptive privacy budgets have been investigated as a potential solution. This\nwork implements DP methods using Laplace and Gaussian mechanisms with an\nadaptive privacy budget, extending the SelecEval simulator. We introduce an\nadaptive clipping approach in the Gaussian mechanism, ensuring that gradients\nof the model are dynamically updated rather than using a fixed sensitivity. We\nconduct extensive experiments with various privacy budgets, IID and non-IID\ndatasets, and different numbers of selected clients per round. While our\nexperiments were limited to 200 training rounds, the results suggest that\nadaptive privacy budgets and adaptive clipping can help maintain model accuracy\nwhile preserving privacy.",
    "updated" : "2025-10-09T11:32:36Z",
    "published" : "2025-10-09T11:32:36Z",
    "authors" : [
      {
        "name" : "Tejash Varsani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "68T07, 68M14",
      "I.2.6; I.2.11; K.6.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.00478v2",
    "title" : "Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving\n  Domain Adaptation",
    "summary" : "Recent work on latent diffusion models (LDMs) has focused almost exclusively\non generative tasks, leaving their potential for discriminative transfer\nlargely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a\nnovel LDM-based framework for a more practical variant of source-free domain\nadaptation (SFDA): the source provider may share not only a pre-trained\nclassifier but also an auxiliary latent diffusion module, trained once on the\nsource data and never exposing raw source samples. DVD encodes each source\nfeature's label information into its latent vicinity by fitting a Gaussian\nprior over its k-nearest neighbors and training the diffusion network to drift\nnoisy samples back to label-consistent representations. During adaptation, we\nsample from each target feature's latent vicinity, apply the frozen diffusion\nmodule to generate source-like cues, and use a simple InfoNCE loss to align the\ntarget encoder to these cues, explicitly transferring decision boundaries\nwithout source access. Across standard SFDA benchmarks, DVD outperforms\nstate-of-the-art methods. We further show that the same latent diffusion module\nenhances the source classifier's accuracy on in-domain data and boosts\nperformance in supervised classification and domain generalization experiments.\nDVD thus reinterprets LDMs as practical, privacy-preserving bridges for\nexplicit knowledge transfer, addressing a core challenge in source-free domain\nadaptation that prior methods have yet to solve.",
    "updated" : "2025-10-11T21:35:10Z",
    "published" : "2025-10-01T03:58:26Z",
    "authors" : [
      {
        "name" : "Jing Wang"
      },
      {
        "name" : "Wonho Bae"
      },
      {
        "name" : "Jiahong Chen"
      },
      {
        "name" : "Wenxu Wang"
      },
      {
        "name" : "Junhyug Noh"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12780v1",
    "title" : "Content Anonymization for Privacy in Long-form Audio",
    "summary" : "Voice anonymization techniques have been found to successfully obscure a\nspeaker's acoustic identity in short, isolated utterances in benchmarks such as\nthe VoicePrivacy Challenge. In practice, however, utterances seldom occur in\nisolation: long-form audio is commonplace in domains such as interviews, phone\ncalls, and meetings. In these cases, many utterances from the same speaker are\navailable, which pose a significantly greater privacy risk: given multiple\nutterances from the same speaker, an attacker could exploit an individual's\nvocabulary, syntax, and turns of phrase to re-identify them, even when their\nvoice is completely disguised. To address this risk, we propose new content\nanonymization approaches. Our approach performs a contextual rewriting of the\ntranscripts in an ASR-TTS pipeline to eliminate speaker-specific style while\npreserving meaning. We present results in a long-form telephone conversation\nsetting demonstrating the effectiveness of a content-based attack on\nvoice-anonymized speech. Then we show how the proposed content-based\nanonymization methods can mitigate this risk while preserving speech utility.\nOverall, we find that paraphrasing is an effective defense against\ncontent-based attacks and recommend that stakeholders adopt this step to ensure\nanonymity in long-form audio.",
    "updated" : "2025-10-14T17:52:50Z",
    "published" : "2025-10-14T17:52:50Z",
    "authors" : [
      {
        "name" : "Cristina Aggazzotti"
      },
      {
        "name" : "Ashi Garg"
      },
      {
        "name" : "Zexin Cai"
      },
      {
        "name" : "Nicholas Andrews"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12549v1",
    "title" : "Privacy-Preserving Distributed Estimation with Limited Data Rate",
    "summary" : "This paper focuses on the privacy-preserving distributed estimation problem\nwith a limited data rate, where the observations are the sensitive information.\nSpecifically, a binary-valued quantizer-based privacy-preserving distributed\nestimation algorithm is developed, which improves the algorithm's\nprivacy-preserving capability and simultaneously reduces the communication\ncosts. The algorithm's privacy-preserving capability, measured by the Fisher\ninformation matrix, is dynamically enhanced over time. Notably, the Fisher\ninformation matrix of the output signals with respect to the sensitive\ninformation converges to zero at a polynomial rate, and the improvement in\nprivacy brought by the quantizers is quantitatively characterized as a\nmultiplicative effect. Regarding the communication costs, each sensor transmits\nonly 1 bit of information to its neighbours at each time step. Additionally,\nthe assumption on the negligible quantization error for real-valued messages is\nnot required. While achieving the requirements of privacy preservation and\nreducing communication costs, the algorithm ensures that its estimates converge\nalmost surely to the true value of the unknown parameter by establishing a\nco-design guideline for the time-varying privacy noises and step-sizes. A\npolynomial almost sure convergence rate is obtained, and then the trade-off\nbetween privacy and convergence rate is established. Numerical examples\ndemonstrate the main results.",
    "updated" : "2025-10-14T14:13:32Z",
    "published" : "2025-10-14T14:13:32Z",
    "authors" : [
      {
        "name" : "Jieming Ke"
      },
      {
        "name" : "Jimin Wang"
      },
      {
        "name" : "Ji-Feng Zhang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12153v1",
    "title" : "VeilAudit: Breaking the Deadlock Between Privacy and Accountability\n  Across Blockchains",
    "summary" : "Cross chain interoperability in blockchain systems exposes a fundamental\ntension between user privacy and regulatory accountability. Existing solutions\nenforce an all or nothing choice between full anonymity and mandatory identity\ndisclosure, which limits adoption in regulated financial settings. We present\nVeilAudit, a cross chain auditing framework that introduces Auditor Only\nLinkability, which allows auditors to link transaction behaviors that originate\nfrom the same anonymous entity without learning its identity. VeilAudit\nachieves this with a user generated Linkable Audit Tag that embeds a zero\nknowledge proof to attest to its validity without exposing the user master\nwallet address, and with a special ciphertext that only designated auditors can\ntest for linkage. To balance privacy and compliance, VeilAudit also supports\nthreshold gated identity revelation under due process. VeilAudit further\nprovides a mechanism for building reputation in pseudonymous environments,\nwhich enables applications such as cross chain credit scoring based on\nverifiable behavioral history. We formalize the security guarantees and develop\na prototype that spans multiple EVM chains. Our evaluation shows that the\nframework is practical for today multichain environments.",
    "updated" : "2025-10-14T05:16:23Z",
    "published" : "2025-10-14T05:16:23Z",
    "authors" : [
      {
        "name" : "Minhao Qiao"
      },
      {
        "name" : "Iqbal Gondal"
      },
      {
        "name" : "Hai Dong"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12031v1",
    "title" : "Security and Privacy Assessment of U.S. and Non-U.S. Android E-Commerce\n  Applications",
    "summary" : "E-commerce mobile applications are central to global financial transactions,\nmaking their security and privacy crucial. In this study, we analyze 92\ntop-grossing Android e-commerce apps (58 U.S.-based and 34 international) using\nMobSF, AndroBugs, and RiskInDroid. Our analysis shows widespread SSL and\ncertificate weaknesses, with approximately 92% using unsecured HTTP connections\nand an average MobSF security score of 40.92/100. Over-privileged permissions\nwere identified in 77 apps. While U.S. apps exhibited fewer manifest, code, and\ncertificate vulnerabilities, both groups showed similar network-related issues.\nWe advocate for the adoption of stronger, standardized, and user-focused\nsecurity practices across regions.",
    "updated" : "2025-10-14T00:30:57Z",
    "published" : "2025-10-14T00:30:57Z",
    "authors" : [
      {
        "name" : "Urvashi Kishnani"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11895v1",
    "title" : "High-Probability Bounds For Heterogeneous Local Differential Privacy",
    "summary" : "We study statistical estimation under local differential privacy (LDP) when\nusers may hold heterogeneous privacy levels and accuracy must be guaranteed\nwith high probability. Departing from the common in-expectation analyses, and\nfor one-dimensional and multi-dimensional mean estimation problems, we develop\nfinite sample upper bounds in $\\ell_2$-norm that hold with probability at least\n$1-\\beta$. We complement these results with matching minimax lower bounds,\nestablishing the optimality (up to constants) of our guarantees in the\nheterogeneous LDP regime. We further study distribution learning in\n$\\ell_\\infty$-distance, designing an algorithm with high-probability guarantees\nunder heterogeneous privacy demands. Our techniques offer principled guidance\nfor designing mechanisms in settings with user-specific privacy levels.",
    "updated" : "2025-10-13T19:54:44Z",
    "published" : "2025-10-13T19:54:44Z",
    "authors" : [
      {
        "name" : "Maryam Aliakbarpour"
      },
      {
        "name" : "Alireza Fallah"
      },
      {
        "name" : "Swaha Roy"
      },
      {
        "name" : "Ria Stevens"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.DS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11514v2",
    "title" : "Toward Efficient and Privacy-Aware eHealth Systems: An Integrated\n  Sensing, Computing, and Semantic Communication Approach",
    "summary" : "Real-time and contactless monitoring of vital signs, such as respiration and\nheartbeat, alongside reliable communication, is essential for modern healthcare\nsystems, especially in remote and privacy-sensitive environments. Traditional\nwireless communication and sensing networks fall short in meeting all the\nstringent demands of eHealth, including accurate sensing, high data efficiency,\nand privacy preservation. To overcome the challenges, we propose a novel\nintegrated sensing, computing, and semantic communication (ISCSC) framework. In\nthe proposed system, a service robot utilises radar to detect patient positions\nand monitor their vital signs, while sending updates to the medical devices.\nInstead of transmitting raw physiological information, the robot computes and\ncommunicates semantically extracted health features to medical devices. This\nsemantic processing improves data throughput and preserves the clinical\nrelevance of the messages, while enhancing data privacy by avoiding the\ntransmission of sensitive data. Leveraging the estimated patient locations, the\nrobot employs an interacting multiple model (IMM) filter to actively track\npatient motion, thereby enabling robust beam steering for continuous and\nreliable monitoring. We then propose a joint optimisation of the beamforming\nmatrices and the semantic extraction ratio, subject to computing capability and\npower budget constraints, with the objective of maximising both the semantic\nsecrecy rate and sensing accuracy. Simulation results validate that the ISCSC\nframework achieves superior sensing accuracy, improved semantic transmission\nefficiency, and enhanced privacy preservation compared to conventional joint\nsensing and communication methods.",
    "updated" : "2025-10-14T13:55:01Z",
    "published" : "2025-10-13T15:21:32Z",
    "authors" : [
      {
        "name" : "Yinchao Yang"
      },
      {
        "name" : "Yahao Ding"
      },
      {
        "name" : "Zhaohui Yang"
      },
      {
        "name" : "Chongwen Huang"
      },
      {
        "name" : "Zhaoyang Zhang"
      },
      {
        "name" : "Dusit Niyato"
      },
      {
        "name" : "Mohammad Shikh-Bahaei"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.13528v1",
    "title" : "Experiments \\& Analysis of Privacy-Preserving SQL Query Sanitization\n  Systems",
    "summary" : "Analytical SQL queries are essential for extracting insights from relational\ndatabases but concurrently introduce significant privacy risks by potentially\nexposing sensitive information. To mitigate these risks, numerous query\nsanitization systems have been developed, employing diverse approaches that\ncreate a complex landscape for both researchers and practitioners. These\nsystems vary fundamentally in their design, including the underlying privacy\nmodel, such as k-anonymity or Differential Privacy; the protected privacy unit,\nwhether at the tuple- or user-level; and the software architecture, which can\nbe proxy-based or integrated. This paper provides a systematic classification\nof state-of-the-art SQL sanitization systems based on these qualitative\ncriteria and the scope of queries they support. Furthermore, we present a\nquantitative analysis of leading systems, empirically measuring the trade-offs\nbetween data utility, query execution overhead, and privacy guarantees across a\nrange of analytical queries. This work offers a structured overview and\nperformance assessment intended to clarify the capabilities and limitations of\ncurrent privacy-preserving database technologies.",
    "updated" : "2025-10-15T13:21:59Z",
    "published" : "2025-10-15T13:21:59Z",
    "authors" : [
      {
        "name" : "Los Ecoffet"
      },
      {
        "name" : "Veronika Rehn-Sonigo"
      },
      {
        "name" : "Jean-Franois Couchot"
      },
      {
        "name" : "Catuscia Palamidessi"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.13512v1",
    "title" : "Offline and Online KL-Regularized RLHF under Differential Privacy",
    "summary" : "In this paper, we study the offline and online settings of reinforcement\nlearning from human feedback (RLHF) with KL-regularization -- a widely used\nobjective function in large language model alignment -- under the $\\epsilon$\nlocal differential privacy ($\\epsilon$-LDP) model on the label of the human\npreference. In the offline setting, we design an algorithm based on the\nprinciple of pessimism and derive a new suboptimality gap of\n$\\tilde{O}(1/[(e^\\epsilon-1)^2 n])$ on the KL-regularized objective under\nsingle-policy concentrability. We also prove its optimality by providing a\nmatching lower bound where $n$ is the sample size.\n  In the online setting, we are the first one to theoretically investigate the\nproblem of KL-regularized RLHF with LDP. We design an optimism-based algorithm\nand derive a logarithmic regret bound of $O(d_{\\mathcal{F}}\\log\n(N_{\\mathcal{F}}\\cdot T) /(e^\\epsilon-1)^2 )$, where $T$ is the total time\nstep, $N_{\\mathcal{F}}$ is cardinality of the reward function space\n$\\mathcal{F}$ and $d_{\\mathcal{F}}$ is a variant of eluder dimension for RLHF.\nAs a by-product of our analysis, our results also imply the first analysis for\nonline KL-regularized RLHF without privacy. We implement our algorithm in the\noffline setting to verify our theoretical results and release our open source\ncode at: https://github.com/rushil-thareja/PPKL-RLHF-Official.",
    "updated" : "2025-10-15T13:04:19Z",
    "published" : "2025-10-15T13:04:19Z",
    "authors" : [
      {
        "name" : "Yulian Wu"
      },
      {
        "name" : "Rushil Thareja"
      },
      {
        "name" : "Praneeth Vepakomma"
      },
      {
        "name" : "Francesco Orabona"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.13468v1",
    "title" : "Privacy, freedom of expression, and the right to be forgotten in Europe",
    "summary" : "In this chapter we discuss the relation between privacy and freedom of\nexpression in Europe. In principle, the two rights have equal weight in Europe\n- which right prevails depends on the circumstances of a case. We use the\nGoogle Spain judgment of the Court of Justice of the European Union, sometimes\ncalled the 'right to be forgotten' judgment, to illustrate the difficulties\nwhen balancing the two rights. The court decided in Google Spain that people\nhave, under certain conditions, the right to have search results for their name\ndelisted. We discuss how Google and Data Protection Authorities deal with such\ndelisting requests in practice. Delisting requests illustrate that balancing\nprivacy and freedom of expression interests will always remain difficult.",
    "updated" : "2025-10-15T12:13:52Z",
    "published" : "2025-10-15T12:13:52Z",
    "authors" : [
      {
        "name" : "Stefan Kulk"
      },
      {
        "name" : "Frederik Zuiderveen Borgesius"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.13136v1",
    "title" : "Privacy-Aware Framework of Robust Malware Detection in Indoor Robots:\n  Hybrid Quantum Computing and Deep Neural Networks",
    "summary" : "Indoor robotic systems within Cyber-Physical Systems (CPS) are increasingly\nexposed to Denial of Service (DoS) attacks that compromise localization,\ncontrol and telemetry integrity. We propose a privacy-aware malware detection\nframework for indoor robotic systems, which leverages hybrid quantum computing\nand deep neural networks to counter DoS threats in CPS, while preserving\nprivacy information. By integrating quantum-enhanced feature encoding with\ndropout-optimized deep learning, our architecture achieves up to 95.2%\ndetection accuracy under privacy-constrained conditions. The system operates\nwithout handcrafted thresholds or persistent beacon data, enabling scalable\ndeployment in adversarial environments. Benchmarking reveals robust\ngeneralization, interpretability and resilience against training instability\nthrough modular circuit design. This work advances trustworthy AI for secure,\nautonomous CPS operations.",
    "updated" : "2025-10-15T04:25:33Z",
    "published" : "2025-10-15T04:25:33Z",
    "authors" : [
      {
        "name" : "Tan Le"
      },
      {
        "name" : "Van Le"
      },
      {
        "name" : "Sachin Shetty"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12908v1",
    "title" : "Local Differential Privacy for Federated Learning with Fixed Memory\n  Usage and Per-Client Privacy",
    "summary" : "Federated learning (FL) enables organizations to collaboratively train models\nwithout sharing their datasets. Despite this advantage, recent studies show\nthat both client updates and the global model can leak private information,\nlimiting adoption in sensitive domains such as healthcare. Local differential\nprivacy (LDP) offers strong protection by letting each participant privatize\nupdates before transmission. However, existing LDP methods were designed for\ncentralized training and introduce challenges in FL, including high resource\ndemands that can cause client dropouts and the lack of reliable privacy\nguarantees under asynchronous participation. These issues undermine model\ngeneralizability, fairness, and compliance with regulations such as HIPAA and\nGDPR. To address them, we propose L-RDP, a DP method designed for LDP that\nensures constant, lower memory usage to reduce dropouts and provides rigorous\nper-client privacy guarantees by accounting for intermittent participation.",
    "updated" : "2025-10-14T18:32:08Z",
    "published" : "2025-10-14T18:32:08Z",
    "authors" : [
      {
        "name" : "Rouzbeh Behnia"
      },
      {
        "name" : "Jeremiah Birrell"
      },
      {
        "name" : "Arman Riasi"
      },
      {
        "name" : "Reza Ebrahimi"
      },
      {
        "name" : "Kaushik Dutta"
      },
      {
        "name" : "Thang Hoang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.14894v1",
    "title" : "Secure Sparse Matrix Multiplications and their Applications to\n  Privacy-Preserving Machine Learning",
    "summary" : "To preserve privacy, multi-party computation (MPC) enables executing Machine\nLearning (ML) algorithms on secret-shared or encrypted data. However, existing\nMPC frameworks are not optimized for sparse data. This makes them unsuitable\nfor ML applications involving sparse data, e.g., recommender systems or\ngenomics. Even in plaintext, such applications involve high-dimensional sparse\ndata, that cannot be processed without sparsity-related optimizations due to\nprohibitively large memory requirements.\n  Since matrix multiplication is central in ML algorithms, we propose MPC\nalgorithms to multiply secret sparse matrices. On the one hand, our algorithms\navoid the memory issues of the \"dense\" data representation of classic secure\nmatrix multiplication algorithms. On the other hand, our algorithms can\nsignificantly reduce communication costs (some experiments show a factor 1000)\nfor realistic problem sizes. We validate our algorithms in two ML applications\nin which existing protocols are impractical.\n  An important question when developing MPC algorithms is what assumptions can\nbe made. In our case, if the number of non-zeros in a row is a sensitive piece\nof information then a short runtime may reveal that the number of non-zeros is\nsmall. Existing approaches make relatively simple assumptions, e.g., that there\nis a universal upper bound to the number of non-zeros in a row. This often\ndoesn't align with statistical reality, in a lot of sparse datasets the amount\nof data per instance satisfies a power law. We propose an approach which allows\nadopting a safe upper bound on the distribution of non-zeros in rows/columns of\nsparse matrices.",
    "updated" : "2025-10-16T17:12:18Z",
    "published" : "2025-10-16T17:12:18Z",
    "authors" : [
      {
        "name" : "Marc Damie"
      },
      {
        "name" : "Florian Hahn"
      },
      {
        "name" : "Andreas Peter"
      },
      {
        "name" : "Jan Ramon"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.14312v1",
    "title" : "Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy,\n  and Security Studies",
    "summary" : "A multi-agent system (MAS) powered by large language models (LLMs) can\nautomate tedious user tasks such as meeting scheduling that requires\ninter-agent collaboration. LLMs enable nuanced protocols that account for\nunstructured private data, user constraints, and preferences. However, this\ndesign introduces new risks, including misalignment and attacks by malicious\nparties that compromise agents or steal user data. In this paper, we propose\nthe Terrarium framework for fine-grained study on safety, privacy, and security\nin LLM-based MAS. We repurpose the blackboard design, an early approach in\nmulti-agent systems, to create a modular, configurable testbed for multi-agent\ncollaboration. We identify key attack vectors such as misalignment, malicious\nagents, compromised communication, and data poisoning. We implement three\ncollaborative MAS scenarios with four representative attacks to demonstrate the\nframework's flexibility. By providing tools to rapidly prototype, evaluate, and\niterate on defenses and designs, Terrarium aims to accelerate progress toward\ntrustworthy multi-agent systems.",
    "updated" : "2025-10-16T05:19:13Z",
    "published" : "2025-10-16T05:19:13Z",
    "authors" : [
      {
        "name" : "Mason Nakamura"
      },
      {
        "name" : "Abhinav Kumar"
      },
      {
        "name" : "Saaduddin Mahmud"
      },
      {
        "name" : "Sahar Abdelnabi"
      },
      {
        "name" : "Shlomo Zilberstein"
      },
      {
        "name" : "Eugene Bagdasarian"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "I.2.7; I.2.11"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.14151v1",
    "title" : "Privacy-Preserving and Incentive-Driven Relay-Based Framework for\n  Cross-Domain Blockchain Interoperability",
    "summary" : "Interoperability is essential for transforming blockchains from isolated\nnetworks into collaborative ecosystems, unlocking their full potential. While\nsignificant progress has been made in public blockchain interoperability,\nbridging permissioned and permissionless blockchains poses unique challenges\ndue to differences in access control, architectures, and security requirements.\nThis paper introduces a blockchain-agnostic framework to enable\ninteroperability between permissioned and permissionless networks. Leveraging\ncryptographic techniques, the framework ensures secure data exchanges. Its\nlightweight architectural design simplifies implementation and maintenance,\nwhile the integration of Clover and Dandelion++ protocols enhances transaction\nanonymity. Performance evaluations demonstrate the framework's effectiveness in\nachieving secure and efficient interoperability by measuring the forwarding\ntime, the throughput, the availability, and their collusion impact of the\nsystem across heterogeneous blockchain ecosystems.",
    "updated" : "2025-10-15T22:59:02Z",
    "published" : "2025-10-15T22:59:02Z",
    "authors" : [
      {
        "name" : "Saeed Moradi"
      },
      {
        "name" : "Koosha Esmaeilzadeh Khorasani"
      },
      {
        "name" : "Sara Rouhani"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12153v2",
    "title" : "VeilAudit: Breaking the Deadlock Between Privacy and Accountability\n  Across Blockchains",
    "summary" : "Cross chain interoperability in blockchain systems exposes a fundamental\ntension between user privacy and regulatory accountability. Existing solutions\nenforce an all or nothing choice between full anonymity and mandatory identity\ndisclosure, which limits adoption in regulated financial settings. We present\nVeilAudit, a cross chain auditing framework that introduces Auditor Only\nLinkability, which allows auditors to link transaction behaviors that originate\nfrom the same anonymous entity without learning its identity. VeilAudit\nachieves this with a user generated Linkable Audit Tag that embeds a zero\nknowledge proof to attest to its validity without exposing the user master\nwallet address, and with a special ciphertext that only designated auditors can\ntest for linkage. To balance privacy and compliance, VeilAudit also supports\nthreshold gated identity revelation under due process. VeilAudit further\nprovides a mechanism for building reputation in pseudonymous environments,\nwhich enables applications such as cross chain credit scoring based on\nverifiable behavioral history. We formalize the security guarantees and develop\na prototype that spans multiple EVM chains. Our evaluation shows that the\nframework is practical for today multichain environments.",
    "updated" : "2025-10-16T04:48:58Z",
    "published" : "2025-10-14T05:16:23Z",
    "authors" : [
      {
        "name" : "Minhao Qiao"
      },
      {
        "name" : "Hai Dong"
      },
      {
        "name" : "Iqbal Gondal"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.13890v1",
    "title" : "A Survey on Collaborating Small and Large Language Models for\n  Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness",
    "summary" : "Large language models (LLMs) have advanced many domains and applications but\nface high fine-tuning costs, inference latency, limited edge deployability, and\nreliability concerns. Small language models (SLMs), compact, efficient, and\nadaptable, offer complementary remedies. Recent work explores collaborative\nframeworks that fuse SLMs' specialization and efficiency with LLMs'\ngeneralization and reasoning to meet diverse objectives across tasks and\ndeployment scenarios. Motivated by these developments, this paper presents a\nsystematic survey of SLM-LLM collaboration organized by collaboration\nobjectives. We propose a taxonomy with four goals: performance enhancement,\ncost-effectiveness, cloud-edge privacy, and trustworthiness. Within this\nframework, we review representative methods, summarize design paradigms, and\noutline open challenges and future directions toward efficient, secure, and\nscalable SLM-LLM collaboration.",
    "updated" : "2025-10-14T04:16:47Z",
    "published" : "2025-10-14T04:16:47Z",
    "authors" : [
      {
        "name" : "Fali Wang"
      },
      {
        "name" : "Jihai Chen"
      },
      {
        "name" : "Shuhua Yang"
      },
      {
        "name" : "Ali Al-Lawati"
      },
      {
        "name" : "Linli Tang"
      },
      {
        "name" : "Hui Liu"
      },
      {
        "name" : "Suhang Wang"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "68T50 (Primary) 68T07 (Secondary)",
      "I.2.7"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.15186v1",
    "title" : "MAGPIE: A benchmark for Multi-AGent contextual PrIvacy Evaluation",
    "summary" : "A core challenge for autonomous LLM agents in collaborative settings is\nbalancing robust privacy understanding and preservation alongside task\nefficacy. Existing privacy benchmarks only focus on simplistic, single-turn\ninteractions where private information can be trivially omitted without\naffecting task outcomes. In this paper, we introduce MAGPIE (Multi-AGent\ncontextual PrIvacy Evaluation), a novel benchmark of 200 high-stakes tasks\ndesigned to evaluate privacy understanding and preservation in multi-agent\ncollaborative, non-adversarial scenarios. MAGPIE integrates private information\nas essential for task resolution, forcing agents to balance effective\ncollaboration with strategic information control. Our evaluation reveals that\nstate-of-the-art agents, including GPT-5 and Gemini 2.5-Pro, exhibit\nsignificant privacy leakage, with Gemini 2.5-Pro leaking up to 50.7% and GPT-5\nup to 35.1% of the sensitive information even when explicitly instructed not\nto. Moreover, these agents struggle to achieve consensus or task completion and\noften resort to undesirable behaviors such as manipulation and power-seeking\n(e.g., Gemini 2.5-Pro demonstrating manipulation in 38.2% of the cases). These\nfindings underscore that current LLM agents lack robust privacy understanding\nand are not yet adequately aligned to simultaneously preserve privacy and\nmaintain effective collaboration in complex environments.",
    "updated" : "2025-10-16T23:12:12Z",
    "published" : "2025-10-16T23:12:12Z",
    "authors" : [
      {
        "name" : "Gurusha Juneja"
      },
      {
        "name" : "Jayanth Naga Sai Pasupulati"
      },
      {
        "name" : "Alon Albalak"
      },
      {
        "name" : "Wenyue Hua"
      },
      {
        "name" : "William Yang Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.15112v1",
    "title" : "AndroByte: LLM-Driven Privacy Analysis through Bytecode Summarization\n  and Dynamic Dataflow Call Graph Generation",
    "summary" : "With the exponential growth in mobile applications, protecting user privacy\nhas become even more crucial. Android applications are often known for\ncollecting, storing, and sharing sensitive user information such as contacts,\nlocation, camera, and microphone data often without the user's clear consent or\nawareness raising significant privacy risks and exposure. In the context of\nprivacy assessment, dataflow analysis is particularly valuable for identifying\ndata usage and potential leaks. Traditionally, this type of analysis has relied\non formal methods, heuristics, and rule-based matching. However, these\ntechniques are often complex to implement and prone to errors, such as taint\nexplosion for large programs. Moreover, most existing Android dataflow analysis\nmethods depend heavily on predefined list of sinks, limiting their flexibility\nand scalability. To address the limitations of these existing techniques, we\npropose AndroByte, an AI-driven privacy analysis tool that leverages LLM\nreasoning on bytecode summarization to dynamically generate accurate and\nexplainable dataflow call graphs from static code analysis. AndroByte achieves\na significant F\\b{eta}-Score of 89% in generating dynamic dataflow call graphs\non the fly, outperforming the effectiveness of traditional tools like FlowDroid\nand Amandroid in leak detection without relying on predefined propagation rules\nor sink lists. Moreover, AndroByte's iterative bytecode summarization provides\ncomprehensive and explainable insights into dataflow and leak detection,\nachieving high, quantifiable scores based on the G-Eval metric.",
    "updated" : "2025-10-16T20:10:20Z",
    "published" : "2025-10-16T20:10:20Z",
    "authors" : [
      {
        "name" : "Mst Eshita Khatun"
      },
      {
        "name" : "Lamine Noureddine"
      },
      {
        "name" : "Zhiyong Sui"
      },
      {
        "name" : "Aisha Ali-Gombe"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.15083v1",
    "title" : "SMOTE and Mirrors: Exposing Privacy Leakage from Synthetic Minority\n  Oversampling",
    "summary" : "The Synthetic Minority Over-sampling Technique (SMOTE) is one of the most\nwidely used methods for addressing class imbalance and generating synthetic\ndata. Despite its popularity, little attention has been paid to its privacy\nimplications; yet, it is used in the wild in many privacy-sensitive\napplications. In this work, we conduct the first systematic study of privacy\nleakage in SMOTE: We begin by showing that prevailing evaluation practices,\ni.e., naive distinguishing and distance-to-closest-record metrics, completely\nfail to detect any leakage and that membership inference attacks (MIAs) can be\ninstantiated with high accuracy. Then, by exploiting SMOTE's geometric\nproperties, we build two novel attacks with very limited assumptions:\nDistinSMOTE, which perfectly distinguishes real from synthetic records in\naugmented datasets, and ReconSMOTE, which reconstructs real minority records\nfrom synthetic datasets with perfect precision and recall approaching one under\nrealistic imbalance ratios. We also provide theoretical guarantees for both\nattacks. Experiments on eight standard imbalanced datasets confirm the\npracticality and effectiveness of these attacks. Overall, our work reveals that\nSMOTE is inherently non-private and disproportionately exposes minority\nrecords, highlighting the need to reconsider its use in privacy-sensitive\napplications.",
    "updated" : "2025-10-16T18:55:46Z",
    "published" : "2025-10-16T18:55:46Z",
    "authors" : [
      {
        "name" : "Georgi Ganev"
      },
      {
        "name" : "Reza Nazari"
      },
      {
        "name" : "Rees Davison"
      },
      {
        "name" : "Amir Dizche"
      },
      {
        "name" : "Xinmin Wu"
      },
      {
        "name" : "Ralph Abbey"
      },
      {
        "name" : "Jorge Silva"
      },
      {
        "name" : "Emiliano De Cristofaro"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11299v2",
    "title" : "How to Get Actual Privacy and Utility from Privacy Models: the\n  k-Anonymity and Differential Privacy Families",
    "summary" : "Privacy models were introduced in privacy-preserving data publishing and\nstatistical disclosure control with the promise to end the need for costly\nempirical assessment of disclosure risk. We examine how well this promise is\nkept by the main privacy models. We find they may fail to provide adequate\nprotection guarantees because of problems in their definition or incur\nunacceptable trade-offs between privacy protection and utility preservation.\nSpecifically, k-anonymity may not entirely exclude disclosure if enforced with\ndeterministic mechanisms or without constraints on the confidential values. On\nthe other hand, differential privacy (DP) incurs unacceptable utility loss for\nsmall budgets and its privacy guarantee becomes meaningless for large budgets.\nIn the latter case, an ex post empirical assessment of disclosure risk becomes\nnecessary, undermining the main appeal of privacy models. Whereas the utility\npreservation of DP can only be improved by relaxing its privacy guarantees, we\nargue that a semantic reformulation of k-anonymity can offer more robust\nprivacy without losing utility with respect to traditional syntactic\nk-anonymity.",
    "updated" : "2025-10-17T09:07:58Z",
    "published" : "2025-10-13T11:41:12Z",
    "authors" : [
      {
        "name" : "Josep Domingo-Ferrer"
      },
      {
        "name" : "David Snchez"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "68",
      "K.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05172v2",
    "title" : "Learning More with Less: A Generalizable, Self-Supervised Framework for\n  Privacy-Preserving Capacity Estimation with EV Charging Data",
    "summary" : "Accurate battery capacity estimation is key to alleviating consumer concerns\nabout battery performance and reliability of electric vehicles (EVs). However,\npractical data limitations imposed by stringent privacy regulations and labeled\ndata shortages hamper the development of generalizable capacity estimation\nmodels that remain robust to real-world data distribution shifts. While\nself-supervised learning can leverage unlabeled data, existing techniques are\nnot particularly designed to learn effectively from challenging field data --\nlet alone from privacy-friendly data, which are often less feature-rich and\nnoisier. In this work, we propose a first-of-its-kind capacity estimation model\nbased on self-supervised pre-training, developed on a large-scale dataset of\nprivacy-friendly charging data snippets from real-world EV operations. Our\npre-training framework, snippet similarity-weighted masked input\nreconstruction, is designed to learn rich, generalizable representations even\nfrom less feature-rich and fragmented privacy-friendly data. Our key innovation\nlies in harnessing contrastive learning to first capture high-level\nsimilarities among fragmented snippets that otherwise lack meaningful context.\nWith our snippet-wise contrastive learning and subsequent similarity-weighted\nmasked reconstruction, we are able to learn rich representations of both\ngranular charging patterns within individual snippets and high-level\nassociative relationships across different snippets. Bolstered by this rich\nrepresentation learning, our model consistently outperforms state-of-the-art\nbaselines, achieving 31.9% lower test error than the best-performing benchmark,\neven under challenging domain-shifted settings affected by both manufacturer\nand age-induced distribution shifts. Source code is available at\nhttps://github.com/en-research/GenEVBattery.",
    "updated" : "2025-10-17T03:22:40Z",
    "published" : "2025-10-05T08:58:35Z",
    "authors" : [
      {
        "name" : "Anushiya Arunan"
      },
      {
        "name" : "Yan Qin"
      },
      {
        "name" : "Xiaoli Li"
      },
      {
        "name" : "U-Xuan Tan"
      },
      {
        "name" : "H. Vincent Poor"
      },
      {
        "name" : "Chau Yuen"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.17480v1",
    "title" : "Unified Privacy Guarantees for Decentralized Learning via Matrix\n  Factorization",
    "summary" : "Decentralized Learning (DL) enables users to collaboratively train models\nwithout sharing raw data by iteratively averaging local updates with neighbors\nin a network graph. This setting is increasingly popular for its scalability\nand its ability to keep data local under user control. Strong privacy\nguarantees in DL are typically achieved through Differential Privacy (DP), with\nresults showing that DL can even amplify privacy by disseminating noise across\npeer-to-peer communications. Yet in practice, the observed privacy-utility\ntrade-off often appears worse than in centralized training, which may be due to\nlimitations in current DP accounting methods for DL. In this paper, we show\nthat recent advances in centralized DP accounting based on Matrix Factorization\n(MF) for analyzing temporal noise correlations can also be leveraged in DL. By\ngeneralizing existing MF results, we show how to cast both standard DL\nalgorithms and common trust models into a unified formulation. This yields\ntighter privacy accounting for existing DP-DL algorithms and provides a\nprincipled way to develop new ones. To demonstrate the approach, we introduce\nMAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that\noutperforms existing methods on synthetic and real-world graphs.",
    "updated" : "2025-10-20T12:24:27Z",
    "published" : "2025-10-20T12:24:27Z",
    "authors" : [
      {
        "name" : "Aurlien Bellet"
      },
      {
        "name" : "Edwige Cyffers"
      },
      {
        "name" : "Davide Frey"
      },
      {
        "name" : "Romaric Gaudel"
      },
      {
        "name" : "Dimitri Lervrend"
      },
      {
        "name" : "Franois Taani"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.17372v1",
    "title" : "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition\n  Performance without Privacy Compromise",
    "summary" : "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research.",
    "updated" : "2025-10-20T10:08:53Z",
    "published" : "2025-10-20T10:08:53Z",
    "authors" : [
      {
        "name" : "Pawe Borsukiewicz"
      },
      {
        "name" : "Fadi Boutros"
      },
      {
        "name" : "Iyiola E. Olatunji"
      },
      {
        "name" : "Charles Beumier"
      },
      {
        "name" : "Wendkuni C. Ouedraogo"
      },
      {
        "name" : "Jacques Klein"
      },
      {
        "name" : "Tegawend F. Bissyand"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.17348v1",
    "title" : "Optimal Best Arm Identification under Differential Privacy",
    "summary" : "Best Arm Identification (BAI) algorithms are deployed in data-sensitive\napplications, such as adaptive clinical trials or user studies. Driven by the\nprivacy concerns of these applications, we study the problem of\nfixed-confidence BAI under global Differential Privacy (DP) for Bernoulli\ndistributions. While numerous asymptotically optimal BAI algorithms exist in\nthe non-private setting, a significant gap remains between the best lower and\nupper bounds in the global DP setting. This work reduces this gap to a small\nmultiplicative constant, for any privacy budget $\\epsilon$. First, we provide a\ntighter lower bound on the expected sample complexity of any $\\delta$-correct\nand $\\epsilon$-global DP strategy. Our lower bound replaces the\nKullback-Leibler (KL) divergence in the transportation cost used by the\nnon-private characteristic time with a new information-theoretic quantity that\noptimally trades off between the KL divergence and the Total Variation distance\nscaled by $\\epsilon$. Second, we introduce a stopping rule based on these\ntransportation costs and a private estimator of the means computed using an\narm-dependent geometric batching. En route to proving the correctness of our\nstopping rule, we derive concentration results of independent interest for the\nLaplace distribution and for the sum of Bernoulli and Laplace distributions.\nThird, we propose a Top Two sampling rule based on these transportation costs.\nFor any budget $\\epsilon$, we show an asymptotic upper bound on its expected\nsample complexity that matches our lower bound to a multiplicative constant\nsmaller than $8$. Our algorithm outperforms existing $\\delta$-correct and\n$\\epsilon$-global DP BAI algorithms for different values of $\\epsilon$.",
    "updated" : "2025-10-20T09:46:09Z",
    "published" : "2025-10-20T09:46:09Z",
    "authors" : [
      {
        "name" : "Marc Jourdan"
      },
      {
        "name" : "Achraf Azize"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.17162v1",
    "title" : "ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for\n  Dynamic Edge Crowdsensing",
    "summary" : "Mobile edge crowdsensing (MECS) systems continuously generate and transmit\nuser data in dynamic, resource-constrained environments, exposing users to\nsignificant privacy threats. In practice, many privacy-preserving mechanisms\nbuild on differential privacy (DP). However, static DP mechanisms often fail to\nadapt to evolving risks, for example, shifts in adversarial capabilities,\nresource constraints and task requirements, resulting in either excessive noise\nor inadequate protection. To address this challenge, we propose ALPINE, a\nlightweight, adaptive framework that empowers terminal devices to autonomously\nadjust differential privacy levels in real time. ALPINE operates as a\nclosed-loop control system consisting of four modules: dynamic risk perception,\nprivacy decision via twin delayed deep deterministic policy gradient (TD3),\nlocal privacy execution and performance verification from edge nodes. Based on\nenvironmental risk assessments, we design a reward function that balances\nprivacy gains, data utility and energy cost, guiding the TD3 agent to\nadaptively tune noise magnitude across diverse risk scenarios and achieve a\ndynamic equilibrium among privacy, utility and cost. Both the collaborative\nrisk model and pretrained TD3-based agent are designed for low-overhead\ndeployment. Extensive theoretical analysis and real-world simulations\ndemonstrate that ALPINE effectively mitigates inference attacks while\npreserving utility and cost, making it practical for large-scale edge\napplications.",
    "updated" : "2025-10-20T05:03:25Z",
    "published" : "2025-10-20T05:03:25Z",
    "authors" : [
      {
        "name" : "Guanjie Cheng"
      },
      {
        "name" : "Siyang Liu"
      },
      {
        "name" : "Junqin Huang"
      },
      {
        "name" : "Xinkui Zhao"
      },
      {
        "name" : "Yin Wang"
      },
      {
        "name" : "Mengying Zhu"
      },
      {
        "name" : "Linghe Kong"
      },
      {
        "name" : "Shuiguang Deng"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.16744v1",
    "title" : "Cryptanalysis of a Privacy-Preserving Ride-Hailing Service from NSS 2022",
    "summary" : "Ride-Hailing Services (RHS) match a ride request initiated by a rider with a\nsuitable driver responding to the ride request. A Privacy-Preserving RHS\n(PP-RHS) aims to facilitate ride matching while ensuring the privacy of riders'\nand drivers' location data w.r.t. the Service Provider (SP). At NSS 2022, Xie\net al. proposed a PP-RHS. In this work, we demonstrate a passive attack on\ntheir PP-RHS protocol. Our attack allows the SP to completely recover the\nlocations of the rider as well as that of the responding drivers in every ride\nrequest. Further, our attack is very efficient as it is independent of the\nsecurity parameter.",
    "updated" : "2025-10-19T08:05:25Z",
    "published" : "2025-10-19T08:05:25Z",
    "authors" : [
      {
        "name" : "Srinivas Vivek"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.16687v1",
    "title" : "High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient\n  Descent on Least Squares",
    "summary" : "The interplay between optimization and privacy has become a central theme in\nprivacy-preserving machine learning. Noisy stochastic gradient descent (SGD)\nhas emerged as a cornerstone algorithm, particularly in large-scale settings.\nThese variants of gradient methods inject carefully calibrated noise into each\nupdate to achieve differential privacy, the gold standard notion of rigorous\nprivacy guarantees. Prior work primarily provides various bounds on statistical\nrisk and privacy loss for noisy SGD, yet the \\textit{exact} behavior of the\nprocess remains unclear, particularly in high-dimensional settings. This work\nleverages a diffusion approach to analyze noisy SGD precisely, providing a\ncontinuous-time perspective that captures both statistical risk evolution and\nprivacy loss dynamics in high dimensions. Moreover, we study a variant of noisy\nSGD that does not require explicit knowledge of gradient sensitivity, unlike\nexisting work that assumes or enforces sensitivity through gradient clipping.\nSpecifically, we focus on the least squares problem with $\\ell_2$\nregularization.",
    "updated" : "2025-10-19T02:28:27Z",
    "published" : "2025-10-19T02:28:27Z",
    "authors" : [
      {
        "name" : "Shurong Lin"
      },
      {
        "name" : "Eric D. Kolaczyk"
      },
      {
        "name" : "Adam Smith"
      },
      {
        "name" : "Elliot Paquette"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.16331v1",
    "title" : "Efficient and Privacy-Preserving Binary Dot Product via Multi-Party\n  Computation",
    "summary" : "Striking a balance between protecting data privacy and enabling collaborative\ncomputation is a critical challenge for distributed machine learning. While\nprivacy-preserving techniques for federated learning have been extensively\ndeveloped, methods for scenarios involving bitwise operations, such as\ntree-based vertical federated learning (VFL), are still underexplored.\nTraditional mechanisms, including Shamir's secret sharing and multi-party\ncomputation (MPC), are not optimized for bitwise operations over binary data,\nparticularly in settings where each participant holds a different part of the\nbinary vector. This paper addresses the limitations of existing methods by\nproposing a novel binary multi-party computation (BiMPC) framework. The BiMPC\nmechanism facilitates privacy-preserving bitwise operations, with a particular\nfocus on dot product computations of binary vectors, ensuring the privacy of\neach individual bit. The core of BiMPC is a novel approach called Dot Product\nvia Modular Addition (DoMA), which uses regular and modular additions for\nefficient binary dot product calculation. To ensure privacy, BiMPC uses random\nmasking in a higher field for linear computations and a three-party oblivious\ntransfer (triot) protocol for non-linear binary operations. The privacy\nguarantees of the BiMPC framework are rigorously analyzed, demonstrating its\nefficiency and scalability in distributed settings.",
    "updated" : "2025-10-18T03:35:42Z",
    "published" : "2025-10-18T03:35:42Z",
    "authors" : [
      {
        "name" : "Fatemeh Jafarian Dehkordi"
      },
      {
        "name" : "Elahe Vedadi"
      },
      {
        "name" : "Alireza Feizbakhsh"
      },
      {
        "name" : "Yasaman Keshtkarjahromi"
      },
      {
        "name" : "Hulya Seferoglu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.16083v1",
    "title" : "PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction\n  via Graph-Based Federated Learning for Representing Password Reuse between\n  Websites",
    "summary" : "Credential stuffing attacks have caused significant harm to online users who\nfrequently reuse passwords across multiple websites. While prior research has\nattempted to detect users with reused passwords or identify malicious login\nattempts, existing methods often compromise usability by restricting password\ncreation or website access, and their reliance on complex account-sharing\nmechanisms hinders real-world deployment. To address these limitations, we\npropose PassREfinder-FL, a novel framework that predicts credential stuffing\nrisks across websites. We introduce the concept of password reuse relations --\ndefined as the likelihood of users reusing passwords between websites -- and\nrepresent them as edges in a website graph. Using graph neural networks (GNNs),\nwe perform a link prediction task to assess credential reuse risk between\nsites. Our approach scales to a large number of arbitrary websites by\nincorporating public website information and linking newly observed websites as\nnodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a\nfederated learning (FL) approach that eliminates the need to share user\nsensitive information across administrators. Evaluation on a real-world dataset\nof 360 million breached accounts from 22,378 websites shows that\nPassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further\nvalidate that our FL-based GNN achieves a 4-11% performance improvement over\nother state-of-the-art GNN models through an ablation study. Finally, we\ndemonstrate that the predicted results can be used to quantify password reuse\nlikelihood as actionable risk scores.",
    "updated" : "2025-10-17T14:59:24Z",
    "published" : "2025-10-17T14:59:24Z",
    "authors" : [
      {
        "name" : "Jaehan Kim"
      },
      {
        "name" : "Minkyoo Song"
      },
      {
        "name" : "Minjae Seo"
      },
      {
        "name" : "Youngjin Jin"
      },
      {
        "name" : "Seungwon Shin"
      },
      {
        "name" : "Jinwoo Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.16054v1",
    "title" : "PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware\n  Delegation",
    "summary" : "When users submit queries to Large Language Models (LLMs), their prompts can\noften contain sensitive data, forcing a difficult choice: Send the query to a\npowerful proprietary LLM providers to achieving state-of-the-art performance\nand risk data exposure, or relying on smaller, local models guarantees data\nprivacy but often results in a degradation of task performance. Prior\napproaches have relied on static pipelines that use LLM rewriting, which\nshatters linguistic coherence and indiscriminately removes privacy-sensitive\ninformation, including task-critical content. We reformulate this challenge\n(Privacy-Conscious Delegation) as a sequential decision-making problem and\nintroduce a novel reinforcement learning (RL) framework called PrivacyPAD to\nsolve it. Our framework trains an agent to dynamically route text chunks,\nlearning a policy that optimally balances the trade-off between privacy leakage\nand task performance. It implicitly distinguishes between replaceable\nPersonally Identifiable Information (PII) (which it shields locally) and\ntask-critical PII (which it strategically sends to the remote model for maximal\nutility). To validate our approach in complex scenarios, we also introduce a\nnew medical dataset with high PII density. Our framework achieves a new\nstate-of-the-art on the privacy-utility frontier, demonstrating the necessity\nof learned, adaptive policies for deploying LLMs in sensitive environments.",
    "updated" : "2025-10-16T19:38:36Z",
    "published" : "2025-10-16T19:38:36Z",
    "authors" : [
      {
        "name" : "Zheng Hui"
      },
      {
        "name" : "Yijiang River Dong"
      },
      {
        "name" : "Sanhanat Sivapiromrat"
      },
      {
        "name" : "Ehsan Shareghi"
      },
      {
        "name" : "Nigel Collier"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.18568v1",
    "title" : "Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with\n  Deep Learning and Blockchain",
    "summary" : "The integration of Internet of Things (IoT) devices in healthcare has\nrevolutionized patient care by enabling real-time monitoring, personalized\ntreatments, and efficient data management. However, this technological\nadvancement introduces significant security risks, particularly concerning the\nconfidentiality, integrity, and availability of sensitive medical data.\nTraditional security measures are often insufficient to address the unique\nchallenges posed by IoT environments, such as heterogeneity, resource\nconstraints, and the need for real-time processing. To tackle these challenges,\nwe propose a comprehensive three-phase security framework designed to enhance\nthe security and reliability of IoT-enabled healthcare systems. In the first\nphase, the framework assesses the reliability of IoT devices using a\nreputation-based trust estimation mechanism, which combines device behavior\nanalytics with off-chain data storage to ensure scalability. The second phase\nintegrates blockchain technology with a lightweight proof-of-work mechanism,\nensuring data immutability, secure communication, and resistance to\nunauthorized access. The third phase employs a lightweight Long Short-Term\nMemory (LSTM) model for anomaly detection and classification, enabling\nreal-time identification of cyber threats. Simulation results demonstrate that\nthe proposed framework outperforms existing methods, achieving a 2% increase in\nprecision, accuracy, and recall, a 5% higher attack detection rate, and a 3%\nreduction in false alarm rate. These improvements highlight the framework's\nability to address critical security concerns while maintaining scalability and\nreal-time performance.",
    "updated" : "2025-10-21T12:21:49Z",
    "published" : "2025-10-21T12:21:49Z",
    "authors" : [
      {
        "name" : "Behnam Rezaei Bezanjani"
      },
      {
        "name" : "Seyyed Hamid Ghafouri"
      },
      {
        "name" : "Reza Gholamrezaei"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.18493v1",
    "title" : "One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for\n  Customizable Privacy-Preserving Phone Scam Detection",
    "summary" : "Phone scams remain a pervasive threat to both personal safety and financial\nsecurity worldwide. Recent advances in large language models (LLMs) have\ndemonstrated strong potential in detecting fraudulent behavior by analyzing\ntranscribed phone conversations. However, these capabilities introduce notable\nprivacy risks, as such conversations frequently contain sensitive personal\ninformation that may be exposed to third-party service providers during\nprocessing. In this work, we explore how to harness LLMs for phone scam\ndetection while preserving user privacy. We propose MASK (Modular Adaptive\nSanitization Kit), a trainable and extensible framework that enables dynamic\nprivacy adjustment based on individual preferences. MASK provides a pluggable\narchitecture that accommodates diverse sanitization methods - from traditional\nkeyword-based techniques for high-privacy users to sophisticated neural\napproaches for those prioritizing accuracy. We also discuss potential modeling\napproaches and loss function designs for future development, enabling the\ncreation of truly personalized, privacy-aware LLM-based detection systems that\nbalance user trust and detection effectiveness, even beyond phone scam context.",
    "updated" : "2025-10-21T10:30:36Z",
    "published" : "2025-10-21T10:30:36Z",
    "authors" : [
      {
        "name" : "Kangzhong Wang"
      },
      {
        "name" : "Zitong Shen"
      },
      {
        "name" : "Youqian Zhang"
      },
      {
        "name" : "Michael MK Cheung"
      },
      {
        "name" : "Xiapu Luo"
      },
      {
        "name" : "Grace Ngai"
      },
      {
        "name" : "Eugene Yujun Fu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.HC",
      "68M25",
      "I.2.7"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.18379v1",
    "title" : "Uniformity Testing under User-Level Local Privacy",
    "summary" : "We initiate the study of distribution testing under \\emph{user-level} local\ndifferential privacy, where each of $n$ users contributes $m$ samples from the\nunknown underlying distribution. This setting, albeit very natural, is\nsignificantly more challenging that the usual locally private setting, as for\nthe same parameter $\\varepsilon$ the privacy guarantee must now apply to a full\nbatch of $m$ data points. While some recent work consider distribution\n\\emph{learning} in this user-level setting, nothing was known for even the most\nfundamental testing task, uniformity testing (and its generalization, identity\ntesting).\n  We address this gap, by providing (nearly) sample-optimal user-level LDP\nalgorithms for uniformity and identity testing. Motivated by practical\nconsiderations, our main focus is on the private-coin, symmetric setting, which\ndoes not require users to share a common random seed nor to have been assigned\na globally unique identifier.",
    "updated" : "2025-10-21T07:52:41Z",
    "published" : "2025-10-21T07:52:41Z",
    "authors" : [
      {
        "name" : "Clment L. Canonne"
      },
      {
        "name" : "Abigail Gentle"
      },
      {
        "name" : "Vikrant Singhal"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.DM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.18109v1",
    "title" : "PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data\n  Marketplaces",
    "summary" : "Evaluating the relevance of data is a critical task for model builders\nseeking to acquire datasets that enhance model performance. Ideally, such\nevaluation should allow the model builder to assess the utility of candidate\ndata without exposing proprietary details of the model. At the same time, data\nproviders must be assured that no information about their data - beyond the\ncomputed utility score - is disclosed to the model builder.\n  In this paper, we present PrivaDE, a cryptographic protocol for\nprivacy-preserving utility scoring and selection of data for machine learning.\nWhile prior works have proposed data evaluation protocols, our approach\nadvances the state of the art through a practical, blockchain-centric design.\nLeveraging the trustless nature of blockchains, PrivaDE enforces\nmalicious-security guarantees and ensures strong privacy protection for both\nmodels and datasets. To achieve efficiency, we integrate several techniques -\nincluding model distillation, model splitting, and cut-and-choose\nzero-knowledge proofs - bringing the runtime to a practical level. Furthermore,\nwe propose a unified utility scoring function that combines empirical loss,\npredictive entropy, and feature-space diversity, and that can be seamlessly\nintegrated into active-learning workflows. Evaluation shows that PrivaDE\nperforms data evaluation effectively, achieving online runtimes within 15\nminutes even for models with millions of parameters.\n  Our work lays the foundation for fair and automated data marketplaces in\ndecentralized machine learning ecosystems.",
    "updated" : "2025-10-20T21:14:32Z",
    "published" : "2025-10-20T21:14:32Z",
    "authors" : [
      {
        "name" : "Wan Ki Wong"
      },
      {
        "name" : "Sahel Torkamani"
      },
      {
        "name" : "Michele Ciampi"
      },
      {
        "name" : "Rik Sarkar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.19537v1",
    "title" : "Privacy-Preserving Spiking Neural Networks: A Deep Dive into Encryption\n  Parameter Optimisation",
    "summary" : "Deep learning is widely applied to modern problems through neural networks,\nbut the growing computational and energy demands of these models have driven\ninterest in more efficient approaches. Spiking Neural Networks (SNNs), the\nthird generation of neural networks, mimic the brain's event-driven behaviour,\noffering improved performance and reduced power use. At the same time, concerns\nabout data privacy during cloud-based model execution have led to the adoption\nof cryptographic methods. This article introduces BioEncryptSNN, a spiking\nneural network based encryption-decryption framework for secure and\nnoise-resilient data protection. Unlike conventional algorithms, BioEncryptSNN\nconverts ciphertext into spike trains and exploits temporal neural dynamics to\nmodel encryption and decryption, optimising parameters such as key length,\nspike timing, and synaptic connectivity. Benchmarked against AES-128, RSA-2048,\nand DES, BioEncryptSNN preserved data integrity while achieving up to 4.1x\nfaster encryption and decryption than PyCryptodome's AES implementation. The\nframework demonstrates scalability and adaptability across symmetric and\nasymmetric ciphers, positioning SNNs as a promising direction for secure,\nenergy-efficient computing.",
    "updated" : "2025-10-22T12:43:46Z",
    "published" : "2025-10-22T12:43:46Z",
    "authors" : [
      {
        "name" : "Mahitha Pulivathi"
      },
      {
        "name" : "Ana Fontes Rodrigues"
      },
      {
        "name" : "Isibor Kennedy Ihianle"
      },
      {
        "name" : "Andreas Oikonomou"
      },
      {
        "name" : "Srinivas Boppu"
      },
      {
        "name" : "Pedro Machado"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.19026v1",
    "title" : "Fusion of Machine Learning and Blockchain-based Privacy-Preserving\n  Approach for Health Care Data in the Internet of Things",
    "summary" : "In recent years, the rapid integration of Internet of Things (IoT) devices\ninto the healthcare sector has brought about revolutionary advancements in\npatient care and data management. While these technological innovations hold\nimmense promise, they concurrently raise critical security concerns,\nparticularly in safeguarding medical data against potential cyber threats. The\nsensitive nature of health-related information requires robust measures to\nensure the confidentiality, integrity, and availability of patient data in\nIoT-enabled medical environments. Addressing the imperative need for enhanced\nsecurity in IoT-based healthcare systems, we propose a comprehensive method\nencompassing three distinct phases. In the first phase, we implement\nBlockchain-Enabled Request and Transaction Encryption to strengthen data\ntransaction security, providing an immutable and transparent framework. In the\nsecond phase, we introduce a Request Pattern Recognition Check that leverages\ndiverse data sources to identify and block potential unauthorized access\nattempts. Finally, the third phase incorporates Feature Selection and a BiLSTM\nnetwork to enhance the accuracy and efficiency of intrusion detection using\nadvanced machine learning techniques. We compared the simulation results of the\nproposed method with three recent related methods: AIBPSF-IoMT, OMLIDS-PBIoT,\nand AIMMFIDS. The evaluation criteria include detection rate, false alarm rate,\nprecision, recall, and accuracy - crucial benchmarks for assessing the overall\nperformance of intrusion detection systems. Our findings show that the proposed\nmethod outperforms existing approaches across all evaluated criteria,\ndemonstrating its effectiveness in improving the security of IoT-based\nhealthcare systems.",
    "updated" : "2025-10-21T19:09:46Z",
    "published" : "2025-10-21T19:09:46Z",
    "authors" : [
      {
        "name" : "Behnam Rezaei Bezanjani"
      },
      {
        "name" : "Seyyed Hamid Ghafouri"
      },
      {
        "name" : "Reza Gholamrezaei"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.20721v1",
    "title" : "User Perceptions of Privacy and Helpfulness in LLM Responses to\n  Privacy-Sensitive Scenarios",
    "summary" : "Large language models (LLMs) have seen rapid adoption for tasks such as\ndrafting emails, summarizing meetings, and answering health questions. In such\nuses, users may need to share private information (e.g., health records,\ncontact details). To evaluate LLMs' ability to identify and redact such private\ninformation, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) with\nreal-life scenarios. Using these benchmarks, researchers have found that LLMs\nsometimes fail to keep secrets private when responding to complex tasks (e.g.,\nleaking employee salaries in meeting summaries). However, these evaluations\nrely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlooking\nreal users' perceptions. Moreover, prior work primarily focused on the\nprivacy-preservation quality of responses, without investigating nuanced\ndifferences in helpfulness. To understand how users perceive the\nprivacy-preservation quality and helpfulness of LLM responses to\nprivacy-sensitive scenarios, we conducted a user study with 94 participants\nusing 90 scenarios from PrivacyLens. We found that, when evaluating identical\nresponses to the same scenario, users showed low agreement with each other on\nthe privacy-preservation quality and helpfulness of the LLM response. Further,\nwe found high agreement among five proxy LLMs, while each individual LLM had\nlow correlation with users' evaluations. These results indicate that the\nprivacy and helpfulness of LLM responses are often specific to individuals, and\nproxy LLMs are poor estimates of how real users would perceive these responses\nin privacy-sensitive scenarios. Our results suggest the need to conduct\nuser-centered studies on measuring LLMs' ability to help users while preserving\nprivacy. Additionally, future research could investigate ways to improve the\nalignment between proxy LLMs and users for better estimation of users'\nperceived privacy and utility.",
    "updated" : "2025-10-23T16:38:26Z",
    "published" : "2025-10-23T16:38:26Z",
    "authors" : [
      {
        "name" : "Xiaoyuan Wu"
      },
      {
        "name" : "Roshni Kaushik"
      },
      {
        "name" : "Wenkai Li"
      },
      {
        "name" : "Lujo Bauer"
      },
      {
        "name" : "Koichi Onoue"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.20300v1",
    "title" : "Privacy Protection of Automotive Location Data Based on\n  Format-Preserving Encryption of Geographical Coordinates",
    "summary" : "There are increasing risks of privacy disclosure when sharing the automotive\nlocation data in particular functions such as route navigation, driving\nmonitoring and vehicle scheduling. These risks could lead to the attacks\nincluding user behavior recognition, sensitive location inference and\ntrajectory reconstruction. In order to mitigate the data security risk caused\nby the automotive location sharing, this paper proposes a high-precision\nprivacy protection mechanism based on format-preserving encryption (FPE) of\ngeographical coordinates. The automotive coordinate data key mapping mechanism\nis designed to reduce to the accuracy loss of the geographical location data\ncaused by the repeated encryption and decryption. The experimental results\ndemonstrate that the average relative distance retention rate (RDR) reached\n0.0844, and the number of hotspots in the critical area decreased by 98.9%\nafter encryption. To evaluate the accuracy loss of the proposed encryption\nalgorithm on automotive geographical location data, this paper presents the\nexperimental analysis of decryption accuracy, and the result indicates that the\ndecrypted coordinate data achieves a restoration accuracy of 100%. This work\npresents a high-precision privacy protection method for automotive location\ndata, thereby providing an efficient data security solution for the sensitive\ndata sharing in autonomous driving.",
    "updated" : "2025-10-23T07:39:59Z",
    "published" : "2025-10-23T07:39:59Z",
    "authors" : [
      {
        "name" : "Haojie Ji"
      },
      {
        "name" : "Long Jin"
      },
      {
        "name" : "Haowen Li"
      },
      {
        "name" : "Chongshi Xin"
      },
      {
        "name" : "Te Hu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.20243v1",
    "title" : "HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine\n  Learning on Edge",
    "summary" : "Privacy-preserving machine learning (PPML) is an emerging topic to handle\nsecure machine learning inference over sensitive data in untrusted\nenvironments. Fully homomorphic encryption (FHE) enables computation directly\non encrypted data on the server side, making it a promising approach for PPML.\nHowever, it introduces significant communication and computation overhead on\nthe client side, making it impractical for edge devices. Hybrid homomorphic\nencryption (HHE) addresses this limitation by combining symmetric encryption\n(SE) with FHE to reduce the computational cost on the client side, and\ncombining with an FHE-friendly SE can also lessen the processing overhead on\nthe server side, making it a more balanced and efficient alternative. Our work\nproposes a hardware-accelerated HHE architecture built around a lightweight\nsymmetric cipher optimized for FHE compatibility and implemented as a dedicated\nhardware accelerator. To the best of our knowledge, this is the first design to\nintegrate an end-to-end HHE framework with hardware acceleration. Beyond this,\nwe also present several microarchitectural optimizations to achieve higher\nperformance and energy efficiency. The proposed work is integrated into a full\nPPML pipeline, enabling secure inference with significantly lower latency and\npower consumption than software implementations. Our contributions validate the\nfeasibility of low-power, hardware- accelerated HHE for edge deployment and\nprovide a hardware- software co-design methodology for building scalable,\nsecure machine learning systems in resource-constrained environments.\nExperiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x\nreduction in client-side encryption latency and nearly a 2x gain in hardware\nthroughput compared to existing FPGA-based HHE accelerators.",
    "updated" : "2025-10-23T05:51:55Z",
    "published" : "2025-10-23T05:51:55Z",
    "authors" : [
      {
        "name" : "Yu Hin Chan"
      },
      {
        "name" : "Hao Yang"
      },
      {
        "name" : "Shiyu Shen"
      },
      {
        "name" : "Xingyu Fan"
      },
      {
        "name" : "Shengzhe Lyu"
      },
      {
        "name" : "Patrick S. Y. Hung"
      },
      {
        "name" : "Ray C. C. Cheung"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.20157v1",
    "title" : "ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via\n  Variance-Reduced Stochastic Gradient Push",
    "summary" : "Differential privacy is widely employed in decentralized learning to\nsafeguard sensitive data by introducing noise into model updates. However,\nexisting approaches that use fixed-variance noise often degrade model\nperformance and reduce training efficiency. To address these limitations, we\npropose a novel approach called decentralized learning with adaptive\ndifferential privacy via variance-reduced stochastic gradient push (ADP-VRSGP).\nThis method dynamically adjusts both the noise variance and the learning rate\nusing a stepwise-decaying schedule, which accelerates training and enhances\nfinal model performance while providing node-level personalized privacy\nguarantees. To counteract the slowed convergence caused by large-variance noise\nin early iterations, we introduce a progressive gradient fusion strategy that\nleverages historical gradients. Furthermore, ADP-VRSGP incorporates\ndecentralized push-sum and aggregation techniques, making it particularly\nsuitable for time-varying communication topologies. Through rigorous\ntheoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence\nwith an appropriate learning rate, significantly improving training stability\nand speed. Experimental results validate that our method outperforms existing\nbaselines across multiple scenarios, highlighting its efficacy in addressing\nthe challenges of privacy-preserving decentralized learning.",
    "updated" : "2025-10-23T03:14:59Z",
    "published" : "2025-10-23T03:14:59Z",
    "authors" : [
      {
        "name" : "Xiaoming Wu"
      },
      {
        "name" : "Teng Liu"
      },
      {
        "name" : "Xin Wang"
      },
      {
        "name" : "Ming Yang"
      },
      {
        "name" : "Jiguo Yu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.20007v1",
    "title" : "zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust\n  in Confidential Agreements",
    "summary" : "Digital transactions currently exceed trillions of dollars annually, yet\ntraditional paper-based agreements remain a bottleneck for automation,\nenforceability, and dispute resolution. Natural language contracts introduce\nambiguity, require manual processing, and lack computational verifiability, all\nof which hinder efficient digital commerce. Computable legal contracts,\nexpressed in machine-readable formats, offer a potential solution by enabling\nautomated execution and verification. Blockchain-based smart contracts further\nstrengthen enforceability and accelerate dispute resolution; however, current\nimplementations risk exposing sensitive agreement terms on public ledgers,\nraising serious privacy and competitive intelligence concerns that limit\nenterprise adoption.\n  We introduce zk-agreements, a protocol designed to transition from\npaper-based trust to cryptographic trust while preserving confidentiality. Our\ndesign combines zero-knowledge proofs to protect private agreement terms,\nsecure two-party computation to enable private compliance evaluation, and smart\ncontracts to guarantee automated enforcement. Together, these components\nachieve both privacy preservation and computational enforceability, resolving\nthe fundamental tension between transparency and confidentiality in\nblockchain-based agreements.",
    "updated" : "2025-10-22T20:11:57Z",
    "published" : "2025-10-22T20:11:57Z",
    "authors" : [
      {
        "name" : "To-Wen Liu"
      },
      {
        "name" : "Matthew Green"
      }
    ],
    "categories" : [
      "cs.CR",
      "94A60, 68M14, 68Q85",
      "D.4.6; K.6.5; E.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.19979v1",
    "title" : "SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical\n  Tensors for Large Language Model Deployment",
    "summary" : "With the increasing deployment of Large Language Models (LLMs) on mobile and\nedge platforms, securing them against model extraction attacks has become a\npressing concern. However, protecting model privacy without sacrificing the\nperformance benefits of untrusted AI accelerators, such as GPUs, presents a\nchallenging trade-off. In this paper, we initiate the study of high-performance\nexecution on LLMs and present SecureInfer, a hybrid framework that leverages a\nheterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate\nprivacy-critical components while offloading compute-intensive operations to\nuntrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts\nan information-theoretic and threat-informed partitioning strategy:\nsecurity-sensitive components, including non-linear layers, projection of\nattention head, FNN transformations, and LoRA adapters, are executed inside an\nSGX enclave, while other linear operations (matrix multiplication) are\nperformed on the GPU after encryption and are securely restored within the\nenclave. We implement a prototype of SecureInfer using the LLaMA-2 model and\nevaluate it across performance and security metrics. Our results show that\nSecureInfer offers strong security guarantees with reasonable performance,\noffering a practical solution for secure on-device model inference.",
    "updated" : "2025-10-22T19:17:31Z",
    "published" : "2025-10-22T19:17:31Z",
    "authors" : [
      {
        "name" : "Tushar Nayan"
      },
      {
        "name" : "Ziqi Zhang"
      },
      {
        "name" : "Ruimin Sun"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.19934v1",
    "title" : "Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning\n  via $f$-Differential Privacy",
    "summary" : "Differentially private (DP) decentralized Federated Learning (FL) allows\nlocal users to collaborate without sharing their data with a central server.\nHowever, accurately quantifying the privacy budget of private FL algorithms is\nchallenging due to the co-existence of complex algorithmic components such as\ndecentralized communication and local updates. This paper addresses privacy\naccounting for two decentralized FL algorithms within the $f$-differential\nprivacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods\ntailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which\nquantifies privacy leakage between user pairs under random-walk communication,\nand Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise\ninjection via shared secrets. By combining tools from $f$-DP theory and Markov\nchain concentration, our accounting framework captures privacy amplification\narising from sparse communication, local iterations, and correlated noise.\nExperiments on synthetic and real datasets demonstrate that our methods yield\nconsistently tighter $(\\epsilon,\\delta)$ bounds and improved utility compared\nto R\\'enyi DP-based approaches, illustrating the benefits of $f$-DP in\ndecentralized privacy accounting.",
    "updated" : "2025-10-22T18:01:08Z",
    "published" : "2025-10-22T18:01:08Z",
    "authors" : [
      {
        "name" : "Xiang Li"
      },
      {
        "name" : "Buxin Su"
      },
      {
        "name" : "Chendi Wang"
      },
      {
        "name" : "Qi Long"
      },
      {
        "name" : "Weijie J. Su"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "math.ST",
      "stat.ME",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09114v2",
    "title" : "On the Fairness of Privacy Protection: Measuring and Mitigating the\n  Disparity of Group Privacy Risks for Differentially Private Machine Learning",
    "summary" : "While significant progress has been made in conventional fairness-aware\nmachine learning (ML) and differentially private ML (DPML), the fairness of\nprivacy protection across groups remains underexplored. Existing studies have\nproposed methods to assess group privacy risks, but these are based on the\naverage-case privacy risks of data records. Such approaches may underestimate\nthe group privacy risks, thereby potentially underestimating the disparity\nacross group privacy risks. Moreover, the current method for assessing the\nworst-case privacy risks of data records is time-consuming, limiting their\npractical applicability. To address these limitations, we introduce a novel\nmembership inference game that can efficiently audit the approximate worst-case\nprivacy risks of data records. Experimental results demonstrate that our method\nprovides a more stringent measurement of group privacy risks, yielding a\nreliable assessment of the disparity in group privacy risks. Furthermore, to\npromote privacy protection fairness in DPML, we enhance the standard DP-SGD\nalgorithm with an adaptive group-specific gradient clipping strategy, inspired\nby the design of canaries in differential privacy auditing studies. Extensive\nexperiments confirm that our algorithm effectively reduces the disparity in\ngroup privacy risks, thereby enhancing the fairness of privacy protection in\nDPML.",
    "updated" : "2025-10-23T13:48:13Z",
    "published" : "2025-10-10T08:09:08Z",
    "authors" : [
      {
        "name" : "Zhi Yang"
      },
      {
        "name" : "Changwu Huang"
      },
      {
        "name" : "Ke Tang"
      },
      {
        "name" : "Xin Yao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.21668v1",
    "title" : "Privacy Guarantee for Nash Equilibrium Computation of Aggregative Games\n  Based on Pointwise Maximal Leakage",
    "summary" : "Privacy preservation has served as a key metric in designing Nash equilibrium\n(NE) computation algorithms. Although differential privacy (DP) has been widely\nemployed for privacy guarantees, it does not exploit prior distributional\nknowledge of datasets and is ineffective in assessing information leakage for\ncorrelated datasets. To address these concerns, we establish a pointwise\nmaximal leakage (PML) framework when computing NE in aggregative games. By\nincorporating prior knowledge of players' cost function datasets, we obtain a\nprecise and computable upper bound of privacy leakage with PML guarantees. In\nthe entire view, we show PML refines DP by offering a tighter privacy\nguarantee, enabling flexibility in designing NE computation. Also, in the\nindividual view, we reveal that the lower bound of PML can exceed the upper\nbound of DP by constructing specific correlated datasets. The results emphasize\nthat PML is a more proper privacy measure than DP since the latter fails to\nadequately capture privacy leakage in correlated datasets. Moreover, we conduct\nexperiments with adversaries who attempt to infer players' private information\nto illustrate the effectiveness of our framework.",
    "updated" : "2025-10-24T17:24:24Z",
    "published" : "2025-10-24T17:24:24Z",
    "authors" : [
      {
        "name" : "Zhaoyang Cheng"
      },
      {
        "name" : "Guanpu Chen"
      },
      {
        "name" : "Tobias J. Oechtering"
      },
      {
        "name" : "Mikael Skoglund"
      }
    ],
    "categories" : [
      "cs.GT",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.21601v1",
    "title" : "PTMF: A Privacy Threat Modeling Framework for IoT with Expert-Driven\n  Threat Propagation Analysis",
    "summary" : "Previous studies on PTA have focused on analyzing privacy threats based on\nthe potential areas of occurrence and their likelihood of occurrence. However,\nan in-depth understanding of the threat actors involved, their actions, and the\nintentions that result in privacy threats is essential. In this paper, we\npresent a novel Privacy Threat Model Framework (PTMF) that analyzes privacy\nthreats through different phases.\n  The PTMF development is motivated through the selected tactics from the MITRE\nATT\\&CK framework and techniques from the LINDDUN privacy threat model, making\nPTMF a privacy-centered framework. The proposed PTMF can be employed in various\nways, including analyzing the activities of threat actors during privacy\nthreats and assessing privacy risks in IoT systems, among others. In this\npaper, we conducted a user study on 12 privacy threats associated with IoT by\ndeveloping a questionnaire based on PTMF and recruited experts from both\nindustry and academia in the fields of security and privacy to gather their\nopinions. The collected data were analyzed and mapped to identify the threat\nactors involved in the identification of IoT users (IU) and the remaining 11\nprivacy threats. Our observation revealed the top three threat actors and the\ncritical paths they used during the IU privacy threat, as well as the remaining\n11 privacy threats. This study could provide a solid foundation for\nunderstanding how and where privacy measures can be proactively and effectively\ndeployed in IoT systems to mitigate privacy threats based on the activities and\nintentions of threat actors within these systems.",
    "updated" : "2025-10-24T16:06:04Z",
    "published" : "2025-10-24T16:06:04Z",
    "authors" : [
      {
        "name" : "Emmanuel Dare Alalade"
      },
      {
        "name" : "Ashraf Matrawy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.21591v1",
    "title" : "Privacy by Design: Aligning GDPR and Software Engineering Specifications\n  with a Requirements Engineering Approach",
    "summary" : "Context: Consistent requirements and system specifications are essential for\nthe compliance of software systems towards the General Data Protection\nRegulation (GDPR). Both artefacts need to be grounded in the original text and\nconjointly assure the achievement of privacy by design (PbD). Objectives: There\nis little understanding of the perspectives of practitioners on specification\nobjectives and goals to address PbD. Existing approaches do not account for the\ncomplex intersection between problem and solution space expressed in GDPR. In\nthis study we explore the demand for conjoint requirements and system\nspecification for PbD and suggest an approach to address this demand. Methods:\nWe reviewed secondary and related primary studies and conducted interviews with\npractitioners to (1) investigate the state-of-practice and (2) understand the\nunderlying specification objectives and goals (e.g., traceability). We\ndeveloped and evaluated an approach for requirements and systems specification\nfor PbD, and evaluated it against the specification objectives. Results: The\nrelationship between problem and solution space, as expressed in GDPR, is\ninstrumental in supporting PbD. We demonstrate how our approach, based on the\nmodeling GDPR content with original legal concepts, contributes to\nspecification objectives of capturing legal knowledge, supporting specification\ntransparency, and traceability. Conclusion: GDPR demands need to be addressed\nthroughout different levels of abstraction in the engineering lifecycle to\nachieve PbD. Legal knowledge specified in the GDPR text should be captured in\nspecifications to address the demands of different stakeholders and ensure\ncompliance. While our results confirm the suitability of our approach to\naddress practical needs, we also revealed specific needs for the future\neffective operationalization of the approach.",
    "updated" : "2025-10-24T15:59:34Z",
    "published" : "2025-10-24T15:59:34Z",
    "authors" : [
      {
        "name" : "Oleksandr Kosenkov"
      },
      {
        "name" : "Ehsan Zabardast"
      },
      {
        "name" : "Davide Fucci"
      },
      {
        "name" : "Daniel Mendez"
      },
      {
        "name" : "Michael Unterkalmsteiner"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.23463v1",
    "title" : "Differential Privacy as a Perk: Federated Learning over Multiple-Access\n  Fading Channels with a Multi-Antenna Base Station",
    "summary" : "Federated Learning (FL) is a distributed learning paradigm that preserves\nprivacy by eliminating the need to exchange raw data during training. In its\nprototypical edge instantiation with underlying wireless transmissions enabled\nby analog over-the-air computing (AirComp), referred to as \\emph{over-the-air\nFL (AirFL)}, the inherent channel noise plays a unique role of \\emph{frenemy}\nin the sense that it degrades training due to noisy global aggregation while\nproviding a natural source of randomness for privacy-preserving mechanisms,\nformally quantified by \\emph{differential privacy (DP)}. It remains,\nnevertheless, challenging to effectively harness such channel impairments, as\nprior arts, under assumptions of either simple channel models or restricted\ntypes of loss functions, mostly considering (local) DP enhancement with a\nsingle-round or non-convergent bound on privacy loss. In this paper, we study\nAirFL over multiple-access fading channels with a multi-antenna base station\n(BS) subject to user-level DP requirements. Despite a recent study, which\nclaimed in similar settings that artificial noise (AN) must be injected to\nensure DP in general, we demonstrate, on the contrary, that DP can be gained as\na \\emph{perk} even \\emph{without} employing any AN. Specifically, we derive a\nnovel bound on DP that converges under general bounded-domain assumptions on\nmodel parameters, along with a convergence bound with general smooth and\nnon-convex loss functions. Next, we optimize over receive beamforming and power\nallocations to characterize the optimal convergence-privacy trade-offs, which\nalso reveal explicit conditions in which DP is achievable without compromising\ntraining. Finally, our theoretical findings are validated by extensive\nnumerical results.",
    "updated" : "2025-10-27T16:01:15Z",
    "published" : "2025-10-27T16:01:15Z",
    "authors" : [
      {
        "name" : "Hao Liang"
      },
      {
        "name" : "Haifeng Wen"
      },
      {
        "name" : "Kaishun Wu"
      },
      {
        "name" : "Hong Xing"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.23427v1",
    "title" : "PrivacyGuard: A Modular Framework for Privacy Auditing in Machine\n  Learning",
    "summary" : "The increasing deployment of Machine Learning (ML) models in sensitive\ndomains motivates the need for robust, practical privacy assessment tools.\nPrivacyGuard is a comprehensive tool for empirical differential privacy (DP)\nanalysis, designed to evaluate privacy risks in ML models through\nstate-of-the-art inference attacks and advanced privacy measurement techniques.\nTo this end, PrivacyGuard implements a diverse suite of privacy attack --\nincluding membership inference , extraction, and reconstruction attacks --\nenabling both off-the-shelf and highly configurable privacy analyses. Its\nmodular architecture allows for the seamless integration of new attacks, and\nprivacy metrics, supporting rapid adaptation to emerging research advances. We\nmake PrivacyGuard available at\nhttps://github.com/facebookresearch/PrivacyGuard.",
    "updated" : "2025-10-27T15:33:01Z",
    "published" : "2025-10-27T15:33:01Z",
    "authors" : [
      {
        "name" : "Luca Melis"
      },
      {
        "name" : "Matthew Grange"
      },
      {
        "name" : "Iden Kalemaj"
      },
      {
        "name" : "Karan Chadha"
      },
      {
        "name" : "Shengyuan Hu"
      },
      {
        "name" : "Elena Kashtelyan"
      },
      {
        "name" : "Will Bullock"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.23274v1",
    "title" : "Privacy-Preserving Semantic Communication over Wiretap Channels with\n  Learnable Differential Privacy",
    "summary" : "While semantic communication (SemCom) improves transmission efficiency by\nfocusing on task-relevant information, it also raises critical privacy\nconcerns. Many existing secure SemCom approaches rely on restrictive or\nimpractical assumptions, such as favorable channel conditions for the\nlegitimate user or prior knowledge of the eavesdropper's model. To address\nthese limitations, this paper proposes a novel secure SemCom framework for\nimage transmission over wiretap channels, leveraging differential privacy (DP)\nto provide approximate privacy guarantees. Specifically, our approach first\nextracts disentangled semantic representations from source images using\ngenerative adversarial network (GAN) inversion method, and then selectively\nperturbs private semantic representations with approximate DP noise. Distinct\nfrom conventional DP-based protection methods, we introduce DP noise with\nlearnable pattern, instead of traditional white Gaussian or Laplace noise,\nachieved through adversarial training of neural networks (NNs). This design\nmitigates the inherent non-invertibility of DP while effectively protecting\nprivate information. Moreover, it enables explicitly controllable security\nlevels by adjusting the privacy budget according to specific security\nrequirements, which is not achieved in most existing secure SemCom approaches.\nExperimental results demonstrate that, compared with the previous DP-based\nmethod and direct transmission, the proposed method significantly degrades the\nreconstruction quality for the eavesdropper, while introducing only slight\ndegradation in task performance. Under comparable security levels, our approach\nachieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86\nfor the legitimate user compared with the previous DP-based method.",
    "updated" : "2025-10-27T12:34:20Z",
    "published" : "2025-10-27T12:34:20Z",
    "authors" : [
      {
        "name" : "Weixuan Chen"
      },
      {
        "name" : "Qianqian Yang"
      },
      {
        "name" : "Shuo Shao"
      },
      {
        "name" : "Shunpu Tang"
      },
      {
        "name" : "Zhiguo Shi"
      },
      {
        "name" : "Shui Yu"
      }
    ],
    "categories" : [
      "cs.CR",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.23024v1",
    "title" : "A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem",
    "summary" : "Virtual Reality (VR) has gained increasing traction among various domains in\nrecent years, with major companies such as Meta, Pico, and Microsoft launching\ntheir application stores to support third-party developers in releasing their\napplications (or simply apps). These apps offer rich functionality but\ninherently collect privacy-sensitive data, such as user biometrics, behaviors,\nand the surrounding environment. Nevertheless, there is still a lack of\ndomain-specific regulations to govern the data handling of VR apps, resulting\nin significant variations in their privacy practices among app stores.\n  In this work, we present the first comprehensive multi-store study of privacy\npractices in the current VR app ecosystem, covering a large-scale dataset\ninvolving 6,565 apps collected from five major app stores. We assess both\ndeclarative and behavioral privacy practices of VR apps, using a multi-faceted\napproach based on natural language processing, reverse engineering, and static\nanalysis. Our assessment reveals significant privacy compliance issues across\nall stores, underscoring the premature status of privacy protection in this\nrapidly growing ecosystem. For instance, one third of apps fail to declare\ntheir use of sensitive data, and 21.5\\% of apps neglect to provide valid\nprivacy policies. Our work sheds light on the status quo of privacy protection\nwithin the VR app ecosystem for the first time. Our findings should raise an\nalert to VR app developers and users, and encourage store operators to\nimplement stringent regulations on privacy compliance among VR apps.",
    "updated" : "2025-10-27T05:42:29Z",
    "published" : "2025-10-27T05:42:29Z",
    "authors" : [
      {
        "name" : "Chuan Yan"
      },
      {
        "name" : "Zeng Li"
      },
      {
        "name" : "Kunlin Cai"
      },
      {
        "name" : "Liuhuo Wan"
      },
      {
        "name" : "Ruomai Ren"
      },
      {
        "name" : "Yiran Shen"
      },
      {
        "name" : "Guangdong Bai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.22387v1",
    "title" : "Privacy-Aware Federated nnU-Net for ECG Page Digitization",
    "summary" : "Deep neural networks can convert ECG page images into analyzable waveforms,\nyet centralized training often conflicts with cross-institutional privacy and\ndeployment constraints. A cross-silo federated digitization framework is\npresented that trains a full-model nnU-Net segmentation backbone without\nsharing images and aggregates updates across sites under realistic non-IID\nheterogeneity (layout, grid style, scanner profile, noise).\n  The protocol integrates three standard server-side aggregators--FedAvg,\nFedProx, and FedAdam--and couples secure aggregation with central, user-level\ndifferential privacy to align utility with formal guarantees. Key features\ninclude: (i) end-to-end full-model training and synchronization across clients;\n(ii) secure aggregation so the server only observes a clipped, weighted sum\nonce a participation threshold is met; (iii) central Gaussian DP with Renyi\naccounting applied post-aggregation for auditable user-level privacy; and (iv)\na calibration-aware digitization pipeline comprising page normalization, trace\nsegmentation, grid-leakage suppression, and vectorization to twelve-lead\nsignals.\n  Experiments on ECG pages rendered from PTB-XL show consistently faster\nconvergence and higher late-round plateaus with adaptive server updates\n(FedAdam) relative to FedAvg and FedProx, while approaching centralized\nperformance. The privacy mechanism maintains competitive accuracy while\npreventing exposure of raw images or per-client updates, yielding deployable,\nauditable guarantees suitable for multi-institution settings.",
    "updated" : "2025-10-25T18:10:05Z",
    "published" : "2025-10-25T18:10:05Z",
    "authors" : [
      {
        "name" : "Nader Nemati"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.21946v1",
    "title" : "$$-STEAL: LLM Stealing Attack with Local Differential Privacy",
    "summary" : "Large language models (LLMs) demonstrate remarkable capabilities across\nvarious tasks. However, their deployment introduces significant risks related\nto intellectual property. In this context, we focus on model stealing attacks,\nwhere adversaries replicate the behaviors of these models to steal services.\nThese attacks are highly relevant to proprietary LLMs and pose serious threats\nto revenue and financial stability. To mitigate these risks, the watermarking\nsolution embeds imperceptible patterns in LLM outputs, enabling model\ntraceability and intellectual property verification. In this paper, we study\nthe vulnerability of LLM service providers by introducing $\\delta$-STEAL, a\nnovel model stealing attack that bypasses the service provider's watermark\ndetectors while preserving the adversary's model utility. $\\delta$-STEAL\ninjects noise into the token embeddings of the adversary's model during\nfine-tuning in a way that satisfies local differential privacy (LDP)\nguarantees. The adversary queries the service provider's model to collect\noutputs and form input-output training pairs. By applying LDP-preserving noise\nto these pairs, $\\delta$-STEAL obfuscates watermark signals, making it\ndifficult for the service provider to determine whether its outputs were used,\nthereby preventing claims of model theft. Our experiments show that\n$\\delta$-STEAL with lightweight modifications achieves attack success rates of\nup to $96.95\\%$ without significantly compromising the adversary's model\nutility. The noise scale in LDP controls the trade-off between attack\neffectiveness and model utility. This poses a significant risk, as even robust\nwatermarks can be bypassed, allowing adversaries to deceive watermark detectors\nand undermine current intellectual property protection methods.",
    "updated" : "2025-10-24T18:19:38Z",
    "published" : "2025-10-24T18:19:38Z",
    "authors" : [
      {
        "name" : "Kieu Dang"
      },
      {
        "name" : "Phung Lai"
      },
      {
        "name" : "NhatHai Phan"
      },
      {
        "name" : "Yelong Shen"
      },
      {
        "name" : "Ruoming Jin"
      },
      {
        "name" : "Abdallah Khreishah"
      }
    ],
    "categories" : [
      "cs.CR",
      "68T07, 68T50",
      "I.2.6; I.2.7; K.6.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.21858v1",
    "title" : "Privacy-preserving Decision-focused Learning for Multi-energy Systems",
    "summary" : "Decision-making for multi-energy system (MES) dispatch depends on accurate\nload forecasting. Traditionally, load forecasting and decision-making for MES\nare implemented separately. Forecasting models are typically trained to\nminimize forecasting errors, overlooking their impact on downstream\ndecision-making. To address this, decision-focused learning (DFL) has been\nstudied to minimize decision-making costs instead. However, practical adoption\nof DFL in MES faces significant challenges: the process requires sharing\nsensitive load data and model parameters across multiple sectors, raising\nserious privacy issues. To this end, we propose a privacy-preserving DFL\nframework tailored for MES. Our approach introduces information masking to\nsafeguard private data while enabling recovery of decision variables and\ngradients required for model training. To further enhance security for DFL, we\ndesign a safety protocol combining matrix decomposition and homomorphic\nencryption, effectively preventing collusion and unauthorized data access.\nAdditionally, we developed a privacy-preserving load pattern recognition\nalgorithm, enabling the training of specialized DFL models for heterogeneous\nload patterns. Theoretical analysis and comprehensive case studies, including\nreal-world MES data, demonstrate that our framework not only protects privacy\nbut also consistently achieves lower average daily dispatch costs compared to\nexisting methods.",
    "updated" : "2025-10-23T04:20:50Z",
    "published" : "2025-10-23T04:20:50Z",
    "authors" : [
      {
        "name" : "Yangze Zhou"
      },
      {
        "name" : "Ruiyang Yao"
      },
      {
        "name" : "Dalin Qin"
      },
      {
        "name" : "Yixiong Jia"
      },
      {
        "name" : "Yi Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.24498v1",
    "title" : "Design and Optimization of Cloud Native Homomorphic Encryption Workflows\n  for Privacy-Preserving ML Inference",
    "summary" : "As machine learning (ML) models become increasingly deployed through cloud\ninfrastructures, the confidentiality of user data during inference poses a\nsignificant security challenge. Homomorphic Encryption (HE) has emerged as a\ncompelling cryptographic technique that enables computation on encrypted data,\nallowing predictions to be generated without decrypting sensitive inputs.\nHowever, the integration of HE within large scale cloud native pipelines\nremains constrained by high computational overhead, orchestration complexity,\nand model compatibility issues.\n  This paper presents a systematic framework for the design and optimization of\ncloud native homomorphic encryption workflows that support privacy-preserving\nML inference. The proposed architecture integrates containerized HE modules\nwith Kubernetes-based orchestration, enabling elastic scaling and parallel\nencrypted computation across distributed environments. Furthermore,\noptimization strategies including ciphertext packing, polynomial modulus\nadjustment, and operator fusion are employed to minimize latency and resource\nconsumption while preserving cryptographic integrity. Experimental results\ndemonstrate that the proposed system achieves up to 3.2times inference\nacceleration and 40% reduction in memory utilization compared to conventional\nHE pipelines. These findings illustrate a practical pathway for deploying\nsecure ML-as-a-Service (MLaaS) systems that guarantee data confidentiality\nunder zero-trust cloud conditions.",
    "updated" : "2025-10-28T15:13:32Z",
    "published" : "2025-10-28T15:13:32Z",
    "authors" : [
      {
        "name" : "Tejaswini Bollikonda"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.24233v1",
    "title" : "PRIVET: Privacy Metric Based on Extreme Value Theory",
    "summary" : "Deep generative models are often trained on sensitive data, such as genetic\nsequences, health data, or more broadly, any copyrighted, licensed or protected\ncontent. This raises critical concerns around privacy-preserving synthetic\ndata, and more specifically around privacy leakage, an issue closely tied to\noverfitting. Existing methods almost exclusively rely on global criteria to\nestimate the risk of privacy failure associated to a model, offering only\nquantitative non interpretable insights. The absence of rigorous evaluation\nmethods for data privacy at the sample-level may hinder the practical\ndeployment of synthetic data in real-world applications. Using extreme value\nstatistics on nearest-neighbor distances, we propose PRIVET, a generic\nsample-based, modality-agnostic algorithm that assigns an individual privacy\nleak score to each synthetic sample. We empirically demonstrate that PRIVET\nreliably detects instances of memorization and privacy leakage across diverse\ndata modalities, including settings with very high dimensionality, limited\nsample sizes such as genetic data and even under underfitting regimes. We\ncompare our method to existing approaches under controlled settings and show\nits advantage in providing both dataset level and sample level assessments\nthrough qualitative and quantitative outputs. Additionally, our analysis\nreveals limitations in existing computer vision embeddings to yield\nperceptually meaningful distances when identifying near-duplicate samples.",
    "updated" : "2025-10-28T09:42:03Z",
    "published" : "2025-10-28T09:42:03Z",
    "authors" : [
      {
        "name" : "Antoine Szatkownik"
      },
      {
        "name" : "Aurlien Decelle"
      },
      {
        "name" : "Beatriz Seoane"
      },
      {
        "name" : "Nicolas Bereux"
      },
      {
        "name" : "Lo Planche"
      },
      {
        "name" : "Guillaume Charpiat"
      },
      {
        "name" : "Burak Yelmen"
      },
      {
        "name" : "Flora Jay"
      },
      {
        "name" : "Cyril Furtlehner"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.24072v1",
    "title" : "Covert Surveillance in Smart Devices: A SCOUR Framework Analysis of\n  Youth Privacy Implications",
    "summary" : "This paper investigates how smart devices covertly capture private\nconversations and discusses in more in-depth the implications of this for youth\nprivacy. Using a structured review guided by the PRISMA methodology, the\nanalysis focuses on privacy concerns, data capture methods, data storage and\nsharing practices, and proposed technical mitigations. To structure and\nsynthesize findings, we introduce the SCOUR framework, encompassing\nSurveillance mechanisms, Consent and awareness, Operational data flow, Usage\nand exploitation, and Regulatory and technical safeguards. Findings reveal that\nsmart devices have been covertly capturing personal data, especially with smart\ntoys and voice-activated smart gadgets built for youth. These issues are\nworsened by unclear data collection practices and insufficient transparency in\nsmart device applications. Balancing privacy and utility in smart devices is\ncrucial, as youth are becoming more aware of privacy breaches and value their\npersonal data more. Strategies to improve regulatory and technical safeguards\nare also provided. The review identifies research gaps and suggests future\ndirections. The limitations of this literature review are also explained. The\nfindings have significant implications for policy development and the\ntransparency of data collection for smart devices.",
    "updated" : "2025-10-28T05:10:10Z",
    "published" : "2025-10-28T05:10:10Z",
    "authors" : [
      {
        "name" : "Austin Shouli"
      },
      {
        "name" : "Yulia Bobkova"
      },
      {
        "name" : "Ajay Kumar Shrestha"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.23931v1",
    "title" : "Differential Privacy: Gradient Leakage Attacks in Federated Learning\n  Environments",
    "summary" : "Federated Learning (FL) allows for the training of Machine Learning models in\na collaborative manner without the need to share sensitive data. However, it\nremains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private\ninformation from the shared model updates. In this work, we investigate the\neffectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD\nand a variant based on explicit regularization (PDP-SGD) - as defenses against\nGLAs. To this end, we evaluate the performance of several computer vision\nmodels trained under varying privacy levels on a simple classification task,\nand then analyze the quality of private data reconstructions obtained from the\nintercepted gradients in a simulated FL environment. Our results demonstrate\nthat DP-SGD significantly mitigates the risk of gradient leakage attacks,\nalbeit with a moderate trade-off in model utility. In contrast, PDP-SGD\nmaintains strong classification performance but proves ineffective as a\npractical defense against reconstruction attacks. These findings highlight the\nimportance of empirically evaluating privacy mechanisms beyond their\ntheoretical guarantees, particularly in distributed learning scenarios where\ninformation leakage may represent an unassumable critical threat to data\nsecurity and privacy.",
    "updated" : "2025-10-27T23:33:21Z",
    "published" : "2025-10-27T23:33:21Z",
    "authors" : [
      {
        "name" : "Miguel Fernandez-de-Retana"
      },
      {
        "name" : "Unai Zulaika"
      },
      {
        "name" : "Rubn Snchez-Corcuera"
      },
      {
        "name" : "Aitor Almeida"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "68T07 (Primary) 68M14, 68P27, 68Q32, 94A16, 62H35 (Secondary)",
      "I.2.11; I.2.6; C.2.4; D.4.6; K.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04527v2",
    "title" : "Quantum capacity amplification via privacy",
    "summary" : "We investigate superadditivity of quantum capacity through private channels\nwhose Choi-Jamiolkowski operators are private states. This perspective links\nthe security structure of private states to quantum capacity and clarifies the\nrole of the shield system: information encoded in the shield system that would\notherwise leak to the environment can be recycled when paired with an assisting\nchannel, thereby boosting capacity. Our main contributions are threefold:\nFirstly, we develop a general framework that provides a sufficient condition\nfor capacity amplification, which is formulated in terms of the assisting\nchannel's Holevo information. As examples, we give explicit, dimension and\nparameter dependent amplification thresholds for erasure and depolarizing\nchannels. Secondly, assuming the Spin alignment conjecture, we derive a\nsingle-letter expression for the quantum capacity of a family of private\nchannels that are neither degradable, anti-degradable, nor PPT; as an\napplication, we construct channels with vanishing quantum capacity yet\nunbounded private capacity. Thirdly, we further analyze approximate private\nchannels: we give an alternative proof of superactivation that extends its\nvalidity to a broader parameter regime, and, by combining amplification bounds\nwith continuity estimates, we establish a metric separation showing that\nchannels exhibiting capacity amplification have nonzero diamond distance from\nthe set of anti-degradable channels, indicating that existing approximate\n(anti-)degradability bounds are not tight. We also revisit the computability of\nthe regularized quantum capacity and modestly suggest that this fundamental\nquestion still remains open.",
    "updated" : "2025-10-28T03:47:53Z",
    "published" : "2025-10-06T06:35:19Z",
    "authors" : [
      {
        "name" : "Peixue Wu"
      },
      {
        "name" : "Yunkai Wang"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.IT",
      "math.MP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.25670v1",
    "title" : "Spectral Perturbation Bounds for Low-Rank Approximation with\n  Applications to Privacy",
    "summary" : "A central challenge in machine learning is to understand how noise or\nmeasurement errors affect low-rank approximations, particularly in the spectral\nnorm. This question is especially important in differentially private low-rank\napproximation, where one aims to preserve the top-$p$ structure of a\ndata-derived matrix while ensuring privacy. Prior work often analyzes Frobenius\nnorm error or changes in reconstruction quality, but these metrics can over- or\nunder-estimate true subspace distortion. The spectral norm, by contrast,\ncaptures worst-case directional error and provides the strongest utility\nguarantees. We establish new high-probability spectral-norm perturbation bounds\nfor symmetric matrices that refine the classical Eckart--Young--Mirsky theorem\nand explicitly capture interactions between a matrix $A \\in \\mathbb{R}^{n\n\\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and\nnorm conditions, our bounds yield sharp estimates for $\\|(A + E)_p - A_p\\|$,\nwhere $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up\nto a factor of $\\sqrt{n}$. As an application, we derive improved utility\nguarantees for differentially private PCA, resolving an open problem in the\nliterature. Our analysis relies on a novel contour bootstrapping method from\ncomplex analysis and extends it to a broad class of spectral functionals,\nincluding polynomials and matrix exponentials. Empirical results on real-world\ndatasets confirm that our bounds closely track the actual spectral error under\ndiverse perturbation regimes.",
    "updated" : "2025-10-29T16:36:00Z",
    "published" : "2025-10-29T16:36:00Z",
    "authors" : [
      {
        "name" : "Phuc Tran"
      },
      {
        "name" : "Nisheeth K. Vishnoi"
      },
      {
        "name" : "Van H. Vu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DS",
      "cs.NA",
      "math.NA",
      "math.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.25477v1",
    "title" : "A Study on Privacy-Preserving Scholarship Evaluation Based on\n  Decentralized Identity and Zero-Knowledge Proofs",
    "summary" : "Traditional centralized scholarship evaluation processes typically require\nstudents to submit detailed academic records and qualification information,\nwhich exposes them to risks of data leakage and misuse, making it difficult to\nsimultaneously ensure privacy protection and transparent auditability. To\naddress these challenges, this paper proposes a scholarship evaluation system\nbased on Decentralized Identity (DID) and Zero-Knowledge Proofs (ZKP). The\nsystem aggregates multidimensional ZKPs off-chain, and smart contracts verify\ncompliance with evaluation criteria without revealing raw scores or\ncomputational details. Experimental results demonstrate that the proposed\nsolution not only automates the evaluation efficiently but also maximally\npreserves student privacy and data integrity, offering a practical and\ntrustworthy technical paradigm for higher education scholarship programs.",
    "updated" : "2025-10-29T12:56:02Z",
    "published" : "2025-10-29T12:56:02Z",
    "authors" : [
      {
        "name" : "Yi Chen"
      },
      {
        "name" : "Bin Chen"
      },
      {
        "name" : "Peichang Zhang"
      },
      {
        "name" : "Da Che"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.25277v1",
    "title" : "A Privacy-Preserving Ecosystem for Developing Machine Learning\n  Algorithms Using Patient Data: Insights from the TUM.ai Makeathon",
    "summary" : "The integration of clinical data offers significant potential for the\ndevelopment of personalized medicine. However, its use is severely restricted\nby the General Data Protection Regulation (GDPR), especially for small cohorts\nwith rare diseases. High-quality, structured data is essential for the\ndevelopment of predictive medical AI. In this case study, we propose a novel,\nmulti-stage approach to secure AI training: (1) The model is designed on a\nsimulated clinical knowledge graph (cKG). This graph is used exclusively to\nrepresent the structural characteristics of the real cKG without revealing any\nsensitive content. (2) The model is then integrated into the FeatureCloud (FC)\nfederated learning framework, where it is prepared in a single-client\nconfiguration within a protected execution environment. (3) Training then takes\nplace within the hospital environment on the real cKG, either under the direct\nsupervision of hospital staff or via a fully automated pipeline controlled by\nthe hospital. (4) Finally, verified evaluation scripts are executed, which only\nreturn aggregated performance metrics. This enables immediate performance\nfeedback without sensitive patient data or individual predictions, leaving the\nclinic. A fundamental element of this approach involves the incorporation of a\ncKG, which serves to organize multi-omics and patient data within the context\nof real-world hospital environments. This approach was successfully validated\nduring the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner\nChildren's Hospital (HCH-LMU): 50 students developed models for patient\nclassification and diagnosis without access to real data. Deploying secure\nalgorithms via federated frameworks, such as the FC framework, could be a\npractical way of achieving privacy-preserving AI in healthcare.",
    "updated" : "2025-10-29T08:37:12Z",
    "published" : "2025-10-29T08:37:12Z",
    "authors" : [
      {
        "name" : "Simon Swer"
      },
      {
        "name" : "Mai Khanh Mai"
      },
      {
        "name" : "Christoph Klein"
      },
      {
        "name" : "Nicola Gtzenberger"
      },
      {
        "name" : "Denis Dali"
      },
      {
        "name" : "Andreas Maier"
      },
      {
        "name" : "Jan Baumbach"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.24807v1",
    "title" : "Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases",
    "summary" : "Privacy concerns have become increasingly critical in modern AI and data\nscience applications, where sensitive information is collected, analyzed, and\nshared across diverse domains such as healthcare, finance, and mobility. While\nprior research has focused on protecting privacy in a single data release, many\nreal-world systems operate under sequential or continuous data publishing,\nwhere the same or related data are released over time. Such sequential\ndisclosures introduce new vulnerabilities, as temporal correlations across\nreleases may enable adversaries to infer sensitive information that remains\nhidden in any individual release. In this paper, we investigate whether an\nattacker can compromise privacy in sequential data releases by exploiting\ndependencies between consecutive publications, even when each individual\nrelease satisfies standard privacy guarantees. To this end, we propose a novel\nattack model that captures these sequential dependencies by integrating a\nHidden Markov Model with a reinforcement learning-based bi-directional\ninference mechanism. This enables the attacker to leverage both earlier and\nlater observations in the sequence to infer private information. We instantiate\nour framework in the context of trajectory data, demonstrating how an adversary\ncan recover sensitive locations from sequential mobility datasets. Extensive\nexperiments on Geolife, Porto Taxi, and SynMob datasets show that our model\nconsistently outperforms baseline approaches that treat each release\nindependently. The results reveal a fundamental privacy risk inherent to\nsequential data publishing, where individually protected releases can\ncollectively leak sensitive information when analyzed temporally. These\nfindings underscore the need for new privacy-preserving frameworks that\nexplicitly model temporal dependencies, such as time-aware differential privacy\nor sequential data obfuscation strategies.",
    "updated" : "2025-10-28T04:32:42Z",
    "published" : "2025-10-28T04:32:42Z",
    "authors" : [
      {
        "name" : "Ziyao Cui"
      },
      {
        "name" : "Minxing Zhang"
      },
      {
        "name" : "Jian Pei"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.23463v2",
    "title" : "Differential Privacy as a Perk: Federated Learning over Multiple-Access\n  Fading Channels with a Multi-Antenna Base Station",
    "summary" : "Federated Learning (FL) is a distributed learning paradigm that preserves\nprivacy by eliminating the need to exchange raw data during training. In its\nprototypical edge instantiation with underlying wireless transmissions enabled\nby analog over-the-air computing (AirComp), referred to as \\emph{over-the-air\nFL (AirFL)}, the inherent channel noise plays a unique role of \\emph{frenemy}\nin the sense that it degrades training due to noisy global aggregation while\nproviding a natural source of randomness for privacy-preserving mechanisms,\nformally quantified by \\emph{differential privacy (DP)}. It remains,\nnevertheless, challenging to effectively harness such channel impairments, as\nprior arts, under assumptions of either simple channel models or restricted\ntypes of loss functions, mostly considering (local) DP enhancement with a\nsingle-round or non-convergent bound on privacy loss. In this paper, we study\nAirFL over multiple-access fading channels with a multi-antenna base station\n(BS) subject to user-level DP requirements. Despite a recent study, which\nclaimed in similar settings that artificial noise (AN) must be injected to\nensure DP in general, we demonstrate, on the contrary, that DP can be gained as\na \\emph{perk} even \\emph{without} employing any AN. Specifically, we derive a\nnovel bound on DP that converges under general bounded-domain assumptions on\nmodel parameters, along with a convergence bound with general smooth and\nnon-convex loss functions. Next, we optimize over receive beamforming and power\nallocations to characterize the optimal convergence-privacy trade-offs, which\nalso reveal explicit conditions in which DP is achievable without compromising\ntraining. Finally, our theoretical findings are validated by extensive\nnumerical results.",
    "updated" : "2025-10-29T11:16:37Z",
    "published" : "2025-10-27T16:01:15Z",
    "authors" : [
      {
        "name" : "Hao Liang"
      },
      {
        "name" : "Haifeng Wen"
      },
      {
        "name" : "Kaishun Wu"
      },
      {
        "name" : "Hong Xing"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.26523v1",
    "title" : "Interdependent Privacy in Smart Homes: Hunting for Bystanders in Privacy\n  Policies",
    "summary" : "Smart home devices such as video doorbells and security cameras are becoming\nincreasingly common in everyday life. While these devices offer convenience and\nsafety, they also raise new privacy concerns: how these devices affect others,\nlike neighbors, visitors, or people passing by. This issue is generally known\nas interdependent privacy, where one person's actions (or inaction) may impact\nthe privacy of others, and, specifically, bystander privacy in the context of\nsmart homes. Given lax data protection regulations in terms of shared physical\nspaces and amateur joint data controllers, we expect that the privacy policies\nof smart home products reflect the missing regulatory incentives. This paper\npresents a focused privacy policy analysis of 20 video doorbell and smart\ncamera products, concentrating explicitly on the bystander aspect. We show that\nalthough some of the vendors acknowledge bystanders, they address it only to\nthe extent of including disclaimers, shifting the ethical responsibility for\ncollecting the data of non-users to the device owner. In addition, we identify\nand examine real-world cases related to bystander privacy, demonstrating how\ncurrent deployments can impact non-users. Based on our findings, we analyze\nvendor privacy policies in light of existing legal frameworks and technical\ncapabilities, and we provide practical recommendations for both policy language\nand system design to enhance transparency and empower both bystanders and\ndevice owners.",
    "updated" : "2025-10-30T14:16:21Z",
    "published" : "2025-10-30T14:16:21Z",
    "authors" : [
      {
        "name" : "Shuaishuai Liu"
      },
      {
        "name" : "Gergely Acs"
      },
      {
        "name" : "Gergely Biczk"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.26148v1",
    "title" : "STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human\n  Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing\n  Environments",
    "summary" : "Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)\npresents a privacy-preserving, contactless sensing approach suitable for smart\nhomes, healthcare monitoring, and mobile IoT systems. However, existing methods\noften encounter computational inefficiency, high latency, and limited\nfeasibility within resource-constrained, embedded mobile edge environments.\nThis paper proposes STAR (Sensing Technology for Activity Recognition), an\nedge-AI-optimized framework that integrates a lightweight neural architecture,\nadaptive signal processing, and hardware-aware co-optimization to enable\nreal-time, energy-efficient HAR on low-power embedded devices. STAR\nincorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural\nnetwork, reducing model parameters by 33% compared to conventional LSTM models\nwhile maintaining effective temporal modeling capability. A multi-stage\npre-processing pipeline combining median filtering, 8th-order Butterworth\nlow-pass filtering, and Empirical Mode Decomposition (EMD) is employed to\ndenoise CSI amplitude data and extract spatial-temporal features. For on-device\ndeployment, STAR is implemented on a Rockchip RV1126 processor equipped with an\nembedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI\nacquisition module. Experimental results demonstrate a mean recognition\naccuracy of 93.52% across seven activity classes and 99.11% for human presence\ndetection, utilizing a compact 97.6k-parameter model. INT8 quantized inference\nachieves a processing speed of 33 MHz with just 8% CPU utilization, delivering\nsixfold speed improvements over CPU-based execution. With sub-second response\nlatency and low power consumption, the system ensures real-time,\nprivacy-preserving HAR, offering a practical, scalable solution for mobile and\npervasive computing environments.",
    "updated" : "2025-10-30T05:08:25Z",
    "published" : "2025-10-30T05:08:25Z",
    "authors" : [
      {
        "name" : "Kexing Liu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.26102v1",
    "title" : "PEEL: A Poisoning-Exposing Encoding Theoretical Framework for Local\n  Differential Privacy",
    "summary" : "Local Differential Privacy (LDP) is a widely adopted privacy-protection model\nin the Internet of Things (IoT) due to its lightweight, decentralized, and\nscalable nature. However, it is vulnerable to poisoning attacks, and existing\ndefenses either incur prohibitive resource overheads or rely on domain-specific\nprior knowledge, limiting their practical deployment. To address these\nlimitations, we propose PEEL, a Poisoning-Exposing Encoding theoretical\nframework for LDP, which departs from resource- or prior-dependent\ncountermeasures and instead leverages the inherent structural consistency of\nLDP-perturbed data. As a non-intrusive post-processing module, PEEL amplifies\nstealthy poisoning effects by re-encoding LDP-perturbed data via\nsparsification, normalization, and low-rank projection, thereby revealing both\noutput and rule poisoning attacks through structural inconsistencies in the\nreconstructed space. Theoretical analysis proves that PEEL, integrated with\nLDP, retains unbiasedness and statistical accuracy, while being robust to\nexpose both output and rule poisoning attacks. Moreover, evaluation results\nshow that LDP-integrated PEEL not only outperforms four state-of-the-art\ndefenses in terms of poisoning exposure accuracy but also significantly reduces\nclient-side computational costs, making it highly suitable for large-scale IoT\ndeployments.",
    "updated" : "2025-10-30T03:29:02Z",
    "published" : "2025-10-30T03:29:02Z",
    "authors" : [
      {
        "name" : "Lisha Shuai"
      },
      {
        "name" : "Jiuling Dong"
      },
      {
        "name" : "Nan Zhang"
      },
      {
        "name" : "Shaofeng Tan"
      },
      {
        "name" : "Haokun Zhang"
      },
      {
        "name" : "Zilong Song"
      },
      {
        "name" : "Gaoya Dong"
      },
      {
        "name" : "Xiaolong Yang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.25932v1",
    "title" : "FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for\n  Facebook and X",
    "summary" : "Social platforms distribute information at unprecedented speed, which in turn\naccelerates the spread of misinformation and threatens public discourse. We\npresent FakeZero, a fully client-side, cross-platform browser extension that\nflags unreliable posts on Facebook and X (formerly Twitter) while the user\nscrolls. All computation, DOM scraping, tokenisation, Transformer inference,\nand UI rendering run locally through the Chromium messaging API, so no personal\ndata leaves the device.FakeZero employs a three-stage training curriculum:\nbaseline fine-tuning and domain-adaptive training enhanced with focal loss,\nadversarial augmentation, and post-training quantisation. Evaluated on a\ndataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1%\nmacro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of\napproximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant\nvariant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to\n14.7 MB and lowering latency to approximately 40 ms, showing that high-quality\nfake-news detection is feasible under tight resource budgets with only modest\nperformance loss.By providing inline credibility cues, the extension can serve\nas a valuable tool for policymakers seeking to curb the spread of\nmisinformation across social networks. With user consent, FakeZero also opens\nthe door for researchers to collect large-scale datasets of fake news in the\nwild, enabling deeper analysis and the development of more robust detection\ntechniques.",
    "updated" : "2025-10-29T20:11:48Z",
    "published" : "2025-10-29T20:11:48Z",
    "authors" : [
      {
        "name" : "Soufiane Essahli"
      },
      {
        "name" : "Oussama Sarsar"
      },
      {
        "name" : "Imane Fouad"
      },
      {
        "name" : "Anas Motii"
      },
      {
        "name" : "Ahmed Bentajer"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.27275v1",
    "title" : "Prevalence of Security and Privacy Risk-Inducing Usage of AI-based\n  Conversational Agents",
    "summary" : "Recent improvement gains in large language models (LLMs) have lead to\neveryday usage of AI-based Conversational Agents (CAs). At the same time, LLMs\nare vulnerable to an array of threats, including jailbreaks and, for example,\ncausing remote code execution when fed specific inputs. As a result, users may\nunintentionally introduce risks, for example, by uploading malicious files or\ndisclosing sensitive information. However, the extent to which such user\nbehaviors occur and thus potentially facilitate exploits remains largely\nunclear. To shed light on this issue, we surveyed a representative sample of\n3,270 UK adults in 2024 using Prolific. A third of these use CA services such\nas ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a\nthird exhibited behaviors that may enable attacks, and a fourth have tried\njailbreaking (often out of understandable reasons such as curiosity, fun or\ninformation seeking). Half state that they sanitize data and most participants\nreport not sharing sensitive data. However, few share very sensitive data such\nas passwords. The majority are unaware that their data can be used to train\nmodels and that they can opt-out. Our findings suggest that current academic\nthreat models manifest in the wild, and mitigations or guidelines for the\nsecure usage of CAs should be developed. In areas critical to security and\nprivacy, CAs must be equipped with effective AI guardrails to prevent, for\nexample, revealing sensitive information to curious employees. Vendors need to\nincrease efforts to prevent the entry of sensitive data, and to create\ntransparency with regard to data usage policies and settings.",
    "updated" : "2025-10-31T08:35:42Z",
    "published" : "2025-10-31T08:35:42Z",
    "authors" : [
      {
        "name" : "Kathrin Grosse"
      },
      {
        "name" : "Nico Ebert"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.27213v1",
    "title" : "Privacy-Aware Continual Self-Supervised Learning on Multi-Window Chest\n  Computed Tomography for Domain-Shift Robustness",
    "summary" : "We propose a novel continual self-supervised learning (CSSL) framework for\nsimultaneously learning diverse features from multi-window-obtained chest\ncomputed tomography (CT) images and ensuring data privacy. Achieving a robust\nand highly generalizable model in medical image diagnosis is challenging,\nmainly because of issues, such as the scarcity of large-scale, accurately\nannotated datasets and domain shifts inherent to dynamic healthcare\nenvironments. Specifically, in chest CT, these domain shifts often arise from\ndifferences in window settings, which are optimized for distinct clinical\npurposes. Previous CSSL frameworks often mitigated domain shift by reusing past\ndata, a typically impractical approach owing to privacy constraints. Our\napproach addresses these challenges by effectively capturing the relationship\nbetween previously learned knowledge and new information across different\ntraining stages through continual pretraining on unlabeled images.\nSpecifically, by incorporating a latent replay-based mechanism into CSSL, our\nmethod mitigates catastrophic forgetting due to domain shifts during continual\npretraining while ensuring data privacy. Additionally, we introduce a feature\ndistillation technique that integrates Wasserstein distance-based knowledge\ndistillation (WKD) and batch-knowledge ensemble (BKE), enhancing the ability of\nthe model to learn meaningful, domain-shift-robust representations. Finally, we\nvalidate our approach using chest CT images obtained across two different\nwindow settings, demonstrating superior performance compared with other\napproaches.",
    "updated" : "2025-10-31T06:16:31Z",
    "published" : "2025-10-31T06:16:31Z",
    "authors" : [
      {
        "name" : "Ren Tasai"
      },
      {
        "name" : "Guang Li"
      },
      {
        "name" : "Ren Togo"
      },
      {
        "name" : "Takahiro Ogawa"
      },
      {
        "name" : "Kenji Hirata"
      },
      {
        "name" : "Minghui Tang"
      },
      {
        "name" : "Takaaki Yoshimura"
      },
      {
        "name" : "Hiroyuki Sugimori"
      },
      {
        "name" : "Noriko Nishioka"
      },
      {
        "name" : "Yukie Shimizu"
      },
      {
        "name" : "Kohsuke Kudo"
      },
      {
        "name" : "Miki Haseyama"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.27016v1",
    "title" : "Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI\n  Services",
    "summary" : "With the increasing use of conversational AI systems, there is growing\nconcern over privacy leaks, especially when users share sensitive personal data\nin interactions with Large Language Models (LLMs). Conversations shared with\nthese models may contain Personally Identifiable Information (PII), which, if\nexposed, could lead to security breaches or identity theft. To address this\nchallenge, we present the Local Optimizations for Pseudonymization with\nSemantic Integrity Directed Entity Detection (LOPSIDED) framework, a\nsemantically-aware privacy agent designed to safeguard sensitive PII data when\nusing remote LLMs. Unlike prior work that often degrade response quality, our\napproach dynamically replaces sensitive PII entities in user prompts with\nsemantically consistent pseudonyms, preserving the contextual integrity of\nconversations. Once the model generates its response, the pseudonyms are\nautomatically depseudonymized, ensuring the user receives an accurate,\nprivacy-preserving output. We evaluate our approach using real-world\nconversations sourced from ShareGPT, which we further augment and annotate to\nassess whether named entities are contextually relevant to the model's\nresponse. Our results show that LOPSIDED reduces semantic utility errors by a\nfactor of 5 compared to baseline techniques, all while enhancing privacy.",
    "updated" : "2025-10-30T21:34:23Z",
    "published" : "2025-10-30T21:34:23Z",
    "authors" : [
      {
        "name" : "Jayden Serenari"
      },
      {
        "name" : "Stephen Lee"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.26841v1",
    "title" : "Accurate Target Privacy Preserving Federated Learning Balancing Fairness\n  and Utility",
    "summary" : "Federated Learning (FL) enables collaborative model training without data\nsharing, yet participants face a fundamental challenge, e.g., simultaneously\nensuring fairness across demographic groups while protecting sensitive client\ndata. We introduce a differentially private fair FL algorithm (\\textit{FedPF})\nthat transforms this multi-objective optimization into a zero-sum game where\nfairness and privacy constraints compete against model utility. Our theoretical\nanalysis reveals a surprising inverse relationship, i.e., stricter privacy\nprotection fundamentally limits the system's ability to detect and correct\ndemographic biases, creating an inherent tension between privacy and fairness.\nCounterintuitively, we prove that moderate fairness constraints initially\nimprove model generalization before causing performance degradation, where a\nnon-monotonic relationship that challenges conventional wisdom about\nfairness-utility tradeoffs. Experimental validation demonstrates up to 42.9 %\ndiscrimination reduction across three datasets while maintaining competitive\naccuracy, but more importantly, reveals that the privacy-fairness tension is\nunavoidable, i.e., achieving both objectives simultaneously requires carefully\nbalanced compromises rather than optimization of either in isolation. The\nsource code for our proposed algorithm is publicly accessible at\nhttps://github.com/szpsunkk/FedPF.",
    "updated" : "2025-10-30T07:14:55Z",
    "published" : "2025-10-30T07:14:55Z",
    "authors" : [
      {
        "name" : "Kangkang Sun"
      },
      {
        "name" : "Jun Wu"
      },
      {
        "name" : "Minyi Guo"
      },
      {
        "name" : "Jianhua Li"
      },
      {
        "name" : "Jianwei Huang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "F.2.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.21591v2",
    "title" : "Privacy by Design: Aligning GDPR and Software Engineering Specifications\n  with a Requirements Engineering Approach",
    "summary" : "Context: Consistent requirements and system specifications are essential for\nthe compliance of software systems towards the General Data Protection\nRegulation (GDPR). Both artefacts need to be grounded in the original text and\nconjointly assure the achievement of privacy by design (PbD). Objectives: There\nis little understanding of the perspectives of practitioners on specification\nobjectives and goals to address PbD. Existing approaches do not account for the\ncomplex intersection between problem and solution space expressed in GDPR. In\nthis study we explore the demand for conjoint requirements and system\nspecification for PbD and suggest an approach to address this demand. Methods:\nWe reviewed secondary and related primary studies and conducted interviews with\npractitioners to (1) investigate the state-of-practice and (2) understand the\nunderlying specification objectives and goals (e.g., traceability). We\ndeveloped and evaluated an approach for requirements and systems specification\nfor PbD, and evaluated it against the specification objectives. Results: The\nrelationship between problem and solution space, as expressed in GDPR, is\ninstrumental in supporting PbD. We demonstrate how our approach, based on the\nmodeling GDPR content with original legal concepts, contributes to\nspecification objectives of capturing legal knowledge, supporting specification\ntransparency, and traceability. Conclusion: GDPR demands need to be addressed\nthroughout different levels of abstraction in the engineering lifecycle to\nachieve PbD. Legal knowledge specified in the GDPR text should be captured in\nspecifications to address the demands of different stakeholders and ensure\ncompliance. While our results confirm the suitability of our approach to\naddress practical needs, we also revealed specific needs for the future\neffective operationalization of the approach.",
    "updated" : "2025-10-31T10:51:22Z",
    "published" : "2025-10-24T15:59:34Z",
    "authors" : [
      {
        "name" : "Oleksandr Kosenkov"
      },
      {
        "name" : "Ehsan Zabardast"
      },
      {
        "name" : "Davide Fucci"
      },
      {
        "name" : "Daniel Mendez"
      },
      {
        "name" : "Michael Unterkalmsteiner"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.19537v2",
    "title" : "Privacy-Preserving Spiking Neural Networks: A Deep Dive into Encryption\n  Parameter Optimisation",
    "summary" : "Deep learning is widely applied to modern problems through neural networks,\nbut the growing computational and energy demands of these models have driven\ninterest in more efficient approaches. Spiking Neural Networks (SNNs), the\nthird generation of neural networks, mimic the brain's event-driven behaviour,\noffering improved performance and reduced power use. At the same time, concerns\nabout data privacy during cloud-based model execution have led to the adoption\nof cryptographic methods. This article introduces BioEncryptSNN, a spiking\nneural network based encryption-decryption framework for secure and\nnoise-resilient data protection. Unlike conventional algorithms, BioEncryptSNN\nconverts ciphertext into spike trains and exploits temporal neural dynamics to\nmodel encryption and decryption, optimising parameters such as key length,\nspike timing, and synaptic connectivity. Benchmarked against AES-128, RSA-2048,\nand DES, BioEncryptSNN preserved data integrity while achieving up to 4.1x\nfaster encryption and decryption than PyCryptodome's AES implementation. The\nframework demonstrates scalability and adaptability across symmetric and\nasymmetric ciphers, positioning SNNs as a promising direction for secure,\nenergy-efficient computing.",
    "updated" : "2025-11-02T09:47:24Z",
    "published" : "2025-10-22T12:43:46Z",
    "authors" : [
      {
        "name" : "Mahitha Pulivathi"
      },
      {
        "name" : "Ana Fontes Rodrigues"
      },
      {
        "name" : "Isibor Kennedy Ihianle"
      },
      {
        "name" : "Andreas Oikonomou"
      },
      {
        "name" : "Srinivas Boppu"
      },
      {
        "name" : "Pedro Machado"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  }
]