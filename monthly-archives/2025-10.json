[
  {
    "id" : "http://arxiv.org/abs/2510.01793v1",
    "title" : "Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of\n  Privacy Filters for Synthetic Data Generation",
    "summary" : "The generation of privacy-preserving synthetic datasets is a promising avenue\nfor overcoming data scarcity in medical AI research. Post-hoc privacy filtering\ntechniques, designed to remove samples containing personally identifiable\ninformation, have recently been proposed as a solution. However, their\neffectiveness remains largely unverified. This work presents a rigorous\nevaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary\nto claims from the original publications, our results demonstrate that current\nfilters exhibit limited specificity and consistency, achieving high sensitivity\nonly for real images while failing to reliably detect near-duplicates generated\nfrom training data. These results demonstrate a critical limitation of post-hoc\nfiltering: rather than effectively safeguarding patient privacy, these methods\nmay provide a false sense of security while leaving unacceptable levels of\npatient information exposed. We conclude that substantial advances in filter\ndesign are needed before these methods can be confidently deployed in sensitive\napplications.",
    "updated" : "2025-10-02T08:32:20Z",
    "published" : "2025-10-02T08:32:20Z",
    "authors" : [
      {
        "name" : "Adil Koeken"
      },
      {
        "name" : "Alexander Ziller"
      },
      {
        "name" : "Moritz Knolle"
      },
      {
        "name" : "Daniel Rueckert"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.01645v1",
    "title" : "Position: Privacy Is Not Just Memorization!",
    "summary" : "The discourse on privacy risks in Large Language Models (LLMs) has\ndisproportionately focused on verbatim memorization of training data, while a\nconstellation of more immediate and scalable privacy threats remain\nunderexplored. This position paper argues that the privacy landscape of LLM\nsystems extends far beyond training data extraction, encompassing risks from\ndata collection practices, inference-time context leakage, autonomous agent\ncapabilities, and the democratization of surveillance through deep inference\nattacks. We present a comprehensive taxonomy of privacy risks across the LLM\nlifecycle -- from data collection through deployment -- and demonstrate through\ncase studies how current privacy frameworks fail to address these multifaceted\nthreats. Through a longitudinal analysis of 1,322 AI/ML privacy papers\npublished at leading conferences over the past decade (2016--2025), we reveal\nthat while memorization receives outsized attention in technical research, the\nmost pressing privacy harms lie elsewhere, where current technical approaches\noffer little traction and viable paths forward remain unclear. We call for a\nfundamental shift in how the research community approaches LLM privacy, moving\nbeyond the narrow focus of current technical solutions and embracing\ninterdisciplinary approaches that address the sociotechnical nature of these\nemerging threats.",
    "updated" : "2025-10-02T04:02:06Z",
    "published" : "2025-10-02T04:02:06Z",
    "authors" : [
      {
        "name" : "Niloofar Mireshghallah"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.01113v1",
    "title" : "Privacy Preserved Federated Learning with Attention-Based Aggregation\n  for Biometric Recognition",
    "summary" : "Because biometric data is sensitive, centralized training poses a privacy\nrisk, even though biometric recognition is essential for contemporary\napplications. Federated learning (FL), which permits decentralized training,\nprovides a privacy-preserving substitute. Conventional FL, however, has trouble\nwith interpretability and heterogeneous data (non-IID). In order to handle\nnon-IID biometric data, this framework adds an attention mechanism at the\ncentral server that weights local model updates according to their\nsignificance. Differential privacy and secure update protocols safeguard data\nwhile preserving accuracy. The A3-FL framework is evaluated in this study using\nFVC2004 fingerprint data, with each client's features extracted using a Siamese\nConvolutional Neural Network (Siamese-CNN). By dynamically modifying client\ncontributions, the attention mechanism increases the accuracy of the global\nmodel.The accuracy, convergence speed, and robustness of the A3-FL framework\nare superior to those of standard FL (FedAvg) and static baselines, according\nto experimental evaluations using fingerprint data (FVC2004). The accuracy of\nthe attention-based approach was 0.8413, while FedAvg, Local-only, and\nCentralized approaches were 0.8164, 0.7664, and 0.7997, respectively. Accuracy\nstayed high at 0.8330 even with differential privacy. A scalable and\nprivacy-sensitive biometric system for secure and effective recognition in\ndispersed environments is presented in this work.",
    "updated" : "2025-10-01T16:58:59Z",
    "published" : "2025-10-01T16:58:59Z",
    "authors" : [
      {
        "name" : "Kassahun Azezew"
      },
      {
        "name" : "Minyechil Alehegn"
      },
      {
        "name" : "Tsega Asresa"
      },
      {
        "name" : "Bitew Mekuria"
      },
      {
        "name" : "Tizazu Bayh"
      },
      {
        "name" : "Ayenew Kassie"
      },
      {
        "name" : "Amsalu Tesema"
      },
      {
        "name" : "Animut Embiyale"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.00909v1",
    "title" : "\"We are not Future-ready\": Understanding AI Privacy Risks and Existing\n  Mitigation Strategies from the Perspective of AI Developers in Europe",
    "summary" : "The proliferation of AI has sparked privacy concerns related to training\ndata, model interfaces, downstream applications, and more. We interviewed 25 AI\ndevelopers based in Europe to understand which privacy threats they believe\npose the greatest risk to users, developers, and businesses and what protective\nstrategies, if any, would help to mitigate them. We find that there is little\nconsensus among AI developers on the relative ranking of privacy risks. These\ndifferences stem from salient reasoning patterns that often relate to human\nrather than purely technical factors. Furthermore, while AI developers are\naware of proposed mitigation strategies for addressing these risks, they\nreported minimal real-world adoption. Our findings highlight both gaps and\nopportunities for empowering AI developers to better address privacy risks in\nAI.",
    "updated" : "2025-10-01T13:51:33Z",
    "published" : "2025-10-01T13:51:33Z",
    "authors" : [
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Patrick Gage Kelley"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Kurt Thomas"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.00478v1",
    "title" : "Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving\n  Domain Adaptation",
    "summary" : "Recent work on latent diffusion models (LDMs) has focused almost exclusively\non generative tasks, leaving their potential for discriminative transfer\nlargely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a\nnovel LDM-based framework for a more practical variant of source-free domain\nadaptation (SFDA): the source provider may share not only a pre-trained\nclassifier but also an auxiliary latent diffusion module, trained once on the\nsource data and never exposing raw source samples. DVD encodes each source\nfeature's label information into its latent vicinity by fitting a Gaussian\nprior over its k-nearest neighbors and training the diffusion network to drift\nnoisy samples back to label-consistent representations. During adaptation, we\nsample from each target feature's latent vicinity, apply the frozen diffusion\nmodule to generate source-like cues, and use a simple InfoNCE loss to align the\ntarget encoder to these cues, explicitly transferring decision boundaries\nwithout source access. Across standard SFDA benchmarks, DVD outperforms\nstate-of-the-art methods. We further show that the same latent diffusion module\nenhances the source classifier's accuracy on in-domain data and boosts\nperformance in supervised classification and domain generalization experiments.\nDVD thus reinterprets LDMs as practical, privacy-preserving bridges for\nexplicit knowledge transfer, addressing a core challenge in source-free domain\nadaptation that prior methods have yet to solve.",
    "updated" : "2025-10-01T03:58:26Z",
    "published" : "2025-10-01T03:58:26Z",
    "authors" : [
      {
        "name" : "Jing Wang"
      },
      {
        "name" : "Wonho Bae"
      },
      {
        "name" : "Jiahong Chen"
      },
      {
        "name" : "Wenxu Wang"
      },
      {
        "name" : "Junhyug Noh"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03035v1",
    "title" : "Protecting Persona Biometric Data: The Case of Facial Privacy",
    "summary" : "The proliferation of digital technologies has led to unprecedented data\ncollection, with facial data emerging as a particularly sensitive commodity.\nCompanies are increasingly leveraging advanced facial recognition technologies,\noften without the explicit consent or awareness of individuals, to build\nsophisticated surveillance capabilities. This practice, fueled by weak and\nfragmented laws in many jurisdictions, has created a regulatory vacuum that\nallows for the commercialization of personal identity and poses significant\nthreats to individual privacy and autonomy. This article introduces the concept\nof Facial Privacy. It analyzes the profound challenges posed by unregulated\nfacial recognition by conducting a comprehensive review of existing legal\nframeworks. It examines and compares regulations such as the GDPR, Brazil's\nLGPD, Canada's PIPEDA, and privacy acts in China, Singapore, South Korea, and\nJapan, alongside sector-specific laws in the United States like the Illinois\nBiometric Information Privacy Act (BIPA). The analysis highlights the societal\nimpacts of this technology, including the potential for discriminatory bias and\nthe long-lasting harm that can result from the theft of immutable biometric\ndata. Ultimately, the paper argues that existing legal loopholes and\nambiguities leave individuals vulnerable. It proposes a new policy framework\nthat shifts the paradigm from data as property to a model of inalienable\nrights, ensuring that fundamental human rights are upheld against unchecked\ntechnological expansion.",
    "updated" : "2025-10-03T14:16:33Z",
    "published" : "2025-10-03T14:16:33Z",
    "authors" : [
      {
        "name" : "Lambert Hogenhout"
      },
      {
        "name" : "Rinzin Wangmo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.02487v1",
    "title" : "Interplay between Security, Privacy and Trust in 6G-enabled Intelligent\n  Transportation Systems",
    "summary" : "The advancement of 6G technology has the potential to revolutionize the\ntransportation sector and significantly improve how we travel. 6G-enabled\nIntelligent Transportation Systems (ITS) promise to offer high-speed,\nlow-latency communication and advanced data analytics capabilities, supporting\nthe development of safer, more efficient, and more sustainable transportation\nsolutions. However, various security and privacy challenges were identified in\nthe literature that must be addressed to enable the safe and secure deployment\nof 6G-ITS and ensure people's trust in using these technologies. This paper\nreviews the opportunities and challenges of 6G-ITS, particularly focusing on\ntrust, security, and privacy, with special attention to quantum technologies\nthat both enhance security through quantum key distribution and introduce new\nvulnerabilities. It discusses the potential benefits of 6G technology in the\ntransportation sector, including improved communication, device\ninteroperability support, data analytic capabilities, and increased automation\nfor different components, such as transportation management and communication\nsystems. A taxonomy of different attack models in 6G-ITS is proposed, and a\ncomparison of the security threats in 5G-ITS and 6G-ITS is provided, along with\npotential mitigating solutions. This research highlights the urgent need for a\ncomprehensive, multi-layered security framework spanning physical\ninfrastructure protection, network protocol security, data management\nsafeguards, application security measures, and trust management systems to\neffectively mitigate emerging security and privacy risks and ensure the\nintegrity and resilience of future transportation ecosystems.",
    "updated" : "2025-10-02T18:47:36Z",
    "published" : "2025-10-02T18:47:36Z",
    "authors" : [
      {
        "name" : "Ahmed Danladi Abdullahi"
      },
      {
        "name" : "Erfan Bahrami"
      },
      {
        "name" : "Tooska Dargahi"
      },
      {
        "name" : "Mohammed Al-Khalidi"
      },
      {
        "name" : "Mohammad Hammoudeh"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05068v1",
    "title" : "Multi-Agent Distributed Optimization With Feasible Set Privacy",
    "summary" : "We consider the problem of decentralized constrained optimization with\nmultiple agents $E_1,\\ldots,E_N$ who jointly wish to learn the optimal solution\nset while keeping their feasible sets $\\mathcal{P}_1,\\ldots,\\mathcal{P}_N$\nprivate from each other. We assume that the objective function $f$ is known to\nall agents and each feasible set is a collection of points from a universal\nalphabet $\\mathcal{P}_{alph}$. A designated agent (leader) starts the\ncommunication with the remaining (non-leader) agents, and is the first to\nretrieve the solution set. The leader searches for the solution by sending\nqueries to and receiving answers from the non-leaders, such that the\ninformation on the individual feasible sets revealed to the leader should be no\nmore than nominal, i.e., what is revealed from learning the solution set alone.\nWe develop achievable schemes for obtaining the solution set at nominal\ninformation leakage, and characterize their communication costs under two\ncommunication setups between agents. In this work, we focus on two kinds of\nnetwork setups: i) ring, where each agent communicates with two adjacent\nagents, and ii) star, where only the leader communicates with the remaining\nagents. We show that, if the leader first learns the joint feasible set through\nan existing private set intersection (PSI) protocol and then deduces the\nsolution set, the information leaked to the leader is greater than nominal.\nMoreover, we draw connection of our schemes to threshold PSI (ThPSI), which is\na PSI-variant where the intersection is revealed only when its cardinality is\nlarger than a threshold value. Finally, for various realizations of $f$ mapped\nuniformly at random to a fixed range of values, our schemes are more\ncommunication-efficient with a high probability compared to retrieving the\nentire feasible set through PSI.",
    "updated" : "2025-10-06T17:45:57Z",
    "published" : "2025-10-06T17:45:57Z",
    "authors" : [
      {
        "name" : "Shreya Meel"
      },
      {
        "name" : "Sennur Ulukus"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.DC",
      "cs.NI",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04527v1",
    "title" : "Quantum capacity amplification via privacy",
    "summary" : "We investigate superadditivity of quantum capacity through private channels\nwhose Choi-Jamiolkowski operators are private states. This perspective links\nthe security structure of private states to quantum capacity and clarifies the\nrole of the shield system: information encoded in the shield system that would\notherwise leak to the environment can be recycled when paired with an assisting\nchannel, thereby boosting capacity. Our main contributions are threefold:\nFirstly, we develop a general framework that provides a sufficient condition\nfor capacity amplification, which is formulated in terms of the assisting\nchannel's Holevo information. As examples, we give explicit, dimension and\nparameter dependent amplification thresholds for erasure and depolarizing\nchannels. Secondly, assuming the Spin alignment conjecture, we derive a\nsingle-letter expression for the quantum capacity of a family of private\nchannels that are neither degradable, anti-degradable, nor PPT; as an\napplication, we construct channels with vanishing quantum capacity yet\nunbounded private capacity. Thirdly, we further analyze approximate private\nchannels: we give an alternative proof of superactivation that extends its\nvalidity to a broader parameter regime, and, by combining amplification bounds\nwith continuity estimates, we establish a metric separation showing that\nchannels exhibiting capacity amplification have nonzero diamond distance from\nthe set of anti-degradable channels, indicating that existing approximate\n(anti-)degradability bounds are not tight. We also revisit the computability of\nthe regularized quantum capacity and modestly suggest that this fundamental\nquestion still remains open.",
    "updated" : "2025-10-06T06:35:19Z",
    "published" : "2025-10-06T06:35:19Z",
    "authors" : [
      {
        "name" : "Peixue Wu"
      },
      {
        "name" : "Yunkai Wang"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT",
      "math-ph",
      "math.IT",
      "math.MP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04465v1",
    "title" : "Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM\n  Agents",
    "summary" : "Large Language Model (LLM) agents require personal information for\npersonalization in order to better act on users' behalf in daily tasks, but\nthis raises privacy concerns and a personalization-privacy dilemma. Agent's\nautonomy introduces both risks and opportunities, yet its effects remain\nunclear. To better understand this, we conducted a 3$\\times$3 between-subjects\nexperiment ($N=450$) to study how agent's autonomy level and personalization\ninfluence users' privacy concerns, trust and willingness to use, as well as the\nunderlying psychological processes. We find that personalization without\nconsidering users' privacy preferences increases privacy concerns and decreases\ntrust and willingness to use. Autonomy moderates these effects: Intermediate\nautonomy flattens the impact of personalization compared to No- and Full\nautonomy conditions. Our results suggest that rather than aiming for perfect\nmodel alignment in output generation, balancing autonomy of agent's action and\nuser control offers a promising path to mitigate the personalization-privacy\ndilemma.",
    "updated" : "2025-10-06T03:38:54Z",
    "published" : "2025-10-06T03:38:54Z",
    "authors" : [
      {
        "name" : "Zhiping Zhang"
      },
      {
        "name" : "Yi Evie Zhang"
      },
      {
        "name" : "Freda Shi"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04261v1",
    "title" : "VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient\n  Extraction of User Privacy",
    "summary" : "Large language models (LLMs) have been widely deployed in Conversational AIs\n(CAIs), while exposing privacy and security threats. Recent research shows that\nLLM-based CAIs can be manipulated to extract private information from human\nusers, posing serious security threats. However, the methods proposed in that\nstudy rely on a white-box setting that adversaries can directly modify the\nsystem prompt. This condition is unlikely to hold in real-world deployments.\nThe limitation raises a critical question: can unprivileged attackers still\ninduce such privacy risks in practical LLM-integrated applications? To address\nthis question, we propose \\textsc{VortexPIA}, a novel indirect prompt injection\nattack that induces privacy extraction in LLM-integrated applications under\nblack-box settings. By injecting token-efficient data containing false\nmemories, \\textsc{VortexPIA} misleads LLMs to actively request private\ninformation in batches. Unlike prior methods, \\textsc{VortexPIA} allows\nattackers to flexibly define multiple categories of sensitive data. We evaluate\n\\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,\nacross four benchmark datasets. The results show that \\textsc{VortexPIA}\nsignificantly outperforms baselines and achieves state-of-the-art (SOTA)\nperformance. It also demonstrates efficient privacy requests, reduced token\nconsumption, and enhanced robustness against defense mechanisms. We further\nvalidate \\textsc{VortexPIA} on multiple realistic open-source LLM-integrated\napplications, demonstrating its practical effectiveness.",
    "updated" : "2025-10-05T15:58:55Z",
    "published" : "2025-10-05T15:58:55Z",
    "authors" : [
      {
        "name" : "Yu Cui"
      },
      {
        "name" : "Sicheng Pan"
      },
      {
        "name" : "Yifei Liu"
      },
      {
        "name" : "Haibin Zhang"
      },
      {
        "name" : "Cong Zuo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04153v1",
    "title" : "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation",
    "summary" : "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost.",
    "updated" : "2025-10-05T11:09:10Z",
    "published" : "2025-10-05T11:09:10Z",
    "authors" : [
      {
        "name" : "Haoqi Wu"
      },
      {
        "name" : "Wei Dai"
      },
      {
        "name" : "Ming Xu"
      },
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Qiang Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.04027v1",
    "title" : "Multi-Class Support Vector Machine with Differential Privacy",
    "summary" : "With the increasing need to safeguard data privacy in machine learning\nmodels, differential privacy (DP) is one of the major frameworks to build\nprivacy-preserving models. Support Vector Machines (SVMs) are widely used\ntraditional machine learning models due to their robust margin guarantees and\nstrong empirical performance in binary classification. However, applying DP to\nmulti-class SVMs is inadequate, as the standard one-versus-rest (OvR) and\none-versus-one (OvO) approaches repeatedly query each data sample when building\nmultiple binary classifiers, thus consuming the privacy budget proportionally\nto the number of classes. To overcome this limitation, we explore all-in-one\nSVM approaches for DP, which access each data sample only once to construct\nmulti-class SVM boundaries with margin maximization properties. We propose a\nnovel differentially Private Multi-class SVM (PMSVM) with weight and gradient\nperturbation methods, providing rigorous sensitivity and convergence analyses\nto ensure DP in all-in-one SVMs. Empirical results demonstrate that our\napproach surpasses existing DP-SVM methods in multi-class scenarios.",
    "updated" : "2025-10-05T04:25:16Z",
    "published" : "2025-10-05T04:25:16Z",
    "authors" : [
      {
        "name" : "Jinseong Park"
      },
      {
        "name" : "Yujin Choi"
      },
      {
        "name" : "Jaewook Lee"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03996v1",
    "title" : "FHEON: A Configurable Framework for Developing Privacy-Preserving Neural\n  Networks Using Homomorphic Encryption",
    "summary" : "The widespread adoption of Machine Learning as a Service raises critical\nprivacy and security concerns, particularly about data confidentiality and\ntrust in both cloud providers and the machine learning models. Homomorphic\nEncryption (HE) has emerged as a promising solution to this problems, allowing\ncomputations on encrypted data without decryption. Despite its potential,\nexisting approaches to integrate HE into neural networks are often limited to\nspecific architectures, leaving a wide gap in providing a framework for easy\ndevelopment of HE-friendly privacy-preserving neural network models similar to\nwhat we have in the broader field of machine learning. In this paper, we\npresent FHEON, a configurable framework for developing privacy-preserving\nconvolutional neural network (CNN) models for inference using HE. FHEON\nintroduces optimized and configurable implementations of privacy-preserving CNN\nlayers including convolutional layers, average pooling layers, ReLU activation\nfunctions, and fully connected layers. These layers are configured using\nparameters like input channels, output channels, kernel size, stride, and\npadding to support arbitrary CNN architectures. We assess the performance of\nFHEON using several CNN architectures, including LeNet-5, VGG-11, VGG- 16,\nResNet-20, and ResNet-34. FHEON maintains encrypted-domain accuracies within\n+/- 1% of their plaintext counterparts for ResNet-20 and LeNet-5 models.\nNotably, on a consumer-grade CPU, the models build on FHEON achieved 98.5%\naccuracy with a latency of 13 seconds on MNIST using LeNet-5, and 92.2%\naccuracy with a latency of 403 seconds on CIFAR-10 using ResNet-20.\nAdditionally, FHEON operates within a practical memory budget requiring not\nmore than 42.3 GB for VGG-16.",
    "updated" : "2025-10-05T02:12:44Z",
    "published" : "2025-10-05T02:12:44Z",
    "authors" : [
      {
        "name" : "Nges Brian Njungle"
      },
      {
        "name" : "Eric Jahns"
      },
      {
        "name" : "Michel A. Kinsy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03860v1",
    "title" : "Privacy Enhancement in Over-the-Air Federated Learning via Adaptive\n  Receive Scaling",
    "summary" : "In Federated Learning (FL) with over-the-air aggregation, the quality of the\nsignal received at the server critically depends on the receive scaling\nfactors. While a larger scaling factor can reduce the effective noise power and\nimprove training performance, it also compromises the privacy of devices by\nreducing uncertainty. In this work, we aim to adaptively design the receive\nscaling factors across training rounds to balance the trade-off between\ntraining convergence and privacy in an FL system under dynamic channel\nconditions. We formulate a stochastic optimization problem that minimizes the\noverall R\\'enyi differential privacy (RDP) leakage over the entire training\nprocess, subject to a long-term constraint that ensures convergence of the\nglobal loss function. Our problem depends on unknown future information, and we\nobserve that standard Lyapunov optimization is not applicable. Thus, we develop\na new online algorithm, termed AdaScale, based on a sequence of novel per-round\nproblems that can be solved efficiently. We further derive upper bounds on the\ndynamic regret and constraint violation of AdaSacle, establishing that it\nachieves diminishing dynamic regret in terms of time-averaged RDP leakage while\nensuring convergence of FL training to a stationary point. Numerical\nexperiments on canonical classification tasks show that our approach\neffectively reduces RDP and DP leakages compared with state-of-the-art\nbenchmarks without compromising learning performance.",
    "updated" : "2025-10-04T16:15:19Z",
    "published" : "2025-10-04T16:15:19Z",
    "authors" : [
      {
        "name" : "Faeze Moradi Kalarde"
      },
      {
        "name" : "Ben Liang"
      },
      {
        "name" : "Min Dong"
      },
      {
        "name" : "Yahia A. Eldemerdash Ahmed"
      },
      {
        "name" : "Ho Ting Cheng"
      }
    ],
    "categories" : [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03662v1",
    "title" : "Operationalizing Data Minimization for Privacy-Preserving LLM Prompting",
    "summary" : "The rapid deployment of large language models (LLMs) in consumer applications\nhas led to frequent exchanges of personal information. To obtain useful\nresponses, users often share more than necessary, increasing privacy risks via\nmemorization, context-based personalization, or security breaches. We present a\nframework to formally define and operationalize data minimization: for a given\nuser prompt and response model, quantifying the least privacy-revealing\ndisclosure that maintains utility, and we propose a priority-queue tree search\nto locate this optimal point within a privacy-ordered transformation space. We\nevaluated the framework on four datasets spanning open-ended conversations\n(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth\nanswers (CaseHold, MedQA), quantifying achievable data minimization with nine\nLLMs as the response model. Our results demonstrate that larger frontier LLMs\ncan tolerate stronger data minimization while maintaining task quality than\nsmaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for\nQwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that\nLLMs struggle to predict optimal data minimization directly, showing a bias\ntoward abstraction that leads to oversharing. This suggests not just a privacy\ngap, but a capability gap: models may lack awareness of what information they\nactually need to solve a task.",
    "updated" : "2025-10-04T04:20:18Z",
    "published" : "2025-10-04T04:20:18Z",
    "authors" : [
      {
        "name" : "Jijie Zhou"
      },
      {
        "name" : "Niloofar Mireshghallah"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03559v1",
    "title" : "PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating\n  Privacy Reviews in UX Design",
    "summary" : "UX professionals routinely conduct design reviews, yet privacy concerns are\noften overlooked -- not only due to limited tools, but more critically because\nof low intrinsic motivation. Limited privacy knowledge, weak empathy for\nunexpectedly affected users, and low confidence in identifying harms make it\ndifficult to address risks. We present PrivacyMotiv, an LLM-powered system that\nsupports privacy-oriented design diagnosis by generating speculative personas\nwith UX user journeys centered on individuals vulnerable to privacy risks.\nDrawing on narrative strategies, the system constructs relatable and\nattention-drawing scenarios that show how ordinary design choices may cause\nunintended harms, expanding the scope of privacy reflection in UX. In a\nwithin-subjects study with professional UX practitioners (N=16), we compared\nparticipants' self-proposed methods with PrivacyMotiv across two privacy review\ntasks. Results show significant improvements in empathy, intrinsic motivation,\nand perceived usefulness. This work contributes a promising privacy review\napproach which addresses the motivational barriers in privacy-aware UX.",
    "updated" : "2025-10-03T23:14:22Z",
    "published" : "2025-10-03T23:14:22Z",
    "authors" : [
      {
        "name" : "Zeya Chen"
      },
      {
        "name" : "Jianing Wen"
      },
      {
        "name" : "Ruth Schmidt"
      },
      {
        "name" : "Yaxing Yao"
      },
      {
        "name" : "Toby Jia-Jun Li"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.03513v1",
    "title" : "A Lightweight Federated Learning Approach for Privacy-Preserving Botnet\n  Detection in IoT",
    "summary" : "The rapid growth of the Internet of Things (IoT) has expanded opportunities\nfor innovation but also increased exposure to botnet-driven cyberattacks.\nConventional detection methods often struggle with scalability, privacy, and\nadaptability in resource-constrained IoT environments. To address these\nchallenges, we present a lightweight and privacy-preserving botnet detection\nframework based on federated learning. This approach enables distributed\ndevices to collaboratively train models without exchanging raw data, thus\nmaintaining user privacy while preserving detection accuracy. A\ncommunication-efficient aggregation strategy is introduced to reduce overhead,\nensuring suitability for constrained IoT networks. Experiments on benchmark IoT\nbotnet datasets demonstrate that the framework achieves high detection accuracy\nwhile substantially reducing communication costs. These findings highlight\nfederated learning as a practical path toward scalable, secure, and\nprivacy-aware intrusion detection for IoT ecosystems.",
    "updated" : "2025-10-03T20:54:58Z",
    "published" : "2025-10-03T20:54:58Z",
    "authors" : [
      {
        "name" : "Taha M. Mahmoud"
      },
      {
        "name" : "Naima Kaabouch"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05959v1",
    "title" : "Distributed Platoon Control Under Quantization: Stability Analysis and\n  Privacy Preservation",
    "summary" : "Distributed control of connected and automated vehicles has attracted\nconsiderable interest for its potential to improve traffic efficiency and\nsafety. However, such control schemes require sharing privacy-sensitive vehicle\ndata, which introduces risks of information leakage and potential malicious\nactivities. This paper investigates the stability and privacy-preserving\nproperties of distributed platoon control under two types of quantizers:\ndeterministic and probabilistic. For deterministic quantization, we show that\nthe resulting control strategy ensures the system errors remain uniformly\nultimately bounded. Moreover, in the absence of auxiliary information, an\neavesdropper cannot uniquely infer sensitive vehicle states. In contrast, the\nuse of probabilistic quantization enables asymptotic convergence of the vehicle\nplatoon in expectation with bounded variance. Importantly, probabilistic\nquantizers can satisfy differential privacy guarantees, thereby preserving\nprivacy even when the eavesdropper possesses arbitrary auxiliary information.\nWe further analyze the trade-off between control performance and privacy by\nformulating an optimization problem that characterizes the impact of the\nquantization step on both metrics. Numerical simulations are provided to\nillustrate the performance differences between the two quantization strategies.",
    "updated" : "2025-10-07T14:16:59Z",
    "published" : "2025-10-07T14:16:59Z",
    "authors" : [
      {
        "name" : "Kaixiang Zhang"
      },
      {
        "name" : "Zhaojian Li"
      },
      {
        "name" : "Wei Lin"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05860v1",
    "title" : "Automated Boilerplate: Prevalence and Quality of Contract Generators in\n  the Context of Swiss Privacy Policies",
    "summary" : "It has become increasingly challenging for firms to comply with a plethora of\nnovel digital regulations. This is especially true for smaller businesses that\noften lack both the resources and know-how to draft complex legal documents.\nInstead of seeking costly legal advice from attorneys, firms may turn to\ncheaper alternative legal service providers such as automated contract\ngenerators. While these services have a long-standing presence, there is little\nempirical evidence on their prevalence and output quality.\n  We address this gap in the context of a 2023 Swiss privacy law revision. To\nenable a systematic evaluation, we create and annotate a multilingual benchmark\ndataset that captures key compliance obligations under Swiss and EU privacy\nlaw. Using this dataset, we validate a novel GPT-5-based method for large-scale\ncompliance assessment of privacy policies, allowing us to measure the impact of\nthe revision. We observe compliance increases indicating an effect of the\nrevision. Generators, explicitly referenced by 18% of local websites, are\nassociated with substantially higher levels of compliance, with increases of up\nto 15 percentage points compared to privacy policies without generator use.\nThese findings contribute to three debates: the potential of LLMs for\ncross-lingual legal analysis, the Brussels Effect of EU regulations, and,\ncrucially, the role of automated tools in improving compliance and contractual\nquality.",
    "updated" : "2025-10-07T12:30:01Z",
    "published" : "2025-10-07T12:30:01Z",
    "authors" : [
      {
        "name" : "Luka Nenadic"
      },
      {
        "name" : "David Rodriguez"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05807v1",
    "title" : "Privacy-Preserving On-chain Permissioning for KYC-Compliant\n  Decentralized Applications",
    "summary" : "Decentralized applications (dApps) in Decentralized Finance (DeFi) face a\nfundamental tension between regulatory compliance requirements like Know Your\nCustomer (KYC) and maintaining decentralization and privacy. Existing\npermissioned DeFi solutions often fail to adequately protect private attributes\nof dApp users and introduce implicit trust assumptions, undermining the\nblockchain's decentralization. Addressing these limitations, this paper\npresents a novel synthesis of Self-Sovereign Identity (SSI), Zero-Knowledge\nProofs (ZKPs), and Attribute-Based Access Control to enable privacy-preserving\non-chain permissioning based on decentralized policy decisions. We provide a\ncomprehensive framework for permissioned dApps that aligns decentralized trust,\nprivacy, and transparency, harmonizing blockchain principles with regulatory\ncompliance. Our framework supports multiple proof types (equality, range,\nmembership, and time-dependent) with efficient proof generation through a\ncommit-and-prove scheme that moves credential authenticity verification outside\nthe ZKP circuit. Experimental evaluation of our KYC-compliant DeFi\nimplementation shows considerable performance improvement for different proof\ntypes compared to baseline approaches. We advance the state-of-the-art through\na holistic approach, flexible proof mechanisms addressing diverse real-world\nrequirements, and optimized proof generation enabling practical deployment.",
    "updated" : "2025-10-07T11:24:51Z",
    "published" : "2025-10-07T11:24:51Z",
    "authors" : [
      {
        "name" : "Fabian Piper"
      },
      {
        "name" : "Karl Wolf"
      },
      {
        "name" : "Jonathan Heiss"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05803v1",
    "title" : "The Five Safes as a Privacy Context",
    "summary" : "The Five Safes is a framework used by national statistical offices (NSO) for\nassessing and managing the disclosure risk of data sharing. This paper makes\ntwo points: Firstly, the Five Safes can be understood as a specialization of a\nbroader concept $\\unicode{x2013}$ contextual integrity $\\unicode{x2013}$ to the\nsituation of statistical dissemination by an NSO. We demonstrate this by\nmapping the five parameters of contextual integrity onto the five dimensions of\nthe Five Safes. Secondly, the Five Safes contextualizes narrow, technical\nnotions of privacy within a holistic risk assessment. We demonstrate this with\nthe example of differential privacy (DP). This contextualization allows NSOs to\nplace DP within their Five Safes toolkit while also guiding the design of DP\nimplementations within the broader privacy context, as delineated by both their\nregulation and the relevant social norms.",
    "updated" : "2025-10-07T11:19:22Z",
    "published" : "2025-10-07T11:19:22Z",
    "authors" : [
      {
        "name" : "James Bailie"
      },
      {
        "name" : "Ruobin Gong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05535v1",
    "title" : "Permutation-Invariant Representation Learning for Robust and\n  Privacy-Preserving Feature Selection",
    "summary" : "Feature selection eliminates redundancy among features to improve downstream\ntask performance while reducing computational overhead. Existing methods often\nstruggle to capture intricate feature interactions and adapt across diverse\napplication scenarios. Recent advances employ generative intelligence to\nalleviate these drawbacks. However, these methods remain constrained by\npermutation sensitivity in embedding and reliance on convexity assumptions in\ngradient-based search. To address these limitations, our initial work\nintroduces a novel framework that integrates permutation-invariant embedding\nwith policy-guided search. Although effective, it still left opportunities to\nadapt to realistic distributed scenarios. In practice, data across local\nclients is highly imbalanced, heterogeneous and constrained by strict privacy\nregulations, limiting direct sharing. These challenges highlight the need for a\nframework that can integrate feature selection knowledge across clients without\nexposing sensitive information. In this extended journal version, we advance\nthe framework from two perspectives: 1) developing a privacy-preserving\nknowledge fusion strategy to derive a unified representation space without\nsharing sensitive raw data. 2) incorporating a sample-aware weighting strategy\nto address distributional imbalance among heterogeneous local clients.\nExtensive experiments validate the effectiveness, robustness, and efficiency of\nour framework. The results further demonstrate its strong generalization\nability in federated learning scenarios. The code and data are publicly\navailable: https://anonymous.4open.science/r/FedCAPS-08BF.",
    "updated" : "2025-10-07T02:53:32Z",
    "published" : "2025-10-07T02:53:32Z",
    "authors" : [
      {
        "name" : "Rui Liu"
      },
      {
        "name" : "Tao Zhe"
      },
      {
        "name" : "Yanjie Fu"
      },
      {
        "name" : "Feng Xia"
      },
      {
        "name" : "Ted Senator"
      },
      {
        "name" : "Dongjie Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05288v1",
    "title" : "DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language\n  Models Using Adam Optimization with Adaptive Clipping",
    "summary" : "Large language models (LLMs) such as ChatGPT have evolved into powerful and\nubiquitous tools. Fine-tuning on small datasets allows LLMs to acquire\nspecialized skills for specific tasks efficiently. Although LLMs provide great\nutility in both general and task-specific use cases, they are limited by two\nsecurity-related concerns. First, traditional LLM hardware requirements make\nthem infeasible to run locally on consumer-grade devices. A remote network\nconnection with the LLM provider's server is usually required, making the\nsystem vulnerable to network attacks. Second, fine-tuning an LLM for a\nsensitive task may involve sensitive data. Non-private fine-tuning algorithms\nproduce models vulnerable to training data reproduction attacks. Our work\naddresses these security concerns by enhancing differentially private\noptimization algorithms and applying them to fine-tune localizable language\nmodels. We introduce adaptable gradient clipping along with other engineering\nenhancements to the standard DP-Adam optimizer to create DP-Adam-AC. We use our\noptimizer to fine-tune examples of two localizable LLM designs, small language\nmodel (Qwen2.5-0.5B) and 1.58 bit quantization (Bitnet-b1.58-2B). We\ndemonstrate promising improvements in loss through experimentation with two\nsynthetic datasets.",
    "updated" : "2025-10-06T18:56:15Z",
    "published" : "2025-10-06T18:56:15Z",
    "authors" : [
      {
        "name" : "Ruoxing Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05172v1",
    "title" : "Learning More with Less: A Generalizable, Self-Supervised Framework for\n  Privacy-Preserving Capacity Estimation with EV Charging Data",
    "summary" : "Accurate battery capacity estimation is key to alleviating consumer concerns\nabout battery performance and reliability of electric vehicles (EVs). However,\npractical data limitations imposed by stringent privacy regulations and labeled\ndata shortages hamper the development of generalizable capacity estimation\nmodels that remain robust to real-world data distribution shifts. While\nself-supervised learning can leverage unlabeled data, existing techniques are\nnot particularly designed to learn effectively from challenging field data --\nlet alone from privacy-friendly data, which are often less feature-rich and\nnoisier. In this work, we propose a first-of-its-kind capacity estimation model\nbased on self-supervised pre-training, developed on a large-scale dataset of\nprivacy-friendly charging data snippets from real-world EV operations. Our\npre-training framework, snippet similarity-weighted masked input\nreconstruction, is designed to learn rich, generalizable representations even\nfrom less feature-rich and fragmented privacy-friendly data. Our key innovation\nlies in harnessing contrastive learning to first capture high-level\nsimilarities among fragmented snippets that otherwise lack meaningful context.\nWith our snippet-wise contrastive learning and subsequent similarity-weighted\nmasked reconstruction, we are able to learn rich representations of both\ngranular charging patterns within individual snippets and high-level\nassociative relationships across different snippets. Bolstered by this rich\nrepresentation learning, our model consistently outperforms state-of-the-art\nbaselines, achieving 31.9% lower test error than the best-performing benchmark,\neven under challenging domain-shifted settings affected by both manufacturer\nand age-induced distribution shifts.",
    "updated" : "2025-10-05T08:58:35Z",
    "published" : "2025-10-05T08:58:35Z",
    "authors" : [
      {
        "name" : "Anushiya Arunan"
      },
      {
        "name" : "Yan Qin"
      },
      {
        "name" : "Xiaoli Li"
      },
      {
        "name" : "U-Xuan Tan"
      },
      {
        "name" : "H. Vincent Poor"
      },
      {
        "name" : "Chau Yuen"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07176v1",
    "title" : "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of\n  Privacy Risks in LLM Agent Interactions",
    "summary" : "Large Language Models (LLMs) are increasingly deployed as agents that\norchestrate tasks and integrate external tools to execute complex workflows. We\ndemonstrate that these interactive behaviors leave distinctive fingerprints in\nencrypted traffic exchanged between users and LLM agents. By analyzing traffic\npatterns associated with agent workflows and tool invocations, adversaries can\ninfer agent activities, distinguish specific agents, and even profile sensitive\nuser attributes. To highlight this risk, we develop AgentPrint, which achieves\nan F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3\naccuracy in user attribute inference for simulated- and real-user settings,\nrespectively. These results uncover an overlooked risk: the very interactivity\nthat empowers LLM agents also exposes user privacy, underscoring the urgent\nneed for technical countermeasures alongside regulatory and policy safeguards.",
    "updated" : "2025-10-08T16:16:23Z",
    "published" : "2025-10-08T16:16:23Z",
    "authors" : [
      {
        "name" : "Yixiang Zhang"
      },
      {
        "name" : "Xinhao Deng"
      },
      {
        "name" : "Zhongyi Gu"
      },
      {
        "name" : "Yihao Chen"
      },
      {
        "name" : "Ke Xu"
      },
      {
        "name" : "Qi Li"
      },
      {
        "name" : "Jianping Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07136v1",
    "title" : "Spectral Graph Clustering under Differential Privacy: Balancing Privacy,\n  Accuracy, and Efficiency",
    "summary" : "We study the problem of spectral graph clustering under edge differential\nprivacy (DP). Specifically, we develop three mechanisms: (i) graph perturbation\nvia randomized edge flipping combined with adjacency matrix shuffling, which\nenforces edge privacy while preserving key spectral properties of the graph.\nImportantly, shuffling considerably amplifies the guarantees: whereas flipping\nedges with a fixed probability alone provides only a constant epsilon edge DP\nguarantee as the number of nodes grows, the shuffled mechanism achieves\n(epsilon, delta) edge DP with parameters that tend to zero as the number of\nnodes increase; (ii) private graph projection with additive Gaussian noise in a\nlower-dimensional space to reduce dimensionality and computational complexity;\nand (iii) a noisy power iteration method that distributes Gaussian noise across\niterations to ensure edge DP while maintaining convergence. Our analysis\nprovides rigorous privacy guarantees and a precise characterization of the\nmisclassification error rate. Experiments on synthetic and real-world networks\nvalidate our theoretical analysis and illustrate the practical privacy-utility\ntrade-offs.",
    "updated" : "2025-10-08T15:30:27Z",
    "published" : "2025-10-08T15:30:27Z",
    "authors" : [
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "H. Vincent Poor"
      },
      {
        "name" : "Andrea J. Goldsmith"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.LG",
      "cs.SI",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.06326v1",
    "title" : "Composable privacy of networked quantum sensing",
    "summary" : "Networks of sensors are a promising scheme to deliver the benefits of quantum\ntechnologies in coming years, offering enhanced precision and accuracy for\ndistributed metrology through the use of large entangled states. Recent work\nhas additionally explored the privacy of these schemes, meaning that local\nparameters can be kept secret while a joint function of these is estimated by\nthe network. In this work, we use the abstract cryptography framework to relate\nthe two proposed definitions of quasi-privacy, showing that both are\ncomposable, which enables the protocol to be securely included as a sub-routine\nto other schemes. We give an explicit example that estimating the mean of a set\nof parameters using GHZ states is composably fully secure.",
    "updated" : "2025-10-07T18:00:04Z",
    "published" : "2025-10-07T18:00:04Z",
    "authors" : [
      {
        "name" : "Naomi R. Solomons"
      },
      {
        "name" : "Damian Markham"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.06267v1",
    "title" : "RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating\n  Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases",
    "summary" : "We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion\nframework that generates realistic yet privacy-preserving synthetic\nelectronic-health-record (EHR) trajectories for ultra-rare diseases.\nRareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human\nPhenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA\nAdverse Event Reporting System (FAERS) into a heterogeneous knowledge graph\ncomprising approximately 8 M typed edges. Meta-path scores extracted from this\n8-million-edge KG modulate the per-token noise schedule in the forward\nstochastic differential equation, steering generation toward biologically\nplausible lab-medication-adverse-event co-occurrences while retaining\nscore-based diffusion model stability. The reverse denoiser then produces\ntimestamped sequences of lab-code, medication-code, and adverse-event-flag\ntriples that contain no protected health information. On simulated\nultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean\nDiscrepancy by 40 percent relative to an unguided diffusion baseline and by\ngreater than 60 percent versus GAN counterparts, without sacrificing downstream\npredictive utility. A black-box membership-inference evaluation using the\nDOMIAS attacker yields AUROC approximately 0.53, well below the 0.55\nsafe-release threshold and substantially better than the approximately 0.61\nplus or minus 0.03 observed for non-KG baselines, demonstrating strong\nresistance to re-identification. These results suggest that integrating\nbiomedical knowledge graphs directly into diffusion noise schedules can\nsimultaneously enhance fidelity and privacy, enabling safer data sharing for\nrare-disease research.",
    "updated" : "2025-10-06T03:59:09Z",
    "published" : "2025-10-06T03:59:09Z",
    "authors" : [
      {
        "name" : "Khartik Uppalapati"
      },
      {
        "name" : "Shakeel Abdulkareem"
      },
      {
        "name" : "Bora Yimenicioglu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "I.2.6; H.2.8; J.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.08355v1",
    "title" : "ExPrESSO: Zero-Knowledge backed Extensive Privacy Preserving Single\n  Sign-on",
    "summary" : "User authentication is one of the most important aspects for secure\ncommunication between services and end-users over the Internet. Service\nproviders leverage Single-Sign On (SSO) to make it easier for their users to\nauthenticate themselves. However, standardized systems for SSO, such as OIDC,\ndo not guarantee user privacy as identity providers can track user activities.\nWe propose a zero-knowledge-based mechanism that integrates with OIDC to let\nusers authenticate through SSO without revealing information about the service\nprovider. Our system leverages Groth's zk-SNARK to prove membership of\nsubscribed service providers without revealing their identity. We adopt a\ndecentralized and verifiable approach to set up the prerequisites of our\nconstruction that further secures and establishes trust in the system. We set\nup high security targets and achieve them with minimal storage and latency\ncost, proving that our research can be adopted for production.",
    "updated" : "2025-10-09T15:42:01Z",
    "published" : "2025-10-09T15:42:01Z",
    "authors" : [
      {
        "name" : "Kaustabh Barman"
      },
      {
        "name" : "Fabian Piper"
      },
      {
        "name" : "Sanjeet Raj Pandey"
      },
      {
        "name" : "Axel Kuepper"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.08247v1",
    "title" : "The Right to Communications Confidentiality in Europe: Protecting\n  Privacy, Freedom of Expression, and Trust",
    "summary" : "In the European Union, the General Data Protection Regulation (GDPR) provides\ncomprehensive rules for the processing of personal data. In addition, the EU\nlawmaker intends to adopt specific rules to protect confidentiality of\ncommunications, in a separate ePrivacy Regulation. Some have argued that there\nis no need for such additional rules for communications confidentiality. This\nArticle discusses the protection of the right to confidentiality of\ncommunications in Europe. We look at the right's origins to assess the\nrationale for protecting it. We also analyze how the right is currently\nprotected under the European Convention on Human Rights and under EU law. We\nshow that at its core the right to communications confidentiality protects\nthree individual and collective values: privacy, freedom of expression, and\ntrust in communication services. The right aims to ensure that individuals and\norganizations can safely entrust communication to service providers. Initially,\nthe right protected only postal letters, but it has gradually developed into a\nstrong safeguard for the protection of confidentiality of communications,\nregardless of the technology used. Hence, the right does not merely serve\nindividual privacy interests, but also other more collective interests that are\ncrucial for the functioning of our information society. We conclude that\nseparate EU rules to protect communications confidentiality, next to the GDPR,\nare justified and necessary.",
    "updated" : "2025-10-09T14:05:36Z",
    "published" : "2025-10-09T14:05:36Z",
    "authors" : [
      {
        "name" : "Frederik J. Zuiderveen Borgesius"
      },
      {
        "name" : "Wilfred Steenbruggen"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07976v1",
    "title" : "The impact of abstract and object tags on image privacy classification",
    "summary" : "Object tags denote concrete entities and are central to many computer vision\ntasks, whereas abstract tags capture higher-level information, which is\nrelevant for tasks that require a contextual, potentially subjective scene\nunderstanding. Object and abstract tags extracted from images also facilitate\ninterpretability. In this paper, we explore which type of tags is more suitable\nfor the context-dependent and inherently subjective task of image privacy.\nWhile object tags are generally used for privacy classification, we show that\nabstract tags are more effective when the tag budget is limited. Conversely,\nwhen a larger number of tags per image is available, object-related information\nis as useful. We believe that these findings will guide future research in\ndeveloping more accurate image privacy classifiers, informed by the role of tag\ntypes and quantity.",
    "updated" : "2025-10-09T09:09:02Z",
    "published" : "2025-10-09T09:09:02Z",
    "authors" : [
      {
        "name" : "Darya Baranouskaya"
      },
      {
        "name" : "Andrea Cavallaro"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07457v1",
    "title" : "Comparison of Fully Homomorphic Encryption and Garbled Circuit\n  Techniques in Privacy-Preserving Machine Learning Inference",
    "summary" : "Machine Learning (ML) is making its way into fields such as healthcare,\nfinance, and Natural Language Processing (NLP), and concerns over data privacy\nand model confidentiality continue to grow. Privacy-preserving Machine Learning\n(PPML) addresses this challenge by enabling inference on private data without\nrevealing sensitive inputs or proprietary models. Leveraging Secure Computation\ntechniques from Cryptography, two widely studied approaches in this domain are\nFully Homomorphic Encryption (FHE) and Garbled Circuits (GC). This work\npresents a comparative evaluation of FHE and GC for secure neural network\ninference. A two-layer neural network (NN) was implemented using the CKKS\nscheme from the Microsoft SEAL library (FHE) and the TinyGarble2.0 framework\n(GC) by IntelLabs. Both implementations are evaluated under the semi-honest\nthreat model, measuring inference output error, round-trip time, peak memory\nusage, communication overhead, and communication rounds. Results reveal a\ntrade-off: modular GC offers faster execution and lower memory consumption,\nwhile FHE supports non-interactive inference.",
    "updated" : "2025-10-08T19:03:40Z",
    "published" : "2025-10-08T19:03:40Z",
    "authors" : [
      {
        "name" : "Kalyan Cheerla"
      },
      {
        "name" : "Lotfi Ben Othmane"
      },
      {
        "name" : "Kirill Morozov"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.07452v1",
    "title" : "PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware\n  Targeted Circuit PatcHing",
    "summary" : "Language models (LMs) may memorize personally identifiable information (PII)\nfrom training data, enabling adversaries to extract it during inference.\nExisting defense mechanisms such as differential privacy (DP) reduce this\nleakage, but incur large drops in utility. Based on a comprehensive study using\ncircuit discovery to identify the computational circuits responsible PII\nleakage in LMs, we hypothesize that specific PII leakage circuits in LMs should\nbe responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware\nTargeted Circuit PatcHing), a novel approach that first identifies and\nsubsequently directly edits PII circuits to reduce leakage. PATCH achieves\nbetter privacy-utility trade-off than existing defenses, e.g., reducing recall\nof PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to\nreduce recall of residual leakage of an LM to as low as 0.01%. Our analysis\nshows that PII leakage circuits persist even after the application of existing\ndefense mechanisms. In contrast, PATCH can effectively mitigate their impact.",
    "updated" : "2025-10-08T18:58:41Z",
    "published" : "2025-10-08T18:58:41Z",
    "authors" : [
      {
        "name" : "Anthony Hughes"
      },
      {
        "name" : "Vasisht Duddu"
      },
      {
        "name" : "N. Asokan"
      },
      {
        "name" : "Nikolaos Aletras"
      },
      {
        "name" : "Ning Ma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09443v1",
    "title" : "The Impact of Sanctions on decentralised Privacy Tools: A Case Study of\n  Tornado Cash",
    "summary" : "This paper investigates the impact of sanctions on Tornado Cash, a smart\ncontract protocol designed to enhance transaction privacy. Following the U.S.\nDepartment of the Treasury's sanctions against Tornado Cash in August 2022,\nplatform activity declined sharply. We document a significant and sustained\nreduction in transaction volume, user diversity, and overall protocol\nutilization after the sanctions were imposed. Our analysis draws on transaction\ndata from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We\nfurther examine developments following the partial lifting and eventual removal\nof sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025.\nAlthough activity partially recovered, the rebound remained limited. The\nTornado Cash case illustrates how regulatory interventions can affect\ndecentralized protocols, while also highlighting the challenges of fully\nenforcing such measures in decentralized environments.",
    "updated" : "2025-10-10T14:55:32Z",
    "published" : "2025-10-10T14:55:32Z",
    "authors" : [
      {
        "name" : "Raffaele Cristodaro"
      },
      {
        "name" : "Benjamin Kramer"
      },
      {
        "name" : "Claudio J. Tessone"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09253v1",
    "title" : "Zero-shot image privacy classification with Vision-Language Models",
    "summary" : "While specialized learning-based models have historically dominated image\nprivacy prediction, the current literature increasingly favours adopting large\nVision-Language Models (VLMs) designed for generic tasks. This trend risks\noverlooking the performance ceiling set by purpose-built models due to a lack\nof systematic evaluation. To address this problem, we establish a zero-shot\nbenchmark for image privacy classification, enabling a fair comparison. We\nevaluate the top-3 open-source VLMs, according to a privacy benchmark, using\ntask-aligned prompts and we contrast their performance, efficiency, and\nrobustness against established vision-only and multi-modal methods.\nCounter-intuitively, our results show that VLMs, despite their\nresource-intensive nature in terms of high parameter count and slower\ninference, currently lag behind specialized, smaller models in privacy\nprediction accuracy. We also find that VLMs exhibit higher robustness to image\nperturbations.",
    "updated" : "2025-10-10T10:50:16Z",
    "published" : "2025-10-10T10:50:16Z",
    "authors" : [
      {
        "name" : "Alina Elena Baia"
      },
      {
        "name" : "Alessio Xompero"
      },
      {
        "name" : "Andrea Cavallaro"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09155v1",
    "title" : "Federated Data Analytics for Cancer Immunotherapy: A Privacy-Preserving\n  Collaborative Platform for Patient Management",
    "summary" : "Connected health is a multidisciplinary approach focused on health\nmanagement, prioritizing pa-tient needs in the creation of tools, services, and\ntreatments. This paradigm ensures proactive and efficient care by facilitating\nthe timely exchange of accurate patient information among all stake-holders in\nthe care continuum. The rise of digital technologies and process innovations\npromises to enhance connected health by integrating various healthcare data\nsources. This integration aims to personalize care, predict health outcomes,\nand streamline patient management, though challeng-es remain, particularly in\ndata architecture, application interoperability, and security. Data analytics\ncan provide critical insights for informed decision-making and health\nco-creation, but solutions must prioritize end-users, including patients and\nhealthcare professionals. This perspective was explored through an agile System\nDevelopment Lifecycle in an EU-funded project aimed at developing an integrated\nAI-generated solution for managing cancer patients undergoing immunotherapy.\nThis paper contributes with a collaborative digital framework integrating\nstakeholders across the care continuum, leveraging federated big data analytics\nand artificial intelligence for improved decision-making while ensuring\nprivacy. Analytical capabilities, such as treatment recommendations and adverse\nevent predictions, were validated using real-life data, achieving 70%-90%\naccuracy in a pilot study with the medical partners, demonstrating the\nframework's effectiveness.",
    "updated" : "2025-10-10T08:57:41Z",
    "published" : "2025-10-10T08:57:41Z",
    "authors" : [
      {
        "name" : "Mira Raheem"
      },
      {
        "name" : "Michael Papazoglou"
      },
      {
        "name" : "Bernd Krmer"
      },
      {
        "name" : "Neamat El-Tazi"
      },
      {
        "name" : "Amal Elgammal"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09114v1",
    "title" : "On the Fairness of Privacy Protection: Measuring and Mitigating the\n  Disparity of Group Privacy Risks for Differentially Private Machine Learning",
    "summary" : "While significant progress has been made in conventional fairness-aware\nmachine learning (ML) and differentially private ML (DPML), the fairness of\nprivacy protection across groups remains underexplored. Existing studies have\nproposed methods to assess group privacy risks, but these are based on the\naverage-case privacy risks of data records. Such approaches may underestimate\nthe group privacy risks, thereby potentially underestimating the disparity\nacross group privacy risks. Moreover, the current method for assessing the\nworst-case privacy risks of data records is time-consuming, limiting their\npractical applicability. To address these limitations, we introduce a novel\nmembership inference game that can efficiently audit the approximate worst-case\nprivacy risks of data records. Experimental results demonstrate that our method\nprovides a more stringent measurement of group privacy risks, yielding a\nreliable assessment of the disparity in group privacy risks. Furthermore, to\npromote privacy protection fairness in DPML, we enhance the standard DP-SGD\nalgorithm with an adaptive group-specific gradient clipping strategy, inspired\nby the design of canaries in differential privacy auditing studies. Extensive\nexperiments confirm that our algorithm effectively reduces the disparity in\ngroup privacy risks, thereby enhancing the fairness of privacy protection in\nDPML.",
    "updated" : "2025-10-10T08:09:08Z",
    "published" : "2025-10-10T08:09:08Z",
    "authors" : [
      {
        "name" : "Zhi Yang"
      },
      {
        "name" : "Changwu Huang"
      },
      {
        "name" : "Ke Tang"
      },
      {
        "name" : "Xin Yao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.08813v1",
    "title" : "The Model's Language Matters: A Comparative Privacy Analysis of LLMs",
    "summary" : "Large Language Models (LLMs) are increasingly deployed across multilingual\napplications that handle sensitive data, yet their scale and linguistic\nvariability introduce major privacy risks. Mostly evaluated for English, this\npaper investigates how language structure affects privacy leakage in LLMs\ntrained on English, Spanish, French, and Italian medical corpora. We quantify\nsix linguistic indicators and evaluate three attack vectors: extraction,\ncounterfactual memorization, and membership inference. Results show that\nprivacy vulnerability scales with linguistic redundancy and tokenization\ngranularity: Italian exhibits the strongest leakage, while English shows higher\nmembership separability. In contrast, French and Spanish display greater\nresilience due to higher morphological complexity. Overall, our findings\nprovide the first quantitative evidence that language matters in privacy\nleakage, underscoring the need for language-aware privacy-preserving mechanisms\nin LLM deployments.",
    "updated" : "2025-10-09T20:59:42Z",
    "published" : "2025-10-09T20:59:42Z",
    "authors" : [
      {
        "name" : "Abhishek K. Mishra"
      },
      {
        "name" : "Antoine Boutet"
      },
      {
        "name" : "Lucas Magnana"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11640v1",
    "title" : "Continual Release of Densest Subgraphs: Privacy Amplification &\n  Sublinear Space via Subsampling",
    "summary" : "We study the sublinear space continual release model for edge-differentially\nprivate (DP) graph algorithms, with a focus on the densest subgraph problem\n(DSG) in the insertion-only setting. Our main result is the first continual\nrelease DSG algorithm that matches the additive error of the best static DP\nalgorithms and the space complexity of the best non-private streaming\nalgorithms, up to constants. The key idea is a refined use of subsampling that\nsimultaneously achieves privacy amplification and sparsification, a connection\nnot previously formalized in graph DP. Via a simple black-box reduction to the\nstatic setting, we obtain both pure and approximate-DP algorithms with $O(\\log\nn)$ additive error and $O(n\\log n)$ space, improving both accuracy and space\ncomplexity over the previous state of the art. Along the way, we introduce\ngraph densification in the graph DP setting, adding edges to trigger earlier\nsubsampling, which removes the extra logarithmic factors in error and space\nincurred by prior work [ELMZ25]. We believe this simple idea may be of\nindependent interest.",
    "updated" : "2025-10-13T17:20:13Z",
    "published" : "2025-10-13T17:20:13Z",
    "authors" : [
      {
        "name" : "Felix Zhou"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11514v1",
    "title" : "Toward Efficient and Privacy-Aware eHealth Systems: An Integrated\n  Sensing, Computing, and Semantic Communication Approach",
    "summary" : "Real-time and contactless monitoring of vital signs, such as respiration and\nheartbeat, alongside reliable communication, is essential for modern healthcare\nsystems, especially in remote and privacy-sensitive environments. Traditional\nwireless communication and sensing networks fall short in meeting all the\nstringent demands of eHealth, including accurate sensing, high data efficiency,\nand privacy preservation. To overcome the challenges, we propose a novel\nintegrated sensing, computing, and semantic communication (ISCSC) framework. In\nthe proposed system, a service robot utilises radar to detect patient positions\nand monitor their vital signs, while sending updates to the medical devices.\nInstead of transmitting raw physiological information, the robot computes and\ncommunicates semantically extracted health features to medical devices. This\nsemantic processing improves data throughput and preserves the clinical\nrelevance of the messages, while enhancing data privacy by avoiding the\ntransmission of sensitive data. Leveraging the estimated patient locations, the\nrobot employs an interacting multiple model (IMM) filter to actively track\npatient motion, thereby enabling robust beam steering for continuous and\nreliable monitoring. We then propose a joint optimisation of the beamforming\nmatrices and the semantic extraction ratio, subject to computing capability and\npower budget constraints, with the objective of maximising both the semantic\nsecrecy rate and sensing accuracy. Simulation results validate that the ISCSC\nframework achieves superior sensing accuracy, improved semantic transmission\nefficiency, and enhanced privacy preservation compared to conventional joint\nsensing and communication methods.",
    "updated" : "2025-10-13T15:21:32Z",
    "published" : "2025-10-13T15:21:32Z",
    "authors" : [
      {
        "name" : "Yinchao Yang"
      },
      {
        "name" : "Yahao Ding"
      },
      {
        "name" : "Zhaohui Yang"
      },
      {
        "name" : "Chongwen Huang"
      },
      {
        "name" : "Zhaoyang Zhang"
      },
      {
        "name" : "Dusit Niyato"
      },
      {
        "name" : "Mohammad Shikh-Bahaei"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11347v1",
    "title" : "Multi-View Graph Feature Propagation for Privacy Preservation and\n  Feature Sparsity",
    "summary" : "Graph Neural Networks (GNNs) have demonstrated remarkable success in node\nclassification tasks over relational data, yet their effectiveness often\ndepends on the availability of complete node features. In many real-world\nscenarios, however, feature matrices are highly sparse or contain sensitive\ninformation, leading to degraded performance and increased privacy risks.\nFurthermore, direct exposure of information can result in unintended data\nleakage, enabling adversaries to infer sensitive information. To address these\nchallenges, we propose a novel Multi-view Feature Propagation (MFP) framework\nthat enhances node classification under feature sparsity while promoting\nprivacy preservation. MFP extends traditional Feature Propagation (FP) by\ndividing the available features into multiple Gaussian-noised views, each\npropagating information independently through the graph topology. The\naggregated representations yield expressive and robust node embeddings. This\nframework is novel in two respects: it introduces a mechanism that improves\nrobustness under extreme sparsity, and it provides a principled way to balance\nutility with privacy. Extensive experiments conducted on graph datasets\ndemonstrate that MFP outperforms state-of-the-art baselines in node\nclassification while substantially reducing privacy leakage. Moreover, our\nanalysis demonstrates that propagated outputs serve as alternative imputations\nrather than reconstructions of the original features, preserving utility\nwithout compromising privacy. A comprehensive sensitivity analysis further\nconfirms the stability and practical applicability of MFP across diverse\nscenarios. Overall, MFP provides an effective and privacy-aware framework for\ngraph learning in domains characterized by missing or sensitive features.",
    "updated" : "2025-10-13T12:42:00Z",
    "published" : "2025-10-13T12:42:00Z",
    "authors" : [
      {
        "name" : "Etzion Harari"
      },
      {
        "name" : "Moshe Unger"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11299v1",
    "title" : "How to Get Actual Privacy and Utility from Privacy Models: the\n  k-Anonymity and Differential Privacy Families",
    "summary" : "Privacy models were introduced in privacy-preserving data publishing and\nstatistical disclosure control with the promise to end the need for costly\nempirical assessment of disclosure risk. We examine how well this promise is\nkept by the main privacy models. We find they may fail to provide adequate\nprotection guarantees because of problems in their definition or incur\nunacceptable trade-offs between privacy protection and utility preservation.\nSpecifically, k-anonymity may not entirely exclude disclosure if enforced with\ndeterministic mechanisms or without constraints on the confidential values. On\nthe other hand, differential privacy (DP) incurs unacceptable utility loss for\nsmall budgets and its privacy guarantee becomes meaningless for large budgets.\nIn the latter case, an ex post empirical assessment of disclosure risk becomes\nnecessary, undermining the main appeal of privacy models. Whereas the utility\npreservation of DP can only be improved by relaxing its privacy guarantees, we\nargue that a semantic reformulation of k-anonymity can offer more robust\nprivacy without losing utility with respect to traditional syntactic\nk-anonymity.",
    "updated" : "2025-10-13T11:41:12Z",
    "published" : "2025-10-13T11:41:12Z",
    "authors" : [
      {
        "name" : "Josep Domingo-Ferrer"
      },
      {
        "name" : "David Snchez"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "68",
      "K.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11116v1",
    "title" : "N-output Mechanism: Estimating Statistical Information from Numerical\n  Data under Local Differential Privacy",
    "summary" : "Local Differential Privacy (LDP) addresses significant privacy concerns in\nsensitive data collection. In this work, we focus on numerical data collection\nunder LDP, targeting a significant gap in the literature: existing LDP\nmechanisms are optimized for either a very small ($|\\Omega| \\in \\{2, 3\\}$) or\ninfinite output spaces. However, no generalized method for constructing an\noptimal mechanism for an arbitrary output size $N$ exists. To fill this gap, we\npropose the \\textbf{N-output mechanism}, a generalized framework that maps\nnumerical data to one of $N$ discrete outputs.\n  We formulate the mechanism's design as an optimization problem to minimize\nestimation variance for any given $N \\geq 2$ and develop both numerical and\nanalytical solutions. This results in a mechanism that is highly accurate and\nadaptive, as its design is determined by solving an optimization problem for\nany chosen $N$. Furthermore, we extend our framework and existing mechanisms to\nthe task of distribution estimation. Empirical evaluations show that the\nN-output mechanism achieves state-of-the-art accuracy for mean, variance, and\ndistribution estimation with small communication costs.",
    "updated" : "2025-10-13T08:06:59Z",
    "published" : "2025-10-13T08:06:59Z",
    "authors" : [
      {
        "name" : "Incheol Baek"
      },
      {
        "name" : "Yon Dohn Chung"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.10805v1",
    "title" : "Therapeutic AI and the Hidden Risks of Over-Disclosure: An Embedded\n  AI-Literacy Framework for Mental Health Privacy",
    "summary" : "Large Language Models (LLMs) are increasingly deployed in mental health\ncontexts, from structured therapeutic support tools to informal chat-based\nwell-being assistants. While these systems increase accessibility, scalability,\nand personalization, their integration into mental health care brings privacy\nand safety challenges that have not been well-examined. Unlike traditional\nclinical interactions, LLM-mediated therapy often lacks a clear structure for\nwhat information is collected, how it is processed, and how it is stored or\nreused. Users without clinical guidance may over-disclose personal information,\nwhich is sometimes irrelevant to their presenting concern, due to misplaced\ntrust, lack of awareness of data risks, or the conversational design of the\nsystem. This overexposure raises privacy concerns and also increases the\npotential for LLM bias, misinterpretation, and long-term data misuse. We\npropose a framework embedding Artificial Intelligence (AI) literacy\ninterventions directly into mental health conversational systems, and outline a\nstudy plan to evaluate their impact on disclosure safety, trust, and user\nexperience.",
    "updated" : "2025-10-12T20:50:06Z",
    "published" : "2025-10-12T20:50:06Z",
    "authors" : [
      {
        "name" : "Soraya S. Anvari"
      },
      {
        "name" : "Rina R. Wehbe"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.10316v1",
    "title" : "An information theorist's tour of differential privacy",
    "summary" : "Since being proposed in 2006, differential privacy has become a standard\nmethod for quantifying certain risks in publishing or sharing analyses of\nsensitive data. At its heart, differential privacy measures risk in terms of\nthe differences between probability distributions, which is a central topic in\ninformation theory. A differentially private algorithm is a channel between the\nunderlying data and the output of the analysis. Seen in this way, the\nguarantees made by differential privacy can be understood in terms of\nproperties of this channel. In this article we examine a few of the key\nconnections between information theory and the formulation/application of\ndifferential privacy, giving an ``operational significance'' for relevant\ninformation measures.",
    "updated" : "2025-10-11T18:54:05Z",
    "published" : "2025-10-11T18:54:05Z",
    "authors" : [
      {
        "name" : "Anand D. Sarwate"
      },
      {
        "name" : "Flavio P. Calmon"
      },
      {
        "name" : "Oliver Kosut"
      },
      {
        "name" : "Lalitha Sankar"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "math.IT",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09985v1",
    "title" : "Prismo: A Decision Support System for Privacy-Preserving ML Framework\n  Selection",
    "summary" : "Machine learning has become a crucial part of our lives, with applications\nspanning nearly every aspect of our daily activities. However, using personal\ninformation in machine learning applications has sparked significant security\nand privacy concerns about user data. To address these challenges, different\nprivacy-preserving machine learning (PPML) frameworks have been developed to\nprotect sensitive information in machine learning applications. These\nframeworks generally attempt to balance design trade-offs such as computational\nefficiency, communication overhead, security guarantees, and scalability.\nDespite the advancements, selecting the optimal framework and parameters for\nspecific deployment scenarios remains a complex and critical challenge for\nprivacy and security application developers.\n  We present Prismo, an open-source recommendation system designed to aid in\nselecting optimal parameters and frameworks for different PPML application\nscenarios. Prismo enables users to explore a comprehensive space of PPML\nframeworks through various properties based on user-defined objectives. It\nsupports automated filtering of suitable candidate frameworks by considering\nparameters such as the number of parties in multi-party computation or\nfederated learning and computation cost constraints in homomorphic encryption.\nPrismo models every use case into a Linear Integer Programming optimization\nproblem, ensuring tailored solutions are recommended for each scenario. We\nevaluate Prismo's effectiveness through multiple use cases, demonstrating its\nability to deliver best-fit solutions in different deployment scenarios.",
    "updated" : "2025-10-11T03:27:15Z",
    "published" : "2025-10-11T03:27:15Z",
    "authors" : [
      {
        "name" : "Nges Brian Njungle"
      },
      {
        "name" : "Eric Jahns"
      },
      {
        "name" : "Luigi Mastromauro"
      },
      {
        "name" : "Edwin P. Kayang"
      },
      {
        "name" : "Milan Stojkov"
      },
      {
        "name" : "Michel A. Kinsy"
      }
    ],
    "categories" : [
      "cs.CR",
      "I.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09443v2",
    "title" : "The Impact of Sanctions on decentralised Privacy Tools: A Case Study of\n  Tornado Cash",
    "summary" : "This paper investigates the impact of sanctions on Tornado Cash, a smart\ncontract protocol designed to enhance transaction privacy. Following the U.S.\nDepartment of the Treasury's sanctions against Tornado Cash in August 2022,\nplatform activity declined sharply. We document a significant and sustained\nreduction in transaction volume, user diversity, and overall protocol\nutilization after the sanctions were imposed. Our analysis draws on transaction\ndata from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We\nfurther examine developments following the partial lifting and eventual removal\nof sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025.\nAlthough activity partially recovered, the rebound remained limited. The\nTornado Cash case illustrates how regulatory interventions can affect\ndecentralized protocols, while also highlighting the challenges of fully\nenforcing such measures in decentralized environments.",
    "updated" : "2025-10-13T09:46:22Z",
    "published" : "2025-10-10T14:55:32Z",
    "authors" : [
      {
        "name" : "Raffaele Cristodaro"
      },
      {
        "name" : "Benjamin Kraner"
      },
      {
        "name" : "Claudio J. Tessone"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09715v1",
    "title" : "A Scalable, Privacy-Preserving Decentralized Identity and Verifiable\n  Data Sharing Framework based on Zero-Knowledge Proofs",
    "summary" : "With the proliferation of decentralized applications (DApps), the conflict\nbetween the transparency of blockchain technology and user data privacy has\nbecome increasingly prominent. While Decentralized Identity (DID) and\nVerifiable Credentials (VCs) provide a standardized framework for user data\nsovereignty, achieving trusted identity verification and data sharing without\ncompromising privacy remains a significant challenge. This paper proposes a\nnovel, comprehensive framework that integrates DIDs and VCs with efficient\nZero-Knowledge Proof (ZKP) schemes to address this core issue. The key\ncontributions of this framework are threefold: first, it constructs a set of\nstrong privacy-preserving protocols based on zk-STARKs, allowing users to prove\nthat their credentials satisfy specific conditions (e.g., \"age is over 18\")\nwithout revealing any underlying sensitive data. Second, it designs a scalable,\nprivacy-preserving credential revocation mechanism based on cryptographic\naccumulators, effectively solving credential management challenges in\nlarge-scale scenarios. Finally, it integrates a practical social key recovery\nscheme, significantly enhancing system usability and security. Through a\nprototype implementation and performance evaluation, this paper quantitatively\nanalyzes the framework's performance in terms of proof generation time,\nverification overhead, and on-chain costs. Compared to existing\nstate-of-the-art systems based on zk-SNARKs, our framework, at the cost of a\nlarger proof size, significantly improves prover efficiency for complex\ncomputations and provides stronger security guarantees, including no trusted\nsetup and post-quantum security. Finally, a case study in the decentralized\nfinance (DeFi) credit scoring scenario demonstrates the framework's immense\npotential for unlocking capital efficiency and fostering a trusted data\neconomy.",
    "updated" : "2025-10-10T06:06:05Z",
    "published" : "2025-10-10T06:06:05Z",
    "authors" : [
      {
        "name" : "Hui Yuan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.09691v1",
    "title" : "Evaluation of Differential Privacy Mechanisms on Federated Learning",
    "summary" : "Federated learning is distributed model training across several clients\nwithout disclosing raw data. Despite advancements in data privacy, risks still\nremain. Differential Privacy (DP) is a technique to protect sensitive data by\nadding noise to model updates, usually controlled by a fixed privacy budget.\nHowever, this approach can introduce excessive noise, particularly when the\nmodel converges, which compromises performance. To address this problem,\nadaptive privacy budgets have been investigated as a potential solution. This\nwork implements DP methods using Laplace and Gaussian mechanisms with an\nadaptive privacy budget, extending the SelecEval simulator. We introduce an\nadaptive clipping approach in the Gaussian mechanism, ensuring that gradients\nof the model are dynamically updated rather than using a fixed sensitivity. We\nconduct extensive experiments with various privacy budgets, IID and non-IID\ndatasets, and different numbers of selected clients per round. While our\nexperiments were limited to 200 training rounds, the results suggest that\nadaptive privacy budgets and adaptive clipping can help maintain model accuracy\nwhile preserving privacy.",
    "updated" : "2025-10-09T11:32:36Z",
    "published" : "2025-10-09T11:32:36Z",
    "authors" : [
      {
        "name" : "Tejash Varsani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "68T07, 68M14",
      "I.2.6; I.2.11; K.6.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.00478v2",
    "title" : "Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving\n  Domain Adaptation",
    "summary" : "Recent work on latent diffusion models (LDMs) has focused almost exclusively\non generative tasks, leaving their potential for discriminative transfer\nlargely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a\nnovel LDM-based framework for a more practical variant of source-free domain\nadaptation (SFDA): the source provider may share not only a pre-trained\nclassifier but also an auxiliary latent diffusion module, trained once on the\nsource data and never exposing raw source samples. DVD encodes each source\nfeature's label information into its latent vicinity by fitting a Gaussian\nprior over its k-nearest neighbors and training the diffusion network to drift\nnoisy samples back to label-consistent representations. During adaptation, we\nsample from each target feature's latent vicinity, apply the frozen diffusion\nmodule to generate source-like cues, and use a simple InfoNCE loss to align the\ntarget encoder to these cues, explicitly transferring decision boundaries\nwithout source access. Across standard SFDA benchmarks, DVD outperforms\nstate-of-the-art methods. We further show that the same latent diffusion module\nenhances the source classifier's accuracy on in-domain data and boosts\nperformance in supervised classification and domain generalization experiments.\nDVD thus reinterprets LDMs as practical, privacy-preserving bridges for\nexplicit knowledge transfer, addressing a core challenge in source-free domain\nadaptation that prior methods have yet to solve.",
    "updated" : "2025-10-11T21:35:10Z",
    "published" : "2025-10-01T03:58:26Z",
    "authors" : [
      {
        "name" : "Jing Wang"
      },
      {
        "name" : "Wonho Bae"
      },
      {
        "name" : "Jiahong Chen"
      },
      {
        "name" : "Wenxu Wang"
      },
      {
        "name" : "Junhyug Noh"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12780v1",
    "title" : "Content Anonymization for Privacy in Long-form Audio",
    "summary" : "Voice anonymization techniques have been found to successfully obscure a\nspeaker's acoustic identity in short, isolated utterances in benchmarks such as\nthe VoicePrivacy Challenge. In practice, however, utterances seldom occur in\nisolation: long-form audio is commonplace in domains such as interviews, phone\ncalls, and meetings. In these cases, many utterances from the same speaker are\navailable, which pose a significantly greater privacy risk: given multiple\nutterances from the same speaker, an attacker could exploit an individual's\nvocabulary, syntax, and turns of phrase to re-identify them, even when their\nvoice is completely disguised. To address this risk, we propose new content\nanonymization approaches. Our approach performs a contextual rewriting of the\ntranscripts in an ASR-TTS pipeline to eliminate speaker-specific style while\npreserving meaning. We present results in a long-form telephone conversation\nsetting demonstrating the effectiveness of a content-based attack on\nvoice-anonymized speech. Then we show how the proposed content-based\nanonymization methods can mitigate this risk while preserving speech utility.\nOverall, we find that paraphrasing is an effective defense against\ncontent-based attacks and recommend that stakeholders adopt this step to ensure\nanonymity in long-form audio.",
    "updated" : "2025-10-14T17:52:50Z",
    "published" : "2025-10-14T17:52:50Z",
    "authors" : [
      {
        "name" : "Cristina Aggazzotti"
      },
      {
        "name" : "Ashi Garg"
      },
      {
        "name" : "Zexin Cai"
      },
      {
        "name" : "Nicholas Andrews"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12549v1",
    "title" : "Privacy-Preserving Distributed Estimation with Limited Data Rate",
    "summary" : "This paper focuses on the privacy-preserving distributed estimation problem\nwith a limited data rate, where the observations are the sensitive information.\nSpecifically, a binary-valued quantizer-based privacy-preserving distributed\nestimation algorithm is developed, which improves the algorithm's\nprivacy-preserving capability and simultaneously reduces the communication\ncosts. The algorithm's privacy-preserving capability, measured by the Fisher\ninformation matrix, is dynamically enhanced over time. Notably, the Fisher\ninformation matrix of the output signals with respect to the sensitive\ninformation converges to zero at a polynomial rate, and the improvement in\nprivacy brought by the quantizers is quantitatively characterized as a\nmultiplicative effect. Regarding the communication costs, each sensor transmits\nonly 1 bit of information to its neighbours at each time step. Additionally,\nthe assumption on the negligible quantization error for real-valued messages is\nnot required. While achieving the requirements of privacy preservation and\nreducing communication costs, the algorithm ensures that its estimates converge\nalmost surely to the true value of the unknown parameter by establishing a\nco-design guideline for the time-varying privacy noises and step-sizes. A\npolynomial almost sure convergence rate is obtained, and then the trade-off\nbetween privacy and convergence rate is established. Numerical examples\ndemonstrate the main results.",
    "updated" : "2025-10-14T14:13:32Z",
    "published" : "2025-10-14T14:13:32Z",
    "authors" : [
      {
        "name" : "Jieming Ke"
      },
      {
        "name" : "Jimin Wang"
      },
      {
        "name" : "Ji-Feng Zhang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12153v1",
    "title" : "VeilAudit: Breaking the Deadlock Between Privacy and Accountability\n  Across Blockchains",
    "summary" : "Cross chain interoperability in blockchain systems exposes a fundamental\ntension between user privacy and regulatory accountability. Existing solutions\nenforce an all or nothing choice between full anonymity and mandatory identity\ndisclosure, which limits adoption in regulated financial settings. We present\nVeilAudit, a cross chain auditing framework that introduces Auditor Only\nLinkability, which allows auditors to link transaction behaviors that originate\nfrom the same anonymous entity without learning its identity. VeilAudit\nachieves this with a user generated Linkable Audit Tag that embeds a zero\nknowledge proof to attest to its validity without exposing the user master\nwallet address, and with a special ciphertext that only designated auditors can\ntest for linkage. To balance privacy and compliance, VeilAudit also supports\nthreshold gated identity revelation under due process. VeilAudit further\nprovides a mechanism for building reputation in pseudonymous environments,\nwhich enables applications such as cross chain credit scoring based on\nverifiable behavioral history. We formalize the security guarantees and develop\na prototype that spans multiple EVM chains. Our evaluation shows that the\nframework is practical for today multichain environments.",
    "updated" : "2025-10-14T05:16:23Z",
    "published" : "2025-10-14T05:16:23Z",
    "authors" : [
      {
        "name" : "Minhao Qiao"
      },
      {
        "name" : "Iqbal Gondal"
      },
      {
        "name" : "Hai Dong"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12031v1",
    "title" : "Security and Privacy Assessment of U.S. and Non-U.S. Android E-Commerce\n  Applications",
    "summary" : "E-commerce mobile applications are central to global financial transactions,\nmaking their security and privacy crucial. In this study, we analyze 92\ntop-grossing Android e-commerce apps (58 U.S.-based and 34 international) using\nMobSF, AndroBugs, and RiskInDroid. Our analysis shows widespread SSL and\ncertificate weaknesses, with approximately 92% using unsecured HTTP connections\nand an average MobSF security score of 40.92/100. Over-privileged permissions\nwere identified in 77 apps. While U.S. apps exhibited fewer manifest, code, and\ncertificate vulnerabilities, both groups showed similar network-related issues.\nWe advocate for the adoption of stronger, standardized, and user-focused\nsecurity practices across regions.",
    "updated" : "2025-10-14T00:30:57Z",
    "published" : "2025-10-14T00:30:57Z",
    "authors" : [
      {
        "name" : "Urvashi Kishnani"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11895v1",
    "title" : "High-Probability Bounds For Heterogeneous Local Differential Privacy",
    "summary" : "We study statistical estimation under local differential privacy (LDP) when\nusers may hold heterogeneous privacy levels and accuracy must be guaranteed\nwith high probability. Departing from the common in-expectation analyses, and\nfor one-dimensional and multi-dimensional mean estimation problems, we develop\nfinite sample upper bounds in $\\ell_2$-norm that hold with probability at least\n$1-\\beta$. We complement these results with matching minimax lower bounds,\nestablishing the optimality (up to constants) of our guarantees in the\nheterogeneous LDP regime. We further study distribution learning in\n$\\ell_\\infty$-distance, designing an algorithm with high-probability guarantees\nunder heterogeneous privacy demands. Our techniques offer principled guidance\nfor designing mechanisms in settings with user-specific privacy levels.",
    "updated" : "2025-10-13T19:54:44Z",
    "published" : "2025-10-13T19:54:44Z",
    "authors" : [
      {
        "name" : "Maryam Aliakbarpour"
      },
      {
        "name" : "Alireza Fallah"
      },
      {
        "name" : "Swaha Roy"
      },
      {
        "name" : "Ria Stevens"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.DS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11514v2",
    "title" : "Toward Efficient and Privacy-Aware eHealth Systems: An Integrated\n  Sensing, Computing, and Semantic Communication Approach",
    "summary" : "Real-time and contactless monitoring of vital signs, such as respiration and\nheartbeat, alongside reliable communication, is essential for modern healthcare\nsystems, especially in remote and privacy-sensitive environments. Traditional\nwireless communication and sensing networks fall short in meeting all the\nstringent demands of eHealth, including accurate sensing, high data efficiency,\nand privacy preservation. To overcome the challenges, we propose a novel\nintegrated sensing, computing, and semantic communication (ISCSC) framework. In\nthe proposed system, a service robot utilises radar to detect patient positions\nand monitor their vital signs, while sending updates to the medical devices.\nInstead of transmitting raw physiological information, the robot computes and\ncommunicates semantically extracted health features to medical devices. This\nsemantic processing improves data throughput and preserves the clinical\nrelevance of the messages, while enhancing data privacy by avoiding the\ntransmission of sensitive data. Leveraging the estimated patient locations, the\nrobot employs an interacting multiple model (IMM) filter to actively track\npatient motion, thereby enabling robust beam steering for continuous and\nreliable monitoring. We then propose a joint optimisation of the beamforming\nmatrices and the semantic extraction ratio, subject to computing capability and\npower budget constraints, with the objective of maximising both the semantic\nsecrecy rate and sensing accuracy. Simulation results validate that the ISCSC\nframework achieves superior sensing accuracy, improved semantic transmission\nefficiency, and enhanced privacy preservation compared to conventional joint\nsensing and communication methods.",
    "updated" : "2025-10-14T13:55:01Z",
    "published" : "2025-10-13T15:21:32Z",
    "authors" : [
      {
        "name" : "Yinchao Yang"
      },
      {
        "name" : "Yahao Ding"
      },
      {
        "name" : "Zhaohui Yang"
      },
      {
        "name" : "Chongwen Huang"
      },
      {
        "name" : "Zhaoyang Zhang"
      },
      {
        "name" : "Dusit Niyato"
      },
      {
        "name" : "Mohammad Shikh-Bahaei"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.13528v1",
    "title" : "Experiments \\& Analysis of Privacy-Preserving SQL Query Sanitization\n  Systems",
    "summary" : "Analytical SQL queries are essential for extracting insights from relational\ndatabases but concurrently introduce significant privacy risks by potentially\nexposing sensitive information. To mitigate these risks, numerous query\nsanitization systems have been developed, employing diverse approaches that\ncreate a complex landscape for both researchers and practitioners. These\nsystems vary fundamentally in their design, including the underlying privacy\nmodel, such as k-anonymity or Differential Privacy; the protected privacy unit,\nwhether at the tuple- or user-level; and the software architecture, which can\nbe proxy-based or integrated. This paper provides a systematic classification\nof state-of-the-art SQL sanitization systems based on these qualitative\ncriteria and the scope of queries they support. Furthermore, we present a\nquantitative analysis of leading systems, empirically measuring the trade-offs\nbetween data utility, query execution overhead, and privacy guarantees across a\nrange of analytical queries. This work offers a structured overview and\nperformance assessment intended to clarify the capabilities and limitations of\ncurrent privacy-preserving database technologies.",
    "updated" : "2025-10-15T13:21:59Z",
    "published" : "2025-10-15T13:21:59Z",
    "authors" : [
      {
        "name" : "Los Ecoffet"
      },
      {
        "name" : "Veronika Rehn-Sonigo"
      },
      {
        "name" : "Jean-Franois Couchot"
      },
      {
        "name" : "Catuscia Palamidessi"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.13512v1",
    "title" : "Offline and Online KL-Regularized RLHF under Differential Privacy",
    "summary" : "In this paper, we study the offline and online settings of reinforcement\nlearning from human feedback (RLHF) with KL-regularization -- a widely used\nobjective function in large language model alignment -- under the $\\epsilon$\nlocal differential privacy ($\\epsilon$-LDP) model on the label of the human\npreference. In the offline setting, we design an algorithm based on the\nprinciple of pessimism and derive a new suboptimality gap of\n$\\tilde{O}(1/[(e^\\epsilon-1)^2 n])$ on the KL-regularized objective under\nsingle-policy concentrability. We also prove its optimality by providing a\nmatching lower bound where $n$ is the sample size.\n  In the online setting, we are the first one to theoretically investigate the\nproblem of KL-regularized RLHF with LDP. We design an optimism-based algorithm\nand derive a logarithmic regret bound of $O(d_{\\mathcal{F}}\\log\n(N_{\\mathcal{F}}\\cdot T) /(e^\\epsilon-1)^2 )$, where $T$ is the total time\nstep, $N_{\\mathcal{F}}$ is cardinality of the reward function space\n$\\mathcal{F}$ and $d_{\\mathcal{F}}$ is a variant of eluder dimension for RLHF.\nAs a by-product of our analysis, our results also imply the first analysis for\nonline KL-regularized RLHF without privacy. We implement our algorithm in the\noffline setting to verify our theoretical results and release our open source\ncode at: https://github.com/rushil-thareja/PPKL-RLHF-Official.",
    "updated" : "2025-10-15T13:04:19Z",
    "published" : "2025-10-15T13:04:19Z",
    "authors" : [
      {
        "name" : "Yulian Wu"
      },
      {
        "name" : "Rushil Thareja"
      },
      {
        "name" : "Praneeth Vepakomma"
      },
      {
        "name" : "Francesco Orabona"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.13468v1",
    "title" : "Privacy, freedom of expression, and the right to be forgotten in Europe",
    "summary" : "In this chapter we discuss the relation between privacy and freedom of\nexpression in Europe. In principle, the two rights have equal weight in Europe\n- which right prevails depends on the circumstances of a case. We use the\nGoogle Spain judgment of the Court of Justice of the European Union, sometimes\ncalled the 'right to be forgotten' judgment, to illustrate the difficulties\nwhen balancing the two rights. The court decided in Google Spain that people\nhave, under certain conditions, the right to have search results for their name\ndelisted. We discuss how Google and Data Protection Authorities deal with such\ndelisting requests in practice. Delisting requests illustrate that balancing\nprivacy and freedom of expression interests will always remain difficult.",
    "updated" : "2025-10-15T12:13:52Z",
    "published" : "2025-10-15T12:13:52Z",
    "authors" : [
      {
        "name" : "Stefan Kulk"
      },
      {
        "name" : "Frederik Zuiderveen Borgesius"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.13136v1",
    "title" : "Privacy-Aware Framework of Robust Malware Detection in Indoor Robots:\n  Hybrid Quantum Computing and Deep Neural Networks",
    "summary" : "Indoor robotic systems within Cyber-Physical Systems (CPS) are increasingly\nexposed to Denial of Service (DoS) attacks that compromise localization,\ncontrol and telemetry integrity. We propose a privacy-aware malware detection\nframework for indoor robotic systems, which leverages hybrid quantum computing\nand deep neural networks to counter DoS threats in CPS, while preserving\nprivacy information. By integrating quantum-enhanced feature encoding with\ndropout-optimized deep learning, our architecture achieves up to 95.2%\ndetection accuracy under privacy-constrained conditions. The system operates\nwithout handcrafted thresholds or persistent beacon data, enabling scalable\ndeployment in adversarial environments. Benchmarking reveals robust\ngeneralization, interpretability and resilience against training instability\nthrough modular circuit design. This work advances trustworthy AI for secure,\nautonomous CPS operations.",
    "updated" : "2025-10-15T04:25:33Z",
    "published" : "2025-10-15T04:25:33Z",
    "authors" : [
      {
        "name" : "Tan Le"
      },
      {
        "name" : "Van Le"
      },
      {
        "name" : "Sachin Shetty"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12908v1",
    "title" : "Local Differential Privacy for Federated Learning with Fixed Memory\n  Usage and Per-Client Privacy",
    "summary" : "Federated learning (FL) enables organizations to collaboratively train models\nwithout sharing their datasets. Despite this advantage, recent studies show\nthat both client updates and the global model can leak private information,\nlimiting adoption in sensitive domains such as healthcare. Local differential\nprivacy (LDP) offers strong protection by letting each participant privatize\nupdates before transmission. However, existing LDP methods were designed for\ncentralized training and introduce challenges in FL, including high resource\ndemands that can cause client dropouts and the lack of reliable privacy\nguarantees under asynchronous participation. These issues undermine model\ngeneralizability, fairness, and compliance with regulations such as HIPAA and\nGDPR. To address them, we propose L-RDP, a DP method designed for LDP that\nensures constant, lower memory usage to reduce dropouts and provides rigorous\nper-client privacy guarantees by accounting for intermittent participation.",
    "updated" : "2025-10-14T18:32:08Z",
    "published" : "2025-10-14T18:32:08Z",
    "authors" : [
      {
        "name" : "Rouzbeh Behnia"
      },
      {
        "name" : "Jeremiah Birrell"
      },
      {
        "name" : "Arman Riasi"
      },
      {
        "name" : "Reza Ebrahimi"
      },
      {
        "name" : "Kaushik Dutta"
      },
      {
        "name" : "Thang Hoang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.14894v1",
    "title" : "Secure Sparse Matrix Multiplications and their Applications to\n  Privacy-Preserving Machine Learning",
    "summary" : "To preserve privacy, multi-party computation (MPC) enables executing Machine\nLearning (ML) algorithms on secret-shared or encrypted data. However, existing\nMPC frameworks are not optimized for sparse data. This makes them unsuitable\nfor ML applications involving sparse data, e.g., recommender systems or\ngenomics. Even in plaintext, such applications involve high-dimensional sparse\ndata, that cannot be processed without sparsity-related optimizations due to\nprohibitively large memory requirements.\n  Since matrix multiplication is central in ML algorithms, we propose MPC\nalgorithms to multiply secret sparse matrices. On the one hand, our algorithms\navoid the memory issues of the \"dense\" data representation of classic secure\nmatrix multiplication algorithms. On the other hand, our algorithms can\nsignificantly reduce communication costs (some experiments show a factor 1000)\nfor realistic problem sizes. We validate our algorithms in two ML applications\nin which existing protocols are impractical.\n  An important question when developing MPC algorithms is what assumptions can\nbe made. In our case, if the number of non-zeros in a row is a sensitive piece\nof information then a short runtime may reveal that the number of non-zeros is\nsmall. Existing approaches make relatively simple assumptions, e.g., that there\nis a universal upper bound to the number of non-zeros in a row. This often\ndoesn't align with statistical reality, in a lot of sparse datasets the amount\nof data per instance satisfies a power law. We propose an approach which allows\nadopting a safe upper bound on the distribution of non-zeros in rows/columns of\nsparse matrices.",
    "updated" : "2025-10-16T17:12:18Z",
    "published" : "2025-10-16T17:12:18Z",
    "authors" : [
      {
        "name" : "Marc Damie"
      },
      {
        "name" : "Florian Hahn"
      },
      {
        "name" : "Andreas Peter"
      },
      {
        "name" : "Jan Ramon"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.14312v1",
    "title" : "Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy,\n  and Security Studies",
    "summary" : "A multi-agent system (MAS) powered by large language models (LLMs) can\nautomate tedious user tasks such as meeting scheduling that requires\ninter-agent collaboration. LLMs enable nuanced protocols that account for\nunstructured private data, user constraints, and preferences. However, this\ndesign introduces new risks, including misalignment and attacks by malicious\nparties that compromise agents or steal user data. In this paper, we propose\nthe Terrarium framework for fine-grained study on safety, privacy, and security\nin LLM-based MAS. We repurpose the blackboard design, an early approach in\nmulti-agent systems, to create a modular, configurable testbed for multi-agent\ncollaboration. We identify key attack vectors such as misalignment, malicious\nagents, compromised communication, and data poisoning. We implement three\ncollaborative MAS scenarios with four representative attacks to demonstrate the\nframework's flexibility. By providing tools to rapidly prototype, evaluate, and\niterate on defenses and designs, Terrarium aims to accelerate progress toward\ntrustworthy multi-agent systems.",
    "updated" : "2025-10-16T05:19:13Z",
    "published" : "2025-10-16T05:19:13Z",
    "authors" : [
      {
        "name" : "Mason Nakamura"
      },
      {
        "name" : "Abhinav Kumar"
      },
      {
        "name" : "Saaduddin Mahmud"
      },
      {
        "name" : "Sahar Abdelnabi"
      },
      {
        "name" : "Shlomo Zilberstein"
      },
      {
        "name" : "Eugene Bagdasarian"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "I.2.7; I.2.11"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.14151v1",
    "title" : "Privacy-Preserving and Incentive-Driven Relay-Based Framework for\n  Cross-Domain Blockchain Interoperability",
    "summary" : "Interoperability is essential for transforming blockchains from isolated\nnetworks into collaborative ecosystems, unlocking their full potential. While\nsignificant progress has been made in public blockchain interoperability,\nbridging permissioned and permissionless blockchains poses unique challenges\ndue to differences in access control, architectures, and security requirements.\nThis paper introduces a blockchain-agnostic framework to enable\ninteroperability between permissioned and permissionless networks. Leveraging\ncryptographic techniques, the framework ensures secure data exchanges. Its\nlightweight architectural design simplifies implementation and maintenance,\nwhile the integration of Clover and Dandelion++ protocols enhances transaction\nanonymity. Performance evaluations demonstrate the framework's effectiveness in\nachieving secure and efficient interoperability by measuring the forwarding\ntime, the throughput, the availability, and their collusion impact of the\nsystem across heterogeneous blockchain ecosystems.",
    "updated" : "2025-10-15T22:59:02Z",
    "published" : "2025-10-15T22:59:02Z",
    "authors" : [
      {
        "name" : "Saeed Moradi"
      },
      {
        "name" : "Koosha Esmaeilzadeh Khorasani"
      },
      {
        "name" : "Sara Rouhani"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.12153v2",
    "title" : "VeilAudit: Breaking the Deadlock Between Privacy and Accountability\n  Across Blockchains",
    "summary" : "Cross chain interoperability in blockchain systems exposes a fundamental\ntension between user privacy and regulatory accountability. Existing solutions\nenforce an all or nothing choice between full anonymity and mandatory identity\ndisclosure, which limits adoption in regulated financial settings. We present\nVeilAudit, a cross chain auditing framework that introduces Auditor Only\nLinkability, which allows auditors to link transaction behaviors that originate\nfrom the same anonymous entity without learning its identity. VeilAudit\nachieves this with a user generated Linkable Audit Tag that embeds a zero\nknowledge proof to attest to its validity without exposing the user master\nwallet address, and with a special ciphertext that only designated auditors can\ntest for linkage. To balance privacy and compliance, VeilAudit also supports\nthreshold gated identity revelation under due process. VeilAudit further\nprovides a mechanism for building reputation in pseudonymous environments,\nwhich enables applications such as cross chain credit scoring based on\nverifiable behavioral history. We formalize the security guarantees and develop\na prototype that spans multiple EVM chains. Our evaluation shows that the\nframework is practical for today multichain environments.",
    "updated" : "2025-10-16T04:48:58Z",
    "published" : "2025-10-14T05:16:23Z",
    "authors" : [
      {
        "name" : "Minhao Qiao"
      },
      {
        "name" : "Hai Dong"
      },
      {
        "name" : "Iqbal Gondal"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.13890v1",
    "title" : "A Survey on Collaborating Small and Large Language Models for\n  Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness",
    "summary" : "Large language models (LLMs) have advanced many domains and applications but\nface high fine-tuning costs, inference latency, limited edge deployability, and\nreliability concerns. Small language models (SLMs), compact, efficient, and\nadaptable, offer complementary remedies. Recent work explores collaborative\nframeworks that fuse SLMs' specialization and efficiency with LLMs'\ngeneralization and reasoning to meet diverse objectives across tasks and\ndeployment scenarios. Motivated by these developments, this paper presents a\nsystematic survey of SLM-LLM collaboration organized by collaboration\nobjectives. We propose a taxonomy with four goals: performance enhancement,\ncost-effectiveness, cloud-edge privacy, and trustworthiness. Within this\nframework, we review representative methods, summarize design paradigms, and\noutline open challenges and future directions toward efficient, secure, and\nscalable SLM-LLM collaboration.",
    "updated" : "2025-10-14T04:16:47Z",
    "published" : "2025-10-14T04:16:47Z",
    "authors" : [
      {
        "name" : "Fali Wang"
      },
      {
        "name" : "Jihai Chen"
      },
      {
        "name" : "Shuhua Yang"
      },
      {
        "name" : "Ali Al-Lawati"
      },
      {
        "name" : "Linli Tang"
      },
      {
        "name" : "Hui Liu"
      },
      {
        "name" : "Suhang Wang"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "68T50 (Primary) 68T07 (Secondary)",
      "I.2.7"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.15186v1",
    "title" : "MAGPIE: A benchmark for Multi-AGent contextual PrIvacy Evaluation",
    "summary" : "A core challenge for autonomous LLM agents in collaborative settings is\nbalancing robust privacy understanding and preservation alongside task\nefficacy. Existing privacy benchmarks only focus on simplistic, single-turn\ninteractions where private information can be trivially omitted without\naffecting task outcomes. In this paper, we introduce MAGPIE (Multi-AGent\ncontextual PrIvacy Evaluation), a novel benchmark of 200 high-stakes tasks\ndesigned to evaluate privacy understanding and preservation in multi-agent\ncollaborative, non-adversarial scenarios. MAGPIE integrates private information\nas essential for task resolution, forcing agents to balance effective\ncollaboration with strategic information control. Our evaluation reveals that\nstate-of-the-art agents, including GPT-5 and Gemini 2.5-Pro, exhibit\nsignificant privacy leakage, with Gemini 2.5-Pro leaking up to 50.7% and GPT-5\nup to 35.1% of the sensitive information even when explicitly instructed not\nto. Moreover, these agents struggle to achieve consensus or task completion and\noften resort to undesirable behaviors such as manipulation and power-seeking\n(e.g., Gemini 2.5-Pro demonstrating manipulation in 38.2% of the cases). These\nfindings underscore that current LLM agents lack robust privacy understanding\nand are not yet adequately aligned to simultaneously preserve privacy and\nmaintain effective collaboration in complex environments.",
    "updated" : "2025-10-16T23:12:12Z",
    "published" : "2025-10-16T23:12:12Z",
    "authors" : [
      {
        "name" : "Gurusha Juneja"
      },
      {
        "name" : "Jayanth Naga Sai Pasupulati"
      },
      {
        "name" : "Alon Albalak"
      },
      {
        "name" : "Wenyue Hua"
      },
      {
        "name" : "William Yang Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.15112v1",
    "title" : "AndroByte: LLM-Driven Privacy Analysis through Bytecode Summarization\n  and Dynamic Dataflow Call Graph Generation",
    "summary" : "With the exponential growth in mobile applications, protecting user privacy\nhas become even more crucial. Android applications are often known for\ncollecting, storing, and sharing sensitive user information such as contacts,\nlocation, camera, and microphone data often without the user's clear consent or\nawareness raising significant privacy risks and exposure. In the context of\nprivacy assessment, dataflow analysis is particularly valuable for identifying\ndata usage and potential leaks. Traditionally, this type of analysis has relied\non formal methods, heuristics, and rule-based matching. However, these\ntechniques are often complex to implement and prone to errors, such as taint\nexplosion for large programs. Moreover, most existing Android dataflow analysis\nmethods depend heavily on predefined list of sinks, limiting their flexibility\nand scalability. To address the limitations of these existing techniques, we\npropose AndroByte, an AI-driven privacy analysis tool that leverages LLM\nreasoning on bytecode summarization to dynamically generate accurate and\nexplainable dataflow call graphs from static code analysis. AndroByte achieves\na significant F\\b{eta}-Score of 89% in generating dynamic dataflow call graphs\non the fly, outperforming the effectiveness of traditional tools like FlowDroid\nand Amandroid in leak detection without relying on predefined propagation rules\nor sink lists. Moreover, AndroByte's iterative bytecode summarization provides\ncomprehensive and explainable insights into dataflow and leak detection,\nachieving high, quantifiable scores based on the G-Eval metric.",
    "updated" : "2025-10-16T20:10:20Z",
    "published" : "2025-10-16T20:10:20Z",
    "authors" : [
      {
        "name" : "Mst Eshita Khatun"
      },
      {
        "name" : "Lamine Noureddine"
      },
      {
        "name" : "Zhiyong Sui"
      },
      {
        "name" : "Aisha Ali-Gombe"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.15083v1",
    "title" : "SMOTE and Mirrors: Exposing Privacy Leakage from Synthetic Minority\n  Oversampling",
    "summary" : "The Synthetic Minority Over-sampling Technique (SMOTE) is one of the most\nwidely used methods for addressing class imbalance and generating synthetic\ndata. Despite its popularity, little attention has been paid to its privacy\nimplications; yet, it is used in the wild in many privacy-sensitive\napplications. In this work, we conduct the first systematic study of privacy\nleakage in SMOTE: We begin by showing that prevailing evaluation practices,\ni.e., naive distinguishing and distance-to-closest-record metrics, completely\nfail to detect any leakage and that membership inference attacks (MIAs) can be\ninstantiated with high accuracy. Then, by exploiting SMOTE's geometric\nproperties, we build two novel attacks with very limited assumptions:\nDistinSMOTE, which perfectly distinguishes real from synthetic records in\naugmented datasets, and ReconSMOTE, which reconstructs real minority records\nfrom synthetic datasets with perfect precision and recall approaching one under\nrealistic imbalance ratios. We also provide theoretical guarantees for both\nattacks. Experiments on eight standard imbalanced datasets confirm the\npracticality and effectiveness of these attacks. Overall, our work reveals that\nSMOTE is inherently non-private and disproportionately exposes minority\nrecords, highlighting the need to reconsider its use in privacy-sensitive\napplications.",
    "updated" : "2025-10-16T18:55:46Z",
    "published" : "2025-10-16T18:55:46Z",
    "authors" : [
      {
        "name" : "Georgi Ganev"
      },
      {
        "name" : "Reza Nazari"
      },
      {
        "name" : "Rees Davison"
      },
      {
        "name" : "Amir Dizche"
      },
      {
        "name" : "Xinmin Wu"
      },
      {
        "name" : "Ralph Abbey"
      },
      {
        "name" : "Jorge Silva"
      },
      {
        "name" : "Emiliano De Cristofaro"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.11299v2",
    "title" : "How to Get Actual Privacy and Utility from Privacy Models: the\n  k-Anonymity and Differential Privacy Families",
    "summary" : "Privacy models were introduced in privacy-preserving data publishing and\nstatistical disclosure control with the promise to end the need for costly\nempirical assessment of disclosure risk. We examine how well this promise is\nkept by the main privacy models. We find they may fail to provide adequate\nprotection guarantees because of problems in their definition or incur\nunacceptable trade-offs between privacy protection and utility preservation.\nSpecifically, k-anonymity may not entirely exclude disclosure if enforced with\ndeterministic mechanisms or without constraints on the confidential values. On\nthe other hand, differential privacy (DP) incurs unacceptable utility loss for\nsmall budgets and its privacy guarantee becomes meaningless for large budgets.\nIn the latter case, an ex post empirical assessment of disclosure risk becomes\nnecessary, undermining the main appeal of privacy models. Whereas the utility\npreservation of DP can only be improved by relaxing its privacy guarantees, we\nargue that a semantic reformulation of k-anonymity can offer more robust\nprivacy without losing utility with respect to traditional syntactic\nk-anonymity.",
    "updated" : "2025-10-17T09:07:58Z",
    "published" : "2025-10-13T11:41:12Z",
    "authors" : [
      {
        "name" : "Josep Domingo-Ferrer"
      },
      {
        "name" : "David Snchez"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "68",
      "K.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.05172v2",
    "title" : "Learning More with Less: A Generalizable, Self-Supervised Framework for\n  Privacy-Preserving Capacity Estimation with EV Charging Data",
    "summary" : "Accurate battery capacity estimation is key to alleviating consumer concerns\nabout battery performance and reliability of electric vehicles (EVs). However,\npractical data limitations imposed by stringent privacy regulations and labeled\ndata shortages hamper the development of generalizable capacity estimation\nmodels that remain robust to real-world data distribution shifts. While\nself-supervised learning can leverage unlabeled data, existing techniques are\nnot particularly designed to learn effectively from challenging field data --\nlet alone from privacy-friendly data, which are often less feature-rich and\nnoisier. In this work, we propose a first-of-its-kind capacity estimation model\nbased on self-supervised pre-training, developed on a large-scale dataset of\nprivacy-friendly charging data snippets from real-world EV operations. Our\npre-training framework, snippet similarity-weighted masked input\nreconstruction, is designed to learn rich, generalizable representations even\nfrom less feature-rich and fragmented privacy-friendly data. Our key innovation\nlies in harnessing contrastive learning to first capture high-level\nsimilarities among fragmented snippets that otherwise lack meaningful context.\nWith our snippet-wise contrastive learning and subsequent similarity-weighted\nmasked reconstruction, we are able to learn rich representations of both\ngranular charging patterns within individual snippets and high-level\nassociative relationships across different snippets. Bolstered by this rich\nrepresentation learning, our model consistently outperforms state-of-the-art\nbaselines, achieving 31.9% lower test error than the best-performing benchmark,\neven under challenging domain-shifted settings affected by both manufacturer\nand age-induced distribution shifts. Source code is available at\nhttps://github.com/en-research/GenEVBattery.",
    "updated" : "2025-10-17T03:22:40Z",
    "published" : "2025-10-05T08:58:35Z",
    "authors" : [
      {
        "name" : "Anushiya Arunan"
      },
      {
        "name" : "Yan Qin"
      },
      {
        "name" : "Xiaoli Li"
      },
      {
        "name" : "U-Xuan Tan"
      },
      {
        "name" : "H. Vincent Poor"
      },
      {
        "name" : "Chau Yuen"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.17480v1",
    "title" : "Unified Privacy Guarantees for Decentralized Learning via Matrix\n  Factorization",
    "summary" : "Decentralized Learning (DL) enables users to collaboratively train models\nwithout sharing raw data by iteratively averaging local updates with neighbors\nin a network graph. This setting is increasingly popular for its scalability\nand its ability to keep data local under user control. Strong privacy\nguarantees in DL are typically achieved through Differential Privacy (DP), with\nresults showing that DL can even amplify privacy by disseminating noise across\npeer-to-peer communications. Yet in practice, the observed privacy-utility\ntrade-off often appears worse than in centralized training, which may be due to\nlimitations in current DP accounting methods for DL. In this paper, we show\nthat recent advances in centralized DP accounting based on Matrix Factorization\n(MF) for analyzing temporal noise correlations can also be leveraged in DL. By\ngeneralizing existing MF results, we show how to cast both standard DL\nalgorithms and common trust models into a unified formulation. This yields\ntighter privacy accounting for existing DP-DL algorithms and provides a\nprincipled way to develop new ones. To demonstrate the approach, we introduce\nMAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that\noutperforms existing methods on synthetic and real-world graphs.",
    "updated" : "2025-10-20T12:24:27Z",
    "published" : "2025-10-20T12:24:27Z",
    "authors" : [
      {
        "name" : "Aurlien Bellet"
      },
      {
        "name" : "Edwige Cyffers"
      },
      {
        "name" : "Davide Frey"
      },
      {
        "name" : "Romaric Gaudel"
      },
      {
        "name" : "Dimitri Lervrend"
      },
      {
        "name" : "Franois Taani"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.17372v1",
    "title" : "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition\n  Performance without Privacy Compromise",
    "summary" : "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research.",
    "updated" : "2025-10-20T10:08:53Z",
    "published" : "2025-10-20T10:08:53Z",
    "authors" : [
      {
        "name" : "Pawe Borsukiewicz"
      },
      {
        "name" : "Fadi Boutros"
      },
      {
        "name" : "Iyiola E. Olatunji"
      },
      {
        "name" : "Charles Beumier"
      },
      {
        "name" : "Wendkuni C. Ouedraogo"
      },
      {
        "name" : "Jacques Klein"
      },
      {
        "name" : "Tegawend F. Bissyand"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.17348v1",
    "title" : "Optimal Best Arm Identification under Differential Privacy",
    "summary" : "Best Arm Identification (BAI) algorithms are deployed in data-sensitive\napplications, such as adaptive clinical trials or user studies. Driven by the\nprivacy concerns of these applications, we study the problem of\nfixed-confidence BAI under global Differential Privacy (DP) for Bernoulli\ndistributions. While numerous asymptotically optimal BAI algorithms exist in\nthe non-private setting, a significant gap remains between the best lower and\nupper bounds in the global DP setting. This work reduces this gap to a small\nmultiplicative constant, for any privacy budget $\\epsilon$. First, we provide a\ntighter lower bound on the expected sample complexity of any $\\delta$-correct\nand $\\epsilon$-global DP strategy. Our lower bound replaces the\nKullback-Leibler (KL) divergence in the transportation cost used by the\nnon-private characteristic time with a new information-theoretic quantity that\noptimally trades off between the KL divergence and the Total Variation distance\nscaled by $\\epsilon$. Second, we introduce a stopping rule based on these\ntransportation costs and a private estimator of the means computed using an\narm-dependent geometric batching. En route to proving the correctness of our\nstopping rule, we derive concentration results of independent interest for the\nLaplace distribution and for the sum of Bernoulli and Laplace distributions.\nThird, we propose a Top Two sampling rule based on these transportation costs.\nFor any budget $\\epsilon$, we show an asymptotic upper bound on its expected\nsample complexity that matches our lower bound to a multiplicative constant\nsmaller than $8$. Our algorithm outperforms existing $\\delta$-correct and\n$\\epsilon$-global DP BAI algorithms for different values of $\\epsilon$.",
    "updated" : "2025-10-20T09:46:09Z",
    "published" : "2025-10-20T09:46:09Z",
    "authors" : [
      {
        "name" : "Marc Jourdan"
      },
      {
        "name" : "Achraf Azize"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.17162v1",
    "title" : "ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for\n  Dynamic Edge Crowdsensing",
    "summary" : "Mobile edge crowdsensing (MECS) systems continuously generate and transmit\nuser data in dynamic, resource-constrained environments, exposing users to\nsignificant privacy threats. In practice, many privacy-preserving mechanisms\nbuild on differential privacy (DP). However, static DP mechanisms often fail to\nadapt to evolving risks, for example, shifts in adversarial capabilities,\nresource constraints and task requirements, resulting in either excessive noise\nor inadequate protection. To address this challenge, we propose ALPINE, a\nlightweight, adaptive framework that empowers terminal devices to autonomously\nadjust differential privacy levels in real time. ALPINE operates as a\nclosed-loop control system consisting of four modules: dynamic risk perception,\nprivacy decision via twin delayed deep deterministic policy gradient (TD3),\nlocal privacy execution and performance verification from edge nodes. Based on\nenvironmental risk assessments, we design a reward function that balances\nprivacy gains, data utility and energy cost, guiding the TD3 agent to\nadaptively tune noise magnitude across diverse risk scenarios and achieve a\ndynamic equilibrium among privacy, utility and cost. Both the collaborative\nrisk model and pretrained TD3-based agent are designed for low-overhead\ndeployment. Extensive theoretical analysis and real-world simulations\ndemonstrate that ALPINE effectively mitigates inference attacks while\npreserving utility and cost, making it practical for large-scale edge\napplications.",
    "updated" : "2025-10-20T05:03:25Z",
    "published" : "2025-10-20T05:03:25Z",
    "authors" : [
      {
        "name" : "Guanjie Cheng"
      },
      {
        "name" : "Siyang Liu"
      },
      {
        "name" : "Junqin Huang"
      },
      {
        "name" : "Xinkui Zhao"
      },
      {
        "name" : "Yin Wang"
      },
      {
        "name" : "Mengying Zhu"
      },
      {
        "name" : "Linghe Kong"
      },
      {
        "name" : "Shuiguang Deng"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.16744v1",
    "title" : "Cryptanalysis of a Privacy-Preserving Ride-Hailing Service from NSS 2022",
    "summary" : "Ride-Hailing Services (RHS) match a ride request initiated by a rider with a\nsuitable driver responding to the ride request. A Privacy-Preserving RHS\n(PP-RHS) aims to facilitate ride matching while ensuring the privacy of riders'\nand drivers' location data w.r.t. the Service Provider (SP). At NSS 2022, Xie\net al. proposed a PP-RHS. In this work, we demonstrate a passive attack on\ntheir PP-RHS protocol. Our attack allows the SP to completely recover the\nlocations of the rider as well as that of the responding drivers in every ride\nrequest. Further, our attack is very efficient as it is independent of the\nsecurity parameter.",
    "updated" : "2025-10-19T08:05:25Z",
    "published" : "2025-10-19T08:05:25Z",
    "authors" : [
      {
        "name" : "Srinivas Vivek"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.16687v1",
    "title" : "High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient\n  Descent on Least Squares",
    "summary" : "The interplay between optimization and privacy has become a central theme in\nprivacy-preserving machine learning. Noisy stochastic gradient descent (SGD)\nhas emerged as a cornerstone algorithm, particularly in large-scale settings.\nThese variants of gradient methods inject carefully calibrated noise into each\nupdate to achieve differential privacy, the gold standard notion of rigorous\nprivacy guarantees. Prior work primarily provides various bounds on statistical\nrisk and privacy loss for noisy SGD, yet the \\textit{exact} behavior of the\nprocess remains unclear, particularly in high-dimensional settings. This work\nleverages a diffusion approach to analyze noisy SGD precisely, providing a\ncontinuous-time perspective that captures both statistical risk evolution and\nprivacy loss dynamics in high dimensions. Moreover, we study a variant of noisy\nSGD that does not require explicit knowledge of gradient sensitivity, unlike\nexisting work that assumes or enforces sensitivity through gradient clipping.\nSpecifically, we focus on the least squares problem with $\\ell_2$\nregularization.",
    "updated" : "2025-10-19T02:28:27Z",
    "published" : "2025-10-19T02:28:27Z",
    "authors" : [
      {
        "name" : "Shurong Lin"
      },
      {
        "name" : "Eric D. Kolaczyk"
      },
      {
        "name" : "Adam Smith"
      },
      {
        "name" : "Elliot Paquette"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.16331v1",
    "title" : "Efficient and Privacy-Preserving Binary Dot Product via Multi-Party\n  Computation",
    "summary" : "Striking a balance between protecting data privacy and enabling collaborative\ncomputation is a critical challenge for distributed machine learning. While\nprivacy-preserving techniques for federated learning have been extensively\ndeveloped, methods for scenarios involving bitwise operations, such as\ntree-based vertical federated learning (VFL), are still underexplored.\nTraditional mechanisms, including Shamir's secret sharing and multi-party\ncomputation (MPC), are not optimized for bitwise operations over binary data,\nparticularly in settings where each participant holds a different part of the\nbinary vector. This paper addresses the limitations of existing methods by\nproposing a novel binary multi-party computation (BiMPC) framework. The BiMPC\nmechanism facilitates privacy-preserving bitwise operations, with a particular\nfocus on dot product computations of binary vectors, ensuring the privacy of\neach individual bit. The core of BiMPC is a novel approach called Dot Product\nvia Modular Addition (DoMA), which uses regular and modular additions for\nefficient binary dot product calculation. To ensure privacy, BiMPC uses random\nmasking in a higher field for linear computations and a three-party oblivious\ntransfer (triot) protocol for non-linear binary operations. The privacy\nguarantees of the BiMPC framework are rigorously analyzed, demonstrating its\nefficiency and scalability in distributed settings.",
    "updated" : "2025-10-18T03:35:42Z",
    "published" : "2025-10-18T03:35:42Z",
    "authors" : [
      {
        "name" : "Fatemeh Jafarian Dehkordi"
      },
      {
        "name" : "Elahe Vedadi"
      },
      {
        "name" : "Alireza Feizbakhsh"
      },
      {
        "name" : "Yasaman Keshtkarjahromi"
      },
      {
        "name" : "Hulya Seferoglu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.16083v1",
    "title" : "PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction\n  via Graph-Based Federated Learning for Representing Password Reuse between\n  Websites",
    "summary" : "Credential stuffing attacks have caused significant harm to online users who\nfrequently reuse passwords across multiple websites. While prior research has\nattempted to detect users with reused passwords or identify malicious login\nattempts, existing methods often compromise usability by restricting password\ncreation or website access, and their reliance on complex account-sharing\nmechanisms hinders real-world deployment. To address these limitations, we\npropose PassREfinder-FL, a novel framework that predicts credential stuffing\nrisks across websites. We introduce the concept of password reuse relations --\ndefined as the likelihood of users reusing passwords between websites -- and\nrepresent them as edges in a website graph. Using graph neural networks (GNNs),\nwe perform a link prediction task to assess credential reuse risk between\nsites. Our approach scales to a large number of arbitrary websites by\nincorporating public website information and linking newly observed websites as\nnodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a\nfederated learning (FL) approach that eliminates the need to share user\nsensitive information across administrators. Evaluation on a real-world dataset\nof 360 million breached accounts from 22,378 websites shows that\nPassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further\nvalidate that our FL-based GNN achieves a 4-11% performance improvement over\nother state-of-the-art GNN models through an ablation study. Finally, we\ndemonstrate that the predicted results can be used to quantify password reuse\nlikelihood as actionable risk scores.",
    "updated" : "2025-10-17T14:59:24Z",
    "published" : "2025-10-17T14:59:24Z",
    "authors" : [
      {
        "name" : "Jaehan Kim"
      },
      {
        "name" : "Minkyoo Song"
      },
      {
        "name" : "Minjae Seo"
      },
      {
        "name" : "Youngjin Jin"
      },
      {
        "name" : "Seungwon Shin"
      },
      {
        "name" : "Jinwoo Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.16054v1",
    "title" : "PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware\n  Delegation",
    "summary" : "When users submit queries to Large Language Models (LLMs), their prompts can\noften contain sensitive data, forcing a difficult choice: Send the query to a\npowerful proprietary LLM providers to achieving state-of-the-art performance\nand risk data exposure, or relying on smaller, local models guarantees data\nprivacy but often results in a degradation of task performance. Prior\napproaches have relied on static pipelines that use LLM rewriting, which\nshatters linguistic coherence and indiscriminately removes privacy-sensitive\ninformation, including task-critical content. We reformulate this challenge\n(Privacy-Conscious Delegation) as a sequential decision-making problem and\nintroduce a novel reinforcement learning (RL) framework called PrivacyPAD to\nsolve it. Our framework trains an agent to dynamically route text chunks,\nlearning a policy that optimally balances the trade-off between privacy leakage\nand task performance. It implicitly distinguishes between replaceable\nPersonally Identifiable Information (PII) (which it shields locally) and\ntask-critical PII (which it strategically sends to the remote model for maximal\nutility). To validate our approach in complex scenarios, we also introduce a\nnew medical dataset with high PII density. Our framework achieves a new\nstate-of-the-art on the privacy-utility frontier, demonstrating the necessity\nof learned, adaptive policies for deploying LLMs in sensitive environments.",
    "updated" : "2025-10-16T19:38:36Z",
    "published" : "2025-10-16T19:38:36Z",
    "authors" : [
      {
        "name" : "Zheng Hui"
      },
      {
        "name" : "Yijiang River Dong"
      },
      {
        "name" : "Sanhanat Sivapiromrat"
      },
      {
        "name" : "Ehsan Shareghi"
      },
      {
        "name" : "Nigel Collier"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.18568v1",
    "title" : "Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with\n  Deep Learning and Blockchain",
    "summary" : "The integration of Internet of Things (IoT) devices in healthcare has\nrevolutionized patient care by enabling real-time monitoring, personalized\ntreatments, and efficient data management. However, this technological\nadvancement introduces significant security risks, particularly concerning the\nconfidentiality, integrity, and availability of sensitive medical data.\nTraditional security measures are often insufficient to address the unique\nchallenges posed by IoT environments, such as heterogeneity, resource\nconstraints, and the need for real-time processing. To tackle these challenges,\nwe propose a comprehensive three-phase security framework designed to enhance\nthe security and reliability of IoT-enabled healthcare systems. In the first\nphase, the framework assesses the reliability of IoT devices using a\nreputation-based trust estimation mechanism, which combines device behavior\nanalytics with off-chain data storage to ensure scalability. The second phase\nintegrates blockchain technology with a lightweight proof-of-work mechanism,\nensuring data immutability, secure communication, and resistance to\nunauthorized access. The third phase employs a lightweight Long Short-Term\nMemory (LSTM) model for anomaly detection and classification, enabling\nreal-time identification of cyber threats. Simulation results demonstrate that\nthe proposed framework outperforms existing methods, achieving a 2% increase in\nprecision, accuracy, and recall, a 5% higher attack detection rate, and a 3%\nreduction in false alarm rate. These improvements highlight the framework's\nability to address critical security concerns while maintaining scalability and\nreal-time performance.",
    "updated" : "2025-10-21T12:21:49Z",
    "published" : "2025-10-21T12:21:49Z",
    "authors" : [
      {
        "name" : "Behnam Rezaei Bezanjani"
      },
      {
        "name" : "Seyyed Hamid Ghafouri"
      },
      {
        "name" : "Reza Gholamrezaei"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.18493v1",
    "title" : "One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for\n  Customizable Privacy-Preserving Phone Scam Detection",
    "summary" : "Phone scams remain a pervasive threat to both personal safety and financial\nsecurity worldwide. Recent advances in large language models (LLMs) have\ndemonstrated strong potential in detecting fraudulent behavior by analyzing\ntranscribed phone conversations. However, these capabilities introduce notable\nprivacy risks, as such conversations frequently contain sensitive personal\ninformation that may be exposed to third-party service providers during\nprocessing. In this work, we explore how to harness LLMs for phone scam\ndetection while preserving user privacy. We propose MASK (Modular Adaptive\nSanitization Kit), a trainable and extensible framework that enables dynamic\nprivacy adjustment based on individual preferences. MASK provides a pluggable\narchitecture that accommodates diverse sanitization methods - from traditional\nkeyword-based techniques for high-privacy users to sophisticated neural\napproaches for those prioritizing accuracy. We also discuss potential modeling\napproaches and loss function designs for future development, enabling the\ncreation of truly personalized, privacy-aware LLM-based detection systems that\nbalance user trust and detection effectiveness, even beyond phone scam context.",
    "updated" : "2025-10-21T10:30:36Z",
    "published" : "2025-10-21T10:30:36Z",
    "authors" : [
      {
        "name" : "Kangzhong Wang"
      },
      {
        "name" : "Zitong Shen"
      },
      {
        "name" : "Youqian Zhang"
      },
      {
        "name" : "Michael MK Cheung"
      },
      {
        "name" : "Xiapu Luo"
      },
      {
        "name" : "Grace Ngai"
      },
      {
        "name" : "Eugene Yujun Fu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.HC",
      "68M25",
      "I.2.7"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.18379v1",
    "title" : "Uniformity Testing under User-Level Local Privacy",
    "summary" : "We initiate the study of distribution testing under \\emph{user-level} local\ndifferential privacy, where each of $n$ users contributes $m$ samples from the\nunknown underlying distribution. This setting, albeit very natural, is\nsignificantly more challenging that the usual locally private setting, as for\nthe same parameter $\\varepsilon$ the privacy guarantee must now apply to a full\nbatch of $m$ data points. While some recent work consider distribution\n\\emph{learning} in this user-level setting, nothing was known for even the most\nfundamental testing task, uniformity testing (and its generalization, identity\ntesting).\n  We address this gap, by providing (nearly) sample-optimal user-level LDP\nalgorithms for uniformity and identity testing. Motivated by practical\nconsiderations, our main focus is on the private-coin, symmetric setting, which\ndoes not require users to share a common random seed nor to have been assigned\na globally unique identifier.",
    "updated" : "2025-10-21T07:52:41Z",
    "published" : "2025-10-21T07:52:41Z",
    "authors" : [
      {
        "name" : "Clment L. Canonne"
      },
      {
        "name" : "Abigail Gentle"
      },
      {
        "name" : "Vikrant Singhal"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.DM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2510.18109v1",
    "title" : "PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data\n  Marketplaces",
    "summary" : "Evaluating the relevance of data is a critical task for model builders\nseeking to acquire datasets that enhance model performance. Ideally, such\nevaluation should allow the model builder to assess the utility of candidate\ndata without exposing proprietary details of the model. At the same time, data\nproviders must be assured that no information about their data - beyond the\ncomputed utility score - is disclosed to the model builder.\n  In this paper, we present PrivaDE, a cryptographic protocol for\nprivacy-preserving utility scoring and selection of data for machine learning.\nWhile prior works have proposed data evaluation protocols, our approach\nadvances the state of the art through a practical, blockchain-centric design.\nLeveraging the trustless nature of blockchains, PrivaDE enforces\nmalicious-security guarantees and ensures strong privacy protection for both\nmodels and datasets. To achieve efficiency, we integrate several techniques -\nincluding model distillation, model splitting, and cut-and-choose\nzero-knowledge proofs - bringing the runtime to a practical level. Furthermore,\nwe propose a unified utility scoring function that combines empirical loss,\npredictive entropy, and feature-space diversity, and that can be seamlessly\nintegrated into active-learning workflows. Evaluation shows that PrivaDE\nperforms data evaluation effectively, achieving online runtimes within 15\nminutes even for models with millions of parameters.\n  Our work lays the foundation for fair and automated data marketplaces in\ndecentralized machine learning ecosystems.",
    "updated" : "2025-10-20T21:14:32Z",
    "published" : "2025-10-20T21:14:32Z",
    "authors" : [
      {
        "name" : "Wan Ki Wong"
      },
      {
        "name" : "Sahel Torkamani"
      },
      {
        "name" : "Michele Ciampi"
      },
      {
        "name" : "Rik Sarkar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  }
]