[
  {
    "id" : "http://arxiv.org/abs/2511.02797v1",
    "title" : "Fast, Private, and Protected: Safeguarding Data Privacy and Defending\n  Against Model Poisoning Attacks in Federated Learning",
    "summary" : "Federated Learning (FL) is a distributed training paradigm wherein\nparticipants collaborate to build a global model while ensuring the privacy of\nthe involved data, which remains stored on participant devices. However,\nproposals aiming to ensure such privacy also make it challenging to protect\nagainst potential attackers seeking to compromise the training outcome. In this\ncontext, we present Fast, Private, and Protected (FPP), a novel approach that\naims to safeguard federated training while enabling secure aggregation to\npreserve data privacy. This is accomplished by evaluating rounds using\nparticipants' assessments and enabling training recovery after an attack. FPP\nalso employs a reputation-based mechanism to mitigate the participation of\nattackers. We created a dockerized environment to validate the performance of\nFPP compared to other approaches in the literature (FedAvg, Power-of-Choice,\nand aggregation via Trimmed Mean and Median). Our experiments demonstrate that\nFPP achieves a rapid convergence rate and can converge even in the presence of\nmalicious participants performing model poisoning attacks.",
    "updated" : "2025-11-04T18:20:45Z",
    "published" : "2025-11-04T18:20:45Z",
    "authors" : [
      {
        "name" : "Nicolas Riccieri Gardin Assumpcao"
      },
      {
        "name" : "Leandro Villas"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02785v1",
    "title" : "Enhancing Federated Learning Privacy with QUBO",
    "summary" : "Federated learning (FL) is a widely used method for training machine learning\n(ML) models in a scalable way while preserving privacy (i.e., without\ncentralizing raw data). Prior research shows that the risk of exposing\nsensitive data increases cumulatively as the number of iterations where a\nclient's updates are included in the aggregated model increase. Attackers can\nlaunch membership inference attacks (MIA; deciding whether a sample or client\nparticipated), property inference attacks (PIA; inferring attributes of a\nclient's data), and model inversion attacks (MI; reconstructing inputs),\nthereby inferring client-specific attributes and, in some cases, reconstructing\ninputs. In this paper, we mitigate risk by substantially reducing per client\nexposure using a quantum computing-inspired quadratic unconstrained binary\noptimization (QUBO) formulation that selects a small subset of client updates\nmost relevant for each training round. In this work, we focus on two threat\nvectors: (i) information leakage by clients during training and (ii)\nadversaries who can query or obtain the global model. We assume a trusted\ncentral server and do not model server compromise. This method also assumes\nthat the server has access to a validation/test set with global data\ndistribution. Experiments on the MNIST dataset with 300 clients in 20 rounds\nshowed a 95.2% per-round and 49% cumulative privacy exposure reduction, with\n147 clients' updates never being used during training while maintaining in\ngeneral the full-aggregation accuracy or even better. The method proved to be\nefficient at lower scale and more complex model as well. A CINIC-10\ndataset-based experiment with 30 clients resulted in 82% per-round privacy\nimprovement and 33% cumulative privacy.",
    "updated" : "2025-11-04T18:06:30Z",
    "published" : "2025-11-04T18:06:30Z",
    "authors" : [
      {
        "name" : "Andras Ferenczi"
      },
      {
        "name" : "Sutapa Samanta"
      },
      {
        "name" : "Dagen Wang"
      },
      {
        "name" : "Todd Hodges"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02297v1",
    "title" : "Two-Parameter RÃ©nyi Information Quantities with Applications to\n  Privacy Amplification and Soft Covering",
    "summary" : "There are no universally accepted definitions of R\\'enyi conditional entropy\nand R\\'enyi mutual information, although motivated by different applications,\nseveral definitions have been proposed in the literature. In this paper, we\nconsider a family of two-parameter R\\'enyi conditional entropy and a family of\ntwo-parameter R\\'enyi mutual information. By performing a change of variables\nfor the parameters, the two-parameter R\\'enyi conditional entropy we study\ncoincides precisely with the definition introduced by Hayashi and Tan [IEEE\nTrans. Inf. Theory, 2016], and it also emerges naturally as the classical\nspecialization of the three-parameter quantum R\\'enyi conditional entropy\nrecently put forward by Rubboli, Goodarzi, and Tomamichel [arXiv:2410.21976\n(2024)]. We establish several fundamental properties of the two-parameter\nR\\'enyi conditional entropy, including monotonicity with respect to the\nparameters and variational expression. The associated two-parameter R\\'enyi\nmutual information considered in this paper is new and it unifies three\ncommonly used variants of R\\'enyi mutual information. For this quantity, we\nprove several important properties, including the non-negativity, additivity,\ndata processing inequality, monotonicity with respect to the parameters,\nvariational expression, as well as convexity and concavity. Finally, we\ndemonstrate that these two-parameter R\\'enyi information quantities can be used\nto characterize the strong converse exponents in privacy amplification and soft\ncovering problems under R\\'enyi divergence of order $\\alpha \\in (0, \\infty)$.",
    "updated" : "2025-11-04T06:21:38Z",
    "published" : "2025-11-04T06:21:38Z",
    "authors" : [
      {
        "name" : "Shi-Bing Li"
      },
      {
        "name" : "Ke Li"
      },
      {
        "name" : "Lei Yu"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02283v1",
    "title" : "Distributed Nonconvex Optimization with Double Privacy Protection and\n  Exact Convergence",
    "summary" : "Motivated by the pervasive lack of privacy protection in existing distributed\nnonconvex optimization methods, this paper proposes a decentralized proximal\nprimal-dual algorithm enabling double protection of privacy ($\\text{DPP}^2$)\nfor minimizing nonconvex sum-utility functions over multi-agent networks, which\nensures zero leakage of critical local information during inter-agent\ncommunications. We develop a two-tier privacy protection mechanism that first\nmerges the primal and dual variables by means of a variable transformation,\nfollowed by embedding an additional random perturbation to further obfuscate\nthe transmitted information. We theoretically establish that $\\text{DPP}^2$\nensures differential privacy for local objectives while achieving exact\nconvergence under nonconvex settings. Specifically, $\\text{DPP}^2$ converges\nsublinearly to a stationary point and attains a linear convergence rate under\nthe additional Polyak-{\\L}ojasiewicz (P-{\\L}) condition. Finally, a numerical\nexample demonstrates the superiority of $\\text{DPP}^2$ over a number of\nstate-of-the-art algorithms, showcasing the faster, exact convergence achieved\nby $\\text{DPP}^2$ under the same level of differential privacy.",
    "updated" : "2025-11-04T05:51:34Z",
    "published" : "2025-11-04T05:51:34Z",
    "authors" : [
      {
        "name" : "Zichong Ou"
      },
      {
        "name" : "Dandan Wang"
      },
      {
        "name" : "Zixuan Liu"
      },
      {
        "name" : "Jie Lu"
      }
    ],
    "categories" : [
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02227v1",
    "title" : "Interval Estimation for Binomial Proportions Under Differential Privacy",
    "summary" : "When releasing binary proportions computed using sensitive data, several\ngovernment agencies and other data stewards protect confidentiality of the\nunderlying values by ensuring the released statistics satisfy differential\nprivacy. Typically, this is done by adding carefully chosen noise to the sample\nproportion computed using the confidential data. In this article, we describe\nand compare methods for turning this differentially private proportion into an\ninterval estimate for an underlying population probability. Specifically, we\nconsider differentially private versions of the Wald and Wilson intervals,\nBayesian credible intervals based on denoising the differentially private\nproportion, and an exact interval motivated by the Clopper-Pearson confidence\ninterval. We examine the repeated sampling performances of the intervals using\nsimulation studies under both the Laplace mechanism and discrete Gaussian\nmechanism across a range of privacy guarantees. We find that while several\nmethods can offer reasonable performances, the Bayesian credible intervals are\nthe most attractive.",
    "updated" : "2025-11-04T03:41:10Z",
    "published" : "2025-11-04T03:41:10Z",
    "authors" : [
      {
        "name" : "Hsuan-Chen Kao"
      },
      {
        "name" : "Jerome P. Reiter"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01752v1",
    "title" : "An assessment of the Commission's Proposal on Privacy and Electronic\n  Communications",
    "summary" : "This study, commissioned by the European Parliament's Policy Department for\nCitizens Rights and Constitutional Affairs at the request of the LIBE\nCommittee, appraises the European Commission's proposal for an ePrivacy\nRegulation. The study assesses whether the proposal would ensure that the right\nto the protection of personal data, the right to respect for private life and\ncommunications, and related rights enjoy a high standard of protection. The\nstudy also highlights the proposal's potential benefits and drawbacks more\ngenerally.",
    "updated" : "2025-11-03T17:01:35Z",
    "published" : "2025-11-03T17:01:35Z",
    "authors" : [
      {
        "name" : "Frederik Zuiderveen Borgesius"
      },
      {
        "name" : "Joris van Hoboken"
      },
      {
        "name" : "Ronan Fahy"
      },
      {
        "name" : "Kristina Irion"
      },
      {
        "name" : "Max Rozendaal"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01654v1",
    "title" : "Panther: A Cost-Effective Privacy-Preserving Framework for GNN Training\n  and Inference Services in Cloud Environments",
    "summary" : "Graph Neural Networks (GNNs) have marked significant impact in traffic state\nprediction, social recommendation, knowledge-aware question answering and so\non. As more and more users move towards cloud computing, it has become a\ncritical issue to unleash the power of GNNs while protecting the privacy in\ncloud environments. Specifically, the training data and inference data for GNNs\nneed to be protected from being stolen by external adversaries. Meanwhile, the\nfinancial cost of cloud computing is another primary concern for users.\nTherefore, although existing studies have proposed privacy-preserving\ntechniques for GNNs in cloud environments, their additional computational and\ncommunication overhead remain relatively high, causing high financial costs\nthat limit their widespread adoption among users.\n  To protect GNN privacy while lowering the additional financial costs, we\nintroduce Panther, a cost-effective privacy-preserving framework for GNN\ntraining and inference services in cloud environments. Technically, Panther\nleverages four-party computation to asynchronously executing the secure array\naccess protocol, and randomly pads the neighbor information of GNN nodes. We\nprove that Panther can protect privacy for both training and inference of GNN\nmodels. Our evaluation shows that Panther reduces the training and inference\ntime by an average of 75.28% and 82.80%, respectively, and communication\noverhead by an average of 52.61% and 50.26% compared with the state-of-the-art,\nwhich is estimated to save an average of 55.05% and 59.00% in financial costs\n(based on on-demand pricing model) for the GNN training and inference process\non Google Cloud Platform.",
    "updated" : "2025-11-03T15:15:40Z",
    "published" : "2025-11-03T15:15:40Z",
    "authors" : [
      {
        "name" : "Congcong Chen"
      },
      {
        "name" : "Xinyu Liu"
      },
      {
        "name" : "Kaifeng Huang"
      },
      {
        "name" : "Lifei Wei"
      },
      {
        "name" : "Yang Shi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01583v1",
    "title" : "Federated Cyber Defense: Privacy-Preserving Ransomware Detection Across\n  Distributed Systems",
    "summary" : "Detecting malware, especially ransomware, is essential to securing today's\ninterconnected ecosystems, including cloud storage, enterprise file-sharing,\nand database services. Training high-performing artificial intelligence (AI)\ndetectors requires diverse datasets, which are often distributed across\nmultiple organizations, making centralization necessary. However, centralized\nlearning is often impractical due to security, privacy regulations, data\nownership issues, and legal barriers to cross-organizational sharing.\nCompounding this challenge, ransomware evolves rapidly, demanding models that\nare both robust and adaptable.\n  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL\nplatform, which enables multiple organizations to collaboratively train a\nransomware detection model while keeping raw data local and secure. This\nparadigm is particularly relevant for cybersecurity companies (including both\nsoftware and hardware vendors) that deploy ransomware detection or firewall\nsystems across millions of endpoints. In such environments, data cannot be\ntransferred outside the customer's device due to strict security, privacy, or\nregulatory constraints. Although FL applies broadly to malware threats, we\nvalidate the approach using the Ransomware Storage Access Patterns (RanSAP)\ndataset.\n  Our experiments demonstrate that FL improves ransomware detection accuracy by\na relative 9% over server-local models and achieves performance comparable to\ncentralized training. These results indicate that FL offers a scalable,\nhigh-performing, and privacy-preserving framework for proactive ransomware\ndetection across organizational and regulatory boundaries.",
    "updated" : "2025-11-03T13:54:13Z",
    "published" : "2025-11-03T13:54:13Z",
    "authors" : [
      {
        "name" : "Daniel M. Jimenez-Gutierrez"
      },
      {
        "name" : "Enrique Zuazua"
      },
      {
        "name" : "Joaquin Del Rio"
      },
      {
        "name" : "Oleksii Sliusarenko"
      },
      {
        "name" : "Xabi Uribe-Etxebarria"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01467v1",
    "title" : "Quantum Blackwell's Ordering and Differential Privacy",
    "summary" : "We develop a framework for quantum differential privacy (QDP) based on\nquantum hypothesis testing and Blackwell's ordering. This approach\ncharacterizes $(\\eps,\\delta)$-QDP via hypothesis testing divergences and\nidentifies the most informative quantum state pairs under privacy constraints.\nWe apply this to analyze the stability of quantum learning algorithms,\ngeneralizing classical results to the case $\\delta>0$. Additionally, we study\nprivatized quantum parameter estimation, deriving tight bounds on the quantum\nFisher information under QDP. Finally, we establish near-optimal contraction\nbounds for differentially private quantum channels with respect to the\nhockey-stick divergence.",
    "updated" : "2025-11-03T11:24:52Z",
    "published" : "2025-11-03T11:24:52Z",
    "authors" : [
      {
        "name" : "Ayanava Dasgupta"
      },
      {
        "name" : "Naqueeb Ahmad Warsi"
      },
      {
        "name" : "Masahito Hayashi"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01449v1",
    "title" : "Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained\n  Fruit Quality Prediction",
    "summary" : "To effectively manage the wastage of perishable fruits, it is crucial to\naccurately predict their freshness or shelf life using non-invasive methods\nthat rely on visual data. In this regard, deep learning techniques can offer a\nviable solution. However, obtaining fine-grained fruit freshness labels from\nexperts is costly, leading to a scarcity of data. Closed proprietary Vision\nLanguage Models (VLMs), such as Gemini, have demonstrated strong performance in\nfruit freshness detection task in both zero-shot and few-shot settings.\nNonetheless, food retail organizations are unable to utilize these proprietary\nmodels due to concerns related to data privacy, while existing open-source VLMs\nyield sub-optimal performance for the task. Fine-tuning these open-source\nmodels with limited data fails to achieve the performance levels of proprietary\nmodels. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning\n(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes\nmeta-learning to address data sparsity and leverages label ordinality, thereby\nachieving state-of-the-art performance in the fruit freshness classification\ntask under both zero-shot and few-shot settings. Our method achieves an\nindustry-standard accuracy of 92.71%, averaged across all fruits.\n  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,\nOrdinal Regression",
    "updated" : "2025-11-03T11:03:54Z",
    "published" : "2025-11-03T11:03:54Z",
    "authors" : [
      {
        "name" : "Riddhi Jain"
      },
      {
        "name" : "Manasi Patwardhan"
      },
      {
        "name" : "Aayush Mishra"
      },
      {
        "name" : "Parijat Deshpande"
      },
      {
        "name" : "Beena Rai"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01307v1",
    "title" : "Perturb a Model, Not an Image: Towards Robust Privacy Protection via\n  Anti-Personalized Diffusion Models",
    "summary" : "Recent advances in diffusion models have enabled high-quality synthesis of\nspecific subjects, such as identities or objects. This capability, while\nunlocking new possibilities in content creation, also introduces significant\nprivacy risks, as personalization techniques can be misused by malicious users\nto generate unauthorized content. Although several studies have attempted to\ncounter this by generating adversarially perturbed samples designed to disrupt\npersonalization, they rely on unrealistic assumptions and become ineffective in\nthe presence of even a few clean images or under simple image transformations.\nTo address these challenges, we shift the protection target from the images to\nthe diffusion model itself to hinder the personalization of specific subjects,\nthrough our novel framework called Anti-Personalized Diffusion Models (APDM).\nWe first provide a theoretical analysis demonstrating that a naive approach of\nexisting loss functions to diffusion models is inherently incapable of ensuring\nconvergence for robust anti-personalization. Motivated by this finding, we\nintroduce Direct Protective Optimization (DPO), a novel loss function that\neffectively disrupts subject personalization in the target model without\ncompromising generative quality. Moreover, we propose a new dual-path\noptimization strategy, coined Learning to Protect (L2P). By alternating between\npersonalization and protection paths, L2P simulates future personalization\ntrajectories and adaptively reinforces protection at each step. Experimental\nresults demonstrate that our framework outperforms existing methods, achieving\nstate-of-the-art performance in preventing unauthorized personalization. The\ncode is available at https://github.com/KU-VGI/APDM.",
    "updated" : "2025-11-03T07:42:05Z",
    "published" : "2025-11-03T07:42:05Z",
    "authors" : [
      {
        "name" : "Tae-Young Lee"
      },
      {
        "name" : "Juwon Seo"
      },
      {
        "name" : "Jong Hwan Ko"
      },
      {
        "name" : "Gyeong-Moon Park"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01197v2",
    "title" : "CryptoMoE: Privacy-Preserving and Scalable Mixture of Experts Inference\n  via Balanced Expert Routing",
    "summary" : "Private large language model (LLM) inference based on cryptographic\nprimitives offers a promising path towards privacy-preserving deep learning.\nHowever, existing frameworks only support dense LLMs like LLaMA-1 and struggle\nto scale to mixture-of-experts (MoE) architectures. The key challenge comes\nfrom securely evaluating the dynamic routing mechanism in MoE layers, which may\nreveal sensitive input information if not fully protected. In this paper, we\npropose CryptoMoE, the first framework that enables private, efficient, and\naccurate inference for MoE-based models. CryptoMoE balances expert loads to\nprotect expert routing information and proposes novel protocols for secure\nexpert dispatch and combine. CryptoMoE also develops a confidence-aware token\nselection strategy and a batch matrix multiplication protocol to improve\naccuracy and efficiency further. Extensive experiments on DeepSeekMoE-16.4B,\nOLMoE-6.9B, and QWenMoE-14.3B show that CryptoMoE achieves $2.8\\sim3.5\\times$\nend-to-end latency reduction and $2.9\\sim4.3\\times$ communication reduction\nover a dense baseline with minimum accuracy loss. We also adapt CipherPrune\n(ICLR'25) for MoE inference and demonstrate CryptoMoE can reduce the\ncommunication by up to $4.3 \\times$. Code is available at:\nhttps://github.com/PKU-SEC-Lab/CryptoMoE.",
    "updated" : "2025-11-04T03:48:37Z",
    "published" : "2025-11-03T03:45:08Z",
    "authors" : [
      {
        "name" : "Yifan Zhou"
      },
      {
        "name" : "Tianshi Xu"
      },
      {
        "name" : "Jue Hong"
      },
      {
        "name" : "Ye Wu"
      },
      {
        "name" : "Meng Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00795v1",
    "title" : "FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated\n  Tumor Segmentation with Synthetic CT Data",
    "summary" : "Federated Learning (FL) allows multiple institutions to cooperatively train\nmachine learning models while retaining sensitive data at the source, which has\ngreat utility in privacy-sensitive environments. However, FL systems remain\nvulnerable to membership-inference attacks and data heterogeneity. This paper\npresents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using\nsynthetic oncologic CT scans with tumor annotations. It evaluates segmentation\nperformance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and\nFedAvg with DP-SGD. Results show a distinct trade-off between privacy and\nutility: FedAvg is high performance (Dice around 0.85) with more privacy\nleakage (attack AUC about 0.72), while DP-SGD provides a higher level of\nprivacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx\nand FedBN offer balanced performance under heterogeneous data, especially with\nnon-identical distributed client data. FedOnco-Bench serves as a standardized,\nopen-source platform for benchmarking and developing privacy-preserving FL\nmethods for medical image segmentation.",
    "updated" : "2025-11-02T04:17:14Z",
    "published" : "2025-11-02T04:17:14Z",
    "authors" : [
      {
        "name" : "Viswa Chaitanya Marella"
      },
      {
        "name" : "Suhasnadh Reddy Veluru"
      },
      {
        "name" : "Sai Teja Erukude"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00737v1",
    "title" : "EP-HDC: Hyperdimensional Computing with Encrypted Parameters for\n  High-Throughput Privacy-Preserving Inference",
    "summary" : "While homomorphic encryption (HE) provides strong privacy protection, its\nhigh computational cost has restricted its application to simple tasks.\nRecently, hyperdimensional computing (HDC) applied to HE has shown promising\nperformance for privacy-preserving machine learning (PPML). However, when\napplied to more realistic scenarios such as batch inference, the HDC-based HE\nhas still very high compute time as well as high encryption and data\ntransmission overheads. To address this problem, we propose HDC with encrypted\nparameters (EP-HDC), which is a novel PPML approach featuring client-side HE,\ni.e., inference is performed on a client using a homomorphically encrypted\nmodel. Our EP-HDC can effectively mitigate the encryption and data transmission\noverhead, as well as providing high scalability with many clients while\nproviding strong protection for user data and model parameters. In addition to\napplication examples for our client-side PPML, we also present design space\nexploration involving quantization, architecture, and HE-related parameters.\nOur experimental results using the BFV scheme and the Face/Emotion datasets\ndemonstrate that our method can improve throughput and latency of batch\ninference by orders of magnitude over previous PPML methods (36.52~1068x and\n6.45~733x, respectively) with less than 1% accuracy degradation.",
    "updated" : "2025-11-01T23:22:01Z",
    "published" : "2025-11-01T23:22:01Z",
    "authors" : [
      {
        "name" : "Jaewoo Park"
      },
      {
        "name" : "Chenghao Quan"
      },
      {
        "name" : "Jongeun Lee"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00700v1",
    "title" : "Privacy-Aware Time Series Synthesis via Public Knowledge Distillation",
    "summary" : "Sharing sensitive time series data in domains such as finance, healthcare,\nand energy consumption, such as patient records or investment accounts, is\noften restricted due to privacy concerns. Privacy-aware synthetic time series\ngeneration addresses this challenge by enforcing noise during training,\ninherently introducing a trade-off between privacy and utility. In many cases,\nsensitive sequences is correlated with publicly available, non-sensitive\ncontextual metadata (e.g., household electricity consumption may be influenced\nby weather conditions and electricity prices). However, existing privacy-aware\ndata generation methods often overlook this opportunity, resulting in\nsuboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a\nnovel framework for generating private time series data by leveraging\nheterogeneous public knowledge. Our model employs a self-attention mechanism to\nencode public data into temporal and feature embeddings, which serve as\nconditional inputs for a diffusion model to generate synthetic private\nsequences. Additionally, we introduce a practical metric to assess privacy by\nevaluating the identifiability of the synthetic data. Experimental results show\nthat Pub2Priv consistently outperforms state-of-the-art benchmarks in improving\nthe privacy-utility trade-off across finance, energy, and commodity trading\ndomains.",
    "updated" : "2025-11-01T20:44:24Z",
    "published" : "2025-11-01T20:44:24Z",
    "authors" : [
      {
        "name" : "Penghang Liu"
      },
      {
        "name" : "Haibei Zhu"
      },
      {
        "name" : "Eleonora Kreacic"
      },
      {
        "name" : "Svitlana Vyetrenko"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00487v1",
    "title" : "With Privacy, Size Matters: On the Importance of Dataset Size in\n  Differentially Private Text Rewriting",
    "summary" : "Recent work in Differential Privacy with Natural Language Processing (DP NLP)\nhas proposed numerous promising techniques in the form of text rewriting\nmechanisms. In the evaluation of these mechanisms, an often-ignored aspect is\nthat of dataset size, or rather, the effect of dataset size on a mechanism's\nefficacy for utility and privacy preservation. In this work, we are the first\nto introduce this factor in the evaluation of DP text privatization, where we\ndesign utility and privacy tests on large-scale datasets with dynamic split\nsizes. We run these tests on datasets of varying size with up to one million\ntexts, and we focus on quantifying the effect of increasing dataset size on the\nprivacy-utility trade-off. Our findings reveal that dataset size plays an\nintegral part in evaluating DP text rewriting mechanisms; additionally, these\nfindings call for more rigorous evaluation procedures in DP NLP, as well as\nshed light on the future of DP NLP in practice and at scale.",
    "updated" : "2025-11-01T10:41:05Z",
    "published" : "2025-11-01T10:41:05Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00467v1",
    "title" : "A Big Step Forward? A User-Centric Examination of iOS App Privacy Report\n  and Enhancements",
    "summary" : "The prevalent engagement with mobile apps underscores the importance of\nunderstanding their data practices. Transparency plays a crucial role in this\ncontext, ensuring users to be informed and give consent before any data access\noccurs. Apple introduced a new feature since iOS 15.2, App Privacy Report, to\ninform users about detailed insights into apps' data access and sharing. This\nfeature continues Apple's trend of privacy-focused innovations (following\nPrivacy Nutrition Labels), and has been marketed as a big step forward in user\nprivacy. However, its real-world impacts on user privacy and control remain\nunexamined. We thus proposed an end-to-end study involving systematic\nassessment of the App Privacy Report's real-world benefits and limitations,\nLLM-enabled and multi-technique synthesized enhancements, and comprehensive\nevaluation from both system and user perspectives. Through a structured focus\ngroup study with twelve everyday iOS users, we explored their experiences,\nunderstanding, and perceptions of the feature, suggesting its limited practical\nimpact resulting from missing important details. We identified two primary user\nconcerns: the clarity of data access purpose and domain description. In\nresponse, we proposed enhancements including a purpose inference framework and\ndomain clarification pipeline. We demonstrated the effectiveness and benefits\nof such enhancements for mobile app users. This work provides practical\ninsights that could help enhance user privacy transparency and discusses areas\nfor future research.",
    "updated" : "2025-11-01T09:29:04Z",
    "published" : "2025-11-01T09:29:04Z",
    "authors" : [
      {
        "name" : "Liu Wang"
      },
      {
        "name" : "Dong Wang"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Zheng Jiang"
      },
      {
        "name" : "Haoyu Wang"
      },
      {
        "name" : "Yi Wang"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00414v1",
    "title" : "Embedding based Encoding Scheme for Privacy Preserving Record Linkage",
    "summary" : "To discover new insights from data, there is a growing need to share\ninformation that is often held by different organisations. One key task in data\nintegration is the calculation of similarities between records in different\ndatabases to identify pairs or sets of records that correspond to the same\nreal-world entities. Due to privacy and confidentiality concerns, however, the\nowners of sensitive databases are often not allowed or willing to exchange or\nshare their data with other organisations to allow such similarity\ncalculations. Privacy-preserving record linkage (PPRL) is the process of\nmatching records that refer to the same entity across sensitive databases held\nby different organisations while ensuring no information about the entities is\nrevealed to the participating parties. In this paper, we study how embedding\nbased encoding techniques can be applied in the PPRL context to ensure the\nprivacy of the entities that are being linked. We first convert individual\nq-grams into the embedded space and then convert the embedding of a set of\nq-grams of a given record into a binary representation. The final binary\nrepresentations can be used to link records into matches and non-matches. We\nempirically evaluate our proposed encoding technique against different\nreal-world datasets. The results suggest that our proposed encoding approach\ncan provide better linkage accuracy and protect the privacy of individuals\nagainst attack compared to state-of-the-art techniques for short record values.",
    "updated" : "2025-11-01T05:57:21Z",
    "published" : "2025-11-01T05:57:21Z",
    "authors" : [
      {
        "name" : "Sirintra Vaiwsri"
      },
      {
        "name" : "Thilina Ranbaduge"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.03665v1",
    "title" : "A Lightweight 3D-CNN for Event-Based Human Action Recognition with\n  Privacy-Preserving Potential",
    "summary" : "This paper presents a lightweight three-dimensional convolutional neural\nnetwork (3DCNN) for human activity recognition (HAR) using event-based vision\ndata. Privacy preservation is a key challenge in human monitoring systems, as\nconventional frame-based cameras capture identifiable personal information. In\ncontrast, event cameras record only changes in pixel intensity, providing an\ninherently privacy-preserving sensing modality. The proposed network\neffectively models both spatial and temporal dynamics while maintaining a\ncompact design suitable for edge deployment. To address class imbalance and\nenhance generalization, focal loss with class reweighting and targeted data\naugmentation strategies are employed. The model is trained and evaluated on a\ncomposite dataset derived from the Toyota Smart Home and ETRI datasets.\nExperimental results demonstrate an F1-score of 0.9415 and an overall accuracy\nof 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D,\nand MC3_18 by up to 3%. These results highlight the potential of event-based\ndeep learning for developing accurate, efficient, and privacy-aware human\naction recognition systems suitable for real-world edge applications.",
    "updated" : "2025-11-05T17:30:31Z",
    "published" : "2025-11-05T17:30:31Z",
    "authors" : [
      {
        "name" : "Mehdi Sefidgar Dilmaghani"
      },
      {
        "name" : "Francis Fowley"
      },
      {
        "name" : "Peter Corcoran"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.03538v1",
    "title" : "Security and Privacy Management of IoT Using Quantum Computing",
    "summary" : "The convergence of the Internet of Things (IoT) and quantum computing is\nredefining the security paradigm of interconnected digital systems. Classical\ncryptographic algorithms such as RSA, Elliptic Curve Cryptography (ECC), and\nAdvanced Encryption Standard (AES) have long provided the foundation for\nsecuring IoT communication. However, the emergence of quantum algorithms such\nas Shor's and Grover's threatens to render these techniques vulnerable,\nnecessitating the development of quantum-resilient alternatives. This chapter\nexamines the implications of quantum computing for IoT security and explores\nstrategies for building cryptographically robust systems in the post-quantum\nera. It presents an overview of Post-Quantum Cryptographic (PQC) families,\nincluding lattice-based, code-based, hash-based, and multivariate approaches,\nanalyzing their potential for deployment in resource-constrained IoT\nenvironments. In addition, quantum-based methods such as Quantum Key\nDistribution (QKD) and Quantum Random Number Generators (QRNGs) are discussed\nfor their ability to enhance confidentiality and privacy through physics-based\nsecurity guarantees. The chapter also highlights issues of privacy management,\nregulatory compliance, and standardization, emphasizing the need for\ncollaborative efforts across academia, industry, and governance. Overall, it\nprovides a comprehensive perspective on security IoT ecosystems against quantum\nthreats and ensures resilience in the next generation of intelligent networks.",
    "updated" : "2025-11-05T15:08:55Z",
    "published" : "2025-11-05T15:08:55Z",
    "authors" : [
      {
        "name" : "Jaydip Sen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.03248v1",
    "title" : "Auditing M-LLMs for Privacy Risks: A Synthetic Benchmark and Evaluation\n  Framework",
    "summary" : "Recent advances in multi-modal Large Language Models (M-LLMs) have\ndemonstrated a powerful ability to synthesize implicit information from\ndisparate sources, including images and text. These resourceful data from\nsocial media also introduce a significant and underexplored privacy risk: the\ninference of sensitive personal attributes from seemingly daily media content.\nHowever, the lack of benchmarks and comprehensive evaluations of\nstate-of-the-art M-LLM capabilities hinders the research of private attribute\nprofiling on social media. Accordingly, we propose (1) PRISM, the first\nmulti-modal, multi-dimensional and fine-grained synthesized dataset\nincorporating a comprehensive privacy landscape and dynamic user history; (2)\nan Efficient evaluation framework that measures the cross-modal privacy\ninference capabilities of advanced M-LLM. Specifically, PRISM is a large-scale\nsynthetic benchmark designed to evaluate cross-modal privacy risks. Its key\nfeature is 12 sensitive attribute labels across a diverse set of multi-modal\nprofiles, which enables targeted privacy analysis. These profiles are generated\nvia a sophisticated LLM agentic workflow, governed by a prior distribution to\nensure they realistically mimic social media users. Additionally, we propose a\nMulti-Agent Inference Framework that leverages a pipeline of specialized LLMs\nto enhance evaluation capabilities. We evaluate the inference capabilities of\nsix leading M-LLMs (Qwen, Gemini, GPT-4o, GLM, Doubao, and Grok) on PRISM. The\ncomparison with human performance reveals that these MLLMs significantly\noutperform in accuracy and efficiency, highlighting the threat of potential\nprivacy risks and the urgent need for robust defenses.",
    "updated" : "2025-11-05T07:23:21Z",
    "published" : "2025-11-05T07:23:21Z",
    "authors" : [
      {
        "name" : "Junhao Li"
      },
      {
        "name" : "Jiahao Chen"
      },
      {
        "name" : "Zhou Feng"
      },
      {
        "name" : "Chunyi Zhou"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02993v1",
    "title" : "PrivyWave: Privacy-Aware Wireless Sensing of Heartbeat",
    "summary" : "Wireless sensing technologies can now detect heartbeats using radio frequency\nand acoustic signals, raising significant privacy concerns. Existing privacy\nsolutions either protect from all sensing systems indiscriminately preventing\nany utility or operate post-data collection, failing to enable selective access\nwhere authorized devices can monitor while unauthorized ones cannot. We present\na key-based physical obfuscation system, PrivyWave, that addresses this\nchallenge by generating controlled decoy heartbeat signals at\ncryptographically-determined frequencies. Unauthorized sensors receive a\nmixture of real and decoy signals that are indistinguishable without the secret\nkey, while authorized sensors use the key to filter out decoys and recover\naccurate measurements. Our evaluation with 13 participants demonstrates\neffective protection across both sensing modalities: for mmWave radar,\nunauthorized sensors show 21.3 BPM mean absolute error while authorized sensors\nmaintain a much smaller 5.8 BPM; for acoustic sensing, unauthorized error\nincreases to 42.0 BPM while authorized sensors achieve 9.7 BPM. The system\noperates across multiple sensing modalities without per-modality customization\nand provides cryptographic obfuscation guarantees. Performance benchmarks show\nrobust protection across different distances (30-150 cm), orientations\n(120{\\deg} field of view), and diverse indoor environments, establishing\nphysical-layer obfuscation as a viable approach for selective privacy in\npervasive health monitoring.",
    "updated" : "2025-11-04T20:54:59Z",
    "published" : "2025-11-04T20:54:59Z",
    "authors" : [
      {
        "name" : "Yixuan Gao"
      },
      {
        "name" : "Tanvir Ahmed"
      },
      {
        "name" : "Zekun Chang"
      },
      {
        "name" : "Thijs Roumen"
      },
      {
        "name" : "Rajalakshmi Nandakumar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02785v1",
    "title" : "Enhancing Federated Learning Privacy with QUBO",
    "summary" : "Federated learning (FL) is a widely used method for training machine learning\n(ML) models in a scalable way while preserving privacy (i.e., without\ncentralizing raw data). Prior research shows that the risk of exposing\nsensitive data increases cumulatively as the number of iterations where a\nclient's updates are included in the aggregated model increase. Attackers can\nlaunch membership inference attacks (MIA; deciding whether a sample or client\nparticipated), property inference attacks (PIA; inferring attributes of a\nclient's data), and model inversion attacks (MI; reconstructing inputs),\nthereby inferring client-specific attributes and, in some cases, reconstructing\ninputs. In this paper, we mitigate risk by substantially reducing per client\nexposure using a quantum computing-inspired quadratic unconstrained binary\noptimization (QUBO) formulation that selects a small subset of client updates\nmost relevant for each training round. In this work, we focus on two threat\nvectors: (i) information leakage by clients during training and (ii)\nadversaries who can query or obtain the global model. We assume a trusted\ncentral server and do not model server compromise. This method also assumes\nthat the server has access to a validation/test set with global data\ndistribution. Experiments on the MNIST dataset with 300 clients in 20 rounds\nshowed a 95.2% per-round and 49% cumulative privacy exposure reduction, with\n147 clients' updates never being used during training while maintaining in\ngeneral the full-aggregation accuracy or even better. The method proved to be\nefficient at lower scale and more complex model as well. A CINIC-10\ndataset-based experiment with 30 clients resulted in 82% per-round privacy\nimprovement and 33% cumulative privacy.",
    "updated" : "2025-11-04T18:06:30Z",
    "published" : "2025-11-04T18:06:30Z",
    "authors" : [
      {
        "name" : "Andras Ferenczi"
      },
      {
        "name" : "Sutapa Samanta"
      },
      {
        "name" : "Dagen Wang"
      },
      {
        "name" : "Todd Hodges"
      }
    ],
    "categories" : [
      "cs.LG",
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02227v2",
    "title" : "Interval Estimation for Binomial Proportions Under Differential Privacy",
    "summary" : "When releasing binary proportions computed using sensitive data, several\ngovernment agencies and other data stewards protect confidentiality of the\nunderlying values by ensuring the released statistics satisfy differential\nprivacy. Typically, this is done by adding carefully chosen noise to the sample\nproportion computed using the confidential data. In this article, we describe\nand compare methods for turning this differentially private proportion into an\ninterval estimate for an underlying population probability. Specifically, we\nconsider differentially private versions of the Wald and Wilson intervals,\nBayesian credible intervals based on denoising the differentially private\nproportion, and an exact interval motivated by the Clopper-Pearson confidence\ninterval. We examine the repeated sampling performances of the intervals using\nsimulation studies under both the Laplace mechanism and discrete Gaussian\nmechanism across a range of privacy guarantees. We find that while several\nmethods can offer reasonable performances, the Bayesian credible intervals are\nthe most attractive.",
    "updated" : "2025-11-05T16:55:15Z",
    "published" : "2025-11-04T03:41:10Z",
    "authors" : [
      {
        "name" : "Hsuan-Chen Kao"
      },
      {
        "name" : "Jerome P. Reiter"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.04438v1",
    "title" : "Limiting one-way distillable secret key via privacy testing of\n  extendible states",
    "summary" : "The notions of privacy tests and $k$-extendible states have both been\ninstrumental in quantum information theory, particularly in understanding the\nlimits of secure communication. In this paper, we determine the maximum\nprobability with which an arbitrary $k$-extendible state can pass a privacy\ntest, and we prove that it is equal to the maximum fidelity between an\narbitrary $k$-extendible state and the standard maximally entangled state. Our\nfindings, coupled with the resource theory of $k$-unextendibility, lead to an\nefficiently computable upper bound on the one-shot, one-way distillable key of\na bipartite state, and we prove that it is equal to the best-known efficiently\ncomputable upper bound on the one-shot, one-way distillable entanglement. We\nalso establish efficiently computable upper bounds on the one-shot,\nforward-assisted private capacity of channels. Extending our formalism to the\nindependent and identically distributed setting, we obtain single-letter\nefficiently computable bounds on the $n$-shot, one-way distillable key of a\nstate and the $n$-shot, forward-assisted private capacity of a channel. For\nsome key examples of interest, our bounds are significantly tighter than other\nknown efficiently computable bounds.",
    "updated" : "2025-11-06T15:11:54Z",
    "published" : "2025-11-06T15:11:54Z",
    "authors" : [
      {
        "name" : "Vishal Singh"
      },
      {
        "name" : "Karol Horodecki"
      },
      {
        "name" : "Aby Philip"
      },
      {
        "name" : "Mark M. Wilde"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.04261v1",
    "title" : "A Parallel Region-Adaptive Differential Privacy Framework for Image\n  Pixelization",
    "summary" : "The widespread deployment of high-resolution visual sensing systems, coupled\nwith the rise of foundation models, has amplified privacy risks in video-based\napplications. Differentially private pixelization offers mathematically\nguaranteed protection for visual data through grid-based noise addition, but\nchallenges remain in preserving task-relevant fidelity, achieving scalability,\nand enabling efficient real-time deployment. To address this, we propose a\nnovel parallel, region-adaptive pixelization framework that combines the\ntheoretical rigor of differential privacy with practical efficiency. Our method\nadaptively adjusts grid sizes and noise scales based on regional complexity,\nleveraging GPU parallelism to achieve significant runtime acceleration compared\nto the classical baseline. A lightweight storage scheme is introduced by\nretaining only essential noisy statistics, significantly reducing space\noverhead. Formal privacy analysis is provided under the Laplace mechanism and\nparallel composition theorem. Extensive experiments on the PETS, Venice-2, and\nPPM-100 datasets demonstrate favorable privacy-utility trade-offs and\nsignificant runtime/storage reductions. A face re-identification attack\nexperiment on CelebA further confirms the method's effectiveness in preventing\nidentity inference. This validates its suitability for real-time\nprivacy-critical applications such as elderly care, smart home monitoring,\ndriver behavior analysis, and crowd behavior monitoring.",
    "updated" : "2025-11-06T10:51:20Z",
    "published" : "2025-11-06T10:51:20Z",
    "authors" : [
      {
        "name" : "Ming Liu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.03966v1",
    "title" : "PrivacyCD: Hierarchical Unlearning for Protecting Student Privacy in\n  Cognitive Diagnosis",
    "summary" : "The need to remove specific student data from cognitive diagnosis (CD) models\nhas become a pressing requirement, driven by users' growing assertion of their\n\"right to be forgotten\". However, existing CD models are largely designed\nwithout privacy considerations and lack effective data unlearning mechanisms.\nDirectly applying general purpose unlearning algorithms is suboptimal, as they\nstruggle to balance unlearning completeness, model utility, and efficiency when\nconfronted with the unique heterogeneous structure of CD models. To address\nthis, our paper presents the first systematic study of the data unlearning\nproblem for CD models, proposing a novel and efficient algorithm: hierarchical\nimportanceguided forgetting (HIF). Our key insight is that parameter importance\nin CD models exhibits distinct layer wise characteristics. HIF leverages this\nvia an innovative smoothing mechanism that combines individual and layer, level\nimportance, enabling a more precise distinction of parameters associated with\nthe data to be unlearned. Experiments on three real world datasets show that\nHIF significantly outperforms baselines on key metrics, offering the first\neffective solution for CD models to respond to user data removal requests and\nfor deploying high-performance, privacy preserving AI systems",
    "updated" : "2025-11-06T01:39:59Z",
    "published" : "2025-11-06T01:39:59Z",
    "authors" : [
      {
        "name" : "Mingliang Hou"
      },
      {
        "name" : "Yinuo Wang"
      },
      {
        "name" : "Teng Guo"
      },
      {
        "name" : "Zitao Liu"
      },
      {
        "name" : "Wenzhou Dou"
      },
      {
        "name" : "Jiaqi Zheng"
      },
      {
        "name" : "Renqiang Luo"
      },
      {
        "name" : "Mi Tian"
      },
      {
        "name" : "Weiqi Luo"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.03753v1",
    "title" : "Federated Learning with Gramian Angular Fields for Privacy-Preserving\n  ECG Classification on Heterogeneous IoT Devices",
    "summary" : "This study presents a federated learning (FL) framework for\nprivacy-preserving electrocardiogram (ECG) classification in Internet of Things\n(IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian\nAngular Field (GAF) images, the proposed approach enables efficient feature\nextraction through Convolutional Neural Networks (CNNs) while ensuring that\nsensitive medical data remain local to each device. This work is among the\nfirst to experimentally validate GAF-based federated ECG classification across\nheterogeneous IoT devices, quantifying both performance and communication\nefficiency. To evaluate feasibility in realistic IoT settings, we deployed the\nframework across a server, a laptop, and a resource-constrained Raspberry Pi 4,\nreflecting edge-cloud integration in IoT ecosystems. Experimental results\ndemonstrate that the FL-GAF model achieves a high classification accuracy of\n95.18% in a multi-client setup, significantly outperforming a single-client\nbaseline in both accuracy and training time. Despite the added computational\ncomplexity of GAF transformations, the framework maintains efficient resource\nutilization and communication overhead. These findings highlight the potential\nof lightweight, privacy-preserving AI for IoT-based healthcare monitoring,\nsupporting scalable and secure edge deployments in smart health systems.",
    "updated" : "2025-11-04T22:23:59Z",
    "published" : "2025-11-04T22:23:59Z",
    "authors" : [
      {
        "name" : "Youssef Elmir"
      },
      {
        "name" : "Yassine Himeur"
      },
      {
        "name" : "Abbes Amira"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02993v2",
    "title" : "PrivyWave: Privacy-Aware Wireless Sensing of Heartbeat",
    "summary" : "Wireless sensing technologies can now detect heartbeats using radio frequency\nand acoustic signals, raising significant privacy concerns. Existing privacy\nsolutions either protect from all sensing systems indiscriminately preventing\nany utility or operate post-data collection, failing to enable selective access\nwhere authorized devices can monitor while unauthorized ones cannot. We present\na key-based physical obfuscation system, PrivyWave, that addresses this\nchallenge by generating controlled decoy heartbeat signals at\ncryptographically-determined frequencies. Unauthorized sensors receive a\nmixture of real and decoy signals that are indistinguishable without the secret\nkey, while authorized sensors use the key to filter out decoys and recover\naccurate measurements. Our evaluation with 13 participants demonstrates\neffective protection across both sensing modalities: for mmWave radar,\nunauthorized sensors show 21.3 BPM mean absolute error while authorized sensors\nmaintain a much smaller 5.8 BPM; for acoustic sensing, unauthorized error\nincreases to 42.0 BPM while authorized sensors achieve 9.7 BPM. The system\noperates across multiple sensing modalities without per-modality customization\nand provides cryptographic obfuscation guarantees. Performance benchmarks show\nrobust protection across different distances (30-150 cm), orientations\n(120{\\deg} field of view), and diverse indoor environments, establishing\nphysical-layer obfuscation as a viable approach for selective privacy in\npervasive health monitoring.",
    "updated" : "2025-11-06T02:34:25Z",
    "published" : "2025-11-04T20:54:59Z",
    "authors" : [
      {
        "name" : "Yixuan Gao"
      },
      {
        "name" : "Tanvir Ahmed"
      },
      {
        "name" : "Zekun Chang"
      },
      {
        "name" : "Thijs Roumen"
      },
      {
        "name" : "Rajalakshmi Nandakumar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.05327v1",
    "title" : "Privacy-Preserving CramÃ©r-Rao Lower Bound",
    "summary" : "This paper establishes the privacy-preserving Cram\\'er-Rao (CR) lower bound\ntheory, characterizing the fundamental limit of identification accuracy under\nprivacy constraint. An identifiability criterion under privacy constraint is\nderived by using Fisher information matrix as the privacy metric. In the\nidentifiable case, the privacy-preserving CR lower bound is established and its\nattainability is demonstrated, thereby ensuring the existence of the\nprivacy-preserving Fisher information matrix with explicit expression. Then,\nthe privacy-preserving CR lower bound theory is extended to the multi-sensor\nmulti-measurement system. Specifically, the additivity principle of\nprivacy-preserving Fisher information matrices across both spatial and temporal\ndimensions is established, building a relationship between privacy-preserving\nCR lower bounds for the multi-sensor multi-measurement system and its\nsubsystems. Using this additivity principle, distributed identification\nalgorithms capable of achieving the privacy-preserving CR lower bound are\nfurther proposed. Numerical examples are provided to demonstrate the\nprivacy-preserving CR lower bound and show the effectiveness of the proposed\nalgorithms.",
    "updated" : "2025-11-07T15:26:19Z",
    "published" : "2025-11-07T15:26:19Z",
    "authors" : [
      {
        "name" : "Jieming Ke"
      },
      {
        "name" : "Jimin Wang"
      },
      {
        "name" : "Ji-Feng Zhang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.05092v1",
    "title" : "A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person\n  Re-Identification",
    "summary" : "With growing concerns over data privacy, researchers have started using\nvirtual data as an alternative to sensitive real-world images for training\nperson re-identification (Re-ID) models. However, existing virtual datasets\nproduced by game engines still face challenges such as complex construction and\npoor domain generalization, making them difficult to apply in real scenarios.\nTo address these challenges, we propose a Dual-stage Prompt-driven\nPrivacy-preserving Paradigm (DPPP). In the first stage, we generate rich\nprompts incorporating multi-dimensional attributes such as pedestrian\nappearance, illumination, and viewpoint that drive the diffusion model to\nsynthesize diverse data end-to-end, building a large-scale virtual dataset\nnamed GenePerson with 130,519 images of 6,641 identities. In the second stage,\nwe propose a Prompt-driven Disentanglement Mechanism (PDM) to learn\ndomain-invariant generalization features. With the aid of contrastive learning,\nwe employ two textual inversion networks to map images into pseudo-words\nrepresenting style and content, respectively, thereby constructing\nstyle-disentangled content prompts to guide the model in learning\ndomain-invariant content features at the image level. Experiments demonstrate\nthat models trained on GenePerson with PDM achieve state-of-the-art\ngeneralization performance, surpassing those on popular real and virtual Re-ID\ndatasets.",
    "updated" : "2025-11-07T09:17:48Z",
    "published" : "2025-11-07T09:17:48Z",
    "authors" : [
      {
        "name" : "Ruolin Li"
      },
      {
        "name" : "Min Liu"
      },
      {
        "name" : "Yuan Bian"
      },
      {
        "name" : "Zhaoyang Li"
      },
      {
        "name" : "Yuzhen Li"
      },
      {
        "name" : "Xueping Wang"
      },
      {
        "name" : "Yaonan Wang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.10516v1",
    "title" : "How Worrying Are Privacy Attacks Against Machine Learning?",
    "summary" : "In several jurisdictions, the regulatory framework on the release and sharing of personal data is being extended to machine learning (ML). The implicit assumption is that disclosing a trained ML model entails a privacy risk for any personal data used in training comparable to directly releasing those data. However, given a trained model, it is necessary to mount a privacy attack to make inferences on the training data. In this concept paper, we examine the main families of privacy attacks against predictive and generative ML, including membership inference attacks (MIAs), property inference attacks, and reconstruction attacks. Our discussion shows that most of these attacks seem less effective in the real world than what a prima face interpretation of the related literature could suggest.",
    "updated" : "2025-11-13T17:22:07Z",
    "published" : "2025-11-13T17:22:07Z",
    "authors" : [
      {
        "name" : "Josep Domingo-Ferrer"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.10423v1",
    "title" : "Enhanced Privacy Leakage from Noise-Perturbed Gradients via Gradient-Guided Conditional Diffusion Models",
    "summary" : "Federated learning synchronizes models through gradient transmission and aggregation. However, these gradients pose significant privacy risks, as sensitive training data is embedded within them. Existing gradient inversion attacks suffer from significantly degraded reconstruction performance when gradients are perturbed by noise-a common defense mechanism. In this paper, we introduce Gradient-Guided Conditional Diffusion Models (GG-CDMs) for reconstructing private images from leaked gradients without prior knowledge of the target data distribution. Our approach leverages the inherent denoising capability of diffusion models to circumvent the partial protection offered by noise perturbation, thereby improving attack performance under such defenses. We further provide a theoretical analysis of the reconstruction error bounds and the convergence properties of attack loss, characterizing the impact of key factors-such as noise magnitude and attacked model architecture-on reconstruction quality. Extensive experiments demonstrate our attack's superior reconstruction performance with Gaussian noise-perturbed gradients, and confirm our theoretical findings.",
    "updated" : "2025-11-13T15:43:45Z",
    "published" : "2025-11-13T15:43:45Z",
    "authors" : [
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Hong Chen"
      },
      {
        "name" : "Chen Hou"
      },
      {
        "name" : "Guolong Zheng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.10284v1",
    "title" : "Beyond Verification: Abductive Explanations for Post-AI Assessment of Privacy Leakage",
    "summary" : "Privacy leakage in AI-based decision processes poses significant risks, particularly when sensitive information can be inferred. We propose a formal framework to audit privacy leakage using abductive explanations, which identifies minimal sufficient evidence justifying model decisions and determines whether sensitive information disclosed. Our framework formalizes both individual and system-level leakage, introducing the notion of Potentially Applicable Explanations (PAE) to identify individuals whose outcomes can shield those with sensitive features. This approach provides rigorous privacy guarantees while producing human understandable explanations, a key requirement for auditing tools. Experimental evaluation on the German Credit Dataset illustrates how the importance of sensitive literal in the model decision process affects privacy leakage. Despite computational challenges and simplifying assumptions, our results demonstrate that abductive reasoning enables interpretable privacy auditing, offering a practical pathway to reconcile transparency, model interpretability, and privacy preserving in AI decision-making.",
    "updated" : "2025-11-13T13:14:24Z",
    "published" : "2025-11-13T13:14:24Z",
    "authors" : [
      {
        "name" : "Belona Sonna"
      },
      {
        "name" : "Alban Grastien"
      },
      {
        "name" : "Claire Benn"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.10226v1",
    "title" : "Privacy Structure and Blackwell Frontier",
    "summary" : "This paper characterizes the set of feasible posterior distributions subject to graph-based inferential privacy constraint, including both differential and inferential privacy. This characterization can be done through enumerating all extreme points of the feasible posterior set. A connection between extreme posteriors and strongly connected semi-chains is then established. All these semi-chains can be constructed through successive unfolding operations on semi-chains with two partitions, which can be constructed through classical spanning tree algorithm. A sharper characterization of semi-chains with two partitions for differential privacy is provided.",
    "updated" : "2025-11-13T11:56:02Z",
    "published" : "2025-11-13T11:56:02Z",
    "authors" : [
      {
        "name" : "Zhang Xu"
      },
      {
        "name" : "Wei Zhao"
      }
    ],
    "categories" : [
      "econ.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.10001v1",
    "title" : "Mailing address aliasing as a method to protect consumer privacy",
    "summary" : "During online commerce, a customer will typically share his or her mailing address with a merchant to allow product delivery. This creates privacy risks for the customer, where the information may be misused, sold, or leaked by multiple merchants. While physical and virtual PO boxes can reduce the privacy risk, these solutions have associated costs that prevent greater adoption. Here, we introduce the concept of mailing address aliasing, which may offer lower cost and greater control in some cases. With this approach, an alias address is created that maps to the customer's true address. The mapping is kept private from the merchant but shared with the carrier. We discuss the advantages and disadvantages of this approach compared with traditional methods for mailing address privacy. We find that mailing address aliasing is likely to reduce unsolicited mail to a greater extent than physical or virtual PO boxes. However, mailing address aliasing may not be compatible with all merchants' ordering systems.",
    "updated" : "2025-11-13T06:12:07Z",
    "published" : "2025-11-13T06:12:07Z",
    "authors" : [
      {
        "name" : "Greg Hather"
      },
      {
        "name" : "Daniel Aranki"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.09882v1",
    "title" : "Truth, Justice, and Secrecy: Cake Cutting Under Privacy Constraints",
    "summary" : "Cake-cutting algorithms, which aim to fairly allocate a continuous resource based on individual agent preferences, have seen significant progress over the past two decades. Much of the research has concentrated on fairness, with comparatively less attention given to other important aspects. Chen et al. (2010) introduced an algorithm that, in addition to ensuring fairness, was strategyproof -- meaning agents had no incentive to misreport their valuations. However, even in the absence of strategic incentives to misreport, agents may still hesitate to reveal their true preferences due to privacy concerns (e.g., when allocating advertising time between firms, revealing preferences could inadvertently expose planned marketing strategies or product launch timelines). In this work, we extend the strategyproof algorithm of Chen et al. by introducing a privacy-preserving dimension. To the best of our knowledge, we present the first private cake-cutting protocol, and, in addition, this protocol is also envy-free and strategyproof. Our approach replaces the algorithm's centralized computation with a novel adaptation of cryptographic techniques, enabling privacy without compromising fairness or strategyproofness. Thus, our protocol encourages agents to report their true preferences not only because they are not incentivized to lie, but also because they are protected from having their preferences exposed.",
    "updated" : "2025-11-13T02:28:01Z",
    "published" : "2025-11-13T02:28:01Z",
    "authors" : [
      {
        "name" : "Yaron Salman"
      },
      {
        "name" : "Tamir Tassa"
      },
      {
        "name" : "Omer Lev"
      },
      {
        "name" : "Roie Zivan"
      }
    ],
    "categories" : [
      "cs.GT",
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.09846v1",
    "title" : "Real-Time Lightweight Gaze Privacy-Preservation Techniques Validated via Offline Gaze-Based Interaction Simulation",
    "summary" : "This study examines the effectiveness of the real-time privacy-preserving techniques through an offline gaze-based interaction simulation framework. Those techniques aim to reduce the amount of identity-related information in eye-tracking data while improving the efficacy of the gaze-based interaction. Although some real-time gaze privatization methods were previously explored, their validation on the large dataset was not conducted. We propose a functional framework that allows to study the efficacy of real-time gaze privatization on an already collected offline dataset. The key metric used to assess the reduction of identity-related information is the identification rate, while improvements in gaze-based interactions are evaluated through signal quality during interaction. Our additional contribution is the employment of an extremely lightweight Kalman filter framework that reduces the amount of identity-related information in the gaze signal and improves gaze-based interaction performance.",
    "updated" : "2025-11-13T01:01:34Z",
    "published" : "2025-11-13T01:01:34Z",
    "authors" : [
      {
        "name" : "Mehedi Hasan Raju"
      },
      {
        "name" : "Oleg V. Komogortsev"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.09775v1",
    "title" : "Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization",
    "summary" : "The widespread integration of Artificial Intelligence of Things (AIoT) in smart home environments has amplified the demand for transparent and interpretable machine learning models. To foster user trust and comply with emerging regulatory frameworks, the Explainable AI (XAI) methods, particularly post-hoc techniques such as SHapley Additive exPlanations (SHAP), and Local Interpretable Model-Agnostic Explanations (LIME), are widely employed to elucidate model behavior. However, recent studies have shown that these explanation methods can inadvertently expose sensitive user attributes and behavioral patterns, thereby introducing new privacy risks. To address these concerns, we propose a novel privacy-preserving approach based on SHAP entropy regularization to mitigate privacy leakage in explainable AIoT applications. Our method incorporates an entropy-based regularization objective that penalizes low-entropy SHAP attribution distributions during training, promoting a more uniform spread of feature contributions. To evaluate the effectiveness of our approach, we developed a suite of SHAP-based privacy attacks that strategically leverage model explanation outputs to infer sensitive information. We validate our method through comparative evaluations using these attacks alongside utility metrics on benchmark smart home energy consumption datasets. Experimental results demonstrate that SHAP entropy regularization substantially reduces privacy leakage compared to baseline models, while maintaining high predictive accuracy and faithful explanation fidelity. This work contributes to the development of privacy-preserving explainable AI techniques for secure and trustworthy AIoT applications.",
    "updated" : "2025-11-12T22:13:32Z",
    "published" : "2025-11-12T22:13:32Z",
    "authors" : [
      {
        "name" : "Dilli Prasad Sharma"
      },
      {
        "name" : "Xiaowei Sun"
      },
      {
        "name" : "Liang Xue"
      },
      {
        "name" : "Xiaodong Lin"
      },
      {
        "name" : "Pulei Xiong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.09696v1",
    "title" : "Cooperative Local Differential Privacy: Securing Time Series Data in Distributed Environments",
    "summary" : "The rapid growth of smart devices such as phones, wearables, IoT sensors, and connected vehicles has led to an explosion of continuous time series data that offers valuable insights in healthcare, transportation, and more. However, this surge raises significant privacy concerns, as sensitive patterns can reveal personal details. While traditional differential privacy (DP) relies on trusted servers, local differential privacy (LDP) enables users to perturb their own data. However, traditional LDP methods perturb time series data by adding user-specific noise but exhibit vulnerabilities. For instance, noise applied within fixed time windows can be canceled during aggregation (e.g., averaging), enabling adversaries to infer individual statistics over time, thereby eroding privacy guarantees.\n  To address these issues, we introduce a Cooperative Local Differential Privacy (CLDP) mechanism that enhances privacy by distributing noise vectors across multiple users. In our approach, noise is collaboratively generated and assigned so that when all users' perturbed data is aggregated, the noise cancels out preserving overall statistical properties while protecting individual privacy. This cooperative strategy not only counters vulnerabilities inherent in time-window-based methods but also scales effectively for large, real-time datasets, striking a better balance between data utility and privacy in multiuser environments.",
    "updated" : "2025-11-12T19:52:03Z",
    "published" : "2025-11-12T19:52:03Z",
    "authors" : [
      {
        "name" : "Bikash Chandra Singh"
      },
      {
        "name" : "Md Jakir Hossain"
      },
      {
        "name" : "Rafael Diaz"
      },
      {
        "name" : "Sandip Roy"
      },
      {
        "name" : "Ravi Mukkamala"
      },
      {
        "name" : "Sachin Shetty"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.09400v1",
    "title" : "Abstract Gradient Training: A Unified Certification Framework for Data Poisoning, Unlearning, and Differential Privacy",
    "summary" : "The impact of inference-time data perturbation (e.g., adversarial attacks) has been extensively studied in machine learning, leading to well-established certification techniques for adversarial robustness. In contrast, certifying models against training data perturbations remains a relatively under-explored area. These perturbations can arise in three critical contexts: adversarial data poisoning, where an adversary manipulates training samples to corrupt model performance; machine unlearning, which requires certifying model behavior under the removal of specific training data; and differential privacy, where guarantees must be given with respect to substituting individual data points. This work introduces Abstract Gradient Training (AGT), a unified framework for certifying robustness of a given model and training procedure to training data perturbations, including bounded perturbations, the removal of data points, and the addition of new samples. By bounding the reachable set of parameters, i.e., establishing provable parameter-space bounds, AGT provides a formal approach to analyzing the behavior of models trained via first-order optimization methods.",
    "updated" : "2025-11-12T15:15:15Z",
    "published" : "2025-11-12T15:15:15Z",
    "authors" : [
      {
        "name" : "Philip Sosnin"
      },
      {
        "name" : "Matthew Wicker"
      },
      {
        "name" : "Josh Collyer"
      },
      {
        "name" : "Calvin Tsay"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.09094v1",
    "title" : "RIS-based Communication Enhancement and Location Privacy Protection in UAV Networks",
    "summary" : "With the explosive advancement of unmanned aerial vehicles (UAVs), the security of efficient UAV networks has become increasingly critical. Owing to the open nature of its communication environment, illegitimate malicious UAVs (MUs) can infer the position of the source UAV (SU) by analyzing received signals, thus compromising the SU location privacy. To protect the SU location privacy while ensuring efficient communication with legitimate receiving UAVs (RUs), we propose an Active Reconfigurable Intelligent Surface (ARIS)-assisted covert communication scheme based on virtual partitioning and artificial noise (AN). Specifically, we design a novel ARIS architecture integrated with an AN module. This architecture dynamically partitions its reflecting elements into multiple sub-regions: one subset is optimized to enhance the communication rate between the SU and RUs, while the other subset generates AN to interfere with the localization of the SU by MUs. We first derive the CramÃ©r-Rao Lower Bound (CRLB) for the localization with received signal strength (RSS), based on which, we establish a joint optimization framework for communication enhancement and localization interference. Subsequently, we derive and validate the optimal ARIS partitioning and power allocation under average channel conditions. Finally, tailored optimization methods are proposed for the reflection precoding and AN design of the two partitions. Simulation results validate that, compared to baseline schemes, the proposed scheme significantly increases the localization error of the SU by MUs while maintaining efficient communication between the SU and RUs, thereby effectively protecting the SU location privacy.",
    "updated" : "2025-11-12T08:10:52Z",
    "published" : "2025-11-12T08:10:52Z",
    "authors" : [
      {
        "name" : "Ziqi Chen"
      },
      {
        "name" : "Jun Du"
      },
      {
        "name" : "Chunxiao Jiang"
      },
      {
        "name" : "Tony Q. S. Quek"
      },
      {
        "name" : "Zhu Han"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.09043v1",
    "title" : "MedHE: Communication-Efficient Privacy-Preserving Federated Learning with Adaptive Gradient Sparsification for Healthcare",
    "summary" : "Healthcare federated learning requires strong privacy guarantees while maintaining computational efficiency across resource-constrained medical institutions. This paper presents MedHE, a novel framework combining adaptive gradient sparsification with CKKS homomorphic encryption to enable privacy-preserving collaborative learning on sensitive medical data. Our approach introduces a dynamic threshold mechanism with error compensation for top-k gradient selection, achieving 97.5 percent communication reduction while preserving model utility. We provide formal security analysis under Ring Learning with Errors assumptions and demonstrate differential privacy guarantees with epsilon less than or equal to 1.0. Statistical testing across 5 independent trials shows MedHE achieves 89.5 percent plus or minus 0.8 percent accuracy, maintaining comparable performance to standard federated learning (p=0.32) while reducing communication from 1277 MB to 32 MB per training round. Comprehensive evaluation demonstrates practical feasibility for real-world medical deployments with HIPAA compliance and scalability to 100 plus institutions.",
    "updated" : "2025-11-12T06:50:48Z",
    "published" : "2025-11-12T06:50:48Z",
    "authors" : [
      {
        "name" : "Farjana Yesmin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.08998v1",
    "title" : "Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science",
    "summary" : "Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.",
    "updated" : "2025-11-12T05:39:11Z",
    "published" : "2025-11-12T05:39:11Z",
    "authors" : [
      {
        "name" : "Zilinghan Li"
      },
      {
        "name" : "Aditya Sinha"
      },
      {
        "name" : "Yijiang Li"
      },
      {
        "name" : "Kyle Chard"
      },
      {
        "name" : "Kibaek Kim"
      },
      {
        "name" : "Ravi Madduri"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.08666v1",
    "title" : "Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding",
    "summary" : "We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.",
    "updated" : "2025-11-11T18:56:27Z",
    "published" : "2025-11-11T18:56:27Z",
    "authors" : [
      {
        "name" : "Joseph Fioresi"
      },
      {
        "name" : "Ishan Rajendrakumar Dave"
      },
      {
        "name" : "Mubarak Shah"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.08059v1",
    "title" : "\"I need to learn better searching tactics for privacy policy laws.'' Investigating Software Developers' Behavior When Using Sources on Privacy Issues",
    "summary" : "Since the introduction of the European General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), software developers increasingly have to make privacy-related decisions during system design and implementation. However, past research showed that they often lack legal expertise and struggle with privacy-compliant development. To shed light on how effective current information sources are in supporting them with privacy-sensitive implementation, we conducted a qualitative study with 30 developers. Participants were presented with a privacy-sensitive scenario and asked to identify privacy issues and suggest measures using their knowledge, online resources, and an AI assistant. We observed developers' decision-making in think-aloud sessions and discussed it in follow-up interviews. We found that participants struggled with all three sources: personal knowledge was insufficient, web content was often too complex, and while AI assistants provided clear and user-tailored responses, they lacked contextual relevance and failed to identify scenario-specific issues. Our study highlights major shortcomings in existing support for privacy-related development tasks. Based on our findings, we discuss the need for more accessible, understandable, and actionable privacy resources for developers.",
    "updated" : "2025-11-11T09:58:06Z",
    "published" : "2025-11-11T09:58:06Z",
    "authors" : [
      {
        "name" : "Stefan Albert Horstmann"
      },
      {
        "name" : "Sandy Hong"
      },
      {
        "name" : "Maziar Niazian"
      },
      {
        "name" : "Cristiana Santos"
      },
      {
        "name" : "Alena Naiakshina"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.07818v1",
    "title" : "Blockchain-Integrated Privacy-Preserving Medical Insurance Claim Processing Using Homomorphic Encryption",
    "summary" : "This research proposes a decentralized and cryptographically secure framework to address the most acute issues of privacy, data security, and protection in the ecosystem of medical insurance claim processing. The scope of this study focuses on enabling the management of insurance claims in a transparent, privacy-protecting manner while maintaining the efficiency and trust level needed by the patients, healthcare providers, and insurers. To accomplish this, the proposed system adds blockchain technology to provide an unchangeable, decentralized, and auditable claim transactions ledger which enhances overall claim-related processes and trust among all stakeholders. To protect critical patient information, the framework employs homomorphic encryption a modern form of cryptography to allow authorized insurance providers to perform necessary operations like claim adjudication and reimbursement on encrypted medical records without any decryption during the process. This method significantly reduces the third-party processing privacy risk because patient data can be kept secret even when third-party processing is done. In addition, smart contracts improve automation of the most important procedures in the claim processing pipeline, which decreases manual, operational, and susceptibility towards human blunders or deceitful acts. The integration of these two transformative technologiesblockchain and homomorphic encryption represents the core contribution of this work, enabling the coexistence of transparency and privacy which are usually viewed as competing objectives in traditional systems. As a result, these technologies are expected to foster the creation of a reliable, effective, and privacy safeguarding architecture that could transform the medical claim submission systems paradigm.",
    "updated" : "2025-11-11T04:22:36Z",
    "published" : "2025-11-11T04:22:36Z",
    "authors" : [
      {
        "name" : "Diya Mamoria"
      },
      {
        "name" : "Harshit Jain"
      },
      {
        "name" : "Aswani Kumar Cherukuri"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.07807v1",
    "title" : "PRISM: Privacy-preserving Inference System with Homomorphic Encryption and Modular Activation",
    "summary" : "With the rapid advancements in machine learning, models have become increasingly capable of learning and making predictions in various industries. However, deploying these models in critical infrastructures presents a major challenge, as concerns about data privacy prevent unrestricted data sharing. Homomorphic encryption (HE) offers a solution by enabling computations on encrypted data, but it remains incompatible with machine learning models like convolutional neural networks (CNNs), due to their reliance on non-linear activation functions. To bridge this gap, this work proposes an optimized framework that replaces standard non-linear functions with homomorphically compatible approximations, ensuring secure computations while minimizing computational overhead. The proposed approach restructures the CNN architecture and introduces an efficient activation function approximation method to mitigate the performance trade-offs introduced by encryption. Experiments on CIFAR-10 achieve 94.4% accuracy with 2.42 s per single encrypted sample and 24,000 s per 10,000 encrypted samples, using a degree-4 polynomial and Softplus activation under CKKS, balancing accuracy and privacy.",
    "updated" : "2025-11-11T03:57:12Z",
    "published" : "2025-11-11T03:57:12Z",
    "authors" : [
      {
        "name" : "Zeinab Elkhatib"
      },
      {
        "name" : "Ali Sekmen"
      },
      {
        "name" : "Kamrul Hasan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.07505v1",
    "title" : "FedRW: Efficient Privacy-Preserving Data Reweighting for Enhancing Federated Learning of Language Models",
    "summary" : "Data duplication within large-scale corpora often impedes large language models' (LLMs) performance and privacy. In privacy-concerned federated learning scenarios, conventional deduplication methods typically rely on trusted third parties to perform uniform deletion, risking loss of informative samples while introducing privacy vulnerabilities. To address these gaps, we propose Federated ReWeighting (FedRW), the first privacy-preserving framework, to the best of our knowledge, that performs soft deduplication via sample reweighting instead of deletion in federated LLM training, without assuming a trusted third party. At its core, FedRW proposes a secure, frequency-aware reweighting protocol through secure multi-party computation, coupled with a parallel orchestration strategy to ensure efficiency and scalability. During training, FedRW utilizes an adaptive reweighting mechanism with global sample frequencies to adjust individual loss contributions, effectively improving generalization and robustness. Empirical results demonstrate that FedRW outperforms the state-of-the-art method by achieving up to 28.78x speedup in preprocessing and approximately 11.42% improvement in perplexity, while offering enhanced security guarantees. FedRW thus establishes a new paradigm for managing duplication in federated LLM training.",
    "updated" : "2025-11-10T18:29:55Z",
    "published" : "2025-11-10T18:29:55Z",
    "authors" : [
      {
        "name" : "Pukang Ye"
      },
      {
        "name" : "Junwei Luo"
      },
      {
        "name" : "Xiaolei Dong"
      },
      {
        "name" : "Yunbo Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.07242v2",
    "title" : "Privacy on the Fly: A Predictive Adversarial Transformation Network for Mobile Sensor Data",
    "summary" : "Mobile motion sensors such as accelerometers and gyroscopes are now ubiquitously accessible by third-party apps via standard APIs. While enabling rich functionalities like activity recognition and step counting, this openness has also enabled unregulated inference of sensitive user traits, such as gender, age, and even identity, without user consent. Existing privacy-preserving techniques, such as GAN-based obfuscation or differential privacy, typically require access to the full input sequence, introducing latency that is incompatible with real-time scenarios. Worse, they tend to distort temporal and semantic patterns, degrading the utility of the data for benign tasks like activity recognition. To address these limitations, we propose the Predictive Adversarial Transformation Network (PATN), a real-time privacy-preserving framework that leverages historical signals to generate adversarial perturbations proactively. The perturbations are applied immediately upon data acquisition, enabling continuous protection without disrupting application functionality. Experiments on two datasets demonstrate that PATN substantially degrades the performance of privacy inference models, achieving Attack Success Rate (ASR) of 40.11% and 44.65% (reducing inference accuracy to near-random) and increasing the Equal Error Rate (EER) from 8.30% and 7.56% to 41.65% and 46.22%. On ASR, PATN outperforms baseline methods by 16.16% and 31.96%, respectively.",
    "updated" : "2025-11-12T15:45:52Z",
    "published" : "2025-11-10T15:57:17Z",
    "authors" : [
      {
        "name" : "Tianle Song"
      },
      {
        "name" : "Chenhao Lin"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Zhengyu Zhao"
      },
      {
        "name" : "Jiahao Sun"
      },
      {
        "name" : "Chong Zhang"
      },
      {
        "name" : "Le Yang"
      },
      {
        "name" : "Chao Shen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.07073v1",
    "title" : "Breaking Privacy in Federated Clustering: Perfect Input Reconstruction via Temporal Correlations",
    "summary" : "Federated clustering allows multiple parties to discover patterns in distributed data without sharing raw samples. To reduce overhead, many protocols disclose intermediate centroids during training. While often treated as harmless for efficiency, whether such disclosure compromises privacy remains an open question. Prior analyses modeled the problem as a so-called Hidden Subset Sum Problem (HSSP) and argued that centroid release may be safe, since classical HSSP attacks fail to recover inputs.\n  We revisit this question and uncover a new leakage mechanism: temporal regularities in $k$-means iterations create exploitable structure that enables perfect input reconstruction. Building on this insight, we propose Trajectory-Aware Reconstruction (TAR), an attack that combines temporal assignment information with algebraic analysis to recover exact original inputs. Our findings provide the first rigorous evidence, supported by a practical attack, that centroid disclosure in federated clustering significantly compromises privacy, exposing a fundamental tension between privacy and efficiency.",
    "updated" : "2025-11-10T13:06:16Z",
    "published" : "2025-11-10T13:06:16Z",
    "authors" : [
      {
        "name" : "Guang Yang"
      },
      {
        "name" : "Lixia Luo"
      },
      {
        "name" : "Qiongxiu Li"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.06778v2",
    "title" : "SAFENLIDB: A Privacy-Preserving Safety Alignment Framework for LLM-based Natural Language Database Interfaces",
    "summary" : "The rapid advancement of Large Language Models (LLMs) has driven significant progress in Natural Language Interface to Database (NLIDB). However, the widespread adoption of LLMs has raised critical privacy and security concerns. During interactions, LLMs may unintentionally expose confidential database contents or be manipulated by attackers to exfiltrate data through seemingly benign queries. While current efforts typically rely on rule-based heuristics or LLM agents to mitigate this leakage risk, these methods still struggle with complex inference-based attacks, suffer from high false positive rates, and often compromise the reliability of SQL queries. To address these challenges, we propose \\textsc{SafeNlidb}, a novel privacy-security alignment framework for LLM-based NLIDB. The framework features an automated pipeline that generates hybrid chain-of-thought interaction data from scratch, seamlessly combining implicit security reasoning with SQL generation. Additionally, we introduce reasoning warm-up and alternating preference optimization to overcome the multi-preference oscillations of Direct Preference Optimization (DPO), enabling LLMs to produce security-aware SQL through fine-grained reasoning without the need for human-annotated preference data. Extensive experiments demonstrate that our method outperforms both larger-scale LLMs and ideal-setting baselines, achieving significant security improvements while preserving high utility. WARNING: This work may contain content that is offensive and harmful!",
    "updated" : "2025-11-11T08:03:28Z",
    "published" : "2025-11-10T07:05:59Z",
    "authors" : [
      {
        "name" : "Ruiheng Liu"
      },
      {
        "name" : "XiaoBing Chen"
      },
      {
        "name" : "Jinyu Zhang"
      },
      {
        "name" : "Qiongwen Zhang"
      },
      {
        "name" : "Yu Zhang"
      },
      {
        "name" : "Bailong Yang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.06363v1",
    "title" : "Privacy-Preserving Federated Learning for Fair and Efficient Urban Traffic Optimization",
    "summary" : "The optimization of urban traffic is threatened by the complexity of achieving a balance between transport efficiency and the maintenance of privacy, as well as the equitable distribution of traffic based on socioeconomically diverse neighborhoods. Current centralized traffic management schemes invade user location privacy and further entrench traffic disparity by offering disadvantaged route suggestions, whereas current federated learning frameworks do not consider fairness constraints in multi-objective traffic settings. This study presents a privacy-preserving federated learning framework, termed FedFair-Traffic, that jointly and simultaneously optimizes travel efficiency, traffic fairness, and differential privacy protection. This is the first attempt to integrate three conflicting objectives to improve urban transportation systems. The proposed methodology enables collaborative learning between related vehicles with data locality by integrating Graph Neural Networks with differential privacy mechanisms ($Îµ$-privacy guarantees) and Gini coefficient-based fair constraints using multi-objective optimization. The framework uses federated aggregation methods of gradient clipping and noise injection to provide differential privacy and optimize Pareto-efficient solutions for the efficiency-fairness tradeoff. Real-world comprehensive experiments on the METR-LA traffic dataset showed that FedFair-Traffic can reduce the average travel time by 7\\% (14.2 minutes) compared with their centralized baselines, promote traffic fairness by 73\\% (Gini coefficient, 0.78), and offer high privacy protection (privacy score, 0.8) with an 89\\% reduction in communication overhead. These outcomes demonstrate that FedFair-Traffic is a scalable privacy-aware smart city infrastructure with possible use-cases in metropolitan traffic flow control and federated transportation networks.",
    "updated" : "2025-11-09T13:03:27Z",
    "published" : "2025-11-09T13:03:27Z",
    "authors" : [
      {
        "name" : "Rathin Chandra Shit"
      },
      {
        "name" : "Sharmila Subudhi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.NI",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.06305v1",
    "title" : "Setting $\\varepsilon$ is not the Issue in Differential Privacy",
    "summary" : "This position paper argues that setting the privacy budget in differential privacy should not be viewed as an important limitation of differential privacy compared to alternative methods for privacy-preserving machine learning. The so-called problem of interpreting the privacy budget is often presented as a major hindrance to the wider adoption of differential privacy in real-world deployments and is sometimes used to promote alternative mitigation techniques for data protection. We believe this misleads decision-makers into choosing unsafe methods. We argue that the difficulty in interpreting privacy budgets does not stem from the definition of differential privacy itself, but from the intrinsic difficulty of estimating privacy risks in context, a challenge that any rigorous method for privacy risk assessment face. Moreover, we claim that any sound method for estimating privacy risks should, given the current state of research, be expressible within the differential privacy framework or justify why it cannot.",
    "updated" : "2025-11-09T10:03:45Z",
    "published" : "2025-11-09T10:03:45Z",
    "authors" : [
      {
        "name" : "Edwige Cyffers"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.06231v1",
    "title" : "Synheart Emotion: Privacy-Preserving On-Device Emotion Recognition from Biosignals",
    "summary" : "Human-computer interaction increasingly demands systems that recognize not only explicit user inputs but also implicit emotional states. While substantial progress has been made in affective computing, most emotion recognition systems rely on cloud-based inference, introducing privacy vulnerabilities and latency constraints unsuitable for real-time applications. This work presents a comprehensive evaluation of machine learning architectures for on-device emotion recognition from wrist-based photoplethysmography (PPG), systematically comparing different models spanning classical ensemble methods, deep neural networks, and transformers on the WESAD stress detection dataset. Results demonstrate that classical ensemble methods substantially outperform deep learning on small physiological datasets, with ExtraTrees achieving F1 = 0.826 on combined features and F1 = 0.623 on wrist-only features, compared to transformers achieving only F1 = 0.509-0.577. We deploy the wrist-only ExtraTrees model optimized via ONNX conversion, achieving a 4.08 MB footprint, 0.05 ms inference latency, and 152x speedup over the original implementation. Furthermore, ONNX optimization yields a 30.5% average storage reduction and 40.1x inference speedup, highlighting the feasibility of privacy-preserving on-device emotion recognition for real-world wearables.",
    "updated" : "2025-11-09T05:15:04Z",
    "published" : "2025-11-09T05:15:04Z",
    "authors" : [
      {
        "name" : "Henok Ademtew"
      },
      {
        "name" : "Israel Goytom"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.06064v1",
    "title" : "A Privacy-Preserving Federated Learning Method with Homomorphic Encryption in Omics Data",
    "summary" : "Omics data is widely employed in medical research to identify disease mechanisms and contains highly sensitive personal information. Federated Learning (FL) with Differential Privacy (DP) can ensure the protection of omics data privacy against malicious user attacks. However, FL with the DP method faces an inherent trade-off: stronger privacy protection degrades predictive accuracy due to injected noise. On the other hand, Homomorphic Encryption (HE) allows computations on encrypted data and enables aggregation of encrypted gradients without DP-induced noise can increase the predictive accuracy. However, it may increase the computation cost. To improve the predictive accuracy while considering the computational ability of heterogeneous clients, we propose a Privacy-Preserving Machine Learning (PPML)-Hybrid method by introducing HE. In the proposed PPML-Hybrid method, clients distributed select either HE or DP based on their computational resources, so that HE clients contribute noise-free updates while DP clients reduce computational overhead. Meanwhile, clients with high computational resources clients can flexibly adopt HE or DP according to their privacy needs. Performance evaluation on omics datasets show that our proposed method achieves comparable predictive accuracy while significantly reducing computation time relative to HE-only. Additionally, it outperforms DP-only methods under equivalent or stricter privacy budgets.",
    "updated" : "2025-11-08T16:18:42Z",
    "published" : "2025-11-08T16:18:42Z",
    "authors" : [
      {
        "name" : "Yusaku Negoya"
      },
      {
        "name" : "Feifei Cui"
      },
      {
        "name" : "Zilong Zhang"
      },
      {
        "name" : "Miao Pan"
      },
      {
        "name" : "Tomoaki Ohtsuki"
      },
      {
        "name" : "Aohan Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.05327v1",
    "title" : "Privacy-Preserving CramÃ©r-Rao Lower Bound",
    "summary" : "This paper establishes the privacy-preserving CramÃ©r-Rao (CR) lower bound theory, characterizing the fundamental limit of identification accuracy under privacy constraint. An identifiability criterion under privacy constraint is derived by using Fisher information matrix as the privacy metric. In the identifiable case, the privacy-preserving CR lower bound is established and its attainability is demonstrated, thereby ensuring the existence of the privacy-preserving Fisher information matrix with explicit expression. Then, the privacy-preserving CR lower bound theory is extended to the multi-sensor multi-measurement system. Specifically, the additivity principle of privacy-preserving Fisher information matrices across both spatial and temporal dimensions is established, building a relationship between privacy-preserving CR lower bounds for the multi-sensor multi-measurement system and its subsystems. Using this additivity principle, distributed identification algorithms capable of achieving the privacy-preserving CR lower bound are further proposed. Numerical examples are provided to demonstrate the privacy-preserving CR lower bound and show the effectiveness of the proposed algorithms.",
    "updated" : "2025-11-07T15:26:19Z",
    "published" : "2025-11-07T15:26:19Z",
    "authors" : [
      {
        "name" : "Jieming Ke"
      },
      {
        "name" : "Jimin Wang"
      },
      {
        "name" : "Ji-Feng Zhang"
      }
    ],
    "categories" : [
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.05092v1",
    "title" : "A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification",
    "summary" : "With growing concerns over data privacy, researchers have started using virtual data as an alternative to sensitive real-world images for training person re-identification (Re-ID) models. However, existing virtual datasets produced by game engines still face challenges such as complex construction and poor domain generalization, making them difficult to apply in real scenarios. To address these challenges, we propose a Dual-stage Prompt-driven Privacy-preserving Paradigm (DPPP). In the first stage, we generate rich prompts incorporating multi-dimensional attributes such as pedestrian appearance, illumination, and viewpoint that drive the diffusion model to synthesize diverse data end-to-end, building a large-scale virtual dataset named GenePerson with 130,519 images of 6,641 identities. In the second stage, we propose a Prompt-driven Disentanglement Mechanism (PDM) to learn domain-invariant generalization features. With the aid of contrastive learning, we employ two textual inversion networks to map images into pseudo-words representing style and content, respectively, thereby constructing style-disentangled content prompts to guide the model in learning domain-invariant content features at the image level. Experiments demonstrate that models trained on GenePerson with PDM achieve state-of-the-art generalization performance, surpassing those on popular real and virtual Re-ID datasets.",
    "updated" : "2025-11-07T09:17:48Z",
    "published" : "2025-11-07T09:17:48Z",
    "authors" : [
      {
        "name" : "Ruolin Li"
      },
      {
        "name" : "Min Liu"
      },
      {
        "name" : "Yuan Bian"
      },
      {
        "name" : "Zhaoyang Li"
      },
      {
        "name" : "Yuzhen Li"
      },
      {
        "name" : "Xueping Wang"
      },
      {
        "name" : "Yaonan Wang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.04438v1",
    "title" : "Limiting one-way distillable secret key via privacy testing of extendible states",
    "summary" : "The notions of privacy tests and $k$-extendible states have both been instrumental in quantum information theory, particularly in understanding the limits of secure communication. In this paper, we determine the maximum probability with which an arbitrary $k$-extendible state can pass a privacy test, and we prove that it is equal to the maximum fidelity between an arbitrary $k$-extendible state and the standard maximally entangled state. Our findings, coupled with the resource theory of $k$-unextendibility, lead to an efficiently computable upper bound on the one-shot, one-way distillable key of a bipartite state, and we prove that it is equal to the best-known efficiently computable upper bound on the one-shot, one-way distillable entanglement. We also establish efficiently computable upper bounds on the one-shot, forward-assisted private capacity of channels. Extending our formalism to the independent and identically distributed setting, we obtain single-letter efficiently computable bounds on the $n$-shot, one-way distillable key of a state and the $n$-shot, forward-assisted private capacity of a channel. For some key examples of interest, our bounds are significantly tighter than other known efficiently computable bounds.",
    "updated" : "2025-11-06T15:11:54Z",
    "published" : "2025-11-06T15:11:54Z",
    "authors" : [
      {
        "name" : "Vishal Singh"
      },
      {
        "name" : "Karol Horodecki"
      },
      {
        "name" : "Aby Philip"
      },
      {
        "name" : "Mark M. Wilde"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.04261v1",
    "title" : "A Parallel Region-Adaptive Differential Privacy Framework for Image Pixelization",
    "summary" : "The widespread deployment of high-resolution visual sensing systems, coupled with the rise of foundation models, has amplified privacy risks in video-based applications. Differentially private pixelization offers mathematically guaranteed protection for visual data through grid-based noise addition, but challenges remain in preserving task-relevant fidelity, achieving scalability, and enabling efficient real-time deployment. To address this, we propose a novel parallel, region-adaptive pixelization framework that combines the theoretical rigor of differential privacy with practical efficiency. Our method adaptively adjusts grid sizes and noise scales based on regional complexity, leveraging GPU parallelism to achieve significant runtime acceleration compared to the classical baseline. A lightweight storage scheme is introduced by retaining only essential noisy statistics, significantly reducing space overhead. Formal privacy analysis is provided under the Laplace mechanism and parallel composition theorem. Extensive experiments on the PETS, Venice-2, and PPM-100 datasets demonstrate favorable privacy-utility trade-offs and significant runtime/storage reductions. A face re-identification attack experiment on CelebA further confirms the method's effectiveness in preventing identity inference. This validates its suitability for real-time privacy-critical applications such as elderly care, smart home monitoring, driver behavior analysis, and crowd behavior monitoring.",
    "updated" : "2025-11-06T10:51:20Z",
    "published" : "2025-11-06T10:51:20Z",
    "authors" : [
      {
        "name" : "Ming Liu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.03966v1",
    "title" : "PrivacyCD: Hierarchical Unlearning for Protecting Student Privacy in Cognitive Diagnosis",
    "summary" : "The need to remove specific student data from cognitive diagnosis (CD) models has become a pressing requirement, driven by users' growing assertion of their \"right to be forgotten\". However, existing CD models are largely designed without privacy considerations and lack effective data unlearning mechanisms. Directly applying general purpose unlearning algorithms is suboptimal, as they struggle to balance unlearning completeness, model utility, and efficiency when confronted with the unique heterogeneous structure of CD models. To address this, our paper presents the first systematic study of the data unlearning problem for CD models, proposing a novel and efficient algorithm: hierarchical importanceguided forgetting (HIF). Our key insight is that parameter importance in CD models exhibits distinct layer wise characteristics. HIF leverages this via an innovative smoothing mechanism that combines individual and layer, level importance, enabling a more precise distinction of parameters associated with the data to be unlearned. Experiments on three real world datasets show that HIF significantly outperforms baselines on key metrics, offering the first effective solution for CD models to respond to user data removal requests and for deploying high-performance, privacy preserving AI systems",
    "updated" : "2025-11-06T01:39:59Z",
    "published" : "2025-11-06T01:39:59Z",
    "authors" : [
      {
        "name" : "Mingliang Hou"
      },
      {
        "name" : "Yinuo Wang"
      },
      {
        "name" : "Teng Guo"
      },
      {
        "name" : "Zitao Liu"
      },
      {
        "name" : "Wenzhou Dou"
      },
      {
        "name" : "Jiaqi Zheng"
      },
      {
        "name" : "Renqiang Luo"
      },
      {
        "name" : "Mi Tian"
      },
      {
        "name" : "Weiqi Luo"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.03665v1",
    "title" : "A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential",
    "summary" : "This paper presents a lightweight three-dimensional convolutional neural network (3DCNN) for human activity recognition (HAR) using event-based vision data. Privacy preservation is a key challenge in human monitoring systems, as conventional frame-based cameras capture identifiable personal information. In contrast, event cameras record only changes in pixel intensity, providing an inherently privacy-preserving sensing modality. The proposed network effectively models both spatial and temporal dynamics while maintaining a compact design suitable for edge deployment. To address class imbalance and enhance generalization, focal loss with class reweighting and targeted data augmentation strategies are employed. The model is trained and evaluated on a composite dataset derived from the Toyota Smart Home and ETRI datasets. Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D, and MC3_18 by up to 3%. These results highlight the potential of event-based deep learning for developing accurate, efficient, and privacy-aware human action recognition systems suitable for real-world edge applications.",
    "updated" : "2025-11-05T17:30:31Z",
    "published" : "2025-11-05T17:30:31Z",
    "authors" : [
      {
        "name" : "Mehdi Sefidgar Dilmaghani"
      },
      {
        "name" : "Francis Fowley"
      },
      {
        "name" : "Peter Corcoran"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.03538v1",
    "title" : "Security and Privacy Management of IoT Using Quantum Computing",
    "summary" : "The convergence of the Internet of Things (IoT) and quantum computing is redefining the security paradigm of interconnected digital systems. Classical cryptographic algorithms such as RSA, Elliptic Curve Cryptography (ECC), and Advanced Encryption Standard (AES) have long provided the foundation for securing IoT communication. However, the emergence of quantum algorithms such as Shor's and Grover's threatens to render these techniques vulnerable, necessitating the development of quantum-resilient alternatives. This chapter examines the implications of quantum computing for IoT security and explores strategies for building cryptographically robust systems in the post-quantum era. It presents an overview of Post-Quantum Cryptographic (PQC) families, including lattice-based, code-based, hash-based, and multivariate approaches, analyzing their potential for deployment in resource-constrained IoT environments. In addition, quantum-based methods such as Quantum Key Distribution (QKD) and Quantum Random Number Generators (QRNGs) are discussed for their ability to enhance confidentiality and privacy through physics-based security guarantees. The chapter also highlights issues of privacy management, regulatory compliance, and standardization, emphasizing the need for collaborative efforts across academia, industry, and governance. Overall, it provides a comprehensive perspective on security IoT ecosystems against quantum threats and ensures resilience in the next generation of intelligent networks.",
    "updated" : "2025-11-05T15:08:55Z",
    "published" : "2025-11-05T15:08:55Z",
    "authors" : [
      {
        "name" : "Jaydip Sen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.03248v2",
    "title" : "Auditing M-LLMs for Privacy Risks: A Synthetic Benchmark and Evaluation Framework",
    "summary" : "Recent advances in multi-modal Large Language Models (M-LLMs) have demonstrated a powerful ability to synthesize implicit information from disparate sources, including images and text. These resourceful data from social media also introduce a significant and underexplored privacy risk: the inference of sensitive personal attributes from seemingly daily media content. However, the lack of benchmarks and comprehensive evaluations of state-of-the-art M-LLM capabilities hinders the research of private attribute profiling on social media. Accordingly, we propose (1) PRISM, the first multi-modal, multi-dimensional and fine-grained synthesized dataset incorporating a comprehensive privacy landscape and dynamic user history; (2) an Efficient evaluation framework that measures the cross-modal privacy inference capabilities of advanced M-LLM. Specifically, PRISM is a large-scale synthetic benchmark designed to evaluate cross-modal privacy risks. Its key feature is 12 sensitive attribute labels across a diverse set of multi-modal profiles, which enables targeted privacy analysis. These profiles are generated via a sophisticated LLM agentic workflow, governed by a prior distribution to ensure they realistically mimic social media users. Additionally, we propose a Multi-Agent Inference Framework that leverages a pipeline of specialized LLMs to enhance evaluation capabilities. We evaluate the inference capabilities of six leading M-LLMs (Qwen, Gemini, GPT-4o, GLM, Doubao, and Grok) on PRISM. The comparison with human performance reveals that these MLLMs significantly outperform in accuracy and efficiency, highlighting the threat of potential privacy risks and the urgent need for robust defenses. Dataset available at https://huggingface.co/datasets/xaddh/multimodal-privacy",
    "updated" : "2025-11-09T02:46:31Z",
    "published" : "2025-11-05T07:23:21Z",
    "authors" : [
      {
        "name" : "Junhao Li"
      },
      {
        "name" : "Jiahao Chen"
      },
      {
        "name" : "Zhou Feng"
      },
      {
        "name" : "Chunyi Zhou"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.03753v2",
    "title" : "Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices",
    "summary" : "This study presents a federated learning (FL) framework for privacy-preserving electrocardiogram (ECG) classification in Internet of Things (IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian Angular Field (GAF) images, the proposed approach enables efficient feature extraction through Convolutional Neural Networks (CNNs) while ensuring that sensitive medical data remain local to each device. This work is among the first to experimentally validate GAF-based federated ECG classification across heterogeneous IoT devices, quantifying both performance and communication efficiency. To evaluate feasibility in realistic IoT settings, we deployed the framework across a server, a laptop, and a resource-constrained Raspberry Pi 4, reflecting edge-cloud integration in IoT ecosystems. Experimental results demonstrate that the FL-GAF model achieves a high classification accuracy of 95.18% in a multi-client setup, significantly outperforming a single-client baseline in both accuracy and training time. Despite the added computational complexity of GAF transformations, the framework maintains efficient resource utilization and communication overhead. These findings highlight the potential of lightweight, privacy-preserving AI for IoT-based healthcare monitoring, supporting scalable and secure edge deployments in smart health systems.",
    "updated" : "2025-11-11T17:37:37Z",
    "published" : "2025-11-04T22:23:59Z",
    "authors" : [
      {
        "name" : "Youssef Elmir"
      },
      {
        "name" : "Yassine Himeur"
      },
      {
        "name" : "Abbes Amira"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02993v2",
    "title" : "PrivyWave: Privacy-Aware Wireless Sensing of Heartbeat",
    "summary" : "Wireless sensing technologies can now detect heartbeats using radio frequency and acoustic signals, raising significant privacy concerns. Existing privacy solutions either protect from all sensing systems indiscriminately preventing any utility or operate post-data collection, failing to enable selective access where authorized devices can monitor while unauthorized ones cannot. We present a key-based physical obfuscation system, PrivyWave, that addresses this challenge by generating controlled decoy heartbeat signals at cryptographically-determined frequencies. Unauthorized sensors receive a mixture of real and decoy signals that are indistinguishable without the secret key, while authorized sensors use the key to filter out decoys and recover accurate measurements. Our evaluation with 13 participants demonstrates effective protection across both sensing modalities: for mmWave radar, unauthorized sensors show 21.3 BPM mean absolute error while authorized sensors maintain a much smaller 5.8 BPM; for acoustic sensing, unauthorized error increases to 42.0 BPM while authorized sensors achieve 9.7 BPM. The system operates across multiple sensing modalities without per-modality customization and provides cryptographic obfuscation guarantees. Performance benchmarks show robust protection across different distances (30-150 cm), orientations (120Â° field of view), and diverse indoor environments, establishing physical-layer obfuscation as a viable approach for selective privacy in pervasive health monitoring.",
    "updated" : "2025-11-06T02:34:25Z",
    "published" : "2025-11-04T20:54:59Z",
    "authors" : [
      {
        "name" : "Yixuan Gao"
      },
      {
        "name" : "Tanvir Ahmed"
      },
      {
        "name" : "Zekun Chang"
      },
      {
        "name" : "Thijs Roumen"
      },
      {
        "name" : "Rajalakshmi Nandakumar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02797v1",
    "title" : "Fast, Private, and Protected: Safeguarding Data Privacy and Defending Against Model Poisoning Attacks in Federated Learning",
    "summary" : "Federated Learning (FL) is a distributed training paradigm wherein participants collaborate to build a global model while ensuring the privacy of the involved data, which remains stored on participant devices. However, proposals aiming to ensure such privacy also make it challenging to protect against potential attackers seeking to compromise the training outcome. In this context, we present Fast, Private, and Protected (FPP), a novel approach that aims to safeguard federated training while enabling secure aggregation to preserve data privacy. This is accomplished by evaluating rounds using participants' assessments and enabling training recovery after an attack. FPP also employs a reputation-based mechanism to mitigate the participation of attackers. We created a dockerized environment to validate the performance of FPP compared to other approaches in the literature (FedAvg, Power-of-Choice, and aggregation via Trimmed Mean and Median). Our experiments demonstrate that FPP achieves a rapid convergence rate and can converge even in the presence of malicious participants performing model poisoning attacks.",
    "updated" : "2025-11-04T18:20:45Z",
    "published" : "2025-11-04T18:20:45Z",
    "authors" : [
      {
        "name" : "Nicolas Riccieri Gardin Assumpcao"
      },
      {
        "name" : "Leandro Villas"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02785v1",
    "title" : "Enhancing Federated Learning Privacy with QUBO",
    "summary" : "Federated learning (FL) is a widely used method for training machine learning (ML) models in a scalable way while preserving privacy (i.e., without centralizing raw data). Prior research shows that the risk of exposing sensitive data increases cumulatively as the number of iterations where a client's updates are included in the aggregated model increase. Attackers can launch membership inference attacks (MIA; deciding whether a sample or client participated), property inference attacks (PIA; inferring attributes of a client's data), and model inversion attacks (MI; reconstructing inputs), thereby inferring client-specific attributes and, in some cases, reconstructing inputs. In this paper, we mitigate risk by substantially reducing per client exposure using a quantum computing-inspired quadratic unconstrained binary optimization (QUBO) formulation that selects a small subset of client updates most relevant for each training round. In this work, we focus on two threat vectors: (i) information leakage by clients during training and (ii) adversaries who can query or obtain the global model. We assume a trusted central server and do not model server compromise. This method also assumes that the server has access to a validation/test set with global data distribution. Experiments on the MNIST dataset with 300 clients in 20 rounds showed a 95.2% per-round and 49% cumulative privacy exposure reduction, with 147 clients' updates never being used during training while maintaining in general the full-aggregation accuracy or even better. The method proved to be efficient at lower scale and more complex model as well. A CINIC-10 dataset-based experiment with 30 clients resulted in 82% per-round privacy improvement and 33% cumulative privacy.",
    "updated" : "2025-11-04T18:06:30Z",
    "published" : "2025-11-04T18:06:30Z",
    "authors" : [
      {
        "name" : "Andras Ferenczi"
      },
      {
        "name" : "Sutapa Samanta"
      },
      {
        "name" : "Dagen Wang"
      },
      {
        "name" : "Todd Hodges"
      }
    ],
    "categories" : [
      "cs.LG",
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02297v1",
    "title" : "Two-Parameter RÃ©nyi Information Quantities with Applications to Privacy Amplification and Soft Covering",
    "summary" : "There are no universally accepted definitions of RÃ©nyi conditional entropy and RÃ©nyi mutual information, although motivated by different applications, several definitions have been proposed in the literature. In this paper, we consider a family of two-parameter RÃ©nyi conditional entropy and a family of two-parameter RÃ©nyi mutual information. By performing a change of variables for the parameters, the two-parameter RÃ©nyi conditional entropy we study coincides precisely with the definition introduced by Hayashi and Tan [IEEE Trans. Inf. Theory, 2016], and it also emerges naturally as the classical specialization of the three-parameter quantum RÃ©nyi conditional entropy recently put forward by Rubboli, Goodarzi, and Tomamichel [arXiv:2410.21976 (2024)]. We establish several fundamental properties of the two-parameter RÃ©nyi conditional entropy, including monotonicity with respect to the parameters and variational expression. The associated two-parameter RÃ©nyi mutual information considered in this paper is new and it unifies three commonly used variants of RÃ©nyi mutual information. For this quantity, we prove several important properties, including the non-negativity, additivity, data processing inequality, monotonicity with respect to the parameters, variational expression, as well as convexity and concavity. Finally, we demonstrate that these two-parameter RÃ©nyi information quantities can be used to characterize the strong converse exponents in privacy amplification and soft covering problems under RÃ©nyi divergence of order $Î±\\in (0, \\infty)$.",
    "updated" : "2025-11-04T06:21:38Z",
    "published" : "2025-11-04T06:21:38Z",
    "authors" : [
      {
        "name" : "Shi-Bing Li"
      },
      {
        "name" : "Ke Li"
      },
      {
        "name" : "Lei Yu"
      }
    ],
    "categories" : [
      "cs.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02283v1",
    "title" : "Distributed Nonconvex Optimization with Double Privacy Protection and Exact Convergence",
    "summary" : "Motivated by the pervasive lack of privacy protection in existing distributed nonconvex optimization methods, this paper proposes a decentralized proximal primal-dual algorithm enabling double protection of privacy ($\\text{DPP}^2$) for minimizing nonconvex sum-utility functions over multi-agent networks, which ensures zero leakage of critical local information during inter-agent communications. We develop a two-tier privacy protection mechanism that first merges the primal and dual variables by means of a variable transformation, followed by embedding an additional random perturbation to further obfuscate the transmitted information. We theoretically establish that $\\text{DPP}^2$ ensures differential privacy for local objectives while achieving exact convergence under nonconvex settings. Specifically, $\\text{DPP}^2$ converges sublinearly to a stationary point and attains a linear convergence rate under the additional Polyak-Åojasiewicz (P-Å) condition. Finally, a numerical example demonstrates the superiority of $\\text{DPP}^2$ over a number of state-of-the-art algorithms, showcasing the faster, exact convergence achieved by $\\text{DPP}^2$ under the same level of differential privacy.",
    "updated" : "2025-11-04T05:51:34Z",
    "published" : "2025-11-04T05:51:34Z",
    "authors" : [
      {
        "name" : "Zichong Ou"
      },
      {
        "name" : "Dandan Wang"
      },
      {
        "name" : "Zixuan Liu"
      },
      {
        "name" : "Jie Lu"
      }
    ],
    "categories" : [
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.02227v2",
    "title" : "Interval Estimation for Binomial Proportions Under Differential Privacy",
    "summary" : "When releasing binary proportions computed using sensitive data, several government agencies and other data stewards protect confidentiality of the underlying values by ensuring the released statistics satisfy differential privacy. Typically, this is done by adding carefully chosen noise to the sample proportion computed using the confidential data. In this article, we describe and compare methods for turning this differentially private proportion into an interval estimate for an underlying population probability. Specifically, we consider differentially private versions of the Wald and Wilson intervals, Bayesian credible intervals based on denoising the differentially private proportion, and an exact interval motivated by the Clopper-Pearson confidence interval. We examine the repeated sampling performances of the intervals using simulation studies under both the Laplace mechanism and discrete Gaussian mechanism across a range of privacy guarantees. We find that while several methods can offer reasonable performances, the Bayesian credible intervals are the most attractive.",
    "updated" : "2025-11-05T16:55:15Z",
    "published" : "2025-11-04T03:41:10Z",
    "authors" : [
      {
        "name" : "Hsuan-Chen Kao"
      },
      {
        "name" : "Jerome P. Reiter"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.07441v1",
    "title" : "AudAgent: Automated Auditing of Privacy Policy Compliance in AI Agents",
    "summary" : "AI agents can autonomously perform tasks and, often without explicit user consent, collect or disclose users' sensitive local data, which raises serious privacy concerns. Although AI agents' privacy policies may describe their intended data practices, there remains limited transparency and accountability about whether runtime behavior matches those policies. To close this gap, we introduce AudAgent, a visual framework that continuously monitors AI agents' data practices in real time and guards compliance with stated privacy policies.\n  AudAgent consists of four components for automated privacy auditing of AI agents. (i) Policy parsing: an ensemble of LLMs translates natural-language privacy policies into a structured privacy-policy model, where cross-LLM voting guarantees confidence of the parsing results. (ii) Runtime annotation: a lightweight Presidio-based analyzer detects sensitive data and annotates how the data is used based on the context of the AI agent's operations and the privacy-policy model. (iii) Compliance auditing: ontology alignment and automata-based evaluation connect the policy model with runtime annotations, enabling on-the-fly compliance checks between the natural-language policy and observed unordered data practices of AI agents. (iv) User interface: a platform-independent implementation visualizes the real-time execution trace of AI agents along with potential privacy risks detected during auditing, providing user-friendly transparency and accountability.\n  In addition to common formatted privacy policies, AudAgent also supports user-defined policies for fine-grained control and customization. We evaluate AudAgent on AI agents built upon mainstream programming frameworks such as AutoGen, experiments show that AudAgent effectively identifies potential privacy policy violations in real time.",
    "updated" : "2025-11-03T17:32:08Z",
    "published" : "2025-11-03T17:32:08Z",
    "authors" : [
      {
        "name" : "Ye Zheng"
      },
      {
        "name" : "Yidan Hu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01752v1",
    "title" : "An assessment of the Commission's Proposal on Privacy and Electronic Communications",
    "summary" : "This study, commissioned by the European Parliament's Policy Department for Citizens Rights and Constitutional Affairs at the request of the LIBE Committee, appraises the European Commission's proposal for an ePrivacy Regulation. The study assesses whether the proposal would ensure that the right to the protection of personal data, the right to respect for private life and communications, and related rights enjoy a high standard of protection. The study also highlights the proposal's potential benefits and drawbacks more generally.",
    "updated" : "2025-11-03T17:01:35Z",
    "published" : "2025-11-03T17:01:35Z",
    "authors" : [
      {
        "name" : "Frederik Zuiderveen Borgesius"
      },
      {
        "name" : "Joris van Hoboken"
      },
      {
        "name" : "Ronan Fahy"
      },
      {
        "name" : "Kristina Irion"
      },
      {
        "name" : "Max Rozendaal"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01654v1",
    "title" : "Panther: A Cost-Effective Privacy-Preserving Framework for GNN Training and Inference Services in Cloud Environments",
    "summary" : "Graph Neural Networks (GNNs) have marked significant impact in traffic state prediction, social recommendation, knowledge-aware question answering and so on. As more and more users move towards cloud computing, it has become a critical issue to unleash the power of GNNs while protecting the privacy in cloud environments. Specifically, the training data and inference data for GNNs need to be protected from being stolen by external adversaries. Meanwhile, the financial cost of cloud computing is another primary concern for users. Therefore, although existing studies have proposed privacy-preserving techniques for GNNs in cloud environments, their additional computational and communication overhead remain relatively high, causing high financial costs that limit their widespread adoption among users.\n  To protect GNN privacy while lowering the additional financial costs, we introduce Panther, a cost-effective privacy-preserving framework for GNN training and inference services in cloud environments. Technically, Panther leverages four-party computation to asynchronously executing the secure array access protocol, and randomly pads the neighbor information of GNN nodes. We prove that Panther can protect privacy for both training and inference of GNN models. Our evaluation shows that Panther reduces the training and inference time by an average of 75.28% and 82.80%, respectively, and communication overhead by an average of 52.61% and 50.26% compared with the state-of-the-art, which is estimated to save an average of 55.05% and 59.00% in financial costs (based on on-demand pricing model) for the GNN training and inference process on Google Cloud Platform.",
    "updated" : "2025-11-03T15:15:40Z",
    "published" : "2025-11-03T15:15:40Z",
    "authors" : [
      {
        "name" : "Congcong Chen"
      },
      {
        "name" : "Xinyu Liu"
      },
      {
        "name" : "Kaifeng Huang"
      },
      {
        "name" : "Lifei Wei"
      },
      {
        "name" : "Yang Shi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01583v1",
    "title" : "Federated Cyber Defense: Privacy-Preserving Ransomware Detection Across Distributed Systems",
    "summary" : "Detecting malware, especially ransomware, is essential to securing today's interconnected ecosystems, including cloud storage, enterprise file-sharing, and database services. Training high-performing artificial intelligence (AI) detectors requires diverse datasets, which are often distributed across multiple organizations, making centralization necessary. However, centralized learning is often impractical due to security, privacy regulations, data ownership issues, and legal barriers to cross-organizational sharing. Compounding this challenge, ransomware evolves rapidly, demanding models that are both robust and adaptable.\n  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, which enables multiple organizations to collaboratively train a ransomware detection model while keeping raw data local and secure. This paradigm is particularly relevant for cybersecurity companies (including both software and hardware vendors) that deploy ransomware detection or firewall systems across millions of endpoints. In such environments, data cannot be transferred outside the customer's device due to strict security, privacy, or regulatory constraints. Although FL applies broadly to malware threats, we validate the approach using the Ransomware Storage Access Patterns (RanSAP) dataset.\n  Our experiments demonstrate that FL improves ransomware detection accuracy by a relative 9% over server-local models and achieves performance comparable to centralized training. These results indicate that FL offers a scalable, high-performing, and privacy-preserving framework for proactive ransomware detection across organizational and regulatory boundaries.",
    "updated" : "2025-11-03T13:54:13Z",
    "published" : "2025-11-03T13:54:13Z",
    "authors" : [
      {
        "name" : "Daniel M. Jimenez-Gutierrez"
      },
      {
        "name" : "Enrique Zuazua"
      },
      {
        "name" : "Joaquin Del Rio"
      },
      {
        "name" : "Oleksii Sliusarenko"
      },
      {
        "name" : "Xabi Uribe-Etxebarria"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01467v2",
    "title" : "Quantum Information Ordering and Differential Privacy",
    "summary" : "We study quantum differential privacy (QDP) by defining a notion of the order of informativeness between two pairs of quantum states. In particular, we show that if the hypothesis testing divergence of the one pair dominates over that of the other pair, then this dominance holds for every $f$-divergence. This approach completely characterizes $(\\varepsilon,Î´)$-QDP mechanisms by identifying the most informative $(\\varepsilon,Î´)$-DP quantum state pairs. We apply this to analyze the stability of quantum differentially private learning algorithms, generalizing classical results to the case $Î´>0$. Additionally, we study precise limits for privatized hypothesis testing and privatized quantum parameter estimation, including tight upper-bounds on the quantum Fisher information under QDP. Finally, we establish near-optimal contraction bounds for differentially private quantum channels with respect to the hockey-stick divergence.",
    "updated" : "2025-11-13T10:40:50Z",
    "published" : "2025-11-03T11:24:52Z",
    "authors" : [
      {
        "name" : "Ayanava Dasgupta"
      },
      {
        "name" : "Naqueeb Ahmad Warsi"
      },
      {
        "name" : "Masahito Hayashi"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01449v1",
    "title" : "Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction",
    "summary" : "To effectively manage the wastage of perishable fruits, it is crucial to accurately predict their freshness or shelf life using non-invasive methods that rely on visual data. In this regard, deep learning techniques can offer a viable solution. However, obtaining fine-grained fruit freshness labels from experts is costly, leading to a scarcity of data. Closed proprietary Vision Language Models (VLMs), such as Gemini, have demonstrated strong performance in fruit freshness detection task in both zero-shot and few-shot settings. Nonetheless, food retail organizations are unable to utilize these proprietary models due to concerns related to data privacy, while existing open-source VLMs yield sub-optimal performance for the task. Fine-tuning these open-source models with limited data fails to achieve the performance levels of proprietary models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning (MAOML) algorithm, designed to train smaller VLMs. This approach utilizes meta-learning to address data sparsity and leverages label ordinality, thereby achieving state-of-the-art performance in the fruit freshness classification task under both zero-shot and few-shot settings. Our method achieves an industry-standard accuracy of 92.71%, averaged across all fruits.\n  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning, Ordinal Regression",
    "updated" : "2025-11-03T11:03:54Z",
    "published" : "2025-11-03T11:03:54Z",
    "authors" : [
      {
        "name" : "Riddhi Jain"
      },
      {
        "name" : "Manasi Patwardhan"
      },
      {
        "name" : "Aayush Mishra"
      },
      {
        "name" : "Parijat Deshpande"
      },
      {
        "name" : "Beena Rai"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01307v1",
    "title" : "Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models",
    "summary" : "Recent advances in diffusion models have enabled high-quality synthesis of specific subjects, such as identities or objects. This capability, while unlocking new possibilities in content creation, also introduces significant privacy risks, as personalization techniques can be misused by malicious users to generate unauthorized content. Although several studies have attempted to counter this by generating adversarially perturbed samples designed to disrupt personalization, they rely on unrealistic assumptions and become ineffective in the presence of even a few clean images or under simple image transformations. To address these challenges, we shift the protection target from the images to the diffusion model itself to hinder the personalization of specific subjects, through our novel framework called Anti-Personalized Diffusion Models (APDM). We first provide a theoretical analysis demonstrating that a naive approach of existing loss functions to diffusion models is inherently incapable of ensuring convergence for robust anti-personalization. Motivated by this finding, we introduce Direct Protective Optimization (DPO), a novel loss function that effectively disrupts subject personalization in the target model without compromising generative quality. Moreover, we propose a new dual-path optimization strategy, coined Learning to Protect (L2P). By alternating between personalization and protection paths, L2P simulates future personalization trajectories and adaptively reinforces protection at each step. Experimental results demonstrate that our framework outperforms existing methods, achieving state-of-the-art performance in preventing unauthorized personalization. The code is available at https://github.com/KU-VGI/APDM.",
    "updated" : "2025-11-03T07:42:05Z",
    "published" : "2025-11-03T07:42:05Z",
    "authors" : [
      {
        "name" : "Tae-Young Lee"
      },
      {
        "name" : "Juwon Seo"
      },
      {
        "name" : "Jong Hwan Ko"
      },
      {
        "name" : "Gyeong-Moon Park"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.01197v3",
    "title" : "CryptoMoE: Privacy-Preserving and Scalable Mixture of Experts Inference via Balanced Expert Routing",
    "summary" : "Private large language model (LLM) inference based on cryptographic primitives offers a promising path towards privacy-preserving deep learning. However, existing frameworks only support dense LLMs like LLaMA-1 and struggle to scale to mixture-of-experts (MoE) architectures. The key challenge comes from securely evaluating the dynamic routing mechanism in MoE layers, which may reveal sensitive input information if not fully protected. In this paper, we propose CryptoMoE, the first framework that enables private, efficient, and accurate inference for MoE-based models. CryptoMoE balances expert loads to protect expert routing information and proposes novel protocols for secure expert dispatch and combine. CryptoMoE also develops a confidence-aware token selection strategy and a batch matrix multiplication protocol to improve accuracy and efficiency further. Extensive experiments on DeepSeekMoE-16.4B, OLMoE-6.9B, and QWenMoE-14.3B show that CryptoMoE achieves $2.8\\sim3.5\\times$ end-to-end latency reduction and $2.9\\sim4.3\\times$ communication reduction over a dense baseline with minimum accuracy loss. We also adapt CipherPrune (ICLR'25) for MoE inference and demonstrate CryptoMoE can reduce the communication by up to $4.3 \\times$. Code is available at: https://github.com/PKU-SEC-Lab/CryptoMoE.",
    "updated" : "2025-11-11T12:04:37Z",
    "published" : "2025-11-03T03:45:08Z",
    "authors" : [
      {
        "name" : "Yifan Zhou"
      },
      {
        "name" : "Tianshi Xu"
      },
      {
        "name" : "Jue Hong"
      },
      {
        "name" : "Ye Wu"
      },
      {
        "name" : "Meng Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00795v1",
    "title" : "FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data",
    "summary" : "Federated Learning (FL) allows multiple institutions to cooperatively train machine learning models while retaining sensitive data at the source, which has great utility in privacy-sensitive environments. However, FL systems remain vulnerable to membership-inference attacks and data heterogeneity. This paper presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using synthetic oncologic CT scans with tumor annotations. It evaluates segmentation performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and FedAvg with DP-SGD. Results show a distinct trade-off between privacy and utility: FedAvg is high performance (Dice around 0.85) with more privacy leakage (attack AUC about 0.72), while DP-SGD provides a higher level of privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx and FedBN offer balanced performance under heterogeneous data, especially with non-identical distributed client data. FedOnco-Bench serves as a standardized, open-source platform for benchmarking and developing privacy-preserving FL methods for medical image segmentation.",
    "updated" : "2025-11-02T04:17:14Z",
    "published" : "2025-11-02T04:17:14Z",
    "authors" : [
      {
        "name" : "Viswa Chaitanya Marella"
      },
      {
        "name" : "Suhasnadh Reddy Veluru"
      },
      {
        "name" : "Sai Teja Erukude"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00737v1",
    "title" : "EP-HDC: Hyperdimensional Computing with Encrypted Parameters for High-Throughput Privacy-Preserving Inference",
    "summary" : "While homomorphic encryption (HE) provides strong privacy protection, its high computational cost has restricted its application to simple tasks. Recently, hyperdimensional computing (HDC) applied to HE has shown promising performance for privacy-preserving machine learning (PPML). However, when applied to more realistic scenarios such as batch inference, the HDC-based HE has still very high compute time as well as high encryption and data transmission overheads. To address this problem, we propose HDC with encrypted parameters (EP-HDC), which is a novel PPML approach featuring client-side HE, i.e., inference is performed on a client using a homomorphically encrypted model. Our EP-HDC can effectively mitigate the encryption and data transmission overhead, as well as providing high scalability with many clients while providing strong protection for user data and model parameters. In addition to application examples for our client-side PPML, we also present design space exploration involving quantization, architecture, and HE-related parameters. Our experimental results using the BFV scheme and the Face/Emotion datasets demonstrate that our method can improve throughput and latency of batch inference by orders of magnitude over previous PPML methods (36.52~1068x and 6.45~733x, respectively) with less than 1% accuracy degradation.",
    "updated" : "2025-11-01T23:22:01Z",
    "published" : "2025-11-01T23:22:01Z",
    "authors" : [
      {
        "name" : "Jaewoo Park"
      },
      {
        "name" : "Chenghao Quan"
      },
      {
        "name" : "Jongeun Lee"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00700v1",
    "title" : "Privacy-Aware Time Series Synthesis via Public Knowledge Distillation",
    "summary" : "Sharing sensitive time series data in domains such as finance, healthcare, and energy consumption, such as patient records or investment accounts, is often restricted due to privacy concerns. Privacy-aware synthetic time series generation addresses this challenge by enforcing noise during training, inherently introducing a trade-off between privacy and utility. In many cases, sensitive sequences is correlated with publicly available, non-sensitive contextual metadata (e.g., household electricity consumption may be influenced by weather conditions and electricity prices). However, existing privacy-aware data generation methods often overlook this opportunity, resulting in suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a novel framework for generating private time series data by leveraging heterogeneous public knowledge. Our model employs a self-attention mechanism to encode public data into temporal and feature embeddings, which serve as conditional inputs for a diffusion model to generate synthetic private sequences. Additionally, we introduce a practical metric to assess privacy by evaluating the identifiability of the synthetic data. Experimental results show that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving the privacy-utility trade-off across finance, energy, and commodity trading domains.",
    "updated" : "2025-11-01T20:44:24Z",
    "published" : "2025-11-01T20:44:24Z",
    "authors" : [
      {
        "name" : "Penghang Liu"
      },
      {
        "name" : "Haibei Zhu"
      },
      {
        "name" : "Eleonora Kreacic"
      },
      {
        "name" : "Svitlana Vyetrenko"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00487v1",
    "title" : "With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting",
    "summary" : "Recent work in Differential Privacy with Natural Language Processing (DP NLP) has proposed numerous promising techniques in the form of text rewriting mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is that of dataset size, or rather, the effect of dataset size on a mechanism's efficacy for utility and privacy preservation. In this work, we are the first to introduce this factor in the evaluation of DP text privatization, where we design utility and privacy tests on large-scale datasets with dynamic split sizes. We run these tests on datasets of varying size with up to one million texts, and we focus on quantifying the effect of increasing dataset size on the privacy-utility trade-off. Our findings reveal that dataset size plays an integral part in evaluating DP text rewriting mechanisms; additionally, these findings call for more rigorous evaluation procedures in DP NLP, as well as shed light on the future of DP NLP in practice and at scale.",
    "updated" : "2025-11-01T10:41:05Z",
    "published" : "2025-11-01T10:41:05Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00467v1",
    "title" : "A Big Step Forward? A User-Centric Examination of iOS App Privacy Report and Enhancements",
    "summary" : "The prevalent engagement with mobile apps underscores the importance of understanding their data practices. Transparency plays a crucial role in this context, ensuring users to be informed and give consent before any data access occurs. Apple introduced a new feature since iOS 15.2, App Privacy Report, to inform users about detailed insights into apps' data access and sharing. This feature continues Apple's trend of privacy-focused innovations (following Privacy Nutrition Labels), and has been marketed as a big step forward in user privacy. However, its real-world impacts on user privacy and control remain unexamined. We thus proposed an end-to-end study involving systematic assessment of the App Privacy Report's real-world benefits and limitations, LLM-enabled and multi-technique synthesized enhancements, and comprehensive evaluation from both system and user perspectives. Through a structured focus group study with twelve everyday iOS users, we explored their experiences, understanding, and perceptions of the feature, suggesting its limited practical impact resulting from missing important details. We identified two primary user concerns: the clarity of data access purpose and domain description. In response, we proposed enhancements including a purpose inference framework and domain clarification pipeline. We demonstrated the effectiveness and benefits of such enhancements for mobile app users. This work provides practical insights that could help enhance user privacy transparency and discusses areas for future research.",
    "updated" : "2025-11-01T09:29:04Z",
    "published" : "2025-11-01T09:29:04Z",
    "authors" : [
      {
        "name" : "Liu Wang"
      },
      {
        "name" : "Dong Wang"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Zheng Jiang"
      },
      {
        "name" : "Haoyu Wang"
      },
      {
        "name" : "Yi Wang"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.00414v1",
    "title" : "Embedding based Encoding Scheme for Privacy Preserving Record Linkage",
    "summary" : "To discover new insights from data, there is a growing need to share information that is often held by different organisations. One key task in data integration is the calculation of similarities between records in different databases to identify pairs or sets of records that correspond to the same real-world entities. Due to privacy and confidentiality concerns, however, the owners of sensitive databases are often not allowed or willing to exchange or share their data with other organisations to allow such similarity calculations. Privacy-preserving record linkage (PPRL) is the process of matching records that refer to the same entity across sensitive databases held by different organisations while ensuring no information about the entities is revealed to the participating parties. In this paper, we study how embedding based encoding techniques can be applied in the PPRL context to ensure the privacy of the entities that are being linked. We first convert individual q-grams into the embedded space and then convert the embedding of a set of q-grams of a given record into a binary representation. The final binary representations can be used to link records into matches and non-matches. We empirically evaluate our proposed encoding technique against different real-world datasets. The results suggest that our proposed encoding approach can provide better linkage accuracy and protect the privacy of individuals against attack compared to state-of-the-art techniques for short record values.",
    "updated" : "2025-11-01T05:57:21Z",
    "published" : "2025-11-01T05:57:21Z",
    "authors" : [
      {
        "name" : "Sirintra Vaiwsri"
      },
      {
        "name" : "Thilina Ranbaduge"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.11347v1",
    "title" : "Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions",
    "summary" : "Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.",
    "updated" : "2025-11-14T14:33:58Z",
    "published" : "2025-11-14T14:33:58Z",
    "authors" : [
      {
        "name" : "Shaowei Guan"
      },
      {
        "name" : "Hin Chi Kwok"
      },
      {
        "name" : "Ngai Fong Law"
      },
      {
        "name" : "Gregor Stiglic"
      },
      {
        "name" : "Vivian Hui"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.11249v1",
    "title" : "Bridging Local and Federated Data Normalization in Federated Learning: A Privacy-Preserving Approach",
    "summary" : "Data normalization is a crucial preprocessing step for enhancing model performance and training stability. In federated learning (FL), where data remains distributed across multiple parties during collaborative model training, normalization presents unique challenges due to the decentralized and often heterogeneous nature of the data. Traditional methods rely on either independent client-side processing, i.e., local normalization, or normalizing the entire dataset before distributing it to parties, i.e., pooled normalization. Local normalization can be problematic when data distributions across parties are non-IID, while the pooled normalization approach conflicts with the decentralized nature of FL. In this paper, we explore the adaptation of widely used normalization techniques to FL and define the term federated normalization. Federated normalization simulates pooled normalization by enabling the collaborative exchange of normalization parameters among parties. Thus, it achieves performance on par with pooled normalization without compromising data locality. However, sharing normalization parameters such as the mean introduces potential privacy risks, which we further mitigate through a robust privacy-preserving solution. Our contributions include: (i) We systematically evaluate the impact of various federated and local normalization techniques in heterogeneous FL scenarios, (ii) We propose a novel homomorphically encrypted $k$-th ranked element (and median) calculation tailored for the federated setting, enabling secure and efficient federated normalization, (iii) We propose privacy-preserving implementations of widely used normalization techniques for FL, leveraging multiparty fully homomorphic encryption (MHE).",
    "updated" : "2025-11-14T12:48:58Z",
    "published" : "2025-11-14T12:48:58Z",
    "authors" : [
      {
        "name" : "Melih CoÅÄun"
      },
      {
        "name" : "Mert GenÃ§tÃ¼rk"
      },
      {
        "name" : "Sinem Sav"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.11209v1",
    "title" : "Towards Usable Privacy Management for IoT TAPs: Deriving Privacy Clusters and Preference Profiles",
    "summary" : "IoT Trigger-Action Platforms (TAPs) typically offer coarse-grained permission controls. Even when fine-grained controls are available, users are likely overwhelmed by the complexity of setting privacy preferences. This paper contributes to usable privacy management for TAPs by deriving privacy clusters and profiles for different types of users that can be semi-automatically assigned or suggested to them. We developed and validated a questionnaire, based on users' privacy concerns regarding confidentiality and control and their requirements towards transparency in TAPs. In an online study (N=301), where participants were informed about potential privacy risks, we clustered users by their privacy concerns and requirements into Basic, Medium and High Privacy clusters. These clusters were then characterized by the users' data sharing preferences, based on a factorial vignette approach, considering the data categories, the data recipient types, and the purpose of data sharing. Our findings show three distinct privacy profiles, providing a foundation for more usable privacy controls in TAPs.",
    "updated" : "2025-11-14T12:08:58Z",
    "published" : "2025-11-14T12:08:58Z",
    "authors" : [
      {
        "name" : "Piero Romare"
      },
      {
        "name" : "Farzaneh Karegar"
      },
      {
        "name" : "Simone Fischer-HÃ¼bner"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.10771v1",
    "title" : "Privacy protection under the exposure of systems' prior information",
    "summary" : "For systems whose states implicate sensitive information, their privacy is of great concern. While notions like differential privacy have been successfully introduced to dynamical systems, it is still unclear how a system's privacy can be properly protected when facing the challenging yet frequently-encountered scenario where an adversary possesses prior knowledge, e.g., the steady state, of the system. This paper presents a new systematic approach to protect the privacy of a discrete-time linear time-invariant system against adversaries knowledgeable of the system's prior information. We employ a tailored \\emph{pointwise maximal leakage (PML) privacy} criterion. PML characterizes the worst-case privacy performance, which is sharply different from that of the better-known mutual-information privacy. We derive necessary and sufficient conditions for PML privacy and construct tractable design procedures. Furthermore, our analysis leads to insight into how PML privacy, differential privacy, and mutual-information privacy are related. We then revisit Kalman filters from the perspective of PML privacy and derive a lower bound on the steady-state estimation-error covariance in terms of the PML parameters. Finally, the derived results are illustrated in a case study of privacy protection for distributed sensing in smart buildings.",
    "updated" : "2025-11-13T19:47:40Z",
    "published" : "2025-11-13T19:47:40Z",
    "authors" : [
      {
        "name" : "Le Liu"
      },
      {
        "name" : "Yu Kawano"
      },
      {
        "name" : "Ming Cao"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.IT",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.13576v1",
    "title" : "Exploring the Effectiveness of Google Play Store's Privacy Transparency Channels",
    "summary" : "With the requirements and emphases on privacy transparency placed by regulations such as GDPR and CCPA, the Google Play Store requires Android developers to more responsibly communicate their apps' privacy practices to potential users by providing the proper information via the data safety, privacy policy, and permission manifest privacy transparency channels. However, it is unclear how effective those channels are in helping users make informed decisions in the app selection and installation process. In this article, we conducted a study for 190 participants to interact with our simulated privacy transparency channels of mobile apps. We quantitatively analyzed (supplemented by qualitative analysis) participants' responses to five sets of questions. We found that data safety provides the most intuitive user interfaces, privacy policy is most informative and effective, while permission manifest excels at raising participants' concerns about an app's overall privacy risks. These channels complement each other and should all be improved.",
    "updated" : "2025-11-17T16:40:19Z",
    "published" : "2025-11-17T16:40:19Z",
    "authors" : [
      {
        "name" : "Anhao Xiang"
      },
      {
        "name" : "Weiping Pei"
      },
      {
        "name" : "Chuan Yue"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.13502v1",
    "title" : "Tight and Practical Privacy Auditing for Differentially Private In-Context Learning",
    "summary" : "Large language models (LLMs) perform in-context learning (ICL) by adapting to tasks from prompt demonstrations, which in practice often contain private or proprietary data. Although differential privacy (DP) with private voting is a pragmatic mitigation, DP-ICL implementations are error-prone, and worst-case DP bounds may substantially overestimate actual leakage, calling for practical auditing tools. We present a tight and efficient privacy auditing framework for DP-ICL systems that runs membership inference attacks and translates their success rates into empirical privacy guarantees using Gaussian DP. Our analysis of the private voting mechanism identifies vote configurations that maximize the auditing signal, guiding the design of audit queries that reliably reveal whether a canary demonstration is present in the context. The framework supports both black-box (API-only) and white-box (internal vote) threat models, and unifies auditing for classification and generation by reducing both to a binary decision problem. Experiments on standard text classification and generation benchmarks show that our empirical leakage estimates closely match theoretical DP budgets on classification tasks and are consistently lower on generation tasks due to conservative embedding-sensitivity bounds, making our framework a practical privacy auditor and verifier for real-world DP-ICL deployments.",
    "updated" : "2025-11-17T15:39:54Z",
    "published" : "2025-11-17T15:39:54Z",
    "authors" : [
      {
        "name" : "Yuyang Xia"
      },
      {
        "name" : "Ruixuan Liu"
      },
      {
        "name" : "Li Xiong"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.13365v1",
    "title" : "InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference",
    "summary" : "Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at https://github.com/SASA-cloud/InfoDecom.",
    "updated" : "2025-11-17T13:36:40Z",
    "published" : "2025-11-17T13:36:40Z",
    "authors" : [
      {
        "name" : "Ruijun Deng"
      },
      {
        "name" : "Zhihui Lu"
      },
      {
        "name" : "Qiang Duan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.13319v1",
    "title" : "Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs",
    "summary" : "Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.\n  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $Îµ$-local differential privacy ($Îµ$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.",
    "updated" : "2025-11-17T12:56:33Z",
    "published" : "2025-11-17T12:56:33Z",
    "authors" : [
      {
        "name" : "Chelsea McMurray"
      },
      {
        "name" : "Hayder Tirmazi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.12936v1",
    "title" : "Privacy-Preserving Federated Learning from Partial Decryption Verifiable Threshold Multi-Client Functional Encryption",
    "summary" : "In federated learning, multiple parties can cooperate to train the model without directly exchanging their own private data, but the gradient leakage problem still threatens the privacy security and model integrity. Although the existing scheme uses threshold cryptography to mitigate the inference attack, it can not guarantee the verifiability of the aggregation results, making the system vulnerable to the threat of poisoning attack. We construct a partial decryption verifiable threshold multi client function encryption scheme, and apply it to Federated learning to implement the federated learning verifiable threshold security aggregation protocol (VTSAFL). VTSAFL empowers clients to verify aggregation results, concurrently minimizing both computational and communication overhead. The size of the functional key and partial decryption results of the scheme are constant, which provides efficiency guarantee for large-scale deployment. The experimental results on MNIST dataset show that vtsafl can achieve the same accuracy as the existing scheme, while reducing the total training time by more than 40%, and reducing the communication overhead by up to 50%. This efficiency is critical for overcoming the resource constraints inherent in Internet of Things (IoT) devices.",
    "updated" : "2025-11-17T03:44:47Z",
    "published" : "2025-11-17T03:44:47Z",
    "authors" : [
      {
        "name" : "Minjie Wang"
      },
      {
        "name" : "Jinguang Han"
      },
      {
        "name" : "Weizhi Meng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.12841v1",
    "title" : "SoK: Synthesizing Smart Home Privacy Protection Mechanisms Across Academic Proposals and Commercial Documentations",
    "summary" : "Pervasive data collection by Smart Home Devices (SHDs) demands robust Privacy Protection Mechanisms (PPMs). The effectiveness of many PPMs, particularly user-facing controls, depends on user awareness and adoption, which are shaped by manufacturers' public documentations. However, the landscape of academic proposals and commercial disclosures remains underexplored. To address this gap, we investigate: (1) What PPMs have academics proposed, and how are these PPMs evaluated? (2) What PPMs do manufacturers document and what factors affect these documentation? To address these questions, we conduct a two-phase study, synthesizing a systematic review of 117 academic papers with an empirical analysis of 86 SHDs' publicly disclosed documentations. Our review of academic literature reveals a strong focus on novel system- and algorithm-based PPMs. However, these proposals neglect deployment barriers (e.g., cost, interoperability), and lack real-world field validation and legal analysis. Concurrently, our analysis of commercial SHDs finds that advanced academic proposals are absent from public discourse. Industry postures are fundamentally reactive, prioritizing compliance via post-hoc data management (e.g., deletion options), rather than the preventative controls favored by academia. The documented protections correspondingly converge on a small set of practical mechanisms, such as physical buttons and localized processing. By synthesizing these findings, we advocate for research to analyze challenges, provide deployable frameworks, real-world field validation, and interoperability solutions to advance practical PPMs.",
    "updated" : "2025-11-17T00:08:50Z",
    "published" : "2025-11-17T00:08:50Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Yijing Liu"
      },
      {
        "name" : "Yuyu Liu"
      },
      {
        "name" : "Ying Ma"
      },
      {
        "name" : "Shixuan Li"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Qian Wu"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.12575v1",
    "title" : "Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection",
    "summary" : "Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.",
    "updated" : "2025-11-16T12:27:59Z",
    "published" : "2025-11-16T12:27:59Z",
    "authors" : [
      {
        "name" : "Jiayi Zhu"
      },
      {
        "name" : "Yihao Huang"
      },
      {
        "name" : "Yue Cao"
      },
      {
        "name" : "Xiaojun Jia"
      },
      {
        "name" : "Qing Guo"
      },
      {
        "name" : "Felix Juefei-Xu"
      },
      {
        "name" : "Geguang Pu"
      },
      {
        "name" : "Bin Wang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.12377v1",
    "title" : "On the Security and Privacy of AI-based Mobile Health Chatbots",
    "summary" : "The rise of Artificial Intelligence (AI) has impacted the development of mobile health (mHealth) apps, most notably with the advent of AI-based chatbots used as ubiquitous ``companions'' for various services, from fitness to mental health assistants. While these mHealth chatbots offer clear benefits, such as personalized health information and predictive diagnoses, they also raise significant concerns regarding security and privacy. This study empirically assesses 16 AI-based mHealth chatbots identified from the Google Play Store. The empirical assessment follows a three-phase approach (manual inspection, static code analysis, and dynamic analysis) to evaluate technical robustness and how design and implementation choices impact end users. Our findings revealed security vulnerabilities (e.g., enabling Remote WebView debugging), privacy issues, and non-compliance with Google Play policies (e.g., failure to provide publicly accessible privacy policies). Based on our findings, we offer several recommendations to enhance the security and privacy of mHealth chatbots. These recommendations focus on improving data handling processes, disclosure, and user security. Therefore, this work also seeks to support mHealth developers and security/privacy engineers in designing more transparent, privacy-friendly, and secure mHealth chatbots.",
    "updated" : "2025-11-15T22:49:07Z",
    "published" : "2025-11-15T22:49:07Z",
    "authors" : [
      {
        "name" : "Samuel Wairimu"
      },
      {
        "name" : "Leonardo Horn Iwaya"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.12295v1",
    "title" : "Privacy-Preserving Prompt Injection Detection for LLMs Using Federated Learning and Embedding-Based NLP Classification",
    "summary" : "Prompt injection attacks are an emerging threat to large language models (LLMs), enabling malicious users to manipulate outputs through carefully designed inputs. Existing detection approaches often require centralizing prompt data, creating significant privacy risks. This paper proposes a privacy-preserving prompt injection detection framework based on federated learning and embedding-based classification. A curated dataset of benign and adversarial prompts was encoded with sentence embedding and used to train both centralized and federated logistic regression models. The federated approach preserved privacy by sharing only model parameters across clients, while achieving detection performance comparable to centralized training. Results demonstrate that effective prompt injection detection is feasible without exposing raw data, making this one of the first explorations of federated security for LLMs. Although the dataset is limited in scale, the findings establish a strong proof-of-concept and highlight new directions for building secure and privacy-aware LLM systems.",
    "updated" : "2025-11-15T17:11:14Z",
    "published" : "2025-11-15T17:11:14Z",
    "authors" : [
      {
        "name" : "Hasini Jayathilaka"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.11811v1",
    "title" : "Lessons Learned from Developing a Privacy-Preserving Multimodal Wearable for Local Voice-and-Vision Inference",
    "summary" : "Many promising applications of multimodal wearables require continuous sensing and heavy computation, yet users reject such devices due to privacy concerns. This paper shares our experiences building an ear-mounted voice-and-vision wearable that performs local AI inference using a paired smartphone as a trusted personal edge. We describe the hardware--software co-design of this privacy-preserving system, including challenges in integrating a camera, microphone, and speaker within a 30-gram form factor, enabling wake word-triggered capture, and running quantized vision-language and large-language models entirely offline. Through iterative prototyping, we identify key design hurdles in power budgeting, connectivity, latency, and social acceptability. Our initial evaluation shows that fully local multimodal inference is feasible on commodity mobile hardware with interactive latency. We conclude with design lessons for researchers developing embedded AI systems that balance privacy, responsiveness, and usability in everyday settings.",
    "updated" : "2025-11-14T19:04:52Z",
    "published" : "2025-11-14T19:04:52Z",
    "authors" : [
      {
        "name" : "Yonatan Tussa"
      },
      {
        "name" : "Andy Heredia"
      },
      {
        "name" : "Nirupam Roy"
      }
    ],
    "categories" : [
      "cs.HC",
      "eess.AS",
      "eess.IV",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.11347v2",
    "title" : "Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions",
    "summary" : "Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.",
    "updated" : "2025-11-17T03:23:50Z",
    "published" : "2025-11-14T14:33:58Z",
    "authors" : [
      {
        "name" : "Shaowei Guan"
      },
      {
        "name" : "Hin Chi Kwok"
      },
      {
        "name" : "Ngai Fong Law"
      },
      {
        "name" : "Gregor Stiglic"
      },
      {
        "name" : "Harry Qin"
      },
      {
        "name" : "Vivian Hui"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.10423v2",
    "title" : "Enhanced Privacy Leakage from Noise-Perturbed Gradients via Gradient-Guided Conditional Diffusion Models",
    "summary" : "Federated learning synchronizes models through gradient transmission and aggregation. However, these gradients pose significant privacy risks, as sensitive training data is embedded within them. Existing gradient inversion attacks suffer from significantly degraded reconstruction performance when gradients are perturbed by noise-a common defense mechanism. In this paper, we introduce gradient-guided conditional diffusion models for reconstructing private images from leaked gradients, without prior knowledge of the target data distribution. Our approach leverages the inherent denoising capability of diffusion models to circumvent the partial protection offered by noise perturbation, thereby improving attack performance under such defenses. We further provide a theoretical analysis of the reconstruction error bounds and the convergence properties of the attack loss, characterizing the impact of key factors-such as noise magnitude and attacked model architecture-on reconstruction quality. Extensive experiments demonstrate our attack's superior reconstruction performance with Gaussian noise-perturbed gradients, and confirm our theoretical findings.",
    "updated" : "2025-11-16T12:37:26Z",
    "published" : "2025-11-13T15:43:45Z",
    "authors" : [
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Hong Chen"
      },
      {
        "name" : "Chen Hou"
      },
      {
        "name" : "Guolong Zheng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.14524v1",
    "title" : "Compression with Privacy-Preserving Random Access",
    "summary" : "It is shown that an i.i.d. binary source sequence $X_1, \\ldots, X_n$ can be losslessly compressed at any rate above entropy such that the individual decoding of any $X_i$ reveals \\emph{no} information about the other bits $\\{X_j : j \\neq i\\}$.",
    "updated" : "2025-11-18T14:24:25Z",
    "published" : "2025-11-18T14:24:25Z",
    "authors" : [
      {
        "name" : "Venkat Chandar"
      },
      {
        "name" : "Aslan Tchamkerten"
      },
      {
        "name" : "Shashank Vatedka"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.14084v1",
    "title" : "Observational Auditing of Label Privacy",
    "summary" : "Differential privacy (DP) auditing is essential for evaluating privacy guarantees in machine learning systems. Existing auditing methods, however, pose a significant challenge for large-scale systems since they require modifying the training dataset -- for instance, by injecting out-of-distribution canaries or removing samples from training. Such interventions on the training data pipeline are resource-intensive and involve considerable engineering overhead. We introduce a novel observational auditing framework that leverages the inherent randomness of data distributions, enabling privacy evaluation without altering the original dataset. Our approach extends privacy auditing beyond traditional membership inference to protected attributes, with labels as a special case, addressing a key gap in existing techniques. We provide theoretical foundations for our method and perform experiments on Criteo and CIFAR-10 datasets that demonstrate its effectiveness in auditing label privacy guarantees. This work opens new avenues for practical privacy auditing in large-scale production environments.",
    "updated" : "2025-11-18T03:12:59Z",
    "published" : "2025-11-18T03:12:59Z",
    "authors" : [
      {
        "name" : "Iden Kalemaj"
      },
      {
        "name" : "Luca Melis"
      },
      {
        "name" : "Maxime Boucher"
      },
      {
        "name" : "Ilya Mironov"
      },
      {
        "name" : "Saeed Mahloujifar"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.14045v1",
    "title" : "GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards",
    "summary" : "Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.\n  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.\n  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.",
    "updated" : "2025-11-18T01:51:34Z",
    "published" : "2025-11-18T01:51:34Z",
    "authors" : [
      {
        "name" : "Yule Liu"
      },
      {
        "name" : "Heyi Zhang"
      },
      {
        "name" : "Jinyi Zheng"
      },
      {
        "name" : "Zhen Sun"
      },
      {
        "name" : "Zifan Peng"
      },
      {
        "name" : "Tianshuo Cong"
      },
      {
        "name" : "Yilong Yang"
      },
      {
        "name" : "Xinlei He"
      },
      {
        "name" : "Zhuo Ma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.07242v3",
    "title" : "Privacy on the Fly: A Predictive Adversarial Transformation Network for Mobile Sensor Data",
    "summary" : "Mobile motion sensors such as accelerometers and gyroscopes are now ubiquitously accessible by third-party apps via standard APIs. While enabling rich functionalities like activity recognition and step counting, this openness has also enabled unregulated inference of sensitive user traits, such as gender, age, and even identity, without user consent. Existing privacy-preserving techniques, such as GAN-based obfuscation or differential privacy, typically require access to the full input sequence, introducing latency that is incompatible with real-time scenarios. Worse, they tend to distort temporal and semantic patterns, degrading the utility of the data for benign tasks like activity recognition. To address these limitations, we propose the Predictive Adversarial Transformation Network (PATN), a real-time privacy-preserving framework that leverages historical signals to generate adversarial perturbations proactively. The perturbations are applied immediately upon data acquisition, enabling continuous protection without disrupting application functionality. Experiments on two datasets demonstrate that PATN substantially degrades the performance of privacy inference models, achieving Attack Success Rate (ASR) of 40.11% and 44.65% (reducing inference accuracy to near-random) and increasing the Equal Error Rate (EER) from 8.30% and 7.56% to 41.65% and 46.22%. On ASR, PATN outperforms baseline methods by 16.16% and 31.96%, respectively.",
    "updated" : "2025-11-18T14:23:51Z",
    "published" : "2025-11-10T15:57:17Z",
    "authors" : [
      {
        "name" : "Tianle Song"
      },
      {
        "name" : "Chenhao Lin"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Zhengyu Zhao"
      },
      {
        "name" : "Jiahao Sun"
      },
      {
        "name" : "Chong Zhang"
      },
      {
        "name" : "Le Yang"
      },
      {
        "name" : "Chao Shen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.15634v1",
    "title" : "RÃ©nyi Differential Privacy for Heavy-Tailed SDEs via Fractional PoincarÃ© Inequalities",
    "summary" : "Characterizing the differential privacy (DP) of learning algorithms has become a major challenge in recent years. In parallel, many studies suggested investigating the behavior of stochastic gradient descent (SGD) with heavy-tailed noise, both as a model for modern deep learning models and to improve their performance. However, most DP bounds focus on light-tailed noise, where satisfactory guarantees have been obtained but the proposed techniques do not directly extend to the heavy-tailed setting. Recently, the first DP guarantees for heavy-tailed SGD were obtained. These results provide $(0,Î´)$-DP guarantees without requiring gradient clipping. Despite casting new light on the link between DP and heavy-tailed algorithms, these results have a strong dependence on the number of parameters and cannot be extended to other DP notions like the well-established RÃ©nyi differential privacy (RDP). In this work, we propose to address these limitations by deriving the first RDP guarantees for heavy-tailed SDEs, as well as their discretized counterparts. Our framework is based on new RÃ©nyi flow computations and the use of well-established fractional PoincarÃ© inequalities. Under the assumption that such inequalities are satisfied, we obtain DP guarantees that have a much weaker dependence on the dimension compared to prior art.",
    "updated" : "2025-11-19T17:18:54Z",
    "published" : "2025-11-19T17:18:54Z",
    "authors" : [
      {
        "name" : "Benjamin Dupuis"
      },
      {
        "name" : "Mert GÃ¼rbÃ¼zbalaban"
      },
      {
        "name" : "Umut ÅimÅekli"
      },
      {
        "name" : "Jian Wang"
      },
      {
        "name" : "Sinan Yildirim"
      },
      {
        "name" : "Lingjiong Zhu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.15434v1",
    "title" : "Small Language Models for Phishing Website Detection: Cost, Performance, and Privacy Trade-Offs",
    "summary" : "Phishing websites pose a major cybersecurity threat, exploiting unsuspecting users and causing significant financial and organisational harm. Traditional machine learning approaches for phishing detection often require extensive feature engineering, continuous retraining, and costly infrastructure maintenance. At the same time, proprietary large language models (LLMs) have demonstrated strong performance in phishing-related classification tasks, but their operational costs and reliance on external providers limit their practical adoption in many business environments. This paper investigates the feasibility of small language models (SLMs) for detecting phishing websites using only their raw HTML code. A key advantage of these models is that they can be deployed on local infrastructure, providing organisations with greater control over data and operations. We systematically evaluate 15 commonly used Small Language Models (SLMs), ranging from 1 billion to 70 billion parameters, benchmarking their classification accuracy, computational requirements, and cost-efficiency. Our results highlight the trade-offs between detection performance and resource consumption, demonstrating that while SLMs underperform compared to state-of-the-art proprietary LLMs, they can still provide a viable and scalable alternative to external LLM services. By presenting a comparative analysis of costs and benefits, this work lays the foundation for future research on the adaptation, fine-tuning, and deployment of SLMs in phishing detection systems, aiming to balance security effectiveness and economic practicality.",
    "updated" : "2025-11-19T13:45:07Z",
    "published" : "2025-11-19T13:45:07Z",
    "authors" : [
      {
        "name" : "Georg Goldenits"
      },
      {
        "name" : "Philip Koenig"
      },
      {
        "name" : "Sebastian Raubitzek"
      },
      {
        "name" : "Andreas Ekelhart"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.15278v1",
    "title" : "Privacy-Preserving IoT in Connected Aircraft Cabin",
    "summary" : "The proliferation of IoT devices in shared, multi-vendor environments like the modern aircraft cabin creates a fundamental conflict between the promise of data collaboration and the risks to passenger privacy, vendor intellectual property (IP), and regulatory compliance. While emerging standards like the Cabin Secure Media-Independent Messaging (CSMIM) protocol provide a secure communication backbone, they do not resolve data governance challenges at the application layer, leaving a privacy gap that impedes trust. This paper proposes and evaluates a framework that closes this gap by integrating a configurable layer of Privacy-Enhancing Technologies (PETs) atop a CSMIM-like architecture. We conduct a rigorous, empirical analysis of two pragmatic PETs: Differential Privacy (DP) for statistical sharing, and an additive secret sharing scheme (ASS) for data obfuscation. Using a high-fidelity testbed with resource-constrained hardware, we quantify the trade-offs between data privacy, utility, and computing performance. Our results demonstrate that the computational overhead of PETs is often negligible compared to inherent network and protocol latencies. We prove that architectural choices, such as on-device versus virtualized processing, have a far greater impact on end-to-end latency and computational performance than the PETs themselves. The findings provide a practical roadmap for system architects to select and configure appropriate PETs, enabling the design of trustworthy collaborative IoT ecosystems in avionics and other critical domains.",
    "updated" : "2025-11-19T09:41:25Z",
    "published" : "2025-11-19T09:41:25Z",
    "authors" : [
      {
        "name" : "Nilesh Vyas"
      },
      {
        "name" : "Benjamin Zhao"
      },
      {
        "name" : "AygÃ¼n Baltaci"
      },
      {
        "name" : "Gustavo de Carvalho Bertoli"
      },
      {
        "name" : "Hassan Asghar"
      },
      {
        "name" : "Markus KlÃ¼gel"
      },
      {
        "name" : "Gerrit Schramm"
      },
      {
        "name" : "Martin Kubisch"
      },
      {
        "name" : "Dali Kaafar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.14936v1",
    "title" : "How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding",
    "summary" : "Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\\varepsilon \\in \\{4, 6\\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.",
    "updated" : "2025-11-18T21:51:04Z",
    "published" : "2025-11-18T21:51:04Z",
    "authors" : [
      {
        "name" : "Mathieu Dufour"
      },
      {
        "name" : "Andrew Duncan"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.16377v1",
    "title" : "Optimal Fairness under Local Differential Privacy",
    "summary" : "We investigate how to optimally design local differential privacy (LDP) mechanisms that reduce data unfairness and thereby improve fairness in downstream classification. We first derive a closed-form optimal mechanism for binary sensitive attributes and then develop a tractable optimization framework that yields the corresponding optimal mechanism for multi-valued attributes. As a theoretical contribution, we establish that for discrimination-accuracy optimal classifiers, reducing data unfairness necessarily leads to lower classification unfairness, thus providing a direct link between privacy-aware pre-processing and classification fairness. Empirically, we demonstrate that our approach consistently outperforms existing LDP mechanisms in reducing data unfairness across diverse datasets and fairness metrics, while maintaining accuracy close to that of non-private models. Moreover, compared with leading pre-processing and post-processing fairness methods, our mechanism achieves a more favorable accuracy-fairness trade-off while simultaneously preserving the privacy of sensitive attributes. Taken together, these results highlight LDP as a principled and effective pre-processing fairness intervention technique.",
    "updated" : "2025-11-20T14:00:15Z",
    "published" : "2025-11-20T14:00:15Z",
    "authors" : [
      {
        "name" : "Hrad Ghoukasian"
      },
      {
        "name" : "Shahab Asoodeh"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.16940v1",
    "title" : "MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models",
    "summary" : "Modern Vision-Language Models (VLMs) demonstrate sophisticated reasoning, escalating privacy risks beyond simple attribute perception to individual-level linkage. Current privacy benchmarks are structurally insufficient for this new threat, as they primarily evaluate privacy perception while failing to address the more critical risk of privacy reasoning: a VLM's ability to infer and link distributed information to construct individual profiles. To address this critical gap, we propose \\textbf{MultiPriv}, the first benchmark designed to systematically evaluate individual-level privacy reasoning in VLMs. We introduce the \\textbf{Privacy Perception and Reasoning (PPR)} framework and construct a novel, bilingual multimodal dataset to support it. The dataset uniquely features a core component of synthetic individual profiles where identifiers (e.g., faces, names) are meticulously linked to sensitive attributes. This design enables nine challenging tasks evaluating the full PPR spectrum, from attribute detection to cross-image re-identification and chained inference. We conduct a large-scale evaluation of over 50 foundational and commercial VLMs. Our analysis reveals: (1) Many VLMs possess significant, unmeasured reasoning-based privacy risks. (2) Perception-level metrics are poor predictors of these reasoning risks, revealing a critical evaluation gap. (3) Existing safety alignments are inconsistent and ineffective against such reasoning-based attacks. MultiPriv exposes systemic vulnerabilities and provides the necessary framework for developing robust, privacy-preserving VLMs.",
    "updated" : "2025-11-21T04:33:11Z",
    "published" : "2025-11-21T04:33:11Z",
    "authors" : [
      {
        "name" : "Xiongtao Sun"
      },
      {
        "name" : "Hui Li"
      },
      {
        "name" : "Jiaming Zhang"
      },
      {
        "name" : "Yujie Yang"
      },
      {
        "name" : "Kaili Liu"
      },
      {
        "name" : "Ruxin Feng"
      },
      {
        "name" : "Wen Jun Tan"
      },
      {
        "name" : "Wei Yang Bryan Lim"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.07441v2",
    "title" : "AudAgent: Automated Auditing of Privacy Policy Compliance in AI Agents",
    "summary" : "AI agents can autonomously perform tasks and, often without explicit user consent, collect or disclose users' sensitive local data, which raises serious privacy concerns. Although AI agents' privacy policies describe their intended data practices, there remains limited transparency and accountability about whether runtime behavior matches those policies. To close this gap, we introduce AudAgent, a visual tool that continuously monitors AI agents' data practices in real time and guards compliance with stated privacy policies.\n  AudAgent consists of four components for automated privacy auditing of AI agents. (i) Policy formalization: a novel cross-LLM voting mechanism to guarantee confidence of the parsed privacy policy model. (ii) Runtime annotation: a lightweight Presidio-based analyzer detects sensitive data and annotates data practices based on the AI agent's context and the privacy policy model. (iii) Compliance auditing: ontology graphs and automata-based checking connect the privacy policy model with runtime annotations, enabling on-the-fly compliance checking. (iv) User interface: an infrastructure-independent implementation visualizes the real-time execution trace of AI agents along with potential privacy policy violations, providing user-friendly transparency and accountability.\n  We evaluate AudAgent with AI agents built using mainstream frameworks, demonstrating its effectiveness in detecting and visualizing privacy policy violations in real time. Using AudAgent, we also find that most privacy policies omit explicit safeguards for highly sensitive data such as SSNs, whose misuse violates legal requirements, and that many agents do not refuse handling such data via third-party tools, including those controlled by Claude, Gemini, and DeepSeek. AudAgent proactively blocks operations on such data, overriding the agents' original privacy policy and behavior.",
    "updated" : "2025-11-20T21:03:58Z",
    "published" : "2025-11-03T17:32:08Z",
    "authors" : [
      {
        "name" : "Ye Zheng"
      },
      {
        "name" : "Yidan Hu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.19015v1",
    "title" : "A General Framework for Per-record Differential Privacy",
    "summary" : "Differential Privacy (DP) is a widely adopted standard for privacy-preserving data analysis, but it assumes a uniform privacy budget across all records, limiting its applicability when privacy requirements vary with data values. Per-record Differential Privacy (PrDP) addresses this by defining the privacy budget as a function of each record, offering better alignment with real-world needs. However, the dependency between the privacy budget and the data value introduces challenges in protecting the budget's privacy itself. Existing solutions either handle specific privacy functions or adopt relaxed PrDP definitions. A simple workaround is to use the global minimum of the privacy function, but this severely degrades utility, as the minimum is often set extremely low to account for rare records with high privacy needs. In this work, we propose a general and practical framework that enables any standard DP mechanism to support PrDP, with error depending only on the minimal privacy requirement among records actually present in the dataset. Since directly revealing this minimum may leak information, we introduce a core technique called privacy-specified domain partitioning, which ensures accurate estimation without compromising privacy. We also extend our framework to the local DP setting via a novel technique, privacy-specified query augmentation. Using our framework, we present the first PrDP solutions for fundamental tasks such as count, sum, and maximum estimation. Experimental results show that our mechanisms achieve high utility and significantly outperform existing Personalized DP (PDP) methods, which can be viewed as a special case of PrDP with relaxed privacy protection.",
    "updated" : "2025-11-24T11:44:10Z",
    "published" : "2025-11-24T11:44:10Z",
    "authors" : [
      {
        "name" : "Xinghe Chen"
      },
      {
        "name" : "Dajun Sun"
      },
      {
        "name" : "Quanqing Xu"
      },
      {
        "name" : "Wei Dong"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.18965v1",
    "title" : "REFLECTing SPERET: Measuring and Promoting Ethics and Privacy Reflexivity in Eye-Tracking Research",
    "summary" : "The proliferation of eye tracking in high-stakes domains - such as healthcare, marketing and surveillance - underscores the need for researchers to be ethically aware when employing this technology. Although privacy and ethical guidelines have emerged in recent years, empirical research on how scholars reflect on their own work remains scarce. To address this gap, we present two complementary instruments developed with input from more than 70 researchers: REFLECT, a qualitative questionnaire, and SPERET (Latin for \"hope\"), a quantitative psychometric scale that measures privacy and ethics reflexivity in eye tracking. Our findings reveal a research community that is concerned about user privacy, cognisant of methodological constraints, such as sample bias, and that possesses a nuanced sense of ethical responsibility evolving with project maturity. Together, these tools and our analyses offer a systematic examination and a hopeful outlook on reflexivity in eye-tracking research, promoting more privacy and ethics-conscious practice.",
    "updated" : "2025-11-24T10:31:40Z",
    "published" : "2025-11-24T10:31:40Z",
    "authors" : [
      {
        "name" : "Susanne Hindennach"
      },
      {
        "name" : "Mayar Elfares"
      },
      {
        "name" : "CÃ©line Gressel"
      },
      {
        "name" : "Andreas Bulling"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.18876v1",
    "title" : "Fairness Meets Privacy: Integrating Differential Privacy and Demographic Parity in Multi-class Classification",
    "summary" : "The increasing use of machine learning in sensitive applications demands algorithms that simultaneously preserve data privacy and ensure fairness across potentially sensitive sub-populations. While privacy and fairness have each been extensively studied, their joint treatment remains poorly understood. Existing research often frames them as conflicting objectives, with multiple studies suggesting that strong privacy notions such as differential privacy inevitably compromise fairness. In this work, we challenge that perspective by showing that differential privacy can be integrated into a fairness-enhancing pipeline with minimal impact on fairness guarantees. We design a postprocessing algorithm, called DP2DP, that enforces both demographic parity and differential privacy. Our analysis reveals that our algorithm converges towards its demographic parity objective at essentially the same rate (up logarithmic factor) as the best non-private methods from the literature. Experiments on both synthetic and real datasets confirm our theoretical results, showing that the proposed algorithm achieves state-of-the-art accuracy/fairness/privacy trade-offs.",
    "updated" : "2025-11-24T08:31:02Z",
    "published" : "2025-11-24T08:31:02Z",
    "authors" : [
      {
        "name" : "Lilian Say"
      },
      {
        "name" : "Christophe Denis"
      },
      {
        "name" : "Rafael Pinot"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.18583v1",
    "title" : "Differential privacy with dependent data",
    "summary" : "Dependent data underlies many statistical studies in the social and health sciences, which often involve sensitive or private information. Differential privacy (DP) and in particular \\textit{user-level} DP provide a natural formalization of privacy requirements for processing dependent data where each individual provides multiple observations to the dataset. However, dependence introduced, e.g., through repeated measurements challenges the existing statistical theory under DP-constraints. In \\iid{} settings, noisy Winsorized mean estimators have been shown to be minimax optimal for standard (\\textit{item-level}) and \\textit{user-level} DP estimation of a mean $Î¼\\in \\R^d$. Yet, their behavior on potentially dependent observations has not previously been studied. We fill this gap and show that Winsorized mean estimators can also be used under dependence for bounded and unbounded data, and can lead to asymptotic and finite sample guarantees that resemble their \\iid{} counterparts under a weak notion of dependence. For this, we formalize dependence via log-Sobolev inequalities on the joint distribution of observations. This enables us to adapt the stable histogram by Karwa and Vadhan (2018) to a non-\\iid{} setting, which we then use to estimate the private projection intervals of the Winsorized estimator. The resulting guarantees for our item-level mean estimator extend to \\textit{user-level} mean estimation and transfer to the local model via a randomized response histogram. Using the mean estimators as building blocks, we provide extensions to random effects models, longitudinal linear regression and nonparametric regression. Therefore, our work constitutes a first step towards a systematic study of DP for dependent data.",
    "updated" : "2025-11-23T18:56:40Z",
    "published" : "2025-11-23T18:56:40Z",
    "authors" : [
      {
        "name" : "Valentin Roth"
      },
      {
        "name" : "Marco Avella-Medina"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.18268v1",
    "title" : "Privacy Concerns and ChatGPT: Exploring Online Discourse through the Lens of Information Practice on Reddit",
    "summary" : "As millions of people use ChatGPT for tasks such as education, writing assistance, and health advice, concerns have grown about how personal prompts and data are stored and used. This study explores how Reddit users collectively negotiate and respond to these privacy concerns. Posts were collected from three major subreddits -- r/Chatgpt, r/privacy, and r/OpenAI -- between November 2022 and May 2025. An iterative keyword search followed by manual screening resulted in a final dataset of 426 posts and 1,900 comments. Using information practice as the theoretical lens, we conducted a qualitative thematic analysis to identify collective practices of risk negotiation, validated with BERTopic topic modeling to ensure thematic saturation. Findings revealed risk signaling, norm-setting, and resignation as dominant discourses, and collective troubleshooting and advocacy for privacy-preserving alternatives as key adaptive practices. Reddit functions as a site of collective sense-making where users surface risks, establish informal norms, and share strategies for mitigating privacy threats, offering insights for AI design and privacy literacy initiatives.",
    "updated" : "2025-11-23T03:37:49Z",
    "published" : "2025-11-23T03:37:49Z",
    "authors" : [
      {
        "name" : "S M Mehedi Zaman"
      },
      {
        "name" : "Saubhagya Joshi"
      },
      {
        "name" : "Yiyi Wu"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.18025v1",
    "title" : "Correlated-Sequence Differential Privacy",
    "summary" : "Data streams collected from multiple sources are rarely independent. Values evolve over time and influence one another across sequences. These correlations improve prediction in healthcare, finance, and smart-city control yet violate the record-independence assumption built into most Differential Privacy (DP) mechanisms. To restore rigorous privacy guarantees without sacrificing utility, we introduce Correlated-Sequence Differential Privacy (CSDP), a framework specifically designed for preserving privacy in correlated sequential data. CSDP addresses two linked challenges: quantifying the extra information an attacker gains from joint temporal and cross-sequence links, and adding just enough noise to hide that information while keeping the data useful. We model multivariate streams as a Coupling Markov Chain, yielding the derived loose leakage bound expressed with a few spectral terms and revealing a counterintuitive result: stronger coupling can actually decrease worst-case leakage by dispersing perturbations across sequences. Guided by these bounds, we build the Freshness-Regulated Adaptive Noise (FRAN) mechanism--combining data aging, correlation-aware sensitivity scaling, and Laplace noise--that runs in linear time. Tests on two-sequence datasets show that CSDP improves the privacy-utility trade-off by approximately 50% over existing correlated-DP methods and by two orders of magnitude compared to the standard DP approach.",
    "updated" : "2025-11-22T11:28:59Z",
    "published" : "2025-11-22T11:28:59Z",
    "authors" : [
      {
        "name" : "Yifan Luo"
      },
      {
        "name" : "Meng Zhang"
      },
      {
        "name" : "Jin Xu"
      },
      {
        "name" : "Junting Chen"
      },
      {
        "name" : "Jianwei Huang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.17989v1",
    "title" : "Privacy Auditing of Multi-domain Graph Pre-trained Model under Membership Inference Attacks",
    "summary" : "Multi-domain graph pre-training has emerged as a pivotal technique in developing graph foundation models. While it greatly improves the generalization of graph neural networks, its privacy risks under membership inference attacks (MIAs), which aim to identify whether a specific instance was used in training (member), remain largely unexplored. However, effectively conducting MIAs against multi-domain graph pre-trained models is a significant challenge due to: (i) Enhanced Generalization Capability: Multi-domain pre-training reduces the overfitting characteristics commonly exploited by MIAs. (ii) Unrepresentative Shadow Datasets: Diverse training graphs hinder the obtaining of reliable shadow graphs. (iii) Weakened Membership Signals: Embedding-based outputs offer less informative cues than logits for MIAs. To tackle these challenges, we propose MGP-MIA, a novel framework for Membership Inference Attacks against Multi-domain Graph Pre-trained models. Specifically, we first propose a membership signal amplification mechanism that amplifies the overfitting characteristics of target models via machine unlearning. We then design an incremental shadow model construction mechanism that builds a reliable shadow model with limited shadow graphs via incremental learning. Finally, we introduce a similarity-based inference mechanism that identifies members based on their similarity to positive and negative samples. Extensive experiments demonstrate the effectiveness of our proposed MGP-MIA and reveal the privacy risks of multi-domain graph pre-training.",
    "updated" : "2025-11-22T09:04:58Z",
    "published" : "2025-11-22T09:04:58Z",
    "authors" : [
      {
        "name" : "Jiayi Luo"
      },
      {
        "name" : "Qingyun Sun"
      },
      {
        "name" : "Yuecen Wei"
      },
      {
        "name" : "Haonan Yuan"
      },
      {
        "name" : "Xingcheng Fu"
      },
      {
        "name" : "Jianxin Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.17747v1",
    "title" : "AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations",
    "summary" : "The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.",
    "updated" : "2025-11-21T19:57:28Z",
    "published" : "2025-11-21T19:57:28Z",
    "authors" : [
      {
        "name" : "Dawid Wolkiewicz"
      },
      {
        "name" : "Anastasiya Pechko"
      },
      {
        "name" : "PrzemysÅaw Spurek"
      },
      {
        "name" : "Piotr Syga"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.07242v4",
    "title" : "Privacy on the Fly: A Predictive Adversarial Transformation Network for Mobile Sensor Data",
    "summary" : "Mobile motion sensors such as accelerometers and gyroscopes are now ubiquitously accessible by third-party apps via standard APIs. While enabling rich functionalities like activity recognition and step counting, this openness has also enabled unregulated inference of sensitive user traits, such as gender, age, and even identity, without user consent. Existing privacy-preserving techniques, such as GAN-based obfuscation or differential privacy, typically require access to the full input sequence, introducing latency that is incompatible with real-time scenarios. Worse, they tend to distort temporal and semantic patterns, degrading the utility of the data for benign tasks like activity recognition. To address these limitations, we propose the Predictive Adversarial Transformation Network (PATN), a real-time privacy-preserving framework that leverages historical signals to generate adversarial perturbations proactively. The perturbations are applied immediately upon data acquisition, enabling continuous protection without disrupting application functionality. Experiments on two datasets demonstrate that PATN substantially degrades the performance of privacy inference models, achieving Attack Success Rate (ASR) of 40.11% and 44.65% (reducing inference accuracy to near-random) and increasing the Equal Error Rate (EER) from 8.30% and 7.56% to 41.65% and 46.22%. On ASR, PATN outperforms baseline methods by 16.16% and 31.96%, respectively.",
    "updated" : "2025-11-24T08:58:20Z",
    "published" : "2025-11-10T15:57:17Z",
    "authors" : [
      {
        "name" : "Tianle Song"
      },
      {
        "name" : "Chenhao Lin"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Zhengyu Zhao"
      },
      {
        "name" : "Jiahao Sun"
      },
      {
        "name" : "Chong Zhang"
      },
      {
        "name" : "Le Yang"
      },
      {
        "name" : "Chao Shen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.20637v1",
    "title" : "Behavioural Sciences and the Regulation of Privacy on the Internet",
    "summary" : "This chapter examines the policy implications of behavioural sciences insights for the regulation of privacy on the Internet, by focusing in particular on behavioural targeting. This marketing technique involves tracking people's online behaviour to use the collected information to show people individually targeted advertisements. Enforcing data protection law may not be enough to protect privacy in this area. I argue that, if society is better off when certain behavioural targeting practices do not happen, policymakers should consider banning them.",
    "updated" : "2025-11-25T18:55:51Z",
    "published" : "2025-11-25T18:55:51Z",
    "authors" : [
      {
        "name" : "Frederik Zuiderveen Borgesius"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.20497v1",
    "title" : "Quantifying the Privacy Implications of High-Fidelity Synthetic Network Traffic",
    "summary" : "To address the scarcity and privacy concerns of network traffic data, various generative models have been developed to produce synthetic traffic. However, synthetic traffic is not inherently privacy-preserving, and the extent to which it leaks sensitive information, and how to measure such leakage, remain largely unexplored. This challenge is further compounded by the diversity of model architectures, which shape how traffic is represented and synthesized. We introduce a comprehensive set of privacy metrics for synthetic network traffic, combining standard approaches like membership inference attacks (MIA) and data extraction attacks with network-specific identifiers and attributes. Using these metrics, we systematically evaluate the vulnerability of different representative generative models and examine the factors that influence attack success. Our results reveal substantial variability in privacy risks across models and datasets. MIA success ranges from 0% to 88%, and up to 100% of network identifiers can be recovered from generated traffic, highlighting serious privacy vulnerabilities. We further identify key factors that significantly affect attack outcomes, including training data diversity and how well the generative model fits the training data. These findings provide actionable guidance for designing and deploying generative models that minimize privacy leakage, establishing a foundation for safer synthetic network traffic generation.",
    "updated" : "2025-11-25T17:04:02Z",
    "published" : "2025-11-25T17:04:02Z",
    "authors" : [
      {
        "name" : "Van Tran"
      },
      {
        "name" : "Shinan Liu"
      },
      {
        "name" : "Tian Li"
      },
      {
        "name" : "Nick Feamster"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.20252v1",
    "title" : "Hey there! You are using WhatsApp: Enumerating Three Billion Accounts for Security and Privacy",
    "summary" : "WhatsApp, with 3.5 billion active accounts as of early 2025, is the world's largest instant messaging platform. Given its massive user base, WhatsApp plays a critical role in global communication.\n  To initiate conversations, users must first discover whether their contacts are registered on the platform. This is achieved by querying WhatsApp's servers with mobile phone numbers extracted from the user's address book (if they allowed access). This architecture inherently enables phone number enumeration, as the service must allow legitimate users to query contact availability. While rate limiting is a standard defense against abuse, we revisit the problem and show that WhatsApp remains highly vulnerable to enumeration at scale. In our study, we were able to probe over a hundred million phone numbers per hour without encountering blocking or effective rate limiting.\n  Our findings demonstrate not only the persistence but the severity of this vulnerability. We further show that nearly half of the phone numbers disclosed in the 2021 Facebook data leak are still active on WhatsApp, underlining the enduring risks associated with such exposures. Moreover, we were able to perform a census of WhatsApp users, providing a glimpse on the macroscopic insights a large messaging service is able to generate even though the messages themselves are end-to-end encrypted. Using the gathered data, we also discovered the re-use of certain X25519 keys across different devices and phone numbers, indicating either insecure (custom) implementations, or fraudulent activity.\n  In this updated version of the paper, we also provide insights into the collaborative remediation process through which we confirmed that the underlying rate-limiting issue had been resolved.",
    "updated" : "2025-11-25T12:27:05Z",
    "published" : "2025-11-25T12:27:05Z",
    "authors" : [
      {
        "name" : "Gabriel K. Gegenhuber"
      },
      {
        "name" : "Philipp Ã. Frenzel"
      },
      {
        "name" : "Maximilian GÃ¼nther"
      },
      {
        "name" : "Johanna Ullrich"
      },
      {
        "name" : "Aljosha Judmayer"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.20125v1",
    "title" : "N2E: A General Framework to Reduce Node-Differential Privacy to Edge-Differential Privacy for Graph Analytics",
    "summary" : "Differential privacy (DP) has been widely adopted to protect sensitive information in graph analytics. While edge-DP, which protects privacy at the edge level, has been extensively studied, node-DP, offering stronger protection for entire nodes and their incident edges, remains largely underexplored due to its technical challenges. A natural way to bridge this gap is to develop a general framework for reducing node-DP graph analytical tasks to edge-DP ones, enabling the reuse of existing edge-DP mechanisms. A straightforward solution based on group privacy divides the privacy budget by a given degree upper bound, but this leads to poor utility when the bound is set conservatively large to accommodate worst-case inputs. To address this, we propose node-to-edge (N2E), a general framework that reduces any node-DP graph analytical task to an edge-DP one, with the error dependency on the graph's true maximum degree. N2E introduces two novel techniques: a distance-preserving clipping mechanism that bounds edge distance between neighboring graphs after clipping, and the first node-DP mechanism for maximum degree approximation, enabling tight, privacy-preserving clipping thresholds. By instantiating N2E with existing edge-DP mechanisms, we obtain the first node-DP solutions for tasks such as maximum degree estimation. For edge counting, our method theoretically matches the error of the state-of-the-art, which is provably optimal, and significantly outperforms existing approaches for degree distribution estimation. Experimental results demonstrate that our framework achieves up to a 2.5x reduction in error for edge counting and up to an 80x reduction for degree distribution estimation.",
    "updated" : "2025-11-25T09:46:38Z",
    "published" : "2025-11-25T09:46:38Z",
    "authors" : [
      {
        "name" : "Yihua Hu"
      },
      {
        "name" : "Hao Ding"
      },
      {
        "name" : "Wei Dong"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.19958v1",
    "title" : "GFT-GCN: Privacy-Preserving 3D Face Mesh Recognition with Spectral Diffusion",
    "summary" : "3D face recognition offers a robust biometric solution by capturing facial geometry, providing resilience to variations in illumination, pose changes, and presentation attacks. Its strong spoof resistance makes it suitable for high-security applications, but protecting stored biometric templates remains critical. We present GFT-GCN, a privacy-preserving 3D face recognition framework that combines spectral graph learning with diffusion-based template protection. Our approach integrates the Graph Fourier Transform (GFT) and Graph Convolutional Networks (GCN) to extract compact, discriminative spectral features from 3D face meshes. To secure these features, we introduce a spectral diffusion mechanism that produces irreversible, renewable, and unlinkable templates. A lightweight client-server architecture ensures that raw biometric data never leaves the client device. Experiments on the BU-3DFE and FaceScape datasets demonstrate high recognition accuracy and strong resistance to reconstruction attacks. Results show that GFT-GCN effectively balances privacy and performance, offering a practical solution for secure 3D face authentication.",
    "updated" : "2025-11-25T06:07:26Z",
    "published" : "2025-11-25T06:07:26Z",
    "authors" : [
      {
        "name" : "Hichem Felouat"
      },
      {
        "name" : "Hanrui Wang"
      },
      {
        "name" : "Isao Echizen"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.19750v1",
    "title" : "DISCO: A Browser-Based Privacy-Preserving Framework for Distributed Collaborative Learning",
    "summary" : "Data is often impractical to share for a range of well considered reasons, such as concerns over privacy, intellectual property, and legal constraints. This not only fragments the statistical power of predictive models, but creates an accessibility bias, where accuracy becomes inequitably distributed to those who have the resources to overcome these concerns. We present DISCO: an open-source DIStributed COllaborative learning platform accessible to non-technical users, offering a means to collaboratively build machine learning models without sharing any original data or requiring any programming knowledge. DISCO's web application trains models locally directly in the browser, making our tool cross-platform out-of-the-box, including smartphones. The modular design of \\disco offers choices between federated and decentralized paradigms, various levels of privacy guarantees and several approaches to weight aggregation strategies that allow for model personalization and bias resilience in the collaborative training. Code repository is available at https://github.com/epfml/disco and a showcase web interface at https://discolab.ai",
    "updated" : "2025-11-24T22:16:07Z",
    "published" : "2025-11-24T22:16:07Z",
    "authors" : [
      {
        "name" : "Julien T. T. Vignoud"
      },
      {
        "name" : "ValÃ©rian Rousset"
      },
      {
        "name" : "Hugo El Guedj"
      },
      {
        "name" : "Ignacio Aleman"
      },
      {
        "name" : "Walid Bennaceur"
      },
      {
        "name" : "Batuhan Faik Derinbay"
      },
      {
        "name" : "Eduard Äurech"
      },
      {
        "name" : "Damien Gengler"
      },
      {
        "name" : "Lucas Giordano"
      },
      {
        "name" : "Felix Grimberg"
      },
      {
        "name" : "Franziska Lippoldt"
      },
      {
        "name" : "Christina Kopidaki"
      },
      {
        "name" : "Jiafan Liu"
      },
      {
        "name" : "Lauris Lopata"
      },
      {
        "name" : "Nathan Maire"
      },
      {
        "name" : "Paul Mansat"
      },
      {
        "name" : "Martin Milenkoski"
      },
      {
        "name" : "Emmanuel Omont"
      },
      {
        "name" : "GÃ¼neÅ ÃzgÃ¼n"
      },
      {
        "name" : "Mina PetroviÄ"
      },
      {
        "name" : "Francesco Posa"
      },
      {
        "name" : "Morgan Ridel"
      },
      {
        "name" : "Giorgio Savini"
      },
      {
        "name" : "Marcel Torne"
      },
      {
        "name" : "Lucas Trognon"
      },
      {
        "name" : "Alyssa Unell"
      },
      {
        "name" : "Olena Zavertiaieva"
      },
      {
        "name" : "Sai Praneeth Karimireddy"
      },
      {
        "name" : "Tahseen Rabbani"
      },
      {
        "name" : "Mary-Anne Hartley"
      },
      {
        "name" : "Martin Jaggi"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.18583v2",
    "title" : "Differential privacy with dependent data",
    "summary" : "Dependent data underlies many statistical studies in the social and health sciences, which often involve sensitive or private information. Differential privacy (DP) and in particular \\textit{user-level} DP provide a natural formalization of privacy requirements for processing dependent data where each individual provides multiple observations to the dataset. However, dependence introduced, e.g., through repeated measurements challenges the existing statistical theory under DP-constraints. In \\iid{} settings, noisy Winsorized mean estimators have been shown to be minimax optimal for standard (\\textit{item-level}) and \\textit{user-level} DP estimation of a mean $Î¼\\in \\R^d$. Yet, their behavior on potentially dependent observations has not previously been studied. We fill this gap and show that Winsorized mean estimators can also be used under dependence for bounded and unbounded data, and can lead to asymptotic and finite sample guarantees that resemble their \\iid{} counterparts under a weak notion of dependence. For this, we formalize dependence via log-Sobolev inequalities on the joint distribution of observations. This enables us to adapt the stable histogram by Karwa and Vadhan (2018) to a non-\\iid{} setting, which we then use to estimate the private projection intervals of the Winsorized estimator. The resulting guarantees for our item-level mean estimator extend to \\textit{user-level} mean estimation and transfer to the local model via a randomized response histogram. Using the mean estimators as building blocks, we provide extensions to random effects models, longitudinal linear regression and nonparametric regression. Therefore, our work constitutes a first step towards a systematic study of DP for dependent data.",
    "updated" : "2025-11-25T03:41:49Z",
    "published" : "2025-11-23T18:56:40Z",
    "authors" : [
      {
        "name" : "Valentin Roth"
      },
      {
        "name" : "Marco Avella-Medina"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.19498v1",
    "title" : "Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data",
    "summary" : "Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.",
    "updated" : "2025-11-23T15:28:19Z",
    "published" : "2025-11-23T15:28:19Z",
    "authors" : [
      {
        "name" : "Yi Zhang"
      },
      {
        "name" : "Tianxiang Xu"
      },
      {
        "name" : "Zijian Li"
      },
      {
        "name" : "Chao Zhang"
      },
      {
        "name" : "Kunyu Zhang"
      },
      {
        "name" : "Zhan Gao"
      },
      {
        "name" : "Meinuo Li"
      },
      {
        "name" : "Xiaohan Zhang"
      },
      {
        "name" : "Qichao Qi"
      },
      {
        "name" : "Bing Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.11811v2",
    "title" : "Lessons Learned from Developing a Privacy-Preserving Multimodal Wearable for Local Voice-and-Vision Inference",
    "summary" : "Many promising applications of multimodal wearables require continuous sensing and heavy computation, yet users reject such devices due to privacy concerns. This paper shares our experiences building an ear-mounted voice-and-vision wearable that performs local AI inference using a paired smartphone as a trusted personal edge. We describe the hardware-software co-design of this privacy-preserving system, including challenges in integrating a camera, microphone, and speaker within a 30-gram form factor, enabling wake word-triggered capture, and running quantized vision-language and large-language models entirely offline. Through iterative prototyping, we identify key design hurdles in power budgeting, connectivity, latency, and social acceptability. Our initial evaluation shows that fully local multimodal inference is feasible on commodity mobile hardware with interactive latency. We conclude with design lessons for researchers developing embedded AI systems that balance privacy, responsiveness, and usability in everyday settings.",
    "updated" : "2025-11-24T21:49:38Z",
    "published" : "2025-11-14T19:04:52Z",
    "authors" : [
      {
        "name" : "Yonatan Tussa"
      },
      {
        "name" : "Andy Heredia"
      },
      {
        "name" : "Nirupam Roy"
      }
    ],
    "categories" : [
      "cs.HC",
      "eess.AS",
      "eess.IV",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.21196v1",
    "title" : "Privacy-Constrained Signals",
    "summary" : "This paper provides a unified approach to characterize the set of all feasible signals subject to privacy constraints. The Blackwell frontier of feasible signals can be decomposed into minimum informative signals achieving the Blackwell frontier of privacy variables, and conditionally privacy-preserving signals. A complete characterization of the minimum informative signals is then provided. We apply the framework to ex-post privacy (including differential and inferential privacy) and to constraints on posterior means of arbitrary statistics.",
    "updated" : "2025-11-26T09:21:24Z",
    "published" : "2025-11-26T09:21:24Z",
    "authors" : [
      {
        "name" : "Zhang Xu"
      },
      {
        "name" : "Wei Zhao"
      }
    ],
    "categories" : [
      "econ.TH",
      "cs.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.21181v1",
    "title" : "Privacy in Federated Learning with Spiking Neural Networks",
    "summary" : "Spiking neural networks (SNNs) have emerged as prominent candidates for embedded and edge AI. Their inherent low power consumption makes them far more efficient than conventional ANNs in scenarios where energy budgets are tightly constrained. In parallel, federated learning (FL) has become the prevailing training paradigm in such settings, enabling on-device learning while limiting the exposure of raw data. However, gradient inversion attacks represent a critical privacy threat in FL, where sensitive training data can be reconstructed directly from shared gradients. While this vulnerability has been widely investigated in conventional ANNs, its implications for SNNs remain largely unexplored. In this work, we present the first comprehensive empirical study of gradient leakage in SNNs across diverse data domains. SNNs are inherently non-differentiable and are typically trained using surrogate gradients, which we hypothesized would be less correlated with the original input and thus less informative from a privacy perspective. To investigate this, we adapt different gradient leakage attacks to the spike domain. Our experiments reveal a striking contrast with conventional ANNs: whereas ANN gradients reliably expose salient input content, SNN gradients yield noisy, temporally inconsistent reconstructions that fail to recover meaningful spatial or temporal structure. These results indicate that the combination of event-driven dynamics and surrogate-gradient training substantially reduces gradient informativeness. To the best of our knowledge, this work provides the first systematic benchmark of gradient inversion attacks for spiking architectures, highlighting the inherent privacy-preserving potential of neuromorphic computation.",
    "updated" : "2025-11-26T08:55:11Z",
    "published" : "2025-11-26T08:55:11Z",
    "authors" : [
      {
        "name" : "Dogukan Aksu"
      },
      {
        "name" : "Jesus Martinez del Rincon"
      },
      {
        "name" : "Ihsen Alouani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.21020v1",
    "title" : "Road Network-Aware Personalized Trajectory Protection with Differential Privacy under Spatiotemporal Correlations",
    "summary" : "Location-Based Services (LBSs) offer significant convenience to mobile users but pose significant privacy risks, as attackers can infer sensitive personal information through spatiotemporal correlations in user trajectories. Since users' sensitivity to location data varies based on factors such as stay duration, access frequency, and semantic sensitivity, implementing personalized privacy protection is imperative. This paper proposes a Personalized Trajectory Privacy Protection Mechanism (PTPPM) to address these challenges. Our approach begins by modeling an attacker's knowledge of a user's trajectory spatiotemporal correlations, which enables the attacker to identify possible location sets and disregard low-probability location sets. To combat this, we integrate geo-indistinguishability with distortion privacy, allowing users to customize their privacy preferences through a configurable privacy budget and expected inference error bound. This approach provides the theoretical framework for constructing a Protection Location Set (PLS) that obscures users' actual locations. Additionally, we introduce a Personalized Privacy Budget Allocation Algorithm (PPBA), which assesses the sensitivity of locations based on trajectory data and allocates privacy budgets accordingly. This algorithm considers factors such as location semantics and road network constraints. Furthermore, we propose a Permute-and-Flip mechanism that generates perturbed locations while minimizing perturbation distance, thus balancing privacy protection and Quality of Service (QoS). Simulation results demonstrate that our mechanism outperforms existing benchmarks, offering superior privacy protection while maintaining user QoS requirements.",
    "updated" : "2025-11-26T03:33:24Z",
    "published" : "2025-11-26T03:33:24Z",
    "authors" : [
      {
        "name" : "Minghui Min"
      },
      {
        "name" : "Jiahui Liu"
      },
      {
        "name" : "Mingge Cao"
      },
      {
        "name" : "Shiyin Li"
      },
      {
        "name" : "Hongliang Zhang"
      },
      {
        "name" : "Miao Pan"
      },
      {
        "name" : "Zhu Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.20983v1",
    "title" : "Privacy-Preserving Federated Vision Transformer Learning Leveraging Lightweight Homomorphic Encryption in Medical AI",
    "summary" : "Collaborative machine learning across healthcare institutions promises improved diagnostic accuracy by leveraging diverse datasets, yet privacy regulations such as HIPAA prohibit direct patient data sharing. While federated learning (FL) enables decentralized training without raw data exchange, recent studies show that model gradients in conventional FL remain vulnerable to reconstruction attacks, potentially exposing sensitive medical information. This paper presents a privacy-preserving federated learning framework combining Vision Transformers (ViT) with homomorphic encryption (HE) for secure multi-institutional histopathology classification. The approach leverages the ViT CLS token as a compact 768-dimensional feature representation for secure aggregation, encrypting these tokens using CKKS homomorphic encryption before transmission to the server. We demonstrate that encrypting CLS tokens achieves a 30-fold communication reduction compared to gradient encryption while maintaining strong privacy guarantees. Through evaluation on a three-client federated setup for lung cancer histopathology classification, we show that gradients are highly susceptible to model inversion attacks (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), enabling near-perfect image reconstruction. In contrast, the proposed CLS-protected HE approach prevents such attacks while enabling encrypted inference directly on ciphertexts, requiring only 326 KB of encrypted data transmission per aggregation round. The framework achieves 96.12 percent global classification accuracy in the unencrypted domain and 90.02 percent in the encrypted domain.",
    "updated" : "2025-11-26T02:27:40Z",
    "published" : "2025-11-26T02:27:40Z",
    "authors" : [
      {
        "name" : "Al Amin"
      },
      {
        "name" : "Kamrul Hasan"
      },
      {
        "name" : "Liang Hong"
      },
      {
        "name" : "Sharif Ullah"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.20791v1",
    "title" : "Beyond the Legal Lens: A Sociotechnical Taxonomy of Lived Privacy Incidents and Harms",
    "summary" : "To understand how privacy incidents lead to harms, HCI researchers have historically leveraged legal frameworks. However, these frameworks expect acute, tangible harms and thus may not cover the full range of human experience relevant to modern-day digital privacy. To address this gap, our research builds upon these existing frameworks to develop a more comprehensive representation of people's lived experiences with privacy harms. We analyzed 369 privacy incidents reported by individuals from the general public. We found a broader range of privacy incidents and harms than accounted for in existing legal frameworks. The majority of reported privacy harms were not based on tangible harm, but on fear and loss of psychological safety. We also characterize the actors, motives, and information associated with various incidents. This work contributes a new framework for understanding digital privacy harms that can be utilized both in research and practice.",
    "updated" : "2025-11-25T19:31:26Z",
    "published" : "2025-11-25T19:31:26Z",
    "authors" : [
      {
        "name" : "Kirsten Chapman"
      },
      {
        "name" : "Garrett Smith"
      },
      {
        "name" : "Kaitlyn Klabacka"
      },
      {
        "name" : "Harrison Winslow"
      },
      {
        "name" : "Louise Barkhuus"
      },
      {
        "name" : "Cori Faklaris"
      },
      {
        "name" : "Sauvik Das"
      },
      {
        "name" : "Pamela Wisniewski"
      },
      {
        "name" : "Bart Piet Knijnenburg"
      },
      {
        "name" : "Heather Lipford"
      },
      {
        "name" : "Xinru Page"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.20744v1",
    "title" : "Scoping Electronic Communication Privacy Rules: Data, Services and Values",
    "summary" : "We use electronic communication networks for more than simply traditional telecommunications: we access the news, buy goods online, file our taxes, contribute to public debate, and more. As a result, a wider array of privacy interests is implicated for users of electronic communications networks and services. This development calls into question the scope of electronic communications privacy rules. This paper analyses the scope of these rules, taking into account the rationale and the historic background of the European electronic communications privacy framework. We develop a framework for analysing the scope of electronic communications privacy rules using three approaches: (i) a service-centric approach, (ii) a data-centric approach, and (iii) a value-centric approach. We discuss the strengths and weaknesses of each approach. The current e-Privacy Directive contains a complex blend of the three approaches, which does not seem to be based on a thorough analysis of their strengths and weaknesses. The upcoming review of the directive announced by the European Commission provides an opportunity to improve the scoping of the rules.",
    "updated" : "2025-11-25T18:55:47Z",
    "published" : "2025-11-25T18:55:47Z",
    "authors" : [
      {
        "name" : "Joris van Hoboken"
      },
      {
        "name" : "Frederik Zuiderveen Borgesius"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.19958v2",
    "title" : "GFT-GCN: Privacy-Preserving 3D Face Mesh Recognition with Spectral Diffusion",
    "summary" : "3D face recognition offers a robust biometric solution by capturing facial geometry, providing resilience to variations in illumination, pose changes, and presentation attacks. Its strong spoof resistance makes it suitable for high-security applications, but protecting stored biometric templates remains critical. We present GFT-GCN, a privacy-preserving 3D face recognition framework that combines spectral graph learning with diffusion-based template protection. Our approach integrates the Graph Fourier Transform (GFT) and Graph Convolutional Networks (GCN) to extract compact, discriminative spectral features from 3D face meshes. To secure these features, we introduce a spectral diffusion mechanism that produces irreversible, renewable, and unlinkable templates. A lightweight client-server architecture ensures that raw biometric data never leaves the client device. Experiments on the BU-3DFE and FaceScape datasets demonstrate high recognition accuracy and strong resistance to reconstruction attacks. Results show that GFT-GCN effectively balances privacy and performance, offering a practical solution for secure 3D face authentication.",
    "updated" : "2025-11-26T04:15:27Z",
    "published" : "2025-11-25T06:07:26Z",
    "authors" : [
      {
        "name" : "Hichem Felouat"
      },
      {
        "name" : "Hanrui Wang"
      },
      {
        "name" : "Isao Echizen"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.20710v1",
    "title" : "Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?",
    "summary" : "In the age of agentic AI, the growing deployment of multi-modal models (MMs) has introduced new attack vectors that can leak sensitive training data in MMs, causing privacy leakage. This paper investigates a black-box privacy attack, i.e., membership inference attack (MIA) on multi-modal vision-language models (VLMs). State-of-the-art research analyzes privacy attacks primarily to unimodal AI-ML systems, while recent studies indicate MMs can also be vulnerable to privacy attacks. While researchers have demonstrated that biologically inspired neural network representations can improve unimodal model resilience against adversarial attacks, it remains unexplored whether neuro-inspired MMs are resilient against privacy attacks. In this work, we introduce a systematic neuroscience-inspired topological regularization (tau) framework to analyze MM VLMs resilience against image-text-based inference privacy attacks. We examine this phenomenon using three VLMs: BLIP, PaliGemma 2, and ViT-GPT2, across three benchmark datasets: COCO, CC3M, and NoCaps. Our experiments compare the resilience of baseline and neuro VLMs (with topological regularization), where the tau > 0 configuration defines the NEURO variant of VLM. Our results on the BLIP model using the COCO dataset illustrate that MIA attack success in NEURO VLMs drops by 24% mean ROC-AUC, while achieving similar model utility (similarities between generated and reference captions) in terms of MPNet and ROUGE-2 metrics. This shows neuro VLMs are comparatively more resilient against privacy attacks, while not significantly compromising model utility. Our extensive evaluation with PaliGemma 2 and ViT-GPT2 models, on two additional datasets: CC3M and NoCaps, further validates the consistency of the findings. This work contributes to the growing understanding of privacy risks in MMs and provides evidence on neuro VLMs privacy threat resilience.",
    "updated" : "2025-11-24T22:32:03Z",
    "published" : "2025-11-24T22:32:03Z",
    "authors" : [
      {
        "name" : "David Amebley"
      },
      {
        "name" : "Sayanton Dibbo"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.23200v1",
    "title" : "Quantifying the Privacy-Utility Trade-off in GPS-based Daily Stress Recognition using Semantic Features",
    "summary" : "Psychological stress is a widespread issue that significantly impacts student well-being and academic performance. Effective remote stress recognition is crucial, yet existing methods often rely on wearable devices or GPS-based clustering techniques that pose privacy risks. In this study, we introduce a novel, end-to-end privacy-enhanced framework for semantic location encoding using a self-hosted OSM engine and an LLM-bootstrapped static map. We rigorously quantify the privacy-utility trade-off and demonstrate (via LOSO validation) that our Privacy-Aware (PA) model achieves performance statistically indistinguishable from a non-private model, proving that utility does not require sacrificing privacy. Feature importance analysis highlights that recreational activity time, working time, and travel time play a significant role in stress recognition.",
    "updated" : "2025-11-28T14:04:00Z",
    "published" : "2025-11-28T14:04:00Z",
    "authors" : [
      {
        "name" : "Hoang Khang Phan"
      },
      {
        "name" : "Nhat Tan Le"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.22791v1",
    "title" : "An Efficient Privacy-preserving Intrusion Detection Scheme for UAV Swarm Networks",
    "summary" : "The rapid proliferation of unmanned aerial vehicles (UAVs) and their applications in diverse domains, such as surveillance, disaster management, agriculture, and defense, have revolutionized modern technology. While the potential benefits of swarm-based UAV networks are growing significantly, they are vulnerable to various security attacks that can jeopardize the overall mission success by degrading their performance, disrupting decision-making, and compromising the trajectory planning process. The Intrusion Detection System (IDS) plays a vital role in identifying potential security attacks to ensure the secure operation of UAV swarm networks. However, conventional IDS primarily focuses on binary classification with resource-intensive neural networks and faces challenges, including latency, privacy breaches, increased performance overhead, and model drift. This research aims to address these challenges by developing a novel lightweight and federated continuous learning-based IDS scheme. Our proposed model facilitates decentralized training across diverse UAV swarms to ensure data heterogeneity and privacy. The performance evaluation of our model demonstrates significant improvements, with classification accuracies of 99.45% on UKM-IDS, 99.99% on UAV-IDS, 96.85% on TLM-UAV dataset, and 98.05% on Cyber-Physical datasets.",
    "updated" : "2025-11-27T22:37:06Z",
    "published" : "2025-11-27T22:37:06Z",
    "authors" : [
      {
        "name" : "Kanchon Gharami"
      },
      {
        "name" : "Shafika Showkat Moni"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.22788v1",
    "title" : "PRISM: Privacy-Aware Routing for Adaptive Cloud-Edge LLM Inference via Semantic Sketch Collaboration",
    "summary" : "Large Language Models (LLMs) demonstrate impressive capabilities in natural language understanding and generation, but incur high communication overhead and privacy risks in cloud deployments, while facing compute and memory constraints when confined to edge devices. Cloud-edge inference has emerged as a promising paradigm for improving privacy in LLM services by retaining sensitive computations on local devices. However, existing cloud-edge inference approaches apply uniform privacy protection without considering input sensitivity, resulting in unnecessary perturbation and degraded utility even for non-sensitive tokens. To address this limitation, we propose Privacy-aware Routing for Inference with Semantic Modulation (PRISM), a context-aware framework that dynamically balances privacy and inference quality. PRISM executes in four stages: (1) the edge device profiles entity-level sensitivity; (2) a soft gating module on the edge selects an execution mode - cloud, edge, or collaboration; (3) for collaborative paths, the edge applies adaptive two-layer local differential privacy based on entity risks; and (4) the cloud LLM generates a semantic sketch from the perturbed prompt, which is then refined by the edge-side small language model (SLM) using local context. Our results show that PRISM consistently achieves superior privacy-utility trade-offs across various scenarios, reducing energy consumption and latency to 40-50% of baseline methods such as Uniform and Selective LDP, while maintaining high output quality under strong privacy constraints. These findings are validated through comprehensive evaluations involving realistic prompts, actual energy measurements, and heterogeneous cloud-edge model deployments.",
    "updated" : "2025-11-27T22:32:33Z",
    "published" : "2025-11-27T22:32:33Z",
    "authors" : [
      {
        "name" : "Junfei Zhan"
      },
      {
        "name" : "Haoxun Shen"
      },
      {
        "name" : "Zheng Lin"
      },
      {
        "name" : "Tengjiao He"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.22554v1",
    "title" : "Privacy-preserving fall detection at the edge using Sony IMX636 event-based vision sensor and Intel Loihi 2 neuromorphic processor",
    "summary" : "Fall detection for elderly care using non-invasive vision-based systems remains an important yet unsolved problem. Driven by strict privacy requirements, inference must run at the edge of the vision sensor, demanding robust, real-time, and always-on perception under tight hardware constraints. To address these challenges, we propose a neuromorphic fall detection system that integrates the Sony IMX636 event-based vision sensor with the Intel Loihi 2 neuromorphic processor via a dedicated FPGA-based interface, leveraging the sparsity of event data together with near-memory asynchronous processing. Using a newly recorded dataset under diverse environmental conditions, we explore the design space of sparse neural networks deployable on a single Loihi 2 chip and analyze the tradeoffs between detection F1 score and computational cost. Notably, on the Pareto front, our LIF-based convolutional SNN with graded spikes achieves the highest computational efficiency, reaching a 55x synaptic operations sparsity for an F1 score of 58%. The LIF with graded spikes shows a gain of 6% in F1 score with 5x less operations compared to binary spikes. Furthermore, our MCUNet feature extractor with patched inference, combined with the S4D state space model, achieves the highest F1 score of 84% with a synaptic operations sparsity of 2x and a total power consumption of 90 mW on Loihi 2. Overall, our smart security camera proof-of-concept highlights the potential of integrating neuromorphic sensing and processing for edge AI applications where latency, energy consumption, and privacy are critical.",
    "updated" : "2025-11-27T15:44:55Z",
    "published" : "2025-11-27T15:44:55Z",
    "authors" : [
      {
        "name" : "Lyes Khacef"
      },
      {
        "name" : "Philipp Weidel"
      },
      {
        "name" : "Susumu Hogyoku"
      },
      {
        "name" : "Harry Liu"
      },
      {
        "name" : "Claire Alexandra BrÃ¤uer"
      },
      {
        "name" : "Shunsuke Koshino"
      },
      {
        "name" : "Takeshi Oyakawa"
      },
      {
        "name" : "Vincent Parret"
      },
      {
        "name" : "Yoshitaka Miyatani"
      },
      {
        "name" : "Mike Davies"
      },
      {
        "name" : "Mathis Richter"
      }
    ],
    "categories" : [
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.22515v1",
    "title" : "Privacy-Utility-Bias Trade-offs for Privacy-Preserving Recommender Systems",
    "summary" : "Recommender systems (RSs) output ranked lists of items, such as movies or restaurants, that users may find interesting, based on the user's past ratings and ratings from other users. RSs increasingly incorporate differential privacy (DP) to protect user data, raising questions about how privacy mechanisms affect both recommendation accuracy and fairness. We conduct a comprehensive, cross-model evaluation of two DP mechanisms, differentially private stochastic gradient descent (DPSGD) and local differential privacy (LDP), applied to four recommender systems (Neural Collaborative Filtering (NCF), Bayesian Personalized Ranking (BPR), Singular Value Decomposition (SVD), and Variational Autoencoder (VAE)) on the MovieLens-1M and Yelp datasets. We find that stronger privacy consistently reduces utility, but not uniformly. NCF under DPSGD shows the smallest accuracy loss (under 10 percent at epsilon approximately 1), whereas SVD and BPR experience larger drops, especially for users with niche preferences. VAE is the most sensitive to privacy, with sharp declines for sparsely represented groups. The impact on bias metrics is similarly heterogeneous. DPSGD generally reduces the gap between recommendations of popular and less popular items, whereas LDP preserves existing patterns more closely. These results highlight that no single DP mechanism is uniformly superior; instead, each provides trade-offs under different privacy regimes and data conditions.",
    "updated" : "2025-11-27T14:50:20Z",
    "published" : "2025-11-27T14:50:20Z",
    "authors" : [
      {
        "name" : "Shiva Parsarad"
      },
      {
        "name" : "Isabel Wagner"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.22441v1",
    "title" : "GEO-Detective: Unveiling Location Privacy Risks in Images with LLM Agents",
    "summary" : "Images shared on social media often expose geographic cues. While early geolocation methods required expert effort and lacked generalization, the rise of Large Vision Language Models (LVLMs) now enables accurate geolocation even for ordinary users. However, existing approaches are not optimized for this task. To explore the full potential and associated privacy risks, we present Geo-Detective, an agent that mimics human reasoning and tool use for image geolocation inference. It follows a procedure with four steps that adaptively selects strategies based on image difficulty and is equipped with specialized tools such as visual reverse search, which emulates how humans gather external geographic clues. Experimental results show that GEO-Detective outperforms baseline large vision language models (LVLMs) overall, particularly on images lacking visible geographic features. In country level geolocation tasks, it achieves an improvement of over 11.1% compared to baseline LLMs, and even at finer grained levels, it still provides around a 5.2% performance gain. Meanwhile, when equipped with external clues, GEO-Detective becomes more likely to produce accurate predictions, reducing the \"unknown\" prediction rate by more than 50.6%. We further explore multiple defense strategies and find that Geo-Detective exhibits stronger robustness, highlighting the need for more effective privacy safeguards.",
    "updated" : "2025-11-27T13:27:26Z",
    "published" : "2025-11-27T13:27:26Z",
    "authors" : [
      {
        "name" : "Xinyu Zhang"
      },
      {
        "name" : "Yixin Wu"
      },
      {
        "name" : "Boyang Zhang"
      },
      {
        "name" : "Chenhao Lin"
      },
      {
        "name" : "Chao Shen"
      },
      {
        "name" : "Michael Backes"
      },
      {
        "name" : "Yang Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.22180v1",
    "title" : "Personalized 3D Spatiotemporal Trajectory Privacy Protection with Differential and Distortion Geo-Perturbation",
    "summary" : "The rapid advancement of location-based services (LBSs) in three-dimensional (3D) domains, such as smart cities and intelligent transportation, has raised concerns over 3D spatiotemporal trajectory privacy protection. However, existing research has not fully addressed the risk of attackers exploiting the spatiotemporal correlation of 3D spatiotemporal trajectories and the impact of height information, both of which can potentially lead to significant privacy leakage. To address these issues, this paper proposes a personalized 3D spatiotemporal trajectory privacy protection mechanism, named 3DSTPM. First, we analyze the characteristics of attackers that exploit spatiotemporal correlations between locations in a trajectory and present the attack model. Next, we exploit the complementary characteristics of 3D geo-indistinguishability (3D-GI) and distortion privacy to find a protection location set (PLS) that obscures the real location for all possible locations. To address the issue of privacy accumulation caused by continuous trajectory queries, we propose a Window-based Adaptive Privacy Budget Allocation (W-APBA), which dynamically allocates privacy budgets to all locations in the current PLS based on their predictability and sensitivity. Finally, we perturb the real location using the allocated privacy budget by the PF (Permute-and-Flip) mechanism, effectively balancing privacy protection and Quality of Service (QoS). Simulation results demonstrate that the proposed 3DSTPM effectively reduces QoS loss while meeting the user's personalized privacy protection needs.",
    "updated" : "2025-11-27T07:41:14Z",
    "published" : "2025-11-27T07:41:14Z",
    "authors" : [
      {
        "name" : "Minghui Min"
      },
      {
        "name" : "Yulu Li"
      },
      {
        "name" : "Gang Li"
      },
      {
        "name" : "Meng Li"
      },
      {
        "name" : "Hongliang Zhang"
      },
      {
        "name" : "Miao Pan"
      },
      {
        "name" : "Dusit Niyato"
      },
      {
        "name" : "Zhu Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.22117v1",
    "title" : "Privacy-preserving formal concept analysis: A homomorphic encryption-based concept construction",
    "summary" : "Formal Concept Analysis (FCA) is extensively used in knowledge extraction, cognitive concept learning, and data mining. However, its computational demands on large-scale datasets often require outsourcing to external computing services, raising concerns about the leakage of sensitive information. To address this challenge, we propose a novel approach to enhance data security and privacy in FCA-based computations. Specifically, we introduce a Privacy-preserving Formal Context Analysis (PFCA) framework that combines binary data representation with homomorphic encryption techniques. This method enables secure and efficient concept construction without revealing private data. Experimental results and security analysis confirm the effectiveness of our approach in preserving privacy while maintaining computational performance. These findings have important implications for privacy-preserving data mining and secure knowledge discovery in large-scale FCA applications.",
    "updated" : "2025-11-27T05:16:01Z",
    "published" : "2025-11-27T05:16:01Z",
    "authors" : [
      {
        "name" : "Qiangqiang Chen"
      },
      {
        "name" : "Yunfeng Ke"
      },
      {
        "name" : "Shen Li"
      },
      {
        "name" : "Jinhai Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.22099v1",
    "title" : "Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs",
    "summary" : "Large language models (LLMs) have driven major advances across domains, yet their massive size hinders deployment in resource-constrained settings. Model compression addresses this challenge, with low-rank factorization emerging as a particularly effective method for reducing size, memory, and computation while maintaining accuracy. However, while these compressed models boast of benign performance and system-level advantages, their trustworthiness implications remain poorly understood. In this paper, we present the first comprehensive study of how low-rank factorization affects LLM trustworthiness across privacy, adversarial robustness, fairness, and ethical alignment. We evaluate multiple LLMs of different sizes and variants compressed with diverse low-rank algorithms, revealing key insights: (1) low-rank compression preserves or improves training data privacy but weakens PII protection during conversation; (2) adversarial robustness is generally preserved and often enhanced, even under deep compression; (3) ethical reasoning degrades in zero-shot settings but partially recovers with few-shot prompting; (4) fairness declines under compression. Beyond compression, we investigate how model scale and fine-tuning affect trustworthiness, as both are important in low-rank methods. To guide trustworthy compression strategies, we end our paper with a gradient-based attribution analysis to identify which layers in LLMs contribute most to adversarial robustness.",
    "updated" : "2025-11-27T04:40:56Z",
    "published" : "2025-11-27T04:40:56Z",
    "authors" : [
      {
        "name" : "Daniel Agyei Asante"
      },
      {
        "name" : "Md Mokarram Chowdhury"
      },
      {
        "name" : "Yang Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.21876v1",
    "title" : "Differential privacy from axioms",
    "summary" : "Differential privacy (DP) is the de facto notion of privacy both in theory and in practice. However, despite its popularity, DP imposes strict requirements which guard against strong worst-case scenarios. For example, it guards against seemingly unrealistic scenarios where an attacker has full information about all but one point in the data set, and still nothing can be learned about the remaining point. While preventing such a strong attack is desirable, many works have explored whether average-case relaxations of DP are easier to satisfy [HWR13,WLF16,BF16,LWX23].\n  In this work, we are motivated by the question of whether alternate, weaker notions of privacy are possible: can a weakened privacy notion still guarantee some basic level of privacy, and on the other hand, achieve privacy more efficiently and/or for a substantially broader set of tasks? Our main result shows the answer is no: even in the statistical setting, any reasonable measure of privacy satisfying nontrivial composition is equivalent to DP. To prove this, we identify a core set of four axioms or desiderata: pre-processing invariance, prohibition of blatant non-privacy, strong composition, and linear scalability. Our main theorem shows that any privacy measure satisfying our axioms is equivalent to DP, up to polynomial factors in sample complexity. We complement this result by showing our axioms are minimal: removing any one of our axioms enables ill-behaved measures of privacy.",
    "updated" : "2025-11-26T19:53:02Z",
    "published" : "2025-11-26T19:53:02Z",
    "authors" : [
      {
        "name" : "Guy Blanc"
      },
      {
        "name" : "William Pires"
      },
      {
        "name" : "Toniann Pitassi"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.21804v1",
    "title" : "Beyond Membership: Limitations of Add/Remove Adjacency in Differential Privacy",
    "summary" : "Training machine learning models with differential privacy (DP) limits an adversary's ability to infer sensitive information about the training data. It can be interpreted as a bound on adversary's capability to distinguish two adjacent datasets according to chosen adjacency relation. In practice, most DP implementations use the add/remove adjacency relation, where two datasets are adjacent if one can be obtained from the other by adding or removing a single record, thereby protecting membership. In many ML applications, however, the goal is to protect attributes of individual records (e.g., labels used in supervised fine-tuning). We show that privacy accounting under add/remove overstates attribute privacy compared to accounting under the substitute adjacency relation, which permits substituting one record. To demonstrate this gap, we develop novel attacks to audit DP under substitute adjacency, and show empirically that audit results are inconsistent with DP guarantees reported under add/remove, yet remain consistent with the budget accounted under the substitute adjacency relation. Our results highlight that the choice of adjacency when reporting DP guarantees is critical when the protection target is per-record attributes rather than membership.",
    "updated" : "2025-11-26T18:55:13Z",
    "published" : "2025-11-26T18:55:13Z",
    "authors" : [
      {
        "name" : "Gauri Pradhan"
      },
      {
        "name" : "Joonas JÃ¤lkÃ¶"
      },
      {
        "name" : "Santiago Zanella-BÃ¨guelin"
      },
      {
        "name" : "Antti Honkela"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2511.21758v1",
    "title" : "A Longitudinal Measurement of Privacy Policy Evolution for Large Language Models",
    "summary" : "Large language model (LLM) services have been rapidly integrated into people's daily lives as chatbots and agentic systems. They are nourished by collecting rich streams of data, raising privacy concerns around excessive collection of sensitive personal information. Privacy policies are the fundamental mechanism for informing users about data practices in modern information privacy paradigm. Although traditional web and mobile policies are well studied, the privacy policies of LLM providers, their LLM-specific content, and their evolution over time remain largely underexplored. In this paper, we present the first longitudinal empirical study of privacy policies for mainstream LLM providers worldwide. We curate a chronological dataset of 74 historical privacy policies and 115 supplemental privacy documents from 11 LLM providers across 5 countries up to August 2025, and extract over 3,000 sentence-level edits between consecutive policy versions. We compare LLM privacy policies to those of other software formats, propose a taxonomy tailored to LLM privacy policies, annotate policy edits and align them with a timeline of key LLM ecosystem events. Results show they are substantially longer, demand college-level reading ability, and remain highly vague. Our taxonomy analysis reveals patterns in how providers disclose LLM-specific practices and highlights regional disparities in coverage. Policy edits are concentrated in first-party data collection and international/specific-audience sections, and that product releases and regulatory actions are the primary drivers, shedding light on the status quo and the evolution of LLM privacy policies.",
    "updated" : "2025-11-24T12:40:15Z",
    "published" : "2025-11-24T12:40:15Z",
    "authors" : [
      {
        "name" : "Zhen Tao"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Zhenchang Xing"
      },
      {
        "name" : "Emily Black"
      },
      {
        "name" : "Talia Gillis"
      },
      {
        "name" : "Chunyang Chen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ]
  }
]