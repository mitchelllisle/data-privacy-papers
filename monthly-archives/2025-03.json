[
  {
    "id" : "http://arxiv.org/abs/2503.02862v1",
    "title" : "Privacy and Accuracy-Aware AI/ML Model Deduplication",
    "summary" : "With the growing adoption of privacy-preserving machine learning algorithms,\nsuch as Differentially Private Stochastic Gradient Descent (DP-SGD), training\nor fine-tuning models on private datasets has become increasingly prevalent.\nThis shift has led to the need for models offering varying privacy guarantees\nand utility levels to satisfy diverse user requirements. However, managing\nnumerous versions of large models introduces significant operational\nchallenges, including increased inference latency, higher resource consumption,\nand elevated costs. Model deduplication is a technique widely used by many\nmodel serving and database systems to support high-performance and low-cost\ninference queries and model diagnosis queries. However, none of the existing\nmodel deduplication works has considered privacy, leading to unbounded\naggregation of privacy costs for certain deduplicated models and inefficiencies\nwhen applied to deduplicate DP-trained models. We formalize the problems of\ndeduplicating DP-trained models for the first time and propose a novel privacy-\nand accuracy-aware deduplication mechanism to address the problems. We\ndeveloped a greedy strategy to select and assign base models to target models\nto minimize storage and privacy costs. When deduplicating a target model, we\ndynamically schedule accuracy validations and apply the Sparse Vector Technique\nto reduce the privacy costs associated with private validation data. Compared\nto baselines that do not provide privacy guarantees, our approach improved the\ncompression ratio by up to $35\\times$ for individual models (including large\nlanguage models and vision transformers). We also observed up to $43\\times$\ninference speedup due to the reduction of I/O operations.",
    "updated" : "2025-03-04T18:40:38Z",
    "published" : "2025-03-04T18:40:38Z",
    "authors" : [
      {
        "name" : "Hong Guan"
      },
      {
        "name" : "Lei Yu"
      },
      {
        "name" : "Lixi Zhou"
      },
      {
        "name" : "Li Xiong"
      },
      {
        "name" : "Kanchan Chowdhury"
      },
      {
        "name" : "Lulu Xie"
      },
      {
        "name" : "Xusheng Xiao"
      },
      {
        "name" : "Jia Zou"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02693v1",
    "title" : "Federated Learning for Privacy-Preserving Feedforward Control in\n  Multi-Agent Systems",
    "summary" : "Feedforward control (FF) is often combined with feedback control (FB) in many\ncontrol systems, improving tracking performance, efficiency, and stability.\nHowever, designing effective data-driven FF controllers in multi-agent systems\nrequires significant data collection, including transferring private or\nproprietary data, which raises privacy concerns and incurs high communication\ncosts. Therefore, we propose a novel approach integrating Federated Learning\n(FL) into FF control to address these challenges. This approach enables\nprivacy-preserving, communication-efficient, and decentralized continuous\nimprovement of FF controllers across multiple agents without sharing personal\nor proprietary data. By leveraging FL, each agent learns a local, neural FF\ncontroller using its data and contributes only model updates to a global\naggregation process, ensuring data privacy and scalability. We demonstrate the\neffectiveness of our method in an autonomous driving use case. Therein,\nvehicles equipped with a trajectory-tracking feedback controller are enhanced\nby FL-based neural FF control. Simulations highlight significant improvements\nin tracking performance compared to pure FB control, analogous to model-based\nFF control. We achieve comparable tracking performance without exchanging\nprivate vehicle-specific data compared to a centralized neural FF control. Our\nresults underscore the potential of FL-based neural FF control to enable\nprivacy-preserving learning in multi-agent control systems, paving the way for\nscalable and efficient autonomous systems applications.",
    "updated" : "2025-03-04T15:07:25Z",
    "published" : "2025-03-04T15:07:25Z",
    "authors" : [
      {
        "name" : "Jakob Weber"
      },
      {
        "name" : "Markus Gurtner"
      },
      {
        "name" : "Benedikt Alt"
      },
      {
        "name" : "Adrian Trachte"
      },
      {
        "name" : "Andreas Kugi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02549v1",
    "title" : "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation",
    "summary" : "The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the data collected from hospitals are\nstored in one center and used to train the nnU-Net. This centralized approach\nhas various limitations, such as leakage of sensitive patient information and\nviolation of patient privacy. Federated learning is one of the approaches to\ntrain a segmentation model in a decentralized manner that helps preserve\npatient privacy. In this paper, we propose FednnU-Net, a federated learning\nextension of nnU-Net. We introduce two novel federated learning methods to the\nnnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric\nFederated Averaging (AsymFedAvg) - and experimentally show their consistent\nperformance for breast, cardiac and fetal segmentation using 6 datasets\nrepresenting samples from 18 institutions. Additionally, to further promote\nresearch and deployment of decentralized training in privacy constrained\ninstitutions, we make our plug-n-play framework public. The source-code is\navailable at https://github.com/faildeny/FednnUNet .",
    "updated" : "2025-03-04T12:20:06Z",
    "published" : "2025-03-04T12:20:06Z",
    "authors" : [
      {
        "name" : "Grzegorz Skorupko"
      },
      {
        "name" : "Fotios Avgoustidis"
      },
      {
        "name" : "Carlos Martín-Isla"
      },
      {
        "name" : "Lidia Garrucho"
      },
      {
        "name" : "Dimitri A. Kessler"
      },
      {
        "name" : "Esmeralda Ruiz Pujadas"
      },
      {
        "name" : "Oliver Díaz"
      },
      {
        "name" : "Maciej Bobowicz"
      },
      {
        "name" : "Katarzyna Gwoździewicz"
      },
      {
        "name" : "Xavier Bargalló"
      },
      {
        "name" : "Paulius Jaruševičius"
      },
      {
        "name" : "Kaisar Kushibar"
      },
      {
        "name" : "Karim Lekadir"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02455v1",
    "title" : "Privacy Preservation Techniques (PPTs) in IoT Systems: A Scoping Review\n  and Future Directions",
    "summary" : "Privacy preservation in Internet of Things (IoT) systems requires the use of\nprivacy-enhancing technologies (PETs) built from innovative technologies such\nas cryptography and artificial intelligence (AI) to create techniques called\nprivacy preservation techniques (PPTs). These PPTs achieve various privacy\ngoals and address different privacy concerns by mitigating potential privacy\nthreats within IoT systems. This study carried out a scoping review of\ndifferent types of PPTs used in previous research works on IoT systems between\n2010 and early 2023 to further explore the advantages of privacy preservation\nin these systems. This scoping review looks at privacy goals, possible\ntechnologies used for building PET, the integration of PPTs into the computing\nlayer of the IoT architecture, different IoT applications in which PPTs are\ndeployed, and the different privacy types addressed by these techniques within\nIoT systems. Key findings, such as the prominent privacy goal and privacy type\nin IoT, are discussed in this survey, along with identified research gaps that\ncould inform future endeavors in privacy research and benefit the privacy\nresearch community and other stakeholders in IoT systems.",
    "updated" : "2025-03-04T10:03:45Z",
    "published" : "2025-03-04T10:03:45Z",
    "authors" : [
      {
        "name" : "Emmanuel Alalade"
      },
      {
        "name" : "Ashraf Matrawy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02132v1",
    "title" : "Video-DPRP: A Differentially Private Approach for Visual\n  Privacy-Preserving Video Human Activity Recognition",
    "summary" : "Considerable effort has been made in privacy-preserving video human activity\nrecognition (HAR). Two primary approaches to ensure privacy preservation in\nVideo HAR are differential privacy (DP) and visual privacy. Techniques\nenforcing DP during training provide strong theoretical privacy guarantees but\noffer limited capabilities for visual privacy assessment. Conversely methods,\nsuch as low-resolution transformations, data obfuscation and adversarial\nnetworks, emphasize visual privacy but lack clear theoretical privacy\nassurances. In this work, we focus on two main objectives: (1) leveraging DP\nproperties to develop a model-free approach for visual privacy in videos and\n(2) evaluating our proposed technique using both differential privacy and\nvisual privacy assessments on HAR tasks. To achieve goal (1), we introduce\nVideo-DPRP: a Video-sample-wise Differentially Private Random Projection\nframework for privacy-preserved video reconstruction for HAR. By using random\nprojections, noise matrices and right singular vectors derived from the\nsingular value decomposition of videos, Video-DPRP reconstructs DP videos using\nprivacy parameters ($\\epsilon,\\delta$) while enabling visual privacy\nassessment. For goal (2), using UCF101 and HMDB51 datasets, we compare\nVideo-DPRP's performance on activity recognition with traditional DP methods,\nand state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,\nwe assess its effectiveness in preserving privacy-related attributes such as\nfacial features, gender, and skin color, using the PA-HMDB and VISPR datasets.\nVideo-DPRP combines privacy-preservation from both a DP and visual privacy\nperspective unlike SOTA methods that typically address only one of these\naspects.",
    "updated" : "2025-03-03T23:43:12Z",
    "published" : "2025-03-03T23:43:12Z",
    "authors" : [
      {
        "name" : "Allassan Tchangmena A Nken"
      },
      {
        "name" : "Susan Mckeever"
      },
      {
        "name" : "Peter Corcoran"
      },
      {
        "name" : "Ihsan Ullah"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02114v1",
    "title" : "Fairness and/or Privacy on Social Graphs",
    "summary" : "Graph Neural Networks (GNNs) have shown remarkable success in various\ngraph-based learning tasks. However, recent studies have raised concerns about\nfairness and privacy issues in GNNs, highlighting the potential for biased or\ndiscriminatory outcomes and the vulnerability of sensitive information. This\npaper presents a comprehensive investigation of fairness and privacy in GNNs,\nexploring the impact of various fairness-preserving measures on model\nperformance. We conduct experiments across diverse datasets and evaluate the\neffectiveness of different fairness interventions. Our analysis considers the\ntrade-offs between fairness, privacy, and accuracy, providing insights into the\nchallenges and opportunities in achieving both fair and private graph learning.\nThe results highlight the importance of carefully selecting and combining\nfairness-preserving measures based on the specific characteristics of the data\nand the desired fairness objectives. This study contributes to a deeper\nunderstanding of the complex interplay between fairness, privacy, and accuracy\nin GNNs, paving the way for the development of more robust and ethical graph\nlearning models.",
    "updated" : "2025-03-03T22:56:32Z",
    "published" : "2025-03-03T22:56:32Z",
    "authors" : [
      {
        "name" : "Bartlomiej Surma"
      },
      {
        "name" : "Michael Backes"
      },
      {
        "name" : "Yang Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02091v1",
    "title" : "Which Code Statements Implement Privacy Behaviors in Android\n  Applications?",
    "summary" : "A \"privacy behavior\" in software is an action where the software uses\npersonal information for a service or a feature, such as a website using\nlocation to provide content relevant to a user. Programmers are required by\nregulations or application stores to provide privacy notices and labels\ndescribing these privacy behaviors. Although many tools and research prototypes\nhave been developed to help programmers generate these notices by analyzing the\nsource code, these approaches are often fairly coarse-grained (i.e., at the\nlevel of whole methods or files, rather than at the statement level). But this\nis not necessarily how privacy behaviors exist in code. Privacy behaviors are\nembedded in specific statements in code. Current literature does not examine\nwhat statements programmers see as most important, how consistent these views\nare, or how to detect them. In this paper, we conduct an empirical study to\nexamine which statements programmers view as most-related to privacy behaviors.\nWe find that expression statements that make function calls are most associated\nwith privacy behaviors, while the type of privacy label has little effect on\nthe attributes of the selected statements. We then propose an approach to\nautomatically detect these privacy-relevant statements by fine-tuning three\nlarge language models with the data from the study. We observe that the\nagreement between our approach and participants is comparable to or higher than\nan agreement between two participants. Our study and detection approach can\nhelp programmers understand which statements in code affect privacy in mobile\napplications.",
    "updated" : "2025-03-03T22:20:01Z",
    "published" : "2025-03-03T22:20:01Z",
    "authors" : [
      {
        "name" : "Chia-Yi Su"
      },
      {
        "name" : "Aakash Bansal"
      },
      {
        "name" : "Vijayanta Jain"
      },
      {
        "name" : "Sepideh Ghanavati"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Collin McMillan"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02019v1",
    "title" : "SLAP: Secure Location-proof and Anonymous Privacy-preserving Spectrum\n  Access",
    "summary" : "The rapid advancements in wireless technology have significantly increased\nthe demand for communication resources, leading to the development of Spectrum\nAccess Systems (SAS). However, network regulations require disclosing sensitive\nuser information, such as location coordinates and transmission details,\nraising critical privacy concerns. Moreover, as a database-driven architecture\nreliant on user-provided data, SAS necessitates robust location verification to\ncounter identity and location spoofing attacks and remains a primary target for\ndenial-of-service (DoS) attacks. Addressing these security challenges while\nadhering to regulatory requirements is essential. In this paper, we propose\nSLAP, a novel framework that ensures location privacy and anonymity during\nspectrum queries, usage notifications, and location-proof acquisition. Our\nsolution includes an adaptive dual-scenario location verification mechanism\nwith architectural flexibility and a fallback option, along with a counter-DoS\napproach using time-lock puzzles. We prove the security of SLAP and demonstrate\nits advantages over existing solutions through comprehensive performance\nevaluations.",
    "updated" : "2025-03-03T19:52:56Z",
    "published" : "2025-03-03T19:52:56Z",
    "authors" : [
      {
        "name" : "Saleh Darzi"
      },
      {
        "name" : "Attila A. Yavuz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02017v1",
    "title" : "A Lightweight and Secure Deep Learning Model for Privacy-Preserving\n  Federated Learning in Intelligent Enterprises",
    "summary" : "The ever growing Internet of Things (IoT) connections drive a new type of\norganization, the Intelligent Enterprise. In intelligent enterprises, machine\nlearning based models are adopted to extract insights from data. Due to the\nefficiency and privacy challenges of these traditional models, a new federated\nlearning (FL) paradigm has emerged. In FL, multiple enterprises can jointly\ntrain a model to update a final model. However, firstly, FL trained models\nusually perform worse than centralized models, especially when enterprises\ntraining data is non-IID (Independent and Identically Distributed). Second, due\nto the centrality of FL and the untrustworthiness of local enterprises,\ntraditional FL solutions are vulnerable to poisoning and inference attacks and\nviolate privacy. Thirdly, the continuous transfer of parameters between\nenterprises and servers increases communication costs. To this end, the\nFedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep\nLearning Model that includes three main phases. In the first phase, the goal is\nto solve the data type distribution skew challenge. Addressing privacy concerns\nagainst poisoning and inference attacks is covered in the second phase.\nFinally, to alleviate the communication overhead, a novel compression approach\nis proposed that significantly reduces the size of the updates. The experiment\nresults validate that FedAnil+ is secure against inference and poisoning\nattacks with better accuracy. In addition, it shows improvements over existing\napproaches in terms of model accuracy (13%, 16%, and 26%), communication cost\n(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%).",
    "updated" : "2025-03-03T19:51:13Z",
    "published" : "2025-03-03T19:51:13Z",
    "authors" : [
      {
        "name" : "Reza Fotohi"
      },
      {
        "name" : "Fereidoon Shams Aliee"
      },
      {
        "name" : "Bahar Farahani"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01482v1",
    "title" : "Revisiting Locally Differentially Private Protocols: Towards Better\n  Trade-offs in Privacy, Utility, and Attack Resistance",
    "summary" : "Local Differential Privacy (LDP) offers strong privacy protection, especially\nin settings in which the server collecting the data is untrusted. However,\ndesigning LDP mechanisms that achieve an optimal trade-off between privacy,\nutility, and robustness to adversarial inference attacks remains challenging.\nIn this work, we introduce a general multi-objective optimization framework for\nrefining LDP protocols, enabling the joint optimization of privacy and utility\nunder various adversarial settings. While our framework is flexible enough to\naccommodate multiple privacy and security attacks as well as utility metrics,\nin this paper we specifically optimize for Attacker Success Rate (ASR) under\ndistinguishability attack as a measure of privacy and Mean Squared Error (MSE)\nas a measure of utility. We systematically revisit these trade-offs by\nanalyzing eight state-of-the-art LDP protocols and proposing refined\ncounterparts that leverage tailored optimization techniques. Experimental\nresults demonstrate that our proposed adaptive mechanisms consistently\noutperform their non-adaptive counterparts, reducing ASR by up to five orders\nof magnitude while maintaining competitive utility. Analytical derivations also\nconfirm the effectiveness of our mechanisms, moving them closer to the ASR-MSE\nPareto frontier.",
    "updated" : "2025-03-03T12:41:01Z",
    "published" : "2025-03-03T12:41:01Z",
    "authors" : [
      {
        "name" : "Héber H. Arcolezi"
      },
      {
        "name" : "Sébastien Gambs"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01470v1",
    "title" : "Position: Ensuring mutual privacy is necessary for effective external\n  evaluation of proprietary AI systems",
    "summary" : "The external evaluation of AI systems is increasingly recognised as a crucial\napproach for understanding their potential risks. However, facilitating\nexternal evaluation in practice faces significant challenges in balancing\nevaluators' need for system access with AI developers' privacy and security\nconcerns. Additionally, evaluators have reason to protect their own privacy -\nfor example, in order to maintain the integrity of held-out test sets. We refer\nto the challenge of ensuring both developers' and evaluators' privacy as one of\nproviding mutual privacy. In this position paper, we argue that (i) addressing\nthis mutual privacy challenge is essential for effective external evaluation of\nAI systems, and (ii) current methods for facilitating external evaluation\ninadequately address this challenge, particularly when it comes to preserving\nevaluators' privacy. In making these arguments, we formalise the mutual privacy\nproblem; examine the privacy and access requirements of both model owners and\nevaluators; and explore potential solutions to this challenge, including\nthrough the application of cryptographic and hardware-based approaches.",
    "updated" : "2025-03-03T12:24:59Z",
    "published" : "2025-03-03T12:24:59Z",
    "authors" : [
      {
        "name" : "Ben Bucknall"
      },
      {
        "name" : "Robert F. Trager"
      },
      {
        "name" : "Michael A. Osborne"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01208v1",
    "title" : "Watch Out Your Album! On the Inadvertent Privacy Memorization in\n  Multi-Modal Large Language Models",
    "summary" : "Multi-Modal Large Language Models (MLLMs) have exhibited remarkable\nperformance on various vision-language tasks such as Visual Question Answering\n(VQA). Despite accumulating evidence of privacy concerns associated with\ntask-relevant content, it remains unclear whether MLLMs inadvertently memorize\nprivate content that is entirely irrelevant to the training tasks. In this\npaper, we investigate how randomly generated task-irrelevant private content\ncan become spuriously correlated with downstream objectives due to partial\nmini-batch training dynamics, thus causing inadvertent memorization.\nConcretely, we randomly generate task-irrelevant watermarks into VQA\nfine-tuning images at varying probabilities and propose a novel probing\nframework to determine whether MLLMs have inadvertently encoded such content.\nOur experiments reveal that MLLMs exhibit notably different training behaviors\nin partial mini-batch settings with task-irrelevant watermarks embedded.\nFurthermore, through layer-wise probing, we demonstrate that MLLMs trigger\ndistinct representational patterns when encountering previously seen\ntask-irrelevant knowledge, even if this knowledge does not influence their\noutput during prompting. Our code is available at\nhttps://github.com/illusionhi/ProbingPrivacy.",
    "updated" : "2025-03-03T06:10:27Z",
    "published" : "2025-03-03T06:10:27Z",
    "authors" : [
      {
        "name" : "Tianjie Ju"
      },
      {
        "name" : "Yi Hua"
      },
      {
        "name" : "Hao Fei"
      },
      {
        "name" : "Zhenyu Shao"
      },
      {
        "name" : "Yubin Zheng"
      },
      {
        "name" : "Haodong Zhao"
      },
      {
        "name" : "Mong-Li Lee"
      },
      {
        "name" : "Wynne Hsu"
      },
      {
        "name" : "Zhuosheng Zhang"
      },
      {
        "name" : "Gongshen Liu"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01089v1",
    "title" : "Privacy-preserving Machine Learning in Internet of Vehicle Applications:\n  Fundamentals, Recent Advances, and Future Direction",
    "summary" : "Machine learning (ML) has revolutionized Internet of Vehicles (IoV)\napplications by enhancing intelligent transportation, autonomous driving\ncapabilities, and various connected services within a large, heterogeneous\nnetwork. However, the increased connectivity and massive data exchange for ML\napplications introduce significant privacy challenges. Privacy-preserving\nmachine learning (PPML) offers potential solutions to address these challenges\nby preserving privacy at various stages of the ML pipeline. Despite the rapid\ndevelopment of ML-based IoV applications and the growing data privacy concerns,\nthere are limited comprehensive studies on the adoption of PPML within this\ndomain. Therefore, this study provides a comprehensive review of the\nfundamentals, recent advancements, and the challenges of integrating PPML into\nIoV applications. To conduct an extensive study, we first review existing\nsurveys of various PPML techniques and their integration into IoV across\ndifferent scopes. We then discuss the fundamentals of IoV and propose a\nfour-layer IoV architecture. Additionally, we categorize IoV applications into\nthree key domains and analyze the privacy challenges in leveraging ML for these\napplication domains. Next, we provide an overview of various PPML techniques,\nhighlighting their applicability and performance to address the privacy\nchallenges. Building on these fundamentals, we thoroughly review recent\nadvancements in integrating various PPML techniques within IoV applications,\ndiscussing their frameworks, key features, and performance evaluation in terms\nof privacy, utility, and efficiency. Finally, we identify current challenges\nand propose future research directions to enhance privacy and reliability in\nIoV applications.",
    "updated" : "2025-03-03T01:24:04Z",
    "published" : "2025-03-03T01:24:04Z",
    "authors" : [
      {
        "name" : "Nazmul Islam"
      },
      {
        "name" : "Mohammad Zulkernine"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01000v1",
    "title" : "Privacy vs. Profit: The Impact of Google's Manifest Version 3 (MV3)\n  Update on Ad Blocker Effectiveness",
    "summary" : "Google's recent update to the manifest file for Chrome browser\nextensions-transitioning from manifest version 2 (MV2) to manifest version 3\n(MV3)-has raised concerns among users and ad blocker providers, who worry that\nthe new restrictions, notably the shift from the powerful WebRequest API to the\nmore restrictive DeclarativeNetRequest API, might reduce ad blocker\neffectiveness. Because ad blockers play a vital role for millions of users\nseeking a more private and ad-free browsing experience, this study empirically\ninvestigates how the MV3 update affects their ability to block ads and\ntrackers. Through a browser-based experiment conducted across multiple samples\nof ad-supported websites, we compare the MV3 to MV2 instances of four widely\nused ad blockers. Our results reveal no statistically significant reduction in\nad-blocking or anti-tracking effectiveness for MV3 ad blockers compared to\ntheir MV2 counterparts, and in some cases, MV3 instances even exhibit slight\nimprovements in blocking trackers. These findings are reassuring for users,\nindicating that the MV3 instances of popular ad blockers continue to provide\neffective protection against intrusive ads and privacy-infringing trackers.\nWhile some uncertainties remain, ad blocker providers appear to have\nsuccessfully navigated the MV3 update, finding solutions that maintain the core\nfunctionality of their ad blockers.",
    "updated" : "2025-03-02T19:41:34Z",
    "published" : "2025-03-02T19:41:34Z",
    "authors" : [
      {
        "name" : "Karlo Lukic"
      },
      {
        "name" : "Lazaros Papadopoulos"
      }
    ],
    "categories" : [
      "cs.CY",
      "econ.GN",
      "q-fin.EC",
      "K.4.0, K.6.5, H.5.2",
      "K.4.0; K.6.5; H.5.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.00703v1",
    "title" : "Towards hyperparameter-free optimization with differential privacy",
    "summary" : "Differential privacy (DP) is a privacy-preserving paradigm that protects the\ntraining data when training deep learning models. Critically, the performance\nof models is determined by the training hyperparameters, especially those of\nthe learning rate schedule, thus requiring fine-grained hyperparameter tuning\non the data. In practice, it is common to tune the learning rate\nhyperparameters through the grid search that (1) is computationally expensive\nas multiple runs are needed, and (2) increases the risk of data leakage as the\nselection of hyperparameters is data-dependent. In this work, we adapt the\nautomatic learning rate schedule to DP optimization for any models and\noptimizers, so as to significantly mitigate or even eliminate the cost of\nhyperparameter tuning when applied together with automatic per-sample gradient\nclipping. Our hyperparameter-free DP optimization is almost as computationally\nefficient as the standard non-DP optimization, and achieves state-of-the-art DP\nperformance on various language and vision tasks.",
    "updated" : "2025-03-02T02:59:52Z",
    "published" : "2025-03-02T02:59:52Z",
    "authors" : [
      {
        "name" : "Zhiqi Bu"
      },
      {
        "name" : "Ruixuan Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03652v1",
    "title" : "Token-Level Privacy in Large Language Models",
    "summary" : "The use of language models as remote services requires transmitting private\ninformation to external providers, raising significant privacy concerns. This\nprocess not only risks exposing sensitive data to untrusted service providers\nbut also leaves it vulnerable to interception by eavesdroppers. Existing\nprivacy-preserving methods for natural language processing (NLP) interactions\nprimarily rely on semantic similarity, overlooking the role of contextual\ninformation. In this work, we introduce dchi-stencil, a novel token-level\nprivacy-preserving mechanism that integrates contextual and semantic\ninformation while ensuring strong privacy guarantees under the dchi\ndifferential privacy framework, achieving 2epsilon-dchi-privacy. By\nincorporating both semantic and contextual nuances, dchi-stencil achieves a\nrobust balance between privacy and utility. We evaluate dchi-stencil using\nstate-of-the-art language models and diverse datasets, achieving comparable and\neven better trade-off between utility and privacy compared to existing methods.\nThis work highlights the potential of dchi-stencil to set a new standard for\nprivacy-preserving NLP in modern, high-risk applications.",
    "updated" : "2025-03-05T16:27:25Z",
    "published" : "2025-03-05T16:27:25Z",
    "authors" : [
      {
        "name" : "Re'em Harel"
      },
      {
        "name" : "Niv Gilboa"
      },
      {
        "name" : "Yuval Pinter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03587v1",
    "title" : "\"You don't need a university degree to comprehend data protection this\n  way\": LLM-Powered Interactive Privacy Policy Assessment",
    "summary" : "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present the first qualitative user study on Large Language Model\n(LLM)-driven privacy policy assessment. To this end, we build and evaluate an\nLLM-based privacy policy assessment browser extension, which helps users\nunderstand the essence of a lengthy, complex privacy policy while browsing. The\ntool integrates a dashboard and an LLM chat. In our qualitative user study\n(N=22), we evaluate usability, understandability of the information our tool\nprovides, and its impacts on awareness. While providing a comprehensible quick\noverview and a chat for in-depth discussion improves privacy awareness, users\nnote issues with building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
    "updated" : "2025-03-05T15:22:35Z",
    "published" : "2025-03-05T15:22:35Z",
    "authors" : [
      {
        "name" : "Vincent Freiberger"
      },
      {
        "name" : "Arthur Fleig"
      },
      {
        "name" : "Erik Buchmann"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03539v1",
    "title" : "Data Sharing, Privacy and Security Considerations in the Energy Sector:\n  A Review from Technical Landscape to Regulatory Specifications",
    "summary" : "Decarbonization, decentralization and digitalization are the three key\nelements driving the twin energy transition. The energy system is evolving to a\nmore data driven ecosystem, leading to the need of communication and storage of\nlarge amount of data of different resolution from the prosumers and other\nstakeholders in the energy ecosystem. While the energy system is certainly\nadvancing, this paradigm shift is bringing in new privacy and security issues\nrelated to collection, processing and storage of data - not only from the\ntechnical dimension, but also from the regulatory perspective. Understanding\ndata privacy and security in the evolving energy system, regarding regulatory\ncompliance, is an immature field of research. Contextualized knowledge of how\nrelated issues are regulated is still in its infancy, and the practical and\ntechnical basis for the regulatory framework for data privacy and security is\nnot clear. To fill this gap, this paper conducts a comprehensive review of the\ndata-related issues for the energy system by integrating both technical and\nregulatory dimensions. We start by reviewing open-access data, data\ncommunication and data-processing techniques for the energy system, and use it\nas the basis to connect the analysis of data-related issues from the integrated\nperspective. We classify the issues into three categories: (i) data-sharing\namong energy end users and stakeholders (ii) privacy of end users, and (iii)\ncyber security, and then explore these issues from a regulatory perspective. We\nanalyze the evolution of related regulations, and introduce the relevant\nregulatory initiatives for the categorized issues in terms of regulatory\ndefinitions, concepts, principles, rights and obligations in the context of\nenergy systems. Finally, we provide reflections on the gaps that still exist,\nand guidelines for regulatory frameworks for a truly participatory energy\nsystem.",
    "updated" : "2025-03-05T14:23:56Z",
    "published" : "2025-03-05T14:23:56Z",
    "authors" : [
      {
        "name" : "Shiliang Zhang"
      },
      {
        "name" : "Sabita Maharjan"
      },
      {
        "name" : "Lee Andrew Bygrave"
      },
      {
        "name" : "Shui Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03506v1",
    "title" : "Rethinking Synthetic Data definitions: A privacy driven approach",
    "summary" : "Synthetic data is gaining traction as a cost-effective solution for the\nincreasing data demands of AI development and can be generated either from\nexisting knowledge or derived data captured from real-world events. The source\nof the synthetic data generation and the technique used significantly impacts\nits residual privacy risk and therefore its opportunity for sharing.\nTraditional classification of synthetic data types no longer fit the newer\ngeneration techniques and there is a need to better align the classification\nwith practical needs. We suggest a new way of grouping synthetic data types\nthat better supports privacy evaluations to aid regulatory policymaking. Our\nnovel classification provides flexibility to new advancements like deep\ngenerative methods and offers a more practical framework for future\napplications.",
    "updated" : "2025-03-05T13:54:13Z",
    "published" : "2025-03-05T13:54:13Z",
    "authors" : [
      {
        "name" : "Vibeke Binz Vallevik"
      },
      {
        "name" : "Serena Elizabeth Marshall"
      },
      {
        "name" : "Aleksandar Babic"
      },
      {
        "name" : "Jan Franz Nygaard"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03428v1",
    "title" : "Privacy is All You Need: Revolutionizing Wearable Health Data with\n  Advanced PETs",
    "summary" : "In a world where data is the new currency, wearable health devices offer\nunprecedented insights into daily life, continuously monitoring vital signs and\nmetrics. However, this convenience raises privacy concerns, as these devices\ncollect sensitive data that can be misused or breached. Traditional measures\noften fail due to real-time data processing needs and limited device power.\nUsers also lack awareness and control over data sharing and usage. We propose a\nPrivacy-Enhancing Technology (PET) framework for wearable devices, integrating\nfederated learning, lightweight cryptographic methods, and selectively deployed\nblockchain technology. The blockchain acts as a secure ledger triggered only\nupon data transfer requests, granting users real-time notifications and\ncontrol. By dismantling data monopolies, this approach returns data sovereignty\nto individuals. Through real-world applications like secure medical data\nsharing, privacy-preserving fitness tracking, and continuous health monitoring,\nour framework reduces privacy risks by up to 70 percent while preserving data\nutility and performance. This innovation sets a new benchmark for wearable\nprivacy and can scale to broader IoT ecosystems, including smart homes and\nindustry. As data continues to shape our digital landscape, our research\nunderscores the critical need to maintain privacy and user control at the\nforefront of technological progress.",
    "updated" : "2025-03-05T12:01:22Z",
    "published" : "2025-03-05T12:01:22Z",
    "authors" : [
      {
        "name" : "Karthik Barma"
      },
      {
        "name" : "Seshu Babu Barma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03267v1",
    "title" : "Quantum-Inspired Privacy-Preserving Federated Learning Framework for\n  Secure Dementia Classification",
    "summary" : "Dementia, a neurological disorder impacting millions globally, presents\nsignificant challenges in diagnosis and patient care. With the rise of privacy\nconcerns and security threats in healthcare, federated learning (FL) has\nemerged as a promising approach to enable collaborative model training across\ndecentralized datasets without exposing sensitive patient information. However,\nFL remains vulnerable to advanced security breaches such as gradient inversion\nand eavesdropping attacks. This paper introduces a novel framework that\nintegrates federated learning with quantum-inspired encryption techniques for\ndementia classification, emphasizing privacy preservation and security.\nLeveraging quantum key distribution (QKD), the framework ensures secure\ntransmission of model weights, protecting against unauthorized access and\ninterception during training. The methodology utilizes a convolutional neural\nnetwork (CNN) for dementia classification, with federated training conducted\nacross distributed healthcare nodes, incorporating QKD-encrypted weight sharing\nto secure the aggregation process. Experimental evaluations conducted on MRI\ndata from the OASIS dataset demonstrate that the proposed framework achieves\nidentical accuracy levels to a baseline model while enhancing data security and\nreducing loss by almost 1% compared to the classical baseline model. The\nframework offers significant implications for democratizing access to AI-driven\ndementia diagnostics in low- and middle-income countries, addressing critical\nresource and privacy constraints. This work contributes a robust, scalable, and\nsecure federated learning solution for healthcare applications, paving the way\nfor broader adoption of quantum-inspired techniques in AI-driven medical\nresearch.",
    "updated" : "2025-03-05T08:49:31Z",
    "published" : "2025-03-05T08:49:31Z",
    "authors" : [
      {
        "name" : "Gazi Tanbhir"
      },
      {
        "name" : "Md. Farhan Shahriyar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03146v1",
    "title" : "PriFFT: Privacy-preserving Federated Fine-tuning of Large Language\n  Models via Function Secret Sharing",
    "summary" : "Fine-tuning large language models (LLMs) raises privacy concerns due to the\nrisk of exposing sensitive training data. Federated learning (FL) mitigates\nthis risk by keeping training samples on local devices, but recent studies show\nthat adversaries can still infer private information from model updates in FL.\nAdditionally, LLM parameters are typically shared publicly during federated\nfine-tuning, while developers are often reluctant to disclose these parameters,\nposing further security challenges. Inspired by the above problems, we propose\nPriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both\nthe model updates and parameters. In PriFFT, clients and the server share model\ninputs and parameters by secret sharing, performing secure fine-tuning on\nshared values without accessing plaintext data. Due to considerable LLM\nparameters, privacy-preserving federated fine-tuning invokes complex secure\ncalculations and requires substantial communication and computation resources.\nTo optimize the efficiency of privacy-preserving federated fine-tuning of LLMs,\nwe introduce function secret-sharing protocols for various operations,\nincluding reciprocal calculation, tensor products, natural exponentiation,\nsoftmax, hyperbolic tangent, and dropout. The proposed protocols achieve up to\n4.02X speed improvement and reduce 7.19X communication overhead compared to the\nimplementation based on existing secret sharing methods. Besides, PriFFT\nachieves a 2.23X speed improvement and reduces 4.08X communication overhead in\nprivacy-preserving fine-tuning without accuracy drop compared to the existing\nsecret sharing methods.",
    "updated" : "2025-03-05T03:41:57Z",
    "published" : "2025-03-05T03:41:57Z",
    "authors" : [
      {
        "name" : "Zhichao You"
      },
      {
        "name" : "Xuewen Dong"
      },
      {
        "name" : "Ke Cheng"
      },
      {
        "name" : "Xutong Mu"
      },
      {
        "name" : "Jiaxuan Fu"
      },
      {
        "name" : "Shiyang Ma"
      },
      {
        "name" : "Qiang Qu"
      },
      {
        "name" : "Yulong Shen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03087v1",
    "title" : "\"Watch My Health, Not My Data\": Understanding Perceptions, Barriers,\n  Emotional Impact, & Coping Strategies Pertaining to IoT Privacy and Security\n  in Health Monitoring for Older Adults",
    "summary" : "The proliferation of \"Internet of Things (IoT)\" provides older adults with\ncritical support for \"health monitoring\" and independent living, yet\nsignificant concerns about security and privacy persist. In this paper, we\nreport on these issues through a two-phase user study, including a survey (N =\n22) and semi-structured interviews (n = 9) with adults aged 65+. We found that\nwhile 81.82% of our participants are aware of security features like\n\"two-factor authentication (2FA)\" and encryption, 63.64% express serious\nconcerns about unauthorized access to sensitive health data. Only 13.64% feel\nconfident in existing protections, citing confusion over \"data sharing\npolicies\" and frustration with \"complex security settings\" which lead to\ndistrust and anxiety. To cope, our participants adopt various strategies, such\nas relying on family or professional support and limiting feature usage leading\nto disengagement. Thus, we recommend \"adaptive security mechanisms,\" simplified\ninterfaces, and real-time transparency notifications to foster trust and ensure\n\"privacy and security by design\" in IoT health systems for older adults.",
    "updated" : "2025-03-05T01:04:13Z",
    "published" : "2025-03-05T01:04:13Z",
    "authors" : [
      {
        "name" : "Suleiman Saka"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03043v1",
    "title" : "Leveraging Randomness in Model and Data Partitioning for Privacy\n  Amplification",
    "summary" : "We study how inherent randomness in the training process -- where each sample\n(or client in federated learning) contributes only to a randomly selected\nportion of training -- can be leveraged for privacy amplification. This\nincludes (1) data partitioning, where a sample participates in only a subset of\ntraining iterations, and (2) model partitioning, where a sample updates only a\nsubset of the model parameters. We apply our framework to model parallelism in\nfederated learning, where each client updates a randomly selected subnetwork to\nreduce memory and computational overhead, and show that existing methods, e.g.\nmodel splitting or dropout, provide a significant privacy amplification gain\nnot captured by previous privacy analysis techniques. Additionally, we\nintroduce Balanced Iteration Subsampling, a new data partitioning method where\neach sample (or client) participates in a fixed number of training iterations.\nWe show that this method yields stronger privacy amplification than Poisson\n(i.i.d.) sampling of data (or clients). Our results demonstrate that randomness\nin the training process, which is structured rather than i.i.d. and interacts\nwith data in complex ways, can be systematically leveraged for significant\nprivacy amplification.",
    "updated" : "2025-03-04T22:49:59Z",
    "published" : "2025-03-04T22:49:59Z",
    "authors" : [
      {
        "name" : "Andy Dong"
      },
      {
        "name" : "Wei-Ning Chen"
      },
      {
        "name" : "Ayfer Ozgur"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02968v1",
    "title" : "Privacy-Preserving Fair Synthetic Tabular Data",
    "summary" : "Sharing of tabular data containing valuable but private information is\nlimited due to legal and ethical issues. Synthetic data could be an alternative\nsolution to this sharing problem, as it is artificially generated by machine\nlearning algorithms and tries to capture the underlying data distribution.\nHowever, machine learning models are not free from memorization and may\nintroduce biases, as they rely on training data. Producing synthetic data that\npreserves privacy and fairness while maintaining utility close to the real data\nis a challenging task. This research simultaneously addresses both the privacy\nand fairness aspects of synthetic data, an area not explored by other studies.\nIn this work, we present PF-WGAN, a privacy-preserving, fair synthetic tabular\ndata generator based on the WGAN-GP model. We have modified the original\nWGAN-GP by adding privacy and fairness constraints forcing it to produce\nprivacy-preserving fair data. This approach will enable the publication of\ndatasets that protect individual's privacy and remain unbiased toward any\nparticular group. We compared the results with three state-of-the-art synthetic\ndata generator models in terms of utility, privacy, and fairness across four\ndifferent datasets. We found that the proposed model exhibits a more balanced\ntrade-off among utility, privacy, and fairness.",
    "updated" : "2025-03-04T19:51:00Z",
    "published" : "2025-03-04T19:51:00Z",
    "authors" : [
      {
        "name" : "Fatima J. Sarmin"
      },
      {
        "name" : "Atiquer R. Rahman"
      },
      {
        "name" : "Christopher J. Henry"
      },
      {
        "name" : "Noman Mohammed"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04707v1",
    "title" : "Iris Style Transfer: Enhancing Iris Recognition with Style Features and\n  Privacy Preservation through Neural Style Transfer",
    "summary" : "Iris texture is widely regarded as a gold standard biometric modality for\nauthentication and identification. The demand for robust iris recognition\nmethods, coupled with growing security and privacy concerns regarding iris\nattacks, has escalated recently. Inspired by neural style transfer, an advanced\ntechnique that leverages neural networks to separate content and style\nfeatures, we hypothesize that iris texture's style features provide a reliable\nfoundation for recognition and are more resilient to variations like rotation\nand perspective shifts than traditional approaches. Our experimental results\nsupport this hypothesis, showing a significantly higher classification accuracy\ncompared to conventional features. Further, we propose using neural style\ntransfer to mask identifiable iris style features, ensuring the protection of\nsensitive biometric information while maintaining the utility of eye images for\ntasks like eye segmentation and gaze estimation. This work opens new avenues\nfor iris-oriented, secure, and privacy-aware biometric systems.",
    "updated" : "2025-03-06T18:55:21Z",
    "published" : "2025-03-06T18:55:21Z",
    "authors" : [
      {
        "name" : "Mengdi Wang"
      },
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Enkelejda Kasneci"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04652v1",
    "title" : "Evaluation of Privacy-aware Support Vector Machine (SVM) Learning using\n  Homomorphic Encryption",
    "summary" : "The requirement for privacy-aware machine learning increases as we continue\nto use PII (Personally Identifiable Information) within machine training. To\novercome these privacy issues, we can apply Fully Homomorphic Encryption (FHE)\nto encrypt data before it is fed into a machine learning model. This involves\ncreating a homomorphic encryption key pair, and where the associated public key\nwill be used to encrypt the input data, and the private key will decrypt the\noutput. But, there is often a performance hit when we use homomorphic\nencryption, and so this paper evaluates the performance overhead of using the\nSVM machine learning technique with the OpenFHE homomorphic encryption library.\nThis uses Python and the scikit-learn library for its implementation. The\nexperiments include a range of variables such as multiplication depth, scale\nsize, first modulus size, security level, batch size, and ring dimension, along\nwith two different SVM models, SVM-Poly and SVM-Linear. Overall, the results\nshow that the two main parameters which affect performance are the ring\ndimension and the modulus size, and that SVM-Poly and SVM-Linear show similar\nperformance levels.",
    "updated" : "2025-03-06T17:42:23Z",
    "published" : "2025-03-06T17:42:23Z",
    "authors" : [
      {
        "name" : "William J Buchanan"
      },
      {
        "name" : "Hisham Ali"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04451v1",
    "title" : "Privacy Preserving and Robust Aggregation for Cross-Silo Federated\n  Learning in Non-IID Settings",
    "summary" : "Federated Averaging remains the most widely used aggregation strategy in\nfederated learning due to its simplicity and scalability. However, its\nperformance degrades significantly in non-IID data settings, where client\ndistributions are highly imbalanced or skewed. Additionally, it relies on\nclients transmitting metadata, specifically the number of training samples,\nwhich introduces privacy risks and may conflict with regulatory frameworks like\nthe European GDPR. In this paper, we propose a novel aggregation strategy that\naddresses these challenges by introducing class-aware gradient masking. Unlike\ntraditional approaches, our method relies solely on gradient updates,\neliminating the need for any additional client metadata, thereby enhancing\nprivacy protection. Furthermore, our approach validates and dynamically weights\nclient contributions based on class-specific importance, ensuring robustness\nagainst non-IID distributions, convergence prevention, and backdoor attacks.\nExtensive experiments on benchmark datasets demonstrate that our method not\nonly outperforms FedAvg and other widely accepted aggregation strategies in\nnon-IID settings but also preserves model integrity in adversarial scenarios.\nOur results establish the effectiveness of gradient masking as a practical and\nsecure solution for federated learning.",
    "updated" : "2025-03-06T14:06:20Z",
    "published" : "2025-03-06T14:06:20Z",
    "authors" : [
      {
        "name" : "Marco Arazzi"
      },
      {
        "name" : "Mert Cihangiroglu"
      },
      {
        "name" : "Antonino Nocera"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04054v1",
    "title" : "Controlled privacy leakage propagation throughout overlapping grouped\n  learning",
    "summary" : "Federated Learning (FL) is the standard protocol for collaborative learning.\nIn FL, multiple workers jointly train a shared model. They exchange model\nupdates calculated on their data, while keeping the raw data itself local.\nSince workers naturally form groups based on common interests and privacy\npolicies, we are motivated to extend standard FL to reflect a setting with\nmultiple, potentially overlapping groups. In this setup where workers can\nbelong and contribute to more than one group at a time, complexities arise in\nunderstanding privacy leakage and in adhering to privacy policies. To address\nthe challenges, we propose differential private overlapping grouped learning\n(DPOGL), a novel method to implement privacy guarantees within overlapping\ngroups. Under the honest-but-curious threat model, we derive novel privacy\nguarantees between arbitrary pairs of workers. These privacy guarantees\ndescribe and quantify two key effects of privacy leakage in DP-OGL: propagation\ndelay, i.e., the fact that information from one group will leak to other groups\nonly with temporal offset through the common workers and information\ndegradation, i.e., the fact that noise addition over model updates limits\ninformation leakage between workers. Our experiments show that applying DP-OGL\nenhances utility while maintaining strong privacy compared to standard FL\nsetups.",
    "updated" : "2025-03-06T03:14:45Z",
    "published" : "2025-03-06T03:14:45Z",
    "authors" : [
      {
        "name" : "Shahrzad Kiani"
      },
      {
        "name" : "Franziska Boenisch"
      },
      {
        "name" : "Stark C. Draper"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03988v1",
    "title" : "AI-based Programming Assistants for Privacy-related Code Generation: The\n  Developers' Experience",
    "summary" : "With the popularising of generative AI, the existence of AI-based programming\nassistants for developers is no surprise. Developers increasingly use them for\ntheir work, including generating code to fulfil the data protection\nrequirements (privacy) of the apps they build. We wanted to know if the reality\nis the same as expectations of AI-based programming assistants when trying to\nfulfil software privacy requirements, and the challenges developers face when\nusing AI-based programming assistants and how these can be improved. To this\nend, we conducted a survey with 51 developers worldwide. We found that AI-based\nprogramming assistants need to be improved in order for developers to better\ntrust them with generating code that ensures privacy. In this paper, we provide\nsome practical recommendations for developers to consider following when using\nAI-based programming assistants for privacy-related code development, and some\nkey further research directions.",
    "updated" : "2025-03-06T00:34:25Z",
    "published" : "2025-03-06T00:34:25Z",
    "authors" : [
      {
        "name" : "Kashumi Madampe"
      },
      {
        "name" : "John Grundy"
      },
      {
        "name" : "Nalin Arachchilage"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.05684v1",
    "title" : "Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints",
    "summary" : "Pre-trained foundation models can be adapted for specific tasks using\nLow-Rank Adaptation (LoRA). However, the fairness properties of these adapted\nclassifiers remain underexplored. Existing fairness-aware fine-tuning methods\nrely on direct access to sensitive attributes or their predictors, but in\npractice, these sensitive attributes are often held under strict consumer\nprivacy controls, and neither the attributes nor their predictors are available\nto model developers, hampering the development of fair models. To address this\nissue, we introduce a set of LoRA-based fine-tuning methods that can be trained\nin a distributed fashion, where model developers and fairness auditors\ncollaborate without sharing sensitive attributes or predictors. In this paper,\nwe evaluate three such methods - sensitive unlearning, adversarial training,\nand orthogonality loss - against a fairness-unaware baseline, using experiments\non the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Base\nmodel. We find that orthogonality loss consistently reduces bias while\nmaintaining or improving utility, whereas adversarial training improves False\nPositive Rate Parity and Demographic Parity in some cases, and sensitive\nunlearning provides no clear benefit. In tasks where significant biases are\npresent, distributed fairness-aware fine-tuning methods can effectively\neliminate bias without compromising consumer privacy and, in most cases,\nimprove model utility.",
    "updated" : "2025-03-07T18:49:57Z",
    "published" : "2025-03-07T18:49:57Z",
    "authors" : [
      {
        "name" : "Parameswaran Kamalaruban"
      },
      {
        "name" : "Mark Anderson"
      },
      {
        "name" : "Stuart Burrell"
      },
      {
        "name" : "Maeve Madigan"
      },
      {
        "name" : "Piotr Skalski"
      },
      {
        "name" : "David Sutton"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04980v1",
    "title" : "A Consensus Privacy Metrics Framework for Synthetic Data",
    "summary" : "Synthetic data generation is one approach for sharing individual-level data.\nHowever, to meet legislative requirements, it is necessary to demonstrate that\nthe individuals' privacy is adequately protected. There is no consolidated\nstandard for measuring privacy in synthetic data. Through an expert panel and\nconsensus process, we developed a framework for evaluating privacy in synthetic\ndata. Our findings indicate that current similarity metrics fail to measure\nidentity disclosure, and their use is discouraged. For differentially private\nsynthetic data, a privacy budget other than close to zero was not considered\ninterpretable. There was consensus on the importance of membership and\nattribute disclosure, both of which involve inferring personal information\nabout an individual without necessarily revealing their identity. The resultant\nframework provides precise recommendations for metrics that address these types\nof disclosures effectively. Our findings further present specific opportunities\nfor future research that can help with widespread adoption of synthetic data.",
    "updated" : "2025-03-06T21:19:02Z",
    "published" : "2025-03-06T21:19:02Z",
    "authors" : [
      {
        "name" : "Lisa Pilgram"
      },
      {
        "name" : "Fida K. Dankar"
      },
      {
        "name" : "Jorg Drechsler"
      },
      {
        "name" : "Mark Elliot"
      },
      {
        "name" : "Josep Domingo-Ferrer"
      },
      {
        "name" : "Paul Francis"
      },
      {
        "name" : "Murat Kantarcioglu"
      },
      {
        "name" : "Linglong Kong"
      },
      {
        "name" : "Bradley Malin"
      },
      {
        "name" : "Krishnamurty Muralidhar"
      },
      {
        "name" : "Puja Myles"
      },
      {
        "name" : "Fabian Prasser"
      },
      {
        "name" : "Jean Louis Raisaro"
      },
      {
        "name" : "Chao Yan"
      },
      {
        "name" : "Khaled El Emam"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04866v1",
    "title" : "Privacy in Responsible AI: Approaches to Facial Recognition from Cloud\n  Providers",
    "summary" : "As the use of facial recognition technology is expanding in different\ndomains, ensuring its responsible use is gaining more importance. This paper\nconducts a comprehensive literature review of existing studies on facial\nrecognition technology from the perspective of privacy, which is one of the key\nResponsible AI principles.\n  Cloud providers, such as Microsoft, AWS, and Google, are at the forefront of\ndelivering facial-related technology services, but their approaches to\nresponsible use of these technologies vary significantly. This paper compares\nhow these cloud giants implement the privacy principle into their facial\nrecognition and detection services. By analysing their approaches, it\nidentifies both common practices and notable differences. The results of this\nresearch will be valuable for developers and businesses by providing them\ninsights into best practices of three major companies for integration\nresponsible AI, particularly privacy, into their cloud-based facial recognition\ntechnologies.",
    "updated" : "2025-03-06T12:04:12Z",
    "published" : "2025-03-06T12:04:12Z",
    "authors" : [
      {
        "name" : "Anna Elivanova"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07427v1",
    "title" : "Creating and Evaluating Privacy and Security Micro-Lessons for\n  Elementary School Children",
    "summary" : "The growing use of technology in K--8 classrooms highlights a parallel need\nfor formal learning opportunities aimed at helping children use technology\nsafely and protect their personal information. Even the youngest students are\nnow using tablets, laptops, and apps to support their learning; however, there\nare limited curricular materials available for elementary and middle school\nchildren on digital privacy and security topics. To bridge this gap, we\ndeveloped a series of micro-lessons to help K--8 children learn about digital\nprivacy and security at school. We first conducted a formative study by\ninterviewing elementary school teachers to identify the design needs for\ndigital privacy and security lessons. We then developed micro-lessons --\nmultiple 15-20 minute activities designed to be easily inserted into the\nexisting curriculum -- using a co-design approach with multiple rounds of\ndeveloping and revising the micro-lessons in collaboration with teachers.\nThroughout the process, we conducted evaluation sessions where teachers\nimplemented or reviewed the micro-lessons. Our study identifies strengths,\nchallenges, and teachers' tailoring strategies when incorporating micro-lessons\nfor K--8 digital privacy and security topics, providing design implications for\nfacilitating learning about these topics in school classrooms.",
    "updated" : "2025-03-10T15:12:11Z",
    "published" : "2025-03-10T15:12:11Z",
    "authors" : [
      {
        "name" : "Lan Gao"
      },
      {
        "name" : "Elana B Blinder"
      },
      {
        "name" : "Abigail Barnes"
      },
      {
        "name" : "Kevin Song"
      },
      {
        "name" : "Tamara Clegg"
      },
      {
        "name" : "Jessica Vitak"
      },
      {
        "name" : "Marshini Chetty"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07216v1",
    "title" : "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
    "summary" : "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
    "updated" : "2025-03-10T11:55:50Z",
    "published" : "2025-03-10T11:55:50Z",
    "authors" : [
      {
        "name" : "Sangwoo Park"
      },
      {
        "name" : "Seanie Lee"
      },
      {
        "name" : "Byungjoo Kim"
      },
      {
        "name" : "Sung Ju Hwang"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07199v1",
    "title" : "How Well Can Differential Privacy Be Audited in One Run?",
    "summary" : "Recent methods for auditing the privacy of machine learning algorithms have\nimproved computational efficiency by simultaneously intervening on multiple\ntraining examples in a single training run. Steinke et al. (2024) prove that\none-run auditing indeed lower bounds the true privacy parameter of the audited\nalgorithm, and give impressive empirical results. Their work leaves open the\nquestion of how precisely one-run auditing can uncover the true privacy\nparameter of an algorithm, and how that precision depends on the audited\nalgorithm. In this work, we characterize the maximum achievable efficacy of\none-run auditing and show that one-run auditing can only perfectly uncover the\ntrue privacy parameters of algorithms whose structure allows the effects of\nindividual data elements to be isolated. Our characterization helps reveal how\nand when one-run auditing is still a promising technique for auditing real\nmachine learning algorithms, despite these fundamental gaps.",
    "updated" : "2025-03-10T11:32:30Z",
    "published" : "2025-03-10T11:32:30Z",
    "authors" : [
      {
        "name" : "Amit Keinan"
      },
      {
        "name" : "Moshe Shenfeld"
      },
      {
        "name" : "Katrina Ligett"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07048v1",
    "title" : "A Failure-Free and Efficient Discrete Laplace Distribution for\n  Differential Privacy in MPC",
    "summary" : "In an MPC-protected distributed computation, although the use of MPC assures\ndata privacy during computation, sensitive information may still be inferred by\ncurious MPC participants from the computation output. This can be observed, for\ninstance, in the inference attacks on either federated learning or a more\nstandard statistical computation with distributed inputs. In this work, we\naddress this output privacy issue by proposing a discrete and bounded\nLaplace-inspired perturbation mechanism along with a secure realization of this\nmechanism using MPC. The proposed mechanism strictly adheres to a zero failure\nprobability, overcoming the limitation encountered on other existing bounded\nand discrete variants of Laplace perturbation. We provide analyses of the\nproposed differential privacy (DP) perturbation in terms of its privacy and\nutility. Additionally, we designed MPC protocols to implement this mechanism\nand presented performance benchmarks based on our experimental setup. The MPC\nrealization of the proposed mechanism exhibits a complexity similar to the\nstate-of-the-art discrete Gaussian mechanism, which can be considered an\nalternative with comparable efficiency while providing stronger differential\nprivacy guarantee. Moreover, efficiency of the proposed scheme can be further\nenhanced by performing the noise generation offline while leaving the\nperturbation phase online.",
    "updated" : "2025-03-10T08:35:16Z",
    "published" : "2025-03-10T08:35:16Z",
    "authors" : [
      {
        "name" : "Ivan Tjuawinata"
      },
      {
        "name" : "Jiabo Wang"
      },
      {
        "name" : "Mengmeng Yang"
      },
      {
        "name" : "Shanxiang Lyu"
      },
      {
        "name" : "Huaxiong Wang"
      },
      {
        "name" : "Kwok-Yan Lam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06808v1",
    "title" : "Privacy Auditing of Large Language Models",
    "summary" : "Current techniques for privacy auditing of large language models (LLMs) have\nlimited efficacy -- they rely on basic approaches to generate canaries which\nleads to weak membership inference attacks that in turn give loose lower bounds\non the empirical privacy leakage. We develop canaries that are far more\neffective than those used in prior work under threat models that cover a range\nof realistic settings. We demonstrate through extensive experiments on multiple\nfamilies of fine-tuned LLMs that our approach sets a new standard for detection\nof privacy leakage. For measuring the memorization rate of non-privately\ntrained LLMs, our designed canaries surpass prior approaches. For example, on\nthe Qwen2.5-0.5B model, our designed canaries achieve $49.6\\%$ TPR at $1\\%$\nFPR, vastly surpassing the prior approach's $4.2\\%$ TPR at $1\\%$ FPR. Our\nmethod can be used to provide a privacy audit of $\\varepsilon \\approx 1$ for a\nmodel trained with theoretical $\\varepsilon$ of 4. To the best of our\nknowledge, this is the first time that a privacy audit of LLM training has\nachieved nontrivial auditing success in the setting where the attacker cannot\ntrain shadow models, insert gradient canaries, or access the model at every\niteration.",
    "updated" : "2025-03-09T23:32:15Z",
    "published" : "2025-03-09T23:32:15Z",
    "authors" : [
      {
        "name" : "Ashwinee Panda"
      },
      {
        "name" : "Xinyu Tang"
      },
      {
        "name" : "Milad Nasr"
      },
      {
        "name" : "Christopher A. Choquette-Choo"
      },
      {
        "name" : "Prateek Mittal"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06732v1",
    "title" : "Data Efficient Subset Training with Differential Privacy",
    "summary" : "Private machine learning introduces a trade-off between the privacy budget\nand training performance. Training convergence is substantially slower and\nextensive hyper parameter tuning is required. Consequently, efficient methods\nto conduct private training of models is thoroughly investigated in the\nliterature. To this end, we investigate the strength of the data efficient\nmodel training methods in the private training setting. We adapt GLISTER\n(Killamsetty et al., 2021b) to the private setting and extensively assess its\nperformance. We empirically find that practical choices of privacy budgets are\ntoo restrictive for data efficient training in the private setting.",
    "updated" : "2025-03-09T19:05:10Z",
    "published" : "2025-03-09T19:05:10Z",
    "authors" : [
      {
        "name" : "Ninad Jayesh Gandhi"
      },
      {
        "name" : "Moparthy Venkata Subrahmanya Sri Harsha"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06455v1",
    "title" : "Privacy Protection in Prosumer Energy Management Based on Federated\n  Learning",
    "summary" : "With the booming development of prosumers, there is an urgent need for a\nprosumer energy management system to take full advantage of the flexibility of\nprosumers and take into account the interests of other parties. However,\nbuilding such a system will undoubtedly reveal users' privacy. In this paper,\nby solving the non-independent and identical distribution of data (Non-IID)\nproblem in federated learning with federated cluster average(FedClusAvg)\nalgorithm, prosumers' information can efficiently participate in the\nintelligent decision making of the system without revealing privacy. In the\nproposed FedClusAvg algorithm, each client performs cluster stratified sampling\nand multiple iterations. Then, the average weight of the parameters of the\nsub-server is determined according to the degree of deviation of the parameter\nfrom the average parameter. Finally, the sub-server multiple local iterations\nand updates, and then upload to the main server. The advantages of FedClusAvg\nalgorithm are the following two parts. First, the accuracy of the model in the\ncase of Non-IID is improved through the method of clustering and parameter\nweighted average. Second, local multiple iterations and three-tier framework\ncan effectively reduce communication rounds.",
    "updated" : "2025-03-09T05:29:29Z",
    "published" : "2025-03-09T05:29:29Z",
    "authors" : [
      {
        "name" : "Yunfeng Li"
      },
      {
        "name" : "Xiaolin Li Zhitao Li"
      },
      {
        "name" : "Gangqiang Li"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06150v1",
    "title" : "Do Fairness Interventions Come at the Cost of Privacy: Evaluations for\n  Binary Classifiers",
    "summary" : "While in-processing fairness approaches show promise in mitigating biased\npredictions, their potential impact on privacy leakage remains under-explored.\nWe aim to address this gap by assessing the privacy risks of fairness-enhanced\nbinary classifiers via membership inference attacks (MIAs) and attribute\ninference attacks (AIAs). Surprisingly, our results reveal that enhancing\nfairness does not necessarily lead to privacy compromises. For example, these\nfairness interventions exhibit increased resilience against MIAs and AIAs. This\nis because fairness interventions tend to remove sensitive information among\nextracted features and reduce confidence scores for the majority of training\ndata for fairer predictions. However, during the evaluations, we uncover a\npotential threat mechanism that exploits prediction discrepancies between fair\nand biased models, leading to advanced attack results for both MIAs and AIAs.\nThis mechanism reveals potent vulnerabilities of fair models and poses\nsignificant privacy risks of current fairness methods. Extensive experiments\nacross multiple datasets, attack methods, and representative fairness\napproaches confirm our findings and demonstrate the efficacy of the uncovered\nmechanism. Our study exposes the under-explored privacy threats in fairness\nstudies, advocating for thorough evaluations of potential security\nvulnerabilities before model deployments.",
    "updated" : "2025-03-08T10:21:21Z",
    "published" : "2025-03-08T10:21:21Z",
    "authors" : [
      {
        "name" : "Huan Tian"
      },
      {
        "name" : "Guangsheng Zhang"
      },
      {
        "name" : "Bo Liu"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Wanlei Zhou"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06021v1",
    "title" : "FedEM: A Privacy-Preserving Framework for Concurrent Utility\n  Preservation in Federated Learning",
    "summary" : "Federated Learning (FL) enables collaborative training of models across\ndistributed clients without sharing local data, addressing privacy concerns in\ndecentralized systems. However, the gradient-sharing process exposes private\ndata to potential leakage, compromising FL's privacy guarantees in real-world\napplications. To address this issue, we propose Federated Error Minimization\n(FedEM), a novel algorithm that incorporates controlled perturbations through\nadaptive noise injection. This mechanism effectively mitigates gradient leakage\nattacks while maintaining model performance. Experimental results on benchmark\ndatasets demonstrate that FedEM significantly reduces privacy risks and\npreserves model accuracy, achieving a robust balance between privacy protection\nand utility preservation.",
    "updated" : "2025-03-08T02:48:00Z",
    "published" : "2025-03-08T02:48:00Z",
    "authors" : [
      {
        "name" : "Mingcong Xu"
      },
      {
        "name" : "Xiaojin Zhang"
      },
      {
        "name" : "Wei Chen"
      },
      {
        "name" : "Hai Jin"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.05954v1",
    "title" : "A Survey on Tabular Data Generation: Utility, Alignment, Fidelity,\n  Privacy, and Beyond",
    "summary" : "Generative modelling has become the standard approach for synthesising\ntabular data. However, different use cases demand synthetic data to comply with\ndifferent requirements to be useful in practice. In this survey, we review deep\ngenerative modelling approaches for tabular data from the perspective of four\ntypes of requirements: utility of the synthetic data, alignment of the\nsynthetic data with domain-specific knowledge, statistical fidelity of the\nsynthetic data distribution compared to the real data distribution, and\nprivacy-preserving capabilities. We group the approaches along two levels of\ngranularity: (i) based on the primary type of requirements they address and\n(ii) according to the underlying model they utilise. Additionally, we summarise\nthe appropriate evaluation methods for each requirement and the specific\ncharacteristics of each model type. Finally, we discuss future directions for\nthe field, along with opportunities to improve the current evaluation methods.\nOverall, this survey can be seen as a user guide to tabular data generation:\nhelping readers navigate available models and evaluation methods to find those\nbest suited to their needs.",
    "updated" : "2025-03-07T21:47:11Z",
    "published" : "2025-03-07T21:47:11Z",
    "authors" : [
      {
        "name" : "Mihaela Cătălina Stoian"
      },
      {
        "name" : "Eleonora Giunchiglia"
      },
      {
        "name" : "Thomas Lukasiewicz"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07483v1",
    "title" : "Poisoning Attacks to Local Differential Privacy Protocols for Trajectory\n  Data",
    "summary" : "Trajectory data, which tracks movements through geographic locations, is\ncrucial for improving real-world applications. However, collecting such\nsensitive data raises considerable privacy concerns. Local differential privacy\n(LDP) offers a solution by allowing individuals to locally perturb their\ntrajectory data before sharing it. Despite its privacy benefits, LDP protocols\nare vulnerable to data poisoning attacks, where attackers inject fake data to\nmanipulate aggregated results. In this work, we make the first attempt to\nanalyze vulnerabilities in several representative LDP trajectory protocols. We\npropose \\textsc{TraP}, a heuristic algorithm for data \\underline{P}oisoning\nattacks using a prefix-suffix method to optimize fake \\underline{Tra}jectory\nselection, significantly reducing computational complexity. Our experimental\nresults demonstrate that our attack can substantially increase target pattern\noccurrences in the perturbed trajectory dataset with few fake users. This study\nunderscores the urgent need for robust defenses and better protocol designs to\nsafeguard LDP trajectory data against malicious manipulation.",
    "updated" : "2025-03-06T02:31:45Z",
    "published" : "2025-03-06T02:31:45Z",
    "authors" : [
      {
        "name" : "I-Jung Hsu"
      },
      {
        "name" : "Chih-Hsun Lin"
      },
      {
        "name" : "Chia-Mu Yu"
      },
      {
        "name" : "Sy-Yen Kuo"
      },
      {
        "name" : "Chun-Ying Huang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.08568v1",
    "title" : "Privacy Law Enforcement Under Centralized Governance: A Qualitative\n  Analysis of Four Years' Special Privacy Rectification Campaigns",
    "summary" : "In recent years, major privacy laws like the GDPR have brought about positive\nchanges. However, challenges remain in enforcing the laws, particularly due to\nunder-resourced regulators facing a large number of potential privacy-violating\nsoftware applications (apps) and the high costs of investigating them. Since\n2019, China has launched a series of privacy enforcement campaigns known as\nSpecial Privacy Rectification Campaigns (SPRCs) to address widespread privacy\nviolations in its mobile application (app) ecosystem. Unlike the enforcement of\nthe GDPR, SPRCs are characterized by large-scale privacy reviews and strict\nsanctions, under the strong control of central authorities. In SPRCs, central\ngovernment authorities issue administrative orders to mobilize various\nresources for market-wide privacy reviews of mobile apps. They enforce strict\nsanctions by requiring privacy-violating apps to rectify issues within a short\ntimeframe or face removal from app stores. While there are a few reports on\nSPRCs, the effectiveness and potential problems of this campaign-style privacy\nenforcement approach remain unclear to the community. In this study, we\nconducted 18 semi-structured interviews with app-related engineers involved in\nSPRCs to better understand the campaign-style privacy enforcement. Based on the\ninterviews, we reported our findings on a variety of aspects of SPRCs, such as\nthe processes that app engineers regularly follow to achieve privacy compliance\nin SPRCs, the challenges they encounter, the solutions they adopt to address\nthese challenges, and the impacts of SPRCs, etc. We found that app engineers\nface a series of challenges in achieving privacy compliance in their apps...",
    "updated" : "2025-03-11T15:56:09Z",
    "published" : "2025-03-11T15:56:09Z",
    "authors" : [
      {
        "name" : "Tao Jing"
      },
      {
        "name" : "Yao Li"
      },
      {
        "name" : "Jingzhou Ye"
      },
      {
        "name" : "Jie Wang"
      },
      {
        "name" : "Xueqiang Wang"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.08297v1",
    "title" : "Privacy for Free: Leveraging Local Differential Privacy Perturbed Data\n  from Multiple Services",
    "summary" : "Local Differential Privacy (LDP) has emerged as a widely adopted\nprivacy-preserving technique in modern data analytics, enabling users to share\nstatistical insights while maintaining robust privacy guarantees. However,\ncurrent LDP applications assume a single service gathering perturbed\ninformation from users. In reality, multiple services may be interested in\ncollecting users' data, which poses privacy burdens to users as more such\nservices emerge. To address this issue, this paper proposes a framework for\ncollecting and aggregating data based on perturbed information from multiple\nservices, regardless of their estimated statistics (e.g., mean or distribution)\nand perturbation mechanisms.\n  Then for mean estimation, we introduce the Unbiased Averaging (UA) method and\nits optimized version, User-level Weighted Averaging (UWA). The former utilizes\nbiased perturbed data, while the latter assigns weights to different perturbed\nresults based on perturbation information, thereby achieving minimal variance.\nFor distribution estimation, we propose the User-level Likelihood Estimation\n(ULE), which treats all perturbed results from a user as a whole for maximum\nlikelihood estimation. Experimental results demonstrate that our framework and\nconstituting methods significantly improve the accuracy of both mean and\ndistribution estimation.",
    "updated" : "2025-03-11T11:10:03Z",
    "published" : "2025-03-11T11:10:03Z",
    "authors" : [
      {
        "name" : "Rong Du"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Yue Fu"
      },
      {
        "name" : "Haibo Hu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.08175v1",
    "title" : "Privacy-Enhancing Paradigms within Federated Multi-Agent Systems",
    "summary" : "LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving\ncomplex problems by integrating multiple agents, each performing different\nroles. However, in sensitive domains, they face emerging privacy protection\nchallenges. In this paper, we introduce the concept of Federated MAS,\nhighlighting the fundamental differences between Federated MAS and traditional\nFL. We then identify key challenges in developing Federated MAS, including: 1)\nheterogeneous privacy protocols among agents, 2) structural differences in\nmulti-party conversations, and 3) dynamic conversational network structures. To\naddress these challenges, we propose Embedded Privacy-Enhancing Agents\n(EPEAgent), an innovative solution that integrates seamlessly into the\nRetrieval-Augmented Generation (RAG) phase and the context retrieval stage.\nThis solution minimizes data flows, ensuring that only task-relevant,\nagent-specific information is shared. Additionally, we design and generate a\ncomprehensive dataset to evaluate the proposed paradigm. Extensive experiments\ndemonstrate that EPEAgent effectively enhances privacy protection while\nmaintaining strong system performance. The code will be availiable at\nhttps://github.com/ZitongShi/EPEAgent",
    "updated" : "2025-03-11T08:38:45Z",
    "published" : "2025-03-11T08:38:45Z",
    "authors" : [
      {
        "name" : "Zitong Shi"
      },
      {
        "name" : "Guancheng Wan"
      },
      {
        "name" : "Wenke Huang"
      },
      {
        "name" : "Guibin Zhang"
      },
      {
        "name" : "Jiawei Shao"
      },
      {
        "name" : "Mang Ye"
      },
      {
        "name" : "Carl Yang"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.08085v1",
    "title" : "PRISM: Privacy-Preserving Improved Stochastic Masking for Federated\n  Generative Models",
    "summary" : "Despite recent advancements in federated learning (FL), the integration of\ngenerative models into FL has been limited due to challenges such as high\ncommunication costs and unstable training in heterogeneous data environments.\nTo address these issues, we propose PRISM, a FL framework tailored for\ngenerative models that ensures (i) stable performance in heterogeneous data\ndistributions and (ii) resource efficiency in terms of communication cost and\nfinal model size. The key of our method is to search for an optimal stochastic\nbinary mask for a random network rather than updating the model weights,\nidentifying a sparse subnetwork with high generative performance; i.e., a\n``strong lottery ticket''. By communicating binary masks in a stochastic\nmanner, PRISM minimizes communication overhead. This approach, combined with\nthe utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic\nmoving average aggregation method (MADA) on the server side, facilitates stable\nand strong generative capabilities by mitigating local divergence in FL\nscenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a\nlightweight model without extra pruning or quantization, making it ideal for\nenvironments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and\nCIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining\nprivacy with minimal communication costs. PRISM is the first to successfully\ngenerate images under challenging non-IID and privacy-preserving FL\nenvironments on complex datasets, where previous methods have struggled.",
    "updated" : "2025-03-11T06:37:54Z",
    "published" : "2025-03-11T06:37:54Z",
    "authors" : [
      {
        "name" : "Kyeongkook Seo"
      },
      {
        "name" : "Dong-Jun Han"
      },
      {
        "name" : "Jaejun Yoo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07775v1",
    "title" : "Sublinear Algorithms for Wasserstein and Total Variation Distances:\n  Applications to Fairness and Privacy Auditing",
    "summary" : "Resource-efficiently computing representations of probability distributions\nand the distances between them while only having access to the samples is a\nfundamental and useful problem across mathematical sciences. In this paper, we\npropose a generic algorithmic framework to estimate the PDF and CDF of any\nsub-Gaussian distribution while the samples from them arrive in a stream. We\ncompute mergeable summaries of distributions from the stream of samples that\nrequire sublinear space w.r.t. the number of observed samples. This allows us\nto estimate Wasserstein and Total Variation (TV) distances between any two\nsub-Gaussian distributions while samples arrive in streams and from multiple\nsources (e.g. federated learning). Our algorithms significantly improves on the\nexisting methods for distance estimation incurring super-linear time and linear\nspace complexities. In addition, we use the proposed estimators of Wasserstein\nand TV distances to audit the fairness and privacy of the ML algorithms. We\nempirically demonstrate the efficiency of the algorithms for estimating these\ndistances and auditing using both synthetic and real-world datasets.",
    "updated" : "2025-03-10T18:57:48Z",
    "published" : "2025-03-10T18:57:48Z",
    "authors" : [
      {
        "name" : "Debabrota Basu"
      },
      {
        "name" : "Debarshi Chanda"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY",
      "cs.DS",
      "stat.CO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07570v1",
    "title" : "Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with\n  Blockchain-Based Auditability",
    "summary" : "Deep learning, when integrated with a large amount of training data, has the\npotential to outperform machine learning in terms of high accuracy. Recently,\nprivacy-preserving deep learning has drawn significant attention of the\nresearch community. Different privacy notions in deep learning include privacy\nof data provided by data-owners and privacy of parameters and/or\nhyperparameters of the underlying neural network. Federated learning is a\npopular privacy-preserving execution environment where data-owners participate\nin learning the parameters collectively without leaking their respective data\nto other participants. However, federated learning suffers from certain\nsecurity/privacy issues. In this paper, we propose Split-n-Chain, a variant of\nsplit learning where the layers of the network are split among several\ndistributed nodes. Split-n-Chain achieves several privacy properties:\ndata-owners need not share their training data with other nodes, and no nodes\nhave access to the parameters and hyperparameters of the neural network (except\nthat of the respective layers they hold). Moreover, Split-n-Chain uses\nblockchain to audit the computation done by different nodes. Our experimental\nresults show that: Split-n-Chain is efficient, in terms of time required to\nexecute different phases, and the training loss trend is similar to that for\nthe same neural network when implemented in a monolithic fashion.",
    "updated" : "2025-03-10T17:40:05Z",
    "published" : "2025-03-10T17:40:05Z",
    "authors" : [
      {
        "name" : "Mukesh Sahani"
      },
      {
        "name" : "Binanda Sengupta"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07505v1",
    "title" : "From Centralized to Decentralized Federated Learning: Theoretical\n  Insights, Privacy Preservation, and Robustness Challenges",
    "summary" : "Federated Learning (FL) enables collaborative learning without directly\nsharing individual's raw data. FL can be implemented in either a centralized\n(server-based) or decentralized (peer-to-peer) manner. In this survey, we\npresent a novel perspective: the fundamental difference between centralized FL\n(CFL) and decentralized FL (DFL) is not merely the network topology, but the\nunderlying training protocol: separate aggregation vs. joint optimization. We\nargue that this distinction in protocol leads to significant differences in\nmodel utility, privacy preservation, and robustness to attacks. We\nsystematically review and categorize existing works in both CFL and DFL\naccording to the type of protocol they employ. This taxonomy provides deeper\ninsights into prior research and clarifies how various approaches relate or\ndiffer. Through our analysis, we identify key gaps in the literature. In\nparticular, we observe a surprising lack of exploration of DFL approaches based\non distributed optimization methods, despite their potential advantages. We\nhighlight this under-explored direction and call for more research on\nleveraging distributed optimization for federated learning. Overall, this work\noffers a comprehensive overview from centralized to decentralized FL, sheds new\nlight on the core distinctions between approaches, and outlines open challenges\nand future directions for the field.",
    "updated" : "2025-03-10T16:27:40Z",
    "published" : "2025-03-10T16:27:40Z",
    "authors" : [
      {
        "name" : "Qiongxiu Li"
      },
      {
        "name" : "Wenrui Yu"
      },
      {
        "name" : "Yufei Xia"
      },
      {
        "name" : "Jun Pang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07216v2",
    "title" : "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
    "summary" : "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
    "updated" : "2025-03-11T12:49:15Z",
    "published" : "2025-03-10T11:55:50Z",
    "authors" : [
      {
        "name" : "Sangwoo Park"
      },
      {
        "name" : "Seanie Lee"
      },
      {
        "name" : "Byungjoo Kim"
      },
      {
        "name" : "Sung Ju Hwang"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06150v2",
    "title" : "Do Fairness Interventions Come at the Cost of Privacy: Evaluations for\n  Binary Classifiers",
    "summary" : "While in-processing fairness approaches show promise in mitigating biased\npredictions, their potential impact on privacy leakage remains under-explored.\nWe aim to address this gap by assessing the privacy risks of fairness-enhanced\nbinary classifiers via membership inference attacks (MIAs) and attribute\ninference attacks (AIAs). Surprisingly, our results reveal that enhancing\nfairness does not necessarily lead to privacy compromises. For example, these\nfairness interventions exhibit increased resilience against MIAs and AIAs. This\nis because fairness interventions tend to remove sensitive information among\nextracted features and reduce confidence scores for the majority of training\ndata for fairer predictions. However, during the evaluations, we uncover a\npotential threat mechanism that exploits prediction discrepancies between fair\nand biased models, leading to advanced attack results for both MIAs and AIAs.\nThis mechanism reveals potent vulnerabilities of fair models and poses\nsignificant privacy risks of current fairness methods. Extensive experiments\nacross multiple datasets, attack methods, and representative fairness\napproaches confirm our findings and demonstrate the efficacy of the uncovered\nmechanism. Our study exposes the under-explored privacy threats in fairness\nstudies, advocating for thorough evaluations of potential security\nvulnerabilities before model deployments.",
    "updated" : "2025-03-11T11:28:18Z",
    "published" : "2025-03-08T10:21:21Z",
    "authors" : [
      {
        "name" : "Huan Tian"
      },
      {
        "name" : "Guangsheng Zhang"
      },
      {
        "name" : "Bo Liu"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Wanlei Zhou"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09448v1",
    "title" : "Optimizing QoE-Privacy Tradeoff for Proactive VR Streaming",
    "summary" : "Proactive virtual reality (VR) streaming requires users to upload\nviewpoint-related information, raising significant privacy concerns. Existing\nstrategies preserve privacy by introducing errors to viewpoints, which,\nhowever, compromises the quality of experience (QoE) of users. In this paper,\nwe first delve into the analysis of the viewpoint leakage probability achieved\nby existing privacy-preserving approaches. We determine the optimal\ndistribution of viewpoint errors that minimizes the viewpoint leakage\nprobability. Our analyses show that existing approaches cannot fully eliminate\nviewpoint leakage. Then, we propose a novel privacy-preserving approach that\nintroduces noise to uploaded viewpoint prediction errors, which can ensure zero\nviewpoint leakage probability. Given the proposed approach, the tradeoff\nbetween privacy preservation and QoE is optimized to minimize the QoE loss\nwhile satisfying the privacy requirement. Simulation results validate our\nanalysis results and demonstrate that the proposed approach offers a promising\nsolution for balancing privacy and QoE.",
    "updated" : "2025-03-12T14:50:06Z",
    "published" : "2025-03-12T14:50:06Z",
    "authors" : [
      {
        "name" : "Xing Wei"
      },
      {
        "name" : "Shengqian Han"
      },
      {
        "name" : "Chenyang Yang"
      },
      {
        "name" : "Chengjian Sun"
      }
    ],
    "categories" : [
      "cs.MM",
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09381v1",
    "title" : "Faithful and Privacy-Preserving Implementation of Average Consensus",
    "summary" : "We propose a protocol based on mechanism design theory and encrypted control\nto solve average consensus problems among rational and strategic agents while\npreserving their privacy. The proposed protocol provides a mechanism that\nincentivizes the agents to faithfully implement the intended behavior specified\nin the protocol. Furthermore, the protocol runs over encrypted data using\nhomomorphic encryption and secret sharing to protect the privacy of agents. We\nalso analyze the security of the proposed protocol using a simulation paradigm\nin secure multi-party computation. The proposed protocol demonstrates that\nmechanism design and encrypted control can complement each other to achieve\nsecurity under rational adversaries.",
    "updated" : "2025-03-12T13:28:22Z",
    "published" : "2025-03-12T13:28:22Z",
    "authors" : [
      {
        "name" : "Kaoru Teranishi"
      },
      {
        "name" : "Kiminao Kogiso"
      },
      {
        "name" : "Takashi Tanaka"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.CR",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09365v1",
    "title" : "Membership Inference Attacks fueled by Few-Short Learning to detect\n  privacy leakage tackling data integrity",
    "summary" : "Deep learning models have an intrinsic privacy issue as they memorize parts\nof their training data, creating a privacy leakage. Membership Inference\nAttacks (MIA) exploit it to obtain confidential information about the data used\nfor training, aiming to steal information. They can be repurposed as a\nmeasurement of data integrity by inferring whether it was used to train a\nmachine learning model. While state-of-the-art attacks achieve a significant\nprivacy leakage, their requirements are not feasible enough, hindering their\nrole as practical tools to assess the magnitude of the privacy risk. Moreover,\nthe most appropriate evaluation metric of MIA, the True Positive Rate at low\nFalse Positive Rate lacks interpretability. We claim that the incorporation of\nFew-Shot Learning techniques to the MIA field and a proper qualitative and\nquantitative privacy evaluation measure should deal with these issues. In this\ncontext, our proposal is twofold. We propose a Few-Shot learning based MIA,\ncoined as the FeS-MIA model, which eases the evaluation of the privacy breach\nof a deep learning model by significantly reducing the number of resources\nrequired for the purpose. Furthermore, we propose an interpretable quantitative\nand qualitative measure of privacy, referred to as Log-MIA measure. Jointly,\nthese proposals provide new tools to assess the privacy leakage and to ease the\nevaluation of the training data integrity of deep learning models, that is, to\nanalyze the privacy breach of a deep learning model. Experiments carried out\nwith MIA over image classification and language modeling tasks and its\ncomparison to the state-of-the-art show that our proposals excel at reporting\nthe privacy leakage of a deep learning model with little extra information.",
    "updated" : "2025-03-12T13:09:43Z",
    "published" : "2025-03-12T13:09:43Z",
    "authors" : [
      {
        "name" : "Daniel Jiménez-López"
      },
      {
        "name" : "Nuria Rodríguez-Barroso"
      },
      {
        "name" : "M. Victoria Luzón"
      },
      {
        "name" : "Francisco Herrera"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09331v1",
    "title" : "Large-Scale FPGA-Based Privacy Amplification Exceeding $10^8$ Bits for\n  Quantum Key Distribution",
    "summary" : "Privacy Amplification (PA) is indispensable in Quantum Key Distribution (QKD)\npost-processing, as it eliminates information leakage to eavesdroppers.\nField-programmable gate arrays (FPGAs) are highly attractive for QKD systems\ndue to their flexibility and high integration. However, due to limited\nresources, input and output sizes remain the primary bottleneck in FPGA-based\nPA schemes for Discrete Variable (DV)-QKD systems. In this paper, we present a\nlarge-scale FPGA-based PA scheme that supports both input block sizes and\noutput key sizes exceeding $10^8$ bits, effectively addressing the challenges\nposed by the finite-size effect. To accommodate the large input and output\nsizes, we propose a novel PA algorithm and prove its security. We implement and\nevaluate this scheme on a Xilinx XCKU095 FPGA platform. Experimental results\ndemonstrate that our PA implementation can handle an input block size of $10^8$\nbits with flexible output sizes up to the input size. For DV-QKD systems, our\nPA scheme supports an input block size nearly two orders of magnitude larger\nthan current FPGA-based PA schemes, significantly mitigating the impact of the\nfinite-size effect on the final secure key rate.",
    "updated" : "2025-03-12T12:25:13Z",
    "published" : "2025-03-12T12:25:13Z",
    "authors" : [
      {
        "name" : "Xi Cheng"
      },
      {
        "name" : "Hao-kun Mao"
      },
      {
        "name" : "Hong-wei Xu"
      },
      {
        "name" : "Qiong Li"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09317v1",
    "title" : "RaceTEE: A Practical Privacy-Preserving Off-Chain Smart Contract\n  Execution Architecture",
    "summary" : "Decentralized on-chain smart contracts enable trustless collaboration, yet\ntheir inherent data transparency and execution overhead hinder widespread\nadoption. Existing cryptographic approaches incur high computational costs and\nlack generality. Meanwhile, prior TEE-based solutions suffer from practical\nlimitations, such as the inability to support inter-contract interactions,\nreliance on unbreakable TEEs, and compromised usability. We introduce RaceTEE,\na practical and privacy-preserving off-chain execution architecture for smart\ncontracts that leverages Trusted Execution Environments (TEEs). RaceTEE\ndecouples transaction ordering (on-chain) from execution (off-chain), with\ncomputations performed competitively in TEEs, ensuring confidentiality and\nminimizing overhead. It further enhances practicality through three key\nimprovements: supporting secure inter-contract interactions, providing a key\nrotation scheme that enforces forward and backward secrecy even in the event of\nTEE breaches, and enabling full compatibility with existing blockchains without\naltering the user interaction model. To validate its feasibility, we prototype\nRaceTEE using Intel SGX and Ethereum, demonstrating its applicability across\nvarious use cases and evaluating its performance.",
    "updated" : "2025-03-12T12:10:02Z",
    "published" : "2025-03-12T12:10:02Z",
    "authors" : [
      {
        "name" : "Keyu Zhang"
      },
      {
        "name" : "Andrew Martin"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09192v1",
    "title" : "Differential Privacy Personalized Federated Learning Based on\n  Dynamically Sparsified Client Updates",
    "summary" : "Personalized federated learning is extensively utilized in scenarios\ncharacterized by data heterogeneity, facilitating more efficient and automated\nlocal training on data-owning terminals. This includes the automated selection\nof high-performance model parameters for upload, thereby enhancing the overall\ntraining process. However, it entails significant risks of privacy leakage.\nExisting studies have attempted to mitigate these risks by utilizing\ndifferential privacy. Nevertheless, these studies present two major\nlimitations: (1) The integration of differential privacy into personalized\nfederated learning lacks sufficient personalization, leading to the\nintroduction of excessive noise into the model. (2) It fails to adequately\ncontrol the spatial scope of model update information, resulting in a\nsuboptimal balance between data privacy and model effectiveness in differential\nprivacy federated learning. In this paper, we propose a differentially private\npersonalized federated learning approach that employs dynamically sparsified\nclient updates through reparameterization and adaptive norm(DP-pFedDSU).\nReparameterization training effectively selects personalized client update\ninformation, thereby reducing the quantity of updates. This approach minimizes\nthe introduction of noise to the greatest extent possible. Additionally,\ndynamic adaptive norm refers to controlling the norm space of model updates\nduring the training process, mitigating the negative impact of clipping on the\nupdate information. These strategies substantially enhance the effective\nintegration of differential privacy and personalized federated learning.\nExperimental results on EMNIST, CIFAR-10, and CIFAR-100 demonstrate that our\nproposed scheme achieves superior performance and is well-suited for more\ncomplex personalized federated learning scenarios.",
    "updated" : "2025-03-12T09:34:05Z",
    "published" : "2025-03-12T09:34:05Z",
    "authors" : [
      {
        "name" : "Chuanyin Wang"
      },
      {
        "name" : "Yifei Zhang"
      },
      {
        "name" : "Neng Gao"
      },
      {
        "name" : "Qiang Luo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.08085v2",
    "title" : "PRISM: Privacy-Preserving Improved Stochastic Masking for Federated\n  Generative Models",
    "summary" : "Despite recent advancements in federated learning (FL), the integration of\ngenerative models into FL has been limited due to challenges such as high\ncommunication costs and unstable training in heterogeneous data environments.\nTo address these issues, we propose PRISM, a FL framework tailored for\ngenerative models that ensures (i) stable performance in heterogeneous data\ndistributions and (ii) resource efficiency in terms of communication cost and\nfinal model size. The key of our method is to search for an optimal stochastic\nbinary mask for a random network rather than updating the model weights,\nidentifying a sparse subnetwork with high generative performance; i.e., a\n``strong lottery ticket''. By communicating binary masks in a stochastic\nmanner, PRISM minimizes communication overhead. This approach, combined with\nthe utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic\nmoving average aggregation method (MADA) on the server side, facilitates stable\nand strong generative capabilities by mitigating local divergence in FL\nscenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a\nlightweight model without extra pruning or quantization, making it ideal for\nenvironments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and\nCIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining\nprivacy with minimal communication costs. PRISM is the first to successfully\ngenerate images under challenging non-IID and privacy-preserving FL\nenvironments on complex datasets, where previous methods have struggled.",
    "updated" : "2025-03-12T07:22:25Z",
    "published" : "2025-03-11T06:37:54Z",
    "authors" : [
      {
        "name" : "Kyeongkook Seo"
      },
      {
        "name" : "Dong-Jun Han"
      },
      {
        "name" : "Jaejun Yoo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07427v2",
    "title" : "Creating and Evaluating Privacy and Security Micro-Lessons for\n  Elementary School Children",
    "summary" : "The growing use of technology in K--8 classrooms highlights a parallel need\nfor formal learning opportunities aimed at helping children use technology\nsafely and protect their personal information. Even the youngest students are\nnow using tablets, laptops, and apps to support their learning; however, there\nare limited curricular materials available for elementary and middle school\nchildren on digital privacy and security topics. To bridge this gap, we\ndeveloped a series of micro-lessons to help K--8 children learn about digital\nprivacy and security at school. We first conducted a formative study by\ninterviewing elementary school teachers to identify the design needs for\ndigital privacy and security lessons. We then developed micro-lessons --\nmultiple 15-20 minute activities designed to be easily inserted into the\nexisting curriculum -- using a co-design approach with multiple rounds of\ndeveloping and revising the micro-lessons in collaboration with teachers.\nThroughout the process, we conducted evaluation sessions where teachers\nimplemented or reviewed the micro-lessons. Our study identifies strengths,\nchallenges, and teachers' tailoring strategies when incorporating micro-lessons\nfor K--8 digital privacy and security topics, providing design implications for\nfacilitating learning about these topics in school classrooms.",
    "updated" : "2025-03-11T20:36:50Z",
    "published" : "2025-03-10T15:12:11Z",
    "authors" : [
      {
        "name" : "Lan Gao"
      },
      {
        "name" : "Elana B Blinder"
      },
      {
        "name" : "Abigail Barnes"
      },
      {
        "name" : "Kevin Song"
      },
      {
        "name" : "Tamara Clegg"
      },
      {
        "name" : "Jessica Vitak"
      },
      {
        "name" : "Marshini Chetty"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10350v1",
    "title" : "Enhancing Facial Privacy Protection via Weakening Diffusion Purification",
    "summary" : "The rapid growth of social media has led to the widespread sharing of\nindividual portrait images, which pose serious privacy risks due to the\ncapabilities of automatic face recognition (AFR) systems for mass surveillance.\nHence, protecting facial privacy against unauthorized AFR systems is essential.\nInspired by the generation capability of the emerging diffusion models, recent\nmethods employ diffusion models to generate adversarial face images for privacy\nprotection. However, they suffer from the diffusion purification effect,\nleading to a low protection success rate (PSR). In this paper, we first propose\nlearning unconditional embeddings to increase the learning capacity for\nadversarial modifications and then use them to guide the modification of the\nadversarial latent code to weaken the diffusion purification effect. Moreover,\nwe integrate an identity-preserving structure to maintain structural\nconsistency between the original and generated images, allowing human observers\nto recognize the generated image as having the same identity as the original.\nExtensive experiments conducted on two public datasets, i.e., CelebA-HQ and\nLADN, demonstrate the superiority of our approach. The protected faces\ngenerated by our method outperform those produced by existing facial privacy\nprotection approaches in terms of transferability and natural appearance.",
    "updated" : "2025-03-13T13:27:53Z",
    "published" : "2025-03-13T13:27:53Z",
    "authors" : [
      {
        "name" : "Ali Salar"
      },
      {
        "name" : "Qing Liu"
      },
      {
        "name" : "Yingli Tian"
      },
      {
        "name" : "Guoying Zhao"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10239v1",
    "title" : "I Can Tell Your Secrets: Inferring Privacy Attributes from Mini-app\n  Interaction History in Super-apps",
    "summary" : "Super-apps have emerged as comprehensive platforms integrating various\nmini-apps to provide diverse services. While super-apps offer convenience and\nenriched functionality, they can introduce new privacy risks. This paper\nreveals a new privacy leakage source in super-apps: mini-app interaction\nhistory, including mini-app usage history (Mini-H) and operation history\n(Op-H). Mini-H refers to the history of mini-apps accessed by users, such as\ntheir frequency and categories. Op-H captures user interactions within\nmini-apps, including button clicks, bar drags, and image views. Super-apps can\nnaturally collect these data without instrumentation due to the web-based\nfeature of mini-apps. We identify these data types as novel and unexplored\nprivacy risks through a literature review of 30 papers and an empirical\nanalysis of 31 super-apps. We design a mini-app interaction history-oriented\ninference attack (THEFT), to exploit this new vulnerability. Using THEFT, the\ninsider threats within the low-privilege business department of the super-app\nvendor acting as the adversary can achieve more than 95.5% accuracy in\ninferring privacy attributes of over 16.1% of users. THEFT only requires a\nsmall training dataset of 200 users from public breached databases on the\nInternet. We also engage with super-app vendors and a standards association to\nincrease industry awareness and commitment to protect this data. Our\ncontributions are significant in identifying overlooked privacy risks,\ndemonstrating the effectiveness of a new attack, and influencing industry\npractices toward better privacy protection in the super-app ecosystem.",
    "updated" : "2025-03-13T10:29:40Z",
    "published" : "2025-03-13T10:29:40Z",
    "authors" : [
      {
        "name" : "Yifeng Cai"
      },
      {
        "name" : "Ziqi Zhang"
      },
      {
        "name" : "Mengyu Yao"
      },
      {
        "name" : "Junlin Liu"
      },
      {
        "name" : "Xiaoke Zhao"
      },
      {
        "name" : "Xinyi Fu"
      },
      {
        "name" : "Ruoyu Li"
      },
      {
        "name" : "Zhe Li"
      },
      {
        "name" : "Xiangqun Chen"
      },
      {
        "name" : "Yao Guo"
      },
      {
        "name" : "Ding Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10147v1",
    "title" : "Optimal Privacy-Preserving Distributed Median Consensus",
    "summary" : "Distributed median consensus has emerged as a critical paradigm in\nmulti-agent systems due to the inherent robustness of the median against\noutliers and anomalies in measurement. Despite the sensitivity of the data\ninvolved, the development of privacy-preserving mechanisms for median consensus\nremains underexplored. In this work, we present the first rigorous analysis of\nprivacy in distributed median consensus, focusing on an $L_1$-norm minimization\nframework. We establish necessary and sufficient conditions under which exact\nconsensus and perfect privacy-defined as zero information leakage-can be\nachieved simultaneously. Our information-theoretic analysis provides provable\nguarantees against passive and eavesdropping adversaries, ensuring that private\ndata remain concealed. Extensive numerical experiments validate our theoretical\nresults, demonstrating the practical feasibility of achieving both accuracy and\nprivacy in distributed median consensus.",
    "updated" : "2025-03-13T08:19:12Z",
    "published" : "2025-03-13T08:19:12Z",
    "authors" : [
      {
        "name" : "Wenrui Yu"
      },
      {
        "name" : "Qiongxiu Li"
      },
      {
        "name" : "Richard Heusdens"
      },
      {
        "name" : "Sokol Kosta"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09823v1",
    "title" : "Data Traceability for Privacy Alignment",
    "summary" : "This paper offers a new privacy approach for the growing ecosystem of\nservices--ranging from open banking to healthcare--dependent on sensitive\npersonal data sharing between individuals and third-parties. While these\nservices offer significant benefits, individuals want control over their data,\ntransparency regarding how their data is used, and accountability from\nthird-parties for misuse. However, existing legal and technical mechanisms are\ninadequate for supporting these needs. A comprehensive approach to the modern\nprivacy challenges of accountable third-party data sharing requires a closer\nalignment of technical system architecture and legal institutional design. In\norder to achieve this privacy alignment, we extend traditional security threat\nmodeling and analysis to encompass a broader range of privacy notions than has\nbeen typically considered. In particular, we introduce the concept of\ncovert-accountability, which addresses adversaries that may act dishonestly but\nface potential identification and legal consequences. As a concrete instance of\nthis design approach, we present the OTrace protocol, designed to provide\ntraceable, accountable, consumer-control in third-party data sharing\necosystems. OTrace empowers consumers with the knowledge of where their data\nis, who has it, what it is being used for, and whom it is being shared with. By\napplying our alignment framework to OTrace, we demonstrate that OTrace's\ntechnical affordances can provide more confident, scalable regulatory oversight\nwhen combined with complementary legal mechanisms.",
    "updated" : "2025-03-12T20:42:23Z",
    "published" : "2025-03-12T20:42:23Z",
    "authors" : [
      {
        "name" : "Kevin Liao"
      },
      {
        "name" : "Shreya Thipireddy"
      },
      {
        "name" : "Daniel Weitzner"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09780v1",
    "title" : "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents",
    "summary" : "LLM-powered AI agents are an emerging frontier with tremendous potential to\nincrease human productivity. However, empowering AI agents to take action on\ntheir user's behalf in day-to-day tasks involves giving them access to\npotentially sensitive and private information, which leads to a possible risk\nof inadvertent privacy leakage when the agent malfunctions. In this work, we\npropose one way to address that potential risk, by training AI agents to better\nsatisfy the privacy principle of data minimization. For the purposes of this\nbenchmark, by \"data minimization\" we mean instances where private information\nis shared only when it is necessary to fulfill a specific task-relevant\npurpose. We develop a benchmark called AgentDAM to evaluate how well existing\nand future AI agents can limit processing of potentially private information\nthat we designate \"necessary\" to fulfill the task. Our benchmark simulates\nrealistic web interaction scenarios and is adaptable to all existing web\nnavigation agents. We use AgentDAM to evaluate how well AI agents built on top\nof GPT-4, Llama-3 and Claude can limit processing of potentially private\ninformation when unnecessary, and show that these agents are often prone to\ninadvertent use of unnecessary sensitive information. We finally propose a\nprompting-based approach that reduces this.",
    "updated" : "2025-03-12T19:30:31Z",
    "published" : "2025-03-12T19:30:31Z",
    "authors" : [
      {
        "name" : "Arman Zharmagambetov"
      },
      {
        "name" : "Chuan Guo"
      },
      {
        "name" : "Ivan Evtimov"
      },
      {
        "name" : "Maya Pavlova"
      },
      {
        "name" : "Ruslan Salakhutdinov"
      },
      {
        "name" : "Kamalika Chaudhuri"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09381v2",
    "title" : "Faithful and Privacy-Preserving Implementation of Average Consensus",
    "summary" : "We propose a protocol based on mechanism design theory and encrypted control\nto solve average consensus problems among rational and strategic agents while\npreserving their privacy. The proposed protocol provides a mechanism that\nincentivizes the agents to faithfully implement the intended behavior specified\nin the protocol. Furthermore, the protocol runs over encrypted data using\nhomomorphic encryption and secret sharing to protect the privacy of agents. We\nalso analyze the security of the proposed protocol using a simulation paradigm\nin secure multi-party computation. The proposed protocol demonstrates that\nmechanism design and encrypted control can complement each other to achieve\nsecurity under rational adversaries.",
    "updated" : "2025-03-13T02:40:23Z",
    "published" : "2025-03-12T13:28:22Z",
    "authors" : [
      {
        "name" : "Kaoru Teranishi"
      },
      {
        "name" : "Kiminao Kogiso"
      },
      {
        "name" : "Takashi Tanaka"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.CR",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11232v1",
    "title" : "PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature\n  Intervention with Sparse Autoencoders",
    "summary" : "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing but also pose significant privacy risks by\nmemorizing and leaking Personally Identifiable Information (PII). Existing\nmitigation strategies, such as differential privacy and neuron-level\ninterventions, often degrade model utility or fail to effectively prevent\nleakage. To address this challenge, we introduce PrivacyScalpel, a novel\nprivacy-preserving framework that leverages LLM interpretability techniques to\nidentify and mitigate PII leakage while maintaining performance. PrivacyScalpel\ncomprises three key steps: (1) Feature Probing, which identifies layers in the\nmodel that encode PII-rich representations, (2) Sparse Autoencoding, where a\nk-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive\nfeatures,\n  and (3) Feature-Level Interventions, which employ targeted ablation and\nvector steering to suppress PII leakage.\n  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron\ndataset, shows that PrivacyScalpel significantly reduces email leakage from\n5.15\\% to as low as 0.0\\%, while maintaining over 99.4\\% of the original\nmodel's utility. Notably, our method outperforms neuron-level interventions in\nprivacy-utility trade-offs, demonstrating that acting on sparse, monosemantic\nfeatures is more effective than manipulating polysemantic neurons. Beyond\nimproving LLM privacy, our approach offers insights into the mechanisms\nunderlying PII memorization, contributing to the broader field of model\ninterpretability and secure AI deployment.",
    "updated" : "2025-03-14T09:31:01Z",
    "published" : "2025-03-14T09:31:01Z",
    "authors" : [
      {
        "name" : "Ahmed Frikha"
      },
      {
        "name" : "Muhammad Reza Ar Razi"
      },
      {
        "name" : "Krishna Kanth Nakka"
      },
      {
        "name" : "Ricardo Mendes"
      },
      {
        "name" : "Xue Jiang"
      },
      {
        "name" : "Xuebing Zhou"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11169v1",
    "title" : "Security and Privacy: Key Requirements for Molecular Communication in\n  Medicine and Healthcare",
    "summary" : "Molecular communication (MC) is an emerging paradigm that enables data\ntransmission through biochemical signals rather than traditional\nelectromagnetic waves. This approach is particularly promising for environments\nwhere conventional wireless communication is impractical, such as within the\nhuman body. However, security and privacy pose significant challenges that must\nbe addressed to ensure reliable communication. Moreover, MC is often\nevent-triggered, making it logical to adopt goal-oriented communication\nstrategies, similar to those used in message identification. This work explores\nsecure identification strategies for MC, with a focus on the\ninformation-theoretic security of message identification over Poisson wiretap\nchannels (DT-PWC).",
    "updated" : "2025-03-14T08:14:14Z",
    "published" : "2025-03-14T08:14:14Z",
    "authors" : [
      {
        "name" : "Vida Gholamiyan"
      },
      {
        "name" : "Yaning Zhao"
      },
      {
        "name" : "Wafa Labidi"
      },
      {
        "name" : "Holger Boche"
      },
      {
        "name" : "Christian Deppe"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11051v1",
    "title" : "Towards Privacy-preserved Pre-training of Remote Sensing Foundation\n  Models with Federated Mutual-guidance Learning",
    "summary" : "Traditional Remote Sensing Foundation models (RSFMs) are pre-trained with a\ndata-centralized paradigm, through self-supervision on large-scale curated\nremote sensing data. For each institution, however, pre-training RSFMs with\nlimited data in a standalone manner may lead to suboptimal performance, while\naggregating remote sensing data from multiple institutions for centralized\npre-training raises privacy concerns. Seeking for collaboration is a promising\nsolution to resolve this dilemma, where multiple institutions can\ncollaboratively train RSFMs without sharing private data. In this paper, we\npropose a novel privacy-preserved pre-training framework (FedSense), which\nenables multiple institutions to collaboratively train RSFMs without sharing\nprivate data. However, it is a non-trivial task hindered by a vicious cycle,\nwhich results from model drift by remote sensing data heterogeneity and high\ncommunication overhead. To break this vicious cycle, we introduce Federated\nMutual-guidance Learning. Specifically, we propose a Server-to-Clients Guidance\n(SCG) mechanism to guide clients updates towards global-flatness optimal\nsolutions. Additionally, we propose a Clients-to-Server Guidance (CSG)\nmechanism to inject local knowledge into the server by low-bit communication.\nExtensive experiments on four downstream tasks demonstrate the effectiveness of\nour FedSense in both full-precision and communication-reduced scenarios,\nshowcasing remarkable communication efficiency and performance gains.",
    "updated" : "2025-03-14T03:38:49Z",
    "published" : "2025-03-14T03:38:49Z",
    "authors" : [
      {
        "name" : "Jieyi Tan"
      },
      {
        "name" : "Chengwei Zhang"
      },
      {
        "name" : "Bo Dang"
      },
      {
        "name" : "Yansheng Li"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10945v1",
    "title" : "$(\\varepsilon, δ)$ Considered Harmful: Best Practices for Reporting\n  Differential Privacy Guarantees",
    "summary" : "Current practices for reporting the level of differential privacy (DP)\nguarantees for machine learning (ML) algorithms provide an incomplete and\npotentially misleading picture of the guarantees and make it difficult to\ncompare privacy levels across different settings. We argue for using Gaussian\ndifferential privacy (GDP) as the primary means of communicating DP guarantees\nin ML, with the full privacy profile as a secondary option in case GDP is too\ninaccurate. Unlike other widely used alternatives, GDP has only one parameter,\nwhich ensures easy comparability of guarantees, and it can accurately capture\nthe full privacy profile of many important ML applications. To support our\nclaims, we investigate the privacy profiles of state-of-the-art DP large-scale\nimage classification, and the TopDown algorithm for the U.S. Decennial Census,\nobserving that GDP fits the profiles remarkably well in all three cases.\nAlthough GDP is ideal for reporting the final guarantees, other formalisms\n(e.g., privacy loss random variables) are needed for accurate privacy\naccounting. We show that such intermediate representations can be efficiently\nconverted to GDP with minimal loss in tightness.",
    "updated" : "2025-03-13T23:06:30Z",
    "published" : "2025-03-13T23:06:30Z",
    "authors" : [
      {
        "name" : "Juan Felipe Gomez"
      },
      {
        "name" : "Bogdan Kulynych"
      },
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Jamie Hayes"
      },
      {
        "name" : "Borja Balle"
      },
      {
        "name" : "Antti Honkela"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10915v1",
    "title" : "Usable Privacy in Virtual Worlds: Design Implications for Data\n  Collection Awareness and Control Interfaces in Virtual Reality",
    "summary" : "Extended reality (XR) devices have become ubiquitous. They are equipped with\narrays of sensors, collecting extensive user and environmental data, allowing\ninferences about sensitive user information users may not realize they are\nsharing. Current VR privacy notices largely replicate mechanisms from 2D\ninterfaces, failing to leverage the unique affordances of virtual 3D\nenvironments. To address this, we conducted brainstorming and sketching\nsessions with novice game developers and designers, followed by privacy expert\nevaluations, to explore and refine privacy interfaces tailored for VR. Key\nchallenges include balancing user engagement with privacy awareness, managing\ncomplex privacy information with user comprehension, and maintaining compliance\nand trust. We identify design implications such as thoughtful gamification,\nexplicit and purpose-tied consent mechanisms, and granular, modifiable privacy\ncontrol options. Our findings provide actionable guidance to researchers and\npractitioners for developing privacy-aware and user-friendly VR experiences.",
    "updated" : "2025-03-13T22:02:54Z",
    "published" : "2025-03-13T22:02:54Z",
    "authors" : [
      {
        "name" : "Viktorija Paneva"
      },
      {
        "name" : "Verena Winterhalter"
      },
      {
        "name" : "Naga Sai Surya Vamsy Malladi"
      },
      {
        "name" : "Marvin Strauss"
      },
      {
        "name" : "Stefan Schneegass"
      },
      {
        "name" : "Florian Alt"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10727v1",
    "title" : "Word-level Annotation of GDPR Transparency Compliance in Privacy\n  Policies using Large Language Models",
    "summary" : "Ensuring transparency of data practices related to personal information is a\nfundamental requirement under the General Data Protection Regulation (GDPR),\nparticularly as mandated by Articles 13 and 14. However, assessing compliance\nat scale remains a challenge due to the complexity and variability of privacy\npolicy language. Manual audits are resource-intensive and inconsistent, while\nexisting automated approaches lack the granularity needed to capture nuanced\ntransparency disclosures.\n  In this paper, we introduce a large language model (LLM)-based framework for\nword-level GDPR transparency compliance annotation. Our approach comprises a\ntwo-stage annotation pipeline that combines initial LLM-based annotation with a\nself-correction mechanism for iterative refinement. This annotation pipeline\nenables the systematic identification and fine-grained annotation of\ntransparency-related content in privacy policies, aligning with 21 GDPR-derived\ntransparency requirements. To enable large-scale analysis, we compile a dataset\nof 703,791 English-language policies, from which we generate a sample of 200\nmanually annotated privacy policies.\n  To evaluate our approach, we introduce a two-tiered methodology assessing\nboth label- and span-level annotation performance. We conduct a comparative\nanalysis of eight high-profile LLMs, providing insights into their\neffectiveness in identifying GDPR transparency disclosures. Our findings\ncontribute to advancing the automation of GDPR compliance assessments and\nprovide valuable resources for future research in privacy policy analysis.",
    "updated" : "2025-03-13T11:41:25Z",
    "published" : "2025-03-13T11:41:25Z",
    "authors" : [
      {
        "name" : "Thomas Cory"
      },
      {
        "name" : "Wolf Rieder"
      },
      {
        "name" : "Julia Krämer"
      },
      {
        "name" : "Philip Raschke"
      },
      {
        "name" : "Patrick Herbke"
      },
      {
        "name" : "Axel Küpper"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  }
]