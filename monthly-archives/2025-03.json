[
  {
    "id" : "http://arxiv.org/abs/2503.02862v1",
    "title" : "Privacy and Accuracy-Aware AI/ML Model Deduplication",
    "summary" : "With the growing adoption of privacy-preserving machine learning algorithms,\nsuch as Differentially Private Stochastic Gradient Descent (DP-SGD), training\nor fine-tuning models on private datasets has become increasingly prevalent.\nThis shift has led to the need for models offering varying privacy guarantees\nand utility levels to satisfy diverse user requirements. However, managing\nnumerous versions of large models introduces significant operational\nchallenges, including increased inference latency, higher resource consumption,\nand elevated costs. Model deduplication is a technique widely used by many\nmodel serving and database systems to support high-performance and low-cost\ninference queries and model diagnosis queries. However, none of the existing\nmodel deduplication works has considered privacy, leading to unbounded\naggregation of privacy costs for certain deduplicated models and inefficiencies\nwhen applied to deduplicate DP-trained models. We formalize the problems of\ndeduplicating DP-trained models for the first time and propose a novel privacy-\nand accuracy-aware deduplication mechanism to address the problems. We\ndeveloped a greedy strategy to select and assign base models to target models\nto minimize storage and privacy costs. When deduplicating a target model, we\ndynamically schedule accuracy validations and apply the Sparse Vector Technique\nto reduce the privacy costs associated with private validation data. Compared\nto baselines that do not provide privacy guarantees, our approach improved the\ncompression ratio by up to $35\\times$ for individual models (including large\nlanguage models and vision transformers). We also observed up to $43\\times$\ninference speedup due to the reduction of I/O operations.",
    "updated" : "2025-03-04T18:40:38Z",
    "published" : "2025-03-04T18:40:38Z",
    "authors" : [
      {
        "name" : "Hong Guan"
      },
      {
        "name" : "Lei Yu"
      },
      {
        "name" : "Lixi Zhou"
      },
      {
        "name" : "Li Xiong"
      },
      {
        "name" : "Kanchan Chowdhury"
      },
      {
        "name" : "Lulu Xie"
      },
      {
        "name" : "Xusheng Xiao"
      },
      {
        "name" : "Jia Zou"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02693v1",
    "title" : "Federated Learning for Privacy-Preserving Feedforward Control in\n  Multi-Agent Systems",
    "summary" : "Feedforward control (FF) is often combined with feedback control (FB) in many\ncontrol systems, improving tracking performance, efficiency, and stability.\nHowever, designing effective data-driven FF controllers in multi-agent systems\nrequires significant data collection, including transferring private or\nproprietary data, which raises privacy concerns and incurs high communication\ncosts. Therefore, we propose a novel approach integrating Federated Learning\n(FL) into FF control to address these challenges. This approach enables\nprivacy-preserving, communication-efficient, and decentralized continuous\nimprovement of FF controllers across multiple agents without sharing personal\nor proprietary data. By leveraging FL, each agent learns a local, neural FF\ncontroller using its data and contributes only model updates to a global\naggregation process, ensuring data privacy and scalability. We demonstrate the\neffectiveness of our method in an autonomous driving use case. Therein,\nvehicles equipped with a trajectory-tracking feedback controller are enhanced\nby FL-based neural FF control. Simulations highlight significant improvements\nin tracking performance compared to pure FB control, analogous to model-based\nFF control. We achieve comparable tracking performance without exchanging\nprivate vehicle-specific data compared to a centralized neural FF control. Our\nresults underscore the potential of FL-based neural FF control to enable\nprivacy-preserving learning in multi-agent control systems, paving the way for\nscalable and efficient autonomous systems applications.",
    "updated" : "2025-03-04T15:07:25Z",
    "published" : "2025-03-04T15:07:25Z",
    "authors" : [
      {
        "name" : "Jakob Weber"
      },
      {
        "name" : "Markus Gurtner"
      },
      {
        "name" : "Benedikt Alt"
      },
      {
        "name" : "Adrian Trachte"
      },
      {
        "name" : "Andreas Kugi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02549v1",
    "title" : "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation",
    "summary" : "The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the data collected from hospitals are\nstored in one center and used to train the nnU-Net. This centralized approach\nhas various limitations, such as leakage of sensitive patient information and\nviolation of patient privacy. Federated learning is one of the approaches to\ntrain a segmentation model in a decentralized manner that helps preserve\npatient privacy. In this paper, we propose FednnU-Net, a federated learning\nextension of nnU-Net. We introduce two novel federated learning methods to the\nnnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric\nFederated Averaging (AsymFedAvg) - and experimentally show their consistent\nperformance for breast, cardiac and fetal segmentation using 6 datasets\nrepresenting samples from 18 institutions. Additionally, to further promote\nresearch and deployment of decentralized training in privacy constrained\ninstitutions, we make our plug-n-play framework public. The source-code is\navailable at https://github.com/faildeny/FednnUNet .",
    "updated" : "2025-03-04T12:20:06Z",
    "published" : "2025-03-04T12:20:06Z",
    "authors" : [
      {
        "name" : "Grzegorz Skorupko"
      },
      {
        "name" : "Fotios Avgoustidis"
      },
      {
        "name" : "Carlos Martín-Isla"
      },
      {
        "name" : "Lidia Garrucho"
      },
      {
        "name" : "Dimitri A. Kessler"
      },
      {
        "name" : "Esmeralda Ruiz Pujadas"
      },
      {
        "name" : "Oliver Díaz"
      },
      {
        "name" : "Maciej Bobowicz"
      },
      {
        "name" : "Katarzyna Gwoździewicz"
      },
      {
        "name" : "Xavier Bargalló"
      },
      {
        "name" : "Paulius Jaruševičius"
      },
      {
        "name" : "Kaisar Kushibar"
      },
      {
        "name" : "Karim Lekadir"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02455v1",
    "title" : "Privacy Preservation Techniques (PPTs) in IoT Systems: A Scoping Review\n  and Future Directions",
    "summary" : "Privacy preservation in Internet of Things (IoT) systems requires the use of\nprivacy-enhancing technologies (PETs) built from innovative technologies such\nas cryptography and artificial intelligence (AI) to create techniques called\nprivacy preservation techniques (PPTs). These PPTs achieve various privacy\ngoals and address different privacy concerns by mitigating potential privacy\nthreats within IoT systems. This study carried out a scoping review of\ndifferent types of PPTs used in previous research works on IoT systems between\n2010 and early 2023 to further explore the advantages of privacy preservation\nin these systems. This scoping review looks at privacy goals, possible\ntechnologies used for building PET, the integration of PPTs into the computing\nlayer of the IoT architecture, different IoT applications in which PPTs are\ndeployed, and the different privacy types addressed by these techniques within\nIoT systems. Key findings, such as the prominent privacy goal and privacy type\nin IoT, are discussed in this survey, along with identified research gaps that\ncould inform future endeavors in privacy research and benefit the privacy\nresearch community and other stakeholders in IoT systems.",
    "updated" : "2025-03-04T10:03:45Z",
    "published" : "2025-03-04T10:03:45Z",
    "authors" : [
      {
        "name" : "Emmanuel Alalade"
      },
      {
        "name" : "Ashraf Matrawy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02132v1",
    "title" : "Video-DPRP: A Differentially Private Approach for Visual\n  Privacy-Preserving Video Human Activity Recognition",
    "summary" : "Considerable effort has been made in privacy-preserving video human activity\nrecognition (HAR). Two primary approaches to ensure privacy preservation in\nVideo HAR are differential privacy (DP) and visual privacy. Techniques\nenforcing DP during training provide strong theoretical privacy guarantees but\noffer limited capabilities for visual privacy assessment. Conversely methods,\nsuch as low-resolution transformations, data obfuscation and adversarial\nnetworks, emphasize visual privacy but lack clear theoretical privacy\nassurances. In this work, we focus on two main objectives: (1) leveraging DP\nproperties to develop a model-free approach for visual privacy in videos and\n(2) evaluating our proposed technique using both differential privacy and\nvisual privacy assessments on HAR tasks. To achieve goal (1), we introduce\nVideo-DPRP: a Video-sample-wise Differentially Private Random Projection\nframework for privacy-preserved video reconstruction for HAR. By using random\nprojections, noise matrices and right singular vectors derived from the\nsingular value decomposition of videos, Video-DPRP reconstructs DP videos using\nprivacy parameters ($\\epsilon,\\delta$) while enabling visual privacy\nassessment. For goal (2), using UCF101 and HMDB51 datasets, we compare\nVideo-DPRP's performance on activity recognition with traditional DP methods,\nand state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,\nwe assess its effectiveness in preserving privacy-related attributes such as\nfacial features, gender, and skin color, using the PA-HMDB and VISPR datasets.\nVideo-DPRP combines privacy-preservation from both a DP and visual privacy\nperspective unlike SOTA methods that typically address only one of these\naspects.",
    "updated" : "2025-03-03T23:43:12Z",
    "published" : "2025-03-03T23:43:12Z",
    "authors" : [
      {
        "name" : "Allassan Tchangmena A Nken"
      },
      {
        "name" : "Susan Mckeever"
      },
      {
        "name" : "Peter Corcoran"
      },
      {
        "name" : "Ihsan Ullah"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02114v1",
    "title" : "Fairness and/or Privacy on Social Graphs",
    "summary" : "Graph Neural Networks (GNNs) have shown remarkable success in various\ngraph-based learning tasks. However, recent studies have raised concerns about\nfairness and privacy issues in GNNs, highlighting the potential for biased or\ndiscriminatory outcomes and the vulnerability of sensitive information. This\npaper presents a comprehensive investigation of fairness and privacy in GNNs,\nexploring the impact of various fairness-preserving measures on model\nperformance. We conduct experiments across diverse datasets and evaluate the\neffectiveness of different fairness interventions. Our analysis considers the\ntrade-offs between fairness, privacy, and accuracy, providing insights into the\nchallenges and opportunities in achieving both fair and private graph learning.\nThe results highlight the importance of carefully selecting and combining\nfairness-preserving measures based on the specific characteristics of the data\nand the desired fairness objectives. This study contributes to a deeper\nunderstanding of the complex interplay between fairness, privacy, and accuracy\nin GNNs, paving the way for the development of more robust and ethical graph\nlearning models.",
    "updated" : "2025-03-03T22:56:32Z",
    "published" : "2025-03-03T22:56:32Z",
    "authors" : [
      {
        "name" : "Bartlomiej Surma"
      },
      {
        "name" : "Michael Backes"
      },
      {
        "name" : "Yang Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02091v1",
    "title" : "Which Code Statements Implement Privacy Behaviors in Android\n  Applications?",
    "summary" : "A \"privacy behavior\" in software is an action where the software uses\npersonal information for a service or a feature, such as a website using\nlocation to provide content relevant to a user. Programmers are required by\nregulations or application stores to provide privacy notices and labels\ndescribing these privacy behaviors. Although many tools and research prototypes\nhave been developed to help programmers generate these notices by analyzing the\nsource code, these approaches are often fairly coarse-grained (i.e., at the\nlevel of whole methods or files, rather than at the statement level). But this\nis not necessarily how privacy behaviors exist in code. Privacy behaviors are\nembedded in specific statements in code. Current literature does not examine\nwhat statements programmers see as most important, how consistent these views\nare, or how to detect them. In this paper, we conduct an empirical study to\nexamine which statements programmers view as most-related to privacy behaviors.\nWe find that expression statements that make function calls are most associated\nwith privacy behaviors, while the type of privacy label has little effect on\nthe attributes of the selected statements. We then propose an approach to\nautomatically detect these privacy-relevant statements by fine-tuning three\nlarge language models with the data from the study. We observe that the\nagreement between our approach and participants is comparable to or higher than\nan agreement between two participants. Our study and detection approach can\nhelp programmers understand which statements in code affect privacy in mobile\napplications.",
    "updated" : "2025-03-03T22:20:01Z",
    "published" : "2025-03-03T22:20:01Z",
    "authors" : [
      {
        "name" : "Chia-Yi Su"
      },
      {
        "name" : "Aakash Bansal"
      },
      {
        "name" : "Vijayanta Jain"
      },
      {
        "name" : "Sepideh Ghanavati"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Collin McMillan"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02019v1",
    "title" : "SLAP: Secure Location-proof and Anonymous Privacy-preserving Spectrum\n  Access",
    "summary" : "The rapid advancements in wireless technology have significantly increased\nthe demand for communication resources, leading to the development of Spectrum\nAccess Systems (SAS). However, network regulations require disclosing sensitive\nuser information, such as location coordinates and transmission details,\nraising critical privacy concerns. Moreover, as a database-driven architecture\nreliant on user-provided data, SAS necessitates robust location verification to\ncounter identity and location spoofing attacks and remains a primary target for\ndenial-of-service (DoS) attacks. Addressing these security challenges while\nadhering to regulatory requirements is essential. In this paper, we propose\nSLAP, a novel framework that ensures location privacy and anonymity during\nspectrum queries, usage notifications, and location-proof acquisition. Our\nsolution includes an adaptive dual-scenario location verification mechanism\nwith architectural flexibility and a fallback option, along with a counter-DoS\napproach using time-lock puzzles. We prove the security of SLAP and demonstrate\nits advantages over existing solutions through comprehensive performance\nevaluations.",
    "updated" : "2025-03-03T19:52:56Z",
    "published" : "2025-03-03T19:52:56Z",
    "authors" : [
      {
        "name" : "Saleh Darzi"
      },
      {
        "name" : "Attila A. Yavuz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02017v1",
    "title" : "A Lightweight and Secure Deep Learning Model for Privacy-Preserving\n  Federated Learning in Intelligent Enterprises",
    "summary" : "The ever growing Internet of Things (IoT) connections drive a new type of\norganization, the Intelligent Enterprise. In intelligent enterprises, machine\nlearning based models are adopted to extract insights from data. Due to the\nefficiency and privacy challenges of these traditional models, a new federated\nlearning (FL) paradigm has emerged. In FL, multiple enterprises can jointly\ntrain a model to update a final model. However, firstly, FL trained models\nusually perform worse than centralized models, especially when enterprises\ntraining data is non-IID (Independent and Identically Distributed). Second, due\nto the centrality of FL and the untrustworthiness of local enterprises,\ntraditional FL solutions are vulnerable to poisoning and inference attacks and\nviolate privacy. Thirdly, the continuous transfer of parameters between\nenterprises and servers increases communication costs. To this end, the\nFedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep\nLearning Model that includes three main phases. In the first phase, the goal is\nto solve the data type distribution skew challenge. Addressing privacy concerns\nagainst poisoning and inference attacks is covered in the second phase.\nFinally, to alleviate the communication overhead, a novel compression approach\nis proposed that significantly reduces the size of the updates. The experiment\nresults validate that FedAnil+ is secure against inference and poisoning\nattacks with better accuracy. In addition, it shows improvements over existing\napproaches in terms of model accuracy (13%, 16%, and 26%), communication cost\n(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%).",
    "updated" : "2025-03-03T19:51:13Z",
    "published" : "2025-03-03T19:51:13Z",
    "authors" : [
      {
        "name" : "Reza Fotohi"
      },
      {
        "name" : "Fereidoon Shams Aliee"
      },
      {
        "name" : "Bahar Farahani"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01482v1",
    "title" : "Revisiting Locally Differentially Private Protocols: Towards Better\n  Trade-offs in Privacy, Utility, and Attack Resistance",
    "summary" : "Local Differential Privacy (LDP) offers strong privacy protection, especially\nin settings in which the server collecting the data is untrusted. However,\ndesigning LDP mechanisms that achieve an optimal trade-off between privacy,\nutility, and robustness to adversarial inference attacks remains challenging.\nIn this work, we introduce a general multi-objective optimization framework for\nrefining LDP protocols, enabling the joint optimization of privacy and utility\nunder various adversarial settings. While our framework is flexible enough to\naccommodate multiple privacy and security attacks as well as utility metrics,\nin this paper we specifically optimize for Attacker Success Rate (ASR) under\ndistinguishability attack as a measure of privacy and Mean Squared Error (MSE)\nas a measure of utility. We systematically revisit these trade-offs by\nanalyzing eight state-of-the-art LDP protocols and proposing refined\ncounterparts that leverage tailored optimization techniques. Experimental\nresults demonstrate that our proposed adaptive mechanisms consistently\noutperform their non-adaptive counterparts, reducing ASR by up to five orders\nof magnitude while maintaining competitive utility. Analytical derivations also\nconfirm the effectiveness of our mechanisms, moving them closer to the ASR-MSE\nPareto frontier.",
    "updated" : "2025-03-03T12:41:01Z",
    "published" : "2025-03-03T12:41:01Z",
    "authors" : [
      {
        "name" : "Héber H. Arcolezi"
      },
      {
        "name" : "Sébastien Gambs"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01470v1",
    "title" : "Position: Ensuring mutual privacy is necessary for effective external\n  evaluation of proprietary AI systems",
    "summary" : "The external evaluation of AI systems is increasingly recognised as a crucial\napproach for understanding their potential risks. However, facilitating\nexternal evaluation in practice faces significant challenges in balancing\nevaluators' need for system access with AI developers' privacy and security\nconcerns. Additionally, evaluators have reason to protect their own privacy -\nfor example, in order to maintain the integrity of held-out test sets. We refer\nto the challenge of ensuring both developers' and evaluators' privacy as one of\nproviding mutual privacy. In this position paper, we argue that (i) addressing\nthis mutual privacy challenge is essential for effective external evaluation of\nAI systems, and (ii) current methods for facilitating external evaluation\ninadequately address this challenge, particularly when it comes to preserving\nevaluators' privacy. In making these arguments, we formalise the mutual privacy\nproblem; examine the privacy and access requirements of both model owners and\nevaluators; and explore potential solutions to this challenge, including\nthrough the application of cryptographic and hardware-based approaches.",
    "updated" : "2025-03-03T12:24:59Z",
    "published" : "2025-03-03T12:24:59Z",
    "authors" : [
      {
        "name" : "Ben Bucknall"
      },
      {
        "name" : "Robert F. Trager"
      },
      {
        "name" : "Michael A. Osborne"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01208v1",
    "title" : "Watch Out Your Album! On the Inadvertent Privacy Memorization in\n  Multi-Modal Large Language Models",
    "summary" : "Multi-Modal Large Language Models (MLLMs) have exhibited remarkable\nperformance on various vision-language tasks such as Visual Question Answering\n(VQA). Despite accumulating evidence of privacy concerns associated with\ntask-relevant content, it remains unclear whether MLLMs inadvertently memorize\nprivate content that is entirely irrelevant to the training tasks. In this\npaper, we investigate how randomly generated task-irrelevant private content\ncan become spuriously correlated with downstream objectives due to partial\nmini-batch training dynamics, thus causing inadvertent memorization.\nConcretely, we randomly generate task-irrelevant watermarks into VQA\nfine-tuning images at varying probabilities and propose a novel probing\nframework to determine whether MLLMs have inadvertently encoded such content.\nOur experiments reveal that MLLMs exhibit notably different training behaviors\nin partial mini-batch settings with task-irrelevant watermarks embedded.\nFurthermore, through layer-wise probing, we demonstrate that MLLMs trigger\ndistinct representational patterns when encountering previously seen\ntask-irrelevant knowledge, even if this knowledge does not influence their\noutput during prompting. Our code is available at\nhttps://github.com/illusionhi/ProbingPrivacy.",
    "updated" : "2025-03-03T06:10:27Z",
    "published" : "2025-03-03T06:10:27Z",
    "authors" : [
      {
        "name" : "Tianjie Ju"
      },
      {
        "name" : "Yi Hua"
      },
      {
        "name" : "Hao Fei"
      },
      {
        "name" : "Zhenyu Shao"
      },
      {
        "name" : "Yubin Zheng"
      },
      {
        "name" : "Haodong Zhao"
      },
      {
        "name" : "Mong-Li Lee"
      },
      {
        "name" : "Wynne Hsu"
      },
      {
        "name" : "Zhuosheng Zhang"
      },
      {
        "name" : "Gongshen Liu"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01089v1",
    "title" : "Privacy-preserving Machine Learning in Internet of Vehicle Applications:\n  Fundamentals, Recent Advances, and Future Direction",
    "summary" : "Machine learning (ML) has revolutionized Internet of Vehicles (IoV)\napplications by enhancing intelligent transportation, autonomous driving\ncapabilities, and various connected services within a large, heterogeneous\nnetwork. However, the increased connectivity and massive data exchange for ML\napplications introduce significant privacy challenges. Privacy-preserving\nmachine learning (PPML) offers potential solutions to address these challenges\nby preserving privacy at various stages of the ML pipeline. Despite the rapid\ndevelopment of ML-based IoV applications and the growing data privacy concerns,\nthere are limited comprehensive studies on the adoption of PPML within this\ndomain. Therefore, this study provides a comprehensive review of the\nfundamentals, recent advancements, and the challenges of integrating PPML into\nIoV applications. To conduct an extensive study, we first review existing\nsurveys of various PPML techniques and their integration into IoV across\ndifferent scopes. We then discuss the fundamentals of IoV and propose a\nfour-layer IoV architecture. Additionally, we categorize IoV applications into\nthree key domains and analyze the privacy challenges in leveraging ML for these\napplication domains. Next, we provide an overview of various PPML techniques,\nhighlighting their applicability and performance to address the privacy\nchallenges. Building on these fundamentals, we thoroughly review recent\nadvancements in integrating various PPML techniques within IoV applications,\ndiscussing their frameworks, key features, and performance evaluation in terms\nof privacy, utility, and efficiency. Finally, we identify current challenges\nand propose future research directions to enhance privacy and reliability in\nIoV applications.",
    "updated" : "2025-03-03T01:24:04Z",
    "published" : "2025-03-03T01:24:04Z",
    "authors" : [
      {
        "name" : "Nazmul Islam"
      },
      {
        "name" : "Mohammad Zulkernine"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01000v1",
    "title" : "Privacy vs. Profit: The Impact of Google's Manifest Version 3 (MV3)\n  Update on Ad Blocker Effectiveness",
    "summary" : "Google's recent update to the manifest file for Chrome browser\nextensions-transitioning from manifest version 2 (MV2) to manifest version 3\n(MV3)-has raised concerns among users and ad blocker providers, who worry that\nthe new restrictions, notably the shift from the powerful WebRequest API to the\nmore restrictive DeclarativeNetRequest API, might reduce ad blocker\neffectiveness. Because ad blockers play a vital role for millions of users\nseeking a more private and ad-free browsing experience, this study empirically\ninvestigates how the MV3 update affects their ability to block ads and\ntrackers. Through a browser-based experiment conducted across multiple samples\nof ad-supported websites, we compare the MV3 to MV2 instances of four widely\nused ad blockers. Our results reveal no statistically significant reduction in\nad-blocking or anti-tracking effectiveness for MV3 ad blockers compared to\ntheir MV2 counterparts, and in some cases, MV3 instances even exhibit slight\nimprovements in blocking trackers. These findings are reassuring for users,\nindicating that the MV3 instances of popular ad blockers continue to provide\neffective protection against intrusive ads and privacy-infringing trackers.\nWhile some uncertainties remain, ad blocker providers appear to have\nsuccessfully navigated the MV3 update, finding solutions that maintain the core\nfunctionality of their ad blockers.",
    "updated" : "2025-03-02T19:41:34Z",
    "published" : "2025-03-02T19:41:34Z",
    "authors" : [
      {
        "name" : "Karlo Lukic"
      },
      {
        "name" : "Lazaros Papadopoulos"
      }
    ],
    "categories" : [
      "cs.CY",
      "econ.GN",
      "q-fin.EC",
      "K.4.0, K.6.5, H.5.2",
      "K.4.0; K.6.5; H.5.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.00703v1",
    "title" : "Towards hyperparameter-free optimization with differential privacy",
    "summary" : "Differential privacy (DP) is a privacy-preserving paradigm that protects the\ntraining data when training deep learning models. Critically, the performance\nof models is determined by the training hyperparameters, especially those of\nthe learning rate schedule, thus requiring fine-grained hyperparameter tuning\non the data. In practice, it is common to tune the learning rate\nhyperparameters through the grid search that (1) is computationally expensive\nas multiple runs are needed, and (2) increases the risk of data leakage as the\nselection of hyperparameters is data-dependent. In this work, we adapt the\nautomatic learning rate schedule to DP optimization for any models and\noptimizers, so as to significantly mitigate or even eliminate the cost of\nhyperparameter tuning when applied together with automatic per-sample gradient\nclipping. Our hyperparameter-free DP optimization is almost as computationally\nefficient as the standard non-DP optimization, and achieves state-of-the-art DP\nperformance on various language and vision tasks.",
    "updated" : "2025-03-02T02:59:52Z",
    "published" : "2025-03-02T02:59:52Z",
    "authors" : [
      {
        "name" : "Zhiqi Bu"
      },
      {
        "name" : "Ruixuan Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03652v1",
    "title" : "Token-Level Privacy in Large Language Models",
    "summary" : "The use of language models as remote services requires transmitting private\ninformation to external providers, raising significant privacy concerns. This\nprocess not only risks exposing sensitive data to untrusted service providers\nbut also leaves it vulnerable to interception by eavesdroppers. Existing\nprivacy-preserving methods for natural language processing (NLP) interactions\nprimarily rely on semantic similarity, overlooking the role of contextual\ninformation. In this work, we introduce dchi-stencil, a novel token-level\nprivacy-preserving mechanism that integrates contextual and semantic\ninformation while ensuring strong privacy guarantees under the dchi\ndifferential privacy framework, achieving 2epsilon-dchi-privacy. By\nincorporating both semantic and contextual nuances, dchi-stencil achieves a\nrobust balance between privacy and utility. We evaluate dchi-stencil using\nstate-of-the-art language models and diverse datasets, achieving comparable and\neven better trade-off between utility and privacy compared to existing methods.\nThis work highlights the potential of dchi-stencil to set a new standard for\nprivacy-preserving NLP in modern, high-risk applications.",
    "updated" : "2025-03-05T16:27:25Z",
    "published" : "2025-03-05T16:27:25Z",
    "authors" : [
      {
        "name" : "Re'em Harel"
      },
      {
        "name" : "Niv Gilboa"
      },
      {
        "name" : "Yuval Pinter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03587v1",
    "title" : "\"You don't need a university degree to comprehend data protection this\n  way\": LLM-Powered Interactive Privacy Policy Assessment",
    "summary" : "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present the first qualitative user study on Large Language Model\n(LLM)-driven privacy policy assessment. To this end, we build and evaluate an\nLLM-based privacy policy assessment browser extension, which helps users\nunderstand the essence of a lengthy, complex privacy policy while browsing. The\ntool integrates a dashboard and an LLM chat. In our qualitative user study\n(N=22), we evaluate usability, understandability of the information our tool\nprovides, and its impacts on awareness. While providing a comprehensible quick\noverview and a chat for in-depth discussion improves privacy awareness, users\nnote issues with building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
    "updated" : "2025-03-05T15:22:35Z",
    "published" : "2025-03-05T15:22:35Z",
    "authors" : [
      {
        "name" : "Vincent Freiberger"
      },
      {
        "name" : "Arthur Fleig"
      },
      {
        "name" : "Erik Buchmann"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03539v1",
    "title" : "Data Sharing, Privacy and Security Considerations in the Energy Sector:\n  A Review from Technical Landscape to Regulatory Specifications",
    "summary" : "Decarbonization, decentralization and digitalization are the three key\nelements driving the twin energy transition. The energy system is evolving to a\nmore data driven ecosystem, leading to the need of communication and storage of\nlarge amount of data of different resolution from the prosumers and other\nstakeholders in the energy ecosystem. While the energy system is certainly\nadvancing, this paradigm shift is bringing in new privacy and security issues\nrelated to collection, processing and storage of data - not only from the\ntechnical dimension, but also from the regulatory perspective. Understanding\ndata privacy and security in the evolving energy system, regarding regulatory\ncompliance, is an immature field of research. Contextualized knowledge of how\nrelated issues are regulated is still in its infancy, and the practical and\ntechnical basis for the regulatory framework for data privacy and security is\nnot clear. To fill this gap, this paper conducts a comprehensive review of the\ndata-related issues for the energy system by integrating both technical and\nregulatory dimensions. We start by reviewing open-access data, data\ncommunication and data-processing techniques for the energy system, and use it\nas the basis to connect the analysis of data-related issues from the integrated\nperspective. We classify the issues into three categories: (i) data-sharing\namong energy end users and stakeholders (ii) privacy of end users, and (iii)\ncyber security, and then explore these issues from a regulatory perspective. We\nanalyze the evolution of related regulations, and introduce the relevant\nregulatory initiatives for the categorized issues in terms of regulatory\ndefinitions, concepts, principles, rights and obligations in the context of\nenergy systems. Finally, we provide reflections on the gaps that still exist,\nand guidelines for regulatory frameworks for a truly participatory energy\nsystem.",
    "updated" : "2025-03-05T14:23:56Z",
    "published" : "2025-03-05T14:23:56Z",
    "authors" : [
      {
        "name" : "Shiliang Zhang"
      },
      {
        "name" : "Sabita Maharjan"
      },
      {
        "name" : "Lee Andrew Bygrave"
      },
      {
        "name" : "Shui Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03506v1",
    "title" : "Rethinking Synthetic Data definitions: A privacy driven approach",
    "summary" : "Synthetic data is gaining traction as a cost-effective solution for the\nincreasing data demands of AI development and can be generated either from\nexisting knowledge or derived data captured from real-world events. The source\nof the synthetic data generation and the technique used significantly impacts\nits residual privacy risk and therefore its opportunity for sharing.\nTraditional classification of synthetic data types no longer fit the newer\ngeneration techniques and there is a need to better align the classification\nwith practical needs. We suggest a new way of grouping synthetic data types\nthat better supports privacy evaluations to aid regulatory policymaking. Our\nnovel classification provides flexibility to new advancements like deep\ngenerative methods and offers a more practical framework for future\napplications.",
    "updated" : "2025-03-05T13:54:13Z",
    "published" : "2025-03-05T13:54:13Z",
    "authors" : [
      {
        "name" : "Vibeke Binz Vallevik"
      },
      {
        "name" : "Serena Elizabeth Marshall"
      },
      {
        "name" : "Aleksandar Babic"
      },
      {
        "name" : "Jan Franz Nygaard"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03428v1",
    "title" : "Privacy is All You Need: Revolutionizing Wearable Health Data with\n  Advanced PETs",
    "summary" : "In a world where data is the new currency, wearable health devices offer\nunprecedented insights into daily life, continuously monitoring vital signs and\nmetrics. However, this convenience raises privacy concerns, as these devices\ncollect sensitive data that can be misused or breached. Traditional measures\noften fail due to real-time data processing needs and limited device power.\nUsers also lack awareness and control over data sharing and usage. We propose a\nPrivacy-Enhancing Technology (PET) framework for wearable devices, integrating\nfederated learning, lightweight cryptographic methods, and selectively deployed\nblockchain technology. The blockchain acts as a secure ledger triggered only\nupon data transfer requests, granting users real-time notifications and\ncontrol. By dismantling data monopolies, this approach returns data sovereignty\nto individuals. Through real-world applications like secure medical data\nsharing, privacy-preserving fitness tracking, and continuous health monitoring,\nour framework reduces privacy risks by up to 70 percent while preserving data\nutility and performance. This innovation sets a new benchmark for wearable\nprivacy and can scale to broader IoT ecosystems, including smart homes and\nindustry. As data continues to shape our digital landscape, our research\nunderscores the critical need to maintain privacy and user control at the\nforefront of technological progress.",
    "updated" : "2025-03-05T12:01:22Z",
    "published" : "2025-03-05T12:01:22Z",
    "authors" : [
      {
        "name" : "Karthik Barma"
      },
      {
        "name" : "Seshu Babu Barma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03267v1",
    "title" : "Quantum-Inspired Privacy-Preserving Federated Learning Framework for\n  Secure Dementia Classification",
    "summary" : "Dementia, a neurological disorder impacting millions globally, presents\nsignificant challenges in diagnosis and patient care. With the rise of privacy\nconcerns and security threats in healthcare, federated learning (FL) has\nemerged as a promising approach to enable collaborative model training across\ndecentralized datasets without exposing sensitive patient information. However,\nFL remains vulnerable to advanced security breaches such as gradient inversion\nand eavesdropping attacks. This paper introduces a novel framework that\nintegrates federated learning with quantum-inspired encryption techniques for\ndementia classification, emphasizing privacy preservation and security.\nLeveraging quantum key distribution (QKD), the framework ensures secure\ntransmission of model weights, protecting against unauthorized access and\ninterception during training. The methodology utilizes a convolutional neural\nnetwork (CNN) for dementia classification, with federated training conducted\nacross distributed healthcare nodes, incorporating QKD-encrypted weight sharing\nto secure the aggregation process. Experimental evaluations conducted on MRI\ndata from the OASIS dataset demonstrate that the proposed framework achieves\nidentical accuracy levels to a baseline model while enhancing data security and\nreducing loss by almost 1% compared to the classical baseline model. The\nframework offers significant implications for democratizing access to AI-driven\ndementia diagnostics in low- and middle-income countries, addressing critical\nresource and privacy constraints. This work contributes a robust, scalable, and\nsecure federated learning solution for healthcare applications, paving the way\nfor broader adoption of quantum-inspired techniques in AI-driven medical\nresearch.",
    "updated" : "2025-03-05T08:49:31Z",
    "published" : "2025-03-05T08:49:31Z",
    "authors" : [
      {
        "name" : "Gazi Tanbhir"
      },
      {
        "name" : "Md. Farhan Shahriyar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03146v1",
    "title" : "PriFFT: Privacy-preserving Federated Fine-tuning of Large Language\n  Models via Function Secret Sharing",
    "summary" : "Fine-tuning large language models (LLMs) raises privacy concerns due to the\nrisk of exposing sensitive training data. Federated learning (FL) mitigates\nthis risk by keeping training samples on local devices, but recent studies show\nthat adversaries can still infer private information from model updates in FL.\nAdditionally, LLM parameters are typically shared publicly during federated\nfine-tuning, while developers are often reluctant to disclose these parameters,\nposing further security challenges. Inspired by the above problems, we propose\nPriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both\nthe model updates and parameters. In PriFFT, clients and the server share model\ninputs and parameters by secret sharing, performing secure fine-tuning on\nshared values without accessing plaintext data. Due to considerable LLM\nparameters, privacy-preserving federated fine-tuning invokes complex secure\ncalculations and requires substantial communication and computation resources.\nTo optimize the efficiency of privacy-preserving federated fine-tuning of LLMs,\nwe introduce function secret-sharing protocols for various operations,\nincluding reciprocal calculation, tensor products, natural exponentiation,\nsoftmax, hyperbolic tangent, and dropout. The proposed protocols achieve up to\n4.02X speed improvement and reduce 7.19X communication overhead compared to the\nimplementation based on existing secret sharing methods. Besides, PriFFT\nachieves a 2.23X speed improvement and reduces 4.08X communication overhead in\nprivacy-preserving fine-tuning without accuracy drop compared to the existing\nsecret sharing methods.",
    "updated" : "2025-03-05T03:41:57Z",
    "published" : "2025-03-05T03:41:57Z",
    "authors" : [
      {
        "name" : "Zhichao You"
      },
      {
        "name" : "Xuewen Dong"
      },
      {
        "name" : "Ke Cheng"
      },
      {
        "name" : "Xutong Mu"
      },
      {
        "name" : "Jiaxuan Fu"
      },
      {
        "name" : "Shiyang Ma"
      },
      {
        "name" : "Qiang Qu"
      },
      {
        "name" : "Yulong Shen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03087v1",
    "title" : "\"Watch My Health, Not My Data\": Understanding Perceptions, Barriers,\n  Emotional Impact, & Coping Strategies Pertaining to IoT Privacy and Security\n  in Health Monitoring for Older Adults",
    "summary" : "The proliferation of \"Internet of Things (IoT)\" provides older adults with\ncritical support for \"health monitoring\" and independent living, yet\nsignificant concerns about security and privacy persist. In this paper, we\nreport on these issues through a two-phase user study, including a survey (N =\n22) and semi-structured interviews (n = 9) with adults aged 65+. We found that\nwhile 81.82% of our participants are aware of security features like\n\"two-factor authentication (2FA)\" and encryption, 63.64% express serious\nconcerns about unauthorized access to sensitive health data. Only 13.64% feel\nconfident in existing protections, citing confusion over \"data sharing\npolicies\" and frustration with \"complex security settings\" which lead to\ndistrust and anxiety. To cope, our participants adopt various strategies, such\nas relying on family or professional support and limiting feature usage leading\nto disengagement. Thus, we recommend \"adaptive security mechanisms,\" simplified\ninterfaces, and real-time transparency notifications to foster trust and ensure\n\"privacy and security by design\" in IoT health systems for older adults.",
    "updated" : "2025-03-05T01:04:13Z",
    "published" : "2025-03-05T01:04:13Z",
    "authors" : [
      {
        "name" : "Suleiman Saka"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03043v1",
    "title" : "Leveraging Randomness in Model and Data Partitioning for Privacy\n  Amplification",
    "summary" : "We study how inherent randomness in the training process -- where each sample\n(or client in federated learning) contributes only to a randomly selected\nportion of training -- can be leveraged for privacy amplification. This\nincludes (1) data partitioning, where a sample participates in only a subset of\ntraining iterations, and (2) model partitioning, where a sample updates only a\nsubset of the model parameters. We apply our framework to model parallelism in\nfederated learning, where each client updates a randomly selected subnetwork to\nreduce memory and computational overhead, and show that existing methods, e.g.\nmodel splitting or dropout, provide a significant privacy amplification gain\nnot captured by previous privacy analysis techniques. Additionally, we\nintroduce Balanced Iteration Subsampling, a new data partitioning method where\neach sample (or client) participates in a fixed number of training iterations.\nWe show that this method yields stronger privacy amplification than Poisson\n(i.i.d.) sampling of data (or clients). Our results demonstrate that randomness\nin the training process, which is structured rather than i.i.d. and interacts\nwith data in complex ways, can be systematically leveraged for significant\nprivacy amplification.",
    "updated" : "2025-03-04T22:49:59Z",
    "published" : "2025-03-04T22:49:59Z",
    "authors" : [
      {
        "name" : "Andy Dong"
      },
      {
        "name" : "Wei-Ning Chen"
      },
      {
        "name" : "Ayfer Ozgur"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02968v1",
    "title" : "Privacy-Preserving Fair Synthetic Tabular Data",
    "summary" : "Sharing of tabular data containing valuable but private information is\nlimited due to legal and ethical issues. Synthetic data could be an alternative\nsolution to this sharing problem, as it is artificially generated by machine\nlearning algorithms and tries to capture the underlying data distribution.\nHowever, machine learning models are not free from memorization and may\nintroduce biases, as they rely on training data. Producing synthetic data that\npreserves privacy and fairness while maintaining utility close to the real data\nis a challenging task. This research simultaneously addresses both the privacy\nand fairness aspects of synthetic data, an area not explored by other studies.\nIn this work, we present PF-WGAN, a privacy-preserving, fair synthetic tabular\ndata generator based on the WGAN-GP model. We have modified the original\nWGAN-GP by adding privacy and fairness constraints forcing it to produce\nprivacy-preserving fair data. This approach will enable the publication of\ndatasets that protect individual's privacy and remain unbiased toward any\nparticular group. We compared the results with three state-of-the-art synthetic\ndata generator models in terms of utility, privacy, and fairness across four\ndifferent datasets. We found that the proposed model exhibits a more balanced\ntrade-off among utility, privacy, and fairness.",
    "updated" : "2025-03-04T19:51:00Z",
    "published" : "2025-03-04T19:51:00Z",
    "authors" : [
      {
        "name" : "Fatima J. Sarmin"
      },
      {
        "name" : "Atiquer R. Rahman"
      },
      {
        "name" : "Christopher J. Henry"
      },
      {
        "name" : "Noman Mohammed"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04707v1",
    "title" : "Iris Style Transfer: Enhancing Iris Recognition with Style Features and\n  Privacy Preservation through Neural Style Transfer",
    "summary" : "Iris texture is widely regarded as a gold standard biometric modality for\nauthentication and identification. The demand for robust iris recognition\nmethods, coupled with growing security and privacy concerns regarding iris\nattacks, has escalated recently. Inspired by neural style transfer, an advanced\ntechnique that leverages neural networks to separate content and style\nfeatures, we hypothesize that iris texture's style features provide a reliable\nfoundation for recognition and are more resilient to variations like rotation\nand perspective shifts than traditional approaches. Our experimental results\nsupport this hypothesis, showing a significantly higher classification accuracy\ncompared to conventional features. Further, we propose using neural style\ntransfer to mask identifiable iris style features, ensuring the protection of\nsensitive biometric information while maintaining the utility of eye images for\ntasks like eye segmentation and gaze estimation. This work opens new avenues\nfor iris-oriented, secure, and privacy-aware biometric systems.",
    "updated" : "2025-03-06T18:55:21Z",
    "published" : "2025-03-06T18:55:21Z",
    "authors" : [
      {
        "name" : "Mengdi Wang"
      },
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Enkelejda Kasneci"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04652v1",
    "title" : "Evaluation of Privacy-aware Support Vector Machine (SVM) Learning using\n  Homomorphic Encryption",
    "summary" : "The requirement for privacy-aware machine learning increases as we continue\nto use PII (Personally Identifiable Information) within machine training. To\novercome these privacy issues, we can apply Fully Homomorphic Encryption (FHE)\nto encrypt data before it is fed into a machine learning model. This involves\ncreating a homomorphic encryption key pair, and where the associated public key\nwill be used to encrypt the input data, and the private key will decrypt the\noutput. But, there is often a performance hit when we use homomorphic\nencryption, and so this paper evaluates the performance overhead of using the\nSVM machine learning technique with the OpenFHE homomorphic encryption library.\nThis uses Python and the scikit-learn library for its implementation. The\nexperiments include a range of variables such as multiplication depth, scale\nsize, first modulus size, security level, batch size, and ring dimension, along\nwith two different SVM models, SVM-Poly and SVM-Linear. Overall, the results\nshow that the two main parameters which affect performance are the ring\ndimension and the modulus size, and that SVM-Poly and SVM-Linear show similar\nperformance levels.",
    "updated" : "2025-03-06T17:42:23Z",
    "published" : "2025-03-06T17:42:23Z",
    "authors" : [
      {
        "name" : "William J Buchanan"
      },
      {
        "name" : "Hisham Ali"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04451v1",
    "title" : "Privacy Preserving and Robust Aggregation for Cross-Silo Federated\n  Learning in Non-IID Settings",
    "summary" : "Federated Averaging remains the most widely used aggregation strategy in\nfederated learning due to its simplicity and scalability. However, its\nperformance degrades significantly in non-IID data settings, where client\ndistributions are highly imbalanced or skewed. Additionally, it relies on\nclients transmitting metadata, specifically the number of training samples,\nwhich introduces privacy risks and may conflict with regulatory frameworks like\nthe European GDPR. In this paper, we propose a novel aggregation strategy that\naddresses these challenges by introducing class-aware gradient masking. Unlike\ntraditional approaches, our method relies solely on gradient updates,\neliminating the need for any additional client metadata, thereby enhancing\nprivacy protection. Furthermore, our approach validates and dynamically weights\nclient contributions based on class-specific importance, ensuring robustness\nagainst non-IID distributions, convergence prevention, and backdoor attacks.\nExtensive experiments on benchmark datasets demonstrate that our method not\nonly outperforms FedAvg and other widely accepted aggregation strategies in\nnon-IID settings but also preserves model integrity in adversarial scenarios.\nOur results establish the effectiveness of gradient masking as a practical and\nsecure solution for federated learning.",
    "updated" : "2025-03-06T14:06:20Z",
    "published" : "2025-03-06T14:06:20Z",
    "authors" : [
      {
        "name" : "Marco Arazzi"
      },
      {
        "name" : "Mert Cihangiroglu"
      },
      {
        "name" : "Antonino Nocera"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04054v1",
    "title" : "Controlled privacy leakage propagation throughout overlapping grouped\n  learning",
    "summary" : "Federated Learning (FL) is the standard protocol for collaborative learning.\nIn FL, multiple workers jointly train a shared model. They exchange model\nupdates calculated on their data, while keeping the raw data itself local.\nSince workers naturally form groups based on common interests and privacy\npolicies, we are motivated to extend standard FL to reflect a setting with\nmultiple, potentially overlapping groups. In this setup where workers can\nbelong and contribute to more than one group at a time, complexities arise in\nunderstanding privacy leakage and in adhering to privacy policies. To address\nthe challenges, we propose differential private overlapping grouped learning\n(DPOGL), a novel method to implement privacy guarantees within overlapping\ngroups. Under the honest-but-curious threat model, we derive novel privacy\nguarantees between arbitrary pairs of workers. These privacy guarantees\ndescribe and quantify two key effects of privacy leakage in DP-OGL: propagation\ndelay, i.e., the fact that information from one group will leak to other groups\nonly with temporal offset through the common workers and information\ndegradation, i.e., the fact that noise addition over model updates limits\ninformation leakage between workers. Our experiments show that applying DP-OGL\nenhances utility while maintaining strong privacy compared to standard FL\nsetups.",
    "updated" : "2025-03-06T03:14:45Z",
    "published" : "2025-03-06T03:14:45Z",
    "authors" : [
      {
        "name" : "Shahrzad Kiani"
      },
      {
        "name" : "Franziska Boenisch"
      },
      {
        "name" : "Stark C. Draper"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03988v1",
    "title" : "AI-based Programming Assistants for Privacy-related Code Generation: The\n  Developers' Experience",
    "summary" : "With the popularising of generative AI, the existence of AI-based programming\nassistants for developers is no surprise. Developers increasingly use them for\ntheir work, including generating code to fulfil the data protection\nrequirements (privacy) of the apps they build. We wanted to know if the reality\nis the same as expectations of AI-based programming assistants when trying to\nfulfil software privacy requirements, and the challenges developers face when\nusing AI-based programming assistants and how these can be improved. To this\nend, we conducted a survey with 51 developers worldwide. We found that AI-based\nprogramming assistants need to be improved in order for developers to better\ntrust them with generating code that ensures privacy. In this paper, we provide\nsome practical recommendations for developers to consider following when using\nAI-based programming assistants for privacy-related code development, and some\nkey further research directions.",
    "updated" : "2025-03-06T00:34:25Z",
    "published" : "2025-03-06T00:34:25Z",
    "authors" : [
      {
        "name" : "Kashumi Madampe"
      },
      {
        "name" : "John Grundy"
      },
      {
        "name" : "Nalin Arachchilage"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.05684v1",
    "title" : "Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints",
    "summary" : "Pre-trained foundation models can be adapted for specific tasks using\nLow-Rank Adaptation (LoRA). However, the fairness properties of these adapted\nclassifiers remain underexplored. Existing fairness-aware fine-tuning methods\nrely on direct access to sensitive attributes or their predictors, but in\npractice, these sensitive attributes are often held under strict consumer\nprivacy controls, and neither the attributes nor their predictors are available\nto model developers, hampering the development of fair models. To address this\nissue, we introduce a set of LoRA-based fine-tuning methods that can be trained\nin a distributed fashion, where model developers and fairness auditors\ncollaborate without sharing sensitive attributes or predictors. In this paper,\nwe evaluate three such methods - sensitive unlearning, adversarial training,\nand orthogonality loss - against a fairness-unaware baseline, using experiments\non the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Base\nmodel. We find that orthogonality loss consistently reduces bias while\nmaintaining or improving utility, whereas adversarial training improves False\nPositive Rate Parity and Demographic Parity in some cases, and sensitive\nunlearning provides no clear benefit. In tasks where significant biases are\npresent, distributed fairness-aware fine-tuning methods can effectively\neliminate bias without compromising consumer privacy and, in most cases,\nimprove model utility.",
    "updated" : "2025-03-07T18:49:57Z",
    "published" : "2025-03-07T18:49:57Z",
    "authors" : [
      {
        "name" : "Parameswaran Kamalaruban"
      },
      {
        "name" : "Mark Anderson"
      },
      {
        "name" : "Stuart Burrell"
      },
      {
        "name" : "Maeve Madigan"
      },
      {
        "name" : "Piotr Skalski"
      },
      {
        "name" : "David Sutton"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04980v1",
    "title" : "A Consensus Privacy Metrics Framework for Synthetic Data",
    "summary" : "Synthetic data generation is one approach for sharing individual-level data.\nHowever, to meet legislative requirements, it is necessary to demonstrate that\nthe individuals' privacy is adequately protected. There is no consolidated\nstandard for measuring privacy in synthetic data. Through an expert panel and\nconsensus process, we developed a framework for evaluating privacy in synthetic\ndata. Our findings indicate that current similarity metrics fail to measure\nidentity disclosure, and their use is discouraged. For differentially private\nsynthetic data, a privacy budget other than close to zero was not considered\ninterpretable. There was consensus on the importance of membership and\nattribute disclosure, both of which involve inferring personal information\nabout an individual without necessarily revealing their identity. The resultant\nframework provides precise recommendations for metrics that address these types\nof disclosures effectively. Our findings further present specific opportunities\nfor future research that can help with widespread adoption of synthetic data.",
    "updated" : "2025-03-06T21:19:02Z",
    "published" : "2025-03-06T21:19:02Z",
    "authors" : [
      {
        "name" : "Lisa Pilgram"
      },
      {
        "name" : "Fida K. Dankar"
      },
      {
        "name" : "Jorg Drechsler"
      },
      {
        "name" : "Mark Elliot"
      },
      {
        "name" : "Josep Domingo-Ferrer"
      },
      {
        "name" : "Paul Francis"
      },
      {
        "name" : "Murat Kantarcioglu"
      },
      {
        "name" : "Linglong Kong"
      },
      {
        "name" : "Bradley Malin"
      },
      {
        "name" : "Krishnamurty Muralidhar"
      },
      {
        "name" : "Puja Myles"
      },
      {
        "name" : "Fabian Prasser"
      },
      {
        "name" : "Jean Louis Raisaro"
      },
      {
        "name" : "Chao Yan"
      },
      {
        "name" : "Khaled El Emam"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04866v1",
    "title" : "Privacy in Responsible AI: Approaches to Facial Recognition from Cloud\n  Providers",
    "summary" : "As the use of facial recognition technology is expanding in different\ndomains, ensuring its responsible use is gaining more importance. This paper\nconducts a comprehensive literature review of existing studies on facial\nrecognition technology from the perspective of privacy, which is one of the key\nResponsible AI principles.\n  Cloud providers, such as Microsoft, AWS, and Google, are at the forefront of\ndelivering facial-related technology services, but their approaches to\nresponsible use of these technologies vary significantly. This paper compares\nhow these cloud giants implement the privacy principle into their facial\nrecognition and detection services. By analysing their approaches, it\nidentifies both common practices and notable differences. The results of this\nresearch will be valuable for developers and businesses by providing them\ninsights into best practices of three major companies for integration\nresponsible AI, particularly privacy, into their cloud-based facial recognition\ntechnologies.",
    "updated" : "2025-03-06T12:04:12Z",
    "published" : "2025-03-06T12:04:12Z",
    "authors" : [
      {
        "name" : "Anna Elivanova"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07427v1",
    "title" : "Creating and Evaluating Privacy and Security Micro-Lessons for\n  Elementary School Children",
    "summary" : "The growing use of technology in K--8 classrooms highlights a parallel need\nfor formal learning opportunities aimed at helping children use technology\nsafely and protect their personal information. Even the youngest students are\nnow using tablets, laptops, and apps to support their learning; however, there\nare limited curricular materials available for elementary and middle school\nchildren on digital privacy and security topics. To bridge this gap, we\ndeveloped a series of micro-lessons to help K--8 children learn about digital\nprivacy and security at school. We first conducted a formative study by\ninterviewing elementary school teachers to identify the design needs for\ndigital privacy and security lessons. We then developed micro-lessons --\nmultiple 15-20 minute activities designed to be easily inserted into the\nexisting curriculum -- using a co-design approach with multiple rounds of\ndeveloping and revising the micro-lessons in collaboration with teachers.\nThroughout the process, we conducted evaluation sessions where teachers\nimplemented or reviewed the micro-lessons. Our study identifies strengths,\nchallenges, and teachers' tailoring strategies when incorporating micro-lessons\nfor K--8 digital privacy and security topics, providing design implications for\nfacilitating learning about these topics in school classrooms.",
    "updated" : "2025-03-10T15:12:11Z",
    "published" : "2025-03-10T15:12:11Z",
    "authors" : [
      {
        "name" : "Lan Gao"
      },
      {
        "name" : "Elana B Blinder"
      },
      {
        "name" : "Abigail Barnes"
      },
      {
        "name" : "Kevin Song"
      },
      {
        "name" : "Tamara Clegg"
      },
      {
        "name" : "Jessica Vitak"
      },
      {
        "name" : "Marshini Chetty"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07216v1",
    "title" : "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
    "summary" : "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
    "updated" : "2025-03-10T11:55:50Z",
    "published" : "2025-03-10T11:55:50Z",
    "authors" : [
      {
        "name" : "Sangwoo Park"
      },
      {
        "name" : "Seanie Lee"
      },
      {
        "name" : "Byungjoo Kim"
      },
      {
        "name" : "Sung Ju Hwang"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07199v1",
    "title" : "How Well Can Differential Privacy Be Audited in One Run?",
    "summary" : "Recent methods for auditing the privacy of machine learning algorithms have\nimproved computational efficiency by simultaneously intervening on multiple\ntraining examples in a single training run. Steinke et al. (2024) prove that\none-run auditing indeed lower bounds the true privacy parameter of the audited\nalgorithm, and give impressive empirical results. Their work leaves open the\nquestion of how precisely one-run auditing can uncover the true privacy\nparameter of an algorithm, and how that precision depends on the audited\nalgorithm. In this work, we characterize the maximum achievable efficacy of\none-run auditing and show that one-run auditing can only perfectly uncover the\ntrue privacy parameters of algorithms whose structure allows the effects of\nindividual data elements to be isolated. Our characterization helps reveal how\nand when one-run auditing is still a promising technique for auditing real\nmachine learning algorithms, despite these fundamental gaps.",
    "updated" : "2025-03-10T11:32:30Z",
    "published" : "2025-03-10T11:32:30Z",
    "authors" : [
      {
        "name" : "Amit Keinan"
      },
      {
        "name" : "Moshe Shenfeld"
      },
      {
        "name" : "Katrina Ligett"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07048v1",
    "title" : "A Failure-Free and Efficient Discrete Laplace Distribution for\n  Differential Privacy in MPC",
    "summary" : "In an MPC-protected distributed computation, although the use of MPC assures\ndata privacy during computation, sensitive information may still be inferred by\ncurious MPC participants from the computation output. This can be observed, for\ninstance, in the inference attacks on either federated learning or a more\nstandard statistical computation with distributed inputs. In this work, we\naddress this output privacy issue by proposing a discrete and bounded\nLaplace-inspired perturbation mechanism along with a secure realization of this\nmechanism using MPC. The proposed mechanism strictly adheres to a zero failure\nprobability, overcoming the limitation encountered on other existing bounded\nand discrete variants of Laplace perturbation. We provide analyses of the\nproposed differential privacy (DP) perturbation in terms of its privacy and\nutility. Additionally, we designed MPC protocols to implement this mechanism\nand presented performance benchmarks based on our experimental setup. The MPC\nrealization of the proposed mechanism exhibits a complexity similar to the\nstate-of-the-art discrete Gaussian mechanism, which can be considered an\nalternative with comparable efficiency while providing stronger differential\nprivacy guarantee. Moreover, efficiency of the proposed scheme can be further\nenhanced by performing the noise generation offline while leaving the\nperturbation phase online.",
    "updated" : "2025-03-10T08:35:16Z",
    "published" : "2025-03-10T08:35:16Z",
    "authors" : [
      {
        "name" : "Ivan Tjuawinata"
      },
      {
        "name" : "Jiabo Wang"
      },
      {
        "name" : "Mengmeng Yang"
      },
      {
        "name" : "Shanxiang Lyu"
      },
      {
        "name" : "Huaxiong Wang"
      },
      {
        "name" : "Kwok-Yan Lam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06808v1",
    "title" : "Privacy Auditing of Large Language Models",
    "summary" : "Current techniques for privacy auditing of large language models (LLMs) have\nlimited efficacy -- they rely on basic approaches to generate canaries which\nleads to weak membership inference attacks that in turn give loose lower bounds\non the empirical privacy leakage. We develop canaries that are far more\neffective than those used in prior work under threat models that cover a range\nof realistic settings. We demonstrate through extensive experiments on multiple\nfamilies of fine-tuned LLMs that our approach sets a new standard for detection\nof privacy leakage. For measuring the memorization rate of non-privately\ntrained LLMs, our designed canaries surpass prior approaches. For example, on\nthe Qwen2.5-0.5B model, our designed canaries achieve $49.6\\%$ TPR at $1\\%$\nFPR, vastly surpassing the prior approach's $4.2\\%$ TPR at $1\\%$ FPR. Our\nmethod can be used to provide a privacy audit of $\\varepsilon \\approx 1$ for a\nmodel trained with theoretical $\\varepsilon$ of 4. To the best of our\nknowledge, this is the first time that a privacy audit of LLM training has\nachieved nontrivial auditing success in the setting where the attacker cannot\ntrain shadow models, insert gradient canaries, or access the model at every\niteration.",
    "updated" : "2025-03-09T23:32:15Z",
    "published" : "2025-03-09T23:32:15Z",
    "authors" : [
      {
        "name" : "Ashwinee Panda"
      },
      {
        "name" : "Xinyu Tang"
      },
      {
        "name" : "Milad Nasr"
      },
      {
        "name" : "Christopher A. Choquette-Choo"
      },
      {
        "name" : "Prateek Mittal"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06732v1",
    "title" : "Data Efficient Subset Training with Differential Privacy",
    "summary" : "Private machine learning introduces a trade-off between the privacy budget\nand training performance. Training convergence is substantially slower and\nextensive hyper parameter tuning is required. Consequently, efficient methods\nto conduct private training of models is thoroughly investigated in the\nliterature. To this end, we investigate the strength of the data efficient\nmodel training methods in the private training setting. We adapt GLISTER\n(Killamsetty et al., 2021b) to the private setting and extensively assess its\nperformance. We empirically find that practical choices of privacy budgets are\ntoo restrictive for data efficient training in the private setting.",
    "updated" : "2025-03-09T19:05:10Z",
    "published" : "2025-03-09T19:05:10Z",
    "authors" : [
      {
        "name" : "Ninad Jayesh Gandhi"
      },
      {
        "name" : "Moparthy Venkata Subrahmanya Sri Harsha"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06455v1",
    "title" : "Privacy Protection in Prosumer Energy Management Based on Federated\n  Learning",
    "summary" : "With the booming development of prosumers, there is an urgent need for a\nprosumer energy management system to take full advantage of the flexibility of\nprosumers and take into account the interests of other parties. However,\nbuilding such a system will undoubtedly reveal users' privacy. In this paper,\nby solving the non-independent and identical distribution of data (Non-IID)\nproblem in federated learning with federated cluster average(FedClusAvg)\nalgorithm, prosumers' information can efficiently participate in the\nintelligent decision making of the system without revealing privacy. In the\nproposed FedClusAvg algorithm, each client performs cluster stratified sampling\nand multiple iterations. Then, the average weight of the parameters of the\nsub-server is determined according to the degree of deviation of the parameter\nfrom the average parameter. Finally, the sub-server multiple local iterations\nand updates, and then upload to the main server. The advantages of FedClusAvg\nalgorithm are the following two parts. First, the accuracy of the model in the\ncase of Non-IID is improved through the method of clustering and parameter\nweighted average. Second, local multiple iterations and three-tier framework\ncan effectively reduce communication rounds.",
    "updated" : "2025-03-09T05:29:29Z",
    "published" : "2025-03-09T05:29:29Z",
    "authors" : [
      {
        "name" : "Yunfeng Li"
      },
      {
        "name" : "Xiaolin Li Zhitao Li"
      },
      {
        "name" : "Gangqiang Li"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06150v1",
    "title" : "Do Fairness Interventions Come at the Cost of Privacy: Evaluations for\n  Binary Classifiers",
    "summary" : "While in-processing fairness approaches show promise in mitigating biased\npredictions, their potential impact on privacy leakage remains under-explored.\nWe aim to address this gap by assessing the privacy risks of fairness-enhanced\nbinary classifiers via membership inference attacks (MIAs) and attribute\ninference attacks (AIAs). Surprisingly, our results reveal that enhancing\nfairness does not necessarily lead to privacy compromises. For example, these\nfairness interventions exhibit increased resilience against MIAs and AIAs. This\nis because fairness interventions tend to remove sensitive information among\nextracted features and reduce confidence scores for the majority of training\ndata for fairer predictions. However, during the evaluations, we uncover a\npotential threat mechanism that exploits prediction discrepancies between fair\nand biased models, leading to advanced attack results for both MIAs and AIAs.\nThis mechanism reveals potent vulnerabilities of fair models and poses\nsignificant privacy risks of current fairness methods. Extensive experiments\nacross multiple datasets, attack methods, and representative fairness\napproaches confirm our findings and demonstrate the efficacy of the uncovered\nmechanism. Our study exposes the under-explored privacy threats in fairness\nstudies, advocating for thorough evaluations of potential security\nvulnerabilities before model deployments.",
    "updated" : "2025-03-08T10:21:21Z",
    "published" : "2025-03-08T10:21:21Z",
    "authors" : [
      {
        "name" : "Huan Tian"
      },
      {
        "name" : "Guangsheng Zhang"
      },
      {
        "name" : "Bo Liu"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Wanlei Zhou"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06021v1",
    "title" : "FedEM: A Privacy-Preserving Framework for Concurrent Utility\n  Preservation in Federated Learning",
    "summary" : "Federated Learning (FL) enables collaborative training of models across\ndistributed clients without sharing local data, addressing privacy concerns in\ndecentralized systems. However, the gradient-sharing process exposes private\ndata to potential leakage, compromising FL's privacy guarantees in real-world\napplications. To address this issue, we propose Federated Error Minimization\n(FedEM), a novel algorithm that incorporates controlled perturbations through\nadaptive noise injection. This mechanism effectively mitigates gradient leakage\nattacks while maintaining model performance. Experimental results on benchmark\ndatasets demonstrate that FedEM significantly reduces privacy risks and\npreserves model accuracy, achieving a robust balance between privacy protection\nand utility preservation.",
    "updated" : "2025-03-08T02:48:00Z",
    "published" : "2025-03-08T02:48:00Z",
    "authors" : [
      {
        "name" : "Mingcong Xu"
      },
      {
        "name" : "Xiaojin Zhang"
      },
      {
        "name" : "Wei Chen"
      },
      {
        "name" : "Hai Jin"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.05954v1",
    "title" : "A Survey on Tabular Data Generation: Utility, Alignment, Fidelity,\n  Privacy, and Beyond",
    "summary" : "Generative modelling has become the standard approach for synthesising\ntabular data. However, different use cases demand synthetic data to comply with\ndifferent requirements to be useful in practice. In this survey, we review deep\ngenerative modelling approaches for tabular data from the perspective of four\ntypes of requirements: utility of the synthetic data, alignment of the\nsynthetic data with domain-specific knowledge, statistical fidelity of the\nsynthetic data distribution compared to the real data distribution, and\nprivacy-preserving capabilities. We group the approaches along two levels of\ngranularity: (i) based on the primary type of requirements they address and\n(ii) according to the underlying model they utilise. Additionally, we summarise\nthe appropriate evaluation methods for each requirement and the specific\ncharacteristics of each model type. Finally, we discuss future directions for\nthe field, along with opportunities to improve the current evaluation methods.\nOverall, this survey can be seen as a user guide to tabular data generation:\nhelping readers navigate available models and evaluation methods to find those\nbest suited to their needs.",
    "updated" : "2025-03-07T21:47:11Z",
    "published" : "2025-03-07T21:47:11Z",
    "authors" : [
      {
        "name" : "Mihaela Cătălina Stoian"
      },
      {
        "name" : "Eleonora Giunchiglia"
      },
      {
        "name" : "Thomas Lukasiewicz"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07483v1",
    "title" : "Poisoning Attacks to Local Differential Privacy Protocols for Trajectory\n  Data",
    "summary" : "Trajectory data, which tracks movements through geographic locations, is\ncrucial for improving real-world applications. However, collecting such\nsensitive data raises considerable privacy concerns. Local differential privacy\n(LDP) offers a solution by allowing individuals to locally perturb their\ntrajectory data before sharing it. Despite its privacy benefits, LDP protocols\nare vulnerable to data poisoning attacks, where attackers inject fake data to\nmanipulate aggregated results. In this work, we make the first attempt to\nanalyze vulnerabilities in several representative LDP trajectory protocols. We\npropose \\textsc{TraP}, a heuristic algorithm for data \\underline{P}oisoning\nattacks using a prefix-suffix method to optimize fake \\underline{Tra}jectory\nselection, significantly reducing computational complexity. Our experimental\nresults demonstrate that our attack can substantially increase target pattern\noccurrences in the perturbed trajectory dataset with few fake users. This study\nunderscores the urgent need for robust defenses and better protocol designs to\nsafeguard LDP trajectory data against malicious manipulation.",
    "updated" : "2025-03-06T02:31:45Z",
    "published" : "2025-03-06T02:31:45Z",
    "authors" : [
      {
        "name" : "I-Jung Hsu"
      },
      {
        "name" : "Chih-Hsun Lin"
      },
      {
        "name" : "Chia-Mu Yu"
      },
      {
        "name" : "Sy-Yen Kuo"
      },
      {
        "name" : "Chun-Ying Huang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.08568v1",
    "title" : "Privacy Law Enforcement Under Centralized Governance: A Qualitative\n  Analysis of Four Years' Special Privacy Rectification Campaigns",
    "summary" : "In recent years, major privacy laws like the GDPR have brought about positive\nchanges. However, challenges remain in enforcing the laws, particularly due to\nunder-resourced regulators facing a large number of potential privacy-violating\nsoftware applications (apps) and the high costs of investigating them. Since\n2019, China has launched a series of privacy enforcement campaigns known as\nSpecial Privacy Rectification Campaigns (SPRCs) to address widespread privacy\nviolations in its mobile application (app) ecosystem. Unlike the enforcement of\nthe GDPR, SPRCs are characterized by large-scale privacy reviews and strict\nsanctions, under the strong control of central authorities. In SPRCs, central\ngovernment authorities issue administrative orders to mobilize various\nresources for market-wide privacy reviews of mobile apps. They enforce strict\nsanctions by requiring privacy-violating apps to rectify issues within a short\ntimeframe or face removal from app stores. While there are a few reports on\nSPRCs, the effectiveness and potential problems of this campaign-style privacy\nenforcement approach remain unclear to the community. In this study, we\nconducted 18 semi-structured interviews with app-related engineers involved in\nSPRCs to better understand the campaign-style privacy enforcement. Based on the\ninterviews, we reported our findings on a variety of aspects of SPRCs, such as\nthe processes that app engineers regularly follow to achieve privacy compliance\nin SPRCs, the challenges they encounter, the solutions they adopt to address\nthese challenges, and the impacts of SPRCs, etc. We found that app engineers\nface a series of challenges in achieving privacy compliance in their apps...",
    "updated" : "2025-03-11T15:56:09Z",
    "published" : "2025-03-11T15:56:09Z",
    "authors" : [
      {
        "name" : "Tao Jing"
      },
      {
        "name" : "Yao Li"
      },
      {
        "name" : "Jingzhou Ye"
      },
      {
        "name" : "Jie Wang"
      },
      {
        "name" : "Xueqiang Wang"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.08297v1",
    "title" : "Privacy for Free: Leveraging Local Differential Privacy Perturbed Data\n  from Multiple Services",
    "summary" : "Local Differential Privacy (LDP) has emerged as a widely adopted\nprivacy-preserving technique in modern data analytics, enabling users to share\nstatistical insights while maintaining robust privacy guarantees. However,\ncurrent LDP applications assume a single service gathering perturbed\ninformation from users. In reality, multiple services may be interested in\ncollecting users' data, which poses privacy burdens to users as more such\nservices emerge. To address this issue, this paper proposes a framework for\ncollecting and aggregating data based on perturbed information from multiple\nservices, regardless of their estimated statistics (e.g., mean or distribution)\nand perturbation mechanisms.\n  Then for mean estimation, we introduce the Unbiased Averaging (UA) method and\nits optimized version, User-level Weighted Averaging (UWA). The former utilizes\nbiased perturbed data, while the latter assigns weights to different perturbed\nresults based on perturbation information, thereby achieving minimal variance.\nFor distribution estimation, we propose the User-level Likelihood Estimation\n(ULE), which treats all perturbed results from a user as a whole for maximum\nlikelihood estimation. Experimental results demonstrate that our framework and\nconstituting methods significantly improve the accuracy of both mean and\ndistribution estimation.",
    "updated" : "2025-03-11T11:10:03Z",
    "published" : "2025-03-11T11:10:03Z",
    "authors" : [
      {
        "name" : "Rong Du"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Yue Fu"
      },
      {
        "name" : "Haibo Hu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.08175v1",
    "title" : "Privacy-Enhancing Paradigms within Federated Multi-Agent Systems",
    "summary" : "LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving\ncomplex problems by integrating multiple agents, each performing different\nroles. However, in sensitive domains, they face emerging privacy protection\nchallenges. In this paper, we introduce the concept of Federated MAS,\nhighlighting the fundamental differences between Federated MAS and traditional\nFL. We then identify key challenges in developing Federated MAS, including: 1)\nheterogeneous privacy protocols among agents, 2) structural differences in\nmulti-party conversations, and 3) dynamic conversational network structures. To\naddress these challenges, we propose Embedded Privacy-Enhancing Agents\n(EPEAgent), an innovative solution that integrates seamlessly into the\nRetrieval-Augmented Generation (RAG) phase and the context retrieval stage.\nThis solution minimizes data flows, ensuring that only task-relevant,\nagent-specific information is shared. Additionally, we design and generate a\ncomprehensive dataset to evaluate the proposed paradigm. Extensive experiments\ndemonstrate that EPEAgent effectively enhances privacy protection while\nmaintaining strong system performance. The code will be availiable at\nhttps://github.com/ZitongShi/EPEAgent",
    "updated" : "2025-03-11T08:38:45Z",
    "published" : "2025-03-11T08:38:45Z",
    "authors" : [
      {
        "name" : "Zitong Shi"
      },
      {
        "name" : "Guancheng Wan"
      },
      {
        "name" : "Wenke Huang"
      },
      {
        "name" : "Guibin Zhang"
      },
      {
        "name" : "Jiawei Shao"
      },
      {
        "name" : "Mang Ye"
      },
      {
        "name" : "Carl Yang"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.08085v1",
    "title" : "PRISM: Privacy-Preserving Improved Stochastic Masking for Federated\n  Generative Models",
    "summary" : "Despite recent advancements in federated learning (FL), the integration of\ngenerative models into FL has been limited due to challenges such as high\ncommunication costs and unstable training in heterogeneous data environments.\nTo address these issues, we propose PRISM, a FL framework tailored for\ngenerative models that ensures (i) stable performance in heterogeneous data\ndistributions and (ii) resource efficiency in terms of communication cost and\nfinal model size. The key of our method is to search for an optimal stochastic\nbinary mask for a random network rather than updating the model weights,\nidentifying a sparse subnetwork with high generative performance; i.e., a\n``strong lottery ticket''. By communicating binary masks in a stochastic\nmanner, PRISM minimizes communication overhead. This approach, combined with\nthe utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic\nmoving average aggregation method (MADA) on the server side, facilitates stable\nand strong generative capabilities by mitigating local divergence in FL\nscenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a\nlightweight model without extra pruning or quantization, making it ideal for\nenvironments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and\nCIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining\nprivacy with minimal communication costs. PRISM is the first to successfully\ngenerate images under challenging non-IID and privacy-preserving FL\nenvironments on complex datasets, where previous methods have struggled.",
    "updated" : "2025-03-11T06:37:54Z",
    "published" : "2025-03-11T06:37:54Z",
    "authors" : [
      {
        "name" : "Kyeongkook Seo"
      },
      {
        "name" : "Dong-Jun Han"
      },
      {
        "name" : "Jaejun Yoo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07775v1",
    "title" : "Sublinear Algorithms for Wasserstein and Total Variation Distances:\n  Applications to Fairness and Privacy Auditing",
    "summary" : "Resource-efficiently computing representations of probability distributions\nand the distances between them while only having access to the samples is a\nfundamental and useful problem across mathematical sciences. In this paper, we\npropose a generic algorithmic framework to estimate the PDF and CDF of any\nsub-Gaussian distribution while the samples from them arrive in a stream. We\ncompute mergeable summaries of distributions from the stream of samples that\nrequire sublinear space w.r.t. the number of observed samples. This allows us\nto estimate Wasserstein and Total Variation (TV) distances between any two\nsub-Gaussian distributions while samples arrive in streams and from multiple\nsources (e.g. federated learning). Our algorithms significantly improves on the\nexisting methods for distance estimation incurring super-linear time and linear\nspace complexities. In addition, we use the proposed estimators of Wasserstein\nand TV distances to audit the fairness and privacy of the ML algorithms. We\nempirically demonstrate the efficiency of the algorithms for estimating these\ndistances and auditing using both synthetic and real-world datasets.",
    "updated" : "2025-03-10T18:57:48Z",
    "published" : "2025-03-10T18:57:48Z",
    "authors" : [
      {
        "name" : "Debabrota Basu"
      },
      {
        "name" : "Debarshi Chanda"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY",
      "cs.DS",
      "stat.CO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07570v1",
    "title" : "Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with\n  Blockchain-Based Auditability",
    "summary" : "Deep learning, when integrated with a large amount of training data, has the\npotential to outperform machine learning in terms of high accuracy. Recently,\nprivacy-preserving deep learning has drawn significant attention of the\nresearch community. Different privacy notions in deep learning include privacy\nof data provided by data-owners and privacy of parameters and/or\nhyperparameters of the underlying neural network. Federated learning is a\npopular privacy-preserving execution environment where data-owners participate\nin learning the parameters collectively without leaking their respective data\nto other participants. However, federated learning suffers from certain\nsecurity/privacy issues. In this paper, we propose Split-n-Chain, a variant of\nsplit learning where the layers of the network are split among several\ndistributed nodes. Split-n-Chain achieves several privacy properties:\ndata-owners need not share their training data with other nodes, and no nodes\nhave access to the parameters and hyperparameters of the neural network (except\nthat of the respective layers they hold). Moreover, Split-n-Chain uses\nblockchain to audit the computation done by different nodes. Our experimental\nresults show that: Split-n-Chain is efficient, in terms of time required to\nexecute different phases, and the training loss trend is similar to that for\nthe same neural network when implemented in a monolithic fashion.",
    "updated" : "2025-03-10T17:40:05Z",
    "published" : "2025-03-10T17:40:05Z",
    "authors" : [
      {
        "name" : "Mukesh Sahani"
      },
      {
        "name" : "Binanda Sengupta"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07505v1",
    "title" : "From Centralized to Decentralized Federated Learning: Theoretical\n  Insights, Privacy Preservation, and Robustness Challenges",
    "summary" : "Federated Learning (FL) enables collaborative learning without directly\nsharing individual's raw data. FL can be implemented in either a centralized\n(server-based) or decentralized (peer-to-peer) manner. In this survey, we\npresent a novel perspective: the fundamental difference between centralized FL\n(CFL) and decentralized FL (DFL) is not merely the network topology, but the\nunderlying training protocol: separate aggregation vs. joint optimization. We\nargue that this distinction in protocol leads to significant differences in\nmodel utility, privacy preservation, and robustness to attacks. We\nsystematically review and categorize existing works in both CFL and DFL\naccording to the type of protocol they employ. This taxonomy provides deeper\ninsights into prior research and clarifies how various approaches relate or\ndiffer. Through our analysis, we identify key gaps in the literature. In\nparticular, we observe a surprising lack of exploration of DFL approaches based\non distributed optimization methods, despite their potential advantages. We\nhighlight this under-explored direction and call for more research on\nleveraging distributed optimization for federated learning. Overall, this work\noffers a comprehensive overview from centralized to decentralized FL, sheds new\nlight on the core distinctions between approaches, and outlines open challenges\nand future directions for the field.",
    "updated" : "2025-03-10T16:27:40Z",
    "published" : "2025-03-10T16:27:40Z",
    "authors" : [
      {
        "name" : "Qiongxiu Li"
      },
      {
        "name" : "Wenrui Yu"
      },
      {
        "name" : "Yufei Xia"
      },
      {
        "name" : "Jun Pang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07216v2",
    "title" : "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
    "summary" : "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
    "updated" : "2025-03-11T12:49:15Z",
    "published" : "2025-03-10T11:55:50Z",
    "authors" : [
      {
        "name" : "Sangwoo Park"
      },
      {
        "name" : "Seanie Lee"
      },
      {
        "name" : "Byungjoo Kim"
      },
      {
        "name" : "Sung Ju Hwang"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.06150v2",
    "title" : "Do Fairness Interventions Come at the Cost of Privacy: Evaluations for\n  Binary Classifiers",
    "summary" : "While in-processing fairness approaches show promise in mitigating biased\npredictions, their potential impact on privacy leakage remains under-explored.\nWe aim to address this gap by assessing the privacy risks of fairness-enhanced\nbinary classifiers via membership inference attacks (MIAs) and attribute\ninference attacks (AIAs). Surprisingly, our results reveal that enhancing\nfairness does not necessarily lead to privacy compromises. For example, these\nfairness interventions exhibit increased resilience against MIAs and AIAs. This\nis because fairness interventions tend to remove sensitive information among\nextracted features and reduce confidence scores for the majority of training\ndata for fairer predictions. However, during the evaluations, we uncover a\npotential threat mechanism that exploits prediction discrepancies between fair\nand biased models, leading to advanced attack results for both MIAs and AIAs.\nThis mechanism reveals potent vulnerabilities of fair models and poses\nsignificant privacy risks of current fairness methods. Extensive experiments\nacross multiple datasets, attack methods, and representative fairness\napproaches confirm our findings and demonstrate the efficacy of the uncovered\nmechanism. Our study exposes the under-explored privacy threats in fairness\nstudies, advocating for thorough evaluations of potential security\nvulnerabilities before model deployments.",
    "updated" : "2025-03-11T11:28:18Z",
    "published" : "2025-03-08T10:21:21Z",
    "authors" : [
      {
        "name" : "Huan Tian"
      },
      {
        "name" : "Guangsheng Zhang"
      },
      {
        "name" : "Bo Liu"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Wanlei Zhou"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09448v1",
    "title" : "Optimizing QoE-Privacy Tradeoff for Proactive VR Streaming",
    "summary" : "Proactive virtual reality (VR) streaming requires users to upload\nviewpoint-related information, raising significant privacy concerns. Existing\nstrategies preserve privacy by introducing errors to viewpoints, which,\nhowever, compromises the quality of experience (QoE) of users. In this paper,\nwe first delve into the analysis of the viewpoint leakage probability achieved\nby existing privacy-preserving approaches. We determine the optimal\ndistribution of viewpoint errors that minimizes the viewpoint leakage\nprobability. Our analyses show that existing approaches cannot fully eliminate\nviewpoint leakage. Then, we propose a novel privacy-preserving approach that\nintroduces noise to uploaded viewpoint prediction errors, which can ensure zero\nviewpoint leakage probability. Given the proposed approach, the tradeoff\nbetween privacy preservation and QoE is optimized to minimize the QoE loss\nwhile satisfying the privacy requirement. Simulation results validate our\nanalysis results and demonstrate that the proposed approach offers a promising\nsolution for balancing privacy and QoE.",
    "updated" : "2025-03-12T14:50:06Z",
    "published" : "2025-03-12T14:50:06Z",
    "authors" : [
      {
        "name" : "Xing Wei"
      },
      {
        "name" : "Shengqian Han"
      },
      {
        "name" : "Chenyang Yang"
      },
      {
        "name" : "Chengjian Sun"
      }
    ],
    "categories" : [
      "cs.MM",
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09381v1",
    "title" : "Faithful and Privacy-Preserving Implementation of Average Consensus",
    "summary" : "We propose a protocol based on mechanism design theory and encrypted control\nto solve average consensus problems among rational and strategic agents while\npreserving their privacy. The proposed protocol provides a mechanism that\nincentivizes the agents to faithfully implement the intended behavior specified\nin the protocol. Furthermore, the protocol runs over encrypted data using\nhomomorphic encryption and secret sharing to protect the privacy of agents. We\nalso analyze the security of the proposed protocol using a simulation paradigm\nin secure multi-party computation. The proposed protocol demonstrates that\nmechanism design and encrypted control can complement each other to achieve\nsecurity under rational adversaries.",
    "updated" : "2025-03-12T13:28:22Z",
    "published" : "2025-03-12T13:28:22Z",
    "authors" : [
      {
        "name" : "Kaoru Teranishi"
      },
      {
        "name" : "Kiminao Kogiso"
      },
      {
        "name" : "Takashi Tanaka"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.CR",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09365v1",
    "title" : "Membership Inference Attacks fueled by Few-Short Learning to detect\n  privacy leakage tackling data integrity",
    "summary" : "Deep learning models have an intrinsic privacy issue as they memorize parts\nof their training data, creating a privacy leakage. Membership Inference\nAttacks (MIA) exploit it to obtain confidential information about the data used\nfor training, aiming to steal information. They can be repurposed as a\nmeasurement of data integrity by inferring whether it was used to train a\nmachine learning model. While state-of-the-art attacks achieve a significant\nprivacy leakage, their requirements are not feasible enough, hindering their\nrole as practical tools to assess the magnitude of the privacy risk. Moreover,\nthe most appropriate evaluation metric of MIA, the True Positive Rate at low\nFalse Positive Rate lacks interpretability. We claim that the incorporation of\nFew-Shot Learning techniques to the MIA field and a proper qualitative and\nquantitative privacy evaluation measure should deal with these issues. In this\ncontext, our proposal is twofold. We propose a Few-Shot learning based MIA,\ncoined as the FeS-MIA model, which eases the evaluation of the privacy breach\nof a deep learning model by significantly reducing the number of resources\nrequired for the purpose. Furthermore, we propose an interpretable quantitative\nand qualitative measure of privacy, referred to as Log-MIA measure. Jointly,\nthese proposals provide new tools to assess the privacy leakage and to ease the\nevaluation of the training data integrity of deep learning models, that is, to\nanalyze the privacy breach of a deep learning model. Experiments carried out\nwith MIA over image classification and language modeling tasks and its\ncomparison to the state-of-the-art show that our proposals excel at reporting\nthe privacy leakage of a deep learning model with little extra information.",
    "updated" : "2025-03-12T13:09:43Z",
    "published" : "2025-03-12T13:09:43Z",
    "authors" : [
      {
        "name" : "Daniel Jiménez-López"
      },
      {
        "name" : "Nuria Rodríguez-Barroso"
      },
      {
        "name" : "M. Victoria Luzón"
      },
      {
        "name" : "Francisco Herrera"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09331v1",
    "title" : "Large-Scale FPGA-Based Privacy Amplification Exceeding $10^8$ Bits for\n  Quantum Key Distribution",
    "summary" : "Privacy Amplification (PA) is indispensable in Quantum Key Distribution (QKD)\npost-processing, as it eliminates information leakage to eavesdroppers.\nField-programmable gate arrays (FPGAs) are highly attractive for QKD systems\ndue to their flexibility and high integration. However, due to limited\nresources, input and output sizes remain the primary bottleneck in FPGA-based\nPA schemes for Discrete Variable (DV)-QKD systems. In this paper, we present a\nlarge-scale FPGA-based PA scheme that supports both input block sizes and\noutput key sizes exceeding $10^8$ bits, effectively addressing the challenges\nposed by the finite-size effect. To accommodate the large input and output\nsizes, we propose a novel PA algorithm and prove its security. We implement and\nevaluate this scheme on a Xilinx XCKU095 FPGA platform. Experimental results\ndemonstrate that our PA implementation can handle an input block size of $10^8$\nbits with flexible output sizes up to the input size. For DV-QKD systems, our\nPA scheme supports an input block size nearly two orders of magnitude larger\nthan current FPGA-based PA schemes, significantly mitigating the impact of the\nfinite-size effect on the final secure key rate.",
    "updated" : "2025-03-12T12:25:13Z",
    "published" : "2025-03-12T12:25:13Z",
    "authors" : [
      {
        "name" : "Xi Cheng"
      },
      {
        "name" : "Hao-kun Mao"
      },
      {
        "name" : "Hong-wei Xu"
      },
      {
        "name" : "Qiong Li"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09317v1",
    "title" : "RaceTEE: A Practical Privacy-Preserving Off-Chain Smart Contract\n  Execution Architecture",
    "summary" : "Decentralized on-chain smart contracts enable trustless collaboration, yet\ntheir inherent data transparency and execution overhead hinder widespread\nadoption. Existing cryptographic approaches incur high computational costs and\nlack generality. Meanwhile, prior TEE-based solutions suffer from practical\nlimitations, such as the inability to support inter-contract interactions,\nreliance on unbreakable TEEs, and compromised usability. We introduce RaceTEE,\na practical and privacy-preserving off-chain execution architecture for smart\ncontracts that leverages Trusted Execution Environments (TEEs). RaceTEE\ndecouples transaction ordering (on-chain) from execution (off-chain), with\ncomputations performed competitively in TEEs, ensuring confidentiality and\nminimizing overhead. It further enhances practicality through three key\nimprovements: supporting secure inter-contract interactions, providing a key\nrotation scheme that enforces forward and backward secrecy even in the event of\nTEE breaches, and enabling full compatibility with existing blockchains without\naltering the user interaction model. To validate its feasibility, we prototype\nRaceTEE using Intel SGX and Ethereum, demonstrating its applicability across\nvarious use cases and evaluating its performance.",
    "updated" : "2025-03-12T12:10:02Z",
    "published" : "2025-03-12T12:10:02Z",
    "authors" : [
      {
        "name" : "Keyu Zhang"
      },
      {
        "name" : "Andrew Martin"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09192v1",
    "title" : "Differential Privacy Personalized Federated Learning Based on\n  Dynamically Sparsified Client Updates",
    "summary" : "Personalized federated learning is extensively utilized in scenarios\ncharacterized by data heterogeneity, facilitating more efficient and automated\nlocal training on data-owning terminals. This includes the automated selection\nof high-performance model parameters for upload, thereby enhancing the overall\ntraining process. However, it entails significant risks of privacy leakage.\nExisting studies have attempted to mitigate these risks by utilizing\ndifferential privacy. Nevertheless, these studies present two major\nlimitations: (1) The integration of differential privacy into personalized\nfederated learning lacks sufficient personalization, leading to the\nintroduction of excessive noise into the model. (2) It fails to adequately\ncontrol the spatial scope of model update information, resulting in a\nsuboptimal balance between data privacy and model effectiveness in differential\nprivacy federated learning. In this paper, we propose a differentially private\npersonalized federated learning approach that employs dynamically sparsified\nclient updates through reparameterization and adaptive norm(DP-pFedDSU).\nReparameterization training effectively selects personalized client update\ninformation, thereby reducing the quantity of updates. This approach minimizes\nthe introduction of noise to the greatest extent possible. Additionally,\ndynamic adaptive norm refers to controlling the norm space of model updates\nduring the training process, mitigating the negative impact of clipping on the\nupdate information. These strategies substantially enhance the effective\nintegration of differential privacy and personalized federated learning.\nExperimental results on EMNIST, CIFAR-10, and CIFAR-100 demonstrate that our\nproposed scheme achieves superior performance and is well-suited for more\ncomplex personalized federated learning scenarios.",
    "updated" : "2025-03-12T09:34:05Z",
    "published" : "2025-03-12T09:34:05Z",
    "authors" : [
      {
        "name" : "Chuanyin Wang"
      },
      {
        "name" : "Yifei Zhang"
      },
      {
        "name" : "Neng Gao"
      },
      {
        "name" : "Qiang Luo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.08085v2",
    "title" : "PRISM: Privacy-Preserving Improved Stochastic Masking for Federated\n  Generative Models",
    "summary" : "Despite recent advancements in federated learning (FL), the integration of\ngenerative models into FL has been limited due to challenges such as high\ncommunication costs and unstable training in heterogeneous data environments.\nTo address these issues, we propose PRISM, a FL framework tailored for\ngenerative models that ensures (i) stable performance in heterogeneous data\ndistributions and (ii) resource efficiency in terms of communication cost and\nfinal model size. The key of our method is to search for an optimal stochastic\nbinary mask for a random network rather than updating the model weights,\nidentifying a sparse subnetwork with high generative performance; i.e., a\n``strong lottery ticket''. By communicating binary masks in a stochastic\nmanner, PRISM minimizes communication overhead. This approach, combined with\nthe utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic\nmoving average aggregation method (MADA) on the server side, facilitates stable\nand strong generative capabilities by mitigating local divergence in FL\nscenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a\nlightweight model without extra pruning or quantization, making it ideal for\nenvironments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and\nCIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining\nprivacy with minimal communication costs. PRISM is the first to successfully\ngenerate images under challenging non-IID and privacy-preserving FL\nenvironments on complex datasets, where previous methods have struggled.",
    "updated" : "2025-03-12T07:22:25Z",
    "published" : "2025-03-11T06:37:54Z",
    "authors" : [
      {
        "name" : "Kyeongkook Seo"
      },
      {
        "name" : "Dong-Jun Han"
      },
      {
        "name" : "Jaejun Yoo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.07427v2",
    "title" : "Creating and Evaluating Privacy and Security Micro-Lessons for\n  Elementary School Children",
    "summary" : "The growing use of technology in K--8 classrooms highlights a parallel need\nfor formal learning opportunities aimed at helping children use technology\nsafely and protect their personal information. Even the youngest students are\nnow using tablets, laptops, and apps to support their learning; however, there\nare limited curricular materials available for elementary and middle school\nchildren on digital privacy and security topics. To bridge this gap, we\ndeveloped a series of micro-lessons to help K--8 children learn about digital\nprivacy and security at school. We first conducted a formative study by\ninterviewing elementary school teachers to identify the design needs for\ndigital privacy and security lessons. We then developed micro-lessons --\nmultiple 15-20 minute activities designed to be easily inserted into the\nexisting curriculum -- using a co-design approach with multiple rounds of\ndeveloping and revising the micro-lessons in collaboration with teachers.\nThroughout the process, we conducted evaluation sessions where teachers\nimplemented or reviewed the micro-lessons. Our study identifies strengths,\nchallenges, and teachers' tailoring strategies when incorporating micro-lessons\nfor K--8 digital privacy and security topics, providing design implications for\nfacilitating learning about these topics in school classrooms.",
    "updated" : "2025-03-11T20:36:50Z",
    "published" : "2025-03-10T15:12:11Z",
    "authors" : [
      {
        "name" : "Lan Gao"
      },
      {
        "name" : "Elana B Blinder"
      },
      {
        "name" : "Abigail Barnes"
      },
      {
        "name" : "Kevin Song"
      },
      {
        "name" : "Tamara Clegg"
      },
      {
        "name" : "Jessica Vitak"
      },
      {
        "name" : "Marshini Chetty"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10350v1",
    "title" : "Enhancing Facial Privacy Protection via Weakening Diffusion Purification",
    "summary" : "The rapid growth of social media has led to the widespread sharing of\nindividual portrait images, which pose serious privacy risks due to the\ncapabilities of automatic face recognition (AFR) systems for mass surveillance.\nHence, protecting facial privacy against unauthorized AFR systems is essential.\nInspired by the generation capability of the emerging diffusion models, recent\nmethods employ diffusion models to generate adversarial face images for privacy\nprotection. However, they suffer from the diffusion purification effect,\nleading to a low protection success rate (PSR). In this paper, we first propose\nlearning unconditional embeddings to increase the learning capacity for\nadversarial modifications and then use them to guide the modification of the\nadversarial latent code to weaken the diffusion purification effect. Moreover,\nwe integrate an identity-preserving structure to maintain structural\nconsistency between the original and generated images, allowing human observers\nto recognize the generated image as having the same identity as the original.\nExtensive experiments conducted on two public datasets, i.e., CelebA-HQ and\nLADN, demonstrate the superiority of our approach. The protected faces\ngenerated by our method outperform those produced by existing facial privacy\nprotection approaches in terms of transferability and natural appearance.",
    "updated" : "2025-03-13T13:27:53Z",
    "published" : "2025-03-13T13:27:53Z",
    "authors" : [
      {
        "name" : "Ali Salar"
      },
      {
        "name" : "Qing Liu"
      },
      {
        "name" : "Yingli Tian"
      },
      {
        "name" : "Guoying Zhao"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10239v1",
    "title" : "I Can Tell Your Secrets: Inferring Privacy Attributes from Mini-app\n  Interaction History in Super-apps",
    "summary" : "Super-apps have emerged as comprehensive platforms integrating various\nmini-apps to provide diverse services. While super-apps offer convenience and\nenriched functionality, they can introduce new privacy risks. This paper\nreveals a new privacy leakage source in super-apps: mini-app interaction\nhistory, including mini-app usage history (Mini-H) and operation history\n(Op-H). Mini-H refers to the history of mini-apps accessed by users, such as\ntheir frequency and categories. Op-H captures user interactions within\nmini-apps, including button clicks, bar drags, and image views. Super-apps can\nnaturally collect these data without instrumentation due to the web-based\nfeature of mini-apps. We identify these data types as novel and unexplored\nprivacy risks through a literature review of 30 papers and an empirical\nanalysis of 31 super-apps. We design a mini-app interaction history-oriented\ninference attack (THEFT), to exploit this new vulnerability. Using THEFT, the\ninsider threats within the low-privilege business department of the super-app\nvendor acting as the adversary can achieve more than 95.5% accuracy in\ninferring privacy attributes of over 16.1% of users. THEFT only requires a\nsmall training dataset of 200 users from public breached databases on the\nInternet. We also engage with super-app vendors and a standards association to\nincrease industry awareness and commitment to protect this data. Our\ncontributions are significant in identifying overlooked privacy risks,\ndemonstrating the effectiveness of a new attack, and influencing industry\npractices toward better privacy protection in the super-app ecosystem.",
    "updated" : "2025-03-13T10:29:40Z",
    "published" : "2025-03-13T10:29:40Z",
    "authors" : [
      {
        "name" : "Yifeng Cai"
      },
      {
        "name" : "Ziqi Zhang"
      },
      {
        "name" : "Mengyu Yao"
      },
      {
        "name" : "Junlin Liu"
      },
      {
        "name" : "Xiaoke Zhao"
      },
      {
        "name" : "Xinyi Fu"
      },
      {
        "name" : "Ruoyu Li"
      },
      {
        "name" : "Zhe Li"
      },
      {
        "name" : "Xiangqun Chen"
      },
      {
        "name" : "Yao Guo"
      },
      {
        "name" : "Ding Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10147v1",
    "title" : "Optimal Privacy-Preserving Distributed Median Consensus",
    "summary" : "Distributed median consensus has emerged as a critical paradigm in\nmulti-agent systems due to the inherent robustness of the median against\noutliers and anomalies in measurement. Despite the sensitivity of the data\ninvolved, the development of privacy-preserving mechanisms for median consensus\nremains underexplored. In this work, we present the first rigorous analysis of\nprivacy in distributed median consensus, focusing on an $L_1$-norm minimization\nframework. We establish necessary and sufficient conditions under which exact\nconsensus and perfect privacy-defined as zero information leakage-can be\nachieved simultaneously. Our information-theoretic analysis provides provable\nguarantees against passive and eavesdropping adversaries, ensuring that private\ndata remain concealed. Extensive numerical experiments validate our theoretical\nresults, demonstrating the practical feasibility of achieving both accuracy and\nprivacy in distributed median consensus.",
    "updated" : "2025-03-13T08:19:12Z",
    "published" : "2025-03-13T08:19:12Z",
    "authors" : [
      {
        "name" : "Wenrui Yu"
      },
      {
        "name" : "Qiongxiu Li"
      },
      {
        "name" : "Richard Heusdens"
      },
      {
        "name" : "Sokol Kosta"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09823v1",
    "title" : "Data Traceability for Privacy Alignment",
    "summary" : "This paper offers a new privacy approach for the growing ecosystem of\nservices--ranging from open banking to healthcare--dependent on sensitive\npersonal data sharing between individuals and third-parties. While these\nservices offer significant benefits, individuals want control over their data,\ntransparency regarding how their data is used, and accountability from\nthird-parties for misuse. However, existing legal and technical mechanisms are\ninadequate for supporting these needs. A comprehensive approach to the modern\nprivacy challenges of accountable third-party data sharing requires a closer\nalignment of technical system architecture and legal institutional design. In\norder to achieve this privacy alignment, we extend traditional security threat\nmodeling and analysis to encompass a broader range of privacy notions than has\nbeen typically considered. In particular, we introduce the concept of\ncovert-accountability, which addresses adversaries that may act dishonestly but\nface potential identification and legal consequences. As a concrete instance of\nthis design approach, we present the OTrace protocol, designed to provide\ntraceable, accountable, consumer-control in third-party data sharing\necosystems. OTrace empowers consumers with the knowledge of where their data\nis, who has it, what it is being used for, and whom it is being shared with. By\napplying our alignment framework to OTrace, we demonstrate that OTrace's\ntechnical affordances can provide more confident, scalable regulatory oversight\nwhen combined with complementary legal mechanisms.",
    "updated" : "2025-03-12T20:42:23Z",
    "published" : "2025-03-12T20:42:23Z",
    "authors" : [
      {
        "name" : "Kevin Liao"
      },
      {
        "name" : "Shreya Thipireddy"
      },
      {
        "name" : "Daniel Weitzner"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09780v1",
    "title" : "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents",
    "summary" : "LLM-powered AI agents are an emerging frontier with tremendous potential to\nincrease human productivity. However, empowering AI agents to take action on\ntheir user's behalf in day-to-day tasks involves giving them access to\npotentially sensitive and private information, which leads to a possible risk\nof inadvertent privacy leakage when the agent malfunctions. In this work, we\npropose one way to address that potential risk, by training AI agents to better\nsatisfy the privacy principle of data minimization. For the purposes of this\nbenchmark, by \"data minimization\" we mean instances where private information\nis shared only when it is necessary to fulfill a specific task-relevant\npurpose. We develop a benchmark called AgentDAM to evaluate how well existing\nand future AI agents can limit processing of potentially private information\nthat we designate \"necessary\" to fulfill the task. Our benchmark simulates\nrealistic web interaction scenarios and is adaptable to all existing web\nnavigation agents. We use AgentDAM to evaluate how well AI agents built on top\nof GPT-4, Llama-3 and Claude can limit processing of potentially private\ninformation when unnecessary, and show that these agents are often prone to\ninadvertent use of unnecessary sensitive information. We finally propose a\nprompting-based approach that reduces this.",
    "updated" : "2025-03-12T19:30:31Z",
    "published" : "2025-03-12T19:30:31Z",
    "authors" : [
      {
        "name" : "Arman Zharmagambetov"
      },
      {
        "name" : "Chuan Guo"
      },
      {
        "name" : "Ivan Evtimov"
      },
      {
        "name" : "Maya Pavlova"
      },
      {
        "name" : "Ruslan Salakhutdinov"
      },
      {
        "name" : "Kamalika Chaudhuri"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09381v2",
    "title" : "Faithful and Privacy-Preserving Implementation of Average Consensus",
    "summary" : "We propose a protocol based on mechanism design theory and encrypted control\nto solve average consensus problems among rational and strategic agents while\npreserving their privacy. The proposed protocol provides a mechanism that\nincentivizes the agents to faithfully implement the intended behavior specified\nin the protocol. Furthermore, the protocol runs over encrypted data using\nhomomorphic encryption and secret sharing to protect the privacy of agents. We\nalso analyze the security of the proposed protocol using a simulation paradigm\nin secure multi-party computation. The proposed protocol demonstrates that\nmechanism design and encrypted control can complement each other to achieve\nsecurity under rational adversaries.",
    "updated" : "2025-03-13T02:40:23Z",
    "published" : "2025-03-12T13:28:22Z",
    "authors" : [
      {
        "name" : "Kaoru Teranishi"
      },
      {
        "name" : "Kiminao Kogiso"
      },
      {
        "name" : "Takashi Tanaka"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.CR",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11232v1",
    "title" : "PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature\n  Intervention with Sparse Autoencoders",
    "summary" : "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing but also pose significant privacy risks by\nmemorizing and leaking Personally Identifiable Information (PII). Existing\nmitigation strategies, such as differential privacy and neuron-level\ninterventions, often degrade model utility or fail to effectively prevent\nleakage. To address this challenge, we introduce PrivacyScalpel, a novel\nprivacy-preserving framework that leverages LLM interpretability techniques to\nidentify and mitigate PII leakage while maintaining performance. PrivacyScalpel\ncomprises three key steps: (1) Feature Probing, which identifies layers in the\nmodel that encode PII-rich representations, (2) Sparse Autoencoding, where a\nk-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive\nfeatures,\n  and (3) Feature-Level Interventions, which employ targeted ablation and\nvector steering to suppress PII leakage.\n  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron\ndataset, shows that PrivacyScalpel significantly reduces email leakage from\n5.15\\% to as low as 0.0\\%, while maintaining over 99.4\\% of the original\nmodel's utility. Notably, our method outperforms neuron-level interventions in\nprivacy-utility trade-offs, demonstrating that acting on sparse, monosemantic\nfeatures is more effective than manipulating polysemantic neurons. Beyond\nimproving LLM privacy, our approach offers insights into the mechanisms\nunderlying PII memorization, contributing to the broader field of model\ninterpretability and secure AI deployment.",
    "updated" : "2025-03-14T09:31:01Z",
    "published" : "2025-03-14T09:31:01Z",
    "authors" : [
      {
        "name" : "Ahmed Frikha"
      },
      {
        "name" : "Muhammad Reza Ar Razi"
      },
      {
        "name" : "Krishna Kanth Nakka"
      },
      {
        "name" : "Ricardo Mendes"
      },
      {
        "name" : "Xue Jiang"
      },
      {
        "name" : "Xuebing Zhou"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11169v1",
    "title" : "Security and Privacy: Key Requirements for Molecular Communication in\n  Medicine and Healthcare",
    "summary" : "Molecular communication (MC) is an emerging paradigm that enables data\ntransmission through biochemical signals rather than traditional\nelectromagnetic waves. This approach is particularly promising for environments\nwhere conventional wireless communication is impractical, such as within the\nhuman body. However, security and privacy pose significant challenges that must\nbe addressed to ensure reliable communication. Moreover, MC is often\nevent-triggered, making it logical to adopt goal-oriented communication\nstrategies, similar to those used in message identification. This work explores\nsecure identification strategies for MC, with a focus on the\ninformation-theoretic security of message identification over Poisson wiretap\nchannels (DT-PWC).",
    "updated" : "2025-03-14T08:14:14Z",
    "published" : "2025-03-14T08:14:14Z",
    "authors" : [
      {
        "name" : "Vida Gholamiyan"
      },
      {
        "name" : "Yaning Zhao"
      },
      {
        "name" : "Wafa Labidi"
      },
      {
        "name" : "Holger Boche"
      },
      {
        "name" : "Christian Deppe"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11051v1",
    "title" : "Towards Privacy-preserved Pre-training of Remote Sensing Foundation\n  Models with Federated Mutual-guidance Learning",
    "summary" : "Traditional Remote Sensing Foundation models (RSFMs) are pre-trained with a\ndata-centralized paradigm, through self-supervision on large-scale curated\nremote sensing data. For each institution, however, pre-training RSFMs with\nlimited data in a standalone manner may lead to suboptimal performance, while\naggregating remote sensing data from multiple institutions for centralized\npre-training raises privacy concerns. Seeking for collaboration is a promising\nsolution to resolve this dilemma, where multiple institutions can\ncollaboratively train RSFMs without sharing private data. In this paper, we\npropose a novel privacy-preserved pre-training framework (FedSense), which\nenables multiple institutions to collaboratively train RSFMs without sharing\nprivate data. However, it is a non-trivial task hindered by a vicious cycle,\nwhich results from model drift by remote sensing data heterogeneity and high\ncommunication overhead. To break this vicious cycle, we introduce Federated\nMutual-guidance Learning. Specifically, we propose a Server-to-Clients Guidance\n(SCG) mechanism to guide clients updates towards global-flatness optimal\nsolutions. Additionally, we propose a Clients-to-Server Guidance (CSG)\nmechanism to inject local knowledge into the server by low-bit communication.\nExtensive experiments on four downstream tasks demonstrate the effectiveness of\nour FedSense in both full-precision and communication-reduced scenarios,\nshowcasing remarkable communication efficiency and performance gains.",
    "updated" : "2025-03-14T03:38:49Z",
    "published" : "2025-03-14T03:38:49Z",
    "authors" : [
      {
        "name" : "Jieyi Tan"
      },
      {
        "name" : "Chengwei Zhang"
      },
      {
        "name" : "Bo Dang"
      },
      {
        "name" : "Yansheng Li"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10945v1",
    "title" : "$(\\varepsilon, δ)$ Considered Harmful: Best Practices for Reporting\n  Differential Privacy Guarantees",
    "summary" : "Current practices for reporting the level of differential privacy (DP)\nguarantees for machine learning (ML) algorithms provide an incomplete and\npotentially misleading picture of the guarantees and make it difficult to\ncompare privacy levels across different settings. We argue for using Gaussian\ndifferential privacy (GDP) as the primary means of communicating DP guarantees\nin ML, with the full privacy profile as a secondary option in case GDP is too\ninaccurate. Unlike other widely used alternatives, GDP has only one parameter,\nwhich ensures easy comparability of guarantees, and it can accurately capture\nthe full privacy profile of many important ML applications. To support our\nclaims, we investigate the privacy profiles of state-of-the-art DP large-scale\nimage classification, and the TopDown algorithm for the U.S. Decennial Census,\nobserving that GDP fits the profiles remarkably well in all three cases.\nAlthough GDP is ideal for reporting the final guarantees, other formalisms\n(e.g., privacy loss random variables) are needed for accurate privacy\naccounting. We show that such intermediate representations can be efficiently\nconverted to GDP with minimal loss in tightness.",
    "updated" : "2025-03-13T23:06:30Z",
    "published" : "2025-03-13T23:06:30Z",
    "authors" : [
      {
        "name" : "Juan Felipe Gomez"
      },
      {
        "name" : "Bogdan Kulynych"
      },
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Jamie Hayes"
      },
      {
        "name" : "Borja Balle"
      },
      {
        "name" : "Antti Honkela"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10915v1",
    "title" : "Usable Privacy in Virtual Worlds: Design Implications for Data\n  Collection Awareness and Control Interfaces in Virtual Reality",
    "summary" : "Extended reality (XR) devices have become ubiquitous. They are equipped with\narrays of sensors, collecting extensive user and environmental data, allowing\ninferences about sensitive user information users may not realize they are\nsharing. Current VR privacy notices largely replicate mechanisms from 2D\ninterfaces, failing to leverage the unique affordances of virtual 3D\nenvironments. To address this, we conducted brainstorming and sketching\nsessions with novice game developers and designers, followed by privacy expert\nevaluations, to explore and refine privacy interfaces tailored for VR. Key\nchallenges include balancing user engagement with privacy awareness, managing\ncomplex privacy information with user comprehension, and maintaining compliance\nand trust. We identify design implications such as thoughtful gamification,\nexplicit and purpose-tied consent mechanisms, and granular, modifiable privacy\ncontrol options. Our findings provide actionable guidance to researchers and\npractitioners for developing privacy-aware and user-friendly VR experiences.",
    "updated" : "2025-03-13T22:02:54Z",
    "published" : "2025-03-13T22:02:54Z",
    "authors" : [
      {
        "name" : "Viktorija Paneva"
      },
      {
        "name" : "Verena Winterhalter"
      },
      {
        "name" : "Naga Sai Surya Vamsy Malladi"
      },
      {
        "name" : "Marvin Strauss"
      },
      {
        "name" : "Stefan Schneegass"
      },
      {
        "name" : "Florian Alt"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.10727v1",
    "title" : "Word-level Annotation of GDPR Transparency Compliance in Privacy\n  Policies using Large Language Models",
    "summary" : "Ensuring transparency of data practices related to personal information is a\nfundamental requirement under the General Data Protection Regulation (GDPR),\nparticularly as mandated by Articles 13 and 14. However, assessing compliance\nat scale remains a challenge due to the complexity and variability of privacy\npolicy language. Manual audits are resource-intensive and inconsistent, while\nexisting automated approaches lack the granularity needed to capture nuanced\ntransparency disclosures.\n  In this paper, we introduce a large language model (LLM)-based framework for\nword-level GDPR transparency compliance annotation. Our approach comprises a\ntwo-stage annotation pipeline that combines initial LLM-based annotation with a\nself-correction mechanism for iterative refinement. This annotation pipeline\nenables the systematic identification and fine-grained annotation of\ntransparency-related content in privacy policies, aligning with 21 GDPR-derived\ntransparency requirements. To enable large-scale analysis, we compile a dataset\nof 703,791 English-language policies, from which we generate a sample of 200\nmanually annotated privacy policies.\n  To evaluate our approach, we introduce a two-tiered methodology assessing\nboth label- and span-level annotation performance. We conduct a comparative\nanalysis of eight high-profile LLMs, providing insights into their\neffectiveness in identifying GDPR transparency disclosures. Our findings\ncontribute to advancing the automation of GDPR compliance assessments and\nprovide valuable resources for future research in privacy policy analysis.",
    "updated" : "2025-03-13T11:41:25Z",
    "published" : "2025-03-13T11:41:25Z",
    "authors" : [
      {
        "name" : "Thomas Cory"
      },
      {
        "name" : "Wolf Rieder"
      },
      {
        "name" : "Julia Krämer"
      },
      {
        "name" : "Philip Raschke"
      },
      {
        "name" : "Patrick Herbke"
      },
      {
        "name" : "Axel Küpper"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.13173v1",
    "title" : "PAUSE: Low-Latency and Privacy-Aware Active User Selection for Federated\n  Learning",
    "summary" : "Federated learning (FL) enables multiple edge devices to collaboratively\ntrain a machine learning model without the need to share potentially private\ndata. Federated learning proceeds through iterative exchanges of model updates,\nwhich pose two key challenges: First, the accumulation of privacy leakage over\ntime, and second, communication latency. These two limitations are typically\naddressed separately: The former via perturbed updates to enhance privacy and\nthe latter using user selection to mitigate latency - both at the expense of\naccuracy. In this work, we propose a method that jointly addresses the\naccumulation of privacy leakage and communication latency via active user\nselection, aiming to improve the trade-off among privacy, latency, and model\nperformance. To achieve this, we construct a reward function that accounts for\nthese three objectives. Building on this reward, we propose a multi-armed\nbandit (MAB)-based algorithm, termed Privacy-aware Active User SElection\n(PAUSE) which dynamically selects a subset of users each round while ensuring\nbounded overall privacy leakage. We establish a theoretical analysis,\nsystematically showing that the reward growth rate of PAUSE follows that of the\nbest-known rate in MAB literature. To address the complexity overhead of active\nuser selection, we propose a simulated annealing-based relaxation of PAUSE and\nanalyze its ability to approximate the reward-maximizing policy under reduced\ncomplexity. We numerically validate the privacy leakage, associated improved\nlatency, and accuracy gains of our methods for the federated training in\nvarious scenarios.",
    "updated" : "2025-03-17T13:50:35Z",
    "published" : "2025-03-17T13:50:35Z",
    "authors" : [
      {
        "name" : "Ori Peleg"
      },
      {
        "name" : "Natalie Lang"
      },
      {
        "name" : "Stefano Rini"
      },
      {
        "name" : "Nir Shlezinger"
      },
      {
        "name" : "Kobi Cohen"
      }
    ],
    "categories" : [
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12958v1",
    "title" : "FedSDP: Explainable Differential Privacy in Federated Learning via\n  Shapley Values",
    "summary" : "Federated learning (FL) enables participants to store data locally while\ncollaborating in training, yet it remains vulnerable to privacy attacks, such\nas data reconstruction. Existing differential privacy (DP) technologies inject\nnoise dynamically into the training process to mitigate the impact of excessive\nnoise. However, this dynamic scheduling is often grounded in factors indirectly\nrelated to privacy, making it difficult to clearly explain the intricate\nrelationship between dynamic noise adjustments and privacy requirements. To\naddress this issue, we propose FedSDP, a novel and explainable DP-based privacy\nprotection mechanism that guides noise injection based on privacy contribution.\nSpecifically, FedSDP leverages Shapley values to assess the contribution of\nprivate attributes to local model training and dynamically adjusts the amount\nof noise injected accordingly. By providing theoretical insights into the\ninjection of varying scales of noise into local training, FedSDP enhances\ninterpretability. Extensive experiments demonstrate that FedSDP can achieve a\nsuperior balance between privacy preservation and model performance, surpassing\nstate-of-the-art (SOTA) solutions.",
    "updated" : "2025-03-17T09:14:19Z",
    "published" : "2025-03-17T09:14:19Z",
    "authors" : [
      {
        "name" : "Yunbo Li"
      },
      {
        "name" : "Jiaping Gui"
      },
      {
        "name" : "Yue Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12786v1",
    "title" : "Privacy-Preserving Biometric Verification with Handwritten Random Digit\n  String",
    "summary" : "Handwriting verification has stood as a steadfast identity authentication\nmethod for decades. However, this technique risks potential privacy breaches\ndue to the inclusion of personal information in handwritten biometrics such as\nsignatures. To address this concern, we propose using the Random Digit String\n(RDS) for privacy-preserving handwriting verification. This approach allows\nusers to authenticate themselves by writing an arbitrary digit sequence,\neffectively ensuring privacy protection. To evaluate the effectiveness of RDS,\nwe construct a new HRDS4BV dataset composed of online naturally handwritten\nRDS. Unlike conventional handwriting, RDS encompasses unconstrained and\nvariable content, posing significant challenges for modeling consistent\npersonal writing style. To surmount this, we propose the Pattern Attentive\nVErification Network (PAVENet), along with a Discriminative Pattern Mining\n(DPM) module. DPM adaptively enhances the recognition of consistent and\ndiscriminative writing patterns, thus refining handwriting style\nrepresentation. Through comprehensive evaluations, we scrutinize the\napplicability of online RDS verification and showcase a pronounced\noutperformance of our model over existing methods. Furthermore, we discover a\nnoteworthy forgery phenomenon that deviates from prior findings and discuss its\npositive impact in countering malicious impostor attacks. Substantially, our\nwork underscores the feasibility of privacy-preserving biometric verification\nand propels the prospects of its broader acceptance and application.",
    "updated" : "2025-03-17T03:47:25Z",
    "published" : "2025-03-17T03:47:25Z",
    "authors" : [
      {
        "name" : "Peirong Zhang"
      },
      {
        "name" : "Yuliang Liu"
      },
      {
        "name" : "Songxuan Lai"
      },
      {
        "name" : "Hongliang Li"
      },
      {
        "name" : "Lianwen Jin"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12464v1",
    "title" : "Learning Privacy from Visual Entities",
    "summary" : "Subjective interpretation and content diversity make predicting whether an\nimage is private or public a challenging task. Graph neural networks combined\nwith convolutional neural networks (CNNs), which consist of 14,000 to 500\nmillions parameters, generate features for visual entities (e.g., scene and\nobject types) and identify the entities that contribute to the decision. In\nthis paper, we show that using a simpler combination of transfer learning and a\nCNN to relate privacy with scene types optimises only 732 parameters while\nachieving comparable performance to that of graph-based methods. On the\ncontrary, end-to-end training of graph-based methods can mask the contribution\nof individual components to the classification performance. Furthermore, we\nshow that a high-dimensional feature vector, extracted with CNNs for each\nvisual entity, is unnecessary and complexifies the model. The graph component\nhas also negligible impact on performance, which is driven by fine-tuning the\nCNN to optimise image features for privacy nodes.",
    "updated" : "2025-03-16T11:39:08Z",
    "published" : "2025-03-16T11:39:08Z",
    "authors" : [
      {
        "name" : "Alessio Xompero"
      },
      {
        "name" : "Andrea Cavallaro"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12347v1",
    "title" : "Synthesizing Privacy-Preserving Text Data via Finetuning without\n  Finetuning Billion-Scale LLMs",
    "summary" : "Synthetic data offers a promising path to train models while preserving data\nprivacy. Differentially private (DP) finetuning of large language models (LLMs)\nas data generator is effective, but is impractical when computation resources\nare limited. Meanwhile, prompt-based methods such as private evolution, depend\nheavily on the manual prompts, and ineffectively use private information in\ntheir iterative data selection process. To overcome these limitations, we\npropose CTCL (Data Synthesis with ConTrollability and CLustering), a novel\nframework for generating privacy-preserving synthetic data without extensive\nprompt engineering or billion-scale LLM finetuning. CTCL pretrains a\nlightweight 140M conditional generator and a clustering-based topic model on\nlarge-scale public data. To further adapt to the private domain, the generator\nis DP finetuned on private data for fine-grained textual information, while the\ntopic model extracts a DP histogram representing distributional information.\nThe DP generator then samples according to the DP histogram to synthesize a\ndesired number of data examples. Evaluation across five diverse domains\ndemonstrates the effectiveness of our framework, particularly in the strong\nprivacy regime. Systematic ablation validates the design of each framework\ncomponent and highlights the scalability of our approach.",
    "updated" : "2025-03-16T04:00:32Z",
    "published" : "2025-03-16T04:00:32Z",
    "authors" : [
      {
        "name" : "Bowen Tan"
      },
      {
        "name" : "Zheng Xu"
      },
      {
        "name" : "Eric Xing"
      },
      {
        "name" : "Zhiting Hu"
      },
      {
        "name" : "Shanshan Wu"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12314v1",
    "title" : "Empirical Privacy Variance",
    "summary" : "We propose the notion of empirical privacy variance and study it in the\ncontext of differentially private fine-tuning of language models. Specifically,\nwe show that models calibrated to the same $(\\varepsilon, \\delta)$-DP guarantee\nusing DP-SGD with different hyperparameter configurations can exhibit\nsignificant variations in empirical privacy, which we quantify through the lens\nof memorization. We investigate the generality of this phenomenon across\nmultiple dimensions and discuss why it is surprising and relevant. Through\nregression analysis, we examine how individual and composite hyperparameters\ninfluence empirical privacy. The results reveal a no-free-lunch trade-off:\nexisting practices of hyperparameter tuning in DP-SGD, which focus on\noptimizing utility under a fixed privacy budget, often come at the expense of\nempirical privacy. To address this, we propose refined heuristics for\nhyperparameter selection that explicitly account for empirical privacy, showing\nthat they are both precise and practically useful. Finally, we take preliminary\nsteps to understand empirical privacy variance. We propose two hypotheses,\nidentify limitations in existing techniques like privacy auditing, and outline\nopen questions for future research.",
    "updated" : "2025-03-16T01:43:49Z",
    "published" : "2025-03-16T01:43:49Z",
    "authors" : [
      {
        "name" : "Yuzheng Hu"
      },
      {
        "name" : "Fan Wu"
      },
      {
        "name" : "Ruicheng Xian"
      },
      {
        "name" : "Yuhang Liu"
      },
      {
        "name" : "Lydia Zakynthinou"
      },
      {
        "name" : "Pritish Kamath"
      },
      {
        "name" : "Chiyuan Zhang"
      },
      {
        "name" : "David Forsyth"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12232v1",
    "title" : "From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved\n  Visible-Infrared Person Re-Identification",
    "summary" : "Aiming to match pedestrian images captured under varying lighting conditions,\nvisible-infrared person re-identification (VI-ReID) has drawn intensive\nresearch attention and achieved promising results. However, in real-world\nsurveillance contexts, data is distributed across multiple devices/entities,\nraising privacy and ownership concerns that make existing centralized training\nimpractical for VI-ReID. To tackle these challenges, we propose L2RW, a\nbenchmark that brings VI-ReID closer to real-world applications. The rationale\nof L2RW is that integrating decentralized training into VI-ReID can address\nprivacy concerns in scenarios with limited data-sharing regulation.\nSpecifically, we design protocols and corresponding algorithms for different\nprivacy sensitivity levels. In our new benchmark, we ensure the model training\nis done in the conditions that: 1) data from each camera remains completely\nisolated, or 2) different data entities (e.g., data controllers of a certain\nregion) can selectively share the data. In this way, we simulate scenarios with\nstrict privacy constraints which is closer to real-world conditions. Intensive\nexperiments with various server-side federated algorithms are conducted,\nshowing the feasibility of decentralized VI-ReID training. Notably, when\nevaluated in unseen domains (i.e., new data entities), our L2RW, trained with\nisolated data (privacy-preserved), achieves performance comparable to SOTAs\ntrained with shared data (privacy-unrestricted). We hope this work offers a\nnovel research entry for deploying VI-ReID that fits real-world scenarios and\ncan benefit the community.",
    "updated" : "2025-03-15T18:56:29Z",
    "published" : "2025-03-15T18:56:29Z",
    "authors" : [
      {
        "name" : "Yan Jiang"
      },
      {
        "name" : "Hao Yu"
      },
      {
        "name" : "Xu Cheng"
      },
      {
        "name" : "Haoyu Chen"
      },
      {
        "name" : "Zhaodong Sun"
      },
      {
        "name" : "Guoying Zhao"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12226v1",
    "title" : "Research on Large Language Model Cross-Cloud Privacy Protection and\n  Collaborative Training based on Federated Learning",
    "summary" : "The fast development of large language models (LLMs) and popularization of\ncloud computing have led to increasing concerns on privacy safeguarding and\ndata security of cross-cloud model deployment and training as the key\nchallenges. We present a new framework for addressing these issues along with\nenabling privacy preserving collaboration on training between distributed\nclouds based on federated learning. Our mechanism encompasses cutting-edge\ncryptographic primitives, dynamic model aggregation techniques, and cross-cloud\ndata harmonization solutions to enhance security, efficiency, and scalability\nto the traditional federated learning paradigm. Furthermore, we proposed a\nhybrid aggregation scheme to mitigate the threat of Data Leakage and to\noptimize the aggregation of model updates, thus achieving substantial\nenhancement on the model effectiveness and stability. Experimental results\ndemonstrate that the training efficiency, privacy protection, and model\naccuracy of the proposed model compare favorably to those of the traditional\nfederated learning method.",
    "updated" : "2025-03-15T18:44:50Z",
    "published" : "2025-03-15T18:44:50Z",
    "authors" : [
      {
        "name" : "Ze Yang"
      },
      {
        "name" : "Yihong Jin"
      },
      {
        "name" : "Yihan Zhang"
      },
      {
        "name" : "Juntian Liu"
      },
      {
        "name" : "Xinhe Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12225v1",
    "title" : "Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents",
    "summary" : "This article explores the gaps that can manifest when using a large language\nmodel (LLM) to obtain simplified interpretations of data practices from a\ncomplex privacy policy. We exemplify these gaps to showcase issues in accuracy,\ncompleteness, clarity and representation, while advocating for continued\nresearch to realize an LLM's true potential in revolutionizing privacy\nmanagement through personal assistants and automated compliance checking.",
    "updated" : "2025-03-15T18:43:13Z",
    "published" : "2025-03-15T18:43:13Z",
    "authors" : [
      {
        "name" : "Rinku Dewri"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CY",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12220v1",
    "title" : "A Bubble-Cluster Federated Learning Framework for Privacy-Preserving\n  Demand Forecasting on Heterogeneous Retail Data",
    "summary" : "Federated learning (FL) enables retailers to share model parameters for\ndemand forecasting while maintaining privacy. However, heterogeneous data\nacross diverse regions, driven by factors such as varying consumer behavior,\nposes challenges to the effectiveness of federated learning. To tackle this\nchallenge, we propose Bubble-Cluster Federated Learning (BFL), a novel\nclustering-based federated learning framework tailored for sales prediction. By\nleveraging differential privacy and feature importance distribution, BFL groups\nretailers into distinct \"bubbles\", each forming its own federated learning (FL)\nsystem to effectively isolate data heterogeneity. Within each bubble,\nTransformer models are designed to predict local sales for each client. Our\nexperiments demonstrate that BFL significantly surpasses FedAvg and outperforms\nlocal learning in demand forecasting performance across all participating\nclients. Compared to local learning, BFL can achieve a 5.4\\% improvement in\nR\\textsuperscript{2}, a 69\\% reduction in RMSE, and a 45\\% decrease in MAE. Our\nstudy highlights BFL's adaptability in enabling effective federated learning\nthrough dynamic adjustments to noise levels and the range of clients\nparticipating in each bubble. This approach strategically groups participants\ninto distinct \"bubbles\" while proactively identifying and filtering out risky\nclients that could compromise the FL system. The findings demonstrate BFL's\nability to enhance collaborative learning in regression tasks on heterogeneous\ndata, achieving a balance between forecasting accuracy and privacy preservation\nin retail applications. Additionally, BFL's capability to detect and neutralize\npoisoned data from clients enhances the system's robustness and reliability,\nensuring more secure and effective federated learning.",
    "updated" : "2025-03-15T18:07:54Z",
    "published" : "2025-03-15T18:07:54Z",
    "authors" : [
      {
        "name" : "Yunbo Long"
      },
      {
        "name" : "Liming Xu"
      },
      {
        "name" : "Ge Zheng"
      },
      {
        "name" : "Alexandra Brintrup"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12156v1",
    "title" : "Efficient and Privacy-Preserved Link Prediction via Condensed Graphs",
    "summary" : "Link prediction is crucial for uncovering hidden connections within complex\nnetworks, enabling applications such as identifying potential customers and\nproducts. However, this research faces significant challenges, including\nconcerns about data privacy, as well as high computational and storage costs,\nespecially when dealing with large-scale networks. Condensed graphs, which are\nmuch smaller than the original graphs while retaining essential information,\nhas become an effective solution to both maintain data utility and preserve\nprivacy. Existing methods, however, initialize synthetic graphs through random\nnode selection without considering node connectivity, and are mainly designed\nfor node classification tasks. As a result, their potential for\nprivacy-preserving link prediction remains largely unexplored. We introduce\nHyDRO\\textsuperscript{+}, a graph condensation method guided by algebraic\nJaccard similarity, which leverages local connectivity information to optimize\ncondensed graph structures. Extensive experiments on four real-world networks\nshow that our method outperforms state-of-the-art methods and even the original\nnetworks in balancing link prediction accuracy and privacy preservation.\nMoreover, our method achieves nearly 20* faster training and reduces storage\nrequirements by 452*, as demonstrated on the Computers dataset, compared to\nlink prediction on the original networks. This work represents the first\nattempt to leverage condensed graphs for privacy-preserving link prediction\ninformation sharing in real-world complex networks. It offers a promising\npathway for preserving link prediction information while safeguarding privacy,\nadvancing the use of graph condensation in large-scale networks with privacy\nconcerns.",
    "updated" : "2025-03-15T14:54:04Z",
    "published" : "2025-03-15T14:54:04Z",
    "authors" : [
      {
        "name" : "Yunbo Long"
      },
      {
        "name" : "Liming Xu"
      },
      {
        "name" : "Alexandra Brintrup"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12045v1",
    "title" : "Auditing Differential Privacy in the Black-Box Setting",
    "summary" : "This paper introduces a novel theoretical framework for auditing differential\nprivacy (DP) in a black-box setting. Leveraging the concept of $f$-differential\nprivacy, we explicitly define type I and type II errors and propose an auditing\nmechanism based on conformal inference. Our approach robustly controls the type\nI error rate under minimal assumptions. Furthermore, we establish a fundamental\nimpossibility result, demonstrating the inherent difficulty of simultaneously\ncontrolling both type I and type II errors without additional assumptions.\nNevertheless, under a monotone likelihood ratio (MLR) assumption, our auditing\nmechanism effectively controls both errors. We also extend our method to\nconstruct valid confidence bands for the trade-off function in the\nfinite-sample regime.",
    "updated" : "2025-03-15T08:34:40Z",
    "published" : "2025-03-15T08:34:40Z",
    "authors" : [
      {
        "name" : "Kaining Shi"
      },
      {
        "name" : "Cong Ma"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11963v1",
    "title" : "Effective and Efficient Cross-City Traffic Knowledge Transfer A\n  Privacy-Preserving Perspective",
    "summary" : "Traffic prediction targets forecasting future traffic conditions using\nhistorical traffic data, serving a critical role in urban computing and\ntransportation management. To mitigate the scarcity of traffic data while\nmaintaining data privacy, numerous Federated Traffic Knowledge Transfer (FTT)\napproaches have been developed, which use transfer learning and federated\nlearning to transfer traffic knowledge from data-rich cities to data-scarce\ncities, enhancing traffic prediction capabilities for the latter. However,\ncurrent FTT approaches face challenges such as privacy leakage, cross-city data\ndistribution discrepancies, low data quality, and inefficient knowledge\ntransfer, limiting their privacy protection, effectiveness, robustness, and\nefficiency in real-world applications.\n  To this end, we propose FedTT, an effective, efficient, and privacy-aware\ncross-city traffic knowledge transfer framework that transforms the traffic\ndata domain from the data-rich cities and trains traffic models using the\ntransformed data for the data-scarce cities. First, to safeguard data privacy,\nwe propose a traffic secret transmission method that securely transmits and\naggregates traffic domain-transformed data from source cities using a\nlightweight secret aggregation approach. Second, to mitigate the impact of\ntraffic data distribution discrepancies on model performance, we introduce a\ntraffic domain adapter to uniformly transform traffic data from the source\ncities' domains to that of the target city. Third, to improve traffic data\nquality, we design a traffic view imputation method to fill in and predict\nmissing traffic data. Finally, to enhance transfer efficiency, FedTT is\nequipped with a federated parallel training method that enables the\nsimultaneous training of multiple modules. Extensive experiments using 4\nreal-life datasets demonstrate that FedTT outperforms the 14 state-of-the-art\nbaselines.",
    "updated" : "2025-03-15T02:26:24Z",
    "published" : "2025-03-15T02:26:24Z",
    "authors" : [
      {
        "name" : "Zhihao Zeng"
      },
      {
        "name" : "Ziquan Fang"
      },
      {
        "name" : "Yuting Huang"
      },
      {
        "name" : "Lu Chen"
      },
      {
        "name" : "Yunjun Gao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11950v1",
    "title" : "Privacy Ethics Alignment in AI (PEA-AI): A Stakeholder-Centric Based\n  Framework for Ethcial AI",
    "summary" : "The increasing integration of Artificial Intelligence (AI) in digital\necosystems has reshaped privacy dynamics, particularly for young digital\ncitizens navigating data-driven environments. This study explores evolving\nprivacy concerns across three key stakeholder groups, digital citizens (ages\n16-19), parents, educators, and AI professionals, and assesses differences in\ndata ownership, trust, transparency, parental mediation, education, and\nrisk-benefit perceptions. Employing a grounded theory methodology, this\nresearch synthesizes insights from 482 participants through structured surveys,\nqualitative interviews, and focus groups. The findings reveal distinct privacy\nexpectations- Young users emphasize autonomy and digital freedom, while parents\nand educators advocate for regulatory oversight and AI literacy programs. AI\nprofessionals, in contrast, prioritize the balance between ethical system\ndesign and technological efficiency. The data further highlights gaps in AI\nliteracy and transparency, emphasizing the need for comprehensive,\nstakeholder-driven privacy frameworks that accommodate diverse user needs.\nUsing comparative thematic analysis, this study identifies key tensions in\nprivacy governance and develops the novel Privacy-Ethics Alignment in AI\n(PEA-AI) model, which structures privacy decision-making as a dynamic\nnegotiation between stakeholders. By systematically analyzing themes such as\ntransparency, user control, risk perception, and parental mediation, this\nresearch provides a scalable, adaptive foundation for AI governance, ensuring\nthat privacy protections evolve alongside emerging AI technologies and\nyouth-centric digital interactions.",
    "updated" : "2025-03-15T01:42:45Z",
    "published" : "2025-03-15T01:42:45Z",
    "authors" : [
      {
        "name" : "Ankur Barthwal"
      },
      {
        "name" : "Molly Campbell"
      },
      {
        "name" : "Ajay Kumar Shrestha"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11947v1",
    "title" : "Ethical AI for Young Digital Citizens: A Call to Action on Privacy\n  Governance",
    "summary" : "The rapid expansion of Artificial Intelligence (AI) in digital platforms used\nby youth has created significant challenges related to privacy, autonomy, and\ndata protection. While AI-driven personalization offers enhanced user\nexperiences, it often operates without clear ethical boundaries, leaving young\nusers vulnerable to data exploitation and algorithmic biases. This paper\npresents a call to action for ethical AI governance, advocating for a\nstructured framework that ensures youth-centred privacy protections,\ntransparent data practices, and regulatory oversight. We outline key areas\nrequiring urgent intervention, including algorithmic transparency, privacy\neducation, parental data-sharing ethics, and accountability measures. Through\nthis approach, we seek to empower youth with greater control over their digital\nidentities and propose actionable strategies for policymakers, AI developers,\nand educators to build a fairer and more accountable AI ecosystem.",
    "updated" : "2025-03-15T01:35:56Z",
    "published" : "2025-03-15T01:35:56Z",
    "authors" : [
      {
        "name" : "Austin Shouli"
      },
      {
        "name" : "Ankur Barthwal"
      },
      {
        "name" : "Molly Campbell"
      },
      {
        "name" : "Ajay Kumar Shrestha"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11920v1",
    "title" : "Practical Implications of Implementing Local Differential Privacy for\n  Smart grids",
    "summary" : "Recent smart grid advancements enable near-realtime reporting of electricity\nconsumption, raising concerns about consumer privacy. Differential privacy (DP)\nhas emerged as a viable privacy solution, where a calculated amount of noise is\nadded to the data by a trusted third party, or individual users perturb their\ninformation locally, and only send the randomized data to an aggregator for\nanalysis safeguarding users and aggregators privacy. However, the practical\nimplementation of a Local DP-based (LDP) privacy model for smart grids has its\nown challenges. In this paper, we discuss the challenges of implementing an\nLDP-based model for smart grids. We compare existing LDP mechanisms in smart\ngrids for privacy preservation of numerical data and discuss different methods\nfor selecting privacy parameters in the existing literature, their limitations\nand the non-existence of an optimal method for selecting the privacy\nparameters. We also discuss the challenges of translating theoretical models of\nLDP into a practical setting for smart grids for different utility functions,\nthe impact of the size of data set on privacy and accuracy, and vulnerability\nof LDP-based smart grids to manipulation attacks. Finally, we discuss future\ndirections in research for better practical applications in LDP based models\nfor smart grids.",
    "updated" : "2025-03-14T23:11:46Z",
    "published" : "2025-03-14T23:11:46Z",
    "authors" : [
      {
        "name" : "Khadija Hafeez"
      },
      {
        "name" : "Mubashir Husain Rehmani"
      },
      {
        "name" : "Sumita Mishra"
      },
      {
        "name" : "Donna OShea"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11850v1",
    "title" : "Local Pan-Privacy for Federated Analytics",
    "summary" : "Pan-privacy was proposed by Dwork et al. as an approach to designing a\nprivate analytics system that retains its privacy properties in the face of\nintrusions that expose the system's internal state. Motivated by federated\ntelemetry applications, we study local pan-privacy, where privacy should be\nretained under repeated unannounced intrusions on the local state. We consider\nthe problem of monitoring the count of an event in a federated system, where\nevent occurrences on a local device should be hidden even from an intruder on\nthat device. We show that under reasonable constraints, the goal of providing\ninformation-theoretic differential privacy under intrusion is incompatible with\ncollecting telemetry information. We then show that this problem can be solved\nin a scalable way using standard cryptographic primitives.",
    "updated" : "2025-03-14T20:18:33Z",
    "published" : "2025-03-14T20:18:33Z",
    "authors" : [
      {
        "name" : "Vitaly Feldman"
      },
      {
        "name" : "Audra McMillan"
      },
      {
        "name" : "Guy N. Rothblum"
      },
      {
        "name" : "Kunal Talwar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11711v1",
    "title" : "Privacy-Preserved Automated Scoring using Federated Learning for\n  Educational Research",
    "summary" : "Data privacy remains a critical concern in educational research,\nnecessitating Institutional Review Board (IRB) certification and stringent data\nhandling protocols to ensure compliance with ethical standards. Traditional\napproaches rely on anonymization and controlled data-sharing mechanisms to\nfacilitate research while mitigating privacy risks. However, these methods\nstill involve direct access to raw student data, posing potential\nvulnerabilities and being time-consuming. This study proposes a federated\nlearning (FL) framework for automatic scoring in educational assessments,\neliminating the need to share raw data. Our approach leverages client-side\nmodel training, where student responses are processed locally on edge devices,\nand only optimized model parameters are shared with a central aggregation\nserver. To effectively aggregate heterogeneous model updates, we introduce an\nadaptive weighted averaging strategy, which dynamically adjusts weight\ncontributions based on client-specific learning characteristics. This method\nensures robust model convergence while preserving privacy. We evaluate our\nframework using assessment data from nine middle schools, comparing the\naccuracy of federated learning-based scoring models with traditionally trained\ncentralized models. A statistical significance test (paired t-test, $t(8) =\n2.29, p = 0.051$) confirms that the accuracy difference between the two\napproaches is not statistically significant, demonstrating that federated\nlearning achieves comparable performance while safeguarding student data.\nFurthermore, our method significantly reduces data collection, processing, and\ndeployment overhead, accelerating the adoption of AI-driven educational\nassessments in a privacy-compliant manner.",
    "updated" : "2025-03-12T19:06:25Z",
    "published" : "2025-03-12T19:06:25Z",
    "authors" : [
      {
        "name" : "Ehsan Latif"
      },
      {
        "name" : "Xiaoming Zhai"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.13872v1",
    "title" : "Empirical Calibration and Metric Differential Privacy in Language Models",
    "summary" : "NLP models trained with differential privacy (DP) usually adopt the DP-SGD\nframework, and privacy guarantees are often reported in terms of the privacy\nbudget $\\epsilon$. However, $\\epsilon$ does not have any intrinsic meaning, and\nit is generally not possible to compare across variants of the framework. Work\nin image processing has therefore explored how to empirically calibrate noise\nacross frameworks using Membership Inference Attacks (MIAs). However, this kind\nof calibration has not been established for NLP. In this paper, we show that\nMIAs offer little help in calibrating privacy, whereas reconstruction attacks\nare more useful. As a use case, we define a novel kind of directional privacy\nbased on the von Mises-Fisher (VMF) distribution, a metric DP mechanism that\nperturbs angular distance rather than adding (isotropic) Gaussian noise, and\napply this to NLP architectures. We show that, even though formal guarantees\nare incomparable, empirical privacy calibration reveals that each mechanism has\ndifferent areas of strength with respect to utility-privacy trade-offs.",
    "updated" : "2025-03-18T03:52:12Z",
    "published" : "2025-03-18T03:52:12Z",
    "authors" : [
      {
        "name" : "Pedro Faustini"
      },
      {
        "name" : "Natasha Fernandes"
      },
      {
        "name" : "Annabelle McIver"
      },
      {
        "name" : "Mark Dras"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.13816v1",
    "title" : "MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple\n  Depth Views in Multi-Room Environments",
    "summary" : "We introduce a novel diffusion-based approach for generating\nprivacy-preserving digital twins of multi-room indoor environments from depth\nimages only. Central to our approach is a novel Multi-view Overlapped Scene\nAlignment with Implicit Consistency (MOSAIC) model that explicitly considers\ncross-view dependencies within the same scene in the probabilistic sense.\nMOSAIC operates through a novel inference-time optimization that avoids error\naccumulation common in sequential or single-room constraint in panorama-based\napproaches. MOSAIC scales to complex scenes with zero extra training and\nprovably reduces the variance during denoising processes when more overlapping\nviews are added, leading to improved generation quality. Experiments show that\nMOSAIC outperforms state-of-the-art baselines on image fidelity metrics in\nreconstructing complex multi-room environments. Project page is available at:\nhttps://mosaic-cmubig.github.io",
    "updated" : "2025-03-18T01:50:57Z",
    "published" : "2025-03-18T01:50:57Z",
    "authors" : [
      {
        "name" : "Zhixuan Liu"
      },
      {
        "name" : "Haokun Zhu"
      },
      {
        "name" : "Rui Chen"
      },
      {
        "name" : "Jonathan Francis"
      },
      {
        "name" : "Soonmin Hwang"
      },
      {
        "name" : "Ji Zhang"
      },
      {
        "name" : "Jean Oh"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.13550v1",
    "title" : "Towards Privacy-Preserving Data-Driven Education: The Potential of\n  Federated Learning",
    "summary" : "The increasing adoption of data-driven applications in education such as in\nlearning analytics and AI in education has raised significant privacy and data\nprotection concerns. While these challenges have been widely discussed in\nprevious works, there are still limited practical solutions. Federated learning\nhas recently been discoursed as a promising privacy-preserving technique, yet\nits application in education remains scarce. This paper presents an\nexperimental evaluation of federated learning for educational data prediction,\ncomparing its performance to traditional non-federated approaches. Our findings\nindicate that federated learning achieves comparable predictive accuracy.\nFurthermore, under adversarial attacks, federated learning demonstrates greater\nresilience compared to non-federated settings. We summarise that our results\nreinforce the value of federated learning as a potential approach for balancing\npredictive performance and privacy in educational contexts.",
    "updated" : "2025-03-16T14:37:32Z",
    "published" : "2025-03-16T14:37:32Z",
    "authors" : [
      {
        "name" : "Mohammad Khalil"
      },
      {
        "name" : "Ronas Shakya"
      },
      {
        "name" : "Qinyi Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.15287v1",
    "title" : "Distributed Generalized Linear Models: A Privacy-Preserving Approach",
    "summary" : "This paper presents a novel approach to classical linear regression, enabling\nmodel computation from data streams or in a distributed setting while\npreserving data privacy in federated environments. We extend this framework to\ngeneralized linear models (GLMs), ensuring scalability and adaptability to\ndiverse data distributions while maintaining privacy-preserving properties. To\nassess the effectiveness of our approach, we conduct numerical studies on both\nsimulated and real datasets, comparing our method with conventional maximum\nlikelihood estimation for GLMs using iteratively reweighted least squares. Our\nresults demonstrate the advantages of the proposed method in distributed and\nfederated settings.",
    "updated" : "2025-03-19T15:07:41Z",
    "published" : "2025-03-19T15:07:41Z",
    "authors" : [
      {
        "name" : "Daniel Tinoco"
      },
      {
        "name" : "Raquel Menezes"
      },
      {
        "name" : "Carlos Baquero"
      }
    ],
    "categories" : [
      "stat.CO",
      "cs.DC",
      "62J12 (Primary) 68U99 (Secondary)",
      "G.3; C.2.4"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.15238v1",
    "title" : "Your Signal, Their Data: An Empirical Privacy Analysis of\n  Wireless-scanning SDKs in Android",
    "summary" : "Mobile apps frequently use Bluetooth Low Energy (BLE) and WiFi scanning\npermissions to discover nearby devices like peripherals and connect to WiFi\nAccess Points (APs). However, wireless interfaces also serve as a covert proxy\nfor geolocation data, enabling continuous user tracking and profiling. This\nincludes technologies like BLE beacons, which are BLE devices broadcasting\nunique identifiers to determine devices' indoor physical locations; such\nbeacons are easily found in shopping centres. Despite the widespread use of\nwireless scanning APIs and their potential for privacy abuse, the interplay\nbetween commercial mobile SDKs with wireless sensing and beaconing technologies\nremains largely unexplored. In this work, we conduct the first systematic\nanalysis of 52 wireless-scanning SDKs, revealing their data collection\npractices and privacy risks. We develop a comprehensive analysis pipeline that\nenables us to detect beacon scanning capabilities, inject wireless events to\ntrigger app behaviors, and monitor runtime execution on instrumented devices.\nOur findings show that 86% of apps integrating these SDKs collect at least one\nsensitive data type, including device and user identifiers such as AAID, email,\nalong with GPS coordinates, WiFi and Bluetooth scan results. We uncover\nwidespread SDK-to-SDK data sharing and evidence of ID bridging, where\npersistent and resettable identifiers are shared and synchronized within SDKs\nembedded in applications to potentially construct detailed mobility profiles,\ncompromising user anonymity and enabling long-term tracking. We provide\nevidence of key actors engaging in these practices and conclude by proposing\nmitigation strategies such as stronger SDK sandboxing, stricter enforcement of\nplatform policies, and improved transparency mechanisms to limit unauthorized\ntracking.",
    "updated" : "2025-03-19T14:15:02Z",
    "published" : "2025-03-19T14:15:02Z",
    "authors" : [
      {
        "name" : "Aniketh Girish"
      },
      {
        "name" : "Joel Reardon"
      },
      {
        "name" : "Juan Tapiador"
      },
      {
        "name" : "Srdjan Matic"
      },
      {
        "name" : "Narseo Vallina-Rodriguez"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.15015v1",
    "title" : "OFL: Opportunistic Federated Learning for Resource-Heterogeneous and\n  Privacy-Aware Devices",
    "summary" : "Efficient and secure federated learning (FL) is a critical challenge for\nresource-limited devices, especially mobile devices. Existing secure FL\nsolutions commonly incur significant overhead, leading to a contradiction\nbetween efficiency and security. As a result, these two concerns are typically\naddressed separately. This paper proposes Opportunistic Federated Learning\n(OFL), a novel FL framework designed explicitly for resource-heterogenous and\nprivacy-aware FL devices, solving efficiency and security problems jointly. OFL\noptimizes resource utilization and adaptability across diverse devices by\nadopting a novel hierarchical and asynchronous aggregation strategy. OFL\nprovides strong security by introducing a differentially private and\nopportunistic model updating mechanism for intra-cluster model aggregation and\nan advanced threshold homomorphic encryption scheme for inter-cluster\naggregation. Moreover, OFL secures global model aggregation by implementing\npoisoning attack detection using frequency analysis while keeping models\nencrypted. We have implemented OFL in a real-world testbed and evaluated OFL\ncomprehensively. The evaluation results demonstrate that OFL achieves\nsatisfying model performance and improves efficiency and security,\noutperforming existing solutions.",
    "updated" : "2025-03-19T09:12:47Z",
    "published" : "2025-03-19T09:12:47Z",
    "authors" : [
      {
        "name" : "Yunlong Mao"
      },
      {
        "name" : "Mingyang Niu"
      },
      {
        "name" : "Ziqin Dang"
      },
      {
        "name" : "Chengxi Li"
      },
      {
        "name" : "Hanning Xia"
      },
      {
        "name" : "Yuejuan Zhu"
      },
      {
        "name" : "Haoyu Bian"
      },
      {
        "name" : "Yuan Zhang"
      },
      {
        "name" : "Jingyu Hua"
      },
      {
        "name" : "Sheng Zhong"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.14877v1",
    "title" : "Synthesizing Grid Data with Cyber Resilience and Privacy Guarantees",
    "summary" : "Differential privacy (DP) provides a principled approach to synthesizing data\n(e.g., loads) from real-world power systems while limiting the exposure of\nsensitive information. However, adversaries may exploit synthetic data to\ncalibrate cyberattacks on the source grids. To control these risks, we propose\nnew DP algorithms for synthesizing data that provide the source grids with both\ncyber resilience and privacy guarantees. The algorithms incorporate both normal\noperation and attack optimization models to balance the fidelity of synthesized\ndata and cyber resilience. The resulting post-processing optimization is\nreformulated as a robust optimization problem, which is compatible with the\nexponential mechanism of DP to moderate its computational burden.",
    "updated" : "2025-03-19T04:11:33Z",
    "published" : "2025-03-19T04:11:33Z",
    "authors" : [
      {
        "name" : "Shengyang Wu"
      },
      {
        "name" : "Vladimir Dvorkin"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.14539v1",
    "title" : "Ethical Implications of AI in Data Collection: Balancing Innovation with\n  Privacy",
    "summary" : "This article examines the ethical and legal implications of artificial\nintelligence (AI) driven data collection, focusing on developments from 2023 to\n2024. It analyzes recent advancements in AI technologies and their impact on\ndata collection practices across various sectors. The study compares regulatory\napproaches in the European Union, the United States, and China, highlighting\nthe challenges in creating a globally harmonized framework for AI governance.\nKey ethical issues, including informed consent, algorithmic bias, and privacy\nprotection, are critically assessed in the context of increasingly\nsophisticated AI systems. The research explores case studies in healthcare,\nfinance, and smart cities to illustrate the practical challenges of AI\nimplementation. It evaluates the effectiveness of current legal frameworks and\nproposes solutions encompassing legal and policy recommendations, technical\nsafeguards, and ethical frameworks. The article emphasizes the need for\nadaptive governance and international cooperation to address the global nature\nof AI development while balancing innovation with the protection of individual\nrights and societal values.",
    "updated" : "2025-03-17T14:15:59Z",
    "published" : "2025-03-17T14:15:59Z",
    "authors" : [
      {
        "name" : "Shahmar Mirishli"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.16251v1",
    "title" : "RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning\n  by Balancing Privacy, Fairness and Utility in Autonomous Vehicles",
    "summary" : "Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to\nenhance perception models while preserving privacy. However, existing FL\nframeworks struggle to balance privacy, fairness, and robustness, leading to\nperformance disparities across demographic groups. Privacy-preserving\ntechniques like differential privacy mitigate data leakage risks but worsen\nfairness by restricting access to sensitive attributes needed for bias\ncorrection. This work explores the trade-off between privacy and fairness in\nFL-based object detection for AVs and introduces RESFL, an integrated solution\noptimizing both. RESFL incorporates adversarial privacy disentanglement and\nuncertainty-guided fairness-aware aggregation. The adversarial component uses a\ngradient reversal layer to remove sensitive attributes, reducing privacy risks\nwhile maintaining fairness. The uncertainty-aware aggregation employs an\nevidential neural network to weight client updates adaptively, prioritizing\ncontributions with lower fairness disparities and higher confidence. This\nensures robust and equitable FL model updates. We evaluate RESFL on the FACET\ndataset and CARLA simulator, assessing accuracy, fairness, privacy resilience,\nand robustness under varying conditions. RESFL improves detection accuracy,\nreduces fairness disparities, and lowers privacy attack success rates while\ndemonstrating superior robustness to adversarial conditions compared to other\napproaches.",
    "updated" : "2025-03-20T15:46:03Z",
    "published" : "2025-03-20T15:46:03Z",
    "authors" : [
      {
        "name" : "Dawood Wasif"
      },
      {
        "name" : "Terrence J. Moore"
      },
      {
        "name" : "Jin-Hee Cho"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CV",
      "cs.DC",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.16233v1",
    "title" : "Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated\n  Learning: A Step Towards Responsible AI",
    "summary" : "Federated Learning (FL) enables collaborative machine learning while\npreserving data privacy but struggles to balance privacy preservation (PP) and\nfairness. Techniques like Differential Privacy (DP), Homomorphic Encryption\n(HE), and Secure Multi-Party Computation (SMC) protect sensitive data but\nintroduce trade-offs. DP enhances privacy but can disproportionately impact\nunderrepresented groups, while HE and SMC mitigate fairness concerns at the\ncost of computational overhead. This work explores the privacy-fairness\ntrade-offs in FL under IID (Independent and Identically Distributed) and\nnon-IID data distributions, benchmarking q-FedAvg, q-MAML, and Ditto on diverse\ndatasets. Our findings highlight context-dependent trade-offs and offer\nguidelines for designing FL systems that uphold responsible AI principles,\nensuring fairness, privacy, and equitable real-world applications.",
    "updated" : "2025-03-20T15:31:01Z",
    "published" : "2025-03-20T15:31:01Z",
    "authors" : [
      {
        "name" : "Dawood Wasif"
      },
      {
        "name" : "Dian Chen"
      },
      {
        "name" : "Sindhuja Madabushi"
      },
      {
        "name" : "Nithin Alluru"
      },
      {
        "name" : "Terrence J. Moore"
      },
      {
        "name" : "Jin-Hee Cho"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.15972v1",
    "title" : "TVineSynth: A Truncated C-Vine Copula Generator of Synthetic Tabular\n  Data to Balance Privacy and Utility",
    "summary" : "We propose TVineSynth, a vine copula based synthetic tabular data generator,\nwhich is designed to balance privacy and utility, using the vine tree structure\nand its truncation to do the trade-off. Contrary to synthetic data generators\nthat achieve DP by globally adding noise, TVineSynth performs a controlled\napproximation of the estimated data generating distribution, so that it does\nnot suffer from poor utility of the resulting synthetic data for downstream\nprediction tasks. TVineSynth introduces a targeted bias into the vine copula\nmodel that, combined with the specific tree structure of the vine, causes the\nmodel to zero out privacy-leaking dependencies while relying on those that are\nbeneficial for utility. Privacy is here measured with membership (MIA) and\nattribute inference attacks (AIA). Further, we theoretically justify how the\nconstruction of TVineSynth ensures AIA privacy under a natural privacy measure\nfor continuous sensitive attributes. When compared to competitor models, with\nand without DP, on simulated and on real-world data, TVineSynth achieves a\nsuperior privacy-utility balance.",
    "updated" : "2025-03-20T09:16:02Z",
    "published" : "2025-03-20T09:16:02Z",
    "authors" : [
      {
        "name" : "Elisabeth Griesbauer"
      },
      {
        "name" : "Claudia Czado"
      },
      {
        "name" : "Arnoldo Frigessi"
      },
      {
        "name" : "Ingrid Hobæk Haff"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.15966v1",
    "title" : "Privacy-Preserving Utilization of Distribution System Flexibility for\n  Enhanced TSO-DSO Interoperability: A Novel Machine Learning-Based Optimal\n  Power Flow Approach",
    "summary" : "Due to the transformation of the power system, the effective use of\nflexibility from the distribution system (DS) is becoming crucial for efficient\nnetwork management. Leveraging this flexibility requires interoperability among\nstakeholders, including Transmission System Operators (TSOs) and Distribution\nSystem Operators (DSOs). However, data privacy concerns among stakeholders\npresent significant challenges for utilizing this flexibility effectively. To\naddress these challenges, we propose a machine learning (ML)-based method in\nwhich the technical constraints of the DSs are represented by ML models trained\nexclusively on non-sensitive data. Using these models, the TSO can solve the\noptimal power flow (OPF) problem and directly determine the dispatch of\nflexibility-providing units (FPUs), in our case, distributed generators (DGs),\nin a single round of communication. To achieve this, we introduce a novel\nneural network (NN) architecture specifically designed to efficiently represent\nthe feasible region of the DSs, ensuring computational effectiveness.\nFurthermore, we incorporate various PQ charts rather than idealized ones,\ndemonstrating that the proposed method is adaptable to a wide range of FPU\ncharacteristics. To assess the effectiveness of the proposed method, we\nbenchmark it against the standard AC-OPF on multiple DSs with meshed\nconnections and multiple points of common coupling (PCCs) with varying voltage\nmagnitudes. The numerical results indicate that the proposed method achieves\nperformant results while prioritizing data privacy. Additionally, since this\nmethod directly determines the dispatch of FPUs, it eliminates the need for an\nadditional disaggregation step. By representing the DSs technical constraints\nthrough ML models trained exclusively on non-sensitive data, the transfer of\nsensitive information between stakeholders is prevented.",
    "updated" : "2025-03-20T09:08:54Z",
    "published" : "2025-03-20T09:08:54Z",
    "authors" : [
      {
        "name" : "Burak Dindar"
      },
      {
        "name" : "Can Berk Saner"
      },
      {
        "name" : "Hüseyin K. Çakmak"
      },
      {
        "name" : "Veit Hagenmeyer"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.15870v1",
    "title" : "FedSAF: A Federated Learning Framework for Enhanced Gastric Cancer\n  Detection and Privacy Preservation",
    "summary" : "Gastric cancer is one of the most commonly diagnosed cancers and has a high\nmortality rate. Due to limited medical resources, developing machine learning\nmodels for gastric cancer recognition provides an efficient solution for\nmedical institutions. However, such models typically require large sample sizes\nfor training and testing, which can challenge patient privacy. Federated\nlearning offers an effective alternative by enabling model training across\nmultiple institutions without sharing sensitive patient data. This paper\naddresses the limited sample size of publicly available gastric cancer data\nwith a modified data processing method. This paper introduces FedSAF, a novel\nfederated learning algorithm designed to improve the performance of existing\nmethods, particularly in non-independent and identically distributed (non-IID)\ndata scenarios. FedSAF incorporates attention-based message passing and the\nFisher Information Matrix to enhance model accuracy, while a model splitting\nfunction reduces computation and transmission costs. Hyperparameter tuning and\nablation studies demonstrate the effectiveness of this new algorithm, showing\nimprovements in test accuracy on gastric cancer datasets, with FedSAF\noutperforming existing federated learning methods like FedAMP, FedAvg, and\nFedProx. The framework's robustness and generalization ability were further\nvalidated across additional datasets (SEED, BOT, FashionMNIST, and CIFAR-10),\nachieving high performance in diverse environments.",
    "updated" : "2025-03-20T05:48:48Z",
    "published" : "2025-03-20T05:48:48Z",
    "authors" : [
      {
        "name" : "Yuxin Miao"
      },
      {
        "name" : "Xinyuan Yang"
      },
      {
        "name" : "Hongda Fan"
      },
      {
        "name" : "Yichun Li"
      },
      {
        "name" : "Yishu Hong"
      },
      {
        "name" : "Xiechen Guo"
      },
      {
        "name" : "Ali Braytee"
      },
      {
        "name" : "Weidong Huang"
      },
      {
        "name" : "Ali Anaissi"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.15818v1",
    "title" : "Computation-Efficient and Recognition-Friendly 3D Point Cloud Privacy\n  Protection",
    "summary" : "3D point cloud has been widely used in applications such as self-driving\ncars, robotics, CAD models, etc. To the best of our knowledge, these\napplications raised the issue of privacy leakage in 3D point clouds, which has\nnot been studied well. Different from the 2D image privacy, which is related to\ntexture and 2D geometric structure, the 3D point cloud is texture-less and only\nrelevant to 3D geometric structure. In this work, we defined the 3D point cloud\nprivacy problem and proposed an efficient privacy-preserving framework named\nPointFlowGMM that can support downstream classification and segmentation tasks\nwithout seeing the original data. Using a flow-based generative model, the\npoint cloud is projected into a latent Gaussian mixture distributed subspace.\nWe further designed a novel angular similarity loss to obfuscate the original\ngeometric structure and reduce the model size from 767MB to 120MB without a\ndecrease in recognition performance. The projected point cloud in the latent\nspace is orthogonally rotated randomly to further protect the original\ngeometric structure, the class-to-class relationship is preserved after\nrotation, thus, the protected point cloud can support the recognition task. We\nevaluated our model on multiple datasets and achieved comparable recognition\nresults on encrypted point clouds compared to the original point clouds.",
    "updated" : "2025-03-20T03:09:44Z",
    "published" : "2025-03-20T03:09:44Z",
    "authors" : [
      {
        "name" : "Haotian Ma"
      },
      {
        "name" : "Lin Gu"
      },
      {
        "name" : "Siyi Wu"
      },
      {
        "name" : "Yingying Zhu"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.15550v1",
    "title" : "Zero-Knowledge Federated Learning: A New Trustworthy and\n  Privacy-Preserving Distributed Learning Paradigm",
    "summary" : "Federated Learning (FL) has emerged as a promising paradigm in distributed\nmachine learning, enabling collaborative model training while preserving data\nprivacy. However, despite its many advantages, FL still contends with\nsignificant challenges -- most notably regarding security and trust.\nZero-Knowledge Proofs (ZKPs) offer a potential solution by establishing trust\nand enhancing system integrity throughout the FL process. Although several\nstudies have explored ZKP-based FL (ZK-FL), a systematic framework and\ncomprehensive analysis are still lacking. This article makes two key\ncontributions. First, we propose a structured ZK-FL framework that categorizes\nand analyzes the technical roles of ZKPs across various FL stages and tasks.\nSecond, we introduce a novel algorithm, Verifiable Client Selection FL\n(Veri-CS-FL), which employs ZKPs to refine the client selection process. In\nVeri-CS-FL, participating clients generate verifiable proofs for the\nperformance metrics of their local models and submit these concise proofs to\nthe server for efficient verification. The server then selects clients with\nhigh-quality local models for uploading, subsequently aggregating the\ncontributions from these selected clients. By integrating ZKPs, Veri-CS-FL not\nonly ensures the accuracy of performance metrics but also fortifies trust among\nparticipants while enhancing the overall efficiency and security of FL systems.",
    "updated" : "2025-03-18T06:21:08Z",
    "published" : "2025-03-18T06:21:08Z",
    "authors" : [
      {
        "name" : "Yuxin Jin"
      },
      {
        "name" : "Taotao Wang"
      },
      {
        "name" : "Qing Yang"
      },
      {
        "name" : "Long Shi"
      },
      {
        "name" : "Shengli Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.15548v1",
    "title" : "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
    "summary" : "The widespread adoption of Retrieval-Augmented Generation (RAG) systems in\nreal-world applications has heightened concerns about the confidentiality and\nintegrity of their proprietary knowledge bases. These knowledge bases, which\nplay a critical role in enhancing the generative capabilities of Large Language\nModels (LLMs), are increasingly vulnerable to breaches that could compromise\nsensitive information. To address these challenges, this paper proposes an\nadvanced encryption methodology designed to protect RAG systems from\nunauthorized access and data leakage. Our approach encrypts both textual\ncontent and its corresponding embeddings prior to storage, ensuring that all\ndata remains securely encrypted. This mechanism restricts access to authorized\nentities with the appropriate decryption keys, thereby significantly reducing\nthe risk of unintended data exposure. Furthermore, we demonstrate that our\nencryption strategy preserves the performance and functionality of RAG\npipelines, ensuring compatibility across diverse domains and applications. To\nvalidate the robustness of our method, we provide comprehensive security proofs\nthat highlight its resilience against potential threats and vulnerabilities.\nThese proofs also reveal limitations in existing approaches, which often lack\nrobustness, adaptability, or reliance on open-source models. Our findings\nsuggest that integrating advanced encryption techniques into the design and\ndeployment of RAG systems can effectively enhance privacy safeguards. This\nresearch contributes to the ongoing discourse on improving security measures\nfor AI-driven services and advocates for stricter data protection standards\nwithin RAG architectures.",
    "updated" : "2025-03-17T07:45:05Z",
    "published" : "2025-03-17T07:45:05Z",
    "authors" : [
      {
        "name" : "Pengcheng Zhou"
      },
      {
        "name" : "Yinglun Feng"
      },
      {
        "name" : "Zhongliang Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.17011v1",
    "title" : "Privacy Enhanced QKD Networks: Zero Trust Relay Architecture based on\n  Homomorphic Encryption",
    "summary" : "Quantum key distribution (QKD) enables unconditionally secure symmetric key\nexchange between parties. However, terrestrial fibre-optic links face inherent\ndistance constraints due to quantum signal degradation. Traditional solutions\nto overcome these limits rely on trusted relay nodes, which perform\nintermediate re-encryption of keys using one-time pad (OTP) encryption. This\napproach, however, exposes keys as plaintext at each relay, requiring\nsignificant trust and stringent security controls at every intermediate node.\nThese \"trusted\" relays become a security liability if compromised.\n  To address this issue, we propose a zero-trust relay design that applies\nfully homomorphic encryption (FHE) to perform intermediate OTP re-encryption\nwithout exposing plaintext keys, effectively mitigating the risks associated\nwith potentially compromised or malicious relay nodes. Additionally, the\narchitecture enhances crypto-agility by incorporating external quantum random\nnumber generators, thus decoupling key generation from specific QKD hardware\nand reducing vulnerabilities tied to embedded key-generation modules.\n  The solution is designed with the existing European Telecommunication\nStandards Institute (ETSI) QKD standards in mind, enabling straightforward\nintegration into current infrastructures. Its feasibility has been successfully\ndemonstrated through a hybrid network setup combining simulated and\ncommercially available QKD equipment. The proposed zero-trust architecture thus\nsignificantly advances the scalability and practical security of large-scale\nQKD networks, greatly reducing reliance on fully trusted infrastructure.",
    "updated" : "2025-03-21T10:20:06Z",
    "published" : "2025-03-21T10:20:06Z",
    "authors" : [
      {
        "name" : "Aitor Brazaola-Vicario"
      },
      {
        "name" : "Oscar Lage"
      },
      {
        "name" : "Julen Bernabé-Rodríguez"
      },
      {
        "name" : "Eduardo Jacob"
      },
      {
        "name" : "Jasone Astorga"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.16640v1",
    "title" : "Visualizing Privacy-Relevant Data Flows in Android Applications",
    "summary" : "Android applications collecting data from users must protect it according to\nthe current legal frameworks. Such data protection has become even more\nimportant since in 2018 the European Union rolled out the General Data\nProtection Regulation (GDPR). Since app developers are not legal experts, they\nfind it difficult to integrate privacy-aware practices into source code\ndevelopment. Despite these legal obligations, developers have limited tool\nsupport to reason about data protection throughout their app development\nprocess.\n  This paper explores the use of static program slicing and software\nvisualization to analyze privacy-relevant data flows in Android apps. We\nintroduce SliceViz, a web tool that analyzes an Android app by slicing all\nprivacy-relevant data sources detected in the source code on the back-end. It\nthen helps developers by visualizing these privacy-relevant program slices.\n  We conducted a user study with 12 participants demonstrating that SliceViz\neffectively aids developers in identifying privacy-relevant properties in\nAndroid apps.\n  Our findings indicate that program slicing can be employed to identify and\nreason about privacy-relevant data flows in Android applications. With further\nusability improvements, developers can be better equipped to handle\nprivacy-sensitive information.",
    "updated" : "2025-03-20T18:47:02Z",
    "published" : "2025-03-20T18:47:02Z",
    "authors" : [
      {
        "name" : "Mugdha Khedkar"
      },
      {
        "name" : "Michael Schlichtig"
      },
      {
        "name" : "Santhosh Mohan"
      },
      {
        "name" : "Eric Bodden"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.16516v1",
    "title" : "Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering,\n  Fine-Tuning and Explainability",
    "summary" : "Privacy policies are widely used by digital services and often required for\nlegal purposes. Many machine learning based classifiers have been developed to\nautomate detection of different concepts in a given privacy policy, which can\nhelp facilitate other automated tasks such as producing a more reader-friendly\nsummary and detecting legal compliance issues. Despite the successful\napplications of large language models (LLMs) to many NLP tasks in various\ndomains, there is very little work studying the use of LLMs for automated\nprivacy policy analysis, therefore, if and how LLMs can help automate privacy\npolicy analysis remains under-explored. To fill this research gap, we conducted\na comprehensive evaluation of LLM-based privacy policy concept classifiers,\nemploying both prompt engineering and LoRA (low-rank adaptation) fine-tuning,\non four state-of-the-art (SOTA) privacy policy corpora and taxonomies. Our\nexperimental results demonstrated that combining prompt engineering and\nfine-tuning can make LLM-based classifiers outperform other SOTA methods,\n\\emph{significantly} and \\emph{consistently} across privacy policy\ncorpora/taxonomies and concepts. Furthermore, we evaluated the explainability\nof the LLM-based classifiers using three metrics: completeness, logicality, and\ncomprehensibility. For all three metrics, a score exceeding 91.1\\% was observed\nin our evaluation, indicating that LLMs are not only useful to improve the\nclassification performance, but also to enhance the explainability of detection\nresults.",
    "updated" : "2025-03-16T10:50:31Z",
    "published" : "2025-03-16T10:50:31Z",
    "authors" : [
      {
        "name" : "Yuxin Chen"
      },
      {
        "name" : "Peng Tang"
      },
      {
        "name" : "Weidong Qiu"
      },
      {
        "name" : "Shujun Li"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12220v2",
    "title" : "PA-CFL: Privacy-Adaptive Clustered Federated Learning for\n  Transformer-Based Sales Forecasting on Heterogeneous Retail Data",
    "summary" : "Federated learning (FL) enables retailers to share model parameters for\ndemand forecasting while maintaining privacy. However, heterogeneous data\nacross diverse regions, driven by factors such as varying consumer behavior,\nposes challenges to the effectiveness of federated learning. To tackle this\nchallenge, we propose Privacy-Adaptive Clustered Federated Learning (PA-CFL)\ntailored for demand forecasting on heterogeneous retail data. By leveraging\ndifferential privacy and feature importance distribution, PA-CFL groups\nretailers into distinct ``bubbles'', each forming its own federated learning\nsystem to effectively isolate data heterogeneity. Within each bubble,\nTransformer models are designed to predict local sales for each client. Our\nexperiments demonstrate that PA-CFL significantly surpasses FedAvg and\noutperforms local learning in demand forecasting performance across all\nparticipating clients. Compared to local learning, PA-CFL achieves a 5.4%\nimprovement in R^2, a 69% reduction in RMSE, and a 45% decrease in MAE. Our\napproach enables effective FL through adaptive adjustments to diverse noise\nlevels and the range of clients participating in each bubble. By grouping\nparticipants and proactively filtering out high-risk clients, PA-CFL mitigates\npotential threats to the FL system. The findings demonstrate PA-CFL's ability\nto enhance federated learning in time series prediction tasks with\nheterogeneous data, achieving a balance between forecasting accuracy and\nprivacy preservation in retail applications. Additionally, PA-CFL's capability\nto detect and neutralize poisoned data from clients enhances the system's\nrobustness and reliability.",
    "updated" : "2025-03-21T17:13:19Z",
    "published" : "2025-03-15T18:07:54Z",
    "authors" : [
      {
        "name" : "Yunbo Long"
      },
      {
        "name" : "Liming Xu"
      },
      {
        "name" : "Ge Zheng"
      },
      {
        "name" : "Alexandra Brintrup"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.11950v2",
    "title" : "Privacy Ethics Alignment in AI: A Stakeholder-Centric Based Framework\n  for Ethical AI",
    "summary" : "The increasing integration of Artificial Intelligence (AI) in digital\necosystems has reshaped privacy dynamics, particularly for young digital\ncitizens navigating data-driven environments. This study explores evolving\nprivacy concerns across three key stakeholder groups, digital citizens (ages\n16-19), parents/educators, and AI professionals, and assesses differences in\ndata ownership, trust, transparency, parental mediation, education, and\nrisk-benefit perceptions. Employing a grounded theory methodology, this\nresearch synthesizes insights from 482 participants through structured surveys,\nqualitative interviews, and focus groups. The findings reveal distinct privacy\nexpectations: Young users emphasize autonomy and digital freedom, while parents\nand educators advocate for regulatory oversight and AI literacy programs. AI\nprofessionals, in contrast, prioritize the balance between ethical system\ndesign and technological efficiency. The data further highlights gaps in AI\nliteracy and transparency, emphasizing the need for comprehensive,\nstakeholder-driven privacy frameworks that accommodate diverse user needs.\nUsing comparative thematic analysis, this study identifies key tensions in\nprivacy governance and develops the novel Privacy-Ethics Alignment in AI\n(PEA-AI) model, which structures privacy decision-making as a dynamic\nnegotiation between stakeholders. By systematically analyzing themes such as\ntransparency, user control, risk perception, and parental mediation, this\nresearch provides a scalable, adaptive foundation for AI governance, ensuring\nthat privacy protections evolve alongside emerging AI technologies and\nyouth-centric digital interactions.",
    "updated" : "2025-03-21T00:54:33Z",
    "published" : "2025-03-15T01:42:45Z",
    "authors" : [
      {
        "name" : "Ankur Barthwal"
      },
      {
        "name" : "Molly Campbell"
      },
      {
        "name" : "Ajay Kumar Shrestha"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.18729v1",
    "title" : "Two Types of Data Privacy Controls",
    "summary" : "Users share a vast amount of data while using web and mobile applications.\nMost service providers such as email and social media providers provide users\nwith privacy controls, which aim to give users the means to control what, how,\nwhen, and with whom, users share data. Nevertheless, it is not uncommon to hear\nusers say that they feel they have lost control over their data on the web.\n  This article aims to shed light on the often overlooked difference between\ntwo main types of privacy from a control perspective: privacy between a user\nand other users, and privacy between a user and institutions. We argue why this\ndifference is important and what we need to do from here.",
    "updated" : "2025-03-24T14:37:57Z",
    "published" : "2025-03-24T14:37:57Z",
    "authors" : [
      {
        "name" : "Eman Alashwali"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.18008v1",
    "title" : "Personalized Language Models via Privacy-Preserving Evolutionary Model\n  Merging",
    "summary" : "Personalization in large language models (LLMs) seeks to tailor models to\nindividual user or user group preferences. Prompt-based methods augment queries\nwith user preference information, whereas training-based methods directly\nencode preferences into model parameters for more effective personalization.\nDespite achieving some success in personalizing LLMs, prior methods often fail\nto directly optimize task-specific metrics and lack explicit\nprivacy-preservation mechanisms. To address these limitations, we propose\nPrivacy-Preserving Model Merging via Evolutionary Algorithms (PriME), a novel\napproach to personalization that employs gradient-free methods to directly\noptimize task-specific metrics while preserving user privacy. By incorporating\nprivacy preservation into optimization, PriME produces a personalized module\nthat effectively captures the target user's preferences while minimizing the\nprivacy risks for the users sharing their private information. Experiments on\nthe LaMP benchmark show that PriME outperforms both prompt-based and\ntraining-based methods, achieving up to a 45% performance improvement over the\nprior art. Further analysis shows that PriME achieves a significantly better\nprivacy-utility trade-off, highlighting the potential of evolutionary\napproaches for privacy-preserving LLM personalization.",
    "updated" : "2025-03-23T09:46:07Z",
    "published" : "2025-03-23T09:46:07Z",
    "authors" : [
      {
        "name" : "Kyuyoung Kim"
      },
      {
        "name" : "Jinwoo Shin"
      },
      {
        "name" : "Jaehyung Kim"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.17844v1",
    "title" : "Privacy-Preserving Hamming Distance Computation with Property-Preserving\n  Hashing",
    "summary" : "We study the problem of approximating Hamming distance in sublinear time\nunder property-preserving hashing (PPH), where only hashed representations of\ninputs are available. Building on the threshold evaluation framework of\nFleischhacker, Larsen, and Simkin (EUROCRYPT 2022), we present a sequence of\nconstructions with progressively improved complexity: a baseline binary search\nalgorithm, a refined variant with constant repetition per query, and a novel\nhash design that enables constant-time approximation without oracle access. Our\nresults demonstrate that approximate distance recovery is possible under strong\ncryptographic guarantees, bridging efficiency and security in similarity\nestimation.",
    "updated" : "2025-03-22T19:35:59Z",
    "published" : "2025-03-22T19:35:59Z",
    "authors" : [
      {
        "name" : "Dongfang Zhao"
      }
    ],
    "categories" : [
      "cs.CC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.17553v1",
    "title" : "Autonomous Radiotherapy Treatment Planning Using DOLA: A\n  Privacy-Preserving, LLM-Based Optimization Agent",
    "summary" : "Radiotherapy treatment planning is a complex and time-intensive process,\noften impacted by inter-planner variability and subjective decision-making. To\naddress these challenges, we introduce Dose Optimization Language Agent (DOLA),\nan autonomous large language model (LLM)-based agent designed for optimizing\nradiotherapy treatment plans while rigorously protecting patient privacy. DOLA\nintegrates the LLaMa3.1 LLM directly with a commercial treatment planning\nsystem, utilizing chain-of-thought prompting, retrieval-augmented generation\n(RAG), and reinforcement learning (RL). Operating entirely within secure local\ninfrastructure, this agent eliminates external data sharing. We evaluated DOLA\nusing a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in\n20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and\noptimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations.\nThe 70B model demonstrated significantly improved performance, achieving\napproximately 16.4% higher final scores than the 8B model. The RAG approach\noutperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated\nconvergence, highlighting the synergy of retrieval-based memory and\nreinforcement learning. Optimal temperature hyperparameter analysis identified\n0.4 as providing the best balance between exploration and exploitation. This\nproof of concept study represents the first successful deployment of locally\nhosted LLM agents for autonomous optimization of treatment plans within a\ncommercial radiotherapy planning system. By extending human-machine interaction\nthrough interpretable natural language reasoning, DOLA offers a scalable and\nprivacy-conscious framework, with significant potential for clinical\nimplementation and workflow improvement.",
    "updated" : "2025-03-21T22:01:19Z",
    "published" : "2025-03-21T22:01:19Z",
    "authors" : [
      {
        "name" : "Humza Nusrat"
      },
      {
        "name" : "Bing Luo"
      },
      {
        "name" : "Ryan Hall"
      },
      {
        "name" : "Joshua Kim"
      },
      {
        "name" : "Hassan Bagher-Ebadian"
      },
      {
        "name" : "Anthony Doemer"
      },
      {
        "name" : "Benjamin Movsas"
      },
      {
        "name" : "Kundan Thind"
      }
    ],
    "categories" : [
      "physics.med-ph",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.17428v1",
    "title" : "Would you mind being watched by machines? Privacy concerns in data\n  mining",
    "summary" : "Data mining is not an invasion of privacy because access to data is only by\nmachines, not by people: this is the argument that is investigated here. The\ncurrent importance of this problem is developed in a case study of data mining\nin the USA for counterterrorism and other surveillance purposes. After a\nclarification of the relevant nature of privacy, it is argued that access by\nmachines cannot warrant the access to further information, since the analysis\nwill have to be made either by humans or by machines that understand. It\nconcludes that the current data mining violates the right to privacy and should\nbe subject to the standard legal constraints for access to private information\nby people.",
    "updated" : "2025-03-21T12:01:42Z",
    "published" : "2025-03-21T12:01:42Z",
    "authors" : [
      {
        "name" : "Vincent C. Müller"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.15818v2",
    "title" : "Computation-Efficient and Recognition-Friendly 3D Point Cloud Privacy\n  Protection",
    "summary" : "3D point cloud has been widely used in applications such as self-driving\ncars, robotics, CAD models, etc. To the best of our knowledge, these\napplications raised the issue of privacy leakage in 3D point clouds, which has\nnot been studied well. Different from the 2D image privacy, which is related to\ntexture and 2D geometric structure, the 3D point cloud is texture-less and only\nrelevant to 3D geometric structure. In this work, we defined the 3D point cloud\nprivacy problem and proposed an efficient privacy-preserving framework named\nPointFlowGMM that can support downstream classification and segmentation tasks\nwithout seeing the original data. Using a flow-based generative model, the\npoint cloud is projected into a latent Gaussian mixture distributed subspace.\nWe further designed a novel angular similarity loss to obfuscate the original\ngeometric structure and reduce the model size from 767MB to 120MB without a\ndecrease in recognition performance. The projected point cloud in the latent\nspace is orthogonally rotated randomly to further protect the original\ngeometric structure, the class-to-class relationship is preserved after\nrotation, thus, the protected point cloud can support the recognition task. We\nevaluated our model on multiple datasets and achieved comparable recognition\nresults on encrypted point clouds compared to the original point clouds.",
    "updated" : "2025-03-23T19:45:16Z",
    "published" : "2025-03-20T03:09:44Z",
    "authors" : [
      {
        "name" : "Haotian Ma"
      },
      {
        "name" : "Lin Gu"
      },
      {
        "name" : "Siyi Wu"
      },
      {
        "name" : "Yingying Zhu"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.15550v2",
    "title" : "Zero-Knowledge Federated Learning: A New Trustworthy and\n  Privacy-Preserving Distributed Learning Paradigm",
    "summary" : "Federated Learning (FL) has emerged as a promising paradigm in distributed\nmachine learning, enabling collaborative model training while preserving data\nprivacy. However, despite its many advantages, FL still contends with\nsignificant challenges -- most notably regarding security and trust.\nZero-Knowledge Proofs (ZKPs) offer a potential solution by establishing trust\nand enhancing system integrity throughout the FL process. Although several\nstudies have explored ZKP-based FL (ZK-FL), a systematic framework and\ncomprehensive analysis are still lacking. This article makes two key\ncontributions. First, we propose a structured ZK-FL framework that categorizes\nand analyzes the technical roles of ZKPs across various FL stages and tasks.\nSecond, we introduce a novel algorithm, Verifiable Client Selection FL\n(Veri-CS-FL), which employs ZKPs to refine the client selection process. In\nVeri-CS-FL, participating clients generate verifiable proofs for the\nperformance metrics of their local models and submit these concise proofs to\nthe server for efficient verification. The server then selects clients with\nhigh-quality local models for uploading, subsequently aggregating the\ncontributions from these selected clients. By integrating ZKPs, Veri-CS-FL not\nonly ensures the accuracy of performance metrics but also fortifies trust among\nparticipants while enhancing the overall efficiency and security of FL systems.",
    "updated" : "2025-03-24T03:55:23Z",
    "published" : "2025-03-18T06:21:08Z",
    "authors" : [
      {
        "name" : "Yuxin Jin"
      },
      {
        "name" : "Taotao Wang"
      },
      {
        "name" : "Qing Yang"
      },
      {
        "name" : "Long Shi"
      },
      {
        "name" : "Shengli Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.13816v2",
    "title" : "MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple\n  Depth Views in Multi-Room Environments",
    "summary" : "We introduce a novel diffusion-based approach for generating\nprivacy-preserving digital twins of multi-room indoor environments from depth\nimages only. Central to our approach is a novel Multi-view Overlapped Scene\nAlignment with Implicit Consistency (MOSAIC) model that explicitly considers\ncross-view dependencies within the same scene in the probabilistic sense.\nMOSAIC operates through a novel inference-time optimization that avoids error\naccumulation common in sequential or single-room constraint in panorama-based\napproaches. MOSAIC scales to complex scenes with zero extra training and\nprovably reduces the variance during denoising processes when more overlapping\nviews are added, leading to improved generation quality. Experiments show that\nMOSAIC outperforms state-of-the-art baselines on image fidelity metrics in\nreconstructing complex multi-room environments. Project page is available at:\nhttps://mosaic-cmubig.github.io",
    "updated" : "2025-03-24T04:05:07Z",
    "published" : "2025-03-18T01:50:57Z",
    "authors" : [
      {
        "name" : "Zhixuan Liu"
      },
      {
        "name" : "Haokun Zhu"
      },
      {
        "name" : "Rui Chen"
      },
      {
        "name" : "Jonathan Francis"
      },
      {
        "name" : "Soonmin Hwang"
      },
      {
        "name" : "Ji Zhang"
      },
      {
        "name" : "Jean Oh"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.08085v3",
    "title" : "PRISM: Privacy-Preserving Improved Stochastic Masking for Federated\n  Generative Models",
    "summary" : "Despite recent advancements in federated learning (FL), the integration of\ngenerative models into FL has been limited due to challenges such as high\ncommunication costs and unstable training in heterogeneous data environments.\nTo address these issues, we propose PRISM, a FL framework tailored for\ngenerative models that ensures (i) stable performance in heterogeneous data\ndistributions and (ii) resource efficiency in terms of communication cost and\nfinal model size. The key of our method is to search for an optimal stochastic\nbinary mask for a random network rather than updating the model weights,\nidentifying a sparse subnetwork with high generative performance; i.e., a\n``strong lottery ticket''. By communicating binary masks in a stochastic\nmanner, PRISM minimizes communication overhead. This approach, combined with\nthe utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic\nmoving average aggregation method (MADA) on the server side, facilitates stable\nand strong generative capabilities by mitigating local divergence in FL\nscenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a\nlightweight model without extra pruning or quantization, making it ideal for\nenvironments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and\nCIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining\nprivacy with minimal communication costs. PRISM is the first to successfully\ngenerate images under challenging non-IID and privacy-preserving FL\nenvironments on complex datasets, where previous methods have struggled.",
    "updated" : "2025-03-24T16:34:35Z",
    "published" : "2025-03-11T06:37:54Z",
    "authors" : [
      {
        "name" : "Kyeongkook Seo"
      },
      {
        "name" : "Dong-Jun Han"
      },
      {
        "name" : "Jaejun Yoo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.19872v1",
    "title" : "NickPay, an Auditable, Privacy-Preserving, Nickname-Based Payment System",
    "summary" : "In this paper, we describe the motivation, design, security properties, and a\nprototype implementation of NickPay, a new privacy-preserving yet auditable\npayment system built on top of the Ethereum blockchain platform. NickPay offers\na strong level of privacy to participants and prevents successive payment\ntransfers from being linked to their actual owners.\n  It is providing the transparency that blockchains ensure and at the same\ntime, preserving the possibility for a trusted authority to access sensitive\ninformation, e.g., for audit purposes or compliance with financial regulations.\n  NickPay builds upon the Nicknames for Group Signatures (NGS) scheme, a new\nsigning system based on dynamic ``nicknames'' for signers that extends the\nschemes of group signatures and signatures with flexible public keys.\n  NGS enables identified group members to expose their flexible public keys,\nthus allowing direct and natural applications such as auditable private payment\nsystems, NickPay being a blockchain-based prototype of these.",
    "updated" : "2025-03-25T17:36:54Z",
    "published" : "2025-03-25T17:36:54Z",
    "authors" : [
      {
        "name" : "Guillaume Quispe"
      },
      {
        "name" : "Pierre Jouvelot"
      },
      {
        "name" : "Gerard Memmi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.19819v1",
    "title" : "Domain-incremental White Blood Cell Classification with Privacy-aware\n  Continual Learning",
    "summary" : "White blood cell (WBC) classification plays a vital role in hematology for\ndiagnosing various medical conditions. However, it faces significant challenges\ndue to domain shifts caused by variations in sample sources (e.g., blood or\nbone marrow) and differing imaging conditions across hospitals. Traditional\ndeep learning models often suffer from catastrophic forgetting in such dynamic\nenvironments, while foundation models, though generally robust, experience\nperformance degradation when the distribution of inference data differs from\nthat of the training data. To address these challenges, we propose a generative\nreplay-based Continual Learning (CL) strategy designed to prevent forgetting in\nfoundation models for WBC classification. Our method employs lightweight\ngenerators to mimic past data with a synthetic latent representation to enable\nprivacy-preserving replay. To showcase the effectiveness, we carry out\nextensive experiments with a total of four datasets with different task\nordering and four backbone models including ResNet50, RetCCL, CTransPath, and\nUNI. Experimental results demonstrate that conventional fine-tuning methods\ndegrade performance on previously learned tasks and struggle with domain\nshifts. In contrast, our continual learning strategy effectively mitigates\ncatastrophic forgetting, preserving model performance across varying domains.\nThis work presents a practical solution for maintaining reliable WBC\nclassification in real-world clinical settings, where data distributions\nfrequently evolve.",
    "updated" : "2025-03-25T16:30:58Z",
    "published" : "2025-03-25T16:30:58Z",
    "authors" : [
      {
        "name" : "Pratibha Kumari"
      },
      {
        "name" : "Afshin Bozorgpour"
      },
      {
        "name" : "Daniel Reisenbüchler"
      },
      {
        "name" : "Edgar Jost"
      },
      {
        "name" : "Martina Crysandt"
      },
      {
        "name" : "Christian Matek"
      },
      {
        "name" : "Dorit Merhof"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.19453v1",
    "title" : "Average consensus with resilience and privacy guarantees without losing\n  accuracy",
    "summary" : "This paper addresses the challenge of achieving private and resilient average\nconsensus among a group of discrete-time networked agents without compromising\naccuracy. State-of-the-art solutions to attain privacy and resilient consensus\nentail an explicit trade-off between the two with an implicit compromise on\naccuracy. In contrast, in the present work, we propose a methodology that\navoids trade-offs between privacy, resilience, and accuracy. We design a\nmethodology that, under certain conditions, enables non-faulty agents, i.e.,\nagents complying with the established protocol, to reach average consensus in\nthe presence of faulty agents, while keeping the non-faulty agents' initial\nstates private. For privacy, agents strategically add noise to obscure their\noriginal state, while later withdrawing a function of it to ensure accuracy.\nBesides, and unlikely many consensus methods, our approach does not require\neach agent to compute the left-eigenvector of the dynamics matrix associated\nwith the eigenvalue one. Moreover, the proposed framework has a polynomial time\ncomplexity relative to the number of agents and the maximum quantity of faulty\nagents. Finally, we illustrate our method with examples covering diverse faulty\nagents scenarios.",
    "updated" : "2025-03-25T08:41:12Z",
    "published" : "2025-03-25T08:41:12Z",
    "authors" : [
      {
        "name" : "Guilherme Ramos"
      },
      {
        "name" : "Daniel Silvestre"
      },
      {
        "name" : "André M. H. Teixeira"
      },
      {
        "name" : "Sérgio Pequito"
      }
    ],
    "categories" : [
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.09823v2",
    "title" : "Data Traceability for Privacy Alignment",
    "summary" : "This paper offers a new privacy approach for the growing ecosystem of\nservices -- ranging from open banking to healthcare -- dependent on sensitive\npersonal data sharing between individuals and third parties. While these\nservices offer significant benefits, individuals want control over their data,\ntransparency regarding how their data is used, and accountability from third\nparties for misuse. However, existing legal and technical mechanisms are\ninadequate for supporting these needs. A comprehensive approach to the modern\nprivacy challenges of accountable third-party data sharing requires a closer\nalignment of technical system architecture and legal institutional design. In\norder to achieve this privacy alignment, we extend traditional security threat\nmodeling and analysis to encompass a broader range of privacy notions than has\nbeen typically considered. In particular, we introduce the concept of\ncovert-accountability, which addresses the risk from adversaries that may act\ndishonestly but nevertheless face potential identification and legal\nconsequences. As a concrete instance of this design approach, we present the\nOTrace protocol, designed to provide traceable, accountable, consumer-control\nin third-party data sharing ecosystems. OTrace empowers consumers with the\nknowledge of who has their data, what it is being used for, what consent or\nother legal terms apply, and whom it is being shared with. By applying our\nalignment framework, we demonstrate that OTrace's technical affordances can\nprovide more confident, scalable regulatory oversight when combined with\ncomplementary legal mechanisms.",
    "updated" : "2025-03-24T18:48:39Z",
    "published" : "2025-03-12T20:42:23Z",
    "authors" : [
      {
        "name" : "Kevin Liao"
      },
      {
        "name" : "Shreya Thipireddy"
      },
      {
        "name" : "Daniel Weitzner"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.20464v1",
    "title" : "Modelling Privacy Compliance in Cross-border Data Transfers with\n  Bigraphs",
    "summary" : "Advancements in information technology have led to the sharing of users' data\nacross borders, raising privacy concerns, particularly when destination\ncountries lack adequate protection measures. Regulations like the European\nGeneral Data Protection Regulation (GDPR) govern international data transfers,\nimposing significant fines on companies failing to comply. To achieve\ncompliance, we propose a privacy framework based on Milner's Bigraphical\nReactive Systems (BRSs), a formalism modelling spatial and non-spatial\nrelationships between entities. BRSs evolve over time via user-specified\nrewriting rules, defined algebraically and diagrammatically. In this paper, we\nrely on diagrammatic notations, enabling adoption by end-users and privacy\nexperts without formal modelling backgrounds. The framework comprises\npredefined privacy reaction rules modelling GDPR requirements for international\ndata transfers, properties expressed in Computation Tree Logic (CTL) to\nautomatically verify these requirements with a model checker and sorting\nschemes to statically ensure models are well-formed. We demonstrate the\nframework's applicability by modelling WhatsApp's privacy policies.",
    "updated" : "2025-03-26T11:50:55Z",
    "published" : "2025-03-26T11:50:55Z",
    "authors" : [
      {
        "name" : "Ebtihal Althubiti"
      },
      {
        "name" : "Michele Sevegnani"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.20326v1",
    "title" : "Modality-Independent Brain Lesion Segmentation with Privacy-aware\n  Continual Learning",
    "summary" : "Traditional brain lesion segmentation models for multi-modal MRI are\ntypically tailored to specific pathologies, relying on datasets with predefined\nmodalities. Adapting to new MRI modalities or pathologies often requires\ntraining separate models, which contrasts with how medical professionals\nincrementally expand their expertise by learning from diverse datasets over\ntime. Inspired by this human learning process, we propose a unified\nsegmentation model capable of sequentially learning from multiple datasets with\nvarying modalities and pathologies. Our approach leverages a privacy-aware\ncontinual learning framework that integrates a mixture-of-experts mechanism and\ndual knowledge distillation to mitigate catastrophic forgetting while not\ncompromising performance on newly encountered datasets. Extensive experiments\nacross five diverse brain MRI datasets and four dataset sequences demonstrate\nthe effectiveness of our framework in maintaining a single adaptable model,\ncapable of handling varying hospital protocols, imaging modalities, and disease\ntypes. Compared to widely used privacy-aware continual learning methods such as\nLwF, SI, EWC, and MiB, our method achieves an average Dice score improvement of\napproximately 11%. Our framework represents a significant step toward more\nversatile and practical brain lesion segmentation models, with implementation\navailable at \\href{https://github.com/xmindflow/BrainCL}{GitHub}.",
    "updated" : "2025-03-26T08:53:41Z",
    "published" : "2025-03-26T08:53:41Z",
    "authors" : [
      {
        "name" : "Yousef Sadegheih"
      },
      {
        "name" : "Pratibha Kumari"
      },
      {
        "name" : "Dorit Merhof"
      }
    ],
    "categories" : [
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.21159v1",
    "title" : "Multi-Objective Optimization for Privacy-Utility Balance in\n  Differentially Private Federated Learning",
    "summary" : "Federated learning (FL) enables collaborative model training across\ndistributed clients without sharing raw data, making it a promising approach\nfor privacy-preserving machine learning. However, ensuring differential privacy\n(DP) in FL presents challenges due to the trade-off between model utility and\nprivacy protection. Clipping gradients before aggregation is a common strategy\nto limit privacy loss, but selecting an optimal clipping norm is non-trivial,\nas excessively high values compromise privacy, while overly restrictive\nclipping degrades model performance. In this work, we propose an adaptive\nclipping mechanism that dynamically adjusts the clipping norm using a\nmulti-objective optimization framework. By integrating privacy and utility\nconsiderations into the optimization objective, our approach balances privacy\npreservation with model accuracy. We theoretically analyze the convergence\nproperties of our method and demonstrate its effectiveness through extensive\nexperiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Our results show\nthat adaptive clipping consistently outperforms fixed-clipping baselines,\nachieving improved accuracy under the same privacy constraints. This work\nhighlights the potential of dynamic clipping strategies to enhance\nprivacy-utility trade-offs in differentially private federated learning.",
    "updated" : "2025-03-27T04:57:05Z",
    "published" : "2025-03-27T04:57:05Z",
    "authors" : [
      {
        "name" : "Kanishka Ranaweera"
      },
      {
        "name" : "David Smith"
      },
      {
        "name" : "Pubudu N. Pathirana"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      },
      {
        "name" : "Aruna Seneviratne"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.21154v1",
    "title" : "Federated Learning with Differential Privacy: An Utility-Enhanced\n  Approach",
    "summary" : "Federated learning has emerged as an attractive approach to protect data\nprivacy by eliminating the need for sharing clients' data while reducing\ncommunication costs compared with centralized machine learning algorithms.\nHowever, recent studies have shown that federated learning alone does not\nguarantee privacy, as private data may still be inferred from the uploaded\nparameters to the central server. In order to successfully avoid data leakage,\nadopting differential privacy (DP) in the local optimization process or in the\nlocal update aggregation process has emerged as two feasible ways for achieving\nsample-level or user-level privacy guarantees respectively, in federated\nlearning models. However, compared to their non-private equivalents, these\napproaches suffer from a poor utility. To improve the privacy-utility\ntrade-off, we present a modification to these vanilla differentially private\nalgorithms based on a Haar wavelet transformation step and a novel noise\ninjection scheme that significantly lowers the asymptotic bound of the noise\nvariance. We also present a holistic convergence analysis of our proposed\nalgorithm, showing that our method yields better convergence performance than\nthe vanilla DP algorithms. Numerical experiments on real-world datasets\ndemonstrate that our method outperforms existing approaches in model utility\nwhile maintaining the same privacy guarantees.",
    "updated" : "2025-03-27T04:48:29Z",
    "published" : "2025-03-27T04:48:29Z",
    "authors" : [
      {
        "name" : "Kanishka Ranaweera"
      },
      {
        "name" : "Dinh C. Nguyen"
      },
      {
        "name" : "Pubudu N. Pathirana"
      },
      {
        "name" : "David Smith"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      },
      {
        "name" : "Aruna Seneviratne"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.21071v1",
    "title" : "Purifying Approximate Differential Privacy with Randomized\n  Post-processing",
    "summary" : "We propose a framework to convert $(\\varepsilon, \\delta)$-approximate\nDifferential Privacy (DP) mechanisms into $(\\varepsilon, 0)$-pure DP\nmechanisms, a process we call ``purification''. This algorithmic technique\nleverages randomized post-processing with calibrated noise to eliminate the\n$\\delta$ parameter while preserving utility. By combining the tighter utility\nbounds and computational efficiency of approximate DP mechanisms with the\nstronger guarantees of pure DP, our approach achieves the best of both worlds.\nWe illustrate the applicability of this framework in various settings,\nincluding Differentially Private Empirical Risk Minimization (DP-ERM),\ndata-dependent DP mechanisms such as Propose-Test-Release (PTR), and query\nrelease tasks. To the best of our knowledge, this is the first work to provide\na systematic method for transforming approximate DP into pure DP while\nmaintaining competitive accuracy and computational efficiency.",
    "updated" : "2025-03-27T01:10:40Z",
    "published" : "2025-03-27T01:10:40Z",
    "authors" : [
      {
        "name" : "Yingyu Lin"
      },
      {
        "name" : "Erchi Wang"
      },
      {
        "name" : "Yi-An Ma"
      },
      {
        "name" : "Yu-Xiang Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.21010v1",
    "title" : "Privacy in Immersive Extended Reality: Exploring User Perceptions,\n  Concerns, and Coping Strategies",
    "summary" : "Extended Reality (XR) technology is changing online interactions, but its\ngranular data collection sensors may be more invasive to user privacy than web,\nmobile, and the Internet of Things technologies. Despite an increased interest\nin studying developers' concerns about XR device privacy, user perceptions have\nrarely been addressed. We surveyed 464 XR users to assess their awareness,\nconcerns, and coping strategies around XR data in 18 scenarios. Our findings\ndemonstrate that many factors, such as data types and sensitivity, affect\nusers' perceptions of privacy in XR. However, users' limited awareness of XR\nsensors' granular data collection capabilities, such as involuntary body\nsignals of emotional responses, restricted the range of privacy-protective\nstrategies they used. Our results highlight a need to enhance users' awareness\nof data privacy threats in XR, design privacy-choice interfaces tailored to XR\nenvironments, and develop transparent XR data practices.",
    "updated" : "2025-03-26T21:58:19Z",
    "published" : "2025-03-26T21:58:19Z",
    "authors" : [
      {
        "name" : "Hilda Hadan"
      },
      {
        "name" : "Derrick M. Wang"
      },
      {
        "name" : "Lennart E. Nacke"
      },
      {
        "name" : "Leah Zhang-Kennedy"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.20846v1",
    "title" : "Generating Synthetic Data with Formal Privacy Guarantees: State of the\n  Art and the Road Ahead",
    "summary" : "Privacy-preserving synthetic data offers a promising solution to harness\nsegregated data in high-stakes domains where information is compartmentalized\nfor regulatory, privacy, or institutional reasons. This survey provides a\ncomprehensive framework for understanding the landscape of privacy-preserving\nsynthetic data, presenting the theoretical foundations of generative models and\ndifferential privacy followed by a review of state-of-the-art methods across\ntabular data, images, and text. Our synthesis of evaluation approaches\nhighlights the fundamental trade-off between utility for down-stream tasks and\nprivacy guarantees, while identifying critical research gaps: the lack of\nrealistic benchmarks representing specialized domains and insufficient\nempirical evaluations required to contextualise formal guarantees.\n  Through empirical analysis of four leading methods on five real-world\ndatasets from specialized domains, we demonstrate significant performance\ndegradation under realistic privacy constraints ($\\epsilon \\leq 4$), revealing\na substantial gap between results reported on general domain benchmarks and\nperformance on domain-specific data. %Our findings highlight key challenges\nincluding unaccounted privacy leakage, insufficient empirical verification of\nformal guarantees, and a critical deficit of realistic benchmarks. These\nchallenges underscore the need for robust evaluation frameworks, standardized\nbenchmarks for specialized domains, and improved techniques to address the\nunique requirements of privacy-sensitive fields such that this technology can\ndeliver on its considerable potential.",
    "updated" : "2025-03-26T16:06:33Z",
    "published" : "2025-03-26T16:06:33Z",
    "authors" : [
      {
        "name" : "Viktor Schlegel"
      },
      {
        "name" : "Anil A Bharath"
      },
      {
        "name" : "Zilong Zhao"
      },
      {
        "name" : "Kevin Yee"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.22379v1",
    "title" : "Spend Your Budget Wisely: Towards an Intelligent Distribution of the\n  Privacy Budget in Differentially Private Text Rewriting",
    "summary" : "The task of $\\textit{Differentially Private Text Rewriting}$ is a class of\ntext privatization techniques in which (sensitive) input textual documents are\n$\\textit{rewritten}$ under Differential Privacy (DP) guarantees. The motivation\nbehind such methods is to hide both explicit and implicit identifiers that\ncould be contained in text, while still retaining the semantic meaning of the\noriginal text, thus preserving utility. Recent years have seen an uptick in\nresearch output in this field, offering a diverse array of word-, sentence-,\nand document-level DP rewriting methods. Common to these methods is the\nselection of a privacy budget (i.e., the $\\varepsilon$ parameter), which\ngoverns the degree to which a text is privatized. One major limitation of\nprevious works, stemming directly from the unique structure of language itself,\nis the lack of consideration of $\\textit{where}$ the privacy budget should be\nallocated, as not all aspects of language, and therefore text, are equally\nsensitive or personal. In this work, we are the first to address this\nshortcoming, asking the question of how a given privacy budget can be\nintelligently and sensibly distributed amongst a target document. We construct\nand evaluate a toolkit of linguistics- and NLP-based methods used to allocate a\nprivacy budget to constituent tokens in a text document. In a series of privacy\nand utility experiments, we empirically demonstrate that given the same privacy\nbudget, intelligent distribution leads to higher privacy levels and more\npositive trade-offs than a naive distribution of $\\varepsilon$. Our work\nhighlights the intricacies of text privatization with DP, and furthermore, it\ncalls for further work on finding more efficient ways to maximize the\nprivatization benefits offered by DP in text rewriting.",
    "updated" : "2025-03-28T12:33:46Z",
    "published" : "2025-03-28T12:33:46Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Chaeeun Joy Lee"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.22232v1",
    "title" : "Privacy-Preserving Secure Neighbor Discovery for Wireless Networks",
    "summary" : "Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are\nkey elements for network functionality. SND is a hard problem, satisfying not\nonly typical security properties (authentication, integrity) but also\nverification of direct communication, which involves distance estimation based\non time measurements and device coordinates. Defeating relay attacks, also\nknown as \"wormholes\", leading to stealthy Byzantine links and significant\ndegradation of communication and adversarial control, is key in many wireless\nnetworked systems. However, SND is not concerned with privacy; it necessitates\nrevealing the identity and location of the device(s) participating in the\nprotocol execution. This can be a deterrent for deployment, especially\ninvolving user-held devices in the emerging Internet of Things (IoT) enabled\nsmart environments. To address this challenge, we present a novel\nPrivacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling\ndevices to perform SND without revealing their actual identities and locations,\neffectively decoupling discovery from the exposure of sensitive information. We\nuse Homomorphic Encryption (HE) for computing device distances without\nrevealing their actual coordinates, as well as employing a pseudonymous device\nauthentication to hide identities while preserving communication integrity.\nPP-SND provides SND [1] along with pseudonymity, confidentiality, and\nunlinkability. Our presentation here is not specific to one wireless\ntechnology, and we assess the performance of the protocols (cryptographic\noverhead) on a Raspberry Pi 4 and provide a security and privacy analysis.",
    "updated" : "2025-03-28T08:27:47Z",
    "published" : "2025-03-28T08:27:47Z",
    "authors" : [
      {
        "name" : "Ahmed Mohamed Hussain"
      },
      {
        "name" : "Panos Papadimitratos"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.22161v1",
    "title" : "Traffic Modeling for Network Security and Privacy: Challenges Ahead",
    "summary" : "Traffic analysis using machine learning and deep learning models has made\nsignificant progress over the past decades. These models address various tasks\nin network security and privacy, including detection of anomalies and attacks,\ncountering censorship, etc. They also reveal privacy risks to users as\ndemonstrated by the research on LLM token inference as well as fingerprinting\n(and counter-fingerprinting) of user-visiting websites, IoT devices, and\ndifferent applications. However, challenges remain in securing our networks\nfrom threats and attacks. After briefly reviewing the tasks and recent ML\nmodels in network security and privacy, we discuss the challenges that lie\nahead.",
    "updated" : "2025-03-28T05:54:17Z",
    "published" : "2025-03-28T05:54:17Z",
    "authors" : [
      {
        "name" : "Dinil Mon Divakaran"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.22010v1",
    "title" : "Towards Privacy-Preserving Revocation of Verifiable Credentials with\n  Time-Flexibility",
    "summary" : "Self-Sovereign Identity (SSI) is an emerging paradigm for authentication and\ncredential presentation that aims to give users control over their data and\nprevent any kind of tracking by (even trusted) third parties. In the European\nUnion, the EUDI Digital Identity wallet is about to become a concrete\nimplementation of this paradigm. However, a debate is still ongoing, partially\nreflecting some aspects that are not yet consolidated in the scientific state\nof the art. Among these, an effective, efficient, and privacy-preserving\nimplementation of verifiable credential revocation remains a subject of\ndiscussion. In this work-in-progress paper, we propose the basis of a novel\nmethod that customizes the use of anonymous hierarchical identity-based\nencryption to restrict the Verifier access to the temporal authorizations\ngranted by the Holder. This way, the Issuer cannot track the Holder's\ncredential presentations, and the Verifier cannot check revocation information\nbeyond what is permitted by the Holder.",
    "updated" : "2025-03-27T21:58:32Z",
    "published" : "2025-03-27T21:58:32Z",
    "authors" : [
      {
        "name" : "Francesco Buccafurri"
      },
      {
        "name" : "Carmen Licciardi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.24089v1",
    "title" : "Initial State Privacy of Nonlinear Systems on Riemannian Manifolds",
    "summary" : "In this paper, we investigate initial state privacy protection for\ndiscrete-time nonlinear closed systems. By capturing Riemannian geometric\nstructures inherent in such privacy challenges, we refine the concept of\ndifferential privacy through the introduction of an initial state adjacency set\nbased on Riemannian distances. A new differential privacy condition is\nformulated using incremental output boundedness, enabling the design of\ntime-varying Laplacian noise to achieve specified privacy guarantees. The\nproposed framework extends beyond initial state protection to also cover system\nparameter privacy, which is demonstrated as a special application.",
    "updated" : "2025-03-31T13:42:18Z",
    "published" : "2025-03-31T13:42:18Z",
    "authors" : [
      {
        "name" : "Le Liu"
      },
      {
        "name" : "Yu Kawano"
      },
      {
        "name" : "Antai Xie"
      },
      {
        "name" : "Ming Cao"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.23903v1",
    "title" : "Privacy Preservation for Statistical Input in Dynamical Systems",
    "summary" : "This paper addresses the challenge of privacy preservation for statistical\ninputs in dynamical systems. Motivated by an autonomous building application,\nwe formulate a privacy preservation problem for statistical inputs in linear\ntime-invariant systems. What makes this problem widely applicable is that the\ninputs, rather than being assumed to be deterministic, follow a probability\ndistribution, inherently embedding privacy-sensitive information that requires\nprotection. This formulation also presents a technical challenge as\nconventional differential privacy mechanisms are not directly applicable.\nThrough rigorous analysis, we develop strategy to achieve $(0, \\delta)$\ndifferential privacy through adding noise. Finally, the effectiveness of our\nmethods is demonstrated by revisiting the autonomous building application.",
    "updated" : "2025-03-31T09:54:09Z",
    "published" : "2025-03-31T09:54:09Z",
    "authors" : [
      {
        "name" : "Le Liu"
      },
      {
        "name" : "Yu Kawano"
      },
      {
        "name" : "Ming Cao"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.23726v1",
    "title" : "PDSL: Privacy-Preserved Decentralized Stochastic Learning with\n  Heterogeneous Data Distribution",
    "summary" : "In the paradigm of decentralized learning, a group of agents collaborates to\nlearn a global model using distributed datasets without a central server.\nHowever, due to the heterogeneity of the local data across the different\nagents, learning a robust global model is rather challenging. Moreover, the\ncollaboration of the agents relies on their gradient information exchange,\nwhich poses a risk of privacy leakage. In this paper, to address these issues,\nwe propose PDSL, a novel privacy-preserved decentralized stochastic learning\nalgorithm with heterogeneous data distribution. On one hand, we innovate in\nutilizing the notion of Shapley values such that each agent can precisely\nmeasure the contributions of its heterogeneous neighbors to the global learning\ngoal; on the other hand, we leverage the notion of differential privacy to\nprevent each agent from suffering privacy leakage when it contributes gradient\ninformation to its neighbors. We conduct both solid theoretical analysis and\nextensive experiments to demonstrate the efficacy of our PDSL algorithm in\nterms of privacy preservation and convergence.",
    "updated" : "2025-03-31T04:58:05Z",
    "published" : "2025-03-31T04:58:05Z",
    "authors" : [
      {
        "name" : "Lina Wang"
      },
      {
        "name" : "Yunsheng Yuan"
      },
      {
        "name" : "Chunxiao Wang"
      },
      {
        "name" : "Feng Li"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.23533v1",
    "title" : "To See or Not to See: A Privacy Threat Model for Digital Forensics in\n  Crime Investigation",
    "summary" : "Digital forensics is a cornerstone of modern crime investigations, yet it\nraises significant privacy concerns due to the collection, processing, and\nstorage of digital evidence. Despite that, privacy threats in digital forensics\ncrime investigations often remain underexplored, thereby leading to potential\ngaps in forensic practices and regulatory compliance, which may then escalate\ninto harming the freedoms of natural persons. With this clear motivation, the\npresent paper applies the SPADA methodology for threat modelling with the goal\nof incorporating privacy-oriented threat modelling in digital forensics. As a\nresult, we identify a total of 298 privacy threats that may affect digital\nforensics processes through crime investigations. Furthermore, we demonstrate\nan unexplored feature on how SPADA assists in handling domain-dependency during\nthreat elicitation. This yields a second list of privacy threats that are\nuniversally applicable to any domain. We then present a comprehensive and\nsystematic privacy threat model for digital forensics in crime investigation.\nMoreover, we discuss some of the challenges about validating privacy threats in\nthis domain, particularly given the variability of legal frameworks across\njurisdictions. We ultimately propose our privacy threat model as a tool for\nensuring ethical and legally compliant investigative practices.",
    "updated" : "2025-03-30T17:34:35Z",
    "published" : "2025-03-30T17:34:35Z",
    "authors" : [
      {
        "name" : "Mario Raciti"
      },
      {
        "name" : "Simone Di Mauro"
      },
      {
        "name" : "Dimitri Van Landuyt"
      },
      {
        "name" : "Giampaolo Bella"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.23444v1",
    "title" : "The Processing goes far beyond \"the app\" -- Privacy issues of\n  decentralized Digital Contact Tracing using the example of the German\n  Corona-Warn-App (CWA)",
    "summary" : "Since SARS-CoV-2 started spreading in Europe in early 2020, there has been a\nstrong call for technical solutions to combat or contain the pandemic, with\ncontact tracing apps at the heart of the debates. The EU's General Data\nProtection Regulation (GDPR) requires controllers to carry out a data\nprotection impact assessment (DPIA) where their data processing is likely to\nresult in a high risk to the rights and freedoms (Art. 35 GDPR). A DPIA is a\nstructured risk analysis that identifies and evaluates possible consequences of\ndata processing relevant to fundamental rights in advance and describes the\nmeasures envisaged to address these risks or expresses the inability to do so.\nBased on the Standard Data Protection Model (SDM), we present the results of a\nscientific and methodologically clear DPIA of the German German Corona-Warn-App\n(CWA). It shows that even a decentralized architecture involves numerous\nserious weaknesses and risks, including larger ones still left unaddressed in\ncurrent implementations. It also found that none of the proposed designs\noperates on anonymous data or ensures proper anonymisation. It also showed that\ninformed consent would not be a legitimate legal ground for the processing. For\nall points where data subjects' rights are still not sufficiently safeguarded,\nwe briefly outline solutions.",
    "updated" : "2025-03-30T13:48:15Z",
    "published" : "2025-03-30T13:48:15Z",
    "authors" : [
      {
        "name" : "Rainer Rehak"
      },
      {
        "name" : "Christian R. Kuehne"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CR",
      "K.4; J.3; H.1; J.4; K.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.23292v1",
    "title" : "FedCAPrivacy: Privacy-Preserving Heterogeneous Federated Learning with\n  Anonymous Adaptive Clustering",
    "summary" : "Federated learning (FL) is a distributed machine learning paradigm enabling\nmultiple clients to train a model collaboratively without exposing their local\ndata. Among FL schemes, clustering is an effective technique addressing the\nheterogeneity issue (i.e., differences in data distribution and computational\nability affect training performance and effectiveness) via grouping\nparticipants with similar computational resources or data distribution into\nclusters. However, intra-cluster data exchange poses privacy risks, while\ncluster selection and adaptation introduce challenges that may affect overall\nperformance. To address these challenges, this paper introduces anonymous\nadaptive clustering, a novel approach that simultaneously enhances privacy\nprotection and boosts training efficiency. Specifically, an oblivious\nshuffle-based anonymization method is designed to safeguard user identities and\nprevent the aggregation server from inferring similarities through clustering.\nAdditionally, to improve performance, we introduce an iteration-based adaptive\nfrequency decay strategy, which leverages variability in clustering\nprobabilities to optimize training dynamics. With these techniques, we build\nthe FedCAPrivacy; experiments show that FedCAPrivacy achieves ~7X improvement\nin terms of performance while maintaining high privacy.",
    "updated" : "2025-03-30T03:16:54Z",
    "published" : "2025-03-30T03:16:54Z",
    "authors" : [
      {
        "name" : "Yunan Wei"
      },
      {
        "name" : "Shengnan Zhao"
      },
      {
        "name" : "Chuan Zhao"
      },
      {
        "name" : "Zhe Liu"
      },
      {
        "name" : "Zhenxiang Chen"
      },
      {
        "name" : "Minghao Zhao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.23026v1",
    "title" : "Federated Semantic Learning for Privacy-preserving Cross-domain\n  Recommendation",
    "summary" : "In the evolving landscape of recommender systems, the challenge of\neffectively conducting privacy-preserving Cross-Domain Recommendation (CDR),\nespecially under strict non-overlapping constraints, has emerged as a key\nfocus. Despite extensive research has made significant progress, several\nlimitations still exist: 1) Previous semantic-based methods fail to deeply\nexploit rich textual information, since they quantize the text into codes,\nlosing its original rich semantics. 2) The current solution solely relies on\nthe text-modality, while the synergistic effects with the ID-modality are\nignored. 3) Existing studies do not consider the impact of irrelevant semantic\nfeatures, leading to inaccurate semantic representation. To address these\nchallenges, we introduce federated semantic learning and devise FFMSR as our\nsolution. For Limitation 1, we locally learn items'semantic encodings from\ntheir original texts by a multi-layer semantic encoder, and then cluster them\non the server to facilitate the transfer of semantic knowledge between domains.\nTo tackle Limitation 2, we integrate both ID and Text modalities on the\nclients, and utilize them to learn different aspects of items. To handle\nLimitation 3, a Fast Fourier Transform (FFT)-based filter and a gating\nmechanism are developed to alleviate the impact of irrelevant semantic\ninformation in the local model. We conduct extensive experiments on two\nreal-world datasets, and the results demonstrate the superiority of our FFMSR\nmethod over other SOTA methods. Our source codes are publicly available at:\nhttps://github.com/Sapphire-star/FFMSR.",
    "updated" : "2025-03-29T09:37:11Z",
    "published" : "2025-03-29T09:37:11Z",
    "authors" : [
      {
        "name" : "Ziang Lu"
      },
      {
        "name" : "Lei Guo"
      },
      {
        "name" : "Xu Yu"
      },
      {
        "name" : "Zhiyong Cheng"
      },
      {
        "name" : "Xiaohui Han"
      },
      {
        "name" : "Lei Zhu"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.22993v1",
    "title" : "Calculating Connection vs. Risk: Understanding How Youth Negotiate\n  Digital Privacy and Security with Peers Online",
    "summary" : "Youth, while tech-savvy and highly active on social media, are still\nvulnerable to online privacy and security risks. Therefore, it is critical to\nunderstand how they negotiate and manage social connections versus protecting\nthemselves in online contexts. In this work, we conducted a thematic analysis\nof 1,318 private conversations on Instagram from 149 youth aged 13-21 to\nunderstand the digital privacy and security topics they discussed, if and how\nthey engaged in risky privacy behaviors, and how they balanced the benefits and\nrisks (i.e., privacy calculus) of making these decisions. Overall, youth were\nforthcoming when broaching a wide range of topics on digital privacy and\nsecurity, ranging from password management and account access challenges to\nshared experiences of being victims of privacy risks. However, they also openly\nengaged in risky behaviors, such as sharing personal account information with\npeers and even perpetrating privacy and security risks against others.\nNonetheless, we found many of these behaviors could be explained by the unique\n\"privacy calculus\" of youth, where they often prioritized social benefits over\npotential risks; for instance, youth often shared account credentials with\npeers to foster social connection and affirmation. As such, we provide a\nnuanced understanding of youth decision-making regarding digital security and\nprivacy, highlighting both positive behaviors, tensions, and points of concern.\nWe encourage future research to continue to challenge the potentially untrue\nnarratives regarding youth and their digital privacy and security to unpack the\nnuance of their privacy calculus that may differ from that of adults.",
    "updated" : "2025-03-29T06:54:46Z",
    "published" : "2025-03-29T06:54:46Z",
    "authors" : [
      {
        "name" : "Mamtaj Akter"
      },
      {
        "name" : "Jinkyung Katie Park"
      },
      {
        "name" : "Campbell Headrick"
      },
      {
        "name" : "Xinru Page"
      },
      {
        "name" : "Pamela J. Wisniewski"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.22232v2",
    "title" : "Privacy-Preserving Secure Neighbor Discovery for Wireless Networks",
    "summary" : "Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are\nkey elements for network functionality. SND is a hard problem, satisfying not\nonly typical security properties (authentication, integrity) but also\nverification of direct communication, which involves distance estimation based\non time measurements and device coordinates. Defeating relay attacks, also\nknown as \"wormholes\", leading to stealthy Byzantine links and significant\ndegradation of communication and adversarial control, is key in many wireless\nnetworked systems. However, SND is not concerned with privacy; it necessitates\nrevealing the identity and location of the device(s) participating in the\nprotocol execution. This can be a deterrent for deployment, especially\ninvolving user-held devices in the emerging Internet of Things (IoT) enabled\nsmart environments. To address this challenge, we present a novel\nPrivacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling\ndevices to perform SND without revealing their actual identities and locations,\neffectively decoupling discovery from the exposure of sensitive information. We\nuse Homomorphic Encryption (HE) for computing device distances without\nrevealing their actual coordinates, as well as employing a pseudonymous device\nauthentication to hide identities while preserving communication integrity.\nPP-SND provides SND [1] along with pseudonymity, confidentiality, and\nunlinkability. Our presentation here is not specific to one wireless\ntechnology, and we assess the performance of the protocols (cryptographic\noverhead) on a Raspberry Pi 4 and provide a security and privacy analysis.",
    "updated" : "2025-03-31T17:56:29Z",
    "published" : "2025-03-28T08:27:47Z",
    "authors" : [
      {
        "name" : "Ahmed Mohamed Hussain"
      },
      {
        "name" : "Panos Papadimitratos"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.22749v1",
    "title" : "Adaptive Clipping for Privacy-Preserving Few-Shot Learning: Enhancing\n  Generalization with Limited Data",
    "summary" : "In the era of data-driven machine-learning applications, privacy concerns and\nthe scarcity of labeled data have become paramount challenges. These challenges\nare particularly pronounced in the domain of few-shot learning, where the\nability to learn from limited labeled data is crucial. Privacy-preserving\nfew-shot learning algorithms have emerged as a promising solution to address\nsuch pronounced challenges. However, it is well-known that privacy-preserving\ntechniques often lead to a drop in utility due to the fundamental trade-off\nbetween data privacy and model performance. To enhance the utility of\nprivacy-preserving few-shot learning methods, we introduce a novel approach\ncalled Meta-Clip. This technique is specifically designed for meta-learning\nalgorithms, including Differentially Private (DP) model-agnostic meta-learning,\nDP-Reptile, and DP-MetaSGD algorithms, with the objective of balancing data\nprivacy preservation with learning capacity maximization. By dynamically\nadjusting clipping thresholds during the training process, our Adaptive\nClipping method provides fine-grained control over the disclosure of sensitive\ninformation, mitigating overfitting on small datasets and significantly\nimproving the generalization performance of meta-learning models. Through\ncomprehensive experiments on diverse benchmark datasets, we demonstrate the\neffectiveness of our approach in minimizing utility degradation, showcasing a\nsuperior privacy-utility trade-off compared to existing privacy-preserving\ntechniques. The adoption of Adaptive Clipping represents a substantial step\nforward in the field of privacy-preserving few-shot learning, empowering the\ndevelopment of secure and accurate models for real-world applications,\nespecially in scenarios where there are limited data availability.",
    "updated" : "2025-03-27T05:14:18Z",
    "published" : "2025-03-27T05:14:18Z",
    "authors" : [
      {
        "name" : "Kanishka Ranaweera"
      },
      {
        "name" : "Dinh C. Nguyen"
      },
      {
        "name" : "Pubudu N. Pathirana"
      },
      {
        "name" : "David Smith"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      },
      {
        "name" : "Aruna Seneviratne"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00282v1",
    "title" : "Federated Learning for Cross-Domain Data Privacy: A Distributed Approach\n  to Secure Collaboration",
    "summary" : "This paper proposes a data privacy protection framework based on federated\nlearning, which aims to realize effective cross-domain data collaboration under\nthe premise of ensuring data privacy through distributed learning. Federated\nlearning greatly reduces the risk of privacy breaches by training the model\nlocally on each client and sharing only model parameters rather than raw data.\nThe experiment verifies the high efficiency and privacy protection ability of\nfederated learning under different data sources through the simulation of\nmedical, financial, and user data. The results show that federated learning can\nnot only maintain high model performance in a multi-domain data environment but\nalso ensure effective protection of data privacy. The research in this paper\nprovides a new technical path for cross-domain data collaboration and promotes\nthe application of large-scale data analysis and machine learning while\nprotecting privacy.",
    "updated" : "2025-03-31T23:04:45Z",
    "published" : "2025-03-31T23:04:45Z",
    "authors" : [
      {
        "name" : "Yiwei Zhang"
      },
      {
        "name" : "Jie Liu"
      },
      {
        "name" : "Jiawei Wang"
      },
      {
        "name" : "Lu Dai"
      },
      {
        "name" : "Fan Guo"
      },
      {
        "name" : "Guohui Cai"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.01029v1",
    "title" : "Who is Responsible When AI Fails? Mapping Causes, Entities, and\n  Consequences of AI Privacy and Ethical Incidents",
    "summary" : "The rapid growth of artificial intelligence (AI) technologies has changed\ndecision-making in many fields. But, it has also raised major privacy and\nethical concerns. However, many AI incidents taxonomies and guidelines for\nacademia, industry, and government lack grounding in real-world incidents. We\nanalyzed 202 real-world AI privacy and ethical incidents. This produced a\ntaxonomy that classifies incident types across AI lifecycle stages. It accounts\nfor contextual factors such as causes, responsible entities, disclosure\nsources, and impacts. Our findings show insufficient incident reporting from AI\ndevelopers and users. Many incidents are caused by poor organizational\ndecisions and legal non-compliance. Only a few legal actions and corrective\nmeasures exist, while risk-mitigation efforts are limited. Our taxonomy\ncontributes a structured approach in reporting of future AI incidents. Our\nfindings demonstrate that current AI governance frameworks are inadequate. We\nurgently need child-specific protections and AI policies on social media. They\nmust moderate and reduce the spread of harmful AI-generated content. Our\nresearch provides insights for policymakers and practitioners, which lets them\ndesign ethical AI. It also support AI incident detection and risk management.\nFinally, it guides AI policy development. Improved policies will protect people\nfrom harmful AI applications and support innovation in AI systems.",
    "updated" : "2025-03-28T21:57:38Z",
    "published" : "2025-03-28T21:57:38Z",
    "authors" : [
      {
        "name" : "Hilda Hadan"
      },
      {
        "name" : "Reza Hadi Mogavi"
      },
      {
        "name" : "Leah Zhang-Kennedy"
      },
      {
        "name" : "Lennart E. Nacke"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.DB",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.03730v1",
    "title" : "Safeguarding Smart Inhaler Devices and Patient Privacy in Respiratory\n  Health Monitoring",
    "summary" : "The rapid development of Internet of Things (IoT) technology has\nsignificantly impacted various market sectors. According to Li et al. (2024),\nan estimated 75 billion devices will be on the market in 2025. The healthcare\nindustry is a target to improve patient care and ease healthcare provider\nburdens. Chronic respiratory disease is likely to benefit from their inclusion,\nwith 545 million people worldwide recorded to suffer from patients using these\ndevices to track their dosage. At the same time, healthcare providers can\nimprove medication administration and monitor respiratory health (Soriano et\nal., 2020). While IoT medical devices offer numerous benefits, they also have\nsecurity vulnerabilities that can expose patient data to cyberattacks. It's\ncrucial to prioritize security measures in developing and deploying IoT medical\ndevices, especially in personalized health monitoring systems for individuals\nwith respiratory conditions. Efforts are underway to assess the security risks\nassociated with intelligent inhalers and respiratory medical devices by\nunderstanding usability behavior and technological elements to identify and\naddress vulnerabilities effectively. This work analyses usability behavior and\ntechnical vulnerabilities, emphasizing the confidentiality of information\ngained from Smart Inhalers. It then extrapolates to interrogate potential\nvulnerabilities with Implantable Medical Devices (IMDs). Our work explores the\ntensions in device development through the intersection of IoT technology and\nrespiratory health, particularly in the context of intelligent inhalers and\nother breathing medical devices, calling for integrating robust security\nmeasures into the development and deployment of IoT devices to safeguard\npatient data and ensure the secure functioning of these critical healthcare\ntechnologies.",
    "updated" : "2025-03-31T18:16:06Z",
    "published" : "2025-03-31T18:16:06Z",
    "authors" : [
      {
        "name" : "Asaju Babajide"
      },
      {
        "name" : "Almustapha Wakili"
      },
      {
        "name" : "Michaela Barnett"
      },
      {
        "name" : "Lucas Potter"
      },
      {
        "name" : "Xavier-Lewis Palmer"
      },
      {
        "name" : "Woosub Jung"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.22993v2",
    "title" : "Calculating Connection vs. Risk: Understanding How Youth Negotiate\n  Digital Privacy and Security with Peers Online",
    "summary" : "Youth, while tech-savvy and highly active on social media, are still\nvulnerable to online privacy and security risks. Therefore, it is critical to\nunderstand how they negotiate and manage social connections versus protecting\nthemselves in online contexts. In this work, we conducted a thematic analysis\nof 1,318 private conversations on Instagram from 149 youth aged 13-21 to\nunderstand the digital privacy and security topics they discussed, if and how\nthey engaged in risky privacy behaviors, and how they balanced the benefits and\nrisks (i.e., privacy calculus) of making these decisions. Overall, youth were\nforthcoming when broaching a wide range of topics on digital privacy and\nsecurity, ranging from password management and account access challenges to\nshared experiences of being victims of privacy risks. However, they also openly\nengaged in risky behaviors, such as sharing personal account information with\npeers and even perpetrating privacy and security risks against others.\nNonetheless, we found many of these behaviors could be explained by the unique\n\"privacy calculus\" of youth, where they often prioritized social benefits over\npotential risks; for instance, youth often shared account credentials with\npeers to foster social connection and affirmation. As such, we provide a\nnuanced understanding of youth decision-making regarding digital security and\nprivacy, highlighting both positive behaviors, tensions, and points of concern.\nWe encourage future research to continue to challenge the potentially untrue\nnarratives regarding youth and their digital privacy and security to unpack the\nnuance of their privacy calculus that may differ from that of adults.",
    "updated" : "2025-04-05T07:22:05Z",
    "published" : "2025-03-29T06:54:46Z",
    "authors" : [
      {
        "name" : "Mamtaj Akter"
      },
      {
        "name" : "Jinkyung Katie Park"
      },
      {
        "name" : "Campbell Headrick"
      },
      {
        "name" : "Xinru Page"
      },
      {
        "name" : "Pamela J. Wisniewski"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07107v1",
    "title" : "Guarding Digital Privacy: Exploring User Profiling and Security\n  Enhancements",
    "summary" : "User profiling, the practice of collecting user information for personalized\nrecommendations, has become widespread, driving progress in technology.\nHowever, this growth poses a threat to user privacy, as devices often collect\nsensitive data without their owners' awareness. This article aims to\nconsolidate knowledge on user profiling, exploring various approaches and\nassociated challenges. Through the lens of two companies sharing user data and\nan analysis of 18 popular Android applications in India across various\ncategories, including $\\textit{Social, Education, Entertainment, Travel,\nShopping and Others}$, the article unveils privacy vulnerabilities. Further,\nthe article propose an enhanced machine learning framework, employing decision\ntrees and neural networks, that improves state-of-the-art classifiers in\ndetecting personal information exposure. Leveraging the XAI (explainable\nartificial intelligence) algorithm LIME (Local Interpretable Model-agnostic\nExplanations), it enhances interpretability, crucial for reliably identifying\nsensitive data. Results demonstrate a noteworthy performance boost, achieving a\n$75.01\\%$ accuracy with a reduced training time of $3.62$ seconds for neural\nnetworks. Concluding, the paper suggests research directions to strengthen\ndigital security measures.",
    "updated" : "2025-03-17T10:56:46Z",
    "published" : "2025-03-17T10:56:46Z",
    "authors" : [
      {
        "name" : "Rishika Kohli"
      },
      {
        "name" : "Shaifu Gupta"
      },
      {
        "name" : "Manoj Singh Gaur"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.12045v2",
    "title" : "Auditing Differential Privacy in the Black-Box Setting",
    "summary" : "This paper introduces a novel theoretical framework for auditing differential\nprivacy (DP) in a black-box setting. Leveraging the concept of $f$-differential\nprivacy, we explicitly define type I and type II errors and propose an auditing\nmechanism based on conformal inference. Our approach robustly controls the type\nI error rate under minimal assumptions. Furthermore, we establish a fundamental\nimpossibility result, demonstrating the inherent difficulty of simultaneously\ncontrolling both type I and type II errors without additional assumptions.\nNevertheless, under a monotone likelihood ratio (MLR) assumption, our auditing\nmechanism effectively controls both errors. We also extend our method to\nconstruct valid confidence bands for the trade-off function in the\nfinite-sample regime.",
    "updated" : "2025-04-10T18:44:33Z",
    "published" : "2025-03-15T08:34:40Z",
    "authors" : [
      {
        "name" : "Kaining Shi"
      },
      {
        "name" : "Cong Ma"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR",
      "cs.LG"
    ]
  }
]