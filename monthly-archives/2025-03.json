[
  {
    "id" : "http://arxiv.org/abs/2503.02862v1",
    "title" : "Privacy and Accuracy-Aware AI/ML Model Deduplication",
    "summary" : "With the growing adoption of privacy-preserving machine learning algorithms,\nsuch as Differentially Private Stochastic Gradient Descent (DP-SGD), training\nor fine-tuning models on private datasets has become increasingly prevalent.\nThis shift has led to the need for models offering varying privacy guarantees\nand utility levels to satisfy diverse user requirements. However, managing\nnumerous versions of large models introduces significant operational\nchallenges, including increased inference latency, higher resource consumption,\nand elevated costs. Model deduplication is a technique widely used by many\nmodel serving and database systems to support high-performance and low-cost\ninference queries and model diagnosis queries. However, none of the existing\nmodel deduplication works has considered privacy, leading to unbounded\naggregation of privacy costs for certain deduplicated models and inefficiencies\nwhen applied to deduplicate DP-trained models. We formalize the problems of\ndeduplicating DP-trained models for the first time and propose a novel privacy-\nand accuracy-aware deduplication mechanism to address the problems. We\ndeveloped a greedy strategy to select and assign base models to target models\nto minimize storage and privacy costs. When deduplicating a target model, we\ndynamically schedule accuracy validations and apply the Sparse Vector Technique\nto reduce the privacy costs associated with private validation data. Compared\nto baselines that do not provide privacy guarantees, our approach improved the\ncompression ratio by up to $35\\times$ for individual models (including large\nlanguage models and vision transformers). We also observed up to $43\\times$\ninference speedup due to the reduction of I/O operations.",
    "updated" : "2025-03-04T18:40:38Z",
    "published" : "2025-03-04T18:40:38Z",
    "authors" : [
      {
        "name" : "Hong Guan"
      },
      {
        "name" : "Lei Yu"
      },
      {
        "name" : "Lixi Zhou"
      },
      {
        "name" : "Li Xiong"
      },
      {
        "name" : "Kanchan Chowdhury"
      },
      {
        "name" : "Lulu Xie"
      },
      {
        "name" : "Xusheng Xiao"
      },
      {
        "name" : "Jia Zou"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02693v1",
    "title" : "Federated Learning for Privacy-Preserving Feedforward Control in\n  Multi-Agent Systems",
    "summary" : "Feedforward control (FF) is often combined with feedback control (FB) in many\ncontrol systems, improving tracking performance, efficiency, and stability.\nHowever, designing effective data-driven FF controllers in multi-agent systems\nrequires significant data collection, including transferring private or\nproprietary data, which raises privacy concerns and incurs high communication\ncosts. Therefore, we propose a novel approach integrating Federated Learning\n(FL) into FF control to address these challenges. This approach enables\nprivacy-preserving, communication-efficient, and decentralized continuous\nimprovement of FF controllers across multiple agents without sharing personal\nor proprietary data. By leveraging FL, each agent learns a local, neural FF\ncontroller using its data and contributes only model updates to a global\naggregation process, ensuring data privacy and scalability. We demonstrate the\neffectiveness of our method in an autonomous driving use case. Therein,\nvehicles equipped with a trajectory-tracking feedback controller are enhanced\nby FL-based neural FF control. Simulations highlight significant improvements\nin tracking performance compared to pure FB control, analogous to model-based\nFF control. We achieve comparable tracking performance without exchanging\nprivate vehicle-specific data compared to a centralized neural FF control. Our\nresults underscore the potential of FL-based neural FF control to enable\nprivacy-preserving learning in multi-agent control systems, paving the way for\nscalable and efficient autonomous systems applications.",
    "updated" : "2025-03-04T15:07:25Z",
    "published" : "2025-03-04T15:07:25Z",
    "authors" : [
      {
        "name" : "Jakob Weber"
      },
      {
        "name" : "Markus Gurtner"
      },
      {
        "name" : "Benedikt Alt"
      },
      {
        "name" : "Adrian Trachte"
      },
      {
        "name" : "Andreas Kugi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02549v1",
    "title" : "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation",
    "summary" : "The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the data collected from hospitals are\nstored in one center and used to train the nnU-Net. This centralized approach\nhas various limitations, such as leakage of sensitive patient information and\nviolation of patient privacy. Federated learning is one of the approaches to\ntrain a segmentation model in a decentralized manner that helps preserve\npatient privacy. In this paper, we propose FednnU-Net, a federated learning\nextension of nnU-Net. We introduce two novel federated learning methods to the\nnnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric\nFederated Averaging (AsymFedAvg) - and experimentally show their consistent\nperformance for breast, cardiac and fetal segmentation using 6 datasets\nrepresenting samples from 18 institutions. Additionally, to further promote\nresearch and deployment of decentralized training in privacy constrained\ninstitutions, we make our plug-n-play framework public. The source-code is\navailable at https://github.com/faildeny/FednnUNet .",
    "updated" : "2025-03-04T12:20:06Z",
    "published" : "2025-03-04T12:20:06Z",
    "authors" : [
      {
        "name" : "Grzegorz Skorupko"
      },
      {
        "name" : "Fotios Avgoustidis"
      },
      {
        "name" : "Carlos Martín-Isla"
      },
      {
        "name" : "Lidia Garrucho"
      },
      {
        "name" : "Dimitri A. Kessler"
      },
      {
        "name" : "Esmeralda Ruiz Pujadas"
      },
      {
        "name" : "Oliver Díaz"
      },
      {
        "name" : "Maciej Bobowicz"
      },
      {
        "name" : "Katarzyna Gwoździewicz"
      },
      {
        "name" : "Xavier Bargalló"
      },
      {
        "name" : "Paulius Jaruševičius"
      },
      {
        "name" : "Kaisar Kushibar"
      },
      {
        "name" : "Karim Lekadir"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02455v1",
    "title" : "Privacy Preservation Techniques (PPTs) in IoT Systems: A Scoping Review\n  and Future Directions",
    "summary" : "Privacy preservation in Internet of Things (IoT) systems requires the use of\nprivacy-enhancing technologies (PETs) built from innovative technologies such\nas cryptography and artificial intelligence (AI) to create techniques called\nprivacy preservation techniques (PPTs). These PPTs achieve various privacy\ngoals and address different privacy concerns by mitigating potential privacy\nthreats within IoT systems. This study carried out a scoping review of\ndifferent types of PPTs used in previous research works on IoT systems between\n2010 and early 2023 to further explore the advantages of privacy preservation\nin these systems. This scoping review looks at privacy goals, possible\ntechnologies used for building PET, the integration of PPTs into the computing\nlayer of the IoT architecture, different IoT applications in which PPTs are\ndeployed, and the different privacy types addressed by these techniques within\nIoT systems. Key findings, such as the prominent privacy goal and privacy type\nin IoT, are discussed in this survey, along with identified research gaps that\ncould inform future endeavors in privacy research and benefit the privacy\nresearch community and other stakeholders in IoT systems.",
    "updated" : "2025-03-04T10:03:45Z",
    "published" : "2025-03-04T10:03:45Z",
    "authors" : [
      {
        "name" : "Emmanuel Alalade"
      },
      {
        "name" : "Ashraf Matrawy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02132v1",
    "title" : "Video-DPRP: A Differentially Private Approach for Visual\n  Privacy-Preserving Video Human Activity Recognition",
    "summary" : "Considerable effort has been made in privacy-preserving video human activity\nrecognition (HAR). Two primary approaches to ensure privacy preservation in\nVideo HAR are differential privacy (DP) and visual privacy. Techniques\nenforcing DP during training provide strong theoretical privacy guarantees but\noffer limited capabilities for visual privacy assessment. Conversely methods,\nsuch as low-resolution transformations, data obfuscation and adversarial\nnetworks, emphasize visual privacy but lack clear theoretical privacy\nassurances. In this work, we focus on two main objectives: (1) leveraging DP\nproperties to develop a model-free approach for visual privacy in videos and\n(2) evaluating our proposed technique using both differential privacy and\nvisual privacy assessments on HAR tasks. To achieve goal (1), we introduce\nVideo-DPRP: a Video-sample-wise Differentially Private Random Projection\nframework for privacy-preserved video reconstruction for HAR. By using random\nprojections, noise matrices and right singular vectors derived from the\nsingular value decomposition of videos, Video-DPRP reconstructs DP videos using\nprivacy parameters ($\\epsilon,\\delta$) while enabling visual privacy\nassessment. For goal (2), using UCF101 and HMDB51 datasets, we compare\nVideo-DPRP's performance on activity recognition with traditional DP methods,\nand state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,\nwe assess its effectiveness in preserving privacy-related attributes such as\nfacial features, gender, and skin color, using the PA-HMDB and VISPR datasets.\nVideo-DPRP combines privacy-preservation from both a DP and visual privacy\nperspective unlike SOTA methods that typically address only one of these\naspects.",
    "updated" : "2025-03-03T23:43:12Z",
    "published" : "2025-03-03T23:43:12Z",
    "authors" : [
      {
        "name" : "Allassan Tchangmena A Nken"
      },
      {
        "name" : "Susan Mckeever"
      },
      {
        "name" : "Peter Corcoran"
      },
      {
        "name" : "Ihsan Ullah"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02114v1",
    "title" : "Fairness and/or Privacy on Social Graphs",
    "summary" : "Graph Neural Networks (GNNs) have shown remarkable success in various\ngraph-based learning tasks. However, recent studies have raised concerns about\nfairness and privacy issues in GNNs, highlighting the potential for biased or\ndiscriminatory outcomes and the vulnerability of sensitive information. This\npaper presents a comprehensive investigation of fairness and privacy in GNNs,\nexploring the impact of various fairness-preserving measures on model\nperformance. We conduct experiments across diverse datasets and evaluate the\neffectiveness of different fairness interventions. Our analysis considers the\ntrade-offs between fairness, privacy, and accuracy, providing insights into the\nchallenges and opportunities in achieving both fair and private graph learning.\nThe results highlight the importance of carefully selecting and combining\nfairness-preserving measures based on the specific characteristics of the data\nand the desired fairness objectives. This study contributes to a deeper\nunderstanding of the complex interplay between fairness, privacy, and accuracy\nin GNNs, paving the way for the development of more robust and ethical graph\nlearning models.",
    "updated" : "2025-03-03T22:56:32Z",
    "published" : "2025-03-03T22:56:32Z",
    "authors" : [
      {
        "name" : "Bartlomiej Surma"
      },
      {
        "name" : "Michael Backes"
      },
      {
        "name" : "Yang Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02091v1",
    "title" : "Which Code Statements Implement Privacy Behaviors in Android\n  Applications?",
    "summary" : "A \"privacy behavior\" in software is an action where the software uses\npersonal information for a service or a feature, such as a website using\nlocation to provide content relevant to a user. Programmers are required by\nregulations or application stores to provide privacy notices and labels\ndescribing these privacy behaviors. Although many tools and research prototypes\nhave been developed to help programmers generate these notices by analyzing the\nsource code, these approaches are often fairly coarse-grained (i.e., at the\nlevel of whole methods or files, rather than at the statement level). But this\nis not necessarily how privacy behaviors exist in code. Privacy behaviors are\nembedded in specific statements in code. Current literature does not examine\nwhat statements programmers see as most important, how consistent these views\nare, or how to detect them. In this paper, we conduct an empirical study to\nexamine which statements programmers view as most-related to privacy behaviors.\nWe find that expression statements that make function calls are most associated\nwith privacy behaviors, while the type of privacy label has little effect on\nthe attributes of the selected statements. We then propose an approach to\nautomatically detect these privacy-relevant statements by fine-tuning three\nlarge language models with the data from the study. We observe that the\nagreement between our approach and participants is comparable to or higher than\nan agreement between two participants. Our study and detection approach can\nhelp programmers understand which statements in code affect privacy in mobile\napplications.",
    "updated" : "2025-03-03T22:20:01Z",
    "published" : "2025-03-03T22:20:01Z",
    "authors" : [
      {
        "name" : "Chia-Yi Su"
      },
      {
        "name" : "Aakash Bansal"
      },
      {
        "name" : "Vijayanta Jain"
      },
      {
        "name" : "Sepideh Ghanavati"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Collin McMillan"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02019v1",
    "title" : "SLAP: Secure Location-proof and Anonymous Privacy-preserving Spectrum\n  Access",
    "summary" : "The rapid advancements in wireless technology have significantly increased\nthe demand for communication resources, leading to the development of Spectrum\nAccess Systems (SAS). However, network regulations require disclosing sensitive\nuser information, such as location coordinates and transmission details,\nraising critical privacy concerns. Moreover, as a database-driven architecture\nreliant on user-provided data, SAS necessitates robust location verification to\ncounter identity and location spoofing attacks and remains a primary target for\ndenial-of-service (DoS) attacks. Addressing these security challenges while\nadhering to regulatory requirements is essential. In this paper, we propose\nSLAP, a novel framework that ensures location privacy and anonymity during\nspectrum queries, usage notifications, and location-proof acquisition. Our\nsolution includes an adaptive dual-scenario location verification mechanism\nwith architectural flexibility and a fallback option, along with a counter-DoS\napproach using time-lock puzzles. We prove the security of SLAP and demonstrate\nits advantages over existing solutions through comprehensive performance\nevaluations.",
    "updated" : "2025-03-03T19:52:56Z",
    "published" : "2025-03-03T19:52:56Z",
    "authors" : [
      {
        "name" : "Saleh Darzi"
      },
      {
        "name" : "Attila A. Yavuz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02017v1",
    "title" : "A Lightweight and Secure Deep Learning Model for Privacy-Preserving\n  Federated Learning in Intelligent Enterprises",
    "summary" : "The ever growing Internet of Things (IoT) connections drive a new type of\norganization, the Intelligent Enterprise. In intelligent enterprises, machine\nlearning based models are adopted to extract insights from data. Due to the\nefficiency and privacy challenges of these traditional models, a new federated\nlearning (FL) paradigm has emerged. In FL, multiple enterprises can jointly\ntrain a model to update a final model. However, firstly, FL trained models\nusually perform worse than centralized models, especially when enterprises\ntraining data is non-IID (Independent and Identically Distributed). Second, due\nto the centrality of FL and the untrustworthiness of local enterprises,\ntraditional FL solutions are vulnerable to poisoning and inference attacks and\nviolate privacy. Thirdly, the continuous transfer of parameters between\nenterprises and servers increases communication costs. To this end, the\nFedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep\nLearning Model that includes three main phases. In the first phase, the goal is\nto solve the data type distribution skew challenge. Addressing privacy concerns\nagainst poisoning and inference attacks is covered in the second phase.\nFinally, to alleviate the communication overhead, a novel compression approach\nis proposed that significantly reduces the size of the updates. The experiment\nresults validate that FedAnil+ is secure against inference and poisoning\nattacks with better accuracy. In addition, it shows improvements over existing\napproaches in terms of model accuracy (13%, 16%, and 26%), communication cost\n(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%).",
    "updated" : "2025-03-03T19:51:13Z",
    "published" : "2025-03-03T19:51:13Z",
    "authors" : [
      {
        "name" : "Reza Fotohi"
      },
      {
        "name" : "Fereidoon Shams Aliee"
      },
      {
        "name" : "Bahar Farahani"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01482v1",
    "title" : "Revisiting Locally Differentially Private Protocols: Towards Better\n  Trade-offs in Privacy, Utility, and Attack Resistance",
    "summary" : "Local Differential Privacy (LDP) offers strong privacy protection, especially\nin settings in which the server collecting the data is untrusted. However,\ndesigning LDP mechanisms that achieve an optimal trade-off between privacy,\nutility, and robustness to adversarial inference attacks remains challenging.\nIn this work, we introduce a general multi-objective optimization framework for\nrefining LDP protocols, enabling the joint optimization of privacy and utility\nunder various adversarial settings. While our framework is flexible enough to\naccommodate multiple privacy and security attacks as well as utility metrics,\nin this paper we specifically optimize for Attacker Success Rate (ASR) under\ndistinguishability attack as a measure of privacy and Mean Squared Error (MSE)\nas a measure of utility. We systematically revisit these trade-offs by\nanalyzing eight state-of-the-art LDP protocols and proposing refined\ncounterparts that leverage tailored optimization techniques. Experimental\nresults demonstrate that our proposed adaptive mechanisms consistently\noutperform their non-adaptive counterparts, reducing ASR by up to five orders\nof magnitude while maintaining competitive utility. Analytical derivations also\nconfirm the effectiveness of our mechanisms, moving them closer to the ASR-MSE\nPareto frontier.",
    "updated" : "2025-03-03T12:41:01Z",
    "published" : "2025-03-03T12:41:01Z",
    "authors" : [
      {
        "name" : "Héber H. Arcolezi"
      },
      {
        "name" : "Sébastien Gambs"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01470v1",
    "title" : "Position: Ensuring mutual privacy is necessary for effective external\n  evaluation of proprietary AI systems",
    "summary" : "The external evaluation of AI systems is increasingly recognised as a crucial\napproach for understanding their potential risks. However, facilitating\nexternal evaluation in practice faces significant challenges in balancing\nevaluators' need for system access with AI developers' privacy and security\nconcerns. Additionally, evaluators have reason to protect their own privacy -\nfor example, in order to maintain the integrity of held-out test sets. We refer\nto the challenge of ensuring both developers' and evaluators' privacy as one of\nproviding mutual privacy. In this position paper, we argue that (i) addressing\nthis mutual privacy challenge is essential for effective external evaluation of\nAI systems, and (ii) current methods for facilitating external evaluation\ninadequately address this challenge, particularly when it comes to preserving\nevaluators' privacy. In making these arguments, we formalise the mutual privacy\nproblem; examine the privacy and access requirements of both model owners and\nevaluators; and explore potential solutions to this challenge, including\nthrough the application of cryptographic and hardware-based approaches.",
    "updated" : "2025-03-03T12:24:59Z",
    "published" : "2025-03-03T12:24:59Z",
    "authors" : [
      {
        "name" : "Ben Bucknall"
      },
      {
        "name" : "Robert F. Trager"
      },
      {
        "name" : "Michael A. Osborne"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01208v1",
    "title" : "Watch Out Your Album! On the Inadvertent Privacy Memorization in\n  Multi-Modal Large Language Models",
    "summary" : "Multi-Modal Large Language Models (MLLMs) have exhibited remarkable\nperformance on various vision-language tasks such as Visual Question Answering\n(VQA). Despite accumulating evidence of privacy concerns associated with\ntask-relevant content, it remains unclear whether MLLMs inadvertently memorize\nprivate content that is entirely irrelevant to the training tasks. In this\npaper, we investigate how randomly generated task-irrelevant private content\ncan become spuriously correlated with downstream objectives due to partial\nmini-batch training dynamics, thus causing inadvertent memorization.\nConcretely, we randomly generate task-irrelevant watermarks into VQA\nfine-tuning images at varying probabilities and propose a novel probing\nframework to determine whether MLLMs have inadvertently encoded such content.\nOur experiments reveal that MLLMs exhibit notably different training behaviors\nin partial mini-batch settings with task-irrelevant watermarks embedded.\nFurthermore, through layer-wise probing, we demonstrate that MLLMs trigger\ndistinct representational patterns when encountering previously seen\ntask-irrelevant knowledge, even if this knowledge does not influence their\noutput during prompting. Our code is available at\nhttps://github.com/illusionhi/ProbingPrivacy.",
    "updated" : "2025-03-03T06:10:27Z",
    "published" : "2025-03-03T06:10:27Z",
    "authors" : [
      {
        "name" : "Tianjie Ju"
      },
      {
        "name" : "Yi Hua"
      },
      {
        "name" : "Hao Fei"
      },
      {
        "name" : "Zhenyu Shao"
      },
      {
        "name" : "Yubin Zheng"
      },
      {
        "name" : "Haodong Zhao"
      },
      {
        "name" : "Mong-Li Lee"
      },
      {
        "name" : "Wynne Hsu"
      },
      {
        "name" : "Zhuosheng Zhang"
      },
      {
        "name" : "Gongshen Liu"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01089v1",
    "title" : "Privacy-preserving Machine Learning in Internet of Vehicle Applications:\n  Fundamentals, Recent Advances, and Future Direction",
    "summary" : "Machine learning (ML) has revolutionized Internet of Vehicles (IoV)\napplications by enhancing intelligent transportation, autonomous driving\ncapabilities, and various connected services within a large, heterogeneous\nnetwork. However, the increased connectivity and massive data exchange for ML\napplications introduce significant privacy challenges. Privacy-preserving\nmachine learning (PPML) offers potential solutions to address these challenges\nby preserving privacy at various stages of the ML pipeline. Despite the rapid\ndevelopment of ML-based IoV applications and the growing data privacy concerns,\nthere are limited comprehensive studies on the adoption of PPML within this\ndomain. Therefore, this study provides a comprehensive review of the\nfundamentals, recent advancements, and the challenges of integrating PPML into\nIoV applications. To conduct an extensive study, we first review existing\nsurveys of various PPML techniques and their integration into IoV across\ndifferent scopes. We then discuss the fundamentals of IoV and propose a\nfour-layer IoV architecture. Additionally, we categorize IoV applications into\nthree key domains and analyze the privacy challenges in leveraging ML for these\napplication domains. Next, we provide an overview of various PPML techniques,\nhighlighting their applicability and performance to address the privacy\nchallenges. Building on these fundamentals, we thoroughly review recent\nadvancements in integrating various PPML techniques within IoV applications,\ndiscussing their frameworks, key features, and performance evaluation in terms\nof privacy, utility, and efficiency. Finally, we identify current challenges\nand propose future research directions to enhance privacy and reliability in\nIoV applications.",
    "updated" : "2025-03-03T01:24:04Z",
    "published" : "2025-03-03T01:24:04Z",
    "authors" : [
      {
        "name" : "Nazmul Islam"
      },
      {
        "name" : "Mohammad Zulkernine"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.01000v1",
    "title" : "Privacy vs. Profit: The Impact of Google's Manifest Version 3 (MV3)\n  Update on Ad Blocker Effectiveness",
    "summary" : "Google's recent update to the manifest file for Chrome browser\nextensions-transitioning from manifest version 2 (MV2) to manifest version 3\n(MV3)-has raised concerns among users and ad blocker providers, who worry that\nthe new restrictions, notably the shift from the powerful WebRequest API to the\nmore restrictive DeclarativeNetRequest API, might reduce ad blocker\neffectiveness. Because ad blockers play a vital role for millions of users\nseeking a more private and ad-free browsing experience, this study empirically\ninvestigates how the MV3 update affects their ability to block ads and\ntrackers. Through a browser-based experiment conducted across multiple samples\nof ad-supported websites, we compare the MV3 to MV2 instances of four widely\nused ad blockers. Our results reveal no statistically significant reduction in\nad-blocking or anti-tracking effectiveness for MV3 ad blockers compared to\ntheir MV2 counterparts, and in some cases, MV3 instances even exhibit slight\nimprovements in blocking trackers. These findings are reassuring for users,\nindicating that the MV3 instances of popular ad blockers continue to provide\neffective protection against intrusive ads and privacy-infringing trackers.\nWhile some uncertainties remain, ad blocker providers appear to have\nsuccessfully navigated the MV3 update, finding solutions that maintain the core\nfunctionality of their ad blockers.",
    "updated" : "2025-03-02T19:41:34Z",
    "published" : "2025-03-02T19:41:34Z",
    "authors" : [
      {
        "name" : "Karlo Lukic"
      },
      {
        "name" : "Lazaros Papadopoulos"
      }
    ],
    "categories" : [
      "cs.CY",
      "econ.GN",
      "q-fin.EC",
      "K.4.0, K.6.5, H.5.2",
      "K.4.0; K.6.5; H.5.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.00703v1",
    "title" : "Towards hyperparameter-free optimization with differential privacy",
    "summary" : "Differential privacy (DP) is a privacy-preserving paradigm that protects the\ntraining data when training deep learning models. Critically, the performance\nof models is determined by the training hyperparameters, especially those of\nthe learning rate schedule, thus requiring fine-grained hyperparameter tuning\non the data. In practice, it is common to tune the learning rate\nhyperparameters through the grid search that (1) is computationally expensive\nas multiple runs are needed, and (2) increases the risk of data leakage as the\nselection of hyperparameters is data-dependent. In this work, we adapt the\nautomatic learning rate schedule to DP optimization for any models and\noptimizers, so as to significantly mitigate or even eliminate the cost of\nhyperparameter tuning when applied together with automatic per-sample gradient\nclipping. Our hyperparameter-free DP optimization is almost as computationally\nefficient as the standard non-DP optimization, and achieves state-of-the-art DP\nperformance on various language and vision tasks.",
    "updated" : "2025-03-02T02:59:52Z",
    "published" : "2025-03-02T02:59:52Z",
    "authors" : [
      {
        "name" : "Zhiqi Bu"
      },
      {
        "name" : "Ruixuan Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03652v1",
    "title" : "Token-Level Privacy in Large Language Models",
    "summary" : "The use of language models as remote services requires transmitting private\ninformation to external providers, raising significant privacy concerns. This\nprocess not only risks exposing sensitive data to untrusted service providers\nbut also leaves it vulnerable to interception by eavesdroppers. Existing\nprivacy-preserving methods for natural language processing (NLP) interactions\nprimarily rely on semantic similarity, overlooking the role of contextual\ninformation. In this work, we introduce dchi-stencil, a novel token-level\nprivacy-preserving mechanism that integrates contextual and semantic\ninformation while ensuring strong privacy guarantees under the dchi\ndifferential privacy framework, achieving 2epsilon-dchi-privacy. By\nincorporating both semantic and contextual nuances, dchi-stencil achieves a\nrobust balance between privacy and utility. We evaluate dchi-stencil using\nstate-of-the-art language models and diverse datasets, achieving comparable and\neven better trade-off between utility and privacy compared to existing methods.\nThis work highlights the potential of dchi-stencil to set a new standard for\nprivacy-preserving NLP in modern, high-risk applications.",
    "updated" : "2025-03-05T16:27:25Z",
    "published" : "2025-03-05T16:27:25Z",
    "authors" : [
      {
        "name" : "Re'em Harel"
      },
      {
        "name" : "Niv Gilboa"
      },
      {
        "name" : "Yuval Pinter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03587v1",
    "title" : "\"You don't need a university degree to comprehend data protection this\n  way\": LLM-Powered Interactive Privacy Policy Assessment",
    "summary" : "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present the first qualitative user study on Large Language Model\n(LLM)-driven privacy policy assessment. To this end, we build and evaluate an\nLLM-based privacy policy assessment browser extension, which helps users\nunderstand the essence of a lengthy, complex privacy policy while browsing. The\ntool integrates a dashboard and an LLM chat. In our qualitative user study\n(N=22), we evaluate usability, understandability of the information our tool\nprovides, and its impacts on awareness. While providing a comprehensible quick\noverview and a chat for in-depth discussion improves privacy awareness, users\nnote issues with building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
    "updated" : "2025-03-05T15:22:35Z",
    "published" : "2025-03-05T15:22:35Z",
    "authors" : [
      {
        "name" : "Vincent Freiberger"
      },
      {
        "name" : "Arthur Fleig"
      },
      {
        "name" : "Erik Buchmann"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03539v1",
    "title" : "Data Sharing, Privacy and Security Considerations in the Energy Sector:\n  A Review from Technical Landscape to Regulatory Specifications",
    "summary" : "Decarbonization, decentralization and digitalization are the three key\nelements driving the twin energy transition. The energy system is evolving to a\nmore data driven ecosystem, leading to the need of communication and storage of\nlarge amount of data of different resolution from the prosumers and other\nstakeholders in the energy ecosystem. While the energy system is certainly\nadvancing, this paradigm shift is bringing in new privacy and security issues\nrelated to collection, processing and storage of data - not only from the\ntechnical dimension, but also from the regulatory perspective. Understanding\ndata privacy and security in the evolving energy system, regarding regulatory\ncompliance, is an immature field of research. Contextualized knowledge of how\nrelated issues are regulated is still in its infancy, and the practical and\ntechnical basis for the regulatory framework for data privacy and security is\nnot clear. To fill this gap, this paper conducts a comprehensive review of the\ndata-related issues for the energy system by integrating both technical and\nregulatory dimensions. We start by reviewing open-access data, data\ncommunication and data-processing techniques for the energy system, and use it\nas the basis to connect the analysis of data-related issues from the integrated\nperspective. We classify the issues into three categories: (i) data-sharing\namong energy end users and stakeholders (ii) privacy of end users, and (iii)\ncyber security, and then explore these issues from a regulatory perspective. We\nanalyze the evolution of related regulations, and introduce the relevant\nregulatory initiatives for the categorized issues in terms of regulatory\ndefinitions, concepts, principles, rights and obligations in the context of\nenergy systems. Finally, we provide reflections on the gaps that still exist,\nand guidelines for regulatory frameworks for a truly participatory energy\nsystem.",
    "updated" : "2025-03-05T14:23:56Z",
    "published" : "2025-03-05T14:23:56Z",
    "authors" : [
      {
        "name" : "Shiliang Zhang"
      },
      {
        "name" : "Sabita Maharjan"
      },
      {
        "name" : "Lee Andrew Bygrave"
      },
      {
        "name" : "Shui Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03506v1",
    "title" : "Rethinking Synthetic Data definitions: A privacy driven approach",
    "summary" : "Synthetic data is gaining traction as a cost-effective solution for the\nincreasing data demands of AI development and can be generated either from\nexisting knowledge or derived data captured from real-world events. The source\nof the synthetic data generation and the technique used significantly impacts\nits residual privacy risk and therefore its opportunity for sharing.\nTraditional classification of synthetic data types no longer fit the newer\ngeneration techniques and there is a need to better align the classification\nwith practical needs. We suggest a new way of grouping synthetic data types\nthat better supports privacy evaluations to aid regulatory policymaking. Our\nnovel classification provides flexibility to new advancements like deep\ngenerative methods and offers a more practical framework for future\napplications.",
    "updated" : "2025-03-05T13:54:13Z",
    "published" : "2025-03-05T13:54:13Z",
    "authors" : [
      {
        "name" : "Vibeke Binz Vallevik"
      },
      {
        "name" : "Serena Elizabeth Marshall"
      },
      {
        "name" : "Aleksandar Babic"
      },
      {
        "name" : "Jan Franz Nygaard"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03428v1",
    "title" : "Privacy is All You Need: Revolutionizing Wearable Health Data with\n  Advanced PETs",
    "summary" : "In a world where data is the new currency, wearable health devices offer\nunprecedented insights into daily life, continuously monitoring vital signs and\nmetrics. However, this convenience raises privacy concerns, as these devices\ncollect sensitive data that can be misused or breached. Traditional measures\noften fail due to real-time data processing needs and limited device power.\nUsers also lack awareness and control over data sharing and usage. We propose a\nPrivacy-Enhancing Technology (PET) framework for wearable devices, integrating\nfederated learning, lightweight cryptographic methods, and selectively deployed\nblockchain technology. The blockchain acts as a secure ledger triggered only\nupon data transfer requests, granting users real-time notifications and\ncontrol. By dismantling data monopolies, this approach returns data sovereignty\nto individuals. Through real-world applications like secure medical data\nsharing, privacy-preserving fitness tracking, and continuous health monitoring,\nour framework reduces privacy risks by up to 70 percent while preserving data\nutility and performance. This innovation sets a new benchmark for wearable\nprivacy and can scale to broader IoT ecosystems, including smart homes and\nindustry. As data continues to shape our digital landscape, our research\nunderscores the critical need to maintain privacy and user control at the\nforefront of technological progress.",
    "updated" : "2025-03-05T12:01:22Z",
    "published" : "2025-03-05T12:01:22Z",
    "authors" : [
      {
        "name" : "Karthik Barma"
      },
      {
        "name" : "Seshu Babu Barma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03267v1",
    "title" : "Quantum-Inspired Privacy-Preserving Federated Learning Framework for\n  Secure Dementia Classification",
    "summary" : "Dementia, a neurological disorder impacting millions globally, presents\nsignificant challenges in diagnosis and patient care. With the rise of privacy\nconcerns and security threats in healthcare, federated learning (FL) has\nemerged as a promising approach to enable collaborative model training across\ndecentralized datasets without exposing sensitive patient information. However,\nFL remains vulnerable to advanced security breaches such as gradient inversion\nand eavesdropping attacks. This paper introduces a novel framework that\nintegrates federated learning with quantum-inspired encryption techniques for\ndementia classification, emphasizing privacy preservation and security.\nLeveraging quantum key distribution (QKD), the framework ensures secure\ntransmission of model weights, protecting against unauthorized access and\ninterception during training. The methodology utilizes a convolutional neural\nnetwork (CNN) for dementia classification, with federated training conducted\nacross distributed healthcare nodes, incorporating QKD-encrypted weight sharing\nto secure the aggregation process. Experimental evaluations conducted on MRI\ndata from the OASIS dataset demonstrate that the proposed framework achieves\nidentical accuracy levels to a baseline model while enhancing data security and\nreducing loss by almost 1% compared to the classical baseline model. The\nframework offers significant implications for democratizing access to AI-driven\ndementia diagnostics in low- and middle-income countries, addressing critical\nresource and privacy constraints. This work contributes a robust, scalable, and\nsecure federated learning solution for healthcare applications, paving the way\nfor broader adoption of quantum-inspired techniques in AI-driven medical\nresearch.",
    "updated" : "2025-03-05T08:49:31Z",
    "published" : "2025-03-05T08:49:31Z",
    "authors" : [
      {
        "name" : "Gazi Tanbhir"
      },
      {
        "name" : "Md. Farhan Shahriyar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03146v1",
    "title" : "PriFFT: Privacy-preserving Federated Fine-tuning of Large Language\n  Models via Function Secret Sharing",
    "summary" : "Fine-tuning large language models (LLMs) raises privacy concerns due to the\nrisk of exposing sensitive training data. Federated learning (FL) mitigates\nthis risk by keeping training samples on local devices, but recent studies show\nthat adversaries can still infer private information from model updates in FL.\nAdditionally, LLM parameters are typically shared publicly during federated\nfine-tuning, while developers are often reluctant to disclose these parameters,\nposing further security challenges. Inspired by the above problems, we propose\nPriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both\nthe model updates and parameters. In PriFFT, clients and the server share model\ninputs and parameters by secret sharing, performing secure fine-tuning on\nshared values without accessing plaintext data. Due to considerable LLM\nparameters, privacy-preserving federated fine-tuning invokes complex secure\ncalculations and requires substantial communication and computation resources.\nTo optimize the efficiency of privacy-preserving federated fine-tuning of LLMs,\nwe introduce function secret-sharing protocols for various operations,\nincluding reciprocal calculation, tensor products, natural exponentiation,\nsoftmax, hyperbolic tangent, and dropout. The proposed protocols achieve up to\n4.02X speed improvement and reduce 7.19X communication overhead compared to the\nimplementation based on existing secret sharing methods. Besides, PriFFT\nachieves a 2.23X speed improvement and reduces 4.08X communication overhead in\nprivacy-preserving fine-tuning without accuracy drop compared to the existing\nsecret sharing methods.",
    "updated" : "2025-03-05T03:41:57Z",
    "published" : "2025-03-05T03:41:57Z",
    "authors" : [
      {
        "name" : "Zhichao You"
      },
      {
        "name" : "Xuewen Dong"
      },
      {
        "name" : "Ke Cheng"
      },
      {
        "name" : "Xutong Mu"
      },
      {
        "name" : "Jiaxuan Fu"
      },
      {
        "name" : "Shiyang Ma"
      },
      {
        "name" : "Qiang Qu"
      },
      {
        "name" : "Yulong Shen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03087v1",
    "title" : "\"Watch My Health, Not My Data\": Understanding Perceptions, Barriers,\n  Emotional Impact, & Coping Strategies Pertaining to IoT Privacy and Security\n  in Health Monitoring for Older Adults",
    "summary" : "The proliferation of \"Internet of Things (IoT)\" provides older adults with\ncritical support for \"health monitoring\" and independent living, yet\nsignificant concerns about security and privacy persist. In this paper, we\nreport on these issues through a two-phase user study, including a survey (N =\n22) and semi-structured interviews (n = 9) with adults aged 65+. We found that\nwhile 81.82% of our participants are aware of security features like\n\"two-factor authentication (2FA)\" and encryption, 63.64% express serious\nconcerns about unauthorized access to sensitive health data. Only 13.64% feel\nconfident in existing protections, citing confusion over \"data sharing\npolicies\" and frustration with \"complex security settings\" which lead to\ndistrust and anxiety. To cope, our participants adopt various strategies, such\nas relying on family or professional support and limiting feature usage leading\nto disengagement. Thus, we recommend \"adaptive security mechanisms,\" simplified\ninterfaces, and real-time transparency notifications to foster trust and ensure\n\"privacy and security by design\" in IoT health systems for older adults.",
    "updated" : "2025-03-05T01:04:13Z",
    "published" : "2025-03-05T01:04:13Z",
    "authors" : [
      {
        "name" : "Suleiman Saka"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03043v1",
    "title" : "Leveraging Randomness in Model and Data Partitioning for Privacy\n  Amplification",
    "summary" : "We study how inherent randomness in the training process -- where each sample\n(or client in federated learning) contributes only to a randomly selected\nportion of training -- can be leveraged for privacy amplification. This\nincludes (1) data partitioning, where a sample participates in only a subset of\ntraining iterations, and (2) model partitioning, where a sample updates only a\nsubset of the model parameters. We apply our framework to model parallelism in\nfederated learning, where each client updates a randomly selected subnetwork to\nreduce memory and computational overhead, and show that existing methods, e.g.\nmodel splitting or dropout, provide a significant privacy amplification gain\nnot captured by previous privacy analysis techniques. Additionally, we\nintroduce Balanced Iteration Subsampling, a new data partitioning method where\neach sample (or client) participates in a fixed number of training iterations.\nWe show that this method yields stronger privacy amplification than Poisson\n(i.i.d.) sampling of data (or clients). Our results demonstrate that randomness\nin the training process, which is structured rather than i.i.d. and interacts\nwith data in complex ways, can be systematically leveraged for significant\nprivacy amplification.",
    "updated" : "2025-03-04T22:49:59Z",
    "published" : "2025-03-04T22:49:59Z",
    "authors" : [
      {
        "name" : "Andy Dong"
      },
      {
        "name" : "Wei-Ning Chen"
      },
      {
        "name" : "Ayfer Ozgur"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.02968v1",
    "title" : "Privacy-Preserving Fair Synthetic Tabular Data",
    "summary" : "Sharing of tabular data containing valuable but private information is\nlimited due to legal and ethical issues. Synthetic data could be an alternative\nsolution to this sharing problem, as it is artificially generated by machine\nlearning algorithms and tries to capture the underlying data distribution.\nHowever, machine learning models are not free from memorization and may\nintroduce biases, as they rely on training data. Producing synthetic data that\npreserves privacy and fairness while maintaining utility close to the real data\nis a challenging task. This research simultaneously addresses both the privacy\nand fairness aspects of synthetic data, an area not explored by other studies.\nIn this work, we present PF-WGAN, a privacy-preserving, fair synthetic tabular\ndata generator based on the WGAN-GP model. We have modified the original\nWGAN-GP by adding privacy and fairness constraints forcing it to produce\nprivacy-preserving fair data. This approach will enable the publication of\ndatasets that protect individual's privacy and remain unbiased toward any\nparticular group. We compared the results with three state-of-the-art synthetic\ndata generator models in terms of utility, privacy, and fairness across four\ndifferent datasets. We found that the proposed model exhibits a more balanced\ntrade-off among utility, privacy, and fairness.",
    "updated" : "2025-03-04T19:51:00Z",
    "published" : "2025-03-04T19:51:00Z",
    "authors" : [
      {
        "name" : "Fatima J. Sarmin"
      },
      {
        "name" : "Atiquer R. Rahman"
      },
      {
        "name" : "Christopher J. Henry"
      },
      {
        "name" : "Noman Mohammed"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04707v1",
    "title" : "Iris Style Transfer: Enhancing Iris Recognition with Style Features and\n  Privacy Preservation through Neural Style Transfer",
    "summary" : "Iris texture is widely regarded as a gold standard biometric modality for\nauthentication and identification. The demand for robust iris recognition\nmethods, coupled with growing security and privacy concerns regarding iris\nattacks, has escalated recently. Inspired by neural style transfer, an advanced\ntechnique that leverages neural networks to separate content and style\nfeatures, we hypothesize that iris texture's style features provide a reliable\nfoundation for recognition and are more resilient to variations like rotation\nand perspective shifts than traditional approaches. Our experimental results\nsupport this hypothesis, showing a significantly higher classification accuracy\ncompared to conventional features. Further, we propose using neural style\ntransfer to mask identifiable iris style features, ensuring the protection of\nsensitive biometric information while maintaining the utility of eye images for\ntasks like eye segmentation and gaze estimation. This work opens new avenues\nfor iris-oriented, secure, and privacy-aware biometric systems.",
    "updated" : "2025-03-06T18:55:21Z",
    "published" : "2025-03-06T18:55:21Z",
    "authors" : [
      {
        "name" : "Mengdi Wang"
      },
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Enkelejda Kasneci"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04652v1",
    "title" : "Evaluation of Privacy-aware Support Vector Machine (SVM) Learning using\n  Homomorphic Encryption",
    "summary" : "The requirement for privacy-aware machine learning increases as we continue\nto use PII (Personally Identifiable Information) within machine training. To\novercome these privacy issues, we can apply Fully Homomorphic Encryption (FHE)\nto encrypt data before it is fed into a machine learning model. This involves\ncreating a homomorphic encryption key pair, and where the associated public key\nwill be used to encrypt the input data, and the private key will decrypt the\noutput. But, there is often a performance hit when we use homomorphic\nencryption, and so this paper evaluates the performance overhead of using the\nSVM machine learning technique with the OpenFHE homomorphic encryption library.\nThis uses Python and the scikit-learn library for its implementation. The\nexperiments include a range of variables such as multiplication depth, scale\nsize, first modulus size, security level, batch size, and ring dimension, along\nwith two different SVM models, SVM-Poly and SVM-Linear. Overall, the results\nshow that the two main parameters which affect performance are the ring\ndimension and the modulus size, and that SVM-Poly and SVM-Linear show similar\nperformance levels.",
    "updated" : "2025-03-06T17:42:23Z",
    "published" : "2025-03-06T17:42:23Z",
    "authors" : [
      {
        "name" : "William J Buchanan"
      },
      {
        "name" : "Hisham Ali"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04451v1",
    "title" : "Privacy Preserving and Robust Aggregation for Cross-Silo Federated\n  Learning in Non-IID Settings",
    "summary" : "Federated Averaging remains the most widely used aggregation strategy in\nfederated learning due to its simplicity and scalability. However, its\nperformance degrades significantly in non-IID data settings, where client\ndistributions are highly imbalanced or skewed. Additionally, it relies on\nclients transmitting metadata, specifically the number of training samples,\nwhich introduces privacy risks and may conflict with regulatory frameworks like\nthe European GDPR. In this paper, we propose a novel aggregation strategy that\naddresses these challenges by introducing class-aware gradient masking. Unlike\ntraditional approaches, our method relies solely on gradient updates,\neliminating the need for any additional client metadata, thereby enhancing\nprivacy protection. Furthermore, our approach validates and dynamically weights\nclient contributions based on class-specific importance, ensuring robustness\nagainst non-IID distributions, convergence prevention, and backdoor attacks.\nExtensive experiments on benchmark datasets demonstrate that our method not\nonly outperforms FedAvg and other widely accepted aggregation strategies in\nnon-IID settings but also preserves model integrity in adversarial scenarios.\nOur results establish the effectiveness of gradient masking as a practical and\nsecure solution for federated learning.",
    "updated" : "2025-03-06T14:06:20Z",
    "published" : "2025-03-06T14:06:20Z",
    "authors" : [
      {
        "name" : "Marco Arazzi"
      },
      {
        "name" : "Mert Cihangiroglu"
      },
      {
        "name" : "Antonino Nocera"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04054v1",
    "title" : "Controlled privacy leakage propagation throughout overlapping grouped\n  learning",
    "summary" : "Federated Learning (FL) is the standard protocol for collaborative learning.\nIn FL, multiple workers jointly train a shared model. They exchange model\nupdates calculated on their data, while keeping the raw data itself local.\nSince workers naturally form groups based on common interests and privacy\npolicies, we are motivated to extend standard FL to reflect a setting with\nmultiple, potentially overlapping groups. In this setup where workers can\nbelong and contribute to more than one group at a time, complexities arise in\nunderstanding privacy leakage and in adhering to privacy policies. To address\nthe challenges, we propose differential private overlapping grouped learning\n(DPOGL), a novel method to implement privacy guarantees within overlapping\ngroups. Under the honest-but-curious threat model, we derive novel privacy\nguarantees between arbitrary pairs of workers. These privacy guarantees\ndescribe and quantify two key effects of privacy leakage in DP-OGL: propagation\ndelay, i.e., the fact that information from one group will leak to other groups\nonly with temporal offset through the common workers and information\ndegradation, i.e., the fact that noise addition over model updates limits\ninformation leakage between workers. Our experiments show that applying DP-OGL\nenhances utility while maintaining strong privacy compared to standard FL\nsetups.",
    "updated" : "2025-03-06T03:14:45Z",
    "published" : "2025-03-06T03:14:45Z",
    "authors" : [
      {
        "name" : "Shahrzad Kiani"
      },
      {
        "name" : "Franziska Boenisch"
      },
      {
        "name" : "Stark C. Draper"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.03988v1",
    "title" : "AI-based Programming Assistants for Privacy-related Code Generation: The\n  Developers' Experience",
    "summary" : "With the popularising of generative AI, the existence of AI-based programming\nassistants for developers is no surprise. Developers increasingly use them for\ntheir work, including generating code to fulfil the data protection\nrequirements (privacy) of the apps they build. We wanted to know if the reality\nis the same as expectations of AI-based programming assistants when trying to\nfulfil software privacy requirements, and the challenges developers face when\nusing AI-based programming assistants and how these can be improved. To this\nend, we conducted a survey with 51 developers worldwide. We found that AI-based\nprogramming assistants need to be improved in order for developers to better\ntrust them with generating code that ensures privacy. In this paper, we provide\nsome practical recommendations for developers to consider following when using\nAI-based programming assistants for privacy-related code development, and some\nkey further research directions.",
    "updated" : "2025-03-06T00:34:25Z",
    "published" : "2025-03-06T00:34:25Z",
    "authors" : [
      {
        "name" : "Kashumi Madampe"
      },
      {
        "name" : "John Grundy"
      },
      {
        "name" : "Nalin Arachchilage"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.05684v1",
    "title" : "Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints",
    "summary" : "Pre-trained foundation models can be adapted for specific tasks using\nLow-Rank Adaptation (LoRA). However, the fairness properties of these adapted\nclassifiers remain underexplored. Existing fairness-aware fine-tuning methods\nrely on direct access to sensitive attributes or their predictors, but in\npractice, these sensitive attributes are often held under strict consumer\nprivacy controls, and neither the attributes nor their predictors are available\nto model developers, hampering the development of fair models. To address this\nissue, we introduce a set of LoRA-based fine-tuning methods that can be trained\nin a distributed fashion, where model developers and fairness auditors\ncollaborate without sharing sensitive attributes or predictors. In this paper,\nwe evaluate three such methods - sensitive unlearning, adversarial training,\nand orthogonality loss - against a fairness-unaware baseline, using experiments\non the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Base\nmodel. We find that orthogonality loss consistently reduces bias while\nmaintaining or improving utility, whereas adversarial training improves False\nPositive Rate Parity and Demographic Parity in some cases, and sensitive\nunlearning provides no clear benefit. In tasks where significant biases are\npresent, distributed fairness-aware fine-tuning methods can effectively\neliminate bias without compromising consumer privacy and, in most cases,\nimprove model utility.",
    "updated" : "2025-03-07T18:49:57Z",
    "published" : "2025-03-07T18:49:57Z",
    "authors" : [
      {
        "name" : "Parameswaran Kamalaruban"
      },
      {
        "name" : "Mark Anderson"
      },
      {
        "name" : "Stuart Burrell"
      },
      {
        "name" : "Maeve Madigan"
      },
      {
        "name" : "Piotr Skalski"
      },
      {
        "name" : "David Sutton"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04980v1",
    "title" : "A Consensus Privacy Metrics Framework for Synthetic Data",
    "summary" : "Synthetic data generation is one approach for sharing individual-level data.\nHowever, to meet legislative requirements, it is necessary to demonstrate that\nthe individuals' privacy is adequately protected. There is no consolidated\nstandard for measuring privacy in synthetic data. Through an expert panel and\nconsensus process, we developed a framework for evaluating privacy in synthetic\ndata. Our findings indicate that current similarity metrics fail to measure\nidentity disclosure, and their use is discouraged. For differentially private\nsynthetic data, a privacy budget other than close to zero was not considered\ninterpretable. There was consensus on the importance of membership and\nattribute disclosure, both of which involve inferring personal information\nabout an individual without necessarily revealing their identity. The resultant\nframework provides precise recommendations for metrics that address these types\nof disclosures effectively. Our findings further present specific opportunities\nfor future research that can help with widespread adoption of synthetic data.",
    "updated" : "2025-03-06T21:19:02Z",
    "published" : "2025-03-06T21:19:02Z",
    "authors" : [
      {
        "name" : "Lisa Pilgram"
      },
      {
        "name" : "Fida K. Dankar"
      },
      {
        "name" : "Jorg Drechsler"
      },
      {
        "name" : "Mark Elliot"
      },
      {
        "name" : "Josep Domingo-Ferrer"
      },
      {
        "name" : "Paul Francis"
      },
      {
        "name" : "Murat Kantarcioglu"
      },
      {
        "name" : "Linglong Kong"
      },
      {
        "name" : "Bradley Malin"
      },
      {
        "name" : "Krishnamurty Muralidhar"
      },
      {
        "name" : "Puja Myles"
      },
      {
        "name" : "Fabian Prasser"
      },
      {
        "name" : "Jean Louis Raisaro"
      },
      {
        "name" : "Chao Yan"
      },
      {
        "name" : "Khaled El Emam"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2503.04866v1",
    "title" : "Privacy in Responsible AI: Approaches to Facial Recognition from Cloud\n  Providers",
    "summary" : "As the use of facial recognition technology is expanding in different\ndomains, ensuring its responsible use is gaining more importance. This paper\nconducts a comprehensive literature review of existing studies on facial\nrecognition technology from the perspective of privacy, which is one of the key\nResponsible AI principles.\n  Cloud providers, such as Microsoft, AWS, and Google, are at the forefront of\ndelivering facial-related technology services, but their approaches to\nresponsible use of these technologies vary significantly. This paper compares\nhow these cloud giants implement the privacy principle into their facial\nrecognition and detection services. By analysing their approaches, it\nidentifies both common practices and notable differences. The results of this\nresearch will be valuable for developers and businesses by providing them\ninsights into best practices of three major companies for integration\nresponsible AI, particularly privacy, into their cloud-based facial recognition\ntechnologies.",
    "updated" : "2025-03-06T12:04:12Z",
    "published" : "2025-03-06T12:04:12Z",
    "authors" : [
      {
        "name" : "Anna Elivanova"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  }
]