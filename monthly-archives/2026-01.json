[
  {
    "id" : "http://arxiv.org/abs/2601.00715v1",
    "title" : "PDPL Metric: Validating a Scale to Measure Personal Data Privacy Literacy Among University Students",
    "summary" : "Personal data privacy literacy (PDPL) refers to a collection of digital literacy skills related to an individuals ability to understand, evaluate, and manage the collection, use, and protection of personal data in online and digital environments. This study introduces and validates a new psychometric scale (PDPL Metric) designed to measure data privacy literacy among university students, focusing on six key privacy constructs: perceived risk of data misuse, expectations of informed consent, general privacy concern, privacy management awareness, privacy-utility trade-off acceptance, and perceived importance of data security. A 24-item questionnaire was developed and administered to students at U.S.-based research universities. Principal components analysis confirmed the unidimensionality and internal consistency of each construct, and a second-order analysis supported the integration of all six into a unified PDPL construct. No differences in PDPL were found based on basic demographic variables like academic level and gender, although a difference was found based on domestic/international status. The findings of this study offer a validated framework for assessing personal data privacy literacy within the higher education context and support the integration of the core constructs into higher education programs, organizational policies, and digital literacy initiatives on university campuses.",
    "updated" : "2026-01-02T15:12:00Z",
    "published" : "2026-01-02T15:12:00Z",
    "authors" : [
      {
        "name" : "Brady D. Lund"
      },
      {
        "name" : "Nathan Brown"
      },
      {
        "name" : "Ana Roeschley"
      },
      {
        "name" : "Gahangir Hossain"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00418v1",
    "title" : "Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution",
    "summary" : "We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.",
    "updated" : "2026-01-01T18:12:50Z",
    "published" : "2026-01-01T18:12:50Z",
    "authors" : [
      {
        "name" : "Prajwal Panth"
      },
      {
        "name" : "Sahaj Raj Malla"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00385v1",
    "title" : "Exploring the Integration of Differential Privacy in Cybersecurity Analytics: Balancing Data Utility and Privacy in Threat Intelligence",
    "summary" : "To resolve the acute problem of privacy protection and guarantee that data can be used in the context of threat intelligence, this paper considers the implementation of Differential Privacy (DP) in cybersecurity analytics. DP, which is a sound mathematical framework, ensures privacy by adding a controlled noise to data outputs and thus avoids sensitive information disclosure even with auxiliary datasets. The use of DP in Security Information and Event Management (SIEM) systems is highlighted, and it can be seen that DP has the capability to protect event log and threat data analysis without interfering with the analytical efficiency. The utility versus privacy trade-offs linked to the maximization of the epsilon parameter, which is one of the critical components of DP mechanisms, is pointed out. The article shows the transformative power of DP in promoting safe sharing of data and joint threat intelligence through real-world systems and case studies. Finally, this paper makes DP one of the key strategies to improve privacy-preserving analytics in the field of cybersecurity.",
    "updated" : "2026-01-01T16:31:47Z",
    "published" : "2026-01-01T16:31:47Z",
    "authors" : [
      {
        "name" : "Brahim Khalil Sedraoui"
      },
      {
        "name" : "Abdelmadjid Benmachiche"
      },
      {
        "name" : "Amina Makhlouf"
      },
      {
        "name" : "Chaouki Chemam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00382v1",
    "title" : "Unseen Risks of Clinical Speech-to-Text Systems: Transparency, Privacy, and Reliability Challenges in AI-Driven Documentation",
    "summary" : "AI-driven speech-to-text (STT) documentation systems are increasingly adopted in clinical settings to reduce documentation burden and improve workflow efficiency. However, their rapid deployment has outpaced understanding of the associated socio-technical risks, including transparency, reliability, patient autonomy, workflow alignment, and organizational governance. A clearer analysis of these risks is needed to support safe and equitable integration into healthcare practice. This study synthesizes interdisciplinary evidence from technical performance research, regulatory and ethical standards, clinical workflow analyses, and organizational policy guidance. The synthesis was used to develop a multi-layered socio-technical conceptual framework for evaluating and governing STT systems. Findings show that STT systems operate within tightly coupled socio-technical environments in which model performance, clinician oversight, patient rights, workflow design, and institutional governance are interdependent. The study offers a structured socio-technical governance framework and an implementation roadmap that outlines readiness assessment, vendor evaluation, pilot deployment, clinician training, ongoing monitoring, and iterative improvement. The framework emphasizes safeguards that protect patient autonomy, documentation integrity, and institutional trust while enabling the efficient and beneficial use of STT technologies. This work provides actionable guidance for healthcare organizations seeking to adopt STT systems responsibly and equitably.",
    "updated" : "2026-01-01T16:18:54Z",
    "published" : "2026-01-01T16:18:54Z",
    "authors" : [
      {
        "name" : "Nelly Elsayed"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00372v1",
    "title" : "LLM-Powered Analysis of IoT User Reviews: Tracking and Ranking Security and Privacy Concerns",
    "summary" : "Being able to understand the security and privacy (S&P) concerns of IoT users brings benefits to both developers and users. To learn about users' views, we examine Amazon IoT reviews - one of the biggest IoT markets. This work presents a state-of-the-art methodology to identify and categorize reviews in which users express S&P concerns. We developed an automated pipeline by fine-tuning GPT-3.5-Turbo to build two models: the Classifier-Rationalizer-Categorizer and the Thematic Mapper. By leveraging dynamic few-shot prompting and the model's large context size, our pipeline achieved over 97% precision and recall, significantly outperforming keyword-based and classical ML methods. We applied our pipeline to 91K Amazon reviews about fitness trackers, smart speakers and cameras, over multiple years. We found that on average 5% contained S&P concerns, while security camera exhibited the highest prevalence at 10%. Our method detected significantly more S&P-relevant reviews than prior works: 15x more for fitness trackers, 29% more for smart speakers, and 70% more for cameras. Our longitudinal analysis reveals that concerns like surveillance and data control have persisted for years, suggesting limited industry progress. We demonstrate that across all device types, users consistently demand more precise control over what data is collected and shared. We uncover challenges in multi-user and multi-device interactions, identifying two previously unreported themes concerning inadequate controls for account separation and data access. These findings, ranging from broad persistent trends to specific instances of customer loss, offer actionable insights for developers to improve user satisfaction and trust.",
    "updated" : "2026-01-01T15:24:21Z",
    "published" : "2026-01-01T15:24:21Z",
    "authors" : [
      {
        "name" : "Taufiq Islam Protick"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Nina Taft"
      },
      {
        "name" : "Anupam Das"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00337v1",
    "title" : "When Does Quantum Differential Privacy Compose?",
    "summary" : "Composition is a cornerstone of classical differential privacy, enabling strong end-to-end guarantees for complex algorithms through composition theorems (e.g., basic and advanced). In the quantum setting, however, privacy is defined operationally against arbitrary measurements, and classical composition arguments based on scalar privacy-loss random variables no longer apply. As a result, it has remained unclear when meaningful composition guarantees can be obtained for quantum differential privacy (QDP).\n  In this work, we clarify both the limitations and possibilities of composition in the quantum setting. We first show that classical-style composition fails in full generality for POVM-based approximate QDP: even quantum channels that are individually perfectly private can completely lose privacy when combined through correlated joint implementations. We then identify a setting in which clean composition guarantees can be restored. For tensor-product channels acting on product neighboring inputs, we introduce a quantum moments accountant based on an operator-valued notion of privacy loss and a matrix moment-generating function. Although the resulting Rényi-type divergence does not satisfy a data-processing inequality, we prove that controlling its moments suffices to bound measured Rényi divergence, yielding operational privacy guarantees against arbitrary measurements. This leads to advanced-composition-style bounds with the same leading-order behavior as in the classical theory.\n  Our results demonstrate that meaningful composition theorems for quantum differential privacy require carefully articulated structural assumptions on channels, inputs, and adversarial measurements, and provide a principled framework for understanding which classical ideas do and do not extend to the quantum setting.",
    "updated" : "2026-01-01T13:24:09Z",
    "published" : "2026-01-01T13:24:09Z",
    "authors" : [
      {
        "name" : "Daniel Alabi"
      },
      {
        "name" : "Theshani Nuradha"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.02307v1",
    "title" : "Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck",
    "summary" : "We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.",
    "updated" : "2026-01-05T17:49:39Z",
    "published" : "2026-01-05T17:49:39Z",
    "authors" : [
      {
        "name" : "Dina El Zein"
      },
      {
        "name" : "James Henderson"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.02245v1",
    "title" : "MOZAIK: A Privacy-Preserving Analytics Platform for IoT Data Using MPC and FHE",
    "summary" : "The rapid increase of Internet of Things (IoT) systems across several domains has led to the generation of vast volumes of sensitive data, presenting significant challenges in terms of storage and data analytics. Cloud-assisted IoT solutions offer storage, scalability, and computational resources, but introduce new security and privacy risks that conventional trust-based approaches fail to adequately mitigate. To address these challenges, this paper presents MOZAIK, a novel end-to-end privacy-preserving confidential data storage and distributed processing architecture tailored for IoT-to-cloud scenarios. MOZAIK ensures that data remains encrypted throughout its lifecycle, including during transmission, storage, and processing. This is achieved by employing a cryptographic privacy-enhancing technology known as computing on encrypted data (COED). Two distinct COED techniques are explored, specifically secure multi-party computation (MPC) and fully homomorphic encryption (FHE). The paper includes a comprehensive analysis of the MOZAIK architecture, including a proof-of-concept implementation and performance evaluations. The evaluation results demonstrate the feasibility of the MOZAIK system and indicate the cost of an end-to-end privacy-preserving system compared to regular plaintext alternatives. All components of the MOZAIK platform are released as open-source software alongside this publication, with the aim of advancing secure and privacy-preserving data processing practices.",
    "updated" : "2026-01-05T16:25:08Z",
    "published" : "2026-01-05T16:25:08Z",
    "authors" : [
      {
        "name" : "Michiel Van Kenhove"
      },
      {
        "name" : "Erik Pohle"
      },
      {
        "name" : "Leonard Schild"
      },
      {
        "name" : "Martin Zbudila"
      },
      {
        "name" : "Merlijn Sebrechts"
      },
      {
        "name" : "Filip De Turck"
      },
      {
        "name" : "Bruno Volckaert"
      },
      {
        "name" : "Aysajan Abidin"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.01993v1",
    "title" : "MindChat: A Privacy-preserving Large Language Model for Mental Health Support",
    "summary" : "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.",
    "updated" : "2026-01-05T10:54:18Z",
    "published" : "2026-01-05T10:54:18Z",
    "authors" : [
      {
        "name" : "Dong Xue"
      },
      {
        "name" : "Jicheng Tu"
      },
      {
        "name" : "Ming Wang"
      },
      {
        "name" : "Xin Yan"
      },
      {
        "name" : "Fangzhou Liu"
      },
      {
        "name" : "Jie Hu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.01737v1",
    "title" : "Local Layer-wise Differential Privacy in Federated Learning",
    "summary" : "Federated Learning (FL) enables collaborative model training without direct data sharing, yet it remains vulnerable to privacy attacks such as model inversion and membership inference. Existing differential privacy (DP) solutions for FL often inject noise uniformly across the entire model, degrading utility while providing suboptimal privacy-utility tradeoffs. To address this, we propose LaDP, a novel layer-wise adaptive noise injection mechanism for FL that optimizes privacy protection while preserving model accuracy. LaDP leverages two key insights: (1) neural network layers contribute unevenly to model utility, and (2) layer-wise privacy leakage can be quantified via KL divergence between local and global model distributions. LaDP dynamically injects noise into selected layers based on their privacy sensitivity and importance to model performance.\n  We provide a rigorous theoretical analysis, proving that LaDP satisfies $(ε, δ)$-DP guarantees and converges under bounded noise. Extensive experiments on CIFAR-10/100 datasets demonstrate that LaDP reduces noise injection by 46.14% on average compared to state-of-the-art (SOTA) methods while improving accuracy by 102.99%. Under the same privacy budget, LaDP outperforms SOTA solutions like Dynamic Privacy Allocation LDP and AdapLDP by 25.18% and 6.1% in accuracy, respectively. Additionally, LaDP robustly defends against reconstruction attacks, increasing the FID of the reconstructed private data by $>$12.84% compared to all baselines. Our work advances the practical deployment of privacy-preserving FL with minimal utility loss.",
    "updated" : "2026-01-05T02:23:31Z",
    "published" : "2026-01-05T02:23:31Z",
    "authors" : [
      {
        "name" : "Yunbo Li"
      },
      {
        "name" : "Jiaping Gui"
      },
      {
        "name" : "Fanchao Meng"
      },
      {
        "name" : "Yue Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.01710v1",
    "title" : "Publishing Below-Threshold Triangle Counts under Local Weight Differential Privacy",
    "summary" : "We propose an algorithm for counting below-threshold triangles in weighted graphs under local weight differential privacy. While prior work focused on unweighted graphs, many real-world networks naturally include edge weights. We study the setting where the graph topology is public known and the privacy of the influence of an individual on the edge weights is protected. This captures realistic scenarios such as road networks and telecommunication networks. Our approach consists of two rounds of communication. In the first round, each node publishes their incident weight information under local weight differential privacy while in the second round, the nodes locally count below-threshold triangles, for which we introduce a biased and unbiased variant. We further propose two different improvements. We present a pre-computation step that reduces the covariance and thereby lowers the expected error. Secondly, we develop an algorithm for computing the smooth-sensitivity, which significantly reduces the running time compared to a straightforward approach. Finally, we provide experimental results that demonstrate the differences between the biased and unbiased variants and the effectiveness of the proposed improvements.",
    "updated" : "2026-01-05T01:10:56Z",
    "published" : "2026-01-05T01:10:56Z",
    "authors" : [
      {
        "name" : "Kevin Pfisterer"
      },
      {
        "name" : "Quentin Hillebrand"
      },
      {
        "name" : "Vorapong Suppakitpaisarn"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.01668v1",
    "title" : "EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records",
    "summary" : "Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.",
    "updated" : "2026-01-04T21:10:42Z",
    "published" : "2026-01-04T21:10:42Z",
    "authors" : [
      {
        "name" : "Houman Kazemzadeh"
      },
      {
        "name" : "Nima Minaifar"
      },
      {
        "name" : "Kamyar Naderi"
      },
      {
        "name" : "Sho Tabibzadeh"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.01239v1",
    "title" : "IO-RAE: Information-Obfuscation Reversible Adversarial Example for Audio Privacy Protection",
    "summary" : "The rapid advancements in artificial intelligence have significantly accelerated the adoption of speech recognition technology, leading to its widespread integration across various applications. However, this surge in usage also highlights a critical issue: audio data is highly vulnerable to unauthorized exposure and analysis, posing significant privacy risks for businesses and individuals. This paper introduces an Information-Obfuscation Reversible Adversarial Example (IO-RAE) framework, the pioneering method designed to safeguard audio privacy using reversible adversarial examples. IO-RAE leverages large language models to generate misleading yet contextually coherent content, effectively preventing unauthorized eavesdropping by humans and Automatic Speech Recognition (ASR) systems. Additionally, we propose the Cumulative Signal Attack technique, which mitigates high-frequency noise and enhances attack efficacy by targeting low-frequency signals. Our approach ensures the protection of audio data without degrading its quality or our ability. Experimental evaluations demonstrate the superiority of our method, achieving a targeted misguidance rate of 96.5% and a remarkable 100% untargeted misguidance rate in obfuscating target keywords across multiple ASR models, including a commercial black-box system from Google. Furthermore, the quality of the recovered audio, measured by the Perceptual Evaluation of Speech Quality score, reached 4.45, comparable to high-quality original recordings. Notably, the recovered audio processed by ASR systems exhibited an error rate of 0%, indicating nearly lossless recovery. These results highlight the practical applicability and effectiveness of our IO-RAE framework in protecting sensitive audio privacy.",
    "updated" : "2026-01-03T17:08:35Z",
    "published" : "2026-01-03T17:08:35Z",
    "authors" : [
      {
        "name" : "Jiajie Zhu"
      },
      {
        "name" : "Xia Du"
      },
      {
        "name" : "Xiaoyuan Liu"
      },
      {
        "name" : "Jizhe Zhou"
      },
      {
        "name" : "Qizhen Xu"
      },
      {
        "name" : "Zheng Lin"
      },
      {
        "name" : "Chi-Man Pun"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CR",
      "cs.MM",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00911v1",
    "title" : "Device-Native Autonomous Agents for Privacy-Preserving Negotiations",
    "summary" : "Automated negotiations in insurance and business-to-business (B2B) commerce encounter substantial challenges. Current systems force a trade-off between convenience and privacy by routing sensitive financial data through centralized servers, increasing security risks, and diminishing user trust. This study introduces a device-native autonomous Artificial Intelligence (AI) agent system for privacy-preserving negotiations. The proposed system operates exclusively on user hardware, enabling real-time bargaining while maintaining sensitive constraints locally. It integrates zero-knowledge proofs to ensure privacy and employs distilled world models to support advanced on-device reasoning. The architecture incorporates six technical components within an agentic AI workflow. Agents autonomously plan negotiation strategies, conduct secure multi-party bargaining, and generate cryptographic audit trails without exposing user data to external servers. The system is evaluated in insurance and B2B procurement scenarios across diverse device configurations. Results show an average success rate of 87%, a 2.4x latency improvement over cloud baselines, and strong privacy preservation through zero-knowledge proofs. User studies show 27% higher trust scores when decision trails are available. These findings establish a foundation for trustworthy autonomous agents in privacy-sensitive financial domains.",
    "updated" : "2026-01-01T04:29:39Z",
    "published" : "2026-01-01T04:29:39Z",
    "authors" : [
      {
        "name" : "Joyjit Roy"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.02855v1",
    "title" : "Context-aware Privacy Bounds for Linear Queries",
    "summary" : "Linear queries, as the basis of broad analysis tasks, are often released through privacy mechanisms based on differential privacy (DP), the most popular framework for privacy protection. However, DP adopts a context-free definition that operates independently of the data-generating distribution. In this paper, we revisit the privacy analysis of the Laplace mechanism through the lens of pointwise maximal leakage (PML). We demonstrate that the distribution-agnostic definition of the DP framework often mandates excessive noise. To address this, we incorporate an assumption about the prior distribution by lower-bounding the probability of any single record belonging to any specific class. With this assumption, we derive a tight, context-aware leakage bound for general linear queries, and prove that our derived bound is strictly tighter than the standard DP guarantee and converges to the DP guarantee as this probability lower bound approaches zero. Numerical evaluations demonstrate that by exploiting this prior knowledge, the required noise scale can be reduced while maintaining privacy guarantees.",
    "updated" : "2026-01-06T09:34:04Z",
    "published" : "2026-01-06T09:34:04Z",
    "authors" : [
      {
        "name" : "Heng Zhao"
      },
      {
        "name" : "Sara Saeidian"
      },
      {
        "name" : "Tobias J. Oechtering"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.02720v1",
    "title" : "Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System",
    "summary" : "Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.",
    "updated" : "2026-01-06T05:18:03Z",
    "published" : "2026-01-06T05:18:03Z",
    "authors" : [
      {
        "name" : "Yuqiao Xu"
      },
      {
        "name" : "Mina Namazi"
      },
      {
        "name" : "Sahith Reddy Jalapally"
      },
      {
        "name" : "Osama Zafar"
      },
      {
        "name" : "Youngjin Yoo"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.03979v1",
    "title" : "SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems",
    "summary" : "The continued promise of Large Language Models (LLMs), particularly in their natural language understanding and generation capabilities, has driven a rapidly increasing interest in identifying and developing LLM use cases. In an effort to complement the ingrained \"knowledge\" of LLMs, Retrieval-Augmented Generation (RAG) techniques have become widely popular. At its core, RAG involves the coupling of LLMs with domain-specific knowledge bases, whereby the generation of a response to a user question is augmented with contextual and up-to-date information. The proliferation of RAG has sparked concerns about data privacy, particularly with the inherent risks that arise when leveraging databases with potentially sensitive information. Numerous recent works have explored various aspects of privacy risks in RAG systems, from adversarial attacks to proposed mitigations. With the goal of surveying and unifying these works, we ask one simple question: What are the privacy risks in RAG, and how can they be measured and mitigated? To answer this question, we conduct a systematic literature review of RAG works addressing privacy, and we systematize our findings into a comprehensive set of privacy risks, mitigation techniques, and evaluation strategies. We supplement these findings with two primary artifacts: a Taxonomy of RAG Privacy Risks and a RAG Privacy Process Diagram. Our work contributes to the study of privacy in RAG not only by conducting the first systematization of risks and mitigations, but also by uncovering important considerations when mitigating privacy risks in RAG systems and assessing the current maturity of proposed mitigations.",
    "updated" : "2026-01-07T14:50:41Z",
    "published" : "2026-01-07T14:50:41Z",
    "authors" : [
      {
        "name" : "Andreea-Elena Bodea"
      },
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.03587v1",
    "title" : "Deontic Knowledge Graphs for Privacy Compliance in Multimodal Disaster Data Sharing",
    "summary" : "Disaster response requires sharing heterogeneous artifacts, from tabular assistance records to UAS imagery, under overlapping privacy mandates. Operational systems often reduce compliance to binary access control, which is brittle in time-critical workflows. We present a novel deontic knowledge graph-based framework that integrates a Disaster Management Knowledge Graph (DKG) with a Policy Knowledge Graph (PKG) derived from IoT-Reg and FEMA/DHS privacy drivers. Our release decision function supports three outcomes: Allow, Block, and Allow-with-Transform. The latter binds obligations to transforms and verifies post-transform compliance via provenance-linked derived artifacts; blocked requests are logged as semantic privacy incidents. Evaluation on a 5.1M-triple DKG with 316K images shows exact-match decision correctness, sub-second per-decision latency, and interactive query performance across both single-graph and federated workloads.",
    "updated" : "2026-01-07T05:02:12Z",
    "published" : "2026-01-07T05:02:12Z",
    "authors" : [
      {
        "name" : "Kelvin Uzoma Echenim"
      },
      {
        "name" : "Karuna Pande Joshi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.03546v1",
    "title" : "Value-Action Alignment in Large Language Models under Privacy-Prosocial Conflict",
    "summary" : "Large language models (LLMs) are increasingly used to simulate decision-making tasks involving personal data sharing, where privacy concerns and prosocial motivations can push choices in opposite directions. Existing evaluations often measure privacy-related attitudes or sharing intentions in isolation, which makes it difficult to determine whether a model's expressed values jointly predict its downstream data-sharing actions as in real human behaviors. We introduce a context-based assessment protocol that sequentially administers standardized questionnaires for privacy attitudes, prosocialness, and acceptance of data sharing within a bounded, history-carrying session. To evaluate value-action alignments under competing attitudes, we use multi-group structural equation modeling (MGSEM) to identify relations from privacy concerns and prosocialness to data sharing. We propose Value-Action Alignment Rate (VAAR), a human-referenced directional agreement metric that aggregates path-level evidence for expected signs. Across multiple LLMs, we observe stable but model-specific Privacy-PSA-AoDS profiles, and substantial heterogeneity in value-action alignment.",
    "updated" : "2026-01-07T03:30:42Z",
    "published" : "2026-01-07T03:30:42Z",
    "authors" : [
      {
        "name" : "Guanyu Chen"
      },
      {
        "name" : "Chenxiao Yu"
      },
      {
        "name" : "Xiyang Hu"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.03508v1",
    "title" : "A Critical Analysis of the Medibank Health Data Breach and Differential Privacy Solutions",
    "summary" : "This paper critically examines the 2022 Medibank health insurance data breach, which exposed sensitive medical records of 9.7 million individuals due to unencrypted storage, centralized access, and the absence of privacy-preserving analytics. To address these vulnerabilities, we propose an entropy-aware differential privacy (DP) framework that integrates Laplace and Gaussian mechanisms with adaptive budget allocation. The design incorporates TLS-encrypted database access, field-level mechanism selection, and smooth sensitivity models to mitigate re-identification risks. Experimental validation was conducted using synthetic Medibank datasets (N = 131,000) with entropy-calibrated DP mechanisms, where high-entropy attributes received stronger noise injection. Results demonstrate a 90.3% reduction in re-identification probability while maintaining analytical utility loss below 24%. The framework further aligns with GDPR Article 32 and Australian Privacy Principle 11.1, ensuring regulatory compliance. By combining rigorous privacy guarantees with practical usability, this work contributes a scalable and technically feasible solution for healthcare data protection, offering a pathway toward resilient, trustworthy, and regulation-ready medical analytics.",
    "updated" : "2026-01-07T01:42:36Z",
    "published" : "2026-01-07T01:42:36Z",
    "authors" : [
      {
        "name" : "Zhuohan Cui"
      },
      {
        "name" : "Qianqian Lang"
      },
      {
        "name" : "Zikun Song"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.03429v1",
    "title" : "DeepLeak: Privacy Enhancing Hardening of Model Explanations Against Membership Leakage",
    "summary" : "Machine learning (ML) explainability is central to algorithmic transparency in high-stakes settings such as predictive diagnostics and loan approval. However, these same domains require rigorous privacy guaranties, creating tension between interpretability and privacy. Although prior work has shown that explanation methods can leak membership information, practitioners still lack systematic guidance on selecting or deploying explanation techniques that balance transparency with privacy.\n  We present DeepLeak, a system to audit and mitigate privacy risks in post-hoc explanation methods. DeepLeak advances the state-of-the-art in three ways: (1) comprehensive leakage profiling: we develop a stronger explanation-aware membership inference attack (MIA) to quantify how much representative explanation methods leak membership information under default configurations; (2) lightweight hardening strategies: we introduce practical, model-agnostic mitigations, including sensitivity-calibrated noise, attribution clipping, and masking, that substantially reduce membership leakage while preserving explanation utility; and (3) root-cause analysis: through controlled experiments, we pinpoint algorithmic properties (e.g., attribution sparsity and sensitivity) that drive leakage.\n  Evaluating 15 explanation techniques across four families on image benchmarks, DeepLeak shows that default settings can leak up to 74.9% more membership information than previously reported. Our mitigations cut leakage by up to 95% (minimum 46.5%) with only <=3.3% utility loss on average. DeepLeak offers a systematic, reproducible path to safer explainability in privacy-sensitive ML.",
    "updated" : "2026-01-06T21:34:27Z",
    "published" : "2026-01-06T21:34:27Z",
    "authors" : [
      {
        "name" : "Firas Ben Hmida"
      },
      {
        "name" : "Zain Sbeih"
      },
      {
        "name" : "Philemon Hailemariam"
      },
      {
        "name" : "Birhanu Eshete"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00911v1",
    "title" : "Device-Native Autonomous Agents for Privacy-Preserving Negotiations",
    "summary" : "Automated negotiations in insurance and business-to-business (B2B) commerce encounter substantial challenges. Current systems force a trade-off between convenience and privacy by routing sensitive financial data through centralized servers, increasing security risks, and diminishing user trust. This study introduces a device-native autonomous Artificial Intelligence (AI) agent system for privacy-preserving negotiations. The proposed system operates exclusively on user hardware, enabling real-time bargaining while maintaining sensitive constraints locally. It integrates zero-knowledge proofs to ensure privacy and employs distilled world models to support advanced on-device reasoning. The architecture incorporates six technical components within an agentic AI workflow. Agents autonomously plan negotiation strategies, conduct secure multi-party bargaining, and generate cryptographic audit trails without exposing user data to external servers. The system is evaluated in insurance and B2B procurement scenarios across diverse device configurations. Results show an average success rate of 87%, a 2.4x latency improvement over cloud baselines, and strong privacy preservation through zero-knowledge proofs. User studies show 27% higher trust scores when decision trails are available. These findings establish a foundation for trustworthy autonomous agents in privacy-sensitive financial domains.",
    "updated" : "2026-01-01T04:29:39Z",
    "published" : "2026-01-01T04:29:39Z",
    "authors" : [
      {
        "name" : "Joyjit Roy"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.LG"
    ]
  }
]