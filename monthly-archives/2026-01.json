[
  {
    "id" : "http://arxiv.org/abs/2601.00715v1",
    "title" : "PDPL Metric: Validating a Scale to Measure Personal Data Privacy Literacy Among University Students",
    "summary" : "Personal data privacy literacy (PDPL) refers to a collection of digital literacy skills related to an individuals ability to understand, evaluate, and manage the collection, use, and protection of personal data in online and digital environments. This study introduces and validates a new psychometric scale (PDPL Metric) designed to measure data privacy literacy among university students, focusing on six key privacy constructs: perceived risk of data misuse, expectations of informed consent, general privacy concern, privacy management awareness, privacy-utility trade-off acceptance, and perceived importance of data security. A 24-item questionnaire was developed and administered to students at U.S.-based research universities. Principal components analysis confirmed the unidimensionality and internal consistency of each construct, and a second-order analysis supported the integration of all six into a unified PDPL construct. No differences in PDPL were found based on basic demographic variables like academic level and gender, although a difference was found based on domestic/international status. The findings of this study offer a validated framework for assessing personal data privacy literacy within the higher education context and support the integration of the core constructs into higher education programs, organizational policies, and digital literacy initiatives on university campuses.",
    "updated" : "2026-01-02T15:12:00Z",
    "published" : "2026-01-02T15:12:00Z",
    "authors" : [
      {
        "name" : "Brady D. Lund"
      },
      {
        "name" : "Nathan Brown"
      },
      {
        "name" : "Ana Roeschley"
      },
      {
        "name" : "Gahangir Hossain"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00418v1",
    "title" : "Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution",
    "summary" : "We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.",
    "updated" : "2026-01-01T18:12:50Z",
    "published" : "2026-01-01T18:12:50Z",
    "authors" : [
      {
        "name" : "Prajwal Panth"
      },
      {
        "name" : "Sahaj Raj Malla"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00385v1",
    "title" : "Exploring the Integration of Differential Privacy in Cybersecurity Analytics: Balancing Data Utility and Privacy in Threat Intelligence",
    "summary" : "To resolve the acute problem of privacy protection and guarantee that data can be used in the context of threat intelligence, this paper considers the implementation of Differential Privacy (DP) in cybersecurity analytics. DP, which is a sound mathematical framework, ensures privacy by adding a controlled noise to data outputs and thus avoids sensitive information disclosure even with auxiliary datasets. The use of DP in Security Information and Event Management (SIEM) systems is highlighted, and it can be seen that DP has the capability to protect event log and threat data analysis without interfering with the analytical efficiency. The utility versus privacy trade-offs linked to the maximization of the epsilon parameter, which is one of the critical components of DP mechanisms, is pointed out. The article shows the transformative power of DP in promoting safe sharing of data and joint threat intelligence through real-world systems and case studies. Finally, this paper makes DP one of the key strategies to improve privacy-preserving analytics in the field of cybersecurity.",
    "updated" : "2026-01-01T16:31:47Z",
    "published" : "2026-01-01T16:31:47Z",
    "authors" : [
      {
        "name" : "Brahim Khalil Sedraoui"
      },
      {
        "name" : "Abdelmadjid Benmachiche"
      },
      {
        "name" : "Amina Makhlouf"
      },
      {
        "name" : "Chaouki Chemam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00382v1",
    "title" : "Unseen Risks of Clinical Speech-to-Text Systems: Transparency, Privacy, and Reliability Challenges in AI-Driven Documentation",
    "summary" : "AI-driven speech-to-text (STT) documentation systems are increasingly adopted in clinical settings to reduce documentation burden and improve workflow efficiency. However, their rapid deployment has outpaced understanding of the associated socio-technical risks, including transparency, reliability, patient autonomy, workflow alignment, and organizational governance. A clearer analysis of these risks is needed to support safe and equitable integration into healthcare practice. This study synthesizes interdisciplinary evidence from technical performance research, regulatory and ethical standards, clinical workflow analyses, and organizational policy guidance. The synthesis was used to develop a multi-layered socio-technical conceptual framework for evaluating and governing STT systems. Findings show that STT systems operate within tightly coupled socio-technical environments in which model performance, clinician oversight, patient rights, workflow design, and institutional governance are interdependent. The study offers a structured socio-technical governance framework and an implementation roadmap that outlines readiness assessment, vendor evaluation, pilot deployment, clinician training, ongoing monitoring, and iterative improvement. The framework emphasizes safeguards that protect patient autonomy, documentation integrity, and institutional trust while enabling the efficient and beneficial use of STT technologies. This work provides actionable guidance for healthcare organizations seeking to adopt STT systems responsibly and equitably.",
    "updated" : "2026-01-01T16:18:54Z",
    "published" : "2026-01-01T16:18:54Z",
    "authors" : [
      {
        "name" : "Nelly Elsayed"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00372v1",
    "title" : "LLM-Powered Analysis of IoT User Reviews: Tracking and Ranking Security and Privacy Concerns",
    "summary" : "Being able to understand the security and privacy (S&P) concerns of IoT users brings benefits to both developers and users. To learn about users' views, we examine Amazon IoT reviews - one of the biggest IoT markets. This work presents a state-of-the-art methodology to identify and categorize reviews in which users express S&P concerns. We developed an automated pipeline by fine-tuning GPT-3.5-Turbo to build two models: the Classifier-Rationalizer-Categorizer and the Thematic Mapper. By leveraging dynamic few-shot prompting and the model's large context size, our pipeline achieved over 97% precision and recall, significantly outperforming keyword-based and classical ML methods. We applied our pipeline to 91K Amazon reviews about fitness trackers, smart speakers and cameras, over multiple years. We found that on average 5% contained S&P concerns, while security camera exhibited the highest prevalence at 10%. Our method detected significantly more S&P-relevant reviews than prior works: 15x more for fitness trackers, 29% more for smart speakers, and 70% more for cameras. Our longitudinal analysis reveals that concerns like surveillance and data control have persisted for years, suggesting limited industry progress. We demonstrate that across all device types, users consistently demand more precise control over what data is collected and shared. We uncover challenges in multi-user and multi-device interactions, identifying two previously unreported themes concerning inadequate controls for account separation and data access. These findings, ranging from broad persistent trends to specific instances of customer loss, offer actionable insights for developers to improve user satisfaction and trust.",
    "updated" : "2026-01-01T15:24:21Z",
    "published" : "2026-01-01T15:24:21Z",
    "authors" : [
      {
        "name" : "Taufiq Islam Protick"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Nina Taft"
      },
      {
        "name" : "Anupam Das"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00337v1",
    "title" : "When Does Quantum Differential Privacy Compose?",
    "summary" : "Composition is a cornerstone of classical differential privacy, enabling strong end-to-end guarantees for complex algorithms through composition theorems (e.g., basic and advanced). In the quantum setting, however, privacy is defined operationally against arbitrary measurements, and classical composition arguments based on scalar privacy-loss random variables no longer apply. As a result, it has remained unclear when meaningful composition guarantees can be obtained for quantum differential privacy (QDP).\n  In this work, we clarify both the limitations and possibilities of composition in the quantum setting. We first show that classical-style composition fails in full generality for POVM-based approximate QDP: even quantum channels that are individually perfectly private can completely lose privacy when combined through correlated joint implementations. We then identify a setting in which clean composition guarantees can be restored. For tensor-product channels acting on product neighboring inputs, we introduce a quantum moments accountant based on an operator-valued notion of privacy loss and a matrix moment-generating function. Although the resulting Rényi-type divergence does not satisfy a data-processing inequality, we prove that controlling its moments suffices to bound measured Rényi divergence, yielding operational privacy guarantees against arbitrary measurements. This leads to advanced-composition-style bounds with the same leading-order behavior as in the classical theory.\n  Our results demonstrate that meaningful composition theorems for quantum differential privacy require carefully articulated structural assumptions on channels, inputs, and adversarial measurements, and provide a principled framework for understanding which classical ideas do and do not extend to the quantum setting.",
    "updated" : "2026-01-01T13:24:09Z",
    "published" : "2026-01-01T13:24:09Z",
    "authors" : [
      {
        "name" : "Daniel Alabi"
      },
      {
        "name" : "Theshani Nuradha"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.02307v1",
    "title" : "Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck",
    "summary" : "We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.",
    "updated" : "2026-01-05T17:49:39Z",
    "published" : "2026-01-05T17:49:39Z",
    "authors" : [
      {
        "name" : "Dina El Zein"
      },
      {
        "name" : "James Henderson"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.02245v1",
    "title" : "MOZAIK: A Privacy-Preserving Analytics Platform for IoT Data Using MPC and FHE",
    "summary" : "The rapid increase of Internet of Things (IoT) systems across several domains has led to the generation of vast volumes of sensitive data, presenting significant challenges in terms of storage and data analytics. Cloud-assisted IoT solutions offer storage, scalability, and computational resources, but introduce new security and privacy risks that conventional trust-based approaches fail to adequately mitigate. To address these challenges, this paper presents MOZAIK, a novel end-to-end privacy-preserving confidential data storage and distributed processing architecture tailored for IoT-to-cloud scenarios. MOZAIK ensures that data remains encrypted throughout its lifecycle, including during transmission, storage, and processing. This is achieved by employing a cryptographic privacy-enhancing technology known as computing on encrypted data (COED). Two distinct COED techniques are explored, specifically secure multi-party computation (MPC) and fully homomorphic encryption (FHE). The paper includes a comprehensive analysis of the MOZAIK architecture, including a proof-of-concept implementation and performance evaluations. The evaluation results demonstrate the feasibility of the MOZAIK system and indicate the cost of an end-to-end privacy-preserving system compared to regular plaintext alternatives. All components of the MOZAIK platform are released as open-source software alongside this publication, with the aim of advancing secure and privacy-preserving data processing practices.",
    "updated" : "2026-01-05T16:25:08Z",
    "published" : "2026-01-05T16:25:08Z",
    "authors" : [
      {
        "name" : "Michiel Van Kenhove"
      },
      {
        "name" : "Erik Pohle"
      },
      {
        "name" : "Leonard Schild"
      },
      {
        "name" : "Martin Zbudila"
      },
      {
        "name" : "Merlijn Sebrechts"
      },
      {
        "name" : "Filip De Turck"
      },
      {
        "name" : "Bruno Volckaert"
      },
      {
        "name" : "Aysajan Abidin"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.01993v1",
    "title" : "MindChat: A Privacy-preserving Large Language Model for Mental Health Support",
    "summary" : "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.",
    "updated" : "2026-01-05T10:54:18Z",
    "published" : "2026-01-05T10:54:18Z",
    "authors" : [
      {
        "name" : "Dong Xue"
      },
      {
        "name" : "Jicheng Tu"
      },
      {
        "name" : "Ming Wang"
      },
      {
        "name" : "Xin Yan"
      },
      {
        "name" : "Fangzhou Liu"
      },
      {
        "name" : "Jie Hu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.01737v1",
    "title" : "Local Layer-wise Differential Privacy in Federated Learning",
    "summary" : "Federated Learning (FL) enables collaborative model training without direct data sharing, yet it remains vulnerable to privacy attacks such as model inversion and membership inference. Existing differential privacy (DP) solutions for FL often inject noise uniformly across the entire model, degrading utility while providing suboptimal privacy-utility tradeoffs. To address this, we propose LaDP, a novel layer-wise adaptive noise injection mechanism for FL that optimizes privacy protection while preserving model accuracy. LaDP leverages two key insights: (1) neural network layers contribute unevenly to model utility, and (2) layer-wise privacy leakage can be quantified via KL divergence between local and global model distributions. LaDP dynamically injects noise into selected layers based on their privacy sensitivity and importance to model performance.\n  We provide a rigorous theoretical analysis, proving that LaDP satisfies $(ε, δ)$-DP guarantees and converges under bounded noise. Extensive experiments on CIFAR-10/100 datasets demonstrate that LaDP reduces noise injection by 46.14% on average compared to state-of-the-art (SOTA) methods while improving accuracy by 102.99%. Under the same privacy budget, LaDP outperforms SOTA solutions like Dynamic Privacy Allocation LDP and AdapLDP by 25.18% and 6.1% in accuracy, respectively. Additionally, LaDP robustly defends against reconstruction attacks, increasing the FID of the reconstructed private data by $>$12.84% compared to all baselines. Our work advances the practical deployment of privacy-preserving FL with minimal utility loss.",
    "updated" : "2026-01-05T02:23:31Z",
    "published" : "2026-01-05T02:23:31Z",
    "authors" : [
      {
        "name" : "Yunbo Li"
      },
      {
        "name" : "Jiaping Gui"
      },
      {
        "name" : "Fanchao Meng"
      },
      {
        "name" : "Yue Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.01710v1",
    "title" : "Publishing Below-Threshold Triangle Counts under Local Weight Differential Privacy",
    "summary" : "We propose an algorithm for counting below-threshold triangles in weighted graphs under local weight differential privacy. While prior work focused on unweighted graphs, many real-world networks naturally include edge weights. We study the setting where the graph topology is public known and the privacy of the influence of an individual on the edge weights is protected. This captures realistic scenarios such as road networks and telecommunication networks. Our approach consists of two rounds of communication. In the first round, each node publishes their incident weight information under local weight differential privacy while in the second round, the nodes locally count below-threshold triangles, for which we introduce a biased and unbiased variant. We further propose two different improvements. We present a pre-computation step that reduces the covariance and thereby lowers the expected error. Secondly, we develop an algorithm for computing the smooth-sensitivity, which significantly reduces the running time compared to a straightforward approach. Finally, we provide experimental results that demonstrate the differences between the biased and unbiased variants and the effectiveness of the proposed improvements.",
    "updated" : "2026-01-05T01:10:56Z",
    "published" : "2026-01-05T01:10:56Z",
    "authors" : [
      {
        "name" : "Kevin Pfisterer"
      },
      {
        "name" : "Quentin Hillebrand"
      },
      {
        "name" : "Vorapong Suppakitpaisarn"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.01668v1",
    "title" : "EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records",
    "summary" : "Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.",
    "updated" : "2026-01-04T21:10:42Z",
    "published" : "2026-01-04T21:10:42Z",
    "authors" : [
      {
        "name" : "Houman Kazemzadeh"
      },
      {
        "name" : "Nima Minaifar"
      },
      {
        "name" : "Kamyar Naderi"
      },
      {
        "name" : "Sho Tabibzadeh"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.01239v1",
    "title" : "IO-RAE: Information-Obfuscation Reversible Adversarial Example for Audio Privacy Protection",
    "summary" : "The rapid advancements in artificial intelligence have significantly accelerated the adoption of speech recognition technology, leading to its widespread integration across various applications. However, this surge in usage also highlights a critical issue: audio data is highly vulnerable to unauthorized exposure and analysis, posing significant privacy risks for businesses and individuals. This paper introduces an Information-Obfuscation Reversible Adversarial Example (IO-RAE) framework, the pioneering method designed to safeguard audio privacy using reversible adversarial examples. IO-RAE leverages large language models to generate misleading yet contextually coherent content, effectively preventing unauthorized eavesdropping by humans and Automatic Speech Recognition (ASR) systems. Additionally, we propose the Cumulative Signal Attack technique, which mitigates high-frequency noise and enhances attack efficacy by targeting low-frequency signals. Our approach ensures the protection of audio data without degrading its quality or our ability. Experimental evaluations demonstrate the superiority of our method, achieving a targeted misguidance rate of 96.5% and a remarkable 100% untargeted misguidance rate in obfuscating target keywords across multiple ASR models, including a commercial black-box system from Google. Furthermore, the quality of the recovered audio, measured by the Perceptual Evaluation of Speech Quality score, reached 4.45, comparable to high-quality original recordings. Notably, the recovered audio processed by ASR systems exhibited an error rate of 0%, indicating nearly lossless recovery. These results highlight the practical applicability and effectiveness of our IO-RAE framework in protecting sensitive audio privacy.",
    "updated" : "2026-01-03T17:08:35Z",
    "published" : "2026-01-03T17:08:35Z",
    "authors" : [
      {
        "name" : "Jiajie Zhu"
      },
      {
        "name" : "Xia Du"
      },
      {
        "name" : "Xiaoyuan Liu"
      },
      {
        "name" : "Jizhe Zhou"
      },
      {
        "name" : "Qizhen Xu"
      },
      {
        "name" : "Zheng Lin"
      },
      {
        "name" : "Chi-Man Pun"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CR",
      "cs.MM",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00911v1",
    "title" : "Device-Native Autonomous Agents for Privacy-Preserving Negotiations",
    "summary" : "Automated negotiations in insurance and business-to-business (B2B) commerce encounter substantial challenges. Current systems force a trade-off between convenience and privacy by routing sensitive financial data through centralized servers, increasing security risks, and diminishing user trust. This study introduces a device-native autonomous Artificial Intelligence (AI) agent system for privacy-preserving negotiations. The proposed system operates exclusively on user hardware, enabling real-time bargaining while maintaining sensitive constraints locally. It integrates zero-knowledge proofs to ensure privacy and employs distilled world models to support advanced on-device reasoning. The architecture incorporates six technical components within an agentic AI workflow. Agents autonomously plan negotiation strategies, conduct secure multi-party bargaining, and generate cryptographic audit trails without exposing user data to external servers. The system is evaluated in insurance and B2B procurement scenarios across diverse device configurations. Results show an average success rate of 87%, a 2.4x latency improvement over cloud baselines, and strong privacy preservation through zero-knowledge proofs. User studies show 27% higher trust scores when decision trails are available. These findings establish a foundation for trustworthy autonomous agents in privacy-sensitive financial domains.",
    "updated" : "2026-01-01T04:29:39Z",
    "published" : "2026-01-01T04:29:39Z",
    "authors" : [
      {
        "name" : "Joyjit Roy"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.02855v1",
    "title" : "Context-aware Privacy Bounds for Linear Queries",
    "summary" : "Linear queries, as the basis of broad analysis tasks, are often released through privacy mechanisms based on differential privacy (DP), the most popular framework for privacy protection. However, DP adopts a context-free definition that operates independently of the data-generating distribution. In this paper, we revisit the privacy analysis of the Laplace mechanism through the lens of pointwise maximal leakage (PML). We demonstrate that the distribution-agnostic definition of the DP framework often mandates excessive noise. To address this, we incorporate an assumption about the prior distribution by lower-bounding the probability of any single record belonging to any specific class. With this assumption, we derive a tight, context-aware leakage bound for general linear queries, and prove that our derived bound is strictly tighter than the standard DP guarantee and converges to the DP guarantee as this probability lower bound approaches zero. Numerical evaluations demonstrate that by exploiting this prior knowledge, the required noise scale can be reduced while maintaining privacy guarantees.",
    "updated" : "2026-01-06T09:34:04Z",
    "published" : "2026-01-06T09:34:04Z",
    "authors" : [
      {
        "name" : "Heng Zhao"
      },
      {
        "name" : "Sara Saeidian"
      },
      {
        "name" : "Tobias J. Oechtering"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.02720v1",
    "title" : "Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System",
    "summary" : "Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.",
    "updated" : "2026-01-06T05:18:03Z",
    "published" : "2026-01-06T05:18:03Z",
    "authors" : [
      {
        "name" : "Yuqiao Xu"
      },
      {
        "name" : "Mina Namazi"
      },
      {
        "name" : "Sahith Reddy Jalapally"
      },
      {
        "name" : "Osama Zafar"
      },
      {
        "name" : "Youngjin Yoo"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.03979v1",
    "title" : "SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems",
    "summary" : "The continued promise of Large Language Models (LLMs), particularly in their natural language understanding and generation capabilities, has driven a rapidly increasing interest in identifying and developing LLM use cases. In an effort to complement the ingrained \"knowledge\" of LLMs, Retrieval-Augmented Generation (RAG) techniques have become widely popular. At its core, RAG involves the coupling of LLMs with domain-specific knowledge bases, whereby the generation of a response to a user question is augmented with contextual and up-to-date information. The proliferation of RAG has sparked concerns about data privacy, particularly with the inherent risks that arise when leveraging databases with potentially sensitive information. Numerous recent works have explored various aspects of privacy risks in RAG systems, from adversarial attacks to proposed mitigations. With the goal of surveying and unifying these works, we ask one simple question: What are the privacy risks in RAG, and how can they be measured and mitigated? To answer this question, we conduct a systematic literature review of RAG works addressing privacy, and we systematize our findings into a comprehensive set of privacy risks, mitigation techniques, and evaluation strategies. We supplement these findings with two primary artifacts: a Taxonomy of RAG Privacy Risks and a RAG Privacy Process Diagram. Our work contributes to the study of privacy in RAG not only by conducting the first systematization of risks and mitigations, but also by uncovering important considerations when mitigating privacy risks in RAG systems and assessing the current maturity of proposed mitigations.",
    "updated" : "2026-01-07T14:50:41Z",
    "published" : "2026-01-07T14:50:41Z",
    "authors" : [
      {
        "name" : "Andreea-Elena Bodea"
      },
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.03587v1",
    "title" : "Deontic Knowledge Graphs for Privacy Compliance in Multimodal Disaster Data Sharing",
    "summary" : "Disaster response requires sharing heterogeneous artifacts, from tabular assistance records to UAS imagery, under overlapping privacy mandates. Operational systems often reduce compliance to binary access control, which is brittle in time-critical workflows. We present a novel deontic knowledge graph-based framework that integrates a Disaster Management Knowledge Graph (DKG) with a Policy Knowledge Graph (PKG) derived from IoT-Reg and FEMA/DHS privacy drivers. Our release decision function supports three outcomes: Allow, Block, and Allow-with-Transform. The latter binds obligations to transforms and verifies post-transform compliance via provenance-linked derived artifacts; blocked requests are logged as semantic privacy incidents. Evaluation on a 5.1M-triple DKG with 316K images shows exact-match decision correctness, sub-second per-decision latency, and interactive query performance across both single-graph and federated workloads.",
    "updated" : "2026-01-07T05:02:12Z",
    "published" : "2026-01-07T05:02:12Z",
    "authors" : [
      {
        "name" : "Kelvin Uzoma Echenim"
      },
      {
        "name" : "Karuna Pande Joshi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.03546v1",
    "title" : "Value-Action Alignment in Large Language Models under Privacy-Prosocial Conflict",
    "summary" : "Large language models (LLMs) are increasingly used to simulate decision-making tasks involving personal data sharing, where privacy concerns and prosocial motivations can push choices in opposite directions. Existing evaluations often measure privacy-related attitudes or sharing intentions in isolation, which makes it difficult to determine whether a model's expressed values jointly predict its downstream data-sharing actions as in real human behaviors. We introduce a context-based assessment protocol that sequentially administers standardized questionnaires for privacy attitudes, prosocialness, and acceptance of data sharing within a bounded, history-carrying session. To evaluate value-action alignments under competing attitudes, we use multi-group structural equation modeling (MGSEM) to identify relations from privacy concerns and prosocialness to data sharing. We propose Value-Action Alignment Rate (VAAR), a human-referenced directional agreement metric that aggregates path-level evidence for expected signs. Across multiple LLMs, we observe stable but model-specific Privacy-PSA-AoDS profiles, and substantial heterogeneity in value-action alignment.",
    "updated" : "2026-01-07T03:30:42Z",
    "published" : "2026-01-07T03:30:42Z",
    "authors" : [
      {
        "name" : "Guanyu Chen"
      },
      {
        "name" : "Chenxiao Yu"
      },
      {
        "name" : "Xiyang Hu"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.03508v1",
    "title" : "A Critical Analysis of the Medibank Health Data Breach and Differential Privacy Solutions",
    "summary" : "This paper critically examines the 2022 Medibank health insurance data breach, which exposed sensitive medical records of 9.7 million individuals due to unencrypted storage, centralized access, and the absence of privacy-preserving analytics. To address these vulnerabilities, we propose an entropy-aware differential privacy (DP) framework that integrates Laplace and Gaussian mechanisms with adaptive budget allocation. The design incorporates TLS-encrypted database access, field-level mechanism selection, and smooth sensitivity models to mitigate re-identification risks. Experimental validation was conducted using synthetic Medibank datasets (N = 131,000) with entropy-calibrated DP mechanisms, where high-entropy attributes received stronger noise injection. Results demonstrate a 90.3% reduction in re-identification probability while maintaining analytical utility loss below 24%. The framework further aligns with GDPR Article 32 and Australian Privacy Principle 11.1, ensuring regulatory compliance. By combining rigorous privacy guarantees with practical usability, this work contributes a scalable and technically feasible solution for healthcare data protection, offering a pathway toward resilient, trustworthy, and regulation-ready medical analytics.",
    "updated" : "2026-01-07T01:42:36Z",
    "published" : "2026-01-07T01:42:36Z",
    "authors" : [
      {
        "name" : "Zhuohan Cui"
      },
      {
        "name" : "Qianqian Lang"
      },
      {
        "name" : "Zikun Song"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.03429v1",
    "title" : "DeepLeak: Privacy Enhancing Hardening of Model Explanations Against Membership Leakage",
    "summary" : "Machine learning (ML) explainability is central to algorithmic transparency in high-stakes settings such as predictive diagnostics and loan approval. However, these same domains require rigorous privacy guaranties, creating tension between interpretability and privacy. Although prior work has shown that explanation methods can leak membership information, practitioners still lack systematic guidance on selecting or deploying explanation techniques that balance transparency with privacy.\n  We present DeepLeak, a system to audit and mitigate privacy risks in post-hoc explanation methods. DeepLeak advances the state-of-the-art in three ways: (1) comprehensive leakage profiling: we develop a stronger explanation-aware membership inference attack (MIA) to quantify how much representative explanation methods leak membership information under default configurations; (2) lightweight hardening strategies: we introduce practical, model-agnostic mitigations, including sensitivity-calibrated noise, attribution clipping, and masking, that substantially reduce membership leakage while preserving explanation utility; and (3) root-cause analysis: through controlled experiments, we pinpoint algorithmic properties (e.g., attribution sparsity and sensitivity) that drive leakage.\n  Evaluating 15 explanation techniques across four families on image benchmarks, DeepLeak shows that default settings can leak up to 74.9% more membership information than previously reported. Our mitigations cut leakage by up to 95% (minimum 46.5%) with only <=3.3% utility loss on average. DeepLeak offers a systematic, reproducible path to safer explainability in privacy-sensitive ML.",
    "updated" : "2026-01-06T21:34:27Z",
    "published" : "2026-01-06T21:34:27Z",
    "authors" : [
      {
        "name" : "Firas Ben Hmida"
      },
      {
        "name" : "Zain Sbeih"
      },
      {
        "name" : "Philemon Hailemariam"
      },
      {
        "name" : "Birhanu Eshete"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00911v1",
    "title" : "Device-Native Autonomous Agents for Privacy-Preserving Negotiations",
    "summary" : "Automated negotiations in insurance and business-to-business (B2B) commerce encounter substantial challenges. Current systems force a trade-off between convenience and privacy by routing sensitive financial data through centralized servers, increasing security risks, and diminishing user trust. This study introduces a device-native autonomous Artificial Intelligence (AI) agent system for privacy-preserving negotiations. The proposed system operates exclusively on user hardware, enabling real-time bargaining while maintaining sensitive constraints locally. It integrates zero-knowledge proofs to ensure privacy and employs distilled world models to support advanced on-device reasoning. The architecture incorporates six technical components within an agentic AI workflow. Agents autonomously plan negotiation strategies, conduct secure multi-party bargaining, and generate cryptographic audit trails without exposing user data to external servers. The system is evaluated in insurance and B2B procurement scenarios across diverse device configurations. Results show an average success rate of 87%, a 2.4x latency improvement over cloud baselines, and strong privacy preservation through zero-knowledge proofs. User studies show 27% higher trust scores when decision trails are available. These findings establish a foundation for trustworthy autonomous agents in privacy-sensitive financial domains.",
    "updated" : "2026-01-01T04:29:39Z",
    "published" : "2026-01-01T04:29:39Z",
    "authors" : [
      {
        "name" : "Joyjit Roy"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.05180v1",
    "title" : "The Adverse Effects of Omitting Records in Differential Privacy: How Sampling and Suppression Degrade the Privacy-Utility Tradeoff (Long Version)",
    "summary" : "Sampling is renowned for its privacy amplification in differential privacy (DP), and is often assumed to improve the utility of a DP mechanism by allowing a noise reduction. In this paper, we further show that this last assumption is flawed: When measuring utility at equal privacy levels, sampling as preprocessing consistently yields penalties due to utility loss from omitting records over all canonical DP mechanisms -- Laplace, Gaussian, exponential, and report noisy max -- as well as recent applications of sampling, such as clustering.\n  Extending this analysis, we investigate suppression as a generalized method of choosing, or omitting, records. Developing a theoretical analysis of this technique, we derive privacy bounds for arbitrary suppression strategies under unbounded approximate DP. We find that our tested suppression strategy also fails to improve the privacy-utility tradeoff. Surprisingly, uniform sampling emerges as one of the best suppression methods -- despite its still degrading effect. Our results call into question common preprocessing assumptions in DP practice.",
    "updated" : "2026-01-08T18:03:57Z",
    "published" : "2026-01-08T18:03:57Z",
    "authors" : [
      {
        "name" : "Àlex Miranda-Pascual"
      },
      {
        "name" : "Javier Parra-Arnau"
      },
      {
        "name" : "Thorsten Strufe"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.04912v1",
    "title" : "Decentralized Privacy-Preserving Federal Learning of Computer Vision Models on Edge Devices",
    "summary" : "Collaborative training of a machine learning model comes with a risk of sharing sensitive or private data. Federated learning offers a way of collectively training a single global model without the need to share client data, by sharing only the updated parameters from each client's local model. A central server is then used to aggregate parameters from all clients and redistribute the aggregated model back to the clients. Recent findings have shown that even in this scenario, private data can be reconstructed only using information about model parameters. Current efforts to mitigate this are mainly focused on reducing privacy risks on the server side, assuming that other clients will not act maliciously. In this work, we analyzed various methods for improving the privacy of client data concerning both the server and other clients for neural networks. Some of these methods include homomorphic encryption, gradient compression, gradient noising, and discussion on possible usage of modified federated learning systems such as split learning, swarm learning or fully encrypted models. We have analyzed the negative effects of gradient compression and gradient noising on the accuracy of convolutional neural networks used for classification. We have shown the difficulty of data reconstruction in the case of segmentation networks. We have also implemented a proof of concept on the NVIDIA Jetson TX2 module used in edge devices and simulated a federated learning process.",
    "updated" : "2026-01-08T13:10:33Z",
    "published" : "2026-01-08T13:10:33Z",
    "authors" : [
      {
        "name" : "Damian Harenčák"
      },
      {
        "name" : "Lukáš Gajdošech"
      },
      {
        "name" : "Martin Madaras"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.04815v1",
    "title" : "Privacy-Utility Trade-offs Under Multi-Level Point-Wise Leakage Constraints",
    "summary" : "An information-theoretic privacy mechanism design is studied, where an agent observes useful data $Y$ which is correlated with the private data $X$. The agent wants to reveal the information to a user, hence, the agent utilizes a privacy mechanism to produce disclosed data $U$ that can be revealed. We assume that the agent has no direct access to $X$, i.e., the private data is hidden. We study privacy mechanism design that maximizes the disclosed information about $Y$, measured by the mutual information between $Y$ and $U$, while satisfying a point-wise constraint with different privacy leakage budgets. We introduce a new measure, called the \\emph{multi-level point-wise leakage}, which allows us to impose different leakage levels for different realizations of $U$. In contrast to previous studies on point-wise measures, which use the same leakage level for each realization, we consider a more general scenario in which each data point can leak information up to a different threshold. As a result, this concept also covers cases in which some data points should not leak any information about the private data, i.e., they must satisfy perfect privacy. In other words, a combination of perfect privacy and non-zero leakage can be considered. When the leakage is sufficiently small, concepts from information geometry allow us to locally approximate the mutual information. We show that when the leakage matrix $P_{X|Y}$ is invertible, utilizing this approximation leads to a quadratic optimization problem that has closed-form solution under some constraints. In particular, we show that it is sufficient to consider only binary $U$ to attain the optimal utility. This leads to simple privacy designs with low complexity which are based on finding the maximum singular value and singular vector of a matrix.",
    "updated" : "2026-01-08T10:47:50Z",
    "published" : "2026-01-08T10:47:50Z",
    "authors" : [
      {
        "name" : "Amirreza Zamani"
      },
      {
        "name" : "Parastoo Sadeghi"
      },
      {
        "name" : "Mikael Skoglund"
      }
    ],
    "categories" : [
      "cs.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.04641v1",
    "title" : "DP-MGTD: Privacy-Preserving Machine-Generated Text Detection via Adaptive Differentially Private Entity Sanitization",
    "summary" : "The deployment of Machine-Generated Text (MGT) detection systems necessitates processing sensitive user data, creating a fundamental conflict between authorship verification and privacy preservation. Standard anonymization techniques often disrupt linguistic fluency, while rigorous Differential Privacy (DP) mechanisms typically degrade the statistical signals required for accurate detection. To resolve this dilemma, we propose \\textbf{DP-MGTD}, a framework incorporating an Adaptive Differentially Private Entity Sanitization algorithm. Our approach utilizes a two-stage mechanism that performs noisy frequency estimation and dynamically calibrates privacy budgets, applying Laplace and Exponential mechanisms to numerical and textual entities respectively. Crucially, we identify a counter-intuitive phenomenon where the application of DP noise amplifies the distinguishability between human and machine text by exposing distinct sensitivity patterns to perturbation. Extensive experiments on the MGTBench-2.0 dataset show that our method achieves near-perfect detection accuracy, significantly outperforming non-private baselines while satisfying strict privacy guarantees.",
    "updated" : "2026-01-08T06:33:15Z",
    "published" : "2026-01-08T06:33:15Z",
    "authors" : [
      {
        "name" : "Lionel Z. Wang"
      },
      {
        "name" : "Yusheng Zhao"
      },
      {
        "name" : "Jiabin Luo"
      },
      {
        "name" : "Xinfeng Li"
      },
      {
        "name" : "Lixu Wang"
      },
      {
        "name" : "Yinan Peng"
      },
      {
        "name" : "Haoyang Li"
      },
      {
        "name" : "XiaoFeng Wang"
      },
      {
        "name" : "Wei Dong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.04488v1",
    "title" : "Invisible Walls: Privacy-Preserving ISAC Empowered by Reconfigurable Intelligent Surfaces",
    "summary" : "The environmental and target-related information inherently carried in wireless signals, such as channel state information (CSI), has brought increasing attention to integrated sensing and communication (ISAC). However, it also raises pressing concerns about privacy leakage through eavesdropping. While existing efforts have attempted to mitigate this issue, they either fail to account for the needs of legitimate communication and sensing users or rely on hardware with high complexity and cost. To overcome these limitations, we propose PrivISAC, a plug-and-play, low-cost solution that leverages RIS to protect user privacy while preserving ISAC performance. At the core of PrivISAC is a novel strategy in which each RIS row is assigned two distinct beamforming vectors, from which we deliberately construct a limited set of RIS configurations. During operation, exactly one configuration is randomly activated at each time slot to introduce additional perturbations, effectively masking sensitive sensing information from unauthorized eavesdroppers. To jointly ensure privacy protection and communication performance, we design the two vectors such that their responses remain nearly identical in the communication direction, thereby preserving stable, high-throughput transmission, while exhibiting pronounced differences in the sensing direction, which introduces sufficient perturbations to thwart eavesdroppers. Additionally, to enable legitimate sensing under such randomized configurations, we introduce a time-domain masking and demasking method that allows the authorized receiver to associate each CSI sample with its underlying configuration and eliminate configuration-induced discrepancies, thereby recovering valid CSI. We implement PrivISAC on commodity wireless devices and experiment results show that PrivISAC provides strong privacy protection while preserving high-quality legitimate ISAC.",
    "updated" : "2026-01-08T01:47:51Z",
    "published" : "2026-01-08T01:47:51Z",
    "authors" : [
      {
        "name" : "Yinghui He"
      },
      {
        "name" : "Long Fan"
      },
      {
        "name" : "Lei Xie"
      },
      {
        "name" : "Dusit Niyato"
      },
      {
        "name" : "Chau Yuen"
      },
      {
        "name" : "Jun Luo"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.04403v1",
    "title" : "Balancing Usability and Compliance in AI Smart Devices: A Privacy-by-Design Audit of Google Home, Alexa, and Siri",
    "summary" : "This paper investigates the privacy and usability of AI-enabled smart devices commonly used by youth, focusing on Google Home Mini, Amazon Alexa, and Apple Siri. While these devices provide convenience and efficiency, they also raise privacy and transparency concerns due to their always-listening design and complex data management processes. The study proposes and applies a combined framework of Heuristic Evaluation, Personal Information Protection and Electronic Documents Act (PIPEDA) Compliance Assessment, and Youth-Centered Usability Testing to assess whether these devices align with Privacy-by-Design principles and support meaningful user control. Results show that Google Home achieved the highest usability score, while Siri scored highest in regulatory compliance, indicating a trade-off between user convenience and privacy protection. Alexa demonstrated clearer task navigation but weaker transparency in data retention. Findings suggest that although youth may feel capable of managing their data, their privacy self-efficacy remains limited by technical design, complex settings, and unclear data policies. The paper concludes that enhancing transparency, embedding privacy guidance during onboarding, and improving policy alignment are critical steps toward ensuring that smart devices are both usable and compliant with privacy standards that protect young users.",
    "updated" : "2026-01-07T21:20:58Z",
    "published" : "2026-01-07T21:20:58Z",
    "authors" : [
      {
        "name" : "Trevor De Clark"
      },
      {
        "name" : "Yulia Bobkova"
      },
      {
        "name" : "Ajay Kumar Shrestha"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.04399v1",
    "title" : "Convenience vs. Control: A Qualitative Study of Youth Privacy with Smart Voice Assistants",
    "summary" : "Smart voice assistants (SVAs) are embedded in the daily lives of youth, yet their privacy controls often remain opaque and difficult to manage. Through five semi-structured focus groups (N=26) with young Canadians (ages 16-24), we investigate how perceived privacy risks (PPR) and benefits (PPBf) intersect with algorithmic transparency and trust (ATT) and privacy self-efficacy (PSE) to shape privacy-protective behaviors (PPB). Our analysis reveals that policy overload, fragmented settings, and unclear data retention undermine self-efficacy and discourage protective actions. Conversely, simple transparency cues were associated with greater confidence without diminishing the utility of hands-free tasks and entertainment. We synthesize these findings into a qualitative model in which transparency friction erodes PSE, which in turn weakens PPB. From this model, we derive actionable design guidance for SVAs, including a unified privacy hub, plain-language \"data nutrition\" labels, clear retention defaults, and device-conditional micro-tutorials. This work foregrounds youth perspectives and offers a path for SVA governance and design that empowers young digital citizens while preserving convenience.",
    "updated" : "2026-01-07T21:15:29Z",
    "published" : "2026-01-07T21:15:29Z",
    "authors" : [
      {
        "name" : "Molly Campbell"
      },
      {
        "name" : "Trevor De Clark"
      },
      {
        "name" : "Mohamad Sheikho Al Jasem"
      },
      {
        "name" : "Sandhya Joshi"
      },
      {
        "name" : "Ajay Kumar Shrestha"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.04298v1",
    "title" : "Privacy at Scale in Networked Healthcare",
    "summary" : "Digitized, networked healthcare promises earlier detection, precision therapeutics, and continuous care; yet, it also expands the surface for privacy loss and compliance risk. We argue for a shift from siloed, application-specific protections to privacy-by-design at scale, centered on decision-theoretic differential privacy (DP) across the full healthcare data lifecycle; network-aware privacy accounting for interdependence in people, sensors, and organizations; and compliance-as-code tooling that lets health systems share evidence while demonstrating regulatory due care. We synthesize the privacy-enhancing technology (PET) landscape in health (federated analytics, DP, cryptographic computation), identify practice gaps, and outline a deployable agenda involving privacy-budget ledgers, a control plane to coordinate PET components across sites, shared testbeds, and PET literacy, to make lawful, trustworthy sharing the default. We illustrate with use cases (multi-site trials, genomics, disease surveillance, mHealth) and highlight distributed inference as a workhorse for multi-institution learning under explicit privacy budgets.",
    "updated" : "2026-01-07T17:58:58Z",
    "published" : "2026-01-07T17:58:58Z",
    "authors" : [
      {
        "name" : "M. Amin Rahimian"
      },
      {
        "name" : "Benjamin Panny"
      },
      {
        "name" : "James Joshi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.ET",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.04280v1",
    "title" : "A Privacy-Preserving Localization Scheme with Node Selection in Mobile Networks",
    "summary" : "Localization in mobile networks has been widely applied in many scenarios. However, an entity responsible for location estimation exposes both the target and anchors to potential location leakage at any time, creating serious security risks. Although existing studies have proposed privacy-preserving localization algorithms, they still face challenges of insufficient positioning accuracy and excessive communication overhead. In this article, we propose a privacy-preserving localization scheme, named PPLZN. PPLZN protects protects the location privacy of both the target and anchor nodes in crowdsourced localization. Simulation results validate the effectiveness of PPLZN. Evidently, it can achieve accurate position estimation without location leakage and outperform state-of-the-art approaches in both positioning accuracy and communication overhead. In addition, PPLZN significantly reduces computational and communication overhead in large-scale deployments, making it well-fitted for practical privacy-preserving localization in resource-constrained networks.",
    "updated" : "2026-01-07T12:48:45Z",
    "published" : "2026-01-07T12:48:45Z",
    "authors" : [
      {
        "name" : "Liangbo Xie"
      },
      {
        "name" : "Mude Cai"
      },
      {
        "name" : "Xiaolong Yang"
      },
      {
        "name" : "Mu Zhou"
      },
      {
        "name" : "Jiacheng Wang"
      },
      {
        "name" : "Dusit Niyato"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.04265v1",
    "title" : "You Only Anonymize What Is Not Intent-Relevant: Suppressing Non-Intent Privacy Evidence",
    "summary" : "Anonymizing sensitive information in user text is essential for privacy, yet existing methods often apply uniform treatment across attributes, which can conflict with communicative intent and obscure necessary information. This is particularly problematic when personal attributes are integral to expressive or pragmatic goals. The central challenge lies in determining which attributes to protect, and to what extent, while preserving semantic and pragmatic functions. We propose IntentAnony, a utility-preserving anonymization approach that performs intent-conditioned exposure control. IntentAnony models pragmatic intent and constructs privacy inference evidence chains to capture how distributed cues support attribute inference. Conditioned on intent, it assigns each attribute an exposure budget and selectively suppresses non-intent inference pathways while preserving intent-relevant content, semantic structure, affective nuance, and interactional function. We evaluate IntentAnony using privacy inference success rates, text utility metrics, and human evaluation. The results show an approximately 30% improvement in the overall privacy--utility trade-off, with notably stronger usability of anonymized text compared to prior state-of-the-art methods. Our code is available at https://github.com/Nevaeh7/IntentAnony.",
    "updated" : "2026-01-07T07:54:23Z",
    "published" : "2026-01-07T07:54:23Z",
    "authors" : [
      {
        "name" : "Weihao Shen"
      },
      {
        "name" : "Yaxin Xu"
      },
      {
        "name" : "Shuang Li"
      },
      {
        "name" : "Wei Chen"
      },
      {
        "name" : "Yuqin Lan"
      },
      {
        "name" : "Meng Yuan"
      },
      {
        "name" : "Fuzhen Zhuang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  }
]