[
  {
    "id" : "http://arxiv.org/abs/2601.00715v1",
    "title" : "PDPL Metric: Validating a Scale to Measure Personal Data Privacy Literacy Among University Students",
    "summary" : "Personal data privacy literacy (PDPL) refers to a collection of digital literacy skills related to an individuals ability to understand, evaluate, and manage the collection, use, and protection of personal data in online and digital environments. This study introduces and validates a new psychometric scale (PDPL Metric) designed to measure data privacy literacy among university students, focusing on six key privacy constructs: perceived risk of data misuse, expectations of informed consent, general privacy concern, privacy management awareness, privacy-utility trade-off acceptance, and perceived importance of data security. A 24-item questionnaire was developed and administered to students at U.S.-based research universities. Principal components analysis confirmed the unidimensionality and internal consistency of each construct, and a second-order analysis supported the integration of all six into a unified PDPL construct. No differences in PDPL were found based on basic demographic variables like academic level and gender, although a difference was found based on domestic/international status. The findings of this study offer a validated framework for assessing personal data privacy literacy within the higher education context and support the integration of the core constructs into higher education programs, organizational policies, and digital literacy initiatives on university campuses.",
    "updated" : "2026-01-02T15:12:00Z",
    "published" : "2026-01-02T15:12:00Z",
    "authors" : [
      {
        "name" : "Brady D. Lund"
      },
      {
        "name" : "Nathan Brown"
      },
      {
        "name" : "Ana Roeschley"
      },
      {
        "name" : "Gahangir Hossain"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00418v1",
    "title" : "Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution",
    "summary" : "We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.",
    "updated" : "2026-01-01T18:12:50Z",
    "published" : "2026-01-01T18:12:50Z",
    "authors" : [
      {
        "name" : "Prajwal Panth"
      },
      {
        "name" : "Sahaj Raj Malla"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00385v1",
    "title" : "Exploring the Integration of Differential Privacy in Cybersecurity Analytics: Balancing Data Utility and Privacy in Threat Intelligence",
    "summary" : "To resolve the acute problem of privacy protection and guarantee that data can be used in the context of threat intelligence, this paper considers the implementation of Differential Privacy (DP) in cybersecurity analytics. DP, which is a sound mathematical framework, ensures privacy by adding a controlled noise to data outputs and thus avoids sensitive information disclosure even with auxiliary datasets. The use of DP in Security Information and Event Management (SIEM) systems is highlighted, and it can be seen that DP has the capability to protect event log and threat data analysis without interfering with the analytical efficiency. The utility versus privacy trade-offs linked to the maximization of the epsilon parameter, which is one of the critical components of DP mechanisms, is pointed out. The article shows the transformative power of DP in promoting safe sharing of data and joint threat intelligence through real-world systems and case studies. Finally, this paper makes DP one of the key strategies to improve privacy-preserving analytics in the field of cybersecurity.",
    "updated" : "2026-01-01T16:31:47Z",
    "published" : "2026-01-01T16:31:47Z",
    "authors" : [
      {
        "name" : "Brahim Khalil Sedraoui"
      },
      {
        "name" : "Abdelmadjid Benmachiche"
      },
      {
        "name" : "Amina Makhlouf"
      },
      {
        "name" : "Chaouki Chemam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00382v1",
    "title" : "Unseen Risks of Clinical Speech-to-Text Systems: Transparency, Privacy, and Reliability Challenges in AI-Driven Documentation",
    "summary" : "AI-driven speech-to-text (STT) documentation systems are increasingly adopted in clinical settings to reduce documentation burden and improve workflow efficiency. However, their rapid deployment has outpaced understanding of the associated socio-technical risks, including transparency, reliability, patient autonomy, workflow alignment, and organizational governance. A clearer analysis of these risks is needed to support safe and equitable integration into healthcare practice. This study synthesizes interdisciplinary evidence from technical performance research, regulatory and ethical standards, clinical workflow analyses, and organizational policy guidance. The synthesis was used to develop a multi-layered socio-technical conceptual framework for evaluating and governing STT systems. Findings show that STT systems operate within tightly coupled socio-technical environments in which model performance, clinician oversight, patient rights, workflow design, and institutional governance are interdependent. The study offers a structured socio-technical governance framework and an implementation roadmap that outlines readiness assessment, vendor evaluation, pilot deployment, clinician training, ongoing monitoring, and iterative improvement. The framework emphasizes safeguards that protect patient autonomy, documentation integrity, and institutional trust while enabling the efficient and beneficial use of STT technologies. This work provides actionable guidance for healthcare organizations seeking to adopt STT systems responsibly and equitably.",
    "updated" : "2026-01-01T16:18:54Z",
    "published" : "2026-01-01T16:18:54Z",
    "authors" : [
      {
        "name" : "Nelly Elsayed"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00372v1",
    "title" : "LLM-Powered Analysis of IoT User Reviews: Tracking and Ranking Security and Privacy Concerns",
    "summary" : "Being able to understand the security and privacy (S&P) concerns of IoT users brings benefits to both developers and users. To learn about users' views, we examine Amazon IoT reviews - one of the biggest IoT markets. This work presents a state-of-the-art methodology to identify and categorize reviews in which users express S&P concerns. We developed an automated pipeline by fine-tuning GPT-3.5-Turbo to build two models: the Classifier-Rationalizer-Categorizer and the Thematic Mapper. By leveraging dynamic few-shot prompting and the model's large context size, our pipeline achieved over 97% precision and recall, significantly outperforming keyword-based and classical ML methods. We applied our pipeline to 91K Amazon reviews about fitness trackers, smart speakers and cameras, over multiple years. We found that on average 5% contained S&P concerns, while security camera exhibited the highest prevalence at 10%. Our method detected significantly more S&P-relevant reviews than prior works: 15x more for fitness trackers, 29% more for smart speakers, and 70% more for cameras. Our longitudinal analysis reveals that concerns like surveillance and data control have persisted for years, suggesting limited industry progress. We demonstrate that across all device types, users consistently demand more precise control over what data is collected and shared. We uncover challenges in multi-user and multi-device interactions, identifying two previously unreported themes concerning inadequate controls for account separation and data access. These findings, ranging from broad persistent trends to specific instances of customer loss, offer actionable insights for developers to improve user satisfaction and trust.",
    "updated" : "2026-01-01T15:24:21Z",
    "published" : "2026-01-01T15:24:21Z",
    "authors" : [
      {
        "name" : "Taufiq Islam Protick"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Nina Taft"
      },
      {
        "name" : "Anupam Das"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2601.00337v1",
    "title" : "When Does Quantum Differential Privacy Compose?",
    "summary" : "Composition is a cornerstone of classical differential privacy, enabling strong end-to-end guarantees for complex algorithms through composition theorems (e.g., basic and advanced). In the quantum setting, however, privacy is defined operationally against arbitrary measurements, and classical composition arguments based on scalar privacy-loss random variables no longer apply. As a result, it has remained unclear when meaningful composition guarantees can be obtained for quantum differential privacy (QDP).\n  In this work, we clarify both the limitations and possibilities of composition in the quantum setting. We first show that classical-style composition fails in full generality for POVM-based approximate QDP: even quantum channels that are individually perfectly private can completely lose privacy when combined through correlated joint implementations. We then identify a setting in which clean composition guarantees can be restored. For tensor-product channels acting on product neighboring inputs, we introduce a quantum moments accountant based on an operator-valued notion of privacy loss and a matrix moment-generating function. Although the resulting Rényi-type divergence does not satisfy a data-processing inequality, we prove that controlling its moments suffices to bound measured Rényi divergence, yielding operational privacy guarantees against arbitrary measurements. This leads to advanced-composition-style bounds with the same leading-order behavior as in the classical theory.\n  Our results demonstrate that meaningful composition theorems for quantum differential privacy require carefully articulated structural assumptions on channels, inputs, and adversarial measurements, and provide a principled framework for understanding which classical ideas do and do not extend to the quantum setting.",
    "updated" : "2026-01-01T13:24:09Z",
    "published" : "2026-01-01T13:24:09Z",
    "authors" : [
      {
        "name" : "Daniel Alabi"
      },
      {
        "name" : "Theshani Nuradha"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.IT"
    ]
  }
]