[
  {
    "id" : "http://arxiv.org/abs/2401.01294v1",
    "title" : "Efficient Sparse Least Absolute Deviation Regression with Differential\n  Privacy",
    "summary" : "In recent years, privacy-preserving machine learning algorithms have\nattracted increasing attention because of their important applications in many\nscientific fields. However, in the literature, most privacy-preserving\nalgorithms demand learning objectives to be strongly convex and Lipschitz\nsmooth, which thus cannot cover a wide class of robust loss functions (e.g.,\nquantile/least absolute loss). In this work, we aim to develop a fast\nprivacy-preserving learning solution for a sparse robust regression problem.\nOur learning loss consists of a robust least absolute loss and an $\\ell_1$\nsparse penalty term. To fast solve the non-smooth loss under a given privacy\nbudget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)\nalgorithm for least absolute deviation regression. Our algorithm achieves a\nfast estimation by reformulating the sparse LAD problem as a penalized least\nsquare estimation problem and adopts a three-stage noise injection to guarantee\nthe $(\\epsilon,\\delta)$-differential privacy. We show that our algorithm can\nachieve better privacy and statistical accuracy trade-off compared with the\nstate-of-the-art privacy-preserving regression algorithms. In the end, we\nconduct experiments to verify the efficiency of our proposed FRAPPE algorithm.",
    "updated" : "2024-01-02T17:13:34Z",
    "published" : "2024-01-02T17:13:34Z",
    "authors" : [
      {
        "name" : "Weidong Liu"
      },
      {
        "name" : "Xiaojun Mao"
      },
      {
        "name" : "Xiaofei Zhang"
      },
      {
        "name" : "Xin Zhang"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "stat.ME",
      "62J07"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01204v1",
    "title" : "PPBFL: A Privacy Protected Blockchain-based Federated Learning Model",
    "summary" : "With the rapid development of machine learning and growing concerns about\ndata privacy, federated learning has become an increasingly prominent focus.\nHowever, challenges such as attacks on model parameters and the lack of\nincentive mechanisms hinder the effectiveness of federated learning. Therefore,\nwe propose a Privacy Protected Blockchain-based Federated Learning Model\n(PPBFL) to enhance the security of federated learning and promote the active\nparticipation of nodes in model training. Blockchain ensures that model\nparameters stored in the InterPlanetary File System (IPFS) remain unaltered. A\nnovel adaptive differential privacy addition algorithm is simultaneously\napplied to local and global models, preserving the privacy of local models and\npreventing a decrease in the security of the global model due to the presence\nof numerous local models in federated learning. Additionally, we introduce a\nnew mix transactions mechanism to better protect the identity privacy of local\ntraining clients. Security analysis and experimental results demonstrate that\nPPBFL outperforms baseline methods in both model performance and security.",
    "updated" : "2024-01-02T13:13:28Z",
    "published" : "2024-01-02T13:13:28Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Chunhe Xia"
      },
      {
        "name" : "Wanshuang Lin"
      },
      {
        "name" : "Tianbo Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01146v1",
    "title" : "Privacy Preserving Personal Assistant with On-Device Diarization and\n  Spoken Dialogue System for Home and Beyond",
    "summary" : "In the age of personal voice assistants, the question of privacy arises.\nThese digital companions often lack memory of past interactions, while relying\nheavily on the internet for speech processing, raising privacy concerns. Modern\nsmartphones now enable on-device speech processing, making cloud-based\nsolutions unnecessary. Personal assistants for the elderly should excel at\nmemory recall, especially in medical examinations. The e-ViTA project developed\na versatile conversational application with local processing and speaker\nrecognition. This paper highlights the importance of speaker diarization\nenriched with sensor data fusion for contextualized conversation preservation.\nThe use cases applied to the e-VITA project have shown that truly personalized\ndialogue is pivotal for individual voice assistants. Secure local processing\nand sensor data fusion ensure virtual companions meet individual user needs\nwithout compromising privacy or data security.",
    "updated" : "2024-01-02T10:56:24Z",
    "published" : "2024-01-02T10:56:24Z",
    "authors" : [
      {
        "name" : "Gérard Chollet"
      },
      {
        "name" : "Hugues Sansen"
      },
      {
        "name" : "Yannis Tevissen"
      },
      {
        "name" : "Jérôme Boudy"
      },
      {
        "name" : "Mossaab Hariz"
      },
      {
        "name" : "Christophe Lohr"
      },
      {
        "name" : "Fathy Yassa"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00973v1",
    "title" : "Facebook Report on Privacy of fNIRS data",
    "summary" : "The primary goal of this project is to develop privacy-preserving machine\nlearning model training techniques for fNIRS data. This project will build a\nlocal model in a centralized setting with both differential privacy (DP) and\ncertified robustness. It will also explore collaborative federated learning to\ntrain a shared model between multiple clients without sharing local fNIRS\ndatasets. To prevent unintentional private information leakage of such clients'\nprivate datasets, we will also implement DP in the federated learning setting.",
    "updated" : "2024-01-01T23:30:31Z",
    "published" : "2024-01-01T23:30:31Z",
    "authors" : [
      {
        "name" : "Md Imran Hossen"
      },
      {
        "name" : "Sai Venkatesh Chilukoti"
      },
      {
        "name" : "Liqun Shan"
      },
      {
        "name" : "Vijay Srinivas Tida"
      },
      {
        "name" : "Xiali Hei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "I.2.0"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00794v1",
    "title" : "Privacy-Preserving Data in IoT-based Cloud Systems: A Comprehensive\n  Survey with AI Integration",
    "summary" : "As the integration of Internet of Things devices with cloud computing\nproliferates, the paramount importance of privacy preservation comes to the\nforefront. This survey paper meticulously explores the landscape of privacy\nissues in the dynamic intersection of IoT and cloud systems. The comprehensive\nliterature review synthesizes existing research, illuminating key challenges\nand discerning emerging trends in privacy preserving techniques. The\ncategorization of diverse approaches unveils a nuanced understanding of\nencryption techniques, anonymization strategies, access control mechanisms, and\nthe burgeoning integration of artificial intelligence. Notable trends include\nthe infusion of machine learning for dynamic anonymization, homomorphic\nencryption for secure computation, and AI-driven access control systems. The\nculmination of this survey contributes a holistic view, laying the groundwork\nfor understanding the multifaceted strategies employed in securing sensitive\ndata within IoT-based cloud environments. The insights garnered from this\nsurvey provide a valuable resource for researchers, practitioners, and\npolicymakers navigating the complex terrain of privacy preservation in the\nevolving landscape of IoT and cloud computing",
    "updated" : "2024-01-01T15:48:39Z",
    "published" : "2024-01-01T15:48:39Z",
    "authors" : [
      {
        "name" : "D. Dhinakaran"
      },
      {
        "name" : "S. M. Udhaya Sankar"
      },
      {
        "name" : "D. Selvaraj"
      },
      {
        "name" : "S. Edwin Raja"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00793v1",
    "title" : "SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\n  Large Language Models",
    "summary" : "With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and are difficult to circumvent or optimize\neffectively. To address this concern, we introduce an advanced optimization\nframework called SecFormer, designed to strike an optimal balance between\nperformance and efficiency in PPI for Transformer models. By implementing\nknowledge distillation techniques, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials and Goldschmidt's method to handle\nother complex nonlinear functions within PPI, such as GeLU, LayerNorm, and\nSoftmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer\nin performance, showing improvements of $5.6\\%$ and $24.2\\%$ for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of\nefficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its\neffectiveness and speed.",
    "updated" : "2024-01-01T15:40:35Z",
    "published" : "2024-01-01T15:40:35Z",
    "authors" : [
      {
        "name" : "Jinglong Luo"
      },
      {
        "name" : "Yehong Zhang"
      },
      {
        "name" : "Jiaqi Zhang"
      },
      {
        "name" : "Xin Mu"
      },
      {
        "name" : "Hui Wang"
      },
      {
        "name" : "Yue Yu"
      },
      {
        "name" : "Zenglin Xu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01589v1",
    "title" : "The Security and Privacy of Mobile Edge Computing: An Artificial\n  Intelligence Perspective",
    "summary" : "Mobile Edge Computing (MEC) is a new computing paradigm that enables cloud\ncomputing and information technology (IT) services to be delivered at the\nnetwork's edge. By shifting the load of cloud computing to individual local\nservers, MEC helps meet the requirements of ultralow latency, localized data\nprocessing, and extends the potential of Internet of Things (IoT) for\nend-users. However, the crosscutting nature of MEC and the multidisciplinary\ncomponents necessary for its deployment have presented additional security and\nprivacy concerns. Fortunately, Artificial Intelligence (AI) algorithms can cope\nwith excessively unpredictable and complex data, which offers a distinct\nadvantage in dealing with sophisticated and developing adversaries in the\nsecurity industry. Hence, in this paper we comprehensively provide a survey of\nsecurity and privacy in MEC from the perspective of AI. On the one hand, we use\nEuropean Telecommunications Standards Institute (ETSI) MEC reference\narchitecture as our based framework while merging the Software Defined Network\n(SDN) and Network Function Virtualization (NFV) to better illustrate a\nserviceable platform of MEC. On the other hand, we focus on new security and\nprivacy issues, as well as potential solutions from the viewpoints of AI.\nFinally, we comprehensively discuss the opportunities and challenges associated\nwith applying AI to MEC security and privacy as possible future research\ndirections.",
    "updated" : "2024-01-03T07:47:22Z",
    "published" : "2024-01-03T07:47:22Z",
    "authors" : [
      {
        "name" : "Cheng Wang"
      },
      {
        "name" : "Zenghui Yuan"
      },
      {
        "name" : "Pan Zhou"
      },
      {
        "name" : "Zichuan Xu"
      },
      {
        "name" : "Ruixuan Li"
      },
      {
        "name" : "Dapeng Oliver Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01575v1",
    "title" : "Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient\n  Accumulation",
    "summary" : "The blooming of social media and face recognition (FR) systems has increased\npeople's concern about privacy and security. A new type of adversarial privacy\ncloak (class-universal) can be applied to all the images of regular users, to\nprevent malicious FR systems from acquiring their identity information. In this\nwork, we discover the optimization dilemma in the existing methods -- the local\noptima problem in large-batch optimization and the gradient information\nelimination problem in small-batch optimization. To solve these problems, we\npropose Gradient Accumulation (GA) to aggregate multiple small-batch gradients\ninto a one-step iterative gradient to enhance the gradient stability and reduce\nthe usage of quantization operations. Experiments show that our proposed method\nachieves high performance on the Privacy-Commons dataset against black-box face\nrecognition models.",
    "updated" : "2024-01-03T07:00:32Z",
    "published" : "2024-01-03T07:00:32Z",
    "authors" : [
      {
        "name" : "Xuannan Liu"
      },
      {
        "name" : "Yaoyao Zhong"
      },
      {
        "name" : "Weihong Deng"
      },
      {
        "name" : "Hongzhi Shi"
      },
      {
        "name" : "Xingchen Cui"
      },
      {
        "name" : "Yunfeng Yin"
      },
      {
        "name" : "Dongchao Wen"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.02453v1",
    "title" : "Adaptive Differential Privacy in Federated Learning: A Priority-Based\n  Approach",
    "summary" : "Federated learning (FL) as one of the novel branches of distributed machine\nlearning (ML), develops global models through a private procedure without\ndirect access to local datasets. However, access to model updates (e.g.\ngradient updates in deep neural networks) transferred between clients and\nservers can reveal sensitive information to adversaries. Differential privacy\n(DP) offers a framework that gives a privacy guarantee by adding certain\namounts of noise to parameters. This approach, although being effective in\nterms of privacy, adversely affects model performance due to noise involvement.\nHence, it is always needed to find a balance between noise injection and the\nsacrificed accuracy. To address this challenge, we propose adaptive noise\naddition in FL which decides the value of injected noise based on features'\nrelative importance. Here, we first propose two effective methods for\nprioritizing features in deep neural network models and then perturb models'\nweights based on this information. Specifically, we try to figure out whether\nthe idea of adding more noise to less important parameters and less noise to\nmore important parameters can effectively save the model accuracy while\npreserving privacy. Our experiments confirm this statement under some\nconditions. The amount of noise injected, the proportion of parameters\ninvolved, and the number of global iterations can significantly change the\noutput. While a careful choice of parameters by considering the properties of\ndatasets can improve privacy without intense loss of accuracy, a bad choice can\nmake the model performance worse.",
    "updated" : "2024-01-04T03:01:15Z",
    "published" : "2024-01-04T03:01:15Z",
    "authors" : [
      {
        "name" : "Mahtab Talaei"
      },
      {
        "name" : "Iman Izadi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.04076v1",
    "title" : "Security and Privacy Issues in Cloud Storage",
    "summary" : "Even with the vast potential that cloud computing has, so far, it has not\nbeen adopted by the consumers with the enthusiasm and pace that it be worthy;\nthis is a very reason statement why consumers still hesitated of using cloud\ncomputing for their sensitive data and the threats that prevent the consumers\nfrom shifting to use cloud computing in general and cloud storage in\nparticular. The cloud computing inherits the traditional potential security and\nprivacy threats besides its own issues due to its unique structures. Some\nthreats related to cloud computing are the insider malicious attacks from the\nemployees that even sometime the provider unconscious about, the lack of\ntransparency of agreement between consumer and provider, data loss, traffic\nhijacking, shared technology and insecure application interface. Such threats\nneed remedies to make the consumer use its features in secure way. In this\nreview, we spot the light on the most security and privacy issues which can be\nattributed as gaps that sometimes the consumers or even the enterprises are not\naware of. We also define the parties that involve in scenario of cloud\ncomputing that also may attack the entire cloud systems. We also show the\nconsequences of these threats.",
    "updated" : "2024-01-08T18:27:57Z",
    "published" : "2024-01-08T18:27:57Z",
    "authors" : [
      {
        "name" : "Norah Asiri"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03883v1",
    "title" : "The Impact of Differential Privacy on Recommendation Accuracy and\n  Popularity Bias",
    "summary" : "Collaborative filtering-based recommender systems leverage vast amounts of\nbehavioral user data, which poses severe privacy risks. Thus, often, random\nnoise is added to the data to ensure Differential Privacy (DP). However, to\ndate, it is not well understood, in which ways this impacts personalized\nrecommendations. In this work, we study how DP impacts recommendation accuracy\nand popularity bias, when applied to the training data of state-of-the-art\nrecommendation models. Our findings are three-fold: First, we find that nearly\nall users' recommendations change when DP is applied. Second, recommendation\naccuracy drops substantially while recommended item popularity experiences a\nsharp increase, suggesting that popularity bias worsens. Third, we find that DP\nexacerbates popularity bias more severely for users who prefer unpopular items\nthan for users that prefer popular items.",
    "updated" : "2024-01-08T13:31:02Z",
    "published" : "2024-01-08T13:31:02Z",
    "authors" : [
      {
        "name" : "Peter Müllner"
      },
      {
        "name" : "Elisabeth Lex"
      },
      {
        "name" : "Markus Schedl"
      },
      {
        "name" : "Dominik Kowald"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03552v1",
    "title" : "Privacy-Preserving in Blockchain-based Federated Learning Systems",
    "summary" : "Federated Learning (FL) has recently arisen as a revolutionary approach to\ncollaborative training Machine Learning models. According to this novel\nframework, multiple participants train a global model collaboratively,\ncoordinating with a central aggregator without sharing their local data. As FL\ngains popularity in diverse domains, security, and privacy concerns arise due\nto the distributed nature of this solution. Therefore, integrating this\nstrategy with Blockchain technology has been consolidated as a preferred choice\nto ensure the privacy and security of participants.\n  This paper explores the research efforts carried out by the scientific\ncommunity to define privacy solutions in scenarios adopting Blockchain-Enabled\nFL. It comprehensively summarizes the background related to FL and Blockchain,\nevaluates existing architectures for their integration, and the primary attacks\nand possible countermeasures to guarantee privacy in this setting. Finally, it\nreviews the main application scenarios where Blockchain-Enabled FL approaches\nhave been proficiently applied. This survey can help academia and industry\npractitioners understand which theories and techniques exist to improve the\nperformance of FL through Blockchain to preserve privacy and which are the main\nchallenges and future directions in this novel and still under-explored\ncontext. We believe this work provides a novel contribution respect to the\nprevious surveys and is a valuable tool to explore the current landscape,\nunderstand perspectives, and pave the way for advancements or improvements in\nthis amalgamation of Blockchain and Federated Learning.",
    "updated" : "2024-01-07T17:23:55Z",
    "published" : "2024-01-07T17:23:55Z",
    "authors" : [
      {
        "name" : "Sameera K. M."
      },
      {
        "name" : "Serena Nicolazzo"
      },
      {
        "name" : "Marco Arazzi"
      },
      {
        "name" : "Antonino Nocera"
      },
      {
        "name" : "Rafidha Rehiman K. A."
      },
      {
        "name" : "Vinod P"
      },
      {
        "name" : "Mauro Conti"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03218v1",
    "title" : "MiniScope: Automated UI Exploration and Privacy Inconsistency Detection\n  of MiniApps via Two-phase Iterative Hybrid Analysis",
    "summary" : "The advent of MiniApps, operating within larger SuperApps, has revolutionized\nuser experiences by offering a wide range of services without the need for\nindividual app downloads. However, this convenience has raised significant\nprivacy concerns, as these MiniApps often require access to sensitive data,\npotentially leading to privacy violations. Our research addresses the critical\ngaps in the analysis of MiniApps' privacy practices, especially focusing on\nWeChat MiniApps in the Android ecosystem. Despite existing privacy regulations\nand platform guidelines, there is a lack of effective mechanisms to safeguard\nuser privacy fully. We introduce MiniScope, a novel two-phase hybrid analysis\napproach, specifically designed for the MiniApp environment. This approach\novercomes the limitations of existing static analysis techniques by\nincorporating dynamic UI exploration for complete code coverage and accurate\nprivacy practice identification. Our methodology includes modeling UI\ntransition states, resolving cross-package callback control flows, and\nautomated iterative UI exploration. This allows for a comprehensive\nunderstanding of MiniApps' privacy practices, addressing the unique challenges\nof sub-package loading and event-driven callbacks. Our empirical evaluation of\nover 120K MiniApps using MiniScope demonstrates its effectiveness in\nidentifying privacy inconsistencies. The results reveal significant issues,\nwith 5.7% of MiniApps over-collecting private data and 33.4% overclaiming data\ncollection. These findings emphasize the urgent need for more precise privacy\nmonitoring systems and highlight the responsibility of SuperApp operators to\nenforce stricter privacy measures.",
    "updated" : "2024-01-06T13:54:36Z",
    "published" : "2024-01-06T13:54:36Z",
    "authors" : [
      {
        "name" : "Shenao Wang"
      },
      {
        "name" : "Yuekang Li"
      },
      {
        "name" : "Kailong Wang"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Chao Wang"
      },
      {
        "name" : "Yanjie Zhao"
      },
      {
        "name" : "Gelei Deng"
      },
      {
        "name" : "Ling Shi"
      },
      {
        "name" : "Hui Li"
      },
      {
        "name" : "Yang Liu"
      },
      {
        "name" : "Haoyu Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01204v2",
    "title" : "PPBFL: A Privacy Protected Blockchain-based Federated Learning Model",
    "summary" : "With the rapid development of machine learning and a growing concern for data\nprivacy, federated learning has become a focal point of attention. However,\nattacks on model parameters and a lack of incentive mechanisms hinder the\neffectiveness of federated learning. Therefore, we propose A Privacy Protected\nBlockchain-based Federated Learning Model (PPBFL) to enhance the security of\nfederated learning and encourage active participation of nodes in model\ntraining. Blockchain technology ensures the integrity of model parameters\nstored in the InterPlanetary File System (IPFS), providing protection against\ntampering. Within the blockchain, we introduce a Proof of Training Work (PoTW)\nconsensus algorithm tailored for federated learning, aiming to incentive\ntraining nodes. This algorithm rewards nodes with greater computational power,\npromoting increased participation and effort in the federated learning process.\nA novel adaptive differential privacy algorithm is simultaneously applied to\nlocal and global models. This safeguards the privacy of local data at training\nclients, preventing malicious nodes from launching inference attacks.\nAdditionally, it enhances the security of the global model, preventing\npotential security degradation resulting from the combination of numerous local\nmodels. The possibility of security degradation is derived from the composition\ntheorem. By introducing reverse noise in the global model, a zero-bias estimate\nof differential privacy noise between local and global models is achieved.\nFurthermore, we propose a new mix transactions mechanism utilizing ring\nsignature technology to better protect the identity privacy of local training\nclients. Security analysis and experimental results demonstrate that PPBFL,\ncompared to baseline methods, not only exhibits superior model performance but\nalso achieves higher security.",
    "updated" : "2024-01-08T15:38:22Z",
    "published" : "2024-01-02T13:13:28Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Chunhe Xia"
      },
      {
        "name" : "Wanshuang Lin"
      },
      {
        "name" : "Tianbo Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00793v2",
    "title" : "SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\n  Large Language Models",
    "summary" : "With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and difficult to circumvent or optimize effectively. To\naddress this concern, we introduce an advanced optimization framework called\nSecFormer, to achieve fast and accurate PPI for Transformer models. By\nimplementing model design optimization, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials, Fourier series and Goldschmidt's\nmethod to handle other complex nonlinear functions within PPI, such as GeLU,\nLayerNorm, and Softmax. Our extensive experiments reveal that SecFormer\noutperforms MPCFormer in performance, showing improvements of $5.6\\%$ and\n$24.2\\%$ for BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In\nterms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, demonstrating its effectiveness\nand speed.",
    "updated" : "2024-01-06T10:05:23Z",
    "published" : "2024-01-01T15:40:35Z",
    "authors" : [
      {
        "name" : "Jinglong Luo"
      },
      {
        "name" : "Yehong Zhang"
      },
      {
        "name" : "Jiaqi Zhang"
      },
      {
        "name" : "Xin Mu"
      },
      {
        "name" : "Hui Wang"
      },
      {
        "name" : "Yue Yu"
      },
      {
        "name" : "Zenglin Xu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.04423v1",
    "title" : "Privacy-Preserving Sequential Recommendation with Collaborative\n  Confusion",
    "summary" : "Sequential recommendation has attracted a lot of attention from both academia\nand industry, however the privacy risks associated to gathering and\ntransferring users' personal interaction data are often underestimated or\nignored. Existing privacy-preserving studies are mainly applied to traditional\ncollaborative filtering or matrix factorization rather than sequential\nrecommendation. Moreover, these studies are mostly based on differential\nprivacy or federated learning, which often leads to significant performance\ndegradation, or has high requirements for communication. In this work, we\naddress privacy-preserving from a different perspective. Unlike existing\nresearch, we capture collaborative signals of neighbor interaction sequences\nand directly inject indistinguishable items into the target sequence before the\nrecommendation process begins, thereby increasing the perplexity of the target\nsequence. Even if the target interaction sequence is obtained by attackers, it\nis difficult to discern which ones are the actual user interaction records. To\nachieve this goal, we propose a CoLlaborative-cOnfusion seqUential recommenDer,\nnamely CLOUD, which incorporates a collaborative confusion mechanism to edit\nthe raw interaction sequences before conducting recommendation. Specifically,\nCLOUD first calculates the similarity between the target interaction sequence\nand other neighbor sequences to find similar sequences. Then, CLOUD considers\nthe shared representation of the target sequence and similar sequences to\ndetermine the operation to be performed: keep, delete, or insert. We design a\ncopy mechanism to make items from similar sequences have a higher probability\nto be inserted into the target sequence. Finally, the modified sequence is used\nto train the recommender and predict the next item.",
    "updated" : "2024-01-09T08:30:50Z",
    "published" : "2024-01-09T08:30:50Z",
    "authors" : [
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Yujie Lin"
      },
      {
        "name" : "Pengjie Ren"
      },
      {
        "name" : "Zhumin Chen"
      },
      {
        "name" : "Tsunenori Mine"
      },
      {
        "name" : "Jianli Zhao"
      },
      {
        "name" : "Qiang Zhao"
      },
      {
        "name" : "Moyan Zhang"
      },
      {
        "name" : "Xianye Ben"
      },
      {
        "name" : "Yujun Li"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.04306v1",
    "title" : "Renyi Differential Privacy in the Shuffle Model: Enhanced Amplification\n  Bounds",
    "summary" : "The shuffle model of Differential Privacy (DP) has gained significant\nattention in privacy-preserving data analysis due to its remarkable tradeoff\nbetween privacy and utility. It is characterized by adding a shuffling\nprocedure after each user's locally differentially private perturbation, which\nleads to a privacy amplification effect, meaning that the privacy guarantee of\na small level of noise, say $\\epsilon_0$, can be enhanced to\n$O(\\epsilon_0/\\sqrt{n})$ (the smaller, the more private) after shuffling all\n$n$ users' perturbed data. Most studies in the shuffle DP focus on proving a\ntighter privacy guarantee of privacy amplification. However, the current\nresults assume that the local privacy budget $\\epsilon_0$ is within a limited\nrange. In addition, there remains a gap between the tightest lower bound and\nthe known upper bound of the privacy amplification. In this work, we push\nforward the state-of-the-art by making the following contributions. Firstly, we\npresent the first asymptotically optimal analysis of Renyi Differential Privacy\n(RDP) in the shuffle model without constraints on $\\epsilon_0$. Secondly, we\nintroduce hypothesis testing for privacy amplification through shuffling,\noffering a distinct analysis technique and a tighter upper bound. Furthermore,\nwe propose a DP-SGD algorithm based on RDP. Experiments demonstrate that our\napproach outperforms existing methods significantly at the same privacy level.",
    "updated" : "2024-01-09T01:47:46Z",
    "published" : "2024-01-09T01:47:46Z",
    "authors" : [
      {
        "name" : "E Chen"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Yifei Ge"
      }
    ],
    "categories" : [
      "math.CO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.05126v1",
    "title" : "Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving\n  Vision Transformer",
    "summary" : "We propose a novel method for privacy-preserving deep neural networks (DNNs)\nwith the Vision Transformer (ViT). The method allows us not only to train\nmodels and test with visually protected images but to also avoid the\nperformance degradation caused from the use of encrypted images, whereas\nconventional methods cannot avoid the influence of image encryption. A domain\nadaptation method is used to efficiently fine-tune ViT with encrypted images.\nIn experiments, the method is demonstrated to outperform conventional methods\nin an image classification task on the CIFAR-10 and ImageNet datasets in terms\nof classification accuracy.",
    "updated" : "2024-01-10T12:46:31Z",
    "published" : "2024-01-10T12:46:31Z",
    "authors" : [
      {
        "name" : "Teru Nagamori"
      },
      {
        "name" : "Sayaka Shiota"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.05835v1",
    "title" : "Privacy Analysis of Affine Transformations in Cloud-based MPC:\n  Vulnerability to Side-knowledge",
    "summary" : "Search for the optimizer in computationally demanding model predictive\ncontrol (MPC) setups can be facilitated by Cloud as a service provider in\ncyber-physical systems. This advantage introduces the risk that Cloud can\nobtain unauthorized access to the privacy-sensitive parameters of the system\nand cost function. To solve this issue, i.e., preventing Cloud from accessing\nthe parameters while benefiting from Cloud computation, random affine\ntransformations provide an exact yet light weight in computation solution. This\nresearch deals with analyzing privacy preserving properties of these\ntransformations when they are adopted for MPC problems. We consider two common\nstrategies for outsourcing the optimization required in MPC problems, namely\nseparate and dense forms, and establish that random affine transformations\nutilized in these forms are vulnerable to side-knowledge from Cloud.\nSpecifically, we prove that the privacy guarantees of these methods and their\nextensions for separate form are undermined when a mild side-knowledge about\nthe problem in terms of structure of MPC cost function is available. In\naddition, while we prove that outsourcing the MPC problem in the dense form\ninherently leads to some degree of privacy for the system and cost function\nparameters, we also establish that affine transformations applied to this form\nare nevertheless prone to be undermined by a Cloud with mild side-knowledge.\nNumerical simulations confirm our results.",
    "updated" : "2024-01-11T11:08:20Z",
    "published" : "2024-01-11T11:08:20Z",
    "authors" : [
      {
        "name" : "Teimour Hosseinalizadeh"
      },
      {
        "name" : "Nils Schlüter"
      },
      {
        "name" : "Moritz Schulze Darup"
      },
      {
        "name" : "Nima Monshizadeh"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.05562v1",
    "title" : "Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated\n  Learning",
    "summary" : "Federated learning (FL) enables multiple participants to train a global\nmachine learning model without sharing their private training data.\nPeer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating\nthe server that aggregates local models from participants and then updates the\nglobal model. However, P2P FL is vulnerable to (i) honest-but-curious\nparticipants whose objective is to infer private training data of other\nparticipants, and (ii) Byzantine participants who can transmit arbitrarily\nmanipulated local models to corrupt the learning process. P2P FL schemes that\nsimultaneously guarantee Byzantine resilience and preserve privacy have been\nless studied. In this paper, we develop Brave, a protocol that ensures\nByzantine Resilience And privacy-preserving property for P2P FL in the presence\nof both types of adversaries. We show that Brave preserves privacy by\nestablishing that any honest-but-curious adversary cannot infer other\nparticipants' private data by observing their models. We further prove that\nBrave is Byzantine-resilient, which guarantees that all benign participants\nconverge to an identical model that deviates from a global model trained\nwithout Byzantine adversaries by a bounded distance. We evaluate Brave against\nthree state-of-the-art adversaries on a P2P FL for image classification tasks\non benchmark datasets CIFAR10 and MNIST. Our results show that the global model\nlearned with Brave in the presence of adversaries achieves comparable\nclassification accuracy to a global model trained in the absence of any\nadversary.",
    "updated" : "2024-01-10T22:07:40Z",
    "published" : "2024-01-10T22:07:40Z",
    "authors" : [
      {
        "name" : "Zhangchen Xu"
      },
      {
        "name" : "Fengqing Jiang"
      },
      {
        "name" : "Luyao Niu"
      },
      {
        "name" : "Jinyuan Jia"
      },
      {
        "name" : "Radha Poovendran"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.06657v1",
    "title" : "Accelerating Tactile Internet with QUIC: A Security and Privacy\n  Perspective",
    "summary" : "The Tactile Internet paradigm is set to revolutionize human society by\nenabling skill-set delivery and haptic communication over ultra-reliable,\nlow-latency networks. The emerging sixth-generation (6G) mobile communication\nsystems are envisioned to underpin this Tactile Internet ecosystem at the\nnetwork edge by providing ubiquitous global connectivity. However, apart from a\nmultitude of opportunities of the Tactile Internet, security and privacy\nchallenges emerge at the forefront. We believe that the recently standardized\nQUIC protocol, characterized by end-to-end encryption and reduced round-trip\ndelay would serve as the backbone of Tactile Internet. In this article, we\nenvision a futuristic scenario where a QUIC-enabled network uses the underlying\n6G communication infrastructure to achieve the requirements for Tactile\nInternet. Interestingly this requires a deeper investigation of a wide range of\nsecurity and privacy challenges in QUIC, that need to be mitigated for its\nadoption in Tactile Internet. Henceforth, this article reviews the existing\nsecurity and privacy attacks in QUIC and their implication on users. Followed\nby that, we discuss state-of-the-art attack mitigation strategies and\ninvestigate some of their drawbacks with possible directions for future work",
    "updated" : "2024-01-12T16:05:13Z",
    "published" : "2024-01-12T16:05:13Z",
    "authors" : [
      {
        "name" : "Jayasree Sengupta"
      },
      {
        "name" : "Debasmita Dey"
      },
      {
        "name" : "Simone Ferlin"
      },
      {
        "name" : "Nirnay Ghosh"
      },
      {
        "name" : "Vaibhav Bajpai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.06601v1",
    "title" : "A proposal to increase data utility on Global Differential Privacy data\n  based on data use predictions",
    "summary" : "This paper presents ongoing research focused on improving the utility of data\nprotected by Global Differential Privacy(DP) in the scenario of summary\nstatistics. Our approach is based on predictions on how an analyst will use\nstatistics released under DP protection, so that a developer can optimise data\nutility on further usage of the data in the privacy budget allocation. This\nnovel approach can potentially improve the utility of data without compromising\nprivacy constraints. We also propose a metric that can be used by the developer\nto optimise the budget allocation process.",
    "updated" : "2024-01-12T14:34:30Z",
    "published" : "2024-01-12T14:34:30Z",
    "authors" : [
      {
        "name" : "Henry C. Nunes"
      },
      {
        "name" : "Marlon P. da Silva"
      },
      {
        "name" : "Charles V. Neu"
      },
      {
        "name" : "Avelino F. Zorzo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08038v1",
    "title" : "Calpric: Inclusive and Fine-grain Labeling of Privacy Policies with\n  Crowdsourcing and Active Learning",
    "summary" : "A significant challenge to training accurate deep learning models on privacy\npolicies is the cost and difficulty of obtaining a large and comprehensive set\nof training data. To address these challenges, we present Calpric , which\ncombines automatic text selection and segmentation, active learning and the use\nof crowdsourced annotators to generate a large, balanced training set for\nprivacy policies at low cost. Automated text selection and segmentation\nsimplifies the labeling task, enabling untrained annotators from crowdsourcing\nplatforms, like Amazon's Mechanical Turk, to be competitive with trained\nannotators, such as law students, and also reduces inter-annotator agreement,\nwhich decreases labeling cost. Having reliable labels for training enables the\nuse of active learning, which uses fewer training samples to efficiently cover\nthe input space, further reducing cost and improving class and data category\nbalance in the data set. The combination of these techniques allows Calpric to\nproduce models that are accurate over a wider range of data categories, and\nprovide more detailed, fine-grain labels than previous work. Our crowdsourcing\nprocess enables Calpric to attain reliable labeled data at a cost of roughly\n$0.92-$1.71 per labeled text segment. Calpric 's training process also\ngenerates a labeled data set of 16K privacy policy text segments across 9 Data\ncategories with balanced positive and negative samples.",
    "updated" : "2024-01-16T01:27:26Z",
    "published" : "2024-01-16T01:27:26Z",
    "authors" : [
      {
        "name" : "Wenjun Qiu"
      },
      {
        "name" : "David Lie"
      },
      {
        "name" : "Lisa Austin"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.HC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08037v1",
    "title" : "Understanding factors behind IoT privacy -- A user's perspective on RF\n  sensors",
    "summary" : "While IoT sensors in physical spaces have provided utility and comfort in our\nlives, their instrumentation in private and personal spaces has led to growing\nconcerns regarding privacy. The existing notion behind IoT privacy is that the\nsensors whose data can easily be understood and interpreted by humans (such as\ncameras) are more privacy-invasive than sensors that are not\nhuman-understandable, such as RF (radio-frequency) sensors. However, given\nrecent advancements in machine learning, we can not only make sensitive\ninferences on RF data but also translate between modalities. Thus, the existing\nnotions of privacy for IoT sensors need to be revisited. In this paper, our\ngoal is to understand what factors affect the privacy notions of a non-expert\nuser (someone who is not well-versed in privacy concepts). To this regard, we\nconduct an online study of 162 participants from the USA to find out what\nfactors affect the privacy perception of a user regarding an RF-based device or\na sensor. Our findings show that a user's perception of privacy not only\ndepends upon the data collected by the sensor but also on the inferences that\ncan be made on that data, familiarity with the device and its form factor as\nwell as the control a user has over the device design and its data policies.\nWhen the data collected by the sensor is not human-interpretable, it is the\ninferences that can be made on the data and not the data itself that users care\nabout when making informed decisions regarding device privacy.",
    "updated" : "2024-01-16T01:13:14Z",
    "published" : "2024-01-16T01:13:14Z",
    "authors" : [
      {
        "name" : "Akash Deep Singh"
      },
      {
        "name" : "Brian Wang"
      },
      {
        "name" : "Luis Garcia"
      },
      {
        "name" : "Xiang Chen"
      },
      {
        "name" : "Mani Srivastava"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07691v1",
    "title" : "Privacy-Aware Single-Nucleotide Polymorphisms (SNPs) using Bilinear\n  Group Accumulators in Batch Mode",
    "summary" : "Biometric data is often highly sensitive, and a leak of this data can lead to\nserious privacy breaches. Some of the most sensitive of this type of data\nrelates to the usage of DNA data on individuals. A leak of this type of data\nwithout consent could lead to privacy breaches of data protection laws. Along\nwith this, there have been several recent data breaches related to the leak of\nDNA information, including from 23andMe and Ancestry. It is thus fundamental\nthat a citizen should have the right to know if their DNA data is contained\nwithin a DNA database and ask for it to be removed if they are concerned about\nits usage. This paper outlines a method of hashing the core information\ncontained within the data stores - known as Single-Nucleotide Polymorphisms\n(SNPs) - into a bilinear group accumulator in batch mode, which can then be\nsearched by a trusted entity for matches. The time to create the witness proof\nand to verify were measured at 0.86 ms and 10.90 ms, respectively.",
    "updated" : "2024-01-15T13:59:51Z",
    "published" : "2024-01-15T13:59:51Z",
    "authors" : [
      {
        "name" : "William J Buchanan"
      },
      {
        "name" : "Sam Grierson"
      },
      {
        "name" : "Daniel Uribe"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07464v1",
    "title" : "Quantum Privacy Aggregation of Teacher Ensembles (QPATE) for\n  Privacy-preserving Quantum Machine Learning",
    "summary" : "The utility of machine learning has rapidly expanded in the last two decades\nand presents an ethical challenge. Papernot et. al. developed a technique,\nknown as Private Aggregation of Teacher Ensembles (PATE) to enable federated\nlearning in which multiple teacher models are trained on disjoint datasets.\nThis study is the first to apply PATE to an ensemble of quantum neural networks\n(QNN) to pave a new way of ensuring privacy in quantum machine learning (QML)\nmodels.",
    "updated" : "2024-01-15T04:38:06Z",
    "published" : "2024-01-15T04:38:06Z",
    "authors" : [
      {
        "name" : "William Watkins"
      },
      {
        "name" : "Heehwan Wang"
      },
      {
        "name" : "Sangyoon Bae"
      },
      {
        "name" : "Huan-Hsin Tseng"
      },
      {
        "name" : "Jiook Cha"
      },
      {
        "name" : "Samuel Yen-Chi Chen"
      },
      {
        "name" : "Shinjae Yoo"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07348v1",
    "title" : "Generative AI in EU Law: Liability, Privacy, Intellectual Property, and\n  Cybersecurity",
    "summary" : "The advent of Generative AI, particularly through Large Language Models\n(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI\nlandscape. Advanced LLMs exhibit multimodality, handling diverse data formats,\nthereby broadening their application scope. However, the complexity and\nemergent autonomy of these models introduce challenges in predictability and\nlegal compliance. This paper delves into the legal and regulatory implications\nof Generative AI and LLMs in the European Union context, analyzing aspects of\nliability, privacy, intellectual property, and cybersecurity. It critically\nexamines the adequacy of the existing and proposed EU legislation, including\nthe Artificial Intelligence Act (AIA) draft, in addressing the unique\nchallenges posed by Generative AI in general and LLMs in particular. The paper\nidentifies potential gaps and shortcomings in the legislative framework and\nproposes recommendations to ensure the safe and compliant deployment of\ngenerative models, ensuring they align with the EU's evolving digital landscape\nand legal standards.",
    "updated" : "2024-01-14T19:16:29Z",
    "published" : "2024-01-14T19:16:29Z",
    "authors" : [
      {
        "name" : "Claudio Novelli"
      },
      {
        "name" : "Federico Casolari"
      },
      {
        "name" : "Philipp Hacker"
      },
      {
        "name" : "Giorgio Spedicato"
      },
      {
        "name" : "Luciano Floridi"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07343v1",
    "title" : "Privacy-Preserving Intrusion Detection in Software-defined VANET using\n  Federated Learning with BERT",
    "summary" : "The absence of robust security protocols renders the VANET (Vehicle ad-hoc\nNetworks) network open to cyber threats by compromising passengers and road\nsafety. Intrusion Detection Systems (IDS) are widely employed to detect network\nsecurity threats. With vehicles' high mobility on the road and diverse\nenvironments, VANETs devise ever-changing network topologies, lack privacy and\nsecurity, and have limited bandwidth efficiency. The absence of privacy\nprecautions, End-to-End Encryption methods, and Local Data Processing systems\nin VANET also present many privacy and security difficulties. So, assessing\nwhether a novel real-time processing IDS approach can be utilized for this\nemerging technology is crucial. The present study introduces a novel approach\nfor intrusion detection using Federated Learning (FL) capabilities in\nconjunction with the BERT model for sequence classification (FL-BERT). The\nsignificance of data privacy is duly recognized. According to FL methodology,\neach client has its own local model and dataset. They train their models\nlocally and then send the model's weights to the server. After aggregation, the\nserver aggregates the weights from all clients to update a global model. After\naggregation, the global model's weights are shared with the clients. This\npractice guarantees the secure storage of sensitive raw data on individual\nclients' devices, effectively protecting privacy. After conducting the\nfederated learning procedure, we assessed our models' performance using a\nseparate test dataset. The FL-BERT technique has yielded promising results,\nopening avenues for further investigation in this particular area of research.\nWe reached the result of our approaches by comparing existing research works\nand found that FL-BERT is more effective for privacy and security concerns. Our\nresults suggest that FL-BERT is a promising technique for enhancing attack\ndetection.",
    "updated" : "2024-01-14T18:32:25Z",
    "published" : "2024-01-14T18:32:25Z",
    "authors" : [
      {
        "name" : "Shakil Ibne Ahsan"
      },
      {
        "name" : "Phil Legg"
      },
      {
        "name" : "S M Iftekharul Alam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07316v1",
    "title" : "Finding Privacy-relevant Source Code",
    "summary" : "Privacy code review is a critical process that enables developers and legal\nexperts to ensure compliance with data protection regulations. However, the\ntask is challenging due to resource constraints. To address this, we introduce\nthe concept of privacy-relevant methods - specific methods in code that are\ndirectly involved in the processing of personal data. We then present an\nautomated approach to assist in code review by identifying and categorizing\nthese privacy-relevant methods in source code.\n  Using static analysis, we identify a set of methods based on their\noccurrences in 50 commonly used libraries. We then rank these methods according\nto their frequency of invocation with actual personal data in the top 30 GitHub\napplications. The highest-ranked methods are the ones we designate as\nprivacy-relevant in practice. For our evaluation, we examined 100 open-source\napplications and found that our approach identifies fewer than 5% of the\nmethods as privacy-relevant for personal data processing. This reduces the time\nrequired for code reviews. Case studies on Signal Desktop and Cal.com further\nvalidate the effectiveness of our approach in aiding code reviewers to produce\nenhanced reports that facilitate compliance with privacy regulations.",
    "updated" : "2024-01-14T15:38:29Z",
    "published" : "2024-01-14T15:38:29Z",
    "authors" : [
      {
        "name" : "Feiyang Tang"
      },
      {
        "name" : "Bjarte M. Østvold"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.06894v1",
    "title" : "On Coded Caching Systems with Offline Users, with and without Demand\n  Privacy against Colluding Users",
    "summary" : "Coded caching is a technique that leverages locally cached contents at the\nend users to reduce the network's peak-time communication load. Coded caching\nhas been shown to achieve significant performance gains compared to uncoded\nschemes and is thus considered a promising technique to boost performance in\nfuture networks by effectively trading off bandwidth for storage. The original\ncoded caching model introduced by Maddah-Ali and Niesen does not consider the\ncase where some users involved in the placement phase, may be offline during\nthe delivery phase. If so, the delivery may not start or it may be wasteful to\nperform the delivery with fictitious demands for the offline users. In\naddition, the active users may require their demand to be kept private. This\npaper formally defines a coded caching system where some users are offline, and\ninvestigates the optimal performance with and without demand privacy against\ncolluding users. For this novel coded caching model with offline users,\nachievable and converse bounds are proposed. These bounds are shown to meet\nunder certain conditions, and otherwise to be to within a constant\nmultiplicative gap of one another. In addition, the proposed achievable schemes\nhave lower subpacketization and lower load compared to baseline schemes (that\ntrivially extend known schemes so as to accommodate for privacy) in some memory\nregimes.",
    "updated" : "2024-01-12T21:06:42Z",
    "published" : "2024-01-12T21:06:42Z",
    "authors" : [
      {
        "name" : "Yinbin Ma"
      },
      {
        "name" : "Daniela Tuninetti"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.06883v1",
    "title" : "Scaling While Privacy Preserving: A Comprehensive Synthetic Tabular Data\n  Generation and Evaluation in Learning Analytics",
    "summary" : "Privacy poses a significant obstacle to the progress of learning analytics\n(LA), presenting challenges like inadequate anonymization and data misuse that\ncurrent solutions struggle to address. Synthetic data emerges as a potential\nremedy, offering robust privacy protection. However, prior LA research on\nsynthetic data lacks thorough evaluation, essential for assessing the delicate\nbalance between privacy and data utility. Synthetic data must not only enhance\nprivacy but also remain practical for data analytics. Moreover, diverse LA\nscenarios come with varying privacy and utility needs, making the selection of\nan appropriate synthetic data approach a pressing challenge. To address these\ngaps, we propose a comprehensive evaluation of synthetic data, which\nencompasses three dimensions of synthetic data quality, namely resemblance,\nutility, and privacy. We apply this evaluation to three distinct LA datasets,\nusing three different synthetic data generation methods. Our results show that\nsynthetic data can maintain similar utility (i.e., predictive performance) as\nreal data, while preserving privacy. Furthermore, considering different privacy\nand data utility requirements in different LA scenarios, we make customized\nrecommendations for synthetic data generation. This paper not only presents a\ncomprehensive evaluation of synthetic data but also illustrates its potential\nin mitigating privacy concerns within the field of LA, thus contributing to a\nwider application of synthetic data in LA and promoting a better practice for\nopen science.",
    "updated" : "2024-01-12T20:27:55Z",
    "published" : "2024-01-12T20:27:55Z",
    "authors" : [
      {
        "name" : "Qinyi Liu"
      },
      {
        "name" : "Mohammad Khalil"
      },
      {
        "name" : "Ronas Shakya"
      },
      {
        "name" : "Jelena Jovanovic"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.04076v2",
    "title" : "Security and Privacy Issues in Cloud Storage",
    "summary" : "Even with the vast potential that cloud computing has, so far, it has not\nbeen adopted by the consumers with the enthusiasm and pace that it be worthy;\nthis is a very reason statement why consumers still hesitated of using cloud\ncomputing for their sensitive data and the threats that prevent the consumers\nfrom shifting to use cloud computing in general and cloud storage in\nparticular. The cloud computing inherits the traditional potential security and\nprivacy threats besides its own issues due to its unique structures. Some\nthreats related to cloud computing are the insider malicious attacks from the\nemployees that even sometime the provider unconscious about, the lack of\ntransparency of agreement between consumer and provider, data loss, traffic\nhijacking, shared technology and insecure application interface. Such threats\nneed remedies to make the consumer use its features in secure way. In this\nreview, we spot the light on the most security and privacy issues which can be\nattributed as gaps that sometimes the consumers or even the enterprises are not\naware of. We also define the parties that involve in scenario of cloud\ncomputing that also may attack the entire cloud systems. We also show the\nconsequences of these threats.",
    "updated" : "2024-01-14T20:12:02Z",
    "published" : "2024-01-08T18:27:57Z",
    "authors" : [
      {
        "name" : "Norah Asiri"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03883v2",
    "title" : "The Impact of Differential Privacy on Recommendation Accuracy and\n  Popularity Bias",
    "summary" : "Collaborative filtering-based recommender systems leverage vast amounts of\nbehavioral user data, which poses severe privacy risks. Thus, often, random\nnoise is added to the data to ensure Differential Privacy (DP). However, to\ndate, it is not well understood, in which ways this impacts personalized\nrecommendations. In this work, we study how DP impacts recommendation accuracy\nand popularity bias, when applied to the training data of state-of-the-art\nrecommendation models. Our findings are three-fold: First, we find that nearly\nall users' recommendations change when DP is applied. Second, recommendation\naccuracy drops substantially while recommended item popularity experiences a\nsharp increase, suggesting that popularity bias worsens. Third, we find that DP\nexacerbates popularity bias more severely for users who prefer unpopular items\nthan for users that prefer popular items.",
    "updated" : "2024-01-15T15:09:00Z",
    "published" : "2024-01-08T13:31:02Z",
    "authors" : [
      {
        "name" : "Peter Müllner"
      },
      {
        "name" : "Elisabeth Lex"
      },
      {
        "name" : "Markus Schedl"
      },
      {
        "name" : "Dominik Kowald"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03218v2",
    "title" : "MiniScope: Automated UI Exploration and Privacy Inconsistency Detection\n  of MiniApps via Two-phase Iterative Hybrid Analysis",
    "summary" : "The advent of MiniApps, operating within larger SuperApps, has revolutionized\nuser experiences by offering a wide range of services without the need for\nindividual app downloads. However, this convenience has raised significant\nprivacy concerns, as these MiniApps often require access to sensitive data,\npotentially leading to privacy violations. Our research addresses the critical\ngaps in the analysis of MiniApps' privacy practices, especially focusing on\nWeChat MiniApps in the Android ecosystem. Despite existing privacy regulations\nand platform guidelines, there is a lack of effective mechanisms to safeguard\nuser privacy fully. We introduce MiniScope, a novel two-phase hybrid analysis\napproach, specifically designed for the MiniApp environment. This approach\novercomes the limitations of existing static analysis techniques by\nincorporating dynamic UI exploration for complete code coverage and accurate\nprivacy practice identification. Our methodology includes modeling UI\ntransition states, resolving cross-package callback control flows, and\nautomated iterative UI exploration. This allows for a comprehensive\nunderstanding of MiniApps' privacy practices, addressing the unique challenges\nof sub-package loading and event-driven callbacks. Our empirical evaluation of\nover 120K MiniApps using MiniScope demonstrates its effectiveness in\nidentifying privacy inconsistencies. The results reveal significant issues,\nwith 5.7% of MiniApps over-collecting private data and 33.4% overclaiming data\ncollection. These findings emphasize the urgent need for more precise privacy\nmonitoring systems and highlight the responsibility of SuperApp operators to\nenforce stricter privacy measures.",
    "updated" : "2024-01-16T03:07:22Z",
    "published" : "2024-01-06T13:54:36Z",
    "authors" : [
      {
        "name" : "Shenao Wang"
      },
      {
        "name" : "Yuekang Li"
      },
      {
        "name" : "Kailong Wang"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Hui Li"
      },
      {
        "name" : "Yang Liu"
      },
      {
        "name" : "Haoyu Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08935v1",
    "title" : "Privacy Protected Contactless Cardio-respiratory Monitoring using\n  Defocused Cameras during Sleep",
    "summary" : "The monitoring of vital signs such as heart rate (HR) and respiratory rate\n(RR) during sleep is important for the assessment of sleep quality and\ndetection of sleep disorders. Camera-based HR and RR monitoring gained\npopularity in sleep monitoring in recent years. However, they are all facing\nwith serious privacy issues when using a video camera in the sleeping scenario.\nIn this paper, we propose to use the defocused camera to measure vital signs\nfrom optically blurred images, which can fundamentally eliminate the privacy\ninvasion as face is difficult to be identified in obtained blurry images. A\nspatial-redundant framework involving living-skin detection is used to extract\nHR and RR from the defocused camera in NIR, and a motion metric is designed to\nexclude outliers caused by body motions. In the benchmark, the overall Mean\nAbsolute Error (MAE) for HR measurement is 4.4 bpm, for RR measurement is 5.9\nbpm. Both have quality drops as compared to the measurement using a focused\ncamera, but the degradation in HR is much less, i.e. HR measurement has strong\ncorrelation with the reference ($R \\geq 0.90$). Preliminary experiments suggest\nthat it is feasible to use a defocused camera for cardio-respiratory monitoring\nwhile protecting the privacy. Further improvement is needed for robust RR\nmeasurement, such as by PPG-modulation based RR extraction.",
    "updated" : "2024-01-17T03:05:24Z",
    "published" : "2024-01-17T03:05:24Z",
    "authors" : [
      {
        "name" : "Yingen Zhu"
      },
      {
        "name" : "Jia Huang"
      },
      {
        "name" : "Hongzhou Lu"
      },
      {
        "name" : "Wenjin Wang"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08458v1",
    "title" : "Security and Privacy Issues and Solutions in Federated Learning for\n  Digital Healthcare",
    "summary" : "The advent of Federated Learning has enabled the creation of a\nhigh-performing model as if it had been trained on a considerable amount of\ndata. A multitude of participants and a server cooperatively train a model\nwithout the need for data disclosure or collection. The healthcare industry,\nwhere security and privacy are paramount, can substantially benefit from this\nnew learning paradigm, as data collection is no longer feasible due to\nstringent data policies. Nonetheless, unaddressed challenges and insufficient\nattack mitigation are hampering its adoption. Attack surfaces differ from\ntraditional centralized learning in that the server and clients communicate\nbetween each round of training. In this paper, we thus present vulnerabilities,\nattacks, and defenses based on the widened attack surfaces, as well as suggest\npromising new research directions toward a more robust FL.",
    "updated" : "2024-01-16T16:07:53Z",
    "published" : "2024-01-16T16:07:53Z",
    "authors" : [
      {
        "name" : "Hyejun Jeong"
      },
      {
        "name" : "Tai-Myoung Chung"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08723v1",
    "title" : "HierSFL: Local Differential Privacy-aided Split Federated Learning in\n  Mobile Edge Computing",
    "summary" : "Federated Learning is a promising approach for learning from user data while\npreserving data privacy. However, the high requirements of the model training\nprocess make it difficult for clients with limited memory or bandwidth to\nparticipate. To tackle this problem, Split Federated Learning is utilized,\nwhere clients upload their intermediate model training outcomes to a cloud\nserver for collaborative server-client model training. This methodology\nfacilitates resource-constrained clients' participation in model training but\nalso increases the training time and communication overhead. To overcome these\nlimitations, we propose a novel algorithm, called Hierarchical Split Federated\nLearning (HierSFL), that amalgamates models at the edge and cloud phases,\npresenting qualitative directives for determining the best aggregation\ntimeframes to reduce computation and communication expenses. By implementing\nlocal differential privacy at the client and edge server levels, we enhance\nprivacy during local model parameter updates. Our experiments using CIFAR-10\nand MNIST datasets show that HierSFL outperforms standard FL approaches with\nbetter training accuracy, training time, and communication-computing\ntrade-offs. HierSFL offers a promising solution to mobile edge computing's\nchallenges, ultimately leading to faster content delivery and improved mobile\nservice quality.",
    "updated" : "2024-01-16T09:34:10Z",
    "published" : "2024-01-16T09:34:10Z",
    "authors" : [
      {
        "name" : "Minh K. Quan"
      },
      {
        "name" : "Dinh C. Nguyen"
      },
      {
        "name" : "Van-Dinh Nguyen"
      },
      {
        "name" : "Mayuri Wijayasundara"
      },
      {
        "name" : "Sujeeva Setunge"
      },
      {
        "name" : "Pubudu N. Pathirana"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07343v2",
    "title" : "Privacy-Preserving Intrusion Detection in Software-defined VANET using\n  Federated Learning with BERT",
    "summary" : "The absence of robust security protocols renders the VANET (Vehicle ad-hoc\nNetworks) network open to cyber threats by compromising passengers and road\nsafety. Intrusion Detection Systems (IDS) are widely employed to detect network\nsecurity threats. With vehicles' high mobility on the road and diverse\nenvironments, VANETs devise ever-changing network topologies, lack privacy and\nsecurity, and have limited bandwidth efficiency. The absence of privacy\nprecautions, End-to-End Encryption methods, and Local Data Processing systems\nin VANET also present many privacy and security difficulties. So, assessing\nwhether a novel real-time processing IDS approach can be utilized for this\nemerging technology is crucial. The present study introduces a novel approach\nfor intrusion detection using Federated Learning (FL) capabilities in\nconjunction with the BERT model for sequence classification (FL-BERT). The\nsignificance of data privacy is duly recognized. According to FL methodology,\neach client has its own local model and dataset. They train their models\nlocally and then send the model's weights to the server. After aggregation, the\nserver aggregates the weights from all clients to update a global model. After\naggregation, the global model's weights are shared with the clients. This\npractice guarantees the secure storage of sensitive raw data on individual\nclients' devices, effectively protecting privacy. After conducting the\nfederated learning procedure, we assessed our models' performance using a\nseparate test dataset. The FL-BERT technique has yielded promising results,\nopening avenues for further investigation in this particular area of research.\nWe reached the result of our approaches by comparing existing research works\nand found that FL-BERT is more effective for privacy and security concerns. Our\nresults suggest that FL-BERT is a promising technique for enhancing attack\ndetection.",
    "updated" : "2024-01-17T09:25:48Z",
    "published" : "2024-01-14T18:32:25Z",
    "authors" : [
      {
        "name" : "Shakil Ibne Ahsan"
      },
      {
        "name" : "Phil Legg"
      },
      {
        "name" : "S M Iftekharul Alam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.09604v1",
    "title" : "MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical\n  Images with Transformers and Fully Homomorphic Encryption",
    "summary" : "Advancements in machine learning (ML) have significantly revolutionized\nmedical image analysis, prompting hospitals to rely on external ML services.\nHowever, the exchange of sensitive patient data, such as chest X-rays, poses\ninherent privacy risks when shared with third parties. Addressing this concern,\nwe propose MedBlindTuner, a privacy-preserving framework leveraging fully\nhomomorphic encryption (FHE) and a data-efficient image transformer (DEiT).\nMedBlindTuner enables the training of ML models exclusively on FHE-encrypted\nmedical images. Our experimental evaluation demonstrates that MedBlindTuner\nachieves comparable accuracy to models trained on non-encrypted images,\noffering a secure solution for outsourcing ML computations while preserving\npatient data privacy. To the best of our knowledge, this is the first work that\nuses data-efficient image transformers and fully homomorphic encryption in this\ndomain.",
    "updated" : "2024-01-17T21:30:22Z",
    "published" : "2024-01-17T21:30:22Z",
    "authors" : [
      {
        "name" : "Prajwal Panzade"
      },
      {
        "name" : "Daniel Takabi"
      },
      {
        "name" : "Zhipeng Cai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.09519v1",
    "title" : "Privacy Engineering in Smart Home (SH) Systems: A Comprehensive Privacy\n  Threat Analysis and Risk Management Approach",
    "summary" : "Addressing trust concerns in Smart Home (SH) systems is imperative due to the\nlimited study on preservation approaches that focus on analyzing and evaluating\nprivacy threats for effective risk management. While most research focuses\nprimarily on user privacy, device data privacy, especially identity privacy, is\nalmost neglected, which can significantly impact overall user privacy within\nthe SH system. To this end, our study incorporates privacy engineering (PE)\nprinciples in the SH system that consider user and device data privacy. We\nstart with a comprehensive reference model for a typical SH system. Based on\nthe initial stage of LINDDUN PRO for the PE framework, we present a data flow\ndiagram (DFD) based on a typical SH reference model to better understand SH\nsystem operations. To identify potential areas of privacy threat and perform a\nprivacy threat analysis (PTA), we employ the LINDDUN PRO threat model. Then, a\nprivacy impact assessment (PIA) was carried out to implement privacy risk\nmanagement by prioritizing privacy threats based on their likelihood of\noccurrence and potential consequences. Finally, we suggest possible privacy\nenhancement techniques (PETs) that can mitigate some of these threats. The\nstudy aims to elucidate the main threats to privacy, associated risks, and\neffective prioritization of privacy control in SH systems. The outcomes of this\nstudy are expected to benefit SH stakeholders, including vendors, cloud\nproviders, users, researchers, and regulatory bodies in the SH systems domain.",
    "updated" : "2024-01-17T17:34:52Z",
    "published" : "2024-01-17T17:34:52Z",
    "authors" : [
      {
        "name" : "Emmanuel Dare Alalade"
      },
      {
        "name" : "Mohammed Mahyoub"
      },
      {
        "name" : "Ashraf Matrawy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.10158v1",
    "title" : "DISTINQT: A Distributed Privacy Aware Learning Framework for QoS\n  Prediction for Future Mobile and Wireless Networks",
    "summary" : "Beyond 5G and 6G networks are expected to support new and challenging use\ncases and applications that depend on a certain level of Quality of Service\n(QoS) to operate smoothly. Predicting the QoS in a timely manner is of high\nimportance, especially for safety-critical applications as in the case of\nvehicular communications. Although until recent years the QoS prediction has\nbeen carried out by centralized Artificial Intelligence (AI) solutions, a\nnumber of privacy, computational, and operational concerns have emerged.\nAlternative solutions have been surfaced (e.g. Split Learning, Federated\nLearning), distributing AI tasks of reduced complexity across nodes, while\npreserving the privacy of the data. However, new challenges rise when it comes\nto scalable distributed learning approaches, taking into account the\nheterogeneous nature of future wireless networks. The current work proposes\nDISTINQT, a privacy-aware distributed learning framework for QoS prediction.\nOur framework supports multiple heterogeneous nodes, in terms of data types and\nmodel architectures, by sharing computations across them. This, enables the\nincorporation of diverse knowledge into a sole learning process that will\nenhance the robustness and generalization capabilities of the final QoS\nprediction model. DISTINQT also contributes to data privacy preservation by\nencoding any raw input data into a non-linear latent representation before any\ntransmission. Evaluation results showcase that our framework achieves a\nstatistically identical performance compared to its centralized version and an\naverage performance improvement of up to 65% against six state-of-the-art\ncentralized baseline solutions in the Tele-Operated Driving use case.",
    "updated" : "2024-01-15T13:00:48Z",
    "published" : "2024-01-15T13:00:48Z",
    "authors" : [
      {
        "name" : "Nikolaos Koursioumpas"
      },
      {
        "name" : "Lina Magoula"
      },
      {
        "name" : "Ioannis Stavrakakis"
      },
      {
        "name" : "Nancy Alonistioti"
      },
      {
        "name" : "M. A. Gutierrez-Estevez"
      },
      {
        "name" : "Ramin Khalili"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.10765v1",
    "title" : "Starlit: Privacy-Preserving Federated Learning to Enhance Financial\n  Fraud Detection",
    "summary" : "Federated Learning (FL) is a data-minimization approach enabling\ncollaborative model training across diverse clients with local data, avoiding\ndirect data exchange. However, state-of-the-art FL solutions to identify\nfraudulent financial transactions exhibit a subset of the following\nlimitations. They (1) lack a formal security definition and proof, (2) assume\nprior freezing of suspicious customers' accounts by financial institutions\n(limiting the solutions' adoption), (3) scale poorly, involving either $O(n^2)$\ncomputationally expensive modular exponentiation (where $n$ is the total number\nof financial institutions) or highly inefficient fully homomorphic encryption,\n(4) assume the parties have already completed the identity alignment phase,\nhence excluding it from the implementation, performance evaluation, and\nsecurity analysis, and (5) struggle to resist clients' dropouts. This work\nintroduces Starlit, a novel scalable privacy-preserving FL mechanism that\novercomes these limitations. It has various applications, such as enhancing\nfinancial fraud detection, mitigating terrorism, and enhancing digital health.\nWe implemented Starlit and conducted a thorough performance analysis using\nsynthetic data from a key player in global financial transactions. The\nevaluation indicates Starlit's scalability, efficiency, and accuracy.",
    "updated" : "2024-01-19T15:37:11Z",
    "published" : "2024-01-19T15:37:11Z",
    "authors" : [
      {
        "name" : "Aydin Abadi"
      },
      {
        "name" : "Bradley Doyle"
      },
      {
        "name" : "Francesco Gini"
      },
      {
        "name" : "Kieron Guinamard"
      },
      {
        "name" : "Sasi Kumar Murakonda"
      },
      {
        "name" : "Jack Liddell"
      },
      {
        "name" : "Paul Mellor"
      },
      {
        "name" : "Steven J. Murdoch"
      },
      {
        "name" : "Mohammad Naseri"
      },
      {
        "name" : "Hector Page"
      },
      {
        "name" : "George Theodorakopoulos"
      },
      {
        "name" : "Suzanne Weller"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.11983v1",
    "title" : "Lightweight Protection for Privacy in Offloaded Speech Understanding",
    "summary" : "Speech is a common input method for mobile embedded devices, but cloud-based\nspeech recognition systems pose privacy risks. Disentanglement-based encoders,\ndesigned to safeguard user privacy by filtering sensitive information from\nspeech signals, unfortunately require substantial memory and computational\nresources, which limits their use in less powerful devices. To overcome this,\nwe introduce a novel system, XXX, optimized for such devices. XXX is built on\nthe insight that speech understanding primarily relies on understanding the\nentire utterance's long-term dependencies, while privacy concerns are often\nlinked to short-term details. Therefore, XXX focuses on selectively masking\nthese short-term elements, preserving the quality of long-term speech\nunderstanding. The core of XXX is an innovative differential mask generator,\ngrounded in interpretable learning, which fine-tunes the masking process. We\ntested XXX on the STM32H7 microcontroller, assessing its performance in various\npotential attack scenarios. The results show that XXX maintains speech\nunderstanding accuracy and privacy at levels comparable to existing encoders,\nbut with a significant improvement in efficiency, achieving up to 53.3$\\times$\nfaster processing and a 134.1$\\times$ smaller memory footprint.",
    "updated" : "2024-01-22T14:36:01Z",
    "published" : "2024-01-22T14:36:01Z",
    "authors" : [
      {
        "name" : "Dongqi Cai"
      },
      {
        "name" : "Shangguang Wang"
      },
      {
        "name" : "Zeling Zhang"
      },
      {
        "name" : "Felix Xiaozhu Lin"
      },
      {
        "name" : "Mengwei Xu"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CR",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.11857v1",
    "title" : "Adversarial speech for voice privacy protection from Personalized Speech\n  generation",
    "summary" : "The rapid progress in personalized speech generation technology, including\npersonalized text-to-speech (TTS) and voice conversion (VC), poses a challenge\nin distinguishing between generated and real speech for human listeners,\nresulting in an urgent demand in protecting speakers' voices from malicious\nmisuse. In this regard, we propose a speaker protection method based on\nadversarial attacks. The proposed method perturbs speech signals by minimally\naltering the original speech while rendering downstream speech generation\nmodels unable to accurately generate the voice of the target speaker. For\nvalidation, we employ the open-source pre-trained YourTTS model for speech\ngeneration and protect the target speaker's speech in the white-box scenario.\nAutomatic speaker verification (ASV) evaluations were carried out on the\ngenerated speech as the assessment of the voice protection capability. Our\nexperimental results show that we successfully perturbed the speaker encoder of\nthe YourTTS model using the gradient-based I-FGSM adversarial perturbation\nmethod. Furthermore, the adversarial perturbation is effective in preventing\nthe YourTTS model from generating the speech of the target speaker. Audio\nsamples can be found in\nhttps://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS.",
    "updated" : "2024-01-22T11:26:59Z",
    "published" : "2024-01-22T11:26:59Z",
    "authors" : [
      {
        "name" : "Shihao Chen"
      },
      {
        "name" : "Liping Chen"
      },
      {
        "name" : "Jie Zhang"
      },
      {
        "name" : "KongAik Lee"
      },
      {
        "name" : "Zhenhua Ling"
      },
      {
        "name" : "Lirong Dai"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.11836v1",
    "title" : "Privacy-Preserving Data Fusion for Traffic State Estimation: A Vertical\n  Federated Learning Approach",
    "summary" : "This paper proposes a privacy-preserving data fusion method for traffic state\nestimation (TSE). Unlike existing works that assume all data sources to be\naccessible by a single trusted party, we explicitly address data privacy\nconcerns that arise in the collaboration and data sharing between multiple data\nowners, such as municipal authorities (MAs) and mobility providers (MPs). To\nthis end, we propose a novel vertical federated learning (FL) approach, FedTSE,\nthat enables multiple data owners to collaboratively train and apply a TSE\nmodel without having to exchange their private data. To enhance the\napplicability of the proposed FedTSE in common TSE scenarios with limited\navailability of ground-truth data, we further propose a privacy-preserving\nphysics-informed FL approach, i.e., FedTSE-PI, that integrates traffic models\ninto FL. Real-world data validation shows that the proposed methods can protect\nprivacy while yielding similar accuracy to the oracle method without privacy\nconsiderations.",
    "updated" : "2024-01-22T10:52:22Z",
    "published" : "2024-01-22T10:52:22Z",
    "authors" : [
      {
        "name" : "Qiqing Wang"
      },
      {
        "name" : "Kaidi Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.11735v1",
    "title" : "zkLogin: Privacy-Preserving Blockchain Authentication with Existing\n  Credentials",
    "summary" : "For many users, a private key based wallet serves as the primary entry point\nto blockchains. Commonly recommended wallet authentication methods, such as\nmnemonics or hardware wallets, can be cumbersome. This difficulty in user\nonboarding has significantly hindered the adoption of blockchain-based\napplications.\n  We develop zkLogin, a novel technique that leverages identity tokens issued\nby popular platforms (any OpenID Connect enabled platform e.g. Google,\nFacebook, etc.) to authenticate transactions. At the heart of zkLogin lies a\nsignature scheme allowing the signer to \\textit{sign using their existing\nOpenID accounts} and nothing else. This improves the user experience\nsignificantly as users do not need to remember a new secret and can reuse their\nexisting accounts.\n  zkLogin provides strong security and privacy guarantees. By design, zkLogin\nbuilds on top of the underlying platform's authentication mechanisms, and\nderives its security from there. Unlike prior related works however, zkLogin\navoids the use of additional trusted parties (e.g., trusted hardware or\noracles) for its security guarantees. zkLogin leverages zero-knowledge proofs\n(ZKP) to ensure that the link between a user's off-chain and on-chain\nidentities is hidden, even from the platform itself.\n  We have implemented and deployed zkLogin on the Sui blockchain as an\nalternative to traditional digital signature-based addresses. Due to the ease\nof web3 on-boarding just with social login, without requiring mnemonics, many\nhundreds of thousands zkLogin accounts have already been generated in various\nindustries such as gaming, DeFi, direct payments, NFT collections, ride\nsharing, sports racing and many more.",
    "updated" : "2024-01-22T07:23:58Z",
    "published" : "2024-01-22T07:23:58Z",
    "authors" : [
      {
        "name" : "Foteini Baldimtsi"
      },
      {
        "name" : "Konstantinos Kryptos Chalkias"
      },
      {
        "name" : "Yan Ji"
      },
      {
        "name" : "Jonas Lindstrøm"
      },
      {
        "name" : "Deepak Maram"
      },
      {
        "name" : "Ben Riva"
      },
      {
        "name" : "Arnab Roy"
      },
      {
        "name" : "Mahdi Sedaghat"
      },
      {
        "name" : "Joy Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.11592v1",
    "title" : "Differential Privacy in Hierarchical Federated Learning: A Formal\n  Analysis and Evaluation",
    "summary" : "While federated learning (FL) eliminates the transmission of raw data over a\nnetwork, it is still vulnerable to privacy breaches from the communicated model\nparameters. In this work, we formalize Differentially Private Hierarchical\nFederated Learning (DP-HFL), a DP-enhanced FL methodology that seeks to improve\nthe privacy-utility tradeoff inherent in FL. Building upon recent proposals for\nHierarchical Differential Privacy (HDP), one of the key concepts of DP-HFL is\nadapting DP noise injection at different layers of an established FL hierarchy\n-- edge devices, edge servers, and cloud servers -- according to the trust\nmodels within particular subnetworks. We conduct a comprehensive analysis of\nthe convergence behavior of DP-HFL, revealing conditions on parameter tuning\nunder which the model training process converges sublinearly to a stationarity\ngap, with this gap depending on the network hierarchy, trust model, and target\nprivacy level. Subsequent numerical evaluations demonstrate that DP-HFL obtains\nsubstantial improvements in convergence speed over baselines for different\nprivacy budgets, and validate the impact of network configuration on training.",
    "updated" : "2024-01-21T20:46:21Z",
    "published" : "2024-01-21T20:46:21Z",
    "authors" : [
      {
        "name" : "Frank Po-Chen Lin"
      },
      {
        "name" : "Christopher Brinton"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.11305v1",
    "title" : "Progress in Privacy Protection: A Review of Privacy Preserving\n  Techniques in Recommender Systems, Edge Computing, and Cloud Computing",
    "summary" : "As digital technology evolves, the increasing use of connected devices brings\nboth challenges and opportunities in the areas of mobile crowdsourcing, edge\ncomputing, and recommender systems. This survey focuses on these dynamic\nfields, emphasizing the critical need for privacy protection in our\nincreasingly data-oriented world. It explores the latest trends in these\ninterconnected areas, with a special emphasis on privacy and data security. Our\nmethod involves an in-depth analysis of various academic works, which helps us\nto gain a comprehensive understanding of these sectors and their shifting focus\ntowards privacy concerns. We present new insights and marks a significant\nadvancement in addressing privacy issues within these technologies. The survey\nis a valuable resource for researchers, industry practitioners, and policy\nmakers, offering an extensive overview of these fields and their related\nprivacy challenges, catering to a wide audience in the modern digital era.",
    "updated" : "2024-01-20T19:32:56Z",
    "published" : "2024-01-20T19:32:56Z",
    "authors" : [
      {
        "name" : "Syed Raza Bashir"
      },
      {
        "name" : "Shaina Raza"
      },
      {
        "name" : "Vojislav Misic"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.11249v1",
    "title" : "Evaluating if trust and personal information privacy concerns are\n  barriers to using health insurance that explicitly utilizes AI",
    "summary" : "Trust and privacy have emerged as significant concerns in online\ntransactions. Sharing information on health is especially sensitive but it is\nnecessary for purchasing and utilizing health insurance. Evidence shows that\nconsumers are increasingly comfortable with technology in place of humans, but\nthe expanding use of AI potentially changes this. This research explores\nwhether trust and privacy concern are barriers to the adoption of AI in health\ninsurance. Two scenarios are compared: The first scenario has limited AI that\nis not in the interface and its presence is not explicitly revealed to the\nconsumer. In the second scenario there is an AI interface and AI evaluation,\nand this is explicitly revealed to the consumer. The two scenarios were modeled\nand compared using SEM PLS-MGA. The findings show that trust is significantly\nlower in the second scenario where AI is visible. Privacy concerns are higher\nwith AI but the difference is not statistically significant within the model.",
    "updated" : "2024-01-20T15:02:56Z",
    "published" : "2024-01-20T15:02:56Z",
    "authors" : [
      {
        "name" : "Alex Zarifis"
      },
      {
        "name" : "Peter Kawalek"
      },
      {
        "name" : "Aida Azadegan"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "H.0; A.0; K.4; K.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.11225v1",
    "title" : "Protecting Personalized Trajectory with Differential Privacy under\n  Temporal Correlations",
    "summary" : "Location-based services (LBSs) in vehicular ad hoc networks (VANETs) offer\nusers numerous conveniences. However, the extensive use of LBSs raises concerns\nabout the privacy of users' trajectories, as adversaries can exploit temporal\ncorrelations between different locations to extract personal information.\nAdditionally, users have varying privacy requirements depending on the time and\nlocation. To address these issues, this paper proposes a personalized\ntrajectory privacy protection mechanism (PTPPM). This mechanism first uses the\ntemporal correlation between trajectory locations to determine the possible\nlocation set for each time instant. We identify a protection location set (PLS)\nfor each location by employing the Hilbert curve-based minimum distance search\nalgorithm. This approach incorporates the complementary features of\ngeo-indistinguishability and distortion privacy. We put forth a novel\nPermute-and-Flip mechanism for location perturbation, which maps its initial\napplication in data publishing privacy protection to a location perturbation\nmechanism. This mechanism generates fake locations with smaller perturbation\ndistances while improving the balance between privacy and quality of service\n(QoS). Simulation results show that our mechanism outperforms the benchmark by\nproviding enhanced privacy protection while meeting user's QoS requirements.",
    "updated" : "2024-01-20T12:59:08Z",
    "published" : "2024-01-20T12:59:08Z",
    "authors" : [
      {
        "name" : "Mingge Cao"
      },
      {
        "name" : "Haopeng Zhu"
      },
      {
        "name" : "Minghui Min"
      },
      {
        "name" : "Yulu Li"
      },
      {
        "name" : "Shiyin Li"
      },
      {
        "name" : "Hongliang Zhang"
      },
      {
        "name" : "Zhu Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.11089v1",
    "title" : "FedRKG: A Privacy-preserving Federated Recommendation Framework via\n  Knowledge Graph Enhancement",
    "summary" : "Federated Learning (FL) has emerged as a promising approach for preserving\ndata privacy in recommendation systems by training models locally. Recently,\nGraph Neural Networks (GNN) have gained popularity in recommendation tasks due\nto their ability to capture high-order interactions between users and items.\nHowever, privacy concerns prevent the global sharing of the entire user-item\ngraph. To address this limitation, some methods create pseudo-interacted items\nor users in the graph to compensate for missing information for each client.\nUnfortunately, these methods introduce random noise and raise privacy concerns.\nIn this paper, we propose FedRKG, a novel federated recommendation system,\nwhere a global knowledge graph (KG) is constructed and maintained on the server\nusing publicly available item information, enabling higher-order user-item\ninteractions. On the client side, a relation-aware GNN model leverages diverse\nKG relationships. To protect local interaction items and obscure gradients, we\nemploy pseudo-labeling and Local Differential Privacy (LDP). Extensive\nexperiments conducted on three real-world datasets demonstrate the competitive\nperformance of our approach compared to centralized algorithms while ensuring\nprivacy preservation. Moreover, FedRKG achieves an average accuracy improvement\nof 4% compared to existing federated learning baselines.",
    "updated" : "2024-01-20T02:38:21Z",
    "published" : "2024-01-20T02:38:21Z",
    "authors" : [
      {
        "name" : "Dezhong Yao"
      },
      {
        "name" : "Tongtong Liu"
      },
      {
        "name" : "Qi Cao"
      },
      {
        "name" : "Hai Jin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.10765v2",
    "title" : "Starlit: Privacy-Preserving Federated Learning to Enhance Financial\n  Fraud Detection",
    "summary" : "Federated Learning (FL) is a data-minimization approach enabling\ncollaborative model training across diverse clients with local data, avoiding\ndirect data exchange. However, state-of-the-art FL solutions to identify\nfraudulent financial transactions exhibit a subset of the following\nlimitations. They (1) lack a formal security definition and proof, (2) assume\nprior freezing of suspicious customers' accounts by financial institutions\n(limiting the solutions' adoption), (3) scale poorly, involving either $O(n^2)$\ncomputationally expensive modular exponentiation (where $n$ is the total number\nof financial institutions) or highly inefficient fully homomorphic encryption,\n(4) assume the parties have already completed the identity alignment phase,\nhence excluding it from the implementation, performance evaluation, and\nsecurity analysis, and (5) struggle to resist clients' dropouts. This work\nintroduces Starlit, a novel scalable privacy-preserving FL mechanism that\novercomes these limitations. It has various applications, such as enhancing\nfinancial fraud detection, mitigating terrorism, and enhancing digital health.\nWe implemented Starlit and conducted a thorough performance analysis using\nsynthetic data from a key player in global financial transactions. The\nevaluation indicates Starlit's scalability, efficiency, and accuracy.",
    "updated" : "2024-01-22T08:17:42Z",
    "published" : "2024-01-19T15:37:11Z",
    "authors" : [
      {
        "name" : "Aydin Abadi"
      },
      {
        "name" : "Bradley Doyle"
      },
      {
        "name" : "Francesco Gini"
      },
      {
        "name" : "Kieron Guinamard"
      },
      {
        "name" : "Sasi Kumar Murakonda"
      },
      {
        "name" : "Jack Liddell"
      },
      {
        "name" : "Paul Mellor"
      },
      {
        "name" : "Steven J. Murdoch"
      },
      {
        "name" : "Mohammad Naseri"
      },
      {
        "name" : "Hector Page"
      },
      {
        "name" : "George Theodorakopoulos"
      },
      {
        "name" : "Suzanne Weller"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.12436v1",
    "title" : "Wasserstein Differential Privacy",
    "summary" : "Differential privacy (DP) has achieved remarkable results in the field of\nprivacy-preserving machine learning. However, existing DP frameworks do not\nsatisfy all the conditions for becoming metrics, which prevents them from\nderiving better basic private properties and leads to exaggerated values on\nprivacy budgets. We propose Wasserstein differential privacy (WDP), an\nalternative DP framework to measure the risk of privacy leakage, which\nsatisfies the properties of symmetry and triangle inequality. We show and prove\nthat WDP has 13 excellent properties, which can be theoretical supports for the\nbetter performance of WDP than other DP frameworks. In addition, we derive a\ngeneral privacy accounting method called Wasserstein accountant, which enables\nWDP to be applied in stochastic gradient descent (SGD) scenarios containing\nsub-sampling. Experiments on basic mechanisms, compositions and deep learning\nshow that the privacy budgets obtained by Wasserstein accountant are relatively\nstable and less influenced by order. Moreover, the overestimation on privacy\nbudgets can be effectively alleviated. The code is available at\nhttps://github.com/Hifipsysta/WDP.",
    "updated" : "2024-01-23T02:08:20Z",
    "published" : "2024-01-23T02:08:20Z",
    "authors" : [
      {
        "name" : "Chengyi Yang"
      },
      {
        "name" : "Jiayin Qi"
      },
      {
        "name" : "Aimin Zhou"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.12393v1",
    "title" : "A Learning-based Declarative Privacy-Preserving Framework for Federated\n  Data Management",
    "summary" : "It is challenging to balance the privacy and accuracy for federated query\nprocessing over multiple private data silos. In this work, we will demonstrate\nan end-to-end workflow for automating an emerging privacy-preserving technique\nthat uses a deep learning model trained using the Differentially-Private\nStochastic Gradient Descent (DP-SGD) algorithm to replace portions of actual\ndata to answer a query. Our proposed novel declarative privacy-preserving\nworkflow allows users to specify \"what private information to protect\" rather\nthan \"how to protect\". Under the hood, the system automatically chooses\nquery-model transformation plans as well as hyper-parameters. At the same time,\nthe proposed workflow also allows human experts to review and tune the selected\nprivacy-preserving mechanism for audit/compliance, and optimization purposes.",
    "updated" : "2024-01-22T22:50:59Z",
    "published" : "2024-01-22T22:50:59Z",
    "authors" : [
      {
        "name" : "Hong Guan"
      },
      {
        "name" : "Summer Gautier"
      },
      {
        "name" : "Deepti Gupta"
      },
      {
        "name" : "Rajan Hari Ambrish"
      },
      {
        "name" : "Yancheng Wang"
      },
      {
        "name" : "Harsha Lakamsani"
      },
      {
        "name" : "Dhanush Giriyan"
      },
      {
        "name" : "Saajan Maslanka"
      },
      {
        "name" : "Chaowei Xiao"
      },
      {
        "name" : "Yingzhen Yang"
      },
      {
        "name" : "Jia Zou"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.12391v1",
    "title" : "Approximation of Pufferfish Privacy for Gaussian Priors",
    "summary" : "This paper studies how to approximate pufferfish privacy when the adversary's\nprior belief of the published data is Gaussian distributed. Using Monge's\noptimal transport plan, we show that $(\\epsilon, \\delta)$-pufferfish privacy is\nattained if the additive Laplace noise is calibrated to the differences in mean\nand variance of the Gaussian distributions conditioned on every discriminative\nsecret pair. A typical application is the private release of the summation (or\naverage) query, for which sufficient conditions are derived for approximating\n$\\epsilon$-statistical indistinguishability in individual's sensitive data. The\nresult is then extended to arbitrary prior beliefs trained by Gaussian mixture\nmodels (GMMs): calibrating Laplace noise to a convex combination of differences\nin mean and variance between Gaussian components attains\n$(\\epsilon,\\delta)$-pufferfish privacy.",
    "updated" : "2024-01-22T22:43:38Z",
    "published" : "2024-01-22T22:43:38Z",
    "authors" : [
      {
        "name" : "Ni Ding"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.13386v1",
    "title" : "Privacy-Preserving Face Recognition in Hybrid Frequency-Color Domain",
    "summary" : "Face recognition technology has been deployed in various real-life\napplications. The most sophisticated deep learning-based face recognition\nsystems rely on training millions of face images through complex deep neural\nnetworks to achieve high accuracy. It is quite common for clients to upload\nface images to the service provider in order to access the model inference.\nHowever, the face image is a type of sensitive biometric attribute tied to the\nidentity information of each user. Directly exposing the raw face image to the\nservice provider poses a threat to the user's privacy. Current\nprivacy-preserving approaches to face recognition focus on either concealing\nvisual information on model input or protecting model output face embedding.\nThe noticeable drop in recognition accuracy is a pitfall for most methods. This\npaper proposes a hybrid frequency-color fusion approach to reduce the input\ndimensionality of face recognition in the frequency domain. Moreover, sparse\ncolor information is also introduced to alleviate significant accuracy\ndegradation after adding differential privacy noise. Besides, an\nidentity-specific embedding mapping scheme is applied to protect original face\nembedding by enlarging the distance among identities. Lastly, secure multiparty\ncomputation is implemented for safely computing the embedding distance during\nmodel inference. The proposed method performs well on multiple widely used\nverification datasets. Moreover, it has around 2.6% to 4.2% higher accuracy\nthan the state-of-the-art in the 1:N verification scenario.",
    "updated" : "2024-01-24T11:27:32Z",
    "published" : "2024-01-24T11:27:32Z",
    "authors" : [
      {
        "name" : "Dong Han"
      },
      {
        "name" : "Yong Li"
      },
      {
        "name" : "Joachim Denzler"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.13327v1",
    "title" : "Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable\n  Stress Detection",
    "summary" : "Smartwatch health sensor data is increasingly utilized in smart health\napplications and patient monitoring, including stress detection. However, such\nmedical data often comprises sensitive personal information and is\nresource-intensive to acquire for research purposes. In response to this\nchallenge, we introduce the privacy-aware synthetization of multi-sensor\nsmartwatch health readings related to moments of stress. Our method involves\nthe generation of synthetic sequence data through Generative Adversarial\nNetworks (GANs), coupled with the implementation of Differential Privacy (DP)\nsafeguards for protecting patient information during model training. To ensure\nthe integrity of our synthetic data, we employ a range of quality assessments\nand monitor the plausibility between synthetic and original data. To test the\nusefulness, we create private machine learning models on a commonly used,\nalbeit small, stress detection dataset, exploring strategies for enhancing the\nexisting data foundation with our synthetic data. Through our GAN-based\naugmentation methods, we observe improvements in model performance, both in\nnon-private (0.45% F1) and private (11.90-15.48% F1) training scenarios. We\nunderline the potential of differentially private synthetic data in optimizing\nutility-privacy trade-offs, especially with limited availability of real\ntraining samples.",
    "updated" : "2024-01-24T09:44:57Z",
    "published" : "2024-01-24T09:44:57Z",
    "authors" : [
      {
        "name" : "Lucas Lange"
      },
      {
        "name" : "Nils Wenzlitschke"
      },
      {
        "name" : "Erhard Rahm"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.13952v1",
    "title" : "Randomized Response with Gradual Release of Privacy Budget",
    "summary" : "An algorithm is developed to gradually relax the Differential Privacy (DP)\nguarantee of a randomized response. The output from each relaxation maintains\nthe same probability distribution as a standard randomized response with the\nequivalent DP guarantee, ensuring identical utility as the standard approach.\nThe entire relaxation process is proven to have the same DP guarantee as the\nmost recent relaxed guarantee.\n  The DP relaxation algorithm is adaptable to any Local Differential Privacy\n(LDP) mechanisms relying on randomized response. It has been seamlessly\nintegrated into RAPPOR, an LDP crowdsourcing string-collecting tool, to\noptimize the utility of estimating the frequency of collected data.\nAdditionally, it facilitates the relaxation of the DP guarantee for mean\nestimation based on randomized response. Finally, numerical experiments have\nbeen conducted to validate the utility and DP guarantee of the algorithm.",
    "updated" : "2024-01-25T05:18:47Z",
    "published" : "2024-01-25T05:18:47Z",
    "authors" : [
      {
        "name" : "Mingen Pan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.13848v1",
    "title" : "A V2X-based Privacy Preserving Federated Measuring and Learning System",
    "summary" : "Future autonomous vehicles (AVs) will use a variety of sensors that generate\na vast amount of data. Naturally, this data not only serves self-driving\nalgorithms; but can also assist other vehicles or the infrastructure in\nreal-time decision-making. Consequently, vehicles shall exchange their\nmeasurement data over Vehicle-to-Everything (V2X) technologies. Moreover,\npredicting the state of the road network might be beneficial too. With such a\nprediction, we might mitigate road congestion, balance parking lot usage, or\noptimize the traffic flow. That would decrease transportation costs as well as\nreduce its environmental impact.\n  In this paper, we propose a federated measurement and learning system that\nprovides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V)\ncommunication while also operating a federated learning (FL) scheme over the\nVehicle-to-Network (V2N) link to create a predictive model of the\ntransportation network. As we are yet to have real-world AV data, we model it\nwith a non-IID (independent and identically distributed) dataset to evaluate\nthe capabilities of the proposed system in terms of performance and privacy.\nResults indicate that the proposed FL scheme improves learning performance and\nprevents eavesdropping at the aggregator server side.",
    "updated" : "2024-01-24T23:11:11Z",
    "published" : "2024-01-24T23:11:11Z",
    "authors" : [
      {
        "name" : "Levente Alekszejenkó"
      },
      {
        "name" : "Tadeusz Dobrowiecki"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML",
      "68T07, 68T42, 68P27, 68P25",
      "I.2.6; I.2.11"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.13692v1",
    "title" : "Local Privacy-preserving Mechanisms and Applications in Machine Learning",
    "summary" : "The emergence and evolution of Local Differential Privacy (LDP) and its\nvarious adaptations play a pivotal role in tackling privacy issues related to\nthe vast amounts of data generated by intelligent devices, which are crucial\nfor data-informed decision-making in the realm of crowdsensing. Utilizing these\nextensive datasets can provide critical insights but also introduces\nsubstantial privacy concerns for the individuals involved. LDP, noted for its\ndecentralized framework, excels in providing strong privacy protection for\nindividual users during the stages of data collection and processing. The core\nprinciple of LDP lies in its technique of altering each user's data locally at\nthe client end before it is sent to the server, thus preventing privacy\nviolations at both stages. There are many LDP variances in the privacy research\ncommunity aimed to improve the utility-privacy tradeoff. On the other hand, one\nof the major applications of the privacy-preserving mechanisms is machine\nlearning. In this paper, we firstly delves into a comprehensive analysis of LDP\nand its variances, focusing on their various models, the diverse range of its\nadaptations, and the underlying structure of privacy mechanisms; then we\ndiscuss the state-of-art privacy mechanisms applications in machine learning.",
    "updated" : "2024-01-08T22:29:00Z",
    "published" : "2024-01-08T22:29:00Z",
    "authors" : [
      {
        "name" : "Likun Qin"
      },
      {
        "name" : "Tianshuo Qiu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.14884v1",
    "title" : "P3LS: Partial Least Squares under Privacy Preservation",
    "summary" : "Modern manufacturing value chains require intelligent orchestration of\nprocesses across company borders in order to maximize profits while fostering\nsocial and environmental sustainability. However, the implementation of\nintegrated, systems-level approaches for data-informed decision-making along\nvalue chains is currently hampered by privacy concerns associated with\ncross-organizational data exchange and integration. We here propose\nPrivacy-Preserving Partial Least Squares (P3LS) regression, a novel federated\nlearning technique that enables cross-organizational data integration and\nprocess modeling with privacy guarantees. P3LS involves a singular value\ndecomposition (SVD) based PLS algorithm and employs removable, random masks\ngenerated by a trusted authority in order to protect the privacy of the data\ncontributed by each data holder. We demonstrate the capability of P3LS to\nvertically integrate process data along a hypothetical value chain consisting\nof three parties and to improve the prediction performance on several\nprocess-related key performance indicators. Furthermore, we show the numerical\nequivalence of P3LS and PLS model components on simulated data and provide a\nthorough privacy analysis of the former. Moreover, we propose a mechanism for\ndetermining the relevance of the contributed data to the problem being\naddressed, thus creating a basis for quantifying the contribution of\nparticipants.",
    "updated" : "2024-01-26T14:08:43Z",
    "published" : "2024-01-26T14:08:43Z",
    "authors" : [
      {
        "name" : "Du Nguyen Duy"
      },
      {
        "name" : "Ramin Nikzad-Langerodi"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.14840v1",
    "title" : "GuardML: Efficient Privacy-Preserving Machine Learning Services Through\n  Hybrid Homomorphic Encryption",
    "summary" : "Machine Learning (ML) has emerged as one of data science's most\ntransformative and influential domains. However, the widespread adoption of ML\nintroduces privacy-related concerns owing to the increasing number of malicious\nattacks targeting ML models. To address these concerns, Privacy-Preserving\nMachine Learning (PPML) methods have been introduced to safeguard the privacy\nand security of ML models. One such approach is the use of Homomorphic\nEncryption (HE). However, the significant drawbacks and inefficiencies of\ntraditional HE render it impractical for highly scalable scenarios.\nFortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption\n(HHE), has recently emerged, combining the strengths of symmetric cryptography\nand HE to surmount these challenges. Our work seeks to introduce HHE to ML by\ndesigning a PPML scheme tailored for end devices. We leverage HHE as the\nfundamental building block to enable secure learning of classification outcomes\nover encrypted data, all while preserving the privacy of the input data and ML\nmodel. We demonstrate the real-world applicability of our construction by\ndeveloping and evaluating an HHE-based PPML application for classifying heart\ndisease based on sensitive ECG data. Notably, our evaluations revealed a slight\nreduction in accuracy compared to inference on plaintext data. Additionally,\nboth the analyst and end devices experience minimal communication and\ncomputation costs, underscoring the practical viability of our approach. The\nsuccessful integration of HHE into PPML provides a glimpse into a more secure\nand privacy-conscious future for machine learning on relatively constrained end\ndevices.",
    "updated" : "2024-01-26T13:12:52Z",
    "published" : "2024-01-26T13:12:52Z",
    "authors" : [
      {
        "name" : "Eugene Frimpong"
      },
      {
        "name" : "Khoa Nguyen"
      },
      {
        "name" : "Mindaugas Budzys"
      },
      {
        "name" : "Tanveer Khan"
      },
      {
        "name" : "Antonis Michalas"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.14792v1",
    "title" : "Deep Variational Privacy Funnel: General Modeling with Applications in\n  Face Recognition",
    "summary" : "In this study, we harness the information-theoretic Privacy Funnel (PF) model\nto develop a method for privacy-preserving representation learning using an\nend-to-end training framework. We rigorously address the trade-off between\nobfuscation and utility. Both are quantified through the logarithmic loss, a\nmeasure also recognized as self-information loss. This exploration deepens the\ninterplay between information-theoretic privacy and representation learning,\noffering substantive insights into data protection mechanisms for both\ndiscriminative and generative models. Importantly, we apply our model to\nstate-of-the-art face recognition systems. The model demonstrates adaptability\nacross diverse inputs, from raw facial images to both derived or refined\nembeddings, and is competent in tasks such as classification, reconstruction,\nand generation.",
    "updated" : "2024-01-26T11:32:53Z",
    "published" : "2024-01-26T11:32:53Z",
    "authors" : [
      {
        "name" : "Behrooz Razeghi"
      },
      {
        "name" : "Parsa Rahimi"
      },
      {
        "name" : "Sébastien Marcel"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.14549v1",
    "title" : "Privacy-preserving Quantile Treatment Effect Estimation for Randomized\n  Controlled Trials",
    "summary" : "In accordance with the principle of \"data minimization\", many internet\ncompanies are opting to record less data. However, this is often at odds with\nA/B testing efficacy. For experiments with units with multiple observations,\none popular data minimizing technique is to aggregate data for each unit.\nHowever, exact quantile estimation requires the full observation-level data. In\nthis paper, we develop a method for approximate Quantile Treatment Effect (QTE)\nanalysis using histogram aggregation. In addition, we can also achieve formal\nprivacy guarantees using differential privacy.",
    "updated" : "2024-01-25T22:35:33Z",
    "published" : "2024-01-25T22:35:33Z",
    "authors" : [
      {
        "name" : "Leon Yao"
      },
      {
        "name" : "Paul Yiming Li"
      },
      {
        "name" : "Jiannan Lu"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.14436v1",
    "title" : "Trust model of privacy-concerned, emotionally-aware agents in a\n  cooperative logistics problem",
    "summary" : "In this paper we propose a trust model to be used into a hypothetical mixed\nenvironment where humans and unmanned vehicles cooperate. We address the\ninclusion of emotions inside a trust model in a coherent way to the practical\napproaches to the current psychology theories. The most innovative contribution\nis how privacy issues play a role in the cooperation decisions of the emotional\ntrust model. Both, emotions and trust have been cognitively modeled and managed\nwith the Beliefs, Desires and Intentions (BDI) paradigm into autonomous agents\nimplemented in GAML (the programming language of GAMA agent platform) that\ncommunicates using the IEEE FIPA standard. The trusting behaviour of these\nemotional agents is tested in a cooperative logistics problem where: agents\nhave to move objects to destinations and some of the objects and places have\nprivacy issues. The execution of simulations of this logistic problem shows how\nemotions and trust contribute to improve the performance of agents in terms of\nboth, time savings and privacy protection",
    "updated" : "2024-01-25T13:31:43Z",
    "published" : "2024-01-25T13:31:43Z",
    "authors" : [
      {
        "name" : "J. Carbo"
      },
      {
        "name" : "J. M. Molina"
      }
    ],
    "categories" : [
      "cs.MA",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.16251v1",
    "title" : "Cross-silo Federated Learning with Record-level Personalized\n  Differential Privacy",
    "summary" : "Federated learning enhanced by differential privacy has emerged as a popular\napproach to better safeguard the privacy of client-side data by protecting\nclients' contributions during the training process. Existing solutions\ntypically assume a uniform privacy budget for all records and provide\none-size-fits-all solutions that may not be adequate to meet each record's\nprivacy requirement. In this paper, we explore the uncharted territory of\ncross-silo FL with record-level personalized differential privacy. We devise a\nnovel framework named rPDP-FL, employing a two-stage hybrid sampling scheme\nwith both client-level sampling and non-uniform record-level sampling to\naccommodate varying privacy requirements. A critical and non-trivial problem is\nto select the ideal per-record sampling probability q given the personalized\nprivacy budget {\\epsilon}. We introduce a versatile solution named\nSimulation-CurveFitting, allowing us to uncover a significant insight into the\nnonlinear correlation between q and {\\epsilon} and derive an elegant\nmathematical model to tackle the problem. Our evaluation demonstrates that our\nsolution can provide significant performance gains over the baselines that do\nnot consider personalized privacy preservation.",
    "updated" : "2024-01-29T16:01:46Z",
    "published" : "2024-01-29T16:01:46Z",
    "authors" : [
      {
        "name" : "Junxu Liu"
      },
      {
        "name" : "Jian Lou"
      },
      {
        "name" : "Li Xiong"
      },
      {
        "name" : "Jinfei Liu"
      },
      {
        "name" : "Xiaofeng Meng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.16170v1",
    "title" : "A Privacy-preserving key transmission protocol to distribute QRNG keys\n  using zk-SNARKs",
    "summary" : "High-entropy random numbers are an essential part of cryptography, and\nQuantum Random Number Generators (QRNG) are an emergent technology that can\nprovide high-quality keys for cryptographic algorithms but unfortunately are\ncurrently difficult to access. Existing Entropy-as-a-Service solutions require\nusers to trust the central authority distributing the key material, which is\nnot desirable in a high-privacy environment. In this paper, we present a novel\nkey transmission protocol that allows users to obtain cryptographic material\ngenerated by a QRNG in such a way that the server is unable to identify which\nuser is receiving each key. This is achieved with the inclusion of Zero\nKnowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARK), a\ncryptographic primitive that allow users to prove knowledge of some value\nwithout needing to reveal it. The security analysis of the protocol proves that\nit satisfies the properties of Anonymity, Unforgeability and Confidentiality,\nas defined in this document. We also provide an implementation of the protocol\ndemonstrating its functionality and performance, using NFC as the transmission\nchannel for the QRNG key.",
    "updated" : "2024-01-29T14:00:37Z",
    "published" : "2024-01-29T14:00:37Z",
    "authors" : [
      {
        "name" : "David Soler"
      },
      {
        "name" : "Carlos Dafonte"
      },
      {
        "name" : "Manuel Fernández-Veiga"
      },
      {
        "name" : "Ana Fernández Vilas"
      },
      {
        "name" : "Francisco J. Nóvoa"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.16094v1",
    "title" : "Federated unsupervised random forest for privacy-preserving patient\n  stratification",
    "summary" : "In the realm of precision medicine, effective patient stratification and\ndisease subtyping demand innovative methodologies tailored for multi-omics\ndata. Clustering techniques applied to multi-omics data have become\ninstrumental in identifying distinct subgroups of patients, enabling a\nfiner-grained understanding of disease variability. This work establishes a\npowerful framework for advancing precision medicine through unsupervised\nrandom-forest-based clustering and federated computing. We introduce a novel\nmulti-omics clustering approach utilizing unsupervised random-forests. The\nunsupervised nature of the random forest enables the determination of\ncluster-specific feature importance, unraveling key molecular contributors to\ndistinct patient groups. Moreover, our methodology is designed for federated\nexecution, a crucial aspect in the medical domain where privacy concerns are\nparamount. We have validated our approach on machine learning benchmark data\nsets as well as on cancer data from The Cancer Genome Atlas (TCGA). Our method\nis competitive with the state-of-the-art in terms of disease subtyping, but at\nthe same time substantially improves the cluster interpretability. Experiments\nindicate that local clustering performance can be improved through federated\ncomputing.",
    "updated" : "2024-01-29T12:04:14Z",
    "published" : "2024-01-29T12:04:14Z",
    "authors" : [
      {
        "name" : "Bastian Pfeifer"
      },
      {
        "name" : "Christel Sirocchi"
      },
      {
        "name" : "Marcus D. Bloice"
      },
      {
        "name" : "Markus Kreuzthaler"
      },
      {
        "name" : "Martin Urschler"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "q-bio.QM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.15906v1",
    "title" : "Mean Estimation with User-Level Privacy for Spatio-Temporal IoT Datasets",
    "summary" : "This paper considers the problem of the private release of sample means of\nspeed values from traffic datasets. Our key contribution is the development of\nuser-level differentially private algorithms that incorporate carefully chosen\nparameter values to ensure low estimation errors on real-world datasets, while\nensuring privacy. We test our algorithms on ITMS (Intelligent Traffic\nManagement System) data from an Indian city, where the speeds of different\nbuses are drawn in a potentially non-i.i.d. manner from an unknown\ndistribution, and where the number of speed samples contributed by different\nbuses is potentially different. We then apply our algorithms to a synthetic\ndataset, generated based on the ITMS data, having either a large number of\nusers or a large number of samples per user. Here, we provide recommendations\nfor the choices of parameters and algorithm subroutines that result in low\nestimation errors, while guaranteeing user-level privacy.",
    "updated" : "2024-01-29T06:21:29Z",
    "published" : "2024-01-29T06:21:29Z",
    "authors" : [
      {
        "name" : "V. Arvind Rameshwar"
      },
      {
        "name" : "Anshoo Tandon"
      },
      {
        "name" : "Prajjwal Gupta"
      },
      {
        "name" : "Novoneel Chakraborty"
      },
      {
        "name" : "Abhay Sharma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT",
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.15774v1",
    "title" : "Integrating Differential Privacy and Contextual Integrity",
    "summary" : "In this work, we propose the first framework for integrating Differential\nPrivacy (DP) and Contextual Integrity (CI). DP is a property of an algorithm\nthat injects statistical noise to obscure information about individuals\nrepresented within a database. CI defines privacy as information flow that is\nappropriate to social context. Analyzed together, these paradigms outline two\ndimensions on which to analyze privacy of information flows: descriptive and\nnormative properties. We show that our new integrated framework provides\nbenefits to both CI and DP that cannot be attained when each definition is\nconsidered in isolation: it enables contextually-guided tuning of the epsilon\nparameter in DP, and it enables CI to be applied to a broader set of\ninformation flows occurring in real-world systems, such as those involving PETs\nand machine learning. We conclude with a case study based on the use of DP in\nthe U.S. Census Bureau.",
    "updated" : "2024-01-28T21:28:07Z",
    "published" : "2024-01-28T21:28:07Z",
    "authors" : [
      {
        "name" : "Sebastian Benthall"
      },
      {
        "name" : "Rachel Cummings"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.15561v1",
    "title" : "A Parameter Privacy-Preserving Strategy for Mixed-Autonomy Platoon\n  Control",
    "summary" : "It has been demonstrated that leading cruise control (LCC) can improve the\noperation of mixed-autonomy platoons by allowing connected and automated\nvehicles (CAVs) to make longitudinal control decisions based on the information\nprovided by surrounding vehicles. However, LCC generally requires surrounding\nhuman-driven vehicles (HDVs) to share their real-time states, which can be used\nby adversaries to infer drivers' car-following behavior, potentially leading to\nfinancial losses or safety concerns. This paper aims to address such privacy\nconcerns and protect the behavioral characteristics of HDVs by devising a\nparameter privacy-preserving approach for mixed-autonomy platoon control.\nFirst, we integrate a parameter privacy filter into LCC to protect sensitive\ncar-following parameters. The privacy filter allows each vehicle to generate\nseemingly realistic pseudo states by distorting the true parameters to pseudo\nparameters, which can protect drivers' privacy in behavioral parameters without\nsignificantly influencing the control performance. Second, to enhance the\npracticality and reliability of the privacy filter within LCC, we first extend\nthe current approach to accommodate continuous parameter spaces through a\nneural network estimator. Subsequently, we introduce an individual-level\nparameter privacy preservation constraint, focusing on the privacy level of\neach individual parameter pair, further enhancing the approach's reliability.\nThird, analysis of head-to-tail string stability reveals the potential impact\nof privacy filters in degrading mixed traffic flow performance. Simulation\nshows that this approach can effectively trade off privacy and control\nperformance in LCC. We further demonstrate the benefit of such an approach in\nnetworked systems, i.e., by applying the privacy filter to a proceeding\nvehicle, one can also achieve a certain level of privacy for the following\nvehicle.",
    "updated" : "2024-01-28T04:00:00Z",
    "published" : "2024-01-28T04:00:00Z",
    "authors" : [
      {
        "name" : "Jingyuan Zhou"
      },
      {
        "name" : "Kaidi Yang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.15491v1",
    "title" : "General Inferential Limits Under Differential and Pufferfish Privacy",
    "summary" : "Differential privacy (DP) is a class of mathematical standards for assessing\nthe privacy provided by a data-release mechanism. This work concerns two\nimportant flavors of DP that are related yet conceptually distinct: pure\n$\\epsilon$-differential privacy ($\\epsilon$-DP) and Pufferfish privacy. We\nrestate $\\epsilon$-DP and Pufferfish privacy as Lipschitz continuity conditions\nand provide their formulations in terms of an object from the imprecise\nprobability literature: the interval of measures. We use these formulations to\nderive limits on key quantities in frequentist hypothesis testing and in\nBayesian inference using data that are sanitised according to either of these\ntwo privacy standards. Under very mild conditions, the results in this work are\nvalid for arbitrary parameters, priors and data generating models. These bounds\nare weaker than those attainable when analysing specific data generating models\nor data-release mechanisms. However, they provide generally applicable limits\non the ability to learn from differentially private data - even when the\nanalyst's knowledge of the model or mechanism is limited. They also shed light\non the semantic interpretations of the two DP flavors under examination, a\nsubject of contention in the current literature.",
    "updated" : "2024-01-27T19:44:46Z",
    "published" : "2024-01-27T19:44:46Z",
    "authors" : [
      {
        "name" : "James Bailie"
      },
      {
        "name" : "Ruobin Gong"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.15369v1",
    "title" : "Privacy-Preserving Cross-Domain Sequential Recommendation",
    "summary" : "Cross-domain sequential recommendation is an important development direction\nof recommender systems. It combines the characteristics of sequential\nrecommender systems and cross-domain recommender systems, which can capture the\ndynamic preferences of users and alleviate the problem of cold-start users.\nHowever, in recent years, people pay more and more attention to their privacy.\nThey do not want other people to know what they just bought, what videos they\njust watched, and where they just came from. How to protect the users' privacy\nhas become an urgent problem to be solved. In this paper, we propose a novel\nprivacy-preserving cross-domain sequential recommender system (PriCDSR), which\ncan provide users with recommendation services while preserving their privacy\nat the same time. Specifically, we define a new differential privacy on the\ndata, taking into account both the ID information and the order information.\nThen, we design a random mechanism that satisfies this differential privacy and\nprovide its theoretical proof. Our PriCDSR is a non-invasive method that can\nadopt any cross-domain sequential recommender system as a base model without\nany modification to it. To the best of our knowledge, our PriCDSR is the first\nwork to investigate privacy issues in cross-domain sequential recommender\nsystems. We conduct experiments on three domains, and the results demonstrate\nthat our PriCDSR, despite introducing noise, still outperforms recommender\nsystems that only use data from a single domain.",
    "updated" : "2024-01-27T10:14:21Z",
    "published" : "2024-01-27T10:14:21Z",
    "authors" : [
      {
        "name" : "Zhaohao Lin"
      },
      {
        "name" : "Weike Pan"
      },
      {
        "name" : "Zhong Ming"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.15221v1",
    "title" : "Designing and Testing a Mobile Application for Collecting WhatsApp Chat\n  Data While Preserving Privacy",
    "summary" : "It is common practice for researchers to join public WhatsApp chats and\nscrape their contents for analysis. However, research shows collecting data\nthis way contradicts user expectations and preferences, even if the data is\neffectively public. To overcome these issues, we outline design considerations\nfor collecting WhatsApp chat data with improved user privacy by heightening\nuser control and oversight of data collection and taking care to minimize the\ndata researchers collect and process off a user's device. We refer to these\ndesign principles as User-Centered Data Sharing (UCDS). To evaluate our UCDS\nprinciples, we implemented a mobile application representing one possible\ninstance of these improved data collection techniques and evaluated the\nviability of using the app to collect WhatsApp chat data. Second, we surveyed\nWhatsApp users to gather user perceptions on common existing WhatsApp data\ncollection methods as well as UCDS methods. Our results show that we were able\nto glean similar informative insights into WhatsApp chats using UCDS principles\nin our prototype app to common, less privacy-preserving methods. Our survey\nshowed that methods following the UCDS principles are preferred by users\nbecause they offered users more control over the data collection process.\nFuture user studies could further expand upon UCDS principles to overcome\ncomplications of researcher-to-group communication in research on WhatsApp\nchats and evaluate these principles in other data sharing contexts.",
    "updated" : "2024-01-26T22:18:28Z",
    "published" : "2024-01-26T22:18:28Z",
    "authors" : [
      {
        "name" : "Brennan Schaffner"
      },
      {
        "name" : "Archie Brohn"
      },
      {
        "name" : "Jason Chee"
      },
      {
        "name" : "K. J. Feng"
      },
      {
        "name" : "Marshini Chetty"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.17127v1",
    "title" : "Personalized Differential Privacy for Ridge Regression",
    "summary" : "The increased application of machine learning (ML) in sensitive domains\nrequires protecting the training data through privacy frameworks, such as\ndifferential privacy (DP). DP requires to specify a uniform privacy level\n$\\varepsilon$ that expresses the maximum privacy loss that each data point in\nthe entire dataset is willing to tolerate. Yet, in practice, different data\npoints often have different privacy requirements. Having to set one uniform\nprivacy level is usually too restrictive, often forcing a learner to guarantee\nthe stringent privacy requirement, at a large cost to accuracy. To overcome\nthis limitation, we introduce our novel Personalized-DP Output Perturbation\nmethod (PDP-OP) that enables to train Ridge regression models with individual\nper data point privacy levels. We provide rigorous privacy proofs for our\nPDP-OP as well as accuracy guarantees for the resulting model. This work is the\nfirst to provide such theoretical accuracy guarantees when it comes to\npersonalized DP in machine learning, whereas previous work only provided\nempirical evaluations. We empirically evaluate PDP-OP on synthetic and real\ndatasets and with diverse privacy distributions. We show that by enabling each\ndata point to specify their own privacy requirement, we can significantly\nimprove the privacy-accuracy trade-offs in DP. We also show that PDP-OP\noutperforms the personalized privacy techniques of Jorgensen et al. (2015).",
    "updated" : "2024-01-30T16:00:14Z",
    "published" : "2024-01-30T16:00:14Z",
    "authors" : [
      {
        "name" : "Krishna Acharya"
      },
      {
        "name" : "Franziska Boenisch"
      },
      {
        "name" : "Rakshit Naidu"
      },
      {
        "name" : "Juba Ziani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.16596v1",
    "title" : "PrIsing: Privacy-Preserving Peer Effect Estimation via Ising Model",
    "summary" : "The Ising model, originally developed as a spin-glass model for ferromagnetic\nelements, has gained popularity as a network-based model for capturing\ndependencies in agents' outputs. Its increasing adoption in healthcare and the\nsocial sciences has raised privacy concerns regarding the confidentiality of\nagents' responses. In this paper, we present a novel\n$(\\varepsilon,\\delta)$-differentially private algorithm specifically designed\nto protect the privacy of individual agents' outcomes. Our algorithm allows for\nprecise estimation of the natural parameter using a single network through an\nobjective perturbation technique. Furthermore, we establish regret bounds for\nthis algorithm and assess its performance on synthetic datasets and two\nreal-world networks: one involving HIV status in a social network and the other\nconcerning the political leaning of online blogs.",
    "updated" : "2024-01-29T21:56:39Z",
    "published" : "2024-01-29T21:56:39Z",
    "authors" : [
      {
        "name" : "Abhinav Chakraborty"
      },
      {
        "name" : "Anirban Chatterjee"
      },
      {
        "name" : "Abhinandan Dalal"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR",
      "cs.SI",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.16251v2",
    "title" : "Cross-silo Federated Learning with Record-level Personalized\n  Differential Privacy",
    "summary" : "Federated learning enhanced by differential privacy has emerged as a popular\napproach to better safeguard the privacy of client-side data by protecting\nclients' contributions during the training process. Existing solutions\ntypically assume a uniform privacy budget for all records and provide\none-size-fits-all solutions that may not be adequate to meet each record's\nprivacy requirement. In this paper, we explore the uncharted territory of\ncross-silo FL with record-level personalized differential privacy. We devise a\nnovel framework named rPDP-FL, employing a two-stage hybrid sampling scheme\nwith both client-level sampling and non-uniform record-level sampling to\naccommodate varying privacy requirements. A critical and non-trivial problem is\nto select the ideal per-record sampling probability q given the personalized\nprivacy budget {\\epsilon}. We introduce a versatile solution named\nSimulation-CurveFitting, allowing us to uncover a significant insight into the\nnonlinear correlation between q and {\\epsilon} and derive an elegant\nmathematical model to tackle the problem. Our evaluation demonstrates that our\nsolution can provide significant performance gains over the baselines that do\nnot consider personalized privacy preservation.",
    "updated" : "2024-01-30T04:57:20Z",
    "published" : "2024-01-29T16:01:46Z",
    "authors" : [
      {
        "name" : "Junxu Liu"
      },
      {
        "name" : "Jian Lou"
      },
      {
        "name" : "Li Xiong"
      },
      {
        "name" : "Jinfei Liu"
      },
      {
        "name" : "Xiaofeng Meng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.17829v1",
    "title" : "Evolving privacy: drift parameter estimation for discretely observed\n  i.i.d. diffusion processes under LDP",
    "summary" : "The problem of estimating a parameter in the drift coefficient is addressed\nfor $N$ discretely observed independent and identically distributed stochastic\ndifferential equations (SDEs). This is done considering additional constraints,\nwherein only public data can be published and used for inference. The concept\nof local differential privacy (LDP) is formally introduced for a system of\nstochastic differential equations. The objective is to estimate the drift\nparameter by proposing a contrast function based on a pseudo-likelihood\napproach. A suitably scaled Laplace noise is incorporated to meet the privacy\nrequirements. Our key findings encompass the derivation of explicit conditions\ntied to the privacy level. Under these conditions, we establish the consistency\nand asymptotic normality of the associated estimator. Notably, the convergence\nrate is intricately linked to the privacy level, and is some situations may be\ncompletely different from the case where privacy constraints are ignored. Our\nresults hold true as the discretization step approaches zero and the number of\nprocesses $N$ tends to infinity.",
    "updated" : "2024-01-31T13:41:23Z",
    "published" : "2024-01-31T13:41:23Z",
    "authors" : [
      {
        "name" : "Chiara Amorino"
      },
      {
        "name" : "Arnaud Gloter"
      },
      {
        "name" : "Hélène Halconruy"
      }
    ],
    "categories" : [
      "math.ST",
      "cs.IT",
      "math.IT",
      "stat.TH",
      "62F12, 62E20, 62M05, 60G07, 60H10"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.17823v1",
    "title" : "Privacy-preserving data release leveraging optimal transport and\n  particle gradient descent",
    "summary" : "We present a novel approach for differentially private data synthesis of\nprotected tabular datasets, a relevant task in highly sensitive domains such as\nhealthcare and government. Current state-of-the-art methods predominantly use\nmarginal-based approaches, where a dataset is generated from private estimates\nof the marginals. In this paper, we introduce PrivPGD, a new generation method\nfor marginal-based private data synthesis, leveraging tools from optimal\ntransport and particle gradient descent. Our algorithm outperforms existing\nmethods on a large range of datasets while being highly scalable and offering\nthe flexibility to incorporate additional domain-specific constraints.",
    "updated" : "2024-01-31T13:28:07Z",
    "published" : "2024-01-31T13:28:07Z",
    "authors" : [
      {
        "name" : "Konstantin Donhauser"
      },
      {
        "name" : "Javier Abad"
      },
      {
        "name" : "Neha Hulkund"
      },
      {
        "name" : "Fanny Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.17630v1",
    "title" : "Towards Personalized Privacy: User-Governed Data Contribution for\n  Federated Recommendation",
    "summary" : "Federated recommender systems (FedRecs) have gained significant attention for\ntheir potential to protect user's privacy by keeping user privacy data locally\nand only communicating model parameters/gradients to the server. Nevertheless,\nthe currently existing architecture of FedRecs assumes that all users have the\nsame 0-privacy budget, i.e., they do not upload any data to the server, thus\noverlooking those users who are less concerned about privacy and are willing to\nupload data to get a better recommendation service. To bridge this gap, this\npaper explores a user-governed data contribution federated recommendation\narchitecture where users are free to take control of whether they share data\nand the proportion of data they share to the server. To this end, this paper\npresents a cloud-device collaborative graph neural network federated\nrecommendation model, named CDCGNNFed. It trains user-centric ego graphs\nlocally, and high-order graphs based on user-shared data in the server in a\ncollaborative manner via contrastive learning. Furthermore, a graph mending\nstrategy is utilized to predict missing links in the graph on the server, thus\nleveraging the capabilities of graph neural networks over high-order graphs.\nExtensive experiments were conducted on two public datasets, and the results\ndemonstrate the effectiveness of the proposed method.",
    "updated" : "2024-01-31T07:20:56Z",
    "published" : "2024-01-31T07:20:56Z",
    "authors" : [
      {
        "name" : "Liang Qu"
      },
      {
        "name" : "Wei Yuan"
      },
      {
        "name" : "Ruiqi Zheng"
      },
      {
        "name" : "Lizhen Cui"
      },
      {
        "name" : "Yuhui Shi"
      },
      {
        "name" : "Hongzhi Yin"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.17628v1",
    "title" : "Elephants Do Not Forget: Differential Privacy with State Continuity for\n  Privacy Budget",
    "summary" : "Current implementations of differentially-private (DP) systems either lack\nsupport to track the global privacy budget consumed on a dataset, or fail to\nfaithfully maintain the state continuity of this budget. We show that failure\nto maintain a privacy budget enables an adversary to mount replay, rollback and\nfork attacks - obtaining answers to many more queries than what a secure system\nwould allow. As a result the attacker can reconstruct secret data that DP aims\nto protect - even if DP code runs in a Trusted Execution Environment (TEE). We\npropose ElephantDP, a system that aims to provide the same guarantees as a\ntrusted curator in the global DP model would, albeit set in an untrusted\nenvironment. Our system relies on a state continuity module to provide\nprotection for the privacy budget and a TEE to faithfully execute DP code and\nupdate the budget. To provide security, our protocol makes several design\nchoices including the content of the persistent state and the order between\nbudget updates and query answers. We prove that ElephantDP provides liveness\n(i.e., the protocol can restart from a correct state and respond to queries as\nlong as the budget is not exceeded) and DP confidentiality (i.e., an attacker\nlearns about a dataset as much as it would from interacting with a trusted\ncurator). Our implementation and evaluation of the protocol use Intel SGX as a\nTEE to run the DP code and a network of TEEs to maintain state continuity.\nCompared to an insecure baseline, we observe only 1.1-2$\\times$ overheads and\nlower relative overheads for larger datasets and complex DP queries.",
    "updated" : "2024-01-31T07:08:14Z",
    "published" : "2024-01-31T07:08:14Z",
    "authors" : [
      {
        "name" : "Jiankai Jin"
      },
      {
        "name" : "Chitchanok Chuengsatiansup"
      },
      {
        "name" : "Toby Murray"
      },
      {
        "name" : "Benjamin I. P. Rubinstein"
      },
      {
        "name" : "Yuval Yarom"
      },
      {
        "name" : "Olga Ohrimenko"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.17319v1",
    "title" : "Decentralized Federated Learning: A Survey on Security and Privacy",
    "summary" : "Federated learning has been rapidly evolving and gaining popularity in recent\nyears due to its privacy-preserving features, among other advantages.\nNevertheless, the exchange of model updates and gradients in this architecture\nprovides new attack surfaces for malicious users of the network which may\njeopardize the model performance and user and data privacy. For this reason,\none of the main motivations for decentralized federated learning is to\neliminate server-related threats by removing the server from the network and\ncompensating for it through technologies such as blockchain. However, this\nadvantage comes at the cost of challenging the system with new privacy threats.\nThus, performing a thorough security analysis in this new paradigm is\nnecessary. This survey studies possible variations of threats and adversaries\nin decentralized federated learning and overviews the potential defense\nmechanisms. Trustability and verifiability of decentralized federated learning\nare also considered in this study.",
    "updated" : "2024-01-25T23:35:47Z",
    "published" : "2024-01-25T23:35:47Z",
    "authors" : [
      {
        "name" : "Ehsan Hallaji"
      },
      {
        "name" : "Roozbeh Razavi-Far"
      },
      {
        "name" : "Mehrdad Saif"
      },
      {
        "name" : "Boyu Wang"
      },
      {
        "name" : "Qiang Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.06657v2",
    "title" : "Accelerating Tactile Internet with QUIC: A Security and Privacy\n  Perspective",
    "summary" : "The Tactile Internet paradigm is set to revolutionize human society by\nenabling skill-set delivery and haptic communication over ultra-reliable,\nlow-latency networks. The emerging sixth-generation (6G) mobile communication\nsystems are envisioned to underpin this Tactile Internet ecosystem at the\nnetwork edge by providing ubiquitous global connectivity. However, apart from a\nmultitude of opportunities of the Tactile Internet, security and privacy\nchallenges emerge at the forefront. We believe that the recently standardized\nQUIC protocol, characterized by end-to-end encryption and reduced round-trip\ndelay would serve as the backbone of Tactile Internet. In this article, we\nenvision a futuristic scenario where a QUIC-enabled network uses the underlying\n6G communication infrastructure to achieve the requirements for Tactile\nInternet. Interestingly this requires a deeper investigation of a wide range of\nsecurity and privacy challenges in QUIC, that need to be mitigated for its\nadoption in Tactile Internet. Henceforth, this article reviews the existing\nsecurity and privacy attacks in QUIC and their implication on users. Followed\nby that, we discuss state-of-the-art attack mitigation strategies and\ninvestigate some of their drawbacks with possible directions for future work",
    "updated" : "2024-01-31T16:48:28Z",
    "published" : "2024-01-12T16:05:13Z",
    "authors" : [
      {
        "name" : "Jayasree Sengupta"
      },
      {
        "name" : "Debasmita Dey"
      },
      {
        "name" : "Simone Ferlin"
      },
      {
        "name" : "Nirnay Ghosh"
      },
      {
        "name" : "Vaibhav Bajpai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.00205v1",
    "title" : "Decentralised, Collaborative, and Privacy-preserving Machine Learning\n  for Multi-Hospital Data",
    "summary" : "Machine Learning (ML) has demonstrated its great potential on medical data\nanalysis. Large datasets collected from diverse sources and settings are\nessential for ML models in healthcare to achieve better accuracy and\ngeneralizability. Sharing data across different healthcare institutions is\nchallenging because of complex and varying privacy and regulatory requirements.\nHence, it is hard but crucial to allow multiple parties to collaboratively\ntrain an ML model leveraging the private datasets available at each party\nwithout the need for direct sharing of those datasets or compromising the\nprivacy of the datasets through collaboration. In this paper, we address this\nchallenge by proposing Decentralized, Collaborative, and Privacy-preserving ML\nfor Multi-Hospital Data (DeCaPH). It offers the following key benefits: (1) it\nallows different parties to collaboratively train an ML model without\ntransferring their private datasets; (2) it safeguards patient privacy by\nlimiting the potential privacy leakage arising from any contents shared across\nthe parties during the training process; and (3) it facilitates the ML model\ntraining without relying on a centralized server. We demonstrate the\ngeneralizability and power of DeCaPH on three distinct tasks using real-world\ndistributed medical datasets: patient mortality prediction using electronic\nhealth records, cell-type classification using single-cell human genomes, and\npathology identification using chest radiology images. We demonstrate that the\nML models trained with DeCaPH framework have an improved utility-privacy\ntrade-off, showing it enables the models to have good performance while\npreserving the privacy of the training data points. In addition, the ML models\ntrained with DeCaPH framework in general outperform those trained solely with\nthe private datasets from individual parties, showing that DeCaPH enhances the\nmodel generalizability.",
    "updated" : "2024-01-31T22:06:10Z",
    "published" : "2024-01-31T22:06:10Z",
    "authors" : [
      {
        "name" : "Congyu Fang"
      },
      {
        "name" : "Adam Dziedzic"
      },
      {
        "name" : "Lin Zhang"
      },
      {
        "name" : "Laura Oliva"
      },
      {
        "name" : "Amol Verma"
      },
      {
        "name" : "Fahad Razak"
      },
      {
        "name" : "Nicolas Papernot"
      },
      {
        "name" : "Bo Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08224v3",
    "title" : "Privacy Preserving Adaptive Experiment Design",
    "summary" : "Adaptive experiment is widely adopted to estimate conditional average\ntreatment effect (CATE) in clinical trials and many other scenarios. While the\nprimary goal in experiment is to maximize estimation accuracy, due to the\nimperative of social welfare, it's also crucial to provide treatment with\nsuperior outcomes to patients, which is measured by regret in contextual bandit\nframework. These two objectives often lead to contrast optimal allocation\nmechanism. Furthermore, privacy concerns arise in clinical scenarios containing\nsensitive data like patients health records. Therefore, it's essential for the\ntreatment allocation mechanism to incorporate robust privacy protection\nmeasures. In this paper, we investigate the tradeoff between loss of social\nwelfare and statistical power in contextual bandit experiment. We propose a\nmatched upper and lower bound for the multi-objective optimization problem, and\nthen adopt the concept of Pareto optimality to mathematically characterize the\noptimality condition. Furthermore, we propose differentially private algorithms\nwhich still matches the lower bound, showing that privacy is \"almost free\".\nAdditionally, we derive the asymptotic normality of the estimator, which is\nessential in statistical inference and hypothesis testing.",
    "updated" : "2024-02-01T15:02:55Z",
    "published" : "2024-01-16T09:22:12Z",
    "authors" : [
      {
        "name" : "Jiachun Li"
      },
      {
        "name" : "Kaining Shi"
      },
      {
        "name" : "David Simchi-Levi"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.00896v1",
    "title" : "Privacy and Security Implications of Cloud-Based AI Services : A Survey",
    "summary" : "This paper details the privacy and security landscape in today's cloud\necosystem and identifies that there is a gap in addressing the risks introduced\nby machine learning models. As machine learning algorithms continue to evolve\nand find applications across diverse domains, the need to categorize and\nquantify privacy and security risks becomes increasingly critical. With the\nemerging trend of AI-as-a-Service (AIaaS), machine learned AI models (or ML\nmodels) are deployed on the cloud by model providers and used by model\nconsumers. We first survey the AIaaS landscape to document the various kinds of\nliabilities that ML models, especially Deep Neural Networks pose and then\nintroduce a taxonomy to bridge this gap by holistically examining the risks\nthat creators and consumers of ML models are exposed to and their known\ndefences till date. Such a structured approach will be beneficial for ML model\nproviders to create robust solutions. Likewise, ML model consumers will find it\nvaluable to evaluate such solutions and understand the implications of their\nengagement with such services. The proposed taxonomies provide a foundational\nbasis for solutions in private, secure and robust ML, paving the way for more\ntransparent and resilient AI systems.",
    "updated" : "2024-01-31T13:30:20Z",
    "published" : "2024-01-31T13:30:20Z",
    "authors" : [
      {
        "name" : "Alka Luqman"
      },
      {
        "name" : "Riya Mahesh"
      },
      {
        "name" : "Anupam Chattopadhyay"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.00888v1",
    "title" : "Security and Privacy Challenges of Large Language Models: A Survey",
    "summary" : "Large Language Models (LLMs) have demonstrated extraordinary capabilities and\ncontributed to multiple fields, such as generating and summarizing text,\nlanguage translation, and question-answering. Nowadays, LLM is becoming a very\npopular tool in computerized language processing tasks, with the capability to\nanalyze complicated linguistic patterns and provide relevant and appropriate\nresponses depending on the context. While offering significant advantages,\nthese models are also vulnerable to security and privacy attacks, such as\njailbreaking attacks, data poisoning attacks, and Personally Identifiable\nInformation (PII) leakage attacks. This survey provides a thorough review of\nthe security and privacy challenges of LLMs for both training data and users,\nalong with the application-based risks in various domains, such as\ntransportation, education, and healthcare. We assess the extent of LLM\nvulnerabilities, investigate emerging security and privacy attacks for LLMs,\nand review the potential defense mechanisms. Additionally, the survey outlines\nexisting research gaps in this domain and highlights future research\ndirections.",
    "updated" : "2024-01-30T04:00:54Z",
    "published" : "2024-01-30T04:00:54Z",
    "authors" : [
      {
        "name" : "Badhan Chandra Das"
      },
      {
        "name" : "M. Hadi Amini"
      },
      {
        "name" : "Yanzhao Wu"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.15906v2",
    "title" : "Mean Estimation with User-Level Privacy for Spatio-Temporal IoT Datasets",
    "summary" : "This paper considers the problem of the private release of sample means of\nspeed values from traffic datasets. Our key contribution is the development of\nuser-level differentially private algorithms that incorporate carefully chosen\nparameter values to ensure low estimation errors on real-world datasets, while\nensuring privacy. We test our algorithms on ITMS (Intelligent Traffic\nManagement System) data from an Indian city, where the speeds of different\nbuses are drawn in a potentially non-i.i.d. manner from an unknown\ndistribution, and where the number of speed samples contributed by different\nbuses is potentially different. We then apply our algorithms to a synthetic\ndataset, generated based on the ITMS data, having either a large number of\nusers or a large number of samples per user. Here, we provide recommendations\nfor the choices of parameters and algorithm subroutines that result in low\nestimation errors, while guaranteeing user-level privacy.",
    "updated" : "2024-02-03T06:06:09Z",
    "published" : "2024-01-29T06:21:29Z",
    "authors" : [
      {
        "name" : "V. Arvind Rameshwar"
      },
      {
        "name" : "Anshoo Tandon"
      },
      {
        "name" : "Prajjwal Gupta"
      },
      {
        "name" : "Novoneel Chakraborty"
      },
      {
        "name" : "Abhay Sharma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT",
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.11983v1",
    "title" : "Lightweight Protection for Privacy in Offloaded Speech Understanding",
    "summary" : "Speech is a common input method for mobile embedded devices, but cloud-based\nspeech recognition systems pose privacy risks. Disentanglement-based encoders,\ndesigned to safeguard user privacy by filtering sensitive information from\nspeech signals, unfortunately require substantial memory and computational\nresources, which limits their use in less powerful devices. To overcome this,\nwe introduce a novel system, XXX, optimized for such devices. XXX is built on\nthe insight that speech understanding primarily relies on understanding the\nentire utterance's long-term dependencies, while privacy concerns are often\nlinked to short-term details. Therefore, XXX focuses on selectively masking\nthese short-term elements, preserving the quality of long-term speech\nunderstanding. The core of XXX is an innovative differential mask generator,\ngrounded in interpretable learning, which fine-tunes the masking process. We\ntested XXX on the STM32H7 microcontroller, assessing its performance in various\npotential attack scenarios. The results show that XXX maintains speech\nunderstanding accuracy and privacy at levels comparable to existing encoders,\nbut with a significant improvement in efficiency, achieving up to 53.3$\\times$\nfaster processing and a 134.1$\\times$ smaller memory footprint.",
    "updated" : "2024-01-22T14:36:01Z",
    "published" : "2024-01-22T14:36:01Z",
    "authors" : [
      {
        "name" : "Dongqi Cai"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CR",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08224v4",
    "title" : "Privacy Preserving Adaptive Experiment Design",
    "summary" : "Adaptive experiment is widely adopted to estimate conditional average\ntreatment effect (CATE) in clinical trials and many other scenarios. While the\nprimary goal in experiment is to maximize estimation accuracy, due to the\nimperative of social welfare, it's also crucial to provide treatment with\nsuperior outcomes to patients, which is measured by regret in contextual bandit\nframework. These two objectives often lead to contrast optimal allocation\nmechanism. Furthermore, privacy concerns arise in clinical scenarios containing\nsensitive data like patients health records. Therefore, it's essential for the\ntreatment allocation mechanism to incorporate robust privacy protection\nmeasures. In this paper, we investigate the tradeoff between loss of social\nwelfare and statistical power in contextual bandit experiment. We propose a\nmatched upper and lower bound for the multi-objective optimization problem, and\nthen adopt the concept of Pareto optimality to mathematically characterize the\noptimality condition. Furthermore, we propose differentially private algorithms\nwhich still matches the lower bound, showing that privacy is \"almost free\".\nAdditionally, we derive the asymptotic normality of the estimator, which is\nessential in statistical inference and hypothesis testing.",
    "updated" : "2024-02-05T08:34:57Z",
    "published" : "2024-01-16T09:22:12Z",
    "authors" : [
      {
        "name" : "Jiachun Li"
      },
      {
        "name" : "Kaining Shi"
      },
      {
        "name" : "David Simchi-Levi"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.17823v2",
    "title" : "Privacy-preserving data release leveraging optimal transport and\n  particle gradient descent",
    "summary" : "We present a novel approach for differentially private data synthesis of\nprotected tabular datasets, a relevant task in highly sensitive domains such as\nhealthcare and government. Current state-of-the-art methods predominantly use\nmarginal-based approaches, where a dataset is generated from private estimates\nof the marginals. In this paper, we introduce PrivPGD, a new generation method\nfor marginal-based private data synthesis, leveraging tools from optimal\ntransport and particle gradient descent. Our algorithm outperforms existing\nmethods on a large range of datasets while being highly scalable and offering\nthe flexibility to incorporate additional domain-specific constraints.",
    "updated" : "2024-02-12T11:58:23Z",
    "published" : "2024-01-31T13:28:07Z",
    "authors" : [
      {
        "name" : "Konstantin Donhauser"
      },
      {
        "name" : "Javier Abad"
      },
      {
        "name" : "Neha Hulkund"
      },
      {
        "name" : "Fanny Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.05126v2",
    "title" : "Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving\n  Vision Transformer",
    "summary" : "We propose a novel method for privacy-preserving deep neural networks (DNNs)\nwith the Vision Transformer (ViT). The method allows us not only to train\nmodels and test with visually protected images but to also avoid the\nperformance degradation caused from the use of encrypted images, whereas\nconventional methods cannot avoid the influence of image encryption. A domain\nadaptation method is used to efficiently fine-tune ViT with encrypted images.\nIn experiments, the method is demonstrated to outperform conventional methods\nin an image classification task on the CIFAR-10 and ImageNet datasets in terms\nof classification accuracy.",
    "updated" : "2024-02-09T09:55:46Z",
    "published" : "2024-01-10T12:46:31Z",
    "authors" : [
      {
        "name" : "Teru Nagamori"
      },
      {
        "name" : "Sayaka Shiota"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.15906v3",
    "title" : "Mean Estimation with User-Level Privacy for Spatio-Temporal IoT Datasets",
    "summary" : "This paper considers the problem of the private release of sample means of\nspeed values from traffic datasets. Our key contribution is the development of\nuser-level differentially private algorithms that incorporate carefully chosen\nparameter values to ensure low estimation errors on real-world datasets, while\nensuring privacy. We test our algorithms on ITMS (Intelligent Traffic\nManagement System) data from an Indian city, where the speeds of different\nbuses are drawn in a potentially non-i.i.d. manner from an unknown\ndistribution, and where the number of speed samples contributed by different\nbuses is potentially different. We then apply our algorithms to large synthetic\ndatasets, generated based on the ITMS data. Here, we provide theoretical\njustification for the observed performance trends, and also provide\nrecommendations for the choices of algorithm subroutines that result in low\nestimation errors. Finally, we characterize the best performance of pseudo-user\ncreation-based algorithms on worst-case datasets via a minimax approach; this\nthen gives rise to a novel procedure for the creation of pseudo-users, which\noptimizes the worst-case total estimation error.",
    "updated" : "2024-02-26T10:12:30Z",
    "published" : "2024-01-29T06:21:29Z",
    "authors" : [
      {
        "name" : "V. Arvind Rameshwar"
      },
      {
        "name" : "Anshoo Tandon"
      },
      {
        "name" : "Prajjwal Gupta"
      },
      {
        "name" : "Novoneel Chakraborty"
      },
      {
        "name" : "Abhay Sharma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT",
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.15906v4",
    "title" : "Mean Estimation with User-Level Privacy for Spatio-Temporal IoT Datasets",
    "summary" : "This paper considers the problem of the private release of sample means of\nspeed values from traffic datasets. Our key contribution is the development of\nuser-level differentially private algorithms that incorporate carefully chosen\nparameter values to ensure low estimation errors on real-world datasets, while\nensuring privacy. We test our algorithms on ITMS (Intelligent Traffic\nManagement System) data from an Indian city, where the speeds of different\nbuses are drawn in a potentially non-i.i.d. manner from an unknown\ndistribution, and where the number of speed samples contributed by different\nbuses is potentially different. We then apply our algorithms to large synthetic\ndatasets, generated based on the ITMS data. Here, we provide theoretical\njustification for the observed performance trends, and also provide\nrecommendations for the choices of algorithm subroutines that result in low\nestimation errors. Finally, we characterize the best performance of pseudo-user\ncreation-based algorithms on worst-case datasets via a minimax approach; this\nthen gives rise to a novel procedure for the creation of pseudo-users, which\noptimizes the worst-case total estimation error. The algorithms discussed in\nthe paper are readily applicable to general spatio-temporal IoT datasets for\nreleasing a differentially private mean of a desired value.",
    "updated" : "2024-02-27T16:03:06Z",
    "published" : "2024-01-29T06:21:29Z",
    "authors" : [
      {
        "name" : "V. Arvind Rameshwar"
      },
      {
        "name" : "Anshoo Tandon"
      },
      {
        "name" : "Prajjwal Gupta"
      },
      {
        "name" : "Novoneel Chakraborty"
      },
      {
        "name" : "Abhay Sharma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT",
      "stat.AP"
    ]
  }
]