[
  {
    "id" : "http://arxiv.org/abs/2401.01294v1",
    "title" : "Efficient Sparse Least Absolute Deviation Regression with Differential\n  Privacy",
    "summary" : "In recent years, privacy-preserving machine learning algorithms have\nattracted increasing attention because of their important applications in many\nscientific fields. However, in the literature, most privacy-preserving\nalgorithms demand learning objectives to be strongly convex and Lipschitz\nsmooth, which thus cannot cover a wide class of robust loss functions (e.g.,\nquantile/least absolute loss). In this work, we aim to develop a fast\nprivacy-preserving learning solution for a sparse robust regression problem.\nOur learning loss consists of a robust least absolute loss and an $\\ell_1$\nsparse penalty term. To fast solve the non-smooth loss under a given privacy\nbudget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)\nalgorithm for least absolute deviation regression. Our algorithm achieves a\nfast estimation by reformulating the sparse LAD problem as a penalized least\nsquare estimation problem and adopts a three-stage noise injection to guarantee\nthe $(\\epsilon,\\delta)$-differential privacy. We show that our algorithm can\nachieve better privacy and statistical accuracy trade-off compared with the\nstate-of-the-art privacy-preserving regression algorithms. In the end, we\nconduct experiments to verify the efficiency of our proposed FRAPPE algorithm.",
    "updated" : "2024-01-02T17:13:34Z",
    "published" : "2024-01-02T17:13:34Z",
    "authors" : [
      {
        "name" : "Weidong Liu"
      },
      {
        "name" : "Xiaojun Mao"
      },
      {
        "name" : "Xiaofei Zhang"
      },
      {
        "name" : "Xin Zhang"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "stat.ME",
      "62J07"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01204v1",
    "title" : "PPBFL: A Privacy Protected Blockchain-based Federated Learning Model",
    "summary" : "With the rapid development of machine learning and growing concerns about\ndata privacy, federated learning has become an increasingly prominent focus.\nHowever, challenges such as attacks on model parameters and the lack of\nincentive mechanisms hinder the effectiveness of federated learning. Therefore,\nwe propose a Privacy Protected Blockchain-based Federated Learning Model\n(PPBFL) to enhance the security of federated learning and promote the active\nparticipation of nodes in model training. Blockchain ensures that model\nparameters stored in the InterPlanetary File System (IPFS) remain unaltered. A\nnovel adaptive differential privacy addition algorithm is simultaneously\napplied to local and global models, preserving the privacy of local models and\npreventing a decrease in the security of the global model due to the presence\nof numerous local models in federated learning. Additionally, we introduce a\nnew mix transactions mechanism to better protect the identity privacy of local\ntraining clients. Security analysis and experimental results demonstrate that\nPPBFL outperforms baseline methods in both model performance and security.",
    "updated" : "2024-01-02T13:13:28Z",
    "published" : "2024-01-02T13:13:28Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Chunhe Xia"
      },
      {
        "name" : "Wanshuang Lin"
      },
      {
        "name" : "Tianbo Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01146v1",
    "title" : "Privacy Preserving Personal Assistant with On-Device Diarization and\n  Spoken Dialogue System for Home and Beyond",
    "summary" : "In the age of personal voice assistants, the question of privacy arises.\nThese digital companions often lack memory of past interactions, while relying\nheavily on the internet for speech processing, raising privacy concerns. Modern\nsmartphones now enable on-device speech processing, making cloud-based\nsolutions unnecessary. Personal assistants for the elderly should excel at\nmemory recall, especially in medical examinations. The e-ViTA project developed\na versatile conversational application with local processing and speaker\nrecognition. This paper highlights the importance of speaker diarization\nenriched with sensor data fusion for contextualized conversation preservation.\nThe use cases applied to the e-VITA project have shown that truly personalized\ndialogue is pivotal for individual voice assistants. Secure local processing\nand sensor data fusion ensure virtual companions meet individual user needs\nwithout compromising privacy or data security.",
    "updated" : "2024-01-02T10:56:24Z",
    "published" : "2024-01-02T10:56:24Z",
    "authors" : [
      {
        "name" : "Gérard Chollet"
      },
      {
        "name" : "Hugues Sansen"
      },
      {
        "name" : "Yannis Tevissen"
      },
      {
        "name" : "Jérôme Boudy"
      },
      {
        "name" : "Mossaab Hariz"
      },
      {
        "name" : "Christophe Lohr"
      },
      {
        "name" : "Fathy Yassa"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00973v1",
    "title" : "Facebook Report on Privacy of fNIRS data",
    "summary" : "The primary goal of this project is to develop privacy-preserving machine\nlearning model training techniques for fNIRS data. This project will build a\nlocal model in a centralized setting with both differential privacy (DP) and\ncertified robustness. It will also explore collaborative federated learning to\ntrain a shared model between multiple clients without sharing local fNIRS\ndatasets. To prevent unintentional private information leakage of such clients'\nprivate datasets, we will also implement DP in the federated learning setting.",
    "updated" : "2024-01-01T23:30:31Z",
    "published" : "2024-01-01T23:30:31Z",
    "authors" : [
      {
        "name" : "Md Imran Hossen"
      },
      {
        "name" : "Sai Venkatesh Chilukoti"
      },
      {
        "name" : "Liqun Shan"
      },
      {
        "name" : "Vijay Srinivas Tida"
      },
      {
        "name" : "Xiali Hei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "I.2.0"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00794v1",
    "title" : "Privacy-Preserving Data in IoT-based Cloud Systems: A Comprehensive\n  Survey with AI Integration",
    "summary" : "As the integration of Internet of Things devices with cloud computing\nproliferates, the paramount importance of privacy preservation comes to the\nforefront. This survey paper meticulously explores the landscape of privacy\nissues in the dynamic intersection of IoT and cloud systems. The comprehensive\nliterature review synthesizes existing research, illuminating key challenges\nand discerning emerging trends in privacy preserving techniques. The\ncategorization of diverse approaches unveils a nuanced understanding of\nencryption techniques, anonymization strategies, access control mechanisms, and\nthe burgeoning integration of artificial intelligence. Notable trends include\nthe infusion of machine learning for dynamic anonymization, homomorphic\nencryption for secure computation, and AI-driven access control systems. The\nculmination of this survey contributes a holistic view, laying the groundwork\nfor understanding the multifaceted strategies employed in securing sensitive\ndata within IoT-based cloud environments. The insights garnered from this\nsurvey provide a valuable resource for researchers, practitioners, and\npolicymakers navigating the complex terrain of privacy preservation in the\nevolving landscape of IoT and cloud computing",
    "updated" : "2024-01-01T15:48:39Z",
    "published" : "2024-01-01T15:48:39Z",
    "authors" : [
      {
        "name" : "D. Dhinakaran"
      },
      {
        "name" : "S. M. Udhaya Sankar"
      },
      {
        "name" : "D. Selvaraj"
      },
      {
        "name" : "S. Edwin Raja"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00793v1",
    "title" : "SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\n  Large Language Models",
    "summary" : "With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and are difficult to circumvent or optimize\neffectively. To address this concern, we introduce an advanced optimization\nframework called SecFormer, designed to strike an optimal balance between\nperformance and efficiency in PPI for Transformer models. By implementing\nknowledge distillation techniques, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials and Goldschmidt's method to handle\nother complex nonlinear functions within PPI, such as GeLU, LayerNorm, and\nSoftmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer\nin performance, showing improvements of $5.6\\%$ and $24.2\\%$ for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of\nefficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its\neffectiveness and speed.",
    "updated" : "2024-01-01T15:40:35Z",
    "published" : "2024-01-01T15:40:35Z",
    "authors" : [
      {
        "name" : "Jinglong Luo"
      },
      {
        "name" : "Yehong Zhang"
      },
      {
        "name" : "Jiaqi Zhang"
      },
      {
        "name" : "Xin Mu"
      },
      {
        "name" : "Hui Wang"
      },
      {
        "name" : "Yue Yu"
      },
      {
        "name" : "Zenglin Xu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01589v1",
    "title" : "The Security and Privacy of Mobile Edge Computing: An Artificial\n  Intelligence Perspective",
    "summary" : "Mobile Edge Computing (MEC) is a new computing paradigm that enables cloud\ncomputing and information technology (IT) services to be delivered at the\nnetwork's edge. By shifting the load of cloud computing to individual local\nservers, MEC helps meet the requirements of ultralow latency, localized data\nprocessing, and extends the potential of Internet of Things (IoT) for\nend-users. However, the crosscutting nature of MEC and the multidisciplinary\ncomponents necessary for its deployment have presented additional security and\nprivacy concerns. Fortunately, Artificial Intelligence (AI) algorithms can cope\nwith excessively unpredictable and complex data, which offers a distinct\nadvantage in dealing with sophisticated and developing adversaries in the\nsecurity industry. Hence, in this paper we comprehensively provide a survey of\nsecurity and privacy in MEC from the perspective of AI. On the one hand, we use\nEuropean Telecommunications Standards Institute (ETSI) MEC reference\narchitecture as our based framework while merging the Software Defined Network\n(SDN) and Network Function Virtualization (NFV) to better illustrate a\nserviceable platform of MEC. On the other hand, we focus on new security and\nprivacy issues, as well as potential solutions from the viewpoints of AI.\nFinally, we comprehensively discuss the opportunities and challenges associated\nwith applying AI to MEC security and privacy as possible future research\ndirections.",
    "updated" : "2024-01-03T07:47:22Z",
    "published" : "2024-01-03T07:47:22Z",
    "authors" : [
      {
        "name" : "Cheng Wang"
      },
      {
        "name" : "Zenghui Yuan"
      },
      {
        "name" : "Pan Zhou"
      },
      {
        "name" : "Zichuan Xu"
      },
      {
        "name" : "Ruixuan Li"
      },
      {
        "name" : "Dapeng Oliver Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01575v1",
    "title" : "Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient\n  Accumulation",
    "summary" : "The blooming of social media and face recognition (FR) systems has increased\npeople's concern about privacy and security. A new type of adversarial privacy\ncloak (class-universal) can be applied to all the images of regular users, to\nprevent malicious FR systems from acquiring their identity information. In this\nwork, we discover the optimization dilemma in the existing methods -- the local\noptima problem in large-batch optimization and the gradient information\nelimination problem in small-batch optimization. To solve these problems, we\npropose Gradient Accumulation (GA) to aggregate multiple small-batch gradients\ninto a one-step iterative gradient to enhance the gradient stability and reduce\nthe usage of quantization operations. Experiments show that our proposed method\nachieves high performance on the Privacy-Commons dataset against black-box face\nrecognition models.",
    "updated" : "2024-01-03T07:00:32Z",
    "published" : "2024-01-03T07:00:32Z",
    "authors" : [
      {
        "name" : "Xuannan Liu"
      },
      {
        "name" : "Yaoyao Zhong"
      },
      {
        "name" : "Weihong Deng"
      },
      {
        "name" : "Hongzhi Shi"
      },
      {
        "name" : "Xingchen Cui"
      },
      {
        "name" : "Yunfeng Yin"
      },
      {
        "name" : "Dongchao Wen"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.02453v1",
    "title" : "Adaptive Differential Privacy in Federated Learning: A Priority-Based\n  Approach",
    "summary" : "Federated learning (FL) as one of the novel branches of distributed machine\nlearning (ML), develops global models through a private procedure without\ndirect access to local datasets. However, access to model updates (e.g.\ngradient updates in deep neural networks) transferred between clients and\nservers can reveal sensitive information to adversaries. Differential privacy\n(DP) offers a framework that gives a privacy guarantee by adding certain\namounts of noise to parameters. This approach, although being effective in\nterms of privacy, adversely affects model performance due to noise involvement.\nHence, it is always needed to find a balance between noise injection and the\nsacrificed accuracy. To address this challenge, we propose adaptive noise\naddition in FL which decides the value of injected noise based on features'\nrelative importance. Here, we first propose two effective methods for\nprioritizing features in deep neural network models and then perturb models'\nweights based on this information. Specifically, we try to figure out whether\nthe idea of adding more noise to less important parameters and less noise to\nmore important parameters can effectively save the model accuracy while\npreserving privacy. Our experiments confirm this statement under some\nconditions. The amount of noise injected, the proportion of parameters\ninvolved, and the number of global iterations can significantly change the\noutput. While a careful choice of parameters by considering the properties of\ndatasets can improve privacy without intense loss of accuracy, a bad choice can\nmake the model performance worse.",
    "updated" : "2024-01-04T03:01:15Z",
    "published" : "2024-01-04T03:01:15Z",
    "authors" : [
      {
        "name" : "Mahtab Talaei"
      },
      {
        "name" : "Iman Izadi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.04076v1",
    "title" : "Security and Privacy Issues in Cloud Storage",
    "summary" : "Even with the vast potential that cloud computing has, so far, it has not\nbeen adopted by the consumers with the enthusiasm and pace that it be worthy;\nthis is a very reason statement why consumers still hesitated of using cloud\ncomputing for their sensitive data and the threats that prevent the consumers\nfrom shifting to use cloud computing in general and cloud storage in\nparticular. The cloud computing inherits the traditional potential security and\nprivacy threats besides its own issues due to its unique structures. Some\nthreats related to cloud computing are the insider malicious attacks from the\nemployees that even sometime the provider unconscious about, the lack of\ntransparency of agreement between consumer and provider, data loss, traffic\nhijacking, shared technology and insecure application interface. Such threats\nneed remedies to make the consumer use its features in secure way. In this\nreview, we spot the light on the most security and privacy issues which can be\nattributed as gaps that sometimes the consumers or even the enterprises are not\naware of. We also define the parties that involve in scenario of cloud\ncomputing that also may attack the entire cloud systems. We also show the\nconsequences of these threats.",
    "updated" : "2024-01-08T18:27:57Z",
    "published" : "2024-01-08T18:27:57Z",
    "authors" : [
      {
        "name" : "Norah Asiri"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03883v1",
    "title" : "The Impact of Differential Privacy on Recommendation Accuracy and\n  Popularity Bias",
    "summary" : "Collaborative filtering-based recommender systems leverage vast amounts of\nbehavioral user data, which poses severe privacy risks. Thus, often, random\nnoise is added to the data to ensure Differential Privacy (DP). However, to\ndate, it is not well understood, in which ways this impacts personalized\nrecommendations. In this work, we study how DP impacts recommendation accuracy\nand popularity bias, when applied to the training data of state-of-the-art\nrecommendation models. Our findings are three-fold: First, we find that nearly\nall users' recommendations change when DP is applied. Second, recommendation\naccuracy drops substantially while recommended item popularity experiences a\nsharp increase, suggesting that popularity bias worsens. Third, we find that DP\nexacerbates popularity bias more severely for users who prefer unpopular items\nthan for users that prefer popular items.",
    "updated" : "2024-01-08T13:31:02Z",
    "published" : "2024-01-08T13:31:02Z",
    "authors" : [
      {
        "name" : "Peter Müllner"
      },
      {
        "name" : "Elisabeth Lex"
      },
      {
        "name" : "Markus Schedl"
      },
      {
        "name" : "Dominik Kowald"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03552v1",
    "title" : "Privacy-Preserving in Blockchain-based Federated Learning Systems",
    "summary" : "Federated Learning (FL) has recently arisen as a revolutionary approach to\ncollaborative training Machine Learning models. According to this novel\nframework, multiple participants train a global model collaboratively,\ncoordinating with a central aggregator without sharing their local data. As FL\ngains popularity in diverse domains, security, and privacy concerns arise due\nto the distributed nature of this solution. Therefore, integrating this\nstrategy with Blockchain technology has been consolidated as a preferred choice\nto ensure the privacy and security of participants.\n  This paper explores the research efforts carried out by the scientific\ncommunity to define privacy solutions in scenarios adopting Blockchain-Enabled\nFL. It comprehensively summarizes the background related to FL and Blockchain,\nevaluates existing architectures for their integration, and the primary attacks\nand possible countermeasures to guarantee privacy in this setting. Finally, it\nreviews the main application scenarios where Blockchain-Enabled FL approaches\nhave been proficiently applied. This survey can help academia and industry\npractitioners understand which theories and techniques exist to improve the\nperformance of FL through Blockchain to preserve privacy and which are the main\nchallenges and future directions in this novel and still under-explored\ncontext. We believe this work provides a novel contribution respect to the\nprevious surveys and is a valuable tool to explore the current landscape,\nunderstand perspectives, and pave the way for advancements or improvements in\nthis amalgamation of Blockchain and Federated Learning.",
    "updated" : "2024-01-07T17:23:55Z",
    "published" : "2024-01-07T17:23:55Z",
    "authors" : [
      {
        "name" : "Sameera K. M."
      },
      {
        "name" : "Serena Nicolazzo"
      },
      {
        "name" : "Marco Arazzi"
      },
      {
        "name" : "Antonino Nocera"
      },
      {
        "name" : "Rafidha Rehiman K. A."
      },
      {
        "name" : "Vinod P"
      },
      {
        "name" : "Mauro Conti"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03218v1",
    "title" : "MiniScope: Automated UI Exploration and Privacy Inconsistency Detection\n  of MiniApps via Two-phase Iterative Hybrid Analysis",
    "summary" : "The advent of MiniApps, operating within larger SuperApps, has revolutionized\nuser experiences by offering a wide range of services without the need for\nindividual app downloads. However, this convenience has raised significant\nprivacy concerns, as these MiniApps often require access to sensitive data,\npotentially leading to privacy violations. Our research addresses the critical\ngaps in the analysis of MiniApps' privacy practices, especially focusing on\nWeChat MiniApps in the Android ecosystem. Despite existing privacy regulations\nand platform guidelines, there is a lack of effective mechanisms to safeguard\nuser privacy fully. We introduce MiniScope, a novel two-phase hybrid analysis\napproach, specifically designed for the MiniApp environment. This approach\novercomes the limitations of existing static analysis techniques by\nincorporating dynamic UI exploration for complete code coverage and accurate\nprivacy practice identification. Our methodology includes modeling UI\ntransition states, resolving cross-package callback control flows, and\nautomated iterative UI exploration. This allows for a comprehensive\nunderstanding of MiniApps' privacy practices, addressing the unique challenges\nof sub-package loading and event-driven callbacks. Our empirical evaluation of\nover 120K MiniApps using MiniScope demonstrates its effectiveness in\nidentifying privacy inconsistencies. The results reveal significant issues,\nwith 5.7% of MiniApps over-collecting private data and 33.4% overclaiming data\ncollection. These findings emphasize the urgent need for more precise privacy\nmonitoring systems and highlight the responsibility of SuperApp operators to\nenforce stricter privacy measures.",
    "updated" : "2024-01-06T13:54:36Z",
    "published" : "2024-01-06T13:54:36Z",
    "authors" : [
      {
        "name" : "Shenao Wang"
      },
      {
        "name" : "Yuekang Li"
      },
      {
        "name" : "Kailong Wang"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Chao Wang"
      },
      {
        "name" : "Yanjie Zhao"
      },
      {
        "name" : "Gelei Deng"
      },
      {
        "name" : "Ling Shi"
      },
      {
        "name" : "Hui Li"
      },
      {
        "name" : "Yang Liu"
      },
      {
        "name" : "Haoyu Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01204v2",
    "title" : "PPBFL: A Privacy Protected Blockchain-based Federated Learning Model",
    "summary" : "With the rapid development of machine learning and a growing concern for data\nprivacy, federated learning has become a focal point of attention. However,\nattacks on model parameters and a lack of incentive mechanisms hinder the\neffectiveness of federated learning. Therefore, we propose A Privacy Protected\nBlockchain-based Federated Learning Model (PPBFL) to enhance the security of\nfederated learning and encourage active participation of nodes in model\ntraining. Blockchain technology ensures the integrity of model parameters\nstored in the InterPlanetary File System (IPFS), providing protection against\ntampering. Within the blockchain, we introduce a Proof of Training Work (PoTW)\nconsensus algorithm tailored for federated learning, aiming to incentive\ntraining nodes. This algorithm rewards nodes with greater computational power,\npromoting increased participation and effort in the federated learning process.\nA novel adaptive differential privacy algorithm is simultaneously applied to\nlocal and global models. This safeguards the privacy of local data at training\nclients, preventing malicious nodes from launching inference attacks.\nAdditionally, it enhances the security of the global model, preventing\npotential security degradation resulting from the combination of numerous local\nmodels. The possibility of security degradation is derived from the composition\ntheorem. By introducing reverse noise in the global model, a zero-bias estimate\nof differential privacy noise between local and global models is achieved.\nFurthermore, we propose a new mix transactions mechanism utilizing ring\nsignature technology to better protect the identity privacy of local training\nclients. Security analysis and experimental results demonstrate that PPBFL,\ncompared to baseline methods, not only exhibits superior model performance but\nalso achieves higher security.",
    "updated" : "2024-01-08T15:38:22Z",
    "published" : "2024-01-02T13:13:28Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Chunhe Xia"
      },
      {
        "name" : "Wanshuang Lin"
      },
      {
        "name" : "Tianbo Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00793v2",
    "title" : "SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\n  Large Language Models",
    "summary" : "With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and difficult to circumvent or optimize effectively. To\naddress this concern, we introduce an advanced optimization framework called\nSecFormer, to achieve fast and accurate PPI for Transformer models. By\nimplementing model design optimization, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials, Fourier series and Goldschmidt's\nmethod to handle other complex nonlinear functions within PPI, such as GeLU,\nLayerNorm, and Softmax. Our extensive experiments reveal that SecFormer\noutperforms MPCFormer in performance, showing improvements of $5.6\\%$ and\n$24.2\\%$ for BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In\nterms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, demonstrating its effectiveness\nand speed.",
    "updated" : "2024-01-06T10:05:23Z",
    "published" : "2024-01-01T15:40:35Z",
    "authors" : [
      {
        "name" : "Jinglong Luo"
      },
      {
        "name" : "Yehong Zhang"
      },
      {
        "name" : "Jiaqi Zhang"
      },
      {
        "name" : "Xin Mu"
      },
      {
        "name" : "Hui Wang"
      },
      {
        "name" : "Yue Yu"
      },
      {
        "name" : "Zenglin Xu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.04423v1",
    "title" : "Privacy-Preserving Sequential Recommendation with Collaborative\n  Confusion",
    "summary" : "Sequential recommendation has attracted a lot of attention from both academia\nand industry, however the privacy risks associated to gathering and\ntransferring users' personal interaction data are often underestimated or\nignored. Existing privacy-preserving studies are mainly applied to traditional\ncollaborative filtering or matrix factorization rather than sequential\nrecommendation. Moreover, these studies are mostly based on differential\nprivacy or federated learning, which often leads to significant performance\ndegradation, or has high requirements for communication. In this work, we\naddress privacy-preserving from a different perspective. Unlike existing\nresearch, we capture collaborative signals of neighbor interaction sequences\nand directly inject indistinguishable items into the target sequence before the\nrecommendation process begins, thereby increasing the perplexity of the target\nsequence. Even if the target interaction sequence is obtained by attackers, it\nis difficult to discern which ones are the actual user interaction records. To\nachieve this goal, we propose a CoLlaborative-cOnfusion seqUential recommenDer,\nnamely CLOUD, which incorporates a collaborative confusion mechanism to edit\nthe raw interaction sequences before conducting recommendation. Specifically,\nCLOUD first calculates the similarity between the target interaction sequence\nand other neighbor sequences to find similar sequences. Then, CLOUD considers\nthe shared representation of the target sequence and similar sequences to\ndetermine the operation to be performed: keep, delete, or insert. We design a\ncopy mechanism to make items from similar sequences have a higher probability\nto be inserted into the target sequence. Finally, the modified sequence is used\nto train the recommender and predict the next item.",
    "updated" : "2024-01-09T08:30:50Z",
    "published" : "2024-01-09T08:30:50Z",
    "authors" : [
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Yujie Lin"
      },
      {
        "name" : "Pengjie Ren"
      },
      {
        "name" : "Zhumin Chen"
      },
      {
        "name" : "Tsunenori Mine"
      },
      {
        "name" : "Jianli Zhao"
      },
      {
        "name" : "Qiang Zhao"
      },
      {
        "name" : "Moyan Zhang"
      },
      {
        "name" : "Xianye Ben"
      },
      {
        "name" : "Yujun Li"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.04306v1",
    "title" : "Renyi Differential Privacy in the Shuffle Model: Enhanced Amplification\n  Bounds",
    "summary" : "The shuffle model of Differential Privacy (DP) has gained significant\nattention in privacy-preserving data analysis due to its remarkable tradeoff\nbetween privacy and utility. It is characterized by adding a shuffling\nprocedure after each user's locally differentially private perturbation, which\nleads to a privacy amplification effect, meaning that the privacy guarantee of\na small level of noise, say $\\epsilon_0$, can be enhanced to\n$O(\\epsilon_0/\\sqrt{n})$ (the smaller, the more private) after shuffling all\n$n$ users' perturbed data. Most studies in the shuffle DP focus on proving a\ntighter privacy guarantee of privacy amplification. However, the current\nresults assume that the local privacy budget $\\epsilon_0$ is within a limited\nrange. In addition, there remains a gap between the tightest lower bound and\nthe known upper bound of the privacy amplification. In this work, we push\nforward the state-of-the-art by making the following contributions. Firstly, we\npresent the first asymptotically optimal analysis of Renyi Differential Privacy\n(RDP) in the shuffle model without constraints on $\\epsilon_0$. Secondly, we\nintroduce hypothesis testing for privacy amplification through shuffling,\noffering a distinct analysis technique and a tighter upper bound. Furthermore,\nwe propose a DP-SGD algorithm based on RDP. Experiments demonstrate that our\napproach outperforms existing methods significantly at the same privacy level.",
    "updated" : "2024-01-09T01:47:46Z",
    "published" : "2024-01-09T01:47:46Z",
    "authors" : [
      {
        "name" : "E Chen"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Yifei Ge"
      }
    ],
    "categories" : [
      "math.CO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.05126v1",
    "title" : "Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving\n  Vision Transformer",
    "summary" : "We propose a novel method for privacy-preserving deep neural networks (DNNs)\nwith the Vision Transformer (ViT). The method allows us not only to train\nmodels and test with visually protected images but to also avoid the\nperformance degradation caused from the use of encrypted images, whereas\nconventional methods cannot avoid the influence of image encryption. A domain\nadaptation method is used to efficiently fine-tune ViT with encrypted images.\nIn experiments, the method is demonstrated to outperform conventional methods\nin an image classification task on the CIFAR-10 and ImageNet datasets in terms\nof classification accuracy.",
    "updated" : "2024-01-10T12:46:31Z",
    "published" : "2024-01-10T12:46:31Z",
    "authors" : [
      {
        "name" : "Teru Nagamori"
      },
      {
        "name" : "Sayaka Shiota"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.05835v1",
    "title" : "Privacy Analysis of Affine Transformations in Cloud-based MPC:\n  Vulnerability to Side-knowledge",
    "summary" : "Search for the optimizer in computationally demanding model predictive\ncontrol (MPC) setups can be facilitated by Cloud as a service provider in\ncyber-physical systems. This advantage introduces the risk that Cloud can\nobtain unauthorized access to the privacy-sensitive parameters of the system\nand cost function. To solve this issue, i.e., preventing Cloud from accessing\nthe parameters while benefiting from Cloud computation, random affine\ntransformations provide an exact yet light weight in computation solution. This\nresearch deals with analyzing privacy preserving properties of these\ntransformations when they are adopted for MPC problems. We consider two common\nstrategies for outsourcing the optimization required in MPC problems, namely\nseparate and dense forms, and establish that random affine transformations\nutilized in these forms are vulnerable to side-knowledge from Cloud.\nSpecifically, we prove that the privacy guarantees of these methods and their\nextensions for separate form are undermined when a mild side-knowledge about\nthe problem in terms of structure of MPC cost function is available. In\naddition, while we prove that outsourcing the MPC problem in the dense form\ninherently leads to some degree of privacy for the system and cost function\nparameters, we also establish that affine transformations applied to this form\nare nevertheless prone to be undermined by a Cloud with mild side-knowledge.\nNumerical simulations confirm our results.",
    "updated" : "2024-01-11T11:08:20Z",
    "published" : "2024-01-11T11:08:20Z",
    "authors" : [
      {
        "name" : "Teimour Hosseinalizadeh"
      },
      {
        "name" : "Nils Schlüter"
      },
      {
        "name" : "Moritz Schulze Darup"
      },
      {
        "name" : "Nima Monshizadeh"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.05562v1",
    "title" : "Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated\n  Learning",
    "summary" : "Federated learning (FL) enables multiple participants to train a global\nmachine learning model without sharing their private training data.\nPeer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating\nthe server that aggregates local models from participants and then updates the\nglobal model. However, P2P FL is vulnerable to (i) honest-but-curious\nparticipants whose objective is to infer private training data of other\nparticipants, and (ii) Byzantine participants who can transmit arbitrarily\nmanipulated local models to corrupt the learning process. P2P FL schemes that\nsimultaneously guarantee Byzantine resilience and preserve privacy have been\nless studied. In this paper, we develop Brave, a protocol that ensures\nByzantine Resilience And privacy-preserving property for P2P FL in the presence\nof both types of adversaries. We show that Brave preserves privacy by\nestablishing that any honest-but-curious adversary cannot infer other\nparticipants' private data by observing their models. We further prove that\nBrave is Byzantine-resilient, which guarantees that all benign participants\nconverge to an identical model that deviates from a global model trained\nwithout Byzantine adversaries by a bounded distance. We evaluate Brave against\nthree state-of-the-art adversaries on a P2P FL for image classification tasks\non benchmark datasets CIFAR10 and MNIST. Our results show that the global model\nlearned with Brave in the presence of adversaries achieves comparable\nclassification accuracy to a global model trained in the absence of any\nadversary.",
    "updated" : "2024-01-10T22:07:40Z",
    "published" : "2024-01-10T22:07:40Z",
    "authors" : [
      {
        "name" : "Zhangchen Xu"
      },
      {
        "name" : "Fengqing Jiang"
      },
      {
        "name" : "Luyao Niu"
      },
      {
        "name" : "Jinyuan Jia"
      },
      {
        "name" : "Radha Poovendran"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.06657v1",
    "title" : "Accelerating Tactile Internet with QUIC: A Security and Privacy\n  Perspective",
    "summary" : "The Tactile Internet paradigm is set to revolutionize human society by\nenabling skill-set delivery and haptic communication over ultra-reliable,\nlow-latency networks. The emerging sixth-generation (6G) mobile communication\nsystems are envisioned to underpin this Tactile Internet ecosystem at the\nnetwork edge by providing ubiquitous global connectivity. However, apart from a\nmultitude of opportunities of the Tactile Internet, security and privacy\nchallenges emerge at the forefront. We believe that the recently standardized\nQUIC protocol, characterized by end-to-end encryption and reduced round-trip\ndelay would serve as the backbone of Tactile Internet. In this article, we\nenvision a futuristic scenario where a QUIC-enabled network uses the underlying\n6G communication infrastructure to achieve the requirements for Tactile\nInternet. Interestingly this requires a deeper investigation of a wide range of\nsecurity and privacy challenges in QUIC, that need to be mitigated for its\nadoption in Tactile Internet. Henceforth, this article reviews the existing\nsecurity and privacy attacks in QUIC and their implication on users. Followed\nby that, we discuss state-of-the-art attack mitigation strategies and\ninvestigate some of their drawbacks with possible directions for future work",
    "updated" : "2024-01-12T16:05:13Z",
    "published" : "2024-01-12T16:05:13Z",
    "authors" : [
      {
        "name" : "Jayasree Sengupta"
      },
      {
        "name" : "Debasmita Dey"
      },
      {
        "name" : "Simone Ferlin"
      },
      {
        "name" : "Nirnay Ghosh"
      },
      {
        "name" : "Vaibhav Bajpai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.06601v1",
    "title" : "A proposal to increase data utility on Global Differential Privacy data\n  based on data use predictions",
    "summary" : "This paper presents ongoing research focused on improving the utility of data\nprotected by Global Differential Privacy(DP) in the scenario of summary\nstatistics. Our approach is based on predictions on how an analyst will use\nstatistics released under DP protection, so that a developer can optimise data\nutility on further usage of the data in the privacy budget allocation. This\nnovel approach can potentially improve the utility of data without compromising\nprivacy constraints. We also propose a metric that can be used by the developer\nto optimise the budget allocation process.",
    "updated" : "2024-01-12T14:34:30Z",
    "published" : "2024-01-12T14:34:30Z",
    "authors" : [
      {
        "name" : "Henry C. Nunes"
      },
      {
        "name" : "Marlon P. da Silva"
      },
      {
        "name" : "Charles V. Neu"
      },
      {
        "name" : "Avelino F. Zorzo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  }
]