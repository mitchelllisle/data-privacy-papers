[{"id":"http://arxiv.org/abs/2401.01294v1","title":"Efficient Sparse Least Absolute Deviation Regression with Differential\n  Privacy","summary":"In recent years, privacy-preserving machine learning algorithms have\nattracted increasing attention because of their important applications in many\nscientific fields. However, in the literature, most privacy-preserving\nalgorithms demand learning objectives to be strongly convex and Lipschitz\nsmooth, which thus cannot cover a wide class of robust loss functions (e.g.,\nquantile/least absolute loss). In this work, we aim to develop a fast\nprivacy-preserving learning solution for a sparse robust regression problem.\nOur learning loss consists of a robust least absolute loss and an $\\ell_1$\nsparse penalty term. To fast solve the non-smooth loss under a given privacy\nbudget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)\nalgorithm for least absolute deviation regression. Our algorithm achieves a\nfast estimation by reformulating the sparse LAD problem as a penalized least\nsquare estimation problem and adopts a three-stage noise injection to guarantee\nthe $(\\epsilon,\\delta)$-differential privacy. We show that our algorithm can\nachieve better privacy and statistical accuracy trade-off compared with the\nstate-of-the-art privacy-preserving regression algorithms. In the end, we\nconduct experiments to verify the efficiency of our proposed FRAPPE algorithm.","updated":"2024-01-02T17:13:34Z","published":"2024-01-02T17:13:34Z","authors":[{"name":"Weidong Liu"},{"name":"Xiaojun Mao"},{"name":"Xiaofei Zhang"},{"name":"Xin Zhang"}],"categories":["stat.ML","cs.LG","stat.ME","62J07"]},{"id":"http://arxiv.org/abs/2401.01204v1","title":"PPBFL: A Privacy Protected Blockchain-based Federated Learning Model","summary":"With the rapid development of machine learning and growing concerns about\ndata privacy, federated learning has become an increasingly prominent focus.\nHowever, challenges such as attacks on model parameters and the lack of\nincentive mechanisms hinder the effectiveness of federated learning. Therefore,\nwe propose a Privacy Protected Blockchain-based Federated Learning Model\n(PPBFL) to enhance the security of federated learning and promote the active\nparticipation of nodes in model training. Blockchain ensures that model\nparameters stored in the InterPlanetary File System (IPFS) remain unaltered. A\nnovel adaptive differential privacy addition algorithm is simultaneously\napplied to local and global models, preserving the privacy of local models and\npreventing a decrease in the security of the global model due to the presence\nof numerous local models in federated learning. Additionally, we introduce a\nnew mix transactions mechanism to better protect the identity privacy of local\ntraining clients. Security analysis and experimental results demonstrate that\nPPBFL outperforms baseline methods in both model performance and security.","updated":"2024-01-02T13:13:28Z","published":"2024-01-02T13:13:28Z","authors":[{"name":"Yang Li"},{"name":"Chunhe Xia"},{"name":"Wanshuang Lin"},{"name":"Tianbo Wang"}],"categories":["cs.CR","cs.AI"]},{"id":"http://arxiv.org/abs/2401.01146v1","title":"Privacy Preserving Personal Assistant with On-Device Diarization and\n  Spoken Dialogue System for Home and Beyond","summary":"In the age of personal voice assistants, the question of privacy arises.\nThese digital companions often lack memory of past interactions, while relying\nheavily on the internet for speech processing, raising privacy concerns. Modern\nsmartphones now enable on-device speech processing, making cloud-based\nsolutions unnecessary. Personal assistants for the elderly should excel at\nmemory recall, especially in medical examinations. The e-ViTA project developed\na versatile conversational application with local processing and speaker\nrecognition. This paper highlights the importance of speaker diarization\nenriched with sensor data fusion for contextualized conversation preservation.\nThe use cases applied to the e-VITA project have shown that truly personalized\ndialogue is pivotal for individual voice assistants. Secure local processing\nand sensor data fusion ensure virtual companions meet individual user needs\nwithout compromising privacy or data security.","updated":"2024-01-02T10:56:24Z","published":"2024-01-02T10:56:24Z","authors":[{"name":"Gérard Chollet"},{"name":"Hugues Sansen"},{"name":"Yannis Tevissen"},{"name":"Jérôme Boudy"},{"name":"Mossaab Hariz"},{"name":"Christophe Lohr"},{"name":"Fathy Yassa"}],"categories":["cs.HC"]},{"id":"http://arxiv.org/abs/2401.00973v1","title":"Facebook Report on Privacy of fNIRS data","summary":"The primary goal of this project is to develop privacy-preserving machine\nlearning model training techniques for fNIRS data. This project will build a\nlocal model in a centralized setting with both differential privacy (DP) and\ncertified robustness. It will also explore collaborative federated learning to\ntrain a shared model between multiple clients without sharing local fNIRS\ndatasets. To prevent unintentional private information leakage of such clients'\nprivate datasets, we will also implement DP in the federated learning setting.","updated":"2024-01-01T23:30:31Z","published":"2024-01-01T23:30:31Z","authors":[{"name":"Md Imran Hossen"},{"name":"Sai Venkatesh Chilukoti"},{"name":"Liqun Shan"},{"name":"Vijay Srinivas Tida"},{"name":"Xiali Hei"}],"categories":["cs.LG","cs.CR","I.2.0"]},{"id":"http://arxiv.org/abs/2401.00794v1","title":"Privacy-Preserving Data in IoT-based Cloud Systems: A Comprehensive\n  Survey with AI Integration","summary":"As the integration of Internet of Things devices with cloud computing\nproliferates, the paramount importance of privacy preservation comes to the\nforefront. This survey paper meticulously explores the landscape of privacy\nissues in the dynamic intersection of IoT and cloud systems. The comprehensive\nliterature review synthesizes existing research, illuminating key challenges\nand discerning emerging trends in privacy preserving techniques. The\ncategorization of diverse approaches unveils a nuanced understanding of\nencryption techniques, anonymization strategies, access control mechanisms, and\nthe burgeoning integration of artificial intelligence. Notable trends include\nthe infusion of machine learning for dynamic anonymization, homomorphic\nencryption for secure computation, and AI-driven access control systems. The\nculmination of this survey contributes a holistic view, laying the groundwork\nfor understanding the multifaceted strategies employed in securing sensitive\ndata within IoT-based cloud environments. The insights garnered from this\nsurvey provide a valuable resource for researchers, practitioners, and\npolicymakers navigating the complex terrain of privacy preservation in the\nevolving landscape of IoT and cloud computing","updated":"2024-01-01T15:48:39Z","published":"2024-01-01T15:48:39Z","authors":[{"name":"D. Dhinakaran"},{"name":"S. M. Udhaya Sankar"},{"name":"D. Selvaraj"},{"name":"S. Edwin Raja"}],"categories":["cs.CR"]},{"id":"http://arxiv.org/abs/2401.00793v1","title":"SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\n  Large Language Models","summary":"With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and are difficult to circumvent or optimize\neffectively. To address this concern, we introduce an advanced optimization\nframework called SecFormer, designed to strike an optimal balance between\nperformance and efficiency in PPI for Transformer models. By implementing\nknowledge distillation techniques, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials and Goldschmidt's method to handle\nother complex nonlinear functions within PPI, such as GeLU, LayerNorm, and\nSoftmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer\nin performance, showing improvements of $5.6\\%$ and $24.2\\%$ for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of\nefficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its\neffectiveness and speed.","updated":"2024-01-01T15:40:35Z","published":"2024-01-01T15:40:35Z","authors":[{"name":"Jinglong Luo"},{"name":"Yehong Zhang"},{"name":"Jiaqi Zhang"},{"name":"Xin Mu"},{"name":"Hui Wang"},{"name":"Yue Yu"},{"name":"Zenglin Xu"}],"categories":["cs.LG","cs.CL","cs.CR"]}]