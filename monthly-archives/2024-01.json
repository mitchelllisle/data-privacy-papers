[
  {
    "id" : "http://arxiv.org/abs/2401.01294v1",
    "title" : "Efficient Sparse Least Absolute Deviation Regression with Differential\n  Privacy",
    "summary" : "In recent years, privacy-preserving machine learning algorithms have\nattracted increasing attention because of their important applications in many\nscientific fields. However, in the literature, most privacy-preserving\nalgorithms demand learning objectives to be strongly convex and Lipschitz\nsmooth, which thus cannot cover a wide class of robust loss functions (e.g.,\nquantile/least absolute loss). In this work, we aim to develop a fast\nprivacy-preserving learning solution for a sparse robust regression problem.\nOur learning loss consists of a robust least absolute loss and an $\\ell_1$\nsparse penalty term. To fast solve the non-smooth loss under a given privacy\nbudget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)\nalgorithm for least absolute deviation regression. Our algorithm achieves a\nfast estimation by reformulating the sparse LAD problem as a penalized least\nsquare estimation problem and adopts a three-stage noise injection to guarantee\nthe $(\\epsilon,\\delta)$-differential privacy. We show that our algorithm can\nachieve better privacy and statistical accuracy trade-off compared with the\nstate-of-the-art privacy-preserving regression algorithms. In the end, we\nconduct experiments to verify the efficiency of our proposed FRAPPE algorithm.",
    "updated" : "2024-01-02T17:13:34Z",
    "published" : "2024-01-02T17:13:34Z",
    "authors" : [
      {
        "name" : "Weidong Liu"
      },
      {
        "name" : "Xiaojun Mao"
      },
      {
        "name" : "Xiaofei Zhang"
      },
      {
        "name" : "Xin Zhang"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "stat.ME",
      "62J07"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01204v1",
    "title" : "PPBFL: A Privacy Protected Blockchain-based Federated Learning Model",
    "summary" : "With the rapid development of machine learning and growing concerns about\ndata privacy, federated learning has become an increasingly prominent focus.\nHowever, challenges such as attacks on model parameters and the lack of\nincentive mechanisms hinder the effectiveness of federated learning. Therefore,\nwe propose a Privacy Protected Blockchain-based Federated Learning Model\n(PPBFL) to enhance the security of federated learning and promote the active\nparticipation of nodes in model training. Blockchain ensures that model\nparameters stored in the InterPlanetary File System (IPFS) remain unaltered. A\nnovel adaptive differential privacy addition algorithm is simultaneously\napplied to local and global models, preserving the privacy of local models and\npreventing a decrease in the security of the global model due to the presence\nof numerous local models in federated learning. Additionally, we introduce a\nnew mix transactions mechanism to better protect the identity privacy of local\ntraining clients. Security analysis and experimental results demonstrate that\nPPBFL outperforms baseline methods in both model performance and security.",
    "updated" : "2024-01-02T13:13:28Z",
    "published" : "2024-01-02T13:13:28Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Chunhe Xia"
      },
      {
        "name" : "Wanshuang Lin"
      },
      {
        "name" : "Tianbo Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01146v1",
    "title" : "Privacy Preserving Personal Assistant with On-Device Diarization and\n  Spoken Dialogue System for Home and Beyond",
    "summary" : "In the age of personal voice assistants, the question of privacy arises.\nThese digital companions often lack memory of past interactions, while relying\nheavily on the internet for speech processing, raising privacy concerns. Modern\nsmartphones now enable on-device speech processing, making cloud-based\nsolutions unnecessary. Personal assistants for the elderly should excel at\nmemory recall, especially in medical examinations. The e-ViTA project developed\na versatile conversational application with local processing and speaker\nrecognition. This paper highlights the importance of speaker diarization\nenriched with sensor data fusion for contextualized conversation preservation.\nThe use cases applied to the e-VITA project have shown that truly personalized\ndialogue is pivotal for individual voice assistants. Secure local processing\nand sensor data fusion ensure virtual companions meet individual user needs\nwithout compromising privacy or data security.",
    "updated" : "2024-01-02T10:56:24Z",
    "published" : "2024-01-02T10:56:24Z",
    "authors" : [
      {
        "name" : "Gérard Chollet"
      },
      {
        "name" : "Hugues Sansen"
      },
      {
        "name" : "Yannis Tevissen"
      },
      {
        "name" : "Jérôme Boudy"
      },
      {
        "name" : "Mossaab Hariz"
      },
      {
        "name" : "Christophe Lohr"
      },
      {
        "name" : "Fathy Yassa"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00973v1",
    "title" : "Facebook Report on Privacy of fNIRS data",
    "summary" : "The primary goal of this project is to develop privacy-preserving machine\nlearning model training techniques for fNIRS data. This project will build a\nlocal model in a centralized setting with both differential privacy (DP) and\ncertified robustness. It will also explore collaborative federated learning to\ntrain a shared model between multiple clients without sharing local fNIRS\ndatasets. To prevent unintentional private information leakage of such clients'\nprivate datasets, we will also implement DP in the federated learning setting.",
    "updated" : "2024-01-01T23:30:31Z",
    "published" : "2024-01-01T23:30:31Z",
    "authors" : [
      {
        "name" : "Md Imran Hossen"
      },
      {
        "name" : "Sai Venkatesh Chilukoti"
      },
      {
        "name" : "Liqun Shan"
      },
      {
        "name" : "Vijay Srinivas Tida"
      },
      {
        "name" : "Xiali Hei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "I.2.0"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00794v1",
    "title" : "Privacy-Preserving Data in IoT-based Cloud Systems: A Comprehensive\n  Survey with AI Integration",
    "summary" : "As the integration of Internet of Things devices with cloud computing\nproliferates, the paramount importance of privacy preservation comes to the\nforefront. This survey paper meticulously explores the landscape of privacy\nissues in the dynamic intersection of IoT and cloud systems. The comprehensive\nliterature review synthesizes existing research, illuminating key challenges\nand discerning emerging trends in privacy preserving techniques. The\ncategorization of diverse approaches unveils a nuanced understanding of\nencryption techniques, anonymization strategies, access control mechanisms, and\nthe burgeoning integration of artificial intelligence. Notable trends include\nthe infusion of machine learning for dynamic anonymization, homomorphic\nencryption for secure computation, and AI-driven access control systems. The\nculmination of this survey contributes a holistic view, laying the groundwork\nfor understanding the multifaceted strategies employed in securing sensitive\ndata within IoT-based cloud environments. The insights garnered from this\nsurvey provide a valuable resource for researchers, practitioners, and\npolicymakers navigating the complex terrain of privacy preservation in the\nevolving landscape of IoT and cloud computing",
    "updated" : "2024-01-01T15:48:39Z",
    "published" : "2024-01-01T15:48:39Z",
    "authors" : [
      {
        "name" : "D. Dhinakaran"
      },
      {
        "name" : "S. M. Udhaya Sankar"
      },
      {
        "name" : "D. Selvaraj"
      },
      {
        "name" : "S. Edwin Raja"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00793v1",
    "title" : "SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\n  Large Language Models",
    "summary" : "With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and are difficult to circumvent or optimize\neffectively. To address this concern, we introduce an advanced optimization\nframework called SecFormer, designed to strike an optimal balance between\nperformance and efficiency in PPI for Transformer models. By implementing\nknowledge distillation techniques, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials and Goldschmidt's method to handle\nother complex nonlinear functions within PPI, such as GeLU, LayerNorm, and\nSoftmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer\nin performance, showing improvements of $5.6\\%$ and $24.2\\%$ for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of\nefficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its\neffectiveness and speed.",
    "updated" : "2024-01-01T15:40:35Z",
    "published" : "2024-01-01T15:40:35Z",
    "authors" : [
      {
        "name" : "Jinglong Luo"
      },
      {
        "name" : "Yehong Zhang"
      },
      {
        "name" : "Jiaqi Zhang"
      },
      {
        "name" : "Xin Mu"
      },
      {
        "name" : "Hui Wang"
      },
      {
        "name" : "Yue Yu"
      },
      {
        "name" : "Zenglin Xu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01589v1",
    "title" : "The Security and Privacy of Mobile Edge Computing: An Artificial\n  Intelligence Perspective",
    "summary" : "Mobile Edge Computing (MEC) is a new computing paradigm that enables cloud\ncomputing and information technology (IT) services to be delivered at the\nnetwork's edge. By shifting the load of cloud computing to individual local\nservers, MEC helps meet the requirements of ultralow latency, localized data\nprocessing, and extends the potential of Internet of Things (IoT) for\nend-users. However, the crosscutting nature of MEC and the multidisciplinary\ncomponents necessary for its deployment have presented additional security and\nprivacy concerns. Fortunately, Artificial Intelligence (AI) algorithms can cope\nwith excessively unpredictable and complex data, which offers a distinct\nadvantage in dealing with sophisticated and developing adversaries in the\nsecurity industry. Hence, in this paper we comprehensively provide a survey of\nsecurity and privacy in MEC from the perspective of AI. On the one hand, we use\nEuropean Telecommunications Standards Institute (ETSI) MEC reference\narchitecture as our based framework while merging the Software Defined Network\n(SDN) and Network Function Virtualization (NFV) to better illustrate a\nserviceable platform of MEC. On the other hand, we focus on new security and\nprivacy issues, as well as potential solutions from the viewpoints of AI.\nFinally, we comprehensively discuss the opportunities and challenges associated\nwith applying AI to MEC security and privacy as possible future research\ndirections.",
    "updated" : "2024-01-03T07:47:22Z",
    "published" : "2024-01-03T07:47:22Z",
    "authors" : [
      {
        "name" : "Cheng Wang"
      },
      {
        "name" : "Zenghui Yuan"
      },
      {
        "name" : "Pan Zhou"
      },
      {
        "name" : "Zichuan Xu"
      },
      {
        "name" : "Ruixuan Li"
      },
      {
        "name" : "Dapeng Oliver Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01575v1",
    "title" : "Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient\n  Accumulation",
    "summary" : "The blooming of social media and face recognition (FR) systems has increased\npeople's concern about privacy and security. A new type of adversarial privacy\ncloak (class-universal) can be applied to all the images of regular users, to\nprevent malicious FR systems from acquiring their identity information. In this\nwork, we discover the optimization dilemma in the existing methods -- the local\noptima problem in large-batch optimization and the gradient information\nelimination problem in small-batch optimization. To solve these problems, we\npropose Gradient Accumulation (GA) to aggregate multiple small-batch gradients\ninto a one-step iterative gradient to enhance the gradient stability and reduce\nthe usage of quantization operations. Experiments show that our proposed method\nachieves high performance on the Privacy-Commons dataset against black-box face\nrecognition models.",
    "updated" : "2024-01-03T07:00:32Z",
    "published" : "2024-01-03T07:00:32Z",
    "authors" : [
      {
        "name" : "Xuannan Liu"
      },
      {
        "name" : "Yaoyao Zhong"
      },
      {
        "name" : "Weihong Deng"
      },
      {
        "name" : "Hongzhi Shi"
      },
      {
        "name" : "Xingchen Cui"
      },
      {
        "name" : "Yunfeng Yin"
      },
      {
        "name" : "Dongchao Wen"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.02453v1",
    "title" : "Adaptive Differential Privacy in Federated Learning: A Priority-Based\n  Approach",
    "summary" : "Federated learning (FL) as one of the novel branches of distributed machine\nlearning (ML), develops global models through a private procedure without\ndirect access to local datasets. However, access to model updates (e.g.\ngradient updates in deep neural networks) transferred between clients and\nservers can reveal sensitive information to adversaries. Differential privacy\n(DP) offers a framework that gives a privacy guarantee by adding certain\namounts of noise to parameters. This approach, although being effective in\nterms of privacy, adversely affects model performance due to noise involvement.\nHence, it is always needed to find a balance between noise injection and the\nsacrificed accuracy. To address this challenge, we propose adaptive noise\naddition in FL which decides the value of injected noise based on features'\nrelative importance. Here, we first propose two effective methods for\nprioritizing features in deep neural network models and then perturb models'\nweights based on this information. Specifically, we try to figure out whether\nthe idea of adding more noise to less important parameters and less noise to\nmore important parameters can effectively save the model accuracy while\npreserving privacy. Our experiments confirm this statement under some\nconditions. The amount of noise injected, the proportion of parameters\ninvolved, and the number of global iterations can significantly change the\noutput. While a careful choice of parameters by considering the properties of\ndatasets can improve privacy without intense loss of accuracy, a bad choice can\nmake the model performance worse.",
    "updated" : "2024-01-04T03:01:15Z",
    "published" : "2024-01-04T03:01:15Z",
    "authors" : [
      {
        "name" : "Mahtab Talaei"
      },
      {
        "name" : "Iman Izadi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.04076v1",
    "title" : "Security and Privacy Issues in Cloud Storage",
    "summary" : "Even with the vast potential that cloud computing has, so far, it has not\nbeen adopted by the consumers with the enthusiasm and pace that it be worthy;\nthis is a very reason statement why consumers still hesitated of using cloud\ncomputing for their sensitive data and the threats that prevent the consumers\nfrom shifting to use cloud computing in general and cloud storage in\nparticular. The cloud computing inherits the traditional potential security and\nprivacy threats besides its own issues due to its unique structures. Some\nthreats related to cloud computing are the insider malicious attacks from the\nemployees that even sometime the provider unconscious about, the lack of\ntransparency of agreement between consumer and provider, data loss, traffic\nhijacking, shared technology and insecure application interface. Such threats\nneed remedies to make the consumer use its features in secure way. In this\nreview, we spot the light on the most security and privacy issues which can be\nattributed as gaps that sometimes the consumers or even the enterprises are not\naware of. We also define the parties that involve in scenario of cloud\ncomputing that also may attack the entire cloud systems. We also show the\nconsequences of these threats.",
    "updated" : "2024-01-08T18:27:57Z",
    "published" : "2024-01-08T18:27:57Z",
    "authors" : [
      {
        "name" : "Norah Asiri"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03883v1",
    "title" : "The Impact of Differential Privacy on Recommendation Accuracy and\n  Popularity Bias",
    "summary" : "Collaborative filtering-based recommender systems leverage vast amounts of\nbehavioral user data, which poses severe privacy risks. Thus, often, random\nnoise is added to the data to ensure Differential Privacy (DP). However, to\ndate, it is not well understood, in which ways this impacts personalized\nrecommendations. In this work, we study how DP impacts recommendation accuracy\nand popularity bias, when applied to the training data of state-of-the-art\nrecommendation models. Our findings are three-fold: First, we find that nearly\nall users' recommendations change when DP is applied. Second, recommendation\naccuracy drops substantially while recommended item popularity experiences a\nsharp increase, suggesting that popularity bias worsens. Third, we find that DP\nexacerbates popularity bias more severely for users who prefer unpopular items\nthan for users that prefer popular items.",
    "updated" : "2024-01-08T13:31:02Z",
    "published" : "2024-01-08T13:31:02Z",
    "authors" : [
      {
        "name" : "Peter Müllner"
      },
      {
        "name" : "Elisabeth Lex"
      },
      {
        "name" : "Markus Schedl"
      },
      {
        "name" : "Dominik Kowald"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03552v1",
    "title" : "Privacy-Preserving in Blockchain-based Federated Learning Systems",
    "summary" : "Federated Learning (FL) has recently arisen as a revolutionary approach to\ncollaborative training Machine Learning models. According to this novel\nframework, multiple participants train a global model collaboratively,\ncoordinating with a central aggregator without sharing their local data. As FL\ngains popularity in diverse domains, security, and privacy concerns arise due\nto the distributed nature of this solution. Therefore, integrating this\nstrategy with Blockchain technology has been consolidated as a preferred choice\nto ensure the privacy and security of participants.\n  This paper explores the research efforts carried out by the scientific\ncommunity to define privacy solutions in scenarios adopting Blockchain-Enabled\nFL. It comprehensively summarizes the background related to FL and Blockchain,\nevaluates existing architectures for their integration, and the primary attacks\nand possible countermeasures to guarantee privacy in this setting. Finally, it\nreviews the main application scenarios where Blockchain-Enabled FL approaches\nhave been proficiently applied. This survey can help academia and industry\npractitioners understand which theories and techniques exist to improve the\nperformance of FL through Blockchain to preserve privacy and which are the main\nchallenges and future directions in this novel and still under-explored\ncontext. We believe this work provides a novel contribution respect to the\nprevious surveys and is a valuable tool to explore the current landscape,\nunderstand perspectives, and pave the way for advancements or improvements in\nthis amalgamation of Blockchain and Federated Learning.",
    "updated" : "2024-01-07T17:23:55Z",
    "published" : "2024-01-07T17:23:55Z",
    "authors" : [
      {
        "name" : "Sameera K. M."
      },
      {
        "name" : "Serena Nicolazzo"
      },
      {
        "name" : "Marco Arazzi"
      },
      {
        "name" : "Antonino Nocera"
      },
      {
        "name" : "Rafidha Rehiman K. A."
      },
      {
        "name" : "Vinod P"
      },
      {
        "name" : "Mauro Conti"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03218v1",
    "title" : "MiniScope: Automated UI Exploration and Privacy Inconsistency Detection\n  of MiniApps via Two-phase Iterative Hybrid Analysis",
    "summary" : "The advent of MiniApps, operating within larger SuperApps, has revolutionized\nuser experiences by offering a wide range of services without the need for\nindividual app downloads. However, this convenience has raised significant\nprivacy concerns, as these MiniApps often require access to sensitive data,\npotentially leading to privacy violations. Our research addresses the critical\ngaps in the analysis of MiniApps' privacy practices, especially focusing on\nWeChat MiniApps in the Android ecosystem. Despite existing privacy regulations\nand platform guidelines, there is a lack of effective mechanisms to safeguard\nuser privacy fully. We introduce MiniScope, a novel two-phase hybrid analysis\napproach, specifically designed for the MiniApp environment. This approach\novercomes the limitations of existing static analysis techniques by\nincorporating dynamic UI exploration for complete code coverage and accurate\nprivacy practice identification. Our methodology includes modeling UI\ntransition states, resolving cross-package callback control flows, and\nautomated iterative UI exploration. This allows for a comprehensive\nunderstanding of MiniApps' privacy practices, addressing the unique challenges\nof sub-package loading and event-driven callbacks. Our empirical evaluation of\nover 120K MiniApps using MiniScope demonstrates its effectiveness in\nidentifying privacy inconsistencies. The results reveal significant issues,\nwith 5.7% of MiniApps over-collecting private data and 33.4% overclaiming data\ncollection. These findings emphasize the urgent need for more precise privacy\nmonitoring systems and highlight the responsibility of SuperApp operators to\nenforce stricter privacy measures.",
    "updated" : "2024-01-06T13:54:36Z",
    "published" : "2024-01-06T13:54:36Z",
    "authors" : [
      {
        "name" : "Shenao Wang"
      },
      {
        "name" : "Yuekang Li"
      },
      {
        "name" : "Kailong Wang"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Chao Wang"
      },
      {
        "name" : "Yanjie Zhao"
      },
      {
        "name" : "Gelei Deng"
      },
      {
        "name" : "Ling Shi"
      },
      {
        "name" : "Hui Li"
      },
      {
        "name" : "Yang Liu"
      },
      {
        "name" : "Haoyu Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01204v2",
    "title" : "PPBFL: A Privacy Protected Blockchain-based Federated Learning Model",
    "summary" : "With the rapid development of machine learning and a growing concern for data\nprivacy, federated learning has become a focal point of attention. However,\nattacks on model parameters and a lack of incentive mechanisms hinder the\neffectiveness of federated learning. Therefore, we propose A Privacy Protected\nBlockchain-based Federated Learning Model (PPBFL) to enhance the security of\nfederated learning and encourage active participation of nodes in model\ntraining. Blockchain technology ensures the integrity of model parameters\nstored in the InterPlanetary File System (IPFS), providing protection against\ntampering. Within the blockchain, we introduce a Proof of Training Work (PoTW)\nconsensus algorithm tailored for federated learning, aiming to incentive\ntraining nodes. This algorithm rewards nodes with greater computational power,\npromoting increased participation and effort in the federated learning process.\nA novel adaptive differential privacy algorithm is simultaneously applied to\nlocal and global models. This safeguards the privacy of local data at training\nclients, preventing malicious nodes from launching inference attacks.\nAdditionally, it enhances the security of the global model, preventing\npotential security degradation resulting from the combination of numerous local\nmodels. The possibility of security degradation is derived from the composition\ntheorem. By introducing reverse noise in the global model, a zero-bias estimate\nof differential privacy noise between local and global models is achieved.\nFurthermore, we propose a new mix transactions mechanism utilizing ring\nsignature technology to better protect the identity privacy of local training\nclients. Security analysis and experimental results demonstrate that PPBFL,\ncompared to baseline methods, not only exhibits superior model performance but\nalso achieves higher security.",
    "updated" : "2024-01-08T15:38:22Z",
    "published" : "2024-01-02T13:13:28Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Chunhe Xia"
      },
      {
        "name" : "Wanshuang Lin"
      },
      {
        "name" : "Tianbo Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00793v2",
    "title" : "SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\n  Large Language Models",
    "summary" : "With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and difficult to circumvent or optimize effectively. To\naddress this concern, we introduce an advanced optimization framework called\nSecFormer, to achieve fast and accurate PPI for Transformer models. By\nimplementing model design optimization, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials, Fourier series and Goldschmidt's\nmethod to handle other complex nonlinear functions within PPI, such as GeLU,\nLayerNorm, and Softmax. Our extensive experiments reveal that SecFormer\noutperforms MPCFormer in performance, showing improvements of $5.6\\%$ and\n$24.2\\%$ for BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In\nterms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, demonstrating its effectiveness\nand speed.",
    "updated" : "2024-01-06T10:05:23Z",
    "published" : "2024-01-01T15:40:35Z",
    "authors" : [
      {
        "name" : "Jinglong Luo"
      },
      {
        "name" : "Yehong Zhang"
      },
      {
        "name" : "Jiaqi Zhang"
      },
      {
        "name" : "Xin Mu"
      },
      {
        "name" : "Hui Wang"
      },
      {
        "name" : "Yue Yu"
      },
      {
        "name" : "Zenglin Xu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.04423v1",
    "title" : "Privacy-Preserving Sequential Recommendation with Collaborative\n  Confusion",
    "summary" : "Sequential recommendation has attracted a lot of attention from both academia\nand industry, however the privacy risks associated to gathering and\ntransferring users' personal interaction data are often underestimated or\nignored. Existing privacy-preserving studies are mainly applied to traditional\ncollaborative filtering or matrix factorization rather than sequential\nrecommendation. Moreover, these studies are mostly based on differential\nprivacy or federated learning, which often leads to significant performance\ndegradation, or has high requirements for communication. In this work, we\naddress privacy-preserving from a different perspective. Unlike existing\nresearch, we capture collaborative signals of neighbor interaction sequences\nand directly inject indistinguishable items into the target sequence before the\nrecommendation process begins, thereby increasing the perplexity of the target\nsequence. Even if the target interaction sequence is obtained by attackers, it\nis difficult to discern which ones are the actual user interaction records. To\nachieve this goal, we propose a CoLlaborative-cOnfusion seqUential recommenDer,\nnamely CLOUD, which incorporates a collaborative confusion mechanism to edit\nthe raw interaction sequences before conducting recommendation. Specifically,\nCLOUD first calculates the similarity between the target interaction sequence\nand other neighbor sequences to find similar sequences. Then, CLOUD considers\nthe shared representation of the target sequence and similar sequences to\ndetermine the operation to be performed: keep, delete, or insert. We design a\ncopy mechanism to make items from similar sequences have a higher probability\nto be inserted into the target sequence. Finally, the modified sequence is used\nto train the recommender and predict the next item.",
    "updated" : "2024-01-09T08:30:50Z",
    "published" : "2024-01-09T08:30:50Z",
    "authors" : [
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Yujie Lin"
      },
      {
        "name" : "Pengjie Ren"
      },
      {
        "name" : "Zhumin Chen"
      },
      {
        "name" : "Tsunenori Mine"
      },
      {
        "name" : "Jianli Zhao"
      },
      {
        "name" : "Qiang Zhao"
      },
      {
        "name" : "Moyan Zhang"
      },
      {
        "name" : "Xianye Ben"
      },
      {
        "name" : "Yujun Li"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.04306v1",
    "title" : "Renyi Differential Privacy in the Shuffle Model: Enhanced Amplification\n  Bounds",
    "summary" : "The shuffle model of Differential Privacy (DP) has gained significant\nattention in privacy-preserving data analysis due to its remarkable tradeoff\nbetween privacy and utility. It is characterized by adding a shuffling\nprocedure after each user's locally differentially private perturbation, which\nleads to a privacy amplification effect, meaning that the privacy guarantee of\na small level of noise, say $\\epsilon_0$, can be enhanced to\n$O(\\epsilon_0/\\sqrt{n})$ (the smaller, the more private) after shuffling all\n$n$ users' perturbed data. Most studies in the shuffle DP focus on proving a\ntighter privacy guarantee of privacy amplification. However, the current\nresults assume that the local privacy budget $\\epsilon_0$ is within a limited\nrange. In addition, there remains a gap between the tightest lower bound and\nthe known upper bound of the privacy amplification. In this work, we push\nforward the state-of-the-art by making the following contributions. Firstly, we\npresent the first asymptotically optimal analysis of Renyi Differential Privacy\n(RDP) in the shuffle model without constraints on $\\epsilon_0$. Secondly, we\nintroduce hypothesis testing for privacy amplification through shuffling,\noffering a distinct analysis technique and a tighter upper bound. Furthermore,\nwe propose a DP-SGD algorithm based on RDP. Experiments demonstrate that our\napproach outperforms existing methods significantly at the same privacy level.",
    "updated" : "2024-01-09T01:47:46Z",
    "published" : "2024-01-09T01:47:46Z",
    "authors" : [
      {
        "name" : "E Chen"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Yifei Ge"
      }
    ],
    "categories" : [
      "math.CO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.05126v1",
    "title" : "Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving\n  Vision Transformer",
    "summary" : "We propose a novel method for privacy-preserving deep neural networks (DNNs)\nwith the Vision Transformer (ViT). The method allows us not only to train\nmodels and test with visually protected images but to also avoid the\nperformance degradation caused from the use of encrypted images, whereas\nconventional methods cannot avoid the influence of image encryption. A domain\nadaptation method is used to efficiently fine-tune ViT with encrypted images.\nIn experiments, the method is demonstrated to outperform conventional methods\nin an image classification task on the CIFAR-10 and ImageNet datasets in terms\nof classification accuracy.",
    "updated" : "2024-01-10T12:46:31Z",
    "published" : "2024-01-10T12:46:31Z",
    "authors" : [
      {
        "name" : "Teru Nagamori"
      },
      {
        "name" : "Sayaka Shiota"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.05835v1",
    "title" : "Privacy Analysis of Affine Transformations in Cloud-based MPC:\n  Vulnerability to Side-knowledge",
    "summary" : "Search for the optimizer in computationally demanding model predictive\ncontrol (MPC) setups can be facilitated by Cloud as a service provider in\ncyber-physical systems. This advantage introduces the risk that Cloud can\nobtain unauthorized access to the privacy-sensitive parameters of the system\nand cost function. To solve this issue, i.e., preventing Cloud from accessing\nthe parameters while benefiting from Cloud computation, random affine\ntransformations provide an exact yet light weight in computation solution. This\nresearch deals with analyzing privacy preserving properties of these\ntransformations when they are adopted for MPC problems. We consider two common\nstrategies for outsourcing the optimization required in MPC problems, namely\nseparate and dense forms, and establish that random affine transformations\nutilized in these forms are vulnerable to side-knowledge from Cloud.\nSpecifically, we prove that the privacy guarantees of these methods and their\nextensions for separate form are undermined when a mild side-knowledge about\nthe problem in terms of structure of MPC cost function is available. In\naddition, while we prove that outsourcing the MPC problem in the dense form\ninherently leads to some degree of privacy for the system and cost function\nparameters, we also establish that affine transformations applied to this form\nare nevertheless prone to be undermined by a Cloud with mild side-knowledge.\nNumerical simulations confirm our results.",
    "updated" : "2024-01-11T11:08:20Z",
    "published" : "2024-01-11T11:08:20Z",
    "authors" : [
      {
        "name" : "Teimour Hosseinalizadeh"
      },
      {
        "name" : "Nils Schlüter"
      },
      {
        "name" : "Moritz Schulze Darup"
      },
      {
        "name" : "Nima Monshizadeh"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.05562v1",
    "title" : "Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated\n  Learning",
    "summary" : "Federated learning (FL) enables multiple participants to train a global\nmachine learning model without sharing their private training data.\nPeer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating\nthe server that aggregates local models from participants and then updates the\nglobal model. However, P2P FL is vulnerable to (i) honest-but-curious\nparticipants whose objective is to infer private training data of other\nparticipants, and (ii) Byzantine participants who can transmit arbitrarily\nmanipulated local models to corrupt the learning process. P2P FL schemes that\nsimultaneously guarantee Byzantine resilience and preserve privacy have been\nless studied. In this paper, we develop Brave, a protocol that ensures\nByzantine Resilience And privacy-preserving property for P2P FL in the presence\nof both types of adversaries. We show that Brave preserves privacy by\nestablishing that any honest-but-curious adversary cannot infer other\nparticipants' private data by observing their models. We further prove that\nBrave is Byzantine-resilient, which guarantees that all benign participants\nconverge to an identical model that deviates from a global model trained\nwithout Byzantine adversaries by a bounded distance. We evaluate Brave against\nthree state-of-the-art adversaries on a P2P FL for image classification tasks\non benchmark datasets CIFAR10 and MNIST. Our results show that the global model\nlearned with Brave in the presence of adversaries achieves comparable\nclassification accuracy to a global model trained in the absence of any\nadversary.",
    "updated" : "2024-01-10T22:07:40Z",
    "published" : "2024-01-10T22:07:40Z",
    "authors" : [
      {
        "name" : "Zhangchen Xu"
      },
      {
        "name" : "Fengqing Jiang"
      },
      {
        "name" : "Luyao Niu"
      },
      {
        "name" : "Jinyuan Jia"
      },
      {
        "name" : "Radha Poovendran"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.06657v1",
    "title" : "Accelerating Tactile Internet with QUIC: A Security and Privacy\n  Perspective",
    "summary" : "The Tactile Internet paradigm is set to revolutionize human society by\nenabling skill-set delivery and haptic communication over ultra-reliable,\nlow-latency networks. The emerging sixth-generation (6G) mobile communication\nsystems are envisioned to underpin this Tactile Internet ecosystem at the\nnetwork edge by providing ubiquitous global connectivity. However, apart from a\nmultitude of opportunities of the Tactile Internet, security and privacy\nchallenges emerge at the forefront. We believe that the recently standardized\nQUIC protocol, characterized by end-to-end encryption and reduced round-trip\ndelay would serve as the backbone of Tactile Internet. In this article, we\nenvision a futuristic scenario where a QUIC-enabled network uses the underlying\n6G communication infrastructure to achieve the requirements for Tactile\nInternet. Interestingly this requires a deeper investigation of a wide range of\nsecurity and privacy challenges in QUIC, that need to be mitigated for its\nadoption in Tactile Internet. Henceforth, this article reviews the existing\nsecurity and privacy attacks in QUIC and their implication on users. Followed\nby that, we discuss state-of-the-art attack mitigation strategies and\ninvestigate some of their drawbacks with possible directions for future work",
    "updated" : "2024-01-12T16:05:13Z",
    "published" : "2024-01-12T16:05:13Z",
    "authors" : [
      {
        "name" : "Jayasree Sengupta"
      },
      {
        "name" : "Debasmita Dey"
      },
      {
        "name" : "Simone Ferlin"
      },
      {
        "name" : "Nirnay Ghosh"
      },
      {
        "name" : "Vaibhav Bajpai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.06601v1",
    "title" : "A proposal to increase data utility on Global Differential Privacy data\n  based on data use predictions",
    "summary" : "This paper presents ongoing research focused on improving the utility of data\nprotected by Global Differential Privacy(DP) in the scenario of summary\nstatistics. Our approach is based on predictions on how an analyst will use\nstatistics released under DP protection, so that a developer can optimise data\nutility on further usage of the data in the privacy budget allocation. This\nnovel approach can potentially improve the utility of data without compromising\nprivacy constraints. We also propose a metric that can be used by the developer\nto optimise the budget allocation process.",
    "updated" : "2024-01-12T14:34:30Z",
    "published" : "2024-01-12T14:34:30Z",
    "authors" : [
      {
        "name" : "Henry C. Nunes"
      },
      {
        "name" : "Marlon P. da Silva"
      },
      {
        "name" : "Charles V. Neu"
      },
      {
        "name" : "Avelino F. Zorzo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08038v1",
    "title" : "Calpric: Inclusive and Fine-grain Labeling of Privacy Policies with\n  Crowdsourcing and Active Learning",
    "summary" : "A significant challenge to training accurate deep learning models on privacy\npolicies is the cost and difficulty of obtaining a large and comprehensive set\nof training data. To address these challenges, we present Calpric , which\ncombines automatic text selection and segmentation, active learning and the use\nof crowdsourced annotators to generate a large, balanced training set for\nprivacy policies at low cost. Automated text selection and segmentation\nsimplifies the labeling task, enabling untrained annotators from crowdsourcing\nplatforms, like Amazon's Mechanical Turk, to be competitive with trained\nannotators, such as law students, and also reduces inter-annotator agreement,\nwhich decreases labeling cost. Having reliable labels for training enables the\nuse of active learning, which uses fewer training samples to efficiently cover\nthe input space, further reducing cost and improving class and data category\nbalance in the data set. The combination of these techniques allows Calpric to\nproduce models that are accurate over a wider range of data categories, and\nprovide more detailed, fine-grain labels than previous work. Our crowdsourcing\nprocess enables Calpric to attain reliable labeled data at a cost of roughly\n$0.92-$1.71 per labeled text segment. Calpric 's training process also\ngenerates a labeled data set of 16K privacy policy text segments across 9 Data\ncategories with balanced positive and negative samples.",
    "updated" : "2024-01-16T01:27:26Z",
    "published" : "2024-01-16T01:27:26Z",
    "authors" : [
      {
        "name" : "Wenjun Qiu"
      },
      {
        "name" : "David Lie"
      },
      {
        "name" : "Lisa Austin"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.HC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08037v1",
    "title" : "Understanding factors behind IoT privacy -- A user's perspective on RF\n  sensors",
    "summary" : "While IoT sensors in physical spaces have provided utility and comfort in our\nlives, their instrumentation in private and personal spaces has led to growing\nconcerns regarding privacy. The existing notion behind IoT privacy is that the\nsensors whose data can easily be understood and interpreted by humans (such as\ncameras) are more privacy-invasive than sensors that are not\nhuman-understandable, such as RF (radio-frequency) sensors. However, given\nrecent advancements in machine learning, we can not only make sensitive\ninferences on RF data but also translate between modalities. Thus, the existing\nnotions of privacy for IoT sensors need to be revisited. In this paper, our\ngoal is to understand what factors affect the privacy notions of a non-expert\nuser (someone who is not well-versed in privacy concepts). To this regard, we\nconduct an online study of 162 participants from the USA to find out what\nfactors affect the privacy perception of a user regarding an RF-based device or\na sensor. Our findings show that a user's perception of privacy not only\ndepends upon the data collected by the sensor but also on the inferences that\ncan be made on that data, familiarity with the device and its form factor as\nwell as the control a user has over the device design and its data policies.\nWhen the data collected by the sensor is not human-interpretable, it is the\ninferences that can be made on the data and not the data itself that users care\nabout when making informed decisions regarding device privacy.",
    "updated" : "2024-01-16T01:13:14Z",
    "published" : "2024-01-16T01:13:14Z",
    "authors" : [
      {
        "name" : "Akash Deep Singh"
      },
      {
        "name" : "Brian Wang"
      },
      {
        "name" : "Luis Garcia"
      },
      {
        "name" : "Xiang Chen"
      },
      {
        "name" : "Mani Srivastava"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07691v1",
    "title" : "Privacy-Aware Single-Nucleotide Polymorphisms (SNPs) using Bilinear\n  Group Accumulators in Batch Mode",
    "summary" : "Biometric data is often highly sensitive, and a leak of this data can lead to\nserious privacy breaches. Some of the most sensitive of this type of data\nrelates to the usage of DNA data on individuals. A leak of this type of data\nwithout consent could lead to privacy breaches of data protection laws. Along\nwith this, there have been several recent data breaches related to the leak of\nDNA information, including from 23andMe and Ancestry. It is thus fundamental\nthat a citizen should have the right to know if their DNA data is contained\nwithin a DNA database and ask for it to be removed if they are concerned about\nits usage. This paper outlines a method of hashing the core information\ncontained within the data stores - known as Single-Nucleotide Polymorphisms\n(SNPs) - into a bilinear group accumulator in batch mode, which can then be\nsearched by a trusted entity for matches. The time to create the witness proof\nand to verify were measured at 0.86 ms and 10.90 ms, respectively.",
    "updated" : "2024-01-15T13:59:51Z",
    "published" : "2024-01-15T13:59:51Z",
    "authors" : [
      {
        "name" : "William J Buchanan"
      },
      {
        "name" : "Sam Grierson"
      },
      {
        "name" : "Daniel Uribe"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07464v1",
    "title" : "Quantum Privacy Aggregation of Teacher Ensembles (QPATE) for\n  Privacy-preserving Quantum Machine Learning",
    "summary" : "The utility of machine learning has rapidly expanded in the last two decades\nand presents an ethical challenge. Papernot et. al. developed a technique,\nknown as Private Aggregation of Teacher Ensembles (PATE) to enable federated\nlearning in which multiple teacher models are trained on disjoint datasets.\nThis study is the first to apply PATE to an ensemble of quantum neural networks\n(QNN) to pave a new way of ensuring privacy in quantum machine learning (QML)\nmodels.",
    "updated" : "2024-01-15T04:38:06Z",
    "published" : "2024-01-15T04:38:06Z",
    "authors" : [
      {
        "name" : "William Watkins"
      },
      {
        "name" : "Heehwan Wang"
      },
      {
        "name" : "Sangyoon Bae"
      },
      {
        "name" : "Huan-Hsin Tseng"
      },
      {
        "name" : "Jiook Cha"
      },
      {
        "name" : "Samuel Yen-Chi Chen"
      },
      {
        "name" : "Shinjae Yoo"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07348v1",
    "title" : "Generative AI in EU Law: Liability, Privacy, Intellectual Property, and\n  Cybersecurity",
    "summary" : "The advent of Generative AI, particularly through Large Language Models\n(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI\nlandscape. Advanced LLMs exhibit multimodality, handling diverse data formats,\nthereby broadening their application scope. However, the complexity and\nemergent autonomy of these models introduce challenges in predictability and\nlegal compliance. This paper delves into the legal and regulatory implications\nof Generative AI and LLMs in the European Union context, analyzing aspects of\nliability, privacy, intellectual property, and cybersecurity. It critically\nexamines the adequacy of the existing and proposed EU legislation, including\nthe Artificial Intelligence Act (AIA) draft, in addressing the unique\nchallenges posed by Generative AI in general and LLMs in particular. The paper\nidentifies potential gaps and shortcomings in the legislative framework and\nproposes recommendations to ensure the safe and compliant deployment of\ngenerative models, ensuring they align with the EU's evolving digital landscape\nand legal standards.",
    "updated" : "2024-01-14T19:16:29Z",
    "published" : "2024-01-14T19:16:29Z",
    "authors" : [
      {
        "name" : "Claudio Novelli"
      },
      {
        "name" : "Federico Casolari"
      },
      {
        "name" : "Philipp Hacker"
      },
      {
        "name" : "Giorgio Spedicato"
      },
      {
        "name" : "Luciano Floridi"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07343v1",
    "title" : "Privacy-Preserving Intrusion Detection in Software-defined VANET using\n  Federated Learning with BERT",
    "summary" : "The absence of robust security protocols renders the VANET (Vehicle ad-hoc\nNetworks) network open to cyber threats by compromising passengers and road\nsafety. Intrusion Detection Systems (IDS) are widely employed to detect network\nsecurity threats. With vehicles' high mobility on the road and diverse\nenvironments, VANETs devise ever-changing network topologies, lack privacy and\nsecurity, and have limited bandwidth efficiency. The absence of privacy\nprecautions, End-to-End Encryption methods, and Local Data Processing systems\nin VANET also present many privacy and security difficulties. So, assessing\nwhether a novel real-time processing IDS approach can be utilized for this\nemerging technology is crucial. The present study introduces a novel approach\nfor intrusion detection using Federated Learning (FL) capabilities in\nconjunction with the BERT model for sequence classification (FL-BERT). The\nsignificance of data privacy is duly recognized. According to FL methodology,\neach client has its own local model and dataset. They train their models\nlocally and then send the model's weights to the server. After aggregation, the\nserver aggregates the weights from all clients to update a global model. After\naggregation, the global model's weights are shared with the clients. This\npractice guarantees the secure storage of sensitive raw data on individual\nclients' devices, effectively protecting privacy. After conducting the\nfederated learning procedure, we assessed our models' performance using a\nseparate test dataset. The FL-BERT technique has yielded promising results,\nopening avenues for further investigation in this particular area of research.\nWe reached the result of our approaches by comparing existing research works\nand found that FL-BERT is more effective for privacy and security concerns. Our\nresults suggest that FL-BERT is a promising technique for enhancing attack\ndetection.",
    "updated" : "2024-01-14T18:32:25Z",
    "published" : "2024-01-14T18:32:25Z",
    "authors" : [
      {
        "name" : "Shakil Ibne Ahsan"
      },
      {
        "name" : "Phil Legg"
      },
      {
        "name" : "S M Iftekharul Alam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07316v1",
    "title" : "Finding Privacy-relevant Source Code",
    "summary" : "Privacy code review is a critical process that enables developers and legal\nexperts to ensure compliance with data protection regulations. However, the\ntask is challenging due to resource constraints. To address this, we introduce\nthe concept of privacy-relevant methods - specific methods in code that are\ndirectly involved in the processing of personal data. We then present an\nautomated approach to assist in code review by identifying and categorizing\nthese privacy-relevant methods in source code.\n  Using static analysis, we identify a set of methods based on their\noccurrences in 50 commonly used libraries. We then rank these methods according\nto their frequency of invocation with actual personal data in the top 30 GitHub\napplications. The highest-ranked methods are the ones we designate as\nprivacy-relevant in practice. For our evaluation, we examined 100 open-source\napplications and found that our approach identifies fewer than 5% of the\nmethods as privacy-relevant for personal data processing. This reduces the time\nrequired for code reviews. Case studies on Signal Desktop and Cal.com further\nvalidate the effectiveness of our approach in aiding code reviewers to produce\nenhanced reports that facilitate compliance with privacy regulations.",
    "updated" : "2024-01-14T15:38:29Z",
    "published" : "2024-01-14T15:38:29Z",
    "authors" : [
      {
        "name" : "Feiyang Tang"
      },
      {
        "name" : "Bjarte M. Østvold"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.06894v1",
    "title" : "On Coded Caching Systems with Offline Users, with and without Demand\n  Privacy against Colluding Users",
    "summary" : "Coded caching is a technique that leverages locally cached contents at the\nend users to reduce the network's peak-time communication load. Coded caching\nhas been shown to achieve significant performance gains compared to uncoded\nschemes and is thus considered a promising technique to boost performance in\nfuture networks by effectively trading off bandwidth for storage. The original\ncoded caching model introduced by Maddah-Ali and Niesen does not consider the\ncase where some users involved in the placement phase, may be offline during\nthe delivery phase. If so, the delivery may not start or it may be wasteful to\nperform the delivery with fictitious demands for the offline users. In\naddition, the active users may require their demand to be kept private. This\npaper formally defines a coded caching system where some users are offline, and\ninvestigates the optimal performance with and without demand privacy against\ncolluding users. For this novel coded caching model with offline users,\nachievable and converse bounds are proposed. These bounds are shown to meet\nunder certain conditions, and otherwise to be to within a constant\nmultiplicative gap of one another. In addition, the proposed achievable schemes\nhave lower subpacketization and lower load compared to baseline schemes (that\ntrivially extend known schemes so as to accommodate for privacy) in some memory\nregimes.",
    "updated" : "2024-01-12T21:06:42Z",
    "published" : "2024-01-12T21:06:42Z",
    "authors" : [
      {
        "name" : "Yinbin Ma"
      },
      {
        "name" : "Daniela Tuninetti"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.06883v1",
    "title" : "Scaling While Privacy Preserving: A Comprehensive Synthetic Tabular Data\n  Generation and Evaluation in Learning Analytics",
    "summary" : "Privacy poses a significant obstacle to the progress of learning analytics\n(LA), presenting challenges like inadequate anonymization and data misuse that\ncurrent solutions struggle to address. Synthetic data emerges as a potential\nremedy, offering robust privacy protection. However, prior LA research on\nsynthetic data lacks thorough evaluation, essential for assessing the delicate\nbalance between privacy and data utility. Synthetic data must not only enhance\nprivacy but also remain practical for data analytics. Moreover, diverse LA\nscenarios come with varying privacy and utility needs, making the selection of\nan appropriate synthetic data approach a pressing challenge. To address these\ngaps, we propose a comprehensive evaluation of synthetic data, which\nencompasses three dimensions of synthetic data quality, namely resemblance,\nutility, and privacy. We apply this evaluation to three distinct LA datasets,\nusing three different synthetic data generation methods. Our results show that\nsynthetic data can maintain similar utility (i.e., predictive performance) as\nreal data, while preserving privacy. Furthermore, considering different privacy\nand data utility requirements in different LA scenarios, we make customized\nrecommendations for synthetic data generation. This paper not only presents a\ncomprehensive evaluation of synthetic data but also illustrates its potential\nin mitigating privacy concerns within the field of LA, thus contributing to a\nwider application of synthetic data in LA and promoting a better practice for\nopen science.",
    "updated" : "2024-01-12T20:27:55Z",
    "published" : "2024-01-12T20:27:55Z",
    "authors" : [
      {
        "name" : "Qinyi Liu"
      },
      {
        "name" : "Mohammad Khalil"
      },
      {
        "name" : "Ronas Shakya"
      },
      {
        "name" : "Jelena Jovanovic"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.04076v2",
    "title" : "Security and Privacy Issues in Cloud Storage",
    "summary" : "Even with the vast potential that cloud computing has, so far, it has not\nbeen adopted by the consumers with the enthusiasm and pace that it be worthy;\nthis is a very reason statement why consumers still hesitated of using cloud\ncomputing for their sensitive data and the threats that prevent the consumers\nfrom shifting to use cloud computing in general and cloud storage in\nparticular. The cloud computing inherits the traditional potential security and\nprivacy threats besides its own issues due to its unique structures. Some\nthreats related to cloud computing are the insider malicious attacks from the\nemployees that even sometime the provider unconscious about, the lack of\ntransparency of agreement between consumer and provider, data loss, traffic\nhijacking, shared technology and insecure application interface. Such threats\nneed remedies to make the consumer use its features in secure way. In this\nreview, we spot the light on the most security and privacy issues which can be\nattributed as gaps that sometimes the consumers or even the enterprises are not\naware of. We also define the parties that involve in scenario of cloud\ncomputing that also may attack the entire cloud systems. We also show the\nconsequences of these threats.",
    "updated" : "2024-01-14T20:12:02Z",
    "published" : "2024-01-08T18:27:57Z",
    "authors" : [
      {
        "name" : "Norah Asiri"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03883v2",
    "title" : "The Impact of Differential Privacy on Recommendation Accuracy and\n  Popularity Bias",
    "summary" : "Collaborative filtering-based recommender systems leverage vast amounts of\nbehavioral user data, which poses severe privacy risks. Thus, often, random\nnoise is added to the data to ensure Differential Privacy (DP). However, to\ndate, it is not well understood, in which ways this impacts personalized\nrecommendations. In this work, we study how DP impacts recommendation accuracy\nand popularity bias, when applied to the training data of state-of-the-art\nrecommendation models. Our findings are three-fold: First, we find that nearly\nall users' recommendations change when DP is applied. Second, recommendation\naccuracy drops substantially while recommended item popularity experiences a\nsharp increase, suggesting that popularity bias worsens. Third, we find that DP\nexacerbates popularity bias more severely for users who prefer unpopular items\nthan for users that prefer popular items.",
    "updated" : "2024-01-15T15:09:00Z",
    "published" : "2024-01-08T13:31:02Z",
    "authors" : [
      {
        "name" : "Peter Müllner"
      },
      {
        "name" : "Elisabeth Lex"
      },
      {
        "name" : "Markus Schedl"
      },
      {
        "name" : "Dominik Kowald"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.03218v2",
    "title" : "MiniScope: Automated UI Exploration and Privacy Inconsistency Detection\n  of MiniApps via Two-phase Iterative Hybrid Analysis",
    "summary" : "The advent of MiniApps, operating within larger SuperApps, has revolutionized\nuser experiences by offering a wide range of services without the need for\nindividual app downloads. However, this convenience has raised significant\nprivacy concerns, as these MiniApps often require access to sensitive data,\npotentially leading to privacy violations. Our research addresses the critical\ngaps in the analysis of MiniApps' privacy practices, especially focusing on\nWeChat MiniApps in the Android ecosystem. Despite existing privacy regulations\nand platform guidelines, there is a lack of effective mechanisms to safeguard\nuser privacy fully. We introduce MiniScope, a novel two-phase hybrid analysis\napproach, specifically designed for the MiniApp environment. This approach\novercomes the limitations of existing static analysis techniques by\nincorporating dynamic UI exploration for complete code coverage and accurate\nprivacy practice identification. Our methodology includes modeling UI\ntransition states, resolving cross-package callback control flows, and\nautomated iterative UI exploration. This allows for a comprehensive\nunderstanding of MiniApps' privacy practices, addressing the unique challenges\nof sub-package loading and event-driven callbacks. Our empirical evaluation of\nover 120K MiniApps using MiniScope demonstrates its effectiveness in\nidentifying privacy inconsistencies. The results reveal significant issues,\nwith 5.7% of MiniApps over-collecting private data and 33.4% overclaiming data\ncollection. These findings emphasize the urgent need for more precise privacy\nmonitoring systems and highlight the responsibility of SuperApp operators to\nenforce stricter privacy measures.",
    "updated" : "2024-01-16T03:07:22Z",
    "published" : "2024-01-06T13:54:36Z",
    "authors" : [
      {
        "name" : "Shenao Wang"
      },
      {
        "name" : "Yuekang Li"
      },
      {
        "name" : "Kailong Wang"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Hui Li"
      },
      {
        "name" : "Yang Liu"
      },
      {
        "name" : "Haoyu Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08935v1",
    "title" : "Privacy Protected Contactless Cardio-respiratory Monitoring using\n  Defocused Cameras during Sleep",
    "summary" : "The monitoring of vital signs such as heart rate (HR) and respiratory rate\n(RR) during sleep is important for the assessment of sleep quality and\ndetection of sleep disorders. Camera-based HR and RR monitoring gained\npopularity in sleep monitoring in recent years. However, they are all facing\nwith serious privacy issues when using a video camera in the sleeping scenario.\nIn this paper, we propose to use the defocused camera to measure vital signs\nfrom optically blurred images, which can fundamentally eliminate the privacy\ninvasion as face is difficult to be identified in obtained blurry images. A\nspatial-redundant framework involving living-skin detection is used to extract\nHR and RR from the defocused camera in NIR, and a motion metric is designed to\nexclude outliers caused by body motions. In the benchmark, the overall Mean\nAbsolute Error (MAE) for HR measurement is 4.4 bpm, for RR measurement is 5.9\nbpm. Both have quality drops as compared to the measurement using a focused\ncamera, but the degradation in HR is much less, i.e. HR measurement has strong\ncorrelation with the reference ($R \\geq 0.90$). Preliminary experiments suggest\nthat it is feasible to use a defocused camera for cardio-respiratory monitoring\nwhile protecting the privacy. Further improvement is needed for robust RR\nmeasurement, such as by PPG-modulation based RR extraction.",
    "updated" : "2024-01-17T03:05:24Z",
    "published" : "2024-01-17T03:05:24Z",
    "authors" : [
      {
        "name" : "Yingen Zhu"
      },
      {
        "name" : "Jia Huang"
      },
      {
        "name" : "Hongzhou Lu"
      },
      {
        "name" : "Wenjin Wang"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08458v1",
    "title" : "Security and Privacy Issues and Solutions in Federated Learning for\n  Digital Healthcare",
    "summary" : "The advent of Federated Learning has enabled the creation of a\nhigh-performing model as if it had been trained on a considerable amount of\ndata. A multitude of participants and a server cooperatively train a model\nwithout the need for data disclosure or collection. The healthcare industry,\nwhere security and privacy are paramount, can substantially benefit from this\nnew learning paradigm, as data collection is no longer feasible due to\nstringent data policies. Nonetheless, unaddressed challenges and insufficient\nattack mitigation are hampering its adoption. Attack surfaces differ from\ntraditional centralized learning in that the server and clients communicate\nbetween each round of training. In this paper, we thus present vulnerabilities,\nattacks, and defenses based on the widened attack surfaces, as well as suggest\npromising new research directions toward a more robust FL.",
    "updated" : "2024-01-16T16:07:53Z",
    "published" : "2024-01-16T16:07:53Z",
    "authors" : [
      {
        "name" : "Hyejun Jeong"
      },
      {
        "name" : "Tai-Myoung Chung"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.08723v1",
    "title" : "HierSFL: Local Differential Privacy-aided Split Federated Learning in\n  Mobile Edge Computing",
    "summary" : "Federated Learning is a promising approach for learning from user data while\npreserving data privacy. However, the high requirements of the model training\nprocess make it difficult for clients with limited memory or bandwidth to\nparticipate. To tackle this problem, Split Federated Learning is utilized,\nwhere clients upload their intermediate model training outcomes to a cloud\nserver for collaborative server-client model training. This methodology\nfacilitates resource-constrained clients' participation in model training but\nalso increases the training time and communication overhead. To overcome these\nlimitations, we propose a novel algorithm, called Hierarchical Split Federated\nLearning (HierSFL), that amalgamates models at the edge and cloud phases,\npresenting qualitative directives for determining the best aggregation\ntimeframes to reduce computation and communication expenses. By implementing\nlocal differential privacy at the client and edge server levels, we enhance\nprivacy during local model parameter updates. Our experiments using CIFAR-10\nand MNIST datasets show that HierSFL outperforms standard FL approaches with\nbetter training accuracy, training time, and communication-computing\ntrade-offs. HierSFL offers a promising solution to mobile edge computing's\nchallenges, ultimately leading to faster content delivery and improved mobile\nservice quality.",
    "updated" : "2024-01-16T09:34:10Z",
    "published" : "2024-01-16T09:34:10Z",
    "authors" : [
      {
        "name" : "Minh K. Quan"
      },
      {
        "name" : "Dinh C. Nguyen"
      },
      {
        "name" : "Van-Dinh Nguyen"
      },
      {
        "name" : "Mayuri Wijayasundara"
      },
      {
        "name" : "Sujeeva Setunge"
      },
      {
        "name" : "Pubudu N. Pathirana"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.07343v2",
    "title" : "Privacy-Preserving Intrusion Detection in Software-defined VANET using\n  Federated Learning with BERT",
    "summary" : "The absence of robust security protocols renders the VANET (Vehicle ad-hoc\nNetworks) network open to cyber threats by compromising passengers and road\nsafety. Intrusion Detection Systems (IDS) are widely employed to detect network\nsecurity threats. With vehicles' high mobility on the road and diverse\nenvironments, VANETs devise ever-changing network topologies, lack privacy and\nsecurity, and have limited bandwidth efficiency. The absence of privacy\nprecautions, End-to-End Encryption methods, and Local Data Processing systems\nin VANET also present many privacy and security difficulties. So, assessing\nwhether a novel real-time processing IDS approach can be utilized for this\nemerging technology is crucial. The present study introduces a novel approach\nfor intrusion detection using Federated Learning (FL) capabilities in\nconjunction with the BERT model for sequence classification (FL-BERT). The\nsignificance of data privacy is duly recognized. According to FL methodology,\neach client has its own local model and dataset. They train their models\nlocally and then send the model's weights to the server. After aggregation, the\nserver aggregates the weights from all clients to update a global model. After\naggregation, the global model's weights are shared with the clients. This\npractice guarantees the secure storage of sensitive raw data on individual\nclients' devices, effectively protecting privacy. After conducting the\nfederated learning procedure, we assessed our models' performance using a\nseparate test dataset. The FL-BERT technique has yielded promising results,\nopening avenues for further investigation in this particular area of research.\nWe reached the result of our approaches by comparing existing research works\nand found that FL-BERT is more effective for privacy and security concerns. Our\nresults suggest that FL-BERT is a promising technique for enhancing attack\ndetection.",
    "updated" : "2024-01-17T09:25:48Z",
    "published" : "2024-01-14T18:32:25Z",
    "authors" : [
      {
        "name" : "Shakil Ibne Ahsan"
      },
      {
        "name" : "Phil Legg"
      },
      {
        "name" : "S M Iftekharul Alam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.09604v1",
    "title" : "MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical\n  Images with Transformers and Fully Homomorphic Encryption",
    "summary" : "Advancements in machine learning (ML) have significantly revolutionized\nmedical image analysis, prompting hospitals to rely on external ML services.\nHowever, the exchange of sensitive patient data, such as chest X-rays, poses\ninherent privacy risks when shared with third parties. Addressing this concern,\nwe propose MedBlindTuner, a privacy-preserving framework leveraging fully\nhomomorphic encryption (FHE) and a data-efficient image transformer (DEiT).\nMedBlindTuner enables the training of ML models exclusively on FHE-encrypted\nmedical images. Our experimental evaluation demonstrates that MedBlindTuner\nachieves comparable accuracy to models trained on non-encrypted images,\noffering a secure solution for outsourcing ML computations while preserving\npatient data privacy. To the best of our knowledge, this is the first work that\nuses data-efficient image transformers and fully homomorphic encryption in this\ndomain.",
    "updated" : "2024-01-17T21:30:22Z",
    "published" : "2024-01-17T21:30:22Z",
    "authors" : [
      {
        "name" : "Prajwal Panzade"
      },
      {
        "name" : "Daniel Takabi"
      },
      {
        "name" : "Zhipeng Cai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.09519v1",
    "title" : "Privacy Engineering in Smart Home (SH) Systems: A Comprehensive Privacy\n  Threat Analysis and Risk Management Approach",
    "summary" : "Addressing trust concerns in Smart Home (SH) systems is imperative due to the\nlimited study on preservation approaches that focus on analyzing and evaluating\nprivacy threats for effective risk management. While most research focuses\nprimarily on user privacy, device data privacy, especially identity privacy, is\nalmost neglected, which can significantly impact overall user privacy within\nthe SH system. To this end, our study incorporates privacy engineering (PE)\nprinciples in the SH system that consider user and device data privacy. We\nstart with a comprehensive reference model for a typical SH system. Based on\nthe initial stage of LINDDUN PRO for the PE framework, we present a data flow\ndiagram (DFD) based on a typical SH reference model to better understand SH\nsystem operations. To identify potential areas of privacy threat and perform a\nprivacy threat analysis (PTA), we employ the LINDDUN PRO threat model. Then, a\nprivacy impact assessment (PIA) was carried out to implement privacy risk\nmanagement by prioritizing privacy threats based on their likelihood of\noccurrence and potential consequences. Finally, we suggest possible privacy\nenhancement techniques (PETs) that can mitigate some of these threats. The\nstudy aims to elucidate the main threats to privacy, associated risks, and\neffective prioritization of privacy control in SH systems. The outcomes of this\nstudy are expected to benefit SH stakeholders, including vendors, cloud\nproviders, users, researchers, and regulatory bodies in the SH systems domain.",
    "updated" : "2024-01-17T17:34:52Z",
    "published" : "2024-01-17T17:34:52Z",
    "authors" : [
      {
        "name" : "Emmanuel Dare Alalade"
      },
      {
        "name" : "Mohammed Mahyoub"
      },
      {
        "name" : "Ashraf Matrawy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.10158v1",
    "title" : "DISTINQT: A Distributed Privacy Aware Learning Framework for QoS\n  Prediction for Future Mobile and Wireless Networks",
    "summary" : "Beyond 5G and 6G networks are expected to support new and challenging use\ncases and applications that depend on a certain level of Quality of Service\n(QoS) to operate smoothly. Predicting the QoS in a timely manner is of high\nimportance, especially for safety-critical applications as in the case of\nvehicular communications. Although until recent years the QoS prediction has\nbeen carried out by centralized Artificial Intelligence (AI) solutions, a\nnumber of privacy, computational, and operational concerns have emerged.\nAlternative solutions have been surfaced (e.g. Split Learning, Federated\nLearning), distributing AI tasks of reduced complexity across nodes, while\npreserving the privacy of the data. However, new challenges rise when it comes\nto scalable distributed learning approaches, taking into account the\nheterogeneous nature of future wireless networks. The current work proposes\nDISTINQT, a privacy-aware distributed learning framework for QoS prediction.\nOur framework supports multiple heterogeneous nodes, in terms of data types and\nmodel architectures, by sharing computations across them. This, enables the\nincorporation of diverse knowledge into a sole learning process that will\nenhance the robustness and generalization capabilities of the final QoS\nprediction model. DISTINQT also contributes to data privacy preservation by\nencoding any raw input data into a non-linear latent representation before any\ntransmission. Evaluation results showcase that our framework achieves a\nstatistically identical performance compared to its centralized version and an\naverage performance improvement of up to 65% against six state-of-the-art\ncentralized baseline solutions in the Tele-Operated Driving use case.",
    "updated" : "2024-01-15T13:00:48Z",
    "published" : "2024-01-15T13:00:48Z",
    "authors" : [
      {
        "name" : "Nikolaos Koursioumpas"
      },
      {
        "name" : "Lina Magoula"
      },
      {
        "name" : "Ioannis Stavrakakis"
      },
      {
        "name" : "Nancy Alonistioti"
      },
      {
        "name" : "M. A. Gutierrez-Estevez"
      },
      {
        "name" : "Ramin Khalili"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  }
]