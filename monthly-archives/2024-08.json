[
  {
    "id" : "http://arxiv.org/abs/2408.00639v1",
    "title" : "Privacy-preserving datasets by capturing feature distributions with\n  Conditional VAEs",
    "summary" : "Large and well-annotated datasets are essential for advancing deep learning\napplications, however often costly or impossible to obtain by a single entity.\nIn many areas, including the medical domain, approaches relying on data sharing\nhave become critical to address those challenges. While effective in increasing\ndataset size and diversity, data sharing raises significant privacy concerns.\nCommonly employed anonymization methods based on the k-anonymity paradigm often\nfail to preserve data diversity, affecting model robustness. This work\nintroduces a novel approach using Conditional Variational Autoencoders (CVAEs)\ntrained on feature vectors extracted from large pre-trained vision foundation\nmodels. Foundation models effectively detect and represent complex patterns\nacross diverse domains, allowing the CVAE to faithfully capture the embedding\nspace of a given data distribution to generate (sample) a diverse,\nprivacy-respecting, and potentially unbounded set of synthetic feature vectors.\nOur method notably outperforms traditional approaches in both medical and\nnatural image domains, exhibiting greater dataset diversity and higher\nrobustness against perturbations while preserving sample privacy. These results\nunderscore the potential of generative models to significantly impact deep\nlearning applications in data-scarce and privacy-sensitive environments. The\nsource code is available at\nhttps://github.com/francescodisalvo05/cvae-anonymization .",
    "updated" : "2024-08-01T15:26:24Z",
    "published" : "2024-08-01T15:26:24Z",
    "authors" : [
      {
        "name" : "Francesco Di Salvo"
      },
      {
        "name" : "David Tafler"
      },
      {
        "name" : "Sebastian Doerrich"
      },
      {
        "name" : "Christian Ledig"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00382v1",
    "title" : "Long-Term Conversation Analysis: Privacy-Utility Trade-off under Noise\n  and Reverberation",
    "summary" : "Recordings in everyday life require privacy preservation of the speech\ncontent and speaker identity. This contribution explores the influence of noise\nand reverberation on the trade-off between privacy and utility for low-cost\nprivacy-preserving methods feasible for edge computing. These methods\ncompromise spectral and temporal smoothing, speaker anonymization using the\nMcAdams coefficient, sampling with a very low sampling rate, and combinations.\nPrivacy is assessed by automatic speech and speaker recognition, while our\nutility considers voice activity detection and speaker diarization. Overall,\nour evaluation shows that additional noise degrades the performance of all\nmodels more than reverberation. This degradation corresponds to enhanced speech\nprivacy, while utility is less deteriorated for some methods.",
    "updated" : "2024-08-01T08:43:46Z",
    "published" : "2024-08-01T08:43:46Z",
    "authors" : [
      {
        "name" : "Jule Pohlhausen"
      },
      {
        "name" : "Francesco Nespoli"
      },
      {
        "name" : "Joerg Bitzer"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00294v1",
    "title" : "RDP: Ranked Differential Privacy for Facial Feature Protection in\n  Multiscale Sparsified Subspace",
    "summary" : "With the widespread sharing of personal face images in applications' public\ndatabases, face recognition systems faces real threat of being breached by\npotential adversaries who are able to access users' face images and use them to\nintrude the face recognition systems. In this paper, we propose a novel privacy\nprotection method in the multiscale sparsified feature subspaces to protect\nsensitive facial features, by taking care of the influence or weight ranked\nfeature coefficients on the privacy budget, named \"Ranked Differential Privacy\n(RDP)\". After the multiscale feature decomposition, the lightweight Laplacian\nnoise is added to the dimension-reduced sparsified feature coefficients\naccording to the geometric superposition method. Then, we rigorously prove that\nthe RDP satisfies Differential Privacy. After that, the nonlinear Lagrange\nMultiplier (LM) method is formulated for the constraint optimization problem of\nmaximizing the utility of the visualization quality protected face images with\nsanitizing noise, under a given facial features privacy budget. Then, two\nmethods are proposed to solve the nonlinear LM problem and obtain the optimal\nnoise scale parameters: 1) the analytical Normalization Approximation (NA)\nmethod with identical average noise scale parameter for real-time online\napplications; and 2) the LM optimization Gradient Descent (LMGD) numerical\nmethod to obtain the nonlinear solution through iterative updating for more\naccurate offline applications. Experimental results on two real-world datasets\nshow that our proposed RDP outperforms other state-of-the-art methods: at a\nprivacy budget of 0.2, the PSNR (Peak Signal-to-Noise Ratio) of the RDP is\nabout ~10 dB higher than (10 times as high as) the highest PSNR of all compared\nmethods.",
    "updated" : "2024-08-01T05:41:59Z",
    "published" : "2024-08-01T05:41:59Z",
    "authors" : [
      {
        "name" : "Lu Ou"
      },
      {
        "name" : "Shaolin Liao"
      },
      {
        "name" : "Shihui Gao"
      },
      {
        "name" : "Guandong Huang"
      },
      {
        "name" : "Zheng Qi"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01228v1",
    "title" : "The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models",
    "summary" : "Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.",
    "updated" : "2024-08-02T12:36:13Z",
    "published" : "2024-08-02T12:36:13Z",
    "authors" : [
      {
        "name" : "Simone Caldarella"
      },
      {
        "name" : "Massimiliano Mancini"
      },
      {
        "name" : "Elisa Ricci"
      },
      {
        "name" : "Rahaf Aljundi"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01040v1",
    "title" : "Privacy-Preserving Split Learning with Vision Transformers using\n  Patch-Wise Random and Noisy CutMix",
    "summary" : "In computer vision, the vision transformer (ViT) has increasingly superseded\nthe convolutional neural network (CNN) for improved accuracy and robustness.\nHowever, ViT's large model sizes and high sample complexity make it difficult\nto train on resource-constrained edge devices. Split learning (SL) emerges as a\nviable solution, leveraging server-side resources to train ViTs while utilizing\nprivate data from distributed devices. However, SL requires additional\ninformation exchange for weight updates between the device and the server,\nwhich can be exposed to various attacks on private training data. To mitigate\nthe risk of data breaches in classification tasks, inspired from the CutMix\nregularization, we propose a novel privacy-preserving SL framework that injects\nGaussian noise into smashed data and mixes randomly chosen patches of smashed\ndata across clients, coined DP-CutMixSL. Our analysis demonstrates that\nDP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy\nprotection against membership inference attacks during forward propagation.\nThrough simulations, we show that DP-CutMixSL improves privacy protection\nagainst membership inference attacks, reconstruction attacks, and label\ninference attacks, while also improving accuracy compared to DP-SL and\nDP-MixSL.",
    "updated" : "2024-08-02T06:24:39Z",
    "published" : "2024-08-02T06:24:39Z",
    "authors" : [
      {
        "name" : "Seungeun Oh"
      },
      {
        "name" : "Sihun Baek"
      },
      {
        "name" : "Jihong Park"
      },
      {
        "name" : "Hyelin Nam"
      },
      {
        "name" : "Praneeth Vepakomma"
      },
      {
        "name" : "Ramesh Raskar"
      },
      {
        "name" : "Mehdi Bennis"
      },
      {
        "name" : "Seong-Lyun Kim"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00950v1",
    "title" : "PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking\n  Services",
    "summary" : "Eye gaze contains rich information about human attention and cognitive\nprocesses. This capability makes the underlying technology, known as gaze\ntracking, a critical enabler for many ubiquitous applications and has triggered\nthe development of easy-to-use gaze estimation services. Indeed, by utilizing\nthe ubiquitous cameras on tablets and smartphones, users can readily access\nmany gaze estimation services. In using these services, users must provide\ntheir full-face images to the gaze estimator, which is often a black box. This\nposes significant privacy threats to the users, especially when a malicious\nservice provider gathers a large collection of face images to classify\nsensitive user attributes. In this work, we present PrivateGaze, the first\napproach that can effectively preserve users' privacy in black-box gaze\ntracking services without compromising gaze estimation performance.\nSpecifically, we proposed a novel framework to train a privacy preserver that\nconverts full-face images into obfuscated counterparts, which are effective for\ngaze estimation while containing no privacy information. Evaluation on four\ndatasets shows that the obfuscated image can protect users' private\ninformation, such as identity and gender, against unauthorized attribute\nclassification. Meanwhile, when used directly by the black-box gaze estimator\nas inputs, the obfuscated images lead to comparable tracking performance to the\nconventional, unprotected full-face images.",
    "updated" : "2024-08-01T23:11:03Z",
    "published" : "2024-08-01T23:11:03Z",
    "authors" : [
      {
        "name" : "Lingyu Du"
      },
      {
        "name" : "Jinyuan Jia"
      },
      {
        "name" : "Xucong Zhang"
      },
      {
        "name" : "Guohao Lan"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.02373v1",
    "title" : "Operationalizing Contextual Integrity in Privacy-Conscious Assistants",
    "summary" : "Advanced AI assistants combine frontier LLMs and tool access to autonomously\nperform complex tasks on behalf of users. While the helpfulness of such\nassistants can increase dramatically with access to user information including\nemails and documents, this raises privacy concerns about assistants sharing\ninappropriate information with third parties without user supervision. To steer\ninformation-sharing assistants to behave in accordance with privacy\nexpectations, we propose to operationalize $\\textit{contextual integrity}$\n(CI), a framework that equates privacy with the appropriate flow of information\nin a given context. In particular, we design and evaluate a number of\nstrategies to steer assistants' information-sharing actions to be CI compliant.\nOur evaluation is based on a novel form filling benchmark composed of synthetic\ndata and human annotations, and it reveals that prompting frontier LLMs to\nperform CI-based reasoning yields strong results.",
    "updated" : "2024-08-05T10:53:51Z",
    "published" : "2024-08-05T10:53:51Z",
    "authors" : [
      {
        "name" : "Sahra Ghalebikesabi"
      },
      {
        "name" : "Eugene Bagdasaryan"
      },
      {
        "name" : "Ren Yi"
      },
      {
        "name" : "Itay Yona"
      },
      {
        "name" : "Ilia Shumailov"
      },
      {
        "name" : "Aneesh Pappu"
      },
      {
        "name" : "Chongyang Shi"
      },
      {
        "name" : "Laura Weidinger"
      },
      {
        "name" : "Robert Stanforth"
      },
      {
        "name" : "Leonard Berrada"
      },
      {
        "name" : "Pushmeet Kohli"
      },
      {
        "name" : "Po-Sen Huang"
      },
      {
        "name" : "Borja Balle"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01711v1",
    "title" : "Privacy in networks of quantum sensors",
    "summary" : "We treat privacy in a network of quantum sensors where accessible information\nis limited to specific functions of the network parameters, and all other\ninformation remains private. We develop an analysis of privacy in terms of a\nmanipulation of the quantum Fisher information matrix, and find the optimal\nstate achieving maximum privacy in the estimation of linear combination of the\nunknown parameters in a network of quantum sensors. We also discuss the effect\nof uncorrelated noise on the privacy of the network. Moreover, we illustrate\nour results with an example where the goal is to estimate the average value of\nthe unknown parameters in the network. In this example, we also introduce the\nnotion of quasi-privacy ($\\epsilon$-privacy), quantifying how close the state\nis to being private.",
    "updated" : "2024-08-03T08:39:44Z",
    "published" : "2024-08-03T08:39:44Z",
    "authors" : [
      {
        "name" : "Majid Hassani"
      },
      {
        "name" : "Santiago Scheiner"
      },
      {
        "name" : "Matteo G. A. Paris"
      },
      {
        "name" : "Damian Markham"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01609v1",
    "title" : "Fed-RD: Privacy-Preserving Federated Learning for Financial Crime\n  Detection",
    "summary" : "We introduce Federated Learning for Relational Data (Fed-RD), a novel\nprivacy-preserving federated learning algorithm specifically developed for\nfinancial transaction datasets partitioned vertically and horizontally across\nparties. Fed-RD strategically employs differential privacy and secure\nmultiparty computation to guarantee the privacy of training data. We provide\ntheoretical analysis of the end-to-end privacy of the training algorithm and\npresent experimental results on realistic synthetic datasets. Our results\ndemonstrate that Fed-RD achieves high model accuracy with minimal degradation\nas privacy increases, while consistently surpassing benchmark results.",
    "updated" : "2024-08-03T00:07:10Z",
    "published" : "2024-08-03T00:07:10Z",
    "authors" : [
      {
        "name" : "Md. Saikat Islam Khan"
      },
      {
        "name" : "Aparna Gupta"
      },
      {
        "name" : "Oshani Seneviratne"
      },
      {
        "name" : "Stacy Patterson"
      }
    ],
    "categories" : [
      "cs.CE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.03185v1",
    "title" : "MaskAnyone Toolkit: Offering Strategies for Minimizing Privacy Risks and\n  Maximizing Utility in Audio-Visual Data Archiving",
    "summary" : "This paper introduces MaskAnyone, a novel toolkit designed to navigate some\nprivacy and ethical concerns of sharing audio-visual data in research.\nMaskAnyone offers a scalable, user-friendly solution for de-identifying\nindividuals in video and audio content through face-swapping and voice\nalteration, supporting multi-person masking and real-time bulk processing. By\nintegrating this tool within research practices, we aim to enhance data\nreproducibility and utility in social science research. Our approach draws on\nDesign Science Research, proposing that MaskAnyone can facilitate safer data\nsharing and potentially reduce the storage of fully identifiable data. We\ndiscuss the development and capabilities of MaskAnyone, explore its integration\ninto ethical research practices, and consider the broader implications of\naudio-visual data masking, including issues of consent and the risk of misuse.\nThe paper concludes with a preliminary evaluation framework for assessing the\neffectiveness and ethical integration of masking tools in such research\nsettings.",
    "updated" : "2024-08-06T13:35:27Z",
    "published" : "2024-08-06T13:35:27Z",
    "authors" : [
      {
        "name" : "Babajide Alamu Owoyele"
      },
      {
        "name" : "Martin Schilling"
      },
      {
        "name" : "Rohan Sawahn"
      },
      {
        "name" : "Niklas Kaemer"
      },
      {
        "name" : "Pavel Zherebenkov"
      },
      {
        "name" : "Bhuvanesh Verma"
      },
      {
        "name" : "Wim Pouw"
      },
      {
        "name" : "Gerard de Melo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.02927v1",
    "title" : "HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy\n  Protection",
    "summary" : "Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.",
    "updated" : "2024-08-06T03:21:13Z",
    "published" : "2024-08-06T03:21:13Z",
    "authors" : [
      {
        "name" : "Yuxin Wang"
      },
      {
        "name" : "Duanyu Feng"
      },
      {
        "name" : "Yongfu Dai"
      },
      {
        "name" : "Zhengyu Chen"
      },
      {
        "name" : "Jimin Huang"
      },
      {
        "name" : "Sophia Ananiadou"
      },
      {
        "name" : "Qianqian Xie"
      },
      {
        "name" : "Hao Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.02750v1",
    "title" : "Privacy-Safe Iris Presentation Attack Detection",
    "summary" : "This paper proposes a framework for a privacy-safe iris presentation attack\ndetection (PAD) method, designed solely with synthetically-generated,\nidentity-leakage-free iris images. Once trained, the method is evaluated in a\nclassical way using state-of-the-art iris PAD benchmarks. We designed two\ngenerative models for the synthesis of ISO/IEC 19794-6-compliant iris images.\nThe first model synthesizes bona fide-looking samples. To avoid ``identity\nleakage,'' the generated samples that accidentally matched those used in the\nmodel's training were excluded. The second model synthesizes images of irises\nwith textured contact lenses and is conditioned by a given contact lens brand\nto have better control over textured contact lens appearance when forming the\ntraining set. Our experiments demonstrate that models trained solely on\nsynthetic data achieve a lower but still reasonable performance when compared\nto solutions trained with iris images collected from human subjects. This is\nthe first-of-its-kind attempt to use solely synthetic data to train a\nfully-functional iris PAD solution, and despite the performance gap between\nregular and the proposed methods, this study demonstrates that with the\nincreasing fidelity of generative models, creating such privacy-safe iris PAD\nmethods may be possible. The source codes and generative models trained for\nthis work are offered along with the paper.",
    "updated" : "2024-08-05T18:09:02Z",
    "published" : "2024-08-05T18:09:02Z",
    "authors" : [
      {
        "name" : "Mahsa Mitcheff"
      },
      {
        "name" : "Patrick Tinsley"
      },
      {
        "name" : "Adam Czajka"
      }
    ],
    "categories" : [
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.03897v1",
    "title" : "Speech privacy-preserving methods using secret key for convolutional\n  neural network models and their robustness evaluation",
    "summary" : "In this paper, we propose privacy-preserving methods with a secret key for\nconvolutional neural network (CNN)-based models in speech processing tasks. In\nenvironments where untrusted third parties, like cloud servers, provide\nCNN-based systems, ensuring the privacy of speech queries becomes essential.\nThis paper proposes encryption methods for speech queries using secret keys and\na model structure that allows for encrypted queries to be accepted without\ndecryption. Our approach introduces three types of secret keys: Shuffling,\nFlipping, and random orthogonal matrix (ROM). In experiments, we demonstrate\nthat when the proposed methods are used with the correct key, identification\nperformance did not degrade. Conversely, when an incorrect key is used, the\nperformance significantly decreased. Particularly, with the use of ROM, we show\nthat even with a relatively small key space, high privacy-preserving\nperformance can be maintained many speech processing tasks. Furthermore, we\nalso demonstrate the difficulty of recovering original speech from encrypted\nqueries in various robustness evaluations.",
    "updated" : "2024-08-07T16:51:39Z",
    "published" : "2024-08-07T16:51:39Z",
    "authors" : [
      {
        "name" : "Shoko Niwa"
      },
      {
        "name" : "Sayaka Shiota"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.CR",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.03578v1",
    "title" : "Unraveling Privacy Threat Modeling Complexity: Conceptual Privacy\n  Analysis Layers",
    "summary" : "Analyzing privacy threats in software products is an essential part of\nsoftware development to ensure systems are privacy-respecting; yet it is still\na far from trivial activity. While there have been many advancements in the\npast decade, they tend to focus on describing 'what' the threats are. What\nisn't entirely clear yet is 'how' to actually find these threats. Privacy is a\ncomplex domain. We propose to use four conceptual layers (feature, ecosystem,\nbusiness context, and environment) to capture this privacy complexity. These\nlayers can be used as a frame to structure and specify the privacy analysis\nsupport in a more tangible and actionable way, thereby improving applicability\nof the analysis process.",
    "updated" : "2024-08-07T06:30:20Z",
    "published" : "2024-08-07T06:30:20Z",
    "authors" : [
      {
        "name" : "Kim Wuyts"
      },
      {
        "name" : "Avi Douglen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04315v1",
    "title" : "Federated Cubic Regularized Newton Learning with\n  Sparsification-amplified Differential Privacy",
    "summary" : "This paper investigates the use of the cubic-regularized Newton method within\na federated learning framework while addressing two major concerns that\ncommonly arise in federated learning: privacy leakage and communication\nbottleneck. We introduce a federated learning algorithm called Differentially\nPrivate Federated Cubic Regularized Newton (DP-FCRN). By leveraging\nsecond-order techniques, our algorithm achieves lower iteration complexity\ncompared to first-order methods. We also incorporate noise perturbation during\nlocal computations to ensure privacy. Furthermore, we employ sparsification in\nuplink transmission, which not only reduces the communication costs but also\namplifies the privacy guarantee. Specifically, this approach reduces the\nnecessary noise intensity without compromising privacy protection. We analyze\nthe convergence properties of our algorithm and establish the privacy\nguarantee. Finally, we validate the effectiveness of the proposed algorithm\nthrough experiments on a benchmark dataset.",
    "updated" : "2024-08-08T08:48:54Z",
    "published" : "2024-08-08T08:48:54Z",
    "authors" : [
      {
        "name" : "Wei Huo"
      },
      {
        "name" : "Changxin Liu"
      },
      {
        "name" : "Kemi Ding"
      },
      {
        "name" : "Karl Henrik Johansson"
      },
      {
        "name" : "Ling Shi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04188v1",
    "title" : "Trustworthy Semantic-Enabled 6G Communication: A Task-oriented and\n  Privacy-preserving Perspective",
    "summary" : "Trustworthy task-oriented semantic communication (ToSC) emerges as an\ninnovative approach in the 6G landscape, characterized by the transmission of\nonly vital information that is directly pertinent to a specific task. While\nToSC offers an efficient mode of communication, it concurrently raises concerns\nregarding privacy, as sophisticated adversaries might possess the capability to\nreconstruct the original data from the transmitted features. This article\nprovides an in-depth analysis of privacy-preserving strategies specifically\ndesigned for ToSC relying on deep neural network-based joint source and channel\ncoding (DeepJSCC). The study encompasses a detailed comparative assessment of\ntrustworthy feature perturbation methods such as differential privacy and\nencryption, alongside intrinsic security incorporation approaches like\nadversarial learning to train the JSCC and learning-based vector quantization\n(LBVQ). This comparative analysis underscores the integration of advanced\nexplainable learning algorithms into communication systems, positing a new\nbenchmark for privacy standards in the forthcoming 6G era.",
    "updated" : "2024-08-08T03:16:42Z",
    "published" : "2024-08-08T03:16:42Z",
    "authors" : [
      {
        "name" : "Shuaishuai Guo"
      },
      {
        "name" : "Anbang Zhang"
      },
      {
        "name" : "Yanhu Wang"
      },
      {
        "name" : "Chenyuan Feng"
      },
      {
        "name" : "Tony Q. S. Quek"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.05212v1",
    "title" : "Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions",
    "summary" : "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.",
    "updated" : "2024-08-10T05:41:19Z",
    "published" : "2024-08-10T05:41:19Z",
    "authors" : [
      {
        "name" : "Michele Miranda"
      },
      {
        "name" : "Elena Sofia Ruzzetti"
      },
      {
        "name" : "Andrea Santilli"
      },
      {
        "name" : "Fabio Massimo Zanzotto"
      },
      {
        "name" : "Sébastien Bratières"
      },
      {
        "name" : "Emanuele Rodolà"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.05092v1",
    "title" : "PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural\n  Networks",
    "summary" : "The training phase of deep neural networks requires substantial resources and\nas such is often performed on cloud servers. However, this raises privacy\nconcerns when the training dataset contains sensitive content, e.g., face\nimages. In this work, we propose a method to perform the training phase of a\ndeep learning model on both an edge device and a cloud server that prevents\nsensitive content being transmitted to the cloud while retaining the desired\ninformation. The proposed privacy-preserving method uses adversarial early\nexits to suppress the sensitive content at the edge and transmits the\ntask-relevant information to the cloud. This approach incorporates noise\naddition during the training phase to provide a differential privacy guarantee.\nWe extensively test our method on different facial datasets with diverse face\nattributes using various deep learning architectures, showcasing its\noutstanding performance. We also demonstrate the effectiveness of privacy\npreservation through successful defenses against different white-box and deep\nreconstruction attacks.",
    "updated" : "2024-08-09T14:33:34Z",
    "published" : "2024-08-09T14:33:34Z",
    "authors" : [
      {
        "name" : "Yamin Sepehri"
      },
      {
        "name" : "Pedram Pad"
      },
      {
        "name" : "Pascal Frossard"
      },
      {
        "name" : "L. Andrea Dunbar"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.DC",
      "cs.LG",
      "eess.IV",
      "I.2.10; I.2.6; I.2.11; K.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04931v1",
    "title" : "Privacy-Preserved Taxi Demand Prediction System Utilizing Distributed\n  Data",
    "summary" : "Accurate taxi-demand prediction is essential for optimizing taxi operations\nand enhancing urban transportation services. However, using customers' data in\nthese systems raises significant privacy and security concerns. Traditional\nfederated learning addresses some privacy issues by enabling model training\nwithout direct data exchange but often struggles with accuracy due to varying\ndata distributions across different regions or service providers. In this\npaper, we propose CC-Net: a novel approach using collaborative learning\nenhanced with contrastive learning for taxi-demand prediction. Our method\nensures high performance by enabling multiple parties to collaboratively train\na demand-prediction model through hierarchical federated learning. In this\napproach, similar parties are clustered together, and federated learning is\napplied within each cluster. The similarity is defined without data exchange,\nensuring privacy and security. We evaluated our approach using real-world data\nfrom five taxi service providers in Japan over fourteen months. The results\ndemonstrate that CC-Net maintains the privacy of customers' data while\nimproving prediction accuracy by at least 2.2% compared to existing techniques.",
    "updated" : "2024-08-09T08:24:47Z",
    "published" : "2024-08-09T08:24:47Z",
    "authors" : [
      {
        "name" : "Ren Ozeki"
      },
      {
        "name" : "Haruki Yonekura"
      },
      {
        "name" : "Hamada Rizk"
      },
      {
        "name" : "Hirozumi Yamaguchi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04888v1",
    "title" : "Locally Private Histograms in All Privacy Regimes",
    "summary" : "Frequency estimation, a.k.a. histograms, is a workhorse of data analysis, and\nas such has been thoroughly studied under differentially privacy. In\nparticular, computing histograms in the local model of privacy has been the\nfocus of a fruitful recent line of work, and various algorithms have been\nproposed, achieving the order-optimal $\\ell_\\infty$ error in the high-privacy\n(small $\\varepsilon$) regime while balancing other considerations such as time-\nand communication-efficiency. However, to the best of our knowledge, the\npicture is much less clear when it comes to the medium- or low-privacy regime\n(large $\\varepsilon$), despite its increased relevance in practice. In this\npaper, we investigate locally private histograms, and the very related\ndistribution learning task, in this medium-to-low privacy regime, and establish\nnear-tight (and somewhat unexpected) bounds on the $\\ell_\\infty$ error\nachievable. Our theoretical findings emerge from a novel analysis, which\nappears to improve bounds across the board for the locally private histogram\nproblem. We back our theoretical findings by an empirical comparison of\nexisting algorithms in all privacy regimes, to assess their typical performance\nand behaviour beyond the worst-case setting.",
    "updated" : "2024-08-09T06:22:45Z",
    "published" : "2024-08-09T06:22:45Z",
    "authors" : [
      {
        "name" : "Clément L. Canonne"
      },
      {
        "name" : "Abigail Gentle"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.DM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04684v1",
    "title" : "Moving beyond privacy and airspace safety: Guidelines for just drones in\n  policing",
    "summary" : "The use of drones offers police forces potential gains in efficiency and\nsafety. However, their use may also harm public perception of the police if\ndrones are refused. Therefore, police forces should consider the perception of\nbystanders and broader society to maximize drones' potential. This article\nexamines the concerns expressed by members of the public during a field trial\ninvolving 52 test participants. Analysis of the group interviews suggests that\ntheir worries go beyond airspace safety and privacy, broadly discussed in\nexisting literature and regulations. The interpretation of the results\nindicates that the perceived justice of drone use is a significant factor in\nacceptance. Leveraging the concept of organizational justice and data\ncollected, we propose a catalogue of guidelines for just operation of drones to\nsupplement the existing policy. We present the organizational justice\nperspective as a framework to integrate the concerns of the public and\nbystanders into legal work. Finally, we discuss the relevance of justice for\nthe legitimacy of the police's actions and provide implications for research\nand practice.",
    "updated" : "2024-08-08T09:04:01Z",
    "published" : "2024-08-08T09:04:01Z",
    "authors" : [
      {
        "name" : "Mateusz Dolata"
      },
      {
        "name" : "Gerhard Schwabe"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.06197v1",
    "title" : "Lancelot: Towards Efficient and Privacy-Preserving Byzantine-Robust\n  Federated Learning within Fully Homomorphic Encryption",
    "summary" : "In sectors such as finance and healthcare, where data governance is subject\nto rigorous regulatory requirements, the exchange and utilization of data are\nparticularly challenging. Federated Learning (FL) has risen as a pioneering\ndistributed machine learning paradigm that enables collaborative model training\nacross multiple institutions while maintaining data decentralization. Despite\nits advantages, FL is vulnerable to adversarial threats, particularly poisoning\nattacks during model aggregation, a process typically managed by a central\nserver. However, in these systems, neural network models still possess the\ncapacity to inadvertently memorize and potentially expose individual training\ninstances. This presents a significant privacy risk, as attackers could\nreconstruct private data by leveraging the information contained in the model\nitself. Existing solutions fall short of providing a viable, privacy-preserving\nBRFL system that is both completely secure against information leakage and\ncomputationally efficient. To address these concerns, we propose Lancelot, an\ninnovative and computationally efficient BRFL framework that employs fully\nhomomorphic encryption (FHE) to safeguard against malicious client activities\nwhile preserving data privacy. Our extensive testing, which includes medical\nimaging diagnostics and widely-used public image datasets, demonstrates that\nLancelot significantly outperforms existing methods, offering more than a\ntwenty-fold increase in processing speed, all while maintaining data privacy.",
    "updated" : "2024-08-12T14:48:25Z",
    "published" : "2024-08-12T14:48:25Z",
    "authors" : [
      {
        "name" : "Siyang Jiang"
      },
      {
        "name" : "Hao Yang"
      },
      {
        "name" : "Qipeng Xie"
      },
      {
        "name" : "Chuan Ma"
      },
      {
        "name" : "Sen Wang"
      },
      {
        "name" : "Guoliang Xing"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.06167v1",
    "title" : "Blind-Match: Efficient Homomorphic Encryption-Based 1:N Matching for\n  Privacy-Preserving Biometric Identification",
    "summary" : "We present Blind-Match, a novel biometric identification system that\nleverages homomorphic encryption (HE) for efficient and privacy-preserving 1:N\nmatching. Blind-Match introduces a HE-optimized cosine similarity computation\nmethod, where the key idea is to divide the feature vector into smaller parts\nfor processing rather than computing the entire vector at once. By optimizing\nthe number of these parts, Blind-Match minimizes execution time while ensuring\ndata privacy through HE. Blind-Match achieves superior performance compared to\nstate-of-the-art methods across various biometric datasets. On the LFW face\ndataset, Blind-Match attains a 99.63% Rank-1 accuracy with a 128-dimensional\nfeature vector, demonstrating its robustness in face recognition tasks. For\nfingerprint identification, Blind-Match achieves a remarkable 99.55% Rank-1\naccuracy on the PolyU dataset, even with a compact 16-dimensional feature\nvector, significantly outperforming the state-of-the-art method, Blind-Touch,\nwhich achieves only 59.17%. Furthermore, Blind-Match showcases practical\nefficiency in large-scale biometric identification scenarios, such as Naver\nCloud's FaceSign, by processing 6,144 biometric samples in 0.74 seconds using a\n128-dimensional feature vector.",
    "updated" : "2024-08-12T14:13:08Z",
    "published" : "2024-08-12T14:13:08Z",
    "authors" : [
      {
        "name" : "Hyunmin Choi"
      },
      {
        "name" : "Jiwon Kim"
      },
      {
        "name" : "Chiyoung Song"
      },
      {
        "name" : "Simon S. Woo"
      },
      {
        "name" : "Hyoungshick Kim"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.05723v1",
    "title" : "Deep Learning with Data Privacy via Residual Perturbation",
    "summary" : "Protecting data privacy in deep learning (DL) is of crucial importance.\nSeveral celebrated privacy notions have been established and used for\nprivacy-preserving DL. However, many existing mechanisms achieve privacy at the\ncost of significant utility degradation and computational overhead. In this\npaper, we propose a stochastic differential equation-based residual\nperturbation for privacy-preserving DL, which injects Gaussian noise into each\nresidual mapping of ResNets. Theoretically, we prove that residual perturbation\nguarantees differential privacy (DP) and reduces the generalization gap of DL.\nEmpirically, we show that residual perturbation is computationally efficient\nand outperforms the state-of-the-art differentially private stochastic gradient\ndescent (DPSGD) in utility maintenance without sacrificing membership privacy.",
    "updated" : "2024-08-11T08:26:43Z",
    "published" : "2024-08-11T08:26:43Z",
    "authors" : [
      {
        "name" : "Wenqi Tao"
      },
      {
        "name" : "Huaming Ling"
      },
      {
        "name" : "Zuoqiang Shi"
      },
      {
        "name" : "Bao Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.05543v1",
    "title" : "PixelFade: Privacy-preserving Person Re-identification with Noise-guided\n  Progressive Replacement",
    "summary" : "Online person re-identification services face privacy breaches from potential\ndata leakage and recovery attacks, exposing cloud-stored images to malicious\nattackers and triggering public concern. The privacy protection of pedestrian\nimages is crucial. Previous privacy-preserving person re-identification methods\nare unable to resist recovery attacks and compromise accuracy. In this paper,\nwe propose an iterative method (PixelFade) to optimize pedestrian images into\nnoise-like images to resist recovery attacks. We first give an in-depth study\nof protected images from previous privacy methods, which reveal that the chaos\nof protected images can disrupt the learning of recovery models. Accordingly,\nSpecifically, we propose Noise-guided Objective Function with the feature\nconstraints of a specific authorization model, optimizing pedestrian images to\nnormal-distributed noise images while preserving their original identity\ninformation as per the authorization model. To solve the above non-convex\noptimization problem, we propose a heuristic optimization algorithm that\nalternately performs the Constraint Operation and the Partial Replacement\nOperation. This strategy not only safeguards that original pixels are replaced\nwith noises to protect privacy, but also guides the images towards an improved\noptimization direction to effectively preserve discriminative features.\nExtensive experiments demonstrate that our PixelFade outperforms previous\nmethods in resisting recovery attacks and Re-ID performance. The code is\navailable at https://github.com/iSEE-Laboratory/PixelFade.",
    "updated" : "2024-08-10T12:52:54Z",
    "published" : "2024-08-10T12:52:54Z",
    "authors" : [
      {
        "name" : "Delong Zhang"
      },
      {
        "name" : "Yi-Xing Peng"
      },
      {
        "name" : "Xiao-Ming Wu"
      },
      {
        "name" : "Ancong Wu"
      },
      {
        "name" : "Wei-Shi Zheng"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07021v1",
    "title" : "Improved Counting under Continual Observation with Pure Differential\n  Privacy",
    "summary" : "Counting under continual observation is a well-studied problem in the area of\ndifferential privacy. Given a stream of updates $x_1,x_2,\\dots,x_T \\in \\{0,1\\}$\nthe problem is to continuously release estimates of the prefix sums\n$\\sum_{i=1}^t x_i$ for $t=1,\\dots,T$ while protecting each input $x_i$ in the\nstream with differential privacy. Recently, significant leaps have been made in\nour understanding of this problem under $\\textit{approximate}$ differential\nprivacy, aka. $(\\varepsilon,\\delta)$$\\textit{-differential privacy}$. However,\nfor the classical case of $\\varepsilon$-differential privacy, we are not aware\nof any improvement in mean squared error since the work of Honaker (TPDP 2015).\nIn this paper we present such an improvement, reducing the mean squared error\nby a factor of about 4, asymptotically. The key technique is a new\ngeneralization of the binary tree mechanism that uses a $k$-ary number system\nwith $\\textit{negative digits}$ to improve the privacy-accuracy trade-off. Our\nmechanism improves the mean squared error over all 'optimal'\n$(\\varepsilon,\\delta)$-differentially private factorization mechanisms based on\nGaussian noise whenever $\\delta$ is sufficiently small. Specifically, using\n$k=19$ we get an asymptotic improvement over the bound given in the work by\nHenzinger, Upadhyay and Upadhyay (SODA 2023) when $\\delta = O(T^{-0.92})$.",
    "updated" : "2024-08-13T16:36:33Z",
    "published" : "2024-08-13T16:36:33Z",
    "authors" : [
      {
        "name" : "Joel Daniel Andersson"
      },
      {
        "name" : "Rasmus Pagh"
      },
      {
        "name" : "Sahel Torkamani"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07006v1",
    "title" : "The Complexities of Differential Privacy for Survey Data",
    "summary" : "The concept of differential privacy (DP) has gained substantial attention in\nrecent years, most notably since the U.S. Census Bureau announced the adoption\nof the concept for its 2020 Decennial Census. However, despite its attractive\ntheoretical properties, implementing DP in practice remains challenging,\nespecially when it comes to survey data. In this paper we present some results\nfrom an ongoing project funded by the U.S. Census Bureau that is exploring the\npossibilities and limitations of DP for survey data. Specifically, we identify\nfive aspects that need to be considered when adopting DP in the survey context:\nthe multi-staged nature of data production; the limited privacy amplification\nfrom complex sampling designs; the implications of survey-weighted estimates;\nthe weighting adjustments for nonresponse and other data deficiencies, and the\nimputation of missing values. We summarize the project's key findings with\nrespect to each of these aspects and also discuss some of the challenges that\nstill need to be addressed before DP could become the new data protection\nstandard at statistical agencies.",
    "updated" : "2024-08-13T16:15:42Z",
    "published" : "2024-08-13T16:15:42Z",
    "authors" : [
      {
        "name" : "Jörg Drechsler"
      },
      {
        "name" : "James Bailie"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07004v1",
    "title" : "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based\n  Large Language Models",
    "summary" : "Web-based Large Language Model (LLM) services have been widely adopted and\nhave become an integral part of our Internet experience. Third-party plugins\nenhance the functionalities of LLM by enabling access to real-world data and\nservices. However, the privacy consequences associated with these services and\ntheir third-party plugins are not well understood. Sensitive prompt data are\nstored, processed, and shared by cloud-based LLM providers and third-party\nplugins. In this paper, we propose Casper, a prompt sanitization technique that\naims to protect user privacy by detecting and removing sensitive information\nfrom user inputs before sending them to LLM services. Casper runs entirely on\nthe user's device as a browser extension and does not require any changes to\nthe online LLM services. At the core of Casper is a three-layered sanitization\nmechanism consisting of a rule-based filter, a Machine Learning (ML)-based\nnamed entity recognizer, and a browser-based local LLM topic identifier. We\nevaluate Casper on a dataset of 4000 synthesized prompts and show that it can\neffectively filter out Personal Identifiable Information (PII) and\nprivacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.",
    "updated" : "2024-08-13T16:08:37Z",
    "published" : "2024-08-13T16:08:37Z",
    "authors" : [
      {
        "name" : "Chun Jie Chong"
      },
      {
        "name" : "Chenxi Hou"
      },
      {
        "name" : "Zhihao Yao"
      },
      {
        "name" : "Seyed Mohammadjavad Seyed Talebi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.06460v1",
    "title" : "Evaluating Privacy Measures for Load Hiding",
    "summary" : "In smart grids, the use of smart meters to measure electricity consumption at\na household level raises privacy concerns. To address them, researchers have\ndesigned various load hiding algorithms that manipulate the electricity\nconsumption measured. To compare how well these algorithms preserve privacy,\nvarious privacy measures have been proposed. However, there currently is no\nconsensus on which privacy measure is most appropriate to use. In this study,\nwe aim to identify the most effective privacy measure(s) for load hiding\nalgorithms. We have crafted a series of experiments to assess the effectiveness\nof these measures. found 20 of the 25 measures studied to be ineffective. Next,\nfocused on the well-known \"appliance usage\" secret, we have designed synthetic\ndata to find the measure that best deals with this secret. We observe that such\na measure, a variant of mutual information, actually exists.",
    "updated" : "2024-08-12T19:21:34Z",
    "published" : "2024-08-12T19:21:34Z",
    "authors" : [
      {
        "name" : "Vadim Arzamasov"
      },
      {
        "name" : "Klemens Böhm"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT",
      "94A60, 93A14",
      "K.6.5; H.3.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.06395v1",
    "title" : "Fast John Ellipsoid Computation with Differential Privacy Optimization",
    "summary" : "Determining the John ellipsoid - the largest volume ellipsoid contained\nwithin a convex polytope - is a fundamental problem with applications in\nmachine learning, optimization, and data analytics. Recent work has developed\nfast algorithms for approximating the John ellipsoid using sketching and\nleverage score sampling techniques. However, these algorithms do not provide\nprivacy guarantees for sensitive input data. In this paper, we present the\nfirst differentially private algorithm for fast John ellipsoid computation. Our\nmethod integrates noise perturbation with sketching and leverage score sampling\nto achieve both efficiency and privacy. We prove that (1) our algorithm\nprovides $(\\epsilon,\\delta)$-differential privacy, and the privacy guarantee\nholds for neighboring datasets that are $\\epsilon_0$-close, allowing\nflexibility in the privacy definition; (2) our algorithm still converges to a\n$(1+\\xi)$-approximation of the optimal John ellipsoid in\n$O(\\xi^{-2}(\\log(n/\\delta_0) + (L\\epsilon_0)^{-2}))$ iterations where $n$ is\nthe number of data point, $L$ is the Lipschitz constant, $\\delta_0$ is the\nfailure probability, and $\\epsilon_0$ is the closeness of neighboring input\ndatasets. Our theoretical analysis demonstrates the algorithm's convergence and\nprivacy properties, providing a robust approach for balancing utility and\nprivacy in John ellipsoid computation. This is the first differentially private\nalgorithm for fast John ellipsoid computation, opening avenues for future\nresearch in privacy-preserving optimization techniques.",
    "updated" : "2024-08-12T03:47:55Z",
    "published" : "2024-08-12T03:47:55Z",
    "authors" : [
      {
        "name" : "Jiuxiang Gu"
      },
      {
        "name" : "Xiaoyu Li"
      },
      {
        "name" : "Yingyu Liang"
      },
      {
        "name" : "Zhenmei Shi"
      },
      {
        "name" : "Zhao Song"
      },
      {
        "name" : "Junwei Yu"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07614v1",
    "title" : "Practical Considerations for Differential Privacy",
    "summary" : "Differential privacy is the gold standard for statistical data release. Used\nby governments, companies, and academics, its mathematically rigorous\nguarantees and worst-case assumptions on the strength and knowledge of\nattackers make it a robust and compelling framework for reasoning about\nprivacy. However, even with landmark successes, differential privacy has not\nachieved widespread adoption in everyday data use and data protection. In this\nwork we examine some of the practical obstacles that stand in the way.",
    "updated" : "2024-08-14T15:28:28Z",
    "published" : "2024-08-14T15:28:28Z",
    "authors" : [
      {
        "name" : "Kareem Amin"
      },
      {
        "name" : "Alex Kulesza"
      },
      {
        "name" : "Sergei Vassilvitskii"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08107v1",
    "title" : "Communication-robust and Privacy-safe Distributed Estimation for\n  Heterogeneous Community-level Behind-the-meter Solar Power Generation",
    "summary" : "The rapid growth of behind-the-meter (BTM) solar power generation systems\npresents challenges for distribution system planning and scheduling due to\ninvisible solar power generation. To address the data leakage problem of\ncentralized machine-learning methods in BTM solar power generation estimation,\nthe federated learning (FL) method has been investigated for its distributed\nlearning capability. However, the conventional FL method has encountered\nvarious challenges, including heterogeneity, communication failures, and\nmalicious privacy attacks. To overcome these challenges, this study proposes a\ncommunication-robust and privacy-safe distributed estimation method for\nheterogeneous community-level BTM solar power generation. Specifically, this\nstudy adopts multi-task FL as the main structure and learns the common and\nunique features of all communities. Simultaneously, it embeds an updated\nparameters estimation method into the multi-task FL, automatically identifies\nsimilarities between any two clients, and estimates the updated parameters for\nunavailable clients to mitigate the negative effects of communication failures.\nFinally, this study adopts a differential privacy mechanism under the dynamic\nprivacy budget allocation strategy to combat malicious privacy attacks and\nimprove model training efficiency. Case studies show that in the presence of\nheterogeneity and communication failures, the proposed method exhibits better\nestimation accuracy and convergence performance as compared with traditional FL\nand localized learning methods, while providing stronger privacy protection.",
    "updated" : "2024-08-15T12:11:03Z",
    "published" : "2024-08-15T12:11:03Z",
    "authors" : [
      {
        "name" : "Jinglei Feng"
      },
      {
        "name" : "Zhengshuo Li"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08002v1",
    "title" : "Practical Privacy-Preserving Identity Verification using Third-Party\n  Cloud Services and FHE (Role of Data Encoding in Circuit Depth Management)",
    "summary" : "National digital identity verification systems have played a critical role in\nthe effective distribution of goods and services, particularly, in developing\ncountries. Due to the cost involved in deploying and maintaining such systems,\ncombined with a lack of in-house technical expertise, governments seek to\noutsource this service to third-party cloud service providers to the extent\npossible. This leads to increased concerns regarding the privacy of users'\npersonal data. In this work, we propose a practical privacy-preserving digital\nidentity (ID) verification protocol where the third-party cloud services\nprocess the identity data encrypted using a (single-key) Fully Homomorphic\nEncryption (FHE) scheme such as BFV. Though the role of a trusted entity such\nas government is not completely eliminated, our protocol does significantly\nreduces the computation load on such parties.\n  A challenge in implementing a privacy-preserving ID verification protocol\nusing FHE is to support various types of queries such as exact and/or fuzzy\ndemographic and biometric matches including secure age comparisons. From a\ncryptographic engineering perspective, our main technical contribution is a\nuser data encoding scheme that encodes demographic and biometric user data in\nonly two BFV ciphertexts and yet facilitates us to outsource various types of\nID verification queries to a third-party cloud. Our encoding scheme also\nensures that the only computation done by the trusted entity is a\nquery-agnostic \"extended\" decryption. This is in stark contrast with recent\nworks that outsource all the non-arithmetic operations to a trusted server. We\nimplement our protocol using the Microsoft SEAL FHE library and demonstrate its\npracticality.",
    "updated" : "2024-08-15T08:12:07Z",
    "published" : "2024-08-15T08:12:07Z",
    "authors" : [
      {
        "name" : "Deep Inder Mohan"
      },
      {
        "name" : "Srinivas Vivek"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07892v1",
    "title" : "Personhood credentials: Artificial intelligence and the value of\n  privacy-preserving tools to distinguish who is real online",
    "summary" : "Anonymity is an important principle online. However, malicious actors have\nlong used misleading identities to conduct fraud, spread disinformation, and\ncarry out other deceptive schemes. With the advent of increasingly capable AI,\nbad actors can amplify the potential scale and effectiveness of their\noperations, intensifying the challenge of balancing anonymity and\ntrustworthiness online. In this paper, we analyze the value of a new tool to\naddress this challenge: \"personhood credentials\" (PHCs), digital credentials\nthat empower users to demonstrate that they are real people -- not AIs -- to\nonline services, without disclosing any personal information. Such credentials\ncan be issued by a range of trusted institutions -- governments or otherwise. A\nPHC system, according to our definition, could be local or global, and does not\nneed to be biometrics-based. Two trends in AI contribute to the urgency of the\nchallenge: AI's increasing indistinguishability (i.e., lifelike content and\navatars, agentic activity) from people online, and AI's increasing scalability\n(i.e., cost-effectiveness, accessibility). Drawing on a long history of\nresearch into anonymous credentials and \"proof-of-personhood\" systems,\npersonhood credentials give people a way to signal their trustworthiness on\nonline platforms, and offer service providers new tools for reducing misuse by\nbad actors. In contrast, existing countermeasures to automated deception --\nsuch as CAPTCHAs -- are inadequate against sophisticated AI, while stringent\nidentity verification solutions are insufficiently private for many use-cases.\nAfter surveying the benefits of personhood credentials, we also examine\ndeployment risks and design challenges. We conclude with actionable next steps\nfor policymakers, technologists, and standards bodies to consider in\nconsultation with the public.",
    "updated" : "2024-08-15T02:41:25Z",
    "published" : "2024-08-15T02:41:25Z",
    "authors" : [
      {
        "name" : "Steven Adler"
      },
      {
        "name" : "Zoë Hitzig"
      },
      {
        "name" : "Shrey Jain"
      },
      {
        "name" : "Catherine Brewer"
      },
      {
        "name" : "Wayne Chang"
      },
      {
        "name" : "Renée DiResta"
      },
      {
        "name" : "Eddy Lazzarin"
      },
      {
        "name" : "Sean McGregor"
      },
      {
        "name" : "Wendy Seltzer"
      },
      {
        "name" : "Divya Siddarth"
      },
      {
        "name" : "Nouran Soliman"
      },
      {
        "name" : "Tobin South"
      },
      {
        "name" : "Connor Spelliscy"
      },
      {
        "name" : "Manu Sporny"
      },
      {
        "name" : "Varya Srivastava"
      },
      {
        "name" : "John Bailey"
      },
      {
        "name" : "Brian Christian"
      },
      {
        "name" : "Andrew Critch"
      },
      {
        "name" : "Ronnie Falcon"
      },
      {
        "name" : "Heather Flanagan"
      },
      {
        "name" : "Kim Hamilton Duffy"
      },
      {
        "name" : "Eric Ho"
      },
      {
        "name" : "Claire R. Leibowicz"
      },
      {
        "name" : "Srikanth Nadhamuni"
      },
      {
        "name" : "Alan Z. Rozenshtein"
      },
      {
        "name" : "David Schnurr"
      },
      {
        "name" : "Evan Shapiro"
      },
      {
        "name" : "Lacey Strahm"
      },
      {
        "name" : "Andrew Trask"
      },
      {
        "name" : "Zoe Weinberg"
      },
      {
        "name" : "Cedric Whitney"
      },
      {
        "name" : "Tom Zick"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08722v1",
    "title" : "A Novel Buffered Federated Learning Framework for Privacy-Driven Anomaly\n  Detection in IIoT",
    "summary" : "Industrial Internet of Things (IIoT) is highly sensitive to data privacy and\ncybersecurity threats. Federated Learning (FL) has emerged as a solution for\npreserving privacy, enabling private data to remain on local IIoT clients while\ncooperatively training models to detect network anomalies. However, both\nsynchronous and asynchronous FL architectures exhibit limitations, particularly\nwhen dealing with clients with varying speeds due to data heterogeneity and\nresource constraints. Synchronous architecture suffers from straggler effects,\nwhile asynchronous methods encounter communication bottlenecks. Additionally,\nFL models are prone to adversarial inference attacks aimed at disclosing\nprivate training data. To address these challenges, we propose a Buffered FL\n(BFL) framework empowered by homomorphic encryption for anomaly detection in\nheterogeneous IIoT environments. BFL utilizes a novel weighted average time\napproach to mitigate both straggler effects and communication bottlenecks,\nensuring fairness between clients with varying processing speeds through\ncollaboration with a buffer-based server. The performance results, derived from\ntwo datasets, show the superiority of BFL compared to state-of-the-art FL\nmethods, demonstrating improved accuracy and convergence speed while enhancing\nprivacy preservation.",
    "updated" : "2024-08-16T13:01:59Z",
    "published" : "2024-08-16T13:01:59Z",
    "authors" : [
      {
        "name" : "Samira Kamali Poorazad"
      },
      {
        "name" : "Chafika Benzaid"
      },
      {
        "name" : "Tarik Taleb"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08666v1",
    "title" : "A Multivocal Literature Review on Privacy and Fairness in Federated\n  Learning",
    "summary" : "Federated Learning presents a way to revolutionize AI applications by\neliminating the necessity for data sharing. Yet, research has shown that\ninformation can still be extracted during training, making additional\nprivacy-preserving measures such as differential privacy imperative. To\nimplement real-world federated learning applications, fairness, ranging from a\nfair distribution of performance to non-discriminative behaviour, must be\nconsidered. Particularly in high-risk applications (e.g. healthcare), avoiding\nthe repetition of past discriminatory errors is paramount. As recent research\nhas demonstrated an inherent tension between privacy and fairness, we conduct a\nmultivocal literature review to examine the current methods to integrate\nprivacy and fairness in federated learning. Our analyses illustrate that the\nrelationship between privacy and fairness has been neglected, posing a critical\nrisk for real-world applications. We highlight the need to explore the\nrelationship between privacy, fairness, and performance, advocating for the\ncreation of integrated federated learning frameworks.",
    "updated" : "2024-08-16T11:15:52Z",
    "published" : "2024-08-16T11:15:52Z",
    "authors" : [
      {
        "name" : "Beatrice Balbierer"
      },
      {
        "name" : "Lukas Heinlein"
      },
      {
        "name" : "Domenique Zipperling"
      },
      {
        "name" : "Niklas Kühl"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08642v1",
    "title" : "The Power of Bias: Optimizing Client Selection in Federated Learning\n  with Heterogeneous Differential Privacy",
    "summary" : "To preserve the data privacy, the federated learning (FL) paradigm emerges in\nwhich clients only expose model gradients rather than original data for\nconducting model training. To enhance the protection of model gradients in FL,\ndifferentially private federated learning (DPFL) is proposed which incorporates\ndifferentially private (DP) noises to obfuscate gradients before they are\nexposed. Yet, an essential but largely overlooked problem in DPFL is the\nheterogeneity of clients' privacy requirement, which can vary significantly\nbetween clients and extremely complicates the client selection problem in DPFL.\nIn other words, both the data quality and the influence of DP noises should be\ntaken into account when selecting clients. To address this problem, we conduct\nconvergence analysis of DPFL under heterogeneous privacy, a generic client\nselection strategy, popular DP mechanisms and convex loss. Based on convergence\nanalysis, we formulate the client selection problem to minimize the value of\nloss function in DPFL with heterogeneous privacy, which is a convex\noptimization problem and can be solved efficiently. Accordingly, we propose the\nDPFL-BCS (biased client selection) algorithm. The extensive experiment results\nwith real datasets under both convex and non-convex loss functions indicate\nthat DPFL-BCS can remarkably improve model utility compared with the SOTA\nbaselines.",
    "updated" : "2024-08-16T10:19:27Z",
    "published" : "2024-08-16T10:19:27Z",
    "authors" : [
      {
        "name" : "Jiating Ma"
      },
      {
        "name" : "Yipeng Zhou"
      },
      {
        "name" : "Qi Li"
      },
      {
        "name" : "Quan Z. Sheng"
      },
      {
        "name" : "Laizhong Cui"
      },
      {
        "name" : "Jiangchuan Liu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08529v1",
    "title" : "Privacy-Preserving Vision Transformer Using Images Encrypted with\n  Restricted Random Permutation Matrices",
    "summary" : "We propose a novel method for privacy-preserving fine-tuning vision\ntransformers (ViTs) with encrypted images. Conventional methods using encrypted\nimages degrade model performance compared with that of using plain images due\nto the influence of image encryption. In contrast, the proposed encryption\nmethod using restricted random permutation matrices can provide a higher\nperformance than the conventional ones.",
    "updated" : "2024-08-16T04:57:21Z",
    "published" : "2024-08-16T04:57:21Z",
    "authors" : [
      {
        "name" : "Kouki Horio"
      },
      {
        "name" : "Kiyoshi Nishikawa"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08475v1",
    "title" : "Models Matter: Setting Accurate Privacy Expectations for Local and\n  Central Differential Privacy",
    "summary" : "Differential privacy is a popular privacy-enhancing technology that has been\ndeployed both in industry and government agencies. Unfortunately, existing\nexplanations of differential privacy fail to set accurate privacy expectations\nfor data subjects, which depend on the choice of deployment model. We design\nand evaluate new explanations of differential privacy for the local and central\nmodels, drawing inspiration from prior work explaining other privacy-enhancing\ntechnologies. We find that consequences-focused explanations in the style of\nprivacy nutrition labels that lay out the implications of differential privacy\nare a promising approach for setting accurate privacy expectations. Further, we\nfind that while process-focused explanations are not enough to set accurate\nprivacy expectations, combining consequences-focused explanations with a brief\ndescription of how differential privacy works leads to greater trust.",
    "updated" : "2024-08-16T01:21:57Z",
    "published" : "2024-08-16T01:21:57Z",
    "authors" : [
      {
        "name" : "Mary Anne Smart"
      },
      {
        "name" : "Priyanka Nanayakkara"
      },
      {
        "name" : "Rachel Cummings"
      },
      {
        "name" : "Gabriel Kaptchuk"
      },
      {
        "name" : "Elissa Redmiles"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10053v1",
    "title" : "Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory",
    "summary" : "Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards.",
    "updated" : "2024-08-19T14:48:04Z",
    "published" : "2024-08-19T14:48:04Z",
    "authors" : [
      {
        "name" : "Haoran Li"
      },
      {
        "name" : "Wei Fan"
      },
      {
        "name" : "Yulin Chen"
      },
      {
        "name" : "Jiayang Cheng"
      },
      {
        "name" : "Tianshu Chu"
      },
      {
        "name" : "Xuebing Zhou"
      },
      {
        "name" : "Peizhao Hu"
      },
      {
        "name" : "Yangqiu Song"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.09943v1",
    "title" : "Calibrating Noise for Group Privacy in Subsampled Mechanisms",
    "summary" : "Given a group size m and a sensitive dataset D, group privacy (GP) releases\ninformation about D with the guarantee that the adversary cannot infer with\nhigh confidence whether the underlying data is D or a neighboring dataset D'\nthat differs from D by m records. GP generalizes the well-established notion of\ndifferential privacy (DP) for protecting individuals' privacy; in particular,\nwhen m=1, GP reduces to DP. Compared to DP, GP is capable of protecting the\nsensitive aggregate information of a group of up to m individuals, e.g., the\naverage annual income among members of a yacht club. Despite its longstanding\npresence in the research literature and its promising applications, GP is often\ntreated as an afterthought, with most approaches first developing a DP\nmechanism and then using a generic conversion to adapt it for GP, treating the\nDP solution as a black box. As we point out in the paper, this methodology is\nsuboptimal when the underlying DP solution involves subsampling, e.g., in the\nclassic DP-SGD method for training deep learning models. In this case, the\nDP-to-GP conversion is overly pessimistic in its analysis, leading to low\nutility in the published results under GP.\n  Motivated by this, we propose a novel analysis framework that provides tight\nprivacy accounting for subsampled GP mechanisms. Instead of converting a\nblack-box DP mechanism to GP, our solution carefully analyzes and utilizes the\ninherent randomness in subsampled mechanisms, leading to a substantially\nimproved bound on the privacy loss with respect to GP. The proposed solution\napplies to a wide variety of foundational mechanisms with subsampling.\nExtensive experiments with real datasets demonstrate that compared to the\nbaseline convert-from-blackbox-DP approach, our GP mechanisms achieve noise\nreductions of over an order of magnitude in several practical settings,\nincluding deep neural network training.",
    "updated" : "2024-08-19T12:32:50Z",
    "published" : "2024-08-19T12:32:50Z",
    "authors" : [
      {
        "name" : "Yangfan Jiang"
      },
      {
        "name" : "Xinjian Luo"
      },
      {
        "name" : "Yin Yang"
      },
      {
        "name" : "Xiaokui Xiao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.09935v1",
    "title" : "Privacy Technologies for Financial Intelligence",
    "summary" : "Financial crimes like terrorism financing and money laundering can have real\nimpacts on society, including the abuse and mismanagement of public funds,\nincrease in societal problems such as drug trafficking and illicit gambling\nwith attendant economic costs, and loss of innocent lives in the case of\nterrorism activities. Complex financial crimes can be hard to detect primarily\nbecause data related to different pieces of the overall puzzle is usually\ndistributed across a network of financial institutions, regulators, and\nlaw-enforcement agencies and they cannot be easily shared due to privacy\nconstraints. Recent advances in Privacy-Preserving Data Matching and Machine\nLearning provide an opportunity for regulators and the financial industry to\ncome together to solve the risk-discovery problem with technology. This paper\nprovides a survey of the financial intelligence landscape and where\nopportunities lie for privacy technologies to improve the state-of-the-art in\nfinancial-crime detection.",
    "updated" : "2024-08-19T12:13:53Z",
    "published" : "2024-08-19T12:13:53Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Thilina Ranbaduge"
      },
      {
        "name" : "Kee Siong Ng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.09659v1",
    "title" : "An Algorithm for Enhancing Privacy-Utility Tradeoff in the Privacy\n  Funnel and Other Lift-based Measures",
    "summary" : "This paper investigates the privacy funnel, a privacy-utility tradeoff\nproblem in which mutual information quantifies both privacy and utility. The\nobjective is to maximize utility while adhering to a specified privacy budget.\nHowever, the privacy funnel represents a non-convex optimization problem,\nmaking it challenging to achieve an optimal solution. An existing proposed\napproach to this problem involves substituting the mutual information with the\nlift (the exponent of information density) and then solving the optimization.\nSince mutual information is the expectation of the information density, this\nsubstitution overestimates the privacy loss and results in a final smaller\nbound on the privacy of mutual information than what is allowed in the budget.\nThis significantly compromises the utility. To overcome this limitation, we\npropose using a privacy measure that is more relaxed than the lift but stricter\nthan mutual information while still allowing the optimization to be efficiently\nsolved. Instead of directly using information density, our proposed measure is\nthe average of information density over the sensitive data distribution for\neach observed data realization. We then introduce a heuristic algorithm capable\nof achieving solutions that produce extreme privacy values, which enhances\nutility. The numerical results confirm improved utility at the same privacy\nbudget compared to existing solutions in the literature. Additionally, we\nexplore two other privacy measures, $\\ell_{1}$-norm and strong\n$\\chi^2$-divergence, demonstrating the applicability of our algorithm to these\nlift-based measures. We evaluate the performance of our method by comparing its\noutput with previous works. Finally, we validate our heuristic approach with a\ntheoretical framework that estimates the optimal utility for strong\n$\\chi^2$-divergence, numerically showing a perfect match.",
    "updated" : "2024-08-19T02:43:18Z",
    "published" : "2024-08-19T02:43:18Z",
    "authors" : [
      {
        "name" : "Mohammad Amin Zarrabian"
      },
      {
        "name" : "Parastoo Sadeghi"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08475v2",
    "title" : "Models Matter: Setting Accurate Privacy Expectations for Local and\n  Central Differential Privacy",
    "summary" : "Differential privacy is a popular privacy-enhancing technology that has been\ndeployed both in industry and government agencies. Unfortunately, existing\nexplanations of differential privacy fail to set accurate privacy expectations\nfor data subjects, which depend on the choice of deployment model. We design\nand evaluate new explanations of differential privacy for the local and central\nmodels, drawing inspiration from prior work explaining other privacy-enhancing\ntechnologies. We find that consequences-focused explanations in the style of\nprivacy nutrition labels that lay out the implications of differential privacy\nare a promising approach for setting accurate privacy expectations. Further, we\nfind that while process-focused explanations are not enough to set accurate\nprivacy expectations, combining consequences-focused explanations with a brief\ndescription of how differential privacy works leads to greater trust.",
    "updated" : "2024-08-19T01:04:07Z",
    "published" : "2024-08-16T01:21:57Z",
    "authors" : [
      {
        "name" : "Mary Anne Smart"
      },
      {
        "name" : "Priyanka Nanayakkara"
      },
      {
        "name" : "Rachel Cummings"
      },
      {
        "name" : "Gabriel Kaptchuk"
      },
      {
        "name" : "Elissa Redmiles"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08909v1",
    "title" : "An Adaptive Differential Privacy Method Based on Federated Learning",
    "summary" : "Differential privacy is one of the methods to solve the problem of privacy\nprotection in federated learning. Setting the same privacy budget for each\nround will result in reduced accuracy in training. The existing methods of the\nadjustment of privacy budget consider fewer influencing factors and tend to\nignore the boundaries, resulting in unreasonable privacy budgets. Therefore, we\nproposed an adaptive differential privacy method based on federated learning.\nThe method sets the adjustment coefficient and scoring function according to\naccuracy, loss, training rounds, and the number of datasets and clients. And\nthe privacy budget is adjusted based on them. Then the local model update is\nprocessed according to the scaling factor and the noise. Fi-nally, the server\naggregates the noised local model update and distributes the noised global\nmodel. The range of parameters and the privacy of the method are analyzed.\nThrough the experimental evaluation, it can reduce the privacy budget by about\n16%, while the accuracy remains roughly the same.",
    "updated" : "2024-08-13T13:08:11Z",
    "published" : "2024-08-13T13:08:11Z",
    "authors" : [
      {
        "name" : "Zhiqiang Wang"
      },
      {
        "name" : "Xinyue Yu"
      },
      {
        "name" : "Qianli Huang"
      },
      {
        "name" : "Yongguang Gong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08904v1",
    "title" : "Privacy in Federated Learning",
    "summary" : "Federated Learning (FL) represents a significant advancement in distributed\nmachine learning, enabling multiple participants to collaboratively train\nmodels without sharing raw data. This decentralized approach enhances privacy\nby keeping data on local devices. However, FL introduces new privacy\nchallenges, as model updates shared during training can inadvertently leak\nsensitive information. This chapter delves into the core privacy concerns\nwithin FL, including the risks of data reconstruction, model inversion attacks,\nand membership inference. It explores various privacy-preserving techniques,\nsuch as Differential Privacy (DP) and Secure Multi-Party Computation (SMPC),\nwhich are designed to mitigate these risks. The chapter also examines the\ntrade-offs between model accuracy and privacy, emphasizing the importance of\nbalancing these factors in practical implementations. Furthermore, it discusses\nthe role of regulatory frameworks, such as GDPR, in shaping the privacy\nstandards for FL. By providing a comprehensive overview of the current state of\nprivacy in FL, this chapter aims to equip researchers and practitioners with\nthe knowledge necessary to navigate the complexities of secure federated\nlearning environments. The discussion highlights both the potential and\nlimitations of existing privacy-enhancing techniques, offering insights into\nfuture research directions and the development of more robust solutions.",
    "updated" : "2024-08-12T18:41:58Z",
    "published" : "2024-08-12T18:41:58Z",
    "authors" : [
      {
        "name" : "Jaydip Sen"
      },
      {
        "name" : "Hetvi Waghela"
      },
      {
        "name" : "Sneha Rakshit"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01228v2",
    "title" : "The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models",
    "summary" : "Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.",
    "updated" : "2024-08-19T13:35:05Z",
    "published" : "2024-08-02T12:36:13Z",
    "authors" : [
      {
        "name" : "Simone Caldarella"
      },
      {
        "name" : "Massimiliano Mancini"
      },
      {
        "name" : "Elisa Ricci"
      },
      {
        "name" : "Rahaf Aljundi"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10715v1",
    "title" : "Fine-Tuning a Local LLaMA-3 Large Language Model for Automated\n  Privacy-Preserving Physician Letter Generation in Radiation Oncology",
    "summary" : "Generating physician letters is a time-consuming task in daily clinical\npractice. This study investigates local fine-tuning of large language models\n(LLMs), specifically LLaMA models, for physician letter generation in a\nprivacy-preserving manner within the field of radiation oncology. Our findings\ndemonstrate that base LLaMA models, without fine-tuning, are inadequate for\neffectively generating physician letters. The QLoRA algorithm provides an\nefficient method for local intra-institutional fine-tuning of LLMs with limited\ncomputational resources (i.e., a single 48 GB GPU workstation within the\nhospital). The fine-tuned LLM successfully learns radiation oncology-specific\ninformation and generates physician letters in an institution-specific style.\nROUGE scores of the generated summary reports highlight the superiority of the\n8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician\nevaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has\nlimited capacity to generate content beyond the provided input data, it\nsuccessfully generates salutations, diagnoses and treatment histories,\nrecommendations for further treatment, and planned schedules. Overall, clinical\nbenefit was rated highly by the clinical experts (average score of 3.44 on a\n4-point scale). With careful physician review and correction, automated\nLLM-based physician letter generation has significant practical value.",
    "updated" : "2024-08-20T10:31:36Z",
    "published" : "2024-08-20T10:31:36Z",
    "authors" : [
      {
        "name" : "Yihao Hou"
      },
      {
        "name" : "Christoph Bert"
      },
      {
        "name" : "Ahmed Gomaa"
      },
      {
        "name" : "Godehard Lahmer"
      },
      {
        "name" : "Daniel Hoefler"
      },
      {
        "name" : "Thomas Weissmann"
      },
      {
        "name" : "Raphaela Voigt"
      },
      {
        "name" : "Philipp Schubert"
      },
      {
        "name" : "Charlotte Schmitter"
      },
      {
        "name" : "Alina Depardon"
      },
      {
        "name" : "Sabine Semrau"
      },
      {
        "name" : "Andreas Maier"
      },
      {
        "name" : "Rainer Fietkau"
      },
      {
        "name" : "Yixing Huang"
      },
      {
        "name" : "Florian Putz"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10648v1",
    "title" : "Smart Contract Coordinated Privacy Preserving Crowd-Sensing Campaigns",
    "summary" : "Crowd-sensing has emerged as a powerful data retrieval model, enabling\ndiverse applications by leveraging active user participation. However, data\navailability and privacy concerns pose significant challenges. Traditional\nmethods like data encryption and anonymization, while essential, may not fully\naddress these issues. For instance, in sparsely populated areas, anonymized\ndata can still be traced back to individual users. Additionally, the volume of\ndata generated by users can reveal their identities. To develop credible\ncrowd-sensing systems, data must be anonymized, aggregated and separated into\nuniformly sized chunks. Furthermore, decentralizing the data management\nprocess, rather than relying on a single server, can enhance security and\ntrust. This paper proposes a system utilizing smart contracts and blockchain\ntechnologies to manage crowd-sensing campaigns. The smart contract handles user\nsubscriptions, data encryption, and decentralized storage, creating a secure\ndata marketplace. Incentive policies within the smart contract encourage user\nparticipation and data diversity. Simulation results confirm the system's\nviability, highlighting the importance of user participation for data\ncredibility and the impact of geographical data scarcity on rewards. This\napproach aims to balance data origin and reduce cheating risks.",
    "updated" : "2024-08-20T08:41:57Z",
    "published" : "2024-08-20T08:41:57Z",
    "authors" : [
      {
        "name" : "Luca Bedogni"
      },
      {
        "name" : "Stefano Ferretti"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10647v1",
    "title" : "Privacy-preserving Universal Adversarial Defense for Black-box Models",
    "summary" : "Deep neural networks (DNNs) are increasingly used in critical applications\nsuch as identity authentication and autonomous driving, where robustness\nagainst adversarial attacks is crucial. These attacks can exploit minor\nperturbations to cause significant prediction errors, making it essential to\nenhance the resilience of DNNs. Traditional defense methods often rely on\naccess to detailed model information, which raises privacy concerns, as model\nowners may be reluctant to share such data. In contrast, existing black-box\ndefense methods fail to offer a universal defense against various types of\nadversarial attacks. To address these challenges, we introduce DUCD, a\nuniversal black-box defense method that does not require access to the target\nmodel's parameters or architecture. Our approach involves distilling the target\nmodel by querying it with data, creating a white-box surrogate while preserving\ndata privacy. We further enhance this surrogate model using a certified defense\nbased on randomized smoothing and optimized noise selection, enabling robust\ndefense against a broad range of adversarial attacks. Comparative evaluations\nbetween the certified defenses of the surrogate and target models demonstrate\nthe effectiveness of our approach. Experiments on multiple image classification\ndatasets show that DUCD not only outperforms existing black-box defenses but\nalso matches the accuracy of white-box defenses, all while enhancing data\nprivacy and reducing the success rate of membership inference attacks.",
    "updated" : "2024-08-20T08:40:39Z",
    "published" : "2024-08-20T08:40:39Z",
    "authors" : [
      {
        "name" : "Qiao Li"
      },
      {
        "name" : "Cong Wu"
      },
      {
        "name" : "Jing Chen"
      },
      {
        "name" : "Zijun Zhang"
      },
      {
        "name" : "Kun He"
      },
      {
        "name" : "Ruiying Du"
      },
      {
        "name" : "Xinxin Wang"
      },
      {
        "name" : "Qingchuang Zhao"
      },
      {
        "name" : "Yang Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "I.2.10"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10468v1",
    "title" : "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
    "summary" : "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96\\% to 73.71\\% on the PII-E\ndataset and 3.21\\% to 45.93\\% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
    "updated" : "2024-08-20T00:40:49Z",
    "published" : "2024-08-20T00:40:49Z",
    "authors" : [
      {
        "name" : "Jinxin Liu"
      },
      {
        "name" : "Zao Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10442v1",
    "title" : "Feasibility of assessing cognitive impairment via distributed camera\n  network and privacy-preserving edge computing",
    "summary" : "INTRODUCTION: Mild cognitive impairment (MCI) is characterized by a decline\nin cognitive functions beyond typical age and education-related expectations.\nSince, MCI has been linked to reduced social interactions and increased aimless\nmovements, we aimed to automate the capture of these behaviors to enhance\nlongitudinal monitoring.\n  METHODS: Using a privacy-preserving distributed camera network, we collected\nmovement and social interaction data from groups of individuals with MCI\nundergoing therapy within a 1700$m^2$ space. We developed movement and social\ninteraction features, which were then used to train a series of machine\nlearning algorithms to distinguish between higher and lower cognitive\nfunctioning MCI groups.\n  RESULTS: A Wilcoxon rank-sum test revealed statistically significant\ndifferences between high and low-functioning cohorts in features such as linear\npath length, walking speed, change in direction while walking, entropy of\nvelocity and direction change, and number of group formations in the indoor\nspace. Despite lacking individual identifiers to associate with specific levels\nof MCI, a machine learning approach using the most significant features\nprovided a 71% accuracy.\n  DISCUSSION: We provide evidence to show that a privacy-preserving low-cost\ncamera network using edge computing framework has the potential to distinguish\nbetween different levels of cognitive impairment from the movements and social\ninteractions captured during group activities.",
    "updated" : "2024-08-19T22:34:43Z",
    "published" : "2024-08-19T22:34:43Z",
    "authors" : [
      {
        "name" : "Chaitra Hegde"
      },
      {
        "name" : "Yashar Kiarashi"
      },
      {
        "name" : "Allan I Levey"
      },
      {
        "name" : "Amy D Rodriguez"
      },
      {
        "name" : "Hyeokhyen Kwon"
      },
      {
        "name" : "Gari D Clifford"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.11649v1",
    "title" : "Video-to-Text Pedestrian Monitoring (VTPM): Leveraging Computer Vision\n  and Large Language Models for Privacy-Preserve Pedestrian Activity Monitoring\n  at Intersections",
    "summary" : "Computer vision has advanced research methodologies, enhancing system\nservices across various fields. It is a core component in traffic monitoring\nsystems for improving road safety; however, these monitoring systems don't\npreserve the privacy of pedestrians who appear in the videos, potentially\nrevealing their identities. Addressing this issue, our paper introduces\nVideo-to-Text Pedestrian Monitoring (VTPM), which monitors pedestrian movements\nat intersections and generates real-time textual reports, including traffic\nsignal and weather information. VTPM uses computer vision models for pedestrian\ndetection and tracking, achieving a latency of 0.05 seconds per video frame.\nAdditionally, it detects crossing violations with 90.2% accuracy by\nincorporating traffic signal data. The proposed framework is equipped with\nPhi-3 mini-4k to generate real-time textual reports of pedestrian activity\nwhile stating safety concerns like crossing violations, conflicts, and the\nimpact of weather on their behavior with latency of 0.33 seconds. To enhance\ncomprehensive analysis of the generated textual reports, Phi-3 medium is\nfine-tuned for historical analysis of these generated textual reports. This\nfine-tuning enables more reliable analysis about the pedestrian safety at\nintersections, effectively detecting patterns and safety critical events. The\nproposed VTPM offers a more efficient alternative to video footage by using\ntextual reports reducing memory usage, saving up to 253 million percent,\neliminating privacy issues, and enabling comprehensive interactive historical\nanalysis.",
    "updated" : "2024-08-21T14:21:53Z",
    "published" : "2024-08-21T14:21:53Z",
    "authors" : [
      {
        "name" : "Ahmed S. Abdelrahman"
      },
      {
        "name" : "Mohamed Abdel-Aty"
      },
      {
        "name" : "Dongdong Wang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.11290v1",
    "title" : "Privacy Preservation in Delay-Based Localization Systems: Artificial\n  Noise or Artificial Multipath?",
    "summary" : "Localization plays an increasingly pivotal role in 5G/6G systems, enabling\nvarious applications. This paper focuses on the privacy concerns associated\nwith delay-based localization, where unauthorized base stations attempt to\ninfer the location of the end user. We propose a method to disrupt localization\nat unauthorized nodes by injecting artificial components into the pilot signal,\nexploiting model mismatches inherent in these nodes. Specifically, we\ninvestigate the effectiveness of two techniques, namely artificial multipath\n(AM) and artificial noise (AN), in mitigating location leakage. By leveraging\nthe misspecified Cram\\'er-Rao bound framework, we evaluate the impact of these\ntechniques on unauthorized localization performance. Our results demonstrate\nthat pilot manipulation significantly degrades the accuracy of unauthorized\nlocalization while minimally affecting legitimate localization. Moreover, we\nfind that the superiority of AM over AN varies depending on the specific\nscenario.",
    "updated" : "2024-08-21T02:38:44Z",
    "published" : "2024-08-21T02:38:44Z",
    "authors" : [
      {
        "name" : "Yuchen Zhang"
      },
      {
        "name" : "Hui Chen"
      },
      {
        "name" : "Henk Wymeersch"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.11263v1",
    "title" : "Privacy-Preserving Data Management using Blockchains",
    "summary" : "Privacy-preservation policies are guidelines formulated to protect data\nproviders private data. Previous privacy-preservation methodologies have\naddressed privacy in which data are permanently stored in repositories and\ndisconnected from changing data provider privacy preferences. This occurrence\nbecomes evident as data moves to another data repository. Hence, the need for\ndata providers to control and flexibly update their existing privacy\npreferences due to changing data usage continues to remain a problem. This\npaper proposes a blockchain-based methodology for preserving data providers\nprivate and sensitive data. The research proposes to tightly couple data\nproviders private attribute data element to privacy preferences and data\naccessor data element into a privacy tuple. The implementation presents a\nframework of tightly-coupled relational database and blockchains. This delivers\nsecure, tamper-resistant, and query-efficient platform for data management and\nquery processing. The evaluation analysis from the implementation validates\nefficient query processing of privacy-aware queries on the privacy\ninfrastructure.",
    "updated" : "2024-08-21T01:10:39Z",
    "published" : "2024-08-21T01:10:39Z",
    "authors" : [
      {
        "name" : "Michael Mireku Kwakye"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10468v2",
    "title" : "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
    "summary" : "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96\\% to 73.71\\% on the PII-E\ndataset and 3.21\\% to 45.93\\% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
    "updated" : "2024-08-21T14:35:48Z",
    "published" : "2024-08-20T00:40:49Z",
    "authors" : [
      {
        "name" : "Jinxin Liu"
      },
      {
        "name" : "Zao Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.12385v1",
    "title" : "Sharper Bounds for Chebyshev Moment Matching with Applications to\n  Differential Privacy and Beyond",
    "summary" : "We study the problem of approximately recovering a probability distribution\ngiven noisy measurements of its Chebyshev polynomial moments. We sharpen prior\nwork, proving that accurate recovery in the Wasserstein distance is possible\nwith more noise than previously known.\n  As a main application, our result yields a simple \"linear query\" algorithm\nfor constructing a differentially private synthetic data distribution with\nWasserstein-1 error $\\tilde{O}(1/n)$ based on a dataset of $n$ points in\n$[-1,1]$. This bound is optimal up to log factors and matches a recent\nbreakthrough of Boedihardjo, Strohmer, and Vershynin [Probab. Theory. Rel.,\n2024], which uses a more complex \"superregular random walk\" method to beat an\n$O(1/\\sqrt{n})$ accuracy barrier inherent to earlier approaches.\n  We illustrate a second application of our new moment-based recovery bound in\nnumerical linear algebra: by improving an approach of Braverman, Krishnan, and\nMusco [STOC 2022], our result yields a faster algorithm for estimating the\nspectral density of a symmetric matrix up to small error in the Wasserstein\ndistance.",
    "updated" : "2024-08-22T13:26:41Z",
    "published" : "2024-08-22T13:26:41Z",
    "authors" : [
      {
        "name" : "Cameron Musco"
      },
      {
        "name" : "Christopher Musco"
      },
      {
        "name" : "Lucas Rosenblatt"
      },
      {
        "name" : "Apoorv Vikram Singh"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.12353v1",
    "title" : "Distributed quasi-Newton robust estimation under differential privacy",
    "summary" : "For distributed computing with Byzantine machines under Privacy Protection\n(PP) constraints, this paper develops a robust PP distributed quasi-Newton\nestimation, which only requires the node machines to transmit five vectors to\nthe central processor with high asymptotic relative efficiency. Compared with\nthe gradient descent strategy which requires more rounds of transmission and\nthe Newton iteration strategy which requires the entire Hessian matrix to be\ntransmitted, the novel quasi-Newton iteration has advantages in reducing\nprivacy budgeting and transmission cost. Moreover, our PP algorithm does not\ndepend on the boundedness of gradients and second-order derivatives. When\ngradients and second-order derivatives follow sub-exponential distributions, we\noffer a mechanism that can ensure PP with a sufficiently high probability.\nFurthermore, this novel estimator can achieve the optimal convergence rate and\nthe asymptotic normality. The numerical studies on synthetic and real data sets\nevaluate the performance of the proposed algorithm.",
    "updated" : "2024-08-22T12:51:28Z",
    "published" : "2024-08-22T12:51:28Z",
    "authors" : [
      {
        "name" : "Chuhan Wang"
      },
      {
        "name" : "Lixing Zhu"
      },
      {
        "name" : "Xuehu Zhu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.12010v1",
    "title" : "Confounding Privacy and Inverse Composition",
    "summary" : "We introduce a novel privacy notion of ($\\epsilon, \\delta$)-confounding\nprivacy that generalizes both differential privacy and Pufferfish privacy. In\ndifferential privacy, sensitive information is contained in the dataset while\nin Pufferfish privacy, sensitive information determines data distribution.\nConsequently, both assume a chain-rule relationship between the sensitive\ninformation and the output of privacy mechanisms. Confounding privacy, in\ncontrast, considers general causal relationships between the dataset and\nsensitive information. One of the key properties of differential privacy is\nthat it can be easily composed over multiple interactions with the mechanism\nthat maps private data to publicly shared information. In contrast, we show\nthat the quantification of the privacy loss under the composition of\nindependent ($\\epsilon, \\delta$)-confounding private mechanisms using the\noptimal composition of differential privacy \\emph{underestimates} true privacy\nloss. To address this, we characterize an inverse composition framework to\ntightly implement a target global ($\\epsilon_{g}, \\delta_{g}$)-confounding\nprivacy under composition while keeping individual mechanisms independent and\nprivate. In particular, we propose a novel copula-perturbation method which\nensures that (1) each individual mechanism $i$ satisfies a target local\n($\\epsilon_{i}, \\delta_{i}$)-confounding privacy and (2) the target global\n($\\epsilon_{g}, \\delta_{g}$)-confounding privacy is tightly implemented by\nsolving an optimization problem. Finally, we study inverse composition\nempirically on real datasets.",
    "updated" : "2024-08-21T21:45:13Z",
    "published" : "2024-08-21T21:45:13Z",
    "authors" : [
      {
        "name" : "Tao Zhang"
      },
      {
        "name" : "Bradley A. Malin"
      },
      {
        "name" : "Netanel Raviv"
      },
      {
        "name" : "Yevgeniy Vorobeychik"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.12387v1",
    "title" : "Makeup-Guided Facial Privacy Protection via Untrained Neural Network\n  Priors",
    "summary" : "Deep learning-based face recognition (FR) systems pose significant privacy\nrisks by tracking users without their consent. While adversarial attacks can\nprotect privacy, they often produce visible artifacts compromising user\nexperience. To mitigate this issue, recent facial privacy protection approaches\nadvocate embedding adversarial noise into the natural looking makeup styles.\nHowever, these methods require training on large-scale makeup datasets that are\nnot always readily available. In addition, these approaches also suffer from\ndataset bias. For instance, training on makeup data that predominantly contains\nfemale faces could compromise protection efficacy for male faces. To handle\nthese issues, we propose a test-time optimization approach that solely\noptimizes an untrained neural network to transfer makeup style from a reference\nto a source image in an adversarial manner. We introduce two key modules: a\ncorrespondence module that aligns regions between reference and source images\nin latent space, and a decoder with conditional makeup layers. The untrained\ndecoder, optimized via carefully designed structural and makeup consistency\nlosses, generates a protected image that resembles the source but incorporates\nadversarial makeup to deceive FR models. As our approach does not rely on\ntraining with makeup face datasets, it avoids potential male/female dataset\nbiases while providing effective protection. We further extend the proposed\napproach to videos by leveraging on temporal correlations. Experiments on\nbenchmark datasets demonstrate superior performance in face verification and\nidentification tasks and effectiveness against commercial FR systems. Our code\nand models will be available at\nhttps://github.com/fahadshamshad/deep-facial-privacy-prior",
    "updated" : "2024-08-20T17:59:39Z",
    "published" : "2024-08-20T17:59:39Z",
    "authors" : [
      {
        "name" : "Fahad Shamshad"
      },
      {
        "name" : "Muzammal Naseer"
      },
      {
        "name" : "Karthik Nandakumar"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.13038v1",
    "title" : "Improving the Classification Effect of Clinical Images of Diseases for\n  Multi-Source Privacy Protection",
    "summary" : "Privacy data protection in the medical field poses challenges to data\nsharing, limiting the ability to integrate data across hospitals for training\nhigh-precision auxiliary diagnostic models. Traditional centralized training\nmethods are difficult to apply due to violations of privacy protection\nprinciples. Federated learning, as a distributed machine learning framework,\nhelps address this issue, but it requires multiple hospitals to participate in\ntraining simultaneously, which is hard to achieve in practice. To address these\nchallenges, we propose a medical privacy data training framework based on data\nvectors. This framework allows each hospital to fine-tune pre-trained models on\nprivate data, calculate data vectors (representing the optimization direction\nof model parameters in the solution space), and sum them up to generate\nsynthetic weights that integrate model information from multiple hospitals.\nThis approach enhances model performance without exchanging private data or\nrequiring synchronous training. Experimental results demonstrate that this\nmethod effectively utilizes dispersed private data resources while protecting\npatient privacy. The auxiliary diagnostic model trained using this approach\nsignificantly outperforms models trained independently by a single hospital,\nproviding a new perspective for resolving the conflict between medical data\nprivacy protection and model training and advancing the development of medical\nintelligence.",
    "updated" : "2024-08-23T12:52:24Z",
    "published" : "2024-08-23T12:52:24Z",
    "authors" : [
      {
        "name" : "Tian Bowen"
      },
      {
        "name" : "Xu Zhengyang"
      },
      {
        "name" : "Yin Zhihao"
      },
      {
        "name" : "Wang Jingying"
      },
      {
        "name" : "Yue Yutao"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.12787v1",
    "title" : "LLM-PBE: Assessing Data Privacy in Large Language Models",
    "summary" : "Large Language Models (LLMs) have become integral to numerous domains,\nsignificantly advancing applications in data management, mining, and analysis.\nTheir profound capabilities in processing and interpreting complex language\ndata, however, bring to light pressing concerns regarding data privacy,\nespecially the risk of unintentional training data leakage. Despite the\ncritical nature of this issue, there has been no existing literature to offer a\ncomprehensive assessment of data privacy risks in LLMs. Addressing this gap,\nour paper introduces LLM-PBE, a toolkit crafted specifically for the systematic\nevaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze\nprivacy across the entire lifecycle of LLMs, incorporating diverse attack and\ndefense strategies, and handling various data types and metrics. Through\ndetailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth\nexploration of data privacy concerns, shedding light on influential factors\nsuch as model size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in LLMs but\nalso serves as a vital resource for future research in the field. Aimed at\nenhancing the breadth of knowledge in this area, the findings, resources, and\nour full technical report are made available at https://llm-pbe.github.io/,\nproviding an open platform for academic and practical advancements in LLM\nprivacy assessment.",
    "updated" : "2024-08-23T01:37:29Z",
    "published" : "2024-08-23T01:37:29Z",
    "authors" : [
      {
        "name" : "Qinbin Li"
      },
      {
        "name" : "Junyuan Hong"
      },
      {
        "name" : "Chulin Xie"
      },
      {
        "name" : "Jeffrey Tan"
      },
      {
        "name" : "Rachel Xin"
      },
      {
        "name" : "Junyi Hou"
      },
      {
        "name" : "Xavier Yin"
      },
      {
        "name" : "Zhun Wang"
      },
      {
        "name" : "Dan Hendrycks"
      },
      {
        "name" : "Zhangyang Wang"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Bingsheng He"
      },
      {
        "name" : "Dawn Song"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07892v2",
    "title" : "Personhood credentials: Artificial intelligence and the value of\n  privacy-preserving tools to distinguish who is real online",
    "summary" : "Anonymity is an important principle online. However, malicious actors have\nlong used misleading identities to conduct fraud, spread disinformation, and\ncarry out other deceptive schemes. With the advent of increasingly capable AI,\nbad actors can amplify the potential scale and effectiveness of their\noperations, intensifying the challenge of balancing anonymity and\ntrustworthiness online. In this paper, we analyze the value of a new tool to\naddress this challenge: \"personhood credentials\" (PHCs), digital credentials\nthat empower users to demonstrate that they are real people -- not AIs -- to\nonline services, without disclosing any personal information. Such credentials\ncan be issued by a range of trusted institutions -- governments or otherwise. A\nPHC system, according to our definition, could be local or global, and does not\nneed to be biometrics-based. Two trends in AI contribute to the urgency of the\nchallenge: AI's increasing indistinguishability from people online (i.e.,\nlifelike content and avatars, agentic activity), and AI's increasing\nscalability (i.e., cost-effectiveness, accessibility). Drawing on a long\nhistory of research into anonymous credentials and \"proof-of-personhood\"\nsystems, personhood credentials give people a way to signal their\ntrustworthiness on online platforms, and offer service providers new tools for\nreducing misuse by bad actors. In contrast, existing countermeasures to\nautomated deception -- such as CAPTCHAs -- are inadequate against sophisticated\nAI, while stringent identity verification solutions are insufficiently private\nfor many use-cases. After surveying the benefits of personhood credentials, we\nalso examine deployment risks and design challenges. We conclude with\nactionable next steps for policymakers, technologists, and standards bodies to\nconsider in consultation with the public.",
    "updated" : "2024-08-23T00:38:34Z",
    "published" : "2024-08-15T02:41:25Z",
    "authors" : [
      {
        "name" : "Steven Adler"
      },
      {
        "name" : "Zoë Hitzig"
      },
      {
        "name" : "Shrey Jain"
      },
      {
        "name" : "Catherine Brewer"
      },
      {
        "name" : "Wayne Chang"
      },
      {
        "name" : "Renée DiResta"
      },
      {
        "name" : "Eddy Lazzarin"
      },
      {
        "name" : "Sean McGregor"
      },
      {
        "name" : "Wendy Seltzer"
      },
      {
        "name" : "Divya Siddarth"
      },
      {
        "name" : "Nouran Soliman"
      },
      {
        "name" : "Tobin South"
      },
      {
        "name" : "Connor Spelliscy"
      },
      {
        "name" : "Manu Sporny"
      },
      {
        "name" : "Varya Srivastava"
      },
      {
        "name" : "John Bailey"
      },
      {
        "name" : "Brian Christian"
      },
      {
        "name" : "Andrew Critch"
      },
      {
        "name" : "Ronnie Falcon"
      },
      {
        "name" : "Heather Flanagan"
      },
      {
        "name" : "Kim Hamilton Duffy"
      },
      {
        "name" : "Eric Ho"
      },
      {
        "name" : "Claire R. Leibowicz"
      },
      {
        "name" : "Srikanth Nadhamuni"
      },
      {
        "name" : "Alan Z. Rozenshtein"
      },
      {
        "name" : "David Schnurr"
      },
      {
        "name" : "Evan Shapiro"
      },
      {
        "name" : "Lacey Strahm"
      },
      {
        "name" : "Andrew Trask"
      },
      {
        "name" : "Zoe Weinberg"
      },
      {
        "name" : "Cedric Whitney"
      },
      {
        "name" : "Tom Zick"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.14329v1",
    "title" : "PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection\n  Dataset",
    "summary" : "PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection\ndataset. By removing pixel information and providing only de-identified human\nannotations, PHEVA safeguards personally identifiable information. The dataset\nincludes seven indoor/outdoor scenes, featuring one novel, context-specific\ncamera, and offers over 5x the pose-annotated frames compared to the largest\nprevious dataset. This study benchmarks state-of-the-art methods on PHEVA using\na comprehensive set of metrics, including the 10% Error Rate (10ER), a metric\nused for anomaly detection for the first time providing insights relevant to\nreal-world deployment. As the first of its kind, PHEVA bridges the gap between\nconventional training and real-world deployment by introducing continual\nlearning benchmarks, with models outperforming traditional methods in 82.14% of\ncases. The dataset is publicly available at\nhttps://github.com/TeCSAR-UNCC/PHEVA.git.",
    "updated" : "2024-08-26T14:55:23Z",
    "published" : "2024-08-26T14:55:23Z",
    "authors" : [
      {
        "name" : "Ghazal Alinezhad Noghre"
      },
      {
        "name" : "Shanle Yao"
      },
      {
        "name" : "Armin Danesh Pazho"
      },
      {
        "name" : "Babak Rahimi Ardabili"
      },
      {
        "name" : "Vinit Katariya"
      },
      {
        "name" : "Hamed Tabkhi"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.13460v1",
    "title" : "DOPPLER: Differentially Private Optimizers with Low-pass Filter for\n  Privacy Noise Reduction",
    "summary" : "Privacy is a growing concern in modern deep-learning systems and\napplications. Differentially private (DP) training prevents the leakage of\nsensitive information in the collected training data from the trained machine\nlearning models. DP optimizers, including DP stochastic gradient descent\n(DPSGD) and its variants, privatize the training procedure by gradient clipping\nand DP noise injection. However, in practice, DP models trained using DPSGD and\nits variants often suffer from significant model performance degradation. Such\ndegradation prevents the application of DP optimization in many key tasks, such\nas foundation model pretraining. In this paper, we provide a novel signal\nprocessing perspective to the design and analysis of DP optimizers. We show\nthat a ``frequency domain'' operation called low-pass filtering can be used to\neffectively reduce the impact of DP noise. More specifically, by defining the\n``frequency domain'' for both the gradient and differential privacy (DP) noise,\nwe have developed a new component, called DOPPLER. This component is designed\nfor DP algorithms and works by effectively amplifying the gradient while\nsuppressing DP noise within this frequency domain. As a result, it maintains\nprivacy guarantees and enhances the quality of the DP-protected model. Our\nexperiments show that the proposed DP optimizers with a low-pass filter\noutperform their counterparts without the filter by 3%-10% in test accuracy on\nvarious models and datasets. Both theoretical and practical evidence suggest\nthat the DOPPLER is effective in closing the gap between DP and non-DP\ntraining.",
    "updated" : "2024-08-24T04:27:07Z",
    "published" : "2024-08-24T04:27:07Z",
    "authors" : [
      {
        "name" : "Xinwei Zhang"
      },
      {
        "name" : "Zhiqi Bu"
      },
      {
        "name" : "Mingyi Hong"
      },
      {
        "name" : "Meisam Razaviyayn"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.13424v1",
    "title" : "Enabling Humanitarian Applications with Targeted Differential Privacy",
    "summary" : "The proliferation of mobile phones in low- and middle-income countries has\nsuddenly and dramatically increased the extent to which the world's poorest and\nmost vulnerable populations can be observed and tracked by governments and\ncorporations. Millions of historically \"off the grid\" individuals are now\npassively generating digital data; these data, in turn, are being used to make\nlife-altering decisions about those individuals -- including whether or not\nthey receive government benefits, and whether they qualify for a consumer loan.\n  This paper develops an approach to implementing algorithmic decisions based\non personal data, while also providing formal privacy guarantees to data\nsubjects. The approach adapts differential privacy to applications that require\ndecisions about individuals, and gives decision makers granular control over\nthe level of privacy guaranteed to data subjects. We show that stronger privacy\nguarantees typically come at some cost, and use data from two real-world\napplications -- an anti-poverty program in Togo and a consumer lending platform\nin Nigeria -- to illustrate those costs. Our empirical results quantify the\ntradeoff between privacy and predictive accuracy, and characterize how\ndifferent privacy guarantees impact overall program effectiveness. More\nbroadly, our results demonstrate a way for humanitarian programs to responsibly\nuse personal data, and better equip program designers to make informed\ndecisions about data privacy.",
    "updated" : "2024-08-24T01:34:37Z",
    "published" : "2024-08-24T01:34:37Z",
    "authors" : [
      {
        "name" : "Nitin Kohli"
      },
      {
        "name" : "Joshua Blumenstock"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10468v3",
    "title" : "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
    "summary" : "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
    "updated" : "2024-08-26T17:28:23Z",
    "published" : "2024-08-20T00:40:49Z",
    "authors" : [
      {
        "name" : "Jinxin Liu"
      },
      {
        "name" : "Zao Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.09943v2",
    "title" : "Calibrating Noise for Group Privacy in Subsampled Mechanisms",
    "summary" : "Given a group size m and a sensitive dataset D, group privacy (GP) releases\ninformation about D with the guarantee that the adversary cannot infer with\nhigh confidence whether the underlying data is D or a neighboring dataset D'\nthat differs from D by m records. GP generalizes the well-established notion of\ndifferential privacy (DP) for protecting individuals' privacy; in particular,\nwhen m=1, GP reduces to DP. Compared to DP, GP is capable of protecting the\nsensitive aggregate information of a group of up to m individuals, e.g., the\naverage annual income among members of a yacht club. Despite its longstanding\npresence in the research literature and its promising applications, GP is often\ntreated as an afterthought, with most approaches first developing a DP\nmechanism and then using a generic conversion to adapt it for GP, treating the\nDP solution as a black box. As we point out in the paper, this methodology is\nsuboptimal when the underlying DP solution involves subsampling, e.g., in the\nclassic DP-SGD method for training deep learning models. In this case, the\nDP-to-GP conversion is overly pessimistic in its analysis, leading to low\nutility in the published results under GP.\n  Motivated by this, we propose a novel analysis framework that provides tight\nprivacy accounting for subsampled GP mechanisms. Instead of converting a\nblack-box DP mechanism to GP, our solution carefully analyzes and utilizes the\ninherent randomness in subsampled mechanisms, leading to a substantially\nimproved bound on the privacy loss with respect to GP. The proposed solution\napplies to a wide variety of foundational mechanisms with subsampling.\nExtensive experiments with real datasets demonstrate that compared to the\nbaseline convert-from-blackbox-DP approach, our GP mechanisms achieve noise\nreductions of over an order of magnitude in several practical settings,\nincluding deep neural network training.",
    "updated" : "2024-08-24T13:00:41Z",
    "published" : "2024-08-19T12:32:50Z",
    "authors" : [
      {
        "name" : "Yangfan Jiang"
      },
      {
        "name" : "Xinjian Luo"
      },
      {
        "name" : "Yin Yang"
      },
      {
        "name" : "Xiaokui Xiao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.15077v1",
    "title" : "MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of\n  Children with Autism Spectrum Disorder",
    "summary" : "Autism spectrum disorder (ASD) is characterized by significant challenges in\nsocial interaction and comprehending communication signals. Recently,\ntherapeutic interventions for ASD have increasingly utilized Deep learning\npowered-computer vision techniques to monitor individual progress over time.\nThese models are trained on private, non-public datasets from the autism\ncommunity, creating challenges in comparing results across different models due\nto privacy-preserving data-sharing issues. This work introduces MMASD+. MMASD+\nconsists of diverse data modalities, including 3D-Skeleton, 3D Body Mesh, and\nOptical Flow data. It integrates the capabilities of Yolov8 and Deep SORT\nalgorithms to distinguish between the therapist and children, addressing a\nsignificant barrier in the original dataset. Additionally, a Multimodal\nTransformer framework is proposed to predict 11 action types and the presence\nof ASD. This framework achieves an accuracy of 95.03% for predicting action\ntypes and 96.42% for predicting ASD presence, demonstrating over a 10%\nimprovement compared to models trained on single data modalities. These\nfindings highlight the advantages of integrating multiple data modalities\nwithin the Multimodal Transformer framework.",
    "updated" : "2024-08-27T14:05:48Z",
    "published" : "2024-08-27T14:05:48Z",
    "authors" : [
      {
        "name" : "Pavan Uttej Ravva"
      },
      {
        "name" : "Behdokht Kiafar"
      },
      {
        "name" : "Pinar Kullu"
      },
      {
        "name" : "Jicheng Li"
      },
      {
        "name" : "Anjana Bhat"
      },
      {
        "name" : "Roghayeh Leila Barmaki"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.14830v1",
    "title" : "PolicyLR: A Logic Representation For Privacy Policies",
    "summary" : "Privacy policies are crucial in the online ecosystem, defining how services\nhandle user data and adhere to regulations such as GDPR and CCPA. However,\ntheir complexity and frequent updates often make them difficult for\nstakeholders to understand and analyze. Current automated analysis methods,\nwhich utilize natural language processing, have limitations. They typically\nfocus on individual tasks and fail to capture the full context of the policies.\nWe propose PolicyLR, a new paradigm that offers a comprehensive\nmachine-readable representation of privacy policies, serving as an all-in-one\nsolution for multiple downstream tasks. PolicyLR converts privacy policies into\na machine-readable format using valuations of atomic formulae, allowing for\nformal definitions of tasks like compliance and consistency. We have developed\na compiler that transforms unstructured policy text into this format using\noff-the-shelf Large Language Models (LLMs). This compiler breaks down the\ntransformation task into a two-stage translation and entailment procedure. This\nprocedure considers the full context of the privacy policy to infer a complex\nformula, where each formula consists of simpler atomic formulae. The advantage\nof this model is that PolicyLR is interpretable by design and grounded in\nsegments of the privacy policy. We evaluated the compiler using ToS;DR, a\ncommunity-annotated privacy policy entailment dataset. Utilizing open-source\nLLMs, our compiler achieves precision and recall values of 0.91 and 0.88,\nrespectively. Finally, we demonstrate the utility of PolicyLR in three privacy\ntasks: Policy Compliance, Inconsistency Detection, and Privacy Comparison\nShopping.",
    "updated" : "2024-08-27T07:27:16Z",
    "published" : "2024-08-27T07:27:16Z",
    "authors" : [
      {
        "name" : "Ashish Hooda"
      },
      {
        "name" : "Rishabh Khandelwal"
      },
      {
        "name" : "Prasad Chalasani"
      },
      {
        "name" : "Kassem Fawaz"
      },
      {
        "name" : "Somesh Jha"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.14753v1",
    "title" : "CoopASD: Cooperative Machine Anomalous Sound Detection with Privacy\n  Concerns",
    "summary" : "Machine anomalous sound detection (ASD) has emerged as one of the most\npromising applications in the Industrial Internet of Things (IIoT) due to its\nunprecedented efficacy in mitigating risks of malfunctions and promoting\nproduction efficiency. Previous works mainly investigated the machine ASD task\nunder centralized settings. However, developing the ASD system under\ndecentralized settings is crucial in practice, since the machine data are\ndispersed in various factories and the data should not be explicitly shared due\nto privacy concerns. To enable these factories to cooperatively develop a\nscalable ASD model while preserving their privacy, we propose a novel framework\nnamed CoopASD, where each factory trains an ASD model on its local dataset, and\na central server aggregates these local models periodically. We employ a\npre-trained model as the backbone of the ASD model to improve its robustness\nand develop specialized techniques to stabilize the model under a completely\nnon-iid and domain shift setting. Compared with previous state-of-the-art\n(SOTA) models trained in centralized settings, CoopASD showcases competitive\nresults with negligible degradation of 0.08%. We also conduct extensive\nablation studies to demonstrate the effectiveness of CoopASD.",
    "updated" : "2024-08-27T03:07:03Z",
    "published" : "2024-08-27T03:07:03Z",
    "authors" : [
      {
        "name" : "Anbai Jiang"
      },
      {
        "name" : "Yuchen Shi"
      },
      {
        "name" : "Pingyi Fan"
      },
      {
        "name" : "Wei-Qiang Zhang"
      },
      {
        "name" : "Jia Liu"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.AI",
      "cs.DC",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.14735v1",
    "title" : "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
    "summary" : "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
    "updated" : "2024-08-27T02:03:36Z",
    "published" : "2024-08-27T02:03:36Z",
    "authors" : [
      {
        "name" : "Xianzhi Zhang"
      },
      {
        "name" : "Yipeng Zhou"
      },
      {
        "name" : "Di Wu"
      },
      {
        "name" : "Quan Z. Sheng"
      },
      {
        "name" : "Miao Hu"
      },
      {
        "name" : "Linchang Xiao"
      }
    ],
    "categories" : [
      "cs.MM",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.14689v1",
    "title" : "Federated User Preference Modeling for Privacy-Preserving Cross-Domain\n  Recommendation",
    "summary" : "Cross-domain recommendation (CDR) aims to address the data-sparsity problem\nby transferring knowledge across domains. Existing CDR methods generally assume\nthat the user-item interaction data is shareable between domains, which leads\nto privacy leakage. Recently, some privacy-preserving CDR (PPCDR) models have\nbeen proposed to solve this problem. However, they primarily transfer simple\nrepresentations learned only from user-item interaction histories, overlooking\nother useful side information, leading to inaccurate user preferences.\nAdditionally, they transfer differentially private user-item interaction\nmatrices or embeddings across domains to protect privacy. However, these\nmethods offer limited privacy protection, as attackers may exploit external\ninformation to infer the original data. To address these challenges, we propose\na novel Federated User Preference Modeling (FUPM) framework. In FUPM, first, a\nnovel comprehensive preference exploration module is proposed to learn users'\ncomprehensive preferences from both interaction data and additional data\nincluding review texts and potentially positive items. Next, a private\npreference transfer module is designed to first learn differentially private\nlocal and global prototypes, and then privately transfer the global prototypes\nusing a federated learning strategy. These prototypes are generalized\nrepresentations of user groups, making it difficult for attackers to infer\nindividual information. Extensive experiments on four CDR tasks conducted on\nthe Amazon and Douban datasets validate the superiority of FUPM over SOTA\nbaselines. Code is available at https://github.com/Lili1013/FUPM.",
    "updated" : "2024-08-26T23:29:03Z",
    "published" : "2024-08-26T23:29:03Z",
    "authors" : [
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Shoujin Wang"
      },
      {
        "name" : "Quangui Zhang"
      },
      {
        "name" : "Qiang Wu"
      },
      {
        "name" : "Min Xu"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.13460v1",
    "title" : "DOPPLER: Differentially Private Optimizers with Low-pass Filter for\n  Privacy Noise Reduction",
    "summary" : "Privacy is a growing concern in modern deep-learning systems and\napplications. Differentially private (DP) training prevents the leakage of\nsensitive information in the collected training data from the trained machine\nlearning models. DP optimizers, including DP stochastic gradient descent\n(DPSGD) and its variants, privatize the training procedure by gradient clipping\nand DP noise injection. However, in practice, DP models trained using DPSGD and\nits variants often suffer from significant model performance degradation. Such\ndegradation prevents the application of DP optimization in many key tasks, such\nas foundation model pretraining. In this paper, we provide a novel signal\nprocessing perspective to the design and analysis of DP optimizers. We show\nthat a ``frequency domain'' operation called low-pass filtering can be used to\neffectively reduce the impact of DP noise. More specifically, by defining the\n``frequency domain'' for both the gradient and differential privacy (DP) noise,\nwe have developed a new component, called DOPPLER. This component is designed\nfor DP algorithms and works by effectively amplifying the gradient while\nsuppressing DP noise within this frequency domain. As a result, it maintains\nprivacy guarantees and enhances the quality of the DP-protected model. Our\nexperiments show that the proposed DP optimizers with a low-pass filter\noutperform their counterparts without the filter by 3%-10% in test accuracy on\nvarious models and datasets. Both theoretical and practical evidence suggest\nthat the DOPPLER is effective in closing the gap between DP and non-DP\ntraining.",
    "updated" : "2024-08-24T04:27:07Z",
    "published" : "2024-08-24T04:27:07Z",
    "authors" : [
      {
        "name" : "Xinwei Zhang"
      },
      {
        "name" : "Zhiqi Bu"
      },
      {
        "name" : "Mingyi Hong"
      },
      {
        "name" : "Meisam Razaviyayn"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07892v3",
    "title" : "Personhood credentials: Artificial intelligence and the value of\n  privacy-preserving tools to distinguish who is real online",
    "summary" : "Anonymity is an important principle online. However, malicious actors have\nlong used misleading identities to conduct fraud, spread disinformation, and\ncarry out other deceptive schemes. With the advent of increasingly capable AI,\nbad actors can amplify the potential scale and effectiveness of their\noperations, intensifying the challenge of balancing anonymity and\ntrustworthiness online. In this paper, we analyze the value of a new tool to\naddress this challenge: \"personhood credentials\" (PHCs), digital credentials\nthat empower users to demonstrate that they are real people -- not AIs -- to\nonline services, without disclosing any personal information. Such credentials\ncan be issued by a range of trusted institutions -- governments or otherwise. A\nPHC system, according to our definition, could be local or global, and does not\nneed to be biometrics-based. Two trends in AI contribute to the urgency of the\nchallenge: AI's increasing indistinguishability from people online (i.e.,\nlifelike content and avatars, agentic activity), and AI's increasing\nscalability (i.e., cost-effectiveness, accessibility). Drawing on a long\nhistory of research into anonymous credentials and \"proof-of-personhood\"\nsystems, personhood credentials give people a way to signal their\ntrustworthiness on online platforms, and offer service providers new tools for\nreducing misuse by bad actors. In contrast, existing countermeasures to\nautomated deception -- such as CAPTCHAs -- are inadequate against sophisticated\nAI, while stringent identity verification solutions are insufficiently private\nfor many use-cases. After surveying the benefits of personhood credentials, we\nalso examine deployment risks and design challenges. We conclude with\nactionable next steps for policymakers, technologists, and standards bodies to\nconsider in consultation with the public.",
    "updated" : "2024-08-26T19:02:34Z",
    "published" : "2024-08-15T02:41:25Z",
    "authors" : [
      {
        "name" : "Steven Adler"
      },
      {
        "name" : "Zoë Hitzig"
      },
      {
        "name" : "Shrey Jain"
      },
      {
        "name" : "Catherine Brewer"
      },
      {
        "name" : "Wayne Chang"
      },
      {
        "name" : "Renée DiResta"
      },
      {
        "name" : "Eddy Lazzarin"
      },
      {
        "name" : "Sean McGregor"
      },
      {
        "name" : "Wendy Seltzer"
      },
      {
        "name" : "Divya Siddarth"
      },
      {
        "name" : "Nouran Soliman"
      },
      {
        "name" : "Tobin South"
      },
      {
        "name" : "Connor Spelliscy"
      },
      {
        "name" : "Manu Sporny"
      },
      {
        "name" : "Varya Srivastava"
      },
      {
        "name" : "John Bailey"
      },
      {
        "name" : "Brian Christian"
      },
      {
        "name" : "Andrew Critch"
      },
      {
        "name" : "Ronnie Falcon"
      },
      {
        "name" : "Heather Flanagan"
      },
      {
        "name" : "Kim Hamilton Duffy"
      },
      {
        "name" : "Eric Ho"
      },
      {
        "name" : "Claire R. Leibowicz"
      },
      {
        "name" : "Srikanth Nadhamuni"
      },
      {
        "name" : "Alan Z. Rozenshtein"
      },
      {
        "name" : "David Schnurr"
      },
      {
        "name" : "Evan Shapiro"
      },
      {
        "name" : "Lacey Strahm"
      },
      {
        "name" : "Andrew Trask"
      },
      {
        "name" : "Zoe Weinberg"
      },
      {
        "name" : "Cedric Whitney"
      },
      {
        "name" : "Tom Zick"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.15694v1",
    "title" : "Protecting Privacy in Federated Time Series Analysis: A Pragmatic\n  Technology Review for Application Developers",
    "summary" : "The federated analysis of sensitive time series has huge potential in various\ndomains, such as healthcare or manufacturing. Yet, to fully unlock this\npotential, requirements imposed by various stakeholders must be fulfilled,\nregarding, e.g., efficiency or trust assumptions. While many of these\nrequirements can be addressed by deploying advanced secure computation\nparadigms such as fully homomorphic encryption, certain aspects require an\nintegration with additional privacy-preserving technologies.\n  In this work, we perform a qualitative requirements elicitation based on\nselected real-world use cases. We match the derived requirements categories\nagainst the features and guarantees provided by available technologies. For\neach technology, we additionally perform a maturity assessment, including the\nstate of standardization and availability on the market. Furthermore, we\nprovide a decision tree supporting application developers in identifying the\nmost promising technologies available matching their needs. Finally, existing\ngaps are identified, highlighting research potential to advance the field.",
    "updated" : "2024-08-28T10:41:31Z",
    "published" : "2024-08-28T10:41:31Z",
    "authors" : [
      {
        "name" : "Daniel Bachlechner"
      },
      {
        "name" : "Ruben Hetfleisch"
      },
      {
        "name" : "Stephan Krenn"
      },
      {
        "name" : "Thomas Lorünser"
      },
      {
        "name" : "Michael Rader"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.15688v1",
    "title" : "PDSR: A Privacy-Preserving Diversified Service Recommendation Method on\n  Distributed Data",
    "summary" : "The last decade has witnessed a tremendous growth of service computing, while\nefficient service recommendation methods are desired to recommend high-quality\nservices to users. It is well known that collaborative filtering is one of the\nmost popular methods for service recommendation based on QoS, and many existing\nproposals focus on improving recommendation accuracy, i.e., recommending\nhigh-quality redundant services. Nevertheless, users may have different\nrequirements on QoS, and hence diversified recommendation has been attracting\nincreasing attention in recent years to fulfill users' diverse demands and to\nexplore potential services. Unfortunately, the recommendation performances\nrelies on a large volume of data (e.g., QoS data), whereas the data may be\ndistributed across multiple platforms. Therefore, to enable data sharing across\nthe different platforms for diversified service recommendation, we propose a\nPrivacy-preserving Diversified Service Recommendation (PDSR) method.\nSpecifically, we innovate in leveraging the Locality-Sensitive Hashing (LSH)\nmechanism such that privacy-preserved data sharing across different platforms\nis enabled to construct a service similarity graph. Based on the similarity\ngraph, we propose a novel accuracy-diversity metric and design a\n$2$-approximation algorithm to select $K$ services to recommend by maximizing\nthe accuracy-diversity measure. Extensive experiments on real datasets are\nconducted to verify the efficacy of our PDSR method.",
    "updated" : "2024-08-28T10:25:36Z",
    "published" : "2024-08-28T10:25:36Z",
    "authors" : [
      {
        "name" : "Lina Wang"
      },
      {
        "name" : "Huan Yang"
      },
      {
        "name" : "Yiran Shen"
      },
      {
        "name" : "Chao Liu"
      },
      {
        "name" : "Lianyong Qi"
      },
      {
        "name" : "Xiuzhen Cheng"
      },
      {
        "name" : "Feng Li"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.15621v1",
    "title" : "Convergent Differential Privacy Analysis for General Federated Learning:\n  the f-DP Perspective",
    "summary" : "Federated learning (FL) is an efficient collaborative training paradigm\nextensively developed with a focus on local privacy protection, and\ndifferential privacy (DP) is a classical approach to capture and ensure the\nreliability of local privacy. The powerful cooperation of FL and DP provides a\npromising learning framework for large-scale private clients, juggling both\nprivacy securing and trustworthy learning. As the predominant algorithm of DP,\nthe noisy perturbation has been widely studied and incorporated into various\nfederated algorithms, theoretically proven to offer significant privacy\nprotections. However, existing analyses in noisy FL-DP mostly rely on the\ncomposition theorem and cannot tightly quantify the privacy leakage challenges,\nwhich is nearly tight for small numbers of communication rounds but yields an\narbitrarily loose and divergent bound under the large communication rounds.\nThis implies a counterintuitive judgment, suggesting that FL may not provide\nadequate privacy protection during long-term training. To further investigate\nthe convergent privacy and reliability of the FL-DP framework, in this paper,\nwe comprehensively evaluate the worst privacy of two classical methods under\nthe non-convex and smooth objectives based on the f-DP analysis, i.e.\nNoisy-FedAvg and Noisy-FedProx methods. With the aid of the\nshifted-interpolation technique, we successfully prove that the worst privacy\nof the Noisy-FedAvg method achieves a tight convergent lower bound. Moreover,\nin the Noisy-FedProx method, with the regularization of the proxy term, the\nworst privacy has a stable constant lower bound. Our analysis further provides\na solid theoretical foundation for the reliability of privacy protection in\nFL-DP. Meanwhile, our conclusions can also be losslessly converted to other\nclassical DP analytical frameworks, e.g. $(\\epsilon,\\delta)$-DP and\nR$\\acute{\\text{e}}$nyi-DP (RDP).",
    "updated" : "2024-08-28T08:22:21Z",
    "published" : "2024-08-28T08:22:21Z",
    "authors" : [
      {
        "name" : "Yan Sun"
      },
      {
        "name" : "Li Shen"
      },
      {
        "name" : "Dacheng Tao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.15391v1",
    "title" : "Examining the Interplay Between Privacy and Fairness for Speech\n  Processing: A Review and Perspective",
    "summary" : "Speech technology has been increasingly deployed in various areas of daily\nlife including sensitive domains such as healthcare and law enforcement. For\nthese technologies to be effective, they must work reliably for all users while\npreserving individual privacy. Although tradeoffs between privacy and utility,\nas well as fairness and utility, have been extensively researched, the specific\ninterplay between privacy and fairness in speech processing remains\nunderexplored. This review and position paper offers an overview of emerging\nprivacy-fairness tradeoffs throughout the entire machine learning lifecycle for\nspeech processing. By drawing on well-established frameworks on fairness and\nprivacy, we examine existing biases and sources of privacy harm that coexist\nduring the development of speech processing models. We then highlight how\ncorresponding privacy-enhancing technologies have the potential to\ninadvertently increase these biases and how bias mitigation strategies may\nconversely reduce privacy. By raising open questions, we advocate for a\ncomprehensive evaluation of privacy-fairness tradeoffs for speech technology\nand the development of privacy-enhancing and fairness-aware algorithms in this\ndomain.",
    "updated" : "2024-08-27T20:32:01Z",
    "published" : "2024-08-27T20:32:01Z",
    "authors" : [
      {
        "name" : "Anna Leschanowsky"
      },
      {
        "name" : "Sneha Das"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.16304v1",
    "title" : "Understanding Privacy Norms through Web Forms",
    "summary" : "Web forms are one of the primary ways to collect personal information online,\nyet they are relatively under-studied. Unlike web tracking, data collection\nthrough web forms is explicit and contextualized. Users (i) are asked to input\nspecific personal information types, and (ii) know the specific context (i.e.,\non which website and for what purpose). For web forms to be trusted by users,\nthey must meet the common sense standards of appropriate data collection\npractices within a particular context (i.e., privacy norms). In this paper, we\nextract the privacy norms embedded within web forms through a measurement\nstudy. First, we build a specialized crawler to discover web forms on websites.\nWe run it on 11,500 popular websites, and we create a dataset of 293K web\nforms. Second, to process data of this scale, we develop a cost-efficient way\nto annotate web forms with form types and personal information types, using\ntext classifiers trained with assistance of large language models (LLMs).\nThird, by analyzing the annotated dataset, we reveal common patterns of data\ncollection practices. We find that (i) these patterns are explained by\nfunctional necessities and legal obligations, thus reflecting privacy norms,\nand that (ii) deviations from the observed norms often signal unnecessary data\ncollection. In addition, we analyze the privacy policies that accompany web\nforms. We show that, despite their wide adoption and use, there is a disconnect\nbetween privacy policy disclosures and the observed privacy norms.",
    "updated" : "2024-08-29T07:11:09Z",
    "published" : "2024-08-29T07:11:09Z",
    "authors" : [
      {
        "name" : "Hao Cui"
      },
      {
        "name" : "Rahmadi Trimananda"
      },
      {
        "name" : "Athina Markopoulou"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.15077v2",
    "title" : "MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of\n  Children with Autism Spectrum Disorder",
    "summary" : "Autism spectrum disorder (ASD) is characterized by significant challenges in\nsocial interaction and comprehending communication signals. Recently,\ntherapeutic interventions for ASD have increasingly utilized Deep learning\npowered-computer vision techniques to monitor individual progress over time.\nThese models are trained on private, non-public datasets from the autism\ncommunity, creating challenges in comparing results across different models due\nto privacy-preserving data-sharing issues. This work introduces MMASD+, an\nenhanced version of the novel open-source dataset called Multimodal ASD\n(MMASD). MMASD+ consists of diverse data modalities, including 3D-Skeleton, 3D\nBody Mesh, and Optical Flow data. It integrates the capabilities of Yolov8 and\nDeep SORT algorithms to distinguish between the therapist and children,\naddressing a significant barrier in the original dataset. Additionally, a\nMultimodal Transformer framework is proposed to predict 11 action types and the\npresence of ASD. This framework achieves an accuracy of 95.03% for predicting\naction types and 96.42% for predicting ASD presence, demonstrating over a 10%\nimprovement compared to models trained on single data modalities. These\nfindings highlight the advantages of integrating multiple data modalities\nwithin the Multimodal Transformer framework.",
    "updated" : "2024-08-28T20:30:29Z",
    "published" : "2024-08-27T14:05:48Z",
    "authors" : [
      {
        "name" : "Pavan Uttej Ravva"
      },
      {
        "name" : "Behdokht Kiafar"
      },
      {
        "name" : "Pinar Kullu"
      },
      {
        "name" : "Jicheng Li"
      },
      {
        "name" : "Anjana Bhat"
      },
      {
        "name" : "Roghayeh Leila Barmaki"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.17378v1",
    "title" : "Empowering Open Data Sharing for Social Good: A Privacy-Aware Approach",
    "summary" : "The Covid-19 pandemic has affected the world at multiple levels. Data sharing\nwas pivotal for advancing research to understand the underlying causes and\nimplement effective containment strategies. In response, many countries have\npromoted the availability of daily cases to support research initiatives,\nfostering collaboration between organisations and making such data available to\nthe public through open data platforms. Despite the several advantages of data\nsharing, one of the major concerns before releasing health data is its impact\non individuals' privacy. Such a sharing process should be based on\nstate-of-the-art methods in Data Protection by Design and by Default. In this\npaper, we use a data set related to Covid-19 cases in the second largest\nhospital in Portugal to show how it is feasible to ensure data privacy while\nimproving the quality and maintaining the utility of the data. Our goal is to\ndemonstrate how knowledge exchange in multidisciplinary teams of healthcare\npractitioners, data privacy, and data science experts is crucial to\nco-developing strategies that ensure high utility of de-identified data.",
    "updated" : "2024-08-30T16:14:32Z",
    "published" : "2024-08-30T16:14:32Z",
    "authors" : [
      {
        "name" : "Tânia Carvalho"
      },
      {
        "name" : "Luís Antunes"
      },
      {
        "name" : "Cristina Costa"
      },
      {
        "name" : "Nuno Moniz"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.17354v1",
    "title" : "Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language\n  Models for Privacy Leakage",
    "summary" : "Fine-tuning large language models on private data for downstream applications\nposes significant privacy risks in potentially exposing sensitive information.\nSeveral popular community platforms now offer convenient distribution of a\nlarge variety of pre-trained models, allowing anyone to publish without\nrigorous verification. This scenario creates a privacy threat, as pre-trained\nmodels can be intentionally crafted to compromise the privacy of fine-tuning\ndatasets. In this study, we introduce a novel poisoning technique that uses\nmodel-unlearning as an attack tool. This approach manipulates a pre-trained\nlanguage model to increase the leakage of private data during the fine-tuning\nprocess. Our method enhances both membership inference and data extraction\nattacks while preserving model utility. Experimental results across different\nmodels, datasets, and fine-tuning setups demonstrate that our attacks\nsignificantly surpass baseline performance. This work serves as a cautionary\nnote for users who download pre-trained models from unverified sources,\nhighlighting the potential risks involved.",
    "updated" : "2024-08-30T15:35:09Z",
    "published" : "2024-08-30T15:35:09Z",
    "authors" : [
      {
        "name" : "Md Rafi Ur Rashid"
      },
      {
        "name" : "Jing Liu"
      },
      {
        "name" : "Toshiaki Koike-Akino"
      },
      {
        "name" : "Shagufta Mehnaz"
      },
      {
        "name" : "Ye Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.17263v1",
    "title" : "Privacy-Preserving Set-Based Estimation Using Differential Privacy and\n  Zonotopes",
    "summary" : "For large-scale cyber-physical systems, the collaboration of spatially\ndistributed sensors is often needed to perform the state estimation process.\nPrivacy concerns arise from disclosing sensitive measurements to a cloud\nestimator. To solve this issue, we propose a differentially private set-based\nestimation protocol that guarantees true state containment in the estimated set\nand differential privacy for the sensitive measurements throughout the\nset-based state estimation process within the central and local differential\nprivacy models. Zonotopes are employed in the proposed differentially private\nset-based estimator, offering computational advantages in set operations. We\nconsider a plant of a non-linear discrete-time dynamical system with bounded\nmodeling uncertainties, sensors that provide sensitive measurements with\nbounded measurement uncertainties, and a cloud estimator that predicts the\nsystem's state. The privacy-preserving noise perturbs the centers of\nmeasurement zonotopes, thereby concealing the precise position of these\nzonotopes, i.e., ensuring privacy preservation for the sets containing\nsensitive measurements. Compared to existing research, our approach achieves\nless privacy loss and utility loss through the central and local differential\nprivacy models by leveraging a numerically optimized truncated noise\ndistribution. The proposed estimator is perturbed by weaker noise than the\nanalytical approaches in the literature to guarantee the same level of privacy,\ntherefore improving the estimation utility. Numerical and comparison\nexperiments with truncated Laplace noise are presented to support our approach.",
    "updated" : "2024-08-30T13:05:38Z",
    "published" : "2024-08-30T13:05:38Z",
    "authors" : [
      {
        "name" : "Mohammed M. Dawoud"
      },
      {
        "name" : "Changxin Liu"
      },
      {
        "name" : "Karl H. Johansson"
      },
      {
        "name" : "Amr Alanwar"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.17151v1",
    "title" : "Investigating Privacy Leakage in Dimensionality Reduction Methods via\n  Reconstruction Attack",
    "summary" : "This study investigates privacy leakage in dimensionality reduction methods\nthrough a novel machine learning-based reconstruction attack. Employing an\n\\emph{informed adversary} threat model, we develop a neural network capable of\nreconstructing high-dimensional data from low-dimensional embeddings.\n  We evaluate six popular dimensionality reduction techniques: PCA, sparse\nrandom projection (SRP), multidimensional scaling (MDS), Isomap, $t$-SNE, and\nUMAP. Using both MNIST and NIH Chest X-ray datasets, we perform a qualitative\nanalysis to identify key factors affecting reconstruction quality. Furthermore,\nwe assess the effectiveness of an additive noise mechanism in mitigating these\nreconstruction attacks.",
    "updated" : "2024-08-30T09:40:52Z",
    "published" : "2024-08-30T09:40:52Z",
    "authors" : [
      {
        "name" : "Chayadon Lumbut"
      },
      {
        "name" : "Donlapark Ponnoprat"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.17049v1",
    "title" : "SPOQchain: Platform for Secure, Scalable, and Privacy-Preserving Supply\n  Chain Tracing and Counterfeit Protection",
    "summary" : "Product lifecycle tracing is increasingly in the focus of regulators and\nproducers, as shown with the initiative of the Digital Product Pass. Likewise,\nnew methods of counterfeit detection are developed that are, e.g., based on\nPhysical Unclonable Functions (PUFs). In order to ensure trust and integrity of\nproduct lifecycle data, multiple existing supply chain tracing systems are\nbuilt on blockchain technology. However, only few solutions employ secure\nidentifiers such as PUFs. Furthermore, existing systems that publish the data\nof individual products, in part fully transparently, have a detrimental impact\non scalability and the privacy of users. This work proposes SPOQchain, a novel\nblockchain-based platform that provides comprehensive lifecycle traceability\nand originality verification while ensuring high efficiency and user privacy.\nThe improved efficiency is achieved by a sophisticated batching mechanism that\nremoves lifecycle redundancies. In addition to the successful evaluation of\nSPOQchain's scalability, this work provides a comprehensive analysis of privacy\nand security aspects, demonstrating the need and qualification of SPOQchain for\nthe future of supply chain tracing.",
    "updated" : "2024-08-30T07:15:43Z",
    "published" : "2024-08-30T07:15:43Z",
    "authors" : [
      {
        "name" : "Moritz Finke"
      },
      {
        "name" : "Alexandra Dmitrienko"
      },
      {
        "name" : "Jasper Stang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.16913v1",
    "title" : "Analyzing Inference Privacy Risks Through Gradients in Machine Learning",
    "summary" : "In distributed learning settings, models are iteratively updated with shared\ngradients computed from potentially sensitive user data. While previous work\nhas studied various privacy risks of sharing gradients, our paper aims to\nprovide a systematic approach to analyze private information leakage from\ngradients. We present a unified game-based framework that encompasses a broad\nrange of attacks including attribute, property, distributional, and user\ndisclosures. We investigate how different uncertainties of the adversary affect\ntheir inferential power via extensive experiments on five datasets across\nvarious data modalities. Our results demonstrate the inefficacy of solely\nrelying on data aggregation to achieve privacy against inference attacks in\ndistributed learning. We further evaluate five types of defenses, namely,\ngradient pruning, signed gradient descent, adversarial perturbations,\nvariational information bottleneck, and differential privacy, under both static\nand adaptive adversary settings. We provide an information-theoretic view for\nanalyzing the effectiveness of these defenses against inference from gradients.\nFinally, we introduce a method for auditing attribute inference privacy,\nimproving the empirical estimation of worst-case privacy through crafting\nadversarial canary records.",
    "updated" : "2024-08-29T21:21:53Z",
    "published" : "2024-08-29T21:21:53Z",
    "authors" : [
      {
        "name" : "Zhuohang Li"
      },
      {
        "name" : "Andrew Lowy"
      },
      {
        "name" : "Jing Liu"
      },
      {
        "name" : "Toshiaki Koike-Akino"
      },
      {
        "name" : "Kieran Parsons"
      },
      {
        "name" : "Bradley Malin"
      },
      {
        "name" : "Ye Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.00327v1",
    "title" : "Demo: FedCampus: A Real-world Privacy-preserving Mobile Application for\n  Smart Campus via Federated Learning & Analytics",
    "summary" : "In this demo, we introduce FedCampus, a privacy-preserving mobile application\nfor smart \\underline{campus} with \\underline{fed}erated learning (FL) and\nfederated analytics (FA). FedCampus enables cross-platform on-device FL/FA for\nboth iOS and Android, supporting continuously models and algorithms deployment\n(MLOps). Our app integrates privacy-preserving processed data via differential\nprivacy (DP) from smartwatches, where the processed parameters are used for\nFL/FA through the FedCampus backend platform. We distributed 100 smartwatches\nto volunteers at Duke Kunshan University and have successfully completed a\nseries of smart campus tasks featuring capabilities such as sleep tracking,\nphysical activity monitoring, personalized recommendations, and heavy hitters.\nOur project is opensourced at https://github.com/FedCampus/FedCampus_Flutter.\nSee the FedCampus video at https://youtu.be/k5iu46IjA38.",
    "updated" : "2024-08-31T01:58:36Z",
    "published" : "2024-08-31T01:58:36Z",
    "authors" : [
      {
        "name" : "Jiaxiang Geng"
      },
      {
        "name" : "Beilong Tang"
      },
      {
        "name" : "Boyan Zhang"
      },
      {
        "name" : "Jiaqi Shao"
      },
      {
        "name" : "Bing Luo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.00138v1",
    "title" : "PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in\n  Action",
    "summary" : "As language models (LMs) are widely utilized in personalized communication\nscenarios (e.g., sending emails, writing social media posts) and endowed with a\ncertain level of agency, ensuring they act in accordance with the contextual\nprivacy norms becomes increasingly critical. However, quantifying the privacy\nnorm awareness of LMs and the emerging privacy risk in LM-mediated\ncommunication is challenging due to (1) the contextual and long-tailed nature\nof privacy-sensitive cases, and (2) the lack of evaluation approaches that\ncapture realistic application scenarios. To address these challenges, we\npropose PrivacyLens, a novel framework designed to extend privacy-sensitive\nseeds into expressive vignettes and further into agent trajectories, enabling\nmulti-level evaluation of privacy leakage in LM agents' actions. We instantiate\nPrivacyLens with a collection of privacy norms grounded in privacy literature\nand crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM\nperformance in answering probing questions and their actual behavior when\nexecuting user instructions in an agent setup. State-of-the-art LMs, like GPT-4\nand Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even\nwhen prompted with privacy-enhancing instructions. We also demonstrate the\ndynamic nature of PrivacyLens by extending each seed into multiple trajectories\nto red-team LM privacy leakage risk. Dataset and code are available at\nhttps://github.com/SALT-NLP/PrivacyLens.",
    "updated" : "2024-08-29T17:58:38Z",
    "published" : "2024-08-29T17:58:38Z",
    "authors" : [
      {
        "name" : "Yijia Shao"
      },
      {
        "name" : "Tianshi Li"
      },
      {
        "name" : "Weiyan Shi"
      },
      {
        "name" : "Yanchen Liu"
      },
      {
        "name" : "Diyi Yang"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04888v2",
    "title" : "Locally Private Histograms in All Privacy Regimes",
    "summary" : "Frequency estimation, a.k.a. histograms, is a workhorse of data analysis, and\nas such has been thoroughly studied under differentially privacy. In\nparticular, computing histograms in the \\emph{local} model of privacy has been\nthe focus of a fruitful recent line of work, and various algorithms have been\nproposed, achieving the order-optimal $\\ell_\\infty$ error in the high-privacy\n(small $\\varepsilon$) regime while balancing other considerations such as time-\nand communication-efficiency. However, to the best of our knowledge, the\npicture is much less clear when it comes to the medium- or low-privacy regime\n(large $\\varepsilon$), despite its increased relevance in practice. In this\npaper, we investigate locally private histograms, and the very related\ndistribution learning task, in this medium-to-low privacy regime, and establish\nnear-tight (and somewhat unexpected) bounds on the $\\ell_\\infty$ error\nachievable. As a direct corollary of our results, we obtain a protocol for\nhistograms in the \\emph{shuffle} model of differential privacy, with accuracy\nmatching previous algorithms but significantly better message and communication\ncomplexity.\n  Our theoretical findings emerge from a novel analysis, which appears to\nimprove bounds across the board for the locally private histogram problem. We\nback our theoretical findings by an empirical comparison of existing algorithms\nin all privacy regimes, to assess their typical performance and behaviour\nbeyond the worst-case setting.",
    "updated" : "2024-09-04T07:12:41Z",
    "published" : "2024-08-09T06:22:45Z",
    "authors" : [
      {
        "name" : "Clément L. Canonne"
      },
      {
        "name" : "Abigail Gentle"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.DM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.15391v2",
    "title" : "Examining the Interplay Between Privacy and Fairness for Speech\n  Processing: A Review and Perspective",
    "summary" : "Speech technology has been increasingly deployed in various areas of daily\nlife including sensitive domains such as healthcare and law enforcement. For\nthese technologies to be effective, they must work reliably for all users while\npreserving individual privacy. Although tradeoffs between privacy and utility,\nas well as fairness and utility, have been extensively researched, the specific\ninterplay between privacy and fairness in speech processing remains\nunderexplored. This review and position paper offers an overview of emerging\nprivacy-fairness tradeoffs throughout the entire machine learning lifecycle for\nspeech processing. By drawing on well-established frameworks on fairness and\nprivacy, we examine existing biases and sources of privacy harm that coexist\nduring the development of speech processing models. We then highlight how\ncorresponding privacy-enhancing technologies have the potential to\ninadvertently increase these biases and how bias mitigation strategies may\nconversely reduce privacy. By raising open questions, we advocate for a\ncomprehensive evaluation of privacy-fairness tradeoffs for speech technology\nand the development of privacy-enhancing and fairness-aware algorithms in this\ndomain.",
    "updated" : "2024-09-05T06:43:22Z",
    "published" : "2024-08-27T20:32:01Z",
    "authors" : [
      {
        "name" : "Anna Leschanowsky"
      },
      {
        "name" : "Sneha Das"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10468v4",
    "title" : "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
    "summary" : "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
    "updated" : "2024-09-05T15:47:45Z",
    "published" : "2024-08-20T00:40:49Z",
    "authors" : [
      {
        "name" : "Jinxin Liu"
      },
      {
        "name" : "Zao Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.12787v2",
    "title" : "LLM-PBE: Assessing Data Privacy in Large Language Models",
    "summary" : "Large Language Models (LLMs) have become integral to numerous domains,\nsignificantly advancing applications in data management, mining, and analysis.\nTheir profound capabilities in processing and interpreting complex language\ndata, however, bring to light pressing concerns regarding data privacy,\nespecially the risk of unintentional training data leakage. Despite the\ncritical nature of this issue, there has been no existing literature to offer a\ncomprehensive assessment of data privacy risks in LLMs. Addressing this gap,\nour paper introduces LLM-PBE, a toolkit crafted specifically for the systematic\nevaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze\nprivacy across the entire lifecycle of LLMs, incorporating diverse attack and\ndefense strategies, and handling various data types and metrics. Through\ndetailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth\nexploration of data privacy concerns, shedding light on influential factors\nsuch as model size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in LLMs but\nalso serves as a vital resource for future research in the field. Aimed at\nenhancing the breadth of knowledge in this area, the findings, resources, and\nour full technical report are made available at https://llm-pbe.github.io/,\nproviding an open platform for academic and practical advancements in LLM\nprivacy assessment.",
    "updated" : "2024-09-06T04:30:50Z",
    "published" : "2024-08-23T01:37:29Z",
    "authors" : [
      {
        "name" : "Qinbin Li"
      },
      {
        "name" : "Junyuan Hong"
      },
      {
        "name" : "Chulin Xie"
      },
      {
        "name" : "Jeffrey Tan"
      },
      {
        "name" : "Rachel Xin"
      },
      {
        "name" : "Junyi Hou"
      },
      {
        "name" : "Xavier Yin"
      },
      {
        "name" : "Zhun Wang"
      },
      {
        "name" : "Dan Hendrycks"
      },
      {
        "name" : "Zhangyang Wang"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Bingsheng He"
      },
      {
        "name" : "Dawn Song"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  }
]