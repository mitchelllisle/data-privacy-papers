[
  {
    "id" : "http://arxiv.org/abs/2408.00639v1",
    "title" : "Privacy-preserving datasets by capturing feature distributions with\n  Conditional VAEs",
    "summary" : "Large and well-annotated datasets are essential for advancing deep learning\napplications, however often costly or impossible to obtain by a single entity.\nIn many areas, including the medical domain, approaches relying on data sharing\nhave become critical to address those challenges. While effective in increasing\ndataset size and diversity, data sharing raises significant privacy concerns.\nCommonly employed anonymization methods based on the k-anonymity paradigm often\nfail to preserve data diversity, affecting model robustness. This work\nintroduces a novel approach using Conditional Variational Autoencoders (CVAEs)\ntrained on feature vectors extracted from large pre-trained vision foundation\nmodels. Foundation models effectively detect and represent complex patterns\nacross diverse domains, allowing the CVAE to faithfully capture the embedding\nspace of a given data distribution to generate (sample) a diverse,\nprivacy-respecting, and potentially unbounded set of synthetic feature vectors.\nOur method notably outperforms traditional approaches in both medical and\nnatural image domains, exhibiting greater dataset diversity and higher\nrobustness against perturbations while preserving sample privacy. These results\nunderscore the potential of generative models to significantly impact deep\nlearning applications in data-scarce and privacy-sensitive environments. The\nsource code is available at\nhttps://github.com/francescodisalvo05/cvae-anonymization .",
    "updated" : "2024-08-01T15:26:24Z",
    "published" : "2024-08-01T15:26:24Z",
    "authors" : [
      {
        "name" : "Francesco Di Salvo"
      },
      {
        "name" : "David Tafler"
      },
      {
        "name" : "Sebastian Doerrich"
      },
      {
        "name" : "Christian Ledig"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00382v1",
    "title" : "Long-Term Conversation Analysis: Privacy-Utility Trade-off under Noise\n  and Reverberation",
    "summary" : "Recordings in everyday life require privacy preservation of the speech\ncontent and speaker identity. This contribution explores the influence of noise\nand reverberation on the trade-off between privacy and utility for low-cost\nprivacy-preserving methods feasible for edge computing. These methods\ncompromise spectral and temporal smoothing, speaker anonymization using the\nMcAdams coefficient, sampling with a very low sampling rate, and combinations.\nPrivacy is assessed by automatic speech and speaker recognition, while our\nutility considers voice activity detection and speaker diarization. Overall,\nour evaluation shows that additional noise degrades the performance of all\nmodels more than reverberation. This degradation corresponds to enhanced speech\nprivacy, while utility is less deteriorated for some methods.",
    "updated" : "2024-08-01T08:43:46Z",
    "published" : "2024-08-01T08:43:46Z",
    "authors" : [
      {
        "name" : "Jule Pohlhausen"
      },
      {
        "name" : "Francesco Nespoli"
      },
      {
        "name" : "Joerg Bitzer"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00294v1",
    "title" : "RDP: Ranked Differential Privacy for Facial Feature Protection in\n  Multiscale Sparsified Subspace",
    "summary" : "With the widespread sharing of personal face images in applications' public\ndatabases, face recognition systems faces real threat of being breached by\npotential adversaries who are able to access users' face images and use them to\nintrude the face recognition systems. In this paper, we propose a novel privacy\nprotection method in the multiscale sparsified feature subspaces to protect\nsensitive facial features, by taking care of the influence or weight ranked\nfeature coefficients on the privacy budget, named \"Ranked Differential Privacy\n(RDP)\". After the multiscale feature decomposition, the lightweight Laplacian\nnoise is added to the dimension-reduced sparsified feature coefficients\naccording to the geometric superposition method. Then, we rigorously prove that\nthe RDP satisfies Differential Privacy. After that, the nonlinear Lagrange\nMultiplier (LM) method is formulated for the constraint optimization problem of\nmaximizing the utility of the visualization quality protected face images with\nsanitizing noise, under a given facial features privacy budget. Then, two\nmethods are proposed to solve the nonlinear LM problem and obtain the optimal\nnoise scale parameters: 1) the analytical Normalization Approximation (NA)\nmethod with identical average noise scale parameter for real-time online\napplications; and 2) the LM optimization Gradient Descent (LMGD) numerical\nmethod to obtain the nonlinear solution through iterative updating for more\naccurate offline applications. Experimental results on two real-world datasets\nshow that our proposed RDP outperforms other state-of-the-art methods: at a\nprivacy budget of 0.2, the PSNR (Peak Signal-to-Noise Ratio) of the RDP is\nabout ~10 dB higher than (10 times as high as) the highest PSNR of all compared\nmethods.",
    "updated" : "2024-08-01T05:41:59Z",
    "published" : "2024-08-01T05:41:59Z",
    "authors" : [
      {
        "name" : "Lu Ou"
      },
      {
        "name" : "Shaolin Liao"
      },
      {
        "name" : "Shihui Gao"
      },
      {
        "name" : "Guandong Huang"
      },
      {
        "name" : "Zheng Qi"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01228v1",
    "title" : "The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models",
    "summary" : "Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.",
    "updated" : "2024-08-02T12:36:13Z",
    "published" : "2024-08-02T12:36:13Z",
    "authors" : [
      {
        "name" : "Simone Caldarella"
      },
      {
        "name" : "Massimiliano Mancini"
      },
      {
        "name" : "Elisa Ricci"
      },
      {
        "name" : "Rahaf Aljundi"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01040v1",
    "title" : "Privacy-Preserving Split Learning with Vision Transformers using\n  Patch-Wise Random and Noisy CutMix",
    "summary" : "In computer vision, the vision transformer (ViT) has increasingly superseded\nthe convolutional neural network (CNN) for improved accuracy and robustness.\nHowever, ViT's large model sizes and high sample complexity make it difficult\nto train on resource-constrained edge devices. Split learning (SL) emerges as a\nviable solution, leveraging server-side resources to train ViTs while utilizing\nprivate data from distributed devices. However, SL requires additional\ninformation exchange for weight updates between the device and the server,\nwhich can be exposed to various attacks on private training data. To mitigate\nthe risk of data breaches in classification tasks, inspired from the CutMix\nregularization, we propose a novel privacy-preserving SL framework that injects\nGaussian noise into smashed data and mixes randomly chosen patches of smashed\ndata across clients, coined DP-CutMixSL. Our analysis demonstrates that\nDP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy\nprotection against membership inference attacks during forward propagation.\nThrough simulations, we show that DP-CutMixSL improves privacy protection\nagainst membership inference attacks, reconstruction attacks, and label\ninference attacks, while also improving accuracy compared to DP-SL and\nDP-MixSL.",
    "updated" : "2024-08-02T06:24:39Z",
    "published" : "2024-08-02T06:24:39Z",
    "authors" : [
      {
        "name" : "Seungeun Oh"
      },
      {
        "name" : "Sihun Baek"
      },
      {
        "name" : "Jihong Park"
      },
      {
        "name" : "Hyelin Nam"
      },
      {
        "name" : "Praneeth Vepakomma"
      },
      {
        "name" : "Ramesh Raskar"
      },
      {
        "name" : "Mehdi Bennis"
      },
      {
        "name" : "Seong-Lyun Kim"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00950v1",
    "title" : "PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking\n  Services",
    "summary" : "Eye gaze contains rich information about human attention and cognitive\nprocesses. This capability makes the underlying technology, known as gaze\ntracking, a critical enabler for many ubiquitous applications and has triggered\nthe development of easy-to-use gaze estimation services. Indeed, by utilizing\nthe ubiquitous cameras on tablets and smartphones, users can readily access\nmany gaze estimation services. In using these services, users must provide\ntheir full-face images to the gaze estimator, which is often a black box. This\nposes significant privacy threats to the users, especially when a malicious\nservice provider gathers a large collection of face images to classify\nsensitive user attributes. In this work, we present PrivateGaze, the first\napproach that can effectively preserve users' privacy in black-box gaze\ntracking services without compromising gaze estimation performance.\nSpecifically, we proposed a novel framework to train a privacy preserver that\nconverts full-face images into obfuscated counterparts, which are effective for\ngaze estimation while containing no privacy information. Evaluation on four\ndatasets shows that the obfuscated image can protect users' private\ninformation, such as identity and gender, against unauthorized attribute\nclassification. Meanwhile, when used directly by the black-box gaze estimator\nas inputs, the obfuscated images lead to comparable tracking performance to the\nconventional, unprotected full-face images.",
    "updated" : "2024-08-01T23:11:03Z",
    "published" : "2024-08-01T23:11:03Z",
    "authors" : [
      {
        "name" : "Lingyu Du"
      },
      {
        "name" : "Jinyuan Jia"
      },
      {
        "name" : "Xucong Zhang"
      },
      {
        "name" : "Guohao Lan"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.02373v1",
    "title" : "Operationalizing Contextual Integrity in Privacy-Conscious Assistants",
    "summary" : "Advanced AI assistants combine frontier LLMs and tool access to autonomously\nperform complex tasks on behalf of users. While the helpfulness of such\nassistants can increase dramatically with access to user information including\nemails and documents, this raises privacy concerns about assistants sharing\ninappropriate information with third parties without user supervision. To steer\ninformation-sharing assistants to behave in accordance with privacy\nexpectations, we propose to operationalize $\\textit{contextual integrity}$\n(CI), a framework that equates privacy with the appropriate flow of information\nin a given context. In particular, we design and evaluate a number of\nstrategies to steer assistants' information-sharing actions to be CI compliant.\nOur evaluation is based on a novel form filling benchmark composed of synthetic\ndata and human annotations, and it reveals that prompting frontier LLMs to\nperform CI-based reasoning yields strong results.",
    "updated" : "2024-08-05T10:53:51Z",
    "published" : "2024-08-05T10:53:51Z",
    "authors" : [
      {
        "name" : "Sahra Ghalebikesabi"
      },
      {
        "name" : "Eugene Bagdasaryan"
      },
      {
        "name" : "Ren Yi"
      },
      {
        "name" : "Itay Yona"
      },
      {
        "name" : "Ilia Shumailov"
      },
      {
        "name" : "Aneesh Pappu"
      },
      {
        "name" : "Chongyang Shi"
      },
      {
        "name" : "Laura Weidinger"
      },
      {
        "name" : "Robert Stanforth"
      },
      {
        "name" : "Leonard Berrada"
      },
      {
        "name" : "Pushmeet Kohli"
      },
      {
        "name" : "Po-Sen Huang"
      },
      {
        "name" : "Borja Balle"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01711v1",
    "title" : "Privacy in networks of quantum sensors",
    "summary" : "We treat privacy in a network of quantum sensors where accessible information\nis limited to specific functions of the network parameters, and all other\ninformation remains private. We develop an analysis of privacy in terms of a\nmanipulation of the quantum Fisher information matrix, and find the optimal\nstate achieving maximum privacy in the estimation of linear combination of the\nunknown parameters in a network of quantum sensors. We also discuss the effect\nof uncorrelated noise on the privacy of the network. Moreover, we illustrate\nour results with an example where the goal is to estimate the average value of\nthe unknown parameters in the network. In this example, we also introduce the\nnotion of quasi-privacy ($\\epsilon$-privacy), quantifying how close the state\nis to being private.",
    "updated" : "2024-08-03T08:39:44Z",
    "published" : "2024-08-03T08:39:44Z",
    "authors" : [
      {
        "name" : "Majid Hassani"
      },
      {
        "name" : "Santiago Scheiner"
      },
      {
        "name" : "Matteo G. A. Paris"
      },
      {
        "name" : "Damian Markham"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01609v1",
    "title" : "Fed-RD: Privacy-Preserving Federated Learning for Financial Crime\n  Detection",
    "summary" : "We introduce Federated Learning for Relational Data (Fed-RD), a novel\nprivacy-preserving federated learning algorithm specifically developed for\nfinancial transaction datasets partitioned vertically and horizontally across\nparties. Fed-RD strategically employs differential privacy and secure\nmultiparty computation to guarantee the privacy of training data. We provide\ntheoretical analysis of the end-to-end privacy of the training algorithm and\npresent experimental results on realistic synthetic datasets. Our results\ndemonstrate that Fed-RD achieves high model accuracy with minimal degradation\nas privacy increases, while consistently surpassing benchmark results.",
    "updated" : "2024-08-03T00:07:10Z",
    "published" : "2024-08-03T00:07:10Z",
    "authors" : [
      {
        "name" : "Md. Saikat Islam Khan"
      },
      {
        "name" : "Aparna Gupta"
      },
      {
        "name" : "Oshani Seneviratne"
      },
      {
        "name" : "Stacy Patterson"
      }
    ],
    "categories" : [
      "cs.CE"
    ]
  }
]