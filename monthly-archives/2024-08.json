[
  {
    "id" : "http://arxiv.org/abs/2408.00639v1",
    "title" : "Privacy-preserving datasets by capturing feature distributions with\n  Conditional VAEs",
    "summary" : "Large and well-annotated datasets are essential for advancing deep learning\napplications, however often costly or impossible to obtain by a single entity.\nIn many areas, including the medical domain, approaches relying on data sharing\nhave become critical to address those challenges. While effective in increasing\ndataset size and diversity, data sharing raises significant privacy concerns.\nCommonly employed anonymization methods based on the k-anonymity paradigm often\nfail to preserve data diversity, affecting model robustness. This work\nintroduces a novel approach using Conditional Variational Autoencoders (CVAEs)\ntrained on feature vectors extracted from large pre-trained vision foundation\nmodels. Foundation models effectively detect and represent complex patterns\nacross diverse domains, allowing the CVAE to faithfully capture the embedding\nspace of a given data distribution to generate (sample) a diverse,\nprivacy-respecting, and potentially unbounded set of synthetic feature vectors.\nOur method notably outperforms traditional approaches in both medical and\nnatural image domains, exhibiting greater dataset diversity and higher\nrobustness against perturbations while preserving sample privacy. These results\nunderscore the potential of generative models to significantly impact deep\nlearning applications in data-scarce and privacy-sensitive environments. The\nsource code is available at\nhttps://github.com/francescodisalvo05/cvae-anonymization .",
    "updated" : "2024-08-01T15:26:24Z",
    "published" : "2024-08-01T15:26:24Z",
    "authors" : [
      {
        "name" : "Francesco Di Salvo"
      },
      {
        "name" : "David Tafler"
      },
      {
        "name" : "Sebastian Doerrich"
      },
      {
        "name" : "Christian Ledig"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00382v1",
    "title" : "Long-Term Conversation Analysis: Privacy-Utility Trade-off under Noise\n  and Reverberation",
    "summary" : "Recordings in everyday life require privacy preservation of the speech\ncontent and speaker identity. This contribution explores the influence of noise\nand reverberation on the trade-off between privacy and utility for low-cost\nprivacy-preserving methods feasible for edge computing. These methods\ncompromise spectral and temporal smoothing, speaker anonymization using the\nMcAdams coefficient, sampling with a very low sampling rate, and combinations.\nPrivacy is assessed by automatic speech and speaker recognition, while our\nutility considers voice activity detection and speaker diarization. Overall,\nour evaluation shows that additional noise degrades the performance of all\nmodels more than reverberation. This degradation corresponds to enhanced speech\nprivacy, while utility is less deteriorated for some methods.",
    "updated" : "2024-08-01T08:43:46Z",
    "published" : "2024-08-01T08:43:46Z",
    "authors" : [
      {
        "name" : "Jule Pohlhausen"
      },
      {
        "name" : "Francesco Nespoli"
      },
      {
        "name" : "Joerg Bitzer"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00294v1",
    "title" : "RDP: Ranked Differential Privacy for Facial Feature Protection in\n  Multiscale Sparsified Subspace",
    "summary" : "With the widespread sharing of personal face images in applications' public\ndatabases, face recognition systems faces real threat of being breached by\npotential adversaries who are able to access users' face images and use them to\nintrude the face recognition systems. In this paper, we propose a novel privacy\nprotection method in the multiscale sparsified feature subspaces to protect\nsensitive facial features, by taking care of the influence or weight ranked\nfeature coefficients on the privacy budget, named \"Ranked Differential Privacy\n(RDP)\". After the multiscale feature decomposition, the lightweight Laplacian\nnoise is added to the dimension-reduced sparsified feature coefficients\naccording to the geometric superposition method. Then, we rigorously prove that\nthe RDP satisfies Differential Privacy. After that, the nonlinear Lagrange\nMultiplier (LM) method is formulated for the constraint optimization problem of\nmaximizing the utility of the visualization quality protected face images with\nsanitizing noise, under a given facial features privacy budget. Then, two\nmethods are proposed to solve the nonlinear LM problem and obtain the optimal\nnoise scale parameters: 1) the analytical Normalization Approximation (NA)\nmethod with identical average noise scale parameter for real-time online\napplications; and 2) the LM optimization Gradient Descent (LMGD) numerical\nmethod to obtain the nonlinear solution through iterative updating for more\naccurate offline applications. Experimental results on two real-world datasets\nshow that our proposed RDP outperforms other state-of-the-art methods: at a\nprivacy budget of 0.2, the PSNR (Peak Signal-to-Noise Ratio) of the RDP is\nabout ~10 dB higher than (10 times as high as) the highest PSNR of all compared\nmethods.",
    "updated" : "2024-08-01T05:41:59Z",
    "published" : "2024-08-01T05:41:59Z",
    "authors" : [
      {
        "name" : "Lu Ou"
      },
      {
        "name" : "Shaolin Liao"
      },
      {
        "name" : "Shihui Gao"
      },
      {
        "name" : "Guandong Huang"
      },
      {
        "name" : "Zheng Qi"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01228v1",
    "title" : "The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models",
    "summary" : "Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.",
    "updated" : "2024-08-02T12:36:13Z",
    "published" : "2024-08-02T12:36:13Z",
    "authors" : [
      {
        "name" : "Simone Caldarella"
      },
      {
        "name" : "Massimiliano Mancini"
      },
      {
        "name" : "Elisa Ricci"
      },
      {
        "name" : "Rahaf Aljundi"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01040v1",
    "title" : "Privacy-Preserving Split Learning with Vision Transformers using\n  Patch-Wise Random and Noisy CutMix",
    "summary" : "In computer vision, the vision transformer (ViT) has increasingly superseded\nthe convolutional neural network (CNN) for improved accuracy and robustness.\nHowever, ViT's large model sizes and high sample complexity make it difficult\nto train on resource-constrained edge devices. Split learning (SL) emerges as a\nviable solution, leveraging server-side resources to train ViTs while utilizing\nprivate data from distributed devices. However, SL requires additional\ninformation exchange for weight updates between the device and the server,\nwhich can be exposed to various attacks on private training data. To mitigate\nthe risk of data breaches in classification tasks, inspired from the CutMix\nregularization, we propose a novel privacy-preserving SL framework that injects\nGaussian noise into smashed data and mixes randomly chosen patches of smashed\ndata across clients, coined DP-CutMixSL. Our analysis demonstrates that\nDP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy\nprotection against membership inference attacks during forward propagation.\nThrough simulations, we show that DP-CutMixSL improves privacy protection\nagainst membership inference attacks, reconstruction attacks, and label\ninference attacks, while also improving accuracy compared to DP-SL and\nDP-MixSL.",
    "updated" : "2024-08-02T06:24:39Z",
    "published" : "2024-08-02T06:24:39Z",
    "authors" : [
      {
        "name" : "Seungeun Oh"
      },
      {
        "name" : "Sihun Baek"
      },
      {
        "name" : "Jihong Park"
      },
      {
        "name" : "Hyelin Nam"
      },
      {
        "name" : "Praneeth Vepakomma"
      },
      {
        "name" : "Ramesh Raskar"
      },
      {
        "name" : "Mehdi Bennis"
      },
      {
        "name" : "Seong-Lyun Kim"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00950v1",
    "title" : "PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking\n  Services",
    "summary" : "Eye gaze contains rich information about human attention and cognitive\nprocesses. This capability makes the underlying technology, known as gaze\ntracking, a critical enabler for many ubiquitous applications and has triggered\nthe development of easy-to-use gaze estimation services. Indeed, by utilizing\nthe ubiquitous cameras on tablets and smartphones, users can readily access\nmany gaze estimation services. In using these services, users must provide\ntheir full-face images to the gaze estimator, which is often a black box. This\nposes significant privacy threats to the users, especially when a malicious\nservice provider gathers a large collection of face images to classify\nsensitive user attributes. In this work, we present PrivateGaze, the first\napproach that can effectively preserve users' privacy in black-box gaze\ntracking services without compromising gaze estimation performance.\nSpecifically, we proposed a novel framework to train a privacy preserver that\nconverts full-face images into obfuscated counterparts, which are effective for\ngaze estimation while containing no privacy information. Evaluation on four\ndatasets shows that the obfuscated image can protect users' private\ninformation, such as identity and gender, against unauthorized attribute\nclassification. Meanwhile, when used directly by the black-box gaze estimator\nas inputs, the obfuscated images lead to comparable tracking performance to the\nconventional, unprotected full-face images.",
    "updated" : "2024-08-01T23:11:03Z",
    "published" : "2024-08-01T23:11:03Z",
    "authors" : [
      {
        "name" : "Lingyu Du"
      },
      {
        "name" : "Jinyuan Jia"
      },
      {
        "name" : "Xucong Zhang"
      },
      {
        "name" : "Guohao Lan"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.02373v1",
    "title" : "Operationalizing Contextual Integrity in Privacy-Conscious Assistants",
    "summary" : "Advanced AI assistants combine frontier LLMs and tool access to autonomously\nperform complex tasks on behalf of users. While the helpfulness of such\nassistants can increase dramatically with access to user information including\nemails and documents, this raises privacy concerns about assistants sharing\ninappropriate information with third parties without user supervision. To steer\ninformation-sharing assistants to behave in accordance with privacy\nexpectations, we propose to operationalize $\\textit{contextual integrity}$\n(CI), a framework that equates privacy with the appropriate flow of information\nin a given context. In particular, we design and evaluate a number of\nstrategies to steer assistants' information-sharing actions to be CI compliant.\nOur evaluation is based on a novel form filling benchmark composed of synthetic\ndata and human annotations, and it reveals that prompting frontier LLMs to\nperform CI-based reasoning yields strong results.",
    "updated" : "2024-08-05T10:53:51Z",
    "published" : "2024-08-05T10:53:51Z",
    "authors" : [
      {
        "name" : "Sahra Ghalebikesabi"
      },
      {
        "name" : "Eugene Bagdasaryan"
      },
      {
        "name" : "Ren Yi"
      },
      {
        "name" : "Itay Yona"
      },
      {
        "name" : "Ilia Shumailov"
      },
      {
        "name" : "Aneesh Pappu"
      },
      {
        "name" : "Chongyang Shi"
      },
      {
        "name" : "Laura Weidinger"
      },
      {
        "name" : "Robert Stanforth"
      },
      {
        "name" : "Leonard Berrada"
      },
      {
        "name" : "Pushmeet Kohli"
      },
      {
        "name" : "Po-Sen Huang"
      },
      {
        "name" : "Borja Balle"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01711v1",
    "title" : "Privacy in networks of quantum sensors",
    "summary" : "We treat privacy in a network of quantum sensors where accessible information\nis limited to specific functions of the network parameters, and all other\ninformation remains private. We develop an analysis of privacy in terms of a\nmanipulation of the quantum Fisher information matrix, and find the optimal\nstate achieving maximum privacy in the estimation of linear combination of the\nunknown parameters in a network of quantum sensors. We also discuss the effect\nof uncorrelated noise on the privacy of the network. Moreover, we illustrate\nour results with an example where the goal is to estimate the average value of\nthe unknown parameters in the network. In this example, we also introduce the\nnotion of quasi-privacy ($\\epsilon$-privacy), quantifying how close the state\nis to being private.",
    "updated" : "2024-08-03T08:39:44Z",
    "published" : "2024-08-03T08:39:44Z",
    "authors" : [
      {
        "name" : "Majid Hassani"
      },
      {
        "name" : "Santiago Scheiner"
      },
      {
        "name" : "Matteo G. A. Paris"
      },
      {
        "name" : "Damian Markham"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01609v1",
    "title" : "Fed-RD: Privacy-Preserving Federated Learning for Financial Crime\n  Detection",
    "summary" : "We introduce Federated Learning for Relational Data (Fed-RD), a novel\nprivacy-preserving federated learning algorithm specifically developed for\nfinancial transaction datasets partitioned vertically and horizontally across\nparties. Fed-RD strategically employs differential privacy and secure\nmultiparty computation to guarantee the privacy of training data. We provide\ntheoretical analysis of the end-to-end privacy of the training algorithm and\npresent experimental results on realistic synthetic datasets. Our results\ndemonstrate that Fed-RD achieves high model accuracy with minimal degradation\nas privacy increases, while consistently surpassing benchmark results.",
    "updated" : "2024-08-03T00:07:10Z",
    "published" : "2024-08-03T00:07:10Z",
    "authors" : [
      {
        "name" : "Md. Saikat Islam Khan"
      },
      {
        "name" : "Aparna Gupta"
      },
      {
        "name" : "Oshani Seneviratne"
      },
      {
        "name" : "Stacy Patterson"
      }
    ],
    "categories" : [
      "cs.CE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.03185v1",
    "title" : "MaskAnyone Toolkit: Offering Strategies for Minimizing Privacy Risks and\n  Maximizing Utility in Audio-Visual Data Archiving",
    "summary" : "This paper introduces MaskAnyone, a novel toolkit designed to navigate some\nprivacy and ethical concerns of sharing audio-visual data in research.\nMaskAnyone offers a scalable, user-friendly solution for de-identifying\nindividuals in video and audio content through face-swapping and voice\nalteration, supporting multi-person masking and real-time bulk processing. By\nintegrating this tool within research practices, we aim to enhance data\nreproducibility and utility in social science research. Our approach draws on\nDesign Science Research, proposing that MaskAnyone can facilitate safer data\nsharing and potentially reduce the storage of fully identifiable data. We\ndiscuss the development and capabilities of MaskAnyone, explore its integration\ninto ethical research practices, and consider the broader implications of\naudio-visual data masking, including issues of consent and the risk of misuse.\nThe paper concludes with a preliminary evaluation framework for assessing the\neffectiveness and ethical integration of masking tools in such research\nsettings.",
    "updated" : "2024-08-06T13:35:27Z",
    "published" : "2024-08-06T13:35:27Z",
    "authors" : [
      {
        "name" : "Babajide Alamu Owoyele"
      },
      {
        "name" : "Martin Schilling"
      },
      {
        "name" : "Rohan Sawahn"
      },
      {
        "name" : "Niklas Kaemer"
      },
      {
        "name" : "Pavel Zherebenkov"
      },
      {
        "name" : "Bhuvanesh Verma"
      },
      {
        "name" : "Wim Pouw"
      },
      {
        "name" : "Gerard de Melo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.02927v1",
    "title" : "HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy\n  Protection",
    "summary" : "Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.",
    "updated" : "2024-08-06T03:21:13Z",
    "published" : "2024-08-06T03:21:13Z",
    "authors" : [
      {
        "name" : "Yuxin Wang"
      },
      {
        "name" : "Duanyu Feng"
      },
      {
        "name" : "Yongfu Dai"
      },
      {
        "name" : "Zhengyu Chen"
      },
      {
        "name" : "Jimin Huang"
      },
      {
        "name" : "Sophia Ananiadou"
      },
      {
        "name" : "Qianqian Xie"
      },
      {
        "name" : "Hao Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.02750v1",
    "title" : "Privacy-Safe Iris Presentation Attack Detection",
    "summary" : "This paper proposes a framework for a privacy-safe iris presentation attack\ndetection (PAD) method, designed solely with synthetically-generated,\nidentity-leakage-free iris images. Once trained, the method is evaluated in a\nclassical way using state-of-the-art iris PAD benchmarks. We designed two\ngenerative models for the synthesis of ISO/IEC 19794-6-compliant iris images.\nThe first model synthesizes bona fide-looking samples. To avoid ``identity\nleakage,'' the generated samples that accidentally matched those used in the\nmodel's training were excluded. The second model synthesizes images of irises\nwith textured contact lenses and is conditioned by a given contact lens brand\nto have better control over textured contact lens appearance when forming the\ntraining set. Our experiments demonstrate that models trained solely on\nsynthetic data achieve a lower but still reasonable performance when compared\nto solutions trained with iris images collected from human subjects. This is\nthe first-of-its-kind attempt to use solely synthetic data to train a\nfully-functional iris PAD solution, and despite the performance gap between\nregular and the proposed methods, this study demonstrates that with the\nincreasing fidelity of generative models, creating such privacy-safe iris PAD\nmethods may be possible. The source codes and generative models trained for\nthis work are offered along with the paper.",
    "updated" : "2024-08-05T18:09:02Z",
    "published" : "2024-08-05T18:09:02Z",
    "authors" : [
      {
        "name" : "Mahsa Mitcheff"
      },
      {
        "name" : "Patrick Tinsley"
      },
      {
        "name" : "Adam Czajka"
      }
    ],
    "categories" : [
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.03897v1",
    "title" : "Speech privacy-preserving methods using secret key for convolutional\n  neural network models and their robustness evaluation",
    "summary" : "In this paper, we propose privacy-preserving methods with a secret key for\nconvolutional neural network (CNN)-based models in speech processing tasks. In\nenvironments where untrusted third parties, like cloud servers, provide\nCNN-based systems, ensuring the privacy of speech queries becomes essential.\nThis paper proposes encryption methods for speech queries using secret keys and\na model structure that allows for encrypted queries to be accepted without\ndecryption. Our approach introduces three types of secret keys: Shuffling,\nFlipping, and random orthogonal matrix (ROM). In experiments, we demonstrate\nthat when the proposed methods are used with the correct key, identification\nperformance did not degrade. Conversely, when an incorrect key is used, the\nperformance significantly decreased. Particularly, with the use of ROM, we show\nthat even with a relatively small key space, high privacy-preserving\nperformance can be maintained many speech processing tasks. Furthermore, we\nalso demonstrate the difficulty of recovering original speech from encrypted\nqueries in various robustness evaluations.",
    "updated" : "2024-08-07T16:51:39Z",
    "published" : "2024-08-07T16:51:39Z",
    "authors" : [
      {
        "name" : "Shoko Niwa"
      },
      {
        "name" : "Sayaka Shiota"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.CR",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.03578v1",
    "title" : "Unraveling Privacy Threat Modeling Complexity: Conceptual Privacy\n  Analysis Layers",
    "summary" : "Analyzing privacy threats in software products is an essential part of\nsoftware development to ensure systems are privacy-respecting; yet it is still\na far from trivial activity. While there have been many advancements in the\npast decade, they tend to focus on describing 'what' the threats are. What\nisn't entirely clear yet is 'how' to actually find these threats. Privacy is a\ncomplex domain. We propose to use four conceptual layers (feature, ecosystem,\nbusiness context, and environment) to capture this privacy complexity. These\nlayers can be used as a frame to structure and specify the privacy analysis\nsupport in a more tangible and actionable way, thereby improving applicability\nof the analysis process.",
    "updated" : "2024-08-07T06:30:20Z",
    "published" : "2024-08-07T06:30:20Z",
    "authors" : [
      {
        "name" : "Kim Wuyts"
      },
      {
        "name" : "Avi Douglen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04315v1",
    "title" : "Federated Cubic Regularized Newton Learning with\n  Sparsification-amplified Differential Privacy",
    "summary" : "This paper investigates the use of the cubic-regularized Newton method within\na federated learning framework while addressing two major concerns that\ncommonly arise in federated learning: privacy leakage and communication\nbottleneck. We introduce a federated learning algorithm called Differentially\nPrivate Federated Cubic Regularized Newton (DP-FCRN). By leveraging\nsecond-order techniques, our algorithm achieves lower iteration complexity\ncompared to first-order methods. We also incorporate noise perturbation during\nlocal computations to ensure privacy. Furthermore, we employ sparsification in\nuplink transmission, which not only reduces the communication costs but also\namplifies the privacy guarantee. Specifically, this approach reduces the\nnecessary noise intensity without compromising privacy protection. We analyze\nthe convergence properties of our algorithm and establish the privacy\nguarantee. Finally, we validate the effectiveness of the proposed algorithm\nthrough experiments on a benchmark dataset.",
    "updated" : "2024-08-08T08:48:54Z",
    "published" : "2024-08-08T08:48:54Z",
    "authors" : [
      {
        "name" : "Wei Huo"
      },
      {
        "name" : "Changxin Liu"
      },
      {
        "name" : "Kemi Ding"
      },
      {
        "name" : "Karl Henrik Johansson"
      },
      {
        "name" : "Ling Shi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04188v1",
    "title" : "Trustworthy Semantic-Enabled 6G Communication: A Task-oriented and\n  Privacy-preserving Perspective",
    "summary" : "Trustworthy task-oriented semantic communication (ToSC) emerges as an\ninnovative approach in the 6G landscape, characterized by the transmission of\nonly vital information that is directly pertinent to a specific task. While\nToSC offers an efficient mode of communication, it concurrently raises concerns\nregarding privacy, as sophisticated adversaries might possess the capability to\nreconstruct the original data from the transmitted features. This article\nprovides an in-depth analysis of privacy-preserving strategies specifically\ndesigned for ToSC relying on deep neural network-based joint source and channel\ncoding (DeepJSCC). The study encompasses a detailed comparative assessment of\ntrustworthy feature perturbation methods such as differential privacy and\nencryption, alongside intrinsic security incorporation approaches like\nadversarial learning to train the JSCC and learning-based vector quantization\n(LBVQ). This comparative analysis underscores the integration of advanced\nexplainable learning algorithms into communication systems, positing a new\nbenchmark for privacy standards in the forthcoming 6G era.",
    "updated" : "2024-08-08T03:16:42Z",
    "published" : "2024-08-08T03:16:42Z",
    "authors" : [
      {
        "name" : "Shuaishuai Guo"
      },
      {
        "name" : "Anbang Zhang"
      },
      {
        "name" : "Yanhu Wang"
      },
      {
        "name" : "Chenyuan Feng"
      },
      {
        "name" : "Tony Q. S. Quek"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.05212v1",
    "title" : "Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions",
    "summary" : "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.",
    "updated" : "2024-08-10T05:41:19Z",
    "published" : "2024-08-10T05:41:19Z",
    "authors" : [
      {
        "name" : "Michele Miranda"
      },
      {
        "name" : "Elena Sofia Ruzzetti"
      },
      {
        "name" : "Andrea Santilli"
      },
      {
        "name" : "Fabio Massimo Zanzotto"
      },
      {
        "name" : "Sébastien Bratières"
      },
      {
        "name" : "Emanuele Rodolà"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.05092v1",
    "title" : "PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural\n  Networks",
    "summary" : "The training phase of deep neural networks requires substantial resources and\nas such is often performed on cloud servers. However, this raises privacy\nconcerns when the training dataset contains sensitive content, e.g., face\nimages. In this work, we propose a method to perform the training phase of a\ndeep learning model on both an edge device and a cloud server that prevents\nsensitive content being transmitted to the cloud while retaining the desired\ninformation. The proposed privacy-preserving method uses adversarial early\nexits to suppress the sensitive content at the edge and transmits the\ntask-relevant information to the cloud. This approach incorporates noise\naddition during the training phase to provide a differential privacy guarantee.\nWe extensively test our method on different facial datasets with diverse face\nattributes using various deep learning architectures, showcasing its\noutstanding performance. We also demonstrate the effectiveness of privacy\npreservation through successful defenses against different white-box and deep\nreconstruction attacks.",
    "updated" : "2024-08-09T14:33:34Z",
    "published" : "2024-08-09T14:33:34Z",
    "authors" : [
      {
        "name" : "Yamin Sepehri"
      },
      {
        "name" : "Pedram Pad"
      },
      {
        "name" : "Pascal Frossard"
      },
      {
        "name" : "L. Andrea Dunbar"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.DC",
      "cs.LG",
      "eess.IV",
      "I.2.10; I.2.6; I.2.11; K.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04931v1",
    "title" : "Privacy-Preserved Taxi Demand Prediction System Utilizing Distributed\n  Data",
    "summary" : "Accurate taxi-demand prediction is essential for optimizing taxi operations\nand enhancing urban transportation services. However, using customers' data in\nthese systems raises significant privacy and security concerns. Traditional\nfederated learning addresses some privacy issues by enabling model training\nwithout direct data exchange but often struggles with accuracy due to varying\ndata distributions across different regions or service providers. In this\npaper, we propose CC-Net: a novel approach using collaborative learning\nenhanced with contrastive learning for taxi-demand prediction. Our method\nensures high performance by enabling multiple parties to collaboratively train\na demand-prediction model through hierarchical federated learning. In this\napproach, similar parties are clustered together, and federated learning is\napplied within each cluster. The similarity is defined without data exchange,\nensuring privacy and security. We evaluated our approach using real-world data\nfrom five taxi service providers in Japan over fourteen months. The results\ndemonstrate that CC-Net maintains the privacy of customers' data while\nimproving prediction accuracy by at least 2.2% compared to existing techniques.",
    "updated" : "2024-08-09T08:24:47Z",
    "published" : "2024-08-09T08:24:47Z",
    "authors" : [
      {
        "name" : "Ren Ozeki"
      },
      {
        "name" : "Haruki Yonekura"
      },
      {
        "name" : "Hamada Rizk"
      },
      {
        "name" : "Hirozumi Yamaguchi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04888v1",
    "title" : "Locally Private Histograms in All Privacy Regimes",
    "summary" : "Frequency estimation, a.k.a. histograms, is a workhorse of data analysis, and\nas such has been thoroughly studied under differentially privacy. In\nparticular, computing histograms in the local model of privacy has been the\nfocus of a fruitful recent line of work, and various algorithms have been\nproposed, achieving the order-optimal $\\ell_\\infty$ error in the high-privacy\n(small $\\varepsilon$) regime while balancing other considerations such as time-\nand communication-efficiency. However, to the best of our knowledge, the\npicture is much less clear when it comes to the medium- or low-privacy regime\n(large $\\varepsilon$), despite its increased relevance in practice. In this\npaper, we investigate locally private histograms, and the very related\ndistribution learning task, in this medium-to-low privacy regime, and establish\nnear-tight (and somewhat unexpected) bounds on the $\\ell_\\infty$ error\nachievable. Our theoretical findings emerge from a novel analysis, which\nappears to improve bounds across the board for the locally private histogram\nproblem. We back our theoretical findings by an empirical comparison of\nexisting algorithms in all privacy regimes, to assess their typical performance\nand behaviour beyond the worst-case setting.",
    "updated" : "2024-08-09T06:22:45Z",
    "published" : "2024-08-09T06:22:45Z",
    "authors" : [
      {
        "name" : "Clément L. Canonne"
      },
      {
        "name" : "Abigail Gentle"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.DM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04684v1",
    "title" : "Moving beyond privacy and airspace safety: Guidelines for just drones in\n  policing",
    "summary" : "The use of drones offers police forces potential gains in efficiency and\nsafety. However, their use may also harm public perception of the police if\ndrones are refused. Therefore, police forces should consider the perception of\nbystanders and broader society to maximize drones' potential. This article\nexamines the concerns expressed by members of the public during a field trial\ninvolving 52 test participants. Analysis of the group interviews suggests that\ntheir worries go beyond airspace safety and privacy, broadly discussed in\nexisting literature and regulations. The interpretation of the results\nindicates that the perceived justice of drone use is a significant factor in\nacceptance. Leveraging the concept of organizational justice and data\ncollected, we propose a catalogue of guidelines for just operation of drones to\nsupplement the existing policy. We present the organizational justice\nperspective as a framework to integrate the concerns of the public and\nbystanders into legal work. Finally, we discuss the relevance of justice for\nthe legitimacy of the police's actions and provide implications for research\nand practice.",
    "updated" : "2024-08-08T09:04:01Z",
    "published" : "2024-08-08T09:04:01Z",
    "authors" : [
      {
        "name" : "Mateusz Dolata"
      },
      {
        "name" : "Gerhard Schwabe"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  }
]