[
  {
    "id" : "http://arxiv.org/abs/2408.00639v1",
    "title" : "Privacy-preserving datasets by capturing feature distributions with\n  Conditional VAEs",
    "summary" : "Large and well-annotated datasets are essential for advancing deep learning\napplications, however often costly or impossible to obtain by a single entity.\nIn many areas, including the medical domain, approaches relying on data sharing\nhave become critical to address those challenges. While effective in increasing\ndataset size and diversity, data sharing raises significant privacy concerns.\nCommonly employed anonymization methods based on the k-anonymity paradigm often\nfail to preserve data diversity, affecting model robustness. This work\nintroduces a novel approach using Conditional Variational Autoencoders (CVAEs)\ntrained on feature vectors extracted from large pre-trained vision foundation\nmodels. Foundation models effectively detect and represent complex patterns\nacross diverse domains, allowing the CVAE to faithfully capture the embedding\nspace of a given data distribution to generate (sample) a diverse,\nprivacy-respecting, and potentially unbounded set of synthetic feature vectors.\nOur method notably outperforms traditional approaches in both medical and\nnatural image domains, exhibiting greater dataset diversity and higher\nrobustness against perturbations while preserving sample privacy. These results\nunderscore the potential of generative models to significantly impact deep\nlearning applications in data-scarce and privacy-sensitive environments. The\nsource code is available at\nhttps://github.com/francescodisalvo05/cvae-anonymization .",
    "updated" : "2024-08-01T15:26:24Z",
    "published" : "2024-08-01T15:26:24Z",
    "authors" : [
      {
        "name" : "Francesco Di Salvo"
      },
      {
        "name" : "David Tafler"
      },
      {
        "name" : "Sebastian Doerrich"
      },
      {
        "name" : "Christian Ledig"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00382v1",
    "title" : "Long-Term Conversation Analysis: Privacy-Utility Trade-off under Noise\n  and Reverberation",
    "summary" : "Recordings in everyday life require privacy preservation of the speech\ncontent and speaker identity. This contribution explores the influence of noise\nand reverberation on the trade-off between privacy and utility for low-cost\nprivacy-preserving methods feasible for edge computing. These methods\ncompromise spectral and temporal smoothing, speaker anonymization using the\nMcAdams coefficient, sampling with a very low sampling rate, and combinations.\nPrivacy is assessed by automatic speech and speaker recognition, while our\nutility considers voice activity detection and speaker diarization. Overall,\nour evaluation shows that additional noise degrades the performance of all\nmodels more than reverberation. This degradation corresponds to enhanced speech\nprivacy, while utility is less deteriorated for some methods.",
    "updated" : "2024-08-01T08:43:46Z",
    "published" : "2024-08-01T08:43:46Z",
    "authors" : [
      {
        "name" : "Jule Pohlhausen"
      },
      {
        "name" : "Francesco Nespoli"
      },
      {
        "name" : "Joerg Bitzer"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00294v1",
    "title" : "RDP: Ranked Differential Privacy for Facial Feature Protection in\n  Multiscale Sparsified Subspace",
    "summary" : "With the widespread sharing of personal face images in applications' public\ndatabases, face recognition systems faces real threat of being breached by\npotential adversaries who are able to access users' face images and use them to\nintrude the face recognition systems. In this paper, we propose a novel privacy\nprotection method in the multiscale sparsified feature subspaces to protect\nsensitive facial features, by taking care of the influence or weight ranked\nfeature coefficients on the privacy budget, named \"Ranked Differential Privacy\n(RDP)\". After the multiscale feature decomposition, the lightweight Laplacian\nnoise is added to the dimension-reduced sparsified feature coefficients\naccording to the geometric superposition method. Then, we rigorously prove that\nthe RDP satisfies Differential Privacy. After that, the nonlinear Lagrange\nMultiplier (LM) method is formulated for the constraint optimization problem of\nmaximizing the utility of the visualization quality protected face images with\nsanitizing noise, under a given facial features privacy budget. Then, two\nmethods are proposed to solve the nonlinear LM problem and obtain the optimal\nnoise scale parameters: 1) the analytical Normalization Approximation (NA)\nmethod with identical average noise scale parameter for real-time online\napplications; and 2) the LM optimization Gradient Descent (LMGD) numerical\nmethod to obtain the nonlinear solution through iterative updating for more\naccurate offline applications. Experimental results on two real-world datasets\nshow that our proposed RDP outperforms other state-of-the-art methods: at a\nprivacy budget of 0.2, the PSNR (Peak Signal-to-Noise Ratio) of the RDP is\nabout ~10 dB higher than (10 times as high as) the highest PSNR of all compared\nmethods.",
    "updated" : "2024-08-01T05:41:59Z",
    "published" : "2024-08-01T05:41:59Z",
    "authors" : [
      {
        "name" : "Lu Ou"
      },
      {
        "name" : "Shaolin Liao"
      },
      {
        "name" : "Shihui Gao"
      },
      {
        "name" : "Guandong Huang"
      },
      {
        "name" : "Zheng Qi"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01228v1",
    "title" : "The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models",
    "summary" : "Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.",
    "updated" : "2024-08-02T12:36:13Z",
    "published" : "2024-08-02T12:36:13Z",
    "authors" : [
      {
        "name" : "Simone Caldarella"
      },
      {
        "name" : "Massimiliano Mancini"
      },
      {
        "name" : "Elisa Ricci"
      },
      {
        "name" : "Rahaf Aljundi"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01040v1",
    "title" : "Privacy-Preserving Split Learning with Vision Transformers using\n  Patch-Wise Random and Noisy CutMix",
    "summary" : "In computer vision, the vision transformer (ViT) has increasingly superseded\nthe convolutional neural network (CNN) for improved accuracy and robustness.\nHowever, ViT's large model sizes and high sample complexity make it difficult\nto train on resource-constrained edge devices. Split learning (SL) emerges as a\nviable solution, leveraging server-side resources to train ViTs while utilizing\nprivate data from distributed devices. However, SL requires additional\ninformation exchange for weight updates between the device and the server,\nwhich can be exposed to various attacks on private training data. To mitigate\nthe risk of data breaches in classification tasks, inspired from the CutMix\nregularization, we propose a novel privacy-preserving SL framework that injects\nGaussian noise into smashed data and mixes randomly chosen patches of smashed\ndata across clients, coined DP-CutMixSL. Our analysis demonstrates that\nDP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy\nprotection against membership inference attacks during forward propagation.\nThrough simulations, we show that DP-CutMixSL improves privacy protection\nagainst membership inference attacks, reconstruction attacks, and label\ninference attacks, while also improving accuracy compared to DP-SL and\nDP-MixSL.",
    "updated" : "2024-08-02T06:24:39Z",
    "published" : "2024-08-02T06:24:39Z",
    "authors" : [
      {
        "name" : "Seungeun Oh"
      },
      {
        "name" : "Sihun Baek"
      },
      {
        "name" : "Jihong Park"
      },
      {
        "name" : "Hyelin Nam"
      },
      {
        "name" : "Praneeth Vepakomma"
      },
      {
        "name" : "Ramesh Raskar"
      },
      {
        "name" : "Mehdi Bennis"
      },
      {
        "name" : "Seong-Lyun Kim"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.00950v1",
    "title" : "PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking\n  Services",
    "summary" : "Eye gaze contains rich information about human attention and cognitive\nprocesses. This capability makes the underlying technology, known as gaze\ntracking, a critical enabler for many ubiquitous applications and has triggered\nthe development of easy-to-use gaze estimation services. Indeed, by utilizing\nthe ubiquitous cameras on tablets and smartphones, users can readily access\nmany gaze estimation services. In using these services, users must provide\ntheir full-face images to the gaze estimator, which is often a black box. This\nposes significant privacy threats to the users, especially when a malicious\nservice provider gathers a large collection of face images to classify\nsensitive user attributes. In this work, we present PrivateGaze, the first\napproach that can effectively preserve users' privacy in black-box gaze\ntracking services without compromising gaze estimation performance.\nSpecifically, we proposed a novel framework to train a privacy preserver that\nconverts full-face images into obfuscated counterparts, which are effective for\ngaze estimation while containing no privacy information. Evaluation on four\ndatasets shows that the obfuscated image can protect users' private\ninformation, such as identity and gender, against unauthorized attribute\nclassification. Meanwhile, when used directly by the black-box gaze estimator\nas inputs, the obfuscated images lead to comparable tracking performance to the\nconventional, unprotected full-face images.",
    "updated" : "2024-08-01T23:11:03Z",
    "published" : "2024-08-01T23:11:03Z",
    "authors" : [
      {
        "name" : "Lingyu Du"
      },
      {
        "name" : "Jinyuan Jia"
      },
      {
        "name" : "Xucong Zhang"
      },
      {
        "name" : "Guohao Lan"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.02373v1",
    "title" : "Operationalizing Contextual Integrity in Privacy-Conscious Assistants",
    "summary" : "Advanced AI assistants combine frontier LLMs and tool access to autonomously\nperform complex tasks on behalf of users. While the helpfulness of such\nassistants can increase dramatically with access to user information including\nemails and documents, this raises privacy concerns about assistants sharing\ninappropriate information with third parties without user supervision. To steer\ninformation-sharing assistants to behave in accordance with privacy\nexpectations, we propose to operationalize $\\textit{contextual integrity}$\n(CI), a framework that equates privacy with the appropriate flow of information\nin a given context. In particular, we design and evaluate a number of\nstrategies to steer assistants' information-sharing actions to be CI compliant.\nOur evaluation is based on a novel form filling benchmark composed of synthetic\ndata and human annotations, and it reveals that prompting frontier LLMs to\nperform CI-based reasoning yields strong results.",
    "updated" : "2024-08-05T10:53:51Z",
    "published" : "2024-08-05T10:53:51Z",
    "authors" : [
      {
        "name" : "Sahra Ghalebikesabi"
      },
      {
        "name" : "Eugene Bagdasaryan"
      },
      {
        "name" : "Ren Yi"
      },
      {
        "name" : "Itay Yona"
      },
      {
        "name" : "Ilia Shumailov"
      },
      {
        "name" : "Aneesh Pappu"
      },
      {
        "name" : "Chongyang Shi"
      },
      {
        "name" : "Laura Weidinger"
      },
      {
        "name" : "Robert Stanforth"
      },
      {
        "name" : "Leonard Berrada"
      },
      {
        "name" : "Pushmeet Kohli"
      },
      {
        "name" : "Po-Sen Huang"
      },
      {
        "name" : "Borja Balle"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01711v1",
    "title" : "Privacy in networks of quantum sensors",
    "summary" : "We treat privacy in a network of quantum sensors where accessible information\nis limited to specific functions of the network parameters, and all other\ninformation remains private. We develop an analysis of privacy in terms of a\nmanipulation of the quantum Fisher information matrix, and find the optimal\nstate achieving maximum privacy in the estimation of linear combination of the\nunknown parameters in a network of quantum sensors. We also discuss the effect\nof uncorrelated noise on the privacy of the network. Moreover, we illustrate\nour results with an example where the goal is to estimate the average value of\nthe unknown parameters in the network. In this example, we also introduce the\nnotion of quasi-privacy ($\\epsilon$-privacy), quantifying how close the state\nis to being private.",
    "updated" : "2024-08-03T08:39:44Z",
    "published" : "2024-08-03T08:39:44Z",
    "authors" : [
      {
        "name" : "Majid Hassani"
      },
      {
        "name" : "Santiago Scheiner"
      },
      {
        "name" : "Matteo G. A. Paris"
      },
      {
        "name" : "Damian Markham"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01609v1",
    "title" : "Fed-RD: Privacy-Preserving Federated Learning for Financial Crime\n  Detection",
    "summary" : "We introduce Federated Learning for Relational Data (Fed-RD), a novel\nprivacy-preserving federated learning algorithm specifically developed for\nfinancial transaction datasets partitioned vertically and horizontally across\nparties. Fed-RD strategically employs differential privacy and secure\nmultiparty computation to guarantee the privacy of training data. We provide\ntheoretical analysis of the end-to-end privacy of the training algorithm and\npresent experimental results on realistic synthetic datasets. Our results\ndemonstrate that Fed-RD achieves high model accuracy with minimal degradation\nas privacy increases, while consistently surpassing benchmark results.",
    "updated" : "2024-08-03T00:07:10Z",
    "published" : "2024-08-03T00:07:10Z",
    "authors" : [
      {
        "name" : "Md. Saikat Islam Khan"
      },
      {
        "name" : "Aparna Gupta"
      },
      {
        "name" : "Oshani Seneviratne"
      },
      {
        "name" : "Stacy Patterson"
      }
    ],
    "categories" : [
      "cs.CE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.03185v1",
    "title" : "MaskAnyone Toolkit: Offering Strategies for Minimizing Privacy Risks and\n  Maximizing Utility in Audio-Visual Data Archiving",
    "summary" : "This paper introduces MaskAnyone, a novel toolkit designed to navigate some\nprivacy and ethical concerns of sharing audio-visual data in research.\nMaskAnyone offers a scalable, user-friendly solution for de-identifying\nindividuals in video and audio content through face-swapping and voice\nalteration, supporting multi-person masking and real-time bulk processing. By\nintegrating this tool within research practices, we aim to enhance data\nreproducibility and utility in social science research. Our approach draws on\nDesign Science Research, proposing that MaskAnyone can facilitate safer data\nsharing and potentially reduce the storage of fully identifiable data. We\ndiscuss the development and capabilities of MaskAnyone, explore its integration\ninto ethical research practices, and consider the broader implications of\naudio-visual data masking, including issues of consent and the risk of misuse.\nThe paper concludes with a preliminary evaluation framework for assessing the\neffectiveness and ethical integration of masking tools in such research\nsettings.",
    "updated" : "2024-08-06T13:35:27Z",
    "published" : "2024-08-06T13:35:27Z",
    "authors" : [
      {
        "name" : "Babajide Alamu Owoyele"
      },
      {
        "name" : "Martin Schilling"
      },
      {
        "name" : "Rohan Sawahn"
      },
      {
        "name" : "Niklas Kaemer"
      },
      {
        "name" : "Pavel Zherebenkov"
      },
      {
        "name" : "Bhuvanesh Verma"
      },
      {
        "name" : "Wim Pouw"
      },
      {
        "name" : "Gerard de Melo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.02927v1",
    "title" : "HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy\n  Protection",
    "summary" : "Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.",
    "updated" : "2024-08-06T03:21:13Z",
    "published" : "2024-08-06T03:21:13Z",
    "authors" : [
      {
        "name" : "Yuxin Wang"
      },
      {
        "name" : "Duanyu Feng"
      },
      {
        "name" : "Yongfu Dai"
      },
      {
        "name" : "Zhengyu Chen"
      },
      {
        "name" : "Jimin Huang"
      },
      {
        "name" : "Sophia Ananiadou"
      },
      {
        "name" : "Qianqian Xie"
      },
      {
        "name" : "Hao Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.02750v1",
    "title" : "Privacy-Safe Iris Presentation Attack Detection",
    "summary" : "This paper proposes a framework for a privacy-safe iris presentation attack\ndetection (PAD) method, designed solely with synthetically-generated,\nidentity-leakage-free iris images. Once trained, the method is evaluated in a\nclassical way using state-of-the-art iris PAD benchmarks. We designed two\ngenerative models for the synthesis of ISO/IEC 19794-6-compliant iris images.\nThe first model synthesizes bona fide-looking samples. To avoid ``identity\nleakage,'' the generated samples that accidentally matched those used in the\nmodel's training were excluded. The second model synthesizes images of irises\nwith textured contact lenses and is conditioned by a given contact lens brand\nto have better control over textured contact lens appearance when forming the\ntraining set. Our experiments demonstrate that models trained solely on\nsynthetic data achieve a lower but still reasonable performance when compared\nto solutions trained with iris images collected from human subjects. This is\nthe first-of-its-kind attempt to use solely synthetic data to train a\nfully-functional iris PAD solution, and despite the performance gap between\nregular and the proposed methods, this study demonstrates that with the\nincreasing fidelity of generative models, creating such privacy-safe iris PAD\nmethods may be possible. The source codes and generative models trained for\nthis work are offered along with the paper.",
    "updated" : "2024-08-05T18:09:02Z",
    "published" : "2024-08-05T18:09:02Z",
    "authors" : [
      {
        "name" : "Mahsa Mitcheff"
      },
      {
        "name" : "Patrick Tinsley"
      },
      {
        "name" : "Adam Czajka"
      }
    ],
    "categories" : [
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.03897v1",
    "title" : "Speech privacy-preserving methods using secret key for convolutional\n  neural network models and their robustness evaluation",
    "summary" : "In this paper, we propose privacy-preserving methods with a secret key for\nconvolutional neural network (CNN)-based models in speech processing tasks. In\nenvironments where untrusted third parties, like cloud servers, provide\nCNN-based systems, ensuring the privacy of speech queries becomes essential.\nThis paper proposes encryption methods for speech queries using secret keys and\na model structure that allows for encrypted queries to be accepted without\ndecryption. Our approach introduces three types of secret keys: Shuffling,\nFlipping, and random orthogonal matrix (ROM). In experiments, we demonstrate\nthat when the proposed methods are used with the correct key, identification\nperformance did not degrade. Conversely, when an incorrect key is used, the\nperformance significantly decreased. Particularly, with the use of ROM, we show\nthat even with a relatively small key space, high privacy-preserving\nperformance can be maintained many speech processing tasks. Furthermore, we\nalso demonstrate the difficulty of recovering original speech from encrypted\nqueries in various robustness evaluations.",
    "updated" : "2024-08-07T16:51:39Z",
    "published" : "2024-08-07T16:51:39Z",
    "authors" : [
      {
        "name" : "Shoko Niwa"
      },
      {
        "name" : "Sayaka Shiota"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.CR",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.03578v1",
    "title" : "Unraveling Privacy Threat Modeling Complexity: Conceptual Privacy\n  Analysis Layers",
    "summary" : "Analyzing privacy threats in software products is an essential part of\nsoftware development to ensure systems are privacy-respecting; yet it is still\na far from trivial activity. While there have been many advancements in the\npast decade, they tend to focus on describing 'what' the threats are. What\nisn't entirely clear yet is 'how' to actually find these threats. Privacy is a\ncomplex domain. We propose to use four conceptual layers (feature, ecosystem,\nbusiness context, and environment) to capture this privacy complexity. These\nlayers can be used as a frame to structure and specify the privacy analysis\nsupport in a more tangible and actionable way, thereby improving applicability\nof the analysis process.",
    "updated" : "2024-08-07T06:30:20Z",
    "published" : "2024-08-07T06:30:20Z",
    "authors" : [
      {
        "name" : "Kim Wuyts"
      },
      {
        "name" : "Avi Douglen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04315v1",
    "title" : "Federated Cubic Regularized Newton Learning with\n  Sparsification-amplified Differential Privacy",
    "summary" : "This paper investigates the use of the cubic-regularized Newton method within\na federated learning framework while addressing two major concerns that\ncommonly arise in federated learning: privacy leakage and communication\nbottleneck. We introduce a federated learning algorithm called Differentially\nPrivate Federated Cubic Regularized Newton (DP-FCRN). By leveraging\nsecond-order techniques, our algorithm achieves lower iteration complexity\ncompared to first-order methods. We also incorporate noise perturbation during\nlocal computations to ensure privacy. Furthermore, we employ sparsification in\nuplink transmission, which not only reduces the communication costs but also\namplifies the privacy guarantee. Specifically, this approach reduces the\nnecessary noise intensity without compromising privacy protection. We analyze\nthe convergence properties of our algorithm and establish the privacy\nguarantee. Finally, we validate the effectiveness of the proposed algorithm\nthrough experiments on a benchmark dataset.",
    "updated" : "2024-08-08T08:48:54Z",
    "published" : "2024-08-08T08:48:54Z",
    "authors" : [
      {
        "name" : "Wei Huo"
      },
      {
        "name" : "Changxin Liu"
      },
      {
        "name" : "Kemi Ding"
      },
      {
        "name" : "Karl Henrik Johansson"
      },
      {
        "name" : "Ling Shi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04188v1",
    "title" : "Trustworthy Semantic-Enabled 6G Communication: A Task-oriented and\n  Privacy-preserving Perspective",
    "summary" : "Trustworthy task-oriented semantic communication (ToSC) emerges as an\ninnovative approach in the 6G landscape, characterized by the transmission of\nonly vital information that is directly pertinent to a specific task. While\nToSC offers an efficient mode of communication, it concurrently raises concerns\nregarding privacy, as sophisticated adversaries might possess the capability to\nreconstruct the original data from the transmitted features. This article\nprovides an in-depth analysis of privacy-preserving strategies specifically\ndesigned for ToSC relying on deep neural network-based joint source and channel\ncoding (DeepJSCC). The study encompasses a detailed comparative assessment of\ntrustworthy feature perturbation methods such as differential privacy and\nencryption, alongside intrinsic security incorporation approaches like\nadversarial learning to train the JSCC and learning-based vector quantization\n(LBVQ). This comparative analysis underscores the integration of advanced\nexplainable learning algorithms into communication systems, positing a new\nbenchmark for privacy standards in the forthcoming 6G era.",
    "updated" : "2024-08-08T03:16:42Z",
    "published" : "2024-08-08T03:16:42Z",
    "authors" : [
      {
        "name" : "Shuaishuai Guo"
      },
      {
        "name" : "Anbang Zhang"
      },
      {
        "name" : "Yanhu Wang"
      },
      {
        "name" : "Chenyuan Feng"
      },
      {
        "name" : "Tony Q. S. Quek"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.05212v1",
    "title" : "Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions",
    "summary" : "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.",
    "updated" : "2024-08-10T05:41:19Z",
    "published" : "2024-08-10T05:41:19Z",
    "authors" : [
      {
        "name" : "Michele Miranda"
      },
      {
        "name" : "Elena Sofia Ruzzetti"
      },
      {
        "name" : "Andrea Santilli"
      },
      {
        "name" : "Fabio Massimo Zanzotto"
      },
      {
        "name" : "Sébastien Bratières"
      },
      {
        "name" : "Emanuele Rodolà"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.05092v1",
    "title" : "PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural\n  Networks",
    "summary" : "The training phase of deep neural networks requires substantial resources and\nas such is often performed on cloud servers. However, this raises privacy\nconcerns when the training dataset contains sensitive content, e.g., face\nimages. In this work, we propose a method to perform the training phase of a\ndeep learning model on both an edge device and a cloud server that prevents\nsensitive content being transmitted to the cloud while retaining the desired\ninformation. The proposed privacy-preserving method uses adversarial early\nexits to suppress the sensitive content at the edge and transmits the\ntask-relevant information to the cloud. This approach incorporates noise\naddition during the training phase to provide a differential privacy guarantee.\nWe extensively test our method on different facial datasets with diverse face\nattributes using various deep learning architectures, showcasing its\noutstanding performance. We also demonstrate the effectiveness of privacy\npreservation through successful defenses against different white-box and deep\nreconstruction attacks.",
    "updated" : "2024-08-09T14:33:34Z",
    "published" : "2024-08-09T14:33:34Z",
    "authors" : [
      {
        "name" : "Yamin Sepehri"
      },
      {
        "name" : "Pedram Pad"
      },
      {
        "name" : "Pascal Frossard"
      },
      {
        "name" : "L. Andrea Dunbar"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.DC",
      "cs.LG",
      "eess.IV",
      "I.2.10; I.2.6; I.2.11; K.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04931v1",
    "title" : "Privacy-Preserved Taxi Demand Prediction System Utilizing Distributed\n  Data",
    "summary" : "Accurate taxi-demand prediction is essential for optimizing taxi operations\nand enhancing urban transportation services. However, using customers' data in\nthese systems raises significant privacy and security concerns. Traditional\nfederated learning addresses some privacy issues by enabling model training\nwithout direct data exchange but often struggles with accuracy due to varying\ndata distributions across different regions or service providers. In this\npaper, we propose CC-Net: a novel approach using collaborative learning\nenhanced with contrastive learning for taxi-demand prediction. Our method\nensures high performance by enabling multiple parties to collaboratively train\na demand-prediction model through hierarchical federated learning. In this\napproach, similar parties are clustered together, and federated learning is\napplied within each cluster. The similarity is defined without data exchange,\nensuring privacy and security. We evaluated our approach using real-world data\nfrom five taxi service providers in Japan over fourteen months. The results\ndemonstrate that CC-Net maintains the privacy of customers' data while\nimproving prediction accuracy by at least 2.2% compared to existing techniques.",
    "updated" : "2024-08-09T08:24:47Z",
    "published" : "2024-08-09T08:24:47Z",
    "authors" : [
      {
        "name" : "Ren Ozeki"
      },
      {
        "name" : "Haruki Yonekura"
      },
      {
        "name" : "Hamada Rizk"
      },
      {
        "name" : "Hirozumi Yamaguchi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04888v1",
    "title" : "Locally Private Histograms in All Privacy Regimes",
    "summary" : "Frequency estimation, a.k.a. histograms, is a workhorse of data analysis, and\nas such has been thoroughly studied under differentially privacy. In\nparticular, computing histograms in the local model of privacy has been the\nfocus of a fruitful recent line of work, and various algorithms have been\nproposed, achieving the order-optimal $\\ell_\\infty$ error in the high-privacy\n(small $\\varepsilon$) regime while balancing other considerations such as time-\nand communication-efficiency. However, to the best of our knowledge, the\npicture is much less clear when it comes to the medium- or low-privacy regime\n(large $\\varepsilon$), despite its increased relevance in practice. In this\npaper, we investigate locally private histograms, and the very related\ndistribution learning task, in this medium-to-low privacy regime, and establish\nnear-tight (and somewhat unexpected) bounds on the $\\ell_\\infty$ error\nachievable. Our theoretical findings emerge from a novel analysis, which\nappears to improve bounds across the board for the locally private histogram\nproblem. We back our theoretical findings by an empirical comparison of\nexisting algorithms in all privacy regimes, to assess their typical performance\nand behaviour beyond the worst-case setting.",
    "updated" : "2024-08-09T06:22:45Z",
    "published" : "2024-08-09T06:22:45Z",
    "authors" : [
      {
        "name" : "Clément L. Canonne"
      },
      {
        "name" : "Abigail Gentle"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.DM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.04684v1",
    "title" : "Moving beyond privacy and airspace safety: Guidelines for just drones in\n  policing",
    "summary" : "The use of drones offers police forces potential gains in efficiency and\nsafety. However, their use may also harm public perception of the police if\ndrones are refused. Therefore, police forces should consider the perception of\nbystanders and broader society to maximize drones' potential. This article\nexamines the concerns expressed by members of the public during a field trial\ninvolving 52 test participants. Analysis of the group interviews suggests that\ntheir worries go beyond airspace safety and privacy, broadly discussed in\nexisting literature and regulations. The interpretation of the results\nindicates that the perceived justice of drone use is a significant factor in\nacceptance. Leveraging the concept of organizational justice and data\ncollected, we propose a catalogue of guidelines for just operation of drones to\nsupplement the existing policy. We present the organizational justice\nperspective as a framework to integrate the concerns of the public and\nbystanders into legal work. Finally, we discuss the relevance of justice for\nthe legitimacy of the police's actions and provide implications for research\nand practice.",
    "updated" : "2024-08-08T09:04:01Z",
    "published" : "2024-08-08T09:04:01Z",
    "authors" : [
      {
        "name" : "Mateusz Dolata"
      },
      {
        "name" : "Gerhard Schwabe"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.06197v1",
    "title" : "Lancelot: Towards Efficient and Privacy-Preserving Byzantine-Robust\n  Federated Learning within Fully Homomorphic Encryption",
    "summary" : "In sectors such as finance and healthcare, where data governance is subject\nto rigorous regulatory requirements, the exchange and utilization of data are\nparticularly challenging. Federated Learning (FL) has risen as a pioneering\ndistributed machine learning paradigm that enables collaborative model training\nacross multiple institutions while maintaining data decentralization. Despite\nits advantages, FL is vulnerable to adversarial threats, particularly poisoning\nattacks during model aggregation, a process typically managed by a central\nserver. However, in these systems, neural network models still possess the\ncapacity to inadvertently memorize and potentially expose individual training\ninstances. This presents a significant privacy risk, as attackers could\nreconstruct private data by leveraging the information contained in the model\nitself. Existing solutions fall short of providing a viable, privacy-preserving\nBRFL system that is both completely secure against information leakage and\ncomputationally efficient. To address these concerns, we propose Lancelot, an\ninnovative and computationally efficient BRFL framework that employs fully\nhomomorphic encryption (FHE) to safeguard against malicious client activities\nwhile preserving data privacy. Our extensive testing, which includes medical\nimaging diagnostics and widely-used public image datasets, demonstrates that\nLancelot significantly outperforms existing methods, offering more than a\ntwenty-fold increase in processing speed, all while maintaining data privacy.",
    "updated" : "2024-08-12T14:48:25Z",
    "published" : "2024-08-12T14:48:25Z",
    "authors" : [
      {
        "name" : "Siyang Jiang"
      },
      {
        "name" : "Hao Yang"
      },
      {
        "name" : "Qipeng Xie"
      },
      {
        "name" : "Chuan Ma"
      },
      {
        "name" : "Sen Wang"
      },
      {
        "name" : "Guoliang Xing"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.06167v1",
    "title" : "Blind-Match: Efficient Homomorphic Encryption-Based 1:N Matching for\n  Privacy-Preserving Biometric Identification",
    "summary" : "We present Blind-Match, a novel biometric identification system that\nleverages homomorphic encryption (HE) for efficient and privacy-preserving 1:N\nmatching. Blind-Match introduces a HE-optimized cosine similarity computation\nmethod, where the key idea is to divide the feature vector into smaller parts\nfor processing rather than computing the entire vector at once. By optimizing\nthe number of these parts, Blind-Match minimizes execution time while ensuring\ndata privacy through HE. Blind-Match achieves superior performance compared to\nstate-of-the-art methods across various biometric datasets. On the LFW face\ndataset, Blind-Match attains a 99.63% Rank-1 accuracy with a 128-dimensional\nfeature vector, demonstrating its robustness in face recognition tasks. For\nfingerprint identification, Blind-Match achieves a remarkable 99.55% Rank-1\naccuracy on the PolyU dataset, even with a compact 16-dimensional feature\nvector, significantly outperforming the state-of-the-art method, Blind-Touch,\nwhich achieves only 59.17%. Furthermore, Blind-Match showcases practical\nefficiency in large-scale biometric identification scenarios, such as Naver\nCloud's FaceSign, by processing 6,144 biometric samples in 0.74 seconds using a\n128-dimensional feature vector.",
    "updated" : "2024-08-12T14:13:08Z",
    "published" : "2024-08-12T14:13:08Z",
    "authors" : [
      {
        "name" : "Hyunmin Choi"
      },
      {
        "name" : "Jiwon Kim"
      },
      {
        "name" : "Chiyoung Song"
      },
      {
        "name" : "Simon S. Woo"
      },
      {
        "name" : "Hyoungshick Kim"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.05723v1",
    "title" : "Deep Learning with Data Privacy via Residual Perturbation",
    "summary" : "Protecting data privacy in deep learning (DL) is of crucial importance.\nSeveral celebrated privacy notions have been established and used for\nprivacy-preserving DL. However, many existing mechanisms achieve privacy at the\ncost of significant utility degradation and computational overhead. In this\npaper, we propose a stochastic differential equation-based residual\nperturbation for privacy-preserving DL, which injects Gaussian noise into each\nresidual mapping of ResNets. Theoretically, we prove that residual perturbation\nguarantees differential privacy (DP) and reduces the generalization gap of DL.\nEmpirically, we show that residual perturbation is computationally efficient\nand outperforms the state-of-the-art differentially private stochastic gradient\ndescent (DPSGD) in utility maintenance without sacrificing membership privacy.",
    "updated" : "2024-08-11T08:26:43Z",
    "published" : "2024-08-11T08:26:43Z",
    "authors" : [
      {
        "name" : "Wenqi Tao"
      },
      {
        "name" : "Huaming Ling"
      },
      {
        "name" : "Zuoqiang Shi"
      },
      {
        "name" : "Bao Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.05543v1",
    "title" : "PixelFade: Privacy-preserving Person Re-identification with Noise-guided\n  Progressive Replacement",
    "summary" : "Online person re-identification services face privacy breaches from potential\ndata leakage and recovery attacks, exposing cloud-stored images to malicious\nattackers and triggering public concern. The privacy protection of pedestrian\nimages is crucial. Previous privacy-preserving person re-identification methods\nare unable to resist recovery attacks and compromise accuracy. In this paper,\nwe propose an iterative method (PixelFade) to optimize pedestrian images into\nnoise-like images to resist recovery attacks. We first give an in-depth study\nof protected images from previous privacy methods, which reveal that the chaos\nof protected images can disrupt the learning of recovery models. Accordingly,\nSpecifically, we propose Noise-guided Objective Function with the feature\nconstraints of a specific authorization model, optimizing pedestrian images to\nnormal-distributed noise images while preserving their original identity\ninformation as per the authorization model. To solve the above non-convex\noptimization problem, we propose a heuristic optimization algorithm that\nalternately performs the Constraint Operation and the Partial Replacement\nOperation. This strategy not only safeguards that original pixels are replaced\nwith noises to protect privacy, but also guides the images towards an improved\noptimization direction to effectively preserve discriminative features.\nExtensive experiments demonstrate that our PixelFade outperforms previous\nmethods in resisting recovery attacks and Re-ID performance. The code is\navailable at https://github.com/iSEE-Laboratory/PixelFade.",
    "updated" : "2024-08-10T12:52:54Z",
    "published" : "2024-08-10T12:52:54Z",
    "authors" : [
      {
        "name" : "Delong Zhang"
      },
      {
        "name" : "Yi-Xing Peng"
      },
      {
        "name" : "Xiao-Ming Wu"
      },
      {
        "name" : "Ancong Wu"
      },
      {
        "name" : "Wei-Shi Zheng"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07021v1",
    "title" : "Improved Counting under Continual Observation with Pure Differential\n  Privacy",
    "summary" : "Counting under continual observation is a well-studied problem in the area of\ndifferential privacy. Given a stream of updates $x_1,x_2,\\dots,x_T \\in \\{0,1\\}$\nthe problem is to continuously release estimates of the prefix sums\n$\\sum_{i=1}^t x_i$ for $t=1,\\dots,T$ while protecting each input $x_i$ in the\nstream with differential privacy. Recently, significant leaps have been made in\nour understanding of this problem under $\\textit{approximate}$ differential\nprivacy, aka. $(\\varepsilon,\\delta)$$\\textit{-differential privacy}$. However,\nfor the classical case of $\\varepsilon$-differential privacy, we are not aware\nof any improvement in mean squared error since the work of Honaker (TPDP 2015).\nIn this paper we present such an improvement, reducing the mean squared error\nby a factor of about 4, asymptotically. The key technique is a new\ngeneralization of the binary tree mechanism that uses a $k$-ary number system\nwith $\\textit{negative digits}$ to improve the privacy-accuracy trade-off. Our\nmechanism improves the mean squared error over all 'optimal'\n$(\\varepsilon,\\delta)$-differentially private factorization mechanisms based on\nGaussian noise whenever $\\delta$ is sufficiently small. Specifically, using\n$k=19$ we get an asymptotic improvement over the bound given in the work by\nHenzinger, Upadhyay and Upadhyay (SODA 2023) when $\\delta = O(T^{-0.92})$.",
    "updated" : "2024-08-13T16:36:33Z",
    "published" : "2024-08-13T16:36:33Z",
    "authors" : [
      {
        "name" : "Joel Daniel Andersson"
      },
      {
        "name" : "Rasmus Pagh"
      },
      {
        "name" : "Sahel Torkamani"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07006v1",
    "title" : "The Complexities of Differential Privacy for Survey Data",
    "summary" : "The concept of differential privacy (DP) has gained substantial attention in\nrecent years, most notably since the U.S. Census Bureau announced the adoption\nof the concept for its 2020 Decennial Census. However, despite its attractive\ntheoretical properties, implementing DP in practice remains challenging,\nespecially when it comes to survey data. In this paper we present some results\nfrom an ongoing project funded by the U.S. Census Bureau that is exploring the\npossibilities and limitations of DP for survey data. Specifically, we identify\nfive aspects that need to be considered when adopting DP in the survey context:\nthe multi-staged nature of data production; the limited privacy amplification\nfrom complex sampling designs; the implications of survey-weighted estimates;\nthe weighting adjustments for nonresponse and other data deficiencies, and the\nimputation of missing values. We summarize the project's key findings with\nrespect to each of these aspects and also discuss some of the challenges that\nstill need to be addressed before DP could become the new data protection\nstandard at statistical agencies.",
    "updated" : "2024-08-13T16:15:42Z",
    "published" : "2024-08-13T16:15:42Z",
    "authors" : [
      {
        "name" : "Jörg Drechsler"
      },
      {
        "name" : "James Bailie"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07004v1",
    "title" : "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based\n  Large Language Models",
    "summary" : "Web-based Large Language Model (LLM) services have been widely adopted and\nhave become an integral part of our Internet experience. Third-party plugins\nenhance the functionalities of LLM by enabling access to real-world data and\nservices. However, the privacy consequences associated with these services and\ntheir third-party plugins are not well understood. Sensitive prompt data are\nstored, processed, and shared by cloud-based LLM providers and third-party\nplugins. In this paper, we propose Casper, a prompt sanitization technique that\naims to protect user privacy by detecting and removing sensitive information\nfrom user inputs before sending them to LLM services. Casper runs entirely on\nthe user's device as a browser extension and does not require any changes to\nthe online LLM services. At the core of Casper is a three-layered sanitization\nmechanism consisting of a rule-based filter, a Machine Learning (ML)-based\nnamed entity recognizer, and a browser-based local LLM topic identifier. We\nevaluate Casper on a dataset of 4000 synthesized prompts and show that it can\neffectively filter out Personal Identifiable Information (PII) and\nprivacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.",
    "updated" : "2024-08-13T16:08:37Z",
    "published" : "2024-08-13T16:08:37Z",
    "authors" : [
      {
        "name" : "Chun Jie Chong"
      },
      {
        "name" : "Chenxi Hou"
      },
      {
        "name" : "Zhihao Yao"
      },
      {
        "name" : "Seyed Mohammadjavad Seyed Talebi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.06460v1",
    "title" : "Evaluating Privacy Measures for Load Hiding",
    "summary" : "In smart grids, the use of smart meters to measure electricity consumption at\na household level raises privacy concerns. To address them, researchers have\ndesigned various load hiding algorithms that manipulate the electricity\nconsumption measured. To compare how well these algorithms preserve privacy,\nvarious privacy measures have been proposed. However, there currently is no\nconsensus on which privacy measure is most appropriate to use. In this study,\nwe aim to identify the most effective privacy measure(s) for load hiding\nalgorithms. We have crafted a series of experiments to assess the effectiveness\nof these measures. found 20 of the 25 measures studied to be ineffective. Next,\nfocused on the well-known \"appliance usage\" secret, we have designed synthetic\ndata to find the measure that best deals with this secret. We observe that such\na measure, a variant of mutual information, actually exists.",
    "updated" : "2024-08-12T19:21:34Z",
    "published" : "2024-08-12T19:21:34Z",
    "authors" : [
      {
        "name" : "Vadim Arzamasov"
      },
      {
        "name" : "Klemens Böhm"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT",
      "94A60, 93A14",
      "K.6.5; H.3.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.06395v1",
    "title" : "Fast John Ellipsoid Computation with Differential Privacy Optimization",
    "summary" : "Determining the John ellipsoid - the largest volume ellipsoid contained\nwithin a convex polytope - is a fundamental problem with applications in\nmachine learning, optimization, and data analytics. Recent work has developed\nfast algorithms for approximating the John ellipsoid using sketching and\nleverage score sampling techniques. However, these algorithms do not provide\nprivacy guarantees for sensitive input data. In this paper, we present the\nfirst differentially private algorithm for fast John ellipsoid computation. Our\nmethod integrates noise perturbation with sketching and leverage score sampling\nto achieve both efficiency and privacy. We prove that (1) our algorithm\nprovides $(\\epsilon,\\delta)$-differential privacy, and the privacy guarantee\nholds for neighboring datasets that are $\\epsilon_0$-close, allowing\nflexibility in the privacy definition; (2) our algorithm still converges to a\n$(1+\\xi)$-approximation of the optimal John ellipsoid in\n$O(\\xi^{-2}(\\log(n/\\delta_0) + (L\\epsilon_0)^{-2}))$ iterations where $n$ is\nthe number of data point, $L$ is the Lipschitz constant, $\\delta_0$ is the\nfailure probability, and $\\epsilon_0$ is the closeness of neighboring input\ndatasets. Our theoretical analysis demonstrates the algorithm's convergence and\nprivacy properties, providing a robust approach for balancing utility and\nprivacy in John ellipsoid computation. This is the first differentially private\nalgorithm for fast John ellipsoid computation, opening avenues for future\nresearch in privacy-preserving optimization techniques.",
    "updated" : "2024-08-12T03:47:55Z",
    "published" : "2024-08-12T03:47:55Z",
    "authors" : [
      {
        "name" : "Jiuxiang Gu"
      },
      {
        "name" : "Xiaoyu Li"
      },
      {
        "name" : "Yingyu Liang"
      },
      {
        "name" : "Zhenmei Shi"
      },
      {
        "name" : "Zhao Song"
      },
      {
        "name" : "Junwei Yu"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07614v1",
    "title" : "Practical Considerations for Differential Privacy",
    "summary" : "Differential privacy is the gold standard for statistical data release. Used\nby governments, companies, and academics, its mathematically rigorous\nguarantees and worst-case assumptions on the strength and knowledge of\nattackers make it a robust and compelling framework for reasoning about\nprivacy. However, even with landmark successes, differential privacy has not\nachieved widespread adoption in everyday data use and data protection. In this\nwork we examine some of the practical obstacles that stand in the way.",
    "updated" : "2024-08-14T15:28:28Z",
    "published" : "2024-08-14T15:28:28Z",
    "authors" : [
      {
        "name" : "Kareem Amin"
      },
      {
        "name" : "Alex Kulesza"
      },
      {
        "name" : "Sergei Vassilvitskii"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08107v1",
    "title" : "Communication-robust and Privacy-safe Distributed Estimation for\n  Heterogeneous Community-level Behind-the-meter Solar Power Generation",
    "summary" : "The rapid growth of behind-the-meter (BTM) solar power generation systems\npresents challenges for distribution system planning and scheduling due to\ninvisible solar power generation. To address the data leakage problem of\ncentralized machine-learning methods in BTM solar power generation estimation,\nthe federated learning (FL) method has been investigated for its distributed\nlearning capability. However, the conventional FL method has encountered\nvarious challenges, including heterogeneity, communication failures, and\nmalicious privacy attacks. To overcome these challenges, this study proposes a\ncommunication-robust and privacy-safe distributed estimation method for\nheterogeneous community-level BTM solar power generation. Specifically, this\nstudy adopts multi-task FL as the main structure and learns the common and\nunique features of all communities. Simultaneously, it embeds an updated\nparameters estimation method into the multi-task FL, automatically identifies\nsimilarities between any two clients, and estimates the updated parameters for\nunavailable clients to mitigate the negative effects of communication failures.\nFinally, this study adopts a differential privacy mechanism under the dynamic\nprivacy budget allocation strategy to combat malicious privacy attacks and\nimprove model training efficiency. Case studies show that in the presence of\nheterogeneity and communication failures, the proposed method exhibits better\nestimation accuracy and convergence performance as compared with traditional FL\nand localized learning methods, while providing stronger privacy protection.",
    "updated" : "2024-08-15T12:11:03Z",
    "published" : "2024-08-15T12:11:03Z",
    "authors" : [
      {
        "name" : "Jinglei Feng"
      },
      {
        "name" : "Zhengshuo Li"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08002v1",
    "title" : "Practical Privacy-Preserving Identity Verification using Third-Party\n  Cloud Services and FHE (Role of Data Encoding in Circuit Depth Management)",
    "summary" : "National digital identity verification systems have played a critical role in\nthe effective distribution of goods and services, particularly, in developing\ncountries. Due to the cost involved in deploying and maintaining such systems,\ncombined with a lack of in-house technical expertise, governments seek to\noutsource this service to third-party cloud service providers to the extent\npossible. This leads to increased concerns regarding the privacy of users'\npersonal data. In this work, we propose a practical privacy-preserving digital\nidentity (ID) verification protocol where the third-party cloud services\nprocess the identity data encrypted using a (single-key) Fully Homomorphic\nEncryption (FHE) scheme such as BFV. Though the role of a trusted entity such\nas government is not completely eliminated, our protocol does significantly\nreduces the computation load on such parties.\n  A challenge in implementing a privacy-preserving ID verification protocol\nusing FHE is to support various types of queries such as exact and/or fuzzy\ndemographic and biometric matches including secure age comparisons. From a\ncryptographic engineering perspective, our main technical contribution is a\nuser data encoding scheme that encodes demographic and biometric user data in\nonly two BFV ciphertexts and yet facilitates us to outsource various types of\nID verification queries to a third-party cloud. Our encoding scheme also\nensures that the only computation done by the trusted entity is a\nquery-agnostic \"extended\" decryption. This is in stark contrast with recent\nworks that outsource all the non-arithmetic operations to a trusted server. We\nimplement our protocol using the Microsoft SEAL FHE library and demonstrate its\npracticality.",
    "updated" : "2024-08-15T08:12:07Z",
    "published" : "2024-08-15T08:12:07Z",
    "authors" : [
      {
        "name" : "Deep Inder Mohan"
      },
      {
        "name" : "Srinivas Vivek"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.07892v1",
    "title" : "Personhood credentials: Artificial intelligence and the value of\n  privacy-preserving tools to distinguish who is real online",
    "summary" : "Anonymity is an important principle online. However, malicious actors have\nlong used misleading identities to conduct fraud, spread disinformation, and\ncarry out other deceptive schemes. With the advent of increasingly capable AI,\nbad actors can amplify the potential scale and effectiveness of their\noperations, intensifying the challenge of balancing anonymity and\ntrustworthiness online. In this paper, we analyze the value of a new tool to\naddress this challenge: \"personhood credentials\" (PHCs), digital credentials\nthat empower users to demonstrate that they are real people -- not AIs -- to\nonline services, without disclosing any personal information. Such credentials\ncan be issued by a range of trusted institutions -- governments or otherwise. A\nPHC system, according to our definition, could be local or global, and does not\nneed to be biometrics-based. Two trends in AI contribute to the urgency of the\nchallenge: AI's increasing indistinguishability (i.e., lifelike content and\navatars, agentic activity) from people online, and AI's increasing scalability\n(i.e., cost-effectiveness, accessibility). Drawing on a long history of\nresearch into anonymous credentials and \"proof-of-personhood\" systems,\npersonhood credentials give people a way to signal their trustworthiness on\nonline platforms, and offer service providers new tools for reducing misuse by\nbad actors. In contrast, existing countermeasures to automated deception --\nsuch as CAPTCHAs -- are inadequate against sophisticated AI, while stringent\nidentity verification solutions are insufficiently private for many use-cases.\nAfter surveying the benefits of personhood credentials, we also examine\ndeployment risks and design challenges. We conclude with actionable next steps\nfor policymakers, technologists, and standards bodies to consider in\nconsultation with the public.",
    "updated" : "2024-08-15T02:41:25Z",
    "published" : "2024-08-15T02:41:25Z",
    "authors" : [
      {
        "name" : "Steven Adler"
      },
      {
        "name" : "Zoë Hitzig"
      },
      {
        "name" : "Shrey Jain"
      },
      {
        "name" : "Catherine Brewer"
      },
      {
        "name" : "Wayne Chang"
      },
      {
        "name" : "Renée DiResta"
      },
      {
        "name" : "Eddy Lazzarin"
      },
      {
        "name" : "Sean McGregor"
      },
      {
        "name" : "Wendy Seltzer"
      },
      {
        "name" : "Divya Siddarth"
      },
      {
        "name" : "Nouran Soliman"
      },
      {
        "name" : "Tobin South"
      },
      {
        "name" : "Connor Spelliscy"
      },
      {
        "name" : "Manu Sporny"
      },
      {
        "name" : "Varya Srivastava"
      },
      {
        "name" : "John Bailey"
      },
      {
        "name" : "Brian Christian"
      },
      {
        "name" : "Andrew Critch"
      },
      {
        "name" : "Ronnie Falcon"
      },
      {
        "name" : "Heather Flanagan"
      },
      {
        "name" : "Kim Hamilton Duffy"
      },
      {
        "name" : "Eric Ho"
      },
      {
        "name" : "Claire R. Leibowicz"
      },
      {
        "name" : "Srikanth Nadhamuni"
      },
      {
        "name" : "Alan Z. Rozenshtein"
      },
      {
        "name" : "David Schnurr"
      },
      {
        "name" : "Evan Shapiro"
      },
      {
        "name" : "Lacey Strahm"
      },
      {
        "name" : "Andrew Trask"
      },
      {
        "name" : "Zoe Weinberg"
      },
      {
        "name" : "Cedric Whitney"
      },
      {
        "name" : "Tom Zick"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08722v1",
    "title" : "A Novel Buffered Federated Learning Framework for Privacy-Driven Anomaly\n  Detection in IIoT",
    "summary" : "Industrial Internet of Things (IIoT) is highly sensitive to data privacy and\ncybersecurity threats. Federated Learning (FL) has emerged as a solution for\npreserving privacy, enabling private data to remain on local IIoT clients while\ncooperatively training models to detect network anomalies. However, both\nsynchronous and asynchronous FL architectures exhibit limitations, particularly\nwhen dealing with clients with varying speeds due to data heterogeneity and\nresource constraints. Synchronous architecture suffers from straggler effects,\nwhile asynchronous methods encounter communication bottlenecks. Additionally,\nFL models are prone to adversarial inference attacks aimed at disclosing\nprivate training data. To address these challenges, we propose a Buffered FL\n(BFL) framework empowered by homomorphic encryption for anomaly detection in\nheterogeneous IIoT environments. BFL utilizes a novel weighted average time\napproach to mitigate both straggler effects and communication bottlenecks,\nensuring fairness between clients with varying processing speeds through\ncollaboration with a buffer-based server. The performance results, derived from\ntwo datasets, show the superiority of BFL compared to state-of-the-art FL\nmethods, demonstrating improved accuracy and convergence speed while enhancing\nprivacy preservation.",
    "updated" : "2024-08-16T13:01:59Z",
    "published" : "2024-08-16T13:01:59Z",
    "authors" : [
      {
        "name" : "Samira Kamali Poorazad"
      },
      {
        "name" : "Chafika Benzaid"
      },
      {
        "name" : "Tarik Taleb"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08666v1",
    "title" : "A Multivocal Literature Review on Privacy and Fairness in Federated\n  Learning",
    "summary" : "Federated Learning presents a way to revolutionize AI applications by\neliminating the necessity for data sharing. Yet, research has shown that\ninformation can still be extracted during training, making additional\nprivacy-preserving measures such as differential privacy imperative. To\nimplement real-world federated learning applications, fairness, ranging from a\nfair distribution of performance to non-discriminative behaviour, must be\nconsidered. Particularly in high-risk applications (e.g. healthcare), avoiding\nthe repetition of past discriminatory errors is paramount. As recent research\nhas demonstrated an inherent tension between privacy and fairness, we conduct a\nmultivocal literature review to examine the current methods to integrate\nprivacy and fairness in federated learning. Our analyses illustrate that the\nrelationship between privacy and fairness has been neglected, posing a critical\nrisk for real-world applications. We highlight the need to explore the\nrelationship between privacy, fairness, and performance, advocating for the\ncreation of integrated federated learning frameworks.",
    "updated" : "2024-08-16T11:15:52Z",
    "published" : "2024-08-16T11:15:52Z",
    "authors" : [
      {
        "name" : "Beatrice Balbierer"
      },
      {
        "name" : "Lukas Heinlein"
      },
      {
        "name" : "Domenique Zipperling"
      },
      {
        "name" : "Niklas Kühl"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08642v1",
    "title" : "The Power of Bias: Optimizing Client Selection in Federated Learning\n  with Heterogeneous Differential Privacy",
    "summary" : "To preserve the data privacy, the federated learning (FL) paradigm emerges in\nwhich clients only expose model gradients rather than original data for\nconducting model training. To enhance the protection of model gradients in FL,\ndifferentially private federated learning (DPFL) is proposed which incorporates\ndifferentially private (DP) noises to obfuscate gradients before they are\nexposed. Yet, an essential but largely overlooked problem in DPFL is the\nheterogeneity of clients' privacy requirement, which can vary significantly\nbetween clients and extremely complicates the client selection problem in DPFL.\nIn other words, both the data quality and the influence of DP noises should be\ntaken into account when selecting clients. To address this problem, we conduct\nconvergence analysis of DPFL under heterogeneous privacy, a generic client\nselection strategy, popular DP mechanisms and convex loss. Based on convergence\nanalysis, we formulate the client selection problem to minimize the value of\nloss function in DPFL with heterogeneous privacy, which is a convex\noptimization problem and can be solved efficiently. Accordingly, we propose the\nDPFL-BCS (biased client selection) algorithm. The extensive experiment results\nwith real datasets under both convex and non-convex loss functions indicate\nthat DPFL-BCS can remarkably improve model utility compared with the SOTA\nbaselines.",
    "updated" : "2024-08-16T10:19:27Z",
    "published" : "2024-08-16T10:19:27Z",
    "authors" : [
      {
        "name" : "Jiating Ma"
      },
      {
        "name" : "Yipeng Zhou"
      },
      {
        "name" : "Qi Li"
      },
      {
        "name" : "Quan Z. Sheng"
      },
      {
        "name" : "Laizhong Cui"
      },
      {
        "name" : "Jiangchuan Liu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08529v1",
    "title" : "Privacy-Preserving Vision Transformer Using Images Encrypted with\n  Restricted Random Permutation Matrices",
    "summary" : "We propose a novel method for privacy-preserving fine-tuning vision\ntransformers (ViTs) with encrypted images. Conventional methods using encrypted\nimages degrade model performance compared with that of using plain images due\nto the influence of image encryption. In contrast, the proposed encryption\nmethod using restricted random permutation matrices can provide a higher\nperformance than the conventional ones.",
    "updated" : "2024-08-16T04:57:21Z",
    "published" : "2024-08-16T04:57:21Z",
    "authors" : [
      {
        "name" : "Kouki Horio"
      },
      {
        "name" : "Kiyoshi Nishikawa"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08475v1",
    "title" : "Models Matter: Setting Accurate Privacy Expectations for Local and\n  Central Differential Privacy",
    "summary" : "Differential privacy is a popular privacy-enhancing technology that has been\ndeployed both in industry and government agencies. Unfortunately, existing\nexplanations of differential privacy fail to set accurate privacy expectations\nfor data subjects, which depend on the choice of deployment model. We design\nand evaluate new explanations of differential privacy for the local and central\nmodels, drawing inspiration from prior work explaining other privacy-enhancing\ntechnologies. We find that consequences-focused explanations in the style of\nprivacy nutrition labels that lay out the implications of differential privacy\nare a promising approach for setting accurate privacy expectations. Further, we\nfind that while process-focused explanations are not enough to set accurate\nprivacy expectations, combining consequences-focused explanations with a brief\ndescription of how differential privacy works leads to greater trust.",
    "updated" : "2024-08-16T01:21:57Z",
    "published" : "2024-08-16T01:21:57Z",
    "authors" : [
      {
        "name" : "Mary Anne Smart"
      },
      {
        "name" : "Priyanka Nanayakkara"
      },
      {
        "name" : "Rachel Cummings"
      },
      {
        "name" : "Gabriel Kaptchuk"
      },
      {
        "name" : "Elissa Redmiles"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10053v1",
    "title" : "Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory",
    "summary" : "Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards.",
    "updated" : "2024-08-19T14:48:04Z",
    "published" : "2024-08-19T14:48:04Z",
    "authors" : [
      {
        "name" : "Haoran Li"
      },
      {
        "name" : "Wei Fan"
      },
      {
        "name" : "Yulin Chen"
      },
      {
        "name" : "Jiayang Cheng"
      },
      {
        "name" : "Tianshu Chu"
      },
      {
        "name" : "Xuebing Zhou"
      },
      {
        "name" : "Peizhao Hu"
      },
      {
        "name" : "Yangqiu Song"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.09943v1",
    "title" : "Calibrating Noise for Group Privacy in Subsampled Mechanisms",
    "summary" : "Given a group size m and a sensitive dataset D, group privacy (GP) releases\ninformation about D with the guarantee that the adversary cannot infer with\nhigh confidence whether the underlying data is D or a neighboring dataset D'\nthat differs from D by m records. GP generalizes the well-established notion of\ndifferential privacy (DP) for protecting individuals' privacy; in particular,\nwhen m=1, GP reduces to DP. Compared to DP, GP is capable of protecting the\nsensitive aggregate information of a group of up to m individuals, e.g., the\naverage annual income among members of a yacht club. Despite its longstanding\npresence in the research literature and its promising applications, GP is often\ntreated as an afterthought, with most approaches first developing a DP\nmechanism and then using a generic conversion to adapt it for GP, treating the\nDP solution as a black box. As we point out in the paper, this methodology is\nsuboptimal when the underlying DP solution involves subsampling, e.g., in the\nclassic DP-SGD method for training deep learning models. In this case, the\nDP-to-GP conversion is overly pessimistic in its analysis, leading to low\nutility in the published results under GP.\n  Motivated by this, we propose a novel analysis framework that provides tight\nprivacy accounting for subsampled GP mechanisms. Instead of converting a\nblack-box DP mechanism to GP, our solution carefully analyzes and utilizes the\ninherent randomness in subsampled mechanisms, leading to a substantially\nimproved bound on the privacy loss with respect to GP. The proposed solution\napplies to a wide variety of foundational mechanisms with subsampling.\nExtensive experiments with real datasets demonstrate that compared to the\nbaseline convert-from-blackbox-DP approach, our GP mechanisms achieve noise\nreductions of over an order of magnitude in several practical settings,\nincluding deep neural network training.",
    "updated" : "2024-08-19T12:32:50Z",
    "published" : "2024-08-19T12:32:50Z",
    "authors" : [
      {
        "name" : "Yangfan Jiang"
      },
      {
        "name" : "Xinjian Luo"
      },
      {
        "name" : "Yin Yang"
      },
      {
        "name" : "Xiaokui Xiao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.09935v1",
    "title" : "Privacy Technologies for Financial Intelligence",
    "summary" : "Financial crimes like terrorism financing and money laundering can have real\nimpacts on society, including the abuse and mismanagement of public funds,\nincrease in societal problems such as drug trafficking and illicit gambling\nwith attendant economic costs, and loss of innocent lives in the case of\nterrorism activities. Complex financial crimes can be hard to detect primarily\nbecause data related to different pieces of the overall puzzle is usually\ndistributed across a network of financial institutions, regulators, and\nlaw-enforcement agencies and they cannot be easily shared due to privacy\nconstraints. Recent advances in Privacy-Preserving Data Matching and Machine\nLearning provide an opportunity for regulators and the financial industry to\ncome together to solve the risk-discovery problem with technology. This paper\nprovides a survey of the financial intelligence landscape and where\nopportunities lie for privacy technologies to improve the state-of-the-art in\nfinancial-crime detection.",
    "updated" : "2024-08-19T12:13:53Z",
    "published" : "2024-08-19T12:13:53Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Thilina Ranbaduge"
      },
      {
        "name" : "Kee Siong Ng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.09659v1",
    "title" : "An Algorithm for Enhancing Privacy-Utility Tradeoff in the Privacy\n  Funnel and Other Lift-based Measures",
    "summary" : "This paper investigates the privacy funnel, a privacy-utility tradeoff\nproblem in which mutual information quantifies both privacy and utility. The\nobjective is to maximize utility while adhering to a specified privacy budget.\nHowever, the privacy funnel represents a non-convex optimization problem,\nmaking it challenging to achieve an optimal solution. An existing proposed\napproach to this problem involves substituting the mutual information with the\nlift (the exponent of information density) and then solving the optimization.\nSince mutual information is the expectation of the information density, this\nsubstitution overestimates the privacy loss and results in a final smaller\nbound on the privacy of mutual information than what is allowed in the budget.\nThis significantly compromises the utility. To overcome this limitation, we\npropose using a privacy measure that is more relaxed than the lift but stricter\nthan mutual information while still allowing the optimization to be efficiently\nsolved. Instead of directly using information density, our proposed measure is\nthe average of information density over the sensitive data distribution for\neach observed data realization. We then introduce a heuristic algorithm capable\nof achieving solutions that produce extreme privacy values, which enhances\nutility. The numerical results confirm improved utility at the same privacy\nbudget compared to existing solutions in the literature. Additionally, we\nexplore two other privacy measures, $\\ell_{1}$-norm and strong\n$\\chi^2$-divergence, demonstrating the applicability of our algorithm to these\nlift-based measures. We evaluate the performance of our method by comparing its\noutput with previous works. Finally, we validate our heuristic approach with a\ntheoretical framework that estimates the optimal utility for strong\n$\\chi^2$-divergence, numerically showing a perfect match.",
    "updated" : "2024-08-19T02:43:18Z",
    "published" : "2024-08-19T02:43:18Z",
    "authors" : [
      {
        "name" : "Mohammad Amin Zarrabian"
      },
      {
        "name" : "Parastoo Sadeghi"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08475v2",
    "title" : "Models Matter: Setting Accurate Privacy Expectations for Local and\n  Central Differential Privacy",
    "summary" : "Differential privacy is a popular privacy-enhancing technology that has been\ndeployed both in industry and government agencies. Unfortunately, existing\nexplanations of differential privacy fail to set accurate privacy expectations\nfor data subjects, which depend on the choice of deployment model. We design\nand evaluate new explanations of differential privacy for the local and central\nmodels, drawing inspiration from prior work explaining other privacy-enhancing\ntechnologies. We find that consequences-focused explanations in the style of\nprivacy nutrition labels that lay out the implications of differential privacy\nare a promising approach for setting accurate privacy expectations. Further, we\nfind that while process-focused explanations are not enough to set accurate\nprivacy expectations, combining consequences-focused explanations with a brief\ndescription of how differential privacy works leads to greater trust.",
    "updated" : "2024-08-19T01:04:07Z",
    "published" : "2024-08-16T01:21:57Z",
    "authors" : [
      {
        "name" : "Mary Anne Smart"
      },
      {
        "name" : "Priyanka Nanayakkara"
      },
      {
        "name" : "Rachel Cummings"
      },
      {
        "name" : "Gabriel Kaptchuk"
      },
      {
        "name" : "Elissa Redmiles"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08909v1",
    "title" : "An Adaptive Differential Privacy Method Based on Federated Learning",
    "summary" : "Differential privacy is one of the methods to solve the problem of privacy\nprotection in federated learning. Setting the same privacy budget for each\nround will result in reduced accuracy in training. The existing methods of the\nadjustment of privacy budget consider fewer influencing factors and tend to\nignore the boundaries, resulting in unreasonable privacy budgets. Therefore, we\nproposed an adaptive differential privacy method based on federated learning.\nThe method sets the adjustment coefficient and scoring function according to\naccuracy, loss, training rounds, and the number of datasets and clients. And\nthe privacy budget is adjusted based on them. Then the local model update is\nprocessed according to the scaling factor and the noise. Fi-nally, the server\naggregates the noised local model update and distributes the noised global\nmodel. The range of parameters and the privacy of the method are analyzed.\nThrough the experimental evaluation, it can reduce the privacy budget by about\n16%, while the accuracy remains roughly the same.",
    "updated" : "2024-08-13T13:08:11Z",
    "published" : "2024-08-13T13:08:11Z",
    "authors" : [
      {
        "name" : "Zhiqiang Wang"
      },
      {
        "name" : "Xinyue Yu"
      },
      {
        "name" : "Qianli Huang"
      },
      {
        "name" : "Yongguang Gong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.08904v1",
    "title" : "Privacy in Federated Learning",
    "summary" : "Federated Learning (FL) represents a significant advancement in distributed\nmachine learning, enabling multiple participants to collaboratively train\nmodels without sharing raw data. This decentralized approach enhances privacy\nby keeping data on local devices. However, FL introduces new privacy\nchallenges, as model updates shared during training can inadvertently leak\nsensitive information. This chapter delves into the core privacy concerns\nwithin FL, including the risks of data reconstruction, model inversion attacks,\nand membership inference. It explores various privacy-preserving techniques,\nsuch as Differential Privacy (DP) and Secure Multi-Party Computation (SMPC),\nwhich are designed to mitigate these risks. The chapter also examines the\ntrade-offs between model accuracy and privacy, emphasizing the importance of\nbalancing these factors in practical implementations. Furthermore, it discusses\nthe role of regulatory frameworks, such as GDPR, in shaping the privacy\nstandards for FL. By providing a comprehensive overview of the current state of\nprivacy in FL, this chapter aims to equip researchers and practitioners with\nthe knowledge necessary to navigate the complexities of secure federated\nlearning environments. The discussion highlights both the potential and\nlimitations of existing privacy-enhancing techniques, offering insights into\nfuture research directions and the development of more robust solutions.",
    "updated" : "2024-08-12T18:41:58Z",
    "published" : "2024-08-12T18:41:58Z",
    "authors" : [
      {
        "name" : "Jaydip Sen"
      },
      {
        "name" : "Hetvi Waghela"
      },
      {
        "name" : "Sneha Rakshit"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01228v2",
    "title" : "The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models",
    "summary" : "Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.",
    "updated" : "2024-08-19T13:35:05Z",
    "published" : "2024-08-02T12:36:13Z",
    "authors" : [
      {
        "name" : "Simone Caldarella"
      },
      {
        "name" : "Massimiliano Mancini"
      },
      {
        "name" : "Elisa Ricci"
      },
      {
        "name" : "Rahaf Aljundi"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10715v1",
    "title" : "Fine-Tuning a Local LLaMA-3 Large Language Model for Automated\n  Privacy-Preserving Physician Letter Generation in Radiation Oncology",
    "summary" : "Generating physician letters is a time-consuming task in daily clinical\npractice. This study investigates local fine-tuning of large language models\n(LLMs), specifically LLaMA models, for physician letter generation in a\nprivacy-preserving manner within the field of radiation oncology. Our findings\ndemonstrate that base LLaMA models, without fine-tuning, are inadequate for\neffectively generating physician letters. The QLoRA algorithm provides an\nefficient method for local intra-institutional fine-tuning of LLMs with limited\ncomputational resources (i.e., a single 48 GB GPU workstation within the\nhospital). The fine-tuned LLM successfully learns radiation oncology-specific\ninformation and generates physician letters in an institution-specific style.\nROUGE scores of the generated summary reports highlight the superiority of the\n8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician\nevaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has\nlimited capacity to generate content beyond the provided input data, it\nsuccessfully generates salutations, diagnoses and treatment histories,\nrecommendations for further treatment, and planned schedules. Overall, clinical\nbenefit was rated highly by the clinical experts (average score of 3.44 on a\n4-point scale). With careful physician review and correction, automated\nLLM-based physician letter generation has significant practical value.",
    "updated" : "2024-08-20T10:31:36Z",
    "published" : "2024-08-20T10:31:36Z",
    "authors" : [
      {
        "name" : "Yihao Hou"
      },
      {
        "name" : "Christoph Bert"
      },
      {
        "name" : "Ahmed Gomaa"
      },
      {
        "name" : "Godehard Lahmer"
      },
      {
        "name" : "Daniel Hoefler"
      },
      {
        "name" : "Thomas Weissmann"
      },
      {
        "name" : "Raphaela Voigt"
      },
      {
        "name" : "Philipp Schubert"
      },
      {
        "name" : "Charlotte Schmitter"
      },
      {
        "name" : "Alina Depardon"
      },
      {
        "name" : "Sabine Semrau"
      },
      {
        "name" : "Andreas Maier"
      },
      {
        "name" : "Rainer Fietkau"
      },
      {
        "name" : "Yixing Huang"
      },
      {
        "name" : "Florian Putz"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10648v1",
    "title" : "Smart Contract Coordinated Privacy Preserving Crowd-Sensing Campaigns",
    "summary" : "Crowd-sensing has emerged as a powerful data retrieval model, enabling\ndiverse applications by leveraging active user participation. However, data\navailability and privacy concerns pose significant challenges. Traditional\nmethods like data encryption and anonymization, while essential, may not fully\naddress these issues. For instance, in sparsely populated areas, anonymized\ndata can still be traced back to individual users. Additionally, the volume of\ndata generated by users can reveal their identities. To develop credible\ncrowd-sensing systems, data must be anonymized, aggregated and separated into\nuniformly sized chunks. Furthermore, decentralizing the data management\nprocess, rather than relying on a single server, can enhance security and\ntrust. This paper proposes a system utilizing smart contracts and blockchain\ntechnologies to manage crowd-sensing campaigns. The smart contract handles user\nsubscriptions, data encryption, and decentralized storage, creating a secure\ndata marketplace. Incentive policies within the smart contract encourage user\nparticipation and data diversity. Simulation results confirm the system's\nviability, highlighting the importance of user participation for data\ncredibility and the impact of geographical data scarcity on rewards. This\napproach aims to balance data origin and reduce cheating risks.",
    "updated" : "2024-08-20T08:41:57Z",
    "published" : "2024-08-20T08:41:57Z",
    "authors" : [
      {
        "name" : "Luca Bedogni"
      },
      {
        "name" : "Stefano Ferretti"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10647v1",
    "title" : "Privacy-preserving Universal Adversarial Defense for Black-box Models",
    "summary" : "Deep neural networks (DNNs) are increasingly used in critical applications\nsuch as identity authentication and autonomous driving, where robustness\nagainst adversarial attacks is crucial. These attacks can exploit minor\nperturbations to cause significant prediction errors, making it essential to\nenhance the resilience of DNNs. Traditional defense methods often rely on\naccess to detailed model information, which raises privacy concerns, as model\nowners may be reluctant to share such data. In contrast, existing black-box\ndefense methods fail to offer a universal defense against various types of\nadversarial attacks. To address these challenges, we introduce DUCD, a\nuniversal black-box defense method that does not require access to the target\nmodel's parameters or architecture. Our approach involves distilling the target\nmodel by querying it with data, creating a white-box surrogate while preserving\ndata privacy. We further enhance this surrogate model using a certified defense\nbased on randomized smoothing and optimized noise selection, enabling robust\ndefense against a broad range of adversarial attacks. Comparative evaluations\nbetween the certified defenses of the surrogate and target models demonstrate\nthe effectiveness of our approach. Experiments on multiple image classification\ndatasets show that DUCD not only outperforms existing black-box defenses but\nalso matches the accuracy of white-box defenses, all while enhancing data\nprivacy and reducing the success rate of membership inference attacks.",
    "updated" : "2024-08-20T08:40:39Z",
    "published" : "2024-08-20T08:40:39Z",
    "authors" : [
      {
        "name" : "Qiao Li"
      },
      {
        "name" : "Cong Wu"
      },
      {
        "name" : "Jing Chen"
      },
      {
        "name" : "Zijun Zhang"
      },
      {
        "name" : "Kun He"
      },
      {
        "name" : "Ruiying Du"
      },
      {
        "name" : "Xinxin Wang"
      },
      {
        "name" : "Qingchuang Zhao"
      },
      {
        "name" : "Yang Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "I.2.10"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10468v1",
    "title" : "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
    "summary" : "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96\\% to 73.71\\% on the PII-E\ndataset and 3.21\\% to 45.93\\% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
    "updated" : "2024-08-20T00:40:49Z",
    "published" : "2024-08-20T00:40:49Z",
    "authors" : [
      {
        "name" : "Jinxin Liu"
      },
      {
        "name" : "Zao Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.10442v1",
    "title" : "Feasibility of assessing cognitive impairment via distributed camera\n  network and privacy-preserving edge computing",
    "summary" : "INTRODUCTION: Mild cognitive impairment (MCI) is characterized by a decline\nin cognitive functions beyond typical age and education-related expectations.\nSince, MCI has been linked to reduced social interactions and increased aimless\nmovements, we aimed to automate the capture of these behaviors to enhance\nlongitudinal monitoring.\n  METHODS: Using a privacy-preserving distributed camera network, we collected\nmovement and social interaction data from groups of individuals with MCI\nundergoing therapy within a 1700$m^2$ space. We developed movement and social\ninteraction features, which were then used to train a series of machine\nlearning algorithms to distinguish between higher and lower cognitive\nfunctioning MCI groups.\n  RESULTS: A Wilcoxon rank-sum test revealed statistically significant\ndifferences between high and low-functioning cohorts in features such as linear\npath length, walking speed, change in direction while walking, entropy of\nvelocity and direction change, and number of group formations in the indoor\nspace. Despite lacking individual identifiers to associate with specific levels\nof MCI, a machine learning approach using the most significant features\nprovided a 71% accuracy.\n  DISCUSSION: We provide evidence to show that a privacy-preserving low-cost\ncamera network using edge computing framework has the potential to distinguish\nbetween different levels of cognitive impairment from the movements and social\ninteractions captured during group activities.",
    "updated" : "2024-08-19T22:34:43Z",
    "published" : "2024-08-19T22:34:43Z",
    "authors" : [
      {
        "name" : "Chaitra Hegde"
      },
      {
        "name" : "Yashar Kiarashi"
      },
      {
        "name" : "Allan I Levey"
      },
      {
        "name" : "Amy D Rodriguez"
      },
      {
        "name" : "Hyeokhyen Kwon"
      },
      {
        "name" : "Gari D Clifford"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CV"
    ]
  }
]