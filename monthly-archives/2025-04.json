[
  {
    "id" : "http://arxiv.org/abs/2504.01820v1",
    "title" : "Antenna Selection for Enhancing Privacy in Radar-Based Vital Sign\n  Monitoring Systems",
    "summary" : "Radar-based vital sign monitoring (VSM) systems have become valuable for\nnon-contact health monitoring by detecting physiological activities, such as\nrespiration and heartbeat, remotely. However, the conventional phased array\nused in VSM is vulnerable to privacy breaches, as an eavesdropper can extract\nsensitive vital sign information by analyzing the reflected radar signals. In\nthis paper, we propose a novel approach to protect privacy in radar-based VSM\nby modifying the radar transmitter hardware, specifically by strategically\nselecting the transmit antennas from the available antennas in the transmit\narray. By dynamically selecting which antennas connect or disconnect to the\nradio frequency chain, the transmitter introduces additional phase noise to the\nradar echoes, generating false frequencies in the power spectrum of the\nextracted phases at the eavesdropper's receiver. The antenna activation pattern\nis designed to maximize the variance of the phases introduced by antenna\nselection, which effectively makes the false frequencies dominate the spectrum,\nobscuring the actual vital sign frequencies. Meanwhile, the authorized\nreceiver, having knowledge of the antenna selection pattern, can compensate for\nthe phase noise and accurately extract the vital signs. Numerical experiments\nare conducted to validate the effectiveness of the proposed approach in\nenhancing privacy while maintaining vital sign monitoring.",
    "updated" : "2025-04-02T15:28:07Z",
    "published" : "2025-04-02T15:28:07Z",
    "authors" : [
      {
        "name" : "Zhihao Tao"
      },
      {
        "name" : "Athina P. Petropulu"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00952v1",
    "title" : "Personalized Federated Training of Diffusion Models with Privacy\n  Guarantees",
    "summary" : "The scarcity of accessible, compliant, and ethically sourced data presents a\nconsiderable challenge to the adoption of artificial intelligence (AI) in\nsensitive fields like healthcare, finance, and biomedical research.\nFurthermore, access to unrestricted public datasets is increasingly constrained\ndue to rising concerns over privacy, copyright, and competition. Synthetic data\nhas emerged as a promising alternative, and diffusion models -- a cutting-edge\ngenerative AI technology -- provide an effective solution for generating\nhigh-quality and diverse synthetic data. In this paper, we introduce a novel\nfederated learning framework for training diffusion models on decentralized\nprivate datasets. Our framework leverages personalization and the inherent\nnoise in the forward diffusion process to produce high-quality samples while\nensuring robust differential privacy guarantees. Our experiments show that our\nframework outperforms non-collaborative training methods, particularly in\nsettings with high data heterogeneity, and effectively reduces biases and\nimbalances in synthetic data, resulting in fairer downstream models.",
    "updated" : "2025-04-01T16:45:26Z",
    "published" : "2025-04-01T16:45:26Z",
    "authors" : [
      {
        "name" : "Kumar Kshitij Patel"
      },
      {
        "name" : "Weitong Zhang"
      },
      {
        "name" : "Lingxiao Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00919v1",
    "title" : "Nonparametric spectral density estimation using interactive mechanisms\n  under local differential privacy",
    "summary" : "We address the problem of nonparametric estimation of the spectral density\nfor a centered stationary Gaussian time series under local differential privacy\nconstraints. Specifically, we propose new interactive privacy mechanisms for\nthree tasks: estimating a single covariance coefficient, estimating the\nspectral density at a fixed frequency, and estimating the entire spectral\ndensity function. Our approach achieves faster rates through a two-stage\nprocess: we apply first the Laplace mechanism to the truncated value and then\nuse the former privatized sample to gain knowledge on the dependence mechanism\nin the time series. For spectral densities belonging to H\\\"older and Sobolev\nsmoothness classes, we demonstrate that our estimators improve upon the\nnon-interactive mechanism of Kroll (2024) for small privacy parameter $\\alpha$,\nsince the pointwise rates depend on $n\\alpha^2$ instead of $n\\alpha^4$.\nMoreover, we show that the rate $(n\\alpha^4)^{-1}$ is optimal for estimating a\ncovariance coefficient with non-interactive mechanisms. However, the $L_2$ rate\nof our interactive estimator is slower than the pointwise rate. We show how to\nuse these estimators to provide a bona-fide locally differentially private\ncovariance matrix estimator.",
    "updated" : "2025-04-01T15:52:50Z",
    "published" : "2025-04-01T15:52:50Z",
    "authors" : [
      {
        "name" : "Cristina Butucea"
      },
      {
        "name" : "Karolina Klockmann"
      },
      {
        "name" : "Tatyana Krivobokova"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00890v1",
    "title" : "Privacy-Preserving Transfer Learning for Community Detection using\n  Locally Distributed Multiple Networks",
    "summary" : "This paper develops a new spectral clustering-based method called TransNet\nfor transfer learning in community detection of network data. Our goal is to\nimprove the clustering performance of the target network using auxiliary source\nnetworks, which are heterogeneous, privacy-preserved, and locally stored across\nvarious sources. The edges of each locally stored network are perturbed using\nthe randomized response mechanism to achieve differential privacy. Notably, we\nallow the source networks to have distinct privacy-preserving and heterogeneity\nlevels as often desired in practice. To better utilize the information from the\nsource networks, we propose a novel adaptive weighting method to aggregate the\neigenspaces of the source networks multiplied by adaptive weights chosen to\nincorporate the effects of privacy and heterogeneity. We propose a\nregularization method that combines the weighted average eigenspace of the\nsource networks with the eigenspace of the target network to achieve an optimal\nbalance between them. Theoretically, we show that the adaptive weighting method\nenjoys the error-bound-oracle property in the sense that the error bound of the\nestimated eigenspace only depends on informative source networks. We also\ndemonstrate that TransNet performs better than the estimator using only the\ntarget network and the estimator using only the weighted source networks.",
    "updated" : "2025-04-01T15:19:07Z",
    "published" : "2025-04-01T15:19:07Z",
    "authors" : [
      {
        "name" : "Xiao Guo"
      },
      {
        "name" : "Xuming He"
      },
      {
        "name" : "Xiangyu Chang"
      },
      {
        "name" : "Shujie Ma"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00874v1",
    "title" : "P2NIA: Privacy-Preserving Non-Iterative Auditing",
    "summary" : "The emergence of AI legislation has increased the need to assess the ethical\ncompliance of high-risk AI systems. Traditional auditing methods rely on\nplatforms' application programming interfaces (APIs), where responses to\nqueries are examined through the lens of fairness requirements. However, such\napproaches put a significant burden on platforms, as they are forced to\nmaintain APIs while ensuring privacy, facing the possibility of data leaks.\nThis lack of proper collaboration between the two parties, in turn, causes a\nsignificant challenge to the auditor, who is subject to estimation bias as they\nare unaware of the data distribution of the platform. To address these two\nissues, we present P2NIA, a novel auditing scheme that proposes a mutually\nbeneficial collaboration for both the auditor and the platform. Extensive\nexperiments demonstrate P2NIA's effectiveness in addressing both issues. In\nsummary, our work introduces a privacy-preserving and non-iterative audit\nscheme that enhances fairness assessments using synthetic or local data,\navoiding the challenges associated with traditional API-based audits.",
    "updated" : "2025-04-01T15:04:58Z",
    "published" : "2025-04-01T15:04:58Z",
    "authors" : [
      {
        "name" : "Jade Garcia Bourrée"
      },
      {
        "name" : "Hadrien Lautraite"
      },
      {
        "name" : "Sébastien Gambs"
      },
      {
        "name" : "Gilles Tredan"
      },
      {
        "name" : "Erwan Le Merrer"
      },
      {
        "name" : "Benoît Rottembourg"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00858v1",
    "title" : "Whispering Under the Eaves: Protecting User Privacy Against Commercial\n  and LLM-powered Automatic Speech Recognition Systems",
    "summary" : "The widespread application of automatic speech recognition (ASR) supports\nlarge-scale voice surveillance, raising concerns about privacy among users. In\nthis paper, we concentrate on using adversarial examples to mitigate\nunauthorized disclosure of speech privacy thwarted by potential eavesdroppers\nin speech communications. While audio adversarial examples have demonstrated\nthe capability to mislead ASR models or evade ASR surveillance, they are\ntypically constructed through time-intensive offline optimization, restricting\ntheir practicality in real-time voice communication. Recent work overcame this\nlimitation by generating universal adversarial perturbations (UAPs) and\nenhancing their transferability for black-box scenarios. However, they\nintroduced excessive noise that significantly degrades audio quality and\naffects human perception, thereby limiting their effectiveness in practical\nscenarios. To address this limitation and protect live users' speech against\nASR systems, we propose a novel framework, AudioShield. Central to this\nframework is the concept of Transferable Universal Adversarial Perturbations in\nthe Latent Space (LS-TUAP). By transferring the perturbations to the latent\nspace, the audio quality is preserved to a large extent. Additionally, we\npropose target feature adaptation to enhance the transferability of UAPs by\nembedding target text features into the perturbations. Comprehensive evaluation\non four commercial ASR APIs (Google, Amazon, iFlytek, and Alibaba), three voice\nassistants, two LLM-powered ASR and one NN-based ASR demonstrates the\nprotection superiority of AudioShield over existing competitors, and both\nobjective and subjective evaluations indicate that AudioShield significantly\nimproves the audio quality. Moreover, AudioShield also shows high effectiveness\nin real-time end-to-end scenarios, and demonstrates strong resilience against\nadaptive countermeasures.",
    "updated" : "2025-04-01T14:49:39Z",
    "published" : "2025-04-01T14:49:39Z",
    "authors" : [
      {
        "name" : "Weifei Jin"
      },
      {
        "name" : "Yuxin Cao"
      },
      {
        "name" : "Junjie Su"
      },
      {
        "name" : "Derui Wang"
      },
      {
        "name" : "Yedi Zhang"
      },
      {
        "name" : "Minhui Xue"
      },
      {
        "name" : "Jie Hao"
      },
      {
        "name" : "Jin Song Dong"
      },
      {
        "name" : "Yixian Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00411v1",
    "title" : "Forward Learning with Differential Privacy",
    "summary" : "Differential privacy (DP) in deep learning is a critical concern as it\nensures the confidentiality of training data while maintaining model utility.\nExisting DP training algorithms provide privacy guarantees by clipping and then\ninjecting external noise into sample gradients computed by the backpropagation\nalgorithm. Different from backpropagation, forward-learning algorithms based on\nperturbation inherently add noise during the forward pass and utilize\nrandomness to estimate the gradients. Although these algorithms are\nnon-privatized, the introduction of noise during the forward pass indirectly\nprovides internal randomness protection to the model parameters and their\ngradients, suggesting the potential for naturally providing differential\nprivacy. In this paper, we propose a \\blue{privatized} forward-learning\nalgorithm, Differential Private Unified Likelihood Ratio (DP-ULR), and\ndemonstrate its differential privacy guarantees. DP-ULR features a novel batch\nsampling operation with rejection, of which we provide theoretical analysis in\nconjunction with classic differential privacy mechanisms. DP-ULR is also\nunderpinned by a theoretically guided privacy controller that dynamically\nadjusts noise levels to manage privacy costs in each training step. Our\nexperiments indicate that DP-ULR achieves competitive performance compared to\ntraditional differential privacy training algorithms based on backpropagation,\nmaintaining nearly the same privacy loss limits.",
    "updated" : "2025-04-01T04:14:53Z",
    "published" : "2025-04-01T04:14:53Z",
    "authors" : [
      {
        "name" : "Mingqian Feng"
      },
      {
        "name" : "Zeliang Zhang"
      },
      {
        "name" : "Jinyang Jiang"
      },
      {
        "name" : "Yijie Peng"
      },
      {
        "name" : "Chenliang Xu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.02149v1",
    "title" : "Exploring the Privacy and Security Challenges Faced by Migrant Domestic\n  Workers in Chinese Smart Homes",
    "summary" : "The growing use of smart home devices poses considerable privacy and security\nchallenges, especially for individuals like migrant domestic workers (MDWs) who\nmay be surveilled by their employers. This paper explores the privacy and\nsecurity challenges experienced by MDWs in multi-user smart homes through\nin-depth semi-structured interviews with 26 MDWs and 5 staff members of\nagencies that recruit and/or train domestic workers in China. Our findings\nreveal that the relationships between MDWs, their employers, and agencies are\ncharacterized by significant power imbalances, influenced by Chinese cultural\nand social factors (such as Confucianism and collectivism), as well as legal\nones. Furthermore, the widespread and normalized use of surveillance\ntechnologies in China, particularly in public spaces, exacerbates these power\nimbalances, reinforcing a sense of constant monitoring and control. Drawing on\nour findings, we provide recommendations to domestic worker agencies and\npolicymakers to address the privacy and security challenges facing MDWs in\nChinese smart homes.",
    "updated" : "2025-04-02T21:49:15Z",
    "published" : "2025-04-02T21:49:15Z",
    "authors" : [
      {
        "name" : "Shijing He"
      },
      {
        "name" : "Xiao Zhan"
      },
      {
        "name" : "Yaxiong Lei"
      },
      {
        "name" : "Yueyan Liu"
      },
      {
        "name" : "Ruba Abu-Salma"
      },
      {
        "name" : "Jose Such"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.02068v1",
    "title" : "Privacy-Preserving Edge Computing from Pairing-Based Inner Product\n  Functional Encryption",
    "summary" : "Pairing-based inner product functional encryption provides an efficient\ntheoretical construction for privacy-preserving edge computing secured by\nwidely deployed elliptic curve cryptography. In this work, an efficient\nsoftware implementation framework for pairing-based function-hiding inner\nproduct encryption (FHIPE) is presented using the recently proposed and widely\nadopted BLS12-381 pairing-friendly elliptic curve. Algorithmic optimizations\nprovide $\\approx 2.6 \\times$ and $\\approx 3.4 \\times$ speedup in FHIPE\nencryption and decryption respectively, and extensive performance analysis is\npresented using a Raspberry Pi 4B edge device. The proposed optimizations\nenable this implementation framework to achieve performance and ciphertext size\ncomparable to previous work despite being implemented on an edge device with a\nslower processor and supporting a curve at much higher security level with a\nlarger prime field. Practical privacy-preserving edge computing applications\nsuch as encrypted biomedical sensor data classification and secure wireless\nfingerprint-based indoor localization are also demonstrated using the proposed\nimplementation framework.",
    "updated" : "2025-04-02T19:01:10Z",
    "published" : "2025-04-02T19:01:10Z",
    "authors" : [
      {
        "name" : "Utsav Banerjee"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.03173v1",
    "title" : "PPFPL: Cross-silo Privacy-preserving Federated Prototype Learning\n  Against Data Poisoning Attacks on Non-IID Data",
    "summary" : "Privacy-Preserving Federated Learning (PPFL) allows multiple clients to\ncollaboratively train a deep learning model by submitting hidden model updates.\nNonetheless, PPFL is vulnerable to data poisoning attacks due to the\ndistributed training nature of clients. Existing solutions have struggled to\nimprove the performance of cross-silo PPFL in poisoned Non-IID data. To address\nthe issues, this paper proposes a privacy-preserving federated prototype\nlearning framework, named PPFPL, which enhances the cross-silo FL performance\nin poisoned Non-IID data while effectively resisting data poisoning attacks.\nSpecifically, we adopt prototypes as client-submitted model updates to\neliminate the impact of tampered data distribution on federated learning.\nMoreover, we utilize two servers to achieve Byzantine-robust aggregation by\nsecure aggregation protocol, which greatly reduces the impact of malicious\nclients. Theoretical analyses confirm the convergence of PPFPL, and\nexperimental results on publicly available datasets show that PPFPL is\neffective for resisting data poisoning attacks with Non-IID conditions.",
    "updated" : "2025-04-04T05:05:24Z",
    "published" : "2025-04-04T05:05:24Z",
    "authors" : [
      {
        "name" : "Hongliang Zhang"
      },
      {
        "name" : "Jiguo Yu"
      },
      {
        "name" : "Fenghua Xu"
      },
      {
        "name" : "Chunqiang Hu"
      },
      {
        "name" : "Yongzhao Zhang"
      },
      {
        "name" : "Xiaofen Wang"
      },
      {
        "name" : "Zhongyuan Yu"
      },
      {
        "name" : "Xiaosong Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.05202v1",
    "title" : "Infinitely Divisible Noise for Differential Privacy: Nearly Optimal\n  Error in the High $\\varepsilon$ Regime",
    "summary" : "Differential privacy (DP) can be achieved in a distributed manner, where\nmultiple parties add independent noise such that their sum protects the overall\ndataset with DP. A common technique here is for each party to sample their\nnoise from the decomposition of an infinitely divisible distribution. We\nanalyze two mechanisms in this setting: 1) the generalized discrete Laplace\n(GDL) mechanism, whose distribution (which is closed under summation) follows\nfrom differences of i.i.d. negative binomial shares, and 2) the multi-scale\ndiscrete Laplace (MSDLap) mechanism, a novel mechanism following the sum of\nmultiple i.i.d. discrete Laplace shares at different scales.\n  For $\\varepsilon \\geq 1$, our mechanisms can be parameterized to have\n$O\\left(\\Delta^3 e^{-\\varepsilon}\\right)$ and $O\\left(\\min\\left(\\Delta^3\ne^{-\\varepsilon}, \\Delta^2 e^{-2\\varepsilon/3}\\right)\\right)$ MSE,\nrespectively, where $\\Delta$ denote the sensitivity; the latter bound matches\nknown optimality results. We also show a transformation from the discrete\nsetting to the continuous setting, which allows us to transform both mechanisms\nto the continuous setting and thereby achieve the optimal $O\\left(\\Delta^2\ne^{-2\\varepsilon / 3}\\right)$ MSE. To our knowledge, these are the first\ninfinitely divisible additive noise mechanisms that achieve order-optimal MSE\nunder pure DP, so our work shows formally there is no separation in utility\nwhen query-independent noise adding mechanisms are restricted to infinitely\ndivisible noise. For the continuous setting, our result improves upon the Arete\nmechanism from [Pagh and Stausholm, ALT 2022] which gives an MSE of\n$O\\left(\\Delta^2 e^{-\\varepsilon/4}\\right)$. Furthermore, we give an exact\nsampler tuned to efficiently implement the MSDLap mechanism, and we apply our\nresults to improve a state of the art multi-message shuffle DP protocol in the\nhigh $\\varepsilon$ regime.",
    "updated" : "2025-04-07T15:50:46Z",
    "published" : "2025-04-07T15:50:46Z",
    "authors" : [
      {
        "name" : "Charlie Harrison"
      },
      {
        "name" : "Pasin Manurangsi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.04734v1",
    "title" : "Teaching Data Science Students to Sketch Privacy Designs through\n  Heuristics (Extended Technical Report)",
    "summary" : "Recent studies reveal that experienced data practitioners often draw sketches\nto facilitate communication around privacy design concepts. However, there is\nlimited understanding of how we can help novice students develop such\ncommunication skills. This paper studies methods for lowering novice data\nscience students' barriers to creating high-quality privacy sketches. We first\nconducted a need-finding study (N=12) to identify barriers students face when\nsketching privacy designs. We then used a human-centered design approach to\nguide the method development, culminating in three simple, text-based\nheuristics. Our user studies with 24 data science students revealed that simply\npresenting three heuristics to the participants at the beginning of the study\ncan enhance the coverage of privacy-related design decisions in sketches,\nreduce the mental effort required for creating sketches, and improve the\nreadability of the final sketches.",
    "updated" : "2025-04-07T05:12:21Z",
    "published" : "2025-04-07T05:12:21Z",
    "authors" : [
      {
        "name" : "Jinhe Wen"
      },
      {
        "name" : "Yingxi Zhao"
      },
      {
        "name" : "Wenqian Xu"
      },
      {
        "name" : "Yaxing Yao"
      },
      {
        "name" : "Haojian Jin"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.04388v1",
    "title" : "Who's Watching You Zoom? Investigating Privacy of Third-Party Zoom Apps",
    "summary" : "Zoom serves millions of users daily and allows third-party developers to\nintegrate their apps with the Zoom client and reach those users. So far, these\napps' privacy and security aspects, which can access rich audio-visual data\n(among others) from Zoom, have not been scientifically investigated. This paper\nexamines the evolution of the Zoom Marketplace over one year, identifying\ntrends in apps, their data collection behaviors, and the transparency of\nprivacy policies. Our findings include worrisome details about the increasing\nover-collection of user data, non-transparency about purposes and sharing\nbehaviors, and possible non-compliance with relevant laws. We believe these\nfindings will inform future privacy and security research on this platform and\nhelp improve Zoom's app review process and platform policy.",
    "updated" : "2025-04-06T06:48:58Z",
    "published" : "2025-04-06T06:48:58Z",
    "authors" : [
      {
        "name" : "Saharsh Goenka"
      },
      {
        "name" : "Adit Prabhu"
      },
      {
        "name" : "Payge Sakurai"
      },
      {
        "name" : "Mrinaal Ramachandran"
      },
      {
        "name" : "Rakibul Hasan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.04033v1",
    "title" : "Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks\n  and Defenses",
    "summary" : "As machine learning (ML) technologies become more prevalent in\nprivacy-sensitive areas like healthcare and finance, eventually incorporating\nsensitive information in building data-driven algorithms, it is vital to\nscrutinize whether these data face any privacy leakage risks. One potential\nthreat arises from an adversary querying trained models using the public,\nnon-sensitive attributes of entities in the training data to infer their\nprivate, sensitive attributes, a technique known as the attribute inference\nattack. This attack is particularly deceptive because, while it may perform\npoorly in predicting sensitive attributes across the entire dataset, it excels\nat predicting the sensitive attributes of records from a few vulnerable groups,\na phenomenon known as disparate vulnerability. This paper illustrates that an\nadversary can take advantage of this disparity to carry out a series of new\nattacks, showcasing a threat level beyond previous imagination. We first\ndevelop a novel inference attack called the disparity inference attack, which\ntargets the identification of high-risk groups within the dataset. We then\nintroduce two targeted variations of the attribute inference attack that can\nidentify and exploit a vulnerable subset of the training data, marking the\nfirst instances of targeted attacks in this category, achieving significantly\nhigher accuracy than untargeted versions. We are also the first to introduce a\nnovel and effective disparity mitigation technique that simultaneously\npreserves model performance and prevents any risk of targeted attacks.",
    "updated" : "2025-04-05T02:58:37Z",
    "published" : "2025-04-05T02:58:37Z",
    "authors" : [
      {
        "name" : "Ehsanul Kabir"
      },
      {
        "name" : "Lucas Craig"
      },
      {
        "name" : "Shagufta Mehnaz"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.03798v1",
    "title" : "An Intelligent and Privacy-Preserving Digital Twin Model for\n  Aging-in-Place",
    "summary" : "The population of older adults is steadily increasing, with a strong\npreference for aging-in-place rather than moving to care facilities.\nConsequently, supporting this growing demographic has become a significant\nglobal challenge. However, facilitating successful aging-in-place is\nchallenging, requiring consideration of multiple factors such as data privacy,\nhealth status monitoring, and living environments to improve health outcomes.\nIn this paper, we propose an unobtrusive sensor system designed for\ninstallation in older adults' homes. Using data from the sensors, our system\nconstructs a digital twin, a virtual representation of events and activities\nthat occurred in the home. The system uses neural network models and decision\nrules to capture residents' activities and living environments. This digital\ntwin enables continuous health monitoring by providing actionable insights into\nresidents' well-being. Our system is designed to be low-cost and\nprivacy-preserving, with the aim of providing green and safe monitoring for the\nhealth of older adults. We have successfully deployed our system in two homes\nover a time period of two months, and our findings demonstrate the feasibility\nand effectiveness of digital twin technology in supporting independent living\nfor older adults. This study highlights that our system could revolutionize\nelder care by enabling personalized interventions, such as lifestyle\nadjustments, medical treatments, or modifications to the residential\nenvironment, to enhance health outcomes.",
    "updated" : "2025-04-04T05:37:08Z",
    "published" : "2025-04-04T05:37:08Z",
    "authors" : [
      {
        "name" : "Yongjie Wang"
      },
      {
        "name" : "Jonathan Cyril Leung"
      },
      {
        "name" : "Ming Chen"
      },
      {
        "name" : "Zhiwei Zeng"
      },
      {
        "name" : "Benny Toh Hsiang Tan"
      },
      {
        "name" : "Yang Qiu"
      },
      {
        "name" : "Zhiqi Shen"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "68T05,",
      "I.2; J.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.05849v1",
    "title" : "On the Importance of Conditioning for Privacy-Preserving Data\n  Augmentation",
    "summary" : "Latent diffusion models can be used as a powerful augmentation method to\nartificially extend datasets for enhanced training. To the human eye, these\naugmented images look very different to the originals. Previous work has\nsuggested to use this data augmentation technique for data anonymization.\nHowever, we show that latent diffusion models that are conditioned on features\nlike depth maps or edges to guide the diffusion process are not suitable as a\nprivacy preserving method. We use a contrastive learning approach to train a\nmodel that can correctly identify people out of a pool of candidates. Moreover,\nwe demonstrate that anonymization using conditioned diffusion models is\nsusceptible to black box attacks. We attribute the success of the described\nmethods to the conditioning of the latent diffusion model in the anonymization\nprocess. The diffusion model is instructed to produce similar edges for the\nanonymized images. Hence, a model can learn to recognize these patterns for\nidentification.",
    "updated" : "2025-04-08T09:27:51Z",
    "published" : "2025-04-08T09:27:51Z",
    "authors" : [
      {
        "name" : "Julian Lorenz"
      },
      {
        "name" : "Katja Ludwig"
      },
      {
        "name" : "Valentin Haug"
      },
      {
        "name" : "Rainer Lienhart"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.06697v1",
    "title" : "\"Sorry for bugging you so much.\" Exploring Developers' Behavior Towards\n  Privacy-Compliant Implementation",
    "summary" : "While protecting user data is essential, software developers often fail to\nfulfill privacy requirements. However, the reasons why they struggle with\nprivacy-compliant implementation remain unclear. Is it due to a lack of\nknowledge, or is it because of insufficient support? To provide foundational\ninsights in this field, we conducted a qualitative 5-hour programming study\nwith 30 professional software developers implementing 3 privacy-sensitive\nprogramming tasks that were designed with GDPR compliance in mind. To explore\nif and how developers implement privacy requirements, participants were divided\ninto 3 groups: control, privacy prompted, and privacy expert-supported. After\ntask completion, we conducted follow-up interviews. Alarmingly, almost all\nparticipants submitted non-GDPR-compliant solutions (79/90). In particular,\nnone of the 3 tasks were solved privacy-compliant by all 30 participants, with\nthe non-prompted group having the lowest number of 3 out of 30\nprivacy-compliant solution attempts. Privacy prompting and expert support only\nslightly improved participants' submissions, with 6/30 and 8/30\nprivacy-compliant attempts, respectively. In fact, all participants reported\nsevere issues addressing common privacy requirements such as purpose\nlimitation, user consent, or data minimization. Counterintuitively, although\nmost developers exhibited minimal confidence in their solutions, they rarely\nsought online assistance or contacted the privacy expert, with only 4 out of 10\nexpert-supported participants explicitly asking for compliance confirmation.\nInstead, participants often relied on existing implementations and focused on\nimplementing functionality and security first.",
    "updated" : "2025-04-09T08:59:17Z",
    "published" : "2025-04-09T08:59:17Z",
    "authors" : [
      {
        "name" : "Stefan Albert Horstmann"
      },
      {
        "name" : "Sandy Hong"
      },
      {
        "name" : "David Klein"
      },
      {
        "name" : "Raphael Serafini"
      },
      {
        "name" : "Martin Degeling"
      },
      {
        "name" : "Martin Johns"
      },
      {
        "name" : "Veelasha Moonsamy"
      },
      {
        "name" : "Alena Naiakshina"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.06552v1",
    "title" : "Understanding Users' Security and Privacy Concerns and Attitudes Towards\n  Conversational AI Platforms",
    "summary" : "The widespread adoption of conversational AI platforms has introduced new\nsecurity and privacy risks. While these risks and their mitigation strategies\nhave been extensively researched from a technical perspective, users'\nperceptions of these platforms' security and privacy remain largely unexplored.\nIn this paper, we conduct a large-scale analysis of over 2.5M user posts from\nthe r/ChatGPT Reddit community to understand users' security and privacy\nconcerns and attitudes toward conversational AI platforms. Our qualitative\nanalysis reveals that users are concerned about each stage of the data\nlifecycle (i.e., collection, usage, and retention). They seek mitigations for\nsecurity vulnerabilities, compliance with privacy regulations, and greater\ntransparency and control in data handling. We also find that users exhibit\nvaried behaviors and preferences when interacting with these platforms. Some\nusers proactively safeguard their data and adjust privacy settings, while\nothers prioritize convenience over privacy risks, dismissing privacy concerns\nin favor of benefits, or feel resigned to inevitable data sharing. Through\nqualitative content and regression analysis, we discover that users' concerns\nevolve over time with the evolving AI landscape and are influenced by\ntechnological developments and major events. Based on our findings, we provide\nrecommendations for users, platforms, enterprises, and policymakers to enhance\ntransparency, improve data controls, and increase user trust and adoption.",
    "updated" : "2025-04-09T03:22:48Z",
    "published" : "2025-04-09T03:22:48Z",
    "authors" : [
      {
        "name" : "Mutahar Ali"
      },
      {
        "name" : "Arjun Arunasalam"
      },
      {
        "name" : "Habiba Farrukh"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07761v1",
    "title" : "Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection",
    "summary" : "In an increasingly digitalized world, verifying the authenticity of ID\ndocuments has become a critical challenge for real-life applications such as\ndigital banking, crypto-exchanges, renting, etc. This study focuses on the\ntopic of fake ID detection, covering several limitations in the field. In\nparticular, no publicly available data from real ID documents exists, and most\nstudies rely on proprietary in-house databases that are not available due to\nprivacy reasons. In order to shed some light on this critical challenge that\nmakes difficult to advance in the field, we explore a trade-off between privacy\n(i.e., amount of sensitive data available) and performance, proposing a novel\npatch-wise approach for privacy-preserving fake ID detection. Our proposed\napproach explores how privacy can be enhanced through: i) two levels of\nanonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii)\ndifferent patch size configurations, varying the amount of sensitive data\nvisible in the patch image. Also, state-of-the-art methods such as Vision\nTransformers and Foundation Models are considered in the analysis. The\nexperimental framework shows that, on an unseen database (DLC-2021), our\nproposal achieves 13.91% and 0% EERs at patch and ID document level, showing a\ngood generalization to other databases. In addition to this exploration,\nanother key contribution of our study is the release of the first publicly\navailable database that contains 48,400 patches from both real and fake ID\ndocuments, along with the experimental framework and models, which will be\navailable in our GitHub.",
    "updated" : "2025-04-10T14:01:22Z",
    "published" : "2025-04-10T14:01:22Z",
    "authors" : [
      {
        "name" : "Javier Muñoz-Haro"
      },
      {
        "name" : "Ruben Tolosana"
      },
      {
        "name" : "Ruben Vera-Rodriguez"
      },
      {
        "name" : "Aythami Morales"
      },
      {
        "name" : "Julian Fierrez"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07578v1",
    "title" : "Privacy-Preserving Vertical K-Means Clustering",
    "summary" : "Clustering is a fundamental data processing task used for grouping records\nbased on one or more features. In the vertically partitioned setting, data is\ndistributed among entities, with each holding only a subset of those features.\nA key challenge in this scenario is that computing distances between records\nrequires access to all distributed features, which may be privacy-sensitive and\ncannot be directly shared with other parties. The goal is to compute the joint\nclusters while preserving the privacy of each entity's dataset. Existing\nsolutions using secret sharing or garbled circuits implement privacy-preserving\nvariants of Lloyd's algorithm but incur high communication costs, scaling as\nO(nkt), where n is the number of data points, k the number of clusters, and t\nthe number of rounds. These methods become impractical for large datasets or\nseveral parties, limiting their use to LAN settings only. On the other hand, a\ndifferent line of solutions rely on differential privacy (DP) to outsource the\nlocal features of the parties to a central server. However, they often\nsignificantly degrade the utility of the clustering outcome due to excessive\nnoise. In this work, we propose a novel solution based on homomorphic\nencryption and DP, reducing communication complexity to O(n+kt). In our method,\nparties securely outsource their features once, allowing a computing party to\nperform clustering operations under encryption. DP is applied only to the\nclusters' centroids, ensuring privacy with minimal impact on utility. Our\nsolution clusters 100,000 two-dimensional points into five clusters using only\n73MB of communication, compared to 101GB for existing works, and completes in\njust under 3 minutes on a 100Mbps network, whereas existing works take over 1\nday. This makes our solution practical even for WAN deployments, all while\nmaintaining accuracy comparable to plaintext k-means algorithms.",
    "updated" : "2025-04-10T09:20:56Z",
    "published" : "2025-04-10T09:20:56Z",
    "authors" : [
      {
        "name" : "Federico Mazzone"
      },
      {
        "name" : "Trevor Brown"
      },
      {
        "name" : "Florian Kerschbaum"
      },
      {
        "name" : "Kevin H. Wilson"
      },
      {
        "name" : "Maarten Everts"
      },
      {
        "name" : "Florian Hahn"
      },
      {
        "name" : "Andreas Peter"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07414v1",
    "title" : "Decomposition-Based Optimal Bounds for Privacy Amplification via\n  Shuffling",
    "summary" : "Shuffling has been shown to amplify differential privacy guarantees, offering\na stronger privacy-utility trade-off. To characterize and compute this\namplification, two fundamental analytical frameworks have been proposed: the\nprivacy blanket by Balle et al. (CRYPTO 2019) and the clone paradigm (including\nboth the standard clone and stronger clone) by Feldman et al. (FOCS 2021, SODA\n2023). All these methods rely on decomposing local randomizers.\n  In this work, we introduce a unified analysis framework--the general clone\nparadigm--which encompasses all possible decompositions. We identify the\noptimal decomposition within the general clone paradigm. Moreover, we develop a\nsimple and efficient algorithm to compute the exact value of the optimal\nprivacy amplification bounds via Fast Fourier Transform. Experimental results\ndemonstrate that the computed upper bounds for privacy amplification closely\napproximate the lower bounds, highlighting the tightness of our approach.\nFinally, using our algorithm, we conduct the first systematic analysis of the\njoint composition of LDP protocols in the shuffle model.",
    "updated" : "2025-04-10T03:11:17Z",
    "published" : "2025-04-10T03:11:17Z",
    "authors" : [
      {
        "name" : "Pengcheng Su"
      },
      {
        "name" : "Haibo Cheng"
      },
      {
        "name" : "Ping Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07362v1",
    "title" : "Augmented Shuffle Protocols for Accurate and Robust Frequency Estimation\n  under Differential Privacy",
    "summary" : "The shuffle model of DP (Differential Privacy) provides high utility by\nintroducing a shuffler that randomly shuffles noisy data sent from users.\nHowever, recent studies show that existing shuffle protocols suffer from the\nfollowing two major drawbacks. First, they are vulnerable to local data\npoisoning attacks, which manipulate the statistics about input data by sending\ncrafted data, especially when the privacy budget epsilon is small. Second, the\nactual value of epsilon is increased by collusion attacks by the data collector\nand users.\n  In this paper, we address these two issues by thoroughly exploring the\npotential of the augmented shuffle model, which allows the shuffler to perform\nadditional operations, such as random sampling and dummy data addition.\nSpecifically, we propose a generalized framework for local-noise-free protocols\nin which users send (encrypted) input data to the shuffler without adding\nnoise. We show that this generalized protocol provides DP and is robust to the\nabove two attacks if a simpler mechanism that performs the same process on\nbinary input data provides DP. Based on this framework, we propose three\nconcrete protocols providing DP and robustness against the two attacks. Our\nfirst protocol generates the number of dummy values for each item from a\nbinomial distribution and provides higher utility than several state-of-the-art\nexisting shuffle protocols. Our second protocol significantly improves the\nutility of our first protocol by introducing a novel dummy-count distribution:\nasymmetric two-sided geometric distribution. Our third protocol is a special\ncase of our second protocol and provides pure epsilon-DP. We show the\neffectiveness of our protocols through theoretical analysis and comprehensive\nexperiments.",
    "updated" : "2025-04-10T01:06:05Z",
    "published" : "2025-04-10T01:06:05Z",
    "authors" : [
      {
        "name" : "Takao Murakami"
      },
      {
        "name" : "Yuichi Sei"
      },
      {
        "name" : "Reo Eriguchi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07323v1",
    "title" : "Prekey Pogo: Investigating Security and Privacy Issues in WhatsApp's\n  Handshake Mechanism",
    "summary" : "WhatsApp, the world's largest messaging application, uses a version of the\nSignal protocol to provide end-to-end encryption (E2EE) with strong security\nguarantees, including Perfect Forward Secrecy (PFS). To ensure PFS right from\nthe start of a new conversation -- even when the recipient is offline -- a\nstash of ephemeral (one-time) prekeys must be stored on a server. While the\ncritical role of these one-time prekeys in achieving PFS has been outlined in\nthe Signal specification, we are the first to demonstrate a targeted depletion\nattack against them on individual WhatsApp user devices. Our findings not only\nreveal an attack that can degrade PFS for certain messages, but also expose\ninherent privacy risks and serious availability implications arising from the\nrefilling and distribution procedure essential for this security mechanism.",
    "updated" : "2025-04-09T22:53:13Z",
    "published" : "2025-04-09T22:53:13Z",
    "authors" : [
      {
        "name" : "Gabriel K. Gegenhuber"
      },
      {
        "name" : "Philipp É. Frenzel"
      },
      {
        "name" : "Maximilian Günther"
      },
      {
        "name" : "Aljosha Judmayer"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.08616v1",
    "title" : "Preserving Privacy Without Compromising Accuracy: Machine Unlearning for\n  Handwritten Text Recognition",
    "summary" : "Handwritten Text Recognition (HTR) is essential for document analysis and\ndigitization. However, handwritten data often contains user-identifiable\ninformation, such as unique handwriting styles and personal lexicon choices,\nwhich can compromise privacy and erode trust in AI services. Legislation like\nthe ``right to be forgotten'' underscores the necessity for methods that can\nexpunge sensitive information from trained models. Machine unlearning addresses\nthis by selectively removing specific data from models without necessitating\ncomplete retraining. Yet, it frequently encounters a privacy-accuracy tradeoff,\nwhere safeguarding privacy leads to diminished model performance. In this\npaper, we introduce a novel two-stage unlearning strategy for a multi-head\ntransformer-based HTR model, integrating pruning and random labeling. Our\nproposed method utilizes a writer classification head both as an indicator and\na trigger for unlearning, while maintaining the efficacy of the recognition\nhead. To our knowledge, this represents the first comprehensive exploration of\nmachine unlearning within HTR tasks. We further employ Membership Inference\nAttacks (MIA) to evaluate the effectiveness of unlearning user-identifiable\ninformation. Extensive experiments demonstrate that our approach effectively\npreserves privacy while maintaining model accuracy, paving the way for new\nresearch directions in the document analysis community. Our code will be\npublicly available upon acceptance.",
    "updated" : "2025-04-11T15:21:12Z",
    "published" : "2025-04-11T15:21:12Z",
    "authors" : [
      {
        "name" : "Lei Kang"
      },
      {
        "name" : "Xuanshuo Fu"
      },
      {
        "name" : "Lluis Gomez"
      },
      {
        "name" : "Alicia Fornés"
      },
      {
        "name" : "Ernest Valveny"
      },
      {
        "name" : "Dimosthenis Karatzas"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.08254v1",
    "title" : "Understanding the Impact of Data Domain Extraction on Synthetic Data\n  Privacy",
    "summary" : "Privacy attacks, particularly membership inference attacks (MIAs), are widely\nused to assess the privacy of generative models for tabular synthetic data,\nincluding those with Differential Privacy (DP) guarantees. These attacks often\nexploit outliers, which are especially vulnerable due to their position at the\nboundaries of the data domain (e.g., at the minimum and maximum values).\nHowever, the role of data domain extraction in generative models and its impact\non privacy attacks have been overlooked. In this paper, we examine three\nstrategies for defining the data domain: assuming it is externally provided\n(ideally from public data), extracting it directly from the input data, and\nextracting it with DP mechanisms. While common in popular implementations and\nlibraries, we show that the second approach breaks end-to-end DP guarantees and\nleaves models vulnerable. While using a provided domain (if representative) is\npreferable, extracting it with DP can also defend against popular MIAs, even at\nhigh privacy budgets.",
    "updated" : "2025-04-11T04:35:24Z",
    "published" : "2025-04-11T04:35:24Z",
    "authors" : [
      {
        "name" : "Georgi Ganev"
      },
      {
        "name" : "Meenatchi Sundaram Muthu Selva Annamalai"
      },
      {
        "name" : "Sofiane Mahiou"
      },
      {
        "name" : "Emiliano De Cristofaro"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07414v2",
    "title" : "Decomposition-Based Optimal Bounds for Privacy Amplification via\n  Shuffling",
    "summary" : "Shuffling has been shown to amplify differential privacy guarantees, offering\na stronger privacy-utility trade-off. To characterize and compute this\namplification, two fundamental analytical frameworks have been proposed: the\nprivacy blanket by Balle et al. (CRYPTO 2019) and the clone paradigm (including\nboth the standard clone and stronger clone) by Feldman et al. (FOCS 2021, SODA\n2023). All these methods rely on decomposing local randomizers.\n  In this work, we introduce a unified analysis framework--the general clone\nparadigm--which encompasses all possible decompositions. We identify the\noptimal decomposition within the general clone paradigm. Moreover, we develop a\nsimple and efficient algorithm to compute the exact value of the optimal\nprivacy amplification bounds via Fast Fourier Transform. Experimental results\ndemonstrate that the computed upper bounds for privacy amplification closely\napproximate the lower bounds, highlighting the tightness of our approach.\nFinally, using our algorithm, we conduct the first systematic analysis of the\njoint composition of LDP protocols in the shuffle model.",
    "updated" : "2025-04-11T01:35:46Z",
    "published" : "2025-04-10T03:11:17Z",
    "authors" : [
      {
        "name" : "Pengcheng Su"
      },
      {
        "name" : "Haibo Cheng"
      },
      {
        "name" : "Ping Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.10456v1",
    "title" : "Privacy-Preserving Distributed Link Predictions Among Peers in Online\n  Classrooms Using Federated Learning",
    "summary" : "Social interactions among classroom peers, represented as social learning\nnetworks (SLNs), play a crucial role in enhancing learning outcomes. While SLN\nanalysis has recently garnered attention, most existing approaches rely on\ncentralized training, where data is aggregated and processed on a local/cloud\nserver with direct access to raw data. However, in real-world educational\nsettings, such direct access across multiple classrooms is often restricted due\nto privacy concerns. Furthermore, training models on isolated classroom data\nprevents the identification of common interaction patterns that exist across\nmultiple classrooms, thereby limiting model performance. To address these\nchallenges, we propose one of the first frameworks that integrates Federated\nLearning (FL), a distributed and collaborative machine learning (ML) paradigm,\nwith SLNs derived from students' interactions in multiple classrooms' online\nforums to predict future link formations (i.e., interactions) among students.\nBy leveraging FL, our approach enables collaborative model training across\nmultiple classrooms while preserving data privacy, as it eliminates the need\nfor raw data centralization. Recognizing that each classroom may exhibit unique\nstudent interaction dynamics, we further employ model personalization\ntechniques to adapt the FL model to individual classroom characteristics. Our\nresults demonstrate the effectiveness of our approach in capturing both shared\nand classroom-specific representations of student interactions in SLNs.\nAdditionally, we utilize explainable AI (XAI) techniques to interpret model\npredictions, identifying key factors that influence link formation across\ndifferent classrooms. These insights unveil the drivers of social learning\ninteractions within a privacy-preserving, collaborative, and distributed ML\nframework -- an aspect that has not been explored before.",
    "updated" : "2025-04-14T17:43:11Z",
    "published" : "2025-04-14T17:43:11Z",
    "authors" : [
      {
        "name" : "Anurata Prabha Hridi"
      },
      {
        "name" : "Muntasir Hoq"
      },
      {
        "name" : "Zhikai Gao"
      },
      {
        "name" : "Collin Lynch"
      },
      {
        "name" : "Rajeev Sahay"
      },
      {
        "name" : "Seyyedali Hosseinalipour"
      },
      {
        "name" : "Bita Akram"
      }
    ],
    "categories" : [
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.10267v1",
    "title" : "Trade-offs in Privacy-Preserving Eye Tracking through Iris Obfuscation:\n  A Benchmarking Study",
    "summary" : "Recent developments in hardware, computer graphics, and AI may soon enable\nAR/VR head-mounted displays (HMDs) to become everyday devices like smartphones\nand tablets. Eye trackers within HMDs provide a special opportunity for such\nsetups as it is possible to facilitate gaze-based research and interaction.\nHowever, estimating users' gaze information often requires raw eye images and\nvideos that contain iris textures, which are considered a gold standard\nbiometric for user authentication, and this raises privacy concerns. Previous\nresearch in the eye-tracking community focused on obfuscating iris textures\nwhile keeping utility tasks such as gaze estimation accurate. Despite these\nattempts, there is no comprehensive benchmark that evaluates state-of-the-art\napproaches. Considering all, in this paper, we benchmark blurring, noising,\ndownsampling, rubber sheet model, and iris style transfer to obfuscate user\nidentity, and compare their impact on image quality, privacy, utility, and risk\nof imposter attack on two datasets. We use eye segmentation and gaze estimation\nas utility tasks, and reduction in iris recognition accuracy as a measure of\nprivacy protection, and false acceptance rate to estimate risk of attack. Our\nexperiments show that canonical image processing methods like blurring and\nnoising cause a marginal impact on deep learning-based tasks. While\ndownsampling, rubber sheet model, and iris style transfer are effective in\nhiding user identifiers, iris style transfer, with higher computation cost,\noutperforms others in both utility tasks, and is more resilient against spoof\nattacks. Our analyses indicate that there is no universal optimal approach to\nbalance privacy, utility, and computation burden. Therefore, we recommend\npractitioners consider the strengths and weaknesses of each approach, and\npossible combinations of those to reach an optimal privacy-utility trade-off.",
    "updated" : "2025-04-14T14:29:38Z",
    "published" : "2025-04-14T14:29:38Z",
    "authors" : [
      {
        "name" : "Mengdi Wang"
      },
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Enkelejda Kasneci"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.10016v1",
    "title" : "Quantifying Privacy Leakage in Split Inference via Fisher-Approximated\n  Shannon Information Analysis",
    "summary" : "Split inference (SI) partitions deep neural networks into distributed\nsub-models, enabling privacy-preserving collaborative learning. Nevertheless,\nit remains vulnerable to Data Reconstruction Attacks (DRAs), wherein\nadversaries exploit exposed smashed data to reconstruct raw inputs. Despite\nextensive research on adversarial attack-defense games, a shortfall remains in\nthe fundamental analysis of privacy risks. This paper establishes a theoretical\nframework for privacy leakage quantification using information theory, defining\nit as the adversary's certainty and deriving both average-case and worst-case\nerror bounds. We introduce Fisher-approximated Shannon information (FSInfo), a\nnovel privacy metric utilizing Fisher Information (FI) for operational privacy\nleakage computation. We empirically show that our privacy metric correlates\nwell with empirical attacks and investigate some of the factors that affect\nprivacy leakage, namely the data distribution, model size, and overfitting.",
    "updated" : "2025-04-14T09:19:06Z",
    "published" : "2025-04-14T09:19:06Z",
    "authors" : [
      {
        "name" : "Ruijun Deng"
      },
      {
        "name" : "Zhihui Lu"
      },
      {
        "name" : "Qiang Duan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.09961v1",
    "title" : "Privacy Meets Explainability: Managing Confidential Data and\n  Transparency Policies in LLM-Empowered Science",
    "summary" : "As Large Language Models (LLMs) become integral to scientific workflows,\nconcerns over the confidentiality and ethical handling of confidential data\nhave emerged. This paper explores data exposure risks through LLM-powered\nscientific tools, which can inadvertently leak confidential information,\nincluding intellectual property and proprietary data, from scientists'\nperspectives. We propose \"DataShield\", a framework designed to detect\nconfidential data leaks, summarize privacy policies, and visualize data flow,\nensuring alignment with organizational policies and procedures. Our approach\naims to inform scientists about data handling practices, enabling them to make\ninformed decisions and protect sensitive information. Ongoing user studies with\nscientists are underway to evaluate the framework's usability, trustworthiness,\nand effectiveness in tackling real-world privacy challenges.",
    "updated" : "2025-04-14T07:58:26Z",
    "published" : "2025-04-14T07:58:26Z",
    "authors" : [
      {
        "name" : "Yashothara Shanmugarasa"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Dehai Zhao"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.09952v1",
    "title" : "Secrecy and Privacy in Multi-Access Combinatorial Topology",
    "summary" : "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
    "updated" : "2025-04-14T07:30:03Z",
    "published" : "2025-04-14T07:30:03Z",
    "authors" : [
      {
        "name" : "Mallikharjuna Chinnapadamala"
      },
      {
        "name" : "B. Sundar Rajan"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.09517v1",
    "title" : "RoboComm: A DID-based scalable and privacy-preserving Robot-to-Robot\n  interaction over state channels",
    "summary" : "In a multi robot system establishing trust amongst untrusted robots from\ndifferent organisations while preserving a robot's privacy is a challenge.\nRecently decentralized technologies such as smart contract and blockchain are\nbeing explored for applications in robotics. However, the limited transaction\nprocessing and high maintenance cost hinder the widespread adoption of such\napproaches. Moreover, blockchain transactions be they on public or private\npermissioned blockchain are publically readable which further fails to preserve\nthe confidentiality of the robot's data and privacy of the robot.\n  In this work, we propose RoboComm a Decentralized Identity based approach for\nprivacy-preserving interaction between robots. With DID a component of\nSelf-Sovereign Identity; robots can authenticate each other independently\nwithout relying on any third-party service. Verifiable Credentials enable\nprivate data associated with a robot to be stored within the robot's hardware,\nunlike existing blockchain based approaches where the data has to be on the\nblockchain. We improve throughput by allowing message exchange over state\nchannels. Being a blockchain backed solution RoboComm provides a trustworthy\nsystem without relying on a single party. Moreover, we implement our proposed\napproach to demonstrate the feasibility of our solution.",
    "updated" : "2025-04-13T11:10:04Z",
    "published" : "2025-04-13T11:10:04Z",
    "authors" : [
      {
        "name" : "Roshan Singh"
      },
      {
        "name" : "Sushant Pandey"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.09095v1",
    "title" : "Privacy Preservation in Gen AI Applications",
    "summary" : "The ability of machines to comprehend and produce language that is similar to\nthat of humans has revolutionized sectors like customer service, healthcare,\nand finance thanks to the quick advances in Natural Language Processing (NLP),\nwhich are fueled by Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs). However, because LLMs trained on large datasets may\nunintentionally absorb and reveal Personally Identifiable Information (PII)\nfrom user interactions, these capabilities also raise serious privacy concerns.\nDeep neural networks' intricacy makes it difficult to track down or stop the\ninadvertent storing and release of private information, which raises serious\nconcerns about the privacy and security of AI-driven data. This study tackles\nthese issues by detecting Generative AI weaknesses through attacks such as data\nextraction, model inversion, and membership inference. A privacy-preserving\nGenerative AI application that is resistant to these assaults is then\ndeveloped. It ensures privacy without sacrificing functionality by using\nmethods to identify, alter, or remove PII before to dealing with LLMs. In order\nto determine how well cloud platforms like Microsoft Azure, Google Cloud, and\nAWS provide privacy tools for protecting AI applications, the study also\nexamines these technologies. In the end, this study offers a fundamental\nprivacy paradigm for generative AI systems, focusing on data security and moral\nAI implementation, and opening the door to a more secure and conscientious use\nof these tools.",
    "updated" : "2025-04-12T06:19:37Z",
    "published" : "2025-04-12T06:19:37Z",
    "authors" : [
      {
        "name" : "Swetha S"
      },
      {
        "name" : "Ram Sundhar K Shaju"
      },
      {
        "name" : "Rakshana M"
      },
      {
        "name" : "Ganesh R"
      },
      {
        "name" : "Balavedhaa S"
      },
      {
        "name" : "Thiruvaazhi U"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.08254v2",
    "title" : "Understanding the Impact of Data Domain Extraction on Synthetic Data\n  Privacy",
    "summary" : "Privacy attacks, particularly membership inference attacks (MIAs), are widely\nused to assess the privacy of generative models for tabular synthetic data,\nincluding those with Differential Privacy (DP) guarantees. These attacks often\nexploit outliers, which are especially vulnerable due to their position at the\nboundaries of the data domain (e.g., at the minimum and maximum values).\nHowever, the role of data domain extraction in generative models and its impact\non privacy attacks have been overlooked. In this paper, we examine three\nstrategies for defining the data domain: assuming it is externally provided\n(ideally from public data), extracting it directly from the input data, and\nextracting it with DP mechanisms. While common in popular implementations and\nlibraries, we show that the second approach breaks end-to-end DP guarantees and\nleaves models vulnerable. While using a provided domain (if representative) is\npreferable, extracting it with DP can also defend against popular MIAs, even at\nhigh privacy budgets.",
    "updated" : "2025-04-14T02:34:24Z",
    "published" : "2025-04-11T04:35:24Z",
    "authors" : [
      {
        "name" : "Georgi Ganev"
      },
      {
        "name" : "Meenatchi Sundaram Muthu Selva Annamalai"
      },
      {
        "name" : "Sofiane Mahiou"
      },
      {
        "name" : "Emiliano De Cristofaro"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  }
]