[
  {
    "id" : "http://arxiv.org/abs/2504.01820v1",
    "title" : "Antenna Selection for Enhancing Privacy in Radar-Based Vital Sign\n  Monitoring Systems",
    "summary" : "Radar-based vital sign monitoring (VSM) systems have become valuable for\nnon-contact health monitoring by detecting physiological activities, such as\nrespiration and heartbeat, remotely. However, the conventional phased array\nused in VSM is vulnerable to privacy breaches, as an eavesdropper can extract\nsensitive vital sign information by analyzing the reflected radar signals. In\nthis paper, we propose a novel approach to protect privacy in radar-based VSM\nby modifying the radar transmitter hardware, specifically by strategically\nselecting the transmit antennas from the available antennas in the transmit\narray. By dynamically selecting which antennas connect or disconnect to the\nradio frequency chain, the transmitter introduces additional phase noise to the\nradar echoes, generating false frequencies in the power spectrum of the\nextracted phases at the eavesdropper's receiver. The antenna activation pattern\nis designed to maximize the variance of the phases introduced by antenna\nselection, which effectively makes the false frequencies dominate the spectrum,\nobscuring the actual vital sign frequencies. Meanwhile, the authorized\nreceiver, having knowledge of the antenna selection pattern, can compensate for\nthe phase noise and accurately extract the vital signs. Numerical experiments\nare conducted to validate the effectiveness of the proposed approach in\nenhancing privacy while maintaining vital sign monitoring.",
    "updated" : "2025-04-02T15:28:07Z",
    "published" : "2025-04-02T15:28:07Z",
    "authors" : [
      {
        "name" : "Zhihao Tao"
      },
      {
        "name" : "Athina P. Petropulu"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00952v1",
    "title" : "Personalized Federated Training of Diffusion Models with Privacy\n  Guarantees",
    "summary" : "The scarcity of accessible, compliant, and ethically sourced data presents a\nconsiderable challenge to the adoption of artificial intelligence (AI) in\nsensitive fields like healthcare, finance, and biomedical research.\nFurthermore, access to unrestricted public datasets is increasingly constrained\ndue to rising concerns over privacy, copyright, and competition. Synthetic data\nhas emerged as a promising alternative, and diffusion models -- a cutting-edge\ngenerative AI technology -- provide an effective solution for generating\nhigh-quality and diverse synthetic data. In this paper, we introduce a novel\nfederated learning framework for training diffusion models on decentralized\nprivate datasets. Our framework leverages personalization and the inherent\nnoise in the forward diffusion process to produce high-quality samples while\nensuring robust differential privacy guarantees. Our experiments show that our\nframework outperforms non-collaborative training methods, particularly in\nsettings with high data heterogeneity, and effectively reduces biases and\nimbalances in synthetic data, resulting in fairer downstream models.",
    "updated" : "2025-04-01T16:45:26Z",
    "published" : "2025-04-01T16:45:26Z",
    "authors" : [
      {
        "name" : "Kumar Kshitij Patel"
      },
      {
        "name" : "Weitong Zhang"
      },
      {
        "name" : "Lingxiao Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00919v1",
    "title" : "Nonparametric spectral density estimation using interactive mechanisms\n  under local differential privacy",
    "summary" : "We address the problem of nonparametric estimation of the spectral density\nfor a centered stationary Gaussian time series under local differential privacy\nconstraints. Specifically, we propose new interactive privacy mechanisms for\nthree tasks: estimating a single covariance coefficient, estimating the\nspectral density at a fixed frequency, and estimating the entire spectral\ndensity function. Our approach achieves faster rates through a two-stage\nprocess: we apply first the Laplace mechanism to the truncated value and then\nuse the former privatized sample to gain knowledge on the dependence mechanism\nin the time series. For spectral densities belonging to H\\\"older and Sobolev\nsmoothness classes, we demonstrate that our estimators improve upon the\nnon-interactive mechanism of Kroll (2024) for small privacy parameter $\\alpha$,\nsince the pointwise rates depend on $n\\alpha^2$ instead of $n\\alpha^4$.\nMoreover, we show that the rate $(n\\alpha^4)^{-1}$ is optimal for estimating a\ncovariance coefficient with non-interactive mechanisms. However, the $L_2$ rate\nof our interactive estimator is slower than the pointwise rate. We show how to\nuse these estimators to provide a bona-fide locally differentially private\ncovariance matrix estimator.",
    "updated" : "2025-04-01T15:52:50Z",
    "published" : "2025-04-01T15:52:50Z",
    "authors" : [
      {
        "name" : "Cristina Butucea"
      },
      {
        "name" : "Karolina Klockmann"
      },
      {
        "name" : "Tatyana Krivobokova"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00890v1",
    "title" : "Privacy-Preserving Transfer Learning for Community Detection using\n  Locally Distributed Multiple Networks",
    "summary" : "This paper develops a new spectral clustering-based method called TransNet\nfor transfer learning in community detection of network data. Our goal is to\nimprove the clustering performance of the target network using auxiliary source\nnetworks, which are heterogeneous, privacy-preserved, and locally stored across\nvarious sources. The edges of each locally stored network are perturbed using\nthe randomized response mechanism to achieve differential privacy. Notably, we\nallow the source networks to have distinct privacy-preserving and heterogeneity\nlevels as often desired in practice. To better utilize the information from the\nsource networks, we propose a novel adaptive weighting method to aggregate the\neigenspaces of the source networks multiplied by adaptive weights chosen to\nincorporate the effects of privacy and heterogeneity. We propose a\nregularization method that combines the weighted average eigenspace of the\nsource networks with the eigenspace of the target network to achieve an optimal\nbalance between them. Theoretically, we show that the adaptive weighting method\nenjoys the error-bound-oracle property in the sense that the error bound of the\nestimated eigenspace only depends on informative source networks. We also\ndemonstrate that TransNet performs better than the estimator using only the\ntarget network and the estimator using only the weighted source networks.",
    "updated" : "2025-04-01T15:19:07Z",
    "published" : "2025-04-01T15:19:07Z",
    "authors" : [
      {
        "name" : "Xiao Guo"
      },
      {
        "name" : "Xuming He"
      },
      {
        "name" : "Xiangyu Chang"
      },
      {
        "name" : "Shujie Ma"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00874v1",
    "title" : "P2NIA: Privacy-Preserving Non-Iterative Auditing",
    "summary" : "The emergence of AI legislation has increased the need to assess the ethical\ncompliance of high-risk AI systems. Traditional auditing methods rely on\nplatforms' application programming interfaces (APIs), where responses to\nqueries are examined through the lens of fairness requirements. However, such\napproaches put a significant burden on platforms, as they are forced to\nmaintain APIs while ensuring privacy, facing the possibility of data leaks.\nThis lack of proper collaboration between the two parties, in turn, causes a\nsignificant challenge to the auditor, who is subject to estimation bias as they\nare unaware of the data distribution of the platform. To address these two\nissues, we present P2NIA, a novel auditing scheme that proposes a mutually\nbeneficial collaboration for both the auditor and the platform. Extensive\nexperiments demonstrate P2NIA's effectiveness in addressing both issues. In\nsummary, our work introduces a privacy-preserving and non-iterative audit\nscheme that enhances fairness assessments using synthetic or local data,\navoiding the challenges associated with traditional API-based audits.",
    "updated" : "2025-04-01T15:04:58Z",
    "published" : "2025-04-01T15:04:58Z",
    "authors" : [
      {
        "name" : "Jade Garcia Bourrée"
      },
      {
        "name" : "Hadrien Lautraite"
      },
      {
        "name" : "Sébastien Gambs"
      },
      {
        "name" : "Gilles Tredan"
      },
      {
        "name" : "Erwan Le Merrer"
      },
      {
        "name" : "Benoît Rottembourg"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00858v1",
    "title" : "Whispering Under the Eaves: Protecting User Privacy Against Commercial\n  and LLM-powered Automatic Speech Recognition Systems",
    "summary" : "The widespread application of automatic speech recognition (ASR) supports\nlarge-scale voice surveillance, raising concerns about privacy among users. In\nthis paper, we concentrate on using adversarial examples to mitigate\nunauthorized disclosure of speech privacy thwarted by potential eavesdroppers\nin speech communications. While audio adversarial examples have demonstrated\nthe capability to mislead ASR models or evade ASR surveillance, they are\ntypically constructed through time-intensive offline optimization, restricting\ntheir practicality in real-time voice communication. Recent work overcame this\nlimitation by generating universal adversarial perturbations (UAPs) and\nenhancing their transferability for black-box scenarios. However, they\nintroduced excessive noise that significantly degrades audio quality and\naffects human perception, thereby limiting their effectiveness in practical\nscenarios. To address this limitation and protect live users' speech against\nASR systems, we propose a novel framework, AudioShield. Central to this\nframework is the concept of Transferable Universal Adversarial Perturbations in\nthe Latent Space (LS-TUAP). By transferring the perturbations to the latent\nspace, the audio quality is preserved to a large extent. Additionally, we\npropose target feature adaptation to enhance the transferability of UAPs by\nembedding target text features into the perturbations. Comprehensive evaluation\non four commercial ASR APIs (Google, Amazon, iFlytek, and Alibaba), three voice\nassistants, two LLM-powered ASR and one NN-based ASR demonstrates the\nprotection superiority of AudioShield over existing competitors, and both\nobjective and subjective evaluations indicate that AudioShield significantly\nimproves the audio quality. Moreover, AudioShield also shows high effectiveness\nin real-time end-to-end scenarios, and demonstrates strong resilience against\nadaptive countermeasures.",
    "updated" : "2025-04-01T14:49:39Z",
    "published" : "2025-04-01T14:49:39Z",
    "authors" : [
      {
        "name" : "Weifei Jin"
      },
      {
        "name" : "Yuxin Cao"
      },
      {
        "name" : "Junjie Su"
      },
      {
        "name" : "Derui Wang"
      },
      {
        "name" : "Yedi Zhang"
      },
      {
        "name" : "Minhui Xue"
      },
      {
        "name" : "Jie Hao"
      },
      {
        "name" : "Jin Song Dong"
      },
      {
        "name" : "Yixian Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00411v1",
    "title" : "Forward Learning with Differential Privacy",
    "summary" : "Differential privacy (DP) in deep learning is a critical concern as it\nensures the confidentiality of training data while maintaining model utility.\nExisting DP training algorithms provide privacy guarantees by clipping and then\ninjecting external noise into sample gradients computed by the backpropagation\nalgorithm. Different from backpropagation, forward-learning algorithms based on\nperturbation inherently add noise during the forward pass and utilize\nrandomness to estimate the gradients. Although these algorithms are\nnon-privatized, the introduction of noise during the forward pass indirectly\nprovides internal randomness protection to the model parameters and their\ngradients, suggesting the potential for naturally providing differential\nprivacy. In this paper, we propose a \\blue{privatized} forward-learning\nalgorithm, Differential Private Unified Likelihood Ratio (DP-ULR), and\ndemonstrate its differential privacy guarantees. DP-ULR features a novel batch\nsampling operation with rejection, of which we provide theoretical analysis in\nconjunction with classic differential privacy mechanisms. DP-ULR is also\nunderpinned by a theoretically guided privacy controller that dynamically\nadjusts noise levels to manage privacy costs in each training step. Our\nexperiments indicate that DP-ULR achieves competitive performance compared to\ntraditional differential privacy training algorithms based on backpropagation,\nmaintaining nearly the same privacy loss limits.",
    "updated" : "2025-04-01T04:14:53Z",
    "published" : "2025-04-01T04:14:53Z",
    "authors" : [
      {
        "name" : "Mingqian Feng"
      },
      {
        "name" : "Zeliang Zhang"
      },
      {
        "name" : "Jinyang Jiang"
      },
      {
        "name" : "Yijie Peng"
      },
      {
        "name" : "Chenliang Xu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.02149v1",
    "title" : "Exploring the Privacy and Security Challenges Faced by Migrant Domestic\n  Workers in Chinese Smart Homes",
    "summary" : "The growing use of smart home devices poses considerable privacy and security\nchallenges, especially for individuals like migrant domestic workers (MDWs) who\nmay be surveilled by their employers. This paper explores the privacy and\nsecurity challenges experienced by MDWs in multi-user smart homes through\nin-depth semi-structured interviews with 26 MDWs and 5 staff members of\nagencies that recruit and/or train domestic workers in China. Our findings\nreveal that the relationships between MDWs, their employers, and agencies are\ncharacterized by significant power imbalances, influenced by Chinese cultural\nand social factors (such as Confucianism and collectivism), as well as legal\nones. Furthermore, the widespread and normalized use of surveillance\ntechnologies in China, particularly in public spaces, exacerbates these power\nimbalances, reinforcing a sense of constant monitoring and control. Drawing on\nour findings, we provide recommendations to domestic worker agencies and\npolicymakers to address the privacy and security challenges facing MDWs in\nChinese smart homes.",
    "updated" : "2025-04-02T21:49:15Z",
    "published" : "2025-04-02T21:49:15Z",
    "authors" : [
      {
        "name" : "Shijing He"
      },
      {
        "name" : "Xiao Zhan"
      },
      {
        "name" : "Yaxiong Lei"
      },
      {
        "name" : "Yueyan Liu"
      },
      {
        "name" : "Ruba Abu-Salma"
      },
      {
        "name" : "Jose Such"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.02068v1",
    "title" : "Privacy-Preserving Edge Computing from Pairing-Based Inner Product\n  Functional Encryption",
    "summary" : "Pairing-based inner product functional encryption provides an efficient\ntheoretical construction for privacy-preserving edge computing secured by\nwidely deployed elliptic curve cryptography. In this work, an efficient\nsoftware implementation framework for pairing-based function-hiding inner\nproduct encryption (FHIPE) is presented using the recently proposed and widely\nadopted BLS12-381 pairing-friendly elliptic curve. Algorithmic optimizations\nprovide $\\approx 2.6 \\times$ and $\\approx 3.4 \\times$ speedup in FHIPE\nencryption and decryption respectively, and extensive performance analysis is\npresented using a Raspberry Pi 4B edge device. The proposed optimizations\nenable this implementation framework to achieve performance and ciphertext size\ncomparable to previous work despite being implemented on an edge device with a\nslower processor and supporting a curve at much higher security level with a\nlarger prime field. Practical privacy-preserving edge computing applications\nsuch as encrypted biomedical sensor data classification and secure wireless\nfingerprint-based indoor localization are also demonstrated using the proposed\nimplementation framework.",
    "updated" : "2025-04-02T19:01:10Z",
    "published" : "2025-04-02T19:01:10Z",
    "authors" : [
      {
        "name" : "Utsav Banerjee"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.03173v1",
    "title" : "PPFPL: Cross-silo Privacy-preserving Federated Prototype Learning\n  Against Data Poisoning Attacks on Non-IID Data",
    "summary" : "Privacy-Preserving Federated Learning (PPFL) allows multiple clients to\ncollaboratively train a deep learning model by submitting hidden model updates.\nNonetheless, PPFL is vulnerable to data poisoning attacks due to the\ndistributed training nature of clients. Existing solutions have struggled to\nimprove the performance of cross-silo PPFL in poisoned Non-IID data. To address\nthe issues, this paper proposes a privacy-preserving federated prototype\nlearning framework, named PPFPL, which enhances the cross-silo FL performance\nin poisoned Non-IID data while effectively resisting data poisoning attacks.\nSpecifically, we adopt prototypes as client-submitted model updates to\neliminate the impact of tampered data distribution on federated learning.\nMoreover, we utilize two servers to achieve Byzantine-robust aggregation by\nsecure aggregation protocol, which greatly reduces the impact of malicious\nclients. Theoretical analyses confirm the convergence of PPFPL, and\nexperimental results on publicly available datasets show that PPFPL is\neffective for resisting data poisoning attacks with Non-IID conditions.",
    "updated" : "2025-04-04T05:05:24Z",
    "published" : "2025-04-04T05:05:24Z",
    "authors" : [
      {
        "name" : "Hongliang Zhang"
      },
      {
        "name" : "Jiguo Yu"
      },
      {
        "name" : "Fenghua Xu"
      },
      {
        "name" : "Chunqiang Hu"
      },
      {
        "name" : "Yongzhao Zhang"
      },
      {
        "name" : "Xiaofen Wang"
      },
      {
        "name" : "Zhongyuan Yu"
      },
      {
        "name" : "Xiaosong Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.05202v1",
    "title" : "Infinitely Divisible Noise for Differential Privacy: Nearly Optimal\n  Error in the High $\\varepsilon$ Regime",
    "summary" : "Differential privacy (DP) can be achieved in a distributed manner, where\nmultiple parties add independent noise such that their sum protects the overall\ndataset with DP. A common technique here is for each party to sample their\nnoise from the decomposition of an infinitely divisible distribution. We\nanalyze two mechanisms in this setting: 1) the generalized discrete Laplace\n(GDL) mechanism, whose distribution (which is closed under summation) follows\nfrom differences of i.i.d. negative binomial shares, and 2) the multi-scale\ndiscrete Laplace (MSDLap) mechanism, a novel mechanism following the sum of\nmultiple i.i.d. discrete Laplace shares at different scales.\n  For $\\varepsilon \\geq 1$, our mechanisms can be parameterized to have\n$O\\left(\\Delta^3 e^{-\\varepsilon}\\right)$ and $O\\left(\\min\\left(\\Delta^3\ne^{-\\varepsilon}, \\Delta^2 e^{-2\\varepsilon/3}\\right)\\right)$ MSE,\nrespectively, where $\\Delta$ denote the sensitivity; the latter bound matches\nknown optimality results. We also show a transformation from the discrete\nsetting to the continuous setting, which allows us to transform both mechanisms\nto the continuous setting and thereby achieve the optimal $O\\left(\\Delta^2\ne^{-2\\varepsilon / 3}\\right)$ MSE. To our knowledge, these are the first\ninfinitely divisible additive noise mechanisms that achieve order-optimal MSE\nunder pure DP, so our work shows formally there is no separation in utility\nwhen query-independent noise adding mechanisms are restricted to infinitely\ndivisible noise. For the continuous setting, our result improves upon the Arete\nmechanism from [Pagh and Stausholm, ALT 2022] which gives an MSE of\n$O\\left(\\Delta^2 e^{-\\varepsilon/4}\\right)$. Furthermore, we give an exact\nsampler tuned to efficiently implement the MSDLap mechanism, and we apply our\nresults to improve a state of the art multi-message shuffle DP protocol in the\nhigh $\\varepsilon$ regime.",
    "updated" : "2025-04-07T15:50:46Z",
    "published" : "2025-04-07T15:50:46Z",
    "authors" : [
      {
        "name" : "Charlie Harrison"
      },
      {
        "name" : "Pasin Manurangsi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.04734v1",
    "title" : "Teaching Data Science Students to Sketch Privacy Designs through\n  Heuristics (Extended Technical Report)",
    "summary" : "Recent studies reveal that experienced data practitioners often draw sketches\nto facilitate communication around privacy design concepts. However, there is\nlimited understanding of how we can help novice students develop such\ncommunication skills. This paper studies methods for lowering novice data\nscience students' barriers to creating high-quality privacy sketches. We first\nconducted a need-finding study (N=12) to identify barriers students face when\nsketching privacy designs. We then used a human-centered design approach to\nguide the method development, culminating in three simple, text-based\nheuristics. Our user studies with 24 data science students revealed that simply\npresenting three heuristics to the participants at the beginning of the study\ncan enhance the coverage of privacy-related design decisions in sketches,\nreduce the mental effort required for creating sketches, and improve the\nreadability of the final sketches.",
    "updated" : "2025-04-07T05:12:21Z",
    "published" : "2025-04-07T05:12:21Z",
    "authors" : [
      {
        "name" : "Jinhe Wen"
      },
      {
        "name" : "Yingxi Zhao"
      },
      {
        "name" : "Wenqian Xu"
      },
      {
        "name" : "Yaxing Yao"
      },
      {
        "name" : "Haojian Jin"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.04388v1",
    "title" : "Who's Watching You Zoom? Investigating Privacy of Third-Party Zoom Apps",
    "summary" : "Zoom serves millions of users daily and allows third-party developers to\nintegrate their apps with the Zoom client and reach those users. So far, these\napps' privacy and security aspects, which can access rich audio-visual data\n(among others) from Zoom, have not been scientifically investigated. This paper\nexamines the evolution of the Zoom Marketplace over one year, identifying\ntrends in apps, their data collection behaviors, and the transparency of\nprivacy policies. Our findings include worrisome details about the increasing\nover-collection of user data, non-transparency about purposes and sharing\nbehaviors, and possible non-compliance with relevant laws. We believe these\nfindings will inform future privacy and security research on this platform and\nhelp improve Zoom's app review process and platform policy.",
    "updated" : "2025-04-06T06:48:58Z",
    "published" : "2025-04-06T06:48:58Z",
    "authors" : [
      {
        "name" : "Saharsh Goenka"
      },
      {
        "name" : "Adit Prabhu"
      },
      {
        "name" : "Payge Sakurai"
      },
      {
        "name" : "Mrinaal Ramachandran"
      },
      {
        "name" : "Rakibul Hasan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.04033v1",
    "title" : "Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks\n  and Defenses",
    "summary" : "As machine learning (ML) technologies become more prevalent in\nprivacy-sensitive areas like healthcare and finance, eventually incorporating\nsensitive information in building data-driven algorithms, it is vital to\nscrutinize whether these data face any privacy leakage risks. One potential\nthreat arises from an adversary querying trained models using the public,\nnon-sensitive attributes of entities in the training data to infer their\nprivate, sensitive attributes, a technique known as the attribute inference\nattack. This attack is particularly deceptive because, while it may perform\npoorly in predicting sensitive attributes across the entire dataset, it excels\nat predicting the sensitive attributes of records from a few vulnerable groups,\na phenomenon known as disparate vulnerability. This paper illustrates that an\nadversary can take advantage of this disparity to carry out a series of new\nattacks, showcasing a threat level beyond previous imagination. We first\ndevelop a novel inference attack called the disparity inference attack, which\ntargets the identification of high-risk groups within the dataset. We then\nintroduce two targeted variations of the attribute inference attack that can\nidentify and exploit a vulnerable subset of the training data, marking the\nfirst instances of targeted attacks in this category, achieving significantly\nhigher accuracy than untargeted versions. We are also the first to introduce a\nnovel and effective disparity mitigation technique that simultaneously\npreserves model performance and prevents any risk of targeted attacks.",
    "updated" : "2025-04-05T02:58:37Z",
    "published" : "2025-04-05T02:58:37Z",
    "authors" : [
      {
        "name" : "Ehsanul Kabir"
      },
      {
        "name" : "Lucas Craig"
      },
      {
        "name" : "Shagufta Mehnaz"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.03798v1",
    "title" : "An Intelligent and Privacy-Preserving Digital Twin Model for\n  Aging-in-Place",
    "summary" : "The population of older adults is steadily increasing, with a strong\npreference for aging-in-place rather than moving to care facilities.\nConsequently, supporting this growing demographic has become a significant\nglobal challenge. However, facilitating successful aging-in-place is\nchallenging, requiring consideration of multiple factors such as data privacy,\nhealth status monitoring, and living environments to improve health outcomes.\nIn this paper, we propose an unobtrusive sensor system designed for\ninstallation in older adults' homes. Using data from the sensors, our system\nconstructs a digital twin, a virtual representation of events and activities\nthat occurred in the home. The system uses neural network models and decision\nrules to capture residents' activities and living environments. This digital\ntwin enables continuous health monitoring by providing actionable insights into\nresidents' well-being. Our system is designed to be low-cost and\nprivacy-preserving, with the aim of providing green and safe monitoring for the\nhealth of older adults. We have successfully deployed our system in two homes\nover a time period of two months, and our findings demonstrate the feasibility\nand effectiveness of digital twin technology in supporting independent living\nfor older adults. This study highlights that our system could revolutionize\nelder care by enabling personalized interventions, such as lifestyle\nadjustments, medical treatments, or modifications to the residential\nenvironment, to enhance health outcomes.",
    "updated" : "2025-04-04T05:37:08Z",
    "published" : "2025-04-04T05:37:08Z",
    "authors" : [
      {
        "name" : "Yongjie Wang"
      },
      {
        "name" : "Jonathan Cyril Leung"
      },
      {
        "name" : "Ming Chen"
      },
      {
        "name" : "Zhiwei Zeng"
      },
      {
        "name" : "Benny Toh Hsiang Tan"
      },
      {
        "name" : "Yang Qiu"
      },
      {
        "name" : "Zhiqi Shen"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "68T05,",
      "I.2; J.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.05849v1",
    "title" : "On the Importance of Conditioning for Privacy-Preserving Data\n  Augmentation",
    "summary" : "Latent diffusion models can be used as a powerful augmentation method to\nartificially extend datasets for enhanced training. To the human eye, these\naugmented images look very different to the originals. Previous work has\nsuggested to use this data augmentation technique for data anonymization.\nHowever, we show that latent diffusion models that are conditioned on features\nlike depth maps or edges to guide the diffusion process are not suitable as a\nprivacy preserving method. We use a contrastive learning approach to train a\nmodel that can correctly identify people out of a pool of candidates. Moreover,\nwe demonstrate that anonymization using conditioned diffusion models is\nsusceptible to black box attacks. We attribute the success of the described\nmethods to the conditioning of the latent diffusion model in the anonymization\nprocess. The diffusion model is instructed to produce similar edges for the\nanonymized images. Hence, a model can learn to recognize these patterns for\nidentification.",
    "updated" : "2025-04-08T09:27:51Z",
    "published" : "2025-04-08T09:27:51Z",
    "authors" : [
      {
        "name" : "Julian Lorenz"
      },
      {
        "name" : "Katja Ludwig"
      },
      {
        "name" : "Valentin Haug"
      },
      {
        "name" : "Rainer Lienhart"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.06697v1",
    "title" : "\"Sorry for bugging you so much.\" Exploring Developers' Behavior Towards\n  Privacy-Compliant Implementation",
    "summary" : "While protecting user data is essential, software developers often fail to\nfulfill privacy requirements. However, the reasons why they struggle with\nprivacy-compliant implementation remain unclear. Is it due to a lack of\nknowledge, or is it because of insufficient support? To provide foundational\ninsights in this field, we conducted a qualitative 5-hour programming study\nwith 30 professional software developers implementing 3 privacy-sensitive\nprogramming tasks that were designed with GDPR compliance in mind. To explore\nif and how developers implement privacy requirements, participants were divided\ninto 3 groups: control, privacy prompted, and privacy expert-supported. After\ntask completion, we conducted follow-up interviews. Alarmingly, almost all\nparticipants submitted non-GDPR-compliant solutions (79/90). In particular,\nnone of the 3 tasks were solved privacy-compliant by all 30 participants, with\nthe non-prompted group having the lowest number of 3 out of 30\nprivacy-compliant solution attempts. Privacy prompting and expert support only\nslightly improved participants' submissions, with 6/30 and 8/30\nprivacy-compliant attempts, respectively. In fact, all participants reported\nsevere issues addressing common privacy requirements such as purpose\nlimitation, user consent, or data minimization. Counterintuitively, although\nmost developers exhibited minimal confidence in their solutions, they rarely\nsought online assistance or contacted the privacy expert, with only 4 out of 10\nexpert-supported participants explicitly asking for compliance confirmation.\nInstead, participants often relied on existing implementations and focused on\nimplementing functionality and security first.",
    "updated" : "2025-04-09T08:59:17Z",
    "published" : "2025-04-09T08:59:17Z",
    "authors" : [
      {
        "name" : "Stefan Albert Horstmann"
      },
      {
        "name" : "Sandy Hong"
      },
      {
        "name" : "David Klein"
      },
      {
        "name" : "Raphael Serafini"
      },
      {
        "name" : "Martin Degeling"
      },
      {
        "name" : "Martin Johns"
      },
      {
        "name" : "Veelasha Moonsamy"
      },
      {
        "name" : "Alena Naiakshina"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.06552v1",
    "title" : "Understanding Users' Security and Privacy Concerns and Attitudes Towards\n  Conversational AI Platforms",
    "summary" : "The widespread adoption of conversational AI platforms has introduced new\nsecurity and privacy risks. While these risks and their mitigation strategies\nhave been extensively researched from a technical perspective, users'\nperceptions of these platforms' security and privacy remain largely unexplored.\nIn this paper, we conduct a large-scale analysis of over 2.5M user posts from\nthe r/ChatGPT Reddit community to understand users' security and privacy\nconcerns and attitudes toward conversational AI platforms. Our qualitative\nanalysis reveals that users are concerned about each stage of the data\nlifecycle (i.e., collection, usage, and retention). They seek mitigations for\nsecurity vulnerabilities, compliance with privacy regulations, and greater\ntransparency and control in data handling. We also find that users exhibit\nvaried behaviors and preferences when interacting with these platforms. Some\nusers proactively safeguard their data and adjust privacy settings, while\nothers prioritize convenience over privacy risks, dismissing privacy concerns\nin favor of benefits, or feel resigned to inevitable data sharing. Through\nqualitative content and regression analysis, we discover that users' concerns\nevolve over time with the evolving AI landscape and are influenced by\ntechnological developments and major events. Based on our findings, we provide\nrecommendations for users, platforms, enterprises, and policymakers to enhance\ntransparency, improve data controls, and increase user trust and adoption.",
    "updated" : "2025-04-09T03:22:48Z",
    "published" : "2025-04-09T03:22:48Z",
    "authors" : [
      {
        "name" : "Mutahar Ali"
      },
      {
        "name" : "Arjun Arunasalam"
      },
      {
        "name" : "Habiba Farrukh"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07761v1",
    "title" : "Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection",
    "summary" : "In an increasingly digitalized world, verifying the authenticity of ID\ndocuments has become a critical challenge for real-life applications such as\ndigital banking, crypto-exchanges, renting, etc. This study focuses on the\ntopic of fake ID detection, covering several limitations in the field. In\nparticular, no publicly available data from real ID documents exists, and most\nstudies rely on proprietary in-house databases that are not available due to\nprivacy reasons. In order to shed some light on this critical challenge that\nmakes difficult to advance in the field, we explore a trade-off between privacy\n(i.e., amount of sensitive data available) and performance, proposing a novel\npatch-wise approach for privacy-preserving fake ID detection. Our proposed\napproach explores how privacy can be enhanced through: i) two levels of\nanonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii)\ndifferent patch size configurations, varying the amount of sensitive data\nvisible in the patch image. Also, state-of-the-art methods such as Vision\nTransformers and Foundation Models are considered in the analysis. The\nexperimental framework shows that, on an unseen database (DLC-2021), our\nproposal achieves 13.91% and 0% EERs at patch and ID document level, showing a\ngood generalization to other databases. In addition to this exploration,\nanother key contribution of our study is the release of the first publicly\navailable database that contains 48,400 patches from both real and fake ID\ndocuments, along with the experimental framework and models, which will be\navailable in our GitHub.",
    "updated" : "2025-04-10T14:01:22Z",
    "published" : "2025-04-10T14:01:22Z",
    "authors" : [
      {
        "name" : "Javier Muñoz-Haro"
      },
      {
        "name" : "Ruben Tolosana"
      },
      {
        "name" : "Ruben Vera-Rodriguez"
      },
      {
        "name" : "Aythami Morales"
      },
      {
        "name" : "Julian Fierrez"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07578v1",
    "title" : "Privacy-Preserving Vertical K-Means Clustering",
    "summary" : "Clustering is a fundamental data processing task used for grouping records\nbased on one or more features. In the vertically partitioned setting, data is\ndistributed among entities, with each holding only a subset of those features.\nA key challenge in this scenario is that computing distances between records\nrequires access to all distributed features, which may be privacy-sensitive and\ncannot be directly shared with other parties. The goal is to compute the joint\nclusters while preserving the privacy of each entity's dataset. Existing\nsolutions using secret sharing or garbled circuits implement privacy-preserving\nvariants of Lloyd's algorithm but incur high communication costs, scaling as\nO(nkt), where n is the number of data points, k the number of clusters, and t\nthe number of rounds. These methods become impractical for large datasets or\nseveral parties, limiting their use to LAN settings only. On the other hand, a\ndifferent line of solutions rely on differential privacy (DP) to outsource the\nlocal features of the parties to a central server. However, they often\nsignificantly degrade the utility of the clustering outcome due to excessive\nnoise. In this work, we propose a novel solution based on homomorphic\nencryption and DP, reducing communication complexity to O(n+kt). In our method,\nparties securely outsource their features once, allowing a computing party to\nperform clustering operations under encryption. DP is applied only to the\nclusters' centroids, ensuring privacy with minimal impact on utility. Our\nsolution clusters 100,000 two-dimensional points into five clusters using only\n73MB of communication, compared to 101GB for existing works, and completes in\njust under 3 minutes on a 100Mbps network, whereas existing works take over 1\nday. This makes our solution practical even for WAN deployments, all while\nmaintaining accuracy comparable to plaintext k-means algorithms.",
    "updated" : "2025-04-10T09:20:56Z",
    "published" : "2025-04-10T09:20:56Z",
    "authors" : [
      {
        "name" : "Federico Mazzone"
      },
      {
        "name" : "Trevor Brown"
      },
      {
        "name" : "Florian Kerschbaum"
      },
      {
        "name" : "Kevin H. Wilson"
      },
      {
        "name" : "Maarten Everts"
      },
      {
        "name" : "Florian Hahn"
      },
      {
        "name" : "Andreas Peter"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07414v1",
    "title" : "Decomposition-Based Optimal Bounds for Privacy Amplification via\n  Shuffling",
    "summary" : "Shuffling has been shown to amplify differential privacy guarantees, offering\na stronger privacy-utility trade-off. To characterize and compute this\namplification, two fundamental analytical frameworks have been proposed: the\nprivacy blanket by Balle et al. (CRYPTO 2019) and the clone paradigm (including\nboth the standard clone and stronger clone) by Feldman et al. (FOCS 2021, SODA\n2023). All these methods rely on decomposing local randomizers.\n  In this work, we introduce a unified analysis framework--the general clone\nparadigm--which encompasses all possible decompositions. We identify the\noptimal decomposition within the general clone paradigm. Moreover, we develop a\nsimple and efficient algorithm to compute the exact value of the optimal\nprivacy amplification bounds via Fast Fourier Transform. Experimental results\ndemonstrate that the computed upper bounds for privacy amplification closely\napproximate the lower bounds, highlighting the tightness of our approach.\nFinally, using our algorithm, we conduct the first systematic analysis of the\njoint composition of LDP protocols in the shuffle model.",
    "updated" : "2025-04-10T03:11:17Z",
    "published" : "2025-04-10T03:11:17Z",
    "authors" : [
      {
        "name" : "Pengcheng Su"
      },
      {
        "name" : "Haibo Cheng"
      },
      {
        "name" : "Ping Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07362v1",
    "title" : "Augmented Shuffle Protocols for Accurate and Robust Frequency Estimation\n  under Differential Privacy",
    "summary" : "The shuffle model of DP (Differential Privacy) provides high utility by\nintroducing a shuffler that randomly shuffles noisy data sent from users.\nHowever, recent studies show that existing shuffle protocols suffer from the\nfollowing two major drawbacks. First, they are vulnerable to local data\npoisoning attacks, which manipulate the statistics about input data by sending\ncrafted data, especially when the privacy budget epsilon is small. Second, the\nactual value of epsilon is increased by collusion attacks by the data collector\nand users.\n  In this paper, we address these two issues by thoroughly exploring the\npotential of the augmented shuffle model, which allows the shuffler to perform\nadditional operations, such as random sampling and dummy data addition.\nSpecifically, we propose a generalized framework for local-noise-free protocols\nin which users send (encrypted) input data to the shuffler without adding\nnoise. We show that this generalized protocol provides DP and is robust to the\nabove two attacks if a simpler mechanism that performs the same process on\nbinary input data provides DP. Based on this framework, we propose three\nconcrete protocols providing DP and robustness against the two attacks. Our\nfirst protocol generates the number of dummy values for each item from a\nbinomial distribution and provides higher utility than several state-of-the-art\nexisting shuffle protocols. Our second protocol significantly improves the\nutility of our first protocol by introducing a novel dummy-count distribution:\nasymmetric two-sided geometric distribution. Our third protocol is a special\ncase of our second protocol and provides pure epsilon-DP. We show the\neffectiveness of our protocols through theoretical analysis and comprehensive\nexperiments.",
    "updated" : "2025-04-10T01:06:05Z",
    "published" : "2025-04-10T01:06:05Z",
    "authors" : [
      {
        "name" : "Takao Murakami"
      },
      {
        "name" : "Yuichi Sei"
      },
      {
        "name" : "Reo Eriguchi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07323v1",
    "title" : "Prekey Pogo: Investigating Security and Privacy Issues in WhatsApp's\n  Handshake Mechanism",
    "summary" : "WhatsApp, the world's largest messaging application, uses a version of the\nSignal protocol to provide end-to-end encryption (E2EE) with strong security\nguarantees, including Perfect Forward Secrecy (PFS). To ensure PFS right from\nthe start of a new conversation -- even when the recipient is offline -- a\nstash of ephemeral (one-time) prekeys must be stored on a server. While the\ncritical role of these one-time prekeys in achieving PFS has been outlined in\nthe Signal specification, we are the first to demonstrate a targeted depletion\nattack against them on individual WhatsApp user devices. Our findings not only\nreveal an attack that can degrade PFS for certain messages, but also expose\ninherent privacy risks and serious availability implications arising from the\nrefilling and distribution procedure essential for this security mechanism.",
    "updated" : "2025-04-09T22:53:13Z",
    "published" : "2025-04-09T22:53:13Z",
    "authors" : [
      {
        "name" : "Gabriel K. Gegenhuber"
      },
      {
        "name" : "Philipp É. Frenzel"
      },
      {
        "name" : "Maximilian Günther"
      },
      {
        "name" : "Aljosha Judmayer"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.08616v1",
    "title" : "Preserving Privacy Without Compromising Accuracy: Machine Unlearning for\n  Handwritten Text Recognition",
    "summary" : "Handwritten Text Recognition (HTR) is essential for document analysis and\ndigitization. However, handwritten data often contains user-identifiable\ninformation, such as unique handwriting styles and personal lexicon choices,\nwhich can compromise privacy and erode trust in AI services. Legislation like\nthe ``right to be forgotten'' underscores the necessity for methods that can\nexpunge sensitive information from trained models. Machine unlearning addresses\nthis by selectively removing specific data from models without necessitating\ncomplete retraining. Yet, it frequently encounters a privacy-accuracy tradeoff,\nwhere safeguarding privacy leads to diminished model performance. In this\npaper, we introduce a novel two-stage unlearning strategy for a multi-head\ntransformer-based HTR model, integrating pruning and random labeling. Our\nproposed method utilizes a writer classification head both as an indicator and\na trigger for unlearning, while maintaining the efficacy of the recognition\nhead. To our knowledge, this represents the first comprehensive exploration of\nmachine unlearning within HTR tasks. We further employ Membership Inference\nAttacks (MIA) to evaluate the effectiveness of unlearning user-identifiable\ninformation. Extensive experiments demonstrate that our approach effectively\npreserves privacy while maintaining model accuracy, paving the way for new\nresearch directions in the document analysis community. Our code will be\npublicly available upon acceptance.",
    "updated" : "2025-04-11T15:21:12Z",
    "published" : "2025-04-11T15:21:12Z",
    "authors" : [
      {
        "name" : "Lei Kang"
      },
      {
        "name" : "Xuanshuo Fu"
      },
      {
        "name" : "Lluis Gomez"
      },
      {
        "name" : "Alicia Fornés"
      },
      {
        "name" : "Ernest Valveny"
      },
      {
        "name" : "Dimosthenis Karatzas"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.08254v1",
    "title" : "Understanding the Impact of Data Domain Extraction on Synthetic Data\n  Privacy",
    "summary" : "Privacy attacks, particularly membership inference attacks (MIAs), are widely\nused to assess the privacy of generative models for tabular synthetic data,\nincluding those with Differential Privacy (DP) guarantees. These attacks often\nexploit outliers, which are especially vulnerable due to their position at the\nboundaries of the data domain (e.g., at the minimum and maximum values).\nHowever, the role of data domain extraction in generative models and its impact\non privacy attacks have been overlooked. In this paper, we examine three\nstrategies for defining the data domain: assuming it is externally provided\n(ideally from public data), extracting it directly from the input data, and\nextracting it with DP mechanisms. While common in popular implementations and\nlibraries, we show that the second approach breaks end-to-end DP guarantees and\nleaves models vulnerable. While using a provided domain (if representative) is\npreferable, extracting it with DP can also defend against popular MIAs, even at\nhigh privacy budgets.",
    "updated" : "2025-04-11T04:35:24Z",
    "published" : "2025-04-11T04:35:24Z",
    "authors" : [
      {
        "name" : "Georgi Ganev"
      },
      {
        "name" : "Meenatchi Sundaram Muthu Selva Annamalai"
      },
      {
        "name" : "Sofiane Mahiou"
      },
      {
        "name" : "Emiliano De Cristofaro"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07414v2",
    "title" : "Decomposition-Based Optimal Bounds for Privacy Amplification via\n  Shuffling",
    "summary" : "Shuffling has been shown to amplify differential privacy guarantees, offering\na stronger privacy-utility trade-off. To characterize and compute this\namplification, two fundamental analytical frameworks have been proposed: the\nprivacy blanket by Balle et al. (CRYPTO 2019) and the clone paradigm (including\nboth the standard clone and stronger clone) by Feldman et al. (FOCS 2021, SODA\n2023). All these methods rely on decomposing local randomizers.\n  In this work, we introduce a unified analysis framework--the general clone\nparadigm--which encompasses all possible decompositions. We identify the\noptimal decomposition within the general clone paradigm. Moreover, we develop a\nsimple and efficient algorithm to compute the exact value of the optimal\nprivacy amplification bounds via Fast Fourier Transform. Experimental results\ndemonstrate that the computed upper bounds for privacy amplification closely\napproximate the lower bounds, highlighting the tightness of our approach.\nFinally, using our algorithm, we conduct the first systematic analysis of the\njoint composition of LDP protocols in the shuffle model.",
    "updated" : "2025-04-11T01:35:46Z",
    "published" : "2025-04-10T03:11:17Z",
    "authors" : [
      {
        "name" : "Pengcheng Su"
      },
      {
        "name" : "Haibo Cheng"
      },
      {
        "name" : "Ping Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.10456v1",
    "title" : "Privacy-Preserving Distributed Link Predictions Among Peers in Online\n  Classrooms Using Federated Learning",
    "summary" : "Social interactions among classroom peers, represented as social learning\nnetworks (SLNs), play a crucial role in enhancing learning outcomes. While SLN\nanalysis has recently garnered attention, most existing approaches rely on\ncentralized training, where data is aggregated and processed on a local/cloud\nserver with direct access to raw data. However, in real-world educational\nsettings, such direct access across multiple classrooms is often restricted due\nto privacy concerns. Furthermore, training models on isolated classroom data\nprevents the identification of common interaction patterns that exist across\nmultiple classrooms, thereby limiting model performance. To address these\nchallenges, we propose one of the first frameworks that integrates Federated\nLearning (FL), a distributed and collaborative machine learning (ML) paradigm,\nwith SLNs derived from students' interactions in multiple classrooms' online\nforums to predict future link formations (i.e., interactions) among students.\nBy leveraging FL, our approach enables collaborative model training across\nmultiple classrooms while preserving data privacy, as it eliminates the need\nfor raw data centralization. Recognizing that each classroom may exhibit unique\nstudent interaction dynamics, we further employ model personalization\ntechniques to adapt the FL model to individual classroom characteristics. Our\nresults demonstrate the effectiveness of our approach in capturing both shared\nand classroom-specific representations of student interactions in SLNs.\nAdditionally, we utilize explainable AI (XAI) techniques to interpret model\npredictions, identifying key factors that influence link formation across\ndifferent classrooms. These insights unveil the drivers of social learning\ninteractions within a privacy-preserving, collaborative, and distributed ML\nframework -- an aspect that has not been explored before.",
    "updated" : "2025-04-14T17:43:11Z",
    "published" : "2025-04-14T17:43:11Z",
    "authors" : [
      {
        "name" : "Anurata Prabha Hridi"
      },
      {
        "name" : "Muntasir Hoq"
      },
      {
        "name" : "Zhikai Gao"
      },
      {
        "name" : "Collin Lynch"
      },
      {
        "name" : "Rajeev Sahay"
      },
      {
        "name" : "Seyyedali Hosseinalipour"
      },
      {
        "name" : "Bita Akram"
      }
    ],
    "categories" : [
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.10267v1",
    "title" : "Trade-offs in Privacy-Preserving Eye Tracking through Iris Obfuscation:\n  A Benchmarking Study",
    "summary" : "Recent developments in hardware, computer graphics, and AI may soon enable\nAR/VR head-mounted displays (HMDs) to become everyday devices like smartphones\nand tablets. Eye trackers within HMDs provide a special opportunity for such\nsetups as it is possible to facilitate gaze-based research and interaction.\nHowever, estimating users' gaze information often requires raw eye images and\nvideos that contain iris textures, which are considered a gold standard\nbiometric for user authentication, and this raises privacy concerns. Previous\nresearch in the eye-tracking community focused on obfuscating iris textures\nwhile keeping utility tasks such as gaze estimation accurate. Despite these\nattempts, there is no comprehensive benchmark that evaluates state-of-the-art\napproaches. Considering all, in this paper, we benchmark blurring, noising,\ndownsampling, rubber sheet model, and iris style transfer to obfuscate user\nidentity, and compare their impact on image quality, privacy, utility, and risk\nof imposter attack on two datasets. We use eye segmentation and gaze estimation\nas utility tasks, and reduction in iris recognition accuracy as a measure of\nprivacy protection, and false acceptance rate to estimate risk of attack. Our\nexperiments show that canonical image processing methods like blurring and\nnoising cause a marginal impact on deep learning-based tasks. While\ndownsampling, rubber sheet model, and iris style transfer are effective in\nhiding user identifiers, iris style transfer, with higher computation cost,\noutperforms others in both utility tasks, and is more resilient against spoof\nattacks. Our analyses indicate that there is no universal optimal approach to\nbalance privacy, utility, and computation burden. Therefore, we recommend\npractitioners consider the strengths and weaknesses of each approach, and\npossible combinations of those to reach an optimal privacy-utility trade-off.",
    "updated" : "2025-04-14T14:29:38Z",
    "published" : "2025-04-14T14:29:38Z",
    "authors" : [
      {
        "name" : "Mengdi Wang"
      },
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Enkelejda Kasneci"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.10016v1",
    "title" : "Quantifying Privacy Leakage in Split Inference via Fisher-Approximated\n  Shannon Information Analysis",
    "summary" : "Split inference (SI) partitions deep neural networks into distributed\nsub-models, enabling privacy-preserving collaborative learning. Nevertheless,\nit remains vulnerable to Data Reconstruction Attacks (DRAs), wherein\nadversaries exploit exposed smashed data to reconstruct raw inputs. Despite\nextensive research on adversarial attack-defense games, a shortfall remains in\nthe fundamental analysis of privacy risks. This paper establishes a theoretical\nframework for privacy leakage quantification using information theory, defining\nit as the adversary's certainty and deriving both average-case and worst-case\nerror bounds. We introduce Fisher-approximated Shannon information (FSInfo), a\nnovel privacy metric utilizing Fisher Information (FI) for operational privacy\nleakage computation. We empirically show that our privacy metric correlates\nwell with empirical attacks and investigate some of the factors that affect\nprivacy leakage, namely the data distribution, model size, and overfitting.",
    "updated" : "2025-04-14T09:19:06Z",
    "published" : "2025-04-14T09:19:06Z",
    "authors" : [
      {
        "name" : "Ruijun Deng"
      },
      {
        "name" : "Zhihui Lu"
      },
      {
        "name" : "Qiang Duan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.09961v1",
    "title" : "Privacy Meets Explainability: Managing Confidential Data and\n  Transparency Policies in LLM-Empowered Science",
    "summary" : "As Large Language Models (LLMs) become integral to scientific workflows,\nconcerns over the confidentiality and ethical handling of confidential data\nhave emerged. This paper explores data exposure risks through LLM-powered\nscientific tools, which can inadvertently leak confidential information,\nincluding intellectual property and proprietary data, from scientists'\nperspectives. We propose \"DataShield\", a framework designed to detect\nconfidential data leaks, summarize privacy policies, and visualize data flow,\nensuring alignment with organizational policies and procedures. Our approach\naims to inform scientists about data handling practices, enabling them to make\ninformed decisions and protect sensitive information. Ongoing user studies with\nscientists are underway to evaluate the framework's usability, trustworthiness,\nand effectiveness in tackling real-world privacy challenges.",
    "updated" : "2025-04-14T07:58:26Z",
    "published" : "2025-04-14T07:58:26Z",
    "authors" : [
      {
        "name" : "Yashothara Shanmugarasa"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Dehai Zhao"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.09952v1",
    "title" : "Secrecy and Privacy in Multi-Access Combinatorial Topology",
    "summary" : "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
    "updated" : "2025-04-14T07:30:03Z",
    "published" : "2025-04-14T07:30:03Z",
    "authors" : [
      {
        "name" : "Mallikharjuna Chinnapadamala"
      },
      {
        "name" : "B. Sundar Rajan"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.09517v1",
    "title" : "RoboComm: A DID-based scalable and privacy-preserving Robot-to-Robot\n  interaction over state channels",
    "summary" : "In a multi robot system establishing trust amongst untrusted robots from\ndifferent organisations while preserving a robot's privacy is a challenge.\nRecently decentralized technologies such as smart contract and blockchain are\nbeing explored for applications in robotics. However, the limited transaction\nprocessing and high maintenance cost hinder the widespread adoption of such\napproaches. Moreover, blockchain transactions be they on public or private\npermissioned blockchain are publically readable which further fails to preserve\nthe confidentiality of the robot's data and privacy of the robot.\n  In this work, we propose RoboComm a Decentralized Identity based approach for\nprivacy-preserving interaction between robots. With DID a component of\nSelf-Sovereign Identity; robots can authenticate each other independently\nwithout relying on any third-party service. Verifiable Credentials enable\nprivate data associated with a robot to be stored within the robot's hardware,\nunlike existing blockchain based approaches where the data has to be on the\nblockchain. We improve throughput by allowing message exchange over state\nchannels. Being a blockchain backed solution RoboComm provides a trustworthy\nsystem without relying on a single party. Moreover, we implement our proposed\napproach to demonstrate the feasibility of our solution.",
    "updated" : "2025-04-13T11:10:04Z",
    "published" : "2025-04-13T11:10:04Z",
    "authors" : [
      {
        "name" : "Roshan Singh"
      },
      {
        "name" : "Sushant Pandey"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.09095v1",
    "title" : "Privacy Preservation in Gen AI Applications",
    "summary" : "The ability of machines to comprehend and produce language that is similar to\nthat of humans has revolutionized sectors like customer service, healthcare,\nand finance thanks to the quick advances in Natural Language Processing (NLP),\nwhich are fueled by Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs). However, because LLMs trained on large datasets may\nunintentionally absorb and reveal Personally Identifiable Information (PII)\nfrom user interactions, these capabilities also raise serious privacy concerns.\nDeep neural networks' intricacy makes it difficult to track down or stop the\ninadvertent storing and release of private information, which raises serious\nconcerns about the privacy and security of AI-driven data. This study tackles\nthese issues by detecting Generative AI weaknesses through attacks such as data\nextraction, model inversion, and membership inference. A privacy-preserving\nGenerative AI application that is resistant to these assaults is then\ndeveloped. It ensures privacy without sacrificing functionality by using\nmethods to identify, alter, or remove PII before to dealing with LLMs. In order\nto determine how well cloud platforms like Microsoft Azure, Google Cloud, and\nAWS provide privacy tools for protecting AI applications, the study also\nexamines these technologies. In the end, this study offers a fundamental\nprivacy paradigm for generative AI systems, focusing on data security and moral\nAI implementation, and opening the door to a more secure and conscientious use\nof these tools.",
    "updated" : "2025-04-12T06:19:37Z",
    "published" : "2025-04-12T06:19:37Z",
    "authors" : [
      {
        "name" : "Swetha S"
      },
      {
        "name" : "Ram Sundhar K Shaju"
      },
      {
        "name" : "Rakshana M"
      },
      {
        "name" : "Ganesh R"
      },
      {
        "name" : "Balavedhaa S"
      },
      {
        "name" : "Thiruvaazhi U"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.08254v2",
    "title" : "Understanding the Impact of Data Domain Extraction on Synthetic Data\n  Privacy",
    "summary" : "Privacy attacks, particularly membership inference attacks (MIAs), are widely\nused to assess the privacy of generative models for tabular synthetic data,\nincluding those with Differential Privacy (DP) guarantees. These attacks often\nexploit outliers, which are especially vulnerable due to their position at the\nboundaries of the data domain (e.g., at the minimum and maximum values).\nHowever, the role of data domain extraction in generative models and its impact\non privacy attacks have been overlooked. In this paper, we examine three\nstrategies for defining the data domain: assuming it is externally provided\n(ideally from public data), extracting it directly from the input data, and\nextracting it with DP mechanisms. While common in popular implementations and\nlibraries, we show that the second approach breaks end-to-end DP guarantees and\nleaves models vulnerable. While using a provided domain (if representative) is\npreferable, extracting it with DP can also defend against popular MIAs, even at\nhigh privacy budgets.",
    "updated" : "2025-04-14T02:34:24Z",
    "published" : "2025-04-11T04:35:24Z",
    "authors" : [
      {
        "name" : "Georgi Ganev"
      },
      {
        "name" : "Meenatchi Sundaram Muthu Selva Annamalai"
      },
      {
        "name" : "Sofiane Mahiou"
      },
      {
        "name" : "Emiliano De Cristofaro"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.11429v1",
    "title" : "Improving Statistical Privacy by Subsampling",
    "summary" : "Differential privacy (DP) considers a scenario, where an adversary has almost\ncomplete information about the entries of a database This worst-case assumption\nis likely to overestimate the privacy thread for an individual in real life.\nStatistical privacy (SP) denotes a setting where only the distribution of the\ndatabase entries is known to an adversary, but not their exact values. In this\ncase one has to analyze the interaction between noiseless privacy based on the\nentropy of distributions and privacy mechanisms that distort the answers of\nqueries, which can be quite complex.\n  A privacy mechanism often used is to take samples of the data for answering a\nquery. This paper proves precise bounds how much different methods of sampling\nincrease privacy in the statistical setting with respect to database size and\nsampling rate. They allow us to deduce when and how much sampling provides an\nimprovement and how far this depends on the privacy parameter {\\epsilon}. To\nperform these investigations we develop a framework to model sampling\ntechniques.\n  For the DP setting tradeoff functions have been proposed as a finer measure\nfor privacy compared to ({\\epsilon},{\\delta})-pairs. We apply these tools to\nstatistical privacy with subsampling to get a comparable characterization",
    "updated" : "2025-04-15T17:40:45Z",
    "published" : "2025-04-15T17:40:45Z",
    "authors" : [
      {
        "name" : "Dennis Breutigam"
      },
      {
        "name" : "Rüdiger Reischuk"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.10698v1",
    "title" : "Optimising Intrusion Detection Systems in Cloud-Edge Continuum with\n  Knowledge Distillation for Privacy-Preserving and Efficient Communication",
    "summary" : "The growth of the Internet of Things has amplified the need for secure data\ninteractions in cloud-edge ecosystems, where sensitive information is\nconstantly processed across various system layers. Intrusion detection systems\nare commonly used to protect such environments from malicious attacks.\nRecently, Federated Learning has emerged as an effective solution for\nimplementing intrusion detection systems, owing to its decentralised\narchitecture that avoids sharing raw data with a central server, thereby\nenhancing data privacy. Despite its benefits, Federated Learning faces\ncriticism for high communication overhead from frequent model updates,\nespecially in large-scale Cloud-Edge infrastructures. This paper explores\nKnowledge Distillation to reduce communication overhead in Cloud-Edge intrusion\ndetection while preserving accuracy and data privacy. Experiments show\nsignificant improvements over state-of-the-art methods.",
    "updated" : "2025-04-14T20:45:05Z",
    "published" : "2025-04-14T20:45:05Z",
    "authors" : [
      {
        "name" : "Soad Almabdy"
      },
      {
        "name" : "Amjad Ullah"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.10267v2",
    "title" : "Trade-offs in Privacy-Preserving Eye Tracking through Iris Obfuscation:\n  A Benchmarking Study",
    "summary" : "Recent developments in hardware, computer graphics, and AI may soon enable\nAR/VR head-mounted displays (HMDs) to become everyday devices like smartphones\nand tablets. Eye trackers within HMDs provide a special opportunity for such\nsetups as it is possible to facilitate gaze-based research and interaction.\nHowever, estimating users' gaze information often requires raw eye images and\nvideos that contain iris textures, which are considered a gold standard\nbiometric for user authentication, and this raises privacy concerns. Previous\nresearch in the eye-tracking community focused on obfuscating iris textures\nwhile keeping utility tasks such as gaze estimation accurate. Despite these\nattempts, there is no comprehensive benchmark that evaluates state-of-the-art\napproaches. Considering all, in this paper, we benchmark blurring, noising,\ndownsampling, rubber sheet model, and iris style transfer to obfuscate user\nidentity, and compare their impact on image quality, privacy, utility, and risk\nof imposter attack on two datasets. We use eye segmentation and gaze estimation\nas utility tasks, and reduction in iris recognition accuracy as a measure of\nprivacy protection, and false acceptance rate to estimate risk of attack. Our\nexperiments show that canonical image processing methods like blurring and\nnoising cause a marginal impact on deep learning-based tasks. While\ndownsampling, rubber sheet model, and iris style transfer are effective in\nhiding user identifiers, iris style transfer, with higher computation cost,\noutperforms others in both utility tasks, and is more resilient against spoof\nattacks. Our analyses indicate that there is no universal optimal approach to\nbalance privacy, utility, and computation burden. Therefore, we recommend\npractitioners consider the strengths and weaknesses of each approach, and\npossible combinations of those to reach an optimal privacy-utility trade-off.",
    "updated" : "2025-04-15T09:43:41Z",
    "published" : "2025-04-14T14:29:38Z",
    "authors" : [
      {
        "name" : "Mengdi Wang"
      },
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Enkelejda Kasneci"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.09517v2",
    "title" : "RoboComm: A DID-based scalable and privacy-preserving Robot-to-Robot\n  interaction over state channels",
    "summary" : "In a multi robot system establishing trust amongst untrusted robots from\ndifferent organisations while preserving a robot's privacy is a challenge.\nRecently decentralized technologies such as smart contract and blockchain are\nbeing explored for applications in robotics. However, the limited transaction\nprocessing and high maintenance cost hinder the widespread adoption of such\napproaches. Moreover, blockchain transactions be they on public or private\npermissioned blockchain are publically readable which further fails to preserve\nthe confidentiality of the robot's data and privacy of the robot.\n  In this work, we propose RoboComm a Decentralized Identity based approach for\nprivacy-preserving interaction between robots. With DID a component of\nSelf-Sovereign Identity; robots can authenticate each other independently\nwithout relying on any third-party service. Verifiable Credentials enable\nprivate data associated with a robot to be stored within the robot's hardware,\nunlike existing blockchain based approaches where the data has to be on the\nblockchain. We improve throughput by allowing message exchange over state\nchannels. Being a blockchain backed solution RoboComm provides a trustworthy\nsystem without relying on a single party. Moreover, we implement our proposed\napproach to demonstrate the feasibility of our solution.",
    "updated" : "2025-04-15T17:22:20Z",
    "published" : "2025-04-13T11:10:04Z",
    "authors" : [
      {
        "name" : "Roshan Singh"
      },
      {
        "name" : "Sushant Pandey"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.12129v1",
    "title" : "Anti-Aesthetics: Protecting Facial Privacy against Customized\n  Text-to-Image Synthesis",
    "summary" : "The rise of customized diffusion models has spurred a boom in personalized\nvisual content creation, but also poses risks of malicious misuse, severely\nthreatening personal privacy and copyright protection. Some studies show that\nthe aesthetic properties of images are highly positively correlated with human\nperception of image quality. Inspired by this, we approach the problem from a\nnovel and intriguing aesthetic perspective to degrade the generation quality of\nmaliciously customized models, thereby achieving better protection of facial\nidentity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA)\nframework to fully explore aesthetic cues, which consists of two key branches:\n1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward\nmechanism and a global anti-aesthetic loss, it can degrade the overall\naesthetics of the generated content; 2) Local Anti-Aesthetics: A local\nanti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to\nguide adversarial perturbations to disrupt local facial identity. By seamlessly\nintegrating both branches, our HAA effectively achieves the goal of\nanti-aesthetics from a global to a local level during customized generation.\nExtensive experiments show that HAA outperforms existing SOTA methods largely\nin identity removal, providing a powerful tool for protecting facial privacy\nand copyright.",
    "updated" : "2025-04-16T14:44:00Z",
    "published" : "2025-04-16T14:44:00Z",
    "authors" : [
      {
        "name" : "Songping Wang"
      },
      {
        "name" : "Yueming Lyu"
      },
      {
        "name" : "Shiqi Liu"
      },
      {
        "name" : "Ning Li"
      },
      {
        "name" : "Tong Tong"
      },
      {
        "name" : "Hao Sun"
      },
      {
        "name" : "Caifeng Shan"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.11860v1",
    "title" : "From Data Behavior to Code Analysis: A Multimodal Study on Security and\n  Privacy Challenges in Blockchain-Based DApp",
    "summary" : "The recent proliferation of blockchain-based decentralized applications\n(DApp) has catalyzed transformative advancements in distributed systems, with\nextensive deployments observed across financial, entertainment, media, and\ncybersecurity domains. These trustless architectures, characterized by their\ndecentralized nature and elimination of third-party intermediaries, have\ngarnered substantial institutional attention. Consequently, the escalating\nsecurity challenges confronting DApp demand rigorous scholarly investigation.\nThis study initiates with a systematic analysis of behavioral patterns derived\nfrom empirical DApp datasets, establishing foundational insights for subsequent\nmethodological developments. The principal security vulnerabilities in\nEthereum-based smart contracts developed via Solidity are then critically\nexamined. Specifically, reentrancy vulnerability attacks are addressed by\nformally representing contract logic using highly expressive code fragments.\nThis enables precise source code-level detection via bidirectional long\nshort-term memory networks with attention mechanisms (BLSTM-ATT). Regarding\nprivacy preservation challenges, contemporary solutions are evaluated through\ndual analytical lenses: identity privacy preservation and transaction anonymity\nenhancement, while proposing future research trajectories in cryptographic\nobfuscation techniques.",
    "updated" : "2025-04-16T08:30:43Z",
    "published" : "2025-04-16T08:30:43Z",
    "authors" : [
      {
        "name" : "Haoyang Sun"
      },
      {
        "name" : "Yishun Wang"
      },
      {
        "name" : "Xiaoqi Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.11793v1",
    "title" : "Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification",
    "summary" : "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.",
    "updated" : "2025-04-16T05:59:29Z",
    "published" : "2025-04-16T05:59:29Z",
    "authors" : [
      {
        "name" : "Yue Li"
      },
      {
        "name" : "Lihong Zhang"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.11511v1",
    "title" : "Position Paper: Rethinking Privacy in RL for Sequential Decision-making\n  in the Age of LLMs",
    "summary" : "The rise of reinforcement learning (RL) in critical real-world applications\ndemands a fundamental rethinking of privacy in AI systems. Traditional privacy\nframeworks, designed to protect isolated data points, fall short for sequential\ndecision-making systems where sensitive information emerges from temporal\npatterns, behavioral strategies, and collaborative dynamics. Modern RL\nparadigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in\nlarge language models (LLMs), exacerbate these challenges by introducing\ncomplex, interactive, and context-dependent learning environments that\ntraditional methods do not address. In this position paper, we argue for a new\nprivacy paradigm built on four core principles: multi-scale protection,\nbehavioral pattern protection, collaborative privacy preservation, and\ncontext-aware adaptation. These principles expose inherent tensions between\nprivacy, utility, and interpretability that must be navigated as RL systems\nbecome more pervasive in high-stakes domains like healthcare, autonomous\nvehicles, and decision support systems powered by LLMs. To tackle these\nchallenges, we call for the development of new theoretical frameworks,\npractical mechanisms, and rigorous evaluation methodologies that collectively\nenable effective privacy protection in sequential decision-making systems.",
    "updated" : "2025-04-15T10:45:55Z",
    "published" : "2025-04-15T10:45:55Z",
    "authors" : [
      {
        "name" : "Flint Xiaofeng Fan"
      },
      {
        "name" : "Cheston Tan"
      },
      {
        "name" : "Roger Wattenhofer"
      },
      {
        "name" : "Yew-Soon Ong"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.07414v3",
    "title" : "A Unified Framework and Efficient Computation for Privacy Amplification\n  via Shuffling",
    "summary" : "The shuffle model offers significant privacy amplification over local\ndifferential privacy (LDP), enabling improved privacy-utility trade-offs. To\nanalyze and quantify this amplification effect, two primary frameworks have\nbeen proposed: the \\textit{privacy blanket} (Balle et al., CRYPTO 2019) and the\n\\textit{clone paradigm}, which includes both the \\textit{standard clone} and\n\\textit{stronger clone} (Feldman et al., FOCS 2021; SODA 2023). All of these\napproaches are grounded in decomposing the behavior of local randomizers.\n  In this work, we present a unified perspective--termed the \\textit{general\nclone paradigm}--that captures all decomposition-based analyses. We identify\nthe optimal decomposition within this framework and design a simple yet\nefficient algorithm based on the Fast Fourier Transform (FFT) to compute tight\nprivacy amplification bounds. Empirical results show that our computed upper\nbounds nearly match the corresponding lower bounds, demonstrating the accuracy\nand tightness of our method.\n  Furthermore, we apply our algorithm to derive optimal privacy amplification\nbounds for both joint composition and parallel composition of LDP mechanisms in\nthe shuffle model.",
    "updated" : "2025-04-16T12:16:33Z",
    "published" : "2025-04-10T03:11:17Z",
    "authors" : [
      {
        "name" : "Pengcheng Su"
      },
      {
        "name" : "Haibo Cheng"
      },
      {
        "name" : "Ping Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.12931v1",
    "title" : "Explainable AI in Usable Privacy and Security: Challenges and\n  Opportunities",
    "summary" : "Large Language Models (LLMs) are increasingly being used for automated\nevaluations and explaining them. However, concerns about explanation quality,\nconsistency, and hallucinations remain open research challenges, particularly\nin high-stakes contexts like privacy and security, where user trust and\ndecision-making are at stake. In this paper, we investigate these issues in the\ncontext of PRISMe, an interactive privacy policy assessment tool that leverages\nLLMs to evaluate and explain website privacy policies. Based on a prior user\nstudy with 22 participants, we identify key concerns regarding LLM judgment\ntransparency, consistency, and faithfulness, as well as variations in user\npreferences for explanation detail and engagement. We discuss potential\nstrategies to mitigate these concerns, including structured evaluation\ncriteria, uncertainty estimation, and retrieval-augmented generation (RAG). We\nidentify a need for adaptive explanation strategies tailored to different user\nprofiles for LLM-as-a-judge. Our goal is to showcase the application area of\nusable privacy and security to be promising for Human-Centered Explainable AI\n(HCXAI) to make an impact.",
    "updated" : "2025-04-17T13:28:01Z",
    "published" : "2025-04-17T13:28:01Z",
    "authors" : [
      {
        "name" : "Vincent Freiberger"
      },
      {
        "name" : "Arthur Fleig"
      },
      {
        "name" : "Erik Buchmann"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.12747v1",
    "title" : "Privacy Protection Against Personalized Text-to-Image Synthesis via\n  Cross-image Consistency Constraints",
    "summary" : "The rapid advancement of diffusion models and personalization techniques has\nmade it possible to recreate individual portraits from just a few publicly\navailable images. While such capabilities empower various creative\napplications, they also introduce serious privacy concerns, as adversaries can\nexploit them to generate highly realistic impersonations. To counter these\nthreats, anti-personalization methods have been proposed, which add adversarial\nperturbations to published images to disrupt the training of personalization\nmodels. However, existing approaches largely overlook the intrinsic multi-image\nnature of personalization and instead adopt a naive strategy of applying\nperturbations independently, as commonly done in single-image settings. This\nneglects the opportunity to leverage inter-image relationships for stronger\nprivacy protection. Therefore, we advocate for a group-level perspective on\nprivacy protection against personalization. Specifically, we introduce\nCross-image Anti-Personalization (CAP), a novel framework that enhances\nresistance to personalization by enforcing style consistency across perturbed\nimages. Furthermore, we develop a dynamic ratio adjustment strategy that\nadaptively balances the impact of the consistency loss throughout the attack\niterations. Extensive experiments on the classical CelebHQ and VGGFace2\nbenchmarks show that CAP substantially improves existing methods.",
    "updated" : "2025-04-17T08:39:32Z",
    "published" : "2025-04-17T08:39:32Z",
    "authors" : [
      {
        "name" : "Guanyu Wang"
      },
      {
        "name" : "Kailong Wang"
      },
      {
        "name" : "Yihao Huang"
      },
      {
        "name" : "Mingyi Zhou"
      },
      {
        "name" : "Zhang Qing cnwatcher"
      },
      {
        "name" : "Geguang Pu"
      },
      {
        "name" : "Li Li"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.12681v1",
    "title" : "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in\n  LLMs",
    "summary" : "Large Language Models (LLMs) trained on extensive datasets often learn\nsensitive information, which raises significant social and legal concerns under\nprinciples such as the \"Right to be forgotten.\" Retraining entire models from\nscratch to remove undesired information is both costly and impractical.\nFurthermore, existing single-domain unlearning methods fail to address\nmulti-domain scenarios, where knowledge is interwoven across domains such as\nprivacy and copyright, creating overlapping representations that lead to\nexcessive knowledge removal or degraded performance. To tackle these issues, we\npropose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain\nunlearning framework. GRAIL leverages gradient information from multiple\ndomains to precisely distinguish the unlearning scope from the retention scope,\nand applies an adaptive parameter-wise localization strategy to selectively\nremove targeted knowledge while preserving critical parameters for each domain.\nExperimental results on unlearning benchmarks show that GRAIL achieves\nunlearning success on par with the existing approaches, while also\ndemonstrating up to 17% stronger knowledge retention success compared to the\nprevious state-of-art method. Our findings establish a new paradigm for\neffectively managing and regulating sensitive information in large-scale\npre-trained language models.",
    "updated" : "2025-04-17T06:16:32Z",
    "published" : "2025-04-17T06:16:32Z",
    "authors" : [
      {
        "name" : "Kun-Woo Kim"
      },
      {
        "name" : "Ji-Hoon Park"
      },
      {
        "name" : "Ju-Min Han"
      },
      {
        "name" : "Seong-Whan Lee"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.12623v1",
    "title" : "Privacy-Preserving CNN Training with Transfer Learning: Two Hidden\n  Layers",
    "summary" : "In this paper, we present the demonstration of training a four-layer neural\nnetwork entirely using fully homomorphic encryption (FHE), supporting both\nsingle-output and multi-output classification tasks in a non-interactive\nsetting. A key contribution of our work is identifying that replacing\n\\textit{Softmax} with \\textit{Sigmoid}, in conjunction with the Binary\nCross-Entropy (BCE) loss function, provides an effective and scalable solution\nfor homomorphic classification. Moreover, we show that the BCE loss function,\noriginally designed for multi-output tasks, naturally extends to the\nmulti-class setting, thereby enabling broader applicability. We also highlight\nthe limitations of prior loss functions such as the SLE loss and the one\nproposed in the 2019 CVPR Workshop, both of which suffer from vanishing\ngradients as network depth increases. To address the challenges posed by\nlarge-scale encrypted data, we further introduce an improved version of the\npreviously proposed data encoding scheme, \\textit{Double Volley Revolver},\nwhich achieves a better trade-off between computational and memory efficiency,\nmaking FHE-based neural network training more practical. The complete, runnable\nC++ code to implement our work can be found at:\n\\href{https://github.com/petitioner/ML.NNtraining}{$\\texttt{https://github.com/petitioner/ML.NNtraining}$}.",
    "updated" : "2025-04-17T03:58:23Z",
    "published" : "2025-04-17T03:58:23Z",
    "authors" : [
      {
        "name" : "John Chiang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.12552v1",
    "title" : "Privacy-Preserving Operating Room Workflow Analysis using Digital Twins",
    "summary" : "Purpose: The operating room (OR) is a complex environment where optimizing\nworkflows is critical to reduce costs and improve patient outcomes. The use of\ncomputer vision approaches for the automatic recognition of perioperative\nevents enables identification of bottlenecks for OR optimization. However,\nprivacy concerns limit the use of computer vision for automated event detection\nfrom OR videos, which makes privacy-preserving approaches needed for OR\nworkflow analysis. Methods: We propose a two-stage pipeline for\nprivacy-preserving OR video analysis and event detection. In the first stage,\nwe leverage vision foundation models for depth estimation and semantic\nsegmentation to generate de-identified Digital Twins (DT) of the OR from\nconventional RGB videos. In the second stage, we employ the SafeOR model, a\nfused two-stream approach that processes segmentation masks and depth maps for\nOR event detection. We evaluate this method on an internal dataset of 38\nsimulated surgical trials with five event classes. Results: Our results\nindicate that this DT-based approach to the OR event detection model achieves\nperformance on par and sometimes even better than raw RGB video-based models on\ndetecting OR events. Conclusion: DTs enable privacy-preserving OR workflow\nanalysis, facilitating the sharing of de-identified data across institutions\nand they can potentially enhance model generalizability by mitigating\ndomain-specific appearance differences.",
    "updated" : "2025-04-17T00:46:06Z",
    "published" : "2025-04-17T00:46:06Z",
    "authors" : [
      {
        "name" : "Alejandra Perez"
      },
      {
        "name" : "Han Zhang"
      },
      {
        "name" : "Yu-Chun Ku"
      },
      {
        "name" : "Lalithkumar Seenivasan"
      },
      {
        "name" : "Roger Soberanis"
      },
      {
        "name" : "Jose L. Porras"
      },
      {
        "name" : "Richard Day"
      },
      {
        "name" : "Jeff Jopling"
      },
      {
        "name" : "Peter Najjar"
      },
      {
        "name" : "Mathias Unberath"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.12520v1",
    "title" : "Interpreting Network Differential Privacy",
    "summary" : "How do we interpret the differential privacy (DP) guarantee for network data?\nWe take a deep dive into a popular form of network DP ($\\varepsilon$--edge DP)\nto find that many of its common interpretations are flawed. Drawing on prior\nwork for privacy with correlated data, we interpret DP through the lens of\nadversarial hypothesis testing and demonstrate a gap between the pairs of\nhypotheses actually protected under DP (tests of complete networks) and the\nsorts of hypotheses implied to be protected by common claims (tests of\nindividual edges). We demonstrate some conditions under which this gap can be\nbridged, while leaving some questions open. While some discussion is specific\nto edge DP, we offer selected results in terms of abstract DP definitions and\nprovide discussion of the implications for other forms of network DP.",
    "updated" : "2025-04-16T22:45:07Z",
    "published" : "2025-04-16T22:45:07Z",
    "authors" : [
      {
        "name" : "Jonathan Hehir"
      },
      {
        "name" : "Xiaoyue Niu"
      },
      {
        "name" : "Aleksandra Slavkovic"
      }
    ],
    "categories" : [
      "math.ST",
      "cs.CY",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.11793v2",
    "title" : "Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification",
    "summary" : "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.",
    "updated" : "2025-04-17T06:24:14Z",
    "published" : "2025-04-16T05:59:29Z",
    "authors" : [
      {
        "name" : "Yue Li"
      },
      {
        "name" : "Lihong Zhang"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.13526v1",
    "title" : "Multi-class Item Mining under Local Differential Privacy",
    "summary" : "Item mining, a fundamental task for collecting statistical data from users,\nhas raised increasing privacy concerns. To address these concerns, local\ndifferential privacy (LDP) was proposed as a privacy-preserving technique.\nExisting LDP item mining mechanisms primarily concentrate on global statistics,\ni.e., those from the entire dataset. Nevertheless, they fall short of\nuser-tailored tasks such as personalized recommendations, whereas classwise\nstatistics can improve task accuracy with fine-grained information. Meanwhile,\nthe introduction of class labels brings new challenges. Label perturbation may\nresult in invalid items for aggregation. To this end, we propose frameworks for\nmulti-class item mining, along with two mechanisms: validity perturbation to\nreduce the impact of invalid data, and correlated perturbation to preserve the\nrelationship between labels and items. We also apply these optimized methods to\ntwo multi-class item mining queries: frequency estimation and top-$k$ item\nmining. Through theoretical analysis and extensive experiments, we verify the\neffectiveness and superiority of these methods.",
    "updated" : "2025-04-18T07:37:06Z",
    "published" : "2025-04-18T07:37:06Z",
    "authors" : [
      {
        "name" : "Yulian Mao"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Rong Du"
      },
      {
        "name" : "Qi Wang"
      },
      {
        "name" : "Kai Huang"
      },
      {
        "name" : "Haibo Hu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.13267v1",
    "title" : "Leveraging Functional Encryption and Deep Learning for\n  Privacy-Preserving Traffic Forecasting",
    "summary" : "Over the past few years, traffic congestion has continuously plagued the\nnation's transportation system creating several negative impacts including\nlonger travel times, increased pollution rates, and higher collision risks. To\novercome these challenges, Intelligent Transportation Systems (ITS) aim to\nimprove mobility and vehicular systems, ensuring higher levels of safety by\nutilizing cutting-edge technologies, sophisticated sensing capabilities, and\ninnovative algorithms. Drivers' participatory sensing, current/future location\nreporting, and machine learning algorithms have considerably improved real-time\ncongestion monitoring and future traffic management. However, each driver's\nsensitive spatiotemporal location information can create serious privacy\nconcerns. To address these challenges, we propose in this paper a secure,\nprivacy-preserving location reporting and traffic forecasting system that\nguarantees privacy protection of driver data while maintaining high traffic\nforecasting accuracy. Our novel k-anonymity scheme utilizes functional\nencryption to aggregate encrypted location information submitted by drivers\nwhile ensuring the privacy of driver location data. Additionally, using the\naggregated encrypted location information as input, this research proposes a\ndeep learning model that incorporates a Convolutional-Long Short-Term Memory\n(Conv-LSTM) module to capture spatial and short-term temporal features and a\nBidirectional Long Short-Term Memory (Bi-LSTM) module to recover long-term\nperiodic patterns for traffic forecasting. With extensive evaluation on real\ndatasets, we demonstrate the effectiveness of the proposed scheme with less\nthan 10% mean absolute error for a 60-minute forecasting horizon, all while\nprotecting driver privacy.",
    "updated" : "2025-04-17T18:21:55Z",
    "published" : "2025-04-17T18:21:55Z",
    "authors" : [
      {
        "name" : "Isaac Adom"
      },
      {
        "name" : "Mohammmad Iqbal Hossain"
      },
      {
        "name" : "Hassan Mahmoud"
      },
      {
        "name" : "Ahmad Alsharif"
      },
      {
        "name" : "Mahmoud Nabil Mahmoud"
      },
      {
        "name" : "Yang Xiao"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.15233v1",
    "title" : "A Review on Privacy in DAG-Based DLTs",
    "summary" : "Directed Acyclic Graph (DAG)-based Distributed Ledger Technologies (DLTs)\nhave emerged as a promising solution to the scalability issues inherent in\ntraditional blockchains. However, amidst the focus on scalability, the crucial\naspect of privacy within DAG-based DLTs has been largely overlooked. This paper\nseeks to address this gap by providing a comprehensive examination of privacy\nnotions and challenges within DAG-based DLTs. We delve into potential\nmethodologies to enhance privacy within these systems, while also analyzing the\nassociated hurdles and real-world implementations within state-of-the-art\nDAG-based DLTs. By exploring these methodologies, we not only illuminate the\ncurrent landscape of privacy in DAG-based DLTs but also outline future research\ndirections in this evolving field.",
    "updated" : "2025-04-21T17:08:45Z",
    "published" : "2025-04-21T17:08:45Z",
    "authors" : [
      {
        "name" : "Mayank Raikwar"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.15090v1",
    "title" : "Federated Latent Factor Model for Bias-Aware Recommendation with\n  Privacy-Preserving",
    "summary" : "A recommender system (RS) aims to provide users with personalized item\nrecommendations, enhancing their overall experience. Traditional RSs collect\nand process all user data on a central server. However, this centralized\napproach raises significant privacy concerns, as it increases the risk of data\nbreaches and privacy leakages, which are becoming increasingly unacceptable to\nprivacy-sensitive users. To address these privacy challenges, federated\nlearning has been integrated into RSs, ensuring that user data remains secure.\nIn centralized RSs, the issue of rating bias is effectively addressed by\njointly analyzing all users' raw interaction data. However, this becomes a\nsignificant challenge in federated RSs, as raw data is no longer accessible due\nto privacy-preserving constraints. To overcome this problem, we propose a\nFederated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is\nexplicitly incorporated into every local model's loss function, allowing for\nthe effective elimination of rating bias without compromising data privacy.\nExtensive experiments conducted on three real-world datasets demonstrate that\nFBALF achieves significantly higher recommendation accuracy compared to other\nstate-of-the-art federated RSs.",
    "updated" : "2025-04-21T13:24:30Z",
    "published" : "2025-04-21T13:24:30Z",
    "authors" : [
      {
        "name" : "Junxiang Gao"
      },
      {
        "name" : "Yixin Ran"
      },
      {
        "name" : "Jia Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.14993v1",
    "title" : "Dual Utilization of Perturbation for Stream Data Publication under Local\n  Differential Privacy",
    "summary" : "Stream data from real-time distributed systems such as IoT, tele-health, and\ncrowdsourcing has become an important data source. However, the collection and\nanalysis of user-generated stream data raise privacy concerns due to the\npotential exposure of sensitive information. To address these concerns, local\ndifferential privacy (LDP) has emerged as a promising standard. Nevertheless,\napplying LDP to stream data presents significant challenges, as stream data\noften involves a large or even infinite number of values. Allocating a given\nprivacy budget across these data points would introduce overwhelming LDP noise\nto the original stream data.\n  Beyond existing approaches that merely use perturbed values for estimating\nstatistics, our design leverages them for both perturbation and estimation.\nThis dual utilization arises from a key observation: each user knows their own\nground truth and perturbed values, enabling a precise computation of the\ndeviation error caused by perturbation. By incorporating this deviation into\nthe perturbation process of subsequent values, the previous noise can be\ncalibrated. Following this insight, we introduce the Iterative Perturbation\nParameterization (IPP) method, which utilizes current perturbed results to\ncalibrate the subsequent perturbation process. To enhance the robustness of\ncalibration and reduce sensitivity, two algorithms, namely Accumulated\nPerturbation Parameterization (APP) and Clipped Accumulated Perturbation\nParameterization (CAPP) are further developed. We prove that these three\nalgorithms satisfy $w$-event differential privacy while significantly improving\nutility. Experimental results demonstrate that our techniques outperform\nstate-of-the-art LDP stream publishing solutions in terms of utility, while\nretaining the same privacy guarantee.",
    "updated" : "2025-04-21T09:51:18Z",
    "published" : "2025-04-21T09:51:18Z",
    "authors" : [
      {
        "name" : "Rong Du"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Yaxin Xiao"
      },
      {
        "name" : "Liantong Yu"
      },
      {
        "name" : "Yue Fu"
      },
      {
        "name" : "Haibo Hu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.14959v1",
    "title" : "ScaleGuard: Rational and Scalable Configuration Privacy Protection with\n  Topology Expansion",
    "summary" : "As networks grow in size and complexity, safeguarding sensitive data while\nsharing configuration files is critical for network management and research.\nExisting anonymization tools primarily hide fields like IP addresses or AS\nnumbers to mitigate direct data exposure. However, they often lack mechanisms\nto preserve privacy around network scale, an increasingly sensitive aspect that\ncan reveal organizational size or resource distribution. We propose ScaleGuard,\nwhich preserves network functional equivalence while adding fake routers and\nhosts to conceal network scale, and generating complete router configurations\nthat resemble the originals. Our system introduces a graph embedding-based\nexpansion method and k-degree mapping anonymity, reducing unnecessary topology\nmodifications when adversaries only know the original degree sequence. For\nrouting repair, ScaleGuard designs a network repair framework combining SMT and\niterative methods, delivering stable performance under randomized link costs\nand complex cross-protocol routing. Experiment results show that ScaleGuard\nexpands network scale effectively, providing consistent anonymization of\ntopology, scale, and routing, while achieving strong topological rationality,\nconfiguration fidelity, and repairing efficiency.",
    "updated" : "2025-04-21T08:38:52Z",
    "published" : "2025-04-21T08:38:52Z",
    "authors" : [
      {
        "name" : "Qianye Wang"
      },
      {
        "name" : "Yuejie Wang"
      },
      {
        "name" : "Yongting Chen"
      },
      {
        "name" : "Guyue Liu"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.14780v1",
    "title" : "Delay-Angle Information Spoofing for Channel State Information-Free\n  Location-Privacy Enhancement",
    "summary" : "In this paper, a delay-angle information spoofing (DAIS) strategy is proposed\nto enhance the location privacy at the physical layer. More precisely, the\nlocation-relevant delays and angles are artificially shifted without the aid of\nchannel state information (CSI) at the transmitter, such that the location\nperceived by the eavesdropper is incorrect and distinct from the true one. By\nleveraging the intrinsic structure of the wireless channel, a precoder is\ndesigned to achieve DAIS while the legitimate localizer can remove the\nobfuscation via securely receiving a modest amount of information, i.e., the\ndelay-angle shifts. A lower bound on eavesdropper's localization error is\nderived, revealing that location privacy is enhanced not only due to estimation\nerror, but also by the geometric mismatch introduced by DAIS. Furthermore, the\nlower bound is explicitly expressed as a function of the delay-angle shifts,\ncharacterizing performance trends and providing the appropriate design of these\nshift parameters. The statistical hardness of maliciously inferring the\ndelay-angle shifts by a single-antenna eavesdropper as well as the challenges\nfor a multi-antenna eavesdropper are investigated to assess the robustness of\nthe proposed DAIS strategy. Numerical results show that the proposed DAIS\nstrategy results in more than 15 dB performance degradation for the\neavesdropper as compared with that for the legitimate localizer at high\nsignal-to-noise ratios, and provides more effective location-privacy\nenhancement than the prior art.",
    "updated" : "2025-04-21T00:40:45Z",
    "published" : "2025-04-21T00:40:45Z",
    "authors" : [
      {
        "name" : "Jianxiu Li"
      },
      {
        "name" : "Urbashi Mitra"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.14730v1",
    "title" : "Optimal Additive Noise Mechanisms for Differential Privacy",
    "summary" : "We propose a unified optimization framework for designing continuous and\ndiscrete noise distributions that ensure differential privacy (DP) by\nminimizing R\\'enyi DP, a variant of DP, under a cost constraint. R\\'enyi DP has\nthe advantage that by considering different values of the R\\'enyi parameter\n$\\alpha$, we can tailor our optimization for any number of compositions. To\nsolve the optimization problem, we reduce it to a finite-dimensional convex\nformulation and perform preconditioned gradient descent. The resulting noise\ndistributions are then compared to their Gaussian and Laplace counterparts.\nNumerical results demonstrate that our optimized distributions are consistently\nbetter, with significant improvements in $(\\varepsilon, \\delta)$-DP guarantees\nin the moderate composition regimes, compared to Gaussian and Laplace\ndistributions with the same variance.",
    "updated" : "2025-04-20T20:04:41Z",
    "published" : "2025-04-20T20:04:41Z",
    "authors" : [
      {
        "name" : "Atefeh Gilani"
      },
      {
        "name" : "Juan Felipe Gomez"
      },
      {
        "name" : "Shahab Asoodeh"
      },
      {
        "name" : "Flavio P. Calmon"
      },
      {
        "name" : "Oliver Kosut"
      },
      {
        "name" : "Lalitha Sankar"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.14368v1",
    "title" : "Do You Really Need Public Data? Surrogate Public Data for Differential\n  Privacy on Tabular Data",
    "summary" : "Differentially private (DP) machine learning often relies on the availability\nof public data for tasks like privacy-utility trade-off estimation,\nhyperparameter tuning, and pretraining. While public data assumptions may be\nreasonable in text and image domains, they are less likely to hold for tabular\ndata due to tabular data heterogeneity across domains. We propose leveraging\npowerful priors to address this limitation; specifically, we synthesize\nrealistic tabular data directly from schema-level specifications - such as\nvariable names, types, and permissible ranges - without ever accessing\nsensitive records. To that end, this work introduces the notion of \"surrogate\"\npublic data - datasets generated independently of sensitive data, which consume\nno privacy loss budget and are constructed solely from publicly available\nschema or metadata. Surrogate public data are intended to encode plausible\nstatistical assumptions (informed by publicly available information) into a\ndataset with many downstream uses in private mechanisms. We automate the\nprocess of generating surrogate public data with large language models (LLMs);\nin particular, we propose two methods: direct record generation as CSV files,\nand automated structural causal model (SCM) construction for sampling records.\nThrough extensive experiments, we demonstrate that surrogate public tabular\ndata can effectively replace traditional public data when pretraining\ndifferentially private tabular classifiers. To a lesser extent, surrogate\npublic data are also useful for hyperparameter tuning of DP synthetic data\ngenerators, and for estimating the privacy-utility tradeoff.",
    "updated" : "2025-04-19T17:55:10Z",
    "published" : "2025-04-19T17:55:10Z",
    "authors" : [
      {
        "name" : "Shlomi Hod"
      },
      {
        "name" : "Lucas Rosenblatt"
      },
      {
        "name" : "Julia Stoyanovich"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.14301v1",
    "title" : "Balancing Privacy and Action Performance: A Penalty-Driven Approach to\n  Image Anonymization",
    "summary" : "The rapid development of video surveillance systems for object detection,\ntracking, activity recognition, and anomaly detection has revolutionized our\nday-to-day lives while setting alarms for privacy concerns. It isn't easy to\nstrike a balance between visual privacy and action recognition performance in\nmost computer vision models. Is it possible to safeguard privacy without\nsacrificing performance? It poses a formidable challenge, as even minor privacy\nenhancements can lead to substantial performance degradation. To address this\nchallenge, we propose a privacy-preserving image anonymization technique that\noptimizes the anonymizer using penalties from the utility branch, ensuring\nimproved action recognition performance while minimally affecting privacy\nleakage. This approach addresses the trade-off between minimizing privacy\nleakage and maintaining high action performance. The proposed approach is\nprimarily designed to align with the regulatory standards of the EU AI Act and\nGDPR, ensuring the protection of personally identifiable information while\nmaintaining action performance. To the best of our knowledge, we are the first\nto introduce a feature-based penalty scheme that exclusively controls the\naction features, allowing freedom to anonymize private attributes. Extensive\nexperiments were conducted to validate the effectiveness of the proposed\nmethod. The results demonstrate that applying a penalty to anonymizer from\nutility branch enhances action performance while maintaining nearly consistent\nprivacy leakage across different penalty settings.",
    "updated" : "2025-04-19T13:52:33Z",
    "published" : "2025-04-19T13:52:33Z",
    "authors" : [
      {
        "name" : "Nazia Aslam"
      },
      {
        "name" : "Kamal Nasrollahi"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.14208v1",
    "title" : "FedCIA: Federated Collaborative Information Aggregation for\n  Privacy-Preserving Recommendation",
    "summary" : "Recommendation algorithms rely on user historical interactions to deliver\npersonalized suggestions, which raises significant privacy concerns. Federated\nrecommendation algorithms tackle this issue by combining local model training\nwith server-side model aggregation, where most existing algorithms use a\nuniform weighted summation to aggregate item embeddings from different client\nmodels. This approach has three major limitations: 1) information loss during\naggregation, 2) failure to retain personalized local features, and 3)\nincompatibility with parameter-free recommendation algorithms. To address these\nlimitations, we first review the development of recommendation algorithms and\nrecognize that their core function is to share collaborative information,\nspecifically the global relationship between users and items. With this\nunderstanding, we propose a novel aggregation paradigm named collaborative\ninformation aggregation, which focuses on sharing collaborative information\nrather than item parameters. Based on this new paradigm, we introduce the\nfederated collaborative information aggregation (FedCIA) method for\nprivacy-preserving recommendation. This method requires each client to upload\nitem similarity matrices for aggregation, which allows clients to align their\nlocal models without constraining embeddings to a unified vector space. As a\nresult, it mitigates information loss caused by direct summation, preserves the\npersonalized embedding distributions of individual clients, and supports the\naggregation of parameter-free models. Theoretical analysis and experimental\nresults on real-world datasets demonstrate the superior performance of FedCIA\ncompared with the state-of-the-art federated recommendation algorithms. Code is\navailable at https://github.com/Mingzhe-Han/FedCIA.",
    "updated" : "2025-04-19T06:59:34Z",
    "published" : "2025-04-19T06:59:34Z",
    "authors" : [
      {
        "name" : "Mingzhe Han"
      },
      {
        "name" : "Dongsheng Li"
      },
      {
        "name" : "Jiafeng Xia"
      },
      {
        "name" : "Jiahao Liu"
      },
      {
        "name" : "Hansu Gu"
      },
      {
        "name" : "Peng Zhang"
      },
      {
        "name" : "Ning Gu"
      },
      {
        "name" : "Tun Lu"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.11793v3",
    "title" : "Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification",
    "summary" : "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.",
    "updated" : "2025-04-18T20:58:03Z",
    "published" : "2025-04-16T05:59:29Z",
    "authors" : [
      {
        "name" : "Yue Li"
      },
      {
        "name" : "Lihong Zhang"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.16000v1",
    "title" : "How Private is Your Attention? Bridging Privacy with In-Context Learning",
    "summary" : "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings.",
    "updated" : "2025-04-22T16:05:26Z",
    "published" : "2025-04-22T16:05:26Z",
    "authors" : [
      {
        "name" : "Soham Bonnerjee"
      },
      {
        "name" : "Zhen Wei"
      },
      {
        "name" : " Yeon"
      },
      {
        "name" : "Anna Asch"
      },
      {
        "name" : "Sagnik Nandy"
      },
      {
        "name" : "Promit Ghosal"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.15995v1",
    "title" : "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical\n  Federated Learning",
    "summary" : "Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL.",
    "updated" : "2025-04-22T16:00:11Z",
    "published" : "2025-04-22T16:00:11Z",
    "authors" : [
      {
        "name" : "Sindhuja Madabushi"
      },
      {
        "name" : "Ahmad Faraz Khan"
      },
      {
        "name" : "Haider Ali"
      },
      {
        "name" : "Jin-Hee Cho"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.15580v1",
    "title" : "On the Price of Differential Privacy for Hierarchical Clustering",
    "summary" : "Hierarchical clustering is a fundamental unsupervised machine learning task\nwith the aim of organizing data into a hierarchy of clusters. Many applications\nof hierarchical clustering involve sensitive user information, therefore\nmotivating recent studies on differentially private hierarchical clustering\nunder the rigorous framework of Dasgupta's objective. However, it has been\nshown that any privacy-preserving algorithm under edge-level differential\nprivacy necessarily suffers a large error. To capture practical applications of\nthis problem, we focus on the weight privacy model, where each edge of the\ninput graph is at least unit weight. We present a novel algorithm in the weight\nprivacy model that shows significantly better approximation than known\nimpossibility results in the edge-level DP setting. In particular, our\nalgorithm achieves $O(\\log^{1.5}n/\\varepsilon)$ multiplicative error for\n$\\varepsilon$-DP and runs in polynomial time, where $n$ is the size of the\ninput graph, and the cost is never worse than the optimal additive error in\nexisting work. We complement our algorithm by showing if the unit-weight\nconstraint does not apply, the lower bound for weight-level DP hierarchical\nclustering is essentially the same as the edge-level DP, i.e.\n$\\Omega(n^2/\\varepsilon)$ additive error. As a result, we also obtain a new\nlower bound of $\\tilde{\\Omega}(1/\\varepsilon)$ additive error for balanced\nsparsest cuts in the weight-level DP model, which may be of independent\ninterest. Finally, we evaluate our algorithm on synthetic and real-world\ndatasets. Our experimental results show that our algorithm performs well in\nterms of extra cost and has good scalability to large graphs.",
    "updated" : "2025-04-22T04:39:40Z",
    "published" : "2025-04-22T04:39:40Z",
    "authors" : [
      {
        "name" : "Chengyuan Deng"
      },
      {
        "name" : "Jie Gao"
      },
      {
        "name" : "Jalaj Upadhyay"
      },
      {
        "name" : "Chen Wang"
      },
      {
        "name" : "Samson Zhou"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.15525v1",
    "title" : "Federated Latent Factor Learning for Recovering Wireless Sensor Networks\n  Signal with Privacy-Preserving",
    "summary" : "Wireless Sensor Networks (WSNs) are a cutting-edge domain in the field of\nintelligent sensing. Due to sensor failures and energy-saving strategies, the\ncollected data often have massive missing data, hindering subsequent analysis\nand decision-making. Although Latent Factor Learning (LFL) has been proven\neffective in recovering missing data, it fails to sufficiently consider data\nprivacy protection. To address this issue, this paper innovatively proposes a\nfederated latent factor learning (FLFL) based spatial signal recovery (SSR)\nmodel, named FLFL-SSR. Its main idea is two-fold: 1) it designs a sensor-level\nfederated learning framework, where each sensor uploads only gradient updates\ninstead of raw data to optimize the global model, and 2) it proposes a local\nspatial sharing strategy, allowing sensors within the same spatial region to\nshare their latent feature vectors, capturing spatial correlations and\nenhancing recovery accuracy. Experimental results on two real-world WSNs\ndatasets demonstrate that the proposed model outperforms existing federated\nmethods in terms of recovery performance.",
    "updated" : "2025-04-22T02:01:19Z",
    "published" : "2025-04-22T02:01:19Z",
    "authors" : [
      {
        "name" : "Chengjun Yu"
      },
      {
        "name" : "Yixin Ran"
      },
      {
        "name" : "Yangyi Xia"
      },
      {
        "name" : "Jia Wu"
      },
      {
        "name" : "Xiaojing Liu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.16683v1",
    "title" : "MCMC for Bayesian estimation of Differential Privacy from Membership\n  Inference Attacks",
    "summary" : "We propose a new framework for Bayesian estimation of differential privacy,\nincorporating evidence from multiple membership inference attacks (MIA).\nBayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)\nalgorithm, named MCMC-DP-Est, which provides an estimate of the full posterior\ndistribution of the privacy parameter (e.g., instead of just credible\nintervals). Critically, the proposed method does not assume that privacy\nauditing is performed with the most powerful attack on the worst-case (dataset,\nchallenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est\njointly estimates the strengths of MIAs used and the privacy of the training\nalgorithm, yielding a more cautious privacy analysis. We also present an\neconomical way to generate measurements for the performance of an MIA that is\nto be used by the MCMC method to estimate privacy. We present the use of the\nmethods with numerical examples with both artificial and real data.",
    "updated" : "2025-04-23T13:10:37Z",
    "published" : "2025-04-23T13:10:37Z",
    "authors" : [
      {
        "name" : "Ceren Yildirim"
      },
      {
        "name" : "Kamer Kaya"
      },
      {
        "name" : "Sinan Yildirim"
      },
      {
        "name" : "Erkay Savas"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.16557v1",
    "title" : "Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D\n  Vision Tasks",
    "summary" : "We introduce ROAR (Robust Object Removal and Re-annotation), a scalable\nframework for privacy-preserving dataset obfuscation that eliminates sensitive\nobjects instead of modifying them. Our method integrates instance segmentation\nwith generative inpainting to remove identifiable entities while preserving\nscene integrity. Extensive evaluations on 2D COCO-based object detection show\nthat ROAR achieves 87.5% of the baseline detection average precision (AP),\nwhereas image dropping achieves only 74.2% of the baseline AP, highlighting the\nadvantage of scrubbing in preserving dataset utility. The degradation is even\nmore severe for small objects due to occlusion and loss of fine-grained\ndetails. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR\nloss of at most 1.66 dB while maintaining SSIM and improving LPIPS,\ndemonstrating superior perceptual quality. Our findings establish object\nremoval as an effective privacy framework, achieving strong privacy guarantees\nwith minimal performance trade-offs. The results highlight key challenges in\ngenerative inpainting, occlusion-robust segmentation, and task-specific\nscrubbing, setting the foundation for future advancements in privacy-preserving\nvision systems.",
    "updated" : "2025-04-23T09:33:10Z",
    "published" : "2025-04-23T09:33:10Z",
    "authors" : [
      {
        "name" : "Murat Bilgehan Ertan"
      },
      {
        "name" : "Ronak Sahu"
      },
      {
        "name" : "Phuong Ha Nguyen"
      },
      {
        "name" : "Kaleel Mahmood"
      },
      {
        "name" : "Marten van Dijk"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.16535v1",
    "title" : "Decentralized Quantile Regression for Feature-Distributed Massive\n  Datasets with Privacy Guarantees",
    "summary" : "In this paper, we introduce a novel decentralized surrogate gradient-based\nalgorithm for quantile regression in a feature-distributed setting, where\nglobal features are dispersed across multiple machines within a decentralized\nnetwork. The proposed algorithm, \\texttt{DSG-cqr}, utilizes a convolution-type\nsmoothing approach to address the non-smooth nature of the quantile loss\nfunction. \\texttt{DSG-cqr} is fully decentralized, conjugate-free, easy to\nimplement, and achieves linear convergence up to statistical precision. To\nensure privacy, we adopt the Gaussian mechanism to provide\n$(\\epsilon,\\delta)$-differential privacy. To overcome the exact residual\ncalculation problem, we estimate residuals using auxiliary variables and\ndevelop a confidence interval construction method based on Wald statistics.\nTheoretical properties are established, and the practical utility of the\nmethods is also demonstrated through extensive simulations and a real-world\ndata application.",
    "updated" : "2025-04-23T09:04:21Z",
    "published" : "2025-04-23T09:04:21Z",
    "authors" : [
      {
        "name" : "Peiwen Xiao"
      },
      {
        "name" : "Xiaohui Liu"
      },
      {
        "name" : "Guangming Pan"
      },
      {
        "name" : "Wei Long"
      }
    ],
    "categories" : [
      "stat.CO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.16371v1",
    "title" : "The Safety-Privacy Tradeoff in Linear Bandits",
    "summary" : "We consider a collection of linear stochastic bandit problems, each modeling\nthe random response of different agents to proposed interventions, coupled\ntogether by a global safety constraint. We assume a central coordinator must\nchoose actions to play on each bandit with the objective of regret\nminimization, while also ensuring that the expected response of all agents\nsatisfies the global safety constraints at each round, in spite of uncertainty\nabout the bandits' parameters. The agents consider their observed responses to\nbe private and in order to protect their sensitive information, the data\nsharing with the central coordinator is performed under local differential\nprivacy (LDP). However, providing higher level of privacy to different agents\nwould have consequences in terms of safety and regret. We formalize these\ntradeoffs by building on the notion of the sharpness of the safety set - a\nmeasure of how the geometric properties of the safe set affects the growth of\nregret - and propose a unilaterally unimprovable vector of privacy levels for\ndifferent agents given a maximum regret budget.",
    "updated" : "2025-04-23T02:48:02Z",
    "published" : "2025-04-23T02:48:02Z",
    "authors" : [
      {
        "name" : "Arghavan Zibaie"
      },
      {
        "name" : "Spencer Hutchinson"
      },
      {
        "name" : "Ramtin Pedarsani"
      },
      {
        "name" : "Mahnoosh Alizadeh"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.12129v2",
    "title" : "Anti-Aesthetics: Protecting Facial Privacy against Customized\n  Text-to-Image Synthesis",
    "summary" : "The rise of customized diffusion models has spurred a boom in personalized\nvisual content creation, but also poses risks of malicious misuse, severely\nthreatening personal privacy and copyright protection. Some studies show that\nthe aesthetic properties of images are highly positively correlated with human\nperception of image quality. Inspired by this, we approach the problem from a\nnovel and intriguing aesthetic perspective to degrade the generation quality of\nmaliciously customized models, thereby achieving better protection of facial\nidentity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA)\nframework to fully explore aesthetic cues, which consists of two key branches:\n1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward\nmechanism and a global anti-aesthetic loss, it can degrade the overall\naesthetics of the generated content; 2) Local Anti-Aesthetics: A local\nanti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to\nguide adversarial perturbations to disrupt local facial identity. By seamlessly\nintegrating both branches, our HAA effectively achieves the goal of\nanti-aesthetics from a global to a local level during customized generation.\nExtensive experiments show that HAA outperforms existing SOTA methods largely\nin identity removal, providing a powerful tool for protecting facial privacy\nand copyright.",
    "updated" : "2025-04-23T13:23:07Z",
    "published" : "2025-04-16T14:44:00Z",
    "authors" : [
      {
        "name" : "Songping Wang"
      },
      {
        "name" : "Yueming Lyu"
      },
      {
        "name" : "Shiqi Liu"
      },
      {
        "name" : "Ning Li"
      },
      {
        "name" : "Tong Tong"
      },
      {
        "name" : "Hao Sun"
      },
      {
        "name" : "Caifeng Shan"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.17703v1",
    "title" : "Federated Learning: A Survey on Privacy-Preserving Collaborative\n  Intelligence",
    "summary" : "Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems.",
    "updated" : "2025-04-24T16:10:29Z",
    "published" : "2025-04-24T16:10:29Z",
    "authors" : [
      {
        "name" : "Edward Collins"
      },
      {
        "name" : "Michel Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.17523v1",
    "title" : "From Randomized Response to Randomized Index: Answering Subset Counting\n  Queries with Local Differential Privacy",
    "summary" : "Local Differential Privacy (LDP) is the predominant privacy model for\nsafeguarding individual data privacy. Existing perturbation mechanisms\ntypically require perturbing the original values to ensure acceptable privacy,\nwhich inevitably results in value distortion and utility deterioration. In this\nwork, we propose an alternative approach -- instead of perturbing values, we\napply randomization to indexes of values while ensuring rigorous LDP\nguarantees. Inspired by the deniability of randomized indexes, we present CRIAD\nfor answering subset counting queries on set-value data. By integrating a\nmulti-dummy, multi-sample, and multi-group strategy, CRIAD serves as a fully\nscalable solution that offers flexibility across various privacy requirements\nand domain sizes, and achieves more accurate query results than any existing\nmethods. Through comprehensive theoretical analysis and extensive experimental\nevaluations, we validate the effectiveness of CRIAD and demonstrate its\nsuperiority over traditional value-perturbation mechanisms.",
    "updated" : "2025-04-24T13:08:11Z",
    "published" : "2025-04-24T13:08:11Z",
    "authors" : [
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Liantong Yu"
      },
      {
        "name" : "Kai Huang"
      },
      {
        "name" : "Xiaokui Xiao"
      },
      {
        "name" : "Weiran Liu"
      },
      {
        "name" : "Haibo Hu"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.17360v1",
    "title" : "PatientDx: Merging Large Language Models for Protecting Data-Privacy in\n  Healthcare",
    "summary" : "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4.",
    "updated" : "2025-04-24T08:21:04Z",
    "published" : "2025-04-24T08:21:04Z",
    "authors" : [
      {
        "name" : "Jose G. Moreno"
      },
      {
        "name" : "Jesus Lovon"
      },
      {
        "name" : "M'Rick Robin-Charlet"
      },
      {
        "name" : "Christine Damase-Michel"
      },
      {
        "name" : "Lynda Tamine"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.17274v1",
    "title" : "Signal Recovery from Random Dot-Product Graphs Under Local Differential\n  Privacy",
    "summary" : "We consider the problem of recovering latent information from graphs under\n$\\varepsilon$-edge local differential privacy where the presence of\nrelationships/edges between two users/vertices remains confidential, even from\nthe data curator. For the class of generalized random dot-product graphs, we\nshow that a standard local differential privacy mechanism induces a specific\ngeometric distortion in the latent positions. Leveraging this insight, we show\nthat consistent recovery of the latent positions is achievable by appropriately\nadjusting the statistical inference procedure for the privatized graph.\nFurthermore, we prove that our procedure is nearly minimax-optimal under local\nedge differential privacy constraints. Lastly, we show that this framework\nallows for consistent recovery of geometric and topological information\nunderlying the latent positions, as encoded in their persistence diagrams. Our\nresults extend previous work from the private community detection literature to\na substantially richer class of models and inferential tasks.",
    "updated" : "2025-04-24T06:02:02Z",
    "published" : "2025-04-24T06:02:02Z",
    "authors" : [
      {
        "name" : "Siddharth Vishwanath"
      },
      {
        "name" : "Jonathan Hehir"
      }
    ],
    "categories" : [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "68P27, 62H22, 62C20, 62R07"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.16944v1",
    "title" : "Burning some myths on privacy properties of social networks against\n  active attacks",
    "summary" : "This work focuses on showing some arguments addressed to dismantle the\nextended idea about that social networks completely lacks of privacy\nproperties. We consider the so-called active attacks to the privacy of social\nnetworks and the counterpart $(k,\\ell)$-anonymity measure, which is used to\nquantify the privacy satisfied by a social network against active attacks. To\nthis end, we make use of the graph theoretical concept of $k$-metric\nantidimensional graphs for which the case $k=1$ represents those graphs\nachieving the worst scenario in privacy whilst considering the\n$(k,\\ell)$-anonymity measure.\n  As a product of our investigation, we present a large number of computational\nresults stating that social networks might not be as insecure as one often\nthinks. In particular, we develop a large number of experiments on random\ngraphs which show that the number of $1$-metric antidimensional graphs is\nindeed ridiculously small with respect to the total number of graphs that can\nbe considered. Moreover, we search on several real networks in order to check\nif they are $1$-metric antidimensional, and obtain that none of them are such.\nAlong the way, we show some theoretical studies on the mathematical properties\nof the $k$-metric antidimensional graphs for any suitable $k\\ge 1$. In\naddition, we also describe some operations on graphs that are $1$-metric\nantidimensional so that they get embedded into another larger graphs that are\nnot such, in order to obscure their privacy properties against active attacks.",
    "updated" : "2025-04-17T06:03:56Z",
    "published" : "2025-04-17T06:03:56Z",
    "authors" : [
      {
        "name" : "Serafino Cicerone"
      },
      {
        "name" : "Gabriele Di Stefano"
      },
      {
        "name" : "Sandi Klavžar"
      },
      {
        "name" : "Ismael G. Yero"
      }
    ],
    "categories" : [
      "cs.SI",
      "math.CO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.18411v1",
    "title" : "Heavy-Tailed Privacy: The Symmetric alpha-Stable Privacy Mechanism",
    "summary" : "With the rapid growth of digital platforms, there is increasing apprehension\nabout how personal data is collected, stored, and used by various entities.\nThese concerns arise from the increasing frequency of data breaches,\ncyber-attacks, and misuse of personal information for targeted advertising and\nsurveillance. To address these matters, Differential Privacy (DP) has emerged\nas a prominent tool for quantifying a digital system's level of protection. The\nGaussian mechanism is commonly used because the Gaussian density is closed\nunder convolution, and is a common method utilized when aggregating datasets.\nHowever, the Gaussian mechanism only satisfies an approximate form of\nDifferential Privacy. In this work, we present and analyze of the Symmetric\nalpha-Stable (SaS) mechanism. We prove that the mechanism achieves pure\ndifferential privacy while remaining closed under convolution. Additionally, we\nstudy the nuanced relationship between the level of privacy achieved and the\nparameters of the density. Lastly, we compare the expected error introduced to\ndataset queries by the Gaussian and SaS mechanisms. From our analysis, we\nbelieve the SaS Mechanism is an appealing choice for privacy-focused\napplications.",
    "updated" : "2025-04-25T15:14:02Z",
    "published" : "2025-04-25T15:14:02Z",
    "authors" : [
      {
        "name" : "Christopher C. Zawacki"
      },
      {
        "name" : "Eyad H. Abed"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.18078v1",
    "title" : "Privacy-Preserving Personalized Federated Learning for Distributed\n  Photovoltaic Disaggregation under Statistical Heterogeneity",
    "summary" : "The rapid expansion of distributed photovoltaic (PV) installations worldwide,\nmany being behind-the-meter systems, has significantly challenged energy\nmanagement and grid operations, as unobservable PV generation further\ncomplicates the supply-demand balance. Therefore, estimating this generation\nfrom net load, known as PV disaggregation, is critical. Given privacy concerns\nand the need for large training datasets, federated learning becomes a\npromising approach, but statistical heterogeneity, arising from geographical\nand behavioral variations among prosumers, poses new challenges to PV\ndisaggregation. To overcome these challenges, a privacy-preserving distributed\nPV disaggregation framework is proposed using Personalized Federated Learning\n(PFL). The proposed method employs a two-level framework that combines local\nand global modeling. At the local level, a transformer-based PV disaggregation\nmodel is designed to generate solar irradiance embeddings for representing\nlocal PV conditions. A novel adaptive local aggregation mechanism is adopted to\nmitigate the impact of statistical heterogeneity on the local model, extracting\na portion of global information that benefits the local model. At the global\nlevel, a central server aggregates information uploaded from multiple data\ncenters, preserving privacy while enabling cross-center knowledge sharing.\nExperiments on real-world data demonstrate the effectiveness of this proposed\nframework, showing improved accuracy and robustness compared to benchmark\nmethods.",
    "updated" : "2025-04-25T05:09:27Z",
    "published" : "2025-04-25T05:09:27Z",
    "authors" : [
      {
        "name" : "Xiaolu Chen"
      },
      {
        "name" : "Chenghao Huang"
      },
      {
        "name" : "Yanru Zhang"
      },
      {
        "name" : "Hao Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.18032v1",
    "title" : "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in\n  Diffusion Models",
    "summary" : "Text-to-image diffusion models have demonstrated remarkable capabilities in\ncreating images highly aligned with user prompts, yet their proclivity for\nmemorizing training set images has sparked concerns about the originality of\nthe generated images and privacy issues, potentially leading to legal\ncomplications for both model owners and users, particularly when the memorized\nimages contain proprietary content. Although methods to mitigate these issues\nhave been suggested, enhancing privacy often results in a significant decrease\nin the utility of the outputs, as indicated by text-alignment scores. To bridge\nthe research gap, we introduce a novel method, PRSS, which refines the\nclassifier-free guidance approach in diffusion models by integrating prompt\nre-anchoring (PR) to improve privacy and incorporating semantic prompt search\n(SS) to enhance utility. Extensive experiments across various privacy levels\ndemonstrate that our approach consistently improves the privacy-utility\ntrade-off, establishing a new state-of-the-art.",
    "updated" : "2025-04-25T02:51:23Z",
    "published" : "2025-04-25T02:51:23Z",
    "authors" : [
      {
        "name" : "Chen Chen"
      },
      {
        "name" : "Daochang Liu"
      },
      {
        "name" : "Mubarak Shah"
      },
      {
        "name" : "Chang Xu"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.18007v1",
    "title" : "Differential Privacy-Driven Framework for Enhancing Heart Disease\n  Prediction",
    "summary" : "With the rapid digitalization of healthcare systems, there has been a\nsubstantial increase in the generation and sharing of private health data.\nSafeguarding patient information is essential for maintaining consumer trust\nand ensuring compliance with legal data protection regulations. Machine\nlearning is critical in healthcare, supporting personalized treatment, early\ndisease detection, predictive analytics, image interpretation, drug discovery,\nefficient operations, and patient monitoring. It enhances decision-making,\naccelerates research, reduces errors, and improves patient outcomes. In this\npaper, we utilize machine learning methodologies, including differential\nprivacy and federated learning, to develop privacy-preserving models that\nenable healthcare stakeholders to extract insights without compromising\nindividual privacy. Differential privacy introduces noise to data to guarantee\nstatistical privacy, while federated learning enables collaborative model\ntraining across decentralized datasets. We explore applying these technologies\nto Heart Disease Data, demonstrating how they preserve privacy while delivering\nvaluable insights and comprehensive analysis. Our results show that using a\nfederated learning model with differential privacy achieved a test accuracy of\n85%, ensuring patient data remained secure and private throughout the process.",
    "updated" : "2025-04-25T01:27:40Z",
    "published" : "2025-04-25T01:27:40Z",
    "authors" : [
      {
        "name" : "Yazan Otoum"
      },
      {
        "name" : "Amiya Nayak"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.19373v1",
    "title" : "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for\n  Agentic Multi-Modal Large Reasoning Model",
    "summary" : "The increasing capabilities of agentic multi-modal large reasoning models,\nsuch as ChatGPT o3, have raised critical concerns regarding privacy leakage\nthrough inadvertent image geolocation. In this paper, we conduct the first\nsystematic and controlled study on the potential privacy risks associated with\nvisual reasoning abilities of ChatGPT o3. We manually collect and construct a\ndataset comprising 50 real-world images that feature individuals alongside\nprivacy-relevant environmental elements, capturing realistic and sensitive\nscenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can\npredict user locations with high precision, achieving street-level accuracy\n(within one mile) in 60% of cases. Through analysis, we identify key visual\ncues, including street layout and front yard design, that significantly\ncontribute to the model inference success. Additionally, targeted occlusion\nexperiments demonstrate that masking critical features effectively mitigates\ngeolocation accuracy, providing insights into potential defense mechanisms. Our\nfindings highlight an urgent need for privacy-aware development for agentic\nmulti-modal large reasoning models, particularly in applications involving\nprivate imagery.",
    "updated" : "2025-04-27T22:26:45Z",
    "published" : "2025-04-27T22:26:45Z",
    "authors" : [
      {
        "name" : "Weidi Luo"
      },
      {
        "name" : "Qiming Zhang"
      },
      {
        "name" : "Tianyu Lu"
      },
      {
        "name" : "Xiaogeng Liu"
      },
      {
        "name" : "Yue Zhao"
      },
      {
        "name" : "Zhen Xiang"
      },
      {
        "name" : "Chaowei Xiao"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.19274v1",
    "title" : "TeleSparse: Practical Privacy-Preserving Verification of Deep Neural\n  Networks",
    "summary" : "Verification of the integrity of deep learning inference is crucial for\nunderstanding whether a model is being applied correctly. However, such\nverification typically requires access to model weights and (potentially\nsensitive or private) training data. So-called Zero-knowledge Succinct\nNon-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the\ncapability to verify model inference without access to such sensitive data.\nHowever, applying ZK-SNARKs to modern neural networks, such as transformers and\nlarge vision models, introduces significant computational overhead.\n  We present TeleSparse, a ZK-friendly post-processing mechanisms to produce\npractical solutions to this problem. TeleSparse tackles two fundamental\nchallenges inherent in applying ZK-SNARKs to modern neural networks: (1)\nReducing circuit constraints: Over-parameterized models result in numerous\nconstraints for ZK-SNARK verification, driving up memory and proof generation\ncosts. We address this by applying sparsification to neural network models,\nenhancing proof efficiency without compromising accuracy or security. (2)\nMinimizing the size of lookup tables required for non-linear functions, by\noptimizing activation ranges through neural teleportation, a novel adaptation\nfor narrowing activation functions' range.\n  TeleSparse reduces prover memory usage by 67% and proof generation time by\n46% on the same model, with an accuracy trade-off of approximately 1%. We\nimplement our framework using the Halo2 proving system and demonstrate its\neffectiveness across multiple architectures (Vision-transformer, ResNet,\nMobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new\ndirections for ZK-friendly model design, moving toward scalable,\nresource-efficient verifiable deep learning.",
    "updated" : "2025-04-27T15:14:09Z",
    "published" : "2025-04-27T15:14:09Z",
    "authors" : [
      {
        "name" : "Mohammad M Maheri"
      },
      {
        "name" : "Hamed Haddadi"
      },
      {
        "name" : "Alex Davidson"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.19101v1",
    "title" : "Privacy-Preserving Federated Embedding Learning for Localized\n  Retrieval-Augmented Generation",
    "summary" : "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution for enhancing the accuracy and credibility of Large Language Models\n(LLMs), particularly in Question & Answer tasks. This is achieved by\nincorporating proprietary and private data from integrated databases. However,\nprivate RAG systems face significant challenges due to the scarcity of private\ndomain data and critical data privacy issues. These obstacles impede the\ndeployment of private RAG systems, as developing privacy-preserving RAG systems\nrequires a delicate balance between data security and data availability. To\naddress these challenges, we regard federated learning (FL) as a highly\npromising technology for privacy-preserving RAG services. We propose a novel\nframework called Federated Retrieval-Augmented Generation (FedE4RAG). This\nframework facilitates collaborative training of client-side RAG retrieval\nmodels. The parameters of these models are aggregated and distributed on a\ncentral-server, ensuring data privacy without direct sharing of raw data. In\nFedE4RAG, knowledge distillation is employed for communication between the\nserver and client models. This technique improves the generalization of local\nRAG retrievers during the federated learning process. Additionally, we apply\nhomomorphic encryption within federated learning to safeguard model parameters\nand mitigate concerns related to data leakage. Extensive experiments conducted\non the real-world dataset have validated the effectiveness of FedE4RAG. The\nresults demonstrate that our proposed framework can markedly enhance the\nperformance of private RAG systems while maintaining robust data privacy\nprotection.",
    "updated" : "2025-04-27T04:26:02Z",
    "published" : "2025-04-27T04:26:02Z",
    "authors" : [
      {
        "name" : "Qianren Mao"
      },
      {
        "name" : "Qili Zhang"
      },
      {
        "name" : "Hanwen Hao"
      },
      {
        "name" : "Zhentao Han"
      },
      {
        "name" : "Runhua Xu"
      },
      {
        "name" : "Weifeng Jiang"
      },
      {
        "name" : "Qi Hu"
      },
      {
        "name" : "Zhijun Chen"
      },
      {
        "name" : "Tyler Zhou"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Yangqiu Song"
      },
      {
        "name" : "Jin Dong"
      },
      {
        "name" : "Jianxin Li"
      },
      {
        "name" : "Philip S. Yu"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.18596v1",
    "title" : "Optimizing the Privacy-Utility Balance using Synthetic Data and\n  Configurable Perturbation Pipelines",
    "summary" : "This paper explores the strategic use of modern synthetic data generation and\nadvanced data perturbation techniques to enhance security, maintain analytical\nutility, and improve operational efficiency when managing large datasets, with\na particular focus on the Banking, Financial Services, and Insurance (BFSI)\nsector. We contrast these advanced methods encompassing generative models like\nGANs, sophisticated context-aware PII transformation, configurable statistical\nperturbation, and differential privacy with traditional anonymization\napproaches.\n  The goal is to create realistic, privacy-preserving datasets that retain high\nutility for complex machine learning tasks and analytics, a critical need in\nthe data-sensitive industries like BFSI, Healthcare, Retail, and\nTelecommunications. We discuss how these modern techniques potentially offer\nsignificant improvements in balancing privacy preservation while maintaining\ndata utility compared to older methods. Furthermore, we examine the potential\nfor operational gains, such as reduced overhead and accelerated analytics, by\nusing these privacy-enhanced datasets. We also explore key use cases where\nthese methods can mitigate regulatory risks and enable scalable, data-driven\ninnovation without compromising sensitive customer information.",
    "updated" : "2025-04-24T15:52:53Z",
    "published" : "2025-04-24T15:52:53Z",
    "authors" : [
      {
        "name" : "Anantha Sharma"
      },
      {
        "name" : "Swetha Devabhaktuni"
      },
      {
        "name" : "Eklove Mohan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "math.PR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.18581v1",
    "title" : "Enhancing Privacy in Semantic Communication over Wiretap Channels\n  leveraging Differential Privacy",
    "summary" : "Semantic communication (SemCom) improves transmission efficiency by focusing\non task-relevant information. However, transmitting semantic-rich data over\ninsecure channels introduces privacy risks. This paper proposes a novel SemCom\nframework that integrates differential privacy (DP) mechanisms to protect\nsensitive semantic features. This method employs the generative adversarial\nnetwork (GAN) inversion technique to extract disentangled semantic features and\nuses neural networks (NNs) to approximate the DP application and removal\nprocesses, effectively mitigating the non-invertibility issue of DP.\nAdditionally, an NN-based encryption scheme is introduced to strengthen the\nsecurity of channel inputs. Simulation results demonstrate that the proposed\napproach effectively prevents eavesdroppers from reconstructing sensitive\ninformation by generating chaotic or fake images, while ensuring high-quality\nimage reconstruction for legitimate users. The system exhibits robust\nperformance across various privacy budgets and channel conditions, achieving an\noptimal balance between privacy protection and reconstruction fidelity.",
    "updated" : "2025-04-23T08:42:44Z",
    "published" : "2025-04-23T08:42:44Z",
    "authors" : [
      {
        "name" : "Weixuan Chen"
      },
      {
        "name" : "Shunpu Tang"
      },
      {
        "name" : "Qianqian Yang"
      },
      {
        "name" : "Zhiguo Shi"
      },
      {
        "name" : "Dusit Niyato"
      }
    ],
    "categories" : [
      "cs.CR",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.18569v1",
    "title" : "Large Language Model Empowered Privacy-Protected Framework for PHI\n  Annotation in Clinical Notes",
    "summary" : "The de-identification of private information in medical data is a crucial\nprocess to mitigate the risk of confidentiality breaches, particularly when\npatient personal details are not adequately removed before the release of\nmedical records. Although rule-based and learning-based methods have been\nproposed, they often struggle with limited generalizability and require\nsubstantial amounts of annotated data for effective performance. Recent\nadvancements in large language models (LLMs) have shown significant promise in\naddressing these issues due to their superior language comprehension\ncapabilities. However, LLMs present challenges, including potential privacy\nrisks when using commercial LLM APIs and high computational costs for deploying\nopen-source LLMs locally. In this work, we introduce LPPA, an LLM-empowered\nPrivacy-Protected PHI Annotation framework for clinical notes, targeting the\nEnglish language. By fine-tuning LLMs locally with synthetic notes, LPPA\nensures strong privacy protection and high PHI annotation accuracy. Extensive\nexperiments demonstrate LPPA's effectiveness in accurately de-identifying\nprivate information, offering a scalable and efficient solution for enhancing\npatient privacy protection.",
    "updated" : "2025-04-22T03:18:36Z",
    "published" : "2025-04-22T03:18:36Z",
    "authors" : [
      {
        "name" : "Guanchen Wu"
      },
      {
        "name" : "Linzhi Zheng"
      },
      {
        "name" : "Han Xie"
      },
      {
        "name" : "Zhen Xiang"
      },
      {
        "name" : "Jiaying Lu"
      },
      {
        "name" : "Darren Liu"
      },
      {
        "name" : "Delgersuren Bold"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Xiao Hu"
      },
      {
        "name" : "Carl Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.20941v1",
    "title" : "Conformal-DP: Differential Privacy on Riemannian Manifolds via Conformal\n  Transformation",
    "summary" : "Differential Privacy (DP) has been established as a safeguard for private\ndata sharing by adding perturbations to information release. Prior research on\nDP has extended beyond data in the flat Euclidean space and addressed data on\ncurved manifolds, e.g., diffusion tensor MRI, social networks, or organ shape\nanalysis, by adding perturbations along geodesic distances. However, existing\nmanifold-aware DP methods rely on the assumption that samples are uniformly\ndistributed across the manifold. In reality, data densities vary, leading to a\nbiased noise imbalance across manifold regions, weakening the privacy-utility\ntrade-offs. To address this gap, we propose a novel mechanism: Conformal-DP,\nutilizing conformal transformations on the Riemannian manifold to equalize\nlocal sample density and to redefine geodesic distances accordingly while\npreserving the intrinsic geometry of the manifold. Our theoretical analysis\nyields two main results. First, we prove that the conformal factor computed\nfrom local kernel-density estimates is explicitly data-density-aware; Second,\nunder the conformal metric, the mechanism satisfies $ \\varepsilon\n$-differential privacy on any complete Riemannian manifold and admits a\nclosed-form upper bound on the expected geodesic error that depends only on the\nmaximal density ratio, not on global curvatureof the manifold. Our experimental\nresults validate that the mechanism achieves high utility while providing the $\n\\varepsilon $-DP guarantee for both homogeneous and especially heterogeneous\nmanifold data.",
    "updated" : "2025-04-29T17:05:55Z",
    "published" : "2025-04-29T17:05:55Z",
    "authors" : [
      {
        "name" : "Peilin He"
      },
      {
        "name" : "Liou Tang"
      },
      {
        "name" : "M. Amin Rahimian"
      },
      {
        "name" : "James Joshi"
      }
    ],
    "categories" : [
      "cs.CR",
      "math.DG",
      "stat.OT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.20926v1",
    "title" : "Bipartite Randomized Response Mechanism for Local Differential Privacy",
    "summary" : "With the increasing importance of data privacy, Local Differential Privacy\n(LDP) has recently become a strong measure of privacy for protecting each\nuser's privacy from data analysts without relying on a trusted third party. In\nmany cases, both data providers and data analysts hope to maximize the utility\nof released data. In this paper, we study the fundamental trade-off formulated\nas a constrained optimization problem: maximizing data utility subject to the\nconstraint of LDP budgets. In particular, the Generalized Randomized Response\n(GRR) treats all discrete data equally except for the true data. For this, we\nintroduce an adaptive LDP mechanism called Bipartite Randomized Response (BRR),\nwhich solves the above privacy-utility maximization problem from the global\nstandpoint. We prove that for any utility function and any privacy level,\nsolving the maximization problem is equivalent to confirming how many\nhigh-utility data to be treated equally as the true data on release\nprobability, the outcome of which gives the optimal randomized response.\nFurther, solving this linear program can be computationally cheap in theory.\nSeveral examples of utility functions defined by distance metrics and\napplications in decision trees and deep learning are presented. The results of\nvarious experiments show that our BRR significantly outperforms the\nstate-of-the-art LDP mechanisms of both continuous and distributed types.",
    "updated" : "2025-04-29T16:39:50Z",
    "published" : "2025-04-29T16:39:50Z",
    "authors" : [
      {
        "name" : "Shun Zhang"
      },
      {
        "name" : "Hai Zhu"
      },
      {
        "name" : "Zhili Chen"
      },
      {
        "name" : "Neal N. Xiong"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.20827v1",
    "title" : "DP-SMOTE: Integrating Differential Privacy and Oversampling Technique to\n  Preserve Privacy in Smart Homes",
    "summary" : "Smart homes represent intelligent environments where interconnected devices\ngather information, enhancing users living experiences by ensuring comfort,\nsafety, and efficient energy management. To enhance the quality of life,\ncompanies in the smart device industry collect user data, including activities,\npreferences, and power consumption. However, sharing such data necessitates\nprivacy-preserving practices. This paper introduces a robust method for secure\nsharing of data to service providers, grounded in differential privacy (DP).\nThis empowers smart home residents to contribute usage statistics while\nsafeguarding their privacy. The approach incorporates the Synthetic Minority\nOversampling technique (SMOTe) and seamlessly integrates Gaussian noise to\ngenerate synthetic data, enabling data and statistics sharing while preserving\nindividual privacy. The proposed method employs the SMOTe algorithm and applies\nGaussian noise to generate data. Subsequently, it employs a k-anonymity\nfunction to assess reidentification risk before sharing the data. The\nsimulation outcomes demonstrate that our method delivers strong performance in\nsafeguarding privacy and in accuracy, recall, and f-measure metrics. This\napproach is particularly effective in smart homes, offering substantial utility\nin privacy at a reidentification risk of 30%, with Gaussian noise set to 0.3,\nSMOTe at 500%, and the application of a k-anonymity function with k = 2.\nAdditionally, it shows a high classification accuracy, ranging from 90% to 98%,\nacross various classification techniques.",
    "updated" : "2025-04-29T14:50:50Z",
    "published" : "2025-04-29T14:50:50Z",
    "authors" : [
      {
        "name" : "Amr Tarek Elsayed"
      },
      {
        "name" : "Almohammady Sobhi Alsharkawy"
      },
      {
        "name" : "Mohamed Sayed Farag"
      },
      {
        "name" : "Shaban Ebrahim Abu Yusuf"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.20700v1",
    "title" : "Building Trust in Healthcare with Privacy Techniques: Blockchain in the\n  Cloud",
    "summary" : "This study introduces a cutting-edge architecture developed for the\nNewbornTime project, which uses advanced AI to analyze video data at birth and\nduring newborn resuscitation, with the aim of improving newborn care. The\nproposed architecture addresses the crucial issues of patient consent, data\nsecurity, and investing trust in healthcare by integrating Ethereum blockchain\nwith cloud computing. Our blockchain-based consent application simplifies\npatient consent's secure and transparent management. We explain the smart\ncontract mechanisms and privacy measures employed, ensuring data protection\nwhile permitting controlled data sharing among authorized parties. This work\ndemonstrates the potential of combining blockchain and cloud technologies in\nhealthcare, emphasizing their role in maintaining data integrity, with\nimplications for computer science and healthcare innovation.",
    "updated" : "2025-04-29T12:31:37Z",
    "published" : "2025-04-29T12:31:37Z",
    "authors" : [
      {
        "name" : "Ferhat Ozgur Catak"
      },
      {
        "name" : "Chunming Rong"
      },
      {
        "name" : "Øyvind Meinich-Bache"
      },
      {
        "name" : "Sara Brunner"
      },
      {
        "name" : "Kjersti Engan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.20639v1",
    "title" : "Multi-Message Secure Aggregation with Demand Privacy",
    "summary" : "This paper considers a multi-message secure aggregation with privacy problem,\nin which a server aims to compute $\\sf K_c\\geq 1$ linear combinations of local\ninputs from $\\sf K$ distributed users. The problem addresses two tasks: (1)\nsecurity, ensuring that the server can only obtain the desired linear\ncombinations without any else information about the users' inputs, and (2)\nprivacy, preventing users from learning about the server's computation task. In\naddition, the effect of user dropouts is considered, where at most $\\sf{K-U}$\nusers can drop out and the identity of these users cannot be predicted in\nadvance. We propose two schemes for $\\sf K_c$ is equal to (1) and $\\sf 2\\leq\nK_c\\leq U-1$, respectively. For $\\sf K_c$ is equal to (1), we introduce\nmultiplicative encryption of the server's demand using a random variable, where\nusers share coded keys offline and transmit masked models in the first round,\nfollowed by aggregated coded keys in the second round for task recovery. For\n$\\sf{2\\leq K_c \\leq U-1}$, we use robust symmetric private computation to\nrecover linear combinations of keys in the second round. The objective is to\nminimize the number of symbols sent by each user during the two rounds. Our\nproposed schemes have achieved the optimal rate region when $ \\sf K_c $ is\nequal to (1) and the order optimal rate (within 2) when $\\sf{2\\leq K_c \\leq\nU-1}$.",
    "updated" : "2025-04-29T11:11:27Z",
    "published" : "2025-04-29T11:11:27Z",
    "authors" : [
      {
        "name" : "Chenyi Sun"
      },
      {
        "name" : "Ziting Zhang"
      },
      {
        "name" : "Kai Wan"
      },
      {
        "name" : "Giuseppe Caire"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.20350v1",
    "title" : "SoK: Enhancing Privacy-Preserving Software Development from a\n  Developers' Perspective",
    "summary" : "In software development, privacy preservation has become essential with the\nrise of privacy concerns and regulations such as GDPR and CCPA. While several\ntools, guidelines, methods, methodologies, and frameworks have been proposed to\nsupport developers embedding privacy into software applications, most of them\nare proofs-of-concept without empirical evaluations, making their practical\napplicability uncertain. These solutions should be evaluated for different\ntypes of scenarios (e.g., industry settings such as rapid software development\nenvironments, teams with different privacy knowledge, etc.) to determine what\ntheir limitations are in various industry settings and what changes are\nrequired to refine current solutions before putting them into industry and\ndeveloping new developer-supporting approaches. For that, a thorough review of\nempirically evaluated current solutions will be very effective. However, the\nexisting secondary studies that examine the available developer support provide\nbroad overviews but do not specifically analyze empirically evaluated solutions\nand their limitations. Therefore, this Systematic Literature Review (SLR) aims\nto identify and analyze empirically validated solutions that are designed to\nhelp developers in privacy-preserving software development. The findings will\nprovide valuable insights for researchers to improve current privacy-preserving\nsolutions and for practitioners looking for effective and validated solutions\nto embed privacy into software development.",
    "updated" : "2025-04-29T01:38:01Z",
    "published" : "2025-04-29T01:38:01Z",
    "authors" : [
      {
        "name" : "Tharaka Wijesundara"
      },
      {
        "name" : "Nalin Asanka Gamagedara Arachchilage"
      },
      {
        "name" : "Matthew Warren"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.20282v1",
    "title" : "FedCCL: Federated Clustered Continual Learning Framework for\n  Privacy-focused Energy Forecasting",
    "summary" : "Privacy-preserving distributed model training is crucial for modern machine\nlearning applications, yet existing Federated Learning approaches struggle with\nheterogeneous data distributions and varying computational capabilities.\nTraditional solutions either treat all participants uniformly or require costly\ndynamic clustering during training, leading to reduced efficiency and delayed\nmodel specialization. We present FedCCL (Federated Clustered Continual\nLearning), a framework specifically designed for environments with static\norganizational characteristics but dynamic client availability. By combining\nstatic pre-training clustering with an adapted asynchronous FedAvg algorithm,\nFedCCL enables new clients to immediately profit from specialized models\nwithout prior exposure to their data distribution, while maintaining reduced\ncoordination overhead and resilience to client disconnections. Our approach\nimplements an asynchronous Federated Learning protocol with a three-tier model\ntopology - global, cluster-specific, and local models - that efficiently\nmanages knowledge sharing across heterogeneous participants. Evaluation using\nphotovoltaic installations across central Europe demonstrates that FedCCL's\nlocation-based clustering achieves an energy prediction error of 3.93%\n(+-0.21%), while maintaining data privacy and showing that the framework\nmaintains stability for population-independent deployments, with 0.14\npercentage point degradation in performance for new installations. The results\ndemonstrate that FedCCL offers an effective framework for privacy-preserving\ndistributed learning, maintaining high accuracy and adaptability even with\ndynamic participant populations.",
    "updated" : "2025-04-28T21:51:27Z",
    "published" : "2025-04-28T21:51:27Z",
    "authors" : [
      {
        "name" : "Michael A. Helcig"
      },
      {
        "name" : "Stefan Nastic"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.19373v2",
    "title" : "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for\n  Agentic Multi-Modal Large Reasoning Model",
    "summary" : "The increasing capabilities of agentic multi-modal large reasoning models,\nsuch as ChatGPT o3, have raised critical concerns regarding privacy leakage\nthrough inadvertent image geolocation. In this paper, we conduct the first\nsystematic and controlled study on the potential privacy risks associated with\nvisual reasoning abilities of ChatGPT o3. We manually collect and construct a\ndataset comprising 50 real-world images that feature individuals alongside\nprivacy-relevant environmental elements, capturing realistic and sensitive\nscenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can\npredict user locations with high precision, achieving street-level accuracy\n(within one mile) in 60% of cases. Through analysis, we identify key visual\ncues, including street layout and front yard design, that significantly\ncontribute to the model inference success. Additionally, targeted occlusion\nexperiments demonstrate that masking critical features effectively mitigates\ngeolocation accuracy, providing insights into potential defense mechanisms. Our\nfindings highlight an urgent need for privacy-aware development for agentic\nmulti-modal large reasoning models, particularly in applications involving\nprivate imagery.",
    "updated" : "2025-04-29T12:00:08Z",
    "published" : "2025-04-27T22:26:45Z",
    "authors" : [
      {
        "name" : "Weidi Luo"
      },
      {
        "name" : "Qiming Zhang"
      },
      {
        "name" : "Tianyu Lu"
      },
      {
        "name" : "Xiaogeng Liu"
      },
      {
        "name" : "Yue Zhao"
      },
      {
        "name" : "Zhen Xiang"
      },
      {
        "name" : "Chaowei Xiao"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.21752v1",
    "title" : "VDDP: Verifiable Distributed Differential Privacy under the\n  Client-Server-Verifier Setup",
    "summary" : "Despite differential privacy (DP) often being considered the de facto\nstandard for data privacy, its realization is vulnerable to unfaithful\nexecution of its mechanisms by servers, especially in distributed settings.\nSpecifically, servers may sample noise from incorrect distributions or generate\ncorrelated noise while appearing to follow established protocols. This work\nanalyzes these malicious behaviors in a general differential privacy framework\nwithin a distributed client-server-verifier setup. To address these adversarial\nproblems, we propose a novel definition called Verifiable Distributed\nDifferential Privacy (VDDP) by incorporating additional verification\nmechanisms. We also explore the relationship between zero-knowledge proofs\n(ZKP) and DP, demonstrating that while ZKPs are sufficient for achieving DP\nunder verifiability requirements, they are not necessary. Furthermore, we\ndevelop two novel and efficient mechanisms that satisfy VDDP: (1) the\nVerifiable Distributed Discrete Laplacian Mechanism (VDDLM), which offers up to\na $4 \\times 10^5$x improvement in proof generation efficiency with only\n0.1-0.2x error compared to the previous state-of-the-art verifiable\ndifferentially private mechanism; (2) an improved solution to Verifiable\nRandomized Response (VRR) under local DP, a special case of VDDP, achieving up\na reduction of up to 5000x in communication costs and the verifier's overhead.",
    "updated" : "2025-04-30T15:46:55Z",
    "published" : "2025-04-30T15:46:55Z",
    "authors" : [
      {
        "name" : "Haochen Sun"
      },
      {
        "name" : "Xi He"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.21646v1",
    "title" : "Diffusion-based Adversarial Identity Manipulation for Facial Privacy\n  Protection",
    "summary" : "The success of face recognition (FR) systems has led to serious privacy\nconcerns due to potential unauthorized surveillance and user tracking on social\nnetworks. Existing methods for enhancing privacy fail to generate natural face\nimages that can protect facial privacy. In this paper, we propose\ndiffusion-based adversarial identity manipulation (DiffAIM) to generate natural\nand highly transferable adversarial faces against malicious FR systems. To be\nspecific, we manipulate facial identity within the low-dimensional latent space\nof a diffusion model. This involves iteratively injecting gradient-based\nadversarial identity guidance during the reverse diffusion process,\nprogressively steering the generation toward the desired adversarial faces. The\nguidance is optimized for identity convergence towards a target while promoting\nsemantic divergence from the source, facilitating effective impersonation while\nmaintaining visual naturalness. We further incorporate structure-preserving\nregularization to preserve facial structure consistency during manipulation.\nExtensive experiments on both face verification and identification tasks\ndemonstrate that compared with the state-of-the-art, DiffAIM achieves stronger\nblack-box attack transferability while maintaining superior visual quality. We\nalso demonstrate the effectiveness of the proposed approach for commercial FR\nAPIs, including Face++ and Aliyun.",
    "updated" : "2025-04-30T13:49:59Z",
    "published" : "2025-04-30T13:49:59Z",
    "authors" : [
      {
        "name" : "Liqin Wang"
      },
      {
        "name" : "Qianyue Hu"
      },
      {
        "name" : "Wei Lu"
      },
      {
        "name" : "Xiangyang Luo"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.21413v1",
    "title" : "An Inversion Theorem for Buffered Linear Toeplitz (BLT) Matrices and\n  Applications to Streaming Differential Privacy",
    "summary" : "Buffered Linear Toeplitz (BLT) matrices are a family of parameterized\nlower-triangular matrices that play an important role in streaming differential\nprivacy with correlated noise. Our main result is a BLT inversion theorem: the\ninverse of a BLT matrix is itself a BLT matrix with different parameters. We\nalso present an efficient and differentiable $O(d^3)$ algorithm to compute the\nparameters of the inverse BLT matrix, where $d$ is the degree of the original\nBLT (typically $d < 10$). Our characterization enables direct optimization of\nBLT parameters for privacy mechanisms through automatic differentiation.",
    "updated" : "2025-04-30T08:14:09Z",
    "published" : "2025-04-30T08:14:09Z",
    "authors" : [
      {
        "name" : "H. Brendan McMahan"
      },
      {
        "name" : "Krishna Pillutla"
      }
    ],
    "categories" : [
      "cs.CR",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.21297v1",
    "title" : "Participatory AI, Public Sector AI, Differential Privacy, Conversational\n  Interfaces, Explainable AI, Citizen Engagement in AI",
    "summary" : "This paper introduces a conversational interface system that enables\nparticipatory design of differentially private AI systems in public sector\napplications. Addressing the challenge of balancing mathematical privacy\nguarantees with democratic accountability, we propose three key contributions:\n(1) an adaptive $\\epsilon$-selection protocol leveraging TOPSIS multi-criteria\ndecision analysis to align citizen preferences with differential privacy (DP)\nparameters, (2) an explainable noise-injection framework featuring real-time\nMean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and\n(3) an integrated legal-compliance mechanism that dynamically modulates privacy\nbudgets based on evolving regulatory constraints. Our results advance\nparticipatory AI practices by demonstrating how conversational interfaces can\nenhance public engagement in algorithmic privacy mechanisms, ensuring that\nprivacy-preserving AI in public sector governance remains both mathematically\nrobust and democratically accountable.",
    "updated" : "2025-04-30T04:10:50Z",
    "published" : "2025-04-30T04:10:50Z",
    "authors" : [
      {
        "name" : "Wenjun Yang"
      },
      {
        "name" : "Eyhab Al-Masri"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.AI",
      "cs.CY",
      "cs.ET",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.21182v1",
    "title" : "Federated One-Shot Learning with Data Privacy and Objective-Hiding",
    "summary" : "Privacy in federated learning is crucial, encompassing two key aspects:\nsafeguarding the privacy of clients' data and maintaining the privacy of the\nfederator's objective from the clients. While the first aspect has been\nextensively studied, the second has received much less attention.\n  We present a novel approach that addresses both concerns simultaneously,\ndrawing inspiration from techniques in knowledge distillation and private\ninformation retrieval to provide strong information-theoretic privacy\nguarantees.\n  Traditional private function computation methods could be used here; however,\nthey are typically limited to linear or polynomial functions. To overcome these\nconstraints, our approach unfolds in three stages. In stage 0, clients perform\nthe necessary computations locally. In stage 1, these results are shared among\nthe clients, and in stage 2, the federator retrieves its desired objective\nwithout compromising the privacy of the clients' data. The crux of the method\nis a carefully designed protocol that combines secret-sharing-based multi-party\ncomputation and a graph-based private information retrieval scheme. We show\nthat our method outperforms existing tools from the literature when properly\nadapted to this setting.",
    "updated" : "2025-04-29T21:25:34Z",
    "published" : "2025-04-29T21:25:34Z",
    "authors" : [
      {
        "name" : "Maximilian Egger"
      },
      {
        "name" : "Rüdiger Urbanke"
      },
      {
        "name" : "Rawad Bitar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.20350v2",
    "title" : "SoK: Enhancing Privacy-Preserving Software Development from a\n  Developers' Perspective",
    "summary" : "In software development, privacy preservation has become essential with the\nrise of privacy concerns and regulations such as GDPR and CCPA. While several\ntools, guidelines, methods, methodologies, and frameworks have been proposed to\nsupport developers embedding privacy into software applications, most of them\nare proofs-of-concept without empirical evaluations, making their practical\napplicability uncertain. These solutions should be evaluated for different\ntypes of scenarios (e.g., industry settings such as rapid software development\nenvironments, teams with different privacy knowledge, etc.) to determine what\ntheir limitations are in various industry settings and what changes are\nrequired to refine current solutions before putting them into industry and\ndeveloping new developer-supporting approaches. For that, a thorough review of\nempirically evaluated current solutions will be very effective. However, the\nexisting secondary studies that examine the available developer support provide\nbroad overviews but do not specifically analyze empirically evaluated solutions\nand their limitations. Therefore, this Systematic Literature Review (SLR) aims\nto identify and analyze empirically validated solutions that are designed to\nhelp developers in privacy-preserving software development. The findings will\nprovide valuable insights for researchers to improve current privacy-preserving\nsolutions and for practitioners looking for effective and validated solutions\nto embed privacy into software development.",
    "updated" : "2025-04-30T02:38:48Z",
    "published" : "2025-04-29T01:38:01Z",
    "authors" : [
      {
        "name" : "Tharaka Wijesundara"
      },
      {
        "name" : "Matthew Warren"
      },
      {
        "name" : "Nalin Asanka Gamagedara Arachchilage"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.21036v1",
    "title" : "Can Differentially Private Fine-tuning LLMs Protect Against Privacy\n  Attacks?",
    "summary" : "Fine-tuning large language models (LLMs) has become an essential strategy for\nadapting them to specialized tasks; however, this process introduces\nsignificant privacy challenges, as sensitive training data may be inadvertently\nmemorized and exposed. Although differential privacy (DP) offers strong\ntheoretical guarantees against such leakage, its empirical privacy\neffectiveness on LLMs remains unclear, especially under different fine-tuning\nmethods. In this paper, we systematically investigate the impact of DP across\nfine-tuning methods and privacy budgets, using both data extraction and\nmembership inference attacks to assess empirical privacy risks. Our main\nfindings are as follows: (1) Differential privacy reduces model utility, but\nits impact varies significantly across different fine-tuning methods. (2)\nWithout DP, the privacy risks of models fine-tuned with different approaches\ndiffer considerably. (3) When DP is applied, even a relatively high privacy\nbudget can substantially lower privacy risk. (4) The privacy-utility trade-off\nunder DP training differs greatly among fine-tuning methods, with some methods\nbeing unsuitable for DP due to severe utility degradation. Our results provide\npractical guidance for privacy-conscious deployment of LLMs and pave the way\nfor future research on optimizing the privacy-utility trade-off in fine-tuning\nmethodologies.",
    "updated" : "2025-04-28T05:34:53Z",
    "published" : "2025-04-28T05:34:53Z",
    "authors" : [
      {
        "name" : "Hao Du"
      },
      {
        "name" : "Shang Liu"
      },
      {
        "name" : "Yang Cao"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.21035v1",
    "title" : "A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond\n  Surface-level Privacy Leakage",
    "summary" : "Sanitizing sensitive text data typically involves removing personally\nidentifiable information (PII) or generating synthetic data under the\nassumption that these methods adequately protect privacy; however, their\neffectiveness is often only assessed by measuring the leakage of explicit\nidentifiers but ignoring nuanced textual markers that can lead to\nre-identification. We challenge the above illusion of privacy by proposing a\nnew framework that evaluates re-identification attacks to quantify individual\nprivacy risks upon data release. Our approach shows that seemingly innocuous\nauxiliary information -- such as routine social activities -- can be used to\ninfer sensitive attributes like age or substance use history from sanitized\ndata. For instance, we demonstrate that Azure's commercial PII removal tool\nfails to protect 74\\% of information in the MedQA dataset. Although\ndifferential privacy mitigates these risks to some extent, it significantly\nreduces the utility of the sanitized text for downstream tasks. Our findings\nindicate that current sanitization techniques offer a \\textit{false sense of\nprivacy}, highlighting the need for more robust methods that protect against\nsemantic-level information leakage.",
    "updated" : "2025-04-28T01:16:27Z",
    "published" : "2025-04-28T01:16:27Z",
    "authors" : [
      {
        "name" : "Rui Xin"
      },
      {
        "name" : "Niloofar Mireshghallah"
      },
      {
        "name" : "Shuyue Stella Li"
      },
      {
        "name" : "Michael Duan"
      },
      {
        "name" : "Hyunwoo Kim"
      },
      {
        "name" : "Yejin Choi"
      },
      {
        "name" : "Yulia Tsvetkov"
      },
      {
        "name" : "Sewoong Oh"
      },
      {
        "name" : "Pang Wei Koh"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.21413v1",
    "title" : "An Inversion Theorem for Buffered Linear Toeplitz (BLT) Matrices and\n  Applications to Streaming Differential Privacy",
    "summary" : "Buffered Linear Toeplitz (BLT) matrices are a family of parameterized\nlower-triangular matrices that play an important role in streaming differential\nprivacy with correlated noise. Our main result is a BLT inversion theorem: the\ninverse of a BLT matrix is itself a BLT matrix with different parameters. We\nalso present an efficient and differentiable $O(d^3)$ algorithm to compute the\nparameters of the inverse BLT matrix, where $d$ is the degree of the original\nBLT (typically $d < 10$). Our characterization enables direct optimization of\nBLT parameters for privacy mechanisms through automatic differentiation.",
    "updated" : "2025-04-30T08:14:09Z",
    "published" : "2025-04-30T08:14:09Z",
    "authors" : [
      {
        "name" : "H. Brendan McMahan"
      },
      {
        "name" : "Krishna Pillutla"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.21036v2",
    "title" : "Can Differentially Private Fine-tuning LLMs Protect Against Privacy\n  Attacks?",
    "summary" : "Fine-tuning large language models (LLMs) has become an essential strategy for\nadapting them to specialized tasks; however, this process introduces\nsignificant privacy challenges, as sensitive training data may be inadvertently\nmemorized and exposed. Although differential privacy (DP) offers strong\ntheoretical guarantees against such leakage, its empirical privacy\neffectiveness on LLMs remains unclear, especially under different fine-tuning\nmethods. In this paper, we systematically investigate the impact of DP across\nfine-tuning methods and privacy budgets, using both data extraction and\nmembership inference attacks to assess empirical privacy risks. Our main\nfindings are as follows: (1) Differential privacy reduces model utility, but\nits impact varies significantly across different fine-tuning methods. (2)\nWithout DP, the privacy risks of models fine-tuned with different approaches\ndiffer considerably. (3) When DP is applied, even a relatively high privacy\nbudget can substantially lower privacy risk. (4) The privacy-utility trade-off\nunder DP training differs greatly among fine-tuning methods, with some methods\nbeing unsuitable for DP due to severe utility degradation. Our results provide\npractical guidance for privacy-conscious deployment of LLMs and pave the way\nfor future research on optimizing the privacy-utility trade-off in fine-tuning\nmethodologies.",
    "updated" : "2025-05-01T10:10:01Z",
    "published" : "2025-04-28T05:34:53Z",
    "authors" : [
      {
        "name" : "Hao Du"
      },
      {
        "name" : "Shang Liu"
      },
      {
        "name" : "Yang Cao"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.06697v2",
    "title" : "\"Sorry for bugging you so much.\" Exploring Developers' Behavior Towards\n  Privacy-Compliant Implementation",
    "summary" : "While protecting user data is essential, software developers often fail to\nfulfill privacy requirements. However, the reasons why they struggle with\nprivacy-compliant implementation remain unclear. Is it due to a lack of\nknowledge, or is it because of insufficient support? To provide foundational\ninsights in this field, we conducted a qualitative 5-hour programming study\nwith 30 professional software developers implementing 3 privacy-sensitive\nprogramming tasks that were designed with GDPR compliance in mind. To explore\nif and how developers implement privacy requirements, participants were divided\ninto 3 groups: control, privacy prompted, and privacy expert-supported. After\ntask completion, we conducted follow-up interviews. Alarmingly, almost all\nparticipants submitted non-GDPR-compliant solutions (79/90). In particular,\nnone of the 3 tasks were solved privacy-compliant by all 30 participants, with\nthe non-prompted group having the lowest number of 3 out of 30\nprivacy-compliant solution attempts. Privacy prompting and expert support only\nslightly improved participants' submissions, with 6/30 and 8/30\nprivacy-compliant attempts, respectively. In fact, all participants reported\nsevere issues addressing common privacy requirements such as purpose\nlimitation, user consent, or data minimization. Counterintuitively, although\nmost developers exhibited minimal confidence in their solutions, they rarely\nsought online assistance or contacted the privacy expert, with only 4 out of 10\nexpert-supported participants explicitly asking for compliance confirmation.\nInstead, participants often relied on existing implementations and focused on\nimplementing functionality and security first.",
    "updated" : "2025-05-01T07:09:25Z",
    "published" : "2025-04-09T08:59:17Z",
    "authors" : [
      {
        "name" : "Stefan Albert Horstmann"
      },
      {
        "name" : "Sandy Hong"
      },
      {
        "name" : "David Klein"
      },
      {
        "name" : "Raphael Serafini"
      },
      {
        "name" : "Martin Degeling"
      },
      {
        "name" : "Martin Johns"
      },
      {
        "name" : "Veelasha Moonsamy"
      },
      {
        "name" : "Alena Naiakshina"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  }
]