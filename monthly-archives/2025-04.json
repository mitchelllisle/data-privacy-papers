[
  {
    "id" : "http://arxiv.org/abs/2504.01820v1",
    "title" : "Antenna Selection for Enhancing Privacy in Radar-Based Vital Sign\n  Monitoring Systems",
    "summary" : "Radar-based vital sign monitoring (VSM) systems have become valuable for\nnon-contact health monitoring by detecting physiological activities, such as\nrespiration and heartbeat, remotely. However, the conventional phased array\nused in VSM is vulnerable to privacy breaches, as an eavesdropper can extract\nsensitive vital sign information by analyzing the reflected radar signals. In\nthis paper, we propose a novel approach to protect privacy in radar-based VSM\nby modifying the radar transmitter hardware, specifically by strategically\nselecting the transmit antennas from the available antennas in the transmit\narray. By dynamically selecting which antennas connect or disconnect to the\nradio frequency chain, the transmitter introduces additional phase noise to the\nradar echoes, generating false frequencies in the power spectrum of the\nextracted phases at the eavesdropper's receiver. The antenna activation pattern\nis designed to maximize the variance of the phases introduced by antenna\nselection, which effectively makes the false frequencies dominate the spectrum,\nobscuring the actual vital sign frequencies. Meanwhile, the authorized\nreceiver, having knowledge of the antenna selection pattern, can compensate for\nthe phase noise and accurately extract the vital signs. Numerical experiments\nare conducted to validate the effectiveness of the proposed approach in\nenhancing privacy while maintaining vital sign monitoring.",
    "updated" : "2025-04-02T15:28:07Z",
    "published" : "2025-04-02T15:28:07Z",
    "authors" : [
      {
        "name" : "Zhihao Tao"
      },
      {
        "name" : "Athina P. Petropulu"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00952v1",
    "title" : "Personalized Federated Training of Diffusion Models with Privacy\n  Guarantees",
    "summary" : "The scarcity of accessible, compliant, and ethically sourced data presents a\nconsiderable challenge to the adoption of artificial intelligence (AI) in\nsensitive fields like healthcare, finance, and biomedical research.\nFurthermore, access to unrestricted public datasets is increasingly constrained\ndue to rising concerns over privacy, copyright, and competition. Synthetic data\nhas emerged as a promising alternative, and diffusion models -- a cutting-edge\ngenerative AI technology -- provide an effective solution for generating\nhigh-quality and diverse synthetic data. In this paper, we introduce a novel\nfederated learning framework for training diffusion models on decentralized\nprivate datasets. Our framework leverages personalization and the inherent\nnoise in the forward diffusion process to produce high-quality samples while\nensuring robust differential privacy guarantees. Our experiments show that our\nframework outperforms non-collaborative training methods, particularly in\nsettings with high data heterogeneity, and effectively reduces biases and\nimbalances in synthetic data, resulting in fairer downstream models.",
    "updated" : "2025-04-01T16:45:26Z",
    "published" : "2025-04-01T16:45:26Z",
    "authors" : [
      {
        "name" : "Kumar Kshitij Patel"
      },
      {
        "name" : "Weitong Zhang"
      },
      {
        "name" : "Lingxiao Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00919v1",
    "title" : "Nonparametric spectral density estimation using interactive mechanisms\n  under local differential privacy",
    "summary" : "We address the problem of nonparametric estimation of the spectral density\nfor a centered stationary Gaussian time series under local differential privacy\nconstraints. Specifically, we propose new interactive privacy mechanisms for\nthree tasks: estimating a single covariance coefficient, estimating the\nspectral density at a fixed frequency, and estimating the entire spectral\ndensity function. Our approach achieves faster rates through a two-stage\nprocess: we apply first the Laplace mechanism to the truncated value and then\nuse the former privatized sample to gain knowledge on the dependence mechanism\nin the time series. For spectral densities belonging to H\\\"older and Sobolev\nsmoothness classes, we demonstrate that our estimators improve upon the\nnon-interactive mechanism of Kroll (2024) for small privacy parameter $\\alpha$,\nsince the pointwise rates depend on $n\\alpha^2$ instead of $n\\alpha^4$.\nMoreover, we show that the rate $(n\\alpha^4)^{-1}$ is optimal for estimating a\ncovariance coefficient with non-interactive mechanisms. However, the $L_2$ rate\nof our interactive estimator is slower than the pointwise rate. We show how to\nuse these estimators to provide a bona-fide locally differentially private\ncovariance matrix estimator.",
    "updated" : "2025-04-01T15:52:50Z",
    "published" : "2025-04-01T15:52:50Z",
    "authors" : [
      {
        "name" : "Cristina Butucea"
      },
      {
        "name" : "Karolina Klockmann"
      },
      {
        "name" : "Tatyana Krivobokova"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00890v1",
    "title" : "Privacy-Preserving Transfer Learning for Community Detection using\n  Locally Distributed Multiple Networks",
    "summary" : "This paper develops a new spectral clustering-based method called TransNet\nfor transfer learning in community detection of network data. Our goal is to\nimprove the clustering performance of the target network using auxiliary source\nnetworks, which are heterogeneous, privacy-preserved, and locally stored across\nvarious sources. The edges of each locally stored network are perturbed using\nthe randomized response mechanism to achieve differential privacy. Notably, we\nallow the source networks to have distinct privacy-preserving and heterogeneity\nlevels as often desired in practice. To better utilize the information from the\nsource networks, we propose a novel adaptive weighting method to aggregate the\neigenspaces of the source networks multiplied by adaptive weights chosen to\nincorporate the effects of privacy and heterogeneity. We propose a\nregularization method that combines the weighted average eigenspace of the\nsource networks with the eigenspace of the target network to achieve an optimal\nbalance between them. Theoretically, we show that the adaptive weighting method\nenjoys the error-bound-oracle property in the sense that the error bound of the\nestimated eigenspace only depends on informative source networks. We also\ndemonstrate that TransNet performs better than the estimator using only the\ntarget network and the estimator using only the weighted source networks.",
    "updated" : "2025-04-01T15:19:07Z",
    "published" : "2025-04-01T15:19:07Z",
    "authors" : [
      {
        "name" : "Xiao Guo"
      },
      {
        "name" : "Xuming He"
      },
      {
        "name" : "Xiangyu Chang"
      },
      {
        "name" : "Shujie Ma"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00874v1",
    "title" : "P2NIA: Privacy-Preserving Non-Iterative Auditing",
    "summary" : "The emergence of AI legislation has increased the need to assess the ethical\ncompliance of high-risk AI systems. Traditional auditing methods rely on\nplatforms' application programming interfaces (APIs), where responses to\nqueries are examined through the lens of fairness requirements. However, such\napproaches put a significant burden on platforms, as they are forced to\nmaintain APIs while ensuring privacy, facing the possibility of data leaks.\nThis lack of proper collaboration between the two parties, in turn, causes a\nsignificant challenge to the auditor, who is subject to estimation bias as they\nare unaware of the data distribution of the platform. To address these two\nissues, we present P2NIA, a novel auditing scheme that proposes a mutually\nbeneficial collaboration for both the auditor and the platform. Extensive\nexperiments demonstrate P2NIA's effectiveness in addressing both issues. In\nsummary, our work introduces a privacy-preserving and non-iterative audit\nscheme that enhances fairness assessments using synthetic or local data,\navoiding the challenges associated with traditional API-based audits.",
    "updated" : "2025-04-01T15:04:58Z",
    "published" : "2025-04-01T15:04:58Z",
    "authors" : [
      {
        "name" : "Jade Garcia Bourrée"
      },
      {
        "name" : "Hadrien Lautraite"
      },
      {
        "name" : "Sébastien Gambs"
      },
      {
        "name" : "Gilles Tredan"
      },
      {
        "name" : "Erwan Le Merrer"
      },
      {
        "name" : "Benoît Rottembourg"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00858v1",
    "title" : "Whispering Under the Eaves: Protecting User Privacy Against Commercial\n  and LLM-powered Automatic Speech Recognition Systems",
    "summary" : "The widespread application of automatic speech recognition (ASR) supports\nlarge-scale voice surveillance, raising concerns about privacy among users. In\nthis paper, we concentrate on using adversarial examples to mitigate\nunauthorized disclosure of speech privacy thwarted by potential eavesdroppers\nin speech communications. While audio adversarial examples have demonstrated\nthe capability to mislead ASR models or evade ASR surveillance, they are\ntypically constructed through time-intensive offline optimization, restricting\ntheir practicality in real-time voice communication. Recent work overcame this\nlimitation by generating universal adversarial perturbations (UAPs) and\nenhancing their transferability for black-box scenarios. However, they\nintroduced excessive noise that significantly degrades audio quality and\naffects human perception, thereby limiting their effectiveness in practical\nscenarios. To address this limitation and protect live users' speech against\nASR systems, we propose a novel framework, AudioShield. Central to this\nframework is the concept of Transferable Universal Adversarial Perturbations in\nthe Latent Space (LS-TUAP). By transferring the perturbations to the latent\nspace, the audio quality is preserved to a large extent. Additionally, we\npropose target feature adaptation to enhance the transferability of UAPs by\nembedding target text features into the perturbations. Comprehensive evaluation\non four commercial ASR APIs (Google, Amazon, iFlytek, and Alibaba), three voice\nassistants, two LLM-powered ASR and one NN-based ASR demonstrates the\nprotection superiority of AudioShield over existing competitors, and both\nobjective and subjective evaluations indicate that AudioShield significantly\nimproves the audio quality. Moreover, AudioShield also shows high effectiveness\nin real-time end-to-end scenarios, and demonstrates strong resilience against\nadaptive countermeasures.",
    "updated" : "2025-04-01T14:49:39Z",
    "published" : "2025-04-01T14:49:39Z",
    "authors" : [
      {
        "name" : "Weifei Jin"
      },
      {
        "name" : "Yuxin Cao"
      },
      {
        "name" : "Junjie Su"
      },
      {
        "name" : "Derui Wang"
      },
      {
        "name" : "Yedi Zhang"
      },
      {
        "name" : "Minhui Xue"
      },
      {
        "name" : "Jie Hao"
      },
      {
        "name" : "Jin Song Dong"
      },
      {
        "name" : "Yixian Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.00411v1",
    "title" : "Forward Learning with Differential Privacy",
    "summary" : "Differential privacy (DP) in deep learning is a critical concern as it\nensures the confidentiality of training data while maintaining model utility.\nExisting DP training algorithms provide privacy guarantees by clipping and then\ninjecting external noise into sample gradients computed by the backpropagation\nalgorithm. Different from backpropagation, forward-learning algorithms based on\nperturbation inherently add noise during the forward pass and utilize\nrandomness to estimate the gradients. Although these algorithms are\nnon-privatized, the introduction of noise during the forward pass indirectly\nprovides internal randomness protection to the model parameters and their\ngradients, suggesting the potential for naturally providing differential\nprivacy. In this paper, we propose a \\blue{privatized} forward-learning\nalgorithm, Differential Private Unified Likelihood Ratio (DP-ULR), and\ndemonstrate its differential privacy guarantees. DP-ULR features a novel batch\nsampling operation with rejection, of which we provide theoretical analysis in\nconjunction with classic differential privacy mechanisms. DP-ULR is also\nunderpinned by a theoretically guided privacy controller that dynamically\nadjusts noise levels to manage privacy costs in each training step. Our\nexperiments indicate that DP-ULR achieves competitive performance compared to\ntraditional differential privacy training algorithms based on backpropagation,\nmaintaining nearly the same privacy loss limits.",
    "updated" : "2025-04-01T04:14:53Z",
    "published" : "2025-04-01T04:14:53Z",
    "authors" : [
      {
        "name" : "Mingqian Feng"
      },
      {
        "name" : "Zeliang Zhang"
      },
      {
        "name" : "Jinyang Jiang"
      },
      {
        "name" : "Yijie Peng"
      },
      {
        "name" : "Chenliang Xu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.02149v1",
    "title" : "Exploring the Privacy and Security Challenges Faced by Migrant Domestic\n  Workers in Chinese Smart Homes",
    "summary" : "The growing use of smart home devices poses considerable privacy and security\nchallenges, especially for individuals like migrant domestic workers (MDWs) who\nmay be surveilled by their employers. This paper explores the privacy and\nsecurity challenges experienced by MDWs in multi-user smart homes through\nin-depth semi-structured interviews with 26 MDWs and 5 staff members of\nagencies that recruit and/or train domestic workers in China. Our findings\nreveal that the relationships between MDWs, their employers, and agencies are\ncharacterized by significant power imbalances, influenced by Chinese cultural\nand social factors (such as Confucianism and collectivism), as well as legal\nones. Furthermore, the widespread and normalized use of surveillance\ntechnologies in China, particularly in public spaces, exacerbates these power\nimbalances, reinforcing a sense of constant monitoring and control. Drawing on\nour findings, we provide recommendations to domestic worker agencies and\npolicymakers to address the privacy and security challenges facing MDWs in\nChinese smart homes.",
    "updated" : "2025-04-02T21:49:15Z",
    "published" : "2025-04-02T21:49:15Z",
    "authors" : [
      {
        "name" : "Shijing He"
      },
      {
        "name" : "Xiao Zhan"
      },
      {
        "name" : "Yaxiong Lei"
      },
      {
        "name" : "Yueyan Liu"
      },
      {
        "name" : "Ruba Abu-Salma"
      },
      {
        "name" : "Jose Such"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.02068v1",
    "title" : "Privacy-Preserving Edge Computing from Pairing-Based Inner Product\n  Functional Encryption",
    "summary" : "Pairing-based inner product functional encryption provides an efficient\ntheoretical construction for privacy-preserving edge computing secured by\nwidely deployed elliptic curve cryptography. In this work, an efficient\nsoftware implementation framework for pairing-based function-hiding inner\nproduct encryption (FHIPE) is presented using the recently proposed and widely\nadopted BLS12-381 pairing-friendly elliptic curve. Algorithmic optimizations\nprovide $\\approx 2.6 \\times$ and $\\approx 3.4 \\times$ speedup in FHIPE\nencryption and decryption respectively, and extensive performance analysis is\npresented using a Raspberry Pi 4B edge device. The proposed optimizations\nenable this implementation framework to achieve performance and ciphertext size\ncomparable to previous work despite being implemented on an edge device with a\nslower processor and supporting a curve at much higher security level with a\nlarger prime field. Practical privacy-preserving edge computing applications\nsuch as encrypted biomedical sensor data classification and secure wireless\nfingerprint-based indoor localization are also demonstrated using the proposed\nimplementation framework.",
    "updated" : "2025-04-02T19:01:10Z",
    "published" : "2025-04-02T19:01:10Z",
    "authors" : [
      {
        "name" : "Utsav Banerjee"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.03173v1",
    "title" : "PPFPL: Cross-silo Privacy-preserving Federated Prototype Learning\n  Against Data Poisoning Attacks on Non-IID Data",
    "summary" : "Privacy-Preserving Federated Learning (PPFL) allows multiple clients to\ncollaboratively train a deep learning model by submitting hidden model updates.\nNonetheless, PPFL is vulnerable to data poisoning attacks due to the\ndistributed training nature of clients. Existing solutions have struggled to\nimprove the performance of cross-silo PPFL in poisoned Non-IID data. To address\nthe issues, this paper proposes a privacy-preserving federated prototype\nlearning framework, named PPFPL, which enhances the cross-silo FL performance\nin poisoned Non-IID data while effectively resisting data poisoning attacks.\nSpecifically, we adopt prototypes as client-submitted model updates to\neliminate the impact of tampered data distribution on federated learning.\nMoreover, we utilize two servers to achieve Byzantine-robust aggregation by\nsecure aggregation protocol, which greatly reduces the impact of malicious\nclients. Theoretical analyses confirm the convergence of PPFPL, and\nexperimental results on publicly available datasets show that PPFPL is\neffective for resisting data poisoning attacks with Non-IID conditions.",
    "updated" : "2025-04-04T05:05:24Z",
    "published" : "2025-04-04T05:05:24Z",
    "authors" : [
      {
        "name" : "Hongliang Zhang"
      },
      {
        "name" : "Jiguo Yu"
      },
      {
        "name" : "Fenghua Xu"
      },
      {
        "name" : "Chunqiang Hu"
      },
      {
        "name" : "Yongzhao Zhang"
      },
      {
        "name" : "Xiaofen Wang"
      },
      {
        "name" : "Zhongyuan Yu"
      },
      {
        "name" : "Xiaosong Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.05202v1",
    "title" : "Infinitely Divisible Noise for Differential Privacy: Nearly Optimal\n  Error in the High $\\varepsilon$ Regime",
    "summary" : "Differential privacy (DP) can be achieved in a distributed manner, where\nmultiple parties add independent noise such that their sum protects the overall\ndataset with DP. A common technique here is for each party to sample their\nnoise from the decomposition of an infinitely divisible distribution. We\nanalyze two mechanisms in this setting: 1) the generalized discrete Laplace\n(GDL) mechanism, whose distribution (which is closed under summation) follows\nfrom differences of i.i.d. negative binomial shares, and 2) the multi-scale\ndiscrete Laplace (MSDLap) mechanism, a novel mechanism following the sum of\nmultiple i.i.d. discrete Laplace shares at different scales.\n  For $\\varepsilon \\geq 1$, our mechanisms can be parameterized to have\n$O\\left(\\Delta^3 e^{-\\varepsilon}\\right)$ and $O\\left(\\min\\left(\\Delta^3\ne^{-\\varepsilon}, \\Delta^2 e^{-2\\varepsilon/3}\\right)\\right)$ MSE,\nrespectively, where $\\Delta$ denote the sensitivity; the latter bound matches\nknown optimality results. We also show a transformation from the discrete\nsetting to the continuous setting, which allows us to transform both mechanisms\nto the continuous setting and thereby achieve the optimal $O\\left(\\Delta^2\ne^{-2\\varepsilon / 3}\\right)$ MSE. To our knowledge, these are the first\ninfinitely divisible additive noise mechanisms that achieve order-optimal MSE\nunder pure DP, so our work shows formally there is no separation in utility\nwhen query-independent noise adding mechanisms are restricted to infinitely\ndivisible noise. For the continuous setting, our result improves upon the Arete\nmechanism from [Pagh and Stausholm, ALT 2022] which gives an MSE of\n$O\\left(\\Delta^2 e^{-\\varepsilon/4}\\right)$. Furthermore, we give an exact\nsampler tuned to efficiently implement the MSDLap mechanism, and we apply our\nresults to improve a state of the art multi-message shuffle DP protocol in the\nhigh $\\varepsilon$ regime.",
    "updated" : "2025-04-07T15:50:46Z",
    "published" : "2025-04-07T15:50:46Z",
    "authors" : [
      {
        "name" : "Charlie Harrison"
      },
      {
        "name" : "Pasin Manurangsi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.04734v1",
    "title" : "Teaching Data Science Students to Sketch Privacy Designs through\n  Heuristics (Extended Technical Report)",
    "summary" : "Recent studies reveal that experienced data practitioners often draw sketches\nto facilitate communication around privacy design concepts. However, there is\nlimited understanding of how we can help novice students develop such\ncommunication skills. This paper studies methods for lowering novice data\nscience students' barriers to creating high-quality privacy sketches. We first\nconducted a need-finding study (N=12) to identify barriers students face when\nsketching privacy designs. We then used a human-centered design approach to\nguide the method development, culminating in three simple, text-based\nheuristics. Our user studies with 24 data science students revealed that simply\npresenting three heuristics to the participants at the beginning of the study\ncan enhance the coverage of privacy-related design decisions in sketches,\nreduce the mental effort required for creating sketches, and improve the\nreadability of the final sketches.",
    "updated" : "2025-04-07T05:12:21Z",
    "published" : "2025-04-07T05:12:21Z",
    "authors" : [
      {
        "name" : "Jinhe Wen"
      },
      {
        "name" : "Yingxi Zhao"
      },
      {
        "name" : "Wenqian Xu"
      },
      {
        "name" : "Yaxing Yao"
      },
      {
        "name" : "Haojian Jin"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.04388v1",
    "title" : "Who's Watching You Zoom? Investigating Privacy of Third-Party Zoom Apps",
    "summary" : "Zoom serves millions of users daily and allows third-party developers to\nintegrate their apps with the Zoom client and reach those users. So far, these\napps' privacy and security aspects, which can access rich audio-visual data\n(among others) from Zoom, have not been scientifically investigated. This paper\nexamines the evolution of the Zoom Marketplace over one year, identifying\ntrends in apps, their data collection behaviors, and the transparency of\nprivacy policies. Our findings include worrisome details about the increasing\nover-collection of user data, non-transparency about purposes and sharing\nbehaviors, and possible non-compliance with relevant laws. We believe these\nfindings will inform future privacy and security research on this platform and\nhelp improve Zoom's app review process and platform policy.",
    "updated" : "2025-04-06T06:48:58Z",
    "published" : "2025-04-06T06:48:58Z",
    "authors" : [
      {
        "name" : "Saharsh Goenka"
      },
      {
        "name" : "Adit Prabhu"
      },
      {
        "name" : "Payge Sakurai"
      },
      {
        "name" : "Mrinaal Ramachandran"
      },
      {
        "name" : "Rakibul Hasan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.04033v1",
    "title" : "Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks\n  and Defenses",
    "summary" : "As machine learning (ML) technologies become more prevalent in\nprivacy-sensitive areas like healthcare and finance, eventually incorporating\nsensitive information in building data-driven algorithms, it is vital to\nscrutinize whether these data face any privacy leakage risks. One potential\nthreat arises from an adversary querying trained models using the public,\nnon-sensitive attributes of entities in the training data to infer their\nprivate, sensitive attributes, a technique known as the attribute inference\nattack. This attack is particularly deceptive because, while it may perform\npoorly in predicting sensitive attributes across the entire dataset, it excels\nat predicting the sensitive attributes of records from a few vulnerable groups,\na phenomenon known as disparate vulnerability. This paper illustrates that an\nadversary can take advantage of this disparity to carry out a series of new\nattacks, showcasing a threat level beyond previous imagination. We first\ndevelop a novel inference attack called the disparity inference attack, which\ntargets the identification of high-risk groups within the dataset. We then\nintroduce two targeted variations of the attribute inference attack that can\nidentify and exploit a vulnerable subset of the training data, marking the\nfirst instances of targeted attacks in this category, achieving significantly\nhigher accuracy than untargeted versions. We are also the first to introduce a\nnovel and effective disparity mitigation technique that simultaneously\npreserves model performance and prevents any risk of targeted attacks.",
    "updated" : "2025-04-05T02:58:37Z",
    "published" : "2025-04-05T02:58:37Z",
    "authors" : [
      {
        "name" : "Ehsanul Kabir"
      },
      {
        "name" : "Lucas Craig"
      },
      {
        "name" : "Shagufta Mehnaz"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2504.03798v1",
    "title" : "An Intelligent and Privacy-Preserving Digital Twin Model for\n  Aging-in-Place",
    "summary" : "The population of older adults is steadily increasing, with a strong\npreference for aging-in-place rather than moving to care facilities.\nConsequently, supporting this growing demographic has become a significant\nglobal challenge. However, facilitating successful aging-in-place is\nchallenging, requiring consideration of multiple factors such as data privacy,\nhealth status monitoring, and living environments to improve health outcomes.\nIn this paper, we propose an unobtrusive sensor system designed for\ninstallation in older adults' homes. Using data from the sensors, our system\nconstructs a digital twin, a virtual representation of events and activities\nthat occurred in the home. The system uses neural network models and decision\nrules to capture residents' activities and living environments. This digital\ntwin enables continuous health monitoring by providing actionable insights into\nresidents' well-being. Our system is designed to be low-cost and\nprivacy-preserving, with the aim of providing green and safe monitoring for the\nhealth of older adults. We have successfully deployed our system in two homes\nover a time period of two months, and our findings demonstrate the feasibility\nand effectiveness of digital twin technology in supporting independent living\nfor older adults. This study highlights that our system could revolutionize\nelder care by enabling personalized interventions, such as lifestyle\nadjustments, medical treatments, or modifications to the residential\nenvironment, to enhance health outcomes.",
    "updated" : "2025-04-04T05:37:08Z",
    "published" : "2025-04-04T05:37:08Z",
    "authors" : [
      {
        "name" : "Yongjie Wang"
      },
      {
        "name" : "Jonathan Cyril Leung"
      },
      {
        "name" : "Ming Chen"
      },
      {
        "name" : "Zhiwei Zeng"
      },
      {
        "name" : "Benny Toh Hsiang Tan"
      },
      {
        "name" : "Yang Qiu"
      },
      {
        "name" : "Zhiqi Shen"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "68T05,",
      "I.2; J.3"
    ]
  }
]