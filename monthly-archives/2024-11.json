[
  {
    "id" : "http://arxiv.org/abs/2411.01583v1",
    "title" : "Trustworthy Federated Learning: Privacy, Security, and Beyond",
    "summary" : "While recent years have witnessed the advancement in big data and Artificial\nIntelligence (AI), it is of much importance to safeguard data privacy and\nsecurity. As an innovative approach, Federated Learning (FL) addresses these\nconcerns by facilitating collaborative model training across distributed data\nsources without transferring raw data. However, the challenges of robust\nsecurity and privacy across decentralized networks catch significant attention\nin dealing with the distributed data in FL. In this paper, we conduct an\nextensive survey of the security and privacy issues prevalent in FL,\nunderscoring the vulnerability of communication links and the potential for\ncyber threats. We delve into various defensive strategies to mitigate these\nrisks, explore the applications of FL across different sectors, and propose\nresearch directions. We identify the intricate security challenges that arise\nwithin the FL frameworks, aiming to contribute to the development of secure and\nefficient FL systems.",
    "updated" : "2024-11-03T14:18:01Z",
    "published" : "2024-11-03T14:18:01Z",
    "authors" : [
      {
        "name" : "Chunlu Chen"
      },
      {
        "name" : "Ji Liu"
      },
      {
        "name" : "Haowen Tan"
      },
      {
        "name" : "Xingjian Li"
      },
      {
        "name" : "Kevin I-Kai Wang"
      },
      {
        "name" : "Peng Li"
      },
      {
        "name" : "Kouichi Sakurai"
      },
      {
        "name" : "Dejing Dou"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01471v1",
    "title" : "A Practical and Privacy-Preserving Framework for Real-World Large\n  Language Model Services",
    "summary" : "Large language models (LLMs) have demonstrated exceptional capabilities in\ntext understanding and generation, and they are increasingly being utilized\nacross various domains to enhance productivity. However, due to the high costs\nof training and maintaining these models, coupled with the fact that some LLMs\nare proprietary, individuals often rely on online AI as a Service (AIaaS)\nprovided by LLM companies. This business model poses significant privacy risks,\nas service providers may exploit users' trace patterns and behavioral data. In\nthis paper, we propose a practical and privacy-preserving framework that\nensures user anonymity by preventing service providers from linking requests to\nthe individuals who submit them. Our framework is built on partially blind\nsignatures, which guarantee the unlinkability of user requests. Furthermore, we\nintroduce two strategies tailored to both subscription-based and API-based\nservice models, ensuring the protection of both users' privacy and service\nproviders' interests. The framework is designed to integrate seamlessly with\nexisting LLM systems, as it does not require modifications to the underlying\narchitectures. Experimental results demonstrate that our framework incurs\nminimal computation and communication overhead, making it a feasible solution\nfor real-world applications.",
    "updated" : "2024-11-03T07:40:28Z",
    "published" : "2024-11-03T07:40:28Z",
    "authors" : [
      {
        "name" : "Yu Mao"
      },
      {
        "name" : "Xueping Liao"
      },
      {
        "name" : "Wei Liu"
      },
      {
        "name" : "Anjia Yang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01447v1",
    "title" : "Privacy-Preserving Customer Churn Prediction Model in the Context of\n  Telecommunication Industry",
    "summary" : "Data is the main fuel of a successful machine learning model. A dataset may\ncontain sensitive individual records e.g. personal health records, financial\ndata, industrial information, etc. Training a model using this sensitive data\nhas become a new privacy concern when someone uses third-party cloud computing.\nTrained models also suffer privacy attacks which leads to the leaking of\nsensitive information of the training data. This study is conducted to preserve\nthe privacy of training data in the context of customer churn prediction\nmodeling for the telecommunications industry (TCI). In this work, we propose a\nframework for privacy-preserving customer churn prediction (PPCCP) model in the\ncloud environment. We have proposed a novel approach which is a combination of\nGenerative Adversarial Networks (GANs) and adaptive Weight-of-Evidence (aWOE).\nSynthetic data is generated from GANs, and aWOE is applied on the synthetic\ntraining dataset before feeding the data to the classification algorithms. Our\nexperiments were carried out using eight different machine learning (ML)\nclassifiers on three openly accessible datasets from the telecommunication\nsector. We then evaluated the performance using six commonly employed\nevaluation metrics. In addition to presenting a data privacy analysis, we also\nperformed a statistical significance test. The training and prediction\nprocesses achieve data privacy and the prediction classifiers achieve high\nprediction performance (87.1\\% in terms of F-Measure for GANs-aWOE based\nNa\\\"{\\i}ve Bayes model). In contrast to earlier studies, our suggested approach\ndemonstrates a prediction enhancement of up to 28.9\\% and 27.9\\% in terms of\naccuracy and F-measure, respectively.",
    "updated" : "2024-11-03T06:08:59Z",
    "published" : "2024-11-03T06:08:59Z",
    "authors" : [
      {
        "name" : "Joydeb Kumar Sana"
      },
      {
        "name" : "M Sohel Rahman"
      },
      {
        "name" : "M Saifur Rahman"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01357v1",
    "title" : "WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy\n  Principles",
    "summary" : "In this paper, we introduce WaKA (Wasserstein K-nearest neighbors\nAttribution), a novel attribution method that leverages principles from the\nLiRA (Likelihood Ratio Attack) framework and applies them to \\( k \\)-nearest\nneighbors classifiers (\\( k \\)-NN). WaKA efficiently measures the contribution\nof individual data points to the model's loss distribution, analyzing every\npossible \\( k \\)-NN that can be constructed using the training set, without\nrequiring sampling or shadow model training. WaKA can be used \\emph{a\nposteriori} as a membership inference attack (MIA) to assess privacy risks, and\n\\emph{a priori} for data minimization and privacy influence measurement. Thus,\nWaKA can be seen as bridging the gap between data attribution and membership\ninference attack (MIA) literature by distinguishing between the value of a data\npoint and its privacy risk. For instance, we show that self-attribution values\nare more strongly correlated with the attack success rate than the contribution\nof a point to model generalization. WaKA's different usages were also evaluated\nacross diverse real-world datasets, demonstrating performance very close to\nLiRA when used as an MIA on \\( k \\)-NN classifiers, but with greater\ncomputational efficiency.",
    "updated" : "2024-11-02T20:27:51Z",
    "published" : "2024-11-02T20:27:51Z",
    "authors" : [
      {
        "name" : "Patrick Mesana"
      },
      {
        "name" : "Clément Bénesse"
      },
      {
        "name" : "Hadrien Lautraite"
      },
      {
        "name" : "Gilles Caporossi"
      },
      {
        "name" : "Sébastien Gambs"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01344v1",
    "title" : "Can Humans Oversee Agents to Prevent Privacy Leakage? A Study on Privacy\n  Awareness, Preferences, and Trust in Language Model Agents",
    "summary" : "Language model (LM) agents that act on users' behalf for personal tasks can\nboost productivity, but are also susceptible to unintended privacy leakage\nrisks. We present the first study on people's capacity to oversee the privacy\nimplications of the LM agents. By conducting a task-based survey (N=300), we\ninvestigate how people react to and assess the response generated by LM agents\nfor asynchronous interpersonal communication tasks, compared with a response\nthey wrote. We found that people may favor the agent response with more privacy\nleakage over the response they drafted or consider both good, leading to an\nincreased harmful disclosure from 15.7% to 55.0%. We further uncovered distinct\npatterns of privacy behaviors, attitudes, and preferences, and the nuanced\ninteractions between privacy considerations and other factors. Our findings\nshed light on designing agentic systems that enable privacy-preserving\ninteractions and achieve bidirectional alignment on privacy preferences to help\nusers calibrate trust.",
    "updated" : "2024-11-02T19:15:42Z",
    "published" : "2024-11-02T19:15:42Z",
    "authors" : [
      {
        "name" : "Zhiping Zhang"
      },
      {
        "name" : "Bingcan Guo"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01308v1",
    "title" : "ECG-PPS: Privacy Preserving Disease Diagnosis and Monitoring System for\n  Real-Time ECG Signal",
    "summary" : "This study introduces the development of a state of the art, real time ECG\nmonitoring and analysis system, incorporating cutting edge medical technology\nand innovative data security measures. Our system performs three distinct\nfunctions thaat real time ECG monitoring and disease detection, encrypted\nstorage and synchronized visualization, and statistical analysis on encrypted\ndata. At its core, the system uses a three lead ECG preamplifier connected\nthrough a serial port to capture, display, and record real time ECG data. These\nsignals are securely stored in the cloud using robust encryption methods.\nAuthorized medical personnel can access and decrypt this data on their\ncomputers, with AES encryption ensuring synchronized real time data tracking\nand visualization. Furthermore, the system performs statistical operations on\nthe ECG data stored in the cloud without decrypting it, using Fully Homomorphic\nEncryption (FHE). This enables privacy preserving data analysis while ensuring\nthe security and confidentiality of patient information. By integrating these\nindependent functions, our system significantly enhances the security and\nefficiency of health monitoring. It supports critical tasks such as disease\ndetection, patient monitoring, and preliminary intervention, all while\nupholding stringent data privacy standards. We provided detailed discussions on\nthe system's architecture, hardware configuration, software implementation, and\nclinical performance. The results highlight the potential of this system to\nimprove patient care through secure and efficient ECG monitoring and analysis.\nThis work represents a significant leap forward in medical technology. By\nincorporating FHE into both data transmission and storage processes, we ensure\ncontinuous encryption of data throughout its lifecycle while enabling real time\ndisease diagnosis.",
    "updated" : "2024-11-02T17:03:25Z",
    "published" : "2024-11-02T17:03:25Z",
    "authors" : [
      {
        "name" : "Beyazit Bestami Yuksel"
      },
      {
        "name" : "Ayse Yilmazer Metin"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01140v1",
    "title" : "Privacy-Preserving Federated Learning with Differentially Private\n  Hyperdimensional Computing",
    "summary" : "Federated Learning (FL) is essential for efficient data exchange in Internet\nof Things (IoT) environments, as it trains Machine Learning (ML) models locally\nand shares only model updates. However, FL is vulnerable to privacy threats\nlike model inversion and membership inference attacks, which can expose\nsensitive training data. To address these privacy concerns, Differential\nPrivacy (DP) mechanisms are often applied. Yet, adding DP noise to black-box ML\nmodels degrades performance, especially in dynamic IoT systems where\ncontinuous, lifelong FL learning accumulates excessive noise over time. To\nmitigate this issue, we introduce Federated HyperDimensional computing with\nPrivacy-preserving (FedHDPrivacy), an eXplainable Artificial Intelligence (XAI)\nframework that combines the neuro-symbolic paradigm with DP. FedHDPrivacy\ncarefully manages the balance between privacy and performance by theoretically\ntracking cumulative noise from previous rounds and adding only the necessary\nincremental noise to meet privacy requirements. In a real-world case study\ninvolving in-process monitoring of manufacturing machining operations,\nFedHDPrivacy demonstrates robust performance, outperforming standard FL\nframeworks-including Federated Averaging (FedAvg), Federated Stochastic\nGradient Descent (FedSGD), Federated Proximal (FedProx), Federated Normalized\nAveraging (FedNova), and Federated Adam (FedAdam)-by up to 38%. FedHDPrivacy\nalso shows potential for future enhancements, such as multimodal data fusion.",
    "updated" : "2024-11-02T05:00:44Z",
    "published" : "2024-11-02T05:00:44Z",
    "authors" : [
      {
        "name" : "Fardin Jalil Piran"
      },
      {
        "name" : "Zhiling Chen"
      },
      {
        "name" : "Mohsen Imani"
      },
      {
        "name" : "Farhad Imani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01076v1",
    "title" : "Privacy Risks of Speculative Decoding in Large Language Models",
    "summary" : "Speculative decoding in large language models (LLMs) accelerates token\ngeneration by speculatively predicting multiple tokens cheaply and verifying\nthem in parallel, and has been widely deployed. In this paper, we provide the\nfirst study demonstrating the privacy risks of speculative decoding. We observe\nthat input-dependent patterns of correct and incorrect predictions can be\nleaked out to an adversary monitoring token generation times and packet sizes,\nleading to privacy breaches. By observing the pattern of correctly and\nincorrectly speculated tokens, we show that a malicious adversary can\nfingerprint queries and learn private user inputs with more than $90\\%$\naccuracy across three different speculative decoding techniques - BiLD (almost\n$100\\%$ accuracy), LADE (up to $92\\%$ accuracy), and REST (up to $95\\%$\naccuracy). We show that an adversary can also leak out confidential\nintellectual property used to design these techniques, such as data from\ndata-stores used for prediction (in REST) at a rate of more than $25$ tokens\nper second, or even hyper-parameters used for prediction (in LADE). We also\ndiscuss mitigation strategies, such as aggregating tokens across multiple\niterations and padding packets with additional bytes, to avoid such privacy or\nconfidentiality breaches.",
    "updated" : "2024-11-01T23:14:30Z",
    "published" : "2024-11-01T23:14:30Z",
    "authors" : [
      {
        "name" : "Jiankun Wei"
      },
      {
        "name" : "Abdulrahman Abdulrazzag"
      },
      {
        "name" : "Tianchen Zhang"
      },
      {
        "name" : "Adel Muursepp"
      },
      {
        "name" : "Gururaj Saileshwar"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01050v1",
    "title" : "BACSA: A Bias-Aware Client Selection Algorithm for Privacy-Preserving\n  Federated Learning in Wireless Healthcare Networks",
    "summary" : "Federated Learning (FL) has emerged as a transformative approach in\nhealthcare, enabling collaborative model training across decentralized data\nsources while preserving user privacy. However, performance of FL rapidly\ndegrades in practical scenarios due to the inherent bias in non Independent and\nIdentically distributed (non-IID) data among participating clients, which poses\nsignificant challenges to model accuracy and generalization. Therefore, we\npropose the Bias-Aware Client Selection Algorithm (BACSA), which detects user\nbias and strategically selects clients based on their bias profiles. In\naddition, the proposed algorithm considers privacy preservation, fairness and\nconstraints of wireless network environments, making it suitable for sensitive\nhealthcare applications where Quality of Service (QoS), privacy and security\nare paramount. Our approach begins with a novel method for detecting user bias\nby analyzing model parameters and correlating them with the distribution of\nclass-specific data samples. We then formulate a mixed-integer non-linear\nclient selection problem leveraging the detected bias, alongside wireless\nnetwork constraints, to optimize FL performance. We demonstrate that BACSA\nimproves convergence and accuracy, compared to existing benchmarks, through\nevaluations on various data distributions, including Dirichlet and\nclass-constrained scenarios. Additionally, we explore the trade-offs between\naccuracy, fairness, and network constraints, indicating the adaptability and\nrobustness of BACSA to address diverse healthcare applications.",
    "updated" : "2024-11-01T21:34:43Z",
    "published" : "2024-11-01T21:34:43Z",
    "authors" : [
      {
        "name" : "Sushilkumar Yadav"
      },
      {
        "name" : "Irem Bor-Yaliniz"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03053v1",
    "title" : "Gradient-Guided Conditional Diffusion Models for Private Image\n  Reconstruction: Analyzing Adversarial Impacts of Differential Privacy and\n  Denoising",
    "summary" : "We investigate the construction of gradient-guided conditional diffusion\nmodels for reconstructing private images, focusing on the adversarial interplay\nbetween differential privacy noise and the denoising capabilities of diffusion\nmodels. While current gradient-based reconstruction methods struggle with\nhigh-resolution images due to computational complexity and prior knowledge\nrequirements, we propose two novel methods that require minimal modifications\nto the diffusion model's generation process and eliminate the need for prior\nknowledge. Our approach leverages the strong image generation capabilities of\ndiffusion models to reconstruct private images starting from randomly generated\nnoise, even when a small amount of differentially private noise has been added\nto the gradients. We also conduct a comprehensive theoretical analysis of the\nimpact of differential privacy noise on the quality of reconstructed images,\nrevealing the relationship among noise magnitude, the architecture of attacked\nmodels, and the attacker's reconstruction capability. Additionally, extensive\nexperiments validate the effectiveness of our proposed methods and the accuracy\nof our theoretical findings, suggesting new directions for privacy risk\nauditing using conditional diffusion models.",
    "updated" : "2024-11-05T12:39:21Z",
    "published" : "2024-11-05T12:39:21Z",
    "authors" : [
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Hong Chen"
      },
      {
        "name" : "Guolong Zheng"
      },
      {
        "name" : "Xu Yang"
      },
      {
        "name" : "Xun Yi"
      },
      {
        "name" : "Hua Wang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.02926v1",
    "title" : "Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic\n  Encryption for Collaborative Anti-Money Laundering",
    "summary" : "Combating money laundering has become increasingly complex with the rise of\ncybercrime and digitalization of financial transactions. Graph-based machine\nlearning techniques have emerged as promising tools for Anti-Money Laundering\n(AML) detection, capturing intricate relationships within money laundering\nnetworks. However, the effectiveness of AML solutions is hindered by data silos\nwithin financial institutions, limiting collaboration and overall efficacy.\nThis research presents a novel privacy-preserving approach for collaborative\nAML machine learning, facilitating secure data sharing across institutions and\nborders while preserving privacy and regulatory compliance. Leveraging Fully\nHomomorphic Encryption (FHE), computations are directly performed on encrypted\ndata, ensuring the confidentiality of financial data. Notably, FHE over the\nTorus (TFHE) was integrated with graph-based machine learning using Zama\nConcrete ML. The research contributes two key privacy-preserving pipelines.\nFirst, the development of a privacy-preserving Graph Neural Network (GNN)\npipeline was explored. Optimization techniques like quantization and pruning\nwere used to render the GNN FHE-compatible. Second, a privacy-preserving\ngraph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was\nsuccessfully developed. Experiments demonstrated strong predictive performance,\nwith the XGBoost model consistently achieving over 99% accuracy, F1-score,\nprecision, and recall on the balanced AML dataset in both unencrypted and\nFHE-encrypted inference settings. On the imbalanced dataset, the incorporation\nof graph-based features improved the F1-score by 8%. The research highlights\nthe need to balance the trade-off between privacy and computational efficiency.",
    "updated" : "2024-11-05T09:13:53Z",
    "published" : "2024-11-05T09:13:53Z",
    "authors" : [
      {
        "name" : "Fabrianne Effendi"
      },
      {
        "name" : "Anupam Chattopadhyay"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.02622v1",
    "title" : "Pseudo-Probability Unlearning: Towards Efficient and Privacy-Preserving\n  Machine Unlearning",
    "summary" : "Machine unlearning--enabling a trained model to forget specific data--is\ncrucial for addressing biased data and adhering to privacy regulations like the\nGeneral Data Protection Regulation (GDPR)'s \"right to be forgotten\". Recent\nworks have paid little attention to privacy concerns, leaving the data intended\nfor forgetting vulnerable to membership inference attacks. Moreover, they often\ncome with high computational overhead. In this work, we propose\nPseudo-Probability Unlearning (PPU), a novel method that enables models to\nforget data efficiently and in a privacy-preserving manner. Our method replaces\nthe final-layer output probabilities of the neural network with\npseudo-probabilities for the data to be forgotten. These pseudo-probabilities\nfollow either a uniform distribution or align with the model's overall\ndistribution, enhancing privacy and reducing risk of membership inference\nattacks. Our optimization strategy further refines the predictive probability\ndistributions and updates the model's weights accordingly, ensuring effective\nforgetting with minimal impact on the model's overall performance. Through\ncomprehensive experiments on multiple benchmarks, our method achieves over 20%\nimprovements in forgetting error compared to the state-of-the-art.\nAdditionally, our method enhances privacy by preventing the forgotten set from\nbeing inferred to around random guesses.",
    "updated" : "2024-11-04T21:27:06Z",
    "published" : "2024-11-04T21:27:06Z",
    "authors" : [
      {
        "name" : "Zihao Zhao"
      },
      {
        "name" : "Yijiang Li"
      },
      {
        "name" : "Yuchen Yang"
      },
      {
        "name" : "Wenqing Zhang"
      },
      {
        "name" : "Nuno Vasconcelos"
      },
      {
        "name" : "Yinzhi Cao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01076v2",
    "title" : "Privacy Risks of Speculative Decoding in Large Language Models",
    "summary" : "Speculative decoding in large language models (LLMs) accelerates token\ngeneration by speculatively predicting multiple tokens cheaply and verifying\nthem in parallel, and has been widely deployed. In this paper, we provide the\nfirst study demonstrating the privacy risks of speculative decoding. We observe\nthat input-dependent patterns of correct and incorrect predictions can be\nleaked out to an adversary monitoring token generation times and packet sizes,\nleading to privacy breaches. By observing the pattern of correctly and\nincorrectly speculated tokens, we show that a malicious adversary can\nfingerprint queries and learn private user inputs with more than $90\\%$\naccuracy across three different speculative decoding techniques - REST (almost\n$100\\%$ accuracy), LADE (up to $92\\%$ accuracy), and BiLD (up to $95\\%$\naccuracy). We show that an adversary can also leak out confidential\nintellectual property used to design these techniques, such as data from\ndata-stores used for prediction (in REST) at a rate of more than $25$ tokens\nper second, or even hyper-parameters used for prediction (in LADE). We also\ndiscuss mitigation strategies, such as aggregating tokens across multiple\niterations and padding packets with additional bytes, to avoid such privacy or\nconfidentiality breaches.",
    "updated" : "2024-11-05T15:03:45Z",
    "published" : "2024-11-01T23:14:30Z",
    "authors" : [
      {
        "name" : "Jiankun Wei"
      },
      {
        "name" : "Abdulrahman Abdulrazzag"
      },
      {
        "name" : "Tianchen Zhang"
      },
      {
        "name" : "Adel Muursepp"
      },
      {
        "name" : "Gururaj Saileshwar"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03914v1",
    "title" : "Game-Theoretic Machine Unlearning: Mitigating Extra Privacy Leakage",
    "summary" : "With the extensive use of machine learning technologies, data providers\nencounter increasing privacy risks. Recent legislation, such as GDPR, obligates\norganizations to remove requested data and its influence from a trained model.\nMachine unlearning is an emerging technique designed to enable machine learning\nmodels to erase users' private information. Although several efficient machine\nunlearning schemes have been proposed, these methods still have limitations.\nFirst, removing the contributions of partial data may lead to model performance\ndegradation. Second, discrepancies between the original and generated unlearned\nmodels can be exploited by attackers to obtain target sample's information,\nresulting in additional privacy leakage risks. To address above challenges, we\nproposed a game-theoretic machine unlearning algorithm that simulates the\ncompetitive relationship between unlearning performance and privacy protection.\nThis algorithm comprises unlearning and privacy modules. The unlearning module\npossesses a loss function composed of model distance and classification error,\nwhich is used to derive the optimal strategy. The privacy module aims to make\nit difficult for an attacker to infer membership information from the unlearned\ndata, thereby reducing the privacy leakage risk during the unlearning process.\nAdditionally, the experimental results on real-world datasets demonstrate that\nthis game-theoretic unlearning algorithm's effectiveness and its ability to\ngenerate an unlearned model with a performance similar to that of the retrained\none while mitigating extra privacy leakage risks.",
    "updated" : "2024-11-06T13:47:04Z",
    "published" : "2024-11-06T13:47:04Z",
    "authors" : [
      {
        "name" : "Hengzhu Liu"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Lefeng Zhang"
      },
      {
        "name" : "Ping Xiong"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03847v1",
    "title" : "A Novel Access Control and Privacy-Enhancing Approach for Models in Edge\n  Computing",
    "summary" : "With the widespread adoption of edge computing technologies and the\nincreasing prevalence of deep learning models in these environments, the\nsecurity risks and privacy threats to models and data have grown more acute.\nAttackers can exploit various techniques to illegally obtain models or misuse\ndata, leading to serious issues such as intellectual property infringement and\nprivacy breaches. Existing model access control technologies primarily rely on\ntraditional encryption and authentication methods; however, these approaches\nexhibit significant limitations in terms of flexibility and adaptability in\ndynamic environments. Although there have been advancements in model\nwatermarking techniques for marking model ownership, they remain limited in\ntheir ability to proactively protect intellectual property and prevent\nunauthorized access. To address these challenges, we propose a novel model\naccess control method tailored for edge computing environments. This method\nleverages image style as a licensing mechanism, embedding style recognition\ninto the model's operational framework to enable intrinsic access control.\nConsequently, models deployed on edge platforms are designed to correctly infer\nonly on license data with specific style, rendering them ineffective on any\nother data. By restricting the input data to the edge model, this approach not\nonly prevents attackers from gaining unauthorized access to the model but also\nenhances the privacy of data on terminal devices. We conducted extensive\nexperiments on benchmark datasets, including MNIST, CIFAR-10, and FACESCRUB,\nand the results demonstrate that our method effectively prevents unauthorized\naccess to the model while maintaining accuracy. Additionally, the model shows\nstrong resistance against attacks such as forged licenses and fine-tuning.\nThese results underscore the method's usability, security, and robustness.",
    "updated" : "2024-11-06T11:37:30Z",
    "published" : "2024-11-06T11:37:30Z",
    "authors" : [
      {
        "name" : "Peihao Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03730v1",
    "title" : "NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document\n  VQA",
    "summary" : "The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA)\ncompetition challenged the community to develop provably private and\ncommunication-efficient solutions in a federated setting for a real-life use\ncase: invoice processing. The competition introduced a dataset of real invoice\ndocuments, along with associated questions and answers requiring information\nextraction and reasoning over the document images. Thereby, it brings together\nresearchers and expertise from the document analysis, privacy, and federated\nlearning communities. Participants fine-tuned a pre-trained, state-of-the-art\nDocument Visual Question Answering model provided by the organizers for this\nnew domain, mimicking a typical federated invoice processing setup. The base\nmodel is a multi-modal generative language model, and sensitive information\ncould be exposed through either the visual or textual input modality.\nParticipants proposed elegant solutions to reduce communication costs while\nmaintaining a minimum utility threshold in track 1 and to protect all\ninformation from each document provider using differential privacy in track 2.\nThe competition served as a new testbed for developing and testing private\nfederated learning methods, simultaneously raising awareness about privacy\nwithin the document image analysis and recognition community. Ultimately, the\ncompetition analysis provides best practices and recommendations for\nsuccessfully running privacy-focused federated learning challenges in the\nfuture.",
    "updated" : "2024-11-06T07:51:19Z",
    "published" : "2024-11-06T07:51:19Z",
    "authors" : [
      {
        "name" : "Marlon Tobaben"
      },
      {
        "name" : "Mohamed Ali Souibgui"
      },
      {
        "name" : "Rubèn Tito"
      },
      {
        "name" : "Khanh Nguyen"
      },
      {
        "name" : "Raouf Kerkouche"
      },
      {
        "name" : "Kangsoo Jung"
      },
      {
        "name" : "Joonas Jälkö"
      },
      {
        "name" : "Lei Kang"
      },
      {
        "name" : "Andrey Barsky"
      },
      {
        "name" : "Vincent Poulain d'Andecy"
      },
      {
        "name" : "Aurélie Joseph"
      },
      {
        "name" : "Aashiq Muhamed"
      },
      {
        "name" : "Kevin Kuo"
      },
      {
        "name" : "Virginia Smith"
      },
      {
        "name" : "Yusuke Yamasaki"
      },
      {
        "name" : "Takumi Fukami"
      },
      {
        "name" : "Kenta Niwa"
      },
      {
        "name" : "Iifan Tyou"
      },
      {
        "name" : "Hiro Ishii"
      },
      {
        "name" : "Rio Yokota"
      },
      {
        "name" : "Ragul N"
      },
      {
        "name" : "Rintu Kutum"
      },
      {
        "name" : "Josep Llados"
      },
      {
        "name" : "Ernest Valveny"
      },
      {
        "name" : "Antti Honkela"
      },
      {
        "name" : "Mario Fritz"
      },
      {
        "name" : "Dimosthenis Karatzas"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03633v1",
    "title" : "Privacy-Preserving Resilient Vector Consensus",
    "summary" : "This paper studies privacy-preserving resilient vector consensus in\nmulti-agent systems against faulty agents, where normal agents can achieve\nconsensus within the convex hull of their initial states while protecting state\nvectors from being disclosed. Specifically, we consider a modification of an\nexisting algorithm known as Approximate Distributed Robust Convergence Using\nCenterpoints (ADRC), i.e., Privacy-Preserving ADRC (PP-ADRC). Under PP-ADRC,\neach normal agent introduces multivariate Gaussian noise to its state during\neach iteration. We first provide sufficient conditions to ensure that all\nnormal agents' states can achieve mean square convergence under PP-ADRC. Then,\nwe analyze convergence accuracy from two perspectives, i.e., the Mahalanobis\ndistance of the final value from its expectation and the Hausdorff distance\nbased alteration of the convex hull caused by noise when only partial\ndimensions are added with noise. Then, we employ concentrated geo-privacy to\ncharacterize privacy preservation and conduct a thorough comparison with\ndifferential privacy. Finally, numerical simulations demonstrate the\ntheoretical results.",
    "updated" : "2024-11-06T03:17:29Z",
    "published" : "2024-11-06T03:17:29Z",
    "authors" : [
      {
        "name" : "Bing Liu"
      },
      {
        "name" : "Chengcheng Zhao"
      },
      {
        "name" : "Li Chai"
      },
      {
        "name" : "Peng Cheng"
      },
      {
        "name" : "Jiming Chen"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03582v1",
    "title" : "Privacy Preserving Mechanisms for Coordinating Airspace Usage in\n  Advanced Air Mobility",
    "summary" : "Advanced Air Mobility (AAM) operations are expected to transform air\ntransportation while challenging current air traffic management practices. By\nintroducing a novel market-based mechanism, we address the problem of on-demand\nallocation of capacity-constrained airspace to AAM vehicles with heterogeneous\nand private valuations. We model airspace and air infrastructure as a\ncollection of contiguous regions with constraints on the number of vehicles\nthat simultaneously enter, stay, or exit each region. Vehicles request access\nto the airspace with trajectories spanning multiple regions at different times.\nWe use the graph structure of our airspace model to formulate the allocation\nproblem as a path allocation problem on a time-extended graph. To ensure the\ncost information of AAM vehicles remains private, we introduce a novel\nmechanism that allocates each vehicle a budget of \"air-credits\" and anonymously\ncharges prices for traversing the edges of the time-extended graph. We seek to\ncompute a competitive equilibrium that ensures that: (i) capacity constraints\nare satisfied, (ii) a strictly positive resource price implies that the sector\ncapacity is fully utilized, and (iii) the allocation is integral and optimal\nfor each AAM vehicle given current prices, without requiring access to\nindividual vehicle utilities. However, a competitive equilibrium with integral\nallocations may not always exist. We provide sufficient conditions for the\nexistence and computation of a fractional-competitive equilibrium, where\nallocations can be fractional. Building on these theoretical insights, we\npropose a distributed, iterative, two-step algorithm that: 1) computes a\nfractional competitive equilibrium, and 2) derives an integral allocation from\nthis equilibrium. We validate the effectiveness of our approach in allocating\ntrajectories for two emerging urban air mobility services: drone delivery and\nair taxis.",
    "updated" : "2024-11-06T00:47:54Z",
    "published" : "2024-11-06T00:47:54Z",
    "authors" : [
      {
        "name" : "Chinmay Maheshwari"
      },
      {
        "name" : "Maria G. Mendoza"
      },
      {
        "name" : "Victoria Marie Tuck"
      },
      {
        "name" : "Pan-Yang Su"
      },
      {
        "name" : "Victor L. Qin"
      },
      {
        "name" : "Sanjit A. Seshia"
      },
      {
        "name" : "Hamsa Balakrishnan"
      },
      {
        "name" : "Shankar Sastry"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY",
      "91B03, 91A68, 90B06, 90C27"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03351v1",
    "title" : "Tabular Data Synthesis with Differential Privacy: A Survey",
    "summary" : "Data sharing is a prerequisite for collaborative innovation, enabling\norganizations to leverage diverse datasets for deeper insights. In real-world\napplications like FinTech and Smart Manufacturing, transactional data, often in\ntabular form, are generated and analyzed for insight generation. However, such\ndatasets typically contain sensitive personal/business information, raising\nprivacy concerns and regulatory risks. Data synthesis tackles this by\ngenerating artificial datasets that preserve the statistical characteristics of\nreal data, removing direct links to individuals. However, attackers can still\ninfer sensitive information using background knowledge. Differential privacy\noffers a solution by providing provable and quantifiable privacy protection.\nConsequently, differentially private data synthesis has emerged as a promising\napproach to privacy-aware data sharing. This paper provides a comprehensive\noverview of existing differentially private tabular data synthesis methods,\nhighlighting the unique challenges of each generation model for generating\ntabular data under differential privacy constraints. We classify the methods\ninto statistical and deep learning-based approaches based on their generation\nmodels, discussing them in both centralized and distributed environments. We\nevaluate and compare those methods within each category, highlighting their\nstrengths and weaknesses in terms of utility, privacy, and computational\ncomplexity. Additionally, we present and discuss various evaluation methods for\nassessing the quality of the synthesized data, identify research gaps in the\nfield and directions for future research.",
    "updated" : "2024-11-04T06:32:48Z",
    "published" : "2024-11-04T06:32:48Z",
    "authors" : [
      {
        "name" : "Mengmeng Yang"
      },
      {
        "name" : "Chi-Hung Chi"
      },
      {
        "name" : "Kwok-Yan Lam"
      },
      {
        "name" : "Jie Feng"
      },
      {
        "name" : "Taolin Guo"
      },
      {
        "name" : "Wei Ni"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.04710v1",
    "title" : "Differential Privacy Overview and Fundamental Techniques",
    "summary" : "This chapter is meant to be part of the book \"Differential Privacy in\nArtificial Intelligence: From Theory to Practice\" and provides an introduction\nto Differential Privacy. It starts by illustrating various attempts to protect\ndata privacy, emphasizing where and why they failed, and providing the key\ndesiderata of a robust privacy definition. It then defines the key actors,\ntasks, and scopes that make up the domain of privacy-preserving data analysis.\nFollowing that, it formalizes the definition of Differential Privacy and its\ninherent properties, including composition, post-processing immunity, and group\nprivacy. The chapter also reviews the basic techniques and mechanisms commonly\nused to implement Differential Privacy in its pure and approximate forms.",
    "updated" : "2024-11-07T13:52:11Z",
    "published" : "2024-11-07T13:52:11Z",
    "authors" : [
      {
        "name" : "Ferdinando Fioretto"
      },
      {
        "name" : "Pascal Van Hentenryck"
      },
      {
        "name" : "Juba Ziani"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.04509v1",
    "title" : "FedDP: Privacy-preserving method based on federated learning for\n  histopathology image segmentation",
    "summary" : "Hematoxylin and Eosin (H&E) staining of whole slide images (WSIs) is\nconsidered the gold standard for pathologists and medical practitioners for\ntumor diagnosis, surgical planning, and post-operative assessment. With the\nrapid advancement of deep learning technologies, the development of numerous\nmodels based on convolutional neural networks and transformer-based models has\nbeen applied to the precise segmentation of WSIs. However, due to privacy\nregulations and the need to protect patient confidentiality, centralized\nstorage and processing of image data are impractical. Training a centralized\nmodel directly is challenging to implement in medical settings due to these\nprivacy concerns.This paper addresses the dispersed nature and privacy\nsensitivity of medical image data by employing a federated learning framework,\nallowing medical institutions to collaboratively learn while protecting patient\nprivacy. Additionally, to address the issue of original data reconstruction\nthrough gradient inversion during the federated learning training process,\ndifferential privacy introduces noise into the model updates, preventing\nattackers from inferring the contributions of individual samples, thereby\nprotecting the privacy of the training data.Experimental results show that the\nproposed method, FedDP, minimally impacts model accuracy while effectively\nsafeguarding the privacy of cancer pathology image data, with only a slight\ndecrease in Dice, Jaccard, and Acc indices by 0.55%, 0.63%, and 0.42%,\nrespectively. This approach facilitates cross-institutional collaboration and\nknowledge sharing while protecting sensitive data privacy, providing a viable\nsolution for further research and application in the medical field.",
    "updated" : "2024-11-07T08:02:58Z",
    "published" : "2024-11-07T08:02:58Z",
    "authors" : [
      {
        "name" : "Liangrui Pan"
      },
      {
        "name" : "Mao Huang"
      },
      {
        "name" : "Lian Wang"
      },
      {
        "name" : "Pinle Qin"
      },
      {
        "name" : "Shaoliang Peng"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.04490v1",
    "title" : "Smoke Screens and Scapegoats: The Reality of General Data Protection\n  Regulation Compliance -- Privacy and Ethics in the Case of Replika AI",
    "summary" : "Currently artificial intelligence (AI)-enabled chatbots are capturing the\nhearts and imaginations of the public at large. Chatbots that users can build\nand personalize, as well as pre-designed avatars ready for users' selection,\nall of these are on offer in applications to provide social companionship,\nfriends and even love. These systems, however, have demonstrated challenges on\nthe privacy and ethics front. This paper takes a critical approach towards\nexamining the intricacies of these issues within AI companion services. We\nchose Replika as a case and employed close reading to examine the service's\nprivacy policy. We additionally analyze articles from public media about the\ncompany and its practices to gain insight into the trustworthiness and\nintegrity of the information provided in the policy. The aim is to ascertain\nwhether seeming General Data Protection Regulation (GDPR) compliance equals\nreliability of required information, or whether the area of GDPR compliance in\nitself is one riddled with ethical challenges. The paper contributes to a\ngrowing body of scholarship on ethics and privacy related matters in the sphere\nof social chatbots. The results reveal that despite privacy notices, data\ncollection practices might harvest personal data without users' full awareness.\nCross-textual comparison reveals that privacy notice information does not fully\ncorrespond with other information sources.",
    "updated" : "2024-11-07T07:36:19Z",
    "published" : "2024-11-07T07:36:19Z",
    "authors" : [
      {
        "name" : "Joni-Roy Piispanen"
      },
      {
        "name" : "Tinja Myllyviita"
      },
      {
        "name" : "Ville Vakkuri"
      },
      {
        "name" : "Rebekah Rousi"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.05743v1",
    "title" : "Free Record-Level Privacy Risk Evaluation Through Artifact-Based Methods",
    "summary" : "Membership inference attacks (MIAs) are widely used to empirically assess the\nprivacy risks of samples used to train a target machine learning model.\nState-of-the-art methods however require training hundreds of shadow models,\nwith the same size and architecture of the target model, solely to evaluate the\nprivacy risk. While one might be able to afford this for small models, the cost\noften becomes prohibitive for medium and large models.\n  We here instead propose a novel approach to identify the at-risk samples\nusing only artifacts available during training, with little to no additional\ncomputational overhead. Our method analyzes individual per-sample loss traces\nand uses them to identify the vulnerable data samples. We demonstrate the\neffectiveness of our artifact-based approach through experiments on the CIFAR10\ndataset, showing high precision in identifying vulnerable samples as determined\nby a SOTA shadow model-based MIA (LiRA). Impressively, our method reaches the\nsame precision as another SOTA MIA when measured against LiRA, despite it being\norders of magnitude cheaper. We then show LT-IQR to outperform alternative loss\naggregation methods, perform ablation studies on hyperparameters, and validate\nthe robustness of our method to the target metric. Finally, we study the\nevolution of the vulnerability score distribution throughout training as a\nmetric for model-level risk assessment.",
    "updated" : "2024-11-08T18:04:41Z",
    "published" : "2024-11-08T18:04:41Z",
    "authors" : [
      {
        "name" : "Joseph Pollock"
      },
      {
        "name" : "Igor Shilov"
      },
      {
        "name" : "Euodia Dodd"
      },
      {
        "name" : "Yves-Alexandre de Montjoye"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.05733v1",
    "title" : "Differential Privacy Under Class Imbalance: Methods and Empirical\n  Insights",
    "summary" : "Imbalanced learning occurs in classification settings where the distribution\nof class-labels is highly skewed in the training data, such as when predicting\nrare diseases or in fraud detection. This class imbalance presents a\nsignificant algorithmic challenge, which can be further exacerbated when\nprivacy-preserving techniques such as differential privacy are applied to\nprotect sensitive training data. Our work formalizes these challenges and\nprovides a number of algorithmic solutions. We consider DP variants of\npre-processing methods that privately augment the original dataset to reduce\nthe class imbalance; these include oversampling, SMOTE, and private synthetic\ndata generation. We also consider DP variants of in-processing techniques,\nwhich adjust the learning algorithm to account for the imbalance; these include\nmodel bagging, class-weighted empirical risk minimization and class-weighted\ndeep learning. For each method, we either adapt an existing imbalanced learning\ntechnique to the private setting or demonstrate its incompatibility with\ndifferential privacy. Finally, we empirically evaluate these privacy-preserving\nimbalanced learning methods under various data and distributional settings. We\nfind that private synthetic data methods perform well as a data pre-processing\nstep, while class-weighted ERMs are an alternative in higher-dimensional\nsettings where private synthetic data suffers from the curse of dimensionality.",
    "updated" : "2024-11-08T17:46:56Z",
    "published" : "2024-11-08T17:46:56Z",
    "authors" : [
      {
        "name" : "Lucas Rosenblatt"
      },
      {
        "name" : "Yuliia Lut"
      },
      {
        "name" : "Eitan Turok"
      },
      {
        "name" : "Marco Avella-Medina"
      },
      {
        "name" : "Rachel Cummings"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "68P27",
      "I.2.0; F.0; G.3; J.0"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.05483v1",
    "title" : "The Limits of Differential Privacy in Online Learning",
    "summary" : "Differential privacy (DP) is a formal notion that restricts the privacy\nleakage of an algorithm when running on sensitive data, in which\nprivacy-utility trade-off is one of the central problems in private data\nanalysis. In this work, we investigate the fundamental limits of differential\nprivacy in online learning algorithms and present evidence that separates three\ntypes of constraints: no DP, pure DP, and approximate DP. We first describe a\nhypothesis class that is online learnable under approximate DP but not online\nlearnable under pure DP under the adaptive adversarial setting. This indicates\nthat approximate DP must be adopted when dealing with adaptive adversaries. We\nthen prove that any private online learner must make an infinite number of\nmistakes for almost all hypothesis classes. This essentially generalizes\nprevious results and shows a strong separation between private and non-private\nsettings since a finite mistake bound is always attainable (as long as the\nclass is online learnable) when there is no privacy requirement.",
    "updated" : "2024-11-08T11:21:31Z",
    "published" : "2024-11-08T11:21:31Z",
    "authors" : [
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Peng Ye"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.05320v1",
    "title" : "Privacy Protection Framework against Unauthorized Sensing in the 5.8 GHz\n  ISM Band",
    "summary" : "Unauthorized sensing activities pose an increasing threat to individual\nprivacy, particularly in the industrial, scientific, and medical (ISM) band\nwhere regulatory frameworks remain limited. This paper presents a novel signal\nprocess methodology to monitor and counter unauthorized sensing activities.\nSpecifically, we model the pedestrian trajectories as a random process. Then,\nwe leverage the Cram\\'er-Rao bound (CRB) to evaluate sensing performance and\nmodel it as sampling error of such a random process. Through simulation, we\nverify the accuracy of monitoring unauthorized sensing activities in urban\nscenarios, and validate the effectiveness of corresponding mitigation\nstrategies.",
    "updated" : "2024-11-08T04:25:55Z",
    "published" : "2024-11-08T04:25:55Z",
    "authors" : [
      {
        "name" : "Zexin Fang"
      },
      {
        "name" : "Bin Han"
      },
      {
        "name" : "Hans D. Schotten"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.05167v1",
    "title" : "EPIC: Enhancing Privacy through Iterative Collaboration",
    "summary" : "Advancements in genomics technology lead to a rising volume of viral (e.g.,\nSARS-CoV-2) sequence data, resulting in increased usage of machine learning\n(ML) in bioinformatics. Traditional ML techniques require centralized data\ncollection and processing, posing challenges in realistic healthcare scenarios.\nAdditionally, privacy, ownership, and stringent regulation issues exist when\npooling medical data into centralized storage to train a powerful deep learning\n(DL) model. The Federated learning (FL) approach overcomes such issues by\nsetting up a central aggregator server and a shared global model. It also\nfacilitates data privacy by extracting knowledge while keeping the actual data\nprivate. This work proposes a cutting-edge Privacy enhancement through\nIterative Collaboration (EPIC) architecture. The network is divided and\ndistributed between local and centralized servers. We demonstrate the EPIC\napproach to resolve a supervised classification problem to estimate SARS-CoV-2\ngenomic sequence data lineage without explicitly transferring raw sequence\ndata. We aim to create a universal decentralized optimization framework that\nallows various data holders to work together and converge to a single\npredictive model. The findings demonstrate that privacy-preserving strategies\ncan be successfully used with aggregation approaches without materially\naltering the degree of learning convergence. Finally, we highlight a few\npotential issues and prospects for study in FL-based approaches to healthcare\napplications.",
    "updated" : "2024-11-07T20:10:34Z",
    "published" : "2024-11-07T20:10:34Z",
    "authors" : [
      {
        "name" : "Prakash Chourasia"
      },
      {
        "name" : "Heramb Lonkar"
      },
      {
        "name" : "Sarwan Ali"
      },
      {
        "name" : "Murray Patterson"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.05034v1",
    "title" : "Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion",
    "summary" : "Embeddings have become a cornerstone in the functionality of large language\nmodels (LLMs) due to their ability to transform text data into rich, dense\nnumerical representations that capture semantic and syntactic properties. These\nembedding vector databases serve as the long-term memory of LLMs, enabling\nefficient handling of a wide range of natural language processing tasks.\nHowever, the surge in popularity of embedding vector databases in LLMs has been\naccompanied by significant concerns about privacy leakage. Embedding vector\ndatabases are particularly vulnerable to embedding inversion attacks, where\nadversaries can exploit the embeddings to reverse-engineer and extract\nsensitive information from the original text data. Existing defense mechanisms\nhave shown limitations, often struggling to balance security with the\nperformance of downstream tasks. To address these challenges, we introduce\nEguard, a novel defense mechanism designed to mitigate embedding inversion\nattacks. Eguard employs a transformer-based projection network and text mutual\ninformation optimization to safeguard embeddings while preserving the utility\nof LLMs. Our approach significantly reduces privacy risks, protecting over 95%\nof tokens from inversion while maintaining high performance across downstream\ntasks consistent with original embeddings.",
    "updated" : "2024-11-06T14:42:41Z",
    "published" : "2024-11-06T14:42:41Z",
    "authors" : [
      {
        "name" : "Tiantian Liu"
      },
      {
        "name" : "Hongwei Yao"
      },
      {
        "name" : "Tong Wu"
      },
      {
        "name" : "Zhan Qin"
      },
      {
        "name" : "Feng Lin"
      },
      {
        "name" : "Kui Ren"
      },
      {
        "name" : "Chun Chen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.07128v1",
    "title" : "ZT-RIC:A Zero Trust RIC Framework for ensuring data Privacy and\n  Confidentiality in Open RAN",
    "summary" : "The advancement of 5G and NextG networks through Open Radio Access Network\n(O-RAN) architecture enables a shift toward virtualized, modular, and\ndisaggregated configurations. A core component of O-RAN is the RAN Intelligent\nController (RIC), which manages RAN using machine learning-driven xApps that\naccess sensitive data from RAN and User Equipment (UE), stored in the near\nReal-Time RIC (Near-RT RIC) database. This shared, open environment increases\nthe risk of unauthorized data exposure. To address these concerns, this paper\nproposes a zero-trust RIC (ZT-RIC) framework that preserves data privacy across\nthe RIC platform, including the RIC database, xApps, and E2 interface. ZT-RIC\nemploys Inner Product Functional Encryption (IPFE) to encrypt RAN/UE data at\nthe base station, preventing leaks through the E2 interface and shared\ndatabase. Additionally, ZT-RIC enables xApps to perform inference on encrypted\ndata without exposing sensitive information. For evaluation, a state-of-the-art\nInterClass xApp, which detects jamming signals using RAN key performance\nmetrics (KPMs), is implemented. Testing on an LTE/5G O-RAN testbed shows that\nZT-RIC preserves data confidentiality while achieving 97.9% accuracy in jamming\ndetection and meeting sub-second latency requirements, with a round-trip time\n(RTT) of 0.527 seconds.",
    "updated" : "2024-11-11T16:59:22Z",
    "published" : "2024-11-11T16:59:22Z",
    "authors" : [
      {
        "name" : "Diana Lin"
      },
      {
        "name" : "Samarth Bhargav"
      },
      {
        "name" : "Azuka Chiejina"
      },
      {
        "name" : "Mohamed I. Ibrahem"
      },
      {
        "name" : "Vijay K. Shah"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.07070v1",
    "title" : "On Active Privacy Auditing in Supervised Fine-tuning for White-Box\n  Language Models",
    "summary" : "The pretraining and fine-tuning approach has become the leading technique for\nvarious NLP applications. However, recent studies reveal that fine-tuning data,\ndue to their sensitive nature, domain-specific characteristics, and\nidentifiability, pose significant privacy concerns. To help develop more\nprivacy-resilient fine-tuning models, we introduce a novel active privacy\nauditing framework, dubbed Parsing, designed to identify and quantify privacy\nleakage risks during the supervised fine-tuning (SFT) of language models (LMs).\nThe framework leverages improved white-box membership inference attacks (MIAs)\nas the core technology, utilizing novel learning objectives and a two-stage\npipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the\nexposure of privacy risks. Additionally, we have improved the effectiveness of\nMIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our\nresearch aims to provide the SFT community of LMs with a reliable, ready-to-use\nprivacy auditing tool, and to offer valuable insights into safeguarding privacy\nduring the fine-tuning process. Experimental results confirm the framework's\nefficiency across various models and tasks, emphasizing notable privacy\nconcerns in the fine-tuning process. Project code available for\nhttps://github.com/mapleleavesss/PARSING.",
    "updated" : "2024-11-11T15:46:07Z",
    "published" : "2024-11-11T15:46:07Z",
    "authors" : [
      {
        "name" : "Qian Sun"
      },
      {
        "name" : "Hanpeng Wu"
      },
      {
        "name" : "Xi Sheryl Zhang"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.06613v1",
    "title" : "Are Neuromorphic Architectures Inherently Privacy-preserving? An\n  Exploratory Study",
    "summary" : "While machine learning (ML) models are becoming mainstream, especially in\nsensitive application areas, the risk of data leakage has become a growing\nconcern. Attacks like membership inference (MIA) have shown that trained models\ncan reveal sensitive data, jeopardizing confidentiality. While traditional\nArtificial Neural Networks (ANNs) dominate ML applications, neuromorphic\narchitectures, specifically Spiking Neural Networks (SNNs), are emerging as\npromising alternatives due to their low power consumption and event-driven\nprocessing, akin to biological neurons. Privacy in ANNs is well-studied;\nhowever, little work has explored the privacy-preserving properties of SNNs.\nThis paper examines whether SNNs inherently offer better privacy. Using MIAs,\nwe assess the privacy resilience of SNNs versus ANNs across diverse datasets.\nWe analyze the impact of learning algorithms (surrogate gradient and\nevolutionary), frameworks (snnTorch, TENNLab, LAVA), and parameters on SNN\nprivacy. Our findings show that SNNs consistently outperform ANNs in privacy\npreservation, with evolutionary algorithms offering additional resilience. For\ninstance, on CIFAR-10, SNNs achieve an AUC of 0.59, significantly lower than\nANNs' 0.82, and on CIFAR-100, SNNs maintain an AUC of 0.58 compared to ANNs'\n0.88. Additionally, we explore the privacy-utility trade-off with\nDifferentially Private Stochastic Gradient Descent (DPSGD), finding that SNNs\nsustain less accuracy loss than ANNs under similar privacy constraints.",
    "updated" : "2024-11-10T22:18:53Z",
    "published" : "2024-11-10T22:18:53Z",
    "authors" : [
      {
        "name" : "Ayana Moshruba"
      },
      {
        "name" : "Ihsen Alouani"
      },
      {
        "name" : "Maryam Parsa"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.06549v1",
    "title" : "In-Context Learning for Preserving Patient Privacy: A Framework for\n  Synthesizing Realistic Patient Portal Messages",
    "summary" : "Since the COVID-19 pandemic, clinicians have seen a large and sustained\ninflux in patient portal messages, significantly contributing to clinician\nburnout. To the best of our knowledge, there are no large-scale public patient\nportal messages corpora researchers can use to build tools to optimize\nclinician portal workflows. Informed by our ongoing work with a regional\nhospital, this study introduces an LLM-powered framework for configurable and\nrealistic patient portal message generation. Our approach leverages few-shot\ngrounded text generation, requiring only a small number of de-identified\npatient portal messages to help LLMs better match the true style and tone of\nreal data. Clinical experts in our team deem this framework as HIPAA-friendly,\nunlike existing privacy-preserving approaches to synthetic text generation\nwhich cannot guarantee all sensitive attributes will be protected. Through\nextensive quantitative and human evaluation, we show that our framework\nproduces data of higher quality than comparable generation methods as well as\nall related datasets. We believe this work provides a path forward for (i) the\nrelease of large-scale synthetic patient message datasets that are\nstylistically similar to ground-truth samples and (ii) HIPAA-friendly data\ngeneration which requires minimal human de-identification efforts.",
    "updated" : "2024-11-10T18:06:55Z",
    "published" : "2024-11-10T18:06:55Z",
    "authors" : [
      {
        "name" : "Joseph Gatto"
      },
      {
        "name" : "Parker Seegmiller"
      },
      {
        "name" : "Timothy E. Burdick"
      },
      {
        "name" : "Sarah Masud Preum"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.06513v1",
    "title" : "PRISM: Privacy-preserving Inter-Site MRI Harmonization via Disentangled\n  Representation Learning",
    "summary" : "Multi-site MRI studies often suffer from site-specific variations arising\nfrom differences in methodology, hardware, and acquisition protocols, thereby\ncompromising accuracy and reliability in clinical AI/ML tasks. We present PRISM\n(Privacy-preserving Inter-Site MRI Harmonization), a novel Deep Learning\nframework for harmonizing structural brain MRI across multiple sites while\npreserving data privacy. PRISM employs a dual-branch autoencoder with\ncontrastive learning and variational inference to disentangle anatomical\nfeatures from style and site-specific variations, enabling unpaired image\ntranslation without traveling subjects or multiple MRI modalities. Our modular\ndesign allows harmonization to any target site and seamless integration of new\nsites without the need for retraining or fine-tuning. Using multi-site\nstructural MRI data, we demonstrate PRISM's effectiveness in downstream tasks\nsuch as brain tissue segmentation and validate its harmonization performance\nthrough multiple experiments. Our framework addresses key challenges in medical\nAI/ML, including data privacy, distribution shifts, model generalizability and\ninterpretability. Code is available at https://github.com/saranggalada/PRISM",
    "updated" : "2024-11-10T16:29:23Z",
    "published" : "2024-11-10T16:29:23Z",
    "authors" : [
      {
        "name" : "Sarang Galada"
      },
      {
        "name" : "Tanurima Halder"
      },
      {
        "name" : "Kunal Deo"
      },
      {
        "name" : "Ram P Krish"
      },
      {
        "name" : "Kshitij Jadhav"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.06317v1",
    "title" : "Harpocrates: Oblivious Privacy in a Statically Typed World",
    "summary" : "In this paper, we introduce Harpocrates, a compiler plugin and a framework\npair for Scala that binds the privacy policies to the data during data creation\nin form of oblivious membranes. Harpocrates eliminates raw data for a policy\nprotected type from the application, ensuring it can only exist in protected\nform and centralizes the policy checking to the policy declaration site, making\nthe privacy logic easy to maintain and verify. Instead of approaching privacy\nfrom an information flow verification perspective, Harpocrates allow the data\nto flow freely throughout the application, inside the policy membranes but\nenforces the policies when the data is tried to be accessed, mutated,\ndeclassified or passed through the application boundary. The centralization of\nthe policies allow the maintainers to change the enforced logic simply by\nupdating a single function while keeping the rest of the application oblivious\nto the change. Especially in a setting where the data definition is shared by\nmultiple applications, the publisher can update the policies without requiring\nthe dependent applications to make any changes beyond updating the dependency\nversion.",
    "updated" : "2024-11-10T00:28:58Z",
    "published" : "2024-11-10T00:28:58Z",
    "authors" : [
      {
        "name" : "Sinan Pehlivanoglu"
      },
      {
        "name" : "Malte Schwarzkopf"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.06263v1",
    "title" : "Federated Split Learning for Human Activity Recognition with\n  Differential Privacy",
    "summary" : "This paper proposes a novel intelligent human activity recognition (HAR)\nframework based on a new design of Federated Split Learning (FSL) with\nDifferential Privacy (DP) over edge networks. Our FSL-DP framework leverages\nboth accelerometer and gyroscope data, achieving significant improvements in\nHAR accuracy. The evaluation includes a detailed comparison between traditional\nFederated Learning (FL) and our FSL framework, showing that the FSL framework\noutperforms FL models in both accuracy and loss metrics. Additionally, we\nexamine the privacy-performance trade-off under different data settings in the\nDP mechanism, highlighting the balance between privacy guarantees and model\naccuracy. The results also indicate that our FSL framework achieves faster\ncommunication times per training round compared to traditional FL, further\nemphasizing its efficiency and effectiveness. This work provides valuable\ninsight and a novel framework which was tested on a real-life dataset.",
    "updated" : "2024-11-09T19:32:23Z",
    "published" : "2024-11-09T19:32:23Z",
    "authors" : [
      {
        "name" : "Josue Ndeko"
      },
      {
        "name" : "Shaba Shaon"
      },
      {
        "name" : "Aubrey Beal"
      },
      {
        "name" : "Avimanyu Sahoo"
      },
      {
        "name" : "Dinh C. Nguyen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.06107v1",
    "title" : "A capacity renting framework for shared energy storage considering\n  peer-to-peer energy trading of prosumers with privacy protection",
    "summary" : "Shared energy storage systems (ESS) present a promising solution to the\ntemporal imbalance between energy generation from renewable distributed\ngenerators (DGs) and the power demands of prosumers. However, as DG penetration\nrates rise, spatial energy imbalances become increasingly significant,\nnecessitating the integration of peer-to-peer (P2P) energy trading within the\nshared ESS framework. Two key challenges emerge in this context: the absence of\neffective mechanisms and the greater difficulty for privacy protection due to\nincreased data communication. This research proposes a capacity renting\nframework for shared ESS considering P2P energy trading of prosumers. In the\nproposed framework, prosumers can participate in P2P energy trading and rent\ncapacities from shared ESS. A generalized Nash game is formulated to model the\ntrading process and the competitive interactions among prosumers, and the\nvariational equilibrium of the game is proved to be equivalent to the optimal\nsolution of a quadratic programming (QP) problem. To address the privacy\nprotection concern, the problem is solved using the alternating direction\nmethod of multipliers (ADMM) with the Paillier cryptosystem. Finally, numerical\nsimulations demonstrate the impact of P2P energy trading on the shared ESS\nframework and validate the effectiveness of the proposed privacy-preserving\nalgorithm.",
    "updated" : "2024-11-09T08:01:17Z",
    "published" : "2024-11-09T08:01:17Z",
    "authors" : [
      {
        "name" : "Yingcong Sun"
      },
      {
        "name" : "Laijun Chen"
      },
      {
        "name" : "Yue Chen"
      },
      {
        "name" : "Mingrui Tang"
      },
      {
        "name" : "Shengwei Mei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.05901v1",
    "title" : "ViT Enhanced Privacy-Preserving Secure Medical Data Sharing and\n  Classification",
    "summary" : "Privacy-preserving and secure data sharing are critical for medical image\nanalysis while maintaining accuracy and minimizing computational overhead are\nalso crucial. Applying existing deep neural networks (DNNs) to encrypted\nmedical data is not always easy and often compromises performance and security.\nTo address these limitations, this research introduces a secure framework\nconsisting of a learnable encryption method based on the block-pixel operation\nto encrypt the data and subsequently integrate it with the Vision Transformer\n(ViT). The proposed framework ensures data privacy and security by creating\nunique scrambling patterns per key, providing robust performance against\nleading bit attacks and minimum difference attacks.",
    "updated" : "2024-11-08T16:33:20Z",
    "published" : "2024-11-08T16:33:20Z",
    "authors" : [
      {
        "name" : "Al Amin"
      },
      {
        "name" : "Kamrul Hasan"
      },
      {
        "name" : "Sharif Ullah"
      },
      {
        "name" : "M. Shamim Hossain"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.02926v2",
    "title" : "Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic\n  Encryption for Collaborative Anti-Money Laundering",
    "summary" : "Combating money laundering has become increasingly complex with the rise of\ncybercrime and digitalization of financial transactions. Graph-based machine\nlearning techniques have emerged as promising tools for Anti-Money Laundering\n(AML) detection, capturing intricate relationships within money laundering\nnetworks. However, the effectiveness of AML solutions is hindered by data silos\nwithin financial institutions, limiting collaboration and overall efficacy.\nThis research presents a novel privacy-preserving approach for collaborative\nAML machine learning, facilitating secure data sharing across institutions and\nborders while preserving privacy and regulatory compliance. Leveraging Fully\nHomomorphic Encryption (FHE), computations are directly performed on encrypted\ndata, ensuring the confidentiality of financial data. Notably, FHE over the\nTorus (TFHE) was integrated with graph-based machine learning using Zama\nConcrete ML. The research contributes two key privacy-preserving pipelines.\nFirst, the development of a privacy-preserving Graph Neural Network (GNN)\npipeline was explored. Optimization techniques like quantization and pruning\nwere used to render the GNN FHE-compatible. Second, a privacy-preserving\ngraph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was\nsuccessfully developed. Experiments demonstrated strong predictive performance,\nwith the XGBoost model consistently achieving over 99% accuracy, F1-score,\nprecision, and recall on the balanced AML dataset in both unencrypted and\nFHE-encrypted inference settings. On the imbalanced dataset, the incorporation\nof graph-based features improved the F1-score by 8%. The research highlights\nthe need to balance the trade-off between privacy and computational efficiency.",
    "updated" : "2024-11-11T16:47:58Z",
    "published" : "2024-11-05T09:13:53Z",
    "authors" : [
      {
        "name" : "Fabrianne Effendi"
      },
      {
        "name" : "Anupam Chattopadhyay"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.07806v1",
    "title" : "Federated Low-Rank Adaptation with Differential Privacy over Wireless\n  Networks",
    "summary" : "Fine-tuning large pre-trained foundation models (FMs) on distributed edge\ndevices presents considerable computational and privacy challenges. Federated\nfine-tuning (FedFT) mitigates some privacy issues by facilitating collaborative\nmodel training without the need to share raw data. To lessen the computational\nburden on resource-limited devices, combining low-rank adaptation (LoRA) with\nfederated learning enables parameter-efficient fine-tuning. Additionally, the\nsplit FedFT architecture partitions an FM between edge devices and a central\nserver, reducing the necessity for complete model deployment on individual\ndevices. However, the risk of privacy eavesdropping attacks in FedFT remains a\nconcern, particularly in sensitive areas such as healthcare and finance. In\nthis paper, we propose a split FedFT framework with differential privacy (DP)\nover wireless networks, where the inherent wireless channel noise in the uplink\ntransmission is utilized to achieve DP guarantees without adding an extra\nartificial noise. We shall investigate the impact of the wireless noise on\nconvergence performance of the proposed framework. We will also show that by\nupdating only one of the low-rank matrices in the split FedFT with DP, the\nproposed method can mitigate the noise amplification effect. Simulation results\nwill demonstrate that the proposed framework achieves higher accuracy under\nstrict privacy budgets compared to baseline methods.",
    "updated" : "2024-11-12T14:01:08Z",
    "published" : "2024-11-12T14:01:08Z",
    "authors" : [
      {
        "name" : "Tianqu Kang"
      },
      {
        "name" : "Zixin Wang"
      },
      {
        "name" : "Hengtao He"
      },
      {
        "name" : "Jun Zhang"
      },
      {
        "name" : "Shenghui Song"
      },
      {
        "name" : "Khaled B. Letaief"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.07691v1",
    "title" : "New Emerged Security and Privacy of Pre-trained Model: a Survey and\n  Outlook",
    "summary" : "Thanks to the explosive growth of data and the development of computational\nresources, it is possible to build pre-trained models that can achieve\noutstanding performance on various tasks, such as neural language processing,\ncomputer vision, and more. Despite their powerful capabilities, pre-trained\nmodels have also sparked attention to the emerging security challenges\nassociated with their real-world applications. Security and privacy issues,\nsuch as leaking privacy information and generating harmful responses, have\nseriously undermined users' confidence in these powerful models. Concerns are\ngrowing as model performance improves dramatically. Researchers are eager to\nexplore the unique security and privacy issues that have emerged, their\ndistinguishing factors, and how to defend against them. However, the current\nliterature lacks a clear taxonomy of emerging attacks and defenses for\npre-trained models, which hinders a high-level and comprehensive understanding\nof these questions. To fill the gap, we conduct a systematical survey on the\nsecurity risks of pre-trained models, proposing a taxonomy of attack and\ndefense methods based on the accessibility of pre-trained models' input and\nweights in various security test scenarios. This taxonomy categorizes attacks\nand defenses into No-Change, Input-Change, and Model-Change approaches. With\nthe taxonomy analysis, we capture the unique security and privacy issues of\npre-trained models, categorizing and summarizing existing security issues based\non their characteristics. In addition, we offer a timely and comprehensive\nreview of each category's strengths and limitations. Our survey concludes by\nhighlighting potential new research opportunities in the security and privacy\nof pre-trained models.",
    "updated" : "2024-11-12T10:15:33Z",
    "published" : "2024-11-12T10:15:33Z",
    "authors" : [
      {
        "name" : "Meng Yang"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Chi Liu"
      },
      {
        "name" : "WanLei Zhou"
      },
      {
        "name" : "Shui Yu"
      },
      {
        "name" : "Philip S. Yu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.07468v1",
    "title" : "Privacy-Preserving Verifiable Neural Network Inference Service",
    "summary" : "Machine learning has revolutionized data analysis and pattern recognition,\nbut its resource-intensive training has limited accessibility. Machine Learning\nas a Service (MLaaS) simplifies this by enabling users to delegate their data\nsamples to an MLaaS provider and obtain the inference result using a\npre-trained model. Despite its convenience, leveraging MLaaS poses significant\nprivacy and reliability concerns to the client. Specifically, sensitive\ninformation from the client inquiry data can be leaked to an adversarial MLaaS\nprovider. Meanwhile, the lack of a verifiability guarantee can potentially\nresult in biased inference results or even unfair payment issues. While\nexisting trustworthy machine learning techniques, such as those relying on\nverifiable computation or secure computation, offer solutions to privacy and\nreliability concerns, they fall short of simultaneously protecting the privacy\nof client data and providing provable inference verifiability.\n  In this paper, we propose vPIN, a privacy-preserving and verifiable CNN\ninference scheme that preserves privacy for client data samples while ensuring\nverifiability for the inference. vPIN makes use of partial homomorphic\nencryption and commit-and-prove succinct non-interactive argument of knowledge\ntechniques to achieve desirable security properties. In vPIN, we develop\nvarious optimization techniques to minimize the proving circuit for homomorphic\ninference evaluation thereby, improving the efficiency and performance of our\ntechnique. We fully implemented and evaluated our vPIN scheme on standard\ndatasets (e.g., MNIST, CIFAR-10). Our experimental results show that vPIN\nachieves high efficiency in terms of proving time, verification time, and proof\nsize, while providing client data privacy guarantees and provable\nverifiability.",
    "updated" : "2024-11-12T01:09:52Z",
    "published" : "2024-11-12T01:09:52Z",
    "authors" : [
      {
        "name" : "Arman Riasi"
      },
      {
        "name" : "Jorge Guajardo"
      },
      {
        "name" : "Thang Hoang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.07070v2",
    "title" : "On Active Privacy Auditing in Supervised Fine-tuning for White-Box\n  Language Models",
    "summary" : "The pretraining and fine-tuning approach has become the leading technique for\nvarious NLP applications. However, recent studies reveal that fine-tuning data,\ndue to their sensitive nature, domain-specific characteristics, and\nidentifiability, pose significant privacy concerns. To help develop more\nprivacy-resilient fine-tuning models, we introduce a novel active privacy\nauditing framework, dubbed Parsing, designed to identify and quantify privacy\nleakage risks during the supervised fine-tuning (SFT) of language models (LMs).\nThe framework leverages improved white-box membership inference attacks (MIAs)\nas the core technology, utilizing novel learning objectives and a two-stage\npipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the\nexposure of privacy risks. Additionally, we have improved the effectiveness of\nMIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our\nresearch aims to provide the SFT community of LMs with a reliable, ready-to-use\nprivacy auditing tool, and to offer valuable insights into safeguarding privacy\nduring the fine-tuning process. Experimental results confirm the framework's\nefficiency across various models and tasks, emphasizing notable privacy\nconcerns in the fine-tuning process. Project code available for\nhttps://anonymous.4open.science/r/PARSING-4817/.",
    "updated" : "2024-11-12T04:12:32Z",
    "published" : "2024-11-11T15:46:07Z",
    "authors" : [
      {
        "name" : "Qian Sun"
      },
      {
        "name" : "Hanpeng Wu"
      },
      {
        "name" : "Xi Sheryl Zhang"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.08635v1",
    "title" : "Synthesis with Privacy Against an Observer",
    "summary" : "We study automatic synthesis of systems that interact with their environment\nand maintain privacy against an observer to the interaction. The system and the\nenvironment interact via sets $I$ and $O$ of input and output signals. The\ninput to the synthesis problem contains, in addition to a specification, also a\nlist of secrets, a function $cost: I\\cup O\\rightarrow\\mathbb{N}$, which maps\neach signal to the cost of hiding it, and a bound $b\\in\\mathbb{N}$ on the\nbudget that the system may use for hiding of signals. The desired output is an\n$(I/O)$-transducer $T$ and a set $H\\subseteq I\\cup O$ of signals that respects\nthe bound on the budget, thus $\\sum_{s\\in H} cost(s)\\leq b$, such that for\nevery possible interaction of $T$, the generated computation satisfies the\nspecification, yet an observer, from whom the signals in $H$ are hidden, cannot\nevaluate the secrets.\n  We first show that the problem's complexity is 2EXPTIME-complete for\nspecifications and secrets in LTL, making it no harder than synthesis without\nprivacy requirements. We then analyze the complexity further, isolating the two\naspects that do not exist in traditional synthesis: the need to hide secret\nvalues and the need to choose the set $H$. We do this by studying settings in\nwhich traditional synthesis is solvable in polynomial time -- when the\nspecification formalism is deterministic automata and when the system is closed\n-- and show that each of these aspects adds an exponential blow-up in\ncomplexity. We continue and study bounded synthesis with privacy, where the\ninput includes a bound on the synthesized transducer size, as well as a variant\nof the problem in which the observer has knowledge, either about the\nspecification or about the system, which can be helpful in evaluating the\nsecrets. Additionally, we study certified privacy, where the synthesis\nalgorithm provides certification that the secrets remain hidden.",
    "updated" : "2024-11-13T14:22:06Z",
    "published" : "2024-11-13T14:22:06Z",
    "authors" : [
      {
        "name" : "Orna Kupferman"
      },
      {
        "name" : "Ofer Leshkowitz"
      },
      {
        "name" : "Namma Shamash Halevy"
      }
    ],
    "categories" : [
      "cs.LO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.07468v2",
    "title" : "Privacy-Preserving Verifiable Neural Network Inference Service",
    "summary" : "Machine learning has revolutionized data analysis and pattern recognition,\nbut its resource-intensive training has limited accessibility. Machine Learning\nas a Service (MLaaS) simplifies this by enabling users to delegate their data\nsamples to an MLaaS provider and obtain the inference result using a\npre-trained model. Despite its convenience, leveraging MLaaS poses significant\nprivacy and reliability concerns to the client. Specifically, sensitive\ninformation from the client inquiry data can be leaked to an adversarial MLaaS\nprovider. Meanwhile, the lack of a verifiability guarantee can potentially\nresult in biased inference results or even unfair payment issues. While\nexisting trustworthy machine learning techniques, such as those relying on\nverifiable computation or secure computation, offer solutions to privacy and\nreliability concerns, they fall short of simultaneously protecting the privacy\nof client data and providing provable inference verifiability.\n  In this paper, we propose vPIN, a privacy-preserving and verifiable CNN\ninference scheme that preserves privacy for client data samples while ensuring\nverifiability for the inference. vPIN makes use of partial homomorphic\nencryption and commit-and-prove succinct non-interactive argument of knowledge\ntechniques to achieve desirable security properties. In vPIN, we develop\nvarious optimization techniques to minimize the proving circuit for homomorphic\ninference evaluation thereby, improving the efficiency and performance of our\ntechnique. We fully implemented and evaluated our vPIN scheme on standard\ndatasets (e.g., MNIST, CIFAR-10). Our experimental results show that vPIN\nachieves high efficiency in terms of proving time, verification time, and proof\nsize, while providing client data privacy guarantees and provable\nverifiability.",
    "updated" : "2024-11-13T03:07:36Z",
    "published" : "2024-11-12T01:09:52Z",
    "authors" : [
      {
        "name" : "Arman Riasi"
      },
      {
        "name" : "Jorge Guajardo"
      },
      {
        "name" : "Thang Hoang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.11713v1",
    "title" : "FLMarket: Enabling Privacy-preserved Pre-training Data Pricing for\n  Federated Learning",
    "summary" : "Federated Learning (FL), as a mainstream privacy-preserving machine learning\nparadigm, offers promising solutions for privacy-critical domains such as\nhealthcare and finance. Although extensive efforts have been dedicated from\nboth academia and industry to improve the vanilla FL, little work focuses on\nthe data pricing mechanism. In contrast to the straightforward in/post-training\npricing techniques, we study a more difficult problem of pre-training pricing\nwithout direct information from the learning process. We propose FLMarket that\nintegrates a two-stage, auction-based pricing mechanism with a security\nprotocol to address the utility-privacy conflict. Through comprehensive\nexperiments, we show that the client selection according to FLMarket can\nachieve more than 10% higher accuracy in subsequent FL training compared to\nstate-of-the-art methods. In addition, it outperforms the in-training baseline\nwith more than 2% accuracy increase and 3x run-time speedup.",
    "updated" : "2024-11-18T16:37:41Z",
    "published" : "2024-11-18T16:37:41Z",
    "authors" : [
      {
        "name" : "Zhenyu Wen"
      },
      {
        "name" : "Wanglei Feng"
      },
      {
        "name" : "Di Wu"
      },
      {
        "name" : "Haozhen Hu"
      },
      {
        "name" : "Chang Xu"
      },
      {
        "name" : "Bin Qian"
      },
      {
        "name" : "Zhen Hong"
      },
      {
        "name" : "Cong Wang"
      },
      {
        "name" : "Shouling Ji"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.11521v1",
    "title" : "Preempting Text Sanitization Utility in Resource-Constrained\n  Privacy-Preserving LLM Interactions",
    "summary" : "Individuals have been increasingly interacting with online Large Language\nModels (LLMs), both in their work and personal lives. These interactions raise\nprivacy issues as the LLMs are typically hosted by third-parties who can gather\na variety of sensitive information about users and their companies. Text\nSanitization techniques have been proposed in the literature and can be used to\nsanitize user prompts before sending them to the LLM. However, sanitization has\nan impact on the downstream task performed by the LLM, and often to such an\nextent that it leads to unacceptable results for the user. This is not just a\nminor annoyance, with clear monetary consequences as LLM services charge on a\nper use basis as well as great amount of computing resources wasted. We propose\nan architecture leveraging a Small Language Model (SLM) at the user-side to\nhelp estimate the impact of sanitization on a prompt before it is sent to the\nLLM, thus preventing resource losses.\n  Our evaluation of this architecture revealed a significant problem with text\nsanitization based on Differential Privacy, on which we want to draw the\nattention of the community for further investigation.",
    "updated" : "2024-11-18T12:31:22Z",
    "published" : "2024-11-18T12:31:22Z",
    "authors" : [
      {
        "name" : "Robin Carpentier"
      },
      {
        "name" : "Benjamin Zi Hao Zhao"
      },
      {
        "name" : "Hassan Jameel Asghar"
      },
      {
        "name" : "Dali Kaafar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.11044v1",
    "title" : "Efficient Federated Unlearning with Adaptive Differential Privacy\n  Preservation",
    "summary" : "Federated unlearning (FU) offers a promising solution to effectively address\nthe need to erase the impact of specific clients' data on the global model in\nfederated learning (FL), thereby granting individuals the ``Right to be\nForgotten\". The most straightforward approach to achieve unlearning is to train\nthe model from scratch, excluding clients who request data removal, but it is\nresource-intensive. Current state-of-the-art FU methods extend traditional FL\nframeworks by leveraging stored historical updates, enabling more efficient\nunlearning than training from scratch. However, the use of stored updates\nintroduces significant privacy risks. Adversaries with access to these updates\ncan potentially reconstruct clients' local data, a well-known vulnerability in\nthe privacy domain. While privacy-enhanced techniques exist, their applications\nto FU scenarios that balance unlearning efficiency with privacy protection\nremain underexplored. To address this gap, we propose FedADP, a method designed\nto achieve both efficiency and privacy preservation in FU. Our approach\nincorporates an adaptive differential privacy (DP) mechanism, carefully\nbalancing privacy and unlearning performance through a novel budget allocation\nstrategy tailored for FU. FedADP also employs a dual-layered selection process,\nfocusing on global models with significant changes and client updates closely\naligned with the global model, reducing storage and communication costs.\nAdditionally, a novel calibration method is introduced to facilitate effective\nunlearning. Extensive experimental results demonstrate that FedADP effectively\nmanages the trade-off between unlearning efficiency and privacy protection.",
    "updated" : "2024-11-17T11:45:15Z",
    "published" : "2024-11-17T11:45:15Z",
    "authors" : [
      {
        "name" : "Yu Jiang"
      },
      {
        "name" : "Xindi Tong"
      },
      {
        "name" : "Ziyao Liu"
      },
      {
        "name" : "Huanyi Ye"
      },
      {
        "name" : "Chee Wei Tan"
      },
      {
        "name" : "Kwok-Yan Lam"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.10964v1",
    "title" : "Exploring Device-Oriented Video Encryption for Hierarchical Privacy\n  Protection in AR Content Sharing",
    "summary" : "Content sharing across multiple Augmented Reality (AR) displays is becoming\ncommonplace, enhancing team communication and collaboration through devices\nlike smartphones and AR glasses. However, this practice raises significant\nprivacy concerns, especially concerning the physical environment visible in AR,\nwhich may include sensitive personal details like facial features and\nidentifiable information. Our research focuses on protecting privacy within AR\nenvironments, particularly the physical backgrounds visible during content\nsharing across three common AR display methods: projection, smartphone, and AR\nglasses. We analyze the potential privacy risks associated with each method and\nemploy a Region Of Interest (ROI) video encryption system to hierarchically\nencrypt the physical backdrop based on its safety rating. This study pioneers\nthe integration of ROI video encryption at the bitstream level within AR\ncontexts, providing a more efficient solution than traditional pixel-level\nencryption by enhancing encryption speed and reducing the required space. Our\nadaptive system dynamically adjusts the encryption intensity based on the AR\ndisplay method, ensuring tailored privacy protection.",
    "updated" : "2024-11-17T05:03:04Z",
    "published" : "2024-11-17T05:03:04Z",
    "authors" : [
      {
        "name" : "Yongquan Hu"
      },
      {
        "name" : "Dongsheng Zheng"
      },
      {
        "name" : "Kexin Nie"
      },
      {
        "name" : "Junyan Zhang"
      },
      {
        "name" : "Wen Hu"
      },
      {
        "name" : "Aaron Quigley"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.10612v1",
    "title" : "Contextualizing Security and Privacy of Software-Defined Vehicles: State\n  of the Art and Industry Perspectives",
    "summary" : "The growing reliance on software in vehicles has given rise to the concept of\nSoftware-Defined Vehicles (SDVs), fundamentally reshaping the vehicles and the\nautomotive industry. This survey explores the cybersecurity and privacy\nchallenges posed by SDVs, which increasingly integrate features like\nOver-the-Air (OTA) updates and Vehicle-to-Everything (V2X) communication. While\nthese advancements enhance vehicle capabilities and flexibility, they also come\nwith a flip side: increased exposure to security risks including API\nvulnerabilities, third-party software risks, and supply-chain threats. The\ntransition to SDVs also raises significant privacy concerns, with vehicles\ncollecting vast amounts of sensitive data, such as location and driver\nbehavior, that could be exploited using inference attacks. This work aims to\nprovide a detailed overview of security threats, mitigation strategies, and\nprivacy risks in SDVs, primarily through a literature review, enriched with\ninsights from a targeted questionnaire with industry experts. Key topics\ninclude defining SDVs, comparing them to Connected Vehicles (CVs) and\nAutonomous Vehicles (AVs), discussing the security challenges associated with\nOTA updates and the impact of SDV features on data privacy. Our findings\nhighlight the need for robust security frameworks, standardized communication\nprotocols, and privacy-preserving techniques to address the issues of SDVs.\nThis work ultimately emphasizes the importance of a multi-layered defense\nstrategy,integrating both in-vehicle and cloud-based security solutions, to\nsafeguard future SDVs and increase user trust.",
    "updated" : "2024-11-15T22:30:07Z",
    "published" : "2024-11-15T22:30:07Z",
    "authors" : [
      {
        "name" : "Marco De Vincenzi"
      },
      {
        "name" : "Mert D. Pesé"
      },
      {
        "name" : "Chiara Bodei"
      },
      {
        "name" : "Ilaria Matteucci"
      },
      {
        "name" : "Richard R. Brooks"
      },
      {
        "name" : "Monowar Hasan"
      },
      {
        "name" : "Andrea Saracino"
      },
      {
        "name" : "Mohammad Hamad"
      },
      {
        "name" : "Sebastian Steinhorst"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.OS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.10512v1",
    "title" : "On the Privacy Risk of In-context Learning",
    "summary" : "Large language models (LLMs) are excellent few-shot learners. They can\nperform a wide variety of tasks purely based on natural language prompts\nprovided to them. These prompts contain data of a specific downstream task --\noften the private dataset of a party, e.g., a company that wants to leverage\nthe LLM for their purposes. We show that deploying prompted models presents a\nsignificant privacy risk for the data used within the prompt by instantiating a\nhighly effective membership inference attack. We also observe that the privacy\nrisk of prompted models exceeds fine-tuned models at the same utility levels.\nAfter identifying the model's sensitivity to their prompts -- in the form of a\nsignificantly higher prediction confidence on the prompted data -- as a cause\nfor the increased risk, we propose ensembling as a mitigation strategy. By\naggregating over multiple different versions of a prompted model, membership\ninference risk can be decreased.",
    "updated" : "2024-11-15T17:11:42Z",
    "published" : "2024-11-15T17:11:42Z",
    "authors" : [
      {
        "name" : "Haonan Duan"
      },
      {
        "name" : "Adam Dziedzic"
      },
      {
        "name" : "Mohammad Yaghini"
      },
      {
        "name" : "Nicolas Papernot"
      },
      {
        "name" : "Franziska Boenisch"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.09914v1",
    "title" : "mmSpyVR: Exploiting mmWave Radar for Penetrating Obstacles to Uncover\n  Privacy Vulnerability of Virtual Reality",
    "summary" : "Virtual reality (VR), while enhancing user experiences, introduces\nsignificant privacy risks. This paper reveals a novel vulnerability in VR\nsystems that allows attackers to capture VR privacy through obstacles utilizing\nmillimeter-wave (mmWave) signals without physical intrusion and virtual\nconnection with the VR devices. We propose mmSpyVR, a novel attack on VR user's\nprivacy via mmWave radar. The mmSpyVR framework encompasses two main parts: (i)\nA transfer learning-based feature extraction model to achieve VR feature\nextraction from mmWave signal. (ii) An attention-based VR privacy spying module\nto spy VR privacy information from the extracted feature. The mmSpyVR\ndemonstrates the capability to extract critical VR privacy from the mmWave\nsignals that have penetrated through obstacles. We evaluate mmSpyVR through\nIRB-approved user studies. Across 22 participants engaged in four experimental\nscenes utilizing VR devices from three different manufacturers, our system\nachieves an application recognition accuracy of 98.5\\% and keystroke\nrecognition accuracy of 92.6\\%. This newly discovered vulnerability has\nimplications across various domains, such as cybersecurity, privacy protection,\nand VR technology development. We also engage with VR manufacturer Meta to\ndiscuss and explore potential mitigation strategies. Data and code are publicly\navailable for scrutiny and research at https://github.com/luoyumei1-a/mmSpyVR/",
    "updated" : "2024-11-15T03:22:44Z",
    "published" : "2024-11-15T03:22:44Z",
    "authors" : [
      {
        "name" : "Luoyu Mei"
      },
      {
        "name" : "Ruofeng Liu"
      },
      {
        "name" : "Zhimeng Yin"
      },
      {
        "name" : "Qingchuan Zhao"
      },
      {
        "name" : "Wenchao Jiang"
      },
      {
        "name" : "Shuai Wang"
      },
      {
        "name" : "Kangjie Lu"
      },
      {
        "name" : "Tian He"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.09523v1",
    "title" : "Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats\n  in LLM-Based Agents",
    "summary" : "With the continuous development of large language models (LLMs),\ntransformer-based models have made groundbreaking advances in numerous natural\nlanguage processing (NLP) tasks, leading to the emergence of a series of agents\nthat use LLMs as their control hub. While LLMs have achieved success in various\ntasks, they face numerous security and privacy threats, which become even more\nsevere in the agent scenarios. To enhance the reliability of LLM-based\napplications, a range of research has emerged to assess and mitigate these\nrisks from different perspectives.\n  To help researchers gain a comprehensive understanding of various risks, this\nsurvey collects and analyzes the different threats faced by these agents. To\naddress the challenges posed by previous taxonomies in handling cross-module\nand cross-stage threats, we propose a novel taxonomy framework based on the\nsources and impacts. Additionally, we identify six key features of LLM-based\nagents, based on which we summarize the current research progress and analyze\ntheir limitations. Subsequently, we select four representative agents as case\nstudies to analyze the risks they may face in practical use. Finally, based on\nthe aforementioned analyses, we propose future research directions from the\nperspectives of data, methodology, and policy, respectively.",
    "updated" : "2024-11-14T15:40:04Z",
    "published" : "2024-11-14T15:40:04Z",
    "authors" : [
      {
        "name" : "Yuyou Gan"
      },
      {
        "name" : "Yong Yang"
      },
      {
        "name" : "Zhe Ma"
      },
      {
        "name" : "Ping He"
      },
      {
        "name" : "Rui Zeng"
      },
      {
        "name" : "Yiming Wang"
      },
      {
        "name" : "Qingming Li"
      },
      {
        "name" : "Chunyi Zhou"
      },
      {
        "name" : "Songze Li"
      },
      {
        "name" : "Ting Wang"
      },
      {
        "name" : "Yunjun Gao"
      },
      {
        "name" : "Yingcai Wu"
      },
      {
        "name" : "Shouling Ji"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.09287v1",
    "title" : "The Communication-Friendly Privacy-Preserving Machine Learning against\n  Malicious Adversaries",
    "summary" : "With the increasing emphasis on privacy regulations, such as GDPR, protecting\nindividual privacy and ensuring compliance have become critical concerns for\nboth individuals and organizations. Privacy-preserving machine learning (PPML)\nis an innovative approach that allows for secure data analysis while\nsafeguarding sensitive information. It enables organizations to extract\nvaluable insights from data without compromising privacy. Secure multi-party\ncomputation (MPC) is a key tool in PPML, as it allows multiple parties to\njointly compute functions without revealing their private inputs, making it\nessential in multi-server environments. We address the performance overhead of\nexisting maliciously secure protocols, particularly in finite rings like\n$\\mathbb{Z}_{2^\\ell}$, by introducing an efficient protocol for secure linear\nfunction evaluation. We implement our maliciously secure MPC protocol on GPUs,\nsignificantly improving its efficiency and scalability. We extend the protocol\nto handle linear and non-linear layers, ensuring compatibility with a wide\nrange of machine-learning models. Finally, we comprehensively evaluate machine\nlearning models by integrating our protocol into the workflow, enabling secure\nand efficient inference across simple and complex models, such as convolutional\nneural networks (CNNs).",
    "updated" : "2024-11-14T08:55:14Z",
    "published" : "2024-11-14T08:55:14Z",
    "authors" : [
      {
        "name" : "Tianpei Lu"
      },
      {
        "name" : "Bingsheng Zhang"
      },
      {
        "name" : "Lichun Li"
      },
      {
        "name" : "Kui Ren"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.09178v2",
    "title" : "SAFES: Sequential Privacy and Fairness Enhancing Data Synthesis for\n  Responsible AI",
    "summary" : "As data-driven and AI-based decision making gains widespread adoption in most\ndisciplines, it is crucial that both data privacy and decision fairness are\nappropriately addressed. While differential privacy (DP) provides a robust\nframework for guaranteeing privacy and several widely accepted methods have\nbeen proposed for improving fairness, the vast majority of existing literature\ntreats the two concerns independently. For methods that do consider privacy and\nfairness simultaneously, they often only apply to a specific machine learning\ntask, limiting their generalizability. In response, we introduce SAFES, a\nSequential PrivAcy and Fairness Enhancing data Synthesis procedure that\nsequentially combines DP data synthesis with a fairness-aware data\ntransformation. SAFES allows full control over the privacy-fairness-utility\ntrade-off via tunable privacy and fairness parameters. We illustrate SAFES by\ncombining AIM, a graphical model-based DP data synthesizer, with a popular\nfairness-aware data pre-processing transformation. Empirical evaluations on the\nAdult and COMPAS datasets demonstrate that for reasonable privacy loss,\nSAFES-generated synthetic data achieve significantly improved fairness metrics\nwith relatively low utility loss.",
    "updated" : "2024-11-16T03:13:23Z",
    "published" : "2024-11-14T04:36:12Z",
    "authors" : [
      {
        "name" : "Spencer Giddens"
      },
      {
        "name" : "Fang Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.09142v1",
    "title" : "Laplace Transform Interpretation of Differential Privacy",
    "summary" : "We introduce a set of useful expressions of Differential Privacy (DP) notions\nin terms of the Laplace transform of the privacy loss distribution. Its bare\nform expression appears in several related works on analyzing DP, either as an\nintegral or an expectation. We show that recognizing the expression as a\nLaplace transform unlocks a new way to reason about DP properties by exploiting\nthe duality between time and frequency domains. Leveraging our interpretation,\nwe connect the $(q, \\rho(q))$-R\\'enyi DP curve and the $(\\epsilon,\n\\delta(\\epsilon))$-DP curve as being the Laplace and inverse-Laplace transforms\nof one another. This connection shows that the R\\'enyi divergence is\nwell-defined for complex orders $q = \\gamma + i \\omega$. Using our Laplace\ntransform-based analysis, we also prove an adaptive composition theorem for\n$(\\epsilon, \\delta)$-DP guarantees that is exactly tight (i.e., matches even in\nconstants) for all values of $\\epsilon$. Additionally, we resolve an issue\nregarding symmetry of $f$-DP on subsampling that prevented equivalence across\nall functional DP notions.",
    "updated" : "2024-11-14T02:52:47Z",
    "published" : "2024-11-14T02:52:47Z",
    "authors" : [
      {
        "name" : "Rishav Chourasia"
      },
      {
        "name" : "Uzair Javaid"
      },
      {
        "name" : "Biplap Sikdar"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.09064v1",
    "title" : "Minimax Optimal Two-Sample Testing under Local Differential Privacy",
    "summary" : "We explore the trade-off between privacy and statistical utility in private\ntwo-sample testing under local differential privacy (LDP) for both multinomial\nand continuous data. We begin by addressing the multinomial case, where we\nintroduce private permutation tests using practical privacy mechanisms such as\nLaplace, discrete Laplace, and Google's RAPPOR. We then extend our multinomial\napproach to continuous data via binning and study its uniform separation rates\nunder LDP over H\\\"older and Besov smoothness classes. The proposed tests for\nboth discrete and continuous cases rigorously control the type I error for any\nfinite sample size, strictly adhere to LDP constraints, and achieve minimax\nseparation rates under LDP. The attained minimax rates reveal inherent\nprivacy-utility trade-offs that are unavoidable in private testing. To address\nscenarios with unknown smoothness parameters in density testing, we propose an\nadaptive test based on a Bonferroni-type approach that ensures robust\nperformance without prior knowledge of the smoothness parameters. We validate\nour theoretical findings with extensive numerical experiments and demonstrate\nthe practical relevance and effectiveness of our proposed methods.",
    "updated" : "2024-11-13T22:44:25Z",
    "published" : "2024-11-13T22:44:25Z",
    "authors" : [
      {
        "name" : "Jongmin Mun"
      },
      {
        "name" : "Seungwoo Kwak"
      },
      {
        "name" : "Ilmun Kim"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "62G10"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.12451v1",
    "title" : "Empirical Privacy Evaluations of Generative and Predictive Machine\n  Learning Models -- A review and challenges for practice",
    "summary" : "Synthetic data generators, when trained using privacy-preserving techniques\nlike differential privacy, promise to produce synthetic data with formal\nprivacy guarantees, facilitating the sharing of sensitive data. However, it is\ncrucial to empirically assess the privacy risks associated with the generated\nsynthetic data before deploying generative technologies. This paper outlines\nthe key concepts and assumptions underlying empirical privacy evaluation in\nmachine learning-based generative and predictive models. Then, this paper\nexplores the practical challenges for privacy evaluations of generative models\nfor use cases with millions of training records, such as data from statistical\nagencies and healthcare providers. Our findings indicate that methods designed\nto verify the correct operation of the training algorithm are effective for\nlarge datasets, but they often assume an adversary that is unrealistic in many\nscenarios. Based on the findings, we highlight a crucial trade-off between the\ncomputational feasibility of the evaluation and the level of realism of the\nassumed threat model. Finally, we conclude with ideas and suggestions for\nfuture research.",
    "updated" : "2024-11-19T12:19:28Z",
    "published" : "2024-11-19T12:19:28Z",
    "authors" : [
      {
        "name" : "Flavio Hafner"
      },
      {
        "name" : "Chang Sun"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.12223v1",
    "title" : "Perception of Digital Privacy Protection: An Empirical Study using GDPR\n  Framework",
    "summary" : "Perception of privacy is a contested concept, which is also evolving along\nwith the rapid proliferation and expansion of technological advancements.\nInformation systems (IS) applications incorporate various sensing\ninfrastructures, high-speed networks, and computing components that enable\npervasive data collection about people. Any digital privacy breach within such\nsystems can result in harmful and far-reaching impacts on individuals and\nsocieties. Accordingly, IS organisations have a legal and ethical\nresponsibility to respect and protect individuals digital privacy rights. This\nstudy investigates people perception of digital privacy protection of\ngovernment data using the General Data Protection Regulation (GDPR) framework.\nFindings suggest a dichotomy of perception in protecting people privacy rights.\nFor example, people perceive the right to be informed as the most respected and\nprotected in Information Technology (IT) systems. On the contrary, the right to\nobject by granting and with-drawing consent is perceived as the least\nprotected. Second, the study shows evidence of a social dilemma in people\nperception of digital privacy based on their context and culture.",
    "updated" : "2024-11-19T04:36:31Z",
    "published" : "2024-11-19T04:36:31Z",
    "authors" : [
      {
        "name" : "Hamoud Alhazmi"
      },
      {
        "name" : "Ahmed Imran"
      },
      {
        "name" : "Mohammad Abu Alsheikh"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.12045v1",
    "title" : "Fingerprinting and Tracing Shadows: The Development and Impact of\n  Browser Fingerprinting on Digital Privacy",
    "summary" : "Browser fingerprinting is a growing technique for identifying and tracking\nusers online without traditional methods like cookies. This paper gives an\noverview by examining the various fingerprinting techniques and analyzes the\nentropy and uniqueness of the collected data. The analysis highlights that\nbrowser fingerprinting poses a complex challenge from both technical and\nprivacy perspectives, as users often have no control over the collection and\nuse of their data. In addition, it raises significant privacy concerns as users\nare often tracked without their knowledge or consent.",
    "updated" : "2024-11-18T20:32:31Z",
    "published" : "2024-11-18T20:32:31Z",
    "authors" : [
      {
        "name" : "Alexander Lawall"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.12766v1",
    "title" : "Exploiting the Uncoordinated Privacy Protections of Eye Tracking and VR\n  Motion Data for Unauthorized User Identification",
    "summary" : "Virtual reality (VR) devices use a variety of sensors to capture a rich body\nof user-generated data, which can be misused by malicious parties to covertly\ninfer information about the user. Privacy-enhancing techniques seek to reduce\nthe amount of personally identifying information in sensor data, but these\ntechniques are typically developed for a subset of data streams that are\navailable on the platform, without consideration for the auxiliary information\nthat may be readily available from other sensors. In this paper, we evaluate\nwhether body motion data can be used to circumvent the privacy protections\napplied to eye tracking data to enable user identification on a VR platform,\nand vice versa. We empirically show that eye tracking, headset tracking, and\nhand tracking data are not only informative for inferring user identity on\ntheir own, but contain complementary information that can increase the rate of\nsuccessful user identification. Most importantly, we demonstrate that applying\nprivacy protections to only a subset of the data available in VR can create an\nopportunity for an adversary to bypass those privacy protections by using other\nunprotected data streams that are available on the platform, performing a user\nidentification attack as accurately as though a privacy mechanism was never\napplied. These results highlight a new privacy consideration at the\nintersection between eye tracking and VR, and emphasizes the need for\nprivacy-enhancing techniques that address multiple technologies\ncomprehensively.",
    "updated" : "2024-11-17T22:16:37Z",
    "published" : "2024-11-17T22:16:37Z",
    "authors" : [
      {
        "name" : "Samantha Aziz"
      },
      {
        "name" : "Oleg Komogortsev"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.12756v1",
    "title" : "FedCL-Ensemble Learning: A Framework of Federated Continual Learning\n  with Ensemble Transfer Learning Enhanced for Alzheimer's MRI Classifications\n  while Preserving Privacy",
    "summary" : "This research work introduces a novel approach to the classification of\nAlzheimer's disease by using the advanced deep learning techniques combined\nwith secure data processing methods. This research work primary uses transfer\nlearning models such as ResNet, ImageNet, and VNet to extract high-level\nfeatures from medical image data. Thereafter, these pre-trained models were\nfine-tuned for Alzheimer's related subtle patterns such that the model is\ncapable of robust feature extraction over varying data sources. Further, the\nfederated learning approaches were incorporated to tackle a few other\nchallenges related to classification, aimed to provide better prediction\nperformance and protect data privacy. The proposed model was built using\nfederated learning without sharing sensitive patient data. This way, the\ndecentralized model benefits from the large and diversified dataset that it is\ntrained upon while ensuring confidentiality. The cipher-based encryption\nmechanism is added that allows us to secure the transportation of data and\nfurther ensure the privacy and integrity of patient information throughout\ntraining and classification. The results of the experiments not only help to\nimprove the accuracy of the classification of Alzheimer's but at the same time\nprovides a framework for secure and collaborative analysis of health care data.",
    "updated" : "2024-11-15T13:49:22Z",
    "published" : "2024-11-15T13:49:22Z",
    "authors" : [
      {
        "name" : "Rishit Kapoor"
      },
      {
        "name" : "Jesher Joshua"
      },
      {
        "name" : "Muralidharan Vijayarangan"
      },
      {
        "name" : "Natarajan B"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.06317v2",
    "title" : "Harpocrates: A Statically Typed Privacy Conscious Programming Framework",
    "summary" : "In this paper, we introduce Harpocrates, a compiler plugin and a framework\npair for Scala that binds the privacy policies to the data during data creation\nin form of oblivious membranes. Harpocrates eliminates raw data for a policy\nprotected type from the application, ensuring it can only exist in protected\nform and centralizes the policy checking to the policy declaration site, making\nthe privacy logic easy to maintain and verify. Instead of approaching privacy\nfrom an information flow verification perspective, Harpocrates allow the data\nto flow freely throughout the application, inside the policy membranes but\nenforces the policies when the data is tried to be accessed, mutated,\ndeclassified or passed through the application boundary. The centralization of\nthe policies allow the maintainers to change the enforced logic simply by\nupdating a single function while keeping the rest of the application oblivious\nto the change. Especially in a setting where the data definition is shared by\nmultiple applications, the publisher can update the policies without requiring\nthe dependent applications to make any changes beyond updating the dependency\nversion.",
    "updated" : "2024-11-20T16:02:55Z",
    "published" : "2024-11-10T00:28:58Z",
    "authors" : [
      {
        "name" : "Sinan Pehlivanoglu"
      },
      {
        "name" : "Malte Schwarzkopf"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.13598v1",
    "title" : "Preserving Expert-Level Privacy in Offline Reinforcement Learning",
    "summary" : "The offline reinforcement learning (RL) problem aims to learn an optimal\npolicy from historical data collected by one or more behavioural policies\n(experts) by interacting with an environment. However, the individual experts\nmay be privacy-sensitive in that the learnt policy may retain information about\ntheir precise choices. In some domains like personalized retrieval, advertising\nand healthcare, the expert choices are considered sensitive data. To provably\nprotect the privacy of such experts, we propose a novel consensus-based\nexpert-level differentially private offline RL training approach compatible\nwith any existing offline RL algorithm. We prove rigorous differential privacy\nguarantees, while maintaining strong empirical performance. Unlike existing\nwork in differentially private RL, we supplement the theory with\nproof-of-concept experiments on classic RL environments featuring large\ncontinuous state spaces, demonstrating substantial improvements over a natural\nbaseline across multiple tasks.",
    "updated" : "2024-11-18T21:26:53Z",
    "published" : "2024-11-18T21:26:53Z",
    "authors" : [
      {
        "name" : "Navodita Sharma"
      },
      {
        "name" : "Vishnu Vinod"
      },
      {
        "name" : "Abhradeep Thakurta"
      },
      {
        "name" : "Alekh Agarwal"
      },
      {
        "name" : "Borja Balle"
      },
      {
        "name" : "Christoph Dann"
      },
      {
        "name" : "Aravindan Raghuveer"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.14718v1",
    "title" : "GraphTheft: Quantifying Privacy Risks in Graph Prompt Learning",
    "summary" : "Graph Prompt Learning (GPL) represents an innovative approach in graph\nrepresentation learning, enabling task-specific adaptations by fine-tuning\nprompts without altering the underlying pre-trained model. Despite its growing\nprominence, the privacy risks inherent in GPL remain unexplored. In this study,\nwe provide the first evaluation of privacy leakage in GPL across three attacker\ncapabilities: black-box attacks when GPL as a service, and scenarios where node\nembeddings and prompt representations are accessible to third parties. We\nassess GPL's privacy vulnerabilities through Attribute Inference Attacks (AIAs)\nand Link Inference Attacks (LIAs), finding that under any capability, attackers\ncan effectively infer the properties and relationships of sensitive nodes, and\nthe success rate of inference on some data sets is as high as 98%. Importantly,\nwhile targeted inference attacks on specific prompts (e.g., GPF-plus) maintain\nhigh success rates, our analysis suggests that the prompt-tuning in GPL does\nnot significantly elevate privacy risks compared to traditional GNNs. To\nmitigate these risks, we explored defense mechanisms, identifying that\nLaplacian noise perturbation can substantially reduce inference success, though\nbalancing privacy protection with model performance remains challenging. This\nwork highlights critical privacy risks in GPL, offering new insights and\nfoundational directions for future privacy-preserving strategies in graph\nlearning.",
    "updated" : "2024-11-22T04:10:49Z",
    "published" : "2024-11-22T04:10:49Z",
    "authors" : [
      {
        "name" : "Jiani Zhu"
      },
      {
        "name" : "Xi Lin"
      },
      {
        "name" : "Yuxin Qi"
      },
      {
        "name" : "Qinghua Mao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.14565v1",
    "title" : "Privacy-Preserving Video Anomaly Detection: A Survey",
    "summary" : "Video Anomaly Detection (VAD) aims to automatically analyze spatiotemporal\npatterns in surveillance videos collected from open spaces to detect anomalous\nevents that may cause harm without physical contact. However, vision-based\nsurveillance systems such as closed-circuit television often capture personally\nidentifiable information. The lack of transparency and interpretability in\nvideo transmission and usage raises public concerns about privacy and ethics,\nlimiting the real-world application of VAD. Recently, researchers have focused\non privacy concerns in VAD by conducting systematic studies from various\nperspectives including data, features, and systems, making Privacy-Preserving\nVideo Anomaly Detection (P2VAD) a hotspot in the AI community. However, current\nresearch in P2VAD is fragmented, and prior reviews have mostly focused on\nmethods using RGB sequences, overlooking privacy leakage and appearance bias\nconsiderations. To address this gap, this article systematically reviews the\nprogress of P2VAD for the first time, defining its scope and providing an\nintuitive taxonomy. We outline the basic assumptions, learning frameworks, and\noptimization objectives of various approaches, analyzing their strengths,\nweaknesses, and potential correlations. Additionally, we provide open access to\nresearch resources such as benchmark datasets and available code. Finally, we\ndiscuss key challenges and future opportunities from the perspectives of AI\ndevelopment and P2VAD deployment, aiming to guide future work in the field.",
    "updated" : "2024-11-21T20:29:59Z",
    "published" : "2024-11-21T20:29:59Z",
    "authors" : [
      {
        "name" : "Jing Liu"
      },
      {
        "name" : "Yang Liu"
      },
      {
        "name" : "Xiaoguang Zhu"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.14557v1",
    "title" : "Privacy-Preserving Power Flow Analysis via Secure Multi-Party\n  Computation",
    "summary" : "Smart grids feature a bidirectional flow of electricity and data, enhancing\nflexibility, efficiency, and reliability in increasingly volatile energy grids.\nHowever, data from smart meters can reveal sensitive private information.\nConsequently, the adoption of smart meters is often restricted via legal means\nand hampered by limited user acceptance. Since metering data is beneficial for\nfault-free grid operation, power management, and resource allocation, applying\nprivacy-preserving techniques to smart metering data is an important research\nproblem. This work addresses this by using secure multi-party computation\n(SMPC), allowing multiple parties to jointly evaluate functions of their\nprivate inputs without revealing the latter. Concretely, we show how to perform\npower flow analysis on cryptographically hidden prosumer data. More precisely,\nwe present a tailored solution to the power flow problem building on an SMPC\nimplementation of Newtons method. We analyze the security of our approach in\nthe universal composability framework and provide benchmarks for various grid\ntypes, threat models, and solvers. Our results indicate that secure multi-party\ncomputation can be able to alleviate privacy issues in smart grids in certain\napplications.",
    "updated" : "2024-11-21T20:04:16Z",
    "published" : "2024-11-21T20:04:16Z",
    "authors" : [
      {
        "name" : "Jonas von der Heyden"
      },
      {
        "name" : "Nils Schlüter"
      },
      {
        "name" : "Philipp Binfet"
      },
      {
        "name" : "Martin Asman"
      },
      {
        "name" : "Markus Zdrallek"
      },
      {
        "name" : "Tibor Jager"
      },
      {
        "name" : "Moritz Schulze Darup"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.09064v2",
    "title" : "Minimax Optimal Two-Sample Testing under Local Differential Privacy",
    "summary" : "We explore the trade-off between privacy and statistical utility in private\ntwo-sample testing under local differential privacy (LDP) for both multinomial\nand continuous data. We begin by addressing the multinomial case, where we\nintroduce private permutation tests using practical privacy mechanisms such as\nLaplace, discrete Laplace, and Google's RAPPOR. We then extend our multinomial\napproach to continuous data via binning and study its uniform separation rates\nunder LDP over H\\\"older and Besov smoothness classes. The proposed tests for\nboth discrete and continuous cases rigorously control the type I error for any\nfinite sample size, strictly adhere to LDP constraints, and achieve minimax\nseparation rates under LDP. The attained minimax rates reveal inherent\nprivacy-utility trade-offs that are unavoidable in private testing. To address\nscenarios with unknown smoothness parameters in density testing, we propose an\nadaptive test based on a Bonferroni-type approach that ensures robust\nperformance without prior knowledge of the smoothness parameters. We validate\nour theoretical findings with extensive numerical experiments and demonstrate\nthe practical relevance and effectiveness of our proposed methods.",
    "updated" : "2024-11-22T07:00:06Z",
    "published" : "2024-11-13T22:44:25Z",
    "authors" : [
      {
        "name" : "Jongmin Mun"
      },
      {
        "name" : "Seungwoo Kwak"
      },
      {
        "name" : "Ilmun Kim"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "62G10"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.16516v1",
    "title" : "Curator Attack: When Blackbox Differential Privacy Auditing Loses Its\n  Power",
    "summary" : "A surge in data-driven applications enhances everyday life but also raises\nserious concerns about private information leakage. Hence many privacy auditing\ntools are emerging for checking if the data sanitization performed meets the\nprivacy standard of the data owner. Blackbox auditing for differential privacy\nis particularly gaining popularity for its effectiveness and applicability to a\nwide range of scenarios. Yet, we identified that blackbox auditing is\nessentially flawed with its setting: small probabilities or densities are\nignored due to inaccurate observation. Our argument is based on a solid false\npositive analysis from a hypothesis testing perspective, which is missed out by\nprior blackbox auditing tools. This oversight greatly reduces the reliability\nof these tools, as it allows malicious or incapable data curators to pass the\nauditing with an overstated privacy guarantee, posing significant risks to data\nowners. We demonstrate the practical existence of such threats in classical\ndifferential privacy mechanisms against four representative blackbox auditors\nwith experimental validations. Our findings aim to reveal the limitations of\nblackbox auditing tools, empower the data owner with the awareness of risks in\nusing these tools, and encourage the development of more reliable differential\nprivacy auditing methods.",
    "updated" : "2024-11-25T16:00:04Z",
    "published" : "2024-11-25T16:00:04Z",
    "authors" : [
      {
        "name" : "Shiming Wang"
      },
      {
        "name" : "Liyao Xiang"
      },
      {
        "name" : "Bowei Cheng"
      },
      {
        "name" : "Zhe Ji"
      },
      {
        "name" : "Tianran Sun"
      },
      {
        "name" : "Xinbing Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.16440v1",
    "title" : "AnonyNoise: Anonymizing Event Data with Smart Noise to Outsmart\n  Re-Identification and Preserve Privacy",
    "summary" : "The increasing capabilities of deep neural networks for re-identification,\ncombined with the rise in public surveillance in recent years, pose a\nsubstantial threat to individual privacy. Event cameras were initially\nconsidered as a promising solution since their output is sparse and therefore\ndifficult for humans to interpret. However, recent advances in deep learning\nproof that neural networks are able to reconstruct high-quality grayscale\nimages and re-identify individuals using data from event cameras. In our paper,\nwe contribute a crucial ethical discussion on data privacy and present the\nfirst event anonymization pipeline to prevent re-identification not only by\nhumans but also by neural networks. Our method effectively introduces learnable\ndata-dependent noise to cover personally identifiable information in raw event\ndata, reducing attackers' re-identification capabilities by up to 60%, while\nmaintaining substantial information for the performing of downstream tasks.\nMoreover, our anonymization generalizes well on unseen data and is robust\nagainst image reconstruction and inversion attacks. Code:\nhttps://github.com/dfki-av/AnonyNoise",
    "updated" : "2024-11-25T14:43:03Z",
    "published" : "2024-11-25T14:43:03Z",
    "authors" : [
      {
        "name" : "Katharina Bendig"
      },
      {
        "name" : "René Schuster"
      },
      {
        "name" : "Nicole Thiemer"
      },
      {
        "name" : "Karen Joisten"
      },
      {
        "name" : "Didier Stricker"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.16437v1",
    "title" : "Privacy Protection in Personalized Diffusion Models via Targeted\n  Cross-Attention Adversarial Attack",
    "summary" : "The growing demand for customized visual content has led to the rise of\npersonalized text-to-image (T2I) diffusion models. Despite their remarkable\npotential, they pose significant privacy risk when misused for malicious\npurposes. In this paper, we propose a novel and efficient adversarial attack\nmethod, Concept Protection by Selective Attention Manipulation (CoPSAM) which\ntargets only the cross-attention layers of a T2I diffusion model. For this\npurpose, we carefully construct an imperceptible noise to be added to clean\nsamples to get their adversarial counterparts. This is obtained during the\nfine-tuning process by maximizing the discrepancy between the corresponding\ncross-attention maps of the user-specific token and the class-specific token,\nrespectively. Experimental validation on a subset of CelebA-HQ face images\ndataset demonstrates that our approach outperforms existing methods. Besides\nthis, our method presents two important advantages derived from the qualitative\nevaluation: (i) we obtain better protection results for lower noise levels than\nour competitors; and (ii) we protect the content from unauthorized use thereby\nprotecting the individual's identity from potential misuse.",
    "updated" : "2024-11-25T14:39:18Z",
    "published" : "2024-11-25T14:39:18Z",
    "authors" : [
      {
        "name" : "Xide Xu"
      },
      {
        "name" : "Muhammad Atif Butt"
      },
      {
        "name" : "Sandesh Kamath"
      },
      {
        "name" : "Bogdan Raducanu"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.16404v1",
    "title" : "A Survey of Blockchain-Based Privacy Applications: An Analysis of\n  Consent Management and Self-Sovereign Identity Approaches",
    "summary" : "Modern distributed applications in healthcare, supply chain, and the Internet\nof Things handle a large amount of data in a diverse application setting with\nmultiple stakeholders. Such applications leverage advanced artificial\nintelligence (AI) and machine learning algorithms to automate business\nprocesses. The proliferation of modern AI technologies increases the data\ndemand. However, real-world networks often include private and sensitive\ninformation of businesses, users, and other organizations. Emerging\ndata-protection regulations such as the General Data Protection Regulation\n(GDPR) and the California Consumer Privacy Act (CCPA) introduce policies around\ncollecting, storing, and managing digital data. While Blockchain technology\noffers transparency, auditability, and immutability for multi-stakeholder\napplications, it lacks inherent support for privacy. Typically, privacy support\nis added to a blockchain-based application by incorporating cryptographic\nschemes, consent mechanisms, and self-sovereign identity. This article surveys\nthe literature on blockchain-based privacy-preserving systems and identifies\nthe tools for protecting privacy. Besides, consent mechanisms and identity\nmanagement in the context of blockchain-based systems are also analyzed. The\narticle concludes by highlighting the list of open challenges and further\nresearch opportunities.",
    "updated" : "2024-11-25T14:10:30Z",
    "published" : "2024-11-25T14:10:30Z",
    "authors" : [
      {
        "name" : "Rodrigo Dutra Garcia"
      },
      {
        "name" : "Gowri Ramachandran"
      },
      {
        "name" : "Kealan Dunnett"
      },
      {
        "name" : "Raja Jurdak"
      },
      {
        "name" : "Caetano Ranieri"
      },
      {
        "name" : "Bhaskar Krishnamachari"
      },
      {
        "name" : "Jo Ueyama"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.16380v1",
    "title" : "Privacy-Preserving Federated Foundation Model for Generalist Ultrasound\n  Artificial Intelligence",
    "summary" : "Ultrasound imaging is widely used in clinical diagnosis due to its\nnon-invasive nature and real-time capabilities. However, conventional\nultrasound diagnostics face several limitations, including high dependence on\nphysician expertise and suboptimal image quality, which complicates\ninterpretation and increases the likelihood of diagnostic errors. Artificial\nintelligence (AI) has emerged as a promising solution to enhance clinical\ndiagnosis, particularly in detecting abnormalities across various biomedical\nimaging modalities. Nonetheless, current AI models for ultrasound imaging face\ncritical challenges. First, these models often require large volumes of labeled\nmedical data, raising concerns over patient privacy breaches. Second, most\nexisting models are task-specific, which restricts their broader clinical\nutility. To overcome these challenges, we present UltraFedFM, an innovative\nprivacy-preserving ultrasound foundation model. UltraFedFM is collaboratively\npre-trained using federated learning across 16 distributed medical institutions\nin 9 countries, leveraging a dataset of over 1 million ultrasound images\ncovering 19 organs and 10 ultrasound modalities. This extensive and diverse\ndata, combined with a secure training framework, enables UltraFedFM to exhibit\nstrong generalization and diagnostic capabilities. It achieves an average area\nunder the receiver operating characteristic curve of 0.927 for disease\ndiagnosis and a dice similarity coefficient of 0.878 for lesion segmentation.\nNotably, UltraFedFM surpasses the diagnostic accuracy of mid-level\nultrasonographers and matches the performance of expert-level sonographers in\nthe joint diagnosis of 8 common systemic diseases. These findings indicate that\nUltraFedFM can significantly enhance clinical diagnostics while safeguarding\npatient privacy, marking an advancement in AI-driven ultrasound imaging for\nfuture clinical applications.",
    "updated" : "2024-11-25T13:40:11Z",
    "published" : "2024-11-25T13:40:11Z",
    "authors" : [
      {
        "name" : "Yuncheng Jiang"
      },
      {
        "name" : "Chun-Mei Feng"
      },
      {
        "name" : "Jinke Ren"
      },
      {
        "name" : "Jun Wei"
      },
      {
        "name" : "Zixun Zhang"
      },
      {
        "name" : "Yiwen Hu"
      },
      {
        "name" : "Yunbi Liu"
      },
      {
        "name" : "Rui Sun"
      },
      {
        "name" : "Xuemei Tang"
      },
      {
        "name" : "Juan Du"
      },
      {
        "name" : "Xiang Wan"
      },
      {
        "name" : "Yong Xu"
      },
      {
        "name" : "Bo Du"
      },
      {
        "name" : "Xin Gao"
      },
      {
        "name" : "Guangyu Wang"
      },
      {
        "name" : "Shaohua Zhou"
      },
      {
        "name" : "Shuguang Cui"
      },
      {
        "name" : "Rick Siow Mong Goh"
      },
      {
        "name" : "Yong Liu"
      },
      {
        "name" : "Zhen Li"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.16340v1",
    "title" : "Exploring Privacy and Security as Drivers for Environmental\n  Sustainability in Cloud-Based Office Solutions (Extended Abstract)",
    "summary" : "This paper explores the intersection of privacy, cybersecurity, and\nenvironmental impacts, specifically energy consumption and carbon emissions, in\ncloud-based office solutions. We hypothesise that solutions that emphasise\nprivacy and security are typically \"greener\" than solutions that are financed\nthrough data collection and advertising. To test our hypothesis, we first\ninvestigate how the underlying architectures and business models of these\nservices, e.g., monetisation through (personalised) advertising, contribute to\nthe services' environmental impact. We then explore commonly used methodologies\nand identify tools that facilitate environmental assessments of software\nsystems. By combining these tools, we develop an approach to systematically\nassess the environmental footprint of the user-side of online services, which\nwe apply to investigate and compare the influence of service design and\nad-blocking technology on the emissions of common web-mail services. Our\nmeasurements of a limited selection of such services does not yet conclusively\nsupport or falsify our hypothesis regarding primary impacts. However, we are\nalready able to identify the greener web-mail services on the user-side and\ncontinue the investigation towards conclusive assessment strategies for online\noffice solutions.",
    "updated" : "2024-11-25T12:36:25Z",
    "published" : "2024-11-25T12:36:25Z",
    "authors" : [
      {
        "name" : "Jason Kayembe"
      },
      {
        "name" : "Iness Ben Guirat"
      },
      {
        "name" : "Jan Tobias Muehlberg"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.16121v1",
    "title" : "DP-CDA: An Algorithm for Enhanced Privacy Preservation in Dataset\n  Synthesis Through Randomized Mixing",
    "summary" : "In recent years, the growth of data across various sectors, including\nhealthcare, security, finance, and education, has created significant\nopportunities for analysis and informed decision-making. However, these\ndatasets often contain sensitive and personal information, which raises serious\nprivacy concerns. Protecting individual privacy is crucial, yet many existing\nmachine learning and data publishing algorithms struggle with high-dimensional\ndata, facing challenges related to computational efficiency and privacy\npreservation. To address these challenges, we introduce an effective data\npublishing algorithm \\emph{DP-CDA}. Our proposed algorithm generates synthetic\ndatasets by randomly mixing data in a class-specific manner, and inducing\ncarefully-tuned randomness to ensure formal privacy guarantees. Our\ncomprehensive privacy accounting shows that DP-CDA provides a stronger privacy\nguarantee compared to existing methods, allowing for better utility while\nmaintaining strict level of privacy. To evaluate the effectiveness of DP-CDA,\nwe examine the accuracy of predictive models trained on the synthetic data,\nwhich serves as a measure of dataset utility. Importantly, we identify an\noptimal order of mixing that balances privacy guarantee with predictive\naccuracy. Our results indicate that synthetic datasets produced using the\nDP-CDA can achieve superior utility compared to those generated by traditional\ndata publishing algorithms, even when subject to the same privacy requirements.",
    "updated" : "2024-11-25T06:14:06Z",
    "published" : "2024-11-25T06:14:06Z",
    "authors" : [
      {
        "name" : "Utsab Saha"
      },
      {
        "name" : "Tanvir Muntakim Tonoy"
      },
      {
        "name" : "Hafiz Imtiaz"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.15948v1",
    "title" : "Over-the-Air Federated Adaptive Data Analysis: Preserving Accuracy via\n  Opportunistic Differential Privacy",
    "summary" : "Adaptive data analysis (ADA) involves a dynamic interaction between an\nanalyst and a dataset owner, where the analyst submits queries sequentially,\nadapting them based on previous answers. This process can become adversarial,\nas the analyst may attempt to overfit by targeting non-generalizable patterns\nin the data. To counteract this, the dataset owner introduces randomization\ntechniques, such as adding noise to the responses. This noise not only helps\nprevent overfitting but also enhances data privacy. However, it must be\ncarefully calibrated to ensure that the statistical reliability of the\nresponses is not compromised. In this paper, we extend the ADA problem to the\ncontext of distributed datasets. Specifically, we consider a scenario where a\npotentially adversarial analyst interacts with multiple distributed responders\nthrough adaptive queries. We assume that the responses are subject to noise\nintroduced by the channel connecting the responders and the analyst. We\ndemonstrate how, through a federated mechanism, this noise can be\nopportunistically leveraged to enhance the generalizability of ADA, thereby\nincreasing the number of query-response interactions between the analyst and\nthe responders. We illustrate that careful tuning of the transmission\namplitude, based on the theoretically achievable bounds, can significantly\nimpact the number of accurately answerable queries.",
    "updated" : "2024-11-24T18:26:35Z",
    "published" : "2024-11-24T18:26:35Z",
    "authors" : [
      {
        "name" : "Amir Hossein Hadavi"
      },
      {
        "name" : "Mohammad M. Mojahedian"
      },
      {
        "name" : "Mohammad Reza Aref"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.15796v1",
    "title" : "Data Lineage Inference: Uncovering Privacy Vulnerabilities of Dataset\n  Pruning",
    "summary" : "In this work, we systematically explore the data privacy issues of dataset\npruning in machine learning systems. Our findings reveal, for the first time,\nthat even if data in the redundant set is solely used before model training,\nits pruning-phase membership status can still be detected through attacks.\nSince this is a fully upstream process before model training, traditional model\noutput-based privacy inference methods are completely unsuitable. To address\nthis, we introduce a new task called Data-Centric Membership Inference and\npropose the first ever data-centric privacy inference paradigm named Data\nLineage Inference (DaLI). Under this paradigm, four threshold-based attacks are\nproposed, named WhoDis, CumDis, ArraDis and SpiDis. We show that even without\naccess to downstream models, adversaries can accurately identify the redundant\nset with only limited prior knowledge. Furthermore, we find that different\npruning methods involve varying levels of privacy leakage, and even the same\npruning method can present different privacy risks at different pruning\nfractions. We conducted an in-depth analysis of these phenomena and introduced\na metric called the Brimming score to offer guidance for selecting pruning\nmethods with privacy protection in mind.",
    "updated" : "2024-11-24T11:46:59Z",
    "published" : "2024-11-24T11:46:59Z",
    "authors" : [
      {
        "name" : "Qi Li"
      },
      {
        "name" : "Cheng-Long Wang"
      },
      {
        "name" : "Yinzhi Cao"
      },
      {
        "name" : "Di Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01140v2",
    "title" : "Privacy-Preserving Federated Learning with Differentially Private\n  Hyperdimensional Computing",
    "summary" : "Federated Learning (FL) is essential for efficient data exchange in Internet\nof Things (IoT) environments, as it trains Machine Learning (ML) models locally\nand shares only model updates. However, FL is vulnerable to privacy threats\nlike model inversion and membership inference attacks, which can expose\nsensitive training data. To address these privacy concerns, Differential\nPrivacy (DP) mechanisms are often applied. Yet, adding DP noise to black-box ML\nmodels degrades performance, especially in dynamic IoT systems where\ncontinuous, lifelong FL learning accumulates excessive noise over time. To\nmitigate this issue, we introduce Federated HyperDimensional computing with\nPrivacy-preserving (FedHDPrivacy), an eXplainable Artificial Intelligence (XAI)\nframework that combines the neuro-symbolic paradigm with DP. FedHDPrivacy\ncarefully manages the balance between privacy and performance by theoretically\ntracking cumulative noise from previous rounds and adding only the necessary\nincremental noise to meet privacy requirements. In a real-world case study\ninvolving in-process monitoring of manufacturing machining operations,\nFedHDPrivacy demonstrates robust performance, outperforming standard FL\nframeworks-including Federated Averaging (FedAvg), Federated Stochastic\nGradient Descent (FedSGD), Federated Proximal (FedProx), Federated Normalized\nAveraging (FedNova), and Federated Adam (FedAdam)-by up to 38%. FedHDPrivacy\nalso shows potential for future enhancements, such as multimodal data fusion.",
    "updated" : "2024-11-25T02:23:02Z",
    "published" : "2024-11-02T05:00:44Z",
    "authors" : [
      {
        "name" : "Fardin Jalil Piran"
      },
      {
        "name" : "Zhiling Chen"
      },
      {
        "name" : "Mohsen Imani"
      },
      {
        "name" : "Farhad Imani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.17589v1",
    "title" : "Privacy-Preserving Behaviour of Chatbot Users: Steering Through Trust\n  Dynamics",
    "summary" : "Introduction: The use of chatbots is becoming increasingly important across\nvarious aspects of daily life. However, the privacy concerns associated with\nthese communications have not yet been thoroughly addressed. The aim of this\nstudy was to investigate user awareness of privacy risks in chatbot\ninteractions, the privacy-preserving behaviours users practice, and how these\nbehaviours relate to their awareness of privacy threats, even when no immediate\nthreat is perceived. Methods: We developed a novel \"privacy-safe\" setup to\nanalyse user behaviour under the guarantees of anonymization and non-sharing.\nWe employed a mixed-methods approach, starting with the quantification of\nbroader trends by coding responses, followed by conducting a qualitative\ncontent analysis to gain deeper insights. Results: Overall, there was a\nsubstantial lack of understanding among users about how chatbot providers\nhandle data (27% of the participants) and the basics of privacy risks (76% of\nthe participants). Older users, in particular, expressed fears that chatbot\nproviders might sell their data. Moreover, even users with privacy knowledge do\nnot consistently exhibit privacy-preserving behaviours when assured of\ntransparent data processing by chatbots. Notably, under-protective behaviours\nwere observed among more expert users. Discussion: These findings highlight the\nneed for a strategic approach to enhance user education on privacy concepts to\nensure informed decision when interacting with chatbot technology. This\nincludes the development of tools to help users monitor and control the\ninformation they share with chatbots",
    "updated" : "2024-11-26T16:55:58Z",
    "published" : "2024-11-26T16:55:58Z",
    "authors" : [
      {
        "name" : "Julia Ive"
      },
      {
        "name" : "Vishal Yadav"
      },
      {
        "name" : "Mariia Ignashina"
      },
      {
        "name" : "Matthew Rand"
      },
      {
        "name" : "Paulina Bondaronek"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.17321v1",
    "title" : "A Framework for the Security and Privacy of Biometric System\n  Constructions under Defined Computational Assumptions",
    "summary" : "Biometric systems, while offering convenient authentication, often fall short\nin providing rigorous security assurances. A primary reason is the ad-hoc\ndesign of protocols and components, which hinders the establishment of\ncomprehensive security proofs. This paper introduces a formal framework for\nconstructing secure and privacy-preserving biometric systems. By leveraging the\nprinciples of universal composability, we enable the modular analysis and\nverification of individual system components. This approach allows us to derive\nstrong security and privacy properties for the entire system, grounded in\nwell-defined computational assumptions.",
    "updated" : "2024-11-26T11:10:11Z",
    "published" : "2024-11-26T11:10:11Z",
    "authors" : [
      {
        "name" : "Sam Grierson"
      },
      {
        "name" : "William J Buchanan"
      },
      {
        "name" : "Craig Thomson"
      },
      {
        "name" : "Baraq Galeb"
      },
      {
        "name" : "Chris Eckl"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.17287v1",
    "title" : "Privacy Preserving Federated Unsupervised Domain Adaptation with\n  Application to Age Prediction from DNA Methylation Data",
    "summary" : "In computational biology, predictive models are widely used to address\ncomplex tasks, but their performance can suffer greatly when applied to data\nfrom different distributions. The current state-of-the-art domain adaptation\nmethod for high-dimensional data aims to mitigate these issues by aligning the\ninput dependencies between training and test data. However, this approach\nrequires centralized access to both source and target domain data, raising\nconcerns about data privacy, especially when the data comes from multiple\nsources. In this paper, we introduce a privacy-preserving federated framework\nfor unsupervised domain adaptation in high-dimensional settings. Our method\nemploys federated training of Gaussian processes and weighted elastic nets to\neffectively address the problem of distribution shift between domains, while\nutilizing secure aggregation and randomized encoding to protect the local data\nof participating data owners. We evaluate our framework on the task of age\nprediction using DNA methylation data from multiple tissues, demonstrating that\nour approach performs comparably to existing centralized methods while\nmaintaining data privacy, even in distributed environments where data is spread\nacross multiple institutions. Our framework is the first privacy-preserving\nsolution for high-dimensional domain adaptation in federated environments,\noffering a promising tool for fields like computational biology and medicine,\nwhere protecting sensitive data is essential.",
    "updated" : "2024-11-26T10:19:16Z",
    "published" : "2024-11-26T10:19:16Z",
    "authors" : [
      {
        "name" : "Cem Ata Baykara"
      },
      {
        "name" : "Ali Burak Ünal"
      },
      {
        "name" : "Nico Pfeifer"
      },
      {
        "name" : "Mete Akgün"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.17035v1",
    "title" : "Achieving Privacy Utility Balance for Multivariate Time Series Data",
    "summary" : "Utility-preserving data privatization is of utmost importance for\ndata-producing agencies. The popular noise-addition privacy mechanism distorts\nautocorrelation patterns in time series data, thereby marring utility; in\nresponse, McElroy et al. (2023) introduced all-pass filtering (FLIP) as a\nutility-preserving time series data privatization method. Adapting this concept\nto multivariate data is more complex, and in this paper we propose a\nmultivariate all-pass (MAP) filtering method, employing an optimization\nalgorithm to achieve the best balance between data utility and privacy\nprotection. To test the effectiveness of our approach, we apply MAP filtering\nto both simulated and real data, sourced from the U.S. Census Bureau's\nQuarterly Workforce Indicator (QWI) dataset.",
    "updated" : "2024-11-26T01:59:38Z",
    "published" : "2024-11-26T01:59:38Z",
    "authors" : [
      {
        "name" : "Gaurab Hore"
      },
      {
        "name" : "Tucker McElroy"
      },
      {
        "name" : "Anindya Roy"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.16340v2",
    "title" : "Exploring Privacy and Security as Drivers for Environmental\n  Sustainability in Cloud-Based Office Solutions (Extended Abstract)",
    "summary" : "This paper explores the intersection of privacy, cybersecurity, and\nenvironmental impacts, specifically energy consumption and carbon emissions, in\ncloud-based office solutions. We hypothesise that solutions that emphasise\nprivacy and security are typically \"greener\" than solutions that are financed\nthrough data collection and advertising. To test our hypothesis, we first\ninvestigate how the underlying architectures and business models of these\nservices, e.g., monetisation through (personalised) advertising, contribute to\nthe services' environmental impact. We then explore commonly used methodologies\nand identify tools that facilitate environmental assessments of software\nsystems. By combining these tools, we develop an approach to systematically\nassess the environmental footprint of the user-side of online services, which\nwe apply to investigate and compare the influence of service design and\nad-blocking technology on the emissions of common web-mail services. Our\nmeasurements of a limited selection of such services does not yet conclusively\nsupport or falsify our hypothesis regarding primary impacts. However, we are\nalready able to identify the greener web-mail services on the user-side and\ncontinue the investigation towards conclusive assessment strategies for online\noffice solutions.",
    "updated" : "2024-11-26T12:17:22Z",
    "published" : "2024-11-25T12:36:25Z",
    "authors" : [
      {
        "name" : "Jason Kayembe"
      },
      {
        "name" : "Iness Ben Guirat"
      },
      {
        "name" : "Jan Tobias Muehlberg"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.16737v1",
    "title" : "Federated Learning in Chemical Engineering: A Tutorial on a Framework\n  for Privacy-Preserving Collaboration Across Distributed Data Sources",
    "summary" : "Federated Learning (FL) is a decentralized machine learning approach that has\ngained attention for its potential to enable collaborative model training\nacross clients while protecting data privacy, making it an attractive solution\nfor the chemical industry. This work aims to provide the chemical engineering\ncommunity with an accessible introduction to the discipline. Supported by a\nhands-on tutorial and a comprehensive collection of examples, it explores the\napplication of FL in tasks such as manufacturing optimization, multimodal data\nintegration, and drug discovery while addressing the unique challenges of\nprotecting proprietary information and managing distributed datasets. The\ntutorial was built using key frameworks such as $\\texttt{Flower}$ and\n$\\texttt{TensorFlow Federated}$ and was designed to provide chemical engineers\nwith the right tools to adopt FL in their specific needs. We compare the\nperformance of FL against centralized learning across three different datasets\nrelevant to chemical engineering applications, demonstrating that FL will often\nmaintain or improve classification performance, particularly for complex and\nheterogeneous data. We conclude with an outlook on the open challenges in\nfederated learning to be tackled and current approaches designed to remediate\nand improve this framework.",
    "updated" : "2024-11-23T13:16:06Z",
    "published" : "2024-11-23T13:16:06Z",
    "authors" : [
      {
        "name" : "Siddhant Dutta"
      },
      {
        "name" : "Iago Leal de Freitas"
      },
      {
        "name" : "Pedro Maciel Xavier"
      },
      {
        "name" : "Claudio Miceli de Farias"
      },
      {
        "name" : "David Esteban Bernal Neira"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.18380v1",
    "title" : "SoK: Privacy Personalised -- Mapping Personal Attributes \\& Preferences\n  of Privacy Mechanisms for Shoulder Surfing",
    "summary" : "Shoulder surfing is a byproduct of smartphone use that enables bystanders to\naccess personal information (such as text and photos) by making screen\nobservations without consent. To mitigate this, several protection mechanisms\nhave been proposed to protect user privacy. However, the mechanisms that users\nprefer remain unexplored. This paper explores correlations between personal\nattributes and properties of shoulder surfing protection mechanisms. For this,\nwe first conducted a structured literature review and identified ten protection\nmechanism categories against content-based shoulder surfing. We then surveyed\nN=192 users and explored correlations between personal attributes and\nproperties of shoulder surfing protection mechanisms. Our results show that\nusers agreed that the presented mechanisms assisted in protecting their\nprivacy, but they preferred non-digital alternatives. Among the mechanisms,\nparticipants mainly preferred an icon overlay mechanism followed by a tangible\nmechanism. We also found that users who prioritized out-of-device privacy and a\nhigh tendency to interact with technology favoured the personalisation of\nprotection mechanisms. On the contrary, age and smartphone OS did not impact\nusers' preference for perceived usefulness and personalisation of mechanisms.\nBased on the results, we present key takeaways to support the design of future\nprotection mechanisms.",
    "updated" : "2024-11-27T14:27:47Z",
    "published" : "2024-11-27T14:27:47Z",
    "authors" : [
      {
        "name" : "Habiba Farzand"
      },
      {
        "name" : "Karola Marky"
      },
      {
        "name" : "Mohamed Khamis"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.18269v1",
    "title" : "Hidden Data Privacy Breaches in Federated Learning",
    "summary" : "Federated Learning (FL) emerged as a paradigm for conducting machine learning\nacross broad and decentralized datasets, promising enhanced privacy by\nobviating the need for direct data sharing. However, recent studies show that\nattackers can steal private data through model manipulation or gradient\nanalysis. Existing attacks are constrained by low theft quantity or\nlow-resolution data, and they are often detected through anomaly monitoring in\ngradients or weights. In this paper, we propose a novel data-reconstruction\nattack leveraging malicious code injection, supported by two key techniques,\ni.e., distinctive and sparse encoding design and block partitioning. Unlike\nconventional methods that require detectable changes to the model, our method\nstealthily embeds a hidden model using parameter sharing to systematically\nextract sensitive data. The Fibonacci-based index design ensures efficient,\nstructured retrieval of memorized data, while the block partitioning method\nenhances our method's capability to handle high-resolution images by dividing\nthem into smaller, manageable units. Extensive experiments on 4 datasets\nconfirmed that our method is superior to the five state-of-the-art\ndata-reconstruction attacks under the five respective detection methods. Our\nmethod can handle large-scale and high-resolution data without being detected\nor mitigated by state-of-the-art data reconstruction defense methods. In\ncontrast to baselines, our method can be directly applied to both FedAVG and\nFedSGD scenarios, underscoring the need for developers to devise new defenses\nagainst such vulnerabilities. We will open-source our code upon acceptance.",
    "updated" : "2024-11-27T12:04:37Z",
    "published" : "2024-11-27T12:04:37Z",
    "authors" : [
      {
        "name" : "Xueluan Gong"
      },
      {
        "name" : "Yuji Wang"
      },
      {
        "name" : "Shuaike Li"
      },
      {
        "name" : "Mengyuan Sun"
      },
      {
        "name" : "Songze Li"
      },
      {
        "name" : "Qian Wang"
      },
      {
        "name" : "Kwok-Yan Lam"
      },
      {
        "name" : "Chen Chen"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.18027v1",
    "title" : "Privacy-preserving Robotic-based Multi-factor Authentication Scheme for\n  Secure Automated Delivery System",
    "summary" : "Package delivery is a critical aspect of various industries, but it often\nincurs high financial costs and inefficiencies when relying solely on human\nresources. The last-mile transport problem, in particular, contributes\nsignificantly to the expenditure of human resources in major companies.\nRobot-based delivery systems have emerged as a potential solution for last-mile\ndelivery to address this challenge. However, robotic delivery systems still\nface security and privacy issues, like impersonation, replay, man-in-the-middle\nattacks (MITM), unlinkability, and identity theft. In this context, we propose\na privacy-preserving multi-factor authentication scheme specifically designed\nfor robot delivery systems. Additionally, AI-assisted robotic delivery systems\nare susceptible to machine learning-based attacks (e.g. FGSM, PGD, etc.). We\nintroduce the \\emph{first} transformer-based audio-visual fusion defender to\ntackle this issue, which effectively provides resilience against adversarial\nsamples. Furthermore, we provide a rigorous formal analysis of the proposed\nprotocol and also analyse the protocol security using a popular symbolic proof\ntool called ProVerif and Scyther. Finally, we present a real-world\nimplementation of the proposed robotic system with the computation cost and\nenergy consumption analysis. Code and pre-trained models are available at:\nhttps://drive.google.com/drive/folders/18B2YbxtV0Pyj5RSFX-ZzCGtFOyorBHil",
    "updated" : "2024-11-27T03:48:00Z",
    "published" : "2024-11-27T03:48:00Z",
    "authors" : [
      {
        "name" : "Yang Yang"
      },
      {
        "name" : "Aryan Mohammadi Pasikhani"
      },
      {
        "name" : "Prosanta Gope"
      },
      {
        "name" : "Biplab Sikdar"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.17758v1",
    "title" : "PP-LEM: Efficient and Privacy-Preserving Clearance Mechanism for Local\n  Energy Markets",
    "summary" : "In this paper, we propose a novel Privacy-Preserving clearance mechanism for\nLocal Energy Markets (PP-LEM), designed for computational efficiency and social\nwelfare. PP-LEM incorporates a novel competitive game-theoretical clearance\nmechanism, modelled as a Stackelberg Game. Based on this mechanism, a\nprivacy-preserving market model is developed using a partially homomorphic\ncryptosystem, allowing buyers' reaction function calculations to be executed\nover encrypted data without exposing sensitive information of both buyers and\nsellers. The comprehensive performance evaluation demonstrates that PP-LEM is\nhighly effective in delivering an incentive clearance mechanism with\ncomputational efficiency, enabling it to clear the market for 200 users within\nthe order of seconds while concurrently protecting user privacy. Compared to\nthe state of the art, PP-LEM achieves improved computational efficiency without\ncompromising social welfare while still providing user privacy protection.",
    "updated" : "2024-11-26T00:22:31Z",
    "published" : "2024-11-26T00:22:31Z",
    "authors" : [
      {
        "name" : "Kamil Erdayandi"
      },
      {
        "name" : "Mustafa Asan Mustafa"
      }
    ],
    "categories" : [
      "cs.GT",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.07806v2",
    "title" : "Federated Low-Rank Adaptation with Differential Privacy over Wireless\n  Networks",
    "summary" : "Fine-tuning large pre-trained foundation models (FMs) on distributed edge\ndevices presents considerable computational and privacy challenges. Federated\nfine-tuning (FedFT) mitigates some privacy issues by facilitating collaborative\nmodel training without the need to share raw data. To lessen the computational\nburden on resource-limited devices, combining low-rank adaptation (LoRA) with\nfederated learning enables parameter-efficient fine-tuning. Additionally, the\nsplit FedFT architecture partitions an FM between edge devices and a central\nserver, reducing the necessity for complete model deployment on individual\ndevices. However, the risk of privacy eavesdropping attacks in FedFT remains a\nconcern, particularly in sensitive areas such as healthcare and finance. In\nthis paper, we propose a split FedFT framework with differential privacy (DP)\nover wireless networks, where the inherent wireless channel noise in the uplink\ntransmission is utilized to achieve DP guarantees without adding an extra\nartificial noise. We shall investigate the impact of the wireless noise on\nconvergence performance of the proposed framework. We will also show that by\nupdating only one of the low-rank matrices in the split FedFT with DP, the\nproposed method can mitigate the noise amplification effect. Simulation results\nwill demonstrate that the proposed framework achieves higher accuracy under\nstrict privacy budgets compared to baseline methods.",
    "updated" : "2024-11-27T16:55:59Z",
    "published" : "2024-11-12T14:01:08Z",
    "authors" : [
      {
        "name" : "Tianqu Kang"
      },
      {
        "name" : "Zixin Wang"
      },
      {
        "name" : "Hengtao He"
      },
      {
        "name" : "Jun Zhang"
      },
      {
        "name" : "Shenghui Song"
      },
      {
        "name" : "Khaled B. Letaief"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.19678v1",
    "title" : "Privacy-Preserving Orthogonal Aggregation for Guaranteeing Gender\n  Fairness in Federated Recommendation",
    "summary" : "Under stringent privacy constraints, whether federated recommendation systems\ncan achieve group fairness remains an inadequately explored question. Taking\ngender fairness as a representative issue, we identify three phenomena in\nfederated recommendation systems: performance difference, data imbalance, and\npreference disparity. We discover that the state-of-the-art methods only focus\non the first phenomenon. Consequently, their imposition of inappropriate\nfairness constraints detrimentally affects the model training. Moreover, due to\ninsufficient sensitive attribute protection of existing works, we can infer the\ngender of all users with 99.90% accuracy even with the addition of maximal\nnoise. In this work, we propose Privacy-Preserving Orthogonal Aggregation\n(PPOA), which employs the secure aggregation scheme and quantization technique,\nto prevent the suppression of minority groups by the majority and preserve the\ndistinct preferences for better group fairness. PPOA can assist different\ngroups in obtaining their respective model aggregation results through a\ndesigned orthogonal mapping while keeping their attributes private.\nExperimental results on three real-world datasets demonstrate that PPOA\nenhances recommendation effectiveness for both females and males by up to 8.25%\nand 6.36%, respectively, with a maximum overall improvement of 7.30%, and\nachieves optimal fairness in most cases. Extensive ablation experiments and\nvisualizations indicate that PPOA successfully maintains preferences for\ndifferent gender groups.",
    "updated" : "2024-11-29T13:12:11Z",
    "published" : "2024-11-29T13:12:11Z",
    "authors" : [
      {
        "name" : "Siqing Zhang"
      },
      {
        "name" : "Yuchen Ding"
      },
      {
        "name" : "Wei Tang"
      },
      {
        "name" : "Wei Sun"
      },
      {
        "name" : "Yong Liao"
      },
      {
        "name" : "Peng Yuan Zhou"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.19498v1",
    "title" : "Protecting Multiple Types of Privacy Simultaneously in EEG-based\n  Brain-Computer Interfaces",
    "summary" : "A brain-computer interface (BCI) enables direct communication between the\nbrain and an external device. Electroencephalogram (EEG) is the preferred input\nsignal in non-invasive BCIs, due to its convenience and low cost. EEG-based\nBCIs have been successfully used in many applications, such as neurological\nrehabilitation, text input, games, and so on. However, EEG signals inherently\ncarry rich personal information, necessitating privacy protection. This paper\ndemonstrates that multiple types of private information (user identity, gender,\nand BCI-experience) can be easily inferred from EEG data, imposing a serious\nprivacy threat to BCIs. To address this issue, we design perturbations to\nconvert the original EEG data into privacy-protected EEG data, which conceal\nthe private information while maintaining the primary BCI task performance.\nExperimental results demonstrated that the privacy-protected EEG data can\nsignificantly reduce the classification accuracy of user identity, gender and\nBCI-experience, but almost do not affect at all the classification accuracy of\nthe primary BCI task, enabling user privacy protection in EEG-based BCIs.",
    "updated" : "2024-11-29T06:33:31Z",
    "published" : "2024-11-29T06:33:31Z",
    "authors" : [
      {
        "name" : "Lubin Meng"
      },
      {
        "name" : "Xue Jiang"
      },
      {
        "name" : "Tianwang Jia"
      },
      {
        "name" : "Dongrui Wu"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.19142v1",
    "title" : "GDPR-Relevant Privacy Concerns in Mobile Apps Research: A Systematic\n  Literature Review",
    "summary" : "The General Data Protection Regulation (GDPR) is the benchmark in the\nEuropean Union (EU) for privacy and data protection standards. Substantial\nresearch has been conducted in the requirements engineering (RE) literature\ninvestigating the elicitation, representation, and verification of privacy\nrequirements in GDPR. Software systems including mobile apps must comply with\nthe GDPR. With the growing pervasiveness of mobile apps and their increasing\ndemand for personal data, privacy concerns have acquired further interest\nwithin the software engineering (SE) community at large. Despite the extensive\nliterature on GDPR-relevant privacy concerns in mobile apps, there is no\nsecondary study that describes, analyzes, and categorizes the current focus.\nResearch gaps and persistent challenges are thus left unnoticed. In this\narticle, we aim to systematically review existing primary studies highlighting\nvarious GDPR concepts and how these concepts are addressed in mobile apps\nresearch. The objective is to reconcile the existing work on GDPR in the RE\nliterature with the research on GDPR-related privacy concepts in mobile apps in\nthe SE literature. Our findings show that the current research landscape\nreflects a rather shallow understanding of GDPR requirements. Some GDPR\nconcepts such as data subject rights (i.e., the rights of individuals over\ntheir personal data) are fundamental to GDPR, yet under-explored in the\nliterature. In this article, we highlight future directions to be pursued by\nthe SE community for supporting the development of GDPR-compliant mobile apps.",
    "updated" : "2024-11-28T13:42:46Z",
    "published" : "2024-11-28T13:42:46Z",
    "authors" : [
      {
        "name" : "Orlando Amaral Cejas"
      },
      {
        "name" : "Nicolas Sannier"
      },
      {
        "name" : "Sallam Abualhaija"
      },
      {
        "name" : "Marcello Ceci"
      },
      {
        "name" : "Domenico Bianculli"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.18746v1",
    "title" : "Inference Privacy: Properties and Mechanisms",
    "summary" : "Ensuring privacy during inference stage is crucial to prevent malicious third\nparties from reconstructing users' private inputs from outputs of public\nmodels. Despite a large body of literature on privacy preserving learning\n(which ensures privacy of training data), there is no existing systematic\nframework to ensure the privacy of users' data during inference. Motivated by\nthis problem, we introduce the notion of Inference Privacy (IP), which can\nallow a user to interact with a model (for instance, a classifier, or an\nAI-assisted chat-bot) while providing a rigorous privacy guarantee for the\nusers' data at inference. We establish fundamental properties of the IP privacy\nnotion and also contrast it with the notion of Local Differential Privacy\n(LDP). We then present two types of mechanisms for achieving IP: namely, input\nperturbations and output perturbations which are customizable by the users and\ncan allow them to navigate the trade-off between utility and privacy. We also\ndemonstrate the usefulness of our framework via experiments and highlight the\nresulting trade-offs between utility and privacy during inference.",
    "updated" : "2024-11-27T20:47:28Z",
    "published" : "2024-11-27T20:47:28Z",
    "authors" : [
      {
        "name" : "Fengwei Tian"
      },
      {
        "name" : "Ravi Tandon"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.18653v1",
    "title" : "PRSI: Privacy-Preserving Recommendation Model Based on Vector Splitting\n  and Interactive Protocols",
    "summary" : "With the development of the internet, recommending interesting products to\nusers has become a highly valuable research topic for businesses.\nRecommendation systems play a crucial role in addressing this issue. To prevent\nthe leakage of each user's (client's) private data, Federated Recommendation\nSystems (FedRec) have been proposed and widely used. However, extensive\nresearch has shown that FedRec suffers from security issues such as data\nprivacy leakage, and it is challenging to train effective models with FedRec\nwhen each client only holds interaction information for a single user. To\naddress these two problems, this paper proposes a new privacy-preserving\nrecommendation system (PRSI), which includes a preprocessing module and two\nmain phases. The preprocessing module employs split vectors and fake\ninteraction items to protect clients' interaction information and\nrecommendation results. The two main phases are: (1) the collection of\ninteraction information and (2) the sending of recommendation results. In the\ninteraction information collection phase, each client uses the preprocessing\nmodule and random communication methods (according to the designed interactive\nprotocol) to protect their ID information and IP addresses. In the\nrecommendation results sending phase, the central server uses the preprocessing\nmodule and triplets to distribute recommendation results to each client under\nsecure conditions, following the designed interactive protocol. Finally, we\nconducted multiple sets of experiments to verify the security, accuracy, and\ncommunication cost of the proposed method.",
    "updated" : "2024-11-27T05:14:15Z",
    "published" : "2024-11-27T05:14:15Z",
    "authors" : [
      {
        "name" : "Xiaokai Cao"
      },
      {
        "name" : "Wenjin Mo"
      },
      {
        "name" : "Zhenyu He"
      },
      {
        "name" : "Changdong Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01357v2",
    "title" : "WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy\n  Principles",
    "summary" : "In this paper, we introduce WaKA (Wasserstein K-nearest-neighbors\nAttribution), a novel attribution method that leverages principles from the\nLiRA (Likelihood Ratio Attack) framework and k-nearest neighbors classifiers\n(k-NN). WaKA efficiently measures the contribution of individual data points to\nthe model's loss distribution, analyzing every possible k-NN that can be\nconstructed using the training set, without requiring to sample subsets of the\ntraining set. WaKA is versatile and can be used a posteriori as a membership\ninference attack (MIA) to assess privacy risks or a priori for privacy\ninfluence measurement and data valuation. Thus, WaKA can be seen as bridging\nthe gap between data attribution and membership inference attack (MIA) by\nproviding a unified framework to distinguish between a data point's value and\nits privacy risk. For instance, we have shown that self-attribution values are\nmore strongly correlated with the attack success rate than the contribution of\na point to the model generalization. WaKA's different usage were also evaluated\nacross diverse real-world datasets, demonstrating performance very close to\nLiRA when used as an MIA on k-NN classifiers, but with greater computational\nefficiency. Additionally, WaKA shows greater robustness than Shapley Values for\ndata minimization tasks (removal or addition) on imbalanced datasets.",
    "updated" : "2024-12-01T16:18:23Z",
    "published" : "2024-11-02T20:27:51Z",
    "authors" : [
      {
        "name" : "Patrick Mesana"
      },
      {
        "name" : "Clément Bénesse"
      },
      {
        "name" : "Hadrien Lautraite"
      },
      {
        "name" : "Gilles Caporossi"
      },
      {
        "name" : "Sébastien Gambs"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  }
]