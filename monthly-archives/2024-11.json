[
  {
    "id" : "http://arxiv.org/abs/2411.01583v1",
    "title" : "Trustworthy Federated Learning: Privacy, Security, and Beyond",
    "summary" : "While recent years have witnessed the advancement in big data and Artificial\nIntelligence (AI), it is of much importance to safeguard data privacy and\nsecurity. As an innovative approach, Federated Learning (FL) addresses these\nconcerns by facilitating collaborative model training across distributed data\nsources without transferring raw data. However, the challenges of robust\nsecurity and privacy across decentralized networks catch significant attention\nin dealing with the distributed data in FL. In this paper, we conduct an\nextensive survey of the security and privacy issues prevalent in FL,\nunderscoring the vulnerability of communication links and the potential for\ncyber threats. We delve into various defensive strategies to mitigate these\nrisks, explore the applications of FL across different sectors, and propose\nresearch directions. We identify the intricate security challenges that arise\nwithin the FL frameworks, aiming to contribute to the development of secure and\nefficient FL systems.",
    "updated" : "2024-11-03T14:18:01Z",
    "published" : "2024-11-03T14:18:01Z",
    "authors" : [
      {
        "name" : "Chunlu Chen"
      },
      {
        "name" : "Ji Liu"
      },
      {
        "name" : "Haowen Tan"
      },
      {
        "name" : "Xingjian Li"
      },
      {
        "name" : "Kevin I-Kai Wang"
      },
      {
        "name" : "Peng Li"
      },
      {
        "name" : "Kouichi Sakurai"
      },
      {
        "name" : "Dejing Dou"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01471v1",
    "title" : "A Practical and Privacy-Preserving Framework for Real-World Large\n  Language Model Services",
    "summary" : "Large language models (LLMs) have demonstrated exceptional capabilities in\ntext understanding and generation, and they are increasingly being utilized\nacross various domains to enhance productivity. However, due to the high costs\nof training and maintaining these models, coupled with the fact that some LLMs\nare proprietary, individuals often rely on online AI as a Service (AIaaS)\nprovided by LLM companies. This business model poses significant privacy risks,\nas service providers may exploit users' trace patterns and behavioral data. In\nthis paper, we propose a practical and privacy-preserving framework that\nensures user anonymity by preventing service providers from linking requests to\nthe individuals who submit them. Our framework is built on partially blind\nsignatures, which guarantee the unlinkability of user requests. Furthermore, we\nintroduce two strategies tailored to both subscription-based and API-based\nservice models, ensuring the protection of both users' privacy and service\nproviders' interests. The framework is designed to integrate seamlessly with\nexisting LLM systems, as it does not require modifications to the underlying\narchitectures. Experimental results demonstrate that our framework incurs\nminimal computation and communication overhead, making it a feasible solution\nfor real-world applications.",
    "updated" : "2024-11-03T07:40:28Z",
    "published" : "2024-11-03T07:40:28Z",
    "authors" : [
      {
        "name" : "Yu Mao"
      },
      {
        "name" : "Xueping Liao"
      },
      {
        "name" : "Wei Liu"
      },
      {
        "name" : "Anjia Yang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01447v1",
    "title" : "Privacy-Preserving Customer Churn Prediction Model in the Context of\n  Telecommunication Industry",
    "summary" : "Data is the main fuel of a successful machine learning model. A dataset may\ncontain sensitive individual records e.g. personal health records, financial\ndata, industrial information, etc. Training a model using this sensitive data\nhas become a new privacy concern when someone uses third-party cloud computing.\nTrained models also suffer privacy attacks which leads to the leaking of\nsensitive information of the training data. This study is conducted to preserve\nthe privacy of training data in the context of customer churn prediction\nmodeling for the telecommunications industry (TCI). In this work, we propose a\nframework for privacy-preserving customer churn prediction (PPCCP) model in the\ncloud environment. We have proposed a novel approach which is a combination of\nGenerative Adversarial Networks (GANs) and adaptive Weight-of-Evidence (aWOE).\nSynthetic data is generated from GANs, and aWOE is applied on the synthetic\ntraining dataset before feeding the data to the classification algorithms. Our\nexperiments were carried out using eight different machine learning (ML)\nclassifiers on three openly accessible datasets from the telecommunication\nsector. We then evaluated the performance using six commonly employed\nevaluation metrics. In addition to presenting a data privacy analysis, we also\nperformed a statistical significance test. The training and prediction\nprocesses achieve data privacy and the prediction classifiers achieve high\nprediction performance (87.1\\% in terms of F-Measure for GANs-aWOE based\nNa\\\"{\\i}ve Bayes model). In contrast to earlier studies, our suggested approach\ndemonstrates a prediction enhancement of up to 28.9\\% and 27.9\\% in terms of\naccuracy and F-measure, respectively.",
    "updated" : "2024-11-03T06:08:59Z",
    "published" : "2024-11-03T06:08:59Z",
    "authors" : [
      {
        "name" : "Joydeb Kumar Sana"
      },
      {
        "name" : "M Sohel Rahman"
      },
      {
        "name" : "M Saifur Rahman"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01357v1",
    "title" : "WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy\n  Principles",
    "summary" : "In this paper, we introduce WaKA (Wasserstein K-nearest neighbors\nAttribution), a novel attribution method that leverages principles from the\nLiRA (Likelihood Ratio Attack) framework and applies them to \\( k \\)-nearest\nneighbors classifiers (\\( k \\)-NN). WaKA efficiently measures the contribution\nof individual data points to the model's loss distribution, analyzing every\npossible \\( k \\)-NN that can be constructed using the training set, without\nrequiring sampling or shadow model training. WaKA can be used \\emph{a\nposteriori} as a membership inference attack (MIA) to assess privacy risks, and\n\\emph{a priori} for data minimization and privacy influence measurement. Thus,\nWaKA can be seen as bridging the gap between data attribution and membership\ninference attack (MIA) literature by distinguishing between the value of a data\npoint and its privacy risk. For instance, we show that self-attribution values\nare more strongly correlated with the attack success rate than the contribution\nof a point to model generalization. WaKA's different usages were also evaluated\nacross diverse real-world datasets, demonstrating performance very close to\nLiRA when used as an MIA on \\( k \\)-NN classifiers, but with greater\ncomputational efficiency.",
    "updated" : "2024-11-02T20:27:51Z",
    "published" : "2024-11-02T20:27:51Z",
    "authors" : [
      {
        "name" : "Patrick Mesana"
      },
      {
        "name" : "Clément Bénesse"
      },
      {
        "name" : "Hadrien Lautraite"
      },
      {
        "name" : "Gilles Caporossi"
      },
      {
        "name" : "Sébastien Gambs"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01344v1",
    "title" : "Can Humans Oversee Agents to Prevent Privacy Leakage? A Study on Privacy\n  Awareness, Preferences, and Trust in Language Model Agents",
    "summary" : "Language model (LM) agents that act on users' behalf for personal tasks can\nboost productivity, but are also susceptible to unintended privacy leakage\nrisks. We present the first study on people's capacity to oversee the privacy\nimplications of the LM agents. By conducting a task-based survey (N=300), we\ninvestigate how people react to and assess the response generated by LM agents\nfor asynchronous interpersonal communication tasks, compared with a response\nthey wrote. We found that people may favor the agent response with more privacy\nleakage over the response they drafted or consider both good, leading to an\nincreased harmful disclosure from 15.7% to 55.0%. We further uncovered distinct\npatterns of privacy behaviors, attitudes, and preferences, and the nuanced\ninteractions between privacy considerations and other factors. Our findings\nshed light on designing agentic systems that enable privacy-preserving\ninteractions and achieve bidirectional alignment on privacy preferences to help\nusers calibrate trust.",
    "updated" : "2024-11-02T19:15:42Z",
    "published" : "2024-11-02T19:15:42Z",
    "authors" : [
      {
        "name" : "Zhiping Zhang"
      },
      {
        "name" : "Bingcan Guo"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01308v1",
    "title" : "ECG-PPS: Privacy Preserving Disease Diagnosis and Monitoring System for\n  Real-Time ECG Signal",
    "summary" : "This study introduces the development of a state of the art, real time ECG\nmonitoring and analysis system, incorporating cutting edge medical technology\nand innovative data security measures. Our system performs three distinct\nfunctions thaat real time ECG monitoring and disease detection, encrypted\nstorage and synchronized visualization, and statistical analysis on encrypted\ndata. At its core, the system uses a three lead ECG preamplifier connected\nthrough a serial port to capture, display, and record real time ECG data. These\nsignals are securely stored in the cloud using robust encryption methods.\nAuthorized medical personnel can access and decrypt this data on their\ncomputers, with AES encryption ensuring synchronized real time data tracking\nand visualization. Furthermore, the system performs statistical operations on\nthe ECG data stored in the cloud without decrypting it, using Fully Homomorphic\nEncryption (FHE). This enables privacy preserving data analysis while ensuring\nthe security and confidentiality of patient information. By integrating these\nindependent functions, our system significantly enhances the security and\nefficiency of health monitoring. It supports critical tasks such as disease\ndetection, patient monitoring, and preliminary intervention, all while\nupholding stringent data privacy standards. We provided detailed discussions on\nthe system's architecture, hardware configuration, software implementation, and\nclinical performance. The results highlight the potential of this system to\nimprove patient care through secure and efficient ECG monitoring and analysis.\nThis work represents a significant leap forward in medical technology. By\nincorporating FHE into both data transmission and storage processes, we ensure\ncontinuous encryption of data throughout its lifecycle while enabling real time\ndisease diagnosis.",
    "updated" : "2024-11-02T17:03:25Z",
    "published" : "2024-11-02T17:03:25Z",
    "authors" : [
      {
        "name" : "Beyazit Bestami Yuksel"
      },
      {
        "name" : "Ayse Yilmazer Metin"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01140v1",
    "title" : "Privacy-Preserving Federated Learning with Differentially Private\n  Hyperdimensional Computing",
    "summary" : "Federated Learning (FL) is essential for efficient data exchange in Internet\nof Things (IoT) environments, as it trains Machine Learning (ML) models locally\nand shares only model updates. However, FL is vulnerable to privacy threats\nlike model inversion and membership inference attacks, which can expose\nsensitive training data. To address these privacy concerns, Differential\nPrivacy (DP) mechanisms are often applied. Yet, adding DP noise to black-box ML\nmodels degrades performance, especially in dynamic IoT systems where\ncontinuous, lifelong FL learning accumulates excessive noise over time. To\nmitigate this issue, we introduce Federated HyperDimensional computing with\nPrivacy-preserving (FedHDPrivacy), an eXplainable Artificial Intelligence (XAI)\nframework that combines the neuro-symbolic paradigm with DP. FedHDPrivacy\ncarefully manages the balance between privacy and performance by theoretically\ntracking cumulative noise from previous rounds and adding only the necessary\nincremental noise to meet privacy requirements. In a real-world case study\ninvolving in-process monitoring of manufacturing machining operations,\nFedHDPrivacy demonstrates robust performance, outperforming standard FL\nframeworks-including Federated Averaging (FedAvg), Federated Stochastic\nGradient Descent (FedSGD), Federated Proximal (FedProx), Federated Normalized\nAveraging (FedNova), and Federated Adam (FedAdam)-by up to 38%. FedHDPrivacy\nalso shows potential for future enhancements, such as multimodal data fusion.",
    "updated" : "2024-11-02T05:00:44Z",
    "published" : "2024-11-02T05:00:44Z",
    "authors" : [
      {
        "name" : "Fardin Jalil Piran"
      },
      {
        "name" : "Zhiling Chen"
      },
      {
        "name" : "Mohsen Imani"
      },
      {
        "name" : "Farhad Imani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01076v1",
    "title" : "Privacy Risks of Speculative Decoding in Large Language Models",
    "summary" : "Speculative decoding in large language models (LLMs) accelerates token\ngeneration by speculatively predicting multiple tokens cheaply and verifying\nthem in parallel, and has been widely deployed. In this paper, we provide the\nfirst study demonstrating the privacy risks of speculative decoding. We observe\nthat input-dependent patterns of correct and incorrect predictions can be\nleaked out to an adversary monitoring token generation times and packet sizes,\nleading to privacy breaches. By observing the pattern of correctly and\nincorrectly speculated tokens, we show that a malicious adversary can\nfingerprint queries and learn private user inputs with more than $90\\%$\naccuracy across three different speculative decoding techniques - BiLD (almost\n$100\\%$ accuracy), LADE (up to $92\\%$ accuracy), and REST (up to $95\\%$\naccuracy). We show that an adversary can also leak out confidential\nintellectual property used to design these techniques, such as data from\ndata-stores used for prediction (in REST) at a rate of more than $25$ tokens\nper second, or even hyper-parameters used for prediction (in LADE). We also\ndiscuss mitigation strategies, such as aggregating tokens across multiple\niterations and padding packets with additional bytes, to avoid such privacy or\nconfidentiality breaches.",
    "updated" : "2024-11-01T23:14:30Z",
    "published" : "2024-11-01T23:14:30Z",
    "authors" : [
      {
        "name" : "Jiankun Wei"
      },
      {
        "name" : "Abdulrahman Abdulrazzag"
      },
      {
        "name" : "Tianchen Zhang"
      },
      {
        "name" : "Adel Muursepp"
      },
      {
        "name" : "Gururaj Saileshwar"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01050v1",
    "title" : "BACSA: A Bias-Aware Client Selection Algorithm for Privacy-Preserving\n  Federated Learning in Wireless Healthcare Networks",
    "summary" : "Federated Learning (FL) has emerged as a transformative approach in\nhealthcare, enabling collaborative model training across decentralized data\nsources while preserving user privacy. However, performance of FL rapidly\ndegrades in practical scenarios due to the inherent bias in non Independent and\nIdentically distributed (non-IID) data among participating clients, which poses\nsignificant challenges to model accuracy and generalization. Therefore, we\npropose the Bias-Aware Client Selection Algorithm (BACSA), which detects user\nbias and strategically selects clients based on their bias profiles. In\naddition, the proposed algorithm considers privacy preservation, fairness and\nconstraints of wireless network environments, making it suitable for sensitive\nhealthcare applications where Quality of Service (QoS), privacy and security\nare paramount. Our approach begins with a novel method for detecting user bias\nby analyzing model parameters and correlating them with the distribution of\nclass-specific data samples. We then formulate a mixed-integer non-linear\nclient selection problem leveraging the detected bias, alongside wireless\nnetwork constraints, to optimize FL performance. We demonstrate that BACSA\nimproves convergence and accuracy, compared to existing benchmarks, through\nevaluations on various data distributions, including Dirichlet and\nclass-constrained scenarios. Additionally, we explore the trade-offs between\naccuracy, fairness, and network constraints, indicating the adaptability and\nrobustness of BACSA to address diverse healthcare applications.",
    "updated" : "2024-11-01T21:34:43Z",
    "published" : "2024-11-01T21:34:43Z",
    "authors" : [
      {
        "name" : "Sushilkumar Yadav"
      },
      {
        "name" : "Irem Bor-Yaliniz"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03053v1",
    "title" : "Gradient-Guided Conditional Diffusion Models for Private Image\n  Reconstruction: Analyzing Adversarial Impacts of Differential Privacy and\n  Denoising",
    "summary" : "We investigate the construction of gradient-guided conditional diffusion\nmodels for reconstructing private images, focusing on the adversarial interplay\nbetween differential privacy noise and the denoising capabilities of diffusion\nmodels. While current gradient-based reconstruction methods struggle with\nhigh-resolution images due to computational complexity and prior knowledge\nrequirements, we propose two novel methods that require minimal modifications\nto the diffusion model's generation process and eliminate the need for prior\nknowledge. Our approach leverages the strong image generation capabilities of\ndiffusion models to reconstruct private images starting from randomly generated\nnoise, even when a small amount of differentially private noise has been added\nto the gradients. We also conduct a comprehensive theoretical analysis of the\nimpact of differential privacy noise on the quality of reconstructed images,\nrevealing the relationship among noise magnitude, the architecture of attacked\nmodels, and the attacker's reconstruction capability. Additionally, extensive\nexperiments validate the effectiveness of our proposed methods and the accuracy\nof our theoretical findings, suggesting new directions for privacy risk\nauditing using conditional diffusion models.",
    "updated" : "2024-11-05T12:39:21Z",
    "published" : "2024-11-05T12:39:21Z",
    "authors" : [
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Hong Chen"
      },
      {
        "name" : "Guolong Zheng"
      },
      {
        "name" : "Xu Yang"
      },
      {
        "name" : "Xun Yi"
      },
      {
        "name" : "Hua Wang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.02926v1",
    "title" : "Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic\n  Encryption for Collaborative Anti-Money Laundering",
    "summary" : "Combating money laundering has become increasingly complex with the rise of\ncybercrime and digitalization of financial transactions. Graph-based machine\nlearning techniques have emerged as promising tools for Anti-Money Laundering\n(AML) detection, capturing intricate relationships within money laundering\nnetworks. However, the effectiveness of AML solutions is hindered by data silos\nwithin financial institutions, limiting collaboration and overall efficacy.\nThis research presents a novel privacy-preserving approach for collaborative\nAML machine learning, facilitating secure data sharing across institutions and\nborders while preserving privacy and regulatory compliance. Leveraging Fully\nHomomorphic Encryption (FHE), computations are directly performed on encrypted\ndata, ensuring the confidentiality of financial data. Notably, FHE over the\nTorus (TFHE) was integrated with graph-based machine learning using Zama\nConcrete ML. The research contributes two key privacy-preserving pipelines.\nFirst, the development of a privacy-preserving Graph Neural Network (GNN)\npipeline was explored. Optimization techniques like quantization and pruning\nwere used to render the GNN FHE-compatible. Second, a privacy-preserving\ngraph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was\nsuccessfully developed. Experiments demonstrated strong predictive performance,\nwith the XGBoost model consistently achieving over 99% accuracy, F1-score,\nprecision, and recall on the balanced AML dataset in both unencrypted and\nFHE-encrypted inference settings. On the imbalanced dataset, the incorporation\nof graph-based features improved the F1-score by 8%. The research highlights\nthe need to balance the trade-off between privacy and computational efficiency.",
    "updated" : "2024-11-05T09:13:53Z",
    "published" : "2024-11-05T09:13:53Z",
    "authors" : [
      {
        "name" : "Fabrianne Effendi"
      },
      {
        "name" : "Anupam Chattopadhyay"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.02622v1",
    "title" : "Pseudo-Probability Unlearning: Towards Efficient and Privacy-Preserving\n  Machine Unlearning",
    "summary" : "Machine unlearning--enabling a trained model to forget specific data--is\ncrucial for addressing biased data and adhering to privacy regulations like the\nGeneral Data Protection Regulation (GDPR)'s \"right to be forgotten\". Recent\nworks have paid little attention to privacy concerns, leaving the data intended\nfor forgetting vulnerable to membership inference attacks. Moreover, they often\ncome with high computational overhead. In this work, we propose\nPseudo-Probability Unlearning (PPU), a novel method that enables models to\nforget data efficiently and in a privacy-preserving manner. Our method replaces\nthe final-layer output probabilities of the neural network with\npseudo-probabilities for the data to be forgotten. These pseudo-probabilities\nfollow either a uniform distribution or align with the model's overall\ndistribution, enhancing privacy and reducing risk of membership inference\nattacks. Our optimization strategy further refines the predictive probability\ndistributions and updates the model's weights accordingly, ensuring effective\nforgetting with minimal impact on the model's overall performance. Through\ncomprehensive experiments on multiple benchmarks, our method achieves over 20%\nimprovements in forgetting error compared to the state-of-the-art.\nAdditionally, our method enhances privacy by preventing the forgotten set from\nbeing inferred to around random guesses.",
    "updated" : "2024-11-04T21:27:06Z",
    "published" : "2024-11-04T21:27:06Z",
    "authors" : [
      {
        "name" : "Zihao Zhao"
      },
      {
        "name" : "Yijiang Li"
      },
      {
        "name" : "Yuchen Yang"
      },
      {
        "name" : "Wenqing Zhang"
      },
      {
        "name" : "Nuno Vasconcelos"
      },
      {
        "name" : "Yinzhi Cao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.01076v2",
    "title" : "Privacy Risks of Speculative Decoding in Large Language Models",
    "summary" : "Speculative decoding in large language models (LLMs) accelerates token\ngeneration by speculatively predicting multiple tokens cheaply and verifying\nthem in parallel, and has been widely deployed. In this paper, we provide the\nfirst study demonstrating the privacy risks of speculative decoding. We observe\nthat input-dependent patterns of correct and incorrect predictions can be\nleaked out to an adversary monitoring token generation times and packet sizes,\nleading to privacy breaches. By observing the pattern of correctly and\nincorrectly speculated tokens, we show that a malicious adversary can\nfingerprint queries and learn private user inputs with more than $90\\%$\naccuracy across three different speculative decoding techniques - REST (almost\n$100\\%$ accuracy), LADE (up to $92\\%$ accuracy), and BiLD (up to $95\\%$\naccuracy). We show that an adversary can also leak out confidential\nintellectual property used to design these techniques, such as data from\ndata-stores used for prediction (in REST) at a rate of more than $25$ tokens\nper second, or even hyper-parameters used for prediction (in LADE). We also\ndiscuss mitigation strategies, such as aggregating tokens across multiple\niterations and padding packets with additional bytes, to avoid such privacy or\nconfidentiality breaches.",
    "updated" : "2024-11-05T15:03:45Z",
    "published" : "2024-11-01T23:14:30Z",
    "authors" : [
      {
        "name" : "Jiankun Wei"
      },
      {
        "name" : "Abdulrahman Abdulrazzag"
      },
      {
        "name" : "Tianchen Zhang"
      },
      {
        "name" : "Adel Muursepp"
      },
      {
        "name" : "Gururaj Saileshwar"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03914v1",
    "title" : "Game-Theoretic Machine Unlearning: Mitigating Extra Privacy Leakage",
    "summary" : "With the extensive use of machine learning technologies, data providers\nencounter increasing privacy risks. Recent legislation, such as GDPR, obligates\norganizations to remove requested data and its influence from a trained model.\nMachine unlearning is an emerging technique designed to enable machine learning\nmodels to erase users' private information. Although several efficient machine\nunlearning schemes have been proposed, these methods still have limitations.\nFirst, removing the contributions of partial data may lead to model performance\ndegradation. Second, discrepancies between the original and generated unlearned\nmodels can be exploited by attackers to obtain target sample's information,\nresulting in additional privacy leakage risks. To address above challenges, we\nproposed a game-theoretic machine unlearning algorithm that simulates the\ncompetitive relationship between unlearning performance and privacy protection.\nThis algorithm comprises unlearning and privacy modules. The unlearning module\npossesses a loss function composed of model distance and classification error,\nwhich is used to derive the optimal strategy. The privacy module aims to make\nit difficult for an attacker to infer membership information from the unlearned\ndata, thereby reducing the privacy leakage risk during the unlearning process.\nAdditionally, the experimental results on real-world datasets demonstrate that\nthis game-theoretic unlearning algorithm's effectiveness and its ability to\ngenerate an unlearned model with a performance similar to that of the retrained\none while mitigating extra privacy leakage risks.",
    "updated" : "2024-11-06T13:47:04Z",
    "published" : "2024-11-06T13:47:04Z",
    "authors" : [
      {
        "name" : "Hengzhu Liu"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Lefeng Zhang"
      },
      {
        "name" : "Ping Xiong"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03847v1",
    "title" : "A Novel Access Control and Privacy-Enhancing Approach for Models in Edge\n  Computing",
    "summary" : "With the widespread adoption of edge computing technologies and the\nincreasing prevalence of deep learning models in these environments, the\nsecurity risks and privacy threats to models and data have grown more acute.\nAttackers can exploit various techniques to illegally obtain models or misuse\ndata, leading to serious issues such as intellectual property infringement and\nprivacy breaches. Existing model access control technologies primarily rely on\ntraditional encryption and authentication methods; however, these approaches\nexhibit significant limitations in terms of flexibility and adaptability in\ndynamic environments. Although there have been advancements in model\nwatermarking techniques for marking model ownership, they remain limited in\ntheir ability to proactively protect intellectual property and prevent\nunauthorized access. To address these challenges, we propose a novel model\naccess control method tailored for edge computing environments. This method\nleverages image style as a licensing mechanism, embedding style recognition\ninto the model's operational framework to enable intrinsic access control.\nConsequently, models deployed on edge platforms are designed to correctly infer\nonly on license data with specific style, rendering them ineffective on any\nother data. By restricting the input data to the edge model, this approach not\nonly prevents attackers from gaining unauthorized access to the model but also\nenhances the privacy of data on terminal devices. We conducted extensive\nexperiments on benchmark datasets, including MNIST, CIFAR-10, and FACESCRUB,\nand the results demonstrate that our method effectively prevents unauthorized\naccess to the model while maintaining accuracy. Additionally, the model shows\nstrong resistance against attacks such as forged licenses and fine-tuning.\nThese results underscore the method's usability, security, and robustness.",
    "updated" : "2024-11-06T11:37:30Z",
    "published" : "2024-11-06T11:37:30Z",
    "authors" : [
      {
        "name" : "Peihao Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03730v1",
    "title" : "NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document\n  VQA",
    "summary" : "The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA)\ncompetition challenged the community to develop provably private and\ncommunication-efficient solutions in a federated setting for a real-life use\ncase: invoice processing. The competition introduced a dataset of real invoice\ndocuments, along with associated questions and answers requiring information\nextraction and reasoning over the document images. Thereby, it brings together\nresearchers and expertise from the document analysis, privacy, and federated\nlearning communities. Participants fine-tuned a pre-trained, state-of-the-art\nDocument Visual Question Answering model provided by the organizers for this\nnew domain, mimicking a typical federated invoice processing setup. The base\nmodel is a multi-modal generative language model, and sensitive information\ncould be exposed through either the visual or textual input modality.\nParticipants proposed elegant solutions to reduce communication costs while\nmaintaining a minimum utility threshold in track 1 and to protect all\ninformation from each document provider using differential privacy in track 2.\nThe competition served as a new testbed for developing and testing private\nfederated learning methods, simultaneously raising awareness about privacy\nwithin the document image analysis and recognition community. Ultimately, the\ncompetition analysis provides best practices and recommendations for\nsuccessfully running privacy-focused federated learning challenges in the\nfuture.",
    "updated" : "2024-11-06T07:51:19Z",
    "published" : "2024-11-06T07:51:19Z",
    "authors" : [
      {
        "name" : "Marlon Tobaben"
      },
      {
        "name" : "Mohamed Ali Souibgui"
      },
      {
        "name" : "Rubèn Tito"
      },
      {
        "name" : "Khanh Nguyen"
      },
      {
        "name" : "Raouf Kerkouche"
      },
      {
        "name" : "Kangsoo Jung"
      },
      {
        "name" : "Joonas Jälkö"
      },
      {
        "name" : "Lei Kang"
      },
      {
        "name" : "Andrey Barsky"
      },
      {
        "name" : "Vincent Poulain d'Andecy"
      },
      {
        "name" : "Aurélie Joseph"
      },
      {
        "name" : "Aashiq Muhamed"
      },
      {
        "name" : "Kevin Kuo"
      },
      {
        "name" : "Virginia Smith"
      },
      {
        "name" : "Yusuke Yamasaki"
      },
      {
        "name" : "Takumi Fukami"
      },
      {
        "name" : "Kenta Niwa"
      },
      {
        "name" : "Iifan Tyou"
      },
      {
        "name" : "Hiro Ishii"
      },
      {
        "name" : "Rio Yokota"
      },
      {
        "name" : "Ragul N"
      },
      {
        "name" : "Rintu Kutum"
      },
      {
        "name" : "Josep Llados"
      },
      {
        "name" : "Ernest Valveny"
      },
      {
        "name" : "Antti Honkela"
      },
      {
        "name" : "Mario Fritz"
      },
      {
        "name" : "Dimosthenis Karatzas"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03633v1",
    "title" : "Privacy-Preserving Resilient Vector Consensus",
    "summary" : "This paper studies privacy-preserving resilient vector consensus in\nmulti-agent systems against faulty agents, where normal agents can achieve\nconsensus within the convex hull of their initial states while protecting state\nvectors from being disclosed. Specifically, we consider a modification of an\nexisting algorithm known as Approximate Distributed Robust Convergence Using\nCenterpoints (ADRC), i.e., Privacy-Preserving ADRC (PP-ADRC). Under PP-ADRC,\neach normal agent introduces multivariate Gaussian noise to its state during\neach iteration. We first provide sufficient conditions to ensure that all\nnormal agents' states can achieve mean square convergence under PP-ADRC. Then,\nwe analyze convergence accuracy from two perspectives, i.e., the Mahalanobis\ndistance of the final value from its expectation and the Hausdorff distance\nbased alteration of the convex hull caused by noise when only partial\ndimensions are added with noise. Then, we employ concentrated geo-privacy to\ncharacterize privacy preservation and conduct a thorough comparison with\ndifferential privacy. Finally, numerical simulations demonstrate the\ntheoretical results.",
    "updated" : "2024-11-06T03:17:29Z",
    "published" : "2024-11-06T03:17:29Z",
    "authors" : [
      {
        "name" : "Bing Liu"
      },
      {
        "name" : "Chengcheng Zhao"
      },
      {
        "name" : "Li Chai"
      },
      {
        "name" : "Peng Cheng"
      },
      {
        "name" : "Jiming Chen"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03582v1",
    "title" : "Privacy Preserving Mechanisms for Coordinating Airspace Usage in\n  Advanced Air Mobility",
    "summary" : "Advanced Air Mobility (AAM) operations are expected to transform air\ntransportation while challenging current air traffic management practices. By\nintroducing a novel market-based mechanism, we address the problem of on-demand\nallocation of capacity-constrained airspace to AAM vehicles with heterogeneous\nand private valuations. We model airspace and air infrastructure as a\ncollection of contiguous regions with constraints on the number of vehicles\nthat simultaneously enter, stay, or exit each region. Vehicles request access\nto the airspace with trajectories spanning multiple regions at different times.\nWe use the graph structure of our airspace model to formulate the allocation\nproblem as a path allocation problem on a time-extended graph. To ensure the\ncost information of AAM vehicles remains private, we introduce a novel\nmechanism that allocates each vehicle a budget of \"air-credits\" and anonymously\ncharges prices for traversing the edges of the time-extended graph. We seek to\ncompute a competitive equilibrium that ensures that: (i) capacity constraints\nare satisfied, (ii) a strictly positive resource price implies that the sector\ncapacity is fully utilized, and (iii) the allocation is integral and optimal\nfor each AAM vehicle given current prices, without requiring access to\nindividual vehicle utilities. However, a competitive equilibrium with integral\nallocations may not always exist. We provide sufficient conditions for the\nexistence and computation of a fractional-competitive equilibrium, where\nallocations can be fractional. Building on these theoretical insights, we\npropose a distributed, iterative, two-step algorithm that: 1) computes a\nfractional competitive equilibrium, and 2) derives an integral allocation from\nthis equilibrium. We validate the effectiveness of our approach in allocating\ntrajectories for two emerging urban air mobility services: drone delivery and\nair taxis.",
    "updated" : "2024-11-06T00:47:54Z",
    "published" : "2024-11-06T00:47:54Z",
    "authors" : [
      {
        "name" : "Chinmay Maheshwari"
      },
      {
        "name" : "Maria G. Mendoza"
      },
      {
        "name" : "Victoria Marie Tuck"
      },
      {
        "name" : "Pan-Yang Su"
      },
      {
        "name" : "Victor L. Qin"
      },
      {
        "name" : "Sanjit A. Seshia"
      },
      {
        "name" : "Hamsa Balakrishnan"
      },
      {
        "name" : "Shankar Sastry"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY",
      "91B03, 91A68, 90B06, 90C27"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2411.03351v1",
    "title" : "Tabular Data Synthesis with Differential Privacy: A Survey",
    "summary" : "Data sharing is a prerequisite for collaborative innovation, enabling\norganizations to leverage diverse datasets for deeper insights. In real-world\napplications like FinTech and Smart Manufacturing, transactional data, often in\ntabular form, are generated and analyzed for insight generation. However, such\ndatasets typically contain sensitive personal/business information, raising\nprivacy concerns and regulatory risks. Data synthesis tackles this by\ngenerating artificial datasets that preserve the statistical characteristics of\nreal data, removing direct links to individuals. However, attackers can still\ninfer sensitive information using background knowledge. Differential privacy\noffers a solution by providing provable and quantifiable privacy protection.\nConsequently, differentially private data synthesis has emerged as a promising\napproach to privacy-aware data sharing. This paper provides a comprehensive\noverview of existing differentially private tabular data synthesis methods,\nhighlighting the unique challenges of each generation model for generating\ntabular data under differential privacy constraints. We classify the methods\ninto statistical and deep learning-based approaches based on their generation\nmodels, discussing them in both centralized and distributed environments. We\nevaluate and compare those methods within each category, highlighting their\nstrengths and weaknesses in terms of utility, privacy, and computational\ncomplexity. Additionally, we present and discuss various evaluation methods for\nassessing the quality of the synthesized data, identify research gaps in the\nfield and directions for future research.",
    "updated" : "2024-11-04T06:32:48Z",
    "published" : "2024-11-04T06:32:48Z",
    "authors" : [
      {
        "name" : "Mengmeng Yang"
      },
      {
        "name" : "Chi-Hung Chi"
      },
      {
        "name" : "Kwok-Yan Lam"
      },
      {
        "name" : "Jie Feng"
      },
      {
        "name" : "Taolin Guo"
      },
      {
        "name" : "Wei Ni"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ]
  }
]