[
  {
    "id" : "http://arxiv.org/abs/2602.02364v1",
    "title" : "Guaranteeing Privacy in Hybrid Quantum Learning through Theoretical Mechanisms",
    "summary" : "Quantum Machine Learning (QML) is becoming increasingly prevalent due to its potential to enhance classical machine learning (ML) tasks, such as classification. Although quantum noise is often viewed as a major challenge in quantum computing, it also offers a unique opportunity to enhance privacy. In particular, intrinsic quantum noise provides a natural stochastic resource that, when rigorously analyzed within the differential privacy (DP) framework and composed with classical mechanisms, can satisfy formal $(\\varepsilon, δ)$-DP guarantees. This enables a reduction in the required classical perturbation without compromising the privacy budget, potentially improving model utility. However, the integration of classical and quantum noise for privacy preservation remains unexplored. In this work, we propose a hybrid noise-added mechanism, HYPER-Q, that combines classical and quantum noise to protect the privacy of QML models. We provide a comprehensive analysis of its privacy guarantees and establish theoretical bounds on its utility. Empirically, we demonstrate that HYPER-Q outperforms existing classical noise-based mechanisms in terms of adversarial robustness across multiple real-world datasets.",
    "updated" : "2026-02-02T17:23:37Z",
    "published" : "2026-02-02T17:23:37Z",
    "authors" : [
      {
        "name" : "Hoang M. Ngo"
      },
      {
        "name" : "Tre' R. Jeter"
      },
      {
        "name" : "Incheol Shin"
      },
      {
        "name" : "Wanli Xing"
      },
      {
        "name" : "Tamer Kahveci"
      },
      {
        "name" : "My T. Thai"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02296v1",
    "title" : "Decoupling Generalizability and Membership Privacy Risks in Neural Networks",
    "summary" : "A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.",
    "updated" : "2026-02-02T16:32:42Z",
    "published" : "2026-02-02T16:32:42Z",
    "authors" : [
      {
        "name" : "Xingli Fang"
      },
      {
        "name" : "Jung-Eun Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01928v1",
    "title" : "Privacy Amplification by Missing Data",
    "summary" : "Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.",
    "updated" : "2026-02-02T10:28:41Z",
    "published" : "2026-02-02T10:28:41Z",
    "authors" : [
      {
        "name" : "Simon Roburin"
      },
      {
        "name" : "Rafaël Pinot"
      },
      {
        "name" : "Erwan Scornet"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01804v1",
    "title" : "Fostering Data Collaboration in Digital Transportation Marketplaces: The Role of Privacy-Preserving Mechanisms",
    "summary" : "Data collaboration between municipal authorities (MA) and mobility providers (MPs) has brought tremendous benefits to transportation systems in the era of big data. Engaging in collaboration can improve the service operations (e.g., reduced delay) of these data owners, however, it can also raise privacy concerns and discourage data-sharing willingness. Specifically, data owners may be concerned that the shared data may leak sensitive information about their customers' mobility patterns or business secrets, resulting in the failure of collaboration. This paper investigates how privacy-preserving mechanisms can foster data collaboration in such settings. We propose a game-theoretic framework to investigate data-sharing among transportation stakeholders, especially considering perturbation-based privacy-preserving mechanisms. Numerical studies demonstrate that lower data quality expectations can incentivize voluntary data sharing, improving transport-related welfare for both MAs and MPs. Our findings provide actionable insights for policymakers and system designers on how privacy-preserving technologies can help bridge data silos and promote collaborative, privacy-aware transportation systems.",
    "updated" : "2026-02-02T08:35:29Z",
    "published" : "2026-02-02T08:35:29Z",
    "authors" : [
      {
        "name" : "Qiqing Wang"
      },
      {
        "name" : "Haokun Yu"
      },
      {
        "name" : "Kaidi Yang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.GT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01387v1",
    "title" : "Disclose with Care: Designing Privacy Controls in Interview Chatbots",
    "summary" : "Collecting data on sensitive topics remains challenging in HCI, as participants often withhold information due to privacy concerns and social desirability bias. While chatbots' perceived anonymity may reduce these barriers, research paradoxically suggests people tend to over-share personal or sensitive information with chatbots. In this work, we explore privacy controls in chatbot interviews to address this problem. The privacy control allows participants to revise their transcripts at the end of the interview, featuring two design variants: free editing and AI-aided editing. In a between-subjects study \\red{($N=188$)}, we compared no-editing, free-editing, and AI-aided editing conditions in a chatbot-based interview on a sensitive topic. Our results confirm the prevalent issue of oversharing in chatbot-based interviews and show that AI-aided editing serves as an effective privacy-control mechanism, reducing PII disclosure while maintaining data quality and user engagement, thereby offering a promising approach to balancing ethical practice and data quality in such interviews.",
    "updated" : "2026-02-01T18:48:19Z",
    "published" : "2026-02-01T18:48:19Z",
    "authors" : [
      {
        "name" : "Ziwen Li"
      },
      {
        "name" : "Ziang Xiao"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01177v1",
    "title" : "Equivalence of Privacy and Stability with Generalization Guarantees in Quantum Learning",
    "summary" : "We present a unified information-theoretic framework to analyze the generalization performance of differentially private (DP) quantum learning algorithms. By leveraging the connection between privacy and algorithmic stability, we establish that $(\\varepsilon, δ)$-Quantum Differential Privacy (QDP) imposes a strong constraint on the mutual information between the training data and the algorithm's output. We derive a rigorous, mechanism-agnostic upper bound on this mutual information for learning algorithms satisfying a 1-neighbor privacy constraint. Furthermore, we connect this stability guarantee to generalization, proving that the expected generalization error of any $(\\varepsilon, δ)$-QDP learning algorithm is bounded by the square root of the privacy-induced stability term. Finally, we extend our framework to the setting of an untrusted Data Processor, introducing the concept of Information-Theoretic Admissibility (ITA) to characterize the fundamental limits of privacy in scenarios where the learning map itself must remain oblivious to the specific dataset instance.",
    "updated" : "2026-02-01T12:03:07Z",
    "published" : "2026-02-01T12:03:07Z",
    "authors" : [
      {
        "name" : "Ayanava Dasgupta"
      },
      {
        "name" : "Naqueeb Ahmad Warsi"
      },
      {
        "name" : "Masahito Hayashi"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01126v1",
    "title" : "WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity",
    "summary" : "Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.",
    "updated" : "2026-02-01T09:52:57Z",
    "published" : "2026-02-01T09:52:57Z",
    "authors" : [
      {
        "name" : "Mengsha Kou"
      },
      {
        "name" : "Xiaoyu Xia"
      },
      {
        "name" : "Ziqi Wang"
      },
      {
        "name" : "Ibrahim Khalil"
      },
      {
        "name" : "Runkun Luo"
      },
      {
        "name" : "Jingwen Zhou"
      },
      {
        "name" : "Minhui Xue"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03671v1",
    "title" : "mopri - An Analysis Framework for Unveiling Privacy Violations in Mobile Apps",
    "summary" : "Everyday services of society increasingly rely on mobile applications, resulting in a conflicting situation between the possibility of participation on the one side and user privacy and digital freedom on the other. In order to protect users' rights to informational self-determination, regulatory approaches for the collection and processing of personal data have been developed, such as the EU's GDPR. However, inspecting the compliance of mobile apps with privacy regulations remains difficult. Thus, in order to enable end users and enforcement bodies to verify and enforce data protection compliance, we propose mopri, a conceptual framework designed for analyzing the behavior of mobile apps through a comprehensive, adaptable, and user-centered approach. Recognizing the gaps in existing frameworks, mopri serves as a foundation for integrating various analysis tools into a streamlined, modular pipeline that employs static and dynamic analysis methods. Building on this concept, a prototype has been developed which effectively extracts permissions and tracking libraries while employing robust methods for dynamic traffic recording and decryption. Additionally, it incorporates result enrichment and reporting features that enhance the clarity and usability of the analysis outcomes. The prototype showcases the feasibility of a holistic and modular approach to privacy analysis, emphasizing the importance of continuous adaptation to the evolving challenges presented by the mobile app ecosystem.",
    "updated" : "2026-02-03T15:52:31Z",
    "published" : "2026-02-03T15:52:31Z",
    "authors" : [
      {
        "name" : "Cornell Ziepel"
      },
      {
        "name" : "Stephan Escher"
      },
      {
        "name" : "Sebastian Rehms"
      },
      {
        "name" : "Stefan Köpsell"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03611v1",
    "title" : "Explanations Leak: Membership Inference with Differential Privacy and Active Learning Defense",
    "summary" : "Counterfactual explanations (CFs) are increasingly integrated into Machine Learning as a Service (MLaaS) systems to improve transparency; however, ML models deployed via APIs are already vulnerable to privacy attacks such as membership inference and model extraction, and the impact of explanations on this threat landscape remains insufficiently understood. In this work, we focus on the problem of how CFs expand the attack surface of MLaaS by strengthening membership inference attacks (MIAs), and on the need to design defense mechanisms that mitigate this emerging risk without undermining utility and explainability. First, we systematically analyze how exposing CFs through query-based APIs enables more effective shadow-based MIAs. Second, we propose a defense framework that integrates Differential Privacy (DP) with Active Learning (AL) to jointly reduce memorization and limit effective training data exposure. Finally, we conduct an extensive empirical evaluation to characterize the three-way trade-off between privacy leakage, predictive performance, and explanation quality. Our findings highlight the need to carefully balance transparency, utility, and privacy in the responsible deployment of explainable MLaaS systems.",
    "updated" : "2026-02-03T15:04:09Z",
    "published" : "2026-02-03T15:04:09Z",
    "authors" : [
      {
        "name" : "Fatima Ezzeddine"
      },
      {
        "name" : "Osama Zammar"
      },
      {
        "name" : "Silvia Giordano"
      },
      {
        "name" : "Omran Ayoub"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03423v1",
    "title" : "Origin Lens: A Privacy-First Mobile Framework for Cryptographic Image Provenance and AI Detection",
    "summary" : "The proliferation of generative AI poses challenges for information integrity assurance, requiring systems that connect model governance with end-user verification. We present Origin Lens, a privacy-first mobile framework that targets visual disinformation through a layered verification architecture. Unlike server-side detection systems, Origin Lens performs cryptographic image provenance verification and AI detection locally on the device via a Rust/Flutter hybrid architecture. Our system integrates multiple signals - including cryptographic provenance, generative model fingerprints, and optional retrieval-augmented verification - to provide users with graded confidence indicators at the point of consumption. We discuss the framework's alignment with regulatory requirements (EU AI Act, DSA) and its role in verification infrastructure that complements platform-level mechanisms.",
    "updated" : "2026-02-03T11:49:00Z",
    "published" : "2026-02-03T11:49:00Z",
    "authors" : [
      {
        "name" : "Alexander Loth"
      },
      {
        "name" : "Dominique Conceicao Rosario"
      },
      {
        "name" : "Peter Ebinger"
      },
      {
        "name" : "Martin Kappes"
      },
      {
        "name" : "Marc-Oliver Pahl"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03277v1",
    "title" : "BlockRR: A Unified Framework of RR-type Algorithms for Label Differential Privacy",
    "summary" : "In this paper, we introduce BlockRR, a novel and unified randomized-response mechanism for label differential privacy. This framework generalizes existed RR-type mechanisms as special cases under specific parameter settings, which eliminates the need for separate, case-by-case analysis. Theoretically, we prove that BlockRR satisfies $ε$-label DP. We also design a partition method for BlockRR based on a weight matrix derived from label prior information; the parallel composition principle ensures that the composition of two such mechanisms remains $ε$-label DP. Empirically, we evaluate BlockRR on two variants of CIFAR-10 with varying degrees of class imbalance. Results show that in the high-privacy and moderate-privacy regimes ($ε\\leq 3.0$), our propsed method gets a better balance between test accuaracy and the average of per-class accuracy. In the low-privacy regime ($ε\\geq 4.0$), all methods reduce BlockRR to standard RR without additional performance loss.",
    "updated" : "2026-02-03T09:00:45Z",
    "published" : "2026-02-03T09:00:45Z",
    "authors" : [
      {
        "name" : "Haixia Liu"
      },
      {
        "name" : "Yi Ding"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02914v1",
    "title" : "FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction",
    "summary" : "Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\\% matching accuracy and above 96\\% regeneration success, and still exceeds 92\\% matching and 94\\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.",
    "updated" : "2026-02-02T23:41:14Z",
    "published" : "2026-02-02T23:41:14Z",
    "authors" : [
      {
        "name" : "Wenqi Guo"
      },
      {
        "name" : "Shan Du"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02744v1",
    "title" : "An introduction to local differential privacy protocols using block designs",
    "summary" : "The design of protocols for local differential privacy (or LDP) has been a topic of considerable research interest in recent years. LDP protocols utilise the randomised encoding of outcomes of an experiment using a transition probability matrix (TPM). Several authors have observed that balanced incomplete block designs (BIBDs) provide nice examples of TPMs for LDP protocols. Indeed, it has been shown that such BIBD-based LDP protocols provide optimal estimators.\n  In this primarily expository paper, we give a detailed introduction to LDP protocols and their connections with block designs. We prove that a subclass of LDP protocols known as pure LDP protocols are equivalent to $(r,λ)$-designs (which contain balanced incomplete block designs as a special case). An unbiased estimator for an LDP scheme is a left inverse of the transition probability matrix. We show that the optimal estimators for BIBD-based TPMs are precisely those obtained from the Moore-Penrose inverse of the corresponding TPM. We also review some existing work on optimal LDP protocols in the context of pure protocols.",
    "updated" : "2026-02-02T19:58:58Z",
    "published" : "2026-02-02T19:58:58Z",
    "authors" : [
      {
        "name" : "Maura B. Paterson"
      },
      {
        "name" : "Douglas R. Stinson"
      }
    ],
    "categories" : [
      "math.CO",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02718v1",
    "title" : "Composition for Pufferfish Privacy",
    "summary" : "When creating public data products out of confidential datasets, inferential/posterior-based privacy definitions, such as Pufferfish, provide compelling privacy semantics for data with correlations. However, such privacy definitions are rarely used in practice because they do not always compose. For example, it is possible to design algorithms for these privacy definitions that have no leakage when run once but reveal the entire dataset when run more than once. We prove necessary and sufficient conditions that must be added to ensure linear composition for Pufferfish mechanisms, hence avoiding such privacy collapse. These extra conditions turn out to be differential privacy-style inequalities, indicating that achieving both the interpretable semantics of Pufferfish for correlated data and composition benefits requires adopting differentially private mechanisms to Pufferfish. We show that such translation is possible through a concept called the $(a,b)$-influence curve, and many existing differentially private algorithms can be translated with our framework into a composable Pufferfish algorithm. We illustrate the benefit of our new framework by designing composable Pufferfish algorithms for Markov chains that significantly outperform prior work.",
    "updated" : "2026-02-02T19:36:44Z",
    "published" : "2026-02-02T19:36:44Z",
    "authors" : [
      {
        "name" : "Jiamu Bai"
      },
      {
        "name" : "Guanlin He"
      },
      {
        "name" : "Xin Gu"
      },
      {
        "name" : "Daniel Kifer"
      },
      {
        "name" : "Kiwan Maeng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04616v1",
    "title" : "A Human-Centered Privacy Approach (HCP) to AI",
    "summary" : "As the paradigm of Human-Centered AI (HCAI) gains prominence, its benefits to society are accompanied by significant ethical concerns, one of which is the protection of individual privacy. This chapter provides a comprehensive overview of privacy within HCAI, proposing a human-centered privacy (HCP) framework, providing integrated solution from technology, ethics, and human factors perspectives. The chapter begins by mapping privacy risks across each stage of AI development lifecycle, from data collection to deployment and reuse, highlighting the impact of privacy risks on the entire system. The chapter then introduces privacy-preserving techniques such as federated learning and dif erential privacy. Subsequent chapters integrate the crucial user perspective by examining mental models, alongside the evolving regulatory and ethical landscapes as well as privacy governance. Next, advice on design guidelines is provided based on the human-centered privacy framework. After that, we introduce practical case studies across diverse fields. Finally, the chapter discusses persistent open challenges and future research directions, concluding that a multidisciplinary approach, merging technical, design, policy, and ethical expertise, is essential to successfully embed privacy into the core of HCAI, thereby ensuring these technologies advance in a manner that respects and ensures human autonomy, trust and dignity.",
    "updated" : "2026-02-04T14:43:25Z",
    "published" : "2026-02-04T14:43:25Z",
    "authors" : [
      {
        "name" : "Luyi Sun"
      },
      {
        "name" : "Wei Xu"
      },
      {
        "name" : "Zaifeng Gao"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04562v1",
    "title" : "Optimal conversion from Rényi Differential Privacy to $f$-Differential Privacy",
    "summary" : "We prove the conjecture stated in Appendix F.3 of [Zhu et al. (2022)]: among all conversion rules that map a Rényi Differential Privacy (RDP) profile $τ\\mapsto ρ(τ)$ to a valid hypothesis-testing trade-off $f$, the rule based on the intersection of single-order RDP privacy regions is optimal. This optimality holds simultaneously for all valid RDP profiles and for all Type I error levels $α$. Concretely, we show that in the space of trade-off functions, the tightest possible bound is $f_{ρ(\\cdot)}(α) = \\sup_{τ\\geq 0.5} f_{τ,ρ(τ)}(α)$: the pointwise maximum of the single-order bounds for each RDP privacy region. Our proof unifies and sharpens the insights of [Balle et al. (2019)], [Asoodeh et al. (2021)], and [Zhu et al. (2022)]. Our analysis relies on a precise geometric characterization of the RDP privacy region, leveraging its convexity and the fact that its boundary is determined exclusively by Bernoulli mechanisms. Our results establish that the \"intersection-of-RDP-privacy-regions\" rule is not only valid, but optimal: no other black-box conversion can uniformly dominate it in the Blackwell sense, marking the fundamental limit of what can be inferred about a mechanism's privacy solely from its RDP guarantees.",
    "updated" : "2026-02-04T13:49:51Z",
    "published" : "2026-02-04T13:49:51Z",
    "authors" : [
      {
        "name" : "Anneliese Riess"
      },
      {
        "name" : "Juan Felipe Gomez"
      },
      {
        "name" : "Flavio du Pin Calmon"
      },
      {
        "name" : "Julia Anne Schnabel"
      },
      {
        "name" : "Georgios Kaissis"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04482v1",
    "title" : "Proactive Agents, Long-term User Context, VLM Annotation, Privacy Protection, Human-Computer Interaction",
    "summary" : "Proactive agents that anticipate user intentions without explicit prompts represent a significant evolution in human-AI interaction, promising to reduce cognitive load and streamline workflows. However, existing datasets suffer from two critical deficiencies: (1) reliance on LLM-synthesized data that fails to capture authentic human decision-making patterns, and (2) focus on isolated tasks rather than continuous workflows, missing the pre-assistance behavioral context essential for learning proactive intervention signals. To address these gaps, we introduce ProAgentBench, a rigorous benchmark for proactive agents in working scenarios. Our contributions include: (1) a hierarchical task framework that decomposes proactive assistance into timing prediction and assist content generation; (2) a privacy-compliant dataset with 28,000+ events from 500+ hours of real user sessions, preserving bursty interaction patterns (burstiness B=0.787) absent in synthetic data; and (3) extensive experiments that evaluates LLM- and VLM-based baselines. Numerically, we showed that long-term memory and historical context significantly enhance prediction accuracy, while real-world training data substantially outperforms synthetic alternatives. We release our dataset and code at https://anonymous.4open.science/r/ProAgentBench-6BC0.",
    "updated" : "2026-02-04T12:09:15Z",
    "published" : "2026-02-04T12:09:15Z",
    "authors" : [
      {
        "name" : "Yuanbo Tang"
      },
      {
        "name" : "Huaze Tang"
      },
      {
        "name" : "Tingyu Cao"
      },
      {
        "name" : "Lam Nguyen"
      },
      {
        "name" : "Anping Zhang"
      },
      {
        "name" : "Xinwen Cao"
      },
      {
        "name" : "Chunkang Liu"
      },
      {
        "name" : "Wenbo Ding"
      },
      {
        "name" : "Yang Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04262v1",
    "title" : "Parameter Privacy-Preserving Data Sharing: A Particle-Belief MDP Formulation",
    "summary" : "This paper investigates parameter-privacy-preserving data sharing in continuous-state dynamical systems, where a data owner designs a data-sharing policy to support downstream estimation and control while preventing adversarial inference of a sensitive parameter. This data-sharing problem is formulated as an optimization problem that trades off privacy leakage and the impact of data sharing on the data owner's utility, subject to a data-usability constraint. We show that this problem admits an equivalent belief Markov decision process (MDP) formulation, which provides a simplified representation of the optimal policy. To efficiently characterize information-theoretic privacy leakage in continuous state and action spaces, we propose a particle-belief MDP formulation that tracks the parameter posterior via sequential Monte Carlo, yielding a tractable belief-state approximation that converges asymptotically as the number of particles increases. We further derive a tractable closed-form upper bound on particle-based MI via Gaussian mixture approximations, which enables efficient optimization of the particle-belief MDP. Experiments on a mixed-autonomy platoon show that the learned continuous policy substantially impedes inference attacks on human-driving behavior parameters while maintaining data usability and system performance.",
    "updated" : "2026-02-04T06:55:01Z",
    "published" : "2026-02-04T06:55:01Z",
    "authors" : [
      {
        "name" : "Haokun Yu"
      },
      {
        "name" : "Jingyuan Zhou"
      },
      {
        "name" : "Kaidi Yang"
      }
    ],
    "categories" : [
      "eess.SY",
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04124v1",
    "title" : "Privacy Amplification for Synthetic data using Range Restriction",
    "summary" : "We introduce a new class of range restricted formal data privacy standards that condition on owner beliefs about sensitive data ranges. By incorporating this additional information, we can provide a stronger privacy guarantee (e.g. an amplification). The range restricted formal privacy standards protect only a subset (or ball) of data values and exclude ranges (or balls) believed to be already publicly known. The privacy standards are designed for the risk-weighted pseudo posterior (model) mechanism (PPM) used to generate synthetic data under an asymptotic Differential (aDP) privacy guarantee. The PPM downweights the likelihood contribution for each record proportionally to its disclosure risk. The PPM is adapted under inclusion of beliefs by adjusting the risk-weighted pseudo likelihood. We introduce two alternative adjustments. The first expresses data owner knowledge of the sensitive range as a probability, $λ$, that a datum value drawn from the underlying generating distribution lies outside the ball or subspace of values that are sensitive. The portion of each datum likelihood contribution deemed sensitive is then $(1-λ) \\leq 1$ and is the only portion of the likelihood subject to risk down-weighting. The second adjustment encodes knowledge as the difference in probability masses $P(R) \\leq 1$ between the edges of the sensitive range, $R$. We use the resulting conditional (pseudo) likelihood for a sensitive record, which boosts its worst case tail values away from 0. We compare privacy and utility properties for the PPM under the aDP and range restricted privacy standards.",
    "updated" : "2026-02-04T01:36:01Z",
    "published" : "2026-02-04T01:36:01Z",
    "authors" : [
      {
        "name" : "Monika Hu"
      },
      {
        "name" : "Matthew R. Williams"
      },
      {
        "name" : "Terrance D. Savitsky"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03948v1",
    "title" : "Privacy utility trade offs for parameter estimation in degree heterogeneous higher order networks",
    "summary" : "In sensitive applications involving relational datasets, protecting information about individual links from adversarial queries is of paramount importance. In many such settings, the available data are summarized solely through the degrees of the nodes in the network. We adopt the $β$ model, which is the prototypical statistical model adopted for this form of aggregated relational information, and study the problem of minimax-optimal parameter estimation under both local and central differential privacy constraints. We establish finite sample minimax lower bounds that characterize the precise dependence of the estimation risk on the network size and the privacy parameters, and we propose simple estimators that achieve these bounds up to constants and logarithmic factors under both local and central differential privacy frameworks. Our results provide the first comprehensive finite sample characterization of privacy utility trade offs for parameter estimation in $β$ models, addressing the classical graph case and extending the analysis to higher order hypergraph models. We further demonstrate the effectiveness of our methods through experiments on synthetic data and a real world communication network.",
    "updated" : "2026-02-03T19:11:37Z",
    "published" : "2026-02-03T19:11:37Z",
    "authors" : [
      {
        "name" : "Bibhabasu Mandal"
      },
      {
        "name" : "Sagnik Nandy"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "cs.SI",
      "math.ST"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01928v2",
    "title" : "Privacy Amplification by Missing Data",
    "summary" : "Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.",
    "updated" : "2026-02-04T09:07:53Z",
    "published" : "2026-02-02T10:28:41Z",
    "authors" : [
      {
        "name" : "Simon Roburin"
      },
      {
        "name" : "Rafaël Pinot"
      },
      {
        "name" : "Erwan Scornet"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  }
]