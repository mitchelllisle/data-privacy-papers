[
  {
    "id" : "http://arxiv.org/abs/2602.02364v1",
    "title" : "Guaranteeing Privacy in Hybrid Quantum Learning through Theoretical Mechanisms",
    "summary" : "Quantum Machine Learning (QML) is becoming increasingly prevalent due to its potential to enhance classical machine learning (ML) tasks, such as classification. Although quantum noise is often viewed as a major challenge in quantum computing, it also offers a unique opportunity to enhance privacy. In particular, intrinsic quantum noise provides a natural stochastic resource that, when rigorously analyzed within the differential privacy (DP) framework and composed with classical mechanisms, can satisfy formal $(\\varepsilon, δ)$-DP guarantees. This enables a reduction in the required classical perturbation without compromising the privacy budget, potentially improving model utility. However, the integration of classical and quantum noise for privacy preservation remains unexplored. In this work, we propose a hybrid noise-added mechanism, HYPER-Q, that combines classical and quantum noise to protect the privacy of QML models. We provide a comprehensive analysis of its privacy guarantees and establish theoretical bounds on its utility. Empirically, we demonstrate that HYPER-Q outperforms existing classical noise-based mechanisms in terms of adversarial robustness across multiple real-world datasets.",
    "updated" : "2026-02-02T17:23:37Z",
    "published" : "2026-02-02T17:23:37Z",
    "authors" : [
      {
        "name" : "Hoang M. Ngo"
      },
      {
        "name" : "Tre' R. Jeter"
      },
      {
        "name" : "Incheol Shin"
      },
      {
        "name" : "Wanli Xing"
      },
      {
        "name" : "Tamer Kahveci"
      },
      {
        "name" : "My T. Thai"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02296v1",
    "title" : "Decoupling Generalizability and Membership Privacy Risks in Neural Networks",
    "summary" : "A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.",
    "updated" : "2026-02-02T16:32:42Z",
    "published" : "2026-02-02T16:32:42Z",
    "authors" : [
      {
        "name" : "Xingli Fang"
      },
      {
        "name" : "Jung-Eun Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01928v1",
    "title" : "Privacy Amplification by Missing Data",
    "summary" : "Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.",
    "updated" : "2026-02-02T10:28:41Z",
    "published" : "2026-02-02T10:28:41Z",
    "authors" : [
      {
        "name" : "Simon Roburin"
      },
      {
        "name" : "Rafaël Pinot"
      },
      {
        "name" : "Erwan Scornet"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01804v1",
    "title" : "Fostering Data Collaboration in Digital Transportation Marketplaces: The Role of Privacy-Preserving Mechanisms",
    "summary" : "Data collaboration between municipal authorities (MA) and mobility providers (MPs) has brought tremendous benefits to transportation systems in the era of big data. Engaging in collaboration can improve the service operations (e.g., reduced delay) of these data owners, however, it can also raise privacy concerns and discourage data-sharing willingness. Specifically, data owners may be concerned that the shared data may leak sensitive information about their customers' mobility patterns or business secrets, resulting in the failure of collaboration. This paper investigates how privacy-preserving mechanisms can foster data collaboration in such settings. We propose a game-theoretic framework to investigate data-sharing among transportation stakeholders, especially considering perturbation-based privacy-preserving mechanisms. Numerical studies demonstrate that lower data quality expectations can incentivize voluntary data sharing, improving transport-related welfare for both MAs and MPs. Our findings provide actionable insights for policymakers and system designers on how privacy-preserving technologies can help bridge data silos and promote collaborative, privacy-aware transportation systems.",
    "updated" : "2026-02-02T08:35:29Z",
    "published" : "2026-02-02T08:35:29Z",
    "authors" : [
      {
        "name" : "Qiqing Wang"
      },
      {
        "name" : "Haokun Yu"
      },
      {
        "name" : "Kaidi Yang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.GT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01387v1",
    "title" : "Disclose with Care: Designing Privacy Controls in Interview Chatbots",
    "summary" : "Collecting data on sensitive topics remains challenging in HCI, as participants often withhold information due to privacy concerns and social desirability bias. While chatbots' perceived anonymity may reduce these barriers, research paradoxically suggests people tend to over-share personal or sensitive information with chatbots. In this work, we explore privacy controls in chatbot interviews to address this problem. The privacy control allows participants to revise their transcripts at the end of the interview, featuring two design variants: free editing and AI-aided editing. In a between-subjects study \\red{($N=188$)}, we compared no-editing, free-editing, and AI-aided editing conditions in a chatbot-based interview on a sensitive topic. Our results confirm the prevalent issue of oversharing in chatbot-based interviews and show that AI-aided editing serves as an effective privacy-control mechanism, reducing PII disclosure while maintaining data quality and user engagement, thereby offering a promising approach to balancing ethical practice and data quality in such interviews.",
    "updated" : "2026-02-01T18:48:19Z",
    "published" : "2026-02-01T18:48:19Z",
    "authors" : [
      {
        "name" : "Ziwen Li"
      },
      {
        "name" : "Ziang Xiao"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01177v1",
    "title" : "Equivalence of Privacy and Stability with Generalization Guarantees in Quantum Learning",
    "summary" : "We present a unified information-theoretic framework to analyze the generalization performance of differentially private (DP) quantum learning algorithms. By leveraging the connection between privacy and algorithmic stability, we establish that $(\\varepsilon, δ)$-Quantum Differential Privacy (QDP) imposes a strong constraint on the mutual information between the training data and the algorithm's output. We derive a rigorous, mechanism-agnostic upper bound on this mutual information for learning algorithms satisfying a 1-neighbor privacy constraint. Furthermore, we connect this stability guarantee to generalization, proving that the expected generalization error of any $(\\varepsilon, δ)$-QDP learning algorithm is bounded by the square root of the privacy-induced stability term. Finally, we extend our framework to the setting of an untrusted Data Processor, introducing the concept of Information-Theoretic Admissibility (ITA) to characterize the fundamental limits of privacy in scenarios where the learning map itself must remain oblivious to the specific dataset instance.",
    "updated" : "2026-02-01T12:03:07Z",
    "published" : "2026-02-01T12:03:07Z",
    "authors" : [
      {
        "name" : "Ayanava Dasgupta"
      },
      {
        "name" : "Naqueeb Ahmad Warsi"
      },
      {
        "name" : "Masahito Hayashi"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01126v1",
    "title" : "WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity",
    "summary" : "Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.",
    "updated" : "2026-02-01T09:52:57Z",
    "published" : "2026-02-01T09:52:57Z",
    "authors" : [
      {
        "name" : "Mengsha Kou"
      },
      {
        "name" : "Xiaoyu Xia"
      },
      {
        "name" : "Ziqi Wang"
      },
      {
        "name" : "Ibrahim Khalil"
      },
      {
        "name" : "Runkun Luo"
      },
      {
        "name" : "Jingwen Zhou"
      },
      {
        "name" : "Minhui Xue"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03671v1",
    "title" : "mopri - An Analysis Framework for Unveiling Privacy Violations in Mobile Apps",
    "summary" : "Everyday services of society increasingly rely on mobile applications, resulting in a conflicting situation between the possibility of participation on the one side and user privacy and digital freedom on the other. In order to protect users' rights to informational self-determination, regulatory approaches for the collection and processing of personal data have been developed, such as the EU's GDPR. However, inspecting the compliance of mobile apps with privacy regulations remains difficult. Thus, in order to enable end users and enforcement bodies to verify and enforce data protection compliance, we propose mopri, a conceptual framework designed for analyzing the behavior of mobile apps through a comprehensive, adaptable, and user-centered approach. Recognizing the gaps in existing frameworks, mopri serves as a foundation for integrating various analysis tools into a streamlined, modular pipeline that employs static and dynamic analysis methods. Building on this concept, a prototype has been developed which effectively extracts permissions and tracking libraries while employing robust methods for dynamic traffic recording and decryption. Additionally, it incorporates result enrichment and reporting features that enhance the clarity and usability of the analysis outcomes. The prototype showcases the feasibility of a holistic and modular approach to privacy analysis, emphasizing the importance of continuous adaptation to the evolving challenges presented by the mobile app ecosystem.",
    "updated" : "2026-02-03T15:52:31Z",
    "published" : "2026-02-03T15:52:31Z",
    "authors" : [
      {
        "name" : "Cornell Ziepel"
      },
      {
        "name" : "Stephan Escher"
      },
      {
        "name" : "Sebastian Rehms"
      },
      {
        "name" : "Stefan Köpsell"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03611v1",
    "title" : "Explanations Leak: Membership Inference with Differential Privacy and Active Learning Defense",
    "summary" : "Counterfactual explanations (CFs) are increasingly integrated into Machine Learning as a Service (MLaaS) systems to improve transparency; however, ML models deployed via APIs are already vulnerable to privacy attacks such as membership inference and model extraction, and the impact of explanations on this threat landscape remains insufficiently understood. In this work, we focus on the problem of how CFs expand the attack surface of MLaaS by strengthening membership inference attacks (MIAs), and on the need to design defense mechanisms that mitigate this emerging risk without undermining utility and explainability. First, we systematically analyze how exposing CFs through query-based APIs enables more effective shadow-based MIAs. Second, we propose a defense framework that integrates Differential Privacy (DP) with Active Learning (AL) to jointly reduce memorization and limit effective training data exposure. Finally, we conduct an extensive empirical evaluation to characterize the three-way trade-off between privacy leakage, predictive performance, and explanation quality. Our findings highlight the need to carefully balance transparency, utility, and privacy in the responsible deployment of explainable MLaaS systems.",
    "updated" : "2026-02-03T15:04:09Z",
    "published" : "2026-02-03T15:04:09Z",
    "authors" : [
      {
        "name" : "Fatima Ezzeddine"
      },
      {
        "name" : "Osama Zammar"
      },
      {
        "name" : "Silvia Giordano"
      },
      {
        "name" : "Omran Ayoub"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03423v1",
    "title" : "Origin Lens: A Privacy-First Mobile Framework for Cryptographic Image Provenance and AI Detection",
    "summary" : "The proliferation of generative AI poses challenges for information integrity assurance, requiring systems that connect model governance with end-user verification. We present Origin Lens, a privacy-first mobile framework that targets visual disinformation through a layered verification architecture. Unlike server-side detection systems, Origin Lens performs cryptographic image provenance verification and AI detection locally on the device via a Rust/Flutter hybrid architecture. Our system integrates multiple signals - including cryptographic provenance, generative model fingerprints, and optional retrieval-augmented verification - to provide users with graded confidence indicators at the point of consumption. We discuss the framework's alignment with regulatory requirements (EU AI Act, DSA) and its role in verification infrastructure that complements platform-level mechanisms.",
    "updated" : "2026-02-03T11:49:00Z",
    "published" : "2026-02-03T11:49:00Z",
    "authors" : [
      {
        "name" : "Alexander Loth"
      },
      {
        "name" : "Dominique Conceicao Rosario"
      },
      {
        "name" : "Peter Ebinger"
      },
      {
        "name" : "Martin Kappes"
      },
      {
        "name" : "Marc-Oliver Pahl"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03277v1",
    "title" : "BlockRR: A Unified Framework of RR-type Algorithms for Label Differential Privacy",
    "summary" : "In this paper, we introduce BlockRR, a novel and unified randomized-response mechanism for label differential privacy. This framework generalizes existed RR-type mechanisms as special cases under specific parameter settings, which eliminates the need for separate, case-by-case analysis. Theoretically, we prove that BlockRR satisfies $ε$-label DP. We also design a partition method for BlockRR based on a weight matrix derived from label prior information; the parallel composition principle ensures that the composition of two such mechanisms remains $ε$-label DP. Empirically, we evaluate BlockRR on two variants of CIFAR-10 with varying degrees of class imbalance. Results show that in the high-privacy and moderate-privacy regimes ($ε\\leq 3.0$), our propsed method gets a better balance between test accuaracy and the average of per-class accuracy. In the low-privacy regime ($ε\\geq 4.0$), all methods reduce BlockRR to standard RR without additional performance loss.",
    "updated" : "2026-02-03T09:00:45Z",
    "published" : "2026-02-03T09:00:45Z",
    "authors" : [
      {
        "name" : "Haixia Liu"
      },
      {
        "name" : "Yi Ding"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02914v1",
    "title" : "FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction",
    "summary" : "Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\\% matching accuracy and above 96\\% regeneration success, and still exceeds 92\\% matching and 94\\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.",
    "updated" : "2026-02-02T23:41:14Z",
    "published" : "2026-02-02T23:41:14Z",
    "authors" : [
      {
        "name" : "Wenqi Guo"
      },
      {
        "name" : "Shan Du"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02744v1",
    "title" : "An introduction to local differential privacy protocols using block designs",
    "summary" : "The design of protocols for local differential privacy (or LDP) has been a topic of considerable research interest in recent years. LDP protocols utilise the randomised encoding of outcomes of an experiment using a transition probability matrix (TPM). Several authors have observed that balanced incomplete block designs (BIBDs) provide nice examples of TPMs for LDP protocols. Indeed, it has been shown that such BIBD-based LDP protocols provide optimal estimators.\n  In this primarily expository paper, we give a detailed introduction to LDP protocols and their connections with block designs. We prove that a subclass of LDP protocols known as pure LDP protocols are equivalent to $(r,λ)$-designs (which contain balanced incomplete block designs as a special case). An unbiased estimator for an LDP scheme is a left inverse of the transition probability matrix. We show that the optimal estimators for BIBD-based TPMs are precisely those obtained from the Moore-Penrose inverse of the corresponding TPM. We also review some existing work on optimal LDP protocols in the context of pure protocols.",
    "updated" : "2026-02-02T19:58:58Z",
    "published" : "2026-02-02T19:58:58Z",
    "authors" : [
      {
        "name" : "Maura B. Paterson"
      },
      {
        "name" : "Douglas R. Stinson"
      }
    ],
    "categories" : [
      "math.CO",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02718v1",
    "title" : "Composition for Pufferfish Privacy",
    "summary" : "When creating public data products out of confidential datasets, inferential/posterior-based privacy definitions, such as Pufferfish, provide compelling privacy semantics for data with correlations. However, such privacy definitions are rarely used in practice because they do not always compose. For example, it is possible to design algorithms for these privacy definitions that have no leakage when run once but reveal the entire dataset when run more than once. We prove necessary and sufficient conditions that must be added to ensure linear composition for Pufferfish mechanisms, hence avoiding such privacy collapse. These extra conditions turn out to be differential privacy-style inequalities, indicating that achieving both the interpretable semantics of Pufferfish for correlated data and composition benefits requires adopting differentially private mechanisms to Pufferfish. We show that such translation is possible through a concept called the $(a,b)$-influence curve, and many existing differentially private algorithms can be translated with our framework into a composable Pufferfish algorithm. We illustrate the benefit of our new framework by designing composable Pufferfish algorithms for Markov chains that significantly outperform prior work.",
    "updated" : "2026-02-02T19:36:44Z",
    "published" : "2026-02-02T19:36:44Z",
    "authors" : [
      {
        "name" : "Jiamu Bai"
      },
      {
        "name" : "Guanlin He"
      },
      {
        "name" : "Xin Gu"
      },
      {
        "name" : "Daniel Kifer"
      },
      {
        "name" : "Kiwan Maeng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  }
]