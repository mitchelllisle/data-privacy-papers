[
  {
    "id" : "http://arxiv.org/abs/2602.02364v1",
    "title" : "Guaranteeing Privacy in Hybrid Quantum Learning through Theoretical Mechanisms",
    "summary" : "Quantum Machine Learning (QML) is becoming increasingly prevalent due to its potential to enhance classical machine learning (ML) tasks, such as classification. Although quantum noise is often viewed as a major challenge in quantum computing, it also offers a unique opportunity to enhance privacy. In particular, intrinsic quantum noise provides a natural stochastic resource that, when rigorously analyzed within the differential privacy (DP) framework and composed with classical mechanisms, can satisfy formal $(\\varepsilon, δ)$-DP guarantees. This enables a reduction in the required classical perturbation without compromising the privacy budget, potentially improving model utility. However, the integration of classical and quantum noise for privacy preservation remains unexplored. In this work, we propose a hybrid noise-added mechanism, HYPER-Q, that combines classical and quantum noise to protect the privacy of QML models. We provide a comprehensive analysis of its privacy guarantees and establish theoretical bounds on its utility. Empirically, we demonstrate that HYPER-Q outperforms existing classical noise-based mechanisms in terms of adversarial robustness across multiple real-world datasets.",
    "updated" : "2026-02-02T17:23:37Z",
    "published" : "2026-02-02T17:23:37Z",
    "authors" : [
      {
        "name" : "Hoang M. Ngo"
      },
      {
        "name" : "Tre' R. Jeter"
      },
      {
        "name" : "Incheol Shin"
      },
      {
        "name" : "Wanli Xing"
      },
      {
        "name" : "Tamer Kahveci"
      },
      {
        "name" : "My T. Thai"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02296v1",
    "title" : "Decoupling Generalizability and Membership Privacy Risks in Neural Networks",
    "summary" : "A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.",
    "updated" : "2026-02-02T16:32:42Z",
    "published" : "2026-02-02T16:32:42Z",
    "authors" : [
      {
        "name" : "Xingli Fang"
      },
      {
        "name" : "Jung-Eun Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01928v1",
    "title" : "Privacy Amplification by Missing Data",
    "summary" : "Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.",
    "updated" : "2026-02-02T10:28:41Z",
    "published" : "2026-02-02T10:28:41Z",
    "authors" : [
      {
        "name" : "Simon Roburin"
      },
      {
        "name" : "Rafaël Pinot"
      },
      {
        "name" : "Erwan Scornet"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01804v1",
    "title" : "Fostering Data Collaboration in Digital Transportation Marketplaces: The Role of Privacy-Preserving Mechanisms",
    "summary" : "Data collaboration between municipal authorities (MA) and mobility providers (MPs) has brought tremendous benefits to transportation systems in the era of big data. Engaging in collaboration can improve the service operations (e.g., reduced delay) of these data owners, however, it can also raise privacy concerns and discourage data-sharing willingness. Specifically, data owners may be concerned that the shared data may leak sensitive information about their customers' mobility patterns or business secrets, resulting in the failure of collaboration. This paper investigates how privacy-preserving mechanisms can foster data collaboration in such settings. We propose a game-theoretic framework to investigate data-sharing among transportation stakeholders, especially considering perturbation-based privacy-preserving mechanisms. Numerical studies demonstrate that lower data quality expectations can incentivize voluntary data sharing, improving transport-related welfare for both MAs and MPs. Our findings provide actionable insights for policymakers and system designers on how privacy-preserving technologies can help bridge data silos and promote collaborative, privacy-aware transportation systems.",
    "updated" : "2026-02-02T08:35:29Z",
    "published" : "2026-02-02T08:35:29Z",
    "authors" : [
      {
        "name" : "Qiqing Wang"
      },
      {
        "name" : "Haokun Yu"
      },
      {
        "name" : "Kaidi Yang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.GT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01387v1",
    "title" : "Disclose with Care: Designing Privacy Controls in Interview Chatbots",
    "summary" : "Collecting data on sensitive topics remains challenging in HCI, as participants often withhold information due to privacy concerns and social desirability bias. While chatbots' perceived anonymity may reduce these barriers, research paradoxically suggests people tend to over-share personal or sensitive information with chatbots. In this work, we explore privacy controls in chatbot interviews to address this problem. The privacy control allows participants to revise their transcripts at the end of the interview, featuring two design variants: free editing and AI-aided editing. In a between-subjects study \\red{($N=188$)}, we compared no-editing, free-editing, and AI-aided editing conditions in a chatbot-based interview on a sensitive topic. Our results confirm the prevalent issue of oversharing in chatbot-based interviews and show that AI-aided editing serves as an effective privacy-control mechanism, reducing PII disclosure while maintaining data quality and user engagement, thereby offering a promising approach to balancing ethical practice and data quality in such interviews.",
    "updated" : "2026-02-01T18:48:19Z",
    "published" : "2026-02-01T18:48:19Z",
    "authors" : [
      {
        "name" : "Ziwen Li"
      },
      {
        "name" : "Ziang Xiao"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01177v1",
    "title" : "Equivalence of Privacy and Stability with Generalization Guarantees in Quantum Learning",
    "summary" : "We present a unified information-theoretic framework to analyze the generalization performance of differentially private (DP) quantum learning algorithms. By leveraging the connection between privacy and algorithmic stability, we establish that $(\\varepsilon, δ)$-Quantum Differential Privacy (QDP) imposes a strong constraint on the mutual information between the training data and the algorithm's output. We derive a rigorous, mechanism-agnostic upper bound on this mutual information for learning algorithms satisfying a 1-neighbor privacy constraint. Furthermore, we connect this stability guarantee to generalization, proving that the expected generalization error of any $(\\varepsilon, δ)$-QDP learning algorithm is bounded by the square root of the privacy-induced stability term. Finally, we extend our framework to the setting of an untrusted Data Processor, introducing the concept of Information-Theoretic Admissibility (ITA) to characterize the fundamental limits of privacy in scenarios where the learning map itself must remain oblivious to the specific dataset instance.",
    "updated" : "2026-02-01T12:03:07Z",
    "published" : "2026-02-01T12:03:07Z",
    "authors" : [
      {
        "name" : "Ayanava Dasgupta"
      },
      {
        "name" : "Naqueeb Ahmad Warsi"
      },
      {
        "name" : "Masahito Hayashi"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01126v1",
    "title" : "WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity",
    "summary" : "Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.",
    "updated" : "2026-02-01T09:52:57Z",
    "published" : "2026-02-01T09:52:57Z",
    "authors" : [
      {
        "name" : "Mengsha Kou"
      },
      {
        "name" : "Xiaoyu Xia"
      },
      {
        "name" : "Ziqi Wang"
      },
      {
        "name" : "Ibrahim Khalil"
      },
      {
        "name" : "Runkun Luo"
      },
      {
        "name" : "Jingwen Zhou"
      },
      {
        "name" : "Minhui Xue"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03671v1",
    "title" : "mopri - An Analysis Framework for Unveiling Privacy Violations in Mobile Apps",
    "summary" : "Everyday services of society increasingly rely on mobile applications, resulting in a conflicting situation between the possibility of participation on the one side and user privacy and digital freedom on the other. In order to protect users' rights to informational self-determination, regulatory approaches for the collection and processing of personal data have been developed, such as the EU's GDPR. However, inspecting the compliance of mobile apps with privacy regulations remains difficult. Thus, in order to enable end users and enforcement bodies to verify and enforce data protection compliance, we propose mopri, a conceptual framework designed for analyzing the behavior of mobile apps through a comprehensive, adaptable, and user-centered approach. Recognizing the gaps in existing frameworks, mopri serves as a foundation for integrating various analysis tools into a streamlined, modular pipeline that employs static and dynamic analysis methods. Building on this concept, a prototype has been developed which effectively extracts permissions and tracking libraries while employing robust methods for dynamic traffic recording and decryption. Additionally, it incorporates result enrichment and reporting features that enhance the clarity and usability of the analysis outcomes. The prototype showcases the feasibility of a holistic and modular approach to privacy analysis, emphasizing the importance of continuous adaptation to the evolving challenges presented by the mobile app ecosystem.",
    "updated" : "2026-02-03T15:52:31Z",
    "published" : "2026-02-03T15:52:31Z",
    "authors" : [
      {
        "name" : "Cornell Ziepel"
      },
      {
        "name" : "Stephan Escher"
      },
      {
        "name" : "Sebastian Rehms"
      },
      {
        "name" : "Stefan Köpsell"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03611v1",
    "title" : "Explanations Leak: Membership Inference with Differential Privacy and Active Learning Defense",
    "summary" : "Counterfactual explanations (CFs) are increasingly integrated into Machine Learning as a Service (MLaaS) systems to improve transparency; however, ML models deployed via APIs are already vulnerable to privacy attacks such as membership inference and model extraction, and the impact of explanations on this threat landscape remains insufficiently understood. In this work, we focus on the problem of how CFs expand the attack surface of MLaaS by strengthening membership inference attacks (MIAs), and on the need to design defense mechanisms that mitigate this emerging risk without undermining utility and explainability. First, we systematically analyze how exposing CFs through query-based APIs enables more effective shadow-based MIAs. Second, we propose a defense framework that integrates Differential Privacy (DP) with Active Learning (AL) to jointly reduce memorization and limit effective training data exposure. Finally, we conduct an extensive empirical evaluation to characterize the three-way trade-off between privacy leakage, predictive performance, and explanation quality. Our findings highlight the need to carefully balance transparency, utility, and privacy in the responsible deployment of explainable MLaaS systems.",
    "updated" : "2026-02-03T15:04:09Z",
    "published" : "2026-02-03T15:04:09Z",
    "authors" : [
      {
        "name" : "Fatima Ezzeddine"
      },
      {
        "name" : "Osama Zammar"
      },
      {
        "name" : "Silvia Giordano"
      },
      {
        "name" : "Omran Ayoub"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03423v1",
    "title" : "Origin Lens: A Privacy-First Mobile Framework for Cryptographic Image Provenance and AI Detection",
    "summary" : "The proliferation of generative AI poses challenges for information integrity assurance, requiring systems that connect model governance with end-user verification. We present Origin Lens, a privacy-first mobile framework that targets visual disinformation through a layered verification architecture. Unlike server-side detection systems, Origin Lens performs cryptographic image provenance verification and AI detection locally on the device via a Rust/Flutter hybrid architecture. Our system integrates multiple signals - including cryptographic provenance, generative model fingerprints, and optional retrieval-augmented verification - to provide users with graded confidence indicators at the point of consumption. We discuss the framework's alignment with regulatory requirements (EU AI Act, DSA) and its role in verification infrastructure that complements platform-level mechanisms.",
    "updated" : "2026-02-03T11:49:00Z",
    "published" : "2026-02-03T11:49:00Z",
    "authors" : [
      {
        "name" : "Alexander Loth"
      },
      {
        "name" : "Dominique Conceicao Rosario"
      },
      {
        "name" : "Peter Ebinger"
      },
      {
        "name" : "Martin Kappes"
      },
      {
        "name" : "Marc-Oliver Pahl"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03277v1",
    "title" : "BlockRR: A Unified Framework of RR-type Algorithms for Label Differential Privacy",
    "summary" : "In this paper, we introduce BlockRR, a novel and unified randomized-response mechanism for label differential privacy. This framework generalizes existed RR-type mechanisms as special cases under specific parameter settings, which eliminates the need for separate, case-by-case analysis. Theoretically, we prove that BlockRR satisfies $ε$-label DP. We also design a partition method for BlockRR based on a weight matrix derived from label prior information; the parallel composition principle ensures that the composition of two such mechanisms remains $ε$-label DP. Empirically, we evaluate BlockRR on two variants of CIFAR-10 with varying degrees of class imbalance. Results show that in the high-privacy and moderate-privacy regimes ($ε\\leq 3.0$), our propsed method gets a better balance between test accuaracy and the average of per-class accuracy. In the low-privacy regime ($ε\\geq 4.0$), all methods reduce BlockRR to standard RR without additional performance loss.",
    "updated" : "2026-02-03T09:00:45Z",
    "published" : "2026-02-03T09:00:45Z",
    "authors" : [
      {
        "name" : "Haixia Liu"
      },
      {
        "name" : "Yi Ding"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02914v1",
    "title" : "FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction",
    "summary" : "Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\\% matching accuracy and above 96\\% regeneration success, and still exceeds 92\\% matching and 94\\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.",
    "updated" : "2026-02-02T23:41:14Z",
    "published" : "2026-02-02T23:41:14Z",
    "authors" : [
      {
        "name" : "Wenqi Guo"
      },
      {
        "name" : "Shan Du"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02744v1",
    "title" : "An introduction to local differential privacy protocols using block designs",
    "summary" : "The design of protocols for local differential privacy (or LDP) has been a topic of considerable research interest in recent years. LDP protocols utilise the randomised encoding of outcomes of an experiment using a transition probability matrix (TPM). Several authors have observed that balanced incomplete block designs (BIBDs) provide nice examples of TPMs for LDP protocols. Indeed, it has been shown that such BIBD-based LDP protocols provide optimal estimators.\n  In this primarily expository paper, we give a detailed introduction to LDP protocols and their connections with block designs. We prove that a subclass of LDP protocols known as pure LDP protocols are equivalent to $(r,λ)$-designs (which contain balanced incomplete block designs as a special case). An unbiased estimator for an LDP scheme is a left inverse of the transition probability matrix. We show that the optimal estimators for BIBD-based TPMs are precisely those obtained from the Moore-Penrose inverse of the corresponding TPM. We also review some existing work on optimal LDP protocols in the context of pure protocols.",
    "updated" : "2026-02-02T19:58:58Z",
    "published" : "2026-02-02T19:58:58Z",
    "authors" : [
      {
        "name" : "Maura B. Paterson"
      },
      {
        "name" : "Douglas R. Stinson"
      }
    ],
    "categories" : [
      "math.CO",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02718v1",
    "title" : "Composition for Pufferfish Privacy",
    "summary" : "When creating public data products out of confidential datasets, inferential/posterior-based privacy definitions, such as Pufferfish, provide compelling privacy semantics for data with correlations. However, such privacy definitions are rarely used in practice because they do not always compose. For example, it is possible to design algorithms for these privacy definitions that have no leakage when run once but reveal the entire dataset when run more than once. We prove necessary and sufficient conditions that must be added to ensure linear composition for Pufferfish mechanisms, hence avoiding such privacy collapse. These extra conditions turn out to be differential privacy-style inequalities, indicating that achieving both the interpretable semantics of Pufferfish for correlated data and composition benefits requires adopting differentially private mechanisms to Pufferfish. We show that such translation is possible through a concept called the $(a,b)$-influence curve, and many existing differentially private algorithms can be translated with our framework into a composable Pufferfish algorithm. We illustrate the benefit of our new framework by designing composable Pufferfish algorithms for Markov chains that significantly outperform prior work.",
    "updated" : "2026-02-02T19:36:44Z",
    "published" : "2026-02-02T19:36:44Z",
    "authors" : [
      {
        "name" : "Jiamu Bai"
      },
      {
        "name" : "Guanlin He"
      },
      {
        "name" : "Xin Gu"
      },
      {
        "name" : "Daniel Kifer"
      },
      {
        "name" : "Kiwan Maeng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04616v1",
    "title" : "A Human-Centered Privacy Approach (HCP) to AI",
    "summary" : "As the paradigm of Human-Centered AI (HCAI) gains prominence, its benefits to society are accompanied by significant ethical concerns, one of which is the protection of individual privacy. This chapter provides a comprehensive overview of privacy within HCAI, proposing a human-centered privacy (HCP) framework, providing integrated solution from technology, ethics, and human factors perspectives. The chapter begins by mapping privacy risks across each stage of AI development lifecycle, from data collection to deployment and reuse, highlighting the impact of privacy risks on the entire system. The chapter then introduces privacy-preserving techniques such as federated learning and dif erential privacy. Subsequent chapters integrate the crucial user perspective by examining mental models, alongside the evolving regulatory and ethical landscapes as well as privacy governance. Next, advice on design guidelines is provided based on the human-centered privacy framework. After that, we introduce practical case studies across diverse fields. Finally, the chapter discusses persistent open challenges and future research directions, concluding that a multidisciplinary approach, merging technical, design, policy, and ethical expertise, is essential to successfully embed privacy into the core of HCAI, thereby ensuring these technologies advance in a manner that respects and ensures human autonomy, trust and dignity.",
    "updated" : "2026-02-04T14:43:25Z",
    "published" : "2026-02-04T14:43:25Z",
    "authors" : [
      {
        "name" : "Luyi Sun"
      },
      {
        "name" : "Wei Xu"
      },
      {
        "name" : "Zaifeng Gao"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04562v1",
    "title" : "Optimal conversion from Rényi Differential Privacy to $f$-Differential Privacy",
    "summary" : "We prove the conjecture stated in Appendix F.3 of [Zhu et al. (2022)]: among all conversion rules that map a Rényi Differential Privacy (RDP) profile $τ\\mapsto ρ(τ)$ to a valid hypothesis-testing trade-off $f$, the rule based on the intersection of single-order RDP privacy regions is optimal. This optimality holds simultaneously for all valid RDP profiles and for all Type I error levels $α$. Concretely, we show that in the space of trade-off functions, the tightest possible bound is $f_{ρ(\\cdot)}(α) = \\sup_{τ\\geq 0.5} f_{τ,ρ(τ)}(α)$: the pointwise maximum of the single-order bounds for each RDP privacy region. Our proof unifies and sharpens the insights of [Balle et al. (2019)], [Asoodeh et al. (2021)], and [Zhu et al. (2022)]. Our analysis relies on a precise geometric characterization of the RDP privacy region, leveraging its convexity and the fact that its boundary is determined exclusively by Bernoulli mechanisms. Our results establish that the \"intersection-of-RDP-privacy-regions\" rule is not only valid, but optimal: no other black-box conversion can uniformly dominate it in the Blackwell sense, marking the fundamental limit of what can be inferred about a mechanism's privacy solely from its RDP guarantees.",
    "updated" : "2026-02-04T13:49:51Z",
    "published" : "2026-02-04T13:49:51Z",
    "authors" : [
      {
        "name" : "Anneliese Riess"
      },
      {
        "name" : "Juan Felipe Gomez"
      },
      {
        "name" : "Flavio du Pin Calmon"
      },
      {
        "name" : "Julia Anne Schnabel"
      },
      {
        "name" : "Georgios Kaissis"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04482v1",
    "title" : "Proactive Agents, Long-term User Context, VLM Annotation, Privacy Protection, Human-Computer Interaction",
    "summary" : "Proactive agents that anticipate user intentions without explicit prompts represent a significant evolution in human-AI interaction, promising to reduce cognitive load and streamline workflows. However, existing datasets suffer from two critical deficiencies: (1) reliance on LLM-synthesized data that fails to capture authentic human decision-making patterns, and (2) focus on isolated tasks rather than continuous workflows, missing the pre-assistance behavioral context essential for learning proactive intervention signals. To address these gaps, we introduce ProAgentBench, a rigorous benchmark for proactive agents in working scenarios. Our contributions include: (1) a hierarchical task framework that decomposes proactive assistance into timing prediction and assist content generation; (2) a privacy-compliant dataset with 28,000+ events from 500+ hours of real user sessions, preserving bursty interaction patterns (burstiness B=0.787) absent in synthetic data; and (3) extensive experiments that evaluates LLM- and VLM-based baselines. Numerically, we showed that long-term memory and historical context significantly enhance prediction accuracy, while real-world training data substantially outperforms synthetic alternatives. We release our dataset and code at https://anonymous.4open.science/r/ProAgentBench-6BC0.",
    "updated" : "2026-02-04T12:09:15Z",
    "published" : "2026-02-04T12:09:15Z",
    "authors" : [
      {
        "name" : "Yuanbo Tang"
      },
      {
        "name" : "Huaze Tang"
      },
      {
        "name" : "Tingyu Cao"
      },
      {
        "name" : "Lam Nguyen"
      },
      {
        "name" : "Anping Zhang"
      },
      {
        "name" : "Xinwen Cao"
      },
      {
        "name" : "Chunkang Liu"
      },
      {
        "name" : "Wenbo Ding"
      },
      {
        "name" : "Yang Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04262v1",
    "title" : "Parameter Privacy-Preserving Data Sharing: A Particle-Belief MDP Formulation",
    "summary" : "This paper investigates parameter-privacy-preserving data sharing in continuous-state dynamical systems, where a data owner designs a data-sharing policy to support downstream estimation and control while preventing adversarial inference of a sensitive parameter. This data-sharing problem is formulated as an optimization problem that trades off privacy leakage and the impact of data sharing on the data owner's utility, subject to a data-usability constraint. We show that this problem admits an equivalent belief Markov decision process (MDP) formulation, which provides a simplified representation of the optimal policy. To efficiently characterize information-theoretic privacy leakage in continuous state and action spaces, we propose a particle-belief MDP formulation that tracks the parameter posterior via sequential Monte Carlo, yielding a tractable belief-state approximation that converges asymptotically as the number of particles increases. We further derive a tractable closed-form upper bound on particle-based MI via Gaussian mixture approximations, which enables efficient optimization of the particle-belief MDP. Experiments on a mixed-autonomy platoon show that the learned continuous policy substantially impedes inference attacks on human-driving behavior parameters while maintaining data usability and system performance.",
    "updated" : "2026-02-04T06:55:01Z",
    "published" : "2026-02-04T06:55:01Z",
    "authors" : [
      {
        "name" : "Haokun Yu"
      },
      {
        "name" : "Jingyuan Zhou"
      },
      {
        "name" : "Kaidi Yang"
      }
    ],
    "categories" : [
      "eess.SY",
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04124v1",
    "title" : "Privacy Amplification for Synthetic data using Range Restriction",
    "summary" : "We introduce a new class of range restricted formal data privacy standards that condition on owner beliefs about sensitive data ranges. By incorporating this additional information, we can provide a stronger privacy guarantee (e.g. an amplification). The range restricted formal privacy standards protect only a subset (or ball) of data values and exclude ranges (or balls) believed to be already publicly known. The privacy standards are designed for the risk-weighted pseudo posterior (model) mechanism (PPM) used to generate synthetic data under an asymptotic Differential (aDP) privacy guarantee. The PPM downweights the likelihood contribution for each record proportionally to its disclosure risk. The PPM is adapted under inclusion of beliefs by adjusting the risk-weighted pseudo likelihood. We introduce two alternative adjustments. The first expresses data owner knowledge of the sensitive range as a probability, $λ$, that a datum value drawn from the underlying generating distribution lies outside the ball or subspace of values that are sensitive. The portion of each datum likelihood contribution deemed sensitive is then $(1-λ) \\leq 1$ and is the only portion of the likelihood subject to risk down-weighting. The second adjustment encodes knowledge as the difference in probability masses $P(R) \\leq 1$ between the edges of the sensitive range, $R$. We use the resulting conditional (pseudo) likelihood for a sensitive record, which boosts its worst case tail values away from 0. We compare privacy and utility properties for the PPM under the aDP and range restricted privacy standards.",
    "updated" : "2026-02-04T01:36:01Z",
    "published" : "2026-02-04T01:36:01Z",
    "authors" : [
      {
        "name" : "Monika Hu"
      },
      {
        "name" : "Matthew R. Williams"
      },
      {
        "name" : "Terrance D. Savitsky"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.03948v1",
    "title" : "Privacy utility trade offs for parameter estimation in degree heterogeneous higher order networks",
    "summary" : "In sensitive applications involving relational datasets, protecting information about individual links from adversarial queries is of paramount importance. In many such settings, the available data are summarized solely through the degrees of the nodes in the network. We adopt the $β$ model, which is the prototypical statistical model adopted for this form of aggregated relational information, and study the problem of minimax-optimal parameter estimation under both local and central differential privacy constraints. We establish finite sample minimax lower bounds that characterize the precise dependence of the estimation risk on the network size and the privacy parameters, and we propose simple estimators that achieve these bounds up to constants and logarithmic factors under both local and central differential privacy frameworks. Our results provide the first comprehensive finite sample characterization of privacy utility trade offs for parameter estimation in $β$ models, addressing the classical graph case and extending the analysis to higher order hypergraph models. We further demonstrate the effectiveness of our methods through experiments on synthetic data and a real world communication network.",
    "updated" : "2026-02-03T19:11:37Z",
    "published" : "2026-02-03T19:11:37Z",
    "authors" : [
      {
        "name" : "Bibhabasu Mandal"
      },
      {
        "name" : "Sagnik Nandy"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "cs.SI",
      "math.ST"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01928v2",
    "title" : "Privacy Amplification by Missing Data",
    "summary" : "Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.",
    "updated" : "2026-02-04T09:07:53Z",
    "published" : "2026-02-02T10:28:41Z",
    "authors" : [
      {
        "name" : "Simon Roburin"
      },
      {
        "name" : "Rafaël Pinot"
      },
      {
        "name" : "Erwan Scornet"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.05833v1",
    "title" : "Synthesizing Realistic Test Data without Breaking Privacy",
    "summary" : "There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining \"good samples\" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.",
    "updated" : "2026-02-05T16:22:01Z",
    "published" : "2026-02-05T16:22:01Z",
    "authors" : [
      {
        "name" : "Laura Plein"
      },
      {
        "name" : "Alexi Turcotte"
      },
      {
        "name" : "Arina Hallemans"
      },
      {
        "name" : "Andreas Zeller"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.05803v1",
    "title" : "Privacy-Preserving Dynamic Average Consensus by Masking Reference Signals",
    "summary" : "In multi-agent systems, dynamic average consensus (DAC) is a decentralized estimation strategy in which a set of agents tracks the average of time-varying reference signals. Because DAC requires exchanging state information with neighbors, attackers may gain access to these states and infer private information. In this paper, we develop a privacy-preserving method that protects each agent's reference signal from external eavesdroppers and honest-but-curious agents while achieving the same convergence accuracy and convergence rate as conventional DAC. Our approach masks the reference signals by having each agent draw a random real number for each neighbor, exchanges that number over an encrypted channel at the initialization, and computes a masking value to form a masked reference. Then the agents run the conventional DAC algorithm using the masked references. Convergence and privacy analyses show that the proposed algorithm matches the convergence properties of conventional DAC while preserving the privacy of the reference signals. Numerical simulations validate the effectiveness of the proposed privacy-preserving DAC algorithm.",
    "updated" : "2026-02-05T15:57:14Z",
    "published" : "2026-02-05T15:57:14Z",
    "authors" : [
      {
        "name" : "Mihitha Maithripala"
      },
      {
        "name" : "Zongli Lin"
      }
    ],
    "categories" : [
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.05797v1",
    "title" : "Classification Under Local Differential Privacy with Model Reversal and Model Averaging",
    "summary" : "Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.",
    "updated" : "2026-02-05T15:52:34Z",
    "published" : "2026-02-05T15:52:34Z",
    "authors" : [
      {
        "name" : "Caihong Qin"
      },
      {
        "name" : "Yang Bai"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.05016v1",
    "title" : "From Fragmentation to Integration: Exploring the Design Space of AI Agents for Human-as-the-Unit Privacy Management",
    "summary" : "Managing one's digital footprint is overwhelming, as it spans multiple platforms and involves countless context-dependent decisions. Recent advances in agentic AI offer ways forward by enabling holistic, contextual privacy-enhancing solutions. Building on this potential, we adopted a ''human-as-the-unit'' perspective and investigated users' cross-context privacy challenges through 12 semi-structured interviews. Results reveal that people rely on ad hoc manual strategies while lacking comprehensive privacy controls, highlighting nine privacy-management challenges across applications, temporal contexts, and relationships. To explore solutions, we generated nine AI agent concepts and evaluated them via a speed-dating survey with 116 US participants. The three highest-ranked concepts were all post-sharing management tools with half or full agent autonomy, with users expressing greater trust in AI accuracy than in their own efforts. Our findings highlight a promising design space where users see AI agents bridging the fragments in privacy management, particularly through automated, comprehensive post-sharing remediation of users' digital footprints.",
    "updated" : "2026-02-04T20:12:37Z",
    "published" : "2026-02-04T20:12:37Z",
    "authors" : [
      {
        "name" : "Eryue Xu"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04994v1",
    "title" : "SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy",
    "summary" : "With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.",
    "updated" : "2026-02-04T19:30:48Z",
    "published" : "2026-02-04T19:30:48Z",
    "authors" : [
      {
        "name" : "Zhuosen Bao"
      },
      {
        "name" : "Xia Du"
      },
      {
        "name" : "Zheng Lin"
      },
      {
        "name" : "Jizhe Zhou"
      },
      {
        "name" : "Zihan Fang"
      },
      {
        "name" : "Jiening Wu"
      },
      {
        "name" : "Yuxin Zhang"
      },
      {
        "name" : "Zhe Chen"
      },
      {
        "name" : "Chi-man Pun"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "Jun Luo"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04933v1",
    "title" : "The Birthmark Standard: Privacy-Preserving Photo Authentication via Hardware Roots of Trust and Consortium Blockchain",
    "summary" : "The rapid advancement of generative AI systems has collapsed the credibility landscape for photographic evidence. Modern image generation models produce photorealistic images undermining the evidentiary foundation upon which journalism and public discourse depend. Existing authentication approaches, such as the Coalition for Content Provenance and Authenticity (C2PA), embed cryptographically signed metadata directly into image files but suffer from two critical failures: technical vulnerability to metadata stripping during social media reprocessing, and structural dependency on corporate-controlled verification infrastructure where commercial incentives may conflict with public interest. We present the Birthmark Standard, an authentication architecture leveraging manufacturing-unique sensor entropy from non-uniformity correction (NUC) maps and PRNU patterns to generate hardware-rooted authentication keys. During capture, cameras create anonymized authentication certificates proving sensor authenticity without exposing device identity via a key table architecture maintaining anonymity sets exceeding 1,000 devices. Authentication records are stored on a consortium blockchain operated by journalism organizations rather than commercial platforms, enabling verification that survives all metadata loss. We formally verify privacy properties using ProVerif, proving observational equivalence for Manufacturer Non-Correlation and Blockchain Observer Non-Identification under Dolev-Yao adversary assumptions. The architecture is validated through prototype implementation using Raspberry Pi 4 hardware, demonstrating the complete cryptographic pipeline. Performance analysis projects camera overhead below 100ms and verification latency below 500ms at scale of one million daily authentications.",
    "updated" : "2026-02-04T13:16:03Z",
    "published" : "2026-02-04T13:16:03Z",
    "authors" : [
      {
        "name" : "Sam Ryan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04927v1",
    "title" : "PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM",
    "summary" : "Artificial intelligence systems introduce complex privacy risks throughout their lifecycle, especially when processing sensitive or high-dimensional data. Beyond the seven traditional privacy threat categories defined by the LINDDUN framework, AI systems are also exposed to model-centric privacy attacks such as membership inference and model inversion, which LINDDUN does not cover. To address both classical LINDDUN threats and additional AI-driven privacy attacks, PriMod4AI introduces a hybrid privacy threat modeling approach that unifies two structured knowledge sources, a LINDDUN knowledge base representing the established taxonomy, and a model-centric privacy attack knowledge base capturing threats outside LINDDUN. These knowledge bases are embedded into a vector database for semantic retrieval and combined with system level metadata derived from Data Flow Diagram. PriMod4AI uses retrieval-augmented and Data Flow specific prompt generation to guide large language models (LLMs) in identifying, explaining, and categorizing privacy threats across lifecycle stages. The framework produces justified and taxonomy-grounded threat assessments that integrate both classical and AI-driven perspectives. Evaluation on two AI systems indicates that PriMod4AI provides broad coverage of classical privacy categories while additionally identifying model-centric privacy threats. The framework produces consistent, knowledge-grounded outputs across LLMs, as reflected in agreement scores in the observed range.",
    "updated" : "2026-02-04T08:58:19Z",
    "published" : "2026-02-04T08:58:19Z",
    "authors" : [
      {
        "name" : "Gautam Savaliya"
      },
      {
        "name" : "Robert Aufschläger"
      },
      {
        "name" : "Abhishek Subedi"
      },
      {
        "name" : "Michael Heigl"
      },
      {
        "name" : "Martin Schramm"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04895v1",
    "title" : "Privacy Amplification Persists under Unlimited Synthetic Data Release",
    "summary" : "We study privacy amplification by synthetic data release, a phenomenon in which differential privacy guarantees are improved by releasing only synthetic data rather than the private generative model itself. Recent work by Pierquin et al. (2025) established the first formal amplification guarantees for a linear generator, but they apply only in asymptotic regimes where the model dimension far exceeds the number of released synthetic records, limiting their practical relevance. In this work, we show a surprising result: under a bounded-parameter assumption, privacy amplification persists even when releasing an unbounded number of synthetic records, thereby improving upon the bounds of Pierquin et al. (2025). Our analysis provides structural insights that may guide the development of tighter privacy guarantees for more complex release mechanisms.",
    "updated" : "2026-02-03T09:27:42Z",
    "published" : "2026-02-03T09:27:42Z",
    "authors" : [
      {
        "name" : "Clément Pierquin"
      },
      {
        "name" : "Aurélien Bellet"
      },
      {
        "name" : "Marc Tommasi"
      },
      {
        "name" : "Matthieu Boussard"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.01177v2",
    "title" : "Equivalence of Privacy and Stability with Generalization Guarantees in Quantum Learning",
    "summary" : "We present a unified information-theoretic framework elucidating the interplay between stability, privacy, and the generalization performance of quantum learning algorithms. We establish a bound on the expected generalization error in terms of quantum mutual information and derive a probabilistic upper bound that generalizes the classical result by Esposito et al. (2021). Complementing these findings, we provide a lower bound on the expected true loss relative to the expected empirical loss. Additionally, we demonstrate that $(\\varepsilon, δ)$-quantum differentially private learning algorithms are stable, thereby ensuring strong generalization guarantees. Finally, we extend our analysis to dishonest learning algorithms, introducing Information-Theoretic Admissibility (ITA) to characterize the fundamental limits of privacy when the learning algorithm is oblivious to specific dataset instances.",
    "updated" : "2026-02-05T10:06:53Z",
    "published" : "2026-02-01T12:03:07Z",
    "authors" : [
      {
        "name" : "Ayanava Dasgupta"
      },
      {
        "name" : "Naqueeb Ahmad Warsi"
      },
      {
        "name" : "Masahito Hayashi"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.IT",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.06756v1",
    "title" : "$f$-Differential Privacy Filters: Validity and Approximate Solutions",
    "summary" : "Accounting for privacy loss under fully adaptive composition -- where both the choice of mechanisms and their privacy parameters may depend on the entire history of prior outputs -- is a central challenge in differential privacy (DP). In this setting, privacy filters are stopping rules for compositions that ensure a prescribed global privacy budget is not exceeded. It remains unclear whether optimal trade-off-function-based notions, such as $f$-DP, admit valid privacy filters under fully adaptive interaction. We show that the natural approach to defining an $f$-DP filter -- composing individual trade-off curves and stopping when the prescribed $f$-DP curve is crossed -- is fundamentally invalid. We characterise when and why this failure occurs, and establish necessary and sufficient conditions under which the natural filter is valid. Furthermore, we prove a fully adaptive central limit theorem for $f$-DP and construct an approximate Gaussian DP filter for subsampled Gaussian mechanisms at small sampling rates $q<0.2$ and large sampling rates $q>0.8$, yielding tighter privacy guarantees than filters based on Rényi DP in the same setting.",
    "updated" : "2026-02-06T15:04:02Z",
    "published" : "2026-02-06T15:04:02Z",
    "authors" : [
      {
        "name" : "Long Tran"
      },
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "Ossi Räisä"
      },
      {
        "name" : "Antti Honkela"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.06518v1",
    "title" : "Sequential Auditing for f-Differential Privacy",
    "summary" : "We present new auditors to assess Differential Privacy (DP) of an algorithm based on output samples. Such empirical auditors are common to check for algorithmic correctness and implementation bugs. Most existing auditors are batch-based or targeted toward the traditional notion of $(\\varepsilon,δ)$-DP; typically both. In this work, we shift the focus to the highly expressive privacy concept of $f$-DP, in which the entire privacy behavior is captured by a single tradeoff curve. Our auditors detect violations across the full privacy spectrum with statistical significance guarantees, which are supported by theory and simulations. Most importantly, and in contrast to prior work, our auditors do not require a user-specified sample size as an input. Rather, they adaptively determine a near-optimal number of samples needed to reach a decision, thereby avoiding the excessively large sample sizes common in many auditing studies. This reduction in sampling cost becomes especially beneficial for expensive training procedures such as DP-SGD. Our method supports both whitebox and blackbox settings and can also be executed in single-run frameworks.",
    "updated" : "2026-02-06T09:22:24Z",
    "published" : "2026-02-06T09:22:24Z",
    "authors" : [
      {
        "name" : "Tim Kutta"
      },
      {
        "name" : "Martin Dunsche"
      },
      {
        "name" : "Yu Wei"
      },
      {
        "name" : "Vassilis Zikas"
      }
    ],
    "categories" : [
      "cs.CR",
      "stat.ME",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.06390v1",
    "title" : "Generating High-quality Privacy-preserving Synthetic Data",
    "summary" : "Synthetic tabular data enables sharing and analysis of sensitive records, but its practical deployment requires balancing distributional fidelity, downstream utility, and privacy protection. We study a simple, model agnostic post processing framework that can be applied on top of any synthetic data generator to improve this trade off. First, a mode patching step repairs categories that are missing or severely underrepresented in the synthetic data, while largely preserving learned dependencies. Second, a k nearest neighbor filter replaces synthetic records that lie too close to real data points, enforcing a minimum distance between real and synthetic samples. We instantiate this framework for two neural generative models for tabular data, a feed forward generator and a variational autoencoder, and evaluate it on three public datasets covering credit card transactions, cardiovascular health, and census based income. We assess marginal and joint distributional similarity, the performance of models trained on synthetic data and evaluated on real data, and several empirical privacy indicators, including nearest neighbor distances and attribute inference attacks. With moderate thresholds between 0.2 and 0.35, the post processing reduces divergence between real and synthetic categorical distributions by up to 36 percent and improves a combined measure of pairwise dependence preservation by 10 to 14 percent, while keeping downstream predictive performance within about 1 percent of the unprocessed baseline. At the same time, distance based privacy indicators improve and the success rate of attribute inference attacks remains largely unchanged. These results provide practical guidance for selecting thresholds and applying post hoc repairs to improve the quality and empirical privacy of synthetic tabular data, while complementing approaches that provide formal differential privacy guarantees.",
    "updated" : "2026-02-06T05:03:49Z",
    "published" : "2026-02-06T05:03:49Z",
    "authors" : [
      {
        "name" : "David Yavo"
      },
      {
        "name" : "Richard Khoury"
      },
      {
        "name" : "Christophe Pere"
      },
      {
        "name" : "Sadoune Ait Kaci Azzou"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.06238v1",
    "title" : "Private Sum Computation: Trade-Offs between Communication, Randomness, and Privacy",
    "summary" : "Consider multiple users and a fusion center. Each user possesses a sequence of bits and can communicate with the fusion center through a one-way public channel. The fusion center's task is to compute the sum of all the sequences under the privacy requirement that a set of colluding users, along with the fusion center, cannot gain more than a predetermined amount $δ$ of information, measured through mutual information, about the sequences of other users. Our first contribution is to characterize the minimum amount of necessary communication between the users and the fusion center, as well as the minimum amount of necessary randomness at the users. Our second contribution is to establish a connection between private sum computation and secret sharing by showing that secret sharing is necessary to generate the local randomness needed for private sum computation, and prove that it holds true for any $δ\\geq 0$.",
    "updated" : "2026-02-05T22:29:10Z",
    "published" : "2026-02-05T22:29:10Z",
    "authors" : [
      {
        "name" : "Remi A. Chou"
      },
      {
        "name" : "Joerg Kliewer"
      },
      {
        "name" : "Aylin Yener"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.08657v1",
    "title" : "Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction",
    "summary" : "Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.",
    "updated" : "2026-02-09T13:49:42Z",
    "published" : "2026-02-09T13:49:42Z",
    "authors" : [
      {
        "name" : "Xiaotong Liu"
      },
      {
        "name" : "Shao-Bo Lin"
      },
      {
        "name" : "Jun Fan"
      },
      {
        "name" : "Ding-Xuan Zhou"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.08617v1",
    "title" : "ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning",
    "summary" : "Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.",
    "updated" : "2026-02-09T13:05:41Z",
    "published" : "2026-02-09T13:05:41Z",
    "authors" : [
      {
        "name" : "Dario Fenoglio"
      },
      {
        "name" : "Pasquale Polverino"
      },
      {
        "name" : "Jacopo Quizi"
      },
      {
        "name" : "Martin Gjoreski"
      },
      {
        "name" : "Marc Langheinrich"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.08465v1",
    "title" : "A Two-Week In-the-Wild Study of Screen Filters and Camera Sliders for Smartphone Privacy in Public Spaces",
    "summary" : "Smartphone usage in public spaces can raise privacy concerns, in terms of shoulder surfing and unintended camera capture. In real-world public space settings, we investigated the impact of tangible privacy-enhancing tools (here: screen filter and camera slider) on smartphone users' reported privacy perception, behavioral adaptations, usability and social dynamics. We conducted a mixed-method, in-the-wild study ($N = 22$) using off-the-shelf smartphone privacy tools. We investigated subjective behavioral transition by combining questionnaires with semi-structured interviews. Participants used the screen filter and the camera slider for two weeks; they reported changes in attitude and behavior after using a screen filter including screen visibility and comfort when using phones publicly. They explained decreased privacy-protective behaviors, such as actively covering their screens, suggesting a shift in perceived risk. Qualitative findings about the camera slider suggested underlying psychological mechanisms, including privacy awareness and concerns about social perception, while also offering insights regarding the tools' effectiveness.",
    "updated" : "2026-02-09T10:14:22Z",
    "published" : "2026-02-09T10:14:22Z",
    "authors" : [
      {
        "name" : "Andreas Tjeldflaat"
      },
      {
        "name" : "Piero Romare"
      },
      {
        "name" : "Yuki Onishi"
      },
      {
        "name" : "Morten Fjeld"
      },
      {
        "name" : "Bjørn Sætrevik"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.08268v1",
    "title" : "Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI",
    "summary" : "Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.",
    "updated" : "2026-02-09T05:00:48Z",
    "published" : "2026-02-09T05:00:48Z",
    "authors" : [
      {
        "name" : "Akinori Maeda"
      },
      {
        "name" : "Yuto Sekiya"
      },
      {
        "name" : "Sota Sugimura"
      },
      {
        "name" : "Tomoya Asai"
      },
      {
        "name" : "Yu Tsuda"
      },
      {
        "name" : "Kohei Ikeda"
      },
      {
        "name" : "Hiroshi Fujii"
      },
      {
        "name" : "Kohei Watanabe"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.07936v1",
    "title" : "Privacy-Preserving Covert Communication Using Encrypted Wearable Gesture Recognition",
    "summary" : "Secure communication is essential in covert and safety-critical settings where verbal interactions may expose user intent or operational context. Wearable gesture-based communication enables low-effort, nonverbal interaction, but existing systems leak motion data, intermediate representations, or inference outputs to untrusted infrastructure, enabling intent inference, behavioral biometric leakage, and insider attacks. This work proposes a privacy-preserving gesture-based covert communication system that ensures, no raw sensor signals, learned features, or classification outputs are exposed to any third-party. The system employs a multi-party homomorphic learning pipeline for gesture recognition directly over encrypted motion data, preventing adversaries from inferring gesture semantics, replaying sensor traces, or accessing intermediate representations. To our knowledge, this work is the first to apply encrypted gesture recognition in a wearable-based covert communication setting. We design and evaluate haptic and visual feedback mechanisms for covert signal delivery and evaluate the system using 600 gesture samples from a commodity smartwatch, achieving over 94.44% classification accuracy and demonstrating the feasibility of the proposed system with practical deployability from high-performance systems to resource-constrained edge devices.",
    "updated" : "2026-02-08T12:06:01Z",
    "published" : "2026-02-08T12:06:01Z",
    "authors" : [
      {
        "name" : "Tasnia Ashrafi Heya"
      },
      {
        "name" : "Sayed Erfan Arefin"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.07850v1",
    "title" : "Privacy-Preserving Coding Schemes for Multi-Access Distributed Computing Models",
    "summary" : "Distributed computing frameworks such as MapReduce have become essential for large-scale data processing by decomposing tasks across multiple nodes. The multi-access distributed computing (MADC) model further advances this paradigm by decoupling mapper and reducer roles: dedicated mapper nodes store data and compute intermediate values, while reducer nodes are connected to multiple mappers and aggregate results to compute final outputs. This separation reduces communication bottlenecks without requiring file replication. In this paper, we introduce privacy constraints into MADC and develop private coded schemes for two specific connectivity models. We construct new families of extended placement delivery arrays and derive corresponding coding schemes that guarantee privacy of each reducer's assigned function.",
    "updated" : "2026-02-08T07:40:02Z",
    "published" : "2026-02-08T07:40:02Z",
    "authors" : [
      {
        "name" : "Shanuja Sasi"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.07722v1",
    "title" : "IPBAC: Interaction Provenance-Based Access Control for Secure and Privacy-Aware Systems",
    "summary" : "Traditional access control systems, including RBAC, face significant limitations such as inflexible role definitions, difficulty handling dynamic scenarios, and lack of detailed accountability and traceability. To this end, we introduce the Interaction Provenance-based Access Control (IPBAC) model. In this paper, we explore the integration of interaction provenance with access control to overcome these limitations. Interaction provenance refers to the detailed recording of actions and interactions within a system, capturing comprehensive metadata such as the identity of the actor, the time of an action, and the context. IPBAC ensures stronger protection against unauthorized access, enhances traceability for auditing and compliance, and supports adaptive security policies. This provenance-based access control not only strengthens security, but also provides a robust framework for auditing and compliance.",
    "updated" : "2026-02-07T22:32:59Z",
    "published" : "2026-02-07T22:32:59Z",
    "authors" : [
      {
        "name" : "Sharif Noor Zisad"
      },
      {
        "name" : "Ragib Hasan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.07149v1",
    "title" : "Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds",
    "summary" : "The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.",
    "updated" : "2026-02-06T19:47:10Z",
    "published" : "2026-02-06T19:47:10Z",
    "authors" : [
      {
        "name" : "Rawisara Lohanimit"
      },
      {
        "name" : "Yankun Wu"
      },
      {
        "name" : "Amelia Katirai"
      },
      {
        "name" : "Yuta Nakashima"
      },
      {
        "name" : "Noa Garcia"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.07090v1",
    "title" : "Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks",
    "summary" : "Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.",
    "updated" : "2026-02-06T09:28:55Z",
    "published" : "2026-02-06T09:28:55Z",
    "authors" : [
      {
        "name" : "Yu-Che Tsai"
      },
      {
        "name" : "Hsiang Hsiao"
      },
      {
        "name" : "Kuan-Yu Chen"
      },
      {
        "name" : "Shou-De Lin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.04124v2",
    "title" : "Privacy Amplification for Synthetic data using Range Restriction",
    "summary" : "We introduce a new class of range restricted formal data privacy standards that condition on owner beliefs about sensitive data ranges. By incorporating this additional information, we can provide a stronger privacy guarantee (e.g. an amplification). The range restricted formal privacy standards protect only a subset (or ball) of data values and exclude ranges (or balls) believed to be already publicly known. The privacy standards are designed for the risk-weighted pseudo posterior (model) mechanism (PPM) used to generate synthetic data under an asymptotic Differential (aDP) privacy guarantee. The PPM downweights the likelihood contribution for each record proportionally to its disclosure risk. The PPM is adapted under inclusion of beliefs by adjusting the risk-weighted pseudo likelihood. We introduce two alternative adjustments. The first expresses data owner knowledge of the sensitive range as a probability, $λ$, that a datum value drawn from the underlying generating distribution lies outside the ball or subspace of values that are sensitive. The portion of each datum likelihood contribution deemed sensitive is then $(1-λ) \\leq 1$ and is the only portion of the likelihood subject to risk down-weighting. The second adjustment encodes knowledge as the difference in probability masses $P(R) \\leq 1$ between the edges of the sensitive range, $R$. We use the resulting conditional (pseudo) likelihood for a sensitive record, which boosts its worst case tail values away from 0. We compare privacy and utility properties for the PPM under the aDP and range restricted privacy standards.",
    "updated" : "2026-02-06T19:57:59Z",
    "published" : "2026-02-04T01:36:01Z",
    "authors" : [
      {
        "name" : "Jingchen Hu"
      },
      {
        "name" : "Matthew R. Williams"
      },
      {
        "name" : "Terrance D. Savitsky"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.02296v2",
    "title" : "Decoupling Generalizability and Membership Privacy Risks in Neural Networks",
    "summary" : "A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.",
    "updated" : "2026-02-09T18:44:22Z",
    "published" : "2026-02-02T16:32:42Z",
    "authors" : [
      {
        "name" : "Xingli Fang"
      },
      {
        "name" : "Jung-Eun Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10100v1",
    "title" : "Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy",
    "summary" : "Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretable approaches with reduced number of features and less complex internal architecture. In this context, this paper aims to achieve a machine learning (ML) model that combines enhanced data privacy with explainability. So, we propose a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superior explainability than neural networks-based FL systems; (ii) provides additional layer of data privacy protection applying Differential Privacy (DP) to the Tree-Based model. However, there is a side effect adding DP: it harms the explainability of the system. So, this paper also presents the impact of DP protection on the explainability of the ML model. The carried out performance assessment shows improvements of FEXT-DP in terms of a faster training, i.e., numbers of rounds, Mean Squared Error and explainability.",
    "updated" : "2026-02-10T18:58:11Z",
    "published" : "2026-02-10T18:58:11Z",
    "authors" : [
      {
        "name" : "Júlio Oliveira"
      },
      {
        "name" : "Rodrigo Ferreira"
      },
      {
        "name" : "André Riker"
      },
      {
        "name" : "Glaucio H. S. Carvalho"
      },
      {
        "name" : "Eirini Eleni Tsilopoulou"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.09904v1",
    "title" : "Safeguarding Privacy: Privacy-Preserving Detection of Mind Wandering and Disengagement Using Federated Learning in Online Education",
    "summary" : "Since the COVID-19 pandemic, online courses have expanded access to education, yet the absence of direct instructor support challenges learners' ability to self-regulate attention and engagement. Mind wandering and disengagement can be detrimental to learning outcomes, making their automated detection via video-based indicators a promising approach for real-time learner support. However, machine learning-based approaches often require sharing sensitive data, raising privacy concerns. Federated learning offers a privacy-preserving alternative by enabling decentralized model training while also distributing computational load. We propose a framework exploiting cross-device federated learning to address different manifestations of behavioral and cognitive disengagement during remote learning, specifically behavioral disengagement, mind wandering, and boredom. We fit video-based cognitive disengagement detection models using facial expressions and gaze features. By adopting federated learning, we safeguard users' data privacy through privacy-by-design and introduce a novel solution with the potential for real-time learner support. We further address challenges posed by eyeglasses by incorporating related features, enhancing overall model performance. To validate the performance of our approach, we conduct extensive experiments on five datasets and benchmark multiple federated learning algorithms. Our results show great promise for privacy-preserving educational technologies promoting learner engagement.",
    "updated" : "2026-02-10T15:40:01Z",
    "published" : "2026-02-10T15:40:01Z",
    "authors" : [
      {
        "name" : "Anna Bodonhelyi"
      },
      {
        "name" : "Mengdi Wang"
      },
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Babette Bühler"
      },
      {
        "name" : "Enkelejda Kasneci"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.09627v1",
    "title" : "Parallel Composition for Statistical Privacy",
    "summary" : "Differential Privacy (DP) considers a scenario in which an adversary has almost complete information about the entries of a database. This worst-case assumption is likely to overestimate the privacy threat faced by an individual in practice. In contrast, Statistical Privacy (SP), as well as related notions such as noiseless privacy or limited background knowledge privacy, describe a setting in which the adversary knows the distribution of the database entries, but not their exact realizations. In this case, privacy analysis must account for the interaction between uncertainty induced by the entropy of the underlying distributions and privacy mechanisms that distort query answers, which can be highly non-trivial.\n  This paper investigates this problem for multiple queries (composition). A privacy mechanism is proposed that is based on subsampling and randomly partitioning the database to bound the dependency among queries. This way for the first time, to the best of our knowledge, upper privacy bounds against limited adversaries are obtained without any further restriction on the database.\n  These bounds show that in realistic application scenarios taking the entropy of distributions into account yields improvements of privacy and precision guarantees. We illustrate examples where for fixed privacy parameters and utility loss SP allows significantly more queries than DP.",
    "updated" : "2026-02-10T10:13:44Z",
    "published" : "2026-02-10T10:13:44Z",
    "authors" : [
      {
        "name" : "Dennis Breutigam"
      },
      {
        "name" : "Rüdiger Reischuk"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.09357v1",
    "title" : "Data Sharing with Endogenous Choices over Differential Privacy Levels",
    "summary" : "We study coalition formation for data sharing under differential privacy when agents have heterogeneous privacy costs. Each agent holds a sensitive data point and decides whether to participate in a data-sharing coalition and how much noise to add to their data. Privacy choices induce a fundamental trade-off: higher privacy reduces individual data-sharing costs but degrades data utility and statistical accuracy for the coalition. These choices generate externalities across agents, making both participation and privacy levels strategic. Our goal is to understand which coalitions are stable, how privacy choices shape equilibrium outcomes, and how decentralized data sharing compares to a centralized, socially optimal benchmark.\n  We provide a comprehensive equilibrium analysis across a broad range of privacy-cost regimes, from decreasing costs (e.g., privacy amplification from pooling data) to increasing costs (e.g., greater exposure to privacy attacks in larger coalitions). We first characterize Nash equilibrium coalitions with endogenous privacy levels and show that equilibria may fail to exist and can be non-monotonic in problem parameters. We also introduce a weaker equilibrium notion called robust equilibrium (that allows more widespread equilibrium existence by equipping existing players in the coalition with the power to prevent or veto external players from joining) and fully characterize such equilibria. Finally, we analyze, for both Nash and robust equilibria, the efficiency relative to the social optimum in terms of social welfare and estimator accuracy. We derive bounds that depend sharply on the number of players, properties of the cost profile and how privacy costs scale with coalition size.",
    "updated" : "2026-02-10T03:01:14Z",
    "published" : "2026-02-10T03:01:14Z",
    "authors" : [
      {
        "name" : "Raef Bassily"
      },
      {
        "name" : "Kate Donahue"
      },
      {
        "name" : "Diptangshu Sen"
      },
      {
        "name" : "Annuo Zhao"
      },
      {
        "name" : "Juba Ziani"
      }
    ],
    "categories" : [
      "cs.GT",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.09338v1",
    "title" : "Privacy Amplification for BandMF via $b$-Min-Sep Subsampling",
    "summary" : "We study privacy amplification for BandMF, i.e., DP-SGD with correlated noise across iterations via a banded correlation matrix. We propose $b$-min-sep subsampling, a new subsampling scheme that generalizes Poisson and balls-in-bins subsampling, extends prior practical batching strategies for BandMF, and enables stronger privacy amplification than cyclic Poisson while preserving the structural properties needed for analysis. We give a near-exact privacy analysis using Monte Carlo accounting, based on a dynamic program that leverages the Markovian structure in the subsampling procedure. We show that $b$-min-sep matches cyclic Poisson subsampling in the high noise regime and achieves strictly better guarantees in the mid-to-low noise regime, with experimental results that bolster our claims. We further show that unlike previous BandMF subsampling schemes, our $b$-min-sep subsampling naturally extends to the multi-attribution user-level privacy setting.",
    "updated" : "2026-02-10T02:10:38Z",
    "published" : "2026-02-10T02:10:38Z",
    "authors" : [
      {
        "name" : "Andy Dong"
      },
      {
        "name" : "Arun Ganesh"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.09288v1",
    "title" : "Measuring Privacy Risks and Tradeoffs in Financial Synthetic Data Generation",
    "summary" : "We explore the privacy-utility tradeoff of synthetic data generation schemes on tabular financial datasets, a domain characterized by high regulatory risk and severe class imbalance. We consider representative tabular data generators, including autoencoders, generative adversarial networks, diffusion, and copula synthesizers. To address the challenges of the financial domain, we provide novel privacy-preserving implementations of GAN and autoencoder synthesizers. We evaluate whether and how well the generators simultaneously achieve data quality, downstream utility, and privacy, with comparison across balanced and imbalanced input datasets. Our results offer insight into the distinct challenges of generating synthetic data from datasets that exhibit severe class imbalance and mixed-type attributes.",
    "updated" : "2026-02-10T00:14:19Z",
    "published" : "2026-02-10T00:14:19Z",
    "authors" : [
      {
        "name" : "Michael Zuo"
      },
      {
        "name" : "Inwon Kang"
      },
      {
        "name" : "Stacy Patterson"
      },
      {
        "name" : "Oshani Seneviratne"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.09273v1",
    "title" : "The Price of Privacy For Approximating Max-CSP",
    "summary" : "We study approximation algorithms for Maximum Constraint Satisfaction Problems (Max-CSPs) under differential privacy (DP) where the constraints are considered sensitive data. Information-theoretically, we aim to classify the best approximation ratios possible for a given privacy budget $\\varepsilon$. In the high-privacy regime ($\\varepsilon \\ll 1$), we show that any $\\varepsilon$-DP algorithm cannot beat a random assignment by more than $O(\\varepsilon)$ in the approximation ratio. We devise a polynomial-time algorithm which matches this barrier under the assumptions that the instances are bounded-degree and triangle-free. Finally, we show that one or both of these assumptions can be removed for specific CSPs--such as Max-Cut or Max $k$-XOR--albeit at the cost of computational efficiency.",
    "updated" : "2026-02-09T23:16:36Z",
    "published" : "2026-02-09T23:16:36Z",
    "authors" : [
      {
        "name" : "Prathamesh Dharangutte"
      },
      {
        "name" : "Jingcheng Liu"
      },
      {
        "name" : "Pasin Manurangsi"
      },
      {
        "name" : "Akbar Rafiey"
      },
      {
        "name" : "Phanu Vajanopath"
      },
      {
        "name" : "Zongrui Zou"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.09254v1",
    "title" : "Investigating Bystander Privacy in Chinese Smart Home Apps",
    "summary" : "Bystander privacy in smart homes has been widely studied in Western contexts, yet it remains underexplored in non-Western countries such as China. In this study, we analyze 49 Chinese smart home apps using a mixed-methods approach, including privacy policy review, UX/UI evaluation, and assessment of Apple App Store privacy labels. While most apps nominally comply with national regulations, we identify significant gaps between written policies and actual implementation. Our traceability analysis highlights inconsistencies in data controls and a lack of transparency in data-sharing practices. Crucially, bystander privacy -- particularly for visitors and non-user individuals -- is largely absent from both policy documents and interface design. Additionally, discrepancies between privacy labels and actual data practices threaten user trust and undermine informed consent. We provide design recommendations to strengthen bystander protections, improve privacy-oriented UI transparency, and enhance the credibility of privacy labels, supporting the development of inclusive smart home ecosystems in non-Western contexts.",
    "updated" : "2026-02-09T22:38:42Z",
    "published" : "2026-02-09T22:38:42Z",
    "authors" : [
      {
        "name" : "Shijing He"
      },
      {
        "name" : "Xuchen Wang"
      },
      {
        "name" : "Yaxiong Lei"
      },
      {
        "name" : "Chi Zhang"
      },
      {
        "name" : "Ruba Abu-Salma"
      },
      {
        "name" : "Jose Such"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.08268v2",
    "title" : "Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI",
    "summary" : "Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.",
    "updated" : "2026-02-10T05:00:53Z",
    "published" : "2026-02-09T05:00:48Z",
    "authors" : [
      {
        "name" : "Akinori Maeda"
      },
      {
        "name" : "Yuto Sekiya"
      },
      {
        "name" : "Sota Sugimura"
      },
      {
        "name" : "Tomoya Asai"
      },
      {
        "name" : "Yu Tsuda"
      },
      {
        "name" : "Kohei Ikeda"
      },
      {
        "name" : "Hiroshi Fujii"
      },
      {
        "name" : "Kohei Watanabe"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.11026v1",
    "title" : "Normalized Surveillance in the Datafied Car: How Autonomous Vehicle Users Rationalize Privacy Trade-offs",
    "summary" : "Autonomous vehicles (AVs) are characterized by pervasive datafication and surveillance through sensors like in-cabin cameras, LIDAR, and GPS. Drawing on 16 semi-structured interviews with AV drivers analyzed using constructivist grounded theory, this study examines how users make sense of vehicular surveillance within everyday datafication. Findings reveal drivers demonstrate few AV-specific privacy concerns, instead normalizing monitoring through comparisons with established digital platforms. We theorize this indifference by situating AV surveillance within the `surveillance ecology' of platform environments, arguing the datafied car functions as a mobile extension of the `leaky home' -- private spaces rendered permeable through connected technologies continuously transmitting behavioral data.\n  The study contributes to scholarship on surveillance beliefs, datafication, and platform governance by demonstrating how users who have accepted comprehensive smartphone and smart home monitoring encounter AV datafication as just another node in normalized data extraction. We highlight how geographic restrictions on data access -- currently limiting driver log access to California -- create asymmetries that impede informed privacy deliberation, exemplifying `tertiary digital divides.' Finally, we examine how machine learning's reliance on data-intensive approaches creates structural pressure for surveillance that transcends individual manufacturer choices. We propose governance interventions to democratize social learning, including universal data access rights, binding transparency requirements, and data minimization standards to prevent race-to-the-bottom dynamics in automotive datafication.",
    "updated" : "2026-02-11T16:52:06Z",
    "published" : "2026-02-11T16:52:06Z",
    "authors" : [
      {
        "name" : "Yehuda Perry"
      },
      {
        "name" : "Tawfiq Ammari"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.11023v1",
    "title" : "IU-GUARD: Privacy-Preserving Spectrum Coordination for Incumbent Users under Dynamic Spectrum Sharing",
    "summary" : "With the growing demand for wireless spectrum, dynamic spectrum sharing (DSS) frameworks such as the Citizens Broadband Radio Service (CBRS) have emerged as practical solutions to improve utilization while protecting incumbent users (IUs) such as military radars. However, current incumbent protection mechanisms face critical limitations. The Environmental Sensing Capability (ESC) requires costly sensor deployments and remains vulnerable to interference and security risks. Alternatively, the Incumbent Informing Capability (IIC) requires IUs to disclose their identities and operational parameters to the Spectrum Coordination System (SCS), creating linkable records that compromise operational privacy and mission secrecy. We propose IU-GUARD, a privacy-preserving spectrum sharing framework that enables IUs to access spectrum without revealing their identities. Leveraging verifiable credentials (VCs) and zero-knowledge proofs (ZKPs), IU-GUARD allows IUs to prove their authorization to the SCS while disclosing only essential operational parameters. This decouples IU identity from spectrum access, prevents cross-request linkage, and mitigates the risk of centralized SCS data leakage. We implement a prototype, and our evaluation shows that IU-GUARD achieves strong privacy guarantees with practical computation and communication overhead, making it suitable for real-time DSS deployment.",
    "updated" : "2026-02-11T16:49:04Z",
    "published" : "2026-02-11T16:49:04Z",
    "authors" : [
      {
        "name" : "Shaoyu Li"
      },
      {
        "name" : "Hexuan Yu"
      },
      {
        "name" : "Shanghao Shi"
      },
      {
        "name" : "Md Mohaimin Al Barat"
      },
      {
        "name" : "Yang Xiao"
      },
      {
        "name" : "Y. Thomas Hou"
      },
      {
        "name" : "Wenjing Lou"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10877v1",
    "title" : "Beyond Permissions: An Empirical Static Analysis of Privacy and Security Risks in Children-Oriented and General-Audience Mobile Apps for Gaming",
    "summary" : "Mobile gaming applications (apps) have become increasingly pervasive, including a growing number of games designed for children. Despite their popularity, these apps often integrate complex analytics, advertising, and attribution infrastructures that may introduce privacy and security risks. Existing research has primarily focused on tracking behaviors or monetization models, leaving configuration-level privacy exposure and children-oriented apps underexplored. In this study, we conducted a comparative static analysis of Android mobile games to investigate privacy and security risks beyond permission usage. The analysis follows a three-phase methodology comprising (i) designing study protocol, (ii) Android Package Kit (APK) collection and static inspection, and (iii) data analysis. We examined permissions, manifest-level configuration properties (e.g., backup settings, cleartext network traffic, and exported components), and embedded third-party Software Development Kit (SDK) ecosystems across children-oriented and general-audience mobile games. The extracted indicators are synthesized into qualitative privacy-risk categories to support comparative reporting. The results showed that while children-oriented games often request fewer permissions, they frequently exhibit configuration-level risks and embed third-party tracking SDKs similar to general-audience games. Architectural and configuration decisions play a critical role in shaping privacy risks, particularly for apps targeting children. This study contributes a holistic static assessment of privacy exposure in mobile games and provides actionable insights for developers, platform providers, and researchers seeking to improve privacy-by-design practices in mobile applications.",
    "updated" : "2026-02-11T14:06:03Z",
    "published" : "2026-02-11T14:06:03Z",
    "authors" : [
      {
        "name" : "Bakheet Aljedaani"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10735v1",
    "title" : "Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity",
    "summary" : "A narrated e-book combines synchronized audio with digital text, highlighting the currently spoken word or sentence during playback. This format supports early literacy and assists individuals with reading challenges, while also allowing general readers to seamlessly switch between reading and listening. With the emergence of natural-sounding neural Text-to-Speech (TTS) technology, several commercial services have been developed to leverage these technology for converting standard text e-books into high-quality narrated e-books. However, no open-source solutions currently exist to perform this task. In this paper, we present Calliope, an open-source framework designed to fill this gap. Our method leverages state-of-the-art open-source TTS to convert a text e-book into a narrated e-book in the EPUB 3 Media Overlay format. The method offers several innovative steps: audio timestamps are captured directly during TTS, ensuring exact synchronization between narration and text highlighting; the publisher's original typography, styling, and embedded media are strictly preserved; and the entire pipeline operates offline. This offline capability eliminates recurring API costs, mitigates privacy concerns, and avoids copyright compliance issues associated with cloud-based services. The framework currently supports the state-of-the-art open-source TTS systems XTTS-v2 and Chatterbox. A potential alternative approach involves first generating narration via TTS and subsequently synchronizing it with the text using forced alignment. However, while our method ensures exact synchronization, our experiments show that forced alignment introduces drift between the audio and text highlighting significant enough to degrade the reading experience. Source code and usage instructions are available at https://github.com/hugohammer/TTS-Narrated-Ebook-Creator.git.",
    "updated" : "2026-02-11T10:54:59Z",
    "published" : "2026-02-11T10:54:59Z",
    "authors" : [
      {
        "name" : "Hugo L. Hammer"
      },
      {
        "name" : "Vajira Thambawita"
      },
      {
        "name" : "Pål Halvorsen"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10734v1",
    "title" : "Security, Privacy and System-Level Resillience of 6G End-to-End System: Hexa-X-II Perspective",
    "summary" : "The sixth generation (6G) of mobile networks are being developed to overcome limitations in previous generations and meet emerging user demands. As a European project, the Smart Networks and Services Joint Undertaking (SNS JU) 6G Flagship project Hexa-X-II has a leading role for developing technologies and anchoring 6G end-to-end system. This paper summarizes the security, privacy and resilient (SPR) controls identified by Hexa-X-II project and their validation frameworks.",
    "updated" : "2026-02-11T10:53:15Z",
    "published" : "2026-02-11T10:53:15Z",
    "authors" : [
      {
        "name" : "Pawani Porambage"
      },
      {
        "name" : "Diego Lopez"
      },
      {
        "name" : "Antonio Pastor"
      },
      {
        "name" : "Bin Han"
      },
      {
        "name" : "José María Jorquera Valero"
      },
      {
        "name" : "Manuel Gil Pérez"
      },
      {
        "name" : "Noelia Pérez Palma"
      },
      {
        "name" : "Antonio Skarmeta"
      },
      {
        "name" : "Prajnamaya Dass"
      },
      {
        "name" : "Stefan Köpsell"
      },
      {
        "name" : "Sonika Ujjwal"
      },
      {
        "name" : "Javier José Díaz Rivera"
      },
      {
        "name" : "Pol Alemany"
      },
      {
        "name" : "Raul Muñoz"
      },
      {
        "name" : "Jafar Mohammadi"
      },
      {
        "name" : "Chaitanya Aggarwal"
      },
      {
        "name" : "Betul Guvenc Paltun"
      },
      {
        "name" : "Ferhat Karakoc"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10684v1",
    "title" : "Privacy Control in Conversational LLM Platforms: A Walkthrough Study",
    "summary" : "Large language models (LLMs) are increasingly integrated into daily life through conversational interfaces, processing user data via natural language inputs and exhibiting advanced reasoning capabilities, which raises new concerns about user control over privacy. While much research has focused on potential privacy risks, less attention has been paid to the data control mechanisms these platforms provide. This study examines six conversational LLM platforms, analyzing how they define and implement features for users to access, edit, delete, and share data. Our analysis reveals an emerging paradigm of data control in conversational LLM platforms, where user data is generated and derived through interaction itself, natural language enables flexible yet often ambiguous control, and multi-user interactions with shared data raise questions of co-ownership and governance. Based on these findings, we offer practical insights for platform developers, policymakers, and researchers to design more effective and usable privacy controls in LLM-powered conversational interactions.",
    "updated" : "2026-02-11T09:39:19Z",
    "published" : "2026-02-11T09:39:19Z",
    "authors" : [
      {
        "name" : "Zhuoyang Li"
      },
      {
        "name" : "Yanlai Wu"
      },
      {
        "name" : "Yao Li"
      },
      {
        "name" : "Xinning Gui"
      },
      {
        "name" : "Yuhan Luo"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10631v1",
    "title" : "Generative clinical time series models trained on moderate amounts of patient data are privacy preserving",
    "summary" : "Sharing medical data for machine learning model training purposes is often impossible due to the risk of disclosing identifying information about individual patients. Synthetic data produced by generative artificial intelligence (genAI) models trained on real data is often seen as one possible solution to comply with privacy regulations. While powerful genAI models for heterogeneous hospital time series have recently been introduced, such modeling does not guarantee privacy protection, as the generated data may still reveal identifying information about individuals in the models' training cohort. Applying established privacy mechanisms to generative time series models, however, proves challenging as post-hoc data anonymization through k-anonymization or similar techniques is limited, while model-centered privacy mechanisms that implement differential privacy (DP) may lead to unstable training, compromising the utility of generated data. Given these known limitations, privacy audits for generative time series models are currently indispensable regardless of the concrete privacy mechanisms applied to models and/or data. In this work, we use a battery of established privacy attacks to audit state-of-the-art hospital time series models, trained on the public MIMIC-IV dataset, with respect to privacy preservation. Furthermore, the eICU dataset was used to mount a privacy attack against the synthetic data generator trained on the MIMIC-IV dataset. Results show that established privacy attacks are ineffective against generated multivariate clinical time series when synthetic data generators are trained on large enough training datasets. Furthermore, we discuss how the use of existing DP mechanisms for these synthetic data generators would not bring desired improvement in privacy, but only a decrease in utility for machine learning prediction tasks.",
    "updated" : "2026-02-11T08:23:54Z",
    "published" : "2026-02-11T08:23:54Z",
    "authors" : [
      {
        "name" : "Rustam Zhumagambetov"
      },
      {
        "name" : "Niklas Giesa"
      },
      {
        "name" : "Sebastian D. Boie"
      },
      {
        "name" : "Stefan Haufe"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10584v1",
    "title" : "When Gradient Clipping Becomes a Control Mechanism for Differential Privacy in Deep Learning",
    "summary" : "Privacy-preserving training on sensitive data commonly relies on differentially private stochastic optimization with gradient clipping and Gaussian noise. The clipping threshold is a critical control knob: if set too small, systematic over-clipping induces optimization bias; if too large, injected noise dominates updates and degrades accuracy. Existing adaptive clipping methods often depend on per-example gradient norm statistics, adding computational overhead and introducing sensitivity to datasets and architectures. We propose a control-driven clipping strategy that adapts the threshold using a lightweight, weight-only spectral diagnostic computed from model parameters. At periodic probe steps, the method analyzes a designated weight matrix via spectral decomposition and estimates a heavy-tailed spectral indicator associated with training stability. This indicator is smoothed over time and fed into a bounded feedback controller that updates the clipping threshold multiplicatively in the log domain. Because the controller uses only parameters produced during privacy-preserving training, the resulting threshold updates are post-processing and do not increase privacy loss beyond that of the underlying DP optimizer under standard composition accounting.",
    "updated" : "2026-02-11T07:18:25Z",
    "published" : "2026-02-11T07:18:25Z",
    "authors" : [
      {
        "name" : "Mohammad Partohaghighi"
      },
      {
        "name" : "Roummel Marcia"
      },
      {
        "name" : "Bruce J. West"
      },
      {
        "name" : "YangQuan Chen"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10510v1",
    "title" : "Privacy-Utility Tradeoffs in Quantum Information Processing",
    "summary" : "When sensitive information is encoded in data, it is important to ensure the privacy of information when attempting to learn useful information from the data. There is a natural tradeoff whereby increasing privacy requirements may decrease the utility of a learning protocol. In the quantum setting of differential privacy, such tradeoffs between privacy and utility have so far remained largely unexplored. In this work, we study optimal privacy-utility tradeoffs for both generic and application-specific utility metrics when privacy is quantified by $(\\varepsilon,δ)$-quantum local differential privacy. In the generic setting, we focus on optimizing fidelity and trace distance between the original state and the privatized state. We show that the depolarizing mechanism achieves the optimal utility for given privacy requirements. We then study the specific application of learning the expectation of an observable with respect to an input state when only given access to privatized states. We derive a lower bound on the number of samples of privatized data required to achieve a fixed accuracy guarantee with high probability. To prove this result, we employ existing lower bounds on private quantum hypothesis testing, thus showcasing the first operational use of them. We also devise private mechanisms that achieve optimal sample complexity with respect to the privacy parameters and accuracy parameters, demonstrating that utility can be significantly improved for specific tasks in contrast to the generic setting. In addition, we show that the number of samples required to privately learn observable expectation values scales as $Θ((\\varepsilon β)^{-2})$, where $\\varepsilon \\in (0,1)$ is the privacy parameter and $β$ is the accuracy tolerance. We conclude by initiating the study of private classical shadows, which promise useful applications for private learning tasks.",
    "updated" : "2026-02-11T04:21:45Z",
    "published" : "2026-02-11T04:21:45Z",
    "authors" : [
      {
        "name" : "Theshani Nuradha"
      },
      {
        "name" : "Sujeet Bhalerao"
      },
      {
        "name" : "Felix Leditzky"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.IT",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10154v1",
    "title" : "PRISM-XR: Empowering Privacy-Aware XR Collaboration with Multimodal Large Language Models",
    "summary" : "Multimodal Large Language Models (MLLMs) enhance collaboration in Extended Reality (XR) environments by enabling flexible object and animation creation through the combination of natural language and visual inputs. However, visual data captured by XR headsets includes real-world backgrounds that may contain irrelevant or sensitive user information, such as credit cards left on the table or facial identities of other users. Uploading those frames to cloud-based MLLMs poses serious privacy risks, particularly when such data is processed without explicit user consent. Additionally, existing colocation and synchronization mechanisms in commercial XR APIs rely on time-consuming, privacy-invasive environment scanning and struggle to adapt to the highly dynamic nature of MLLM-integrated XR environments. In this paper, we propose PRISM-XR, a novel framework that facilitates multi-user collaboration in XR by providing privacy-aware MLLM integration. PRISM-XR employs intelligent frame preprocessing on the edge server to filter sensitive data and remove irrelevant context before communicating with cloud generative AI models. Additionally, we introduce a lightweight registration process and a fully customizable content-sharing mechanism to enable efficient, accurate, and privacy-preserving content synchronization among users. Our numerical evaluation results indicate that the proposed platform achieves nearly 90% accuracy in fulfilling user requests and less than 0.27 seconds registration time while maintaining spatial inconsistencies of less than 3.5 cm. Furthermore, we conducted an IRB-approved user study with 28 participants, demonstrating that our system could automatically filter highly sensitive objects in over 90% of scenarios while maintaining strong overall usability.",
    "updated" : "2026-02-09T21:28:02Z",
    "published" : "2026-02-09T21:28:02Z",
    "authors" : [
      {
        "name" : "Jiangong Chen"
      },
      {
        "name" : "Mingyu Zhu"
      },
      {
        "name" : "Bin Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10142v1",
    "title" : "Privacy by Voice: Modeling Youth Privacy-Protective Behavior in Smart Voice Assistants",
    "summary" : "Smart Voice Assistants (SVAs) are deeply embedded in the lives of youth, yet the mechanisms driving the privacy-protective behaviors among young users remain poorly understood. This study investigates how Canadian youth (aged 16-24) negotiate privacy with SVAs by developing and testing a structural model grounded in five key constructs: perceived privacy risks (PPR), perceived benefits (PPBf), algorithmic transparency and trust (ATT), privacy self-efficacy (PSE), and privacy-protective behaviors (PPB). A cross-sectional survey of N=469 youth was analyzed using partial least squares structural equation modeling. Results reveal that PSE is the strongest predictor of PPB, while the effect of ATT on PPB is fully mediated by PSE. This identifies a critical efficacy gap, where youth's confidence must first be built up for them to act. The model confirms that PPBf directly discourages protective action, yet also indirectly fosters it by slightly boosting self-efficacy. These findings empirically validate and extend earlier qualitative work, quantifying how policy overload and hidden controls erode the self-efficacy necessary for protective action. This study contributes an evidence-based pathway from perception to action and translates it into design imperatives that empower young digital citizens without sacrificing the utility of SVAs.",
    "updated" : "2026-02-09T05:56:51Z",
    "published" : "2026-02-09T05:56:51Z",
    "authors" : [
      {
        "name" : "Molly Campbell"
      },
      {
        "name" : "Ajay Kumar Shrestha"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10139v1",
    "title" : "Anonymization-Enhanced Privacy Protection for Mobile GUI Agents: Available but Invisible",
    "summary" : "Mobile Graphical User Interface (GUI) agents have demonstrated strong capabilities in automating complex smartphone tasks by leveraging multimodal large language models (MLLMs) and system-level control interfaces. However, this paradigm introduces significant privacy risks, as agents typically capture and process entire screen contents, thereby exposing sensitive personal data such as phone numbers, addresses, messages, and financial information. Existing defenses either reduce UI exposure, obfuscate only task-irrelevant content, or rely on user authorization, but none can protect task-critical sensitive information while preserving seamless agent usability.\n  We propose an anonymization-based privacy protection framework that enforces the principle of available-but-invisible access to sensitive data: sensitive information remains usable for task execution but is never directly visible to the cloud-based agent. Our system detects sensitive UI content using a PII-aware recognition model and replaces it with deterministic, type-preserving placeholders (e.g., PHONE_NUMBER#a1b2c) that retain semantic categories while removing identifying details. A layered architecture comprising a PII Detector, UI Transformer, Secure Interaction Proxy, and Privacy Gatekeeper ensures consistent anonymization across user instructions, XML hierarchies, and screenshots, mediates all agent actions over anonymized interfaces, and supports narrowly scoped local computations when reasoning over raw values is necessary.\n  Extensive experiments on the AndroidLab and PrivScreen benchmarks show that our framework substantially reduces privacy leakage across multiple models while incurring only modest utility degradation, achieving the best observed privacy-utility trade-off among existing methods.",
    "updated" : "2026-02-08T15:50:04Z",
    "published" : "2026-02-08T15:50:04Z",
    "authors" : [
      {
        "name" : "Lepeng Zhao"
      },
      {
        "name" : "Zhenhua Zou"
      },
      {
        "name" : "Shuo Li"
      },
      {
        "name" : "Zhuotao Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.05016v2",
    "title" : "From Fragmentation to Integration: Exploring the Design Space of AI Agents for Human-as-the-Unit Privacy Management",
    "summary" : "Managing one's digital footprint is overwhelming, as it spans multiple platforms and involves countless context-dependent decisions. Recent advances in agentic AI offer ways forward by enabling holistic, contextual privacy-enhancing solutions. Building on this potential, we adopted a ''human-as-the-unit'' perspective and investigated users' cross-context privacy challenges through 12 semi-structured interviews. Results reveal that people rely on ad hoc manual strategies while lacking comprehensive privacy controls, highlighting nine privacy-management challenges across applications, temporal contexts, and relationships. To explore solutions, we generated nine AI agent concepts and evaluated them via a speed-dating survey with 116 US participants. The three highest-ranked concepts were all post-sharing management tools with half or full agent autonomy, with users expressing greater trust in AI accuracy than in their own efforts. Our findings highlight a promising design space where users see AI agents bridging the fragments in privacy management, particularly through automated, comprehensive post-sharing remediation of users' digital footprints.",
    "updated" : "2026-02-11T03:36:49Z",
    "published" : "2026-02-04T20:12:37Z",
    "authors" : [
      {
        "name" : "Eryue Xu"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.12009v1",
    "title" : "On the Sensitivity of Firing Rate-Based Federated Spiking Neural Networks to Differential Privacy",
    "summary" : "Federated Neuromorphic Learning (FNL) enables energy-efficient and privacy-preserving learning on devices without centralizing data. However, real-world deployments require additional privacy mechanisms that can significantly alter training signals. This paper analyzes how Differential Privacy (DP) mechanisms, specifically gradient clipping and noise injection, perturb firing-rate statistics in Spiking Neural Networks (SNNs) and how these perturbations are propagated to rate-based FNL coordination. On a speech recognition task under non-IID settings, ablations across privacy budgets and clipping bounds reveal systematic rate shifts, attenuated aggregation, and ranking instability during client selection. Moreover, we relate these shifts to sparsity and memory indicators. Our findings provide actionable guidance for privacy-preserving FNL, specifically regarding the balance between privacy strength and rate-dependent coordination.",
    "updated" : "2026-02-12T14:40:25Z",
    "published" : "2026-02-12T14:40:25Z",
    "authors" : [
      {
        "name" : "Luiz Pereira"
      },
      {
        "name" : "Mirko Perkusich"
      },
      {
        "name" : "Dalton Valadares"
      },
      {
        "name" : "Kyller Gorgônio"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.11510v1",
    "title" : "AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems",
    "summary" : "Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five models and four domains, the pattern C2 > C1 holds consistently, confirming that inter-agent communication is the primary vulnerability. These findings underscore the need for coordination frameworks that incorporate internal-channel privacy protections and enforce privacy controls on inter-agent communication.",
    "updated" : "2026-02-12T03:10:44Z",
    "published" : "2026-02-12T03:10:44Z",
    "authors" : [
      {
        "name" : "Faouzi El Yagoubi"
      },
      {
        "name" : "Ranwa Al Mallah"
      },
      {
        "name" : "Godwin Badu-Marfo"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.12749v1",
    "title" : "SoK: Understanding the Pedagogical, Health, Ethical, and Privacy Challenges of Extended Reality in Early Childhood Education",
    "summary" : "Extended Reality (XR) combines dense sensing, real-time rendering, and close-range interaction, making its use in early childhood education both promising and high risk. To investigate this, we conduct a Systematization of Knowledge (SoK) of 111 peer-reviewed studies with children aged 3-8, quantifying how technical, pedagogical, health, privacy, and equity challenges arise in practice. We found that AR dominates the landscape (73%), focusing primarily on tablets or phones, while VR remains uncommon and typically relies on head mounted displays (HMDs). We integrate these quantitative patterns into a joint risk and attention matrix and an Augmented Human Development (AHD) model that link XR pipeline properties to cognitive load, sensory conflict, and access inequity. Finally, implementing a seven dimension coding scheme on a 0 - 2 scale, we obtain mean scholarly attention scores of 1.56 for pedagogy, 1.04 for privacy (primarily procedural consent), 0.96 for technical reliability, 0.92 for accessibility in low resource contexts, 0.81 for medical and health issues, 0.52 for accessibility for disabilities, and 0.14 for data security practices. This indicates that pedagogy receives the most systematic scrutiny, while data access practices is largely overlooked. We conclude by offering a roadmap for Child-Centered XR that helps HCI researchers and educators move beyond novelty to design systems that are developmentally aligned, secure by default, and accessible to diverse learners.",
    "updated" : "2026-02-13T09:25:56Z",
    "published" : "2026-02-13T09:25:56Z",
    "authors" : [
      {
        "name" : "Supriya Khadka"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.15028v1",
    "title" : "Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization",
    "summary" : "Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench",
    "updated" : "2026-02-16T18:59:42Z",
    "published" : "2026-02-16T18:59:42Z",
    "authors" : [
      {
        "name" : "Shangding Gu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.13941v1",
    "title" : "Avoiding Social Judgment, Seeking Privacy: Investigating why Mothers Shift from Facebook Groups to Large Language Models",
    "summary" : "Social media platforms, especially Facebook parenting groups, have long been used as informal support networks for mothers seeking advice and reassurance. However, growing concerns about social judgment, privacy exposure, and unreliable information are changing how mothers seek help. This exploratory mixed-method study examines why mothers are moving from Facebook parenting groups to large language models such as ChatGPT and Gemini. We conducted a cross-sectional online survey of 109 mothers. Results show that 41.3% of participants avoided Facebook parenting groups because they expected judgment from others. This difference was statistically significant across location and family structure. Mothers living in their home country and those in joint families were more likely to avoid Facebook groups. Qualitative findings revealed three themes: social judgment and exposure, LLMs as safe and private spaces, and quick and structured support. Participants described LLMs as immediate, emotionally safe, and reliable alternatives that reduce social risk when asking for help. Rather than replacing human support, LLMs appear to fill emotional and practical gaps within existing support systems. These findings show a change in maternal digital support and highlight the need to design LLM systems that support both information and emotional safety.",
    "updated" : "2026-02-15T00:37:21Z",
    "published" : "2026-02-15T00:37:21Z",
    "authors" : [
      {
        "name" : "Shayla Sharmin"
      },
      {
        "name" : "Sadia Afrin"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.13840v1",
    "title" : "PrivAct: Internalizing Contextual Privacy Preservation via Multi-Agent Preference Training",
    "summary" : "Large language model (LLM) agents are increasingly deployed in personalized tasks involving sensitive, context-dependent information, where privacy violations may arise in agents' action due to the implicitness of contextual privacy. Existing approaches rely on external, inference-time interventions which are brittle, scenario-specific, and may expand the privacy attack surface. We propose PrivAct, a contextual privacy-aware multi-agent learning framework that internalizes contextual privacy preservation directly into models' generation behavior for privacy-compliant agentic actions. By embedding privacy preferences into each agent, PrivAct enhances system-wide contextual integrity while achieving a more favorable privacy-helpfulness tradeoff. Experiments across multiple LLM backbones and benchmarks demonstrate consistent improvements in contextual privacy preservation, reducing leakage rates by up to 12.32% while maintaining comparable helpfulness, as well as zero-shot generalization and robustness across diverse multi-agent topologies. Code is available at https://github.com/chengyh23/PrivAct.",
    "updated" : "2026-02-14T18:07:51Z",
    "published" : "2026-02-14T18:07:51Z",
    "authors" : [
      {
        "name" : "Yuhan Cheng"
      },
      {
        "name" : "Hancheng Ye"
      },
      {
        "name" : "Hai Helen Li"
      },
      {
        "name" : "Jingwei Sun"
      },
      {
        "name" : "Yiran Chen"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.13555v1",
    "title" : "Privacy-Concealing Cooperative Perception for BEV Scene Segmentation",
    "summary" : "Cooperative perception systems for autonomous driving aim to overcome the limited perception range of a single vehicle by communicating with adjacent agents to share sensing information. While this improves perception performance, these systems also face a significant privacy-leakage issue, as sensitive visual content can potentially be reconstructed from the shared data. In this paper, we propose a novel Privacy-Concealing Cooperation (PCC) framework for Bird's Eye View (BEV) semantic segmentation. Based on commonly shared BEV features, we design a hiding network to prevent an image reconstruction network from recovering the input images from the shared features. An adversarial learning mechanism is employed to train the network, where the hiding network works to conceal the visual clues in the BEV features while the reconstruction network attempts to uncover these clues. To maintain segmentation performance, the perception network is integrated with the hiding network and optimized end-to-end. The experimental results demonstrate that the proposed PCC framework effectively degrades the quality of the reconstructed images with minimal impact on segmentation performance, providing privacy protection for cooperating vehicles. The source code will be made publicly available upon publication.",
    "updated" : "2026-02-14T02:11:29Z",
    "published" : "2026-02-14T02:11:29Z",
    "authors" : [
      {
        "name" : "Song Wang"
      },
      {
        "name" : "Lingling Li"
      },
      {
        "name" : "Marcus Santos"
      },
      {
        "name" : "Guanghui Wang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2602.10139v2",
    "title" : "Anonymization-Enhanced Privacy Protection for Mobile GUI Agents: Available but Invisible",
    "summary" : "Mobile Graphical User Interface (GUI) agents have demonstrated strong capabilities in automating complex smartphone tasks by leveraging multimodal large language models (MLLMs) and system-level control interfaces. However, this paradigm introduces significant privacy risks, as agents typically capture and process entire screen contents, thereby exposing sensitive personal data such as phone numbers, addresses, messages, and financial information. Existing defenses either reduce UI exposure, obfuscate only task-irrelevant content, or rely on user authorization, but none can protect task-critical sensitive information while preserving seamless agent usability.\n  We propose an anonymization-based privacy protection framework that enforces the principle of available-but-invisible access to sensitive data: sensitive information remains usable for task execution but is never directly visible to the cloud-based agent. Our system detects sensitive UI content using a PII-aware recognition model and replaces it with deterministic, type-preserving placeholders (e.g., PHONE_NUMBER#a1b2c) that retain semantic categories while removing identifying details. A layered architecture comprising a PII Detector, UI Transformer, Secure Interaction Proxy, and Privacy Gatekeeper ensures consistent anonymization across user instructions, XML hierarchies, and screenshots, mediates all agent actions over anonymized interfaces, and supports narrowly scoped local computations when reasoning over raw values is necessary.\n  Extensive experiments on the AndroidLab and PrivScreen benchmarks show that our framework substantially reduces privacy leakage across multiple models while incurring only modest utility degradation, achieving the best observed privacy-utility trade-off among existing methods. Code available at: https://github.com/one-step-beh1nd/gui_privacy_protection",
    "updated" : "2026-02-14T04:21:20Z",
    "published" : "2026-02-08T15:50:04Z",
    "authors" : [
      {
        "name" : "Lepeng Zhao"
      },
      {
        "name" : "Zhenhua Zou"
      },
      {
        "name" : "Shuo Li"
      },
      {
        "name" : "Zhuotao Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  }
]