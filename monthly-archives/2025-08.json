[
  {
    "id" : "http://arxiv.org/abs/2508.00321v1",
    "title" : "Evaluating the Efficacy of Large Language Models for Generating\n  Fine-Grained Visual Privacy Policies in Homes",
    "summary" : "The proliferation of visual sensors in smart home environments, particularly\nthrough wearable devices like smart glasses, introduces profound privacy\nchallenges. Existing privacy controls are often static and coarse-grained,\nfailing to accommodate the dynamic and socially nuanced nature of home\nenvironments. This paper investigates the viability of using Large Language\nModels (LLMs) as the core of a dynamic and adaptive privacy policy engine. We\npropose a conceptual framework where visual data is classified using a\nmulti-dimensional schema that considers data sensitivity, spatial context, and\nsocial presence. An LLM then reasons over this contextual information to\nenforce fine-grained privacy rules, such as selective object obfuscation, in\nreal-time. Through a comparative evaluation of state-of-the-art Vision Language\nModels (including GPT-4o and the Qwen-VL series) in simulated home settings ,\nour findings show the feasibility of this approach. The LLM-based engine\nachieved a top machine-evaluated appropriateness score of 3.99 out of 5, and\nthe policies generated by the models received a top human-evaluated score of\n4.00 out of 5.",
    "updated" : "2025-08-01T05:11:29Z",
    "published" : "2025-08-01T05:11:29Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Ying Ma"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.00287v1",
    "title" : "Privacy-Preserving Driver Drowsiness Detection with Spatial\n  Self-Attention and Federated Learning",
    "summary" : "Driver drowsiness is one of the main causes of road accidents and is\nrecognized as a leading contributor to traffic-related fatalities. However,\ndetecting drowsiness accurately remains a challenging task, especially in\nreal-world settings where facial data from different individuals is\ndecentralized and highly diverse. In this paper, we propose a novel framework\nfor drowsiness detection that is designed to work effectively with\nheterogeneous and decentralized data. Our approach develops a new Spatial\nSelf-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)\nnetwork to better extract key facial features and improve detection\nperformance. To support federated learning, we employ a Gradient Similarity\nComparison (GSC) that selects the most relevant trained models from different\noperators before aggregation. This improves the accuracy and robustness of the\nglobal model while preserving user privacy. We also develop a customized tool\nthat automatically processes video data by extracting frames, detecting and\ncropping faces, and applying data augmentation techniques such as rotation,\nflipping, brightness adjustment, and zooming. Experimental results show that\nour framework achieves a detection accuracy of 89.9% in the federated learning\nsettings, outperforming existing methods under various deployment scenarios.\nThe results demonstrate the effectiveness of our approach in handling\nreal-world data variability and highlight its potential for deployment in\nintelligent transportation systems to enhance road safety through early and\nreliable drowsiness detection.",
    "updated" : "2025-08-01T03:12:01Z",
    "published" : "2025-08-01T03:12:01Z",
    "authors" : [
      {
        "name" : "Tran Viet Khoa"
      },
      {
        "name" : "Do Hai Son"
      },
      {
        "name" : "Mohammad Abu Alsheikh"
      },
      {
        "name" : "Yibeltal F Alem"
      },
      {
        "name" : "Dinh Thai Hoang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.02551v1",
    "title" : "PrivAR: Real-Time Privacy Protection for Location-Based Augmented\n  Reality Applications",
    "summary" : "Location-based augmented reality (LB-AR) applications, such as Pok\\'emon Go,\nstream sub-second GPS updates to deliver responsive and immersive user\nexperiences. However, this high-frequency location reporting introduces serious\nprivacy risks. Protecting privacy in LB-AR is significantly more challenging\nthan in traditional location-based services (LBS), as it demands real-time\nlocation protection with strong per-location and trajectory-level privacy\nguaranteed while maintaining low latency and high quality of service (QoS).\nExisting methods fail to meet these combined demands.\n  To fill the gap, we present PrivAR, the first client-side privacy framework\nfor real-time LB-AR. PrivAR introduces two lightweight mechanisms: (i) Planar\nStaircase Mechanism (PSM) which designs a staircase-shaped distribution to\ngenerate noisy location with strong per-location privacy and low expected\nerror; and (ii) Thresholded Reporting with PSM (TR-PSM), a selective scheme\nthat releases a noisy location update only when a displacement exceeds a\nprivate threshold, enabling many-to-one mappings for enhanced trace-level\nprivacy while preserving high QoS. We present theoretical analysis, extensive\nexperiments on two public datasets and our proprietary GeoTrace dataset, and\nvalidate PrivAR on a Pok\\'emon-Go-style prototype. Results show PrivAR improves\nQoS (Gamescore) by up to 50%, while increasing attacker error by 1.8x over\nbaseline with an additional 0.06 milliseconds runtime overhead.",
    "updated" : "2025-08-04T16:02:10Z",
    "published" : "2025-08-04T16:02:10Z",
    "authors" : [
      {
        "name" : "Shafizur Rahman Seeam"
      },
      {
        "name" : "Ye Zheng"
      },
      {
        "name" : "Zhengxiong Li"
      },
      {
        "name" : "Yidan Hu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.02483v1",
    "title" : "Revisiting the Privacy of Low-Frequency Speech Signals: Exploring\n  Resampling Methods, Evaluation Scenarios, and Speaker Characteristics",
    "summary" : "While audio recordings in real life provide insights into social dynamics and\nconversational behavior, they also raise concerns about the privacy of\npersonal, sensitive data. This article explores the effectiveness of\nrestricting recordings to low-frequency audio to protect spoken content. For\nresampling the audio signals to different sampling rates, we compare the effect\nof employing anti-aliasing filtering. Privacy enhancement is measured by an\nincreased word error rate of automatic speech recognition models. The impact on\nutility performance is measured with voice activity detection models. Our\nexperimental results show that for clean recordings, models trained with a\nsampling rate of up to 800 Hz transcribe the majority of words correctly. For\nboth models, we analyzed the impact of the speaker's sex and pitch, and we\ndemonstrated that missing anti-aliasing filters more strongly compromise speech\nprivacy.",
    "updated" : "2025-08-04T14:53:56Z",
    "published" : "2025-08-04T14:53:56Z",
    "authors" : [
      {
        "name" : "Jule Pohlhausen"
      },
      {
        "name" : "Jörg Bitzer"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.02461v1",
    "title" : "Experimental Evaluation of Post-Quantum Homomorphic Encryption for\n  Privacy-Preserving V2X Communication",
    "summary" : "Intelligent Transportation Systems (ITS) fundamentally rely on\nvehicle-generated data for applications such as congestion monitoring and route\noptimization, making the preservation of user privacy a critical challenge.\nHomomorphic Encryption (HE) offers a promising solution by enabling computation\non encrypted data without revealing underlying content. This study presents the\nfirst real-world experimental evaluation of three post-quantum secure HE\nschemes, i.e., Brakerski-Fan-Vercauteren (BFV), Brakerski-Gentry-Vaikuntanathan\n(BGV), and Cheon-Kim-Kim-Song (CKKS), for vehicular communication scenarios.\nTwo representative privacy-preserving use cases are considered: encrypted\nvehicle counting and average speed aggregation. Experiments are conducted over\nboth Wi-Fi and Ethernet to assess performance under wireless and wired\nvehicle-to-everything (V2X) settings. Results show that BFV and BGV are\nsuitable for latency-tolerant applications such as intersection monitoring and\nregional traffic analysis, with total end-to-end latencies under 10 seconds.\nWhile CKKS experiences higher overhead, it remains viable for periodic\nencrypted aggregation of numerical data. The experimental results demonstrate\nthat HE can be feasibly deployed in ITS environments under 128-bit post-quantum\nsecurity, provided that scheme-specific latency constraints are considered.\nThis reinforces its potential to serve as a foundational tool for secure and\nprivacy-preserving V2X data processing.",
    "updated" : "2025-08-04T14:28:19Z",
    "published" : "2025-08-04T14:28:19Z",
    "authors" : [
      {
        "name" : "Abdullah Al Mamun"
      },
      {
        "name" : "Kyle Yates"
      },
      {
        "name" : "Antsa Rakotondrafara"
      },
      {
        "name" : "Mashrur Chowdhury"
      },
      {
        "name" : "Ryann Cartor"
      },
      {
        "name" : "Shuhong Gao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.02034v1",
    "title" : "Protego: User-Centric Pose-Invariant Privacy Protection Against Face\n  Recognition-Induced Digital Footprint Exposure",
    "summary" : "Face recognition (FR) technologies are increasingly used to power large-scale\nimage retrieval systems, raising serious privacy concerns. Services like\nClearview AI and PimEyes allow anyone to upload a facial photo and retrieve a\nlarge amount of online content associated with that person. This not only\nenables identity inference but also exposes their digital footprint, such as\nsocial media activity, private photos, and news reports, often without their\nconsent. In response to this emerging threat, we propose Protego, a\nuser-centric privacy protection method that safeguards facial images from such\nretrieval-based privacy intrusions. Protego encapsulates a user's 3D facial\nsignatures into a pose-invariant 2D representation, which is dynamically\ndeformed into a natural-looking 3D mask tailored to the pose and expression of\nany facial image of the user, and applied prior to online sharing. Motivated by\na critical limitation of existing methods, Protego amplifies the sensitivity of\nFR models so that protected images cannot be matched even among themselves.\nExperiments show that Protego significantly reduces retrieval accuracy across a\nwide range of black-box FR models and performs at least 2x better than existing\nmethods. It also offers unprecedented visual coherence, particularly in video\nsettings where consistency and natural appearance are essential. Overall,\nProtego contributes to the fight against the misuse of FR for mass surveillance\nand unsolicited identity tracing.",
    "updated" : "2025-08-04T04:03:01Z",
    "published" : "2025-08-04T04:03:01Z",
    "authors" : [
      {
        "name" : "Ziling Wang"
      },
      {
        "name" : "Shuya Yang"
      },
      {
        "name" : "Jialin Lu"
      },
      {
        "name" : "Ka-Ho Chow"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01960v1",
    "title" : "Non-Verbal Vocalisations and their Challenges: Emotion, Privacy,\n  Sparseness, and Real Life",
    "summary" : "Non-Verbal Vocalisations (NVVs) are short `non-word' utterances without\nproper linguistic (semantic) meaning but conveying connotations -- be this\nemotions/affects or other paralinguistic information. We start this\ncontribution with a historic sketch: how they were addressed in psychology and\nlinguistics in the last two centuries, how they were neglected later on, and\nhow they came to the fore with the advent of emotion research. We then give an\noverview of types of NVVs (formal aspects) and functions of NVVs, exemplified\nwith the typical NVV \\textit{ah}. Interesting as they are, NVVs come, however,\nwith a bunch of challenges that should be accounted for: Privacy and general\nethical considerations prevent them of being recorded in real-life (private)\nscenarios to a sufficient extent. Isolated, prompted (acted) exemplars do not\nnecessarily model NVVs in context; yet, this is the preferred strategy so far\nwhen modelling NVVs, especially in AI. To overcome these problems, we argue in\nfavour of corpus-based approaches. This guarantees a more realistic modelling;\nhowever, we are still faced with privacy and sparse data problems.",
    "updated" : "2025-08-03T23:59:43Z",
    "published" : "2025-08-03T23:59:43Z",
    "authors" : [
      {
        "name" : "Anton Batliner"
      },
      {
        "name" : "Shahin Amiriparian"
      },
      {
        "name" : "Björn W. Schuller"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CL",
      "eess.AS",
      "68T10 (Primary) 68T45 (Secondary)",
      "I.2.7"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01898v1",
    "title" : "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution",
    "summary" : "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.",
    "updated" : "2025-08-03T19:16:40Z",
    "published" : "2025-08-03T19:16:40Z",
    "authors" : [
      {
        "name" : "Yijing Zhang"
      },
      {
        "name" : "Md-Ferdous Pervej"
      },
      {
        "name" : "Andreas F. Molisch"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01798v1",
    "title" : "A Survey on Privacy-Preserving Computing in the Automotive Domain",
    "summary" : "As vehicles become increasingly connected and autonomous, they accumulate and\nmanage various personal data, thereby presenting a key challenge in preserving\nprivacy during data sharing and processing. This survey reviews applications of\nSecure Multi-Party Computation (MPC) and Homomorphic Encryption (HE) that\naddress these privacy concerns in the automotive domain. First, we identify the\nscope of privacy-sensitive use cases for these technologies, by surveying\nexisting works that address privacy issues in different automotive contexts,\nsuch as location-based services, mobility infrastructures, traffic management,\netc. Then, we review recent works that employ MPC and HE as solutions for these\nuse cases in detail. Our survey highlights the applicability of these\nprivacy-preserving technologies in the automotive context, while also\nidentifying challenges and gaps in the current research landscape. This work\naims to provide a clear and comprehensive overview of this emerging field and\nto encourage further research in this domain.",
    "updated" : "2025-08-03T15:23:41Z",
    "published" : "2025-08-03T15:23:41Z",
    "authors" : [
      {
        "name" : "Nergiz Yuca"
      },
      {
        "name" : "Nikolay Matyunin"
      },
      {
        "name" : "Ektor Arzoglou"
      },
      {
        "name" : "Nikolaos Athanasios Anagnostopoulos"
      },
      {
        "name" : "Stefan Katzenbeisser"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01749v1",
    "title" : "Improving Noise Efficiency in Privacy-preserving Dataset Distillation",
    "summary" : "Modern machine learning models heavily rely on large datasets that often\ninclude sensitive and private information, raising serious privacy concerns.\nDifferentially private (DP) data generation offers a solution by creating\nsynthetic datasets that limit the leakage of private information within a\npredefined privacy budget; however, it requires a substantial amount of data to\nachieve performance comparable to models trained on the original data. To\nmitigate the significant expense incurred with synthetic data generation,\nDataset Distillation (DD) stands out for its remarkable training and storage\nefficiency. This efficiency is particularly advantageous when integrated with\nDP mechanisms, curating compact yet informative synthetic datasets without\ncompromising privacy. However, current state-of-the-art private DD methods\nsuffer from a synchronized sampling-optimization process and the dependency on\nnoisy training signals from randomly initialized networks. This results in the\ninefficient utilization of private information due to the addition of excessive\nnoise. To address these issues, we introduce a novel framework that decouples\nsampling from optimization for better convergence and improves signal quality\nby mitigating the impact of DP noise through matching in an informative\nsubspace. On CIFAR-10, our method achieves a \\textbf{10.0\\%} improvement with\n50 images per class and \\textbf{8.3\\%} increase with just \\textbf{one-fifth}\nthe distilled set size of previous state-of-the-art methods, demonstrating\nsignificant potential to advance privacy-preserving DD.",
    "updated" : "2025-08-03T13:15:52Z",
    "published" : "2025-08-03T13:15:52Z",
    "authors" : [
      {
        "name" : "Runkai Zheng"
      },
      {
        "name" : "Vishnu Asutosh Dasu"
      },
      {
        "name" : "Yinong Oliver Wang"
      },
      {
        "name" : "Haohan Wang"
      },
      {
        "name" : "Fernando De la Torre"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01636v1",
    "title" : "Privacy-Preserving Inference for Quantized BERT Models",
    "summary" : "With the increasing deployment of generative machine learning models in\nprivacy-sensitive domains such as healthcare and personalized services,\nensuring secure inference has become a critical challenge. Secure multi-party\ncomputation (MPC) enables privacy-preserving model inference but suffers from\nhigh communication and computation overhead. The main bottleneck lies in the\nexpensive secure evaluation of floating-point operations. Quantization offers a\npromising solution by converting floating-point operations into lower-precision\ninteger computations, significantly reducing overhead. However, existing\nMPC-based quantized inference methods either rely on public quantization\nparameters-posing privacy risks-or suffer from inefficiencies, particularly in\nhandling nonlinear functions such as activations and softmax. In this work, we\npropose a fine-grained, layer-wise quantization scheme and support 1-bit weight\nfully connected layers in a secure setting. We design a multi-input lookup\ntable protocol to evaluate softmax efficiently and securely. Furthermore, we\nuse dual secret sharing schemes and perform precision conversions via lookup\ntables, eliminating truncation overhead entirely. Experimental evaluation on\nBERT-base models demonstrates that our approach achieves up to $8\\times$\nspeedup compared to Lu \\emph{et al}. (NDSS 25), $9\\times$ speedup compared to\nGupta \\emph{et al}. (PETS 24) and $22 \\times$ speedup compared to Knott\n\\emph{et al}. (NeurIPS 21).",
    "updated" : "2025-08-03T07:52:08Z",
    "published" : "2025-08-03T07:52:08Z",
    "authors" : [
      {
        "name" : "Tianpei Lu"
      },
      {
        "name" : "Bingsheng Zhang"
      },
      {
        "name" : "Lekun Peng"
      },
      {
        "name" : "Bowen Zheng"
      },
      {
        "name" : "Lichun Li"
      },
      {
        "name" : "Kui Ren"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01587v1",
    "title" : "Lifelong Person Re-identification via Privacy-Preserving Data Replay",
    "summary" : "Lifelong person re-identification (LReID) aims to incrementally accumulate\nknowledge across a sequence of tasks under domain shifts. Recently,\nreplay-based methods have demonstrated strong effectiveness in LReID by\nrehearsing past samples stored in an auxiliary memory. However, storing\nhistorical exemplars raises concerns over data privacy. To avoid this,\nexemplar-free approaches attempt to match the distribution of past data without\nstoring raw samples. Despite being privacy-friendly, these methods often suffer\nfrom performance degradation due to the forgetting of specific past knowledge\nrepresentations. To this end, we propose to condense information from\nsequential data into the pixel space in the replay memory, enabling\nPrivacy-Preserving Replay (Pr^2R). More specifically, by distilling the\ntraining characteristics of multiple real images into a single image, the\ncondensed samples undergo pixel-level changes. This not only protects the\nprivacy of the original data but also makes the replay samples more\nrepresentative for sequential tasks. During the style replay phase, we align\nthe current domain to the previous one while simultaneously adapting the replay\nsamples to match the style of the current domain. This dual-alignment strategy\neffectively mitigates both class-incremental challenges and forgetting caused\nby domain shifts. Extensive experiments on multiple benchmarks show that the\nproposed method significantly improves replay effectiveness while preserving\ndata privacy. Specifically, Pr^2R achieves 4% and 6% higher accuracy on\nsequential tasks compared to the current state-of-the-art and other\nreplay-based methods, respectively.",
    "updated" : "2025-08-03T05:00:19Z",
    "published" : "2025-08-03T05:00:19Z",
    "authors" : [
      {
        "name" : "Mingyu Wang"
      },
      {
        "name" : "Haojie Liu"
      },
      {
        "name" : "Zhiyong Li"
      },
      {
        "name" : "Wei Jiang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01207v1",
    "title" : "Showcasing standards and approaches for cybersecurity, safety, and\n  privacy issues in connected and autonomous vehicles",
    "summary" : "In the automotive industry there is a need to handle broad quality\ndeficiencies, eg, performance, maintainability, cybersecurity, safety, and\nprivacy, to mention a few. The idea is to prevent these issues from reaching\nend-users, ie, road users and inadvertently, pedestrians, aiming to potentially\nreduce accidents, and allow safe operation in dynamic attack surfaces, for the\nbenefit of a host of stakeholders. This paper aims to bridge cybersecurity,\nsafety, and privacy concerns in Connected and Autonomous Vehicles (CAV) with\nrespect to Risk Assessment (RA) and Threat Modelling (TM) altogether.\nPractitioners know the vast literature on this topic given the sheer number of\nrecommendations, standards, best practices, and existing approaches, at times\nimpairing projects and fostering valuable and actionable threat analysis. In\nthis paper we collate key outcomes by highlighting latest standards and\napproaches in RA and TM research to tackle complex attack surfaces as the ones\nposed by automotive settings. We aim to provide the community with a list of\napproaches to align expectations with stakeholders when deciding where and when\nto focus threat related analysis in automotive solutions.",
    "updated" : "2025-08-02T05:45:50Z",
    "published" : "2025-08-02T05:45:50Z",
    "authors" : [
      {
        "name" : "Ricardo M. Czekster"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01140v1",
    "title" : "Open Data Sharing in Clinical Research and Participants Privacy:\n  Challenges and Opportunities in the Era of Artificial Intelligence",
    "summary" : "Sharing clinical research data is key for increasing the pace of medical\ndiscoveries that improve human health. However, concern about study\nparticipants' privacy, confidentiality, and safety is a major factor that\ndeters researchers from openly sharing clinical data, even after\ndeidentification. This concern is further heightened by the evolution of\nartificial intelligence (AI) approaches that pose an ever-increasing threat to\nthe reidentification of study participants. Here, we discuss the challenges AI\napproaches create that blur the lines between identifiable and non-identifiable\ndata. We present a concept of pseudo-reidentification, and discuss how these\nchallenges provide opportunities for rethinking open data sharing practices in\nclinical research. We highlight the novel open data sharing approach we have\nestablished as part of the Artificial Intelligence Ready and Exploratory Atlas\nfor Diabetes Insights project, one of the four Data Generation Projects funded\nby the National Institutes of Health Common Fund's Bridge2AI Program.",
    "updated" : "2025-08-02T01:46:59Z",
    "published" : "2025-08-02T01:46:59Z",
    "authors" : [
      {
        "name" : "Shahin Hallaj"
      },
      {
        "name" : "Anna Heinke"
      },
      {
        "name" : "Fritz Gerald P. Kalaw"
      },
      {
        "name" : "Nayoon Gim"
      },
      {
        "name" : "Marian Blazes"
      },
      {
        "name" : "Julia Owen"
      },
      {
        "name" : "Eamon Dysinger"
      },
      {
        "name" : "Erik S. Benton"
      },
      {
        "name" : "Benjamin A. Cordier"
      },
      {
        "name" : "Nicholas G. Evans"
      },
      {
        "name" : "Jennifer Li-Pook-Than"
      },
      {
        "name" : "Michael P. Snyder"
      },
      {
        "name" : "Camille Nebeker"
      },
      {
        "name" : "Linda M. Zangwill"
      },
      {
        "name" : "Sally L. Baxter"
      },
      {
        "name" : "Shannon McWeeney"
      },
      {
        "name" : "Cecilia S. Lee"
      },
      {
        "name" : "Aaron Y. Lee"
      },
      {
        "name" : "Bhavesh Patel"
      }
    ],
    "categories" : [
      "q-bio.OT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03413v1",
    "title" : "Smart Car Privacy: Survey of Attacks and Privacy Issues",
    "summary" : "Automobiles are becoming increasingly important in our day to day life.\nModern automobiles are highly computerized and hence potentially vulnerable to\nattack. Providing many wireless connectivity for vehicles enables a bridge\nbetween vehicles and their external environments. Such a connected vehicle\nsolution is expected to be the next frontier for automotive revolution and the\nkey to the evolution to next generation intelligent transportation systems.\nVehicular Ad hoc Networks (VANETs) are emerging mobile ad hoc network\ntechnologies incorporating mobile routing protocols for inter-vehicle data\ncommunications to support intelligent transportation systems. Thus security and\nprivacy are the major concerns in VANETs due to the mobility of the vehicles.\nThus designing security mechanisms to remove adversaries from the network\nremarkably important in VANETs.\n  This paper provides an overview of various vehicular network architectures.\nThe evolution of security in modern vehicles. Various security and privacy\nattacks in VANETs with their defending mechanisms with examples and classify\nthese mechanisms. It also provides an overview of various privacy implication\nthat a vehicular network possess.",
    "updated" : "2025-08-05T12:59:17Z",
    "published" : "2025-08-05T12:59:17Z",
    "authors" : [
      {
        "name" : "Akshay Madhav Deshmukh"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03209v1",
    "title" : "GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models\n  via Adversarial Perturbations",
    "summary" : "Vision-Language Models (VLMs) such as GPT-4o now demonstrate a remarkable\nability to infer users' locations from public shared images, posing a\nsubstantial risk to geoprivacy. Although adversarial perturbations offer a\npotential defense, current methods are ill-suited for this scenario: they often\nperform poorly on high-resolution images and low perturbation budgets, and may\nintroduce irrelevant semantic content. To address these limitations, we propose\nGeoShield, a novel adversarial framework designed for robust geoprivacy\nprotection in real-world scenarios. GeoShield comprises three key modules: a\nfeature disentanglement module that separates geographical and non-geographical\ninformation, an exposure element identification module that pinpoints\ngeo-revealing regions within an image, and a scale-adaptive enhancement module\nthat jointly optimizes perturbations at both global and local levels to ensure\neffectiveness across resolutions. Extensive experiments on challenging\nbenchmarks show that GeoShield consistently surpasses prior methods in\nblack-box settings, achieving strong privacy protection with minimal impact on\nvisual or semantic quality. To our knowledge, this work is the first to explore\nadversarial perturbations for defending against geolocation inference by\nadvanced VLMs, providing a practical and effective solution to escalating\nprivacy concerns.",
    "updated" : "2025-08-05T08:37:06Z",
    "published" : "2025-08-05T08:37:06Z",
    "authors" : [
      {
        "name" : "Xinwei Liu"
      },
      {
        "name" : "Xiaojun Jia"
      },
      {
        "name" : "Yuan Xun"
      },
      {
        "name" : "Simeng Qin"
      },
      {
        "name" : "Xiaochun Cao"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03204v1",
    "title" : "Current State in Privacy-Preserving Text Preprocessing for\n  Domain-Agnostic NLP",
    "summary" : "Privacy is a fundamental human right. Data privacy is protected by different\nregulations, such as GDPR. However, modern large language models require a huge\namount of data to learn linguistic variations, and the data often contains\nprivate information. Research has shown that it is possible to extract private\ninformation from such language models. Thus, anonymizing such private and\nsensitive information is of utmost importance. While complete anonymization may\nnot be possible, a number of different pre-processing approaches exist for\nmasking or pseudonymizing private information in textual data. This report\nfocuses on a few of such approaches for domain-agnostic NLP tasks.",
    "updated" : "2025-08-05T08:26:45Z",
    "published" : "2025-08-05T08:26:45Z",
    "authors" : [
      {
        "name" : "Abhirup Sinha"
      },
      {
        "name" : "Pritilata Saha"
      },
      {
        "name" : "Tithi Saha"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03098v1",
    "title" : "Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language\n  Models in Retrieval-Augmented Generation",
    "summary" : "Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large\nlanguage models (LLMs) by conditioning outputs on external knowledge sources.\nHowever, when retrieval involves private or sensitive data, RAG systems are\nsusceptible to extraction attacks that can leak confidential information\nthrough generated responses. We propose Privacy-Aware Decoding (PAD), a\nlightweight, inference-time defense that adaptively injects calibrated Gaussian\nnoise into token logits during generation. PAD integrates confidence-based\nscreening to selectively protect high-risk tokens, efficient sensitivity\nestimation to minimize unnecessary noise, and context-aware noise calibration\nto balance privacy with generation quality. A \\renyi Differential Privacy (RDP)\naccountant rigorously tracks cumulative privacy loss, enabling explicit\nper-response $(\\varepsilon, \\delta)$-DP guarantees for sensitive outputs.\nUnlike prior approaches requiring retraining or corpus-level filtering, PAD is\nmodel-agnostic and operates entirely at decoding time with minimal\ncomputational overhead. Experiments on three real-world datasets demonstrate\nthat PAD substantially reduces private information leakage while preserving\nresponse utility, outperforming existing retrieval- and post-processing-based\ndefenses. Our work takes an important step toward mitigating privacy risks in\nRAG via decoding strategies, paving the way for universal and scalable privacy\nsolutions in sensitive domains. Our code is available:\nhttps://github.com/wang2226/PAD.",
    "updated" : "2025-08-05T05:22:13Z",
    "published" : "2025-08-05T05:22:13Z",
    "authors" : [
      {
        "name" : "Haoran Wang"
      },
      {
        "name" : "Xiongxiao Xu"
      },
      {
        "name" : "Baixiang Huang"
      },
      {
        "name" : "Kai Shu"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.04583v1",
    "title" : "Measuring the Carbon Footprint of Cryptographic Privacy-Enhancing\n  Technologies",
    "summary" : "Privacy-enhancing technologies (PETs) have attracted significant attention in\nresponse to privacy regulations, driving the development of applications that\nprioritize user data protection. At the same time, the information and\ncommunication technology (ICT) sector faces growing pressure to reduce its\nenvironmental footprint, particularly its carbon emissions. While numerous\nstudies have assessed the energy footprint of various ICT applications, the\nenvironmental footprint of cryptographic PETs remains largely unexplored.\n  Our work addresses this gap by proposing a standardized methodology for\nevaluating the carbon footprint of PETs. To demonstrate this methodology, we\nfocus on PETs supporting client-server applications as they are the simplest to\ndeploy. In particular, we measure the energy consumption and carbon footprint\nincrease induced by five cryptographic PETs (compared to their non-private\nequivalent): HTTPS web browsing, encrypted machine learning (ML) inference,\nencrypted ML training, encrypted databases, and encrypted emails. Our findings\nreveal significant variability in carbon footprint increases, ranging from a\ntwofold increase in HTTPS web browsing to a 100,000-fold increase in encrypted\nML.\n  Our study provides essential data to help decision-makers assess\nprivacy-carbon trade-offs in such applications. Finally, we outline key\nresearch directions for developing PETs that balance strong privacy protection\nwith environmental sustainability.",
    "updated" : "2025-08-06T16:07:29Z",
    "published" : "2025-08-06T16:07:29Z",
    "authors" : [
      {
        "name" : "Marc Damie"
      },
      {
        "name" : "Mihai Pop"
      },
      {
        "name" : "Merijn Posthuma"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.04542v1",
    "title" : "Privacy Risk Predictions Based on Fundamental Understanding of Personal\n  Data and an Evolving Threat Landscape",
    "summary" : "It is difficult for individuals and organizations to protect personal\ninformation without a fundamental understanding of relative privacy risks. By\nanalyzing over 5,000 empirical identity theft and fraud cases, this research\nidentifies which types of personal data are exposed, how frequently exposures\noccur, and what the consequences of those exposures are. We construct an\nIdentity Ecosystem graph--a foundational, graph-based model in which nodes\nrepresent personally identifiable information (PII) attributes and edges\nrepresent empirical disclosure relationships between them (e.g., the\nprobability that one PII attribute is exposed due to the exposure of another).\nLeveraging this graph structure, we develop a privacy risk prediction framework\nthat uses graph theory and graph neural networks to estimate the likelihood of\nfurther disclosures when certain PII attributes are compromised. The results\nshow that our approach effectively answers the core question: Can the\ndisclosure of a given identity attribute possibly lead to the disclosure of\nanother attribute?",
    "updated" : "2025-08-06T15:30:07Z",
    "published" : "2025-08-06T15:30:07Z",
    "authors" : [
      {
        "name" : "Haoran Niu"
      },
      {
        "name" : "K. Suzanne Barber"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.04202v1",
    "title" : "Unplug, Mute, Avoid Investigating smart speaker users' privacy\n  protection behaviours in Saudi Homes",
    "summary" : "Smart speakers are increasingly integrated into domestic life worldwide, yet\ntheir privacy risks remain underexplored in non-Western cultural contexts. This\nstudy investigates how Saudi Arabian users of smart speakers navigate privacy\nconcerns within collectivist, gendered, and often multigenerational households.\nUsing cultural probes followed by semi-structured interviews with 16\nparticipants, we uncover everyday privacy-protective behaviours including\nunplugging devices, muting microphones, and avoiding voice interactions\naltogether. These practices are shaped not only by individual risk perceptions\nbut also by household norms, room configurations, and interpersonal dynamics.\nWe contribute empirical insights from an underrepresented region, theoretical\nextensions to contextual integrity frameworks, and design directions for\nculturally responsive voice interfaces. This work expands the global\nconversation on smart speaker privacy and informs more inclusive HCI practices\nin increasingly diverse smart home environments.",
    "updated" : "2025-08-06T08:32:54Z",
    "published" : "2025-08-06T08:32:54Z",
    "authors" : [
      {
        "name" : "Abdulrhman Alorini"
      },
      {
        "name" : "Yufeng Wu"
      },
      {
        "name" : "Abdullah Bin Sawad"
      },
      {
        "name" : "Mukesh Prasad"
      },
      {
        "name" : "A. Baki Kocaballi"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03991v1",
    "title" : "Galaxy: A Cognition-Centered Framework for Proactive,\n  Privacy-Preserving, and Self-Evolving LLM Agents",
    "summary" : "Intelligent personal assistants (IPAs) such as Siri and Google Assistant are\ndesigned to enhance human capabilities and perform tasks on behalf of users.\nThe emergence of LLM agents brings new opportunities for the development of\nIPAs. While responsive capabilities have been widely studied, proactive\nbehaviors remain underexplored. Designing an IPA that is proactive,\nprivacy-preserving, and capable of self-evolution remains a significant\nchallenge. Designing such IPAs relies on the cognitive architecture of LLM\nagents. This work proposes Cognition Forest, a semantic structure designed to\nalign cognitive modeling with system-level design. We unify cognitive\narchitecture and system design into a self-reinforcing loop instead of treating\nthem separately. Based on this principle, we present Galaxy, a framework that\nsupports multidimensional interactions and personalized capability generation.\nTwo cooperative agents are implemented based on Galaxy: KoRa, a\ncognition-enhanced generative agent that supports both responsive and proactive\nskills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's\nself-evolution and privacy preservation. Experimental results show that Galaxy\noutperforms multiple state-of-the-art benchmarks. Ablation studies and\nreal-world interaction cases validate the effectiveness of Galaxy.",
    "updated" : "2025-08-06T00:46:38Z",
    "published" : "2025-08-06T00:46:38Z",
    "authors" : [
      {
        "name" : "Chongyu Bao"
      },
      {
        "name" : "Ruimin Dai"
      },
      {
        "name" : "Yangbo Shen"
      },
      {
        "name" : "Runyang Jian"
      },
      {
        "name" : "Jinghan Zhang"
      },
      {
        "name" : "Xiaolan Liu"
      },
      {
        "name" : "Kunpeng Liu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03989v1",
    "title" : "Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework",
    "summary" : "User-controllable privacy is important in modern sensing systems, as privacy\npreferences can vary significantly from person to person and may evolve over\ntime. This is especially relevant in devices equipped with Inertial Measurement\nUnit (IMU) sensors, such as smartphones and wearables, which continuously\ncollect rich time-series data that can inadvertently expose sensitive user\nbehaviors. While prior work has proposed privacy-preserving methods for sensor\ndata, most rely on static, predefined privacy labels or require large\nquantities of private training data, limiting their adaptability and user\nagency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,\nfew-shot privacy-preserving sensing framework. PrivCLIP allows users to specify\nand modify their privacy preferences by categorizing activities as sensitive\n(black-listed), non-sensitive (white-listed), or neutral (gray-listed).\nLeveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU\nsensor data with natural language activity descriptions in a shared embedding\nspace, enabling few-shot detection of sensitive activities. When a\nprivacy-sensitive activity is identified, the system uses a language-guided\nactivity sanitizer and a motion generation module (IMU-GPT) to transform the\noriginal data into a privacy-compliant version that semantically resembles a\nnon-sensitive activity. We evaluate PrivCLIP on multiple human activity\nrecognition datasets and demonstrate that it significantly outperforms baseline\nmethods in terms of both privacy protection and data utility.",
    "updated" : "2025-08-06T00:44:11Z",
    "published" : "2025-08-06T00:44:11Z",
    "authors" : [
      {
        "name" : "Ajesh Koyatan Chathoth"
      },
      {
        "name" : "Shuhao Yu"
      },
      {
        "name" : "Stephen Lee"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03836v1",
    "title" : "DP-NCB: Privacy Preserving Fair Bandits",
    "summary" : "Multi-armed bandit algorithms are fundamental tools for sequential\ndecision-making under uncertainty, with widespread applications across domains\nsuch as clinical trials and personalized decision-making. As bandit algorithms\nare increasingly deployed in these socially sensitive settings, it becomes\ncritical to protect user data privacy and ensure fair treatment across decision\nrounds. While prior work has independently addressed privacy and fairness in\nbandit settings, the question of whether both objectives can be achieved\nsimultaneously has remained largely open. Existing privacy-preserving bandit\nalgorithms typically optimize average regret, a utilitarian measure, whereas\nfairness-aware approaches focus on minimizing Nash regret, which penalizes\ninequitable reward distributions, but often disregard privacy concerns.\n  To bridge this gap, we introduce Differentially Private Nash Confidence Bound\n(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures\n$\\epsilon$-differential privacy and achieves order-optimal Nash regret,\nmatching known lower bounds up to logarithmic factors. The framework is\nsufficiently general to operate under both global and local differential\nprivacy models, and is anytime, requiring no prior knowledge of the time\nhorizon. We support our theoretical guarantees with simulations on synthetic\nbandit instances, showing that DP-NCB incurs substantially lower Nash regret\nthan state-of-the-art baselines. Our results offer a principled foundation for\ndesigning bandit algorithms that are both privacy-preserving and fair, making\nthem suitable for high-stakes, socially impactful applications.",
    "updated" : "2025-08-05T18:34:00Z",
    "published" : "2025-08-05T18:34:00Z",
    "authors" : [
      {
        "name" : "Dhruv Sarkar"
      },
      {
        "name" : "Nishant Pandey"
      },
      {
        "name" : "Sayak Ray Chowdhury"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03831v1",
    "title" : "A Type System for Data Privacy Compliance in Active Object Languages",
    "summary" : "Data protection laws such as GDPR aim to give users unprecedented control\nover their personal data. Compliance with these regulations requires\nsystematically considering information flow and interactions among entities\nhandling sensitive data. Privacy-by-design principles advocate embedding data\nprotection into system architectures as a default. However, translating these\nabstract principles into concrete, explicit methods remains a significant\nchallenge. This paper addresses this gap by proposing a language-based approach\nto privacy integration, combining static and runtime techniques. By employing\ntype checking and type inference in an active object language, the framework\nenables the tracking of authorised data flows and the automatic generation of\nconstraints checked at runtime based on user consent. This ensures that\npersonal data is processed in compliance with GDPR constraints. The key\ncontribution of this work is a type system that gather the compliance checks\nand the changes to users consent and integrates data privacy compliance\nverification into system execution. The paper demonstrates the feasibility of\nthis approach through a soundness proof and several examples, illustrating how\nthe proposed language addresses common GDPR requirements, such as user consent,\npurpose limitation, and data subject rights. This work advances the state of\nthe art in privacy-aware system design by offering a systematic and automated\nmethod for integrating GDPR compliance into programming languages. This\ncapability has implications for building trustworthy systems in domains such as\nhealthcare or finance, where data privacy is crucial.",
    "updated" : "2025-08-05T18:21:28Z",
    "published" : "2025-08-05T18:21:28Z",
    "authors" : [
      {
        "name" : "Chinmayi Prabhu Baramashetru"
      },
      {
        "name" : "Paola Giannini"
      },
      {
        "name" : "Silvia Lizeth Tapia Tarifa"
      },
      {
        "name" : "Olaf Owe"
      }
    ],
    "categories" : [
      "cs.PL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.05518v1",
    "title" : "Local Distance Query with Differential Privacy",
    "summary" : "Differential Privacy (DP) is commonly employed to safeguard graph analysis or\npublishing. Distance, a critical factor in graph analysis, is typically handled\nusing curator DP, where a trusted curator holds the complete neighbor lists of\nall vertices and answers queries privately. However, in many real-world\nscenarios, such a curator may not be present, posing a significant challenge\nfor implementing differentially private distance queries under Local\nDifferential Privacy (LDP). This paper proposes two approaches to address this\nchallenge. The first approach generates a synthetic graph by randomizing\nresponses and applies bitwise operations to reduce noise interference. However,\nlike other synthetic graph methods, this approach suffers from low utility. To\novercome this limitation, we propose a second approach, the first LDP method\nspecifically designed for distance queries, which captures the global graph\nstructure by continuously aggregating local distance vectors from neighboring\nvertices. This process enables the accurate updating of global distances. We\ndemonstrate the effectiveness of our method through comprehensive theoretical\nanalysis and experimental evaluations on real-world datasets.",
    "updated" : "2025-08-07T15:48:35Z",
    "published" : "2025-08-07T15:48:35Z",
    "authors" : [
      {
        "name" : "Weihong Sheng"
      },
      {
        "name" : "Jiajun Chen"
      },
      {
        "name" : "Bin Cai"
      },
      {
        "name" : "Chunqiang Hu"
      },
      {
        "name" : "Meng Han"
      },
      {
        "name" : "Jiguo Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.05250v1",
    "title" : "Privacy Disclosure of Similarity in Speech and Language Processing",
    "summary" : "Speaker, author, and other biometric identification applications often\ncompare a sample's similarity to a database of templates to determine the\nidentity. Given that data may be noisy and similarity measures can be\ninaccurate, such a comparison may not reliably identify the true identity as\nthe most similar. Still, even the similarity rank based on an inaccurate\nsimilarity measure can disclose private information about the true identity. We\npropose a methodology for quantifying the privacy disclosure of such a\nsimilarity rank by estimating its probability distribution. It is based on\ndetermining the histogram of the similarity rank of the true speaker, or when\ndata is scarce, modeling the histogram with the beta-binomial distribution. We\nexpress the disclosure in terms of entropy (bits), such that the disclosure\nfrom independent features are additive. Our experiments demonstrate that all\ntested speaker and author characterizations contain personally identifying\ninformation (PII) that can aid in identification, with embeddings from speaker\nrecognition algorithms containing the most information, followed by phone\nembeddings, linguistic embeddings, and fundamental frequency. Our initial\nexperiments show that the disclosure of PII increases with the length of test\nsamples, but it is bounded by the length of database templates. The provided\nmetric, similarity rank disclosure, provides a way to compare the disclosure of\nPII between biometric features and merge them to aid identification. It can\nthus aid in the holistic evaluation of threats to privacy in speech and other\nbiometric technologies.",
    "updated" : "2025-08-07T10:40:35Z",
    "published" : "2025-08-07T10:40:35Z",
    "authors" : [
      {
        "name" : "Tom Bäckström"
      },
      {
        "name" : "Mohammad Hassan Vali"
      },
      {
        "name" : "My Nguyen"
      },
      {
        "name" : "Silas Rech"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.04202v1",
    "title" : "Unplug, Mute, Avoid: Investigating smart speaker users' privacy\n  protection behaviours in Saudi Homes",
    "summary" : "Smart speakers are increasingly integrated into domestic life worldwide, yet\ntheir privacy risks remain underexplored in non-Western cultural contexts. This\nstudy investigates how Saudi Arabian users of smart speakers navigate privacy\nconcerns within collectivist, gendered, and often multigenerational households.\nUsing cultural probes followed by semi-structured interviews with 16\nparticipants, we uncover everyday privacy-protective behaviours including\nunplugging devices, muting microphones, and avoiding voice interactions\naltogether. These practices are shaped not only by individual risk perceptions\nbut also by household norms, room configurations, and interpersonal dynamics.\nWe contribute empirical insights from an underrepresented region, theoretical\nextensions to contextual integrity frameworks, and design directions for\nculturally responsive voice interfaces. This work expands the global\nconversation on smart speaker privacy and informs more inclusive HCI practices\nin increasingly diverse smart home environments.",
    "updated" : "2025-08-06T08:32:54Z",
    "published" : "2025-08-06T08:32:54Z",
    "authors" : [
      {
        "name" : "Abdulrhman Alorini"
      },
      {
        "name" : "Yufeng Wu"
      },
      {
        "name" : "Abdullah Bin Sawad"
      },
      {
        "name" : "Mukesh Prasad"
      },
      {
        "name" : "A. Baki Kocaballi"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06301v1",
    "title" : "FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields",
    "summary" : "Neural fields provide a memory-efficient representation of data, which can\neffectively handle diverse modalities and large-scale data. However, learning\nto map neural fields often requires large amounts of training data and\ncomputations, which can be limited to resource-constrained edge devices. One\napproach to tackle this limitation is to leverage Federated Meta-Learning\n(FML), but traditional FML approaches suffer from privacy leakage. To address\nthese issues, we introduce a novel FML approach called FedMeNF. FedMeNF\nutilizes a new privacy-preserving loss function that regulates privacy leakage\nin the local meta-optimization. This enables the local meta-learner to optimize\nquickly and efficiently without retaining the client's private data. Our\nexperiments demonstrate that FedMeNF achieves fast optimization speed and\nrobust reconstruction performance, even with few-shot or non-IID data across\ndiverse data modalities, while preserving client data privacy.",
    "updated" : "2025-08-08T13:24:57Z",
    "published" : "2025-08-08T13:24:57Z",
    "authors" : [
      {
        "name" : "Junhyeog Yun"
      },
      {
        "name" : "Minui Hong"
      },
      {
        "name" : "Gunhee Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06251v1",
    "title" : "Synthetic Data Generation and Differential Privacy using Tensor\n  Networks' Matrix Product States (MPS)",
    "summary" : "Synthetic data generation is a key technique in modern artificial\nintelligence, addressing data scarcity, privacy constraints, and the need for\ndiverse datasets in training robust models. In this work, we propose a method\nfor generating privacy-preserving high-quality synthetic tabular data using\nTensor Networks, specifically Matrix Product States (MPS). We benchmark the\nMPS-based generative model against state-of-the-art models such as CTGAN, VAE,\nand PrivBayes, focusing on both fidelity and privacy-preserving capabilities.\nTo ensure differential privacy (DP), we integrate noise injection and gradient\nclipping during training, enabling privacy guarantees via R\\'enyi Differential\nPrivacy accounting. Across multiple metrics analyzing data fidelity and\ndownstream machine learning task performance, our results show that MPS\noutperforms classical models, particularly under strict privacy constraints.\nThis work highlights MPS as a promising tool for privacy-aware synthetic data\ngeneration. By combining the expressive power of tensor network representations\nwith formal privacy mechanisms, the proposed approach offers an interpretable\nand scalable alternative for secure data sharing. Its structured design\nfacilitates integration into sensitive domains where both data quality and\nconfidentiality are critical.",
    "updated" : "2025-08-08T12:14:57Z",
    "published" : "2025-08-08T12:14:57Z",
    "authors" : [
      {
        "name" : "Alejandro Moreno R."
      },
      {
        "name" : "Desale Fentaw"
      },
      {
        "name" : "Samuel Palmer"
      },
      {
        "name" : "Raúl Salles de Padua"
      },
      {
        "name" : "Ninad Dixit"
      },
      {
        "name" : "Samuel Mugel"
      },
      {
        "name" : "Roman Orús"
      },
      {
        "name" : "Manuel Radons"
      },
      {
        "name" : "Josef Menter"
      },
      {
        "name" : "Ali Abedi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06208v1",
    "title" : "Graph Federated Learning for Personalized Privacy Recommendation",
    "summary" : "Federated recommendation systems (FedRecs) have gained significant attention\nfor providing privacy-preserving recommendation services. However, existing\nFedRecs assume that all users have the same requirements for privacy\nprotection, i.e., they do not upload any data to the server. The approaches\noverlook the potential to enhance the recommendation service by utilizing\npublicly available user data. In real-world applications, users can choose to\nbe private or public. Private users' interaction data is not shared, while\npublic users' interaction data can be shared. Inspired by the issue, this paper\nproposes a novel Graph Federated Learning for Personalized Privacy\nRecommendation (GFed-PP) that adapts to different privacy requirements while\nimproving recommendation performance. GFed-PP incorporates the interaction data\nof public users to build a user-item interaction graph, which is then used to\nform a user relationship graph. A lightweight graph convolutional network (GCN)\nis employed to learn each user's user-specific personalized item embedding. To\nprotect user privacy, each client learns the user embedding and the scoring\nfunction locally. Additionally, GFed-PP achieves optimization of the federated\nrecommendation framework through the initialization of item embedding on\nclients and the aggregation of the user relationship graph on the server.\nExperimental results demonstrate that GFed-PP significantly outperforms\nexisting methods for five datasets, offering superior recommendation accuracy\nwithout compromising privacy. This framework provides a practical solution for\naccommodating varying privacy preferences in federated recommendation systems.",
    "updated" : "2025-08-08T10:44:33Z",
    "published" : "2025-08-08T10:44:33Z",
    "authors" : [
      {
        "name" : "Ce Na"
      },
      {
        "name" : "Kai Yang"
      },
      {
        "name" : "Dengzhao Fang"
      },
      {
        "name" : "Yu Li"
      },
      {
        "name" : "Jingtong Gao"
      },
      {
        "name" : "Chengcheng Zhu"
      },
      {
        "name" : "Jiale Zhang"
      },
      {
        "name" : "Xiaobing Sun"
      },
      {
        "name" : "Yi Chang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06087v1",
    "title" : "Adaptive Backtracking for Privacy Protection in Large Language Models",
    "summary" : "The preservation of privacy has emerged as a critical topic in the era of\nartificial intelligence. However, current work focuses on user-oriented\nprivacy, overlooking severe enterprise data leakage risks exacerbated by the\nRetrieval-Augmented Generation paradigm. To address this gap, our paper\nintroduces a novel objective: enterprise-oriented privacy concerns. Achieving\nthis objective requires overcoming two fundamental challenges: existing methods\nsuch as data sanitization severely degrade model performance, and the field\nlacks public datasets for evaluation. We address these challenges with several\nsolutions. (1) To prevent performance degradation, we propose ABack, a\ntraining-free mechanism that leverages a Hidden State Model to pinpoint the\norigin of a leakage intention and rewrite the output safely. (2) To solve the\nlack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy\nscenarios in healthcare and finance. To ensure a rigorous evaluation, we move\nbeyond simple static attacks by developing a powerful adaptive attacker with\nGroup Relative Policy Optimization. Experiments show that against this superior\nadversary, ABack improves the overall privacy utility score by up to 15\\% over\nstrong baselines, avoiding the performance trade-offs of prior methods.",
    "updated" : "2025-08-08T07:29:33Z",
    "published" : "2025-08-08T07:29:33Z",
    "authors" : [
      {
        "name" : "Zhihao Yao"
      },
      {
        "name" : "Yuxuan Gu"
      },
      {
        "name" : "Xiachong Feng"
      },
      {
        "name" : "Weitao Ma"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Xiaocheng Feng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.08190v1",
    "title" : "Differential Privacy for Regulatory Compliance in Cyberattack Detection\n  on Critical Infrastructure Systems",
    "summary" : "Industrial control systems are a fundamental component of critical\ninfrastructure networks (CIN) such as gas, water and power. With the growing\nrisk of cyberattacks, regulatory compliance requirements are also increasing\nfor large scale critical infrastructure systems comprising multiple utility\nstakeholders. The primary goal of regulators is to ensure overall system\nstability with recourse to trustworthy stakeholder attack detection. However,\nadhering to compliance requirements requires stakeholders to also disclose\nsensor and control data to regulators raising privacy concerns. In this paper,\nwe present a cyberattack detection framework that utilizes differentially\nprivate (DP) hypothesis tests geared towards enhancing regulatory confidence\nwhile alleviating privacy concerns of CIN stakeholders. The hallmark of our\napproach is a two phase privacy scheme that protects the privacy of covariance,\nas well as the associated sensor driven test statistics computed as a means to\ngenerate alarms. Theoretically, we show that our method induces a\nmisclassification error rate comparable to the non-DP cases while delivering\nrobust privacy guarantees. With the help of real-world datasets, we show the\nreliability of our DP-detection outcomes for a wide variety of attack scenarios\nfor interdependent stakeholders.",
    "updated" : "2025-08-11T17:10:49Z",
    "published" : "2025-08-11T17:10:49Z",
    "authors" : [
      {
        "name" : "Paritosh Ramanan"
      },
      {
        "name" : "H. M. Mohaimanul Islam"
      },
      {
        "name" : "Abhiram Reddy Alugula"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07960v1",
    "title" : "VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With\n  Enhanced Security",
    "summary" : "Advancement of machine learning techniques, combined with the availability of\nlarge-scale datasets, has significantly improved the accuracy and efficiency of\nfacial recognition. Modern facial recognition systems are trained using large\nface datasets collected from diverse individuals or public repositories.\nHowever, for training, these datasets are often replicated and stored in\nmultiple workstations, resulting in data replication, which complicates\ndatabase management and oversight. Currently, once a user submits their face\nfor dataset preparation, they lose control over how their data is used, raising\nsignificant privacy and ethical concerns. This paper introduces VOIDFace, a\nnovel framework for facial recognition systems that addresses two major issues.\nFirst, it eliminates the need of data replication and improves data control to\nsecurely store training face data by using visual secret sharing. Second, it\nproposes a patch-based multi-training network that uses this novel training\ndata storage mechanism to develop a robust, privacy-preserving facial\nrecognition system. By integrating these advancements, VOIDFace aims to improve\nthe privacy, security, and efficiency of facial recognition training, while\nensuring greater control over sensitive personal face data. VOIDFace also\nenables users to exercise their Right-To-Be-Forgotten property to control their\npersonal data. Experimental evaluations on the VGGFace2 dataset show that\nVOIDFace provides Right-To-Be-Forgotten, improved data control, security, and\nprivacy while maintaining competitive facial recognition performance. Code is\navailable at: https://github.com/ajnasmuhammed89/VOIDFace",
    "updated" : "2025-08-11T13:15:36Z",
    "published" : "2025-08-11T13:15:36Z",
    "authors" : [
      {
        "name" : "Ajnas Muhammed"
      },
      {
        "name" : "Iurri Medvedev"
      },
      {
        "name" : "Nuno Gonçalves"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07676v1",
    "title" : "Multi-Hop Privacy Propagation for Differentially Private Federated\n  Learning in Social Networks",
    "summary" : "Federated learning (FL) enables collaborative model training across\ndecentralized clients without sharing local data, thereby enhancing privacy and\nfacilitating collaboration among clients connected via social networks.\nHowever, these social connections introduce privacy externalities: a client's\nprivacy loss depends not only on its privacy protection strategy but also on\nthe privacy decisions of others, propagated through the network via multi-hop\ninteractions. In this work, we propose a socially-aware privacy-preserving FL\nmechanism that systematically quantifies indirect privacy leakage through a\nmulti-hop propagation model. We formulate the server-client interaction as a\ntwo-stage Stackelberg game, where the server, as the leader, optimizes\nincentive policies, and clients, as followers, strategically select their\nprivacy budgets, which determine their privacy-preserving levels by controlling\nthe magnitude of added noise. To mitigate information asymmetry in networked\nprivacy estimation, we introduce a mean-field estimator to approximate the\naverage external privacy risk. We theoretically prove the existence and\nconvergence of the fixed point of the mean-field estimator and derive\nclosed-form expressions for the Stackelberg Nash Equilibrium. Despite being\ndesigned from a client-centric incentive perspective, our mechanism achieves\napproximately-optimal social welfare, as revealed by Price of Anarchy (PoA)\nanalysis. Experiments on diverse datasets demonstrate that our approach\nsignificantly improves client utilities and reduces server costs while\nmaintaining model performance, outperforming both Social-Agnostic (SA)\nbaselines and methods that account for social externalities.",
    "updated" : "2025-08-11T06:53:32Z",
    "published" : "2025-08-11T06:53:32Z",
    "authors" : [
      {
        "name" : "Chenchen Lin"
      },
      {
        "name" : "Xuehe Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC",
      "cs.GT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07672v1",
    "title" : "Towards Aligning Personalized Conversational Recommendation Agents with\n  Users' Privacy Preferences",
    "summary" : "The proliferation of AI agents, with their complex and context-dependent\nactions, renders conventional privacy paradigms obsolete. This position paper\nargues that the current model of privacy management, rooted in a user's\nunilateral control over a passive tool, is inherently mismatched with the\ndynamic and interactive nature of AI agents. We contend that ensuring effective\nprivacy protection necessitates that the agents proactively align with users'\nprivacy preferences instead of passively waiting for the user to control. To\nground this shift, and using personalized conversational recommendation agents\nas a case, we propose a conceptual framework built on Contextual Integrity (CI)\ntheory and Privacy Calculus theory. This synthesis first reframes automatically\ncontrolling users' privacy as an alignment problem, where AI agents initially\ndid not know users' preferences, and would learn their privacy preferences\nthrough implicit or explicit feedback. Upon receiving the preference feedback,\nthe agents used alignment and Pareto optimization for aligning preferences and\nbalancing privacy and utility. We introduced formulations and instantiations,\npotential applications, as well as five challenges.",
    "updated" : "2025-08-11T06:51:44Z",
    "published" : "2025-08-11T06:51:44Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Ying Ma"
      },
      {
        "name" : "Jingruo Chen"
      },
      {
        "name" : "Simin Li"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07667v1",
    "title" : "1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent\n  Reasoning",
    "summary" : "Addressing contextual privacy concerns remains challenging in interactive\nsettings where large language models (LLMs) process information from multiple\nsources (e.g., summarizing meetings with private and public information). We\nintroduce a multi-agent framework that decomposes privacy reasoning into\nspecialized subtasks (extraction, classification), reducing the information\nload on any single agent while enabling iterative validation and more reliable\nadherence to contextual privacy norms. To understand how privacy errors emerge\nand propagate, we conduct a systematic ablation over information-flow\ntopologies, revealing when and why upstream detection mistakes cascade into\ndownstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with\nseveral open-source and closed-sourced LLMs demonstrate that our best\nmulti-agent configuration substantially reduces private information leakage\n(\\textbf{18\\%} on ConfAIde and \\textbf{19\\%} on PrivacyLens with GPT-4o) while\npreserving the fidelity of public content, outperforming single-agent\nbaselines. These results highlight the promise of principled information-flow\ndesign in multi-agent systems for contextual privacy with LLMs.",
    "updated" : "2025-08-11T06:34:09Z",
    "published" : "2025-08-11T06:34:09Z",
    "authors" : [
      {
        "name" : "Wenkai Li"
      },
      {
        "name" : "Liwen Sun"
      },
      {
        "name" : "Zhenxiang Guan"
      },
      {
        "name" : "Xuhui Zhou"
      },
      {
        "name" : "Maarten Sap"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07664v1",
    "title" : "Understanding Users' Privacy Perceptions Towards LLM's RAG-based Memory",
    "summary" : "Large Language Models (LLMs) are increasingly integrating memory\nfunctionalities to provide personalized and context-aware interactions.\nHowever, user understanding, practices and expectations regarding these memory\nsystems are not yet well understood. This paper presents a thematic analysis of\nsemi-structured interviews with 18 users to explore their mental models of\nLLM's Retrieval Augmented Generation (RAG)-based memory, current usage\npractices, perceived benefits and drawbacks, privacy concerns and expectations\nfor future memory systems. Our findings reveal diverse and often incomplete\nmental models of how memory operates. While users appreciate the potential for\nenhanced personalization and efficiency, significant concerns exist regarding\nprivacy, control and the accuracy of remembered information. Users express a\ndesire for granular control over memory generation, management, usage and\nupdating, including clear mechanisms for reviewing, editing, deleting and\ncategorizing memories, as well as transparent insight into how memories and\ninferred information are used. We discuss design implications for creating more\nuser-centric, transparent, and trustworthy LLM memory systems.",
    "updated" : "2025-08-11T06:26:30Z",
    "published" : "2025-08-11T06:26:30Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Rongjun Ma"
      },
      {
        "name" : "Ying Ma"
      },
      {
        "name" : "Shixuan Li"
      },
      {
        "name" : "Yiqun Xu"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07505v1",
    "title" : "Enhancing Privacy in Decentralized Min-Max Optimization: A\n  Differentially Private Approach",
    "summary" : "Decentralized min-max optimization allows multi-agent systems to\ncollaboratively solve global min-max optimization problems by facilitating the\nexchange of model updates among neighboring agents, eliminating the need for a\ncentral server. However, sharing model updates in such systems carry a risk of\nexposing sensitive data to inference attacks, raising significant privacy\nconcerns. To mitigate these privacy risks, differential privacy (DP) has become\na widely adopted technique for safeguarding individual data. Despite its\nadvantages, implementing DP in decentralized min-max optimization poses\nchallenges, as the added noise can hinder convergence, particularly in\nnon-convex scenarios with complex agent interactions in min-max optimization\nproblems. In this work, we propose an algorithm called DPMixSGD (Differential\nPrivate Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving\nalgorithm specifically designed for non-convex decentralized min-max\noptimization. Our method builds on the state-of-the-art STORM-based algorithm,\none of the fastest decentralized min-max solutions. We rigorously prove that\nthe noise added to local gradients does not significantly compromise\nconvergence performance, and we provide theoretical bounds to ensure privacy\nguarantees. To validate our theoretical findings, we conduct extensive\nexperiments across various tasks and models, demonstrating the effectiveness of\nour approach.",
    "updated" : "2025-08-10T23:24:27Z",
    "published" : "2025-08-10T23:24:27Z",
    "authors" : [
      {
        "name" : "Yueyang Quan"
      },
      {
        "name" : "Chang Wang"
      },
      {
        "name" : "Shengjie Zhai"
      },
      {
        "name" : "Minghong Fang"
      },
      {
        "name" : "Zhuqing Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07057v1",
    "title" : "Rethinking Privacy Indicators in Extended Reality: Multimodal Design for\n  Situationally Impaired Bystanders",
    "summary" : "As Extended Reality (XR) devices become increasingly prevalent in everyday\nsettings, they raise significant privacy concerns for bystanders: individuals\nin the vicinity of an XR device during its use, whom the device sensors may\naccidentally capture. Current privacy indicators, such as small LEDs, often\npresume that bystanders are attentive enough to interpret the privacy signals.\nHowever, these cues can be easily overlooked when bystanders are distracted or\nhave limited vision. We define such individuals as situationally impaired\nbystanders. This study explores XR privacy indicator designs that are effective\nfor situationally impaired bystanders. A focus group with eight participants\nwas conducted to design five novel privacy indicators. We evaluated these\ndesigns through a user study with seven additional participants. Our results\nshow that visual-only indicators, typical in commercial XR devices, received\nlow ratings for perceived usefulness in impairment scenarios. In contrast,\nmultimodal indicators were preferred in privacy-sensitive scenarios with\nsituationally impaired bystanders. Ultimately, our results highlight the need\nto move toward adaptable, multimodal, and situationally aware designs that\neffectively support bystander privacy in everyday XR environments.",
    "updated" : "2025-08-09T17:48:44Z",
    "published" : "2025-08-09T17:48:44Z",
    "authors" : [
      {
        "name" : "Syed Ibrahim Mustafa Shah Bukhari"
      },
      {
        "name" : "Maha Sajid"
      },
      {
        "name" : "Bo Ji"
      },
      {
        "name" : "Brendan David-John"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07044v1",
    "title" : "Balancing Privacy and Efficiency: Music Information Retrieval via\n  Additive Homomorphic Encryption",
    "summary" : "In the era of generative AI, ensuring the privacy of music data presents\nunique challenges: unlike static artworks such as images, music data is\ninherently temporal and multimodal, and it is sampled, transformed, and remixed\nat an unprecedented scale. These characteristics make its core vector\nembeddings, i.e, the numerical representations of the music, highly susceptible\nto being learned, misused, or even stolen by models without accessing the\noriginal audio files. Traditional methods like copyright licensing and digital\nwatermarking offer limited protection for these abstract mathematical\nrepresentations, thus necessitating a stronger, e.g., cryptographic, approach\nto safeguarding the embeddings themselves. Standard encryption schemes, such as\nAES, render data unintelligible for computation, making such searches\nimpossible. While Fully Homomorphic Encryption (FHE) provides a plausible\nsolution by allowing arbitrary computations on ciphertexts, its substantial\nperformance overhead remains impractical for large-scale vector similarity\nsearches. Given this trade-off, we propose a more practical approach using\nAdditive Homomorphic Encryption (AHE) for vector similarity search. The primary\ncontributions of this paper are threefold: we analyze threat models unique to\nmusic information retrieval systems; we provide a theoretical analysis and\npropose an efficient AHE-based solution through inner products of music\nembeddings to deliver privacy-preserving similarity search; and finally, we\ndemonstrate the efficiency and practicality of the proposed approach through\nempirical evaluation and comparison to FHE schemes on real-world MP3 files.",
    "updated" : "2025-08-09T17:00:34Z",
    "published" : "2025-08-09T17:00:34Z",
    "authors" : [
      {
        "name" : "William Zerong Wang"
      },
      {
        "name" : "Dongfang Zhao"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06760v1",
    "title" : "Understanding Privacy Norms Around LLM-Based Chatbots: A Contextual\n  Integrity Perspective",
    "summary" : "LLM-driven chatbots like ChatGPT have created large volumes of conversational\ndata, but little is known about how user privacy expectations are evolving with\nthis technology. We conduct a survey experiment with 300 US ChatGPT users to\nunderstand emerging privacy norms for sharing chatbot data. Our findings reveal\na stark disconnect between user concerns and behavior: 82% of respondents rated\nchatbot conversations as sensitive or highly sensitive - more than email or\nsocial media posts - but nearly half reported discussing health topics and over\none-third discussed personal finances with ChatGPT. Participants expressed\nstrong privacy concerns (t(299) = 8.5, p < .01) and doubted their conversations\nwould remain private (t(299) = -6.9, p < .01). Despite this, respondents\nuniformly rejected sharing personal data (search history, emails, device\naccess) for improved services, even in exchange for premium features worth\n$200. To identify which factors influence appropriate chatbot data sharing, we\npresented participants with factorial vignettes manipulating seven contextual\nfactors. Linear mixed models revealed that only the transmission factors such\nas informed consent, data anonymization, or the removal of personally\nidentifiable information, significantly affected perceptions of appropriateness\nand concern for data access. Surprisingly, contextual factors including the\nrecipient of the data (hospital vs. tech company), purpose (research vs.\nadvertising), type of content, and geographic location did not show significant\neffects. Our results suggest that users apply consistent baseline privacy\nexpectations to chatbot data, prioritizing procedural safeguards over recipient\ntrustworthiness. This has important implications for emerging agentic AI\nsystems that assume user willingness to integrate personal data across\nplatforms.",
    "updated" : "2025-08-09T00:22:46Z",
    "published" : "2025-08-09T00:22:46Z",
    "authors" : [
      {
        "name" : "Sarah Tran"
      },
      {
        "name" : "Hongfan Lu"
      },
      {
        "name" : "Isaac Slaughter"
      },
      {
        "name" : "Bernease Herman"
      },
      {
        "name" : "Aayushi Dangol"
      },
      {
        "name" : "Yue Fu"
      },
      {
        "name" : "Lufei Chen"
      },
      {
        "name" : "Biniyam Gebreyohannes"
      },
      {
        "name" : "Bill Howe"
      },
      {
        "name" : "Alexis Hiniker"
      },
      {
        "name" : "Nicholas Weber"
      },
      {
        "name" : "Robert Wolfe"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06647v1",
    "title" : "Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN",
    "summary" : "Synthetic data generation has become essential for securely sharing and\nanalyzing sensitive data sets. Traditional anonymization techniques, however,\noften fail to adequately preserve privacy. We introduce the Tabular\nAuto-Regressive Generative Network (TabularARGN), a neural network architecture\nspecifically designed for generating high-quality synthetic tabular data. Using\na discretization-based auto-regressive approach, TabularARGN achieves high data\nfidelity while remaining computationally efficient. We evaluate TabularARGN\nagainst existing synthetic data generation methods, showing competitive results\nin statistical similarity, machine learning utility, and detection robustness.\nWe further perform an in-depth privacy evaluation using systematic\nmembership-inference attacks, highlighting the robustness and effective\nprivacy-utility balance of our approach.",
    "updated" : "2025-08-08T18:57:23Z",
    "published" : "2025-08-08T18:57:23Z",
    "authors" : [
      {
        "name" : "Andrey Sidorenko"
      },
      {
        "name" : "Paul Tiwald"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06577v1",
    "title" : "Leveraging LLMs for Privacy-Aware Predictions in Participatory Budgeting",
    "summary" : "Participatory Budgeting (PB) empowers citizens to propose and vote on public\ninvestment projects. Yet, despite its democratic potential, PB initiatives\noften suffer from low participation rates, limiting their visibility and\nperceived legitimacy. In this work, we aim to strengthen PB elections in two\nkey ways: by supporting project proposers in crafting better proposals, and by\nhelping PB organizers manage large volumes of submissions in a transparent\nmanner. We propose a privacy-preserving approach to predict which PB proposals\nare likely to be funded, using only their textual descriptions and anonymous\nhistorical voting records -- without relying on voter demographics or\npersonally identifiable information. We evaluate the performance of GPT 4 Turbo\nin forecasting proposal outcomes across varying contextual scenarios, observing\nthat the LLM's prior knowledge needs to be complemented by past voting data to\nobtain predictions reflecting real-world PB voting behavior. Our findings\nhighlight the potential of AI-driven tools to support PB processes by improving\ntransparency, planning efficiency, and civic engagement.",
    "updated" : "2025-08-07T15:26:22Z",
    "published" : "2025-08-07T15:26:22Z",
    "authors" : [
      {
        "name" : "Juan Zambrano"
      },
      {
        "name" : "Clément Contet"
      },
      {
        "name" : "Jairo Gudiño"
      },
      {
        "name" : "Felipe Garrido-Lucero"
      },
      {
        "name" : "Umberto Grandi"
      },
      {
        "name" : "Cesar A Hidalgo"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.05250v2",
    "title" : "Privacy Disclosure of Similarity Rank in Speech and Language Processing",
    "summary" : "Speaker, author, and other biometric identification applications often\ncompare a sample's similarity to a database of templates to determine the\nidentity. Given that data may be noisy and similarity measures can be\ninaccurate, such a comparison may not reliably identify the true identity as\nthe most similar. Still, even the similarity rank based on an inaccurate\nsimilarity measure can disclose private information about the true identity. We\npropose a methodology for quantifying the privacy disclosure of such a\nsimilarity rank by estimating its probability distribution. It is based on\ndetermining the histogram of the similarity rank of the true speaker, or when\ndata is scarce, modeling the histogram with the beta-binomial distribution. We\nexpress the disclosure in terms of entropy (bits), such that the disclosure\nfrom independent features are additive. Our experiments demonstrate that all\ntested speaker and author characterizations contain personally identifying\ninformation (PII) that can aid in identification, with embeddings from speaker\nrecognition algorithms containing the most information, followed by phone\nembeddings, linguistic embeddings, and fundamental frequency. Our initial\nexperiments show that the disclosure of PII increases with the length of test\nsamples, but it is bounded by the length of database templates. The provided\nmetric, similarity rank disclosure, provides a way to compare the disclosure of\nPII between biometric features and merge them to aid identification. It can\nthus aid in the holistic evaluation of threats to privacy in speech and other\nbiometric technologies.",
    "updated" : "2025-08-11T10:34:40Z",
    "published" : "2025-08-07T10:40:35Z",
    "authors" : [
      {
        "name" : "Tom Bäckström"
      },
      {
        "name" : "Mohammad Hassan Vali"
      },
      {
        "name" : "My Nguyen"
      },
      {
        "name" : "Silas Rech"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09036v1",
    "title" : "Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy\n  and AI Governance Exams",
    "summary" : "The rapid emergence of large language models (LLMs) has raised urgent\nquestions across the modern workforce about this new technology's strengths,\nweaknesses, and capabilities. For privacy professionals, the question is\nwhether these AI systems can provide reliable support on regulatory compliance,\nprivacy program management, and AI governance. In this study, we evaluate ten\nleading open and closed LLMs, including models from OpenAI, Anthropic, Google\nDeepMind, Meta, and DeepSeek, by benchmarking their performance on\nindustry-standard certification exams: CIPP/US, CIPM, CIPT, and AIGP from the\nInternational Association of Privacy Professionals (IAPP). Each model was\ntested using official sample exams in a closed-book setting and compared to\nIAPP's passing thresholds. Our findings show that several frontier models such\nas Gemini 2.5 Pro and OpenAI's GPT-5 consistently achieve scores exceeding the\nstandards for professional human certification - demonstrating substantial\nexpertise in privacy law, technical controls, and AI governance. The results\nhighlight both the strengths and domain-specific gaps of current LLMs and offer\npractical insights for privacy officers, compliance leads, and technologists\nassessing the readiness of AI tools for high-stakes data governance roles. This\npaper provides an overview for professionals navigating the intersection of AI\nadvancement and regulatory risk and establishes a machine benchmark based on\nhuman-centric evaluations.",
    "updated" : "2025-08-12T15:57:22Z",
    "published" : "2025-08-12T15:57:22Z",
    "authors" : [
      {
        "name" : "Zane Witherspoon"
      },
      {
        "name" : "Thet Mon Aye"
      },
      {
        "name" : "YingYing Hao"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.08785v1",
    "title" : "Privacy-protected Retrieval-Augmented Generation for Knowledge Graph\n  Question Answering",
    "summary" : "LLMs often suffer from hallucinations and outdated or incomplete knowledge.\nRAG is proposed to address these issues by integrating external knowledge like\nthat in KGs into LLMs. However, leveraging private KGs in RAG systems poses\nsignificant privacy risks due to the black-box nature of LLMs and potential\ninsecure data transmission, especially when using third-party LLM APIs lacking\ntransparency and control. In this paper, we investigate the privacy-protected\nRAG scenario for the first time, where entities in KGs are anonymous for LLMs,\nthus preventing them from accessing entity semantics. Due to the loss of\nsemantics of entities, previous RAG systems cannot retrieve question-relevant\nknowledge from KGs by matching questions with the meaningless identifiers of\nanonymous entities. To realize an effective RAG system in this scenario, two\nkey challenges must be addressed: (1) How can anonymous entities be converted\ninto retrievable information. (2) How to retrieve question-relevant anonymous\nentities. Hence, we propose a novel ARoG framework including relation-centric\nabstraction and structure-oriented abstraction strategies. For challenge (1),\nthe first strategy abstracts entities into high-level concepts by dynamically\ncapturing the semantics of their adjacent relations. It supplements meaningful\nsemantics which can further support the retrieval process. For challenge (2),\nthe second strategy transforms unstructured natural language questions into\nstructured abstract concept paths. These paths can be more effectively aligned\nwith the abstracted concepts in KGs, thereby improving retrieval performance.\nTo guide LLMs to effectively retrieve knowledge from KGs, the two strategies\nstrictly protect privacy from being exposed to LLMs. Experiments on three\ndatasets demonstrate that ARoG achieves strong performance and\nprivacy-robustness.",
    "updated" : "2025-08-12T09:38:21Z",
    "published" : "2025-08-12T09:38:21Z",
    "authors" : [
      {
        "name" : "Yunfeng Ning"
      },
      {
        "name" : "Mayi Xu"
      },
      {
        "name" : "Jintao Wen"
      },
      {
        "name" : "Qiankun Pi"
      },
      {
        "name" : "Yuanyuan Zhu"
      },
      {
        "name" : "Ming Zhong"
      },
      {
        "name" : "Jiawei Jiang"
      },
      {
        "name" : "Tieyun Qian"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.08749v1",
    "title" : "Approximate DBSCAN under Differential Privacy",
    "summary" : "This paper revisits the DBSCAN problem under differential privacy (DP).\nExisting DP-DBSCAN algorithms aim at publishing the cluster labels of the input\npoints. However, we show that both empirically and theoretically, this approach\ncannot offer any utility in the published results. We therefore propose an\nalternative definition of DP-DBSCAN based on the notion of spans. We argue that\npublishing the spans actually better serves the purposes of visualization and\nclassification of DBSCAN. Then we present a linear-time DP-DBSCAN algorithm\nachieving the sandwich quality guarantee in any constant dimensions, as well as\nmatching lower bounds on the approximation ratio. A key building block in our\nalgorithm is a linear-time algorithm for constructing a histogram under\npure-DP, which is of independent interest. Finally, we conducted experiments on\nboth synthetic and real-world datasets to verify the practical performance of\nour DP-DBSCAN algorithm.",
    "updated" : "2025-08-12T08:55:41Z",
    "published" : "2025-08-12T08:55:41Z",
    "authors" : [
      {
        "name" : "Yuan Qiu"
      },
      {
        "name" : "Ke Yi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.08502v1",
    "title" : "AirSignatureDB: Exploring In-Air Signature Biometrics in the Wild and\n  its Privacy Concerns",
    "summary" : "Behavioral biometrics based on smartphone motion sensors are growing in\npopularity for authentication purposes. In this study, AirSignatureDB is\npresented: a new publicly accessible dataset of in-air signatures collected\nfrom 108 participants under real-world conditions, using 83 different\nsmartphone models across four sessions. This dataset includes genuine samples\nand skilled forgeries, enabling a comprehensive evaluation of system robustness\nagainst realistic attack scenarios. Traditional and deep learning-based methods\nfor in-air signature verification are benchmarked, while analyzing the\ninfluence of sensor modality and enrollment strategies. Beyond verification, a\nfirst approach to reconstructing the three-dimensional trajectory of in-air\nsignatures from inertial sensor data alone is introduced. Using on-line\nhandwritten signatures as a reference, we demonstrate that the recovery of\naccurate trajectories is feasible, challenging the long-held assumption that\nin-air gestures are inherently traceless. Although this approach enables\nforensic traceability, it also raises critical questions about the privacy\nboundaries of behavioral biometrics. Our findings underscore the need for a\nreevaluation of the privacy assumptions surrounding inertial sensor data, as\nthey can reveal user-specific information that had not previously been\nconsidered in the design of in-air signature systems.",
    "updated" : "2025-08-11T22:24:03Z",
    "published" : "2025-08-11T22:24:03Z",
    "authors" : [
      {
        "name" : "Marta Robledo-Moreno"
      },
      {
        "name" : "Ruben Vera-Rodriguez"
      },
      {
        "name" : "Ruben Tolosana"
      },
      {
        "name" : "Javier Ortega-Garcia"
      },
      {
        "name" : "Andres Huergo"
      },
      {
        "name" : "Julian Fierrez"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09882v1",
    "title" : "Location Privacy-Enabled Beamforming in ISAC Scenarios",
    "summary" : "Integrated sensing and communication (ISAC) technology enables simultaneous\nenvironmental perception and data transmission in wireless networks; however,\nit also exposes user location to receivers. In this paper, we introduce a novel\nbeamforming framework guided by the proposed privacy metric direction of\narrival obfuscation ratio (DAOR) to protect transmitter location privacy in\nISAC scenarios. Unlike previous approaches, we do not suppress the\nline-of-sight (LOS) component while reshaping the angular power distribution so\nthat a false direction appears dominant at the receiver. We derive closed-form\nbounds on the feasible DAOR via generalized eigenvalue analysis and formulate\nan achievable rate-maximization problem under the DAOR constraint. The\nresulting problem is non-convex, which is efficiently solved using semidefinite\nrelaxation, eigenmode selection, and optimal power allocation. A suboptimal\ndesign strategy is also proposed with reduced complexity. Numerical results\ndemonstrate that the proposed DAOR-based beamformer achieves a trade-off\nbetween location privacy and communication rate without nullifying the LOS\npath. Results also show that a suboptimal design achieves a near-optimal\ncommunication rate with nearly an 85% reduction in computation time at a\nsignal-to-noise ratio (SNR) of 10 dB.",
    "updated" : "2025-08-13T15:32:10Z",
    "published" : "2025-08-13T15:32:10Z",
    "authors" : [
      {
        "name" : "Umair Ali Khan"
      },
      {
        "name" : "Lester Ho"
      },
      {
        "name" : "Holger Claussen"
      },
      {
        "name" : "Chinmoy Kundu"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09442v1",
    "title" : "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference",
    "summary" : "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.",
    "updated" : "2025-08-13T02:48:25Z",
    "published" : "2025-08-13T02:48:25Z",
    "authors" : [
      {
        "name" : "Zhifan Luo"
      },
      {
        "name" : "Shuo Shao"
      },
      {
        "name" : "Su Zhang"
      },
      {
        "name" : "Lijing Zhou"
      },
      {
        "name" : "Yuke Hu"
      },
      {
        "name" : "Chenxu Zhao"
      },
      {
        "name" : "Zhihao Liu"
      },
      {
        "name" : "Zhan Qin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09426v1",
    "title" : "Security Analysis of ChatGPT: Threats and Privacy Risks",
    "summary" : "As artificial intelligence technology continues to advance, chatbots are\nbecoming increasingly powerful. Among them, ChatGPT, launched by OpenAI, has\ngarnered widespread attention globally due to its powerful natural language\nprocessing capabilities based on the GPT model, which enables it to engage in\nnatural conversations with users, understand various forms of linguistic\nexpressions, and generate useful information and suggestions. However, as its\napplication scope expands, user demand grows, and malicious attacks related to\nit become increasingly frequent, the security threats and privacy risks faced\nby ChatGPT are gradually coming to the forefront. In this paper, the security\nof ChatGPT is mainly studied from two aspects, security threats and privacy\nrisks. The article systematically analyzes various types of vulnerabilities\ninvolved in the above two types of problems and their causes. Briefly, we\ndiscuss the controversies that ChatGPT may cause at the ethical and moral\nlevels. In addition, this paper reproduces several network attack and defense\ntest scenarios by simulating the attacker's perspective and methodology.\nSimultaneously, it explores the feasibility of using ChatGPT for security\nvulnerability detection and security tool generation from the defender's\nperspective.",
    "updated" : "2025-08-13T02:03:18Z",
    "published" : "2025-08-13T02:03:18Z",
    "authors" : [
      {
        "name" : "Yushan Xiang"
      },
      {
        "name" : "Zhongwen Li"
      },
      {
        "name" : "Xiaoqi Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09245v1",
    "title" : "Beyond Blanket Masking: Examining Granularity for Privacy Protection in\n  Images Captured by Blind and Low Vision Users",
    "summary" : "As visual assistant systems powered by visual language models (VLMs) become\nmore prevalent, concerns over user privacy have grown, particularly for blind\nand low vision users who may unknowingly capture personal private information\nin their images. Existing privacy protection methods rely on coarse-grained\nsegmentation, which uniformly masks entire private objects, often at the cost\nof usability. In this work, we propose FiGPriv, a fine-grained privacy\nprotection framework that selectively masks only high-risk private information\nwhile preserving low-risk information. Our approach integrates fine-grained\nsegmentation with a data-driven risk scoring mechanism. We evaluate our\nframework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%\nof image content, enhancing the ability of VLMs to provide useful responses by\n11% and identify the image content by 45%, while ensuring privacy protection.\nProject Page: https://artcs1.github.io/VLMPrivacy/",
    "updated" : "2025-08-12T17:56:36Z",
    "published" : "2025-08-12T17:56:36Z",
    "authors" : [
      {
        "name" : "Jeffri Murrugarra-LLerena"
      },
      {
        "name" : "Haoran Niu"
      },
      {
        "name" : "K. Suzanne Barber"
      },
      {
        "name" : "Hal Daumé III"
      },
      {
        "name" : "Yang Trista Cao"
      },
      {
        "name" : "Paola Cascante-Bonilla"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.08749v2",
    "title" : "Approximate DBSCAN under Differential Privacy",
    "summary" : "This paper revisits the DBSCAN problem under differential privacy (DP).\nExisting DP-DBSCAN algorithms aim at publishing the cluster labels of the input\npoints. However, we show that both empirically and theoretically, this approach\ncannot offer any utility in the published results. We therefore propose an\nalternative definition of DP-DBSCAN based on the notion of spans. We argue that\npublishing the spans actually better serves the purposes of visualization and\nclassification of DBSCAN. Then we present a linear-time DP-DBSCAN algorithm\nachieving the sandwich quality guarantee in any constant dimensions, as well as\nmatching lower bounds on the approximation ratio. A key building block in our\nalgorithm is a linear-time algorithm for constructing a histogram under\npure-DP, which is of independent interest. Finally, we conducted experiments on\nboth synthetic and real-world datasets to verify the practical performance of\nour DP-DBSCAN algorithm.",
    "updated" : "2025-08-13T14:25:42Z",
    "published" : "2025-08-12T08:55:41Z",
    "authors" : [
      {
        "name" : "Yuan Qiu"
      },
      {
        "name" : "Ke Yi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09232v1",
    "title" : "PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research",
    "summary" : "Social media data presents AI researchers with overlapping obligations under\nthe GDPR, copyright law, and platform terms -- yet existing frameworks fail to\nintegrate these regulatory domains, leaving researchers without unified\nguidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and\nPresent), a compliance framework that embeds legal safeguards directly into\nextended ETL pipelines. Central to PETLP is treating Data Protection Impact\nAssessments as living documents that evolve from pre-registration through\ndissemination. Through systematic Reddit analysis, we demonstrate how\nextraction rights fundamentally differ between qualifying research\norganisations (who can invoke DSM Article 3 to override platform restrictions)\nand commercial entities (bound by terms of service), whilst GDPR obligations\napply universally. We reveal why true anonymisation remains unachievable for\nsocial media data and expose the legal gap between permitted dataset creation\nand uncertain model distribution. By structuring compliance decisions into\npractical workflows and simplifying institutional data management plans, PETLP\nenables researchers to navigate regulatory complexity with confidence, bridging\nthe gap between legal requirements and research practice.",
    "updated" : "2025-08-12T08:33:40Z",
    "published" : "2025-08-12T08:33:40Z",
    "authors" : [
      {
        "name" : "Nick Oh"
      },
      {
        "name" : "Giorgos D. Vrakas"
      },
      {
        "name" : "Siân J. M. Brooke"
      },
      {
        "name" : "Sasha Morinière"
      },
      {
        "name" : "Toju Duke"
      }
    ],
    "categories" : [
      "cs.MM",
      "cs.AI",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09186v1",
    "title" : "RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent\n  Transportation System",
    "summary" : "The proliferation of AI-powered cameras in Intelligent Transportation Systems\n(ITS) creates a severe conflict between the need for rich visual data and the\nfundamental right to privacy. Existing privacy-preserving mechanisms, such as\nblurring or encryption, are often insufficient, creating an undesirable\ntrade-off where either privacy is compromised against advanced reconstruction\nattacks or data utility is critically degraded. To resolve this impasse, we\npropose RL-MoE, a novel framework that transforms sensitive visual data into\nprivacy-preserving textual descriptions, eliminating the need for direct image\ntransmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture\nfor nuanced, multi-aspect scene decomposition with a Reinforcement Learning\n(RL) agent that optimizes the generated text for a dual objective of semantic\naccuracy and privacy preservation. Extensive experiments demonstrate that\nRL-MoE provides superior privacy protection, reducing the success rate of\nreplay attacks to just 9.4\\% on the CFP-FP dataset, while simultaneously\ngenerating richer textual content than baseline methods. Our work provides a\npractical and scalable solution for building trustworthy AI systems in\nprivacy-sensitive domains, paving the way for more secure smart city and\nautonomous vehicle networks.",
    "updated" : "2025-08-07T18:07:54Z",
    "published" : "2025-08-07T18:07:54Z",
    "authors" : [
      {
        "name" : "Abdolazim Rezaei"
      },
      {
        "name" : "Mehdi Sookhak"
      },
      {
        "name" : "Mahboobeh Haghparast"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10880v1",
    "title" : "Searching for Privacy Risks in LLM Agents via Simulation",
    "summary" : "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents.",
    "updated" : "2025-08-14T17:49:09Z",
    "published" : "2025-08-14T17:49:09Z",
    "authors" : [
      {
        "name" : "Yanzhe Zhang"
      },
      {
        "name" : "Diyi Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10737v1",
    "title" : "Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC\n  2025",
    "summary" : "This paper presents a summary of the 2025 Sclera Segmentation Benchmarking\nCompetition (SSBC), which focused on the development of privacy-preserving\nsclera-segmentation models trained using synthetically generated ocular images.\nThe goal of the competition was to evaluate how well models trained on\nsynthetic data perform in comparison to those trained on real-world datasets.\nThe competition featured two tracks: $(i)$ one relying solely on synthetic data\nfor model development, and $(ii)$ one combining/mixing synthetic with (a\nlimited amount of) real-world data. A total of nine research groups submitted\ndiverse segmentation models, employing a variety of architectural designs,\nincluding transformer-based solutions, lightweight models, and segmentation\nnetworks guided by generative frameworks. Experiments were conducted across\nthree evaluation datasets containing both synthetic and real-world images,\ncollected under diverse conditions. Results show that models trained entirely\non synthetic data can achieve competitive performance, particularly when\ndedicated training strategies are employed, as evidenced by the top performing\nmodels that achieved $F_1$ scores of over $0.8$ in the synthetic data track.\nMoreover, performance gains in the mixed track were often driven more by\nmethodological choices rather than by the inclusion of real data, highlighting\nthe promise of synthetic data for privacy-aware biometric development. The code\nand data for the competition is available at:\nhttps://github.com/dariant/SSBC_2025.",
    "updated" : "2025-08-14T15:16:58Z",
    "published" : "2025-08-14T15:16:58Z",
    "authors" : [
      {
        "name" : "Matej Vitek"
      },
      {
        "name" : "Darian Tomašević"
      },
      {
        "name" : "Abhijit Das"
      },
      {
        "name" : "Sabari Nathan"
      },
      {
        "name" : "Gökhan Özbulak"
      },
      {
        "name" : "Gözde Ayşe Tataroğlu Özbulak"
      },
      {
        "name" : "Jean-Paul Calbimonte"
      },
      {
        "name" : "André Anjos"
      },
      {
        "name" : "Hariohm Hemant Bhatt"
      },
      {
        "name" : "Dhruv Dhirendra Premani"
      },
      {
        "name" : "Jay Chaudhari"
      },
      {
        "name" : "Caiyong Wang"
      },
      {
        "name" : "Jian Jiang"
      },
      {
        "name" : "Chi Zhang"
      },
      {
        "name" : "Qi Zhang"
      },
      {
        "name" : "Iyyakutti Iyappan Ganapathi"
      },
      {
        "name" : "Syed Sadaf Ali"
      },
      {
        "name" : "Divya Velayudan"
      },
      {
        "name" : "Maregu Assefa"
      },
      {
        "name" : "Naoufel Werghi"
      },
      {
        "name" : "Zachary A. Daniels"
      },
      {
        "name" : "Leeon John"
      },
      {
        "name" : "Ritesh Vyas"
      },
      {
        "name" : "Jalil Nourmohammadi Khiarak"
      },
      {
        "name" : "Taher Akbari Saeed"
      },
      {
        "name" : "Mahsa Nasehi"
      },
      {
        "name" : "Ali Kianfar"
      },
      {
        "name" : "Mobina Pashazadeh Panahi"
      },
      {
        "name" : "Geetanjali Sharma"
      },
      {
        "name" : "Pushp Raj Panth"
      },
      {
        "name" : "Raghavendra Ramachandra"
      },
      {
        "name" : "Aditya Nigam"
      },
      {
        "name" : "Umapada Pal"
      },
      {
        "name" : "Peter Peer"
      },
      {
        "name" : "Vitomir Štruc"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10672v1",
    "title" : "Hybrid Generative Fusion for Efficient and Privacy-Preserving Face\n  Recognition Dataset Generation",
    "summary" : "In this paper, we present our approach to the DataCV ICCV Challenge, which\ncenters on building a high-quality face dataset to train a face recognition\nmodel. The constructed dataset must not contain identities overlapping with any\nexisting public face datasets. To handle this challenge, we begin with a\nthorough cleaning of the baseline HSFace dataset, identifying and removing\nmislabeled or inconsistent identities through a Mixture-of-Experts (MoE)\nstrategy combining face embedding clustering and GPT-4o-assisted verification.\nWe retain the largest consistent identity cluster and apply data augmentation\nup to a fixed number of images per identity. To further diversify the dataset,\nwe generate synthetic identities using Stable Diffusion with prompt\nengineering. As diffusion models are computationally intensive, we generate\nonly one reference image per identity and efficiently expand it using Vec2Face,\nwhich rapidly produces 49 identity-consistent variants. This hybrid approach\nfuses GAN-based and diffusion-based samples, enabling efficient construction of\na diverse and high-quality dataset. To address the high visual similarity among\nsynthetic identities, we adopt a curriculum learning strategy by placing them\nearly in the training schedule, allowing the model to progress from easier to\nharder samples. Our final dataset contains 50 images per identity, and all\nnewly generated identities are checked with mainstream face datasets to ensure\nno identity leakage. Our method achieves \\textbf{1st place} in the competition,\nand experimental results show that our dataset improves model performance\nacross 10K, 20K, and 100K identity scales. Code is available at\nhttps://github.com/Ferry-Li/datacv_fr.",
    "updated" : "2025-08-14T14:14:18Z",
    "published" : "2025-08-14T14:14:18Z",
    "authors" : [
      {
        "name" : "Feiran Li"
      },
      {
        "name" : "Qianqian Xu"
      },
      {
        "name" : "Shilong Bao"
      },
      {
        "name" : "Boyu Han"
      },
      {
        "name" : "Zhiyong Yang"
      },
      {
        "name" : "Qingming Huang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10482v1",
    "title" : "When Explainability Meets Privacy: An Investigation at the Intersection\n  of Post-hoc Explainability and Differential Privacy in the Context of Natural\n  Language Processing",
    "summary" : "In the study of trustworthy Natural Language Processing (NLP), a number of\nimportant research fields have emerged, including that of\n\\textit{explainability} and \\textit{privacy}. While research interest in both\nexplainable and privacy-preserving NLP has increased considerably in recent\nyears, there remains a lack of investigation at the intersection of the two.\nThis leaves a considerable gap in understanding of whether achieving\n\\textit{both} explainability and privacy is possible, or whether the two are at\nodds with each other. In this work, we conduct an empirical investigation into\nthe privacy-explainability trade-off in the context of NLP, guided by the\npopular overarching methods of \\textit{Differential Privacy} (DP) and Post-hoc\nExplainability. Our findings include a view into the intricate relationship\nbetween privacy and explainability, which is formed by a number of factors,\nincluding the nature of the downstream task and choice of the text\nprivatization and explainability method. In this, we highlight the potential\nfor privacy and explainability to co-exist, and we summarize our findings in a\ncollection of practical recommendations for future work at this important\nintersection.",
    "updated" : "2025-08-14T09:34:29Z",
    "published" : "2025-08-14T09:34:29Z",
    "authors" : [
      {
        "name" : "Mahdi Dhaini"
      },
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Ege Erdogan"
      },
      {
        "name" : "Florian Matthes"
      },
      {
        "name" : "Gjergji Kasneci"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10469v1",
    "title" : "Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human\n  Action Recognition",
    "summary" : "Human Action Recognition (HAR) plays a crucial role in healthcare, fitness\ntracking, and ambient assisted living technologies. While traditional vision\nbased HAR systems are effective, they pose privacy concerns. mmWave radar\nsensors offer a privacy preserving alternative but present challenges due to\nthe sparse and noisy nature of their point cloud data. In the literature, three\nprimary data processing methods: Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering\nhave been widely used to improve the quality and continuity of radar data.\nHowever, a comprehensive evaluation of these methods, both individually and in\ncombination, remains lacking. This paper addresses that gap by conducting a\ndetailed performance analysis of the three methods using the MiliPoint dataset.\nWe evaluate each method individually, all possible pairwise combinations, and\nthe combination of all three, assessing both recognition accuracy and\ncomputational cost. Furthermore, we propose targeted enhancements to the\nindividual methods aimed at improving accuracy. Our results provide crucial\ninsights into the strengths and trade-offs of each method and their\nintegrations, guiding future work on mmWave based HAR systems",
    "updated" : "2025-08-14T09:09:49Z",
    "published" : "2025-08-14T09:09:49Z",
    "authors" : [
      {
        "name" : "Maimunatu Tunau"
      },
      {
        "name" : "Vincent Gbouna Zakka"
      },
      {
        "name" : "Zhuangzhuang Dai"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10373v1",
    "title" : "Privacy-Preserving Approximate Nearest Neighbor Search on\n  High-Dimensional Data",
    "summary" : "In the era of cloud computing and AI, data owners outsource ubiquitous\nvectors to the cloud, which furnish approximate $k$-nearest neighbors\n($k$-ANNS) services to users. To protect data privacy against the untrusted\nserver, privacy-preserving $k$-ANNS (PP-ANNS) on vectors has been a fundamental\nand urgent problem. However, existing PP-ANNS solutions fall short of meeting\nthe requirements of data privacy, efficiency, accuracy, and minimal user\ninvolvement concurrently. To tackle this challenge, we introduce a novel\nsolution that primarily executes PP-ANNS on a single cloud server to avoid the\nheavy communication overhead between the cloud and the user. To ensure data\nprivacy, we introduce a novel encryption method named distance comparison\nencryption, facilitating secure, efficient, and exact distance comparisons. To\noptimize the trade-off between data privacy and search performance, we design a\nprivacy-preserving index that combines the state-of-the-art $k$-ANNS method\nwith an approximate distance computation method. Then, we devise a search\nmethod using a filter-and-refine strategy based on the index. Moreover, we\nprovide the security analysis of our solution and conduct extensive experiments\nto demonstrate its superiority over existing solutions. Based on our\nexperimental results, our method accelerates PP-ANNS by up to 3 orders of\nmagnitude compared to state-of-the-art methods, while not compromising the\naccuracy.",
    "updated" : "2025-08-14T06:09:38Z",
    "published" : "2025-08-14T06:09:38Z",
    "authors" : [
      {
        "name" : "Yingfan Liu"
      },
      {
        "name" : "Yandi Zhang"
      },
      {
        "name" : "Jiadong Xie"
      },
      {
        "name" : "Hui Li"
      },
      {
        "name" : "Jeffrey Xu Yu"
      },
      {
        "name" : "Jiangtao Cui"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.11575v1",
    "title" : "Activate Me!: Designing Efficient Activation Functions for\n  Privacy-Preserving Machine Learning with Fully Homomorphic Encryption",
    "summary" : "The growing adoption of machine learning in sensitive areas such as\nhealthcare and defense introduces significant privacy and security challenges.\nThese domains demand robust data protection, as models depend on large volumes\nof sensitive information for both training and inference. Fully Homomorphic\nEncryption (FHE) presents a compelling solution by enabling computations\ndirectly on encrypted data, maintaining confidentiality across the entire\nmachine learning workflow. However, FHE inherently supports only linear\noperations, making it difficult to implement non-linear activation functions,\nessential components of modern neural networks. This work focuses on designing,\nimplementing, and evaluating activation functions tailored for FHE-based\nmachine learning. We investigate two commonly used functions: the Square\nfunction and Rectified Linear Unit (ReLU), using LeNet-5 and ResNet-20\narchitectures with the CKKS scheme from the OpenFHE library. For ReLU, we\nassess two methods: a conventional low-degree polynomial approximation and a\nnovel scheme-switching technique that securely evaluates ReLU under FHE\nconstraints. Our findings show that the Square function performs well in\nshallow networks like LeNet-5, achieving 99.4% accuracy with 128 seconds per\nimage. In contrast, deeper models like ResNet-20 benefit more from ReLU. The\npolynomial approximation yields 83.8% accuracy with 1,145 seconds per image,\nwhile our scheme-switching method improves accuracy to 89.8%, albeit with a\nlonger inference time of 1,697 seconds. These results underscore a critical\ntrade-off in FHE-based ML: faster activation functions often reduce accuracy,\nwhereas those preserving accuracy demand greater computational resources.",
    "updated" : "2025-08-15T16:31:12Z",
    "published" : "2025-08-15T16:31:12Z",
    "authors" : [
      {
        "name" : "Nges Brian Njungle"
      },
      {
        "name" : "Michel A. Kinsy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.11495v1",
    "title" : "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation",
    "summary" : "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators.",
    "updated" : "2025-08-15T14:17:24Z",
    "published" : "2025-08-15T14:17:24Z",
    "authors" : [
      {
        "name" : "Jingnan Xu"
      },
      {
        "name" : "Leixia Wang"
      },
      {
        "name" : "Xiaofeng Meng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.11419v1",
    "title" : "Training-free Dimensionality Reduction via Feature Truncation: Enhancing\n  Efficiency in Privacy-preserving Multi-Biometric Systems",
    "summary" : "Biometric recognition is widely used, making the privacy and security of\nextracted templates a critical concern. Biometric Template Protection schemes,\nespecially those utilizing Homomorphic Encryption, introduce significant\ncomputational challenges due to increased workload. Recent advances in deep\nneural networks have enabled state-of-the-art feature extraction for face,\nfingerprint, and iris modalities. The ubiquity and affordability of biometric\nsensors further facilitate multi-modal fusion, which can enhance security by\ncombining features from different modalities. This work investigates the\nbiometric performance of reduced multi-biometric template sizes. Experiments\nare conducted on an in-house virtual multi-biometric database, derived from\nDNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,\nand CASIA databases. The evaluated approaches are (i) explainable and\nstraightforward to implement under encryption, (ii) training-free, and (iii)\ncapable of generalization. Dimensionality reduction of feature vectors leads to\nfewer operations in the Homomorphic Encryption (HE) domain, enabling more\nefficient encrypted processing while maintaining biometric accuracy and\nsecurity at a level equivalent to or exceeding single-biometric recognition.\nOur results demonstrate that, by fusing feature vectors from multiple\nmodalities, template size can be reduced by 67 % with no loss in Equal Error\nRate (EER) compared to the best-performing single modality.",
    "updated" : "2025-08-15T11:49:19Z",
    "published" : "2025-08-15T11:49:19Z",
    "authors" : [
      {
        "name" : "Florian Bayer"
      },
      {
        "name" : "Maximilian Russo"
      },
      {
        "name" : "Christian Rathgeb"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.11115v1",
    "title" : "UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous\n  Ergonomic Sitting Posture Monitoring",
    "summary" : "Improper sitting posture during prolonged computer use has become a\nsignificant public health concern. Traditional posture monitoring solutions\nface substantial barriers, including privacy concerns with camera-based systems\nand user discomfort with wearable sensors. This paper presents\nUWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that\nadvances mobile technologies for preventive health management through\ncontinuous, contactless monitoring of ergonomic sitting posture. Our system\nleverages commercial UWB devices, utilizing comprehensive feature engineering\nto extract multiple ergonomic sitting posture features. We develop PoseGBDT to\neffectively capture temporal dependencies in posture patterns, addressing\nlimitations of traditional frame-wise classification approaches. Extensive\nreal-world evaluation across 10 participants and 19 distinct postures\ndemonstrates exceptional performance, achieving 99.11% accuracy while\nmaintaining robustness against environmental variables such as clothing\nthickness, additional devices, and furniture configurations. Our system\nprovides a scalable, privacy-preserving mobile health solution on existing\nplatforms for proactive ergonomic management, improving quality of life at low\ncosts.",
    "updated" : "2025-08-14T23:40:37Z",
    "published" : "2025-08-14T23:40:37Z",
    "authors" : [
      {
        "name" : "Haotang Li"
      },
      {
        "name" : "Zhenyu Qi"
      },
      {
        "name" : "Sen He"
      },
      {
        "name" : "Kebin Peng"
      },
      {
        "name" : "Sheng Tan"
      },
      {
        "name" : "Yili Ren"
      },
      {
        "name" : "Tomas Cerny"
      },
      {
        "name" : "Jiyue Zhao"
      },
      {
        "name" : "Zi Wang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.HC",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10482v2",
    "title" : "When Explainability Meets Privacy: An Investigation at the Intersection\n  of Post-hoc Explainability and Differential Privacy in the Context of Natural\n  Language Processing",
    "summary" : "In the study of trustworthy Natural Language Processing (NLP), a number of\nimportant research fields have emerged, including that of explainability and\nprivacy. While research interest in both explainable and privacy-preserving NLP\nhas increased considerably in recent years, there remains a lack of\ninvestigation at the intersection of the two. This leaves a considerable gap in\nunderstanding of whether achieving both explainability and privacy is possible,\nor whether the two are at odds with each other. In this work, we conduct an\nempirical investigation into the privacy-explainability trade-off in the\ncontext of NLP, guided by the popular overarching methods of Differential\nPrivacy (DP) and Post-hoc Explainability. Our findings include a view into the\nintricate relationship between privacy and explainability, which is formed by a\nnumber of factors, including the nature of the downstream task and choice of\nthe text privatization and explainability method. In this, we highlight the\npotential for privacy and explainability to co-exist, and we summarize our\nfindings in a collection of practical recommendations for future work at this\nimportant intersection.",
    "updated" : "2025-08-15T13:25:21Z",
    "published" : "2025-08-14T09:34:29Z",
    "authors" : [
      {
        "name" : "Mahdi Dhaini"
      },
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Ege Erdogan"
      },
      {
        "name" : "Florian Matthes"
      },
      {
        "name" : "Gjergji Kasneci"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09186v2",
    "title" : "RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent\n  Transportation System",
    "summary" : "The proliferation of AI-powered cameras in Intelligent Transportation Systems\n(ITS) creates a severe conflict between the need for rich visual data and the\nright to privacy. Existing privacy-preserving methods, such as blurring or\nencryption, are often insufficient due to creating an undesirable trade-off\nwhere either privacy is compromised against advanced reconstruction attacks or\ndata utility is critically degraded. To resolve this challenge, we propose\nRL-MoE, a novel framework that transforms sensitive visual data into\nprivacy-preserving textual descriptions, eliminating the need for direct image\ntransmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture\nfor nuanced, multi-aspect scene decomposition with a Reinforcement Learning\n(RL) agent that optimizes the generated text for a dual objective of semantic\naccuracy and privacy preservation. Extensive experiments demonstrate that\nRL-MoE provides superior privacy protection, reducing the success rate of\nreplay attacks to just 9.4\\% on the CFP-FP dataset, while simultaneously\ngenerating richer textual content than baseline methods. Our work provides a\npractical and scalable solution for building trustworthy AI systems in\nprivacy-sensitive domains, paving the way for more secure smart city and\nautonomous vehicle networks.",
    "updated" : "2025-08-15T04:36:03Z",
    "published" : "2025-08-07T18:07:54Z",
    "authors" : [
      {
        "name" : "Abdolazim Rezaei"
      },
      {
        "name" : "Mehdi Sookhak"
      },
      {
        "name" : "Mahboobeh Haghparast"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10918v1",
    "title" : "Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder",
    "summary" : "We present a privacy-enhancing mechanism for gaze signals using a\nlatent-noise autoencoder that prevents users from being re-identified across\nplay sessions without their consent, while retaining the usability of the data\nfor benign tasks. We evaluate privacy-utility trade-offs across biometric\nidentification and gaze prediction tasks, showing that our approach\nsignificantly reduces biometric identifiability with minimal utility\ndegradation. Unlike prior methods in this direction, our framework retains\nphysiologically plausible gaze patterns suitable for downstream use, which\nproduces favorable privacy-utility trade-off. This work advances privacy in\ngaze-based systems by providing a usable and effective mechanism for protecting\nsensitive gaze data.",
    "updated" : "2025-08-01T20:46:52Z",
    "published" : "2025-08-01T20:46:52Z",
    "authors" : [
      {
        "name" : "Samantha Aziz"
      },
      {
        "name" : "Oleg Komogortsev"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.12832v1",
    "title" : "Efficient and Verifiable Privacy-Preserving Convolutional Computation\n  for CNN Inference with Untrusted Clouds",
    "summary" : "The widespread adoption of convolutional neural networks (CNNs) in\nresource-constrained scenarios has driven the development of Machine Learning\nas a Service (MLaaS) system. However, this approach is susceptible to privacy\nleakage, as the data sent from the client to the untrusted cloud server often\ncontains sensitive information. Existing CNN privacy-preserving schemes, while\neffective in ensuring data confidentiality through homomorphic encryption and\nsecret sharing, face efficiency bottlenecks, particularly in convolution\noperations. In this paper, we propose a novel verifiable privacy-preserving\nscheme tailored for CNN convolutional layers. Our scheme enables efficient\nencryption and decryption, allowing resource-constrained clients to securely\noffload computations to the untrusted cloud server. Additionally, we present a\nverification mechanism capable of detecting the correctness of the results with\na success probability of at least $1-\\frac{1}{\\left|Z\\right|}$. Extensive\nexperiments conducted on 10 datasets and various CNN models demonstrate that\nour scheme achieves speedups ranging $26 \\times$ ~ $\\ 87\\times$ compared to the\noriginal plaintext model while maintaining accuracy.",
    "updated" : "2025-08-18T11:17:53Z",
    "published" : "2025-08-18T11:17:53Z",
    "authors" : [
      {
        "name" : "Jinyu Lu"
      },
      {
        "name" : "Xinrong Sun"
      },
      {
        "name" : "Yunting Tao"
      },
      {
        "name" : "Tong Ji"
      },
      {
        "name" : "Fanyu Kong"
      },
      {
        "name" : "Guoqiang Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.13914v1",
    "title" : "Development of a defacing algorithm to protect the privacy of head and\n  neck cancer patients in publicly-accessible radiotherapy datasets",
    "summary" : "Introduction: The rise in public medical imaging datasets has raised concerns\nabout patient reidentification from head CT scans. However, existing defacing\nalgorithms often remove or distort Organs at Risk (OARs) and Planning Target\nVolumes (PTVs) in head and neck cancer (HNC) patients, and ignore DICOM-RT\nStructure Set and Dose data. Therefore, we developed and validated a novel\nautomated defacing algorithm that preserves these critical structures while\nremoving identifiable features from HNC CTs and DICOM-RT data.\n  Methods: Eye contours were used as landmarks to automate the removal of CT\npixels above the inferior-most eye slice and anterior to the eye midpoint.\nPixels within PTVs were retained if they intersected with the removed region.\nThe body contour and dose map were reshaped to reflect the defaced image. We\nvalidated our approach on 829 HNC CTs from 622 patients. Privacy protection was\nevaluated by applying the FaceNet512 facial recognition algorithm before and\nafter defacing on 3D-rendered CT pairs from 70 patients. Research utility was\nassessed by examining the impact of defacing on autocontouring performance\nusing LimbusAI and analyzing PTV locations relative to the defaced regions.\n  Results: Before defacing, FaceNet512 matched 97% of patients' CTs. After\ndefacing, this rate dropped to 4%. LimbusAI effectively autocontoured organs in\nthe defaced CTs, with perfect Dice scores of 1 for OARs below the defaced\nregion, and excellent scores exceeding 0.95 for OARs on the same slices as the\ncrop. We found that 86% of PTVs were entirely below the cropped region, 9.1%\nwere on the same slice as the crop without overlap, and only 4.9% extended into\nthe cropped area.\n  Conclusions: We developed a novel defacing algorithm that anonymizes HNC CT\nscans and related DICOM-RT data while preserving essential structures, enabling\nthe sharing of HNC imaging datasets for Big Data and AI.",
    "updated" : "2025-08-19T15:14:16Z",
    "published" : "2025-08-19T15:14:16Z",
    "authors" : [
      {
        "name" : "Kayla O'Sullivan-Steben"
      },
      {
        "name" : "Luc Galarneau"
      },
      {
        "name" : "John Kildea"
      }
    ],
    "categories" : [
      "physics.med-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.13730v1",
    "title" : "On the Security and Privacy of Federated Learning: A Survey with\n  Attacks, Defenses, Frameworks, Applications, and Future Directions",
    "summary" : "Federated Learning (FL) is an emerging distributed machine learning paradigm\nenabling multiple clients to train a global model collaboratively without\nsharing their raw data. While FL enhances data privacy by design, it remains\nvulnerable to various security and privacy threats. This survey provides a\ncomprehensive overview of more than 200 papers regarding the state-of-the-art\nattacks and defense mechanisms developed to address these challenges,\ncategorizing them into security-enhancing and privacy-preserving techniques.\nSecurity-enhancing methods aim to improve FL robustness against malicious\nbehaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same\ntime, privacy-preserving techniques focus on protecting sensitive data through\ncryptographic approaches, differential privacy, and secure aggregation. We\ncritically analyze the strengths and limitations of existing methods, highlight\nthe trade-offs between privacy, security, and model performance, and discuss\nthe implications of non-IID data distributions on the effectiveness of these\ndefenses. Furthermore, we identify open research challenges and future\ndirections, including the need for scalable, adaptive, and energy-efficient\nsolutions operating in dynamic and heterogeneous FL environments. Our survey\naims to guide researchers and practitioners in developing robust and\nprivacy-preserving FL systems, fostering advancements safeguarding\ncollaborative learning frameworks' integrity and confidentiality.",
    "updated" : "2025-08-19T11:06:20Z",
    "published" : "2025-08-19T11:06:20Z",
    "authors" : [
      {
        "name" : "Daniel M. Jimenez-Gutierrez"
      },
      {
        "name" : "Yelizaveta Falkouskaya"
      },
      {
        "name" : "Jose L. Hernandez-Ramos"
      },
      {
        "name" : "Aris Anagnostopoulos"
      },
      {
        "name" : "Ioannis Chatzigiannakis"
      },
      {
        "name" : "Andrea Vitaletti"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.13425v1",
    "title" : "When Secure Aggregation Falls Short: Achieving Long-Term Privacy in\n  Asynchronous Federated Learning for LEO Satellite Networks",
    "summary" : "Secure aggregation is a common technique in federated learning (FL) for\nprotecting data privacy from both curious internal entities (clients or server)\nand external adversaries (eavesdroppers). However, in dynamic and\nresource-constrained environments such as low Earth orbit (LEO) satellite\nnetworks, traditional secure aggregation methods fall short in two aspects: (1)\nthey assume continuous client availability while LEO satellite visibility is\nintermittent and irregular; (2) they consider privacy in each communication\nround but have overlooked the possible privacy leakage through multiple rounds.\nTo address these limitations, we propose LTP-FLEO, an asynchronous FL framework\nthat preserves long-term privacy (LTP) for LEO satellite networks. LTP-FLEO\nintroduces (i) privacy-aware satellite partitioning, which groups satellites\nbased on their predictable visibility to the server and enforces joint\nparticipation; (ii) model age balancing, which mitigates the adverse impact of\nstale model updates; and (iii) fair global aggregation, which treats satellites\nof different visibility durations in an equitable manner. Theoretical analysis\nand empirical validation demonstrate that LTP-FLEO effectively safeguards both\nmodel and data privacy across multi-round training, promotes fairness in line\nwith satellite contributions, accelerates global convergence, and achieves\ncompetitive model accuracy.",
    "updated" : "2025-08-19T00:55:04Z",
    "published" : "2025-08-19T00:55:04Z",
    "authors" : [
      {
        "name" : "Mohamed Elmahallawy"
      },
      {
        "name" : "Tie Luo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.12832v2",
    "title" : "Efficient and Verifiable Privacy-Preserving Convolutional Computation\n  for CNN Inference with Untrusted Clouds",
    "summary" : "The widespread adoption of convolutional neural networks (CNNs) in\nresource-constrained scenarios has driven the development of Machine Learning\nas a Service (MLaaS) system. However, this approach is susceptible to privacy\nleakage, as the data sent from the client to the untrusted cloud server often\ncontains sensitive information. Existing CNN privacy-preserving schemes, while\neffective in ensuring data confidentiality through homomorphic encryption and\nsecret sharing, face efficiency bottlenecks, particularly in convolution\noperations. In this paper, we propose a novel verifiable privacy-preserving\nscheme tailored for CNN convolutional layers. Our scheme enables efficient\nencryption and decryption, allowing resource-constrained clients to securely\noffload computations to the untrusted cloud server. Additionally, we present a\nverification mechanism capable of detecting the correctness of the results with\na success probability of at least $1-\\frac{1}{\\left|Z\\right|}$. Extensive\nexperiments conducted on 10 datasets and various CNN models demonstrate that\nour scheme achieves speedups ranging $26 \\times$ ~ $\\ 87\\times$ compared to the\noriginal plaintext model while maintaining accuracy.",
    "updated" : "2025-08-19T07:03:27Z",
    "published" : "2025-08-18T11:17:53Z",
    "authors" : [
      {
        "name" : "Jinyu Lu"
      },
      {
        "name" : "Xinrong Sun"
      },
      {
        "name" : "Yunting Tao"
      },
      {
        "name" : "Tong Ji"
      },
      {
        "name" : "Fanyu Kong"
      },
      {
        "name" : "Guoqiang Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.12539v1",
    "title" : "The Hidden Cost of Correlation: Rethinking Privacy Leakage in Local\n  Differential Privacy",
    "summary" : "Local differential privacy (LDP) has emerged as a promising paradigm for\nprivacy-preserving data collection in distributed systems, where users\ncontribute multi-dimensional records with potentially correlated attributes.\nRecent work has highlighted that correlation-induced privacy leakage (CPL)\nplays a critical role in shaping the privacy-utility trade-off under LDP,\nespecially when correlations exist among attributes. Nevertheless, it remains\nunclear to what extent the prevailing assumptions and proposed solutions are\nvalid and how significant CPL is in real-world data. To address this gap, we\nfirst perform a comprehensive statistical analysis of five widely used LDP\nmechanisms -- GRR, RAPPOR, OUE, OLH and Exponential mechanism -- to assess CPL\nacross four real-world datasets. We identify that many primary assumptions and\nmetrics in current approaches fall short of accurately characterising these\nleakages. Moreover, current studies have been limited to a set of pure LDP\n(i.e., {\\delta = 0}) mechanisms. In response, we develop the first algorithmic\nframework to theoretically quantify CPL for any general approximated LDP\n(({\\varepsilon},{\\delta})-LDP) mechanism. We validate our theoretical results\nagainst empirical statistical results and provide a theoretical explanation for\nthe observed statistical patterns. Finally, we propose two novel benchmarks to\nvalidate correlation analysis algorithms and evaluate the utility vs CPL of LDP\nmechanisms. Further, we demonstrate how these findings can be applied to\nachieve an efficient privacy-utility trade-off in real-world data governance.",
    "updated" : "2025-08-18T00:34:04Z",
    "published" : "2025-08-18T00:34:04Z",
    "authors" : [
      {
        "name" : "Sandaru Jayawardana"
      },
      {
        "name" : "Sennur Ulukus"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Kanchana Thilakarathna"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.12158v1",
    "title" : "LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human\n  and LLM Perceptions of Privacy in Textual Data",
    "summary" : "Despite advances in the field of privacy-preserving Natural Language\nProcessing (NLP), a significant challenge remains the accurate evaluation of\nprivacy. As a potential solution, using LLMs as a privacy evaluator presents a\npromising approach $\\unicode{x2013}$ a strategy inspired by its success in\nother subfields of NLP. In particular, the so-called $\\textit{LLM-as-a-Judge}$\nparadigm has achieved impressive results on a variety of natural language\nevaluation tasks, demonstrating high agreement rates with human annotators.\nRecognizing that privacy is both subjective and difficult to define, we\ninvestigate whether LLM-as-a-Judge can also be leveraged to evaluate the\nprivacy sensitivity of textual data. Furthermore, we measure how closely LLM\nevaluations align with human perceptions of privacy in text. Resulting from a\nstudy involving 10 datasets, 13 LLMs, and 677 human survey participants, we\nconfirm that privacy is indeed a difficult concept to measure empirically,\nexhibited by generally low inter-human agreement rates. Nevertheless, we find\nthat LLMs can accurately model a global human privacy perspective, and through\nan analysis of human and LLM reasoning patterns, we discuss the merits and\nlimitations of LLM-as-a-Judge for privacy evaluation in textual data. Our\nfindings pave the way for exploring the feasibility of LLMs as privacy\nevaluators, addressing a core challenge in solving pressing privacy issues with\ninnovative technical solutions.",
    "updated" : "2025-08-16T20:49:41Z",
    "published" : "2025-08-16T20:49:41Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.12093v1",
    "title" : "PP-STAT: An Efficient Privacy-Preserving Statistical Analysis Framework\n  using Homomorphic Encryption",
    "summary" : "With the widespread adoption of cloud computing, the need for outsourcing\nstatistical analysis to third-party platforms is growing rapidly. However,\nhandling sensitive data such as medical records and financial information in\ncloud environments raises serious privacy concerns. In this paper, we present\nPP-STAT, a novel and efficient Homomorphic Encryption (HE)-based framework for\nprivacy-preserving statistical analysis. HE enables computations to be\nperformed directly on encrypted data without revealing the underlying\nplaintext. PP-STAT supports advanced statistical measures, including Z-score\nnormalization, skewness, kurtosis, coefficient of variation, and Pearson\ncorrelation coefficient, all computed securely over encrypted data. To improve\nefficiency, PP-STAT introduces two key optimizations: (1) a Chebyshev-based\napproximation strategy for initializing inverse square root operations, and (2)\na pre-normalization scaling technique that reduces multiplicative depth by\nfolding constant scaling factors into mean and variance computations. These\ntechniques significantly lower computational overhead and minimize the number\nof expensive bootstrapping procedures. Our evaluation on real-world datasets\ndemonstrates that PP-STAT achieves high numerical accuracy, with mean relative\nerror (MRE) below 2.4x10-4. Notably, the encrypted Pearson correlation between\nthe smoker attribute and charges reaches 0.7873, with an MRE of 2.86x10-4.\nThese results confirm the practical utility of PP-STAT for secure and precise\nstatistical analysis in privacy-sensitive domains.",
    "updated" : "2025-08-16T16:24:35Z",
    "published" : "2025-08-16T16:24:35Z",
    "authors" : [
      {
        "name" : "Hyunmin Choi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.11907v1",
    "title" : "Deciphering the Interplay between Attack and Protection Complexity in\n  Privacy-Preserving Federated Learning",
    "summary" : "Federated learning (FL) offers a promising paradigm for collaborative model\ntraining while preserving data privacy. However, its susceptibility to gradient\ninversion attacks poses a significant challenge, necessitating robust privacy\nprotection mechanisms. This paper introduces a novel theoretical framework to\ndecipher the intricate interplay between attack and protection complexities in\nprivacy-preserving FL. We formally define \"Attack Complexity\" as the minimum\ncomputational and data resources an adversary requires to reconstruct private\ndata below a given error threshold, and \"Protection Complexity\" as the expected\ndistortion introduced by privacy mechanisms. Leveraging Maximum Bayesian\nPrivacy (MBP), we derive tight theoretical bounds for protection complexity,\ndemonstrating its scaling with model dimensionality and privacy budget.\nFurthermore, we establish comprehensive bounds for attack complexity, revealing\nits dependence on privacy leakage, gradient distortion, model dimension, and\nthe chosen privacy level. Our findings quantitatively illuminate the\nfundamental trade-offs between privacy guarantees, system utility, and the\neffort required for both attacking and defending. This framework provides\ncritical insights for designing more secure and efficient federated learning\nsystems.",
    "updated" : "2025-08-16T04:39:16Z",
    "published" : "2025-08-16T04:39:16Z",
    "authors" : [
      {
        "name" : "Xiaojin Zhang"
      },
      {
        "name" : "Mingcong Xu"
      },
      {
        "name" : "Yiming Li"
      },
      {
        "name" : "Wei Chen"
      },
      {
        "name" : "Qiang Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.11797v1",
    "title" : "AegisBlock: A Privacy-Preserving Medical Research Framework using\n  Blockchain",
    "summary" : "Due to HIPAA and other privacy regulations, it is imperative to maintain\npatient privacy while conducting research on patient health records. In this\npaper, we propose AegisBlock, a patient-centric access controlled framework to\nshare medical records with researchers such that the anonymity of the patient\nis maintained while ensuring the trustworthiness of the data provided to\nresearchers. AegisBlock allows for patients to provide access to their medical\ndata, verified by miners. A researcher submits a time-based range query to\nrequest access to records from a certain patient, and upon patient approval,\naccess will be granted. Our experimental evaluation results show that\nAegisBlock is scalable with respect to the number of patients and hospitals in\nthe system, and efficient with up to 50% of malicious miners.",
    "updated" : "2025-08-15T20:43:36Z",
    "published" : "2025-08-15T20:43:36Z",
    "authors" : [
      {
        "name" : "Calkin Garg"
      },
      {
        "name" : "Omar Rios Cruz"
      },
      {
        "name" : "Tessa Andersen"
      },
      {
        "name" : "Gaby G. Dagher"
      },
      {
        "name" : "Donald Winiecki"
      },
      {
        "name" : "Min Long"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.11742v1",
    "title" : "Assessing User Privacy Leakage in Synthetic Packet Traces: An\n  Attack-Grounded Approach",
    "summary" : "Current synthetic traffic generators (SynNetGens) promise privacy but lack\ncomprehensive guarantees or empirical validation, even as their fidelity\nsteadily improves. We introduce the first attack-grounded benchmark for\nassessing the privacy of SynNetGens directly from the traffic they produce. We\nframe privacy as membership inference at the traffic-source level--a realistic\nand actionable threat for data holders. To this end, we present TraceBleed, the\nfirst attack that exploits behavioral fingerprints across flows using\ncontrastive learning and temporal chunking, outperforming prior membership\ninference baselines by 172%. Our large-scale study across GAN-, diffusion-, and\nGPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-level\ninformation; (ii) differential privacy either fails to stop these attacks or\nseverely degrades fidelity; and (iii) sharing more synthetic data amplifies\nleakage by 59% on average. Finally, we introduce TracePatch, the first\nSynNetGen-agnostic defense that combines adversarial ML with SMT constraints to\nmitigate leakage while preserving fidelity.",
    "updated" : "2025-08-15T17:54:27Z",
    "published" : "2025-08-15T17:54:27Z",
    "authors" : [
      {
        "name" : "Minhao Jin"
      },
      {
        "name" : "Hongyu He"
      },
      {
        "name" : "Maria Apostolaki"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.11716v1",
    "title" : "Privacy-Aware Detection of Fake Identity Documents: Methodology,\n  Benchmark, and Improved Detection Methods (FakeIDet2)",
    "summary" : "Remote user verification in Internet-based applications is becoming\nincreasingly important nowadays. A popular scenario for it consists of\nsubmitting a picture of the user's Identity Document (ID) to a service\nplatform, authenticating its veracity, and then granting access to the\nrequested digital service. An ID is well-suited to verify the identity of an\nindividual, since it is government issued, unique, and nontransferable.\nHowever, with recent advances in Artificial Intelligence (AI), attackers can\nsurpass security measures in IDs and create very realistic physical and\nsynthetic fake IDs. Researchers are now trying to develop methods to detect an\never-growing number of these AI-based fakes that are almost indistinguishable\nfrom authentic (bona fide) IDs. In this counterattack effort, researchers are\nfaced with an important challenge: the difficulty in using real data to train\nfake ID detectors. This real data scarcity for research and development is\noriginated by the sensitive nature of these documents, which are usually kept\nprivate by the ID owners (the users) and the ID Holders (e.g., government,\npolice, bank, etc.). The main contributions of our study are: 1) We propose and\ndiscuss a patch-based methodology to preserve privacy in fake ID detection\nresearch. 2) We provide a new public database, FakeIDet2-db, comprising over\n900K real/fake ID patches extracted from 2,000 ID images, acquired using\ndifferent smartphone sensors, illumination and height conditions, etc. In\naddition, three physical attacks are considered: print, screen, and composite.\n3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We\nrelease a standard reproducible benchmark that considers physical and synthetic\nattacks from popular databases in the literature.",
    "updated" : "2025-08-14T17:30:36Z",
    "published" : "2025-08-14T17:30:36Z",
    "authors" : [
      {
        "name" : "Javier Muñoz-Haro"
      },
      {
        "name" : "Ruben Tolosana"
      },
      {
        "name" : "Ruben Vera-Rodriguez"
      },
      {
        "name" : "Aythami Morales"
      },
      {
        "name" : "Julian Fierrez"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10672v2",
    "title" : "Hybrid Generative Fusion for Efficient and Privacy-Preserving Face\n  Recognition Dataset Generation",
    "summary" : "In this paper, we present our approach to the DataCV ICCV Challenge, which\ncenters on building a high-quality face dataset to train a face recognition\nmodel. The constructed dataset must not contain identities overlapping with any\nexisting public face datasets. To handle this challenge, we begin with a\nthorough cleaning of the baseline HSFace dataset, identifying and removing\nmislabeled or inconsistent identities through a Mixture-of-Experts (MoE)\nstrategy combining face embedding clustering and GPT-4o-assisted verification.\nWe retain the largest consistent identity cluster and apply data augmentation\nup to a fixed number of images per identity. To further diversify the dataset,\nwe generate synthetic identities using Stable Diffusion with prompt\nengineering. As diffusion models are computationally intensive, we generate\nonly one reference image per identity and efficiently expand it using Vec2Face,\nwhich rapidly produces 49 identity-consistent variants. This hybrid approach\nfuses GAN-based and diffusion-based samples, enabling efficient construction of\na diverse and high-quality dataset. To address the high visual similarity among\nsynthetic identities, we adopt a curriculum learning strategy by placing them\nearly in the training schedule, allowing the model to progress from easier to\nharder samples. Our final dataset contains 50 images per identity, and all\nnewly generated identities are checked with mainstream face datasets to ensure\nno identity leakage. Our method achieves \\textbf{1st place} in the competition,\nand experimental results show that our dataset improves model performance\nacross 10K, 20K, and 100K identity scales. Code is available at\nhttps://github.com/Ferry-Li/datacv_fr.",
    "updated" : "2025-08-18T09:15:35Z",
    "published" : "2025-08-14T14:14:18Z",
    "authors" : [
      {
        "name" : "Feiran Li"
      },
      {
        "name" : "Qianqian Xu"
      },
      {
        "name" : "Shilong Bao"
      },
      {
        "name" : "Boyu Han"
      },
      {
        "name" : "Zhiyong Yang"
      },
      {
        "name" : "Qingming Huang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.00287v2",
    "title" : "Privacy-Preserving Driver Drowsiness Detection with Spatial\n  Self-Attention and Federated Learning",
    "summary" : "Driver drowsiness is one of the main causes of road accidents and is\nrecognized as a leading contributor to traffic-related fatalities. However,\ndetecting drowsiness accurately remains a challenging task, especially in\nreal-world settings where facial data from different individuals is\ndecentralized and highly diverse. In this paper, we propose a novel framework\nfor drowsiness detection that is designed to work effectively with\nheterogeneous and decentralized data. Our approach develops a new Spatial\nSelf-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)\nnetwork to better extract key facial features and improve detection\nperformance. To support federated learning, we employ a Gradient Similarity\nComparison (GSC) that selects the most relevant trained models from different\noperators before aggregation. This improves the accuracy and robustness of the\nglobal model while preserving user privacy. We also develop a customized tool\nthat automatically processes video data by extracting frames, detecting and\ncropping faces, and applying data augmentation techniques such as rotation,\nflipping, brightness adjustment, and zooming. Experimental results show that\nour framework achieves a detection accuracy of 89.9% in the federated learning\nsettings, outperforming existing methods under various deployment scenarios.\nThe results demonstrate the effectiveness of our approach in handling\nreal-world data variability and highlight its potential for deployment in\nintelligent transportation systems to enhance road safety through early and\nreliable drowsiness detection.",
    "updated" : "2025-08-18T02:30:55Z",
    "published" : "2025-08-01T03:12:01Z",
    "authors" : [
      {
        "name" : "Tran Viet Khoa"
      },
      {
        "name" : "Do Hai Son"
      },
      {
        "name" : "Mohammad Abu Alsheikh"
      },
      {
        "name" : "Yibeltal F Alem"
      },
      {
        "name" : "Dinh Thai Hoang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.14815v1",
    "title" : "A Lightweight Privacy-Preserving Smart Metering Billing Protocol with\n  Dynamic Tariff Policy Adjustment",
    "summary" : "The integration of information and communication technology (ICT) with\ntraditional power grids has led to the emergence of smart grids. Advanced\nmetering infrastructure (AMI) plays a crucial role in smart grids by\nfacilitating two-way communication between smart meters and the utility\nprovider. This bidirectional communication allows intelligent meters to report\nfine-grained consumption data at predefined intervals, enabling accurate\nbilling, efficient grid monitoring and management, and rapid outage detection.\nHowever, the collection of detailed consumption data can inadvertently disclose\nconsumers' daily activities, raising privacy concerns and potentially leading\nto privacy violations. To address these issues and preserve individuals'\nprivacy, we propose a lightweight privacy-preserving smart metering protocol\nspecifically designed to support real-time tariff billing service with dynamic\npolicy adjustment. Our scheme employs an efficient data perturbation technique\nto obscure precise energy usage data from internal adversaries, including the\nintermediary gateways and the utility provider. Subsequently, we validate the\nefficiency and security of our protocol through comprehensive performance and\nprivacy evaluations. We examined the computational, memory, and communication\noverhead of the proposed scheme. The execution time of our secure and\nprivacy-aware billing system is approximately 3.94540 seconds for a complete\nyear. Furthermore, we employed the Jensen-Shannon divergence as a privacy\nmetric to demonstrate that our protocol can effectively safeguard users'\nprivacy by increasing the noise scale.",
    "updated" : "2025-08-20T16:06:19Z",
    "published" : "2025-08-20T16:06:19Z",
    "authors" : [
      {
        "name" : "Farid Zaredar"
      },
      {
        "name" : "Morteza Amini"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.14744v1",
    "title" : "A Collusion-Resistance Privacy-Preserving Smart Metering Protocol for\n  Operational Utility",
    "summary" : "Modern grids have adopted advanced metering infrastructure (AMI) to\nfacilitate bidirectional communication between smart meters and control\ncenters. This enables smart meters to report consumption values at predefined\nintervals to utility providers for purposes including demand balancing, load\nforecasting, dynamic billing, and operational efficiency. Compared to\ntraditional power grids, smart grids offer advantages such as enhanced\nreliability, improved energy efficiency, and increased security. However,\nutility providers can compromise user privacy by analyzing fine-grained\nreadings and extracting individuals' daily activities from this time-series\ndata. To address this concern, we propose a collusion-resistant,\nprivacy-preserving aggregation protocol for smart metering in operational\nservices. Our protocol ensures privacy by leveraging techniques such as\npartially additive homomorphic encryption, aggregation, data perturbation, and\ndata minimization. The scheme aggregates perturbed readings using the additive\nhomomorphic property of the Paillier cryptosystem to provide results for\nmultiple operational purposes. We evaluate the protocol in terms of both\nperformance and privacy. Computational, memory, and communication overhead were\nexamined. The total execution time with 1024-bit key size is about 2.21\nseconds. We also evaluated privacy through the normalized conditional entropy\n(NCE) metric. Higher NCE values, closer to 1, indicate stronger privacy. By\nincreasing noise scale, the NCE value rises, showing perturbed values retain\nminimal information about the original, thereby reducing risks. Overall,\nevaluation demonstrates the protocol's efficiency while employing various\nprivacy-preserving techniques.",
    "updated" : "2025-08-20T14:40:33Z",
    "published" : "2025-08-20T14:40:33Z",
    "authors" : [
      {
        "name" : "Farid Zaredar"
      },
      {
        "name" : "Morteza Amini"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.14703v1",
    "title" : "A Lightweight Incentive-Based Privacy-Preserving Smart Metering Protocol\n  for Value-Added Services",
    "summary" : "The emergence of smart grids and advanced metering infrastructure (AMI) has\nrevolutionized energy management. Unlike traditional power grids, smart grids\nbenefit from two-way communication through AMI, which surpasses earlier\nautomated meter reading (AMR). AMI enables diverse demand- and supply-side\nutilities such as accurate billing, outage detection, real-time grid control,\nload forecasting, and value-added services. Smart meters play a key role by\ndelivering consumption values at predefined intervals to the utility provider\n(UP). However, such reports may raise privacy concerns, as adversaries can\ninfer lifestyle patterns, political orientations, and the types of electrical\ndevices in a household, or even sell the data to third parties (TP) such as\ninsurers. In this paper, we propose a lightweight, privacy-preserving smart\nmetering protocol for incentive-based value-added services. The scheme employs\nlocal differential privacy, hash chains, blind digital signatures, pseudonyms,\ntemporal aggregation, and anonymous overlay networks to report coarse-grained\nvalues with adjustable granularity to the UP. This protects consumers' privacy\nwhile preserving data utility. The scheme prevents identity disclosure while\nenabling automatic token redemption. From a performance perspective, our\nresults show that with a 1024-bit RSA key, a 7-day duration, and four reports\nper day, our protocol runs in approximately 0.51s and consumes about 4.5 MB of\nmemory. From a privacy perspective, the protocol resists semi-trusted and\nuntrusted adversaries.",
    "updated" : "2025-08-20T13:28:39Z",
    "published" : "2025-08-20T13:28:39Z",
    "authors" : [
      {
        "name" : "Farid Zaredar"
      },
      {
        "name" : "Morteza Amini"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.15523v1",
    "title" : "Stabilization of Perturbed Loss Function: Differential Privacy without\n  Gradient Noise",
    "summary" : "We propose SPOF (Stabilization of Perturbed Loss Function), a differentially\nprivate training mechanism intended for multi-user local differential privacy\n(LDP). SPOF perturbs a stabilized Taylor expanded polynomial approximation of a\nmodel's training loss function, where each user's data is privatized by\ncalibrated noise added to the coefficients of the polynomial. Unlike\ngradient-based mechanisms such as differentially private stochastic gradient\ndescent (DP-SGD), SPOF does not require injecting noise into the gradients of\nthe loss function, which improves both computational efficiency and stability.\nThis formulation naturally supports simultaneous privacy guarantees across all\nusers. Moreover, SPOF exhibits robustness to environmental noise during\ntraining, maintaining stable performance even when user inputs are corrupted.\nWe compare SPOF with a multi-user extension of DP-SGD, evaluating both methods\nin a wireless body area network (WBAN) scenario involving heterogeneous user\ndata and stochastic channel noise from body sensors. Our results show that SPOF\nachieves, on average, up to 3.5% higher reconstruction accuracy and reduces\nmean training time by up to 57.2% compared to DP-SGD, demonstrating superior\nprivacy-utility trade-offs in multi-user environments.",
    "updated" : "2025-08-21T12:54:19Z",
    "published" : "2025-08-21T12:54:19Z",
    "authors" : [
      {
        "name" : "Salman Habib"
      },
      {
        "name" : "Remi Chou"
      },
      {
        "name" : "Taejoon Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.15421v1",
    "title" : "A Study of Privacy-preserving Language Modeling Approaches",
    "summary" : "Recent developments in language modeling have increased their use in various\napplications and domains. Language models, often trained on sensitive data, can\nmemorize and disclose this information during privacy attacks, raising concerns\nabout protecting individuals' privacy rights. Preserving privacy in language\nmodels has become a crucial area of research, as privacy is one of the\nfundamental human rights. Despite its significance, understanding of how much\nprivacy risk these language models possess and how it can be mitigated is still\nlimited. This research addresses this by providing a comprehensive study of the\nprivacy-preserving language modeling approaches. This study gives an in-depth\noverview of these approaches, highlights their strengths, and investigates\ntheir limitations. The outcomes of this study contribute to the ongoing\nresearch on privacy-preserving language modeling, providing valuable insights\nand outlining future research directions.",
    "updated" : "2025-08-21T10:22:40Z",
    "published" : "2025-08-21T10:22:40Z",
    "authors" : [
      {
        "name" : "Pritilata Saha"
      },
      {
        "name" : "Abhirup Sinha"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.15089v1",
    "title" : "Tighter Privacy Analysis for Truncated Poisson Sampling",
    "summary" : "We give a new privacy amplification analysis for truncated Poisson sampling,\na Poisson sampling variant that truncates a batch if it exceeds a given maximum\nbatch size.",
    "updated" : "2025-08-20T22:00:23Z",
    "published" : "2025-08-20T22:00:23Z",
    "authors" : [
      {
        "name" : "Arun Ganesh"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.15036v1",
    "title" : "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs",
    "summary" : "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services.",
    "updated" : "2025-08-20T20:02:35Z",
    "published" : "2025-08-20T20:02:35Z",
    "authors" : [
      {
        "name" : "Ruyi Ding"
      },
      {
        "name" : "Tianhong Xu"
      },
      {
        "name" : "Xinyi Shen"
      },
      {
        "name" : "Aidong Adam Ding"
      },
      {
        "name" : "Yunsi Fei"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.14905v1",
    "title" : "Privacy Preserving Inference of Personalized Content for Out of Matrix\n  Users",
    "summary" : "Recommender systems for niche and dynamic communities face persistent\nchallenges from data sparsity, cold start users and items, and privacy\nconstraints. Traditional collaborative filtering and content-based approaches\nunderperform in these settings, either requiring invasive user data or failing\nwhen preference histories are absent. We present DeepNaniNet, a deep neural\nrecommendation framework that addresses these challenges through an inductive\ngraph-based architecture combining user-item interactions, item-item relations,\nand rich textual review embeddings derived from BERT. Our design enables cold\nstart recommendations without profile mining, using a novel \"content basket\"\nuser representation and an autoencoder-based generalization strategy for unseen\nusers. We introduce AnimeULike, a new dataset of 10,000 anime titles and 13,000\nusers, to evaluate performance in realistic scenarios with high proportions of\nguest or low-activity users. DeepNaniNet achieves state-of-the-art cold start\nresults on the CiteULike benchmark, matches DropoutNet in user recall without\nperformance degradation for out-of-matrix users, and outperforms Weighted\nMatrix Factorization (WMF) and DropoutNet on AnimeULike warm start by up to 7x\nand 1.5x in Recall@100, respectively. Our findings demonstrate that DeepNaniNet\ndelivers high-quality, privacy-preserving recommendations in data-sparse, cold\nstart-heavy environments while effectively integrating heterogeneous content\nsources.",
    "updated" : "2025-08-12T02:55:29Z",
    "published" : "2025-08-12T02:55:29Z",
    "authors" : [
      {
        "name" : "Michael Sun"
      },
      {
        "name" : "Tai Vu"
      },
      {
        "name" : "Andrew Wang"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.15844v1",
    "title" : "Ransomware Negotiation: Dynamics and Privacy-Preserving Mechanism Design",
    "summary" : "Ransomware attacks have become a pervasive and costly form of cybercrime,\ncausing tens of millions of dollars in losses as organizations increasingly pay\nransoms to mitigate operational disruptions and financial risks. While prior\nresearch has largely focused on proactive defenses, the post-infection\nnegotiation dynamics between attackers and victims remains underexplored. This\npaper presents a formal analysis of attacker-victim interactions in modern\nransomware incidents using a finite-horizon alternating-offers bargaining game\nmodel. Our analysis demonstrates how bargaining alters the optimal strategies\nof both parties. In practice, incomplete information-attackers lacking\nknowledge of victims' data valuations and victims lacking knowledge of\nattackers' reservation ransoms-can prolong negotiations and increase victims'\nbusiness interruption costs. To address this, we design a Bayesian\nincentive-compatible mechanism that facilitates rapid agreement on a fair\nransom without requiring either party to disclose private valuations. We\nfurther implement this mechanism using secure two-party computation based on\ngarbled circuits, thereby eliminating the need for trusted intermediaries and\npreserving the privacy of both parties throughout the negotiation. To the best\nof our knowledge, this is the first automated, privacy-preserving negotiation\nmechanism grounded in a formal analysis of ransomware negotiation dynamics.",
    "updated" : "2025-08-19T20:29:12Z",
    "published" : "2025-08-19T20:29:12Z",
    "authors" : [
      {
        "name" : "Haohui Zhang"
      },
      {
        "name" : "Sirui Shen"
      },
      {
        "name" : "Xinyu Hu"
      },
      {
        "name" : "Chenglu Jin"
      }
    ],
    "categories" : [
      "cs.GT",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.17962v1",
    "title" : "\"Nobody should control the end user\": Exploring Privacy Perspectives of\n  Indian Internet Users in Light of DPDPA",
    "summary" : "With the rapid increase in online interactions, concerns over data privacy\nand transparency of data processing practices have become more pronounced.\nWhile regulations like the GDPR have driven the widespread adoption of cookie\nbanners in the EU, India's Digital Personal Data Protection Act (DPDPA)\npromises similar changes domestically, aiming to introduce a framework for data\nprotection. However, certain clauses within the DPDPA raise concerns about\npotential infringements on user privacy, given the exemptions for government\naccountability and user consent requirements. In this study, for the first\ntime, we explore Indian Internet users' awareness and perceptions of cookie\nbanners, online privacy, and privacy regulations, especially in light of the\nnewly passed DPDPA. We conducted an online anonymous survey with 428 Indian\nparticipants, which addressed: (1) users' perspectives on cookie banners, (2)\ntheir attitudes towards online privacy and privacy regulations, and (3) their\nacceptance of 10 contentious DPDPA clauses that favor state authorities and may\nenable surveillance. Our findings reveal that privacy-conscious users often\nlack consistent awareness of privacy mechanisms, and their concerns do not\nalways lead to protective actions. Our thematic analysis of 143 open ended\nresponses shows that users' privacy and data protection concerns are rooted in\nskepticism towards the government, shaping their perceptions of the DPDPA and\nfueling demands for policy revisions. Our study highlights the need for clearer\ncommunication regarding the DPDPA, user-centric consent mechanisms, and policy\nrefinements to enhance data privacy practices in India.",
    "updated" : "2025-08-25T12:22:25Z",
    "published" : "2025-08-25T12:22:25Z",
    "authors" : [
      {
        "name" : "Sana Athar"
      },
      {
        "name" : "Devashish Gosain"
      },
      {
        "name" : "Anja Feldmann"
      },
      {
        "name" : "Mannat Kaur"
      },
      {
        "name" : "Ha Dao"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.17341v1",
    "title" : "MetaFed: Advancing Privacy, Performance, and Sustainability in Federated\n  Metaverse Systems",
    "summary" : "The rapid expansion of immersive Metaverse applications introduces complex\nchallenges at the intersection of performance, privacy, and environmental\nsustainability. Centralized architectures fall short in addressing these\ndemands, often resulting in elevated energy consumption, latency, and privacy\nconcerns. This paper proposes MetaFed, a decentralized federated learning (FL)\nframework that enables sustainable and intelligent resource orchestration for\nMetaverse environments. MetaFed integrates (i) multi-agent reinforcement\nlearning for dynamic client selection, (ii) privacy-preserving FL using\nhomomorphic encryption, and (iii) carbon-aware scheduling aligned with\nrenewable energy availability. Evaluations on MNIST and CIFAR-10 using\nlightweight ResNet architectures demonstrate that MetaFed achieves up to 25\\%\nreduction in carbon emissions compared to conventional approaches, while\nmaintaining high accuracy and minimal communication overhead. These results\nhighlight MetaFed as a scalable solution for building environmentally\nresponsible and privacy-compliant Metaverse infrastructures.",
    "updated" : "2025-08-24T12:53:06Z",
    "published" : "2025-08-24T12:53:06Z",
    "authors" : [
      {
        "name" : "Muhammet Anil Yagiz"
      },
      {
        "name" : "Zeynep Sude Cengiz"
      },
      {
        "name" : "Polat Goktas"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY",
      "cs.DC",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.17222v1",
    "title" : "Exposing Privacy Risks in Graph Retrieval-Augmented Generation",
    "summary" : "Retrieval-Augmented Generation (RAG) is a powerful technique for enhancing\nLarge Language Models (LLMs) with external, up-to-date knowledge. Graph RAG has\nemerged as an advanced paradigm that leverages graph-based knowledge structures\nto provide more coherent and contextually rich answers. However, the move from\nplain document retrieval to structured graph traversal introduces new,\nunder-explored privacy risks. This paper investigates the data extraction\nvulnerabilities of the Graph RAG systems. We design and execute tailored data\nextraction attacks to probe their susceptibility to leaking both raw text and\nstructured data, such as entities and their relationships. Our findings reveal\na critical trade-off: while Graph RAG systems may reduce raw text leakage, they\nare significantly more vulnerable to the extraction of structured entity and\nrelationship information. We also explore potential defense mechanisms to\nmitigate these novel attack surfaces. This work provides a foundational\nanalysis of the unique privacy challenges in Graph RAG and offers insights for\nbuilding more secure systems.",
    "updated" : "2025-08-24T06:19:44Z",
    "published" : "2025-08-24T06:19:44Z",
    "authors" : [
      {
        "name" : "Jiale Liu"
      },
      {
        "name" : "Jiahao Zhang"
      },
      {
        "name" : "Suhang Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.17135v1",
    "title" : "Rao Differential Privacy",
    "summary" : "Differential privacy (DP) has recently emerged as a definition of privacy to\nrelease private estimates. DP calibrates noise to be on the order of an\nindividuals contribution. Due to the this calibration a private estimate\nobscures any individual while preserving the utility of the estimate. Since the\noriginal definition, many alternate definitions have been proposed. These\nalternates have been proposed for various reasons including improvements on\ncomposition results, relaxations, and formalizations. Nevertheless, thus far\nnearly all definitions of privacy have used a divergence of densities as the\nbasis of the definition. In this paper we take an information geometry\nperspective towards differential privacy. Specifically, rather than define\nprivacy via a divergence, we define privacy via the Rao distance. We show that\nour proposed definition of privacy shares the interpretation of previous\ndefinitions of privacy while improving on sequential composition.",
    "updated" : "2025-08-23T20:25:59Z",
    "published" : "2025-08-23T20:25:59Z",
    "authors" : [
      {
        "name" : "Carlos Soto"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.17043v1",
    "title" : "ZAPS: A Zero-Knowledge Proof Protocol for Secure UAV Authentication with\n  Flight Path Privacy",
    "summary" : "The increasing deployment of Unmanned Aerial Vehicles (UAVs) for military,\ncommercial, and logistics applications has raised significant concerns\nregarding flight path privacy. Conventional UAV communication systems often\nexpose flight path data to third parties, making them vulnerable to tracking,\nsurveillance, and location inference attacks. Existing encryption techniques\nprovide security but fail to ensure complete privacy, as adversaries can still\ninfer movement patterns through metadata analysis. To address these challenges,\nwe propose a zk-SNARK(Zero-Knowledge Succinct Non-Interactive Argument of\nKnowledge)-based privacy-preserving flight path authentication and verification\nframework. Our approach ensures that a UAV can prove its authorisation,\nvalidate its flight path with a control centre, and comply with regulatory\nconstraints without revealing any sensitive trajectory information. By\nleveraging zk-SNARKs, the UAV can generate cryptographic proofs that verify\ncompliance with predefined flight policies while keeping the exact path and\nlocation undisclosed. This method mitigates risks associated with real-time\ntracking, identity exposure, and unauthorised interception, thereby enhancing\nUAV operational security in adversarial environments. Our proposed solution\nbalances privacy, security, and computational efficiency, making it suitable\nfor resource-constrained UAVs in both civilian and military applications.",
    "updated" : "2025-08-23T14:45:25Z",
    "published" : "2025-08-23T14:45:25Z",
    "authors" : [
      {
        "name" : "Shayesta Naziri"
      },
      {
        "name" : "Xu Wang"
      },
      {
        "name" : "Guangsheng Yu"
      },
      {
        "name" : "Christy Jie Liang"
      },
      {
        "name" : "Wei Ni"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.16765v1",
    "title" : "Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions\n  with Cloud-Based AI Models",
    "summary" : "The interactive nature of Large Language Models (LLMs), which closely track\nuser data and context, has prompted users to share personal and private\ninformation in unprecedented ways. Even when users opt out of allowing their\ndata to be used for training, these privacy settings offer limited protection\nwhen LLM providers operate in jurisdictions with weak privacy laws, invasive\ngovernment surveillance, or poor data security practices. In such cases, the\nrisk of sensitive information, including Personally Identifiable Information\n(PII), being mishandled or exposed remains high. To address this, we propose\nthe concept of an \"LLM gatekeeper\", a lightweight, locally run model that\nfilters out sensitive information from user queries before they are sent to the\npotentially untrustworthy, though highly capable, cloud-based LLM. Through\nexperiments with human subjects, we demonstrate that this dual-model approach\nintroduces minimal overhead while significantly enhancing user privacy, without\ncompromising the quality of LLM responses.",
    "updated" : "2025-08-22T19:49:03Z",
    "published" : "2025-08-22T19:49:03Z",
    "authors" : [
      {
        "name" : "GodsGift Uzor"
      },
      {
        "name" : "Hasan Al-Qudah"
      },
      {
        "name" : "Ynes Ineza"
      },
      {
        "name" : "Abdul Serwadda"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.12093v2",
    "title" : "PP-STAT: An Efficient Privacy-Preserving Statistical Analysis Framework\n  using Homomorphic Encryption",
    "summary" : "With the widespread adoption of cloud computing, the need for outsourcing\nstatistical analysis to third-party platforms is growing rapidly. However,\nhandling sensitive data such as medical records and financial information in\ncloud environments raises serious privacy concerns. In this paper, we present\nPP-STAT, a novel and efficient Homomorphic Encryption (HE)-based framework for\nprivacy-preserving statistical analysis. HE enables computations to be\nperformed directly on encrypted data without revealing the underlying\nplaintext. PP-STAT supports advanced statistical measures, including Z-score\nnormalization, skewness, kurtosis, coefficient of variation, and Pearson\ncorrelation coefficient, all computed securely over encrypted data. To improve\nefficiency, PP-STAT introduces two key optimizations: (1) a Chebyshev-based\napproximation strategy for initializing inverse square root operations, and (2)\na pre-normalization scaling technique that reduces multiplicative depth by\nfolding constant scaling factors into mean and variance computations. These\ntechniques significantly lower computational overhead and minimize the number\nof expensive bootstrapping procedures. Our evaluation on real-world datasets\ndemonstrates that PP-STAT achieves high numerical accuracy, with mean relative\nerror (MRE) below 2.4x10-4. Notably, the encrypted Pearson correlation between\nthe smoker attribute and charges reaches 0.7873, with an MRE of 2.86x10-4.\nThese results confirm the practical utility of PP-STAT for secure and precise\nstatistical analysis in privacy-sensitive domains.",
    "updated" : "2025-08-24T04:26:16Z",
    "published" : "2025-08-16T16:24:35Z",
    "authors" : [
      {
        "name" : "Hyunmin Choi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01587v2",
    "title" : "Pr$^2$R: Information-Fused and Style-Aware Privacy-Preserving Replay for\n  Lifelong Person Re-Identification",
    "summary" : "Lifelong person re-identification (LReID) aims to incrementally accumulate\nknowledge across a sequence of tasks under domain shifts. Recently,\nreplay-based methods have demonstrated strong effectiveness in LReID by\nrehearsing past samples stored in an auxiliary memory. However, storing\nhistorical exemplars raises concerns over data privacy. To avoid this,\nexemplar-free approaches attempt to match the distribution of past data without\nstoring raw samples. Despite being privacy-friendly, these methods often suffer\nfrom performance degradation due to the forgetting of specific past knowledge\nrepresentations. To this end, we propose to fuse information from sequential\ndata into the pixel space in the replay memory, enabling Privacy-Preserving\nReplay (Pr$^2$R). More specifically, by distilling the training characteristics\nof multiple real images into a single image, the fused samples undergo\npixel-level changes. This not only protects the privacy of the original data\nbut also makes the replay samples more representative for sequential tasks.\nDuring the style replay phase, we align the current domain to the previous one\nwhile simultaneously adapting the replay samples to match the style of the\ncurrent domain. This dual-alignment strategy effectively mitigates both\nclass-incremental challenges and forgetting caused by domain shifts. Extensive\nexperiments on multiple benchmarks show that the proposed method significantly\nimproves replay effectiveness while preserving data privacy. Specifically,\nPr$^2$R achieves 4% and 6% higher accuracy on sequential tasks compared to the\ncurrent state-of-the-art and other replay-based methods, respectively.",
    "updated" : "2025-08-25T11:48:05Z",
    "published" : "2025-08-03T05:00:19Z",
    "authors" : [
      {
        "name" : "Mingyu Wang"
      },
      {
        "name" : "Haojie Liu"
      },
      {
        "name" : "Zhiyong Li"
      },
      {
        "name" : "Wei Jiang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.19115v1",
    "title" : "SecureV2X: An Efficient and Privacy-Preserving System for\n  Vehicle-to-Everything (V2X) Applications",
    "summary" : "Autonomous driving and V2X technologies have developed rapidly in the past\ndecade, leading to improved safety and efficiency in modern transportation.\nThese systems interact with extensive networks of vehicles, roadside\ninfrastructure, and cloud resources to support their machine learning\ncapabilities. However, the widespread use of machine learning in V2X systems\nraises issues over the privacy of the data involved. This is particularly\nconcerning for smart-transit and driver safety applications which can\nimplicitly reveal user locations or explicitly disclose medical data such as\nEEG signals. To resolve these issues, we propose SecureV2X, a scalable,\nmulti-agent system for secure neural network inferences deployed between the\nserver and each vehicle. Under this setting, we study two multi-agent V2X\napplications: secure drowsiness detection, and secure red-light violation\ndetection. Our system achieves strong performance relative to baselines, and\nscales efficiently to support a large number of secure computation interactions\nsimultaneously. For instance, SecureV2X is $9.4 \\times$ faster, requires\n$143\\times$ fewer computational rounds, and involves $16.6\\times$ less\ncommunication on drowsiness detection compared to other secure systems.\nMoreover, it achieves a runtime nearly $100\\times$ faster than state-of-the-art\nbenchmarks in object detection tasks for red light violation detection.",
    "updated" : "2025-08-26T15:17:46Z",
    "published" : "2025-08-26T15:17:46Z",
    "authors" : [
      {
        "name" : "Joshua Lee"
      },
      {
        "name" : "Ali Arastehfard"
      },
      {
        "name" : "Weiran Liu"
      },
      {
        "name" : "Xuegang Ban"
      },
      {
        "name" : "Yuan Hong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "E.3; I.2.6; I.5.1; F.1.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.18976v1",
    "title" : "The Double-edged Sword of LLM-based Data Reconstruction: Understanding\n  and Mitigating Contextual Vulnerability in Word-level Differential Privacy\n  Text Sanitization",
    "summary" : "Differentially private text sanitization refers to the process of privatizing\ntexts under the framework of Differential Privacy (DP), providing provable\nprivacy guarantees while also empirically defending against adversaries seeking\nto harm privacy. Despite their simplicity, DP text sanitization methods\noperating at the word level exhibit a number of shortcomings, among them the\ntendency to leave contextual clues from the original texts due to randomization\nduring sanitization $\\unicode{x2013}$ this we refer to as $\\textit{contextual\nvulnerability}$. Given the powerful contextual understanding and inference\ncapabilities of Large Language Models (LLMs), we explore to what extent LLMs\ncan be leveraged to exploit the contextual vulnerability of DP-sanitized texts.\nWe expand on previous work not only in the use of advanced LLMs, but also in\ntesting a broader range of sanitization mechanisms at various privacy levels.\nOur experiments uncover a double-edged sword effect of LLM-based data\nreconstruction attacks on privacy and utility: while LLMs can indeed infer\noriginal semantics and sometimes degrade empirical privacy protections, they\ncan also be used for good, to improve the quality and privacy of DP-sanitized\ntexts. Based on our findings, we propose recommendations for using LLM data\nreconstruction as a post-processing step, serving to increase privacy\nprotection by thinking adversarially.",
    "updated" : "2025-08-26T12:22:45Z",
    "published" : "2025-08-26T12:22:45Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Andreea-Elena Bodea"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.18971v1",
    "title" : "Can we make NeRF-based visual localization privacy-preserving?",
    "summary" : "Visual localization (VL) is the task of estimating the camera pose in a known\nscene. VL methods, a.o., can be distinguished based on how they represent the\nscene, e.g., explicitly through a (sparse) point cloud or a collection of\nimages or implicitly through the weights of a neural network. Recently,\nNeRF-based methods have become popular for VL. While NeRFs offer high-quality\nnovel view synthesis, they inadvertently encode fine scene details, raising\nprivacy concerns when deployed in cloud-based localization services as\nsensitive information could be recovered. In this paper, we tackle this\nchallenge on two ends. We first propose a new protocol to assess\nprivacy-preservation of NeRF-based representations. We show that NeRFs trained\nwith photometric losses store fine-grained details in their geometry\nrepresentations, making them vulnerable to privacy attacks, even if the head\nthat predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving\nNeural Segmentation Field), a NeRF variant trained with segmentation\nsupervision instead of RGB images. These segmentation labels are learned in a\nself-supervised manner, ensuring they are coarse enough to obscure identifiable\nscene details while remaining discriminativeness in 3D. The segmentation space\nof ppNeSF can be used for accurate visual localization, yielding\nstate-of-the-art results.",
    "updated" : "2025-08-26T12:17:00Z",
    "published" : "2025-08-26T12:17:00Z",
    "authors" : [
      {
        "name" : "Maxime Pietrantoni"
      },
      {
        "name" : "Martin Humenberger"
      },
      {
        "name" : "Torsten Sattler"
      },
      {
        "name" : "Gabriela Csurka"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.18942v1",
    "title" : "EnerSwap: Large-Scale, Privacy-First Automated Market Maker for V2G\n  Energy Trading",
    "summary" : "With the rapid growth of Electric Vehicle (EV) technology, EVs are destined\nto shape the future of transportation. The large number of EVs facilitates the\ndevelopment of the emerging vehicle-to-grid (V2G) technology, which realizes\nbidirectional energy exchanges between EVs and the power grid. This has led to\nthe setting up of electricity markets that are usually confined to a small\ngeographical location, often with a small number of participants. Usually,\nthese markets are manipulated by intermediaries responsible for collecting bids\nfrom prosumers, determining the market-clearing price, incorporating grid\nconstraints, and accounting for network losses. While centralized models can be\nhighly efficient, they grant excessive power to the intermediary by allowing\nthem to gain exclusive access to prosumers \\textquotesingle price preferences.\nThis opens the door to potential market manipulation and raises significant\nprivacy concerns for users, such as the location of energy providers. This lack\nof protection exposes users to potential risks, as untrustworthy servers and\nmalicious adversaries can exploit this information to infer trading activities\nand real identities. This work proposes a secure, decentralized exchange market\nbuilt on blockchain technology, utilizing a privacy-preserving Automated Market\nMaker (AMM) model to offer open and fair, and equal access to traders, and\nmitigates the most common trading-manipulation attacks. Additionally, it\nincorporates a scalable architecture based on geographical dynamic sharding,\nallowing for efficient resource allocation and improved performance as the\nmarket grows.",
    "updated" : "2025-08-26T11:31:05Z",
    "published" : "2025-08-26T11:31:05Z",
    "authors" : [
      {
        "name" : "Ahmed Mounsf Rafik Bendada"
      },
      {
        "name" : "Yacine Ghamri-Doudane"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.18911v1",
    "title" : "Enhancing Model Privacy in Federated Learning with Random Masking and\n  Quantization",
    "summary" : "Experimental results across various models and tasks demonstrate that our\napproach not only maintains strong model performance in federated learning\nsettings but also achieves enhanced protection of model parameters compared to\nbaseline methods.",
    "updated" : "2025-08-26T10:34:13Z",
    "published" : "2025-08-26T10:34:13Z",
    "authors" : [
      {
        "name" : "Zhibo Xu"
      },
      {
        "name" : "Jianhao Zhu"
      },
      {
        "name" : "Jingwen Xu"
      },
      {
        "name" : "Changze Lv"
      },
      {
        "name" : "Zisu Huang"
      },
      {
        "name" : "Xiaohua Wang"
      },
      {
        "name" : "Muling Wu"
      },
      {
        "name" : "Qi Qian"
      },
      {
        "name" : "Xiaoqing Zheng"
      },
      {
        "name" : "Xuanjing Huang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.18832v1",
    "title" : "A Tight Context-aware Privacy Bound for Histogram Publication",
    "summary" : "We analyze the privacy guarantees of the Laplace mechanism releasing the\nhistogram of a dataset through the lens of pointwise maximal leakage (PML).\nWhile differential privacy is commonly used to quantify the privacy loss, it is\na context-free definition that does not depend on the data distribution. In\ncontrast, PML enables a more refined analysis by incorporating assumptions\nabout the data distribution. We show that when the probability of each\nhistogram bin is bounded away from zero, stronger privacy protection can be\nachieved for a fixed level of noise. Our results demonstrate the advantage of\ncontext-aware privacy measures and show that incorporating assumptions about\nthe data can improve privacy-utility tradeoffs.",
    "updated" : "2025-08-26T09:12:23Z",
    "published" : "2025-08-26T09:12:23Z",
    "authors" : [
      {
        "name" : "Sara Saeidian"
      },
      {
        "name" : "Ata Yavuzyılmaz"
      },
      {
        "name" : "Leonhard Grosse"
      },
      {
        "name" : "Georg Schuppe"
      },
      {
        "name" : "Tobias J. Oechtering"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.18513v1",
    "title" : "An Analytical Approach to Privacy and Performance Trade-Offs in\n  Healthcare Data Sharing",
    "summary" : "The secondary use of healthcare data is vital for research and clinical\ninnovation, but it raises concerns about patient privacy. This study\ninvestigates how to balance privacy preservation and data utility in healthcare\ndata sharing, considering the perspectives of both data providers and data\nusers. Using a dataset of adult patients hospitalized between 2013 and 2015, we\npredict whether sepsis was present at admission or developed during the\nhospital stay. We identify sub-populations, such as older adults, frequently\nhospitalized patients, and racial minorities, that are especially vulnerable to\nprivacy attacks due to their unique combinations of demographic and healthcare\nutilization attributes. These groups are also critical for machine learning\n(ML) model performance. We evaluate three anonymization methods-$k$-anonymity,\nthe technique by Zheng et al., and the MO-OBAM model-based on their ability to\nreduce re-identification risk while maintaining ML utility. Results show that\n$k$-anonymity offers limited protection. The methods of Zheng et al. and\nMO-OBAM provide stronger privacy safeguards, with MO-OBAM yielding the best\nutility outcomes: only a 2% change in precision and recall compared to the\noriginal dataset. This work provides actionable insights for healthcare\norganizations on how to share data responsibly. It highlights the need for\nanonymization methods that protect vulnerable populations without sacrificing\nthe performance of data-driven models.",
    "updated" : "2025-08-25T21:36:47Z",
    "published" : "2025-08-25T21:36:47Z",
    "authors" : [
      {
        "name" : "Yusi Wei"
      },
      {
        "name" : "Hande Y. Benson"
      },
      {
        "name" : "Muge Capan"
      }
    ],
    "categories" : [
      "stat.AP",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.18453v1",
    "title" : "Privacy-Preserving Federated Learning Framework for Risk-Based Adaptive\n  Authentication",
    "summary" : "Balancing robust security with strong privacy guarantees is critical for\nRisk-Based Adaptive Authentication (RBA), particularly in decentralized\nsettings. Federated Learning (FL) offers a promising solution by enabling\ncollaborative risk assessment without centralizing user data. However, existing\nFL approaches struggle with Non-Independent and Identically Distributed\n(Non-IID) user features, resulting in biased, unstable, and poorly generalized\nglobal models. This paper introduces FL-RBA2, a novel Federated Learning\nframework for Risk-Based Adaptive Authentication that addresses Non-IID\nchallenges through a mathematically grounded similarity transformation. By\nconverting heterogeneous user features (including behavioral, biometric,\ncontextual, interaction-based, and knowledge-based modalities) into IID\nsimilarity vectors, FL-RBA2 supports unbiased aggregation and personalized risk\nmodeling across distributed clients. The framework mitigates cold-start\nlimitations via clustering-based risk labeling, incorporates Differential\nPrivacy (DP) to safeguard sensitive information, and employs Message\nAuthentication Codes (MACs) to ensure model integrity and authenticity.\nFederated updates are securely aggregated into a global model, achieving strong\nbalance between user privacy, scalability, and adaptive authentication\nrobustness. Rigorous game-based security proofs in the Random Oracle Model\nformally establish privacy, correctness, and adaptive security guarantees.\nExtensive experiments on keystroke, mouse, and contextual datasets validate\nFL-RBA2's effectiveness in high-risk user detection and its resilience to model\ninversion and inference attacks, even under strong DP constraints.",
    "updated" : "2025-08-25T20:02:07Z",
    "published" : "2025-08-25T20:02:07Z",
    "authors" : [
      {
        "name" : "Yaser Baseri"
      },
      {
        "name" : "Abdelhakim Senhaji Hafid"
      },
      {
        "name" : "Dimitrios Makrakis"
      },
      {
        "name" : "Hamidreza Fereidouni"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.18318v1",
    "title" : "ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable\n  Privacy and Trust-Aware Aggregation for Wind Power Data Imputation",
    "summary" : "Wind power data often suffers from missing values due to sensor faults and\nunstable transmission at edge sites. While federated learning enables\nprivacy-preserving collaboration without sharing raw data, it remains\nvulnerable to anomalous updates and privacy leakage during parameter exchange.\nThese challenges are amplified in open industrial environments, necessitating\nzero-trust mechanisms where no participant is inherently trusted. To address\nthese challenges, this work proposes ZTFed-MAS2S, a zero-trust federated\nlearning framework that integrates a multi-head attention-based\nsequence-to-sequence imputation model. ZTFed integrates verifiable differential\nprivacy with non-interactive zero-knowledge proofs and a confidentiality and\nintegrity verification mechanism to ensure verifiable privacy preservation and\nsecure model parameters transmission. A dynamic trust-aware aggregation\nmechanism is employed, where trust is propagated over similarity graphs to\nenhance robustness, and communication overhead is reduced via sparsity- and\nquantization-based compression. MAS2S captures long-term dependencies in wind\npower data for accurate imputation. Extensive experiments on real-world wind\nfarm datasets validate the superiority of ZTFed-MAS2S in both federated\nlearning performance and missing data imputation, demonstrating its\neffectiveness as a secure and efficient solution for practical applications in\nthe energy sector.",
    "updated" : "2025-08-24T01:50:58Z",
    "published" : "2025-08-24T01:50:58Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Hanjie Wang"
      },
      {
        "name" : "Yuanzheng Li"
      },
      {
        "name" : "Jiazheng Li"
      },
      {
        "name" : "Zhaoyang Dong"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.19640v1",
    "title" : "Optimal Cox regression under federated differential privacy:\n  coefficients and cumulative hazards",
    "summary" : "We study two foundational problems in distributed survival analysis:\nestimating Cox regression coefficients and cumulative hazard functions, under\nfederated differential privacy constraints, allowing for heterogeneous\nper-sever sample sizes and privacy budgets. To quantify the fundamental cost of\nprivacy, we derive minimax lower bounds along with matching (up to\npoly-logarithmic factors) upper bounds. In particular, to estimate the\ncumulative hazard function, we design a private tree-based algorithm for\nnonparametric integral estimation. Our results reveal server-level phase\ntransitions between the private and non-private rates, as well as the reduced\nestimation accuracy from imposing privacy constraints on distributed subsets of\ndata.\n  To address scenarios with partially public information, we also consider a\nrelaxed differential privacy framework and provide a corresponding minimax\nanalysis. To our knowledge, this is the first treatment of partially public\ndata in survival analysis, and it establishes a no-gain in accuracy phenomenon.\nFinally, we conduct extensive numerical experiments, with an accompanying R\npackage FDPCox, validating our theoretical findings. These experiments also\ninclude a fully-interactive algorithm with tighter privacy composition, which\ndemonstrates improved estimation accuracy.",
    "updated" : "2025-08-27T07:29:19Z",
    "published" : "2025-08-27T07:29:19Z",
    "authors" : [
      {
        "name" : "Elly K. H. Hung"
      },
      {
        "name" : "Yi Yu"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.ME",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.19493v1",
    "title" : "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered\n  Smartphone Agents",
    "summary" : "Smartphones bring significant convenience to users but also enable devices to\nextensively record various types of personal information. Existing smartphone\nagents powered by Multimodal Large Language Models (MLLMs) have achieved\nremarkable performance in automating different tasks. However, as the cost,\nthese agents are granted substantial access to sensitive users' personal\ninformation during this operation. To gain a thorough understanding of the\nprivacy awareness of these agents, we present the first large-scale benchmark\nencompassing 7,138 scenarios to the best of our knowledge. In addition, for\nprivacy context in scenarios, we annotate its type (e.g., Account Credentials),\nsensitivity level, and location. We then carefully benchmark seven available\nmainstream smartphone agents. Our results demonstrate that almost all\nbenchmarked agents show unsatisfying privacy awareness (RA), with performance\nremaining below 60% even with explicit hints. Overall, closed-source agents\nshow better privacy ability than open-source ones, and Gemini 2.0-flash\nachieves the best, achieving an RA of 67%. We also find that the agents'\nprivacy detection capability is highly related to scenario sensitivity level,\ni.e., the scenario with a higher sensitivity level is typically more\nidentifiable. We hope the findings enlighten the research community to rethink\nthe unbalanced utility-privacy tradeoff about smartphone agents. Our code and\nbenchmark are available at https://zhixin-l.github.io/SAPA-Bench.",
    "updated" : "2025-08-27T00:41:28Z",
    "published" : "2025-08-27T00:41:28Z",
    "authors" : [
      {
        "name" : "Zhixin Lin"
      },
      {
        "name" : "Jungang Li"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Yibo Shi"
      },
      {
        "name" : "Yue Yao"
      },
      {
        "name" : "Dongliang Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.19458v1",
    "title" : "The Sample Complexity of Membership Inference and Privacy Auditing",
    "summary" : "A membership-inference attack gets the output of a learning algorithm, and a\ntarget individual, and tries to determine whether this individual is a member\nof the training data or an independent sample from the same distribution. A\nsuccessful membership-inference attack typically requires the attacker to have\nsome knowledge about the distribution that the training data was sampled from,\nand this knowledge is often captured through a set of independent reference\nsamples from that distribution. In this work we study how much information the\nattacker needs for membership inference by investigating the sample\ncomplexity-the minimum number of reference samples required-for a successful\nattack. We study this question in the fundamental setting of Gaussian mean\nestimation where the learning algorithm is given $n$ samples from a Gaussian\ndistribution $\\mathcal{N}(\\mu,\\Sigma)$ in $d$ dimensions, and tries to estimate\n$\\hat\\mu$ up to some error $\\mathbb{E}[\\|\\hat \\mu - \\mu\\|^2_{\\Sigma}]\\leq\n\\rho^2 d$. Our result shows that for membership inference in this setting,\n$\\Omega(n + n^2 \\rho^2)$ samples can be necessary to carry out any attack that\ncompetes with a fully informed attacker. Our result is the first to show that\nthe attacker sometimes needs many more samples than the training algorithm uses\nto train the model. This result has significant implications for practice, as\nall attacks used in practice have a restricted form that uses $O(n)$ samples\nand cannot benefit from $\\omega(n)$ samples. Thus, these attacks may be\nunderestimating the possibility of membership inference, and better attacks may\nbe possible when information about the distribution is easy to obtain.",
    "updated" : "2025-08-26T22:19:28Z",
    "published" : "2025-08-26T22:19:28Z",
    "authors" : [
      {
        "name" : "Mahdi Haghifam"
      },
      {
        "name" : "Adam Smith"
      },
      {
        "name" : "Jonathan Ullman"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.19345v1",
    "title" : "Privacy-Preserving Distributed Control for a Networked Battery Energy\n  Storage System",
    "summary" : "The increasing deployment of distributed Battery Energy Storage Systems\n(BESSs) in modern power grids necessitates effective coordination strategies to\nensure state-of-charge (SoC) balancing and accurate power delivery. While\ndistributed control frameworks offer scalability and resilience, they also\nraise significant privacy concerns due to the need for inter-agent information\nexchange. This paper presents a novel privacy-preserving distributed control\nalgorithm for SoC balancing in a networked BESS. The proposed framework\nincludes distributed power allocation law that is designed based on two\nprivacy-preserving distributed estimators, one for the average unit state and\nthe other for the average desired power. The average unit state estimator is\ndesigned via the state decomposition method without disclosing sensitive\ninternal states. The proposed power allocation law based on these estimators\nensures asymptotic SoC balancing and global power delivery while safeguarding\nagent privacy from external eavesdroppers. The effectiveness and\nprivacy-preserving properties of the proposed control strategy are demonstrated\nthrough simulation results.",
    "updated" : "2025-08-26T18:06:37Z",
    "published" : "2025-08-26T18:06:37Z",
    "authors" : [
      {
        "name" : "Mihitha Maithripala"
      },
      {
        "name" : "Zongli Lin"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.18911v2",
    "title" : "Enhancing Model Privacy in Federated Learning with Random Masking and\n  Quantization",
    "summary" : "The primary goal of traditional federated learning is to protect data privacy\nby enabling distributed edge devices to collaboratively train a shared global\nmodel while keeping raw data decentralized at local clients. The rise of large\nlanguage models (LLMs) has introduced new challenges in distributed systems, as\ntheir substantial computational requirements and the need for specialized\nexpertise raise critical concerns about protecting intellectual property (IP).\nThis highlights the need for a federated learning approach that can safeguard\nboth sensitive data and proprietary models. To tackle this challenge, we\npropose FedQSN, a federated learning approach that leverages random masking to\nobscure a subnetwork of model parameters and applies quantization to the\nremaining parameters. Consequently, the server transmits only a\nprivacy-preserving proxy of the global model to clients during each\ncommunication round, thus enhancing the model's confidentiality. Experimental\nresults across various models and tasks demonstrate that our approach not only\nmaintains strong model performance in federated learning settings but also\nachieves enhanced protection of model parameters compared to baseline methods.",
    "updated" : "2025-08-27T04:14:45Z",
    "published" : "2025-08-26T10:34:13Z",
    "authors" : [
      {
        "name" : "Zhibo Xu"
      },
      {
        "name" : "Jianhao Zhu"
      },
      {
        "name" : "Jingwen Xu"
      },
      {
        "name" : "Changze Lv"
      },
      {
        "name" : "Zisu Huang"
      },
      {
        "name" : "Xiaohua Wang"
      },
      {
        "name" : "Muling Wu"
      },
      {
        "name" : "Qi Qian"
      },
      {
        "name" : "Xiaoqing Zheng"
      },
      {
        "name" : "Xuanjing Huang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.19286v1",
    "title" : "RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting",
    "summary" : "The performance of modern machine learning systems depends on access to\nlarge, high-quality datasets, often sourced from user-generated content or\nproprietary, domain-specific corpora. However, these rich datasets inherently\ncontain sensitive personal information, raising significant concerns about\nprivacy, data security, and compliance with regulatory frameworks. While\nconventional anonymization techniques can remove explicit identifiers, such\nremoval may result in performance drop in downstream machine learning tasks.\nMore importantly, simple anonymization may not be effective against inference\nattacks that exploit implicit signals such as writing style, topical focus, or\ndemographic cues, highlighting the need for more robust privacy safeguards\nduring model training. To address the challenging issue of balancing user\nprivacy and data utility, we propose a reinforcement learning framework that\nfine-tunes a large language model (LLM) using a composite reward function that\njointly optimizes for explicit and implicit privacy, semantic fidelity, and\noutput diversity. To effectively capture population level regularities, the\nprivacy reward combines semantic cues with structural patterns derived from a\nminimum spanning tree (MST) over latent representations. By modeling these\nprivacy-sensitive signals in their distributional context, the proposed\napproach guides the model to generate synthetic rewrites that preserve utility\nwhile mitigating privacy risks. Empirical results show that the proposed method\nsignificantly enhances author obfuscation and privacy metrics without degrading\nsemantic quality, providing a scalable and model-agnostic solution for privacy\npreserving data generation in the era of large language models.",
    "updated" : "2025-08-25T04:38:19Z",
    "published" : "2025-08-25T04:38:19Z",
    "authors" : [
      {
        "name" : "Zhan Shi"
      },
      {
        "name" : "Yefeng Yuan"
      },
      {
        "name" : "Yuhong Liu"
      },
      {
        "name" : "Liang Cheng"
      },
      {
        "name" : "Yi Fang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  }
]