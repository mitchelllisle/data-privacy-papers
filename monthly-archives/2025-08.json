[
  {
    "id" : "http://arxiv.org/abs/2508.00321v1",
    "title" : "Evaluating the Efficacy of Large Language Models for Generating\n  Fine-Grained Visual Privacy Policies in Homes",
    "summary" : "The proliferation of visual sensors in smart home environments, particularly\nthrough wearable devices like smart glasses, introduces profound privacy\nchallenges. Existing privacy controls are often static and coarse-grained,\nfailing to accommodate the dynamic and socially nuanced nature of home\nenvironments. This paper investigates the viability of using Large Language\nModels (LLMs) as the core of a dynamic and adaptive privacy policy engine. We\npropose a conceptual framework where visual data is classified using a\nmulti-dimensional schema that considers data sensitivity, spatial context, and\nsocial presence. An LLM then reasons over this contextual information to\nenforce fine-grained privacy rules, such as selective object obfuscation, in\nreal-time. Through a comparative evaluation of state-of-the-art Vision Language\nModels (including GPT-4o and the Qwen-VL series) in simulated home settings ,\nour findings show the feasibility of this approach. The LLM-based engine\nachieved a top machine-evaluated appropriateness score of 3.99 out of 5, and\nthe policies generated by the models received a top human-evaluated score of\n4.00 out of 5.",
    "updated" : "2025-08-01T05:11:29Z",
    "published" : "2025-08-01T05:11:29Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Ying Ma"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.00287v1",
    "title" : "Privacy-Preserving Driver Drowsiness Detection with Spatial\n  Self-Attention and Federated Learning",
    "summary" : "Driver drowsiness is one of the main causes of road accidents and is\nrecognized as a leading contributor to traffic-related fatalities. However,\ndetecting drowsiness accurately remains a challenging task, especially in\nreal-world settings where facial data from different individuals is\ndecentralized and highly diverse. In this paper, we propose a novel framework\nfor drowsiness detection that is designed to work effectively with\nheterogeneous and decentralized data. Our approach develops a new Spatial\nSelf-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)\nnetwork to better extract key facial features and improve detection\nperformance. To support federated learning, we employ a Gradient Similarity\nComparison (GSC) that selects the most relevant trained models from different\noperators before aggregation. This improves the accuracy and robustness of the\nglobal model while preserving user privacy. We also develop a customized tool\nthat automatically processes video data by extracting frames, detecting and\ncropping faces, and applying data augmentation techniques such as rotation,\nflipping, brightness adjustment, and zooming. Experimental results show that\nour framework achieves a detection accuracy of 89.9% in the federated learning\nsettings, outperforming existing methods under various deployment scenarios.\nThe results demonstrate the effectiveness of our approach in handling\nreal-world data variability and highlight its potential for deployment in\nintelligent transportation systems to enhance road safety through early and\nreliable drowsiness detection.",
    "updated" : "2025-08-01T03:12:01Z",
    "published" : "2025-08-01T03:12:01Z",
    "authors" : [
      {
        "name" : "Tran Viet Khoa"
      },
      {
        "name" : "Do Hai Son"
      },
      {
        "name" : "Mohammad Abu Alsheikh"
      },
      {
        "name" : "Yibeltal F Alem"
      },
      {
        "name" : "Dinh Thai Hoang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.02551v1",
    "title" : "PrivAR: Real-Time Privacy Protection for Location-Based Augmented\n  Reality Applications",
    "summary" : "Location-based augmented reality (LB-AR) applications, such as Pok\\'emon Go,\nstream sub-second GPS updates to deliver responsive and immersive user\nexperiences. However, this high-frequency location reporting introduces serious\nprivacy risks. Protecting privacy in LB-AR is significantly more challenging\nthan in traditional location-based services (LBS), as it demands real-time\nlocation protection with strong per-location and trajectory-level privacy\nguaranteed while maintaining low latency and high quality of service (QoS).\nExisting methods fail to meet these combined demands.\n  To fill the gap, we present PrivAR, the first client-side privacy framework\nfor real-time LB-AR. PrivAR introduces two lightweight mechanisms: (i) Planar\nStaircase Mechanism (PSM) which designs a staircase-shaped distribution to\ngenerate noisy location with strong per-location privacy and low expected\nerror; and (ii) Thresholded Reporting with PSM (TR-PSM), a selective scheme\nthat releases a noisy location update only when a displacement exceeds a\nprivate threshold, enabling many-to-one mappings for enhanced trace-level\nprivacy while preserving high QoS. We present theoretical analysis, extensive\nexperiments on two public datasets and our proprietary GeoTrace dataset, and\nvalidate PrivAR on a Pok\\'emon-Go-style prototype. Results show PrivAR improves\nQoS (Gamescore) by up to 50%, while increasing attacker error by 1.8x over\nbaseline with an additional 0.06 milliseconds runtime overhead.",
    "updated" : "2025-08-04T16:02:10Z",
    "published" : "2025-08-04T16:02:10Z",
    "authors" : [
      {
        "name" : "Shafizur Rahman Seeam"
      },
      {
        "name" : "Ye Zheng"
      },
      {
        "name" : "Zhengxiong Li"
      },
      {
        "name" : "Yidan Hu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.02483v1",
    "title" : "Revisiting the Privacy of Low-Frequency Speech Signals: Exploring\n  Resampling Methods, Evaluation Scenarios, and Speaker Characteristics",
    "summary" : "While audio recordings in real life provide insights into social dynamics and\nconversational behavior, they also raise concerns about the privacy of\npersonal, sensitive data. This article explores the effectiveness of\nrestricting recordings to low-frequency audio to protect spoken content. For\nresampling the audio signals to different sampling rates, we compare the effect\nof employing anti-aliasing filtering. Privacy enhancement is measured by an\nincreased word error rate of automatic speech recognition models. The impact on\nutility performance is measured with voice activity detection models. Our\nexperimental results show that for clean recordings, models trained with a\nsampling rate of up to 800 Hz transcribe the majority of words correctly. For\nboth models, we analyzed the impact of the speaker's sex and pitch, and we\ndemonstrated that missing anti-aliasing filters more strongly compromise speech\nprivacy.",
    "updated" : "2025-08-04T14:53:56Z",
    "published" : "2025-08-04T14:53:56Z",
    "authors" : [
      {
        "name" : "Jule Pohlhausen"
      },
      {
        "name" : "Jörg Bitzer"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.02461v1",
    "title" : "Experimental Evaluation of Post-Quantum Homomorphic Encryption for\n  Privacy-Preserving V2X Communication",
    "summary" : "Intelligent Transportation Systems (ITS) fundamentally rely on\nvehicle-generated data for applications such as congestion monitoring and route\noptimization, making the preservation of user privacy a critical challenge.\nHomomorphic Encryption (HE) offers a promising solution by enabling computation\non encrypted data without revealing underlying content. This study presents the\nfirst real-world experimental evaluation of three post-quantum secure HE\nschemes, i.e., Brakerski-Fan-Vercauteren (BFV), Brakerski-Gentry-Vaikuntanathan\n(BGV), and Cheon-Kim-Kim-Song (CKKS), for vehicular communication scenarios.\nTwo representative privacy-preserving use cases are considered: encrypted\nvehicle counting and average speed aggregation. Experiments are conducted over\nboth Wi-Fi and Ethernet to assess performance under wireless and wired\nvehicle-to-everything (V2X) settings. Results show that BFV and BGV are\nsuitable for latency-tolerant applications such as intersection monitoring and\nregional traffic analysis, with total end-to-end latencies under 10 seconds.\nWhile CKKS experiences higher overhead, it remains viable for periodic\nencrypted aggregation of numerical data. The experimental results demonstrate\nthat HE can be feasibly deployed in ITS environments under 128-bit post-quantum\nsecurity, provided that scheme-specific latency constraints are considered.\nThis reinforces its potential to serve as a foundational tool for secure and\nprivacy-preserving V2X data processing.",
    "updated" : "2025-08-04T14:28:19Z",
    "published" : "2025-08-04T14:28:19Z",
    "authors" : [
      {
        "name" : "Abdullah Al Mamun"
      },
      {
        "name" : "Kyle Yates"
      },
      {
        "name" : "Antsa Rakotondrafara"
      },
      {
        "name" : "Mashrur Chowdhury"
      },
      {
        "name" : "Ryann Cartor"
      },
      {
        "name" : "Shuhong Gao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.02034v1",
    "title" : "Protego: User-Centric Pose-Invariant Privacy Protection Against Face\n  Recognition-Induced Digital Footprint Exposure",
    "summary" : "Face recognition (FR) technologies are increasingly used to power large-scale\nimage retrieval systems, raising serious privacy concerns. Services like\nClearview AI and PimEyes allow anyone to upload a facial photo and retrieve a\nlarge amount of online content associated with that person. This not only\nenables identity inference but also exposes their digital footprint, such as\nsocial media activity, private photos, and news reports, often without their\nconsent. In response to this emerging threat, we propose Protego, a\nuser-centric privacy protection method that safeguards facial images from such\nretrieval-based privacy intrusions. Protego encapsulates a user's 3D facial\nsignatures into a pose-invariant 2D representation, which is dynamically\ndeformed into a natural-looking 3D mask tailored to the pose and expression of\nany facial image of the user, and applied prior to online sharing. Motivated by\na critical limitation of existing methods, Protego amplifies the sensitivity of\nFR models so that protected images cannot be matched even among themselves.\nExperiments show that Protego significantly reduces retrieval accuracy across a\nwide range of black-box FR models and performs at least 2x better than existing\nmethods. It also offers unprecedented visual coherence, particularly in video\nsettings where consistency and natural appearance are essential. Overall,\nProtego contributes to the fight against the misuse of FR for mass surveillance\nand unsolicited identity tracing.",
    "updated" : "2025-08-04T04:03:01Z",
    "published" : "2025-08-04T04:03:01Z",
    "authors" : [
      {
        "name" : "Ziling Wang"
      },
      {
        "name" : "Shuya Yang"
      },
      {
        "name" : "Jialin Lu"
      },
      {
        "name" : "Ka-Ho Chow"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01960v1",
    "title" : "Non-Verbal Vocalisations and their Challenges: Emotion, Privacy,\n  Sparseness, and Real Life",
    "summary" : "Non-Verbal Vocalisations (NVVs) are short `non-word' utterances without\nproper linguistic (semantic) meaning but conveying connotations -- be this\nemotions/affects or other paralinguistic information. We start this\ncontribution with a historic sketch: how they were addressed in psychology and\nlinguistics in the last two centuries, how they were neglected later on, and\nhow they came to the fore with the advent of emotion research. We then give an\noverview of types of NVVs (formal aspects) and functions of NVVs, exemplified\nwith the typical NVV \\textit{ah}. Interesting as they are, NVVs come, however,\nwith a bunch of challenges that should be accounted for: Privacy and general\nethical considerations prevent them of being recorded in real-life (private)\nscenarios to a sufficient extent. Isolated, prompted (acted) exemplars do not\nnecessarily model NVVs in context; yet, this is the preferred strategy so far\nwhen modelling NVVs, especially in AI. To overcome these problems, we argue in\nfavour of corpus-based approaches. This guarantees a more realistic modelling;\nhowever, we are still faced with privacy and sparse data problems.",
    "updated" : "2025-08-03T23:59:43Z",
    "published" : "2025-08-03T23:59:43Z",
    "authors" : [
      {
        "name" : "Anton Batliner"
      },
      {
        "name" : "Shahin Amiriparian"
      },
      {
        "name" : "Björn W. Schuller"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CL",
      "eess.AS",
      "68T10 (Primary) 68T45 (Secondary)",
      "I.2.7"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01898v1",
    "title" : "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution",
    "summary" : "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.",
    "updated" : "2025-08-03T19:16:40Z",
    "published" : "2025-08-03T19:16:40Z",
    "authors" : [
      {
        "name" : "Yijing Zhang"
      },
      {
        "name" : "Md-Ferdous Pervej"
      },
      {
        "name" : "Andreas F. Molisch"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01798v1",
    "title" : "A Survey on Privacy-Preserving Computing in the Automotive Domain",
    "summary" : "As vehicles become increasingly connected and autonomous, they accumulate and\nmanage various personal data, thereby presenting a key challenge in preserving\nprivacy during data sharing and processing. This survey reviews applications of\nSecure Multi-Party Computation (MPC) and Homomorphic Encryption (HE) that\naddress these privacy concerns in the automotive domain. First, we identify the\nscope of privacy-sensitive use cases for these technologies, by surveying\nexisting works that address privacy issues in different automotive contexts,\nsuch as location-based services, mobility infrastructures, traffic management,\netc. Then, we review recent works that employ MPC and HE as solutions for these\nuse cases in detail. Our survey highlights the applicability of these\nprivacy-preserving technologies in the automotive context, while also\nidentifying challenges and gaps in the current research landscape. This work\naims to provide a clear and comprehensive overview of this emerging field and\nto encourage further research in this domain.",
    "updated" : "2025-08-03T15:23:41Z",
    "published" : "2025-08-03T15:23:41Z",
    "authors" : [
      {
        "name" : "Nergiz Yuca"
      },
      {
        "name" : "Nikolay Matyunin"
      },
      {
        "name" : "Ektor Arzoglou"
      },
      {
        "name" : "Nikolaos Athanasios Anagnostopoulos"
      },
      {
        "name" : "Stefan Katzenbeisser"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01749v1",
    "title" : "Improving Noise Efficiency in Privacy-preserving Dataset Distillation",
    "summary" : "Modern machine learning models heavily rely on large datasets that often\ninclude sensitive and private information, raising serious privacy concerns.\nDifferentially private (DP) data generation offers a solution by creating\nsynthetic datasets that limit the leakage of private information within a\npredefined privacy budget; however, it requires a substantial amount of data to\nachieve performance comparable to models trained on the original data. To\nmitigate the significant expense incurred with synthetic data generation,\nDataset Distillation (DD) stands out for its remarkable training and storage\nefficiency. This efficiency is particularly advantageous when integrated with\nDP mechanisms, curating compact yet informative synthetic datasets without\ncompromising privacy. However, current state-of-the-art private DD methods\nsuffer from a synchronized sampling-optimization process and the dependency on\nnoisy training signals from randomly initialized networks. This results in the\ninefficient utilization of private information due to the addition of excessive\nnoise. To address these issues, we introduce a novel framework that decouples\nsampling from optimization for better convergence and improves signal quality\nby mitigating the impact of DP noise through matching in an informative\nsubspace. On CIFAR-10, our method achieves a \\textbf{10.0\\%} improvement with\n50 images per class and \\textbf{8.3\\%} increase with just \\textbf{one-fifth}\nthe distilled set size of previous state-of-the-art methods, demonstrating\nsignificant potential to advance privacy-preserving DD.",
    "updated" : "2025-08-03T13:15:52Z",
    "published" : "2025-08-03T13:15:52Z",
    "authors" : [
      {
        "name" : "Runkai Zheng"
      },
      {
        "name" : "Vishnu Asutosh Dasu"
      },
      {
        "name" : "Yinong Oliver Wang"
      },
      {
        "name" : "Haohan Wang"
      },
      {
        "name" : "Fernando De la Torre"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01636v1",
    "title" : "Privacy-Preserving Inference for Quantized BERT Models",
    "summary" : "With the increasing deployment of generative machine learning models in\nprivacy-sensitive domains such as healthcare and personalized services,\nensuring secure inference has become a critical challenge. Secure multi-party\ncomputation (MPC) enables privacy-preserving model inference but suffers from\nhigh communication and computation overhead. The main bottleneck lies in the\nexpensive secure evaluation of floating-point operations. Quantization offers a\npromising solution by converting floating-point operations into lower-precision\ninteger computations, significantly reducing overhead. However, existing\nMPC-based quantized inference methods either rely on public quantization\nparameters-posing privacy risks-or suffer from inefficiencies, particularly in\nhandling nonlinear functions such as activations and softmax. In this work, we\npropose a fine-grained, layer-wise quantization scheme and support 1-bit weight\nfully connected layers in a secure setting. We design a multi-input lookup\ntable protocol to evaluate softmax efficiently and securely. Furthermore, we\nuse dual secret sharing schemes and perform precision conversions via lookup\ntables, eliminating truncation overhead entirely. Experimental evaluation on\nBERT-base models demonstrates that our approach achieves up to $8\\times$\nspeedup compared to Lu \\emph{et al}. (NDSS 25), $9\\times$ speedup compared to\nGupta \\emph{et al}. (PETS 24) and $22 \\times$ speedup compared to Knott\n\\emph{et al}. (NeurIPS 21).",
    "updated" : "2025-08-03T07:52:08Z",
    "published" : "2025-08-03T07:52:08Z",
    "authors" : [
      {
        "name" : "Tianpei Lu"
      },
      {
        "name" : "Bingsheng Zhang"
      },
      {
        "name" : "Lekun Peng"
      },
      {
        "name" : "Bowen Zheng"
      },
      {
        "name" : "Lichun Li"
      },
      {
        "name" : "Kui Ren"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01587v1",
    "title" : "Lifelong Person Re-identification via Privacy-Preserving Data Replay",
    "summary" : "Lifelong person re-identification (LReID) aims to incrementally accumulate\nknowledge across a sequence of tasks under domain shifts. Recently,\nreplay-based methods have demonstrated strong effectiveness in LReID by\nrehearsing past samples stored in an auxiliary memory. However, storing\nhistorical exemplars raises concerns over data privacy. To avoid this,\nexemplar-free approaches attempt to match the distribution of past data without\nstoring raw samples. Despite being privacy-friendly, these methods often suffer\nfrom performance degradation due to the forgetting of specific past knowledge\nrepresentations. To this end, we propose to condense information from\nsequential data into the pixel space in the replay memory, enabling\nPrivacy-Preserving Replay (Pr^2R). More specifically, by distilling the\ntraining characteristics of multiple real images into a single image, the\ncondensed samples undergo pixel-level changes. This not only protects the\nprivacy of the original data but also makes the replay samples more\nrepresentative for sequential tasks. During the style replay phase, we align\nthe current domain to the previous one while simultaneously adapting the replay\nsamples to match the style of the current domain. This dual-alignment strategy\neffectively mitigates both class-incremental challenges and forgetting caused\nby domain shifts. Extensive experiments on multiple benchmarks show that the\nproposed method significantly improves replay effectiveness while preserving\ndata privacy. Specifically, Pr^2R achieves 4% and 6% higher accuracy on\nsequential tasks compared to the current state-of-the-art and other\nreplay-based methods, respectively.",
    "updated" : "2025-08-03T05:00:19Z",
    "published" : "2025-08-03T05:00:19Z",
    "authors" : [
      {
        "name" : "Mingyu Wang"
      },
      {
        "name" : "Haojie Liu"
      },
      {
        "name" : "Zhiyong Li"
      },
      {
        "name" : "Wei Jiang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01207v1",
    "title" : "Showcasing standards and approaches for cybersecurity, safety, and\n  privacy issues in connected and autonomous vehicles",
    "summary" : "In the automotive industry there is a need to handle broad quality\ndeficiencies, eg, performance, maintainability, cybersecurity, safety, and\nprivacy, to mention a few. The idea is to prevent these issues from reaching\nend-users, ie, road users and inadvertently, pedestrians, aiming to potentially\nreduce accidents, and allow safe operation in dynamic attack surfaces, for the\nbenefit of a host of stakeholders. This paper aims to bridge cybersecurity,\nsafety, and privacy concerns in Connected and Autonomous Vehicles (CAV) with\nrespect to Risk Assessment (RA) and Threat Modelling (TM) altogether.\nPractitioners know the vast literature on this topic given the sheer number of\nrecommendations, standards, best practices, and existing approaches, at times\nimpairing projects and fostering valuable and actionable threat analysis. In\nthis paper we collate key outcomes by highlighting latest standards and\napproaches in RA and TM research to tackle complex attack surfaces as the ones\nposed by automotive settings. We aim to provide the community with a list of\napproaches to align expectations with stakeholders when deciding where and when\nto focus threat related analysis in automotive solutions.",
    "updated" : "2025-08-02T05:45:50Z",
    "published" : "2025-08-02T05:45:50Z",
    "authors" : [
      {
        "name" : "Ricardo M. Czekster"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.01140v1",
    "title" : "Open Data Sharing in Clinical Research and Participants Privacy:\n  Challenges and Opportunities in the Era of Artificial Intelligence",
    "summary" : "Sharing clinical research data is key for increasing the pace of medical\ndiscoveries that improve human health. However, concern about study\nparticipants' privacy, confidentiality, and safety is a major factor that\ndeters researchers from openly sharing clinical data, even after\ndeidentification. This concern is further heightened by the evolution of\nartificial intelligence (AI) approaches that pose an ever-increasing threat to\nthe reidentification of study participants. Here, we discuss the challenges AI\napproaches create that blur the lines between identifiable and non-identifiable\ndata. We present a concept of pseudo-reidentification, and discuss how these\nchallenges provide opportunities for rethinking open data sharing practices in\nclinical research. We highlight the novel open data sharing approach we have\nestablished as part of the Artificial Intelligence Ready and Exploratory Atlas\nfor Diabetes Insights project, one of the four Data Generation Projects funded\nby the National Institutes of Health Common Fund's Bridge2AI Program.",
    "updated" : "2025-08-02T01:46:59Z",
    "published" : "2025-08-02T01:46:59Z",
    "authors" : [
      {
        "name" : "Shahin Hallaj"
      },
      {
        "name" : "Anna Heinke"
      },
      {
        "name" : "Fritz Gerald P. Kalaw"
      },
      {
        "name" : "Nayoon Gim"
      },
      {
        "name" : "Marian Blazes"
      },
      {
        "name" : "Julia Owen"
      },
      {
        "name" : "Eamon Dysinger"
      },
      {
        "name" : "Erik S. Benton"
      },
      {
        "name" : "Benjamin A. Cordier"
      },
      {
        "name" : "Nicholas G. Evans"
      },
      {
        "name" : "Jennifer Li-Pook-Than"
      },
      {
        "name" : "Michael P. Snyder"
      },
      {
        "name" : "Camille Nebeker"
      },
      {
        "name" : "Linda M. Zangwill"
      },
      {
        "name" : "Sally L. Baxter"
      },
      {
        "name" : "Shannon McWeeney"
      },
      {
        "name" : "Cecilia S. Lee"
      },
      {
        "name" : "Aaron Y. Lee"
      },
      {
        "name" : "Bhavesh Patel"
      }
    ],
    "categories" : [
      "q-bio.OT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03413v1",
    "title" : "Smart Car Privacy: Survey of Attacks and Privacy Issues",
    "summary" : "Automobiles are becoming increasingly important in our day to day life.\nModern automobiles are highly computerized and hence potentially vulnerable to\nattack. Providing many wireless connectivity for vehicles enables a bridge\nbetween vehicles and their external environments. Such a connected vehicle\nsolution is expected to be the next frontier for automotive revolution and the\nkey to the evolution to next generation intelligent transportation systems.\nVehicular Ad hoc Networks (VANETs) are emerging mobile ad hoc network\ntechnologies incorporating mobile routing protocols for inter-vehicle data\ncommunications to support intelligent transportation systems. Thus security and\nprivacy are the major concerns in VANETs due to the mobility of the vehicles.\nThus designing security mechanisms to remove adversaries from the network\nremarkably important in VANETs.\n  This paper provides an overview of various vehicular network architectures.\nThe evolution of security in modern vehicles. Various security and privacy\nattacks in VANETs with their defending mechanisms with examples and classify\nthese mechanisms. It also provides an overview of various privacy implication\nthat a vehicular network possess.",
    "updated" : "2025-08-05T12:59:17Z",
    "published" : "2025-08-05T12:59:17Z",
    "authors" : [
      {
        "name" : "Akshay Madhav Deshmukh"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03209v1",
    "title" : "GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models\n  via Adversarial Perturbations",
    "summary" : "Vision-Language Models (VLMs) such as GPT-4o now demonstrate a remarkable\nability to infer users' locations from public shared images, posing a\nsubstantial risk to geoprivacy. Although adversarial perturbations offer a\npotential defense, current methods are ill-suited for this scenario: they often\nperform poorly on high-resolution images and low perturbation budgets, and may\nintroduce irrelevant semantic content. To address these limitations, we propose\nGeoShield, a novel adversarial framework designed for robust geoprivacy\nprotection in real-world scenarios. GeoShield comprises three key modules: a\nfeature disentanglement module that separates geographical and non-geographical\ninformation, an exposure element identification module that pinpoints\ngeo-revealing regions within an image, and a scale-adaptive enhancement module\nthat jointly optimizes perturbations at both global and local levels to ensure\neffectiveness across resolutions. Extensive experiments on challenging\nbenchmarks show that GeoShield consistently surpasses prior methods in\nblack-box settings, achieving strong privacy protection with minimal impact on\nvisual or semantic quality. To our knowledge, this work is the first to explore\nadversarial perturbations for defending against geolocation inference by\nadvanced VLMs, providing a practical and effective solution to escalating\nprivacy concerns.",
    "updated" : "2025-08-05T08:37:06Z",
    "published" : "2025-08-05T08:37:06Z",
    "authors" : [
      {
        "name" : "Xinwei Liu"
      },
      {
        "name" : "Xiaojun Jia"
      },
      {
        "name" : "Yuan Xun"
      },
      {
        "name" : "Simeng Qin"
      },
      {
        "name" : "Xiaochun Cao"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03204v1",
    "title" : "Current State in Privacy-Preserving Text Preprocessing for\n  Domain-Agnostic NLP",
    "summary" : "Privacy is a fundamental human right. Data privacy is protected by different\nregulations, such as GDPR. However, modern large language models require a huge\namount of data to learn linguistic variations, and the data often contains\nprivate information. Research has shown that it is possible to extract private\ninformation from such language models. Thus, anonymizing such private and\nsensitive information is of utmost importance. While complete anonymization may\nnot be possible, a number of different pre-processing approaches exist for\nmasking or pseudonymizing private information in textual data. This report\nfocuses on a few of such approaches for domain-agnostic NLP tasks.",
    "updated" : "2025-08-05T08:26:45Z",
    "published" : "2025-08-05T08:26:45Z",
    "authors" : [
      {
        "name" : "Abhirup Sinha"
      },
      {
        "name" : "Pritilata Saha"
      },
      {
        "name" : "Tithi Saha"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03098v1",
    "title" : "Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language\n  Models in Retrieval-Augmented Generation",
    "summary" : "Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large\nlanguage models (LLMs) by conditioning outputs on external knowledge sources.\nHowever, when retrieval involves private or sensitive data, RAG systems are\nsusceptible to extraction attacks that can leak confidential information\nthrough generated responses. We propose Privacy-Aware Decoding (PAD), a\nlightweight, inference-time defense that adaptively injects calibrated Gaussian\nnoise into token logits during generation. PAD integrates confidence-based\nscreening to selectively protect high-risk tokens, efficient sensitivity\nestimation to minimize unnecessary noise, and context-aware noise calibration\nto balance privacy with generation quality. A \\renyi Differential Privacy (RDP)\naccountant rigorously tracks cumulative privacy loss, enabling explicit\nper-response $(\\varepsilon, \\delta)$-DP guarantees for sensitive outputs.\nUnlike prior approaches requiring retraining or corpus-level filtering, PAD is\nmodel-agnostic and operates entirely at decoding time with minimal\ncomputational overhead. Experiments on three real-world datasets demonstrate\nthat PAD substantially reduces private information leakage while preserving\nresponse utility, outperforming existing retrieval- and post-processing-based\ndefenses. Our work takes an important step toward mitigating privacy risks in\nRAG via decoding strategies, paving the way for universal and scalable privacy\nsolutions in sensitive domains. Our code is available:\nhttps://github.com/wang2226/PAD.",
    "updated" : "2025-08-05T05:22:13Z",
    "published" : "2025-08-05T05:22:13Z",
    "authors" : [
      {
        "name" : "Haoran Wang"
      },
      {
        "name" : "Xiongxiao Xu"
      },
      {
        "name" : "Baixiang Huang"
      },
      {
        "name" : "Kai Shu"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.04583v1",
    "title" : "Measuring the Carbon Footprint of Cryptographic Privacy-Enhancing\n  Technologies",
    "summary" : "Privacy-enhancing technologies (PETs) have attracted significant attention in\nresponse to privacy regulations, driving the development of applications that\nprioritize user data protection. At the same time, the information and\ncommunication technology (ICT) sector faces growing pressure to reduce its\nenvironmental footprint, particularly its carbon emissions. While numerous\nstudies have assessed the energy footprint of various ICT applications, the\nenvironmental footprint of cryptographic PETs remains largely unexplored.\n  Our work addresses this gap by proposing a standardized methodology for\nevaluating the carbon footprint of PETs. To demonstrate this methodology, we\nfocus on PETs supporting client-server applications as they are the simplest to\ndeploy. In particular, we measure the energy consumption and carbon footprint\nincrease induced by five cryptographic PETs (compared to their non-private\nequivalent): HTTPS web browsing, encrypted machine learning (ML) inference,\nencrypted ML training, encrypted databases, and encrypted emails. Our findings\nreveal significant variability in carbon footprint increases, ranging from a\ntwofold increase in HTTPS web browsing to a 100,000-fold increase in encrypted\nML.\n  Our study provides essential data to help decision-makers assess\nprivacy-carbon trade-offs in such applications. Finally, we outline key\nresearch directions for developing PETs that balance strong privacy protection\nwith environmental sustainability.",
    "updated" : "2025-08-06T16:07:29Z",
    "published" : "2025-08-06T16:07:29Z",
    "authors" : [
      {
        "name" : "Marc Damie"
      },
      {
        "name" : "Mihai Pop"
      },
      {
        "name" : "Merijn Posthuma"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.04542v1",
    "title" : "Privacy Risk Predictions Based on Fundamental Understanding of Personal\n  Data and an Evolving Threat Landscape",
    "summary" : "It is difficult for individuals and organizations to protect personal\ninformation without a fundamental understanding of relative privacy risks. By\nanalyzing over 5,000 empirical identity theft and fraud cases, this research\nidentifies which types of personal data are exposed, how frequently exposures\noccur, and what the consequences of those exposures are. We construct an\nIdentity Ecosystem graph--a foundational, graph-based model in which nodes\nrepresent personally identifiable information (PII) attributes and edges\nrepresent empirical disclosure relationships between them (e.g., the\nprobability that one PII attribute is exposed due to the exposure of another).\nLeveraging this graph structure, we develop a privacy risk prediction framework\nthat uses graph theory and graph neural networks to estimate the likelihood of\nfurther disclosures when certain PII attributes are compromised. The results\nshow that our approach effectively answers the core question: Can the\ndisclosure of a given identity attribute possibly lead to the disclosure of\nanother attribute?",
    "updated" : "2025-08-06T15:30:07Z",
    "published" : "2025-08-06T15:30:07Z",
    "authors" : [
      {
        "name" : "Haoran Niu"
      },
      {
        "name" : "K. Suzanne Barber"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.04202v1",
    "title" : "Unplug, Mute, Avoid Investigating smart speaker users' privacy\n  protection behaviours in Saudi Homes",
    "summary" : "Smart speakers are increasingly integrated into domestic life worldwide, yet\ntheir privacy risks remain underexplored in non-Western cultural contexts. This\nstudy investigates how Saudi Arabian users of smart speakers navigate privacy\nconcerns within collectivist, gendered, and often multigenerational households.\nUsing cultural probes followed by semi-structured interviews with 16\nparticipants, we uncover everyday privacy-protective behaviours including\nunplugging devices, muting microphones, and avoiding voice interactions\naltogether. These practices are shaped not only by individual risk perceptions\nbut also by household norms, room configurations, and interpersonal dynamics.\nWe contribute empirical insights from an underrepresented region, theoretical\nextensions to contextual integrity frameworks, and design directions for\nculturally responsive voice interfaces. This work expands the global\nconversation on smart speaker privacy and informs more inclusive HCI practices\nin increasingly diverse smart home environments.",
    "updated" : "2025-08-06T08:32:54Z",
    "published" : "2025-08-06T08:32:54Z",
    "authors" : [
      {
        "name" : "Abdulrhman Alorini"
      },
      {
        "name" : "Yufeng Wu"
      },
      {
        "name" : "Abdullah Bin Sawad"
      },
      {
        "name" : "Mukesh Prasad"
      },
      {
        "name" : "A. Baki Kocaballi"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03991v1",
    "title" : "Galaxy: A Cognition-Centered Framework for Proactive,\n  Privacy-Preserving, and Self-Evolving LLM Agents",
    "summary" : "Intelligent personal assistants (IPAs) such as Siri and Google Assistant are\ndesigned to enhance human capabilities and perform tasks on behalf of users.\nThe emergence of LLM agents brings new opportunities for the development of\nIPAs. While responsive capabilities have been widely studied, proactive\nbehaviors remain underexplored. Designing an IPA that is proactive,\nprivacy-preserving, and capable of self-evolution remains a significant\nchallenge. Designing such IPAs relies on the cognitive architecture of LLM\nagents. This work proposes Cognition Forest, a semantic structure designed to\nalign cognitive modeling with system-level design. We unify cognitive\narchitecture and system design into a self-reinforcing loop instead of treating\nthem separately. Based on this principle, we present Galaxy, a framework that\nsupports multidimensional interactions and personalized capability generation.\nTwo cooperative agents are implemented based on Galaxy: KoRa, a\ncognition-enhanced generative agent that supports both responsive and proactive\nskills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's\nself-evolution and privacy preservation. Experimental results show that Galaxy\noutperforms multiple state-of-the-art benchmarks. Ablation studies and\nreal-world interaction cases validate the effectiveness of Galaxy.",
    "updated" : "2025-08-06T00:46:38Z",
    "published" : "2025-08-06T00:46:38Z",
    "authors" : [
      {
        "name" : "Chongyu Bao"
      },
      {
        "name" : "Ruimin Dai"
      },
      {
        "name" : "Yangbo Shen"
      },
      {
        "name" : "Runyang Jian"
      },
      {
        "name" : "Jinghan Zhang"
      },
      {
        "name" : "Xiaolan Liu"
      },
      {
        "name" : "Kunpeng Liu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03989v1",
    "title" : "Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework",
    "summary" : "User-controllable privacy is important in modern sensing systems, as privacy\npreferences can vary significantly from person to person and may evolve over\ntime. This is especially relevant in devices equipped with Inertial Measurement\nUnit (IMU) sensors, such as smartphones and wearables, which continuously\ncollect rich time-series data that can inadvertently expose sensitive user\nbehaviors. While prior work has proposed privacy-preserving methods for sensor\ndata, most rely on static, predefined privacy labels or require large\nquantities of private training data, limiting their adaptability and user\nagency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,\nfew-shot privacy-preserving sensing framework. PrivCLIP allows users to specify\nand modify their privacy preferences by categorizing activities as sensitive\n(black-listed), non-sensitive (white-listed), or neutral (gray-listed).\nLeveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU\nsensor data with natural language activity descriptions in a shared embedding\nspace, enabling few-shot detection of sensitive activities. When a\nprivacy-sensitive activity is identified, the system uses a language-guided\nactivity sanitizer and a motion generation module (IMU-GPT) to transform the\noriginal data into a privacy-compliant version that semantically resembles a\nnon-sensitive activity. We evaluate PrivCLIP on multiple human activity\nrecognition datasets and demonstrate that it significantly outperforms baseline\nmethods in terms of both privacy protection and data utility.",
    "updated" : "2025-08-06T00:44:11Z",
    "published" : "2025-08-06T00:44:11Z",
    "authors" : [
      {
        "name" : "Ajesh Koyatan Chathoth"
      },
      {
        "name" : "Shuhao Yu"
      },
      {
        "name" : "Stephen Lee"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03836v1",
    "title" : "DP-NCB: Privacy Preserving Fair Bandits",
    "summary" : "Multi-armed bandit algorithms are fundamental tools for sequential\ndecision-making under uncertainty, with widespread applications across domains\nsuch as clinical trials and personalized decision-making. As bandit algorithms\nare increasingly deployed in these socially sensitive settings, it becomes\ncritical to protect user data privacy and ensure fair treatment across decision\nrounds. While prior work has independently addressed privacy and fairness in\nbandit settings, the question of whether both objectives can be achieved\nsimultaneously has remained largely open. Existing privacy-preserving bandit\nalgorithms typically optimize average regret, a utilitarian measure, whereas\nfairness-aware approaches focus on minimizing Nash regret, which penalizes\ninequitable reward distributions, but often disregard privacy concerns.\n  To bridge this gap, we introduce Differentially Private Nash Confidence Bound\n(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures\n$\\epsilon$-differential privacy and achieves order-optimal Nash regret,\nmatching known lower bounds up to logarithmic factors. The framework is\nsufficiently general to operate under both global and local differential\nprivacy models, and is anytime, requiring no prior knowledge of the time\nhorizon. We support our theoretical guarantees with simulations on synthetic\nbandit instances, showing that DP-NCB incurs substantially lower Nash regret\nthan state-of-the-art baselines. Our results offer a principled foundation for\ndesigning bandit algorithms that are both privacy-preserving and fair, making\nthem suitable for high-stakes, socially impactful applications.",
    "updated" : "2025-08-05T18:34:00Z",
    "published" : "2025-08-05T18:34:00Z",
    "authors" : [
      {
        "name" : "Dhruv Sarkar"
      },
      {
        "name" : "Nishant Pandey"
      },
      {
        "name" : "Sayak Ray Chowdhury"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.03831v1",
    "title" : "A Type System for Data Privacy Compliance in Active Object Languages",
    "summary" : "Data protection laws such as GDPR aim to give users unprecedented control\nover their personal data. Compliance with these regulations requires\nsystematically considering information flow and interactions among entities\nhandling sensitive data. Privacy-by-design principles advocate embedding data\nprotection into system architectures as a default. However, translating these\nabstract principles into concrete, explicit methods remains a significant\nchallenge. This paper addresses this gap by proposing a language-based approach\nto privacy integration, combining static and runtime techniques. By employing\ntype checking and type inference in an active object language, the framework\nenables the tracking of authorised data flows and the automatic generation of\nconstraints checked at runtime based on user consent. This ensures that\npersonal data is processed in compliance with GDPR constraints. The key\ncontribution of this work is a type system that gather the compliance checks\nand the changes to users consent and integrates data privacy compliance\nverification into system execution. The paper demonstrates the feasibility of\nthis approach through a soundness proof and several examples, illustrating how\nthe proposed language addresses common GDPR requirements, such as user consent,\npurpose limitation, and data subject rights. This work advances the state of\nthe art in privacy-aware system design by offering a systematic and automated\nmethod for integrating GDPR compliance into programming languages. This\ncapability has implications for building trustworthy systems in domains such as\nhealthcare or finance, where data privacy is crucial.",
    "updated" : "2025-08-05T18:21:28Z",
    "published" : "2025-08-05T18:21:28Z",
    "authors" : [
      {
        "name" : "Chinmayi Prabhu Baramashetru"
      },
      {
        "name" : "Paola Giannini"
      },
      {
        "name" : "Silvia Lizeth Tapia Tarifa"
      },
      {
        "name" : "Olaf Owe"
      }
    ],
    "categories" : [
      "cs.PL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.05518v1",
    "title" : "Local Distance Query with Differential Privacy",
    "summary" : "Differential Privacy (DP) is commonly employed to safeguard graph analysis or\npublishing. Distance, a critical factor in graph analysis, is typically handled\nusing curator DP, where a trusted curator holds the complete neighbor lists of\nall vertices and answers queries privately. However, in many real-world\nscenarios, such a curator may not be present, posing a significant challenge\nfor implementing differentially private distance queries under Local\nDifferential Privacy (LDP). This paper proposes two approaches to address this\nchallenge. The first approach generates a synthetic graph by randomizing\nresponses and applies bitwise operations to reduce noise interference. However,\nlike other synthetic graph methods, this approach suffers from low utility. To\novercome this limitation, we propose a second approach, the first LDP method\nspecifically designed for distance queries, which captures the global graph\nstructure by continuously aggregating local distance vectors from neighboring\nvertices. This process enables the accurate updating of global distances. We\ndemonstrate the effectiveness of our method through comprehensive theoretical\nanalysis and experimental evaluations on real-world datasets.",
    "updated" : "2025-08-07T15:48:35Z",
    "published" : "2025-08-07T15:48:35Z",
    "authors" : [
      {
        "name" : "Weihong Sheng"
      },
      {
        "name" : "Jiajun Chen"
      },
      {
        "name" : "Bin Cai"
      },
      {
        "name" : "Chunqiang Hu"
      },
      {
        "name" : "Meng Han"
      },
      {
        "name" : "Jiguo Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.05250v1",
    "title" : "Privacy Disclosure of Similarity in Speech and Language Processing",
    "summary" : "Speaker, author, and other biometric identification applications often\ncompare a sample's similarity to a database of templates to determine the\nidentity. Given that data may be noisy and similarity measures can be\ninaccurate, such a comparison may not reliably identify the true identity as\nthe most similar. Still, even the similarity rank based on an inaccurate\nsimilarity measure can disclose private information about the true identity. We\npropose a methodology for quantifying the privacy disclosure of such a\nsimilarity rank by estimating its probability distribution. It is based on\ndetermining the histogram of the similarity rank of the true speaker, or when\ndata is scarce, modeling the histogram with the beta-binomial distribution. We\nexpress the disclosure in terms of entropy (bits), such that the disclosure\nfrom independent features are additive. Our experiments demonstrate that all\ntested speaker and author characterizations contain personally identifying\ninformation (PII) that can aid in identification, with embeddings from speaker\nrecognition algorithms containing the most information, followed by phone\nembeddings, linguistic embeddings, and fundamental frequency. Our initial\nexperiments show that the disclosure of PII increases with the length of test\nsamples, but it is bounded by the length of database templates. The provided\nmetric, similarity rank disclosure, provides a way to compare the disclosure of\nPII between biometric features and merge them to aid identification. It can\nthus aid in the holistic evaluation of threats to privacy in speech and other\nbiometric technologies.",
    "updated" : "2025-08-07T10:40:35Z",
    "published" : "2025-08-07T10:40:35Z",
    "authors" : [
      {
        "name" : "Tom Bäckström"
      },
      {
        "name" : "Mohammad Hassan Vali"
      },
      {
        "name" : "My Nguyen"
      },
      {
        "name" : "Silas Rech"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.04202v1",
    "title" : "Unplug, Mute, Avoid: Investigating smart speaker users' privacy\n  protection behaviours in Saudi Homes",
    "summary" : "Smart speakers are increasingly integrated into domestic life worldwide, yet\ntheir privacy risks remain underexplored in non-Western cultural contexts. This\nstudy investigates how Saudi Arabian users of smart speakers navigate privacy\nconcerns within collectivist, gendered, and often multigenerational households.\nUsing cultural probes followed by semi-structured interviews with 16\nparticipants, we uncover everyday privacy-protective behaviours including\nunplugging devices, muting microphones, and avoiding voice interactions\naltogether. These practices are shaped not only by individual risk perceptions\nbut also by household norms, room configurations, and interpersonal dynamics.\nWe contribute empirical insights from an underrepresented region, theoretical\nextensions to contextual integrity frameworks, and design directions for\nculturally responsive voice interfaces. This work expands the global\nconversation on smart speaker privacy and informs more inclusive HCI practices\nin increasingly diverse smart home environments.",
    "updated" : "2025-08-06T08:32:54Z",
    "published" : "2025-08-06T08:32:54Z",
    "authors" : [
      {
        "name" : "Abdulrhman Alorini"
      },
      {
        "name" : "Yufeng Wu"
      },
      {
        "name" : "Abdullah Bin Sawad"
      },
      {
        "name" : "Mukesh Prasad"
      },
      {
        "name" : "A. Baki Kocaballi"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06301v1",
    "title" : "FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields",
    "summary" : "Neural fields provide a memory-efficient representation of data, which can\neffectively handle diverse modalities and large-scale data. However, learning\nto map neural fields often requires large amounts of training data and\ncomputations, which can be limited to resource-constrained edge devices. One\napproach to tackle this limitation is to leverage Federated Meta-Learning\n(FML), but traditional FML approaches suffer from privacy leakage. To address\nthese issues, we introduce a novel FML approach called FedMeNF. FedMeNF\nutilizes a new privacy-preserving loss function that regulates privacy leakage\nin the local meta-optimization. This enables the local meta-learner to optimize\nquickly and efficiently without retaining the client's private data. Our\nexperiments demonstrate that FedMeNF achieves fast optimization speed and\nrobust reconstruction performance, even with few-shot or non-IID data across\ndiverse data modalities, while preserving client data privacy.",
    "updated" : "2025-08-08T13:24:57Z",
    "published" : "2025-08-08T13:24:57Z",
    "authors" : [
      {
        "name" : "Junhyeog Yun"
      },
      {
        "name" : "Minui Hong"
      },
      {
        "name" : "Gunhee Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06251v1",
    "title" : "Synthetic Data Generation and Differential Privacy using Tensor\n  Networks' Matrix Product States (MPS)",
    "summary" : "Synthetic data generation is a key technique in modern artificial\nintelligence, addressing data scarcity, privacy constraints, and the need for\ndiverse datasets in training robust models. In this work, we propose a method\nfor generating privacy-preserving high-quality synthetic tabular data using\nTensor Networks, specifically Matrix Product States (MPS). We benchmark the\nMPS-based generative model against state-of-the-art models such as CTGAN, VAE,\nand PrivBayes, focusing on both fidelity and privacy-preserving capabilities.\nTo ensure differential privacy (DP), we integrate noise injection and gradient\nclipping during training, enabling privacy guarantees via R\\'enyi Differential\nPrivacy accounting. Across multiple metrics analyzing data fidelity and\ndownstream machine learning task performance, our results show that MPS\noutperforms classical models, particularly under strict privacy constraints.\nThis work highlights MPS as a promising tool for privacy-aware synthetic data\ngeneration. By combining the expressive power of tensor network representations\nwith formal privacy mechanisms, the proposed approach offers an interpretable\nand scalable alternative for secure data sharing. Its structured design\nfacilitates integration into sensitive domains where both data quality and\nconfidentiality are critical.",
    "updated" : "2025-08-08T12:14:57Z",
    "published" : "2025-08-08T12:14:57Z",
    "authors" : [
      {
        "name" : "Alejandro Moreno R."
      },
      {
        "name" : "Desale Fentaw"
      },
      {
        "name" : "Samuel Palmer"
      },
      {
        "name" : "Raúl Salles de Padua"
      },
      {
        "name" : "Ninad Dixit"
      },
      {
        "name" : "Samuel Mugel"
      },
      {
        "name" : "Roman Orús"
      },
      {
        "name" : "Manuel Radons"
      },
      {
        "name" : "Josef Menter"
      },
      {
        "name" : "Ali Abedi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06208v1",
    "title" : "Graph Federated Learning for Personalized Privacy Recommendation",
    "summary" : "Federated recommendation systems (FedRecs) have gained significant attention\nfor providing privacy-preserving recommendation services. However, existing\nFedRecs assume that all users have the same requirements for privacy\nprotection, i.e., they do not upload any data to the server. The approaches\noverlook the potential to enhance the recommendation service by utilizing\npublicly available user data. In real-world applications, users can choose to\nbe private or public. Private users' interaction data is not shared, while\npublic users' interaction data can be shared. Inspired by the issue, this paper\nproposes a novel Graph Federated Learning for Personalized Privacy\nRecommendation (GFed-PP) that adapts to different privacy requirements while\nimproving recommendation performance. GFed-PP incorporates the interaction data\nof public users to build a user-item interaction graph, which is then used to\nform a user relationship graph. A lightweight graph convolutional network (GCN)\nis employed to learn each user's user-specific personalized item embedding. To\nprotect user privacy, each client learns the user embedding and the scoring\nfunction locally. Additionally, GFed-PP achieves optimization of the federated\nrecommendation framework through the initialization of item embedding on\nclients and the aggregation of the user relationship graph on the server.\nExperimental results demonstrate that GFed-PP significantly outperforms\nexisting methods for five datasets, offering superior recommendation accuracy\nwithout compromising privacy. This framework provides a practical solution for\naccommodating varying privacy preferences in federated recommendation systems.",
    "updated" : "2025-08-08T10:44:33Z",
    "published" : "2025-08-08T10:44:33Z",
    "authors" : [
      {
        "name" : "Ce Na"
      },
      {
        "name" : "Kai Yang"
      },
      {
        "name" : "Dengzhao Fang"
      },
      {
        "name" : "Yu Li"
      },
      {
        "name" : "Jingtong Gao"
      },
      {
        "name" : "Chengcheng Zhu"
      },
      {
        "name" : "Jiale Zhang"
      },
      {
        "name" : "Xiaobing Sun"
      },
      {
        "name" : "Yi Chang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06087v1",
    "title" : "Adaptive Backtracking for Privacy Protection in Large Language Models",
    "summary" : "The preservation of privacy has emerged as a critical topic in the era of\nartificial intelligence. However, current work focuses on user-oriented\nprivacy, overlooking severe enterprise data leakage risks exacerbated by the\nRetrieval-Augmented Generation paradigm. To address this gap, our paper\nintroduces a novel objective: enterprise-oriented privacy concerns. Achieving\nthis objective requires overcoming two fundamental challenges: existing methods\nsuch as data sanitization severely degrade model performance, and the field\nlacks public datasets for evaluation. We address these challenges with several\nsolutions. (1) To prevent performance degradation, we propose ABack, a\ntraining-free mechanism that leverages a Hidden State Model to pinpoint the\norigin of a leakage intention and rewrite the output safely. (2) To solve the\nlack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy\nscenarios in healthcare and finance. To ensure a rigorous evaluation, we move\nbeyond simple static attacks by developing a powerful adaptive attacker with\nGroup Relative Policy Optimization. Experiments show that against this superior\nadversary, ABack improves the overall privacy utility score by up to 15\\% over\nstrong baselines, avoiding the performance trade-offs of prior methods.",
    "updated" : "2025-08-08T07:29:33Z",
    "published" : "2025-08-08T07:29:33Z",
    "authors" : [
      {
        "name" : "Zhihao Yao"
      },
      {
        "name" : "Yuxuan Gu"
      },
      {
        "name" : "Xiachong Feng"
      },
      {
        "name" : "Weitao Ma"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Xiaocheng Feng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.08190v1",
    "title" : "Differential Privacy for Regulatory Compliance in Cyberattack Detection\n  on Critical Infrastructure Systems",
    "summary" : "Industrial control systems are a fundamental component of critical\ninfrastructure networks (CIN) such as gas, water and power. With the growing\nrisk of cyberattacks, regulatory compliance requirements are also increasing\nfor large scale critical infrastructure systems comprising multiple utility\nstakeholders. The primary goal of regulators is to ensure overall system\nstability with recourse to trustworthy stakeholder attack detection. However,\nadhering to compliance requirements requires stakeholders to also disclose\nsensor and control data to regulators raising privacy concerns. In this paper,\nwe present a cyberattack detection framework that utilizes differentially\nprivate (DP) hypothesis tests geared towards enhancing regulatory confidence\nwhile alleviating privacy concerns of CIN stakeholders. The hallmark of our\napproach is a two phase privacy scheme that protects the privacy of covariance,\nas well as the associated sensor driven test statistics computed as a means to\ngenerate alarms. Theoretically, we show that our method induces a\nmisclassification error rate comparable to the non-DP cases while delivering\nrobust privacy guarantees. With the help of real-world datasets, we show the\nreliability of our DP-detection outcomes for a wide variety of attack scenarios\nfor interdependent stakeholders.",
    "updated" : "2025-08-11T17:10:49Z",
    "published" : "2025-08-11T17:10:49Z",
    "authors" : [
      {
        "name" : "Paritosh Ramanan"
      },
      {
        "name" : "H. M. Mohaimanul Islam"
      },
      {
        "name" : "Abhiram Reddy Alugula"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07960v1",
    "title" : "VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With\n  Enhanced Security",
    "summary" : "Advancement of machine learning techniques, combined with the availability of\nlarge-scale datasets, has significantly improved the accuracy and efficiency of\nfacial recognition. Modern facial recognition systems are trained using large\nface datasets collected from diverse individuals or public repositories.\nHowever, for training, these datasets are often replicated and stored in\nmultiple workstations, resulting in data replication, which complicates\ndatabase management and oversight. Currently, once a user submits their face\nfor dataset preparation, they lose control over how their data is used, raising\nsignificant privacy and ethical concerns. This paper introduces VOIDFace, a\nnovel framework for facial recognition systems that addresses two major issues.\nFirst, it eliminates the need of data replication and improves data control to\nsecurely store training face data by using visual secret sharing. Second, it\nproposes a patch-based multi-training network that uses this novel training\ndata storage mechanism to develop a robust, privacy-preserving facial\nrecognition system. By integrating these advancements, VOIDFace aims to improve\nthe privacy, security, and efficiency of facial recognition training, while\nensuring greater control over sensitive personal face data. VOIDFace also\nenables users to exercise their Right-To-Be-Forgotten property to control their\npersonal data. Experimental evaluations on the VGGFace2 dataset show that\nVOIDFace provides Right-To-Be-Forgotten, improved data control, security, and\nprivacy while maintaining competitive facial recognition performance. Code is\navailable at: https://github.com/ajnasmuhammed89/VOIDFace",
    "updated" : "2025-08-11T13:15:36Z",
    "published" : "2025-08-11T13:15:36Z",
    "authors" : [
      {
        "name" : "Ajnas Muhammed"
      },
      {
        "name" : "Iurri Medvedev"
      },
      {
        "name" : "Nuno Gonçalves"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07676v1",
    "title" : "Multi-Hop Privacy Propagation for Differentially Private Federated\n  Learning in Social Networks",
    "summary" : "Federated learning (FL) enables collaborative model training across\ndecentralized clients without sharing local data, thereby enhancing privacy and\nfacilitating collaboration among clients connected via social networks.\nHowever, these social connections introduce privacy externalities: a client's\nprivacy loss depends not only on its privacy protection strategy but also on\nthe privacy decisions of others, propagated through the network via multi-hop\ninteractions. In this work, we propose a socially-aware privacy-preserving FL\nmechanism that systematically quantifies indirect privacy leakage through a\nmulti-hop propagation model. We formulate the server-client interaction as a\ntwo-stage Stackelberg game, where the server, as the leader, optimizes\nincentive policies, and clients, as followers, strategically select their\nprivacy budgets, which determine their privacy-preserving levels by controlling\nthe magnitude of added noise. To mitigate information asymmetry in networked\nprivacy estimation, we introduce a mean-field estimator to approximate the\naverage external privacy risk. We theoretically prove the existence and\nconvergence of the fixed point of the mean-field estimator and derive\nclosed-form expressions for the Stackelberg Nash Equilibrium. Despite being\ndesigned from a client-centric incentive perspective, our mechanism achieves\napproximately-optimal social welfare, as revealed by Price of Anarchy (PoA)\nanalysis. Experiments on diverse datasets demonstrate that our approach\nsignificantly improves client utilities and reduces server costs while\nmaintaining model performance, outperforming both Social-Agnostic (SA)\nbaselines and methods that account for social externalities.",
    "updated" : "2025-08-11T06:53:32Z",
    "published" : "2025-08-11T06:53:32Z",
    "authors" : [
      {
        "name" : "Chenchen Lin"
      },
      {
        "name" : "Xuehe Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC",
      "cs.GT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07672v1",
    "title" : "Towards Aligning Personalized Conversational Recommendation Agents with\n  Users' Privacy Preferences",
    "summary" : "The proliferation of AI agents, with their complex and context-dependent\nactions, renders conventional privacy paradigms obsolete. This position paper\nargues that the current model of privacy management, rooted in a user's\nunilateral control over a passive tool, is inherently mismatched with the\ndynamic and interactive nature of AI agents. We contend that ensuring effective\nprivacy protection necessitates that the agents proactively align with users'\nprivacy preferences instead of passively waiting for the user to control. To\nground this shift, and using personalized conversational recommendation agents\nas a case, we propose a conceptual framework built on Contextual Integrity (CI)\ntheory and Privacy Calculus theory. This synthesis first reframes automatically\ncontrolling users' privacy as an alignment problem, where AI agents initially\ndid not know users' preferences, and would learn their privacy preferences\nthrough implicit or explicit feedback. Upon receiving the preference feedback,\nthe agents used alignment and Pareto optimization for aligning preferences and\nbalancing privacy and utility. We introduced formulations and instantiations,\npotential applications, as well as five challenges.",
    "updated" : "2025-08-11T06:51:44Z",
    "published" : "2025-08-11T06:51:44Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Ying Ma"
      },
      {
        "name" : "Jingruo Chen"
      },
      {
        "name" : "Simin Li"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07667v1",
    "title" : "1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent\n  Reasoning",
    "summary" : "Addressing contextual privacy concerns remains challenging in interactive\nsettings where large language models (LLMs) process information from multiple\nsources (e.g., summarizing meetings with private and public information). We\nintroduce a multi-agent framework that decomposes privacy reasoning into\nspecialized subtasks (extraction, classification), reducing the information\nload on any single agent while enabling iterative validation and more reliable\nadherence to contextual privacy norms. To understand how privacy errors emerge\nand propagate, we conduct a systematic ablation over information-flow\ntopologies, revealing when and why upstream detection mistakes cascade into\ndownstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with\nseveral open-source and closed-sourced LLMs demonstrate that our best\nmulti-agent configuration substantially reduces private information leakage\n(\\textbf{18\\%} on ConfAIde and \\textbf{19\\%} on PrivacyLens with GPT-4o) while\npreserving the fidelity of public content, outperforming single-agent\nbaselines. These results highlight the promise of principled information-flow\ndesign in multi-agent systems for contextual privacy with LLMs.",
    "updated" : "2025-08-11T06:34:09Z",
    "published" : "2025-08-11T06:34:09Z",
    "authors" : [
      {
        "name" : "Wenkai Li"
      },
      {
        "name" : "Liwen Sun"
      },
      {
        "name" : "Zhenxiang Guan"
      },
      {
        "name" : "Xuhui Zhou"
      },
      {
        "name" : "Maarten Sap"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07664v1",
    "title" : "Understanding Users' Privacy Perceptions Towards LLM's RAG-based Memory",
    "summary" : "Large Language Models (LLMs) are increasingly integrating memory\nfunctionalities to provide personalized and context-aware interactions.\nHowever, user understanding, practices and expectations regarding these memory\nsystems are not yet well understood. This paper presents a thematic analysis of\nsemi-structured interviews with 18 users to explore their mental models of\nLLM's Retrieval Augmented Generation (RAG)-based memory, current usage\npractices, perceived benefits and drawbacks, privacy concerns and expectations\nfor future memory systems. Our findings reveal diverse and often incomplete\nmental models of how memory operates. While users appreciate the potential for\nenhanced personalization and efficiency, significant concerns exist regarding\nprivacy, control and the accuracy of remembered information. Users express a\ndesire for granular control over memory generation, management, usage and\nupdating, including clear mechanisms for reviewing, editing, deleting and\ncategorizing memories, as well as transparent insight into how memories and\ninferred information are used. We discuss design implications for creating more\nuser-centric, transparent, and trustworthy LLM memory systems.",
    "updated" : "2025-08-11T06:26:30Z",
    "published" : "2025-08-11T06:26:30Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Rongjun Ma"
      },
      {
        "name" : "Ying Ma"
      },
      {
        "name" : "Shixuan Li"
      },
      {
        "name" : "Yiqun Xu"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07505v1",
    "title" : "Enhancing Privacy in Decentralized Min-Max Optimization: A\n  Differentially Private Approach",
    "summary" : "Decentralized min-max optimization allows multi-agent systems to\ncollaboratively solve global min-max optimization problems by facilitating the\nexchange of model updates among neighboring agents, eliminating the need for a\ncentral server. However, sharing model updates in such systems carry a risk of\nexposing sensitive data to inference attacks, raising significant privacy\nconcerns. To mitigate these privacy risks, differential privacy (DP) has become\na widely adopted technique for safeguarding individual data. Despite its\nadvantages, implementing DP in decentralized min-max optimization poses\nchallenges, as the added noise can hinder convergence, particularly in\nnon-convex scenarios with complex agent interactions in min-max optimization\nproblems. In this work, we propose an algorithm called DPMixSGD (Differential\nPrivate Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving\nalgorithm specifically designed for non-convex decentralized min-max\noptimization. Our method builds on the state-of-the-art STORM-based algorithm,\none of the fastest decentralized min-max solutions. We rigorously prove that\nthe noise added to local gradients does not significantly compromise\nconvergence performance, and we provide theoretical bounds to ensure privacy\nguarantees. To validate our theoretical findings, we conduct extensive\nexperiments across various tasks and models, demonstrating the effectiveness of\nour approach.",
    "updated" : "2025-08-10T23:24:27Z",
    "published" : "2025-08-10T23:24:27Z",
    "authors" : [
      {
        "name" : "Yueyang Quan"
      },
      {
        "name" : "Chang Wang"
      },
      {
        "name" : "Shengjie Zhai"
      },
      {
        "name" : "Minghong Fang"
      },
      {
        "name" : "Zhuqing Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07057v1",
    "title" : "Rethinking Privacy Indicators in Extended Reality: Multimodal Design for\n  Situationally Impaired Bystanders",
    "summary" : "As Extended Reality (XR) devices become increasingly prevalent in everyday\nsettings, they raise significant privacy concerns for bystanders: individuals\nin the vicinity of an XR device during its use, whom the device sensors may\naccidentally capture. Current privacy indicators, such as small LEDs, often\npresume that bystanders are attentive enough to interpret the privacy signals.\nHowever, these cues can be easily overlooked when bystanders are distracted or\nhave limited vision. We define such individuals as situationally impaired\nbystanders. This study explores XR privacy indicator designs that are effective\nfor situationally impaired bystanders. A focus group with eight participants\nwas conducted to design five novel privacy indicators. We evaluated these\ndesigns through a user study with seven additional participants. Our results\nshow that visual-only indicators, typical in commercial XR devices, received\nlow ratings for perceived usefulness in impairment scenarios. In contrast,\nmultimodal indicators were preferred in privacy-sensitive scenarios with\nsituationally impaired bystanders. Ultimately, our results highlight the need\nto move toward adaptable, multimodal, and situationally aware designs that\neffectively support bystander privacy in everyday XR environments.",
    "updated" : "2025-08-09T17:48:44Z",
    "published" : "2025-08-09T17:48:44Z",
    "authors" : [
      {
        "name" : "Syed Ibrahim Mustafa Shah Bukhari"
      },
      {
        "name" : "Maha Sajid"
      },
      {
        "name" : "Bo Ji"
      },
      {
        "name" : "Brendan David-John"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.07044v1",
    "title" : "Balancing Privacy and Efficiency: Music Information Retrieval via\n  Additive Homomorphic Encryption",
    "summary" : "In the era of generative AI, ensuring the privacy of music data presents\nunique challenges: unlike static artworks such as images, music data is\ninherently temporal and multimodal, and it is sampled, transformed, and remixed\nat an unprecedented scale. These characteristics make its core vector\nembeddings, i.e, the numerical representations of the music, highly susceptible\nto being learned, misused, or even stolen by models without accessing the\noriginal audio files. Traditional methods like copyright licensing and digital\nwatermarking offer limited protection for these abstract mathematical\nrepresentations, thus necessitating a stronger, e.g., cryptographic, approach\nto safeguarding the embeddings themselves. Standard encryption schemes, such as\nAES, render data unintelligible for computation, making such searches\nimpossible. While Fully Homomorphic Encryption (FHE) provides a plausible\nsolution by allowing arbitrary computations on ciphertexts, its substantial\nperformance overhead remains impractical for large-scale vector similarity\nsearches. Given this trade-off, we propose a more practical approach using\nAdditive Homomorphic Encryption (AHE) for vector similarity search. The primary\ncontributions of this paper are threefold: we analyze threat models unique to\nmusic information retrieval systems; we provide a theoretical analysis and\npropose an efficient AHE-based solution through inner products of music\nembeddings to deliver privacy-preserving similarity search; and finally, we\ndemonstrate the efficiency and practicality of the proposed approach through\nempirical evaluation and comparison to FHE schemes on real-world MP3 files.",
    "updated" : "2025-08-09T17:00:34Z",
    "published" : "2025-08-09T17:00:34Z",
    "authors" : [
      {
        "name" : "William Zerong Wang"
      },
      {
        "name" : "Dongfang Zhao"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06760v1",
    "title" : "Understanding Privacy Norms Around LLM-Based Chatbots: A Contextual\n  Integrity Perspective",
    "summary" : "LLM-driven chatbots like ChatGPT have created large volumes of conversational\ndata, but little is known about how user privacy expectations are evolving with\nthis technology. We conduct a survey experiment with 300 US ChatGPT users to\nunderstand emerging privacy norms for sharing chatbot data. Our findings reveal\na stark disconnect between user concerns and behavior: 82% of respondents rated\nchatbot conversations as sensitive or highly sensitive - more than email or\nsocial media posts - but nearly half reported discussing health topics and over\none-third discussed personal finances with ChatGPT. Participants expressed\nstrong privacy concerns (t(299) = 8.5, p < .01) and doubted their conversations\nwould remain private (t(299) = -6.9, p < .01). Despite this, respondents\nuniformly rejected sharing personal data (search history, emails, device\naccess) for improved services, even in exchange for premium features worth\n$200. To identify which factors influence appropriate chatbot data sharing, we\npresented participants with factorial vignettes manipulating seven contextual\nfactors. Linear mixed models revealed that only the transmission factors such\nas informed consent, data anonymization, or the removal of personally\nidentifiable information, significantly affected perceptions of appropriateness\nand concern for data access. Surprisingly, contextual factors including the\nrecipient of the data (hospital vs. tech company), purpose (research vs.\nadvertising), type of content, and geographic location did not show significant\neffects. Our results suggest that users apply consistent baseline privacy\nexpectations to chatbot data, prioritizing procedural safeguards over recipient\ntrustworthiness. This has important implications for emerging agentic AI\nsystems that assume user willingness to integrate personal data across\nplatforms.",
    "updated" : "2025-08-09T00:22:46Z",
    "published" : "2025-08-09T00:22:46Z",
    "authors" : [
      {
        "name" : "Sarah Tran"
      },
      {
        "name" : "Hongfan Lu"
      },
      {
        "name" : "Isaac Slaughter"
      },
      {
        "name" : "Bernease Herman"
      },
      {
        "name" : "Aayushi Dangol"
      },
      {
        "name" : "Yue Fu"
      },
      {
        "name" : "Lufei Chen"
      },
      {
        "name" : "Biniyam Gebreyohannes"
      },
      {
        "name" : "Bill Howe"
      },
      {
        "name" : "Alexis Hiniker"
      },
      {
        "name" : "Nicholas Weber"
      },
      {
        "name" : "Robert Wolfe"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06647v1",
    "title" : "Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN",
    "summary" : "Synthetic data generation has become essential for securely sharing and\nanalyzing sensitive data sets. Traditional anonymization techniques, however,\noften fail to adequately preserve privacy. We introduce the Tabular\nAuto-Regressive Generative Network (TabularARGN), a neural network architecture\nspecifically designed for generating high-quality synthetic tabular data. Using\na discretization-based auto-regressive approach, TabularARGN achieves high data\nfidelity while remaining computationally efficient. We evaluate TabularARGN\nagainst existing synthetic data generation methods, showing competitive results\nin statistical similarity, machine learning utility, and detection robustness.\nWe further perform an in-depth privacy evaluation using systematic\nmembership-inference attacks, highlighting the robustness and effective\nprivacy-utility balance of our approach.",
    "updated" : "2025-08-08T18:57:23Z",
    "published" : "2025-08-08T18:57:23Z",
    "authors" : [
      {
        "name" : "Andrey Sidorenko"
      },
      {
        "name" : "Paul Tiwald"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.06577v1",
    "title" : "Leveraging LLMs for Privacy-Aware Predictions in Participatory Budgeting",
    "summary" : "Participatory Budgeting (PB) empowers citizens to propose and vote on public\ninvestment projects. Yet, despite its democratic potential, PB initiatives\noften suffer from low participation rates, limiting their visibility and\nperceived legitimacy. In this work, we aim to strengthen PB elections in two\nkey ways: by supporting project proposers in crafting better proposals, and by\nhelping PB organizers manage large volumes of submissions in a transparent\nmanner. We propose a privacy-preserving approach to predict which PB proposals\nare likely to be funded, using only their textual descriptions and anonymous\nhistorical voting records -- without relying on voter demographics or\npersonally identifiable information. We evaluate the performance of GPT 4 Turbo\nin forecasting proposal outcomes across varying contextual scenarios, observing\nthat the LLM's prior knowledge needs to be complemented by past voting data to\nobtain predictions reflecting real-world PB voting behavior. Our findings\nhighlight the potential of AI-driven tools to support PB processes by improving\ntransparency, planning efficiency, and civic engagement.",
    "updated" : "2025-08-07T15:26:22Z",
    "published" : "2025-08-07T15:26:22Z",
    "authors" : [
      {
        "name" : "Juan Zambrano"
      },
      {
        "name" : "Clément Contet"
      },
      {
        "name" : "Jairo Gudiño"
      },
      {
        "name" : "Felipe Garrido-Lucero"
      },
      {
        "name" : "Umberto Grandi"
      },
      {
        "name" : "Cesar A Hidalgo"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.05250v2",
    "title" : "Privacy Disclosure of Similarity Rank in Speech and Language Processing",
    "summary" : "Speaker, author, and other biometric identification applications often\ncompare a sample's similarity to a database of templates to determine the\nidentity. Given that data may be noisy and similarity measures can be\ninaccurate, such a comparison may not reliably identify the true identity as\nthe most similar. Still, even the similarity rank based on an inaccurate\nsimilarity measure can disclose private information about the true identity. We\npropose a methodology for quantifying the privacy disclosure of such a\nsimilarity rank by estimating its probability distribution. It is based on\ndetermining the histogram of the similarity rank of the true speaker, or when\ndata is scarce, modeling the histogram with the beta-binomial distribution. We\nexpress the disclosure in terms of entropy (bits), such that the disclosure\nfrom independent features are additive. Our experiments demonstrate that all\ntested speaker and author characterizations contain personally identifying\ninformation (PII) that can aid in identification, with embeddings from speaker\nrecognition algorithms containing the most information, followed by phone\nembeddings, linguistic embeddings, and fundamental frequency. Our initial\nexperiments show that the disclosure of PII increases with the length of test\nsamples, but it is bounded by the length of database templates. The provided\nmetric, similarity rank disclosure, provides a way to compare the disclosure of\nPII between biometric features and merge them to aid identification. It can\nthus aid in the holistic evaluation of threats to privacy in speech and other\nbiometric technologies.",
    "updated" : "2025-08-11T10:34:40Z",
    "published" : "2025-08-07T10:40:35Z",
    "authors" : [
      {
        "name" : "Tom Bäckström"
      },
      {
        "name" : "Mohammad Hassan Vali"
      },
      {
        "name" : "My Nguyen"
      },
      {
        "name" : "Silas Rech"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09036v1",
    "title" : "Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy\n  and AI Governance Exams",
    "summary" : "The rapid emergence of large language models (LLMs) has raised urgent\nquestions across the modern workforce about this new technology's strengths,\nweaknesses, and capabilities. For privacy professionals, the question is\nwhether these AI systems can provide reliable support on regulatory compliance,\nprivacy program management, and AI governance. In this study, we evaluate ten\nleading open and closed LLMs, including models from OpenAI, Anthropic, Google\nDeepMind, Meta, and DeepSeek, by benchmarking their performance on\nindustry-standard certification exams: CIPP/US, CIPM, CIPT, and AIGP from the\nInternational Association of Privacy Professionals (IAPP). Each model was\ntested using official sample exams in a closed-book setting and compared to\nIAPP's passing thresholds. Our findings show that several frontier models such\nas Gemini 2.5 Pro and OpenAI's GPT-5 consistently achieve scores exceeding the\nstandards for professional human certification - demonstrating substantial\nexpertise in privacy law, technical controls, and AI governance. The results\nhighlight both the strengths and domain-specific gaps of current LLMs and offer\npractical insights for privacy officers, compliance leads, and technologists\nassessing the readiness of AI tools for high-stakes data governance roles. This\npaper provides an overview for professionals navigating the intersection of AI\nadvancement and regulatory risk and establishes a machine benchmark based on\nhuman-centric evaluations.",
    "updated" : "2025-08-12T15:57:22Z",
    "published" : "2025-08-12T15:57:22Z",
    "authors" : [
      {
        "name" : "Zane Witherspoon"
      },
      {
        "name" : "Thet Mon Aye"
      },
      {
        "name" : "YingYing Hao"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.08785v1",
    "title" : "Privacy-protected Retrieval-Augmented Generation for Knowledge Graph\n  Question Answering",
    "summary" : "LLMs often suffer from hallucinations and outdated or incomplete knowledge.\nRAG is proposed to address these issues by integrating external knowledge like\nthat in KGs into LLMs. However, leveraging private KGs in RAG systems poses\nsignificant privacy risks due to the black-box nature of LLMs and potential\ninsecure data transmission, especially when using third-party LLM APIs lacking\ntransparency and control. In this paper, we investigate the privacy-protected\nRAG scenario for the first time, where entities in KGs are anonymous for LLMs,\nthus preventing them from accessing entity semantics. Due to the loss of\nsemantics of entities, previous RAG systems cannot retrieve question-relevant\nknowledge from KGs by matching questions with the meaningless identifiers of\nanonymous entities. To realize an effective RAG system in this scenario, two\nkey challenges must be addressed: (1) How can anonymous entities be converted\ninto retrievable information. (2) How to retrieve question-relevant anonymous\nentities. Hence, we propose a novel ARoG framework including relation-centric\nabstraction and structure-oriented abstraction strategies. For challenge (1),\nthe first strategy abstracts entities into high-level concepts by dynamically\ncapturing the semantics of their adjacent relations. It supplements meaningful\nsemantics which can further support the retrieval process. For challenge (2),\nthe second strategy transforms unstructured natural language questions into\nstructured abstract concept paths. These paths can be more effectively aligned\nwith the abstracted concepts in KGs, thereby improving retrieval performance.\nTo guide LLMs to effectively retrieve knowledge from KGs, the two strategies\nstrictly protect privacy from being exposed to LLMs. Experiments on three\ndatasets demonstrate that ARoG achieves strong performance and\nprivacy-robustness.",
    "updated" : "2025-08-12T09:38:21Z",
    "published" : "2025-08-12T09:38:21Z",
    "authors" : [
      {
        "name" : "Yunfeng Ning"
      },
      {
        "name" : "Mayi Xu"
      },
      {
        "name" : "Jintao Wen"
      },
      {
        "name" : "Qiankun Pi"
      },
      {
        "name" : "Yuanyuan Zhu"
      },
      {
        "name" : "Ming Zhong"
      },
      {
        "name" : "Jiawei Jiang"
      },
      {
        "name" : "Tieyun Qian"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.08749v1",
    "title" : "Approximate DBSCAN under Differential Privacy",
    "summary" : "This paper revisits the DBSCAN problem under differential privacy (DP).\nExisting DP-DBSCAN algorithms aim at publishing the cluster labels of the input\npoints. However, we show that both empirically and theoretically, this approach\ncannot offer any utility in the published results. We therefore propose an\nalternative definition of DP-DBSCAN based on the notion of spans. We argue that\npublishing the spans actually better serves the purposes of visualization and\nclassification of DBSCAN. Then we present a linear-time DP-DBSCAN algorithm\nachieving the sandwich quality guarantee in any constant dimensions, as well as\nmatching lower bounds on the approximation ratio. A key building block in our\nalgorithm is a linear-time algorithm for constructing a histogram under\npure-DP, which is of independent interest. Finally, we conducted experiments on\nboth synthetic and real-world datasets to verify the practical performance of\nour DP-DBSCAN algorithm.",
    "updated" : "2025-08-12T08:55:41Z",
    "published" : "2025-08-12T08:55:41Z",
    "authors" : [
      {
        "name" : "Yuan Qiu"
      },
      {
        "name" : "Ke Yi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.08502v1",
    "title" : "AirSignatureDB: Exploring In-Air Signature Biometrics in the Wild and\n  its Privacy Concerns",
    "summary" : "Behavioral biometrics based on smartphone motion sensors are growing in\npopularity for authentication purposes. In this study, AirSignatureDB is\npresented: a new publicly accessible dataset of in-air signatures collected\nfrom 108 participants under real-world conditions, using 83 different\nsmartphone models across four sessions. This dataset includes genuine samples\nand skilled forgeries, enabling a comprehensive evaluation of system robustness\nagainst realistic attack scenarios. Traditional and deep learning-based methods\nfor in-air signature verification are benchmarked, while analyzing the\ninfluence of sensor modality and enrollment strategies. Beyond verification, a\nfirst approach to reconstructing the three-dimensional trajectory of in-air\nsignatures from inertial sensor data alone is introduced. Using on-line\nhandwritten signatures as a reference, we demonstrate that the recovery of\naccurate trajectories is feasible, challenging the long-held assumption that\nin-air gestures are inherently traceless. Although this approach enables\nforensic traceability, it also raises critical questions about the privacy\nboundaries of behavioral biometrics. Our findings underscore the need for a\nreevaluation of the privacy assumptions surrounding inertial sensor data, as\nthey can reveal user-specific information that had not previously been\nconsidered in the design of in-air signature systems.",
    "updated" : "2025-08-11T22:24:03Z",
    "published" : "2025-08-11T22:24:03Z",
    "authors" : [
      {
        "name" : "Marta Robledo-Moreno"
      },
      {
        "name" : "Ruben Vera-Rodriguez"
      },
      {
        "name" : "Ruben Tolosana"
      },
      {
        "name" : "Javier Ortega-Garcia"
      },
      {
        "name" : "Andres Huergo"
      },
      {
        "name" : "Julian Fierrez"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09882v1",
    "title" : "Location Privacy-Enabled Beamforming in ISAC Scenarios",
    "summary" : "Integrated sensing and communication (ISAC) technology enables simultaneous\nenvironmental perception and data transmission in wireless networks; however,\nit also exposes user location to receivers. In this paper, we introduce a novel\nbeamforming framework guided by the proposed privacy metric direction of\narrival obfuscation ratio (DAOR) to protect transmitter location privacy in\nISAC scenarios. Unlike previous approaches, we do not suppress the\nline-of-sight (LOS) component while reshaping the angular power distribution so\nthat a false direction appears dominant at the receiver. We derive closed-form\nbounds on the feasible DAOR via generalized eigenvalue analysis and formulate\nan achievable rate-maximization problem under the DAOR constraint. The\nresulting problem is non-convex, which is efficiently solved using semidefinite\nrelaxation, eigenmode selection, and optimal power allocation. A suboptimal\ndesign strategy is also proposed with reduced complexity. Numerical results\ndemonstrate that the proposed DAOR-based beamformer achieves a trade-off\nbetween location privacy and communication rate without nullifying the LOS\npath. Results also show that a suboptimal design achieves a near-optimal\ncommunication rate with nearly an 85% reduction in computation time at a\nsignal-to-noise ratio (SNR) of 10 dB.",
    "updated" : "2025-08-13T15:32:10Z",
    "published" : "2025-08-13T15:32:10Z",
    "authors" : [
      {
        "name" : "Umair Ali Khan"
      },
      {
        "name" : "Lester Ho"
      },
      {
        "name" : "Holger Claussen"
      },
      {
        "name" : "Chinmoy Kundu"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09442v1",
    "title" : "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference",
    "summary" : "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.",
    "updated" : "2025-08-13T02:48:25Z",
    "published" : "2025-08-13T02:48:25Z",
    "authors" : [
      {
        "name" : "Zhifan Luo"
      },
      {
        "name" : "Shuo Shao"
      },
      {
        "name" : "Su Zhang"
      },
      {
        "name" : "Lijing Zhou"
      },
      {
        "name" : "Yuke Hu"
      },
      {
        "name" : "Chenxu Zhao"
      },
      {
        "name" : "Zhihao Liu"
      },
      {
        "name" : "Zhan Qin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09426v1",
    "title" : "Security Analysis of ChatGPT: Threats and Privacy Risks",
    "summary" : "As artificial intelligence technology continues to advance, chatbots are\nbecoming increasingly powerful. Among them, ChatGPT, launched by OpenAI, has\ngarnered widespread attention globally due to its powerful natural language\nprocessing capabilities based on the GPT model, which enables it to engage in\nnatural conversations with users, understand various forms of linguistic\nexpressions, and generate useful information and suggestions. However, as its\napplication scope expands, user demand grows, and malicious attacks related to\nit become increasingly frequent, the security threats and privacy risks faced\nby ChatGPT are gradually coming to the forefront. In this paper, the security\nof ChatGPT is mainly studied from two aspects, security threats and privacy\nrisks. The article systematically analyzes various types of vulnerabilities\ninvolved in the above two types of problems and their causes. Briefly, we\ndiscuss the controversies that ChatGPT may cause at the ethical and moral\nlevels. In addition, this paper reproduces several network attack and defense\ntest scenarios by simulating the attacker's perspective and methodology.\nSimultaneously, it explores the feasibility of using ChatGPT for security\nvulnerability detection and security tool generation from the defender's\nperspective.",
    "updated" : "2025-08-13T02:03:18Z",
    "published" : "2025-08-13T02:03:18Z",
    "authors" : [
      {
        "name" : "Yushan Xiang"
      },
      {
        "name" : "Zhongwen Li"
      },
      {
        "name" : "Xiaoqi Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09245v1",
    "title" : "Beyond Blanket Masking: Examining Granularity for Privacy Protection in\n  Images Captured by Blind and Low Vision Users",
    "summary" : "As visual assistant systems powered by visual language models (VLMs) become\nmore prevalent, concerns over user privacy have grown, particularly for blind\nand low vision users who may unknowingly capture personal private information\nin their images. Existing privacy protection methods rely on coarse-grained\nsegmentation, which uniformly masks entire private objects, often at the cost\nof usability. In this work, we propose FiGPriv, a fine-grained privacy\nprotection framework that selectively masks only high-risk private information\nwhile preserving low-risk information. Our approach integrates fine-grained\nsegmentation with a data-driven risk scoring mechanism. We evaluate our\nframework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%\nof image content, enhancing the ability of VLMs to provide useful responses by\n11% and identify the image content by 45%, while ensuring privacy protection.\nProject Page: https://artcs1.github.io/VLMPrivacy/",
    "updated" : "2025-08-12T17:56:36Z",
    "published" : "2025-08-12T17:56:36Z",
    "authors" : [
      {
        "name" : "Jeffri Murrugarra-LLerena"
      },
      {
        "name" : "Haoran Niu"
      },
      {
        "name" : "K. Suzanne Barber"
      },
      {
        "name" : "Hal Daumé III"
      },
      {
        "name" : "Yang Trista Cao"
      },
      {
        "name" : "Paola Cascante-Bonilla"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.08749v2",
    "title" : "Approximate DBSCAN under Differential Privacy",
    "summary" : "This paper revisits the DBSCAN problem under differential privacy (DP).\nExisting DP-DBSCAN algorithms aim at publishing the cluster labels of the input\npoints. However, we show that both empirically and theoretically, this approach\ncannot offer any utility in the published results. We therefore propose an\nalternative definition of DP-DBSCAN based on the notion of spans. We argue that\npublishing the spans actually better serves the purposes of visualization and\nclassification of DBSCAN. Then we present a linear-time DP-DBSCAN algorithm\nachieving the sandwich quality guarantee in any constant dimensions, as well as\nmatching lower bounds on the approximation ratio. A key building block in our\nalgorithm is a linear-time algorithm for constructing a histogram under\npure-DP, which is of independent interest. Finally, we conducted experiments on\nboth synthetic and real-world datasets to verify the practical performance of\nour DP-DBSCAN algorithm.",
    "updated" : "2025-08-13T14:25:42Z",
    "published" : "2025-08-12T08:55:41Z",
    "authors" : [
      {
        "name" : "Yuan Qiu"
      },
      {
        "name" : "Ke Yi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09232v1",
    "title" : "PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research",
    "summary" : "Social media data presents AI researchers with overlapping obligations under\nthe GDPR, copyright law, and platform terms -- yet existing frameworks fail to\nintegrate these regulatory domains, leaving researchers without unified\nguidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and\nPresent), a compliance framework that embeds legal safeguards directly into\nextended ETL pipelines. Central to PETLP is treating Data Protection Impact\nAssessments as living documents that evolve from pre-registration through\ndissemination. Through systematic Reddit analysis, we demonstrate how\nextraction rights fundamentally differ between qualifying research\norganisations (who can invoke DSM Article 3 to override platform restrictions)\nand commercial entities (bound by terms of service), whilst GDPR obligations\napply universally. We reveal why true anonymisation remains unachievable for\nsocial media data and expose the legal gap between permitted dataset creation\nand uncertain model distribution. By structuring compliance decisions into\npractical workflows and simplifying institutional data management plans, PETLP\nenables researchers to navigate regulatory complexity with confidence, bridging\nthe gap between legal requirements and research practice.",
    "updated" : "2025-08-12T08:33:40Z",
    "published" : "2025-08-12T08:33:40Z",
    "authors" : [
      {
        "name" : "Nick Oh"
      },
      {
        "name" : "Giorgos D. Vrakas"
      },
      {
        "name" : "Siân J. M. Brooke"
      },
      {
        "name" : "Sasha Morinière"
      },
      {
        "name" : "Toju Duke"
      }
    ],
    "categories" : [
      "cs.MM",
      "cs.AI",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.09186v1",
    "title" : "RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent\n  Transportation System",
    "summary" : "The proliferation of AI-powered cameras in Intelligent Transportation Systems\n(ITS) creates a severe conflict between the need for rich visual data and the\nfundamental right to privacy. Existing privacy-preserving mechanisms, such as\nblurring or encryption, are often insufficient, creating an undesirable\ntrade-off where either privacy is compromised against advanced reconstruction\nattacks or data utility is critically degraded. To resolve this impasse, we\npropose RL-MoE, a novel framework that transforms sensitive visual data into\nprivacy-preserving textual descriptions, eliminating the need for direct image\ntransmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture\nfor nuanced, multi-aspect scene decomposition with a Reinforcement Learning\n(RL) agent that optimizes the generated text for a dual objective of semantic\naccuracy and privacy preservation. Extensive experiments demonstrate that\nRL-MoE provides superior privacy protection, reducing the success rate of\nreplay attacks to just 9.4\\% on the CFP-FP dataset, while simultaneously\ngenerating richer textual content than baseline methods. Our work provides a\npractical and scalable solution for building trustworthy AI systems in\nprivacy-sensitive domains, paving the way for more secure smart city and\nautonomous vehicle networks.",
    "updated" : "2025-08-07T18:07:54Z",
    "published" : "2025-08-07T18:07:54Z",
    "authors" : [
      {
        "name" : "Abdolazim Rezaei"
      },
      {
        "name" : "Mehdi Sookhak"
      },
      {
        "name" : "Mahboobeh Haghparast"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10880v1",
    "title" : "Searching for Privacy Risks in LLM Agents via Simulation",
    "summary" : "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents.",
    "updated" : "2025-08-14T17:49:09Z",
    "published" : "2025-08-14T17:49:09Z",
    "authors" : [
      {
        "name" : "Yanzhe Zhang"
      },
      {
        "name" : "Diyi Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10737v1",
    "title" : "Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC\n  2025",
    "summary" : "This paper presents a summary of the 2025 Sclera Segmentation Benchmarking\nCompetition (SSBC), which focused on the development of privacy-preserving\nsclera-segmentation models trained using synthetically generated ocular images.\nThe goal of the competition was to evaluate how well models trained on\nsynthetic data perform in comparison to those trained on real-world datasets.\nThe competition featured two tracks: $(i)$ one relying solely on synthetic data\nfor model development, and $(ii)$ one combining/mixing synthetic with (a\nlimited amount of) real-world data. A total of nine research groups submitted\ndiverse segmentation models, employing a variety of architectural designs,\nincluding transformer-based solutions, lightweight models, and segmentation\nnetworks guided by generative frameworks. Experiments were conducted across\nthree evaluation datasets containing both synthetic and real-world images,\ncollected under diverse conditions. Results show that models trained entirely\non synthetic data can achieve competitive performance, particularly when\ndedicated training strategies are employed, as evidenced by the top performing\nmodels that achieved $F_1$ scores of over $0.8$ in the synthetic data track.\nMoreover, performance gains in the mixed track were often driven more by\nmethodological choices rather than by the inclusion of real data, highlighting\nthe promise of synthetic data for privacy-aware biometric development. The code\nand data for the competition is available at:\nhttps://github.com/dariant/SSBC_2025.",
    "updated" : "2025-08-14T15:16:58Z",
    "published" : "2025-08-14T15:16:58Z",
    "authors" : [
      {
        "name" : "Matej Vitek"
      },
      {
        "name" : "Darian Tomašević"
      },
      {
        "name" : "Abhijit Das"
      },
      {
        "name" : "Sabari Nathan"
      },
      {
        "name" : "Gökhan Özbulak"
      },
      {
        "name" : "Gözde Ayşe Tataroğlu Özbulak"
      },
      {
        "name" : "Jean-Paul Calbimonte"
      },
      {
        "name" : "André Anjos"
      },
      {
        "name" : "Hariohm Hemant Bhatt"
      },
      {
        "name" : "Dhruv Dhirendra Premani"
      },
      {
        "name" : "Jay Chaudhari"
      },
      {
        "name" : "Caiyong Wang"
      },
      {
        "name" : "Jian Jiang"
      },
      {
        "name" : "Chi Zhang"
      },
      {
        "name" : "Qi Zhang"
      },
      {
        "name" : "Iyyakutti Iyappan Ganapathi"
      },
      {
        "name" : "Syed Sadaf Ali"
      },
      {
        "name" : "Divya Velayudan"
      },
      {
        "name" : "Maregu Assefa"
      },
      {
        "name" : "Naoufel Werghi"
      },
      {
        "name" : "Zachary A. Daniels"
      },
      {
        "name" : "Leeon John"
      },
      {
        "name" : "Ritesh Vyas"
      },
      {
        "name" : "Jalil Nourmohammadi Khiarak"
      },
      {
        "name" : "Taher Akbari Saeed"
      },
      {
        "name" : "Mahsa Nasehi"
      },
      {
        "name" : "Ali Kianfar"
      },
      {
        "name" : "Mobina Pashazadeh Panahi"
      },
      {
        "name" : "Geetanjali Sharma"
      },
      {
        "name" : "Pushp Raj Panth"
      },
      {
        "name" : "Raghavendra Ramachandra"
      },
      {
        "name" : "Aditya Nigam"
      },
      {
        "name" : "Umapada Pal"
      },
      {
        "name" : "Peter Peer"
      },
      {
        "name" : "Vitomir Štruc"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10672v1",
    "title" : "Hybrid Generative Fusion for Efficient and Privacy-Preserving Face\n  Recognition Dataset Generation",
    "summary" : "In this paper, we present our approach to the DataCV ICCV Challenge, which\ncenters on building a high-quality face dataset to train a face recognition\nmodel. The constructed dataset must not contain identities overlapping with any\nexisting public face datasets. To handle this challenge, we begin with a\nthorough cleaning of the baseline HSFace dataset, identifying and removing\nmislabeled or inconsistent identities through a Mixture-of-Experts (MoE)\nstrategy combining face embedding clustering and GPT-4o-assisted verification.\nWe retain the largest consistent identity cluster and apply data augmentation\nup to a fixed number of images per identity. To further diversify the dataset,\nwe generate synthetic identities using Stable Diffusion with prompt\nengineering. As diffusion models are computationally intensive, we generate\nonly one reference image per identity and efficiently expand it using Vec2Face,\nwhich rapidly produces 49 identity-consistent variants. This hybrid approach\nfuses GAN-based and diffusion-based samples, enabling efficient construction of\na diverse and high-quality dataset. To address the high visual similarity among\nsynthetic identities, we adopt a curriculum learning strategy by placing them\nearly in the training schedule, allowing the model to progress from easier to\nharder samples. Our final dataset contains 50 images per identity, and all\nnewly generated identities are checked with mainstream face datasets to ensure\nno identity leakage. Our method achieves \\textbf{1st place} in the competition,\nand experimental results show that our dataset improves model performance\nacross 10K, 20K, and 100K identity scales. Code is available at\nhttps://github.com/Ferry-Li/datacv_fr.",
    "updated" : "2025-08-14T14:14:18Z",
    "published" : "2025-08-14T14:14:18Z",
    "authors" : [
      {
        "name" : "Feiran Li"
      },
      {
        "name" : "Qianqian Xu"
      },
      {
        "name" : "Shilong Bao"
      },
      {
        "name" : "Boyu Han"
      },
      {
        "name" : "Zhiyong Yang"
      },
      {
        "name" : "Qingming Huang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10482v1",
    "title" : "When Explainability Meets Privacy: An Investigation at the Intersection\n  of Post-hoc Explainability and Differential Privacy in the Context of Natural\n  Language Processing",
    "summary" : "In the study of trustworthy Natural Language Processing (NLP), a number of\nimportant research fields have emerged, including that of\n\\textit{explainability} and \\textit{privacy}. While research interest in both\nexplainable and privacy-preserving NLP has increased considerably in recent\nyears, there remains a lack of investigation at the intersection of the two.\nThis leaves a considerable gap in understanding of whether achieving\n\\textit{both} explainability and privacy is possible, or whether the two are at\nodds with each other. In this work, we conduct an empirical investigation into\nthe privacy-explainability trade-off in the context of NLP, guided by the\npopular overarching methods of \\textit{Differential Privacy} (DP) and Post-hoc\nExplainability. Our findings include a view into the intricate relationship\nbetween privacy and explainability, which is formed by a number of factors,\nincluding the nature of the downstream task and choice of the text\nprivatization and explainability method. In this, we highlight the potential\nfor privacy and explainability to co-exist, and we summarize our findings in a\ncollection of practical recommendations for future work at this important\nintersection.",
    "updated" : "2025-08-14T09:34:29Z",
    "published" : "2025-08-14T09:34:29Z",
    "authors" : [
      {
        "name" : "Mahdi Dhaini"
      },
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Ege Erdogan"
      },
      {
        "name" : "Florian Matthes"
      },
      {
        "name" : "Gjergji Kasneci"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10469v1",
    "title" : "Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human\n  Action Recognition",
    "summary" : "Human Action Recognition (HAR) plays a crucial role in healthcare, fitness\ntracking, and ambient assisted living technologies. While traditional vision\nbased HAR systems are effective, they pose privacy concerns. mmWave radar\nsensors offer a privacy preserving alternative but present challenges due to\nthe sparse and noisy nature of their point cloud data. In the literature, three\nprimary data processing methods: Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering\nhave been widely used to improve the quality and continuity of radar data.\nHowever, a comprehensive evaluation of these methods, both individually and in\ncombination, remains lacking. This paper addresses that gap by conducting a\ndetailed performance analysis of the three methods using the MiliPoint dataset.\nWe evaluate each method individually, all possible pairwise combinations, and\nthe combination of all three, assessing both recognition accuracy and\ncomputational cost. Furthermore, we propose targeted enhancements to the\nindividual methods aimed at improving accuracy. Our results provide crucial\ninsights into the strengths and trade-offs of each method and their\nintegrations, guiding future work on mmWave based HAR systems",
    "updated" : "2025-08-14T09:09:49Z",
    "published" : "2025-08-14T09:09:49Z",
    "authors" : [
      {
        "name" : "Maimunatu Tunau"
      },
      {
        "name" : "Vincent Gbouna Zakka"
      },
      {
        "name" : "Zhuangzhuang Dai"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.10373v1",
    "title" : "Privacy-Preserving Approximate Nearest Neighbor Search on\n  High-Dimensional Data",
    "summary" : "In the era of cloud computing and AI, data owners outsource ubiquitous\nvectors to the cloud, which furnish approximate $k$-nearest neighbors\n($k$-ANNS) services to users. To protect data privacy against the untrusted\nserver, privacy-preserving $k$-ANNS (PP-ANNS) on vectors has been a fundamental\nand urgent problem. However, existing PP-ANNS solutions fall short of meeting\nthe requirements of data privacy, efficiency, accuracy, and minimal user\ninvolvement concurrently. To tackle this challenge, we introduce a novel\nsolution that primarily executes PP-ANNS on a single cloud server to avoid the\nheavy communication overhead between the cloud and the user. To ensure data\nprivacy, we introduce a novel encryption method named distance comparison\nencryption, facilitating secure, efficient, and exact distance comparisons. To\noptimize the trade-off between data privacy and search performance, we design a\nprivacy-preserving index that combines the state-of-the-art $k$-ANNS method\nwith an approximate distance computation method. Then, we devise a search\nmethod using a filter-and-refine strategy based on the index. Moreover, we\nprovide the security analysis of our solution and conduct extensive experiments\nto demonstrate its superiority over existing solutions. Based on our\nexperimental results, our method accelerates PP-ANNS by up to 3 orders of\nmagnitude compared to state-of-the-art methods, while not compromising the\naccuracy.",
    "updated" : "2025-08-14T06:09:38Z",
    "published" : "2025-08-14T06:09:38Z",
    "authors" : [
      {
        "name" : "Yingfan Liu"
      },
      {
        "name" : "Yandi Zhang"
      },
      {
        "name" : "Jiadong Xie"
      },
      {
        "name" : "Hui Li"
      },
      {
        "name" : "Jeffrey Xu Yu"
      },
      {
        "name" : "Jiangtao Cui"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  }
]