[
  {
    "id" : "http://arxiv.org/abs/2401.00583v1",
    "title" : "Improving the Privacy and Practicality of Objective Perturbation for\n  Differentially Private Linear Learners",
    "summary" : "In the arena of privacy-preserving machine learning, differentially private\nstochastic gradient descent (DP-SGD) has outstripped the objective perturbation\nmechanism in popularity and interest. Though unrivaled in versatility, DP-SGD\nrequires a non-trivial privacy overhead (for privately tuning the model's\nhyperparameters) and a computational complexity which might be extravagant for\nsimple models such as linear and logistic regression. This paper revamps the\nobjective perturbation mechanism with tighter privacy analyses and new\ncomputational tools that boost it to perform competitively with DP-SGD on\nunconstrained convex generalized linear problems.",
    "updated" : "2023-12-31T20:32:30Z",
    "published" : "2023-12-31T20:32:30Z",
    "authors" : [
      {
        "name" : "Rachel Redberg"
      },
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "Yu-Xiang Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00879v1",
    "title" : "SoK: Demystifying Privacy Enhancing Technologies Through the Lens of\n  Software Developers",
    "summary" : "In the absence of data protection measures, software applications lead to\nprivacy breaches, posing threats to end-users and software organisations.\nPrivacy Enhancing Technologies (PETs) are technical measures that protect\npersonal data, thus minimising such privacy breaches. However, for software\napplications to deliver data protection using PETs, software developers should\nactively and correctly incorporate PETs into the software they develop.\nTherefore, to uncover ways to encourage and support developers to embed PETs\ninto software, this Systematic Literature Review (SLR) analyses 39 empirical\nstudies on developers' privacy practices. It reports the usage of six PETs in\nsoftware application scenarios. Then, it discusses challenges developers face\nwhen integrating PETs into software, ranging from intrinsic challenges, such as\nthe unawareness of PETs, to extrinsic challenges, such as the increased\ndevelopment cost. Next, the SLR presents the existing solutions to address\nthese challenges, along with the limitations of the solutions. Further, it\noutlines future research avenues to better understand PETs from a developer\nperspective and minimise the challenges developers face when incorporating PETs\ninto software.",
    "updated" : "2023-12-30T12:24:40Z",
    "published" : "2023-12-30T12:24:40Z",
    "authors" : [
      {
        "name" : "Maisha Boteju"
      },
      {
        "name" : "Thilina Ranbaduge"
      },
      {
        "name" : "Dinusha Vatsalan"
      },
      {
        "name" : "Nalin Asanka Gamagedara Arachchilage"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00870v1",
    "title" : "Teach Large Language Models to Forget Privacy",
    "summary" : "Large Language Models (LLMs) have proven powerful, but the risk of privacy\nleakage remains a significant concern. Traditional privacy-preserving methods,\nsuch as Differential Privacy and Homomorphic Encryption, are inadequate for\nblack-box API-only settings, demanding either model transparency or heavy\ncomputational resources. We propose Prompt2Forget (P2F), the first framework\ndesigned to tackle the LLM local privacy challenge by teaching LLM to forget.\nThe method involves decomposing full questions into smaller segments,\ngenerating fabricated answers, and obfuscating the model's memory of the\noriginal input. A benchmark dataset was crafted with questions containing\nprivacy-sensitive information from diverse fields. P2F achieves zero-shot\ngeneralization, allowing adaptability across a wide range of use cases without\nmanual adjustments. Experimental results indicate P2F's robust capability to\nobfuscate LLM's memory, attaining a forgetfulness score of around 90\\% without\nany utility loss. This represents an enhancement of up to 63\\% when contrasted\nwith the naive direct instruction technique, highlighting P2F's efficacy in\nmitigating memory retention of sensitive information within LLMs. Our findings\nestablish the first benchmark in the novel field of the LLM forgetting task,\nrepresenting a meaningful advancement in privacy preservation in the emerging\nLLM domain.",
    "updated" : "2023-12-30T01:26:42Z",
    "published" : "2023-12-30T01:26:42Z",
    "authors" : [
      {
        "name" : "Ran Yan"
      },
      {
        "name" : "Yujun Li"
      },
      {
        "name" : "Wenqian Li"
      },
      {
        "name" : "Peihua Mai"
      },
      {
        "name" : "Yan Pang"
      },
      {
        "name" : "Yinchuan Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "I.2.7"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.00058v1",
    "title" : "Exploring the language of the sharing economy: Building trust and\n  reducing privacy concern on Airbnb in German and English",
    "summary" : "The text in the profile of those offering their properties in England in\nEnglish and in Germany in German, are compared to explore whether trust is\nbuilt, and privacy concerns are reduced in the same way. Six methods of\nbuilding trust are used by the landlords: (1) the level of formality, (2)\ndistance and proximity, (3) emotiveness and humor, (4) being assertive and\npassive aggressive, (5) conformity to the platform language style and\nterminology and (6) setting boundaries. Privacy concerns are not usually\nreduced directly as this is left to the platform. The findings indicate that\nlanguage has a limited influence and the platform norms and habits are the\nbiggest influence.",
    "updated" : "2023-12-29T19:28:50Z",
    "published" : "2023-12-29T19:28:50Z",
    "authors" : [
      {
        "name" : "Alex Zarifis"
      },
      {
        "name" : "Richard Ingham"
      },
      {
        "name" : "Julia Kroenung"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.17708v1",
    "title" : "The six ways to build trust and reduce privacy concern in a Central Bank\n  Digital Currency (CBDC)",
    "summary" : "Central Bank Digital Currencies (CBDCs) have been implemented by only a\nhandful of countries, but they are being explored by many more. CBDCs are\ndigital currencies issued and backed by a central bank. Consumer trust can\nencourage or discourage the adoption of this currency, which is also a payment\nsystem and a technology. This research attempts to understand consumer trust in\nCBDCs so that the development and adoption stages are more effective and\nsatisfying for all the stakeholders.",
    "updated" : "2023-12-29T17:52:13Z",
    "published" : "2023-12-29T17:52:13Z",
    "authors" : [
      {
        "name" : "Alex Zarifis"
      },
      {
        "name" : "Xusen Cheng"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.17667v1",
    "title" : "AIJack: Security and Privacy Risk Simulator for Machine Learning",
    "summary" : "This paper introduces AIJack, an open-source library designed to assess\nsecurity and privacy risks associated with the training and deployment of\nmachine learning models. Amid the growing interest in big data and AI,\nadvancements in machine learning research and business are accelerating.\nHowever, recent studies reveal potential threats, such as the theft of training\ndata and the manipulation of models by malicious attackers. Therefore, a\ncomprehensive understanding of machine learning's security and privacy\nvulnerabilities is crucial for the safe integration of machine learning into\nreal-world products. AIJack aims to address this need by providing a library\nwith various attack and defense methods through a unified API. The library is\npublicly available on GitHub (https://github.com/Koukyosyumei/AIJack).",
    "updated" : "2023-12-29T16:10:30Z",
    "published" : "2023-12-29T16:10:30Z",
    "authors" : [
      {
        "name" : "Hideaki Takahashi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.16954v1",
    "title" : "Blockchain-based Privacy-Preserving Public Key Searchable Encryption\n  with Strong Traceability",
    "summary" : "Public key searchable encryption (PKSE) scheme allows data users to search\nover encrypted data. To identify illegal users, many traceable PKSE schemes\nhave been proposed. However, existing schemes cannot trace the keywords which\nillegal users searched and protect users' privacy simultaneously. In some\npractical applications, tracing both illegal users' identities and the keywords\nwhich they searched is quite important to against the abuse of data. It is a\nchallenge to bind users' identities and keywords while protecting their\nprivacy. Moreover, existing traceable PKSE schemes do not consider the\nunforgeability and immutability of trapdoor query records, which can lead to\nthe occurrence of frame-up and denying. In this paper, to solve these problems,\nwe propose a blockchain-based privacy-preserving PKSE with strong traceability\n(BP3KSEST) scheme. Our scheme provides the following features: (1) authorized\nusers can authenticate to trapdoor generation center and obtain trapdoors\nwithout releasing their identities and keywords; (2) when data users misbehave\nin the system, the trusted third party (TTP) can trace both their identities\nand the keywords which they searched; (3) trapdoor query records are\nunforgeable; (4) trapdoor query records are immutable because records are\nstored in blockchain. Notably, this scheme is suitable to the scenarios where\nprivacy must be considered, e.g., electronic health record (EHR). We formalize\nboth the definition and security model of our BP3KSEST scheme, and present a\nconcrete construction. Furthermore, the security of the proposed scheme is\nformally proven. Finally, the implementation and evaluation are conducted to\nanalyze its efficiency.",
    "updated" : "2023-12-28T10:58:14Z",
    "published" : "2023-12-28T10:58:14Z",
    "authors" : [
      {
        "name" : "Yue Han"
      },
      {
        "name" : "Jinguang Han"
      },
      {
        "name" : "Weizhi Meng"
      },
      {
        "name" : "Jianchang Lai"
      },
      {
        "name" : "Ge Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.16554v1",
    "title" : "A Theoretical Analysis of Efficiency Constrained Utility-Privacy\n  Bi-Objective Optimization in Federated Learning",
    "summary" : "Federated learning (FL) enables multiple clients to collaboratively learn a\nshared model without sharing their individual data. Concerns about utility,\nprivacy, and training efficiency in FL have garnered significant research\nattention. Differential privacy has emerged as a prevalent technique in FL,\nsafeguarding the privacy of individual user data while impacting utility and\ntraining efficiency. Within Differential Privacy Federated Learning (DPFL),\nprevious studies have primarily focused on the utility-privacy trade-off,\nneglecting training efficiency, which is crucial for timely completion.\nMoreover, differential privacy achieves privacy by introducing controlled\nrandomness (noise) on selected clients in each communication round. Previous\nwork has mainly examined the impact of noise level ($\\sigma$) and communication\nrounds ($T$) on the privacy-utility dynamic, overlooking other influential\nfactors like the sample ratio ($q$, the proportion of selected clients). This\npaper systematically formulates an efficiency-constrained utility-privacy\nbi-objective optimization problem in DPFL, focusing on $\\sigma$, $T$, and $q$.\nWe provide a comprehensive theoretical analysis, yielding analytical solutions\nfor the Pareto front. Extensive empirical experiments verify the validity and\nefficacy of our analysis, offering valuable guidance for low-cost parameter\ndesign in DPFL.",
    "updated" : "2023-12-27T12:37:55Z",
    "published" : "2023-12-27T12:37:55Z",
    "authors" : [
      {
        "name" : "Hanlin Gu"
      },
      {
        "name" : "Xinyuan Zhao"
      },
      {
        "name" : "Yuxing Han"
      },
      {
        "name" : "Yan Kang"
      },
      {
        "name" : "Lixin Fan"
      },
      {
        "name" : "Qiang Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.15608v1",
    "title" : "Federated learning-outcome prediction with multi-layer privacy\n  protection",
    "summary" : "Learning-outcome prediction (LOP) is a long-standing and critical problem in\neducational routes. Many studies have contributed to developing effective\nmodels while often suffering from data shortage and low generalization to\nvarious institutions due to the privacy-protection issue. To this end, this\nstudy proposes a distributed grade prediction model, dubbed FecMap, by\nexploiting the federated learning (FL) framework that preserves the private\ndata of local clients and communicates with others through a global generalized\nmodel. FecMap considers local subspace learning (LSL), which explicitly learns\nthe local features against the global features, and multi-layer privacy\nprotection (MPP), which hierarchically protects the private features, including\nmodel-shareable features and not-allowably shared features, to achieve\nclient-specific classifiers of high performance on LOP per institution. FecMap\nis then achieved in an iteration manner with all datasets distributed on\nclients by training a local neural network composed of a global part, a local\npart, and a classification head in clients and averaging the global parts from\nclients on the server. To evaluate the FecMap model, we collected three\nhigher-educational datasets of student academic records from engineering\nmajors. Experiment results manifest that FecMap benefits from the proposed LSL\nand MPP and achieves steady performance on the task of LOP, compared with the\nstate-of-the-art models. This study makes a fresh attempt at the use of\nfederated learning in the learning-analytical task, potentially paving the way\nto facilitating personalized education with privacy protection.",
    "updated" : "2023-12-25T04:29:05Z",
    "published" : "2023-12-25T04:29:05Z",
    "authors" : [
      {
        "name" : "Yupei Zhang"
      },
      {
        "name" : "Yuxin Li"
      },
      {
        "name" : "Yifei Wang"
      },
      {
        "name" : "Shuangshuang Wei"
      },
      {
        "name" : "Yunan Xu"
      },
      {
        "name" : "Xuequn Shang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "68T07",
      "I.2.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.15591v1",
    "title" : "Privacy-Preserving Neural Graph Databases",
    "summary" : "In the era of big data and rapidly evolving information systems, efficient\nand accurate data retrieval has become increasingly crucial. Neural graph\ndatabases (NGDBs) have emerged as a powerful paradigm that combines the\nstrengths of graph databases (graph DBs) and neural networks to enable\nefficient storage, retrieval, and analysis of graph-structured data. The usage\nof neural embedding storage and complex neural logical query answering provides\nNGDBs with generalization ability. When the graph is incomplete, by extracting\nlatent patterns and representations, neural graph databases can fill gaps in\nthe graph structure, revealing hidden relationships and enabling accurate query\nanswering. Nevertheless, this capability comes with inherent trade-offs, as it\nintroduces additional privacy risks to the database. Malicious attackers can\ninfer more sensitive information in the database using well-designed\ncombinatorial queries, such as by comparing the answer sets of where Turing\nAward winners born before 1950 and after 1940 lived, the living places of\nTuring Award winner Hinton are probably exposed, although the living places may\nhave been deleted in the training due to the privacy concerns. In this work,\ninspired by the privacy protection in graph embeddings, we propose a\nprivacy-preserving neural graph database (P-NGDB) to alleviate the risks of\nprivacy leakage in NGDBs. We introduce adversarial training techniques in the\ntraining stage to force the NGDBs to generate indistinguishable answers when\nqueried with private information, enhancing the difficulty of inferring\nsensitive information through combinations of multiple innocuous queries.\nExtensive experiment results on three datasets show that P-NGDB can effectively\nprotect private information in the graph database while delivering high-quality\npublic answers responses to queries.",
    "updated" : "2023-12-25T02:32:05Z",
    "published" : "2023-12-25T02:32:05Z",
    "authors" : [
      {
        "name" : "Qi Hu"
      },
      {
        "name" : "Haoran Li"
      },
      {
        "name" : "Jiaxin Bai"
      },
      {
        "name" : "Yangqiu Song"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.15420v1",
    "title" : "FedDMF: Privacy-Preserving User Attribute Prediction using Deep Matrix\n  Factorization",
    "summary" : "User attribute prediction is a crucial task in various industries. However,\nsharing user data across different organizations faces challenges due to\nprivacy concerns and legal requirements regarding personally identifiable\ninformation. Regulations such as the General Data Protection Regulation (GDPR)\nin the European Union and the Personal Information Protection Law of the\nPeople's Republic of China impose restrictions on data sharing. To address the\nneed for utilizing features from multiple clients while adhering to legal\nrequirements, federated learning algorithms have been proposed. These\nalgorithms aim to predict user attributes without directly sharing the data.\nHowever, existing approaches typically rely on matching users across companies,\nwhich can result in dishonest partners discovering user lists or the inability\nto utilize all available features. In this paper, we propose a novel algorithm\nfor predicting user attributes without requiring user matching. Our approach\ninvolves training deep matrix factorization models on different clients and\nsharing only the item vectors. This allows us to predict user attributes\nwithout sharing the user vectors themselves. The algorithm is evaluated using\nthe publicly available MovieLens dataset and demonstrate that it achieves\nsimilar performance to the FedAvg algorithm, reaching 96% of a single model's\naccuracy. The proposed algorithm is particularly well-suited for improving\ncustomer targeting and enhancing the overall customer experience. This paper\npresents a valuable contribution to the field of user attribute prediction by\noffering a novel algorithm that addresses some of the most pressing privacy\nconcerns in this area.",
    "updated" : "2023-12-24T06:49:00Z",
    "published" : "2023-12-24T06:49:00Z",
    "authors" : [
      {
        "name" : "Ming Cheung"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.15383v1",
    "title" : "SoK: Technical Implementation and Human Impact of Internet Privacy\n  Regulations",
    "summary" : "Growing recognition of the potential for exploitation of personal data and of\nthe shortcomings of prior privacy regimes has led to the passage of a multitude\nof new online privacy regulations. Some of these laws -- notably the European\nUnion's General Data Protection Regulation (GDPR) and the California Consumer\nPrivacy Act (CCPA) -- have been the focus of large bodies of research by the\ncomputer science community, while others have received less attention. In this\nwork, we analyze a set of Internet privacy and data protection regulations\ndrawn from around the world -- both those that have frequently been studied by\ncomputer scientists and those that have not -- and develop a taxonomy of rights\ngranted and obligations imposed by these laws. We then leverage this taxonomy\nto systematize 270 technical research papers published in computer science\nvenues that investigate the impact of these laws and explore how technical\nsolutions can complement legal protections. Finally, we analyze the results in\nthis space through an interdisciplinary lens and make recommendations for\nfuture work at the intersection of computer science and legal privacy.",
    "updated" : "2023-12-24T01:48:07Z",
    "published" : "2023-12-24T01:48:07Z",
    "authors" : [
      {
        "name" : "Eleanor Birrell"
      },
      {
        "name" : "Jay Rodolitz"
      },
      {
        "name" : "Angel Ding"
      },
      {
        "name" : "Jenna Lee"
      },
      {
        "name" : "Emily McReynolds"
      },
      {
        "name" : "Jevan Hutson"
      },
      {
        "name" : "Ada Lerner"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.15375v1",
    "title" : "An Empirical Study of Efficiency and Privacy of Federated Learning\n  Algorithms",
    "summary" : "In today's world, the rapid expansion of IoT networks and the proliferation\nof smart devices in our daily lives, have resulted in the generation of\nsubstantial amounts of heterogeneous data. These data forms a stream which\nrequires special handling. To handle this data effectively, advanced data\nprocessing technologies are necessary to guarantee the preservation of both\nprivacy and efficiency. Federated learning emerged as a distributed learning\nmethod that trains models locally and aggregates them on a server to preserve\ndata privacy. This paper showcases two illustrative scenarios that highlight\nthe potential of federated learning (FL) as a key to delivering efficient and\nprivacy-preserving machine learning within IoT networks. We first give the\nmathematical foundations for key aggregation algorithms in federated learning,\ni.e., FedAvg and FedProx. Then, we conduct simulations, using Flower Framework,\nto show the \\textit{efficiency} of these algorithms by training deep neural\nnetworks on common datasets and show a comparison between the accuracy and loss\nmetrics of FedAvg and FedProx. Then, we present the results highlighting the\ntrade-off between maintaining privacy versus accuracy via simulations -\ninvolving the implementation of the differential privacy (DP) method - in\nPytorch and Opacus ML frameworks on common FL datasets and data distributions\nfor both FedAvg and FedProx strategies.",
    "updated" : "2023-12-24T00:13:41Z",
    "published" : "2023-12-24T00:13:41Z",
    "authors" : [
      {
        "name" : "Sofia Zahri"
      },
      {
        "name" : "Hajar Bennouri"
      },
      {
        "name" : "Ahmed M. Abdelmoniem"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.15000v1",
    "title" : "The Impact of Cloaking Digital Footprints on User Privacy and\n  Personalization",
    "summary" : "Our online lives generate a wealth of behavioral records -'digital\nfootprints'- which are stored and leveraged by technology platforms. This data\ncan be used to create value for users by personalizing services. At the same\ntime, however, it also poses a threat to people's privacy by offering a highly\nintimate window into their private traits (e.g., their personality, political\nideology, sexual orientation). Prior work has proposed a potential remedy: The\ncloaking of users' footprints. That is, platforms could allow users to hide\nportions of their digital footprints from predictive algorithms to avoid\nundesired inferences. While such an approach has been shown to offer privacy\nprotection in the moment, there are two open questions. First, it remains\nunclear how well cloaking performs over time. As people constantly leave new\ndigital footprints, the algorithm might regain the ability to predict\npreviously cloaked traits. Second, cloaking digital footprints to avoid one\nundesirable inference may degrade the performance of models for other,\ndesirable inferences (e.g., those driving desired personalized content). In the\nlight of these research gaps, our contributions are twofold: 1) We propose a\nnovel cloaking strategy that conceals 'metafeatures' (automatically generated\nhigher-level categories) and compares its effectiveness against existing\ncloaking approaches, and 2) we test the spill-over effects of cloaking one\ntrait on the accuracy of inferences on other traits. A key finding is that the\neffectiveness of cloaking degrades over times, but the rate at which it\ndegrades is significantly smaller when cloaking metafeatures rather than\nindividual footprints. In addition, our findings reveal the expected trade-off\nbetween privacy and personalization: Cloaking an undesired trait also partially\nconceals other desirable traits.",
    "updated" : "2023-12-22T16:32:34Z",
    "published" : "2023-12-22T16:32:34Z",
    "authors" : [
      {
        "name" : "Sofie Goethals"
      },
      {
        "name" : "Sandra Matz"
      },
      {
        "name" : "Foster Provost"
      },
      {
        "name" : "Yanou Ramon"
      },
      {
        "name" : "David Martens"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.14633v1",
    "title" : "Evaluating the Security and Privacy Risk Postures of Virtual Assistants",
    "summary" : "Virtual assistants (VAs) have seen increased use in recent years due to their\nease of use for daily tasks. Despite their growing prevalence, their security\nand privacy implications are still not well understood. To address this gap, we\nconducted a study to evaluate the security and privacy postures of eight widely\nused voice assistants: Alexa, Braina, Cortana, Google Assistant, Kalliope,\nMycroft, Hound, and Extreme. We used three vulnerability testing tools,\nAndroBugs, RiskInDroid, and MobSF, to assess the security and privacy of these\nVAs. Our analysis focused on five areas: code, access control, tracking, binary\nanalysis, and sensitive data confidentiality. The results revealed that these\nVAs are vulnerable to a range of security threats, including not validating SSL\ncertificates, executing raw SQL queries, and using a weak mode of the AES\nalgorithm. These vulnerabilities could allow malicious actors to gain\nunauthorized access to users' personal information. This study is a first step\ntoward understanding the risks associated with these technologies and provides\na foundation for future research to develop more secure and privacy-respecting\nVAs.",
    "updated" : "2023-12-22T12:10:52Z",
    "published" : "2023-12-22T12:10:52Z",
    "authors" : [
      {
        "name" : "Borna Kalhor"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.14521v1",
    "title" : "Tuning Quantum Computing Privacy through Quantum Error Correction",
    "summary" : "Quantum computing is a promising paradigm for efficiently solving large and\nhigh-complexity problems. To protect quantum computing privacy, pioneering\nresearch efforts proposed to redefine differential privacy (DP) in quantum\ncomputing, i.e., quantum differential privacy (QDP), and harvest inherent\nnoises generated by quantum computing to implement QDP. However, such an\nimplementation approach is limited by the amount of inherent noises, which\nmakes the privacy budget of the QDP mechanism fixed and uncontrollable. To\naddress this issue, in this paper, we propose to leverage quantum error\ncorrection (QEC) techniques to reduce quantum computing errors, while tuning\nthe privacy protection levels in QDP. In short, we gradually decrease the\nquantum noise error rate by deciding whether to apply QEC operations on the\ngate in a multiple single qubit gates circuit. We have derived a new\ncalculation formula for the general error rate and corresponding privacy\nbudgets after QEC operation. Then, we expand to achieve further noise reduction\nusing multi-level concatenated QEC operation. Through extensive numerical\nsimulations, we demonstrate that QEC is a feasible way to regulate the degree\nof privacy protection in quantum computing.",
    "updated" : "2023-12-22T08:35:23Z",
    "published" : "2023-12-22T08:35:23Z",
    "authors" : [
      {
        "name" : "Hui Zhong"
      },
      {
        "name" : "Keyi Ju"
      },
      {
        "name" : "Manojna Sistla"
      },
      {
        "name" : "Xinyue Zhang"
      },
      {
        "name" : "Xiaoqi Qin"
      },
      {
        "name" : "Xin Fu"
      },
      {
        "name" : "Miao Pan"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.16191v1",
    "title" : "SoK: Taming the Triangle -- On the Interplays between Fairness,\n  Interpretability and Privacy in Machine Learning",
    "summary" : "Machine learning techniques are increasingly used for high-stakes\ndecision-making, such as college admissions, loan attribution or recidivism\nprediction. Thus, it is crucial to ensure that the models learnt can be audited\nor understood by human users, do not create or reproduce discrimination or\nbias, and do not leak sensitive information regarding their training data.\nIndeed, interpretability, fairness and privacy are key requirements for the\ndevelopment of responsible machine learning, and all three have been studied\nextensively during the last decade. However, they were mainly considered in\nisolation, while in practice they interplay with each other, either positively\nor negatively. In this Systematization of Knowledge (SoK) paper, we survey the\nliterature on the interactions between these three desiderata. More precisely,\nfor each pairwise interaction, we summarize the identified synergies and\ntensions. These findings highlight several fundamental theoretical and\nempirical conflicts, while also demonstrating that jointly considering these\ndifferent requirements is challenging when one aims at preserving a high level\nof utility. To solve this issue, we also discuss possible conciliation\nmechanisms, showing that a careful design can enable to successfully handle\nthese different concerns in practice.",
    "updated" : "2023-12-22T08:11:33Z",
    "published" : "2023-12-22T08:11:33Z",
    "authors" : [
      {
        "name" : "Julien Ferry"
      },
      {
        "name" : "Ulrich Aïvodji"
      },
      {
        "name" : "Sébastien Gambs"
      },
      {
        "name" : "Marie-José Huguet"
      },
      {
        "name" : "Mohamed Siala"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.14407v1",
    "title" : "AdvCloak: Customized Adversarial Cloak for Privacy Protection",
    "summary" : "With extensive face images being shared on social media, there has been a\nnotable escalation in privacy concerns. In this paper, we propose AdvCloak, an\ninnovative framework for privacy protection using generative models. AdvCloak\nis designed to automatically customize class-wise adversarial masks that can\nmaintain superior image-level naturalness while providing enhanced\nfeature-level generalization ability. Specifically, AdvCloak sequentially\noptimizes the generative adversarial networks by employing a two-stage training\nstrategy. This strategy initially focuses on adapting the masks to the unique\nindividual faces via image-specific training and then enhances their\nfeature-level generalization ability to diverse facial variations of\nindividuals via person-specific training. To fully utilize the limited training\ndata, we combine AdvCloak with several general geometric modeling methods, to\nbetter describe the feature subspace of source identities. Extensive\nquantitative and qualitative evaluations on both common and celebrity datasets\ndemonstrate that AdvCloak outperforms existing state-of-the-art methods in\nterms of efficiency and effectiveness.",
    "updated" : "2023-12-22T03:18:04Z",
    "published" : "2023-12-22T03:18:04Z",
    "authors" : [
      {
        "name" : "Xuannan Liu"
      },
      {
        "name" : "Yaoyao Zhong"
      },
      {
        "name" : "Xing Cui"
      },
      {
        "name" : "Yuhang Zhang"
      },
      {
        "name" : "Peipei Li"
      },
      {
        "name" : "Weihong Deng"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.14388v1",
    "title" : "A Generalized Shuffle Framework for Privacy Amplification: Strengthening\n  Privacy Guarantees and Enhancing Utility",
    "summary" : "The shuffle model of local differential privacy is an advanced method of\nprivacy amplification designed to enhance privacy protection with high utility.\nIt achieves this by randomly shuffling sensitive data, making linking\nindividual data points to specific individuals more challenging. However, most\nexisting studies have focused on the shuffle model based on\n$(\\epsilon_0,0)$-Locally Differentially Private (LDP) randomizers, with limited\nconsideration for complex scenarios such as $(\\epsilon_0,\\delta_0)$-LDP or\npersonalized LDP (PLDP). This hinders a comprehensive understanding of the\nshuffle model's potential and limits its application in various settings. To\nbridge this research gap, we propose a generalized shuffle framework that can\nbe applied to any $(\\epsilon_i,\\delta_i)$-PLDP setting with personalized\nprivacy parameters. This generalization allows for a broader exploration of the\nprivacy-utility trade-off and facilitates the design of privacy-preserving\nanalyses in diverse contexts. We prove that shuffled\n$(\\epsilon_i,\\delta_i)$-PLDP process approximately preserves $\\mu$-Gaussian\nDifferential Privacy with \\mu = \\sqrt{\\frac{2}{\\sum_{i=1}^{n}\n\\frac{1-\\delta_i}{1+e^{\\epsilon_i}}-\\max_{i}{\\frac{1-\\delta_{i}}{1+e^{\\epsilon_{i}}}}}}.\n$\n  This approach allows us to avoid the limitations and potential inaccuracies\nassociated with inequality estimations. To strengthen the privacy guarantee, we\nimprove the lower bound by utilizing hypothesis testing} instead of relying on\nrough estimations like the Chernoff bound or Hoeffding's inequality.\nFurthermore, extensive comparative evaluations clearly show that our approach\noutperforms existing methods in achieving strong central privacy guarantees\nwhile preserving the utility of the global model. We have also carefully\ndesigned corresponding algorithms for average function, frequency estimation,\nand stochastic gradient descent.",
    "updated" : "2023-12-22T02:31:46Z",
    "published" : "2023-12-22T02:31:46Z",
    "authors" : [
      {
        "name" : "E Chen"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Yifei Ge"
      }
    ],
    "categories" : [
      "cs.CR",
      "math.CO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.13985v1",
    "title" : "Rényi Pufferfish Privacy: General Additive Noise Mechanisms and\n  Privacy Amplification by Iteration",
    "summary" : "Pufferfish privacy is a flexible generalization of differential privacy that\nallows to model arbitrary secrets and adversary's prior knowledge about the\ndata. Unfortunately, designing general and tractable Pufferfish mechanisms that\ndo not compromise utility is challenging. Furthermore, this framework does not\nprovide the composition guarantees needed for a direct use in iterative machine\nlearning algorithms. To mitigate these issues, we introduce a R\\'enyi\ndivergence-based variant of Pufferfish and show that it allows us to extend the\napplicability of the Pufferfish framework. We first generalize the Wasserstein\nmechanism to cover a wide range of noise distributions and introduce several\nways to improve its utility. We also derive stronger guarantees against\nout-of-distribution adversaries. Finally, as an alternative to composition, we\nprove privacy amplification results for contractive noisy iterations and\nshowcase the first use of Pufferfish in private convex optimization. A common\ningredient underlying our results is the use and extension of shift reduction\nlemmas.",
    "updated" : "2023-12-21T16:18:33Z",
    "published" : "2023-12-21T16:18:33Z",
    "authors" : [
      {
        "name" : "Clément Pierquin"
      },
      {
        "name" : "Aurélien Bellet"
      },
      {
        "name" : "Marc Tommasi"
      },
      {
        "name" : "Matthieu Boussard"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.13813v1",
    "title" : "How Does Connecting Online Activities to Advertising Inferences Impact\n  Privacy Perceptions?",
    "summary" : "Data dashboards are designed to help users manage data collected about them.\nHowever, prior work showed that exposure to some dashboards, notably Google's\nMy Activity dashboard, results in significant decreases in perceived concern\nand increases in perceived benefit from data collection, contrary to\nexpectations. We theorize that this result is due to the fact that data\ndashboards currently do not sufficiently \"connect the dots\" of the data food\nchain, that is, by connecting data collection with the use of that data. To\nevaluate this, we designed a study where participants assigned advertising\ninterest labels to their own real activities, effectively acting as a\nbehavioral advertising engine to \"connect the dots.\" When comparing pre- and\npost-labeling task responses, we find no significant difference in concern with\nGoogle's data collection practices, which indicates that participants' priors\nare maintained after more exposure to the data food chain (differing from prior\nwork), suggesting that data dashboards that offer deeper perspectives of how\ndata collection is used have potential. However, these gains are offset when\nparticipants are exposed to their true interest labels inferred by Google.\nConcern for data collection dropped significantly as participants viewed\nGoogle's labeling as generic compared to their own more specific labeling. This\npresents a possible new paradox that must be overcome when designing data\ndashboards, the generic paradox, which occurs when users misalign individual,\ngeneric inferences from collected data as benign compared to the totality and\nspecificity of many generic inferences made about them.",
    "updated" : "2023-12-21T13:05:09Z",
    "published" : "2023-12-21T13:05:09Z",
    "authors" : [
      {
        "name" : "Florian M. Farke"
      },
      {
        "name" : "David G. Balash"
      },
      {
        "name" : "Maximilian Golla"
      },
      {
        "name" : "Adam J. Aviv"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.13712v1",
    "title" : "Conciliating Privacy and Utility in Data Releases via Individual\n  Differential Privacy and Microaggregation",
    "summary" : "$\\epsilon$-Differential privacy (DP) is a well-known privacy model that\noffers strong privacy guarantees. However, when applied to data releases, DP\nsignificantly deteriorates the analytical utility of the protected outcomes. To\nkeep data utility at reasonable levels, practical applications of DP to data\nreleases have used weak privacy parameters (large $\\epsilon$), which dilute the\nprivacy guarantees of DP. In this work, we tackle this issue by using an\nalternative formulation of the DP privacy guarantees, named\n$\\epsilon$-individual differential privacy (iDP), which causes less data\ndistortion while providing the same protection as DP to subjects. We enforce\niDP in data releases by relying on attribute masking plus a pre-processing step\nbased on data microaggregation. The goal of this step is to reduce the\nsensitivity to record changes, which determines the amount of noise required to\nenforce iDP (and DP). Specifically, we propose data microaggregation strategies\ndesigned for iDP whose sensitivities are significantly lower than those used in\nDP. As a result, we obtain iDP-protected data with significantly better utility\nthan with DP. We report on experiments that show how our approach can provide\nstrong privacy (small $\\epsilon$) while yielding protected data that do not\nsignificantly degrade the accuracy of secondary data analysis.",
    "updated" : "2023-12-21T10:23:18Z",
    "published" : "2023-12-21T10:23:18Z",
    "authors" : [
      {
        "name" : "Jordi Soria-Comas"
      },
      {
        "name" : "David Sánchez"
      },
      {
        "name" : "Josep Domingo-Ferrer"
      },
      {
        "name" : "Sergio Martínez"
      },
      {
        "name" : "Luis Del Vasto-Terrientes"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.13389v1",
    "title" : "Enhancing Trade-offs in Privacy, Utility, and Computational Efficiency\n  through MUltistage Sampling Technique (MUST)",
    "summary" : "Applying a randomized algorithm to a subset of a dataset rather than the\nentire dataset is a common approach to amplify its privacy guarantees in the\nreleased information. We propose a class of subsampling methods named\nMUltistage Sampling Technique (MUST) for privacy amplification (PA) in the\ncontext of differential privacy (DP). We conduct comprehensive analyses of the\nPA effects and utility for several 2-stage MUST procedures, namely, MUST.WO,\nMUST.OW, and MUST.WW that respectively represent sampling with (W), without\n(O), with (W) replacement from the original dataset in stage I and then\nsampling without (O), with (W), with (W) replacement in stage II from the\nsubset drawn in stage I. We also provide the privacy composition analysis over\nrepeated applications of MUST via the Fourier accountant algorithm. Our\ntheoretical and empirical results suggest that MUST.OW and MUST.WW have\nstronger PA in $\\epsilon$ than the common one-stage sampling procedures\nincluding Poisson sampling, sampling without replacement, and sampling with\nreplacement, while the results on $\\delta$ vary case by case. We also prove\nthat MUST.WO is equivalent to sampling with replacement in PA. Furthermore, the\nfinal subset generated by a MUST procedure is a multiset that may contain\nmultiple copies of the same data points due to sampling with replacement\ninvolved, which enhances the computational efficiency of algorithms that\nrequire complex function calculations on distinct data points (e.g., gradient\ndescent). Our utility experiments show that MUST delivers similar or improved\nutility and stability in the privacy-preserving outputs compared to one-stage\nsubsampling methods at similar privacy loss. MUST can be seamlessly integrated\ninto stochastic optimization algorithms or procedures that involve parallel or\nsimultaneous subsampling (e.g., bagging and subsampling bootstrap) when DP\nguarantees are necessary.",
    "updated" : "2023-12-20T19:38:29Z",
    "published" : "2023-12-20T19:38:29Z",
    "authors" : [
      {
        "name" : "Xingyuan Zhao"
      },
      {
        "name" : "Fang Liu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.13334v1",
    "title" : "Transparency and Privacy: The Role of Explainable AI and Federated\n  Learning in Financial Fraud Detection",
    "summary" : "Fraudulent transactions and how to detect them remain a significant problem\nfor financial institutions around the world. The need for advanced fraud\ndetection systems to safeguard assets and maintain customer trust is paramount\nfor financial institutions, but some factors make the development of effective\nand efficient fraud detection systems a challenge. One of such factors is the\nfact that fraudulent transactions are rare and that many transaction datasets\nare imbalanced; that is, there are fewer significant samples of fraudulent\ntransactions than legitimate ones. This data imbalance can affect the\nperformance or reliability of the fraud detection model. Moreover, due to the\ndata privacy laws that all financial institutions are subject to follow,\nsharing customer data to facilitate a higher-performing centralized model is\nimpossible. Furthermore, the fraud detection technique should be transparent so\nthat it does not affect the user experience. Hence, this research introduces a\nnovel approach using Federated Learning (FL) and Explainable AI (XAI) to\naddress these challenges. FL enables financial institutions to collaboratively\ntrain a model to detect fraudulent transactions without directly sharing\ncustomer data, thereby preserving data privacy and confidentiality. Meanwhile,\nthe integration of XAI ensures that the predictions made by the model can be\nunderstood and interpreted by human experts, adding a layer of transparency and\ntrust to the system. Experimental results, based on realistic transaction\ndatasets, reveal that the FL-based fraud detection system consistently\ndemonstrates high performance metrics. This study grounds FL's potential as an\neffective and privacy-preserving tool in the fight against fraud.",
    "updated" : "2023-12-20T18:26:59Z",
    "published" : "2023-12-20T18:26:59Z",
    "authors" : [
      {
        "name" : "Tomisin Awosika"
      },
      {
        "name" : "Raj Mani Shukla"
      },
      {
        "name" : "Bernardi Pranggono"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.13312v1",
    "title" : "Multi-label Learning from Privacy-Label",
    "summary" : "Multi-abel Learning (MLL) often involves the assignment of multiple relevant\nlabels to each instance, which can lead to the leakage of sensitive information\n(such as smoking, diseases, etc.) about the instances. However, existing MLL\nsuffer from failures in protection for sensitive information. In this paper, we\npropose a novel setting named Multi-Label Learning from Privacy-Label (MLLPL),\nwhich Concealing Labels via Privacy-Label Unit (CLPLU). Specifically, during\nthe labeling phase, each privacy-label is randomly combined with a non-privacy\nlabel to form a Privacy-Label Unit (PLU). If any label within a PLU is\npositive, the unit is labeled as positive; otherwise, it is labeled negative,\nas shown in Figure 1. PLU ensures that only non-privacy labels are appear in\nthe label set, while the privacy-labels remain concealed. Moreover, we further\npropose a Privacy-Label Unit Loss (PLUL) to learn the optimal classifier by\nminimizing the empirical risk of PLU. Experimental results on multiple\nbenchmark datasets demonstrate the effectiveness and superiority of the\nproposed method.",
    "updated" : "2023-12-20T09:09:56Z",
    "published" : "2023-12-20T09:09:56Z",
    "authors" : [
      {
        "name" : "Zhongnian Li"
      },
      {
        "name" : "Haotian Ren"
      },
      {
        "name" : "Tongfeng Sun"
      },
      {
        "name" : "Zhichen Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.12216v1",
    "title" : "Sharing is CAIRing: Characterizing Principles and Assessing Properties\n  of Universal Privacy Evaluation for Synthetic Tabular Data",
    "summary" : "Data sharing is a necessity for innovative progress in many domains,\nespecially in healthcare. However, the ability to share data is hindered by\nregulations protecting the privacy of natural persons. Synthetic tabular data\nprovide a promising solution to address data sharing difficulties but does not\ninherently guarantee privacy. Still, there is a lack of agreement on\nappropriate methods for assessing the privacy-preserving capabilities of\nsynthetic data, making it difficult to compare results across studies. To the\nbest of our knowledge, this is the first work to identify properties that\nconstitute good universal privacy evaluation metrics for synthetic tabular\ndata. The goal of such metrics is to enable comparability across studies and to\nallow non-technical stakeholders to understand how privacy is protected. We\nidentify four principles for the assessment of metrics: Comparability,\nApplicability, Interpretability, and Representativeness (CAIR). To quantify and\nrank the degree to which evaluation metrics conform to the CAIR principles, we\ndesign a rubric using a scale of 1-4. Each of the four properties is scored on\nfour parameters, yielding 16 total dimensions. We study the applicability and\nusefulness of the CAIR principles and rubric by assessing a selection of\nmetrics popular in other studies. The results provide granular insights into\nthe strengths and weaknesses of existing metrics that not only rank the metrics\nbut highlight areas of potential improvements. We expect that the CAIR\nprinciples will foster agreement among researchers and organizations on which\nuniversal privacy evaluation metrics are appropriate for synthetic tabular\ndata.",
    "updated" : "2023-12-19T15:05:52Z",
    "published" : "2023-12-19T15:05:52Z",
    "authors" : [
      {
        "name" : "Tobias Hyrup"
      },
      {
        "name" : "Anton Danholt Lautrup"
      },
      {
        "name" : "Arthur Zimek"
      },
      {
        "name" : "Peter Schneider-Kamp"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.12183v2",
    "title" : "Poincaré Differential Privacy for Hierarchy-Aware Graph Embedding",
    "summary" : "Hierarchy is an important and commonly observed topological property in\nreal-world graphs that indicate the relationships between supervisors and\nsubordinates or the organizational behavior of human groups. As hierarchy is\nintroduced as a new inductive bias into the Graph Neural Networks (GNNs) in\nvarious tasks, it implies latent topological relations for attackers to improve\ntheir inference attack performance, leading to serious privacy leakage issues.\nIn addition, existing privacy-preserving frameworks suffer from reduced\nprotection ability in hierarchical propagation due to the deficiency of\nadaptive upper-bound estimation of the hierarchical perturbation boundary. It\nis of great urgency to effectively leverage the hierarchical property of data\nwhile satisfying privacy guarantees. To solve the problem, we propose the\nPoincar\\'e Differential Privacy framework, named PoinDP, to protect the\nhierarchy-aware graph embedding based on hyperbolic geometry. Specifically,\nPoinDP first learns the hierarchy weights for each entity based on the\nPoincar\\'e model in hyperbolic space. Then, the Personalized Hierarchy-aware\nSensitivity is designed to measure the sensitivity of the hierarchical\nstructure and adaptively allocate the privacy protection strength. Besides, the\nHyperbolic Gaussian Mechanism (HGM) is proposed to extend the Gaussian\nmechanism in Euclidean space to hyperbolic space to realize random\nperturbations that satisfy differential privacy under the hyperbolic space\nmetric. Extensive experiment results on five real-world datasets demonstrate\nthe proposed PoinDP's advantages of effective privacy protection while\nmaintaining good performance on the node classification task.",
    "updated" : "2023-12-20T13:29:23Z",
    "published" : "2023-12-19T14:15:20Z",
    "authors" : [
      {
        "name" : "Yuecen Wei"
      },
      {
        "name" : "Haonan Yuan"
      },
      {
        "name" : "Xingcheng Fu"
      },
      {
        "name" : "Qingyun Sun"
      },
      {
        "name" : "Hao Peng"
      },
      {
        "name" : "Xianxian Li"
      },
      {
        "name" : "Chunming Hu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.11845v1",
    "title" : "A Summary of Privacy-Preserving Data Publishing in the Local Setting",
    "summary" : "The exponential growth of collected, processed, and shared data has given\nrise to concerns about individuals' privacy. Consequently, various laws and\nregulations have been established to oversee how organizations handle and\nsafeguard data. One such method is Statistical Disclosure Control, which aims\nto minimize the risk of exposing confidential information by de-identifying it.\nThis de-identification is achieved through specific privacy-preserving\ntechniques. However, a trade-off exists: de-identified data can often lead to a\nloss of information, which might impact the accuracy of data analysis and the\npredictive capability of models. The overarching goal remains to safeguard\nindividual privacy while preserving the data's interpretability, meaning its\noverall usefulness. Despite advances in Statistical Disclosure Control, the\nfield continues to evolve, with no definitive solution that strikes an optimal\nbalance between privacy and utility. This survey delves into the intricate\nprocesses of de-identification. We outline the current privacy-preserving\ntechniques employed in microdata de-identification, delve into privacy measures\ntailored for various disclosure scenarios, and assess metrics for information\nloss and predictive performance. Herein, we tackle the primary challenges posed\nby privacy constraints, overview predominant strategies to mitigate these\nchallenges, categorize privacy-preserving techniques, offer a theoretical\nassessment of current comparative research, and highlight numerous unresolved\nissues in the domain.",
    "updated" : "2023-12-19T04:23:23Z",
    "published" : "2023-12-19T04:23:23Z",
    "authors" : [
      {
        "name" : "Wenjun Lin"
      },
      {
        "name" : "Jiahao Qian"
      },
      {
        "name" : "Wenwen Liu"
      },
      {
        "name" : "Lang Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.11712v1",
    "title" : "A Simple and Practical Method for Reducing the Disparate Impact of\n  Differential Privacy",
    "summary" : "Differentially private (DP) mechanisms have been deployed in a variety of\nhigh-impact social settings (perhaps most notably by the U.S. Census). Since\nall DP mechanisms involve adding noise to results of statistical queries, they\nare expected to impact our ability to accurately analyze and learn from data,\nin effect trading off privacy with utility. Alarmingly, the impact of DP on\nutility can vary significantly among different sub-populations. A simple way to\nreduce this disparity is with stratification. First compute an independent\nprivate estimate for each group in the data set (which may be the intersection\nof several protected classes), then, to compute estimates of global statistics,\nappropriately recombine these group estimates. Our main observation is that\nnaive stratification often yields high-accuracy estimates of population-level\nstatistics, without the need for additional privacy budget. We support this\nobservation theoretically and empirically. Our theoretical results center on\nthe private mean estimation problem, while our empirical results center on\nextensive experiments on private data synthesis to demonstrate the\neffectiveness of stratification on a variety of private mechanisms. Overall, we\nargue that this straightforward approach provides a strong baseline against\nwhich future work on reducing utility disparities of DP mechanisms should be\ncompared.",
    "updated" : "2023-12-18T21:19:35Z",
    "published" : "2023-12-18T21:19:35Z",
    "authors" : [
      {
        "name" : "Lucas Rosenblatt"
      },
      {
        "name" : "Julia Stoyanovich"
      },
      {
        "name" : "Christopher Musco"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.11126v1",
    "title" : "Harnessing Inherent Noises for Privacy Preservation in Quantum Machine\n  Learning",
    "summary" : "Quantum computing revolutionizes the way of solving complex problems and\nhandling vast datasets, which shows great potential to accelerate the machine\nlearning process. However, data leakage in quantum machine learning (QML) may\npresent privacy risks. Although differential privacy (DP), which protects\nprivacy through the injection of artificial noise, is a well-established\napproach, its application in the QML domain remains under-explored. In this\npaper, we propose to harness inherent quantum noises to protect data privacy in\nQML. Especially, considering the Noisy Intermediate-Scale Quantum (NISQ)\ndevices, we leverage the unavoidable shot noise and incoherent noise in quantum\ncomputing to preserve the privacy of QML models for binary classification. We\nmathematically analyze that the gradient of quantum circuit parameters in QML\nsatisfies a Gaussian distribution, and derive the upper and lower bounds on its\nvariance, which can potentially provide the DP guarantee. Through simulations,\nwe show that a target privacy protection level can be achieved by running the\nquantum circuit a different number of times.",
    "updated" : "2023-12-18T11:52:44Z",
    "published" : "2023-12-18T11:52:44Z",
    "authors" : [
      {
        "name" : "Keyi Ju"
      },
      {
        "name" : "Xiaoqi Qin"
      },
      {
        "name" : "Hui Zhong"
      },
      {
        "name" : "Xinyue Zhang"
      },
      {
        "name" : "Miao Pan"
      },
      {
        "name" : "Baoling Liu"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.11581v1",
    "title" : "Protect Your Score: Contact Tracing With Differential Privacy Guarantees",
    "summary" : "The pandemic in 2020 and 2021 had enormous economic and societal\nconsequences, and studies show that contact tracing algorithms can be key in\nthe early containment of the virus. While large strides have been made towards\nmore effective contact tracing algorithms, we argue that privacy concerns\ncurrently hold deployment back. The essence of a contact tracing algorithm\nconstitutes the communication of a risk score. Yet, it is precisely the\ncommunication and release of this score to a user that an adversary can\nleverage to gauge the private health status of an individual. We pinpoint a\nrealistic attack scenario and propose a contact tracing algorithm with\ndifferential privacy guarantees against this attack. The algorithm is tested on\nthe two most widely used agent-based COVID19 simulators and demonstrates\nsuperior performance in a wide range of settings. Especially for realistic test\nscenarios and while releasing each risk score with epsilon=1 differential\nprivacy, we achieve a two to ten-fold reduction in the infection rate of the\nvirus. To the best of our knowledge, this presents the first contact tracing\nalgorithm with differential privacy guarantees when revealing risk scores for\nCOVID19.",
    "updated" : "2023-12-18T11:16:33Z",
    "published" : "2023-12-18T11:16:33Z",
    "authors" : [
      {
        "name" : "Rob Romijnders"
      },
      {
        "name" : "Christos Louizos"
      },
      {
        "name" : "Yuki M. Asano"
      },
      {
        "name" : "Max Welling"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.11575v1",
    "title" : "Blind-Touch: Homomorphic Encryption-Based Distributed Neural Network\n  Inference for Privacy-Preserving Fingerprint Authentication",
    "summary" : "Fingerprint authentication is a popular security mechanism for smartphones\nand laptops. However, its adoption in web and cloud environments has been\nlimited due to privacy concerns over storing and processing biometric data on\nservers. This paper introduces Blind-Touch, a novel machine learning-based\nfingerprint authentication system leveraging homomorphic encryption to address\nthese privacy concerns. Homomorphic encryption allows computations on encrypted\ndata without decrypting. Thus, Blind-Touch can keep fingerprint data encrypted\non the server while performing machine learning operations. Blind-Touch\ncombines three strategies to efficiently utilize homomorphic encryption in\nmachine learning: (1) It optimizes the feature vector for a distributed\narchitecture, processing the first fully connected layer (FC-16) in plaintext\non the client side and the subsequent layer (FC-1) post-encryption on the\nserver, thereby minimizing encrypted computations; (2) It employs a homomorphic\nencryptioncompatible data compression technique capable of handling 8,192\nauthentication results concurrently; and (3) It utilizes a clustered server\narchitecture to simultaneously process authentication results, thereby\nenhancing scalability with increasing user numbers. Blind-Touch achieves high\naccuracy on two benchmark fingerprint datasets, with a 93.6% F1- score for the\nPolyU dataset and a 98.2% F1-score for the SOKOTO dataset. Moreover,\nBlind-Touch can match a fingerprint among 5,000 in about 0.65 seconds. With its\nprivacyfocused design, high accuracy, and efficiency, Blind-Touch is a\npromising alternative to conventional fingerprint authentication for web and\ncloud applications.",
    "updated" : "2023-12-18T09:05:34Z",
    "published" : "2023-12-18T09:05:34Z",
    "authors" : [
      {
        "name" : "Hyunmin Choi"
      },
      {
        "name" : "Simon Woo"
      },
      {
        "name" : "Hyoungshick Kim"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.10951v1",
    "title" : "Viral Privacy: Contextual Integrity as a Lens to Understand Content\n  Creators' Privacy Perceptions and Needs After Sudden Attention",
    "summary" : "When designing multi-stakeholder privacy systems, it is important to consider\nhow different groups of social media users have different goals and\nrequirements for privacy. Additionally, we must acknowledge that it is\nimportant to keep in mind that even a single creator's needs can change as\ntheir online visibility and presence shifts, and that robust multi-stakeholder\nprivacy systems should account for these shifts. Using the framework of\ncontextual integrity, we explain a theoretical basis for how to evaluate the\npotential changing privacy needs of users as their profiles undergo a sudden\nrise in online attention, and ongoing projects to understand these potential\nshifts in perspectives.",
    "updated" : "2023-12-18T06:04:01Z",
    "published" : "2023-12-18T06:04:01Z",
    "authors" : [
      {
        "name" : "Joseph S. Schafer"
      },
      {
        "name" : "Annie Denton"
      },
      {
        "name" : "Chloe Seelhoff"
      },
      {
        "name" : "Jordyn Vo"
      },
      {
        "name" : "Kate Starbird"
      }
    ],
    "categories" : [
      "cs.SI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.11564v1",
    "title" : "Privacy-preserving transactive energy systems: Key topics and open\n  research challenges",
    "summary" : "This manuscript aims to formalize and conclude the discussions initiated\nduring the PriTEM workshop 22-23 March 2023. We present important ideas and\ndiscussion topics in the context of transactive energy systems. Moreover, the\nconclusions from the discussions articulate potential aspects to be explored in\nfuture studies on transactive energy management. Particularly, these\nconclusions cover research topics in energy technology and energy informatics,\nenergy law, data law, energy market and socio-psychology that are relevant to\nthe seamless integration of renewable energy resources and the transactive\nenergy systems-in smart microgrids-focusing on distributed frameworks such as\npeer-to-peer (P2P) energy trading. We clarify issues, identify barriers, and\nsuggest possible solutions to open questions in diversified topics, such as\nblock-chain interoperability, consumer privacy and data sharing, and\nparticipation incentivization. Furthermore, we also elaborate challenges\nassociated with cross-disciplinary collaboration and coordination for\ntransactive energy systems, and enumerate the lessons learned from our work so\nfar.",
    "updated" : "2023-12-17T21:23:44Z",
    "published" : "2023-12-17T21:23:44Z",
    "authors" : [
      {
        "name" : "Daniel Gerbi Duguma"
      },
      {
        "name" : "Juliana Zhang"
      },
      {
        "name" : "Meysam Aboutalebi"
      },
      {
        "name" : "Shiliang Zhang"
      },
      {
        "name" : "Catherine Banet"
      },
      {
        "name" : "Cato Bjørkli"
      },
      {
        "name" : "Chinmayi Baramashetru"
      },
      {
        "name" : "Frank Eliassen"
      },
      {
        "name" : "Hui Zhang"
      },
      {
        "name" : "Jonathan Muringani"
      },
      {
        "name" : "Josef Noll"
      },
      {
        "name" : "Knut Inge Fostervold"
      },
      {
        "name" : "Lars Böcker"
      },
      {
        "name" : "Lee Andrew Bygrave"
      },
      {
        "name" : "Matin Bagherpour"
      },
      {
        "name" : "Maunya Doroudi Moghadam"
      },
      {
        "name" : "Olaf Owe"
      },
      {
        "name" : "Poushali Sengupta"
      },
      {
        "name" : "Roman Vitenberg"
      },
      {
        "name" : "Sabita Maharjan"
      },
      {
        "name" : "Thiago Garrett"
      },
      {
        "name" : "Yushuai Li"
      },
      {
        "name" : "Zhengyu Shan"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.CR",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.10789v1",
    "title" : "Federated learning with differential privacy and an untrusted aggregator",
    "summary" : "Federated learning for training models over mobile devices is gaining\npopularity. Current systems for this task exhibit significant trade-offs\nbetween model accuracy, privacy guarantee, and device efficiency. For instance,\nOort (OSDI 2021) provides excellent accuracy and efficiency but requires a\ntrusted central server. On the other hand, Orchard (OSDI 2020) provides good\naccuracy and the rigorous guarantee of differential privacy over an untrusted\nserver, but creates huge overhead for the devices. This paper describes Aero, a\nnew federated learning system that significantly improves this trade-off. Aero\nguarantees good accuracy, differential privacy over an untrusted server, and\nkeeps the device overhead low. The key idea of Aero is to tune system\narchitecture and design to a specific set of popular, federated learning\nalgorithms. This tuning requires novel optimizations and techniques, e.g., a\nnew protocol to securely aggregate updates from devices. An evaluation of Aero\ndemonstrates that it provides comparable accuracy to plain federated learning\n(without differential privacy), and it improves efficiency (CPU and network)\nover Orchard by up to $10^5\\times$.",
    "updated" : "2023-12-17T18:26:10Z",
    "published" : "2023-12-17T18:26:10Z",
    "authors" : [
      {
        "name" : "Kunlong Liu"
      },
      {
        "name" : "Trinabh Gupta"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.10698v1",
    "title" : "HE-DKSAP: Privacy-Preserving Stealth Address Protocol via Additively\n  Homomorphic Encryption",
    "summary" : "Blockchain transactions have gained widespread adoption across various\nindustries, largely attributable to their unparalleled transparency and robust\nsecurity features. Nevertheless, this technique introduces various privacy\nconcerns, including pseudonymity, Sybil attacks, and potential susceptibilities\nto quantum computing, to name a few. In response to these challenges,\ninnovative privacy-enhancing solutions like zero-knowledge proofs, homomorphic\nencryption, and stealth addresses (SA) have been developed. Among the various\nschemes, SA stands out as it prevents the association of a blockchain\ntransaction's output with the recipient's public address, thereby ensuring\ntransactional anonymity. However, the basic SA schemes have exhibited\nvulnerabilities to key leakage and quantum computing attacks. To address these\nshortcomings, we present a pioneering solution - Homomorphic Encryption-based\nDual-Key Stealth Address Protocol (HE-DKSAP), which can be further extended to\nFully HE-DKSAP (FHE-DKSAP). By leveraging the power of homomorphic encryption,\nHE-DKSAP introduces a novel approach to safeguarding transaction privacy and\npreventing potential quantum computing attacks. This paper delves into the core\nprinciples of HE-DKSAP, highlighting its capacity to enhance privacy,\nscalability, and security in programmable blockchains. Through a comprehensive\nexploration of its design architecture, security analysis, and practical\nimplementations, this work establishes a privacy-preserving, practical, and\nefficient stealth address protocol via additively homomorphic encryption.",
    "updated" : "2023-12-17T12:23:49Z",
    "published" : "2023-12-17T12:23:49Z",
    "authors" : [
      {
        "name" : "Yuping Yan"
      },
      {
        "name" : "George Shao"
      },
      {
        "name" : "Dennis Song"
      },
      {
        "name" : "Mason Song"
      },
      {
        "name" : "Yaochu Jin"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.10645v1",
    "title" : "FedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph\n  Completion",
    "summary" : "Knowledge graph completion (KGC) aims to predict missing facts in knowledge\ngraphs (KGs), which is crucial as modern KGs remain largely incomplete. While\ntraining KGC models on multiple aligned KGs can improve performance, previous\nmethods that rely on transferring raw data among KGs raise privacy concerns. To\naddress this challenge, we propose a new federated learning framework that\nimplicitly aggregates knowledge from multiple KGs without demanding raw data\nexchange and entity alignment. We treat each KG as a client that trains a local\nlanguage model through textbased knowledge representation learning. A central\nserver then aggregates the model weights from clients. As natural language\nprovides a universal representation, the same knowledge thus has similar\nsemantic representations across KGs. As such, the aggregated language model can\nleverage complementary knowledge from multilingual KGs without demanding raw\nuser data sharing. Extensive experiments on a benchmark dataset demonstrate\nthat our method substantially improves KGC on multilingual KGs, achieving\ncomparable performance to state-of-the-art alignment-based models without\nrequiring any labeled alignments or raw user data sharing. Our codes will be\npublicly available.",
    "updated" : "2023-12-17T08:09:27Z",
    "published" : "2023-12-17T08:09:27Z",
    "authors" : [
      {
        "name" : "Wei Tang"
      },
      {
        "name" : "Zhiqian Wu"
      },
      {
        "name" : "Yixin Cao"
      },
      {
        "name" : "Yong Liao"
      },
      {
        "name" : "Pengyuan Zhou"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.10380v1",
    "title" : "PPIDSG: A Privacy-Preserving Image Distribution Sharing Scheme with GAN\n  in Federated Learning",
    "summary" : "Federated learning (FL) has attracted growing attention since it allows for\nprivacy-preserving collaborative training on decentralized clients without\nexplicitly uploading sensitive data to the central server. However, recent\nworks have revealed that it still has the risk of exposing private data to\nadversaries. In this paper, we conduct reconstruction attacks and enhance\ninference attacks on various datasets to better understand that sharing trained\nclassification model parameters to a central server is the main problem of\nprivacy leakage in FL. To tackle this problem, a privacy-preserving image\ndistribution sharing scheme with GAN (PPIDSG) is proposed, which consists of a\nblock scrambling-based encryption algorithm, an image distribution sharing\nmethod, and local classification training. Specifically, our method can capture\nthe distribution of a target image domain which is transformed by the block\nencryption algorithm, and upload generator parameters to avoid classifier\nsharing with negligible influence on model performance. Furthermore, we apply a\nfeature extractor to motivate model utility and train it separately from the\nclassifier. The extensive experimental results and security analyses\ndemonstrate the superiority of our proposed scheme compared to other\nstate-of-the-art defense methods. The code is available at\nhttps://github.com/ytingma/PPIDSG.",
    "updated" : "2023-12-16T08:32:29Z",
    "published" : "2023-12-16T08:32:29Z",
    "authors" : [
      {
        "name" : "Yuting Ma"
      },
      {
        "name" : "Yuanzhi Yao"
      },
      {
        "name" : "Xiaohua Xu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.10108v1",
    "title" : "Privacy-Aware Document Visual Question Answering",
    "summary" : "Document Visual Question Answering (DocVQA) is a fast growing branch of\ndocument understanding. Despite the fact that documents contain sensitive or\ncopyrighted information, none of the current DocVQA methods offers strong\nprivacy guarantees.\n  In this work, we explore privacy in the domain of DocVQA for the first time.\nWe highlight privacy issues in state of the art multi-modal LLM models used for\nDocVQA, and explore possible solutions.\n  Specifically, we focus on the invoice processing use case as a realistic,\nwidely used scenario for document understanding, and propose a large scale\nDocVQA dataset comprising invoice documents and associated questions and\nanswers. We employ a federated learning scheme, that reflects the real-life\ndistribution of documents in different businesses, and we explore the use case\nwhere the ID of the invoice issuer is the sensitive information to be\nprotected.\n  We demonstrate that non-private models tend to memorise, behaviour that can\nlead to exposing private information. We then evaluate baseline training\nschemes employing federated learning and differential privacy in this\nmulti-modal scenario, where the sensitive information might be exposed through\nany of the two input modalities: vision (document image) or language (OCR\ntokens).\n  Finally, we design an attack exploiting the memorisation effect of the model,\nand demonstrate its effectiveness in probing different DocVQA models.",
    "updated" : "2023-12-15T06:30:55Z",
    "published" : "2023-12-15T06:30:55Z",
    "authors" : [
      {
        "name" : "Rubèn Tito"
      },
      {
        "name" : "Khanh Nguyen"
      },
      {
        "name" : "Marlon Tobaben"
      },
      {
        "name" : "Raouf Kerkouche"
      },
      {
        "name" : "Mohamed Ali Souibgui"
      },
      {
        "name" : "Kangsoo Jung"
      },
      {
        "name" : "Lei Kang"
      },
      {
        "name" : "Ernest Valveny"
      },
      {
        "name" : "Antti Honkela"
      },
      {
        "name" : "Mario Fritz"
      },
      {
        "name" : "Dimosthenis Karatzas"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.10096v1",
    "title" : "Open Government Data Programs and Information Privacy Concerns: A\n  Literature Review",
    "summary" : "This study presents a narrative review of the literature on privacy concerns\nof Open Government Data (OGD) programs and identifies suggested technical,\nprocedural, and legal remedies. Peer-reviewed articles were identified and\nanalysed from major bibliographic databases, including Web of Science, Digital\nACM Library, IEEE Explore Digital Library and Science Direct. Included articles\nfocus on identifying individual information privacy concerns from the viewpoint\nof OGD stakeholders or providing solutions for mitigating concerns and risks.\nPapers that discussed and focused on general privacy issues or privacy concerns\nof open data in general or open science privacy concerns were excluded. Three\nstreams of research were identified: 1) exploring privacy concerns and balance\nwith OGD value propositions, 2) proposing solutions for mitigating privacy\nconcerns, and 3) developing risk-based frameworks for the OGD program at\ndifferent governmental levels. Findings suggest that contradictions with Fair\nInformation Practices, reidentification risks, conflicts with OGD value\npropositions, and smart city data practices are significant privacy concerns in\nthe literature. Proposed solutions include technical, legal, and procedural\nmeasures to mitigate privacy concerns. Building on the findings, practical\nimplications and suggested future research directions are provided.",
    "updated" : "2023-12-14T16:03:49Z",
    "published" : "2023-12-14T16:03:49Z",
    "authors" : [
      {
        "name" : "Mehdi Barati"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.DB",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.08685v1",
    "title" : "Privacy Amplification by Iteration for ADMM with (Strongly) Convex\n  Objective Functions",
    "summary" : "We examine a private ADMM variant for (strongly) convex objectives which is a\nprimal-dual iterative method. Each iteration has a user with a private function\nused to update the primal variable, masked by Gaussian noise for local privacy,\nwithout directly adding noise to the dual variable. Privacy amplification by\niteration explores if noises from later iterations can enhance the privacy\nguarantee when releasing final variables after the last iteration. Cyffers et\nal. [ICML 2023] explored privacy amplification by iteration for the proximal\nADMM variant, where a user's entire private function is accessed and noise is\nadded to the primal variable. In contrast, we examine a private ADMM variant\nrequiring just one gradient access to a user's function, but both primal and\ndual variables must be passed between successive iterations. To apply Balle et\nal.'s [NeurIPS 2019] coupling framework to the gradient ADMM variant, we tackle\ntechnical challenges with novel ideas. First, we address the non-expansive\nmapping issue in ADMM iterations by using a customized norm. Second, because\nthe dual variables are not masked with any noise directly, their privacy\nguarantees are achieved by treating two consecutive noisy ADMM iterations as a\nMarkov operator. Our main result is that the privacy guarantee for the gradient\nADMM variant can be amplified proportionally to the number of iterations. For\nstrongly convex objective functions, this amplification exponentially increases\nwith the number of iterations. These amplification results align with the\npreviously studied special case of stochastic gradient descent.",
    "updated" : "2023-12-14T06:52:29Z",
    "published" : "2023-12-14T06:52:29Z",
    "authors" : [
      {
        "name" : "T-H. Hubert Chan"
      },
      {
        "name" : "Hao Xie"
      },
      {
        "name" : "Mengshi Zhao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.08210v1",
    "title" : "Differential Privacy Preserving Quantum Computing via Projection\n  Operator Measurements",
    "summary" : "Quantum computing has been widely applied in various fields, such as quantum\nphysics simulations, quantum machine learning, and big data analysis. However,\nin the domains of data-driven paradigm, how to ensure the privacy of the\ndatabase is becoming a vital problem. For classical computing, we can\nincorporate the concept of differential privacy (DP) to meet the standard of\nprivacy preservation by manually adding the noise. In the quantum computing\nscenario, researchers have extended classic DP to quantum differential privacy\n(QDP) by considering the quantum noise. In this paper, we propose a novel\napproach to satisfy the QDP definition by considering the errors generated by\nthe projection operator measurement, which is denoted as shot noises. Then, we\ndiscuss the amount of privacy budget that can be achieved with shot noises,\nwhich serves as a metric for the level of privacy protection. Furthermore, we\nprovide the QDP of shot noise in quantum circuits with depolarizing noise.\nThrough numerical simulations, we show that shot noise can effectively provide\nprivacy protection in quantum computing.",
    "updated" : "2023-12-13T15:27:26Z",
    "published" : "2023-12-13T15:27:26Z",
    "authors" : [
      {
        "name" : "Yuqing Li"
      },
      {
        "name" : "Yusheng Zhao"
      },
      {
        "name" : "Xinyue Zhang"
      },
      {
        "name" : "Hui Zhong"
      },
      {
        "name" : "Miao Pan"
      },
      {
        "name" : "Chi Zhang"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.08413v1",
    "title" : "Privacy Constrained Fairness Estimation for Decision Trees",
    "summary" : "The protection of sensitive data becomes more vital, as data increases in\nvalue and potency. Furthermore, the pressure increases from regulators and\nsociety on model developers to make their Artificial Intelligence (AI) models\nnon-discriminatory. To boot, there is a need for interpretable, transparent AI\nmodels for high-stakes tasks. In general, measuring the fairness of any AI\nmodel requires the sensitive attributes of the individuals in the dataset, thus\nraising privacy concerns. In this work, the trade-offs between fairness,\nprivacy and interpretability are further explored. We specifically examine the\nStatistical Parity (SP) of Decision Trees (DTs) with Differential Privacy (DP),\nthat are each popular methods in their respective subfield. We propose a novel\nmethod, dubbed Privacy-Aware Fairness Estimation of Rules (PAFER), that can\nestimate SP in a DP-aware manner for DTs. DP, making use of a third-party legal\nentity that securely holds this sensitive data, guarantees privacy by adding\nnoise to the sensitive data. We experimentally compare several DP mechanisms.\nWe show that using the Laplacian mechanism, the method is able to estimate SP\nwith low error while guaranteeing the privacy of the individuals in the dataset\nwith high certainty. We further show experimentally and theoretically that the\nmethod performs better for DTs that humans generally find easier to interpret.",
    "updated" : "2023-12-13T14:54:48Z",
    "published" : "2023-12-13T14:54:48Z",
    "authors" : [
      {
        "name" : "Florian van der Steen"
      },
      {
        "name" : "Fré Vink"
      },
      {
        "name" : "Heysem Kaya"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.08144v1",
    "title" : "Privacy-Preserving Distributed Optimisation using Stochastic PDMM",
    "summary" : "Privacy-preserving distributed processing has received considerable attention\nrecently. The main purpose of these algorithms is to solve certain signal\nprocessing tasks over a network in a decentralised fashion without revealing\nprivate/secret data to the outside world. Because of the iterative nature of\nthese distributed algorithms, computationally complex approaches such as\n(homomorphic) encryption are undesired. Recently, an information theoretic\nmethod called subspace perturbation has been introduced for synchronous update\nschemes. The main idea is to exploit a certain structure in the update\nequations for noise insertion such that the private data is protected without\ncompromising the algorithm's accuracy. This structure, however, is absent in\nasynchronous update schemes. In this paper we will investigate such\nasynchronous schemes and derive a lower bound on the noise variance after\nrandom initialisation of the algorithm. This bound shows that the privacy level\nof asynchronous schemes is always better than or at least equal to that of\nsynchronous schemes. Computer simulations are conducted to consolidate our\ntheoretical results.",
    "updated" : "2023-12-13T13:46:22Z",
    "published" : "2023-12-13T13:46:22Z",
    "authors" : [
      {
        "name" : "Sebastian O. Jordan"
      },
      {
        "name" : "Qiongxiu Li"
      },
      {
        "name" : "Richard Heusdens"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.07992v1",
    "title" : "On the privacy of federated Clustering: A Cryptographic View",
    "summary" : "The privacy concern in federated clustering has attracted considerable\nattention in past decades. Many privacy-preserving clustering algorithms\nleverage cryptographic techniques like homomorphic encryption or secure\nmultiparty computation, to guarantee full privacy, i.e., no additional\ninformation is leaked other than the final output. However, given the iterative\nnature of clustering algorithms, consistently encrypting intermediate outputs,\nsuch as centroids, hampers efficiency. This paper delves into this intricate\ntrade-off, questioning the necessity of continuous encryption in iterative\nalgorithms. Using the federated K-means clustering as an example, we\nmathematically formulate the problem of reconstructing input private data from\nthe intermediate centroids as a classical cryptographic problem called hidden\nsubset sum problem (HSSP)-extended from an NP-complete problem called subset\nsum problem (SSP). Through an in-depth analysis, we show that existing\nlattice-based HSSP attacks fail in reconstructing the private data given the\nknowledge of intermediate centroids, thus it is secure to reveal them for the\nsake of efficiency. To the best of our knowledge, our work is the first to cast\nfederated clustering's privacy concerns as a cryptographic problem HSSP such\nthat a concrete and rigorous analysis can be conducted.",
    "updated" : "2023-12-13T09:04:14Z",
    "published" : "2023-12-13T09:04:14Z",
    "authors" : [
      {
        "name" : "Qiongxiu Li"
      },
      {
        "name" : "Lixia Luo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.07956v1",
    "title" : "Topology-Dependent Privacy Bound For Decentralized Federated Learning",
    "summary" : "Decentralized Federated Learning (FL) has attracted significant attention due\nto its enhanced robustness and scalability compared to its centralized\ncounterpart. It pivots on peer-to-peer communication rather than depending on a\ncentral server for model aggregation. While prior research has delved into\nvarious factors of decentralized FL such as aggregation methods and\nprivacy-preserving techniques, one crucial aspect affecting privacy is\nrelatively unexplored: the underlying graph topology. In this paper, we fill\nthe gap by deriving a stringent privacy bound for decentralized FL under the\ncondition that the accuracy is not compromised, highlighting the pivotal role\nof graph topology. Specifically, we demonstrate that the minimum privacy loss\nat each model aggregation step is dependent on the size of what we term as\n'honest components', the maximally connected subgraphs once all untrustworthy\nparticipants are excluded from the networks, which is closely tied to network\nrobustness. Our analysis suggests that attack-resilient networks will provide a\nsuperior privacy guarantee. We further validate this by studying both Poisson\nand power law networks, showing that the latter, being less robust against\nattacks, indeed reveals more privacy. In addition to a theoretical analysis, we\nconsolidate our findings by examining two distinct privacy attacks: membership\ninference and gradient inversion.",
    "updated" : "2023-12-13T08:07:10Z",
    "published" : "2023-12-13T08:07:10Z",
    "authors" : [
      {
        "name" : "Qiongxiu Li"
      },
      {
        "name" : "Wenrui Yu"
      },
      {
        "name" : "Changlong Ji"
      },
      {
        "name" : "Richard Heusdens"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.07948v1",
    "title" : "Zero-Knowledge Proof of Traffic: A Deterministic and Privacy-Preserving\n  Cross Verification Mechanism for Cooperative Perception Data",
    "summary" : "Cooperative perception is crucial for connected automated vehicles in\nintelligent transportation systems (ITSs); however, ensuring the authenticity\nof perception data remains a challenge as the vehicles cannot verify events\nthat they do not witness independently. Various studies have been conducted on\nestablishing the authenticity of data, such as trust-based statistical methods\nand plausibility-based methods. However, these methods are limited as they\nrequire prior knowledge such as previous sender behaviors or predefined rules\nto evaluate the authenticity. To overcome this limitation, this study proposes\na novel approach called zero-knowledge Proof of Traffic (zk-PoT), which\ninvolves generating cryptographic proofs to the traffic observations. Multiple\nindependent proofs regarding the same vehicle can be deterministically\ncross-verified by any receivers without relying on ground truth, probabilistic,\nor plausibility evaluations. Additionally, no private information is\ncompromised during the entire procedure. A full on-board unit software stack\nthat reflects the behavior of zk-PoT is implemented within a specifically\ndesigned simulator called Flowsim. A comprehensive experimental analysis is\nthen conducted using synthesized city-scale simulations, which demonstrates\nthat zk-PoT's cross-verification ratio ranges between 80 % to 96 %, and 80 % of\nthe verification is achieved in 2 s, with a protocol overhead of approximately\n25 %. Furthermore, the analyses of various attacks indicate that most of the\nattacks could be prevented, and some, such as collusion attacks, can be\nmitigated. The proposed approach can be incorporated into existing works,\nincluding the European Telecommunications Standards Institute (ETSI) and the\nInternational Organization for Standardization (ISO) ITS standards, without\ndisrupting the backward compatibility.",
    "updated" : "2023-12-13T07:53:06Z",
    "published" : "2023-12-13T07:53:06Z",
    "authors" : [
      {
        "name" : "Ye Tao"
      },
      {
        "name" : "Ehsan Javanmardi"
      },
      {
        "name" : "Pengfei Lin"
      },
      {
        "name" : "Jin Nakazato"
      },
      {
        "name" : "Yuze Jiang"
      },
      {
        "name" : "Manabu Tsukada"
      },
      {
        "name" : "Hiroshi Esaki"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.07947v1",
    "title" : "Adaptive Differentially Quantized Subspace Perturbation (ADQSP): A\n  Unified Framework for Privacy-Preserving Distributed Average Consensus",
    "summary" : "Privacy-preserving distributed average consensus has received significant\nattention recently due to its wide applicability. Based on the achieved\nperformances, existing approaches can be broadly classified into perfect\naccuracy-prioritized approaches such as secure multiparty computation (SMPC),\nand worst-case privacy-prioritized approaches such as differential privacy\n(DP). Methods of the first class achieve perfect output accuracy but reveal\nsome private information, while methods from the second class provide privacy\nagainst the strongest adversary at the cost of a loss of accuracy. In this\npaper, we propose a general approach named adaptive differentially quantized\nsubspace perturbation (ADQSP) which combines quantization schemes with\nso-called subspace perturbation. Although not relying on cryptographic\nprimitives, the proposed approach enjoys the benefits of both\naccuracy-prioritized and privacy-prioritized methods and is able to unify them.\nMore specifically, we show that by varying a single quantization parameter the\nproposed method can vary between SMPC-type performances and DP-type\nperformances. Our results show the potential of exploiting traditional\ndistributed signal processing tools for providing cryptographic guarantees. In\naddition to a comprehensive theoretical analysis, numerical validations are\nconducted to substantiate our results.",
    "updated" : "2023-12-13T07:52:16Z",
    "published" : "2023-12-13T07:52:16Z",
    "authors" : [
      {
        "name" : "Qiongxiu Li"
      },
      {
        "name" : "Jaron Skovsted Gundersen"
      },
      {
        "name" : "Milan Lopuhaa-Zwakenberg"
      },
      {
        "name" : "Richard Heusdens"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.07371v1",
    "title" : "Privacy-Aware Energy Consumption Modeling of Connected Battery Electric\n  Vehicles using Federated Learning",
    "summary" : "Battery Electric Vehicles (BEVs) are increasingly significant in modern\ncities due to their potential to reduce air pollution. Precise and real-time\nestimation of energy consumption for them is imperative for effective itinerary\nplanning and optimizing vehicle systems, which can reduce driving range anxiety\nand decrease energy costs. As public awareness of data privacy increases,\nadopting approaches that safeguard data privacy in the context of BEV energy\nconsumption modeling is crucial. Federated Learning (FL) is a promising\nsolution mitigating the risk of exposing sensitive information to third parties\nby allowing local data to remain on devices and only sharing model updates with\na central server. Our work investigates the potential of using FL methods, such\nas FedAvg, and FedPer, to improve BEV energy consumption prediction while\nmaintaining user privacy. We conducted experiments using data from 10 BEVs\nunder simulated real-world driving conditions. Our results demonstrate that the\nFedAvg-LSTM model achieved a reduction of up to 67.84\\% in the MAE value of the\nprediction results. Furthermore, we explored various real-world scenarios and\ndiscussed how FL methods can be employed in those cases. Our findings show that\nFL methods can effectively improve the performance of BEV energy consumption\nprediction while maintaining user privacy.",
    "updated" : "2023-12-12T15:40:38Z",
    "published" : "2023-12-12T15:40:38Z",
    "authors" : [
      {
        "name" : "Sen Yan"
      },
      {
        "name" : "Hongyuan Fang"
      },
      {
        "name" : "Ji Li"
      },
      {
        "name" : "Tomas Ward"
      },
      {
        "name" : "Noel O'Connor"
      },
      {
        "name" : "Mingming Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "physics.soc-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.07060v1",
    "title" : "Layered Randomized Quantization for Communication-Efficient and\n  Privacy-Preserving Distributed Learning",
    "summary" : "Next-generation wireless networks, such as edge intelligence and wireless\ndistributed learning, face two critical challenges: communication efficiency\nand privacy protection. In this work, our focus is on addressing these issues\nin a distributed learning framework. We consider a new approach that\nsimultaneously achieves communication efficiency and privacy protection by\nexploiting the privacy advantage offered by quantization. Specifically, we use\na quantization scheme called \\textbf{Gau}ssian \\textbf{L}ayered\n\\textbf{R}andomized \\textbf{Q}uantization (Gau-LRQ) that compresses the raw\nmodel gradients using a layer multishift coupler. By adjusting the parameters\nof Gau-LRQ, we shape the quantization error to follow the expected Gaussian\ndistribution, thus ensuring client-level differential privacy (CLDP). We\ndemonstrate the effectiveness of our proposed Gau-LRQ in the distributed\nstochastic gradient descent (SGD) framework and theoretically quantify the\ntrade-offs between communication, privacy, and convergence performance. We\nfurther improve the convergence performance by enabling dynamic private budget\nand quantization bit allocation. We achieve this by using an optimization\nformula that minimizes convergence error subject to the privacy budget\nconstraint. We evaluate our approach on multiple datasets, including MNIST,\nCIFAR-10, and CIFAR-100, and show that our proposed method outperforms the\nbaselines in terms of learning performance under various privacy constraints.\nMoreover, we observe that dynamic privacy allocation yields additional accuracy\nimprovements for the models compared to the fixed scheme.",
    "updated" : "2023-12-12T08:27:52Z",
    "published" : "2023-12-12T08:27:52Z",
    "authors" : [
      {
        "name" : "Guangfeng Yan"
      },
      {
        "name" : "Tan Li"
      },
      {
        "name" : "Tian Lan"
      },
      {
        "name" : "Kui Wu"
      },
      {
        "name" : "Linqi Song"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.07055v1",
    "title" : "Communication Cost Reduction for Subgraph Counting under Local\n  Differential Privacy via Hash Functions",
    "summary" : "We suggest the use of hash functions to cut down the communication costs when\ncounting subgraphs under edge local differential privacy. While various\nalgorithms exist for computing graph statistics, including the count of\nsubgraphs, under the edge local differential privacy, many suffer with high\ncommunication costs, making them less efficient for large graphs. Though data\ncompression is a typical approach in differential privacy, its application in\nlocal differential privacy requires a form of compression that every node can\nreproduce. In our study, we introduce linear congruence hashing. With a\nsampling rate of $s$, our method can cut communication costs by a factor of\n$s^2$, albeit at the cost of increasing variance in the published graph\nstatistic by a factor of $s$. The experimental results indicate that, when\nmatched for communication costs, our method achieves a reduction in the\n$\\ell_2$-error for triangle counts by up to 1000 times compared to the\nperformance of leading algorithms.",
    "updated" : "2023-12-12T08:12:18Z",
    "published" : "2023-12-12T08:12:18Z",
    "authors" : [
      {
        "name" : "Quentin Hillebrand"
      },
      {
        "name" : "Vorapong Suppakitpaisarn"
      },
      {
        "name" : "Tetsuo Shibuya"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.06989v1",
    "title" : "Task-Agnostic Privacy-Preserving Representation Learning for Federated\n  Learning Against Attribute Inference Attacks",
    "summary" : "Federated learning (FL) has been widely studied recently due to its property\nto collaboratively train data from different devices without sharing the raw\ndata. Nevertheless, recent studies show that an adversary can still be possible\nto infer private information about devices' data, e.g., sensitive attributes\nsuch as income, race, and sexual orientation. To mitigate the attribute\ninference attacks, various existing privacy-preserving FL methods can be\nadopted/adapted. However, all these existing methods have key limitations: they\nneed to know the FL task in advance, or have intolerable computational\noverheads or utility losses, or do not have provable privacy guarantees.\n  We address these issues and design a task-agnostic privacy-preserving\npresentation learning method for FL ({\\bf TAPPFL}) against attribute inference\nattacks. TAPPFL is formulated via information theory. Specifically, TAPPFL has\ntwo mutual information goals, where one goal learns task-agnostic data\nrepresentations that contain the least information about the private attribute\nin each device's data, and the other goal ensures the learnt data\nrepresentations include as much information as possible about the device data\nto maintain FL utility. We also derive privacy guarantees of TAPPFL against\nworst-case attribute inference attacks, as well as the inherent tradeoff\nbetween utility preservation and privacy protection. Extensive results on\nmultiple datasets and applications validate the effectiveness of TAPPFL to\nprotect data privacy, maintain the FL utility, and be efficient as well.\nExperimental results also show that TAPPFL outperforms the existing\ndefenses\\footnote{Source code and full version:\n\\url{https://github.com/TAPPFL}}.",
    "updated" : "2023-12-12T05:17:34Z",
    "published" : "2023-12-12T05:17:34Z",
    "authors" : [
      {
        "name" : "Caridad Arroyo Arevalo"
      },
      {
        "name" : "Sayedeh Leila Noorbakhsh"
      },
      {
        "name" : "Yun Dong"
      },
      {
        "name" : "Yuan Hong"
      },
      {
        "name" : "Binghui Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.06658v1",
    "title" : "Mean estimation in the add-remove model of differential privacy",
    "summary" : "Differential privacy is often studied under two different models of\nneighboring datasets: the add-remove model and the swap model. While the swap\nmodel is used extensively in the academic literature, many practical libraries\nuse the more conservative add-remove model. However, analysis under the\nadd-remove model can be cumbersome, and obtaining results with tight constants\nrequires some additional work. Here, we study the problem of one-dimensional\nmean estimation under the add-remove model of differential privacy. We propose\na new algorithm and show that it is min-max optimal, that it has the correct\nconstant in the leading term of the mean squared error, and that this constant\nis the same as the optimal algorithm in the swap model. Our results show that,\nfor mean estimation, the add-remove and swap model give nearly identical error\neven though the add-remove model cannot treat the size of the dataset as public\ninformation. In addition, we demonstrate empirically that our proposed\nalgorithm yields a factor of two improvement in mean squared error over\nalgorithms often used in practice.",
    "updated" : "2023-12-11T18:59:35Z",
    "published" : "2023-12-11T18:59:35Z",
    "authors" : [
      {
        "name" : "Alex Kulesza"
      },
      {
        "name" : "Ananda Theertha Suresh"
      },
      {
        "name" : "Yuyan Wang"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.06717v1",
    "title" : "Privacy Issues in Large Language Models: A Survey",
    "summary" : "This is the first survey of the active area of AI research that focuses on\nprivacy issues in Large Language Models (LLMs). Specifically, we focus on work\nthat red-teams models to highlight privacy risks, attempts to build privacy\ninto the training or inference process, enables efficient data deletion from\ntrained models to comply with existing privacy regulations, and tries to\nmitigate copyright issues. Our focus is on summarizing technical research that\ndevelops algorithms, proves theorems, and runs empirical evaluations. While\nthere is an extensive body of legal and policy work addressing these challenges\nfrom a different angle, that is not the focus of our survey. Nevertheless,\nthese works, along with recent legal developments do inform how these technical\nproblems are formalized, and so we discuss them briefly in Section 1. While we\nhave made our best effort to include all the relevant work, due to the fast\nmoving nature of this research we may have missed some recent work. If we have\nmissed some of your work please contact us, as we will attempt to keep this\nsurvey relatively up to date. We are maintaining a repository with the list of\npapers covered in this survey and any relevant code that was publicly available\nat https://github.com/safr-ml-lab/survey-llm.",
    "updated" : "2023-12-11T01:26:53Z",
    "published" : "2023-12-11T01:26:53Z",
    "authors" : [
      {
        "name" : "Seth Neel"
      },
      {
        "name" : "Peter Chang"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.05720v1",
    "title" : "Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer\n  Inputs of Language Models in Federated Learning",
    "summary" : "Federated learning (FL) emphasizes decentralized training by storing data\nlocally and sending only model updates, underlining user privacy. Recently, a\nline of works on privacy attacks impairs user privacy by extracting sensitive\ntraining text from language models in the context of FL. Yet, these attack\ntechniques face distinct hurdles: some work chiefly with limited batch sizes\n(e.g., batch size of 1), and others are easily detectable. This paper\nintroduces an innovative approach that is challenging to detect, significantly\nenhancing the recovery rate of text in various batch-size settings. Building on\nfundamental gradient matching and domain prior knowledge, we enhance the attack\nby recovering the input of the Pooler layer of language models, which enables\nus to provide additional supervised signals at the feature level. Unlike\ngradient data, these signals do not average across sentences and tokens,\nthereby offering more nuanced and effective insights. We benchmark our method\nusing text classification tasks on datasets such as CoLA, SST-2, and Rotten\nTomatoes. Across different batch sizes and models, our approach consistently\noutperforms previous state-of-the-art results.",
    "updated" : "2023-12-10T01:19:59Z",
    "published" : "2023-12-10T01:19:59Z",
    "authors" : [
      {
        "name" : "Jianwei Li"
      },
      {
        "name" : "Sheng Liu"
      },
      {
        "name" : "Qi Lei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.05686v1",
    "title" : "Privacy Preserving Multi-Agent Reinforcement Learning in Supply Chains",
    "summary" : "This paper addresses privacy concerns in multi-agent reinforcement learning\n(MARL), specifically within the context of supply chains where individual\nstrategic data must remain confidential. Organizations within the supply chain\nare modeled as agents, each seeking to optimize their own objectives while\ninteracting with others. As each organization's strategy is contingent on\nneighboring strategies, maintaining privacy of state and action-related\ninformation is crucial. To tackle this challenge, we propose a game-theoretic,\nprivacy-preserving mechanism, utilizing a secure multi-party computation (MPC)\nframework in MARL settings. Our major contribution is the successful\nimplementation of a secure MPC framework, SecFloat on EzPC, to solve this\nproblem. However, simply implementing policy gradient methods such as MADDPG\noperations using SecFloat, while conceptually feasible, would be\nprogrammatically intractable. To overcome this hurdle, we devise a novel\napproach that breaks down the forward and backward pass of the neural network\ninto elementary operations compatible with SecFloat , creating efficient and\nsecure versions of the MADDPG algorithm. Furthermore, we present a learning\nmechanism that carries out floating point operations in a privacy-preserving\nmanner, an important feature for successful learning in MARL framework.\nExperiments reveal that there is on average 68.19% less supply chain wastage in\n2 PC compared to no data share, while also giving on average 42.27% better\naverage cumulative revenue for each player. This work paves the way for\npractical, privacy-preserving MARL, promising significant improvements in\nsecure computation within supply chain contexts and broadly.",
    "updated" : "2023-12-09T21:25:21Z",
    "published" : "2023-12-09T21:25:21Z",
    "authors" : [
      {
        "name" : "Ananta Mukherjee"
      },
      {
        "name" : "Peeyush Kumar"
      },
      {
        "name" : "Boling Yang"
      },
      {
        "name" : "Nishanth Chandran"
      },
      {
        "name" : "Divya Gupta"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.05659v1",
    "title" : "Optimal Unbiased Randomizers for Regression with Label Differential\n  Privacy",
    "summary" : "We propose a new family of label randomizers for training regression models\nunder the constraint of label differential privacy (DP). In particular, we\nleverage the trade-offs between bias and variance to construct better label\nrandomizers depending on a privately estimated prior distribution over the\nlabels. We demonstrate that these randomizers achieve state-of-the-art\nprivacy-utility trade-offs on several datasets, highlighting the importance of\nreducing bias when training neural networks with label DP. We also provide\ntheoretical results shedding light on the structural properties of the optimal\nunbiased randomizers.",
    "updated" : "2023-12-09T19:58:34Z",
    "published" : "2023-12-09T19:58:34Z",
    "authors" : [
      {
        "name" : "Ashwinkumar Badanidiyuru"
      },
      {
        "name" : "Badih Ghazi"
      },
      {
        "name" : "Pritish Kamath"
      },
      {
        "name" : "Ravi Kumar"
      },
      {
        "name" : "Ethan Leeman"
      },
      {
        "name" : "Pasin Manurangsi"
      },
      {
        "name" : "Avinash V Varadarajan"
      },
      {
        "name" : "Chiyuan Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.05436v1",
    "title" : "Trading Off Scalability, Privacy, and Performance in Data Synthesis",
    "summary" : "Synthetic data has been widely applied in the real world recently. One\ntypical example is the creation of synthetic data for privacy concerned\ndatasets. In this scenario, synthetic data substitute the real data which\ncontains the privacy information, and is used to public testing for machine\nlearning models. Another typical example is the unbalance data over-sampling\nwhich the synthetic data is generated in the region of minority samples to\nbalance the positive and negative ratio when training the machine learning\nmodels. In this study, we concentrate on the first example, and introduce (a)\nthe Howso engine, and (b) our proposed random projection based synthetic data\ngeneration framework. We evaluate these two algorithms on the aspects of\nprivacy preservation and accuracy, and compare them to the two state-of-the-art\nsynthetic data generation algorithms DataSynthesizer and Synthetic Data Vault.\nWe show that the synthetic data generated by Howso engine has good privacy and\naccuracy, which results the best overall score. On the other hand, our proposed\nrandom projection based framework can generate synthetic data with highest\naccuracy score, and has the fastest scalability.",
    "updated" : "2023-12-09T02:04:25Z",
    "published" : "2023-12-09T02:04:25Z",
    "authors" : [
      {
        "name" : "Xiao Ling"
      },
      {
        "name" : "Tim Menzies"
      },
      {
        "name" : "Christopher Hazard"
      },
      {
        "name" : "Jack Shu"
      },
      {
        "name" : "Jacob Beel"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.05183v1",
    "title" : "A Privacy-Preserving Framework for Cloud-Based HVAC Control",
    "summary" : "The objective of this work is (i) to develop an encrypted cloud-based HVAC\ncontrol framework to ensure the privacy of occupancy information, (ii) to\nreduce the communication and computation costs of encrypted HVAC control.\nOccupancy of a building is sensitive and private information that can be\naccurately inferred by cloud-based HVAC controllers. To ensure the privacy of\nthe privacy information, in our framework, the measurements of an HVAC system\nare encrypted by a fully homomorphic encryption prior to communication with the\ncloud controller. We first develop an encrypted fast gradient algorithm that\nallows the cloud controller to regulate the indoor temperature and CO$_2$ of a\nbuilding by solving two model predictive control problems. We next develop an\nevent-triggered control policy to reduce the communication and computation\ncosts of the encrypted HVAC control. We cast the optimal design of the\nevent-triggered policy as an optimal control problem wherein the objective is\nto minimize a linear combination of the control and communication costs. Using\nBellman's optimality principle, we study the structural properties of the\noptimal event-triggered policy and show that the optimal triggering policy is a\nfunction of the current state, the last communicated state with the cloud, and\nthe time since the last communication with the cloud. We also show that the\noptimal design of the event-triggered policy can be transformed into a Markov\ndecision process by introducing two new states. We finally study the\nperformance of the developed encrypted HVAC control framework using the TRNSYS\nsimulator. Our numerical results show that the proposed framework not only\nensures efficient control of the indoor temperature and CO$_2$ but also reduces\nthe computation and communication costs of encrypted HVAC control by at least\n60%.",
    "updated" : "2023-12-08T17:09:54Z",
    "published" : "2023-12-08T17:09:54Z",
    "authors" : [
      {
        "name" : "Zhenan Feng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.05114v1",
    "title" : "On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction\n  Attacks against \"Truly Anonymous Synthetic Data''",
    "summary" : "Training generative models to produce synthetic data is meant to provide a\nprivacy-friendly approach to data release. However, we get robust guarantees\nonly when models are trained to satisfy Differential Privacy (DP). Alas, this\nis not the standard in industry as many companies use ad-hoc strategies to\nempirically evaluate privacy based on the statistical similarity between\nsynthetic and real data. In this paper, we review the privacy metrics offered\nby leading companies in this space and shed light on a few critical flaws in\nreasoning about privacy entirely via empirical evaluations. We analyze the\nundesirable properties of the most popular metrics and filters and demonstrate\ntheir unreliability and inconsistency through counter-examples. We then present\na reconstruction attack, ReconSyn, which successfully recovers (i.e., leaks all\nattributes of) at least 78% of the low-density train records (or outliers) with\nonly black-box access to a single fitted generative model and the privacy\nmetrics. Finally, we show that applying DP only to the model or using\nlow-utility generators does not mitigate ReconSyn as the privacy leakage\npredominantly comes from the metrics. Overall, our work serves as a warning to\npractitioners not to deviate from established privacy-preserving mechanisms.",
    "updated" : "2023-12-08T15:42:28Z",
    "published" : "2023-12-08T15:42:28Z",
    "authors" : [
      {
        "name" : "Georgi Ganev"
      },
      {
        "name" : "Emiliano De Cristofaro"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.04903v1",
    "title" : "Differential privacy statistical inference for a directed graph network\n  model with covariates",
    "summary" : "The real network has two characteristics: heterogeneity and homogeneity. A\ndirected network model with covariates is proposed to analyze these two\nfeatures, and the asymptotic theory of parameter Maximum likelihood\nestimators(MLEs) is established. However, in many practical cases, network data\noften carries a lot of sensitive information. How to achieve the trade-off\nbetween privacy and utility has become an important issue in network data\nanalysis. In this paper, we study a directed $\\beta$-model with covariates\nunder differential privacy mechanism. It includes $2n$-dimensional node degree\nparameters $\\boldsymbol{\\theta}$ and a $p$-dimensional homogeneity parameter\n$\\boldsymbol{\\gamma}$ that describes the covariate effect. We use the discrete\nLaplace mechanism to release noise for the bi-degree sequences. Based on moment\nequations, we estimate the parameters of both degree heterogeneity and\nhomogeneity in the model, and derive the consistency and asymptotic normality\nof the differentially private estimators as the number of nodes tends to\ninfinity. Numerical simulations and case studies are provided to demonstrate\nthe validity of our theoretical results.",
    "updated" : "2023-12-08T08:36:18Z",
    "published" : "2023-12-08T08:36:18Z",
    "authors" : [
      {
        "name" : "Jing Luo"
      },
      {
        "name" : "Zhimeng Xu"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.04738v1",
    "title" : "DPI: Ensuring Strict Differential Privacy for Infinite Data Streaming",
    "summary" : "Streaming data, crucial for applications like crowdsourcing analytics,\nbehavior studies, and real-time monitoring, faces significant privacy risks due\nto the large and diverse data linked to individuals. In particular, recent\nefforts to release data streams, using the rigorous privacy notion of\ndifferential privacy (DP), have encountered issues with unbounded privacy\nleakage. This challenge limits their applicability to only a finite number of\ntime slots (''finite data stream'') or relaxation to protecting the events\n(''event or $w$-event DP'') rather than all the records of users. A persistent\nchallenge is managing the sensitivity of outputs to inputs in situations where\nusers contribute many activities and data distributions evolve over time. In\nthis paper, we present a novel technique for Differentially Private data\nstreaming over Infinite disclosure (DPI) that effectively bounds the total\nprivacy leakage of each user in infinite data streams while enabling accurate\ndata collection and analysis. Furthermore, we also maximize the accuracy of DPI\nvia a novel boosting mechanism. Finally, extensive experiments across various\nstreaming applications and real datasets (e.g., COVID-19, Network Traffic, and\nUSDA Production), show that DPI maintains high utility for infinite data\nstreams in diverse settings. Code for DPI is available at\nhttps://github.com/ShuyaFeng/DPI.",
    "updated" : "2023-12-07T22:37:54Z",
    "published" : "2023-12-07T22:37:54Z",
    "authors" : [
      {
        "name" : "Shuya Feng"
      },
      {
        "name" : "Meisam Mohammady"
      },
      {
        "name" : "Han Wang"
      },
      {
        "name" : "Xiaochen Li"
      },
      {
        "name" : "Zhan Qin"
      },
      {
        "name" : "Yuan Hong"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.04692v1",
    "title" : "Diffence: Fencing Membership Privacy With Diffusion Models",
    "summary" : "Deep learning models, while achieving remarkable performance across various\ntasks, are vulnerable to member inference attacks, wherein adversaries identify\nif a specific data point was part of a model's training set. This\nsusceptibility raises substantial privacy concerns, especially when models are\ntrained on sensitive datasets. Current defense methods often struggle to\nprovide robust protection without hurting model utility, and they often require\nretraining the model or using extra data. In this work, we introduce a novel\ndefense framework against membership attacks by leveraging generative models.\nThe key intuition of our defense is to remove the differences between member\nand non-member inputs which can be used to perform membership attacks, by\nre-generating input samples before feeding them to the target model. Therefore,\nour defense works \\emph{pre-inference}, which is unlike prior defenses that are\neither training-time (modify the model) or post-inference time (modify the\nmodel's output).\n  A unique feature of our defense is that it works on input samples only,\nwithout modifying the training or inference phase of the target model.\nTherefore, it can be cascaded with other defense mechanisms as we demonstrate\nthrough experiments. Through extensive experimentation, we show that our\napproach can serve as a robust plug-n-play defense mechanism, enhancing\nmembership privacy without compromising model utility in both baseline and\ndefended settings. For example, our method enhanced the effectiveness of recent\nstate-of-the-art defenses, reducing attack accuracy by an average of 5.7\\% to\n12.4\\% across three datasets, without any impact on the model's accuracy. By\nintegrating our method with prior defenses, we achieve new state-of-the-art\nperformance in the privacy-utility trade-off.",
    "updated" : "2023-12-07T20:45:09Z",
    "published" : "2023-12-07T20:45:09Z",
    "authors" : [
      {
        "name" : "Yuefeng Peng"
      },
      {
        "name" : "Ali Naseh"
      },
      {
        "name" : "Amir Houmansadr"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.04447v1",
    "title" : "Privacy-preserving quantum federated learning via gradient hiding",
    "summary" : "Distributed quantum computing, particularly distributed quantum machine\nlearning, has gained substantial prominence for its capacity to harness the\ncollective power of distributed quantum resources, transcending the limitations\nof individual quantum nodes. Meanwhile, the critical concern of privacy within\ndistributed computing protocols remains a significant challenge, particularly\nin standard classical federated learning (FL) scenarios where data of\nparticipating clients is susceptible to leakage via gradient inversion attacks\nby the server. This paper presents innovative quantum protocols with quantum\ncommunication designed to address the FL problem, strengthen privacy measures,\nand optimize communication efficiency. In contrast to previous works that\nleverage expressive variational quantum circuits or differential privacy\ntechniques, we consider gradient information concealment using quantum states\nand propose two distinct FL protocols, one based on private inner-product\nestimation and the other on incremental learning. These protocols offer\nsubstantial advancements in privacy preservation with low communication\nresources, forging a path toward efficient quantum communication-assisted FL\nprotocols and contributing to the development of secure distributed quantum\nmachine learning, thus addressing critical privacy concerns in the quantum\ncomputing era.",
    "updated" : "2023-12-07T17:16:30Z",
    "published" : "2023-12-07T17:16:30Z",
    "authors" : [
      {
        "name" : "Changhao Li"
      },
      {
        "name" : "Niraj Kumar"
      },
      {
        "name" : "Zhixin Song"
      },
      {
        "name" : "Shouvanik Chakrabarti"
      },
      {
        "name" : "Marco Pistoia"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.04404v2",
    "title" : "On the Impact of Multi-dimensional Local Differential Privacy on\n  Fairness",
    "summary" : "Automated decision systems are increasingly used to make consequential\ndecisions in people's lives. Due to the sensitivity of the manipulated data as\nwell as the resulting decisions, several ethical concerns need to be addressed\nfor the appropriate use of such technologies, in particular, fairness and\nprivacy. Unlike previous work, which focused on centralized differential\nprivacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper,\nwe examine the impact of LDP in the presence of several sensitive attributes\n(i.e., multi-dimensional data) on fairness. Detailed empirical analysis on\nsynthetic and benchmark datasets revealed very relevant observations. In\nparticular, (1) multi-dimensional LDP is an efficient approach to reduce\ndisparity, (2) the multi-dimensional approach of LDP (independent vs. combined)\nmatters only at low privacy guarantees, and (3) the outcome Y distribution has\nan important effect on which group is more sensitive to the obfuscation. Last,\nwe summarize our findings in the form of recommendations to guide practitioners\nin adopting effective privacy-preserving practices while maintaining fairness\nand utility in ML applications.",
    "updated" : "2023-12-08T15:00:31Z",
    "published" : "2023-12-07T16:17:34Z",
    "authors" : [
      {
        "name" : "Karima Makhlouf"
      },
      {
        "name" : "Heber H. Arcolezi"
      },
      {
        "name" : "Sami Zhioua"
      },
      {
        "name" : "Ghassen Ben Brahim"
      },
      {
        "name" : "Catuscia Palamidessi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.04106v1",
    "title" : "Identity-Obscured Neural Radiance Fields: Privacy-Preserving 3D Facial\n  Reconstruction",
    "summary" : "Neural radiance fields (NeRF) typically require a complete set of images\ntaken from multiple camera perspectives to accurately reconstruct geometric\ndetails. However, this approach raise significant privacy concerns in the\ncontext of facial reconstruction. The critical need for privacy protection\noften leads invidividuals to be reluctant in sharing their facial images, due\nto fears of potential misuse or security risks. Addressing these concerns, we\npropose a method that leverages privacy-preserving images for reconstructing 3D\nhead geometry within the NeRF framework. Our method stands apart from\ntraditional facial reconstruction techniques as it does not depend on RGB\ninformation from images containing sensitive facial data. Instead, it\neffectively generates plausible facial geometry using a series of\nidentity-obscured inputs, thereby protecting facial privacy.",
    "updated" : "2023-12-07T07:41:10Z",
    "published" : "2023-12-07T07:41:10Z",
    "authors" : [
      {
        "name" : "Jiayi Kong"
      },
      {
        "name" : "Baixin Xu"
      },
      {
        "name" : "Xurui Song"
      },
      {
        "name" : "Chen Qian"
      },
      {
        "name" : "Jun Luo"
      },
      {
        "name" : "Ying He"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.04068v1",
    "title" : "Making Translators Privacy-aware on the User's Side",
    "summary" : "We propose PRISM to enable users of machine translation systems to preserve\nthe privacy of data on their own initiative. There is a growing demand to apply\nmachine translation systems to data that require privacy protection. While\nseveral machine translation engines claim to prioritize privacy, the extent and\nspecifics of such protection are largely ambiguous. First, there is often a\nlack of clarity on how and to what degree the data is protected. Even if\nservice providers believe they have sufficient safeguards in place,\nsophisticated adversaries might still extract sensitive information. Second,\nvulnerabilities may exist outside of these protective measures, such as within\ncommunication channels, potentially leading to data leakage. As a result, users\nare hesitant to utilize machine translation engines for data demanding high\nlevels of privacy protection, thereby missing out on their benefits. PRISM\nresolves this problem. Instead of relying on the translation service to keep\ndata safe, PRISM provides the means to protect data on the user's side. This\napproach ensures that even machine translation engines with inadequate privacy\nmeasures can be used securely. For platforms already equipped with privacy\nsafeguards, PRISM acts as an additional protection layer, reinforcing their\nsecurity furthermore. PRISM adds these privacy features without significantly\ncompromising translation accuracy. Our experiments demonstrate the\neffectiveness of PRISM using real-world translators, T5 and ChatGPT\n(GPT-3.5-turbo), and the datasets with two languages. PRISM effectively\nbalances privacy protection with translation accuracy.",
    "updated" : "2023-12-07T06:23:17Z",
    "published" : "2023-12-07T06:23:17Z",
    "authors" : [
      {
        "name" : "Ryoma Sato"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.03918v1",
    "title" : "Data Safety vs. App Privacy: Comparing the Usability of Android and iOS\n  Privacy Labels",
    "summary" : "Privacy labels -- standardized, compact representations of data collection\nand data use practices -- have frequently been recommended as a solution to the\nshortcomings of privacy policies. Apple introduced mandatory privacy labels for\napps in its App Store in December 2020; Google introduced data safety labels\nfor Android apps in July 2022. iOS app privacy labels have been evaluated and\ncritiqued in prior work. In this work, we evaluated Android data safety labels\nand explored how differences between the two label designs impact user\ncomprehension and label utility. We conducted a between-subjects,\nsemi-structured interview study with 12 Android users and 12 iOS users. While\nsome users found Android Data Safety Labels informative and helpful, other\nusers found them too vague. Compared to iOS App Privacy Labels, Android users\nfound the distinction between data collection groups more intuitive and found\nexplicit inclusion of omitted data collection groups more salient. However,\nsome users expressed skepticism regarding elided information about collected\ndata type categories. Most users missed critical information due to not\nexpanding the accordion interface, and they were surprised by collection\npractices excluded from Android's definitions. Our findings also revealed that\nAndroid users generally appreciated information about security practices\nincluded in the labels and iOS users wanted that information added.",
    "updated" : "2023-12-06T21:32:32Z",
    "published" : "2023-12-06T21:32:32Z",
    "authors" : [
      {
        "name" : "Yanzi Lin"
      },
      {
        "name" : "Jaideep Juneja"
      },
      {
        "name" : "Eleanor Birrell"
      },
      {
        "name" : "Lorrie Cranor"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.05265v1",
    "title" : "Multimodal Group Emotion Recognition In-the-wild Using Privacy-Compliant\n  Features",
    "summary" : "This paper explores privacy-compliant group-level emotion recognition\n''in-the-wild'' within the EmotiW Challenge 2023. Group-level emotion\nrecognition can be useful in many fields including social robotics,\nconversational agents, e-coaching and learning analytics. This research imposes\nitself using only global features avoiding individual ones, i.e. all features\nthat can be used to identify or track people in videos (facial landmarks, body\nposes, audio diarization, etc.). The proposed multimodal model is composed of a\nvideo and an audio branches with a cross-attention between modalities. The\nvideo branch is based on a fine-tuned ViT architecture. The audio branch\nextracts Mel-spectrograms and feed them through CNN blocks into a transformer\nencoder. Our training paradigm includes a generated synthetic dataset to\nincrease the sensitivity of our model on facial expression within the image in\na data-driven way. The extensive experiments show the significance of our\nmethodology. Our privacy-compliant proposal performs fairly on the EmotiW\nchallenge, with 79.24% and 75.13% of accuracy respectively on validation and\ntest set for the best models. Noticeably, our findings highlight that it is\npossible to reach this accuracy level with privacy-compliant features using\nonly 5 frames uniformly distributed on the video.",
    "updated" : "2023-12-06T08:58:11Z",
    "published" : "2023-12-06T08:58:11Z",
    "authors" : [
      {
        "name" : "Anderson Augusma"
      },
      {
        "name" : "Dominique Vaufreydaz"
      },
      {
        "name" : "Frédérique Letué"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "cs.LG",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.03293v1",
    "title" : "Securing Data Platforms: Strategic Masking Techniques for Privacy and\n  Security for B2B Enterprise Data",
    "summary" : "In today's digital age, the imperative to protect data privacy and security\nis a paramount concern, especially for business-to-business (B2B) enterprises\nthat handle sensitive information. These enterprises are increasingly\nconstructing data platforms, which are integrated suites of technology\nsolutions architected for the efficient management, processing, storage, and\ndata analysis. It has become critical to design these data platforms with\nmechanisms that inherently support data privacy and security, particularly as\nthey encounter the added complexity of safeguarding unstructured data types\nsuch as log files and text documents. Within this context, data masking stands\nout as a vital feature of data platform architecture. It proactively conceals\nsensitive elements, ensuring data privacy while preserving the information's\nvalue for business operations and analytics. This protective measure entails a\nstrategic two-fold process: firstly, accurately pinpointing the sensitive data\nthat necessitates concealment, and secondly, applying sophisticated methods to\ndisguise that data effectively within the data platform infrastructure. This\nresearch delves into the nuances of embedding advanced data masking techniques\nwithin the very fabric of data platforms and an in-depth exploration of how\nenterprises can adopt a comprehensive approach toward effective data masking\nimplementation by exploring different identification and anonymization\ntechniques.",
    "updated" : "2023-12-06T05:04:37Z",
    "published" : "2023-12-06T05:04:37Z",
    "authors" : [
      {
        "name" : "Mandar Khoje"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.03252v1",
    "title" : "Privacy-Preserving Task-Oriented Semantic Communications Against Model\n  Inversion Attacks",
    "summary" : "Semantic communication has been identified as a core technology for the sixth\ngeneration (6G) of wireless networks. Recently, task-oriented semantic\ncommunications have been proposed for low-latency inference with limited\nbandwidth. Although transmitting only task-related information does protect a\ncertain level of user privacy, adversaries could apply model inversion\ntechniques to reconstruct the raw data or extract useful information, thereby\ninfringing on users' privacy. To mitigate privacy infringement, this paper\nproposes an information bottleneck and adversarial learning (IBAL) approach to\nprotect users' privacy against model inversion attacks. Specifically, we\nextract task-relevant features from the input based on the information\nbottleneck (IB) theory. To overcome the difficulty in calculating the mutual\ninformation in high-dimensional space, we derive a variational upper bound to\nestimate the true mutual information. To prevent data reconstruction from\ntask-related features by adversaries, we leverage adversarial learning to train\nencoder to fool adversaries by maximizing reconstruction distortion.\nFurthermore, considering the impact of channel variations on privacy-utility\ntrade-off and the difficulty in manually tuning the weights of each loss, we\npropose an adaptive weight adjustment method. Numerical results demonstrate\nthat the proposed approaches can effectively protect privacy without\nsignificantly affecting task performance and achieve better privacy-utility\ntrade-offs than baseline methods.",
    "updated" : "2023-12-06T02:57:56Z",
    "published" : "2023-12-06T02:57:56Z",
    "authors" : [
      {
        "name" : "Yanhu Wang"
      },
      {
        "name" : "Shuaishuai Guo"
      },
      {
        "name" : "Yiqin Deng"
      },
      {
        "name" : "Haixia Zhang"
      },
      {
        "name" : "Yuguang Fang"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.04594v1",
    "title" : "FedGeo: Privacy-Preserving User Next Location Prediction with Federated\n  Learning",
    "summary" : "A User Next Location Prediction (UNLP) task, which predicts the next location\nthat a user will move to given his/her trajectory, is an indispensable task for\na wide range of applications. Previous studies using large-scale trajectory\ndatasets in a single server have achieved remarkable performance in UNLP task.\nHowever, in real-world applications, legal and ethical issues have been raised\nregarding privacy concerns leading to restrictions against sharing human\ntrajectory datasets to any other server. In response, Federated Learning (FL)\nhas emerged to address the personal privacy issue by collaboratively training\nmultiple clients (i.e., users) and then aggregating them. While previous\nstudies employed FL for UNLP, they are still unable to achieve reliable\nperformance because of the heterogeneity of clients' mobility. To tackle this\nproblem, we propose the Federated Learning for Geographic Information (FedGeo),\na FL framework specialized for UNLP, which alleviates the heterogeneity of\nclients' mobility and guarantees personal privacy protection. Firstly, we\nincorporate prior global geographic adjacency information to the local client\nmodel, since the spatial correlation between locations is trained partially in\neach client who has only a heterogeneous subset of the overall trajectories in\nFL. We also introduce a novel aggregation method that minimizes the gap between\nclient models to solve the problem of client drift caused by differences\nbetween client models when learning with their heterogeneous data. Lastly, we\nprobabilistically exclude clients with extremely heterogeneous data from the FL\nprocess by focusing on clients who visit relatively diverse locations. We show\nthat FedGeo is superior to other FL methods for model performance in UNLP task.\nWe also validated our model in a real-world application using our own\ncustomers' mobile phones and the FL agent system.",
    "updated" : "2023-12-06T01:43:58Z",
    "published" : "2023-12-06T01:43:58Z",
    "authors" : [
      {
        "name" : "Chung Park"
      },
      {
        "name" : "Taekyoon Choi"
      },
      {
        "name" : "Taesan Kim"
      },
      {
        "name" : "Mincheol Cho"
      },
      {
        "name" : "Junui Hong"
      },
      {
        "name" : "Minsung Choi"
      },
      {
        "name" : "Jaegul Choo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.02611v1",
    "title" : "Privacy-Aware Data Acquisition under Data Similarity in Regression\n  Markets",
    "summary" : "Data markets facilitate decentralized data exchange for applications such as\nprediction, learning, or inference. The design of these markets is challenged\nby varying privacy preferences as well as data similarity among data owners.\nRelated works have often overlooked how data similarity impacts pricing and\ndata value through statistical information leakage. We demonstrate that data\nsimilarity and privacy preferences are integral to market design and propose a\nquery-response protocol using local differential privacy for a two-party data\nacquisition mechanism. In our regression data market model, we analyze\nstrategic interactions between privacy-aware owners and the learner as a\nStackelberg game over the asked price and privacy factor. Finally, we\nnumerically evaluate how data similarity affects market participation and\ntraded data value.",
    "updated" : "2023-12-05T09:39:04Z",
    "published" : "2023-12-05T09:39:04Z",
    "authors" : [
      {
        "name" : "Shashi Raj Pandey"
      },
      {
        "name" : "Pierre Pinson"
      },
      {
        "name" : "Petar Popovski"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.GT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.02400v1",
    "title" : "Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic\n  Clipping Threshold and Noise Multiplier Estimation",
    "summary" : "DP-SGD has emerged as a popular method to protect personally identifiable\ninformation in deep learning applications. Unfortunately, DP-SGD's per-sample\ngradient clipping and uniform noise addition during training can significantly\ndegrade model utility. To enhance the model's utility, researchers proposed\nvarious adaptive DP-SGD methods. However, we examine and discover that these\ntechniques result in greater privacy leakage or lower accuracy than the\ntraditional DP-SGD method, or a lack of evaluation on a complex data set such\nas CIFAR100. To address these limitations, we propose an Auto DP-SGD. Our\nmethod automates clipping threshold estimation based on the DL model's gradient\nnorm and scales the gradients of each training sample without losing gradient\ninformation. This helps to improve the algorithm's utility while using a less\nprivacy budget. To further improve accuracy, we introduce automatic noise\nmultiplier decay mechanisms to decrease the noise multiplier after every epoch.\nFinally, we develop closed-form mathematical expressions using tCDP accountant\nfor automatic noise multiplier and automatic clipping threshold estimation.\nThrough extensive experimentation, we demonstrate that Auto DP-SGD outperforms\nexisting SOTA DP-SGD methods in privacy and accuracy on various benchmark\ndatasets. We also show that privacy can be improved by lowering the scale\nfactor and using learning rate schedulers without significantly reducing\naccuracy. Specifically, Auto DP-SGD, when used with a step noise multiplier,\nimproves accuracy by 3.20, 1.57, 6.73, and 1.42 for the MNIST, CIFAR10,\nCIFAR100, and AG News Corpus datasets, respectively. Furthermore, it obtains a\nsubstantial reduction in the privacy budget of 94.9, 79.16, 67.36, and 53.37\nfor the corresponding data sets.",
    "updated" : "2023-12-05T00:09:57Z",
    "published" : "2023-12-05T00:09:57Z",
    "authors" : [
      {
        "name" : "Sai Venkatesh Chilukoti"
      },
      {
        "name" : "Md Imran Hossen"
      },
      {
        "name" : "Liqun Shan"
      },
      {
        "name" : "Vijay Srinivas Tida"
      },
      {
        "name" : "Xiai Hei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "26, 40"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.02327v1",
    "title" : "FLea: Improving federated learning on scarce and label-skewed data via\n  privacy-preserving feature augmentation",
    "summary" : "Learning a global model by abstracting the knowledge, distributed across\nmultiple clients, without aggregating the raw data is the primary goal of\nFederated Learning (FL). Typically, this works in rounds alternating between\nparallel local training at several clients, followed by model aggregation at a\nserver. We found that existing FL methods under-perform when local datasets are\nsmall and present severe label skew as these lead to over-fitting and local\nmodel bias. This is a realistic setting in many real-world applications. To\naddress the problem, we propose \\textit{FLea}, a unified framework that tackles\nover-fitting and local bias by encouraging clients to exchange\nprivacy-protected features to aid local training. The features refer to\nactivations from an intermediate layer of the model, which are obfuscated\nbefore being shared with other clients to protect sensitive information in the\ndata. \\textit{FLea} leverages a novel way of combining local and shared\nfeatures as augmentations to enhance local model learning. Our extensive\nexperiments demonstrate that \\textit{FLea} outperforms the start-of-the-art FL\nmethods, sharing only model parameters, by up to $17.6\\%$, and FL methods that\nshare data augmentations by up to $6.3\\%$, while reducing the privacy\nvulnerability associated with shared data augmentations.",
    "updated" : "2023-12-04T20:24:09Z",
    "published" : "2023-12-04T20:24:09Z",
    "authors" : [
      {
        "name" : "Tong Xia"
      },
      {
        "name" : "Abhirup Ghosh"
      },
      {
        "name" : "Cecilia Mascolo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.02112v1",
    "title" : "Distributed Optimization with Feasible Set Privacy",
    "summary" : "We consider the setup of a constrained optimization problem with two agents\n$E_1$ and $E_2$ who jointly wish to learn the optimal solution set while\nkeeping their feasible sets $\\mathcal{P}_1$ and $\\mathcal{P}_2$ private from\neach other. The objective function $f$ is globally known and each feasible set\nis a collection of points from a global alphabet. We adopt a sequential\nsymmetric private information retrieval (SPIR) framework where one of the\nagents (say $E_1$) privately checks in $\\mathcal{P}_2$, the presence of\ncandidate solutions of the problem constrained to $\\mathcal{P}_1$ only, while\nlearning no further information on $\\mathcal{P}_2$ than the solution alone.\nFurther, we extract an information theoretically private threshold PSI (ThPSI)\nprotocol from our scheme and characterize its download cost. We show that,\ncompared to privately acquiring the feasible set $\\mathcal{P}_1\\cap\n\\mathcal{P}_2$ using an SPIR-based private set intersection (PSI) protocol, and\nfinding the optimum, our scheme is better as it incurs less information leakage\nand less download cost than the former. Over all possible uniform mappings of\n$f$ to a fixed range of values, our scheme outperforms the former with a high\nprobability.",
    "updated" : "2023-12-04T18:45:04Z",
    "published" : "2023-12-04T18:45:04Z",
    "authors" : [
      {
        "name" : "Shreya Meel"
      },
      {
        "name" : "Sennur Ulukus"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.NI",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.02093v1",
    "title" : "Cultural Differences in Students' Privacy Concerns in Learning Analytics\n  across Germany, South Korea, Spain, Sweden, and the United States",
    "summary" : "Applications of learning analytics (LA) can raise concerns from students\nabout their privacy in higher education contexts. Developing effective\nprivacy-enhancing practices requires a systematic understanding of students'\nprivacy concerns and how they vary across national and cultural dimensions. We\nconducted a survey study with established instruments to measure privacy\nconcerns and cultural values for university students in five countries\n(Germany, South Korea, Spain, Sweden, and the United States; N = 762). The\nresults show that students generally trusted institutions with their data and\ndisclosed information as they perceived the risks to be manageable even though\nthey felt somewhat limited in their ability to control their privacy. Across\nthe five countries, German and Swedish students stood out as the most trusting\nand least concerned, especially compared to US students who reported greater\nperceived risk and less control. Students in South Korea and Spain responded\nsimilarly on all five privacy dimensions (perceived privacy risk, perceived\nprivacy control, privacy concerns, trusting beliefs, and non-self-disclosure\nbehavior), despite their significant cultural differences. Culture measured at\nthe individual level affected the antecedents and outcomes of privacy concerns\nmore than country-level culture. Perceived privacy risk and privacy control\nincrease with power distance. Trusting beliefs increase with a desire for\nuncertainty avoidance and lower masculinity. Non-self-disclosure behaviors rise\nwith power distance and masculinity, and decrease with more uncertainty\navoidance. Thus, cultural values related to trust in institutions, social\nequality and risk-taking should be considered when developing privacy-enhancing\npractices and policies in higher education.",
    "updated" : "2023-12-04T18:10:20Z",
    "published" : "2023-12-04T18:10:20Z",
    "authors" : [
      {
        "name" : "Olga Viberg"
      },
      {
        "name" : "René F. Kizilcec"
      },
      {
        "name" : "Ioana Jivet"
      },
      {
        "name" : "Alejandra Martínez Monés"
      },
      {
        "name" : "Alice Oh"
      },
      {
        "name" : "Chantal Mutimukwe"
      },
      {
        "name" : "Stefan Hrastinski"
      },
      {
        "name" : "Maren Scheffel"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.02003v1",
    "title" : "A Survey on Large Language Model (LLM) Security and Privacy: The Good,\n  the Bad, and the Ugly",
    "summary" : "Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized\nnatural language understanding and generation. They possess deep language\ncomprehension, human-like text generation capabilities, contextual awareness,\nand robust problem-solving skills, making them invaluable in various domains\n(e.g., search engines, customer support, translation). In the meantime, LLMs\nhave also gained traction in the security community, revealing security\nvulnerabilities and showcasing their potential in security-related tasks. This\npaper explores the intersection of LLMs with security and privacy.\nSpecifically, we investigate how LLMs positively impact security and privacy,\npotential risks and threats associated with their use, and inherent\nvulnerabilities within LLMs. Through a comprehensive literature review, the\npaper categorizes findings into \"The Good\" (beneficial LLM applications), \"The\nBad\" (offensive applications), and \"The Ugly\" (vulnerabilities and their\ndefenses). We have some interesting findings. For example, LLMs have proven to\nenhance code and data security, outperforming traditional methods. However,\nthey can also be harnessed for various attacks (particularly user-level\nattacks) due to their human-like reasoning abilities. We have identified areas\nthat require further research efforts. For example, research on model and\nparameter extraction attacks is limited and often theoretical, hindered by LLM\nparameter scale and confidentiality. Safe instruction tuning, a recent\ndevelopment, requires more exploration. We hope that our work can shed light on\nthe LLMs' potential to both bolster and jeopardize cybersecurity.",
    "updated" : "2023-12-04T16:25:18Z",
    "published" : "2023-12-04T16:25:18Z",
    "authors" : [
      {
        "name" : "Yifan Yao"
      },
      {
        "name" : "Jinhao Duan"
      },
      {
        "name" : "Kaidi Xu"
      },
      {
        "name" : "Yuanfang Cai"
      },
      {
        "name" : "Eric Sun"
      },
      {
        "name" : "Yue Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.01201v1",
    "title" : "PAC Privacy Preserving Diffusion Models",
    "summary" : "Data privacy protection is garnering increased attention among researchers.\nDiffusion models (DMs), particularly with strict differential privacy, can\npotentially produce images with both high privacy and visual quality. However,\nchallenges arise in ensuring robust protection in privatizing specific data\nattributes, areas where current models often fall short. To address these\nchallenges, we introduce the PAC Privacy Preserving Diffusion Model, a model\nleverages diffusion principles and ensure Probably Approximately Correct (PAC)\nprivacy. We enhance privacy protection by integrating a private classifier\nguidance into the Langevin Sampling Process. Additionally, recognizing the gap\nin measuring the privacy of models, we have developed a novel metric to gauge\nprivacy levels. Our model, assessed with this new metric and supported by\nGaussian matrix computations for the PAC bound, has shown superior performance\nin privacy protection over existing leading private generative models according\nto benchmark tests.",
    "updated" : "2023-12-02T18:42:52Z",
    "published" : "2023-12-02T18:42:52Z",
    "authors" : [
      {
        "name" : "Qipan Xu"
      },
      {
        "name" : "Youlong Ding"
      },
      {
        "name" : "Jie Gao"
      },
      {
        "name" : "Hao Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.01151v1",
    "title" : "Here Is Not There: Measuring Entailment-Based Trajectory Similarity for\n  Location-Privacy Protection and Beyond",
    "summary" : "While the paths humans take play out in social as well as physical space,\nmeasures to describe and compare their trajectories are carried out in\nabstract, typically Euclidean, space. When these measures are applied to\ntrajectories of actual individuals in an application area, alterations that are\ninconsequential in abstract space may suddenly become problematic once overlaid\nwith geographic reality. In this work, we present a different view on\ntrajectory similarity by introducing a measure that utilizes logical\nentailment. This is an inferential perspective that considers facts as triple\nstatements deduced from the social and environmental context in which the\ntravel takes place, and their practical implications. We suggest a\nformalization of entailment-based trajectory similarity, measured as the\noverlapping proportion of facts, which are spatial relation statements in our\ncase study. With the proposed measure, we evaluate LSTM-TrajGAN, a\nprivacy-preserving trajectory-generation model. The entailment-based model\nevaluation reveals potential consequences of disregarding the rich structure of\ngeographic space (e.g., miscalculated insurance risk due to regional shifts in\nour toy example). Our work highlights the advantage of applying logical\nentailment to trajectory-similarity reasoning for location-privacy protection\nand beyond.",
    "updated" : "2023-12-02T14:41:01Z",
    "published" : "2023-12-02T14:41:01Z",
    "authors" : [
      {
        "name" : "Zilong Liu"
      },
      {
        "name" : "Krzysztof Janowicz"
      },
      {
        "name" : "Kitty Currier"
      },
      {
        "name" : "Meilin Shi"
      },
      {
        "name" : "Jinmeng Rao"
      },
      {
        "name" : "Song Gao"
      },
      {
        "name" : "Ling Cai"
      },
      {
        "name" : "Anita Graser"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CL",
      "cs.SC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.01045v1",
    "title" : "PROFL: A Privacy-Preserving Federated Learning Method with Stringent\n  Defense Against Poisoning Attacks",
    "summary" : "Federated Learning (FL) faces two major issues: privacy leakage and poisoning\nattacks, which may seriously undermine the reliability and security of the\nsystem. Overcoming them simultaneously poses a great challenge. This is because\nprivacy protection policies prohibit access to users' local gradients to avoid\nprivacy leakage, while Byzantine-robust methods necessitate access to these\ngradients to defend against poisoning attacks. To address these problems, we\npropose a novel privacy-preserving Byzantine-robust FL framework PROFL. PROFL\nis based on the two-trapdoor additional homomorphic encryption algorithm and\nblinding techniques to ensure the data privacy of the entire FL process. During\nthe defense process, PROFL first utilize secure Multi-Krum algorithm to remove\nmalicious gradients at the user level. Then, according to the Pauta criterion,\nwe innovatively propose a statistic-based privacy-preserving defense algorithm\nto eliminate outlier interference at the feature level and resist impersonation\npoisoning attacks with stronger concealment. Detailed theoretical analysis\nproves the security and efficiency of the proposed method. We conducted\nextensive experiments on two benchmark datasets, and PROFL improved accuracy by\n39% to 75% across different attack settings compared to similar\nprivacy-preserving robust methods, demonstrating its significant advantage in\nrobustness.",
    "updated" : "2023-12-02T06:34:37Z",
    "published" : "2023-12-02T06:34:37Z",
    "authors" : [
      {
        "name" : "Yisheng Zhong"
      },
      {
        "name" : "Li-Ping Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.00989v1",
    "title" : "Scrappy: SeCure Rate Assuring Protocol with PrivacY",
    "summary" : "Preventing abusive activities caused by adversaries accessing online services\nat a rate exceeding that expected by websites has become an ever-increasing\nproblem. CAPTCHAs and SMS authentication are widely used to provide a solution\nby implementing rate limiting, although they are becoming less effective, and\nsome are considered privacy-invasive. In light of this, many studies have\nproposed better rate-limiting systems that protect the privacy of legitimate\nusers while blocking malicious actors. However, they suffer from one or more\nshortcomings: (1) assume trust in the underlying hardware and (2) are\nvulnerable to side-channel attacks. Motivated by the aforementioned issues,\nthis paper proposes Scrappy: SeCure Rate Assuring Protocol with PrivacY.\nScrappy allows clients to generate unforgeable yet unlinkable rate-assuring\nproofs, which provides the server with cryptographic guarantees that the client\nis not misbehaving. We design Scrappy using a combination of DAA and hardware\nsecurity devices. Scrappy is implemented over three types of devices, including\none that can immediately be deployed in the real world. Our baseline evaluation\nshows that the end-to-end latency of Scrappy is minimal, taking only 0.32\nseconds, and uses only 679 bytes of bandwidth when transferring necessary data.\nWe also conduct an extensive security evaluation, showing that the\nrate-limiting capability of Scrappy is unaffected even if the hardware security\ndevice is compromised.",
    "updated" : "2023-12-02T01:07:49Z",
    "published" : "2023-12-02T01:07:49Z",
    "authors" : [
      {
        "name" : "Kosei Akama"
      },
      {
        "name" : "Yoshimichi Nakatsuka"
      },
      {
        "name" : "Masaaki Sato"
      },
      {
        "name" : "Keisuke Uehara"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.00933v1",
    "title" : "Privacy Preserving Event Detection",
    "summary" : "This paper presents a privacy-preserving event detection scheme based on\nmeasurements made by a network of sensors. A diameter-like decision statistic\nmade up of the marginal types of the measurements observed by the sensors is\nemployed. The proposed detection scheme can achieve the best type-I error\nexponent as the type-II error rate is required to be negligible. Detection\nperformance with finite-length observations is also demonstrated through a\nsimulation example of spectrum sensing. Privacy protection is achieved by\nobfuscating the type data with random zero-modulo-sum numbers that are\ngenerated and distributed via the exchange of encrypted messages among the\nsensors. The privacy-preserving performance against ``honest but curious''\nadversaries, including colluding sensors, the fusion center, and external\neavesdroppers, is analyzed through a series of cryptographic games. It is shown\nthat the probability that any probabilistic polynomial time adversary\nsuccessfully estimates the sensors' measured types can not be much better than\nindependent guessing, when there are at least two non-colluding sensors.",
    "updated" : "2023-12-01T21:25:00Z",
    "published" : "2023-12-01T21:25:00Z",
    "authors" : [
      {
        "name" : "Xiaoshan Wang"
      },
      {
        "name" : "Tan F. Wong"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.04579v1",
    "title" : "zkFDL: An efficient and privacy-preserving decentralized federated\n  learning with zero knowledge proof",
    "summary" : "Federated leaning (FL) has been frequently used in various field of studies\nand businesses. Traditional centralized FL systems suffer from serious issues.\nTo address these concerns, decentralized federated learning (DFL) systems have\nbeen introduced in recent years in which with the help of blockchains, try to\nachieve more integrity and efficiency. On the other hand, privacy-preserving is\nan uncovered part of these systems. To address this, and also scaling the\nblockchain-based computations, we propose a zero knowledge proof (ZKP) based\naggregator (zkDFL) that allows clients to share their large-scale model\nparameters with a trusted centralized server without revealing their individual\ndata to other clients. We utilize blockchain technology to manage the\naggregation algorithm via smart contracts. The server performs a ZKP algorithm\nto prove to the clients that the aggregation is done according to the accepted\nalgorithm. The server can also prove that all inputs of clients have been used.\nWe evaluate our measure through a public dataset about wearable internet of\nthings. As demonstrated by numerical evaluations, zkDFL introduces\nverifiability of correctness of aggregation process and enhances the privacy\nprotection and scalability of DFL systems, while the gas cost has declined\nsignificantly.",
    "updated" : "2023-12-01T17:00:30Z",
    "published" : "2023-12-01T17:00:30Z",
    "authors" : [
      {
        "name" : "Mojtaba Ahmadi"
      },
      {
        "name" : "Reza Nourmohammadi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.00645v2",
    "title" : "Hashmarks: Privacy-Preserving Benchmarks for High-Stakes AI Evaluation",
    "summary" : "There is a growing need to gain insight into language model capabilities that\nrelate to sensitive topics, such as bioterrorism or cyberwarfare. However,\ntraditional open source benchmarks are not fit for the task, due to the\nassociated practice of publishing the correct answers in human-readable form.\nAt the same time, enforcing mandatory closed-quarters evaluations might stifle\ndevelopment and erode trust. In this context, we propose hashmarking, a\nprotocol for evaluating language models in the open without having to disclose\nthe correct answers. In its simplest form, a hashmark is a benchmark whose\nreference solutions have been cryptographically hashed prior to publication.\nFollowing an overview of the proposed evaluation protocol, we go on to assess\nits resilience against traditional attack vectors (e.g. rainbow table attacks),\nas well as against failure modes unique to increasingly capable generative\nmodels.",
    "updated" : "2023-12-25T07:45:14Z",
    "published" : "2023-12-01T15:16:00Z",
    "authors" : [
      {
        "name" : "Paul Bricman"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.00519v1",
    "title" : "The Impact of Privacy and Security Attitudes and Concerns of Travellers\n  on Their Willingness to Use Mobility-as-a-Service Systems",
    "summary" : "This paper reports results from an online survey on the impact of travellers'\nprivacy and security attitudes and concerns on their willingness to use\nmobility-as-a-service (MaaS) systems. This study is part of a larger project\nthat aims at investigating barriers to potential MaaS uptake. The online survey\nwas designed to cover data privacy and security attitudes and concerns as well\nas a variety of socio-psychological and socio-demographic variables associated\nwith travellers' intentions to use MaaS systems. The study involved $n=320$ UK\nparticipants recruited via the Prolific survey platform. Overall, correlation\nanalysis and a multiple regression model indicated that, neither attitudes nor\nconcerns of participants over the privacy and security of personal data would\nsignificantly impact their decisions to use MaaS systems, which was an\nunexpected result, however, their trust in (commercial and governmental)\nwebsites would. Another surprising result is that, having been a victim of\nimproper invasion of privacy did not appear to affect individuals' intentions\nto use MaaS systems, whereas frequency with which one heard about misuse of\npersonal data did. Implications of the results and future directions are also\ndiscussed, e.g., MaaS providers are encouraged to work on improving the\ntrustworthiness of their corporate image.",
    "updated" : "2023-12-01T11:51:43Z",
    "published" : "2023-12-01T11:51:43Z",
    "authors" : [
      {
        "name" : "Maria Sophia Heering"
      },
      {
        "name" : "Haiyue Yuan"
      },
      {
        "name" : "Shujun Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.04577v1",
    "title" : "The Evolution of DNS Security and Privacy",
    "summary" : "DNS, one of the fundamental protocols of the TCP/IP stack, has evolved over\nthe years to protect against threats and attacks. This study examines the risks\nassociated with DNS and explores recent advancements that contribute towards\nmaking the DNS ecosystem resilient against various attacks while safeguarding\nuser privacy.",
    "updated" : "2023-12-01T06:14:25Z",
    "published" : "2023-12-01T06:14:25Z",
    "authors" : [
      {
        "name" : "Levente Csikor"
      },
      {
        "name" : "Dinil Mon Divakaran"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01353v1",
    "title" : "The Boomerang protocol: A Decentralised Privacy-Preserving Verifiable\n  Incentive Protocol",
    "summary" : "In the era of data-driven economies, incentive systems and loyalty programs,\nhave become ubiquitous in various sectors, including advertising, retail,\ntravel, and financial services. While these systems offer advantages for both\nusers and companies, they necessitate the transfer and analysis of substantial\namounts of sensitive data. Privacy concerns have become increasingly pertinent,\nnecessitating the development of privacy-preserving incentive protocols.\nDespite the rising demand for secure and decentralised systems, the existing\nlandscape lacks a comprehensive solution. We propose the Boomerang protocol, a\nnovel decentralised privacy-preserving incentive protocol that leverages\ncryptographic black box accumulators to securely store user interactions within\nthe incentive system. Moreover, the protocol employs zero-knowledge proofs\nbased on BulletProofs to transparently compute rewards for users, ensuring\nverifiability while preserving their privacy. To further enhance public\nverifiability and transparency, we utilise a smart contract on a Layer 1\nblockchain to verify these zero-knowledge proofs. The careful combination of\nblack box accumulators with selected elliptic curves in the zero-knowledge\nproofs makes the Boomerang protocol highly efficient. Our proof of concept\nimplementation shows that we can handle up to 23.6 million users per day, on a\nsingle-threaded backend server with financial costs of approximately 2 US$.\nUsing the Solana blockchain we can handle 15.5 million users per day with\napproximate costs of 0.00011 US$ per user. The Boomerang protocol represents a\nsignificant advancement in privacy-preserving incentive protocols, laying the\ngroundwork for a more secure and privacy-centric future.",
    "updated" : "2023-12-06T09:37:45Z",
    "published" : "2023-12-06T09:37:45Z",
    "authors" : [
      {
        "name" : "Ralph Ankele"
      },
      {
        "name" : "Hamed Haddadi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.03918v2",
    "title" : "Data Safety vs. App Privacy: Comparing the Usability of Android and iOS\n  Privacy Labels",
    "summary" : "Privacy labels -- standardized, compact representations of data collection\nand data use practices -- are often presented as a solution to the shortcomings\nof privacy policies. Apple introduced mandatory privacy labels for apps in its\nApp Store in December 2020; Google introduced mandatory labels for Android apps\nin July 2022. iOS app privacy labels have been evaluated and critiqued in prior\nwork. In this work, we evaluated Android Data Safety Labels and explored how\ndifferences between the two label designs impact user comprehension and label\nutility. We conducted a between-subjects, semi-structured interview study with\n12 Android users and 12 iOS users. While some users found Android Data Safety\nLabels informative and helpful, other users found them too vague. Compared to\niOS App Privacy Labels, Android users found the distinction between data\ncollection groups more intuitive and found explicit inclusion of omitted data\ncollection groups more salient. However, some users expressed skepticism\nregarding elided information about collected data type categories. Most users\nmissed critical information due to not expanding the accordion interface, and\nthey were surprised by collection practices excluded from Android's\ndefinitions. Our findings also revealed that Android users generally\nappreciated information about security practices included in the labels, and\niOS users wanted that information added.",
    "updated" : "2024-01-06T14:45:36Z",
    "published" : "2023-12-06T21:32:32Z",
    "authors" : [
      {
        "name" : "Yanzi Lin"
      },
      {
        "name" : "Jaideep Juneja"
      },
      {
        "name" : "Eleanor Birrell"
      },
      {
        "name" : "Lorrie Faith Cranor"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2401.01353v2",
    "title" : "The Boomerang protocol: A Decentralised Privacy-Preserving Verifiable\n  Incentive Protocol",
    "summary" : "In the era of data-driven economies, incentive systems and loyalty programs,\nhave become ubiquitous in various sectors, including advertising, retail,\ntravel, and financial services. While these systems offer advantages for both\nusers and companies, they necessitate the transfer and analysis of substantial\namounts of sensitive data. Privacy concerns have become increasingly pertinent,\nnecessitating the development of privacy-preserving incentive protocols.\nDespite the rising demand for secure and decentralized systems, the existing\nlandscape lacks a comprehensive solution. We propose the Boomerang protocol, a\nnovel decentralized privacy-preserving incentive protocol that leverages\ncryptographic black box accumulators to securely store user interactions within\nthe incentive system. Moreover, the protocol employs zero-knowledge proofs\nbased on BulletProofs to transparently compute rewards for users, ensuring\nverifiability while preserving their privacy. To further enhance public\nverifiability and transparency, we utilize a smart contract on a Layer 1\nblockchain to verify these zero-knowledge proofs. The careful combination of\nblack box accumulators with selected elliptic curves in the zero-knowledge\nproofs makes the Boomerang protocol highly efficient. Our proof of concept\nimplementation shows that we can handle up to 23.6 million users per day, on a\nsingle-threaded backend server with financial costs of approximately 2 USD.\nUsing the Solana blockchain we can handle 15.5 million users per day with\napproximate costs of 0.00011 USD per user. The Boomerang protocol represents a\nsignificant advancement in privacy-preserving incentive protocols, laying the\ngroundwork for a more secure and privacy-centric future.",
    "updated" : "2024-01-09T17:27:33Z",
    "published" : "2023-12-06T09:37:45Z",
    "authors" : [
      {
        "name" : "Ralph Ankele"
      },
      {
        "name" : "Hamed Haddadi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.05720v2",
    "title" : "Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer\n  Inputs of Language Models in Federated Learning",
    "summary" : "Federated learning (FL) emphasizes decentralized training by storing data\nlocally and sending only model updates, underlining user privacy. Recently, a\nline of works on privacy attacks impairs user privacy by extracting sensitive\ntraining text from language models in the context of FL. Yet, these attack\ntechniques face distinct hurdles: some work chiefly with limited batch sizes\n(e.g., batch size of 1), and others are easily detectable. This paper\nintroduces an innovative approach that is challenging to detect, significantly\nenhancing the recovery rate of text in various batch-size settings. Building on\nfundamental gradient matching and domain prior knowledge, we enhance the attack\nby recovering the input of the Pooler layer of language models, which enables\nus to provide additional supervised signals at the feature level. Unlike\ngradient data, these signals do not average across sentences and tokens,\nthereby offering more nuanced and effective insights. We benchmark our method\nusing text classification tasks on datasets such as CoLA, SST-2, and Rotten\nTomatoes. Across different batch sizes and models, our approach consistently\noutperforms previous state-of-the-art results.",
    "updated" : "2024-01-11T04:09:49Z",
    "published" : "2023-12-10T01:19:59Z",
    "authors" : [
      {
        "name" : "Jianwei Li"
      },
      {
        "name" : "Sheng Liu"
      },
      {
        "name" : "Qi Lei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.15591v2",
    "title" : "Privacy-Preserving Neural Graph Databases",
    "summary" : "In the era of big data and rapidly evolving information systems, efficient\nand accurate data retrieval has become increasingly crucial. Neural graph\ndatabases (NGDBs) have emerged as a powerful paradigm that combines the\nstrengths of graph databases (graph DBs) and neural networks to enable\nefficient storage, retrieval, and analysis of graph-structured data. The usage\nof neural embedding storage and complex neural logical query answering provides\nNGDBs with generalization ability. When the graph is incomplete, by extracting\nlatent patterns and representations, neural graph databases can fill gaps in\nthe graph structure, revealing hidden relationships and enabling accurate query\nanswering. Nevertheless, this capability comes with inherent trade-offs, as it\nintroduces additional privacy risks to the database. Malicious attackers can\ninfer more sensitive information in the database using well-designed\ncombinatorial queries, such as by comparing the answer sets of where Turing\nAward winners born before 1950 and after 1940 lived, the living places of\nTuring Award winner Hinton are probably exposed, although the living places may\nhave been deleted in the training due to the privacy concerns. In this work,\ninspired by the privacy protection in graph embeddings, we propose a\nprivacy-preserving neural graph database (P-NGDB) to alleviate the risks of\nprivacy leakage in NGDBs. We introduce adversarial training techniques in the\ntraining stage to force the NGDBs to generate indistinguishable answers when\nqueried with private information, enhancing the difficulty of inferring\nsensitive information through combinations of multiple innocuous queries.\nExtensive experiment results on three datasets show that P-NGDB can effectively\nprotect private information in the graph database while delivering high-quality\npublic answers responses to queries.",
    "updated" : "2024-01-19T14:08:23Z",
    "published" : "2023-12-25T02:32:05Z",
    "authors" : [
      {
        "name" : "Qi Hu"
      },
      {
        "name" : "Haoran Li"
      },
      {
        "name" : "Jiaxin Bai"
      },
      {
        "name" : "Yangqiu Song"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2312.16554v2",
    "title" : "A Theoretical Analysis of Efficiency Constrained Utility-Privacy\n  Bi-Objective Optimization in Federated Learning",
    "summary" : "Federated learning (FL) enables multiple clients to collaboratively learn a\nshared model without sharing their individual data. Concerns about utility,\nprivacy, and training efficiency in FL have garnered significant research\nattention. Differential privacy has emerged as a prevalent technique in FL,\nsafeguarding the privacy of individual user data while impacting utility and\ntraining efficiency. Within Differential Privacy Federated Learning (DPFL),\nprevious studies have primarily focused on the utility-privacy trade-off,\nneglecting training efficiency, which is crucial for timely completion.\nMoreover, differential privacy achieves privacy by introducing controlled\nrandomness (noise) on selected clients in each communication round. Previous\nwork has mainly examined the impact of noise level ($\\sigma$) and communication\nrounds ($T$) on the privacy-utility dynamic, overlooking other influential\nfactors like the sample ratio ($q$, the proportion of selected clients). This\npaper systematically formulates an efficiency-constrained utility-privacy\nbi-objective optimization problem in DPFL, focusing on $\\sigma$, $T$, and $q$.\nWe provide a comprehensive theoretical analysis, yielding analytical solutions\nfor the Pareto front. Extensive empirical experiments verify the validity and\nefficacy of our analysis, offering valuable guidance for low-cost parameter\ndesign in DPFL.",
    "updated" : "2024-01-29T12:27:56Z",
    "published" : "2023-12-27T12:37:55Z",
    "authors" : [
      {
        "name" : "Hanlin Gu"
      },
      {
        "name" : "Xinyuan Zhao"
      },
      {
        "name" : "Gongxi Zhu"
      },
      {
        "name" : "Yuxing Han"
      },
      {
        "name" : "Yan Kang"
      },
      {
        "name" : "Lixin Fan"
      },
      {
        "name" : "Qiang Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2402.00013v1",
    "title" : "No More Trade-Offs. GPT and Fully Informative Privacy Policies",
    "summary" : "The paper reports the results of an experiment aimed at testing to what\nextent ChatGPT 3.5 and 4 is able to answer questions regarding privacy policies\ndesigned in the new format that we propose. In a world of human-only\ninterpreters, there was a trade-off between comprehensiveness and\ncomprehensibility of privacy policies, leading to the actual policies not\ncontaining enough information for users to learn anything meaningful. Having\nshown that GPT performs relatively well with the new format, we provide\nexperimental evidence supporting our policy suggestion, namely that the law\nshould require fully comprehensive privacy policies, even if this means they\nbecome less concise.",
    "updated" : "2023-12-27T11:09:26Z",
    "published" : "2023-12-27T11:09:26Z",
    "authors" : [
      {
        "name" : "Przemysław Pałka"
      },
      {
        "name" : "Marco Lippi"
      },
      {
        "name" : "Francesca Lagioia"
      },
      {
        "name" : "Rūta Liepiņa"
      },
      {
        "name" : "Giovanni Sartor"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CR"
    ]
  }
]