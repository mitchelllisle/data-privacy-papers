[
  {
    "id" : "http://arxiv.org/abs/2407.02268v1",
    "title" : "Footprints of Data in a Classifier Model: The Privacy Issues and Their\n  Mitigation through Data Obfuscation",
    "summary" : "The avalanche of AI deployment and its security-privacy concerns are two\nsides of the same coin. Article 17 of GDPR calls for the Right to Erasure; data\nhas to be obliterated from a system to prevent its compromise. Extant research\nin this aspect focuses on effacing sensitive data attributes. However, several\npassive modes of data compromise are yet to be recognized and redressed. The\nembedding of footprints of training data in a prediction model is one such\nfacet; the difference in performance quality in test and training data causes\npassive identification of data that have trained the model. This research\nfocuses on addressing the vulnerability arising from the data footprints. The\nthree main aspects are -- i] exploring the vulnerabilities of different\nclassifiers (to segregate the vulnerable and the non-vulnerable ones), ii]\nreducing the vulnerability of vulnerable classifiers (through data obfuscation)\nto preserve model and data privacy, and iii] exploring the privacy-performance\ntradeoff to study the usability of the data obfuscation techniques. An\nempirical study is conducted on three datasets and eight classifiers to explore\nthe above objectives. The results of the initial research identify the\nvulnerability in classifiers and segregate the vulnerable and non-vulnerable\nclassifiers. The additional experiments on data obfuscation techniques reveal\ntheir utility to render data and model privacy and also their capability to\nchalk out a privacy-performance tradeoff in most scenarios. The results can aid\nthe practitioners with their choice of classifiers in different scenarios and\ncontexts.",
    "updated" : "2024-07-02T13:56:37Z",
    "published" : "2024-07-02T13:56:37Z",
    "authors" : [
      {
        "name" : "Payel Sadhukhan"
      },
      {
        "name" : "Tanujit Chakraborty"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02226v1",
    "title" : "RollupTheCrowd: Leveraging ZkRollups for a Scalable and\n  Privacy-Preserving Reputation-based Crowdsourcing Platform",
    "summary" : "Current blockchain-based reputation solutions for crowdsourcing fail to\ntackle the challenge of ensuring both efficiency and privacy without\ncompromising the scalability of the blockchain. Developing an effective,\ntransparent, and privacy-preserving reputation model necessitates on-chain\nimplementation using smart contracts. However, managing task evaluation and\nreputation updates alongside crowdsourcing transactions on-chain substantially\nstrains system scalability and performance. This paper introduces\nRollupTheCrowd, a novel blockchain-powered crowdsourcing framework that\nleverages zkRollups to enhance system scalability while protecting user\nprivacy. Our framework includes an effective and privacy-preserving reputation\nmodel that gauges workers' trustworthiness by assessing their crowdsourcing\ninteractions. To alleviate the load on our blockchain, we employ an off-chain\nstorage scheme, optimizing RollupTheCrowd's performance. Utilizing smart\ncontracts and zero-knowledge proofs, our Rollup layer achieves a significant\n20x reduction in gas consumption. To prove the feasibility of the proposed\nframework, we developed a proof-of-concept implementation using cutting-edge\ntools. The experimental results presented in this paper demonstrate the\neffectiveness and scalability of RollupTheCrowd, validating its potential for\nreal-world application scenarios.",
    "updated" : "2024-07-02T12:51:32Z",
    "published" : "2024-07-02T12:51:32Z",
    "authors" : [
      {
        "name" : "Ahmed Mounsf Rafik Bendada"
      },
      {
        "name" : "Mouhamed Amine Bouchiha"
      },
      {
        "name" : "Mourad Rabah"
      },
      {
        "name" : "Yacine Ghamri-Doudane"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02191v1",
    "title" : "Attack-Aware Noise Calibration for Differential Privacy",
    "summary" : "Differential privacy (DP) is a widely used approach for mitigating privacy\nrisks when training machine learning models on sensitive data. DP mechanisms\nadd noise during training to limit the risk of information leakage. The scale\nof the added noise is critical, as it determines the trade-off between privacy\nand utility. The standard practice is to select the noise scale in terms of a\nprivacy budget parameter $\\epsilon$. This parameter is in turn interpreted in\nterms of operational attack risk, such as accuracy, or sensitivity and\nspecificity of inference attacks against the privacy of the data. We\ndemonstrate that this two-step procedure of first calibrating the noise scale\nto a privacy budget $\\epsilon$, and then translating $\\epsilon$ to attack risk\nleads to overly conservative risk assessments and unnecessarily low utility. We\npropose methods to directly calibrate the noise scale to a desired attack risk\nlevel, bypassing the intermediate step of choosing $\\epsilon$. For a target\nattack risk, our approach significantly decreases noise scale, leading to\nincreased utility at the same level of privacy. We empirically demonstrate that\ncalibrating noise to attack sensitivity/specificity, rather than $\\epsilon$,\nwhen training privacy-preserving ML models substantially improves model\naccuracy for the same risk level. Our work provides a principled and practical\nway to improve the utility of privacy-preserving ML without compromising on\nprivacy.",
    "updated" : "2024-07-02T11:49:59Z",
    "published" : "2024-07-02T11:49:59Z",
    "authors" : [
      {
        "name" : "Bogdan Kulynych"
      },
      {
        "name" : "Juan Felipe Gomez"
      },
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Flavio du Pin Calmon"
      },
      {
        "name" : "Carmela Troncoso"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02027v1",
    "title" : "Privacy Risks of General-Purpose AI Systems: A Foundation for\n  Investigating Practitioner Perspectives",
    "summary" : "The rise of powerful AI models, more formally $\\textit{General-Purpose AI\nSystems}$ (GPAIS), has led to impressive leaps in performance across a wide\nrange of tasks. At the same time, researchers and practitioners alike have\nraised a number of privacy concerns, resulting in a wealth of literature\ncovering various privacy risks and vulnerabilities of AI models. Works\nsurveying such risks provide differing focuses, leading to disparate sets of\nprivacy risks with no clear unifying taxonomy. We conduct a systematic review\nof these survey papers to provide a concise and usable overview of privacy\nrisks in GPAIS, as well as proposed mitigation strategies. The developed\nprivacy framework strives to unify the identified privacy risks and mitigations\nat a technical level that is accessible to non-experts. This serves as the\nbasis for a practitioner-focused interview study to assess technical\nstakeholder perceptions of privacy risks and mitigations in GPAIS.",
    "updated" : "2024-07-02T07:49:48Z",
    "published" : "2024-07-02T07:49:48Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Patrick Gage Kelley"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Kurt Thomas"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01817v1",
    "title" : "Race and Privacy in Broadcast Police Communications",
    "summary" : "Radios are essential for the operations of modern police departments, and\nthey function as both a collaborative communication technology and a\nsociotechnical system. However, little prior research has examined their usage\nor their connections to individual privacy and the role of race in policing,\ntwo growing topics of concern in the US. As a case study, we examine the\nChicago Police Department's (CPD's) use of broadcast police communications\n(BPC) to coordinate the activity of law enforcement officers (LEOs) in the\ncity. From a recently assembled archive of 80,775 hours of BPC associated with\nCPD operations, we analyze text transcripts of radio transmissions broadcast\n9:00 AM to 5:00 PM on August 10th, 2018 in one majority Black, one majority\nwhite, and one majority Hispanic area of the city (24 hours of audio) to\nexplore three research questions: (1) Do BPC reflect reported racial\ndisparities in policing? (2) How and when is gender, race/ethnicity, and age\nmentioned in BPC? (3) To what extent do BPC include sensitive information, and\nwho is put at most risk by this practice? (4) To what extent can large language\nmodels (LLMs) heighten this risk? We explore the vocabulary and speech acts\nused by police in BPC, comparing mentions of personal characteristics to local\ndemographics, the personal information shared over BPC, and the privacy\nconcerns that it poses. Analysis indicates (a) policing professionals in the\ncity of Chicago exhibit disproportionate attention to Black members of the\npublic regardless of context, (b) sociodemographic characteristics like gender,\nrace/ethnicity, and age are primarily mentioned in BPC about event information,\nand (c) disproportionate attention introduces disproportionate privacy risks\nfor Black members of the public.",
    "updated" : "2024-07-01T21:34:51Z",
    "published" : "2024-07-01T21:34:51Z",
    "authors" : [
      {
        "name" : "Pranav Narayanan Venkit"
      },
      {
        "name" : "Christopher Graziul"
      },
      {
        "name" : "Miranda Ardith Goodman"
      },
      {
        "name" : "Samantha Nicole Kenny"
      },
      {
        "name" : "Shomir Wilson"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01334v1",
    "title" : "Protecting Privacy in Classifiers by Token Manipulation",
    "summary" : "Using language models as a remote service entails sending private information\nto an untrusted provider. In addition, potential eavesdroppers can intercept\nthe messages, thereby exposing the information. In this work, we explore the\nprospects of avoiding such data exposure at the level of text manipulation. We\nfocus on text classification models, examining various token mapping and\ncontextualized manipulation functions in order to see whether classifier\naccuracy may be maintained while keeping the original text unrecoverable. We\nfind that although some token mapping functions are easy and straightforward to\nimplement, they heavily influence performance on the downstream task, and via a\nsophisticated attacker can be reconstructed. In comparison, the contextualized\nmanipulation provides an improvement in performance.",
    "updated" : "2024-07-01T14:41:59Z",
    "published" : "2024-07-01T14:41:59Z",
    "authors" : [
      {
        "name" : "Re'em Harel"
      },
      {
        "name" : "Yair Elboher"
      },
      {
        "name" : "Yuval Pinter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01167v1",
    "title" : "Information Density Bounds for Privacy",
    "summary" : "This paper explores the implications of guaranteeing privacy by imposing a\nlower bound on the information density between the private and the public data.\nWe introduce an operationally meaningful privacy measure called pointwise\nmaximal cost (PMC) and demonstrate that imposing an upper bound on PMC is\nequivalent to enforcing a lower bound on the information density. PMC\nquantifies the information leakage about a secret to adversaries who aim to\nminimize non-negative cost functions after observing the outcome of a privacy\nmechanism. When restricted to finite alphabets, PMC can equivalently be defined\nas the information leakage to adversaries aiming to minimize the probability of\nincorrectly guessing randomized functions of the secret. We study the\nproperties of PMC and apply it to standard privacy mechanisms to demonstrate\nits practical relevance. Through a detailed examination, we connect PMC with\nother privacy measures that impose upper or lower bounds on the information\ndensity. Our results highlight that lower bounding the information density is a\nmore stringent requirement than upper bounding it. Overall, our work\nsignificantly bridges the gaps in understanding the relationships between\nvarious privacy frameworks and provides insights for selecting a suitable\nframework for a given application.",
    "updated" : "2024-07-01T10:38:02Z",
    "published" : "2024-07-01T10:38:02Z",
    "authors" : [
      {
        "name" : "Sara Saeidian"
      },
      {
        "name" : "Leonhard Grosse"
      },
      {
        "name" : "Parastoo Sadeghi"
      },
      {
        "name" : "Mikael Skoglund"
      },
      {
        "name" : "Tobias J. Oechtering"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.00991v1",
    "title" : "Pre-capture Privacy via Adaptive Single-Pixel Imaging",
    "summary" : "As cameras become ubiquitous in our living environment, invasion of privacy\nis becoming a growing concern. A common approach to privacy preservation is to\nremove personally identifiable information from a captured image, but there is\na risk of the original image being leaked. In this paper, we propose a\npre-capture privacy-aware imaging method that captures images from which the\ndetails of a pre-specified anonymized target have been eliminated. The proposed\nmethod applies a single-pixel imaging framework in which we introduce a\nfeedback mechanism called an aperture pattern generator. The introduced\naperture pattern generator adaptively outputs the next aperture pattern to\navoid sampling the anonymized target by exploiting the data already acquired as\na clue. Furthermore, the anonymized target can be set to any object without\nchanging hardware. Except for detailed features which have been removed from\nthe anonymized target, the captured images are of comparable quality to those\ncaptured by a general camera and can be used for various computer vision\napplications. In our work, we target faces and license plates and\nexperimentally show that the proposed method can capture clear images in which\ndetailed features of the anonymized target are eliminated to achieve both\nprivacy and utility.",
    "updated" : "2024-07-01T06:05:12Z",
    "published" : "2024-07-01T06:05:12Z",
    "authors" : [
      {
        "name" : "Yoko Sogabe"
      },
      {
        "name" : "Shiori Sugimoto"
      },
      {
        "name" : "Ayumi Matsumoto"
      },
      {
        "name" : "Masaki Kitahara"
      }
    ],
    "categories" : [
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.00873v1",
    "title" : "Privacy-First Crowdsourcing: Blockchain and Local Differential Privacy\n  in Crowdsourced Drone Services",
    "summary" : "We introduce a privacy-preserving framework for integrating consumer-grade\ndrones into bushfire management. This system creates a marketplace where\nbushfire management authorities obtain essential data from drone operators. Key\nfeatures include local differential privacy to protect data providers and a\nblockchain-based solution ensuring fair data exchanges and accountability. The\nframework is validated through a proof-of-concept implementation, demonstrating\nits scalability and potential for various large-scale data collection\nscenarios. This approach addresses privacy concerns and compliance with\nregulations like Australia's Privacy Act 1988, offering a practical solution\nfor enhancing bushfire detection and management through crowdsourced drone\nservices.",
    "updated" : "2024-07-01T00:46:25Z",
    "published" : "2024-07-01T00:46:25Z",
    "authors" : [
      {
        "name" : "Junaid Akram"
      },
      {
        "name" : "Ali Anaissi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03289v1",
    "title" : "Correlated Privacy Mechanisms for Differentially Private Distributed\n  Mean Estimation",
    "summary" : "Differentially private distributed mean estimation (DP-DME) is a fundamental\nbuilding block in privacy-preserving federated learning, where a central server\nestimates the mean of $d$-dimensional vectors held by $n$ users while ensuring\n$(\\epsilon,\\delta)$-DP. Local differential privacy (LDP) and distributed DP\nwith secure aggregation (SecAgg) are the most common notions of DP used in\nDP-DME settings with an untrusted server. LDP provides strong resilience to\ndropouts, colluding users, and malicious server attacks, but suffers from poor\nutility. In contrast, SecAgg-based DP-DME achieves an $O(n)$ utility gain over\nLDP in DME, but requires increased communication and computation overheads and\ncomplex multi-round protocols to handle dropouts and malicious attacks. In this\nwork, we propose CorDP-DME, a novel DP-DME mechanism that spans the gap between\nDME with LDP and distributed DP, offering a favorable balance between utility\nand resilience to dropout and collusion. CorDP-DME is based on correlated\nGaussian noise, ensuring DP without the perfect conditional privacy guarantees\nof SecAgg-based approaches. We provide an information-theoretic analysis of\nCorDP-DME, and derive theoretical guarantees for utility under any given\nprivacy parameters and dropout/colluding user thresholds. Our results\ndemonstrate that (anti) correlated Gaussian DP mechanisms can significantly\nimprove utility in mean estimation tasks compared to LDP -- even in adversarial\nsettings -- while maintaining better resilience to dropouts and attacks\ncompared to distributed DP.",
    "updated" : "2024-07-03T17:22:33Z",
    "published" : "2024-07-03T17:22:33Z",
    "authors" : [
      {
        "name" : "Sajani Vithana"
      },
      {
        "name" : "Viveck R. Cadambe"
      },
      {
        "name" : "Flavio P. Calmon"
      },
      {
        "name" : "Haewon Jeong"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02956v1",
    "title" : "IncogniText: Privacy-enhancing Conditional Text Anonymization via\n  LLM-based Private Attribute Randomization",
    "summary" : "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%.\nFinally, we demonstrate the maturity of IncogniText for real-world applications\nby distilling its anonymization capability into a set of LoRA parameters\nassociated with an on-device model.",
    "updated" : "2024-07-03T09:49:03Z",
    "published" : "2024-07-03T09:49:03Z",
    "authors" : [
      {
        "name" : "Ahmed Frikha"
      },
      {
        "name" : "Nassim Walha"
      },
      {
        "name" : "Krishna Kanth Nakka"
      },
      {
        "name" : "Ricardo Mendes"
      },
      {
        "name" : "Xue Jiang"
      },
      {
        "name" : "Xuebing Zhou"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02766v1",
    "title" : "Balancing Patient Privacy and Health Data Security: The Role of\n  Compliance in Protected Health Information (PHI) Sharing",
    "summary" : "Protected Health Information (PHI) sharing significantly enhances patient\ncare quality and coordination, contributing to more accurate diagnoses,\nefficient treatment plans, and a comprehensive understanding of patient\nhistory. Compliance with strict privacy and security policies, such as those\nrequired by laws like HIPAA, is critical to protect PHI. Blockchain technology,\nwhich offers a decentralized and tamper-evident ledger system, hold promise in\npolicy compliance. This system ensures the authenticity and integrity of PHI\nwhile facilitating patient consent management. In this work, we propose a\nblockchain technology that integrates smart contracts to partially automate\nconsent-related processes and ensuring that PHI access and sharing follow\npatient preferences and legal requirements.",
    "updated" : "2024-07-03T02:49:33Z",
    "published" : "2024-07-03T02:49:33Z",
    "authors" : [
      {
        "name" : "Md Al Amin"
      },
      {
        "name" : "Hemanth Tummala"
      },
      {
        "name" : "Rushabh Shah"
      },
      {
        "name" : "Indrajit Ray"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02747v1",
    "title" : "Curvature Clues: Decoding Deep Learning Privacy with Input Loss\n  Curvature",
    "summary" : "In this paper, we explore the properties of loss curvature with respect to\ninput data in deep neural networks. Curvature of loss with respect to input\n(termed input loss curvature) is the trace of the Hessian of the loss with\nrespect to the input. We investigate how input loss curvature varies between\ntrain and test sets, and its implications for train-test distinguishability. We\ndevelop a theoretical framework that derives an upper bound on the train-test\ndistinguishability based on privacy and the size of the training set. This\nnovel insight fuels the development of a new black box membership inference\nattack utilizing input loss curvature. We validate our theoretical findings\nthrough experiments in computer vision classification tasks, demonstrating that\ninput loss curvature surpasses existing methods in membership inference\neffectiveness. Our analysis highlights how the performance of membership\ninference attack (MIA) methods varies with the size of the training set,\nshowing that curvature-based MIA outperforms other methods on sufficiently\nlarge datasets. This condition is often met by real datasets, as demonstrated\nby our results on CIFAR10, CIFAR100, and ImageNet. These findings not only\nadvance our understanding of deep neural network behavior but also improve the\nability to test privacy-preserving techniques in machine learning.",
    "updated" : "2024-07-03T01:47:46Z",
    "published" : "2024-07-03T01:47:46Z",
    "authors" : [
      {
        "name" : "Deepak Ravikumar"
      },
      {
        "name" : "Efstathia Soufleri"
      },
      {
        "name" : "Kaushik Roy"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02226v1",
    "title" : "RollupTheCrowd: Leveraging ZkRollups for a Scalable and\n  Privacy-Preserving Reputation-based Crowdsourcing Platform",
    "summary" : "Current blockchain-based reputation solutions for crowdsourcing fail to\ntackle the challenge of ensuring both efficiency and privacy without\ncompromising the scalability of the blockchain. Developing an effective,\ntransparent, and privacy-preserving reputation model necessitates on-chain\nimplementation using smart contracts. However, managing task evaluation and\nreputation updates alongside crowdsourcing transactions on-chain substantially\nstrains system scalability and performance. This paper introduces\nRollupTheCrowd, a novel blockchain-powered crowdsourcing framework that\nleverages zkRollups to enhance system scalability while protecting user\nprivacy. Our framework includes an effective and privacy-preserving reputation\nmodel that gauges workers' trustworthiness by assessing their crowdsourcing\ninteractions. To alleviate the load on our blockchain, we employ an off-chain\nstorage scheme, optimizing RollupTheCrowd's performance. Utilizing smart\ncontracts and zero-knowledge proofs, our Rollup layer achieves a significant\n20x reduction in gas consumption. To prove the feasibility of the proposed\nframework, we developed a proof-of-concept implementation using cutting-edge\ntools. The experimental results presented in this paper demonstrate the\neffectiveness and scalability of RollupTheCrowd, validating its potential for\nreal-world application scenarios.",
    "updated" : "2024-07-02T12:51:32Z",
    "published" : "2024-07-02T12:51:32Z",
    "authors" : [
      {
        "name" : "Ahmed Mounsf Rafik Bendada"
      },
      {
        "name" : "Mouhamed Amine Bouchiha"
      },
      {
        "name" : "Mourad Rabah"
      },
      {
        "name" : "Yacine Ghamri-Doudane"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01334v2",
    "title" : "Protecting Privacy in Classifiers by Token Manipulation",
    "summary" : "Using language models as a remote service entails sending private information\nto an untrusted provider. In addition, potential eavesdroppers can intercept\nthe messages, thereby exposing the information. In this work, we explore the\nprospects of avoiding such data exposure at the level of text manipulation. We\nfocus on text classification models, examining various token mapping and\ncontextualized manipulation functions in order to see whether classifier\naccuracy may be maintained while keeping the original text unrecoverable. We\nfind that although some token mapping functions are easy and straightforward to\nimplement, they heavily influence performance on the downstream task, and via a\nsophisticated attacker can be reconstructed. In comparison, the contextualized\nmanipulation provides an improvement in performance.",
    "updated" : "2024-07-03T16:31:52Z",
    "published" : "2024-07-01T14:41:59Z",
    "authors" : [
      {
        "name" : "Re'em Harel"
      },
      {
        "name" : "Yair Elboher"
      },
      {
        "name" : "Yuval Pinter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03732v1",
    "title" : "Collection, usage and privacy of mobility data in the enterprise and\n  public administrations",
    "summary" : "Human mobility data is a crucial resource for urban mobility management, but\nit does not come without personal reference. The implementation of security\nmeasures such as anonymization is thus needed to protect individuals' privacy.\nOften, a trade-off arises as such techniques potentially decrease the utility\nof the data and limit its use. While much research on anonymization techniques\nexists, there is little information on the actual implementations by\npractitioners, especially outside the big tech context. Within our study, we\nconducted expert interviews to gain insights into practices in the field. We\ncategorize purposes, data sources, analysis, and modeling tasks to provide a\nprofound understanding of the context such data is used in. We survey\nprivacy-enhancing methods in use, which generally do not comply with\nstate-of-the-art standards of differential privacy. We provide groundwork for\nfurther research on practice-oriented research by identifying privacy needs of\npractitioners and extracting relevant mobility characteristics for future\nstandardized evaluations of privacy-enhancing methods.",
    "updated" : "2024-07-04T08:29:27Z",
    "published" : "2024-07-04T08:29:27Z",
    "authors" : [
      {
        "name" : "Alexandra Kapp"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03496v1",
    "title" : "Releasing Large-Scale Human Mobility Histograms with Differential\n  Privacy",
    "summary" : "Environmental Insights Explorer (EIE) is a Google product that reports\naggregate statistics about human mobility, including various methods of transit\nused by people across roughly 50,000 regions globally. These statistics are\nused to estimate carbon emissions and provided to policymakers to inform their\ndecisions on transportation policy and infrastructure. Due to the inherent\nsensitivity of this type of user data, it is crucial that the statistics\nderived and released from it are computed with appropriate privacy protections.\nIn this work, we use a combination of federated analytics and differential\nprivacy to release these required statistics, while operating under strict\nerror constraints to ensure utility for downstream stakeholders. In this work,\nwe propose a new mechanism that achieves $ \\epsilon \\approx 2 $-DP while\nsatisfying these strict utility constraints, greatly improving over natural\nbaselines. We believe this mechanism may be of more general interest for the\nbroad class of group-by-sum workloads.",
    "updated" : "2024-07-03T20:54:00Z",
    "published" : "2024-07-03T20:54:00Z",
    "authors" : [
      {
        "name" : "Christopher Bian"
      },
      {
        "name" : "Albert Cheu"
      },
      {
        "name" : "Yannis Guzman"
      },
      {
        "name" : "Marco Gruteser"
      },
      {
        "name" : "Peter Kairouz"
      },
      {
        "name" : "Ryan McKenna"
      },
      {
        "name" : "Edo Roth"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03470v1",
    "title" : "Prosody-Driven Privacy-Preserving Dementia Detection",
    "summary" : "Speaker embeddings extracted from voice recordings have been proven valuable\nfor dementia detection. However, by their nature, these embeddings contain\nidentifiable information which raises privacy concerns. In this work, we aim to\nanonymize embeddings while preserving the diagnostic utility for dementia\ndetection. Previous studies rely on adversarial learning and models trained on\nthe target attribute and struggle in limited-resource settings. We propose a\nnovel approach that leverages domain knowledge to disentangle prosody features\nrelevant to dementia from speaker embeddings without relying on a dementia\nclassifier. Our experiments show the effectiveness of our approach in\npreserving speaker privacy (speaker recognition F1-score .01%) while\nmaintaining high dementia detection score F1-score of 74% on the ADReSS\ndataset. Our results are also on par with a more constrained\nclassifier-dependent system on ADReSSo (.01% and .66%), and have no impact on\nsynthesized speech naturalness.",
    "updated" : "2024-07-03T19:34:47Z",
    "published" : "2024-07-03T19:34:47Z",
    "authors" : [
      {
        "name" : "Dominika Woszczyk"
      },
      {
        "name" : "Ranya Aloufi"
      },
      {
        "name" : "Soteris Demetriou"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CL",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03451v1",
    "title" : "The Role of Privacy Guarantees in Voluntary Donation of Private Data for\n  Altruistic Goals",
    "summary" : "Voluntary donation of private information for altruistic purposes, such as\nadvancing research, is common. However, concerns about data misuse and leakage\nmay deter individuals from donating their information. While prior research has\nindicated that Privacy Enhancement Technologies (PETs) can alleviate these\nconcerns, the extent to which these techniques influence willingness to donate\ndata remains unclear.\n  This study conducts a vignette survey (N=485) to examine people's willingness\nto donate medical data for developing new treatments under four privacy\nguarantees: data expiration, anonymization, use restriction, and access\ncontrol. The study explores two mechanisms for verifying these guarantees:\nself-auditing and expert auditing, and evaluates the impact on two types of\ndata recipient entities: for-profit and non-profit institutions.\n  Our findings reveal that the type of entity collecting data strongly\ninfluences respondents' privacy expectations, which in part influence their\nwillingness to donate data. Respondents have such high expectations of the\nprivacy provided by non-profit entities that explicitly stating the privacy\nprotections provided makes little adjustment to those expectations. In\ncontrast, statements about privacy bring respondents' expectations of the\nprivacy provided by for-profit entities nearly in-line with non-profit\nexpectations. We highlight the risks of these respective results as well as the\nneed for future research to better align technical community and end-user\nperceptions about the effectiveness of auditing PETs and to effectively set\nexpectations about the efficacy of PETs in the face of end-user concerns about\ndata breaches.",
    "updated" : "2024-07-03T18:50:48Z",
    "published" : "2024-07-03T18:50:48Z",
    "authors" : [
      {
        "name" : "Ruizhe Wang"
      },
      {
        "name" : "Roberta De Viti"
      },
      {
        "name" : "Aarushi Dubey"
      },
      {
        "name" : "Elissa M. Redmiles"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05450v1",
    "title" : "Understanding Professional Needs to Create Privacy-Preserving and Secure\n  Emergent Digital Artworks",
    "summary" : "In recent years, immersive art installations featuring interactive artworks\nhave been on the rise. These installations are an integral part of museums and\nart centers like selfie museums, teamLab Borderless, ARTECHOUSE, and Meow Wolf.\nMoreover, immersive art have also been increasingly incorporated into\ntraditional museums as well. However, immersive art requires active user\nparticipation and often captures information from viewers and participants\nthrough cameras, sensors, microphones, embodied interaction devices,\nsurveillance, and kinetic mirrors. Therefore, we propose a new line of research\nto examine the security and privacy postures of immersive artworks. In our\npilot study, we conducted a semi-structured interview with five experienced\npractitioners from either the art (2) or cybersecurity (3) fields. Our aim was\nto understand their current security and privacy practices, along with their\nneeds when it comes to immersive art. From their responses, we created a list\nof security and privacy parameters, such as, providing opt-in mechanics for\ndata collection, knowledge of data collection tools such as proximity sensors,\nand creating security awareness amongst participants by communicating security\nprotocols and threat models. These parameters allow us to build\nprivacy-preserving, secure, and accessible software for individuals working in\nmedia arts, who often have no background on security and privacy. In the\nfuture, we plan to utilize these parameters to develop software in response to\nthose needs and then host an art exhibition of immersive artworks utilizing the\nplatform.",
    "updated" : "2024-07-07T17:21:38Z",
    "published" : "2024-07-07T17:21:38Z",
    "authors" : [
      {
        "name" : "Kathryn Lichlyter"
      },
      {
        "name" : "Urvashi Kishnani"
      },
      {
        "name" : "Kate Hollenbach"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05446v1",
    "title" : "Towards Perceived Security, Perceived Privacy, and the Universal Design\n  of E-Payment Applications",
    "summary" : "With the growth of digital monetary transactions and cashless payments,\nencouraged by the COVID-19 pandemic, use of e-payment applications is on the\nrise. It is thus imperative to understand and evaluate the current posture of\ne-payment applications from three major user-facing angles: security, privacy,\nand usability. To this, we created a high-fidelity prototype of an e-payment\napplication that encompassed features that we wanted to test with users. We\nthen conducted a pilot study where we recruited 12 participants who tested our\nprototype. We find that both security and privacy are important for users of\ne-payment applications. Additionally, some participants perceive the strength\nof security and privacy based on the usability of the application. We provide\nrecommendations such as universal design of e-payment applications.",
    "updated" : "2024-07-07T17:15:09Z",
    "published" : "2024-07-07T17:15:09Z",
    "authors" : [
      {
        "name" : "Urvashi Kishnani"
      },
      {
        "name" : "Isabella Cardenas"
      },
      {
        "name" : "Jailene Castillo"
      },
      {
        "name" : "Rosalyn Conry"
      },
      {
        "name" : "Lukas Rodwin"
      },
      {
        "name" : "Rika Ruiz"
      },
      {
        "name" : "Matthew Walther"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05237v1",
    "title" : "Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex\n  composite losses",
    "summary" : "Differentially private stochastic gradient descent (DP-SGD) refers to a\nfamily of optimization algorithms that provide a guaranteed level of\ndifferential privacy (DP) through DP accounting techniques. However, current\naccounting techniques make assumptions that diverge significantly from\npractical DP-SGD implementations. For example, they may assume the loss\nfunction is Lipschitz continuous and convex, sample the batches randomly with\nreplacement, or omit the gradient clipping step.\n  In this work, we analyze the most commonly used variant of DP-SGD, in which\nwe sample batches cyclically with replacement, perform gradient clipping, and\nonly release the last DP-SGD iterate. More specifically - without assuming\nconvexity, smoothness, or Lipschitz continuity of the loss function - we\nestablish new R\\'enyi differential privacy (RDP) bounds for the last DP-SGD\niterate under the mild assumption that (i) the DP-SGD stepsize is small\nrelative to the topological constants in the loss function, and (ii) the loss\nfunction is weakly-convex. Moreover, we show that our bounds converge to\npreviously established convex bounds when the weak-convexity parameter of the\nobjective function approaches zero. In the case of non-Lipschitz smooth loss\nfunctions, we provide a weaker bound that scales well in terms of the number of\nDP-SGD iterations.",
    "updated" : "2024-07-07T02:35:55Z",
    "published" : "2024-07-07T02:35:55Z",
    "authors" : [
      {
        "name" : "Weiwei Kong"
      },
      {
        "name" : "MÃ³nica Ribero"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DS",
      "math.OC",
      "stat.ML",
      "65K10 (Primary), 60G15, 68P27",
      "G.3; G.1.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05045v1",
    "title" : "Robust Skin Color Driven Privacy Preserving Face Recognition via\n  Function Secret Sharing",
    "summary" : "In this work, we leverage the pure skin color patch from the face image as\nthe additional information to train an auxiliary skin color feature extractor\nand face recognition model in parallel to improve performance of\nstate-of-the-art (SOTA) privacy-preserving face recognition (PPFR) systems. Our\nsolution is robust against black-box attacking and well-established generative\nadversarial network (GAN) based image restoration. We analyze the potential\nrisk in previous work, where the proposed cosine similarity computation might\ndirectly leak the protected precomputed embedding stored on the server side. We\npropose a Function Secret Sharing (FSS) based face embedding comparison\nprotocol without any intermediate result leakage. In addition, we show in\nexperiments that the proposed protocol is more efficient compared to the Secret\nSharing (SS) based protocol.",
    "updated" : "2024-07-06T10:51:35Z",
    "published" : "2024-07-06T10:51:35Z",
    "authors" : [
      {
        "name" : "Dong Han"
      },
      {
        "name" : "Yufan Jiang"
      },
      {
        "name" : "Yong Li"
      },
      {
        "name" : "Ricardo Mendes"
      },
      {
        "name" : "Joachim Denzler"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04952v1",
    "title" : "Granular Privacy Control for Geolocation with Vision Language Models",
    "summary" : "Vision Language Models (VLMs) are rapidly advancing in their capability to\nanswer information-seeking questions. As these models are widely deployed in\nconsumer applications, they could lead to new privacy risks due to emergent\nabilities to identify people in photos, geolocate images, etc. As we\ndemonstrate, somewhat surprisingly, current open-source and proprietary VLMs\nare very capable image geolocators, making widespread geolocation with VLMs an\nimmediate privacy risk, rather than merely a theoretical future concern. As a\nfirst step to address this challenge, we develop a new benchmark, GPTGeoChat,\nto test the ability of VLMs to moderate geolocation dialogues with users. We\ncollect a set of 1,000 image geolocation conversations between in-house\nannotators and GPT-4v, which are annotated with the granularity of location\ninformation revealed at each turn. Using this new dataset, we evaluate the\nability of various VLMs to moderate GPT-4v geolocation conversations by\ndetermining when too much location information has been revealed. We find that\ncustom fine-tuned models perform on par with prompted API-based models when\nidentifying leaked location information at the country or city level; however,\nfine-tuning on supervised data appears to be needed to accurately moderate\nfiner granularities, such as the name of a restaurant or building.",
    "updated" : "2024-07-06T04:06:55Z",
    "published" : "2024-07-06T04:06:55Z",
    "authors" : [
      {
        "name" : "Ethan Mendes"
      },
      {
        "name" : "Yang Chen"
      },
      {
        "name" : "James Hays"
      },
      {
        "name" : "Sauvik Das"
      },
      {
        "name" : "Wei Xu"
      },
      {
        "name" : "Alan Ritter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04906v1",
    "title" : "Privacy or Transparency? Negotiated Smartphone Access as a Signifier of\n  Trust in Romantic Relationships",
    "summary" : "In this work, we analyze two large-scale surveys to examine how individuals\nthink about sharing smartphone access with romantic partners as a function of\ntrust in relationships. We find that the majority of couples have access to\neach others' devices, but may have explicit or implicit boundaries on how this\naccess is to be used. Investigating these boundaries and related social norms,\nwe find that there is little consensus about the level of smartphone access\n(i.e., transparency), or lack thereof (i.e., privacy) that is desirable in\nromantic contexts. However, there is broad agreement that the level of access\nshould be mutual and consensual. Most individuals understand trust to be the\nbasis of their decisions about transparency and privacy. Furthermore, we find\nindividuals have crossed these boundaries, violating their partners' privacy\nand betraying their trust. We examine how, when, why, and by whom these\nbetrayals occur. We consider the ramifications of these boundary violations in\nthe case of intimate partner violence. Finally, we provide recommendations for\ndesign changes to enable technological enforcement of boundaries currently\nenforced by trust, bringing access control in line with users' sharing\npreferences.",
    "updated" : "2024-07-06T00:52:34Z",
    "published" : "2024-07-06T00:52:34Z",
    "authors" : [
      {
        "name" : "Periwinkle Doerfler"
      },
      {
        "name" : "Kieron Ivy Turk"
      },
      {
        "name" : "Chris Geeng"
      },
      {
        "name" : "Damon McCoy"
      },
      {
        "name" : "Jeffrey Ackerman"
      },
      {
        "name" : "Molly Dragiewicz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04776v1",
    "title" : "Quantifying Privacy Risks of Public Statistics to Residents of\n  Subsidized Housing",
    "summary" : "As the U.S. Census Bureau implements its controversial new disclosure\navoidance system, researchers and policymakers debate the necessity of new\nprivacy protections for public statistics. With experiments on both published\nstatistics and synthetic data, we explore a particular privacy concern:\nrespondents in subsidized housing may deliberately not mention unauthorized\nchildren and other household members for fear of being evicted. By combining\npublic statistics from the Decennial Census and the Department of Housing and\nUrban Development, we demonstrate a simple, inexpensive reconstruction attack\nthat could identify subsidized households living in violation of occupancy\nguidelines in 2010. Experiments on synthetic data suggest that a random\nswapping mechanism similar to the Census Bureau's 2010 disclosure avoidance\nmeasures does not significantly reduce the precision of this attack, while a\ndifferentially private mechanism similar to the 2020 disclosure avoidance\nsystem does. Our results provide a valuable example for policymakers seeking a\ntrustworthy, accurate census.",
    "updated" : "2024-07-05T18:00:02Z",
    "published" : "2024-07-05T18:00:02Z",
    "authors" : [
      {
        "name" : "Ryan Steed"
      },
      {
        "name" : "Diana Qing"
      },
      {
        "name" : "Zhiwei Steven Wu"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04751v1",
    "title" : "A Unified Learn-to-Distort-Data Framework for Privacy-Utility Trade-off\n  in Trustworthy Federated Learning",
    "summary" : "In this paper, we first give an introduction to the theoretical basis of the\nprivacy-utility equilibrium in federated learning based on Bayesian privacy\ndefinitions and total variation distance privacy definitions. We then present\nthe \\textit{Learn-to-Distort-Data} framework, which provides a principled\napproach to navigate the privacy-utility equilibrium by explicitly modeling the\ndistortion introduced by the privacy-preserving mechanism as a learnable\nvariable and optimizing it jointly with the model parameters. We demonstrate\nthe applicability of our framework to a variety of privacy-preserving\nmechanisms on the basis of data distortion and highlight its connections to\nrelated areas such as adversarial training, input robustness, and unlearnable\nexamples. These connections enable leveraging techniques from these areas to\ndesign effective algorithms for privacy-utility equilibrium in federated\nlearning under the \\textit{Learn-to-Distort-Data} framework.",
    "updated" : "2024-07-05T08:15:09Z",
    "published" : "2024-07-05T08:15:09Z",
    "authors" : [
      {
        "name" : "Xiaojin Zhang"
      },
      {
        "name" : "Mingcong Xu"
      },
      {
        "name" : "Wei Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.07066v1",
    "title" : "Explainable Hyperdimensional Computing for Balancing Privacy and\n  Transparency in Additive Manufacturing Monitoring",
    "summary" : "In-situ sensing, in conjunction with learning models, presents a unique\nopportunity to address persistent defect issues in Additive Manufacturing (AM)\nprocesses. However, this integration introduces significant data privacy\nconcerns, such as data leakage, sensor data compromise, and model inversion\nattacks, revealing critical details about part design, material composition,\nand machine parameters. Differential Privacy (DP) models, which inject noise\ninto data under mathematical guarantees, offer a nuanced balance between data\nutility and privacy by obscuring traces of sensing data. However, the\nintroduction of noise into learning models, often functioning as black boxes,\ncomplicates the prediction of how specific noise levels impact model accuracy.\nThis study introduces the Differential Privacy-HyperDimensional computing\n(DP-HD) framework, leveraging the explainability of the vector symbolic\nparadigm to predict the noise impact on the accuracy of in-situ monitoring,\nsafeguarding sensitive data while maintaining operational efficiency.\nExperimental results on real-world high-speed melt pool data of AM for\ndetecting overhang anomalies demonstrate that DP-HD achieves superior\noperational efficiency, prediction accuracy, and robust privacy protection,\noutperforming state-of-the-art Machine Learning (ML) models. For example, when\nimplementing the same level of privacy protection (with a privacy budget set at\n1), our model achieved an accuracy of 94.43\\%, surpassing the performance of\ntraditional models such as ResNet50 (52.30\\%), GoogLeNet (23.85\\%), AlexNet\n(55.78\\%), DenseNet201 (69.13\\%), and EfficientNet B2 (40.81\\%). Notably, DP-HD\nmaintains high performance under substantial noise additions designed to\nenhance privacy, unlike current models that suffer significant accuracy\ndeclines under high privacy constraints.",
    "updated" : "2024-07-09T17:42:26Z",
    "published" : "2024-07-09T17:42:26Z",
    "authors" : [
      {
        "name" : "Fardin Jalil Piran"
      },
      {
        "name" : "Prathyush P. Poduval"
      },
      {
        "name" : "Hamza Errahmouni Barkam"
      },
      {
        "name" : "Mohsen Imani"
      },
      {
        "name" : "Farhad Imani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.06778v1",
    "title" : "A BERT-based Empirical Study of Privacy Policies' Compliance with GDPR",
    "summary" : "Since its implementation in May 2018, the General Data Protection Regulation\n(GDPR) has prompted businesses to revisit and revise their data handling\npractices to ensure compliance. The privacy policy, which serves as the primary\nmeans of informing users about their privacy rights and the data practices of\ncompanies, has been significantly updated by numerous businesses post-GDPR\nimplementation. However, many privacy policies remain packed with technical\njargon, lengthy explanations, and vague descriptions of data practices and user\nrights. This makes it a challenging task for users and regulatory authorities\nto manually verify the GDPR compliance of these privacy policies. In this\nstudy, we aim to address the challenge of compliance analysis between GDPR\n(Article 13) and privacy policies for 5G networks. We manually collected\nprivacy policies from almost 70 different 5G MNOs, and we utilized an automated\nBERT-based model for classification. We show that an encouraging 51$\\%$ of\ncompanies demonstrate a strong adherence to GDPR. In addition, we present the\nfirst study that provides current empirical evidence on the readability of\nprivacy policies for 5G network. we adopted readability analysis toolset that\nincorporates various established readability metrics. The findings empirically\nshow that the readability of the majority of current privacy policies remains a\nsignificant challenge. Hence, 5G providers need to invest considerable effort\ninto revising these documents to enhance both their utility and the overall\nuser experience.",
    "updated" : "2024-07-09T11:47:52Z",
    "published" : "2024-07-09T11:47:52Z",
    "authors" : [
      {
        "name" : "Lu Zhang"
      },
      {
        "name" : "Nabil Moukafih"
      },
      {
        "name" : "Hamad Alamri"
      },
      {
        "name" : "Gregory Epiphaniou"
      },
      {
        "name" : "Carsten Maple"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.06496v1",
    "title" : "It's Our Loss: No Privacy Amplification for Hidden State DP-SGD With\n  Non-Convex Loss",
    "summary" : "Differentially Private Stochastic Gradient Descent (DP-SGD) is a popular\niterative algorithm used to train machine learning models while formally\nguaranteeing the privacy of users. However the privacy analysis of DP-SGD makes\nthe unrealistic assumption that all intermediate iterates (aka internal state)\nof the algorithm are released since in practice, only the final trained model,\ni.e., the final iterate of the algorithm is released. In this hidden state\nsetting, prior work has provided tighter analyses, albeit only when the loss\nfunction is constrained, e.g., strongly convex and smooth or linear. On the\nother hand, the privacy leakage observed empirically from hidden state DP-SGD,\neven when using non-convex loss functions suggest that there is in fact a gap\nbetween the theoretical privacy analysis and the privacy guarantees achieved in\npractice. Therefore, it remains an open question whether privacy amplification\nfor DP-SGD is possible in the hidden state setting for general loss functions.\n  Unfortunately, this work answers the aforementioned research question\nnegatively. By carefully constructing a loss function for DP-SGD, we show that\nfor specific loss functions, the final iterate of DP-SGD alone leaks as much\ninformation as the sequence of all iterates combined. Furthermore, we\nempirically verify this result by evaluating the privacy leakage from the final\niterate of DP-SGD with our loss function and show that this matches the\ntheoretical upper bound guaranteed by DP exactly. Therefore, we show that the\ncurrent privacy analysis fo DP-SGD is tight for general loss functions and\nconclude that no privacy amplification is possible for DP-SGD in general for\nall (possibly non-convex) loss functions.",
    "updated" : "2024-07-09T01:58:19Z",
    "published" : "2024-07-09T01:58:19Z",
    "authors" : [
      {
        "name" : "Meenatchi Sundaram Muthu Selva Annamalai"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.06495v1",
    "title" : "Impact Evaluation on the European Privacy Laws governing generative-AI\n  models -- Evidence in Relation between Internet Censorship and the Ban of\n  ChatGPT in Italy",
    "summary" : "We proceed an impact evaluation on the European Privacy Laws governing\ngenerative-AI models, especially, focusing on the effects of the Ban of ChatGPT\nin Italy. We investigate on the causal relationship between Internet Censorship\nData and the Ban of ChatGPT in Italy during the period from March 27, 2023 to\nApril 11, 2023. We analyze the relation based on the hidden Markov model with\nPoisson emissions. We find out that the HTTP Invalid Requests, which decreased\nduring those period, can be explained with seven-state model. Our findings\nshows the apparent inability for the users in the internet accesses as a result\nof EU regulations on the generative-AI.",
    "updated" : "2024-07-09T01:56:42Z",
    "published" : "2024-07-09T01:56:42Z",
    "authors" : [
      {
        "name" : "Tatsuru Kikuchi"
      }
    ],
    "categories" : [
      "econ.GN",
      "q-fin.EC",
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.06443v1",
    "title" : "Exposing Privacy Gaps: Membership Inference Attack on Preference Data\n  for LLM Alignment",
    "summary" : "Large Language Models (LLMs) have seen widespread adoption due to their\nremarkable natural language capabilities. However, when deploying them in\nreal-world settings, it is important to align LLMs to generate texts according\nto acceptable human standards. Methods such as Proximal Policy Optimization\n(PPO) and Direct Preference Optimization (DPO) have made significant progress\nin refining LLMs using human preference data. However, the privacy concerns\ninherent in utilizing such preference data have yet to be adequately studied.\nIn this paper, we investigate the vulnerability of LLMs aligned using human\npreference datasets to membership inference attacks (MIAs), highlighting the\nshortcomings of previous MIA approaches with respect to preference data. Our\nstudy has two main contributions: first, we introduce a novel reference-based\nattack framework specifically for analyzing preference data called PREMIA\n(\\uline{Pre}ference data \\uline{MIA}); second, we provide empirical evidence\nthat DPO models are more vulnerable to MIA compared to PPO models. Our findings\nhighlight gaps in current privacy-preserving practices for LLM alignment.",
    "updated" : "2024-07-08T22:53:23Z",
    "published" : "2024-07-08T22:53:23Z",
    "authors" : [
      {
        "name" : "Qizhang Feng"
      },
      {
        "name" : "Siva Rajesh Kasa"
      },
      {
        "name" : "Hyokun Yun"
      },
      {
        "name" : "Choon Hui Teo"
      },
      {
        "name" : "Sravan Babu Bodapati"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04751v2",
    "title" : "A Unified Learn-to-Distort-Data Framework for Privacy-Utility Trade-off\n  in Trustworthy Federated Learning",
    "summary" : "In this paper, we first give an introduction to the theoretical basis of the\nprivacy-utility equilibrium in federated learning based on Bayesian privacy\ndefinitions and total variation distance privacy definitions. We then present\nthe \\textit{Learn-to-Distort-Data} framework, which provides a principled\napproach to navigate the privacy-utility equilibrium by explicitly modeling the\ndistortion introduced by the privacy-preserving mechanism as a learnable\nvariable and optimizing it jointly with the model parameters. We demonstrate\nthe applicability of our framework to a variety of privacy-preserving\nmechanisms on the basis of data distortion and highlight its connections to\nrelated areas such as adversarial training, input robustness, and unlearnable\nexamples. These connections enable leveraging techniques from these areas to\ndesign effective algorithms for privacy-utility equilibrium in federated\nlearning under the \\textit{Learn-to-Distort-Data} framework.",
    "updated" : "2024-07-09T16:11:04Z",
    "published" : "2024-07-05T08:15:09Z",
    "authors" : [
      {
        "name" : "Xiaojin Zhang"
      },
      {
        "name" : "Mingcong Xu"
      },
      {
        "name" : "Wei Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.07737v1",
    "title" : "Fine-Tuning Large Language Models with User-Level Differential Privacy",
    "summary" : "We investigate practical and scalable algorithms for training large language\nmodels (LLMs) with user-level differential privacy (DP) in order to provably\nsafeguard all the examples contributed by each user. We study two variants of\nDP-SGD with: (1) example-level sampling (ELS) and per-example gradient\nclipping, and (2) user-level sampling (ULS) and per-user gradient clipping. We\nderive a novel user-level DP accountant that allows us to compute provably\ntight privacy guarantees for ELS. Using this, we show that while ELS can\noutperform ULS in specific settings, ULS generally yields better results when\neach user has a diverse collection of examples. We validate our findings\nthrough experiments in synthetic mean estimation and LLM fine-tuning tasks\nunder fixed compute budgets. We find that ULS is significantly better in\nsettings where either (1) strong privacy guarantees are required, or (2) the\ncompute budget is large. Notably, our focus on LLM-compatible training\nalgorithms allows us to scale to models with hundreds of millions of parameters\nand datasets with hundreds of thousands of users.",
    "updated" : "2024-07-10T15:07:58Z",
    "published" : "2024-07-10T15:07:58Z",
    "authors" : [
      {
        "name" : "Zachary Charles"
      },
      {
        "name" : "Arun Ganesh"
      },
      {
        "name" : "Ryan McKenna"
      },
      {
        "name" : "H. Brendan McMahan"
      },
      {
        "name" : "Nicole Mitchell"
      },
      {
        "name" : "Krishna Pillutla"
      },
      {
        "name" : "Keith Rush"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.07262v1",
    "title" : "Differential privacy and Sublinear time are incompatible sometimes",
    "summary" : "Differential privacy and sublinear algorithms are both rapidly emerging\nalgorithmic themes in times of big data analysis. Although recent works have\nshown the existence of differentially private sublinear algorithms for many\nproblems including graph parameter estimation and clustering, little is known\nregarding hardness results on these algorithms. In this paper, we initiate the\nstudy of lower bounds for problems that aim for both differentially-private and\nsublinear-time algorithms. Our main result is the incompatibility of both the\ndesiderata in the general case. In particular, we prove that a simple problem\nbased on one-way marginals yields both a differentially-private algorithm, as\nwell as a sublinear-time algorithm, but does not admit a ``strictly''\nsublinear-time algorithm that is also differentially private.",
    "updated" : "2024-07-09T22:33:57Z",
    "published" : "2024-07-09T22:33:57Z",
    "authors" : [
      {
        "name" : "Jeremiah Blocki"
      },
      {
        "name" : "Hendrik Fichtenberger"
      },
      {
        "name" : "Elena Grigorescu"
      },
      {
        "name" : "Tamalika Mukherjee"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.07066v2",
    "title" : "Explainable Hyperdimensional Computing for Balancing Privacy and\n  Transparency in Additive Manufacturing Monitoring",
    "summary" : "In-situ sensing, in conjunction with learning models, presents a unique\nopportunity to address persistent defect issues in Additive Manufacturing (AM)\nprocesses. However, this integration introduces significant data privacy\nconcerns, such as data leakage, sensor data compromise, and model inversion\nattacks, revealing critical details about part design, material composition,\nand machine parameters. Differential Privacy (DP) models, which inject noise\ninto data under mathematical guarantees, offer a nuanced balance between data\nutility and privacy by obscuring traces of sensing data. However, the\nintroduction of noise into learning models, often functioning as black boxes,\ncomplicates the prediction of how specific noise levels impact model accuracy.\nThis study introduces the Differential Privacy-HyperDimensional computing\n(DP-HD) framework, leveraging the explainability of the vector symbolic\nparadigm to predict the noise impact on the accuracy of in-situ monitoring,\nsafeguarding sensitive data while maintaining operational efficiency.\nExperimental results on real-world high-speed melt pool data of AM for\ndetecting overhang anomalies demonstrate that DP-HD achieves superior\noperational efficiency, prediction accuracy, and robust privacy protection,\noutperforming state-of-the-art Machine Learning (ML) models. For example, when\nimplementing the same level of privacy protection (with a privacy budget set at\n1), our model achieved an accuracy of 94.43%, surpassing the performance of\ntraditional models such as ResNet50 (52.30%), GoogLeNet (23.85%), AlexNet\n(55.78%), DenseNet201 (69.13%), and EfficientNet B2 (40.81%). Notably, DP-HD\nmaintains high performance under substantial noise additions designed to\nenhance privacy, unlike current models that suffer significant accuracy\ndeclines under high privacy constraints.",
    "updated" : "2024-07-10T01:37:05Z",
    "published" : "2024-07-09T17:42:26Z",
    "authors" : [
      {
        "name" : "Fardin Jalil Piran"
      },
      {
        "name" : "Prathyush P. Poduval"
      },
      {
        "name" : "Hamza Errahmouni Barkam"
      },
      {
        "name" : "Mohsen Imani"
      },
      {
        "name" : "Farhad Imani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08529v1",
    "title" : "Enhancing Privacy of Spatiotemporal Federated Learning against Gradient\n  Inversion Attacks",
    "summary" : "Spatiotemporal federated learning has recently raised intensive studies due\nto its ability to train valuable models with only shared gradients in various\nlocation-based services. On the other hand, recent studies have shown that\nshared gradients may be subject to gradient inversion attacks (GIA) on images\nor texts. However, so far there has not been any systematic study of the\ngradient inversion attacks in spatiotemporal federated learning. In this paper,\nwe explore the gradient attack problem in spatiotemporal federated learning\nfrom attack and defense perspectives. To understand privacy risks in\nspatiotemporal federated learning, we first propose Spatiotemporal Gradient\nInversion Attack (ST-GIA), a gradient attack algorithm tailored to\nspatiotemporal data that successfully reconstructs the original location from\ngradients. Furthermore, we design an adaptive defense strategy to mitigate\ngradient inversion attacks in spatiotemporal federated learning. By dynamically\nadjusting the perturbation levels, we can offer tailored protection for varying\nrounds of training data, thereby achieving a better trade-off between privacy\nand utility than current state-of-the-art methods. Through intensive\nexperimental analysis on three real-world datasets, we reveal that the proposed\ndefense strategy can well preserve the utility of spatiotemporal federated\nlearning with effective security protection.",
    "updated" : "2024-07-11T14:17:02Z",
    "published" : "2024-07-11T14:17:02Z",
    "authors" : [
      {
        "name" : "Lele Zheng"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Renhe Jiang"
      },
      {
        "name" : "Kenjiro Taura"
      },
      {
        "name" : "Yulong Shen"
      },
      {
        "name" : "Sheng Li"
      },
      {
        "name" : "Masatoshi Yoshikawa"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08152v1",
    "title" : "Privacy-Preserving Data Deduplication for Enhancing Federated Learning\n  of Language Models",
    "summary" : "Deduplication is a vital preprocessing step that enhances machine learning\nmodel performance and saves training time and energy. However, enhancing\nfederated learning through deduplication poses challenges, especially regarding\nscalability and potential privacy violations if deduplication involves sharing\nall clients' data. In this paper, we address the problem of deduplication in a\nfederated setup by introducing a pioneering protocol, Efficient\nPrivacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes\nduplicates from multiple clients' datasets without compromising data privacy.\nEP-MPD is constructed in a modular fashion, utilizing two novel variants of the\nPrivate Set Intersection protocol. Our extensive experiments demonstrate the\nsignificant benefits of deduplication in federated learning of large language\nmodels. For instance, we observe up to 19.61% improvement in perplexity and up\nto 27.95% reduction in running time. EP-MPD effectively balances privacy and\nperformance in federated learning, making it a valuable solution for\nlarge-scale applications.",
    "updated" : "2024-07-11T03:10:27Z",
    "published" : "2024-07-11T03:10:27Z",
    "authors" : [
      {
        "name" : "Aydin Abadi"
      },
      {
        "name" : "Vishnu Asutosh Dasu"
      },
      {
        "name" : "Sumanta Sarkar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.07926v1",
    "title" : "Synthetic Data: Revisiting the Privacy-Utility Trade-off",
    "summary" : "Synthetic data has been considered a better privacy-preserving alternative to\ntraditionally sanitized data across various applications. However, a recent\narticle challenges this notion, stating that synthetic data does not provide a\nbetter trade-off between privacy and utility than traditional anonymization\ntechniques, and that it leads to unpredictable utility loss and highly\nunpredictable privacy gain. The article also claims to have identified a breach\nin the differential privacy guarantees provided by PATEGAN and PrivBayes. When\na study claims to refute or invalidate prior findings, it is crucial to verify\nand validate the study. In our work, we analyzed the implementation of the\nprivacy game described in the article and found that it operated in a highly\nspecialized and constrained environment, which limits the applicability of its\nfindings to general cases. Our exploration also revealed that the game did not\nsatisfy a crucial precondition concerning data distributions, which contributed\nto the perceived violation of the differential privacy guarantees offered by\nPATEGAN and PrivBayes. We also conducted a privacy-utility trade-off analysis\nin a more general and unconstrained environment. Our experimentation\ndemonstrated that synthetic data achieves a more favorable privacy-utility\ntrade-off compared to the provided implementation of k-anonymization, thereby\nreaffirming earlier conclusions.",
    "updated" : "2024-07-09T14:48:43Z",
    "published" : "2024-07-09T14:48:43Z",
    "authors" : [
      {
        "name" : "Fatima Jahan Sarmin"
      },
      {
        "name" : "Atiquer Rahman Sarkar"
      },
      {
        "name" : "Yang Wang"
      },
      {
        "name" : "Noman Mohammed"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09324v1",
    "title" : "Provable Privacy Advantages of Decentralized Federated Learning via\n  Distributed Optimization",
    "summary" : "Federated learning (FL) emerged as a paradigm designed to improve data\nprivacy by enabling data to reside at its source, thus embedding privacy as a\ncore consideration in FL architectures, whether centralized or decentralized.\nContrasting with recent findings by Pasquini et al., which suggest that\ndecentralized FL does not empirically offer any additional privacy or security\nbenefits over centralized models, our study provides compelling evidence to the\ncontrary. We demonstrate that decentralized FL, when deploying distributed\noptimization, provides enhanced privacy protection - both theoretically and\nempirically - compared to centralized approaches. The challenge of quantifying\nprivacy loss through iterative processes has traditionally constrained the\ntheoretical exploration of FL protocols. We overcome this by conducting a\npioneering in-depth information-theoretical privacy analysis for both\nframeworks. Our analysis, considering both eavesdropping and passive adversary\nmodels, successfully establishes bounds on privacy leakage. We show information\ntheoretically that the privacy loss in decentralized FL is upper bounded by the\nloss in centralized FL. Compared to the centralized case where local gradients\nof individual participants are directly revealed, a key distinction of\noptimization-based decentralized FL is that the relevant information includes\ndifferences of local gradients over successive iterations and the aggregated\nsum of different nodes' gradients over the network. This information\ncomplicates the adversary's attempt to infer private data. To bridge our\ntheoretical insights with practical applications, we present detailed case\nstudies involving logistic regression and deep neural networks. These examples\ndemonstrate that while privacy leakage remains comparable in simpler models,\ncomplex models like deep neural networks exhibit lower privacy risks under\ndecentralized FL.",
    "updated" : "2024-07-12T15:01:09Z",
    "published" : "2024-07-12T15:01:09Z",
    "authors" : [
      {
        "name" : "Wenrui Yu"
      },
      {
        "name" : "Qiongxiu Li"
      },
      {
        "name" : "Milan LopuhaÃ¤-Zwakenberg"
      },
      {
        "name" : "Mads GrÃ¦sbÃ¸ll Christensen"
      },
      {
        "name" : "Richard Heusdens"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09239v1",
    "title" : "FedVAE: Trajectory privacy preserving based on Federated Variational\n  AutoEncoder",
    "summary" : "The use of trajectory data with abundant spatial-temporal information is\npivotal in Intelligent Transport Systems (ITS) and various traffic system\ntasks. Location-Based Services (LBS) capitalize on this trajectory data to\noffer users personalized services tailored to their location information.\nHowever, this trajectory data contains sensitive information about users'\nmovement patterns and habits, necessitating confidentiality and protection from\nunknown collectors. To address this challenge, privacy-preserving methods like\nK-anonymity and Differential Privacy have been proposed to safeguard private\ninformation in the dataset. Despite their effectiveness, these methods can\nimpact the original features by introducing perturbations or generating\nunrealistic trajectory data, leading to suboptimal performance in downstream\ntasks. To overcome these limitations, we propose a Federated Variational\nAutoEncoder (FedVAE) approach, which effectively generates a new trajectory\ndataset while preserving the confidentiality of private information and\nretaining the structure of the original features. In addition, FedVAE leverages\nVariational AutoEncoder (VAE) to maintain the original feature space and\ngenerate new trajectory data, and incorporates Federated Learning (FL) during\nthe training stage, ensuring that users' data remains locally stored to protect\ntheir personal information. The results demonstrate its superior performance\ncompared to other existing methods, affirming FedVAE as a promising solution\nfor enhancing data privacy and utility in location-based applications.",
    "updated" : "2024-07-12T13:10:59Z",
    "published" : "2024-07-12T13:10:59Z",
    "authors" : [
      {
        "name" : "Yuchen Jiang"
      },
      {
        "name" : "Ying Wu"
      },
      {
        "name" : "Shiyao Zhang"
      },
      {
        "name" : "James J. Q. Yu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09004v1",
    "title" : "Privacy-Preserving Collaborative Genomic Research: A Real-Life\n  Deployment and Vision",
    "summary" : "The data revolution holds significant promise for the health sector. Vast\namounts of data collected from individuals will be transformed into knowledge,\nAI models, predictive systems, and best practices. One area of health that\nstands to benefit greatly is the genomic domain. Progress in AI, machine\nlearning, and data science has opened new opportunities for genomic research,\npromising breakthroughs in personalized medicine. However, increasing awareness\nof privacy and cybersecurity necessitates robust solutions to protect sensitive\ndata in collaborative research. This paper presents a practical deployment of a\nprivacy-preserving framework for genomic research, developed in collaboration\nwith Lynx.MD, a platform for secure health data collaboration. The framework\naddresses critical cybersecurity and privacy challenges, enabling the\nprivacy-preserving sharing and analysis of genomic data while mitigating risks\nassociated with data breaches. By integrating advanced privacy-preserving\nalgorithms, the solution ensures the protection of individual privacy without\ncompromising data utility. A unique feature of the system is its ability to\nbalance trade-offs between data sharing and privacy, providing stakeholders\ntools to quantify privacy risks and make informed decisions. Implementing the\nframework within Lynx.MD involves encoding genomic data into binary formats and\napplying noise through controlled perturbation techniques. This approach\npreserves essential statistical properties of the data, facilitating effective\nresearch and analysis. Moreover, the system incorporates real-time data\nmonitoring and advanced visualization tools, enhancing user experience and\ndecision-making. The paper highlights the need for tailored privacy attacks and\ndefenses specific to genomic data. Addressing these challenges fosters\ncollaboration in genomic research, advancing personalized medicine and public\nhealth.",
    "updated" : "2024-07-12T05:43:13Z",
    "published" : "2024-07-12T05:43:13Z",
    "authors" : [
      {
        "name" : "Zahra Rahmani"
      },
      {
        "name" : "Nahal Shahini"
      },
      {
        "name" : "Nadav Gat"
      },
      {
        "name" : "Zebin Yun"
      },
      {
        "name" : "Yuzhou Jiang"
      },
      {
        "name" : "Ofir Farchy"
      },
      {
        "name" : "Yaniv Harel"
      },
      {
        "name" : "Vipin Chaudhary"
      },
      {
        "name" : "Mahmood Sharif"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08977v1",
    "title" : "CURE: Privacy-Preserving Split Learning Done Right",
    "summary" : "Training deep neural networks often requires large-scale datasets,\nnecessitating storage and processing on cloud servers due to computational\nconstraints. The procedures must follow strict privacy regulations in domains\nlike healthcare. Split Learning (SL), a framework that divides model layers\nbetween client(s) and server(s), is widely adopted for distributed model\ntraining. While Split Learning reduces privacy risks by limiting server access\nto the full parameter set, previous research has identified that intermediate\noutputs exchanged between server and client can compromise client's data\nprivacy. Homomorphic encryption (HE)-based solutions exist for this scenario\nbut often impose prohibitive computational burdens.\n  To address these challenges, we propose CURE, a novel system based on HE,\nthat encrypts only the server side of the model and optionally the data. CURE\nenables secure SL while substantially improving communication and\nparallelization through advanced packing techniques. We propose two packing\nschemes that consume one HE level for one-layer networks and generalize our\nsolutions to n-layer neural networks. We demonstrate that CURE can achieve\nsimilar accuracy to plaintext SL while being 16x more efficient in terms of the\nruntime compared to the state-of-the-art privacy-preserving alternatives.",
    "updated" : "2024-07-12T04:10:19Z",
    "published" : "2024-07-12T04:10:19Z",
    "authors" : [
      {
        "name" : "Halil Ibrahim Kanpak"
      },
      {
        "name" : "Aqsa Shabbir"
      },
      {
        "name" : "Esra GenÃ§"
      },
      {
        "name" : "Alptekin KÃ¼pÃ§Ã¼"
      },
      {
        "name" : "Sinem Sav"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08954v1",
    "title" : "PriRoAgg: Achieving Robust Model Aggregation with Minimum Privacy\n  Leakage for Federated Learning",
    "summary" : "Federated learning (FL) has recently gained significant momentum due to its\npotential to leverage large-scale distributed user data while preserving user\nprivacy. However, the typical paradigm of FL faces challenges of both privacy\nand robustness: the transmitted model updates can potentially leak sensitive\nuser information, and the lack of central control of the local training process\nleaves the global model susceptible to malicious manipulations on model\nupdates. Current solutions attempting to address both problems under the\none-server FL setting fall short in the following aspects: 1) designed for\nsimple validity checks that are insufficient against advanced attacks (e.g.,\nchecking norm of individual update); and 2) partial privacy leakage for more\ncomplicated robust aggregation algorithms (e.g., distances between model\nupdates are leaked for multi-Krum). In this work, we formalize a novel security\nnotion of aggregated privacy that characterizes the minimum amount of user\ninformation, in the form of some aggregated statistics of users' updates, that\nis necessary to be revealed to accomplish more advanced robust aggregation. We\ndevelop a general framework PriRoAgg, utilizing Lagrange coded computing and\ndistributed zero-knowledge proof, to execute a wide range of robust aggregation\nalgorithms while satisfying aggregated privacy. As concrete instantiations of\nPriRoAgg, we construct two secure and robust protocols based on\nstate-of-the-art robust algorithms, for which we provide full theoretical\nanalyses on security and complexity. Extensive experiments are conducted for\nthese protocols, demonstrating their robustness against various model integrity\nattacks, and their efficiency advantages over baselines.",
    "updated" : "2024-07-12T03:18:08Z",
    "published" : "2024-07-12T03:18:08Z",
    "authors" : [
      {
        "name" : "Sizai Hou"
      },
      {
        "name" : "Songze Li"
      },
      {
        "name" : "Tayyebeh Jahani-Nezhad"
      },
      {
        "name" : "Giuseppe Caire"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08529v2",
    "title" : "Enhancing Privacy of Spatiotemporal Federated Learning against Gradient\n  Inversion Attacks",
    "summary" : "Spatiotemporal federated learning has recently raised intensive studies due\nto its ability to train valuable models with only shared gradients in various\nlocation-based services. On the other hand, recent studies have shown that\nshared gradients may be subject to gradient inversion attacks (GIA) on images\nor texts. However, so far there has not been any systematic study of the\ngradient inversion attacks in spatiotemporal federated learning. In this paper,\nwe explore the gradient attack problem in spatiotemporal federated learning\nfrom attack and defense perspectives. To understand privacy risks in\nspatiotemporal federated learning, we first propose Spatiotemporal Gradient\nInversion Attack (ST-GIA), a gradient attack algorithm tailored to\nspatiotemporal data that successfully reconstructs the original location from\ngradients. Furthermore, we design an adaptive defense strategy to mitigate\ngradient inversion attacks in spatiotemporal federated learning. By dynamically\nadjusting the perturbation levels, we can offer tailored protection for varying\nrounds of training data, thereby achieving a better trade-off between privacy\nand utility than current state-of-the-art methods. Through intensive\nexperimental analysis on three real-world datasets, we reveal that the proposed\ndefense strategy can well preserve the utility of spatiotemporal federated\nlearning with effective security protection.",
    "updated" : "2024-07-12T01:37:44Z",
    "published" : "2024-07-11T14:17:02Z",
    "authors" : [
      {
        "name" : "Lele Zheng"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Renhe Jiang"
      },
      {
        "name" : "Kenjiro Taura"
      },
      {
        "name" : "Yulong Shen"
      },
      {
        "name" : "Sheng Li"
      },
      {
        "name" : "Masatoshi Yoshikawa"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.10094v1",
    "title" : "Work-From-Home and Privacy: What Do Workers Face and What are They Doing\n  About it?",
    "summary" : "The COVID-19 pandemic has reshaped the way people work, normalizing the\npractice of working from home (WFH). However, WFH can cause a blurring of\npersonal and professional boundaries, surfacing new privacy issues, especially\nwhen workers take work meetings from their homes. As WFH arrangements are now\nstandard practice in many organizations, addressing the associated privacy\nconcerns should be a key part of creating healthy work environments for\nworkers. To this end, we conducted a scenario-based survey with 214 US-based\nworkers who currently work from home regularly. Our results suggest that\nprivacy invasions are commonly experienced while working from home and cause\ndiscomfort to many workers. However, only a minority said that the discomfort\nescalated to cause harm to them or others, and the harm was almost always\npsychological. While scenarios that restrict worker autonomy (prohibit turning\noff camera or microphone) are the least experienced scenarios, they are\nassociated with the highest reported discomfort. In addition, participants\nreported measures that violated or would violate their employer's\nautonomy-restricting rules to protect their privacy. We also find that\nconference tool settings that can prevent privacy invasions are not widely used\ncompared to manual privacy-protective measures. Our findings provide better\nunderstanding of the privacy challenges landscape that WFH workers face and how\nthey address them. Furthermore, our discussion raised open questions that can\ninspire future work.",
    "updated" : "2024-07-14T06:15:28Z",
    "published" : "2024-07-14T06:15:28Z",
    "authors" : [
      {
        "name" : "Eman Alashwali"
      },
      {
        "name" : "Joanne Peca"
      },
      {
        "name" : "Mandy Lanyon"
      },
      {
        "name" : "Lorrie Cranor"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.10058v1",
    "title" : "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs",
    "summary" : "Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities.",
    "updated" : "2024-07-14T03:05:53Z",
    "published" : "2024-07-14T03:05:53Z",
    "authors" : [
      {
        "name" : "Zhenhua Liu"
      },
      {
        "name" : "Tong Zhu"
      },
      {
        "name" : "Chuanyuan Tan"
      },
      {
        "name" : "Wenliang Chen"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09809v1",
    "title" : "Preserving the Privacy of Reward Functions in MDPs through Deception",
    "summary" : "Preserving the privacy of preferences (or rewards) of a sequential\ndecision-making agent when decisions are observable is crucial in many physical\nand cybersecurity domains. For instance, in wildlife monitoring, agents must\nallocate patrolling resources without revealing animal locations to poachers.\nThis paper addresses privacy preservation in planning over a sequence of\nactions in MDPs, where the reward function represents the preference structure\nto be protected. Observers can use Inverse RL (IRL) to learn these preferences,\nmaking this a challenging task.\n  Current research on differential privacy in reward functions fails to ensure\nguarantee on the minimum expected reward and offers theoretical guarantees that\nare inadequate against IRL-based observers. To bridge this gap, we propose a\nnovel approach rooted in the theory of deception. Deception includes two\nmodels: dissimulation (hiding the truth) and simulation (showing the wrong).\nOur first contribution theoretically demonstrates significant privacy leaks in\nexisting dissimulation-based methods. Our second contribution is a novel\nRL-based planning algorithm that uses simulation to effectively address these\nprivacy concerns while ensuring a guarantee on the expected reward. Experiments\non multiple benchmark problems show that our approach outperforms previous\nmethods in preserving reward function privacy.",
    "updated" : "2024-07-13T09:03:22Z",
    "published" : "2024-07-13T09:03:22Z",
    "authors" : [
      {
        "name" : "Shashank Reddy Chirra"
      },
      {
        "name" : "Pradeep Varakantham"
      },
      {
        "name" : "Praveen Paruchuri"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09771v1",
    "title" : "Protecting Data Buyer Privacy in Data Markets",
    "summary" : "Data markets serve as crucial platforms facilitating data discovery,\nexchange, sharing, and integration among data users and providers. However, the\nparamount concern of privacy has predominantly centered on protecting privacy\nof data owners and third parties, neglecting the challenges associated with\nprotecting the privacy of data buyers. In this article, we address this gap by\nmodeling the intricacies of data buyer privacy protection and investigating the\ndelicate balance between privacy and purchase cost. Through comprehensive\nexperimentation, our results yield valuable insights, shedding light on the\nefficacy and efficiency of our proposed approaches.",
    "updated" : "2024-07-13T04:45:06Z",
    "published" : "2024-07-13T04:45:06Z",
    "authors" : [
      {
        "name" : "Minxing Zhang"
      },
      {
        "name" : "Jian Pei"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09004v1",
    "title" : "Privacy-Preserving Collaborative Genomic Research: A Real-Life\n  Deployment and Vision",
    "summary" : "The data revolution holds significant promise for the health sector. Vast\namounts of data collected from individuals will be transformed into knowledge,\nAI models, predictive systems, and best practices. One area of health that\nstands to benefit greatly is the genomic domain. Progress in AI, machine\nlearning, and data science has opened new opportunities for genomic research,\npromising breakthroughs in personalized medicine. However, increasing awareness\nof privacy and cybersecurity necessitates robust solutions to protect sensitive\ndata in collaborative research. This paper presents a practical deployment of a\nprivacy-preserving framework for genomic research, developed in collaboration\nwith Lynx$.$MD, a platform for secure health data collaboration. The framework\naddresses critical cybersecurity and privacy challenges, enabling the\nprivacy-preserving sharing and analysis of genomic data while mitigating risks\nassociated with data breaches. By integrating advanced privacy-preserving\nalgorithms, the solution ensures the protection of individual privacy without\ncompromising data utility. A unique feature of the system is its ability to\nbalance trade-offs between data sharing and privacy, providing stakeholders\ntools to quantify privacy risks and make informed decisions. Implementing the\nframework within Lynx$.$MD involves encoding genomic data into binary formats\nand applying noise through controlled perturbation techniques. This approach\npreserves essential statistical properties of the data, facilitating effective\nresearch and analysis. Moreover, the system incorporates real-time data\nmonitoring and advanced visualization tools, enhancing user experience and\ndecision-making. The paper highlights the need for tailored privacy attacks and\ndefenses specific to genomic data. Addressing these challenges fosters\ncollaboration in genomic research, advancing personalized medicine and public\nhealth.",
    "updated" : "2024-07-12T05:43:13Z",
    "published" : "2024-07-12T05:43:13Z",
    "authors" : [
      {
        "name" : "Zahra Rahmani"
      },
      {
        "name" : "Nahal Shahini"
      },
      {
        "name" : "Nadav Gat"
      },
      {
        "name" : "Zebin Yun"
      },
      {
        "name" : "Yuzhou Jiang"
      },
      {
        "name" : "Ofir Farchy"
      },
      {
        "name" : "Yaniv Harel"
      },
      {
        "name" : "Vipin Chaudhary"
      },
      {
        "name" : "Mahmood Sharif"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08529v3",
    "title" : "Enhancing Privacy of Spatiotemporal Federated Learning against Gradient\n  Inversion Attacks",
    "summary" : "Spatiotemporal federated learning has recently raised intensive studies due\nto its ability to train valuable models with only shared gradients in various\nlocation-based services. On the other hand, recent studies have shown that\nshared gradients may be subject to gradient inversion attacks (GIA) on images\nor texts. However, so far there has not been any systematic study of the\ngradient inversion attacks in spatiotemporal federated learning. In this paper,\nwe explore the gradient attack problem in spatiotemporal federated learning\nfrom attack and defense perspectives. To understand privacy risks in\nspatiotemporal federated learning, we first propose Spatiotemporal Gradient\nInversion Attack (ST-GIA), a gradient attack algorithm tailored to\nspatiotemporal data that successfully reconstructs the original location from\ngradients. Furthermore, we design an adaptive defense strategy to mitigate\ngradient inversion attacks in spatiotemporal federated learning. By dynamically\nadjusting the perturbation levels, we can offer tailored protection for varying\nrounds of training data, thereby achieving a better trade-off between privacy\nand utility than current state-of-the-art methods. Through intensive\nexperimental analysis on three real-world datasets, we reveal that the proposed\ndefense strategy can well preserve the utility of spatiotemporal federated\nlearning with effective security protection.",
    "updated" : "2024-07-15T06:42:31Z",
    "published" : "2024-07-11T14:17:02Z",
    "authors" : [
      {
        "name" : "Lele Zheng"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Renhe Jiang"
      },
      {
        "name" : "Kenjiro Taura"
      },
      {
        "name" : "Yulong Shen"
      },
      {
        "name" : "Sheng Li"
      },
      {
        "name" : "Masatoshi Yoshikawa"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.11274v1",
    "title" : "Empirical Mean and Frequency Estimation Under Heterogeneous Privacy: A\n  Worst-Case Analysis",
    "summary" : "Differential Privacy (DP) is the current gold-standard for measuring privacy.\nEstimation problems under DP constraints appearing in the literature have\nlargely focused on providing equal privacy to all users. We consider the\nproblems of empirical mean estimation for univariate data and frequency\nestimation for categorical data, two pillars of data analysis in the industry,\nsubject to heterogeneous privacy constraints. Each user, contributing a sample\nto the dataset, is allowed to have a different privacy demand. The dataset\nitself is assumed to be worst-case and we study both the problems in two\ndifferent formulations -- the correlated and the uncorrelated setting. In the\nformer setting, the privacy demand and the user data can be arbitrarily\ncorrelated while in the latter setting, there is no correlation between the\ndataset and the privacy demand. We prove some optimality results, under both\nPAC error and mean-squared error, for our proposed algorithms and demonstrate\nsuperior performance over other baseline techniques experimentally.",
    "updated" : "2024-07-15T22:46:02Z",
    "published" : "2024-07-15T22:46:02Z",
    "authors" : [
      {
        "name" : "Syomantak Chaudhuri"
      },
      {
        "name" : "Thomas A. Courtade"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.12669v1",
    "title" : "Enhancing the Utility of Privacy-Preserving Cancer Classification using\n  Synthetic Data",
    "summary" : "Deep learning holds immense promise for aiding radiologists in breast cancer\ndetection. However, achieving optimal model performance is hampered by\nlimitations in availability and sharing of data commonly associated to patient\nprivacy concerns. Such concerns are further exacerbated, as traditional deep\nlearning models can inadvertently leak sensitive training information. This\nwork addresses these challenges exploring and quantifying the utility of\nprivacy-preserving deep learning techniques, concretely, (i) differentially\nprivate stochastic gradient descent (DP-SGD) and (ii) fully synthetic training\ndata generated by our proposed malignancy-conditioned generative adversarial\nnetwork. We assess these methods via downstream malignancy classification of\nmammography masses using a transformer model. Our experimental results depict\nthat synthetic data augmentation can improve privacy-utility tradeoffs in\ndifferentially private model training. Further, model pretraining on synthetic\ndata achieves remarkable performance, which can be further increased with\nDP-SGD fine-tuning across all privacy guarantees. With this first in-depth\nexploration of privacy-preserving deep learning in breast imaging, we address\ncurrent and emerging clinical privacy requirements and pave the way towards the\nadoption of private high-utility deep diagnostic models. Our reproducible\ncodebase is publicly available at https://github.com/RichardObi/mammo_dp.",
    "updated" : "2024-07-17T15:52:45Z",
    "published" : "2024-07-17T15:52:45Z",
    "authors" : [
      {
        "name" : "Richard Osuala"
      },
      {
        "name" : "Daniel M. Lang"
      },
      {
        "name" : "Anneliese Riess"
      },
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Zuzanna Szafranowska"
      },
      {
        "name" : "Grzegorz Skorupko"
      },
      {
        "name" : "Oliver Diaz"
      },
      {
        "name" : "Julia A. Schnabel"
      },
      {
        "name" : "Karim Lekadir"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.12589v1",
    "title" : "Privacy-Preserving Adaptive Re-Identification without Image Transfer",
    "summary" : "Re-Identification systems (Re-ID) are crucial for public safety but face the\nchallenge of having to adapt to environments that differ from their training\ndistribution. Furthermore, rigorous privacy protocols in public places are\nbeing enforced as apprehensions regarding individual freedom rise, adding\nlayers of complexity to the deployment of accurate Re-ID systems in new\nenvironments. For example, in the European Union, the principles of ``Data\nMinimization'' and ``Purpose Limitation'' restrict the retention and processing\nof images to what is strictly necessary. These regulations pose a challenge to\nthe conventional Re-ID training schemes that rely on centralizing data on\nservers. In this work, we present a novel setting for privacy-preserving\nDistributed Unsupervised Domain Adaptation for person Re-ID (DUDA-Rid) to\naddress the problem of domain shift without requiring any image transfer\noutside the camera devices. To address this setting, we introduce Fed-Protoid,\na novel solution that adapts person Re-ID models directly within the edge\ndevices. Our proposed solution employs prototypes derived from the source\ndomain to align feature statistics within edge devices. Those source prototypes\nare distributed across the edge devices to minimize a distributed Maximum Mean\nDiscrepancy (MMD) loss tailored for the DUDA-Rid setting. Our experiments\nprovide compelling evidence that Fed-Protoid outperforms all evaluated methods\nin terms of both accuracy and communication efficiency, all while maintaining\ndata privacy.",
    "updated" : "2024-07-17T14:12:44Z",
    "published" : "2024-07-17T14:12:44Z",
    "authors" : [
      {
        "name" : "Hamza Rami"
      },
      {
        "name" : "Jhony H. Giraldo"
      },
      {
        "name" : "Nicolas Winckler"
      },
      {
        "name" : "StÃ©phane LathuiliÃ¨re"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13725v1",
    "title" : "Scalable Optimization for Locally Relevant Geo-Location Privacy",
    "summary" : "Geo-obfuscation functions as a location privacy protection mechanism (LPPM),\nenabling mobile users to share obfuscated locations with servers instead of\ntheir exact locations. This technique protects users' location privacy during\nserver-side data breaches since the obfuscation process is irreversible. To\nminimize the utility loss caused by data obfuscation, linear programming (LP)\nis widely used. However, LP can face a polynomial explosion in decision\nvariables, making it impractical for large-scale geo-obfuscation applications.\nIn this paper, we propose a new LPPM called Locally Relevant Geo-obfuscation\n(LR-Geo) to optimize geo-obfuscation using LP more efficiently. This is\naccomplished by restricting the geo-obfuscation calculations for each user to\nlocally relevant (LR) locations near the user's actual location. To prevent LR\nlocations from inadvertently revealing a user's true whereabouts, users compute\nthe LP coefficients locally and upload only these coefficients to the server,\nrather than the LR locations themselves. The server then solves the LP problem\nusing the provided coefficients. Additionally, we enhance the LP framework with\nan exponential obfuscation mechanism to ensure that the obfuscation\ndistribution is indistinguishable across multiple users. By leveraging the\nconstraint structure of the LP formulation, we apply Benders' decomposition to\nfurther boost computational efficiency. Our theoretical analysis confirms that,\neven though geo-obfuscation is calculated independently for each user, it still\nadheres to geo-indistinguishability constraints across multiple users with high\nprobability. Finally, experimental results using a real-world dataset\ndemonstrate that LR-Geo outperforms existing geo-obfuscation methods in terms\nof computational time, data utility, and privacy protection.",
    "updated" : "2024-07-18T17:25:08Z",
    "published" : "2024-07-18T17:25:08Z",
    "authors" : [
      {
        "name" : "Chenxi Qiu"
      },
      {
        "name" : "Ruiyao Liu"
      },
      {
        "name" : "Primal Pappachan"
      },
      {
        "name" : "Anna Squicciarini"
      },
      {
        "name" : "Xinpeng Xie"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13621v1",
    "title" : "Differential Privacy Mechanisms in Neural Tangent Kernel Regression",
    "summary" : "Training data privacy is a fundamental problem in modern Artificial\nIntelligence (AI) applications, such as face recognition, recommendation\nsystems, language generation, and many others, as it may contain sensitive user\ninformation related to legal issues. To fundamentally understand how privacy\nmechanisms work in AI applications, we study differential privacy (DP) in the\nNeural Tangent Kernel (NTK) regression setting, where DP is one of the most\npowerful tools for measuring privacy under statistical learning, and NTK is one\nof the most popular analysis frameworks for studying the learning mechanisms of\ndeep neural networks. In our work, we can show provable guarantees for both\ndifferential privacy and test accuracy of our NTK regression. Furthermore, we\nconduct experiments on the basic image classification dataset CIFAR10 to\ndemonstrate that NTK regression can preserve good accuracy under a modest\nprivacy budget, supporting the validity of our analysis. To our knowledge, this\nis the first work to provide a DP guarantee for NTK regression.",
    "updated" : "2024-07-18T15:57:55Z",
    "published" : "2024-07-18T15:57:55Z",
    "authors" : [
      {
        "name" : "Jiuxiang Gu"
      },
      {
        "name" : "Yingyu Liang"
      },
      {
        "name" : "Zhizhou Sha"
      },
      {
        "name" : "Zhenmei Shi"
      },
      {
        "name" : "Zhao Song"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13532v1",
    "title" : "PriPL-Tree: Accurate Range Query for Arbitrary Distribution under Local\n  Differential Privacy",
    "summary" : "Answering range queries in the context of Local Differential Privacy (LDP) is\na widely studied problem in Online Analytical Processing (OLAP). Existing LDP\nsolutions all assume a uniform data distribution within each domain partition,\nwhich may not align with real-world scenarios where data distribution is\nvaried, resulting in inaccurate estimates. To address this problem, we\nintroduce PriPL-Tree, a novel data structure that combines hierarchical tree\nstructures with piecewise linear (PL) functions to answer range queries for\narbitrary distributions. PriPL-Tree precisely models the underlying data\ndistribution with a few line segments, leading to more accurate results for\nrange queries. Furthermore, we extend it to multi-dimensional cases with novel\ndata-aware adaptive grids. These grids leverage the insights from marginal\ndistributions obtained through PriPL-Trees to partition the grids adaptively,\nadapting the density of underlying distributions. Our extensive experiments on\nboth real and synthetic datasets demonstrate the effectiveness and superiority\nof PriPL-Tree over state-of-the-art solutions in answering range queries across\narbitrary data distributions.",
    "updated" : "2024-07-18T14:05:35Z",
    "published" : "2024-07-18T14:05:35Z",
    "authors" : [
      {
        "name" : "Leixia Wang"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Haibo Hu"
      },
      {
        "name" : "Xiaofeng Meng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13516v1",
    "title" : "Optimal Mechanisms for Quantum Local Differential Privacy",
    "summary" : "In recent years, centralized differential privacy has been successfully\nextended to quantum computing and information processing to safeguard privacy\nand prevent leaks in neighboring relationships of quantum states. This paper\nintroduces a framework known as quantum local differential privacy (QLDP) and\ninitializes the algorithmic study of QLDP. QLDP utilizes a parameter $\\epsilon$\nto manage privacy leaks and ensure the privacy of individual quantum states.\nThe optimization of the QLDP value $\\epsilon$, denoted as $\\epsilon^*$, for any\nquantum mechanism is addressed as an optimization problem. The introduction of\nquantum noise is shown to provide privacy protections similar to classical\nscenarios, with quantum depolarizing noise identified as the optimal unital\nprivatization mechanism within the QLDP framework. Unital mechanisms represent\na diverse set of quantum mechanisms that encompass frequently employed quantum\nnoise types. Quantum depolarizing noise optimizes both fidelity and trace\ndistance utilities, which are crucial metrics in the field of quantum\ncomputation and information, and can be viewed as a quantum counterpart to\nclassical randomized response methods. Additionally, a composition theorem is\npresented for the application of QLDP framework in distributed (spatially\nseparated) quantum systems, ensuring the validity (additivity of QLDP value)\nirrespective of the states' independence, classical correlation, or\nentanglement (quantum correlation). The study further explores the trade-off\nbetween utility and privacy across different quantum noise mechanisms,\nincluding unital and non-unital quantum noise mechanisms, through both\nanalytical and numerically experimental approaches. Meanwhile, this highlights\nthe optimization of quantum depolarizing noise in QLDP framework.",
    "updated" : "2024-07-18T13:46:16Z",
    "published" : "2024-07-18T13:46:16Z",
    "authors" : [
      {
        "name" : "Ji Guan"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13153v1",
    "title" : "Preset-Voice Matching for Privacy Regulated Speech-to-Speech Translation\n  Systems",
    "summary" : "In recent years, there has been increased demand for speech-to-speech\ntranslation (S2ST) systems in industry settings. Although successfully\ncommercialized, cloning-based S2ST systems expose their distributors to\nliabilities when misused by individuals and can infringe on personality rights\nwhen exploited by media organizations. This work proposes a regulated S2ST\nframework called Preset-Voice Matching (PVM). PVM removes cross-lingual voice\ncloning in S2ST by first matching the input voice to a similar prior consenting\nspeaker voice in the target-language. With this separation, PVM avoids cloning\nthe input speaker, ensuring PVM systems comply with regulations and reduce risk\nof misuse. Our results demonstrate PVM can significantly improve S2ST system\nrun-time in multi-speaker settings and the naturalness of S2ST synthesized\nspeech. To our knowledge, PVM is the first explicitly regulated S2ST framework\nleveraging similarly-matched preset-voices for dynamic S2ST tasks.",
    "updated" : "2024-07-18T04:42:01Z",
    "published" : "2024-07-18T04:42:01Z",
    "authors" : [
      {
        "name" : "Daniel Platnick"
      },
      {
        "name" : "Bishoy Abdelnour"
      },
      {
        "name" : "Eamon Earl"
      },
      {
        "name" : "Rahul Kumar"
      },
      {
        "name" : "Zahra Rezaei"
      },
      {
        "name" : "Thomas Tsangaris"
      },
      {
        "name" : "Faraj Lagum"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13975v1",
    "title" : "Personalized Privacy Protection Mask Against Unauthorized Facial\n  Recognition",
    "summary" : "Face recognition (FR) can be abused for privacy intrusion. Governments,\nprivate companies, or even individual attackers can collect facial images by\nweb scraping to build an FR system identifying human faces without their\nconsent. This paper introduces Chameleon, which learns to generate a\nuser-centric personalized privacy protection mask, coined as P3-Mask, to\nprotect facial images against unauthorized FR with three salient features.\nFirst, we use a cross-image optimization to generate one P3-Mask for each user\ninstead of tailoring facial perturbation for each facial image of a user. It\nenables efficient and instant protection even for users with limited computing\nresources. Second, we incorporate a perceptibility optimization to preserve the\nvisual quality of the protected facial images. Third, we strengthen the\nrobustness of P3-Mask against unknown FR models by integrating focal\ndiversity-optimized ensemble learning into the mask generation process.\nExtensive experiments on two benchmark datasets show that Chameleon outperforms\nthree state-of-the-art methods with instant protection and minimal degradation\nof image quality. Furthermore, Chameleon enables cost-effective FR\nauthorization using the P3-Mask as a personalized de-obfuscation key, and it\ndemonstrates high resilience against adaptive adversaries.",
    "updated" : "2024-07-19T01:59:00Z",
    "published" : "2024-07-19T01:59:00Z",
    "authors" : [
      {
        "name" : "Ka-Ho Chow"
      },
      {
        "name" : "Sihao Hu"
      },
      {
        "name" : "Tiansheng Huang"
      },
      {
        "name" : "Ling Liu"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13881v1",
    "title" : "Privacy-preserving gradient-based fair federated learning",
    "summary" : "Federated learning (FL) schemes allow multiple participants to\ncollaboratively train neural networks without the need to directly share the\nunderlying data.However, in early schemes, all participants eventually obtain\nthe same model. Moreover, the aggregation is typically carried out by a third\nparty, who obtains combined gradients or weights, which may reveal the model.\nThese downsides underscore the demand for fair and privacy-preserving FL\nschemes. Here, collaborative fairness asks for individual model quality\ndepending on the individual data contribution. Privacy is demanded with respect\nto any kind of data outsourced to the third party. Now, there already exist\nsome approaches aiming for either fair or privacy-preserving FL and a few works\neven address both features. In our paper, we build upon these seminal works and\npresent a novel, fair and privacy-preserving FL scheme. Our approach, which\nmainly relies on homomorphic encryption, stands out for exclusively using local\ngradients. This increases the usability in comparison to state-of-the-art\napproaches and thereby opens the door to applications in control.",
    "updated" : "2024-07-18T19:56:39Z",
    "published" : "2024-07-18T19:56:39Z",
    "authors" : [
      {
        "name" : "Janis Adamek"
      },
      {
        "name" : "Moritz Schulze Darup"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.15407v1",
    "title" : "A Solution toward Transparent and Practical AI Regulation: Privacy\n  Nutrition Labels for Open-source Generative AI-based Applications",
    "summary" : "The rapid development and widespread adoption of Generative Artificial\nIntelligence-based (GAI) applications have greatly enriched our daily lives,\nbenefiting people by enhancing creativity, personalizing experiences, improving\naccessibility, and fostering innovation and efficiency across various domains.\nHowever, along with the development of GAI applications, concerns have been\nraised about transparency in their privacy practices. Traditional privacy\npolicies often fail to effectively communicate essential privacy information\ndue to their complexity and length, and open-source community developers often\nneglect privacy practices even more. Only 12.2% of examined open-source GAI\napps provide a privacy policy. To address this, we propose a regulation-driven\nGAI Privacy Label and introduce Repo2Label, a novel framework for automatically\ngenerating these labels based on code repositories. Our user study indicates a\ncommon endorsement of the proposed GAI privacy label format. Additionally,\nRepo2Label achieves a precision of 0.81, recall of 0.88, and F1-score of 0.84\nbased on the benchmark dataset, significantly outperforming the developer\nself-declared privacy notices. We also discuss the common regulatory\n(in)compliance of open-source GAI apps, comparison with other privacy notices,\nand broader impacts to different stakeholders. Our findings suggest that\nRepo2Label could serve as a significant tool for bolstering the privacy\ntransparency of GAI apps and make them more practical and responsible.",
    "updated" : "2024-07-22T06:24:13Z",
    "published" : "2024-07-22T06:24:13Z",
    "authors" : [
      {
        "name" : "Meixue Si"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Dianshu Liao"
      },
      {
        "name" : "Xiaoyu Sun"
      },
      {
        "name" : "Zhen Tao"
      },
      {
        "name" : "Wenchang Shi"
      },
      {
        "name" : "Zhenchang Xing"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.15224v1",
    "title" : "PUFFLE: Balancing Privacy, Utility, and Fairness in Federated Learning",
    "summary" : "Training and deploying Machine Learning models that simultaneously adhere to\nprinciples of fairness and privacy while ensuring good utility poses a\nsignificant challenge. The interplay between these three factors of\ntrustworthiness is frequently underestimated and remains insufficiently\nexplored. Consequently, many efforts focus on ensuring only two of these\nfactors, neglecting one in the process. The decentralization of the datasets\nand the variations in distributions among the clients exacerbate the complexity\nof achieving this ethical trade-off in the context of Federated Learning (FL).\nFor the first time in FL literature, we address these three factors of\ntrustworthiness. We introduce PUFFLE, a high-level parameterised approach that\ncan help in the exploration of the balance between utility, privacy, and\nfairness in FL scenarios. We prove that PUFFLE can be effective across diverse\ndatasets, models, and data distributions, reducing the model unfairness up to\n75%, with a maximum reduction in the utility of 17% in the worst-case scenario,\nwhile maintaining strict privacy guarantees during the FL training.",
    "updated" : "2024-07-21T17:22:18Z",
    "published" : "2024-07-21T17:22:18Z",
    "authors" : [
      {
        "name" : "Luca Corbucci"
      },
      {
        "name" : "Mikko A Heikkila"
      },
      {
        "name" : "David Solans Noguero"
      },
      {
        "name" : "Anna Monreale"
      },
      {
        "name" : "Nicolas Kourtellis"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.15220v1",
    "title" : "Privacy-Preserving Multi-Center Differential Protein Abundance Analysis\n  with FedProt",
    "summary" : "Quantitative mass spectrometry has revolutionized proteomics by enabling\nsimultaneous quantification of thousands of proteins. Pooling patient-derived\ndata from multiple institutions enhances statistical power but raises\nsignificant privacy concerns. Here we introduce FedProt, the first\nprivacy-preserving tool for collaborative differential protein abundance\nanalysis of distributed data, which utilizes federated learning and additive\nsecret sharing. In the absence of a multicenter patient-derived dataset for\nevaluation, we created two, one at five centers from LFQ E.coli experiments and\none at three centers from TMT human serum. Evaluations using these datasets\nconfirm that FedProt achieves accuracy equivalent to DEqMS applied to pooled\ndata, with completely negligible absolute differences no greater than $\\text{$4\n\\times 10^{-12}$}$. In contrast, -log10(p-values) computed by the most accurate\nmeta-analysis methods diverged from the centralized analysis results by up to\n25-27. FedProt is available as a web tool with detailed documentation as a\nFeatureCloud App.",
    "updated" : "2024-07-21T17:09:20Z",
    "published" : "2024-07-21T17:09:20Z",
    "authors" : [
      {
        "name" : "Yuliya Burankova"
      },
      {
        "name" : "Miriam Abele"
      },
      {
        "name" : "Mohammad Bakhtiari"
      },
      {
        "name" : "Christine von TÃ¶rne"
      },
      {
        "name" : "Teresa Barth"
      },
      {
        "name" : "Lisa Schweizer"
      },
      {
        "name" : "Pieter Giesbertz"
      },
      {
        "name" : "Johannes R. Schmidt"
      },
      {
        "name" : "Stefan Kalkhof"
      },
      {
        "name" : "Janina MÃ¼ller-Deile"
      },
      {
        "name" : "Peter A van Veelen"
      },
      {
        "name" : "Yassene Mohammed"
      },
      {
        "name" : "Elke Hammer"
      },
      {
        "name" : "Lis Arend"
      },
      {
        "name" : "Klaudia Adamowicz"
      },
      {
        "name" : "Tanja Laske"
      },
      {
        "name" : "Anne Hartebrodt"
      },
      {
        "name" : "Tobias Frisch"
      },
      {
        "name" : "Chen Meng"
      },
      {
        "name" : "Julian Matschinske"
      },
      {
        "name" : "Julian SpÃ¤th"
      },
      {
        "name" : "Richard RÃ¶ttger"
      },
      {
        "name" : "Veit SchwÃ¤mmle"
      },
      {
        "name" : "Stefanie M. Hauck"
      },
      {
        "name" : "Stefan Lichtenthaler"
      },
      {
        "name" : "Axel Imhof"
      },
      {
        "name" : "Matthias Mann"
      },
      {
        "name" : "Christina Ludwig"
      },
      {
        "name" : "Bernhard Kuster"
      },
      {
        "name" : "Jan Baumbach"
      },
      {
        "name" : "Olga Zolotareva"
      }
    ],
    "categories" : [
      "q-bio.QM",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.14938v1",
    "title" : "From Ad Identifiers to Global Privacy Control: The Status Quo and Future\n  of Opting Out of Ad Tracking on Android",
    "summary" : "Apps and their integrated third party libraries often collect a variety of\ndata from people to show them personalized ads. This practice is often\nprivacy-invasive. Since 2013, Google has therefore allowed users to limit ad\ntracking on Android via system settings. Further, under the 2018 California\nConsumer Privacy Act (CCPA), apps must honor opt-outs from ad tracking under\nthe Global Privacy Control (GPC). The efficacy of these two methods to limit ad\ntracking has not been studied in prior work. Our legal and technical analysis\ndetails how the GPC applies to mobile apps and how it could be integrated\ndirectly into Android, thereby developing a reference design for GPC on\nAndroid. Our empirical analysis of 1,896 top-ranked Android apps shows that\nboth the Android system-level opt-out and the GPC signal rarely restrict ad\ntracking. In our view, deleting the AdID and opting out under the CCPA has the\nsame meaning. Thus, the current AdID setting and APIs should be evolved towards\nGPC and integrated into Android's Privacy Sandbox.",
    "updated" : "2024-07-20T17:06:23Z",
    "published" : "2024-07-20T17:06:23Z",
    "authors" : [
      {
        "name" : "Sebastian Zimmeck"
      },
      {
        "name" : "Nishant Aggarwal"
      },
      {
        "name" : "Zachary Liu"
      },
      {
        "name" : "Konrad Kollnig"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.14719v1",
    "title" : "Universal Medical Imaging Model for Domain Generalization with Data\n  Privacy",
    "summary" : "Achieving domain generalization in medical imaging poses a significant\nchallenge, primarily due to the limited availability of publicly labeled\ndatasets in this domain. This limitation arises from concerns related to data\nprivacy and the necessity for medical expertise to accurately label the data.\nIn this paper, we propose a federated learning approach to transfer knowledge\nfrom multiple local models to a global model, eliminating the need for direct\naccess to the local datasets used to train each model. The primary objective is\nto train a global model capable of performing a wide variety of medical imaging\ntasks. This is done while ensuring the confidentiality of the private datasets\nutilized during the training of these models. To validate the effectiveness of\nour approach, extensive experiments were conducted on eight datasets, each\ncorresponding to a different medical imaging application. The client's data\ndistribution in our experiments varies significantly as they originate from\ndiverse domains. Despite this variation, we demonstrate a statistically\nsignificant improvement over a state-of-the-art baseline utilizing masked image\nmodeling over a diverse pre-training dataset that spans different body parts\nand scanning types. This improvement is achieved by curating information\nlearned from clients without accessing any labeled dataset on the server.",
    "updated" : "2024-07-20T01:24:15Z",
    "published" : "2024-07-20T01:24:15Z",
    "authors" : [
      {
        "name" : "Ahmed Radwan"
      },
      {
        "name" : "Islam Osman"
      },
      {
        "name" : "Mohamed S. Shehata"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.14717v1",
    "title" : "Differential Privacy of Cross-Attention with Provable Guarantee",
    "summary" : "Cross-attention has become a fundamental module nowadays in many important\nartificial intelligence applications, e.g., retrieval-augmented generation\n(RAG), system prompt, guided stable diffusion, and many so on. Ensuring\ncross-attention privacy is crucial and urgently needed because its key and\nvalue matrices may contain sensitive information about companies and their\nusers, many of which profit solely from their system prompts or RAG data. In\nthis work, we design a novel differential privacy (DP) data structure to\naddress the privacy security of cross-attention with a theoretical guarantee.\nIn detail, let $n$ be the input token length of system prompt/RAG data, $d$ be\nthe feature dimension, $0 < \\alpha \\le 1$ be the relative error parameter, $R$\nbe the maximum value of the query and key matrices, $R_w$ be the maximum value\nof the value matrix, and $r,s,\\epsilon_s$ be parameters of polynomial kernel\nmethods. Then, our data structure requires $\\widetilde{O}(ndr^2)$ memory\nconsumption with $\\widetilde{O}(nr^2)$ initialization time complexity and\n$\\widetilde{O}(\\alpha^{-1} r^2)$ query time complexity for a single token\nquery. In addition, our data structure can guarantee that the user query is\n$(\\epsilon, \\delta)$-DP with $\\widetilde{O}(n^{-1} \\epsilon^{-1} \\alpha^{-1/2}\nR^{2s} R_w r^2)$ additive error and $n^{-1} (\\alpha + \\epsilon_s)$ relative\nerror between our output and the true answer. Furthermore, our result is robust\nto adaptive queries in which users can intentionally attack the cross-attention\nsystem. To our knowledge, this is the first work to provide DP for\ncross-attention. We believe it can inspire more privacy algorithm design in\nlarge generative models (LGMs).",
    "updated" : "2024-07-20T01:02:27Z",
    "published" : "2024-07-20T01:02:27Z",
    "authors" : [
      {
        "name" : "Jiuxiang Gu"
      },
      {
        "name" : "Yingyu Liang"
      },
      {
        "name" : "Zhenmei Shi"
      },
      {
        "name" : "Zhao Song"
      },
      {
        "name" : "Yufa Zhou"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.14710v1",
    "title" : "Universally Harmonizing Differential Privacy Mechanisms for Federated\n  Learning: Boosting Accuracy and Convergence",
    "summary" : "Differentially private federated learning (DP-FL) is a promising technique\nfor collaborative model training while ensuring provable privacy for clients.\nHowever, optimizing the tradeoff between privacy and accuracy remains a\ncritical challenge. To our best knowledge, we propose the first DP-FL framework\n(namely UDP-FL), which universally harmonizes any randomization mechanism\n(e.g., an optimal one) with the Gaussian Moments Accountant (viz. DP-SGD) to\nsignificantly boost accuracy and convergence. Specifically, UDP-FL demonstrates\nenhanced model performance by mitigating the reliance on Gaussian noise. The\nkey mediator variable in this transformation is the R\\'enyi Differential\nPrivacy notion, which is carefully used to harmonize privacy budgets. We also\npropose an innovative method to theoretically analyze the convergence for DP-FL\n(including our UDP-FL ) based on mode connectivity analysis. Moreover, we\nevaluate our UDP-FL through extensive experiments benchmarked against\nstate-of-the-art (SOTA) methods, demonstrating superior performance on both\nprivacy guarantees and model performance. Notably, UDP-FL exhibits substantial\nresilience against different inference attacks, indicating a significant\nadvance in safeguarding sensitive data in federated learning environments.",
    "updated" : "2024-07-20T00:11:59Z",
    "published" : "2024-07-20T00:11:59Z",
    "authors" : [
      {
        "name" : "Shuya Feng"
      },
      {
        "name" : "Meisam Mohammady"
      },
      {
        "name" : "Hanbin Hong"
      },
      {
        "name" : "Shenao Yan"
      },
      {
        "name" : "Ashish Kundu"
      },
      {
        "name" : "Binghui Wang"
      },
      {
        "name" : "Yuan Hong"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.14641v1",
    "title" : "Differential Privacy with Multiple Selections",
    "summary" : "We consider the setting where a user with sensitive features wishes to obtain\na recommendation from a server in a differentially private fashion. We propose\na ``multi-selection'' architecture where the server can send back multiple\nrecommendations and the user chooses one from these that matches best with\ntheir private features. When the user feature is one-dimensional -- on an\ninfinite line -- and the accuracy measure is defined w.r.t some increasing\nfunction $\\mathfrak{h}(.)$ of the distance on the line, we precisely\ncharacterize the optimal mechanism that satisfies differential privacy. The\nspecification of the optimal mechanism includes both the distribution of the\nnoise that the user adds to its private value, and the algorithm used by the\nserver to determine the set of results to send back as a response and further\nshow that Laplace is an optimal noise distribution. We further show that this\noptimal mechanism results in an error that is inversely proportional to the\nnumber of results returned when the function $\\mathfrak{h}(.)$ is the identity\nfunction.",
    "updated" : "2024-07-19T19:34:51Z",
    "published" : "2024-07-19T19:34:51Z",
    "authors" : [
      {
        "name" : "Ashish Goel"
      },
      {
        "name" : "Zhihao Jiang"
      },
      {
        "name" : "Aleksandra Korolova"
      },
      {
        "name" : "Kamesh Munagala"
      },
      {
        "name" : "Sahasrajit Sarmasarkar"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.16166v1",
    "title" : "Robust Privacy Amidst Innovation with Large Language Models Through a\n  Critical Assessment of the Risks",
    "summary" : "This study examines integrating EHRs and NLP with large language models\n(LLMs) to improve healthcare data management and patient care. It focuses on\nusing advanced models to create secure, HIPAA-compliant synthetic patient notes\nfor biomedical research. The study used de-identified and re-identified MIMIC\nIII datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.\nText generation employed templates and keyword extraction for contextually\nrelevant notes, with one-shot generation for comparison. Privacy assessment\nchecked PHI occurrence, while text utility was tested using an ICD-9 coding\ntask. Text quality was evaluated with ROUGE and cosine similarity metrics to\nmeasure semantic similarity with source notes. Analysis of PHI occurrence and\ntext utility via the ICD-9 coding task showed that the keyword-based method had\nlow risk and good performance. One-shot generation showed the highest PHI\nexposure and PHI co-occurrence, especially in geographic location and date\ncategories. The Normalized One-shot method achieved the highest classification\naccuracy. Privacy analysis revealed a critical balance between data utility and\nprivacy protection, influencing future data use and sharing. Re-identified data\nconsistently outperformed de-identified data. This study demonstrates the\neffectiveness of keyword-based methods in generating privacy-protecting\nsynthetic clinical notes that retain data usability, potentially transforming\nclinical data-sharing practices. The superior performance of re-identified over\nde-identified data suggests a shift towards methods that enhance utility and\nprivacy by using dummy PHIs to perplex privacy attacks.",
    "updated" : "2024-07-23T04:20:14Z",
    "published" : "2024-07-23T04:20:14Z",
    "authors" : [
      {
        "name" : "Yao-Shun Chuang"
      },
      {
        "name" : "Atiquer Rahman Sarkar"
      },
      {
        "name" : "Noman Mohammed"
      },
      {
        "name" : "Xiaoqian Jiang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.16164v1",
    "title" : "Representation Magnitude has a Liability to Privacy Vulnerability",
    "summary" : "The privacy-preserving approaches to machine learning (ML) models have made\nsubstantial progress in recent years. However, it is still opaque in which\ncircumstances and conditions the model becomes privacy-vulnerable, leading to a\nchallenge for ML models to maintain both performance and privacy. In this\npaper, we first explore the disparity between member and non-member data in the\nrepresentation of models under common training frameworks. We identify how the\nrepresentation magnitude disparity correlates with privacy vulnerability and\naddress how this correlation impacts privacy vulnerability. Based on the\nobservations, we propose Saturn Ring Classifier Module (SRCM), a plug-in\nmodel-level solution to mitigate membership privacy leakage. Through a confined\nyet effective representation space, our approach ameliorates models' privacy\nvulnerability while maintaining generalizability. The code of this work can be\nfound here: \\url{https://github.com/JEKimLab/AIES2024_SRCM}",
    "updated" : "2024-07-23T04:13:52Z",
    "published" : "2024-07-23T04:13:52Z",
    "authors" : [
      {
        "name" : "Xingli Fang"
      },
      {
        "name" : "Jung-Eun Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.15957v1",
    "title" : "Escalation of Commitment: A Case Study of the United States Census\n  Bureau Efforts to Implement Differential Privacy for the 2020 Decennial\n  Census",
    "summary" : "In 2017, the United States Census Bureau announced that because of high\ndisclosure risk in the methodology (data swapping) used to produce tabular data\nfor the 2010 census, a different protection mechanism based on differential\nprivacy would be used for the 2020 census. While there have been many studies\nevaluating the result of this change, there has been no rigorous examination of\ndisclosure risk claims resulting from the released 2010 tabular data. In this\nstudy we perform such an evaluation. We show that the procedures used to\nevaluate disclosure risk are unreliable and resulted in inflated disclosure\nrisk. Demonstration data products released using the new procedure were also\nshown to have poor utility. However, since the Census Bureau had already\ncommitted to a different procedure, they had no option except to escalate their\ncommitment. The result of such escalation is that the 2020 tabular data release\noffers neither privacy nor accuracy.",
    "updated" : "2024-07-22T18:13:52Z",
    "published" : "2024-07-22T18:13:52Z",
    "authors" : [
      {
        "name" : "Krish Muralidhar"
      },
      {
        "name" : "Steven Ruggles"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.15868v1",
    "title" : "A Survey on Differential Privacy for SpatioTemporal Data in\n  Transportation Research",
    "summary" : "With low-cost computing devices, improved sensor technology, and the\nproliferation of data-driven algorithms, we have more data than we know what to\ndo with. In transportation, we are seeing a surge in spatiotemporal data\ncollection. At the same time, concerns over user privacy have led to research\non differential privacy in applied settings. In this paper, we look at some\nrecent developments in differential privacy in the context of spatiotemporal\ndata. Spatiotemporal data contain not only features about users but also the\ngeographical locations of their frequent visits. Hence, the public release of\nsuch data carries extreme risks. To address the need for such data in research\nand inference without exposing private information, significant work has been\nproposed. This survey paper aims to summarize these efforts and provide a\nreview of differential privacy mechanisms and related software. We also discuss\nrelated work in transportation where such mechanisms have been applied.\nFurthermore, we address the challenges in the deployment and mass adoption of\ndifferential privacy in transportation spatiotemporal data for downstream\nanalyses.",
    "updated" : "2024-07-18T03:19:29Z",
    "published" : "2024-07-18T03:19:29Z",
    "authors" : [
      {
        "name" : "Rahul Bhadani"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.17021v1",
    "title" : "The EU-US Data Privacy Framework: Is the Dragon Eating its Own Tail?",
    "summary" : "The European Commission adequacy decision on the EU US Data Privacy\nFramework, adopted on July 10th, 2023, marks a crucial moment in transatlantic\ndata protection. Following an Executive Order issued by President Biden in\nOctober 2022, this decision confirms that the United States meets European\nUnion standards for personal data protection. The decision extends to all\ntransfers from the European Economic Area to US entities participating in the\nframework, promoting privacy rights while facilitating data exchange. Key\naspects include oversight of US public authorities access to transferred data,\nthe introduction of a dual tier redress mechanism, and granting new rights to\nEU individuals, encompassing data access and rectification. However, the\nframework presents both promise and challenges in health data transfers. While\nstreamlining exchange and aligning legal standards, it grapples with the\ncomplexities of divergent privacy laws. The recent bill for the introduction of\na US federal privacy law emphasizes the urgent need for ongoing reform.\nLingering concerns persist regarding the framework resilience, especially amid\npotential legal battles before the Court of Justice of the EU. The history of\ntransatlantic data transfers between the EU and the US is riddled with\nvulnerabilities, reminiscent of the Ouroboros, an ancient symbol of a serpent\nor dragon eating its own tail, hinting at the looming possibility of the\nframework facing invalidation once again. This article delves into the main\nrequirements of the framework and offers insights on how healthcare\norganizations can navigate it effectively.",
    "updated" : "2024-07-24T06:00:47Z",
    "published" : "2024-07-24T06:00:47Z",
    "authors" : [
      {
        "name" : "Marcelo Corrales Compagnucci"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.16929v1",
    "title" : "Synthetic Data, Similarity-based Privacy Metrics, and Regulatory\n  (Non-)Compliance",
    "summary" : "In this paper, we argue that similarity-based privacy metrics cannot ensure\nregulatory compliance of synthetic data. Our analysis and counter-examples show\nthat they do not protect against singling out and linkability and, among other\nfundamental issues, completely ignore the motivated intruder test.",
    "updated" : "2024-07-24T01:45:41Z",
    "published" : "2024-07-24T01:45:41Z",
    "authors" : [
      {
        "name" : "Georgi Ganev"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.16735v1",
    "title" : "Theoretical Analysis of Privacy Leakage in Trustworthy Federated\n  Learning: A Perspective from Linear Algebra and Optimization Theory",
    "summary" : "Federated learning has emerged as a promising paradigm for collaborative\nmodel training while preserving data privacy. However, recent studies have\nshown that it is vulnerable to various privacy attacks, such as data\nreconstruction attacks. In this paper, we provide a theoretical analysis of\nprivacy leakage in federated learning from two perspectives: linear algebra and\noptimization theory. From the linear algebra perspective, we prove that when\nthe Jacobian matrix of the batch data is not full rank, there exist different\nbatches of data that produce the same model update, thereby ensuring a level of\nprivacy. We derive a sufficient condition on the batch size to prevent data\nreconstruction attacks. From the optimization theory perspective, we establish\nan upper bound on the privacy leakage in terms of the batch size, the\ndistortion extent, and several other factors. Our analysis provides insights\ninto the relationship between privacy leakage and various aspects of federated\nlearning, offering a theoretical foundation for designing privacy-preserving\nfederated learning algorithms.",
    "updated" : "2024-07-23T16:23:38Z",
    "published" : "2024-07-23T16:23:38Z",
    "authors" : [
      {
        "name" : "Xiaojin Zhang"
      },
      {
        "name" : "Wei Chen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.16729v1",
    "title" : "PateGail: A Privacy-Preserving Mobility Trajectory Generator with\n  Imitation Learning",
    "summary" : "Generating human mobility trajectories is of great importance to solve the\nlack of large-scale trajectory data in numerous applications, which is caused\nby privacy concerns. However, existing mobility trajectory generation methods\nstill require real-world human trajectories centrally collected as the training\ndata, where there exists an inescapable risk of privacy leakage. To overcome\nthis limitation, in this paper, we propose PateGail, a privacy-preserving\nimitation learning model to generate mobility trajectories, which utilizes the\npowerful generative adversary imitation learning model to simulate the\ndecision-making process of humans. Further, in order to protect user privacy,\nwe train this model collectively based on decentralized mobility data stored in\nuser devices, where personal discriminators are trained locally to distinguish\nand reward the real and generated human trajectories. In the training process,\nonly the generated trajectories and their rewards obtained based on personal\ndiscriminators are shared between the server and devices, whose privacy is\nfurther preserved by our proposed perturbation mechanisms with theoretical\nproof to satisfy differential privacy. Further, to better model the human\ndecision-making process, we propose a novel aggregation mechanism of the\nrewards obtained from personal discriminators. We theoretically prove that\nunder the reward obtained based on the aggregation mechanism, our proposed\nmodel maximizes the lower bound of the discounted total rewards of users.\nExtensive experiments show that the trajectories generated by our model are\nable to resemble real-world trajectories in terms of five key statistical\nmetrics, outperforming state-of-the-art algorithms by over 48.03%. Furthermore,\nwe demonstrate that the synthetic trajectories are able to efficiently support\npractical applications, including mobility prediction and location\nrecommendation.",
    "updated" : "2024-07-23T14:59:23Z",
    "published" : "2024-07-23T14:59:23Z",
    "authors" : [
      {
        "name" : "Huandong Wang"
      },
      {
        "name" : "Changzheng Gao"
      },
      {
        "name" : "Yuchen Wu"
      },
      {
        "name" : "Depeng Jin"
      },
      {
        "name" : "Lina Yao"
      },
      {
        "name" : "Yong Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.14710v2",
    "title" : "Universally Harmonizing Differential Privacy Mechanisms for Federated\n  Learning: Boosting Accuracy and Convergence",
    "summary" : "Differentially private federated learning (DP-FL) is a promising technique\nfor collaborative model training while ensuring provable privacy for clients.\nHowever, optimizing the tradeoff between privacy and accuracy remains a\ncritical challenge. To our best knowledge, we propose the first DP-FL framework\n(namely UDP-FL), which universally harmonizes any randomization mechanism\n(e.g., an optimal one) with the Gaussian Moments Accountant (viz. DP-SGD) to\nsignificantly boost accuracy and convergence. Specifically, UDP-FL demonstrates\nenhanced model performance by mitigating the reliance on Gaussian noise. The\nkey mediator variable in this transformation is the R\\'enyi Differential\nPrivacy notion, which is carefully used to harmonize privacy budgets. We also\npropose an innovative method to theoretically analyze the convergence for DP-FL\n(including our UDP-FL ) based on mode connectivity analysis. Moreover, we\nevaluate our UDP-FL through extensive experiments benchmarked against\nstate-of-the-art (SOTA) methods, demonstrating superior performance on both\nprivacy guarantees and model performance. Notably, UDP-FL exhibits substantial\nresilience against different inference attacks, indicating a significant\nadvance in safeguarding sensitive data in federated learning environments.",
    "updated" : "2024-07-24T01:15:40Z",
    "published" : "2024-07-20T00:11:59Z",
    "authors" : [
      {
        "name" : "Shuya Feng"
      },
      {
        "name" : "Meisam Mohammady"
      },
      {
        "name" : "Hanbin Hong"
      },
      {
        "name" : "Shenao Yan"
      },
      {
        "name" : "Ashish Kundu"
      },
      {
        "name" : "Binghui Wang"
      },
      {
        "name" : "Yuan Hong"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.18157v1",
    "title" : "Enhanced Privacy Bound for Shuffle Model with Personalized Privacy",
    "summary" : "The shuffle model of Differential Privacy (DP) is an enhanced privacy\nprotocol which introduces an intermediate trusted server between local users\nand a central data curator. It significantly amplifies the central DP guarantee\nby anonymizing and shuffling the local randomized data. Yet, deriving a tight\nprivacy bound is challenging due to its complicated randomization protocol.\nWhile most existing work are focused on unified local privacy settings, this\nwork focuses on deriving the central privacy bound for a more practical setting\nwhere personalized local privacy is required by each user. To bound the privacy\nafter shuffling, we first need to capture the probability of each user\ngenerating clones of the neighboring data points. Second, we need to quantify\nthe indistinguishability between two distributions of the number of clones on\nneighboring datasets. Existing works either inaccurately capture the\nprobability, or underestimate the indistinguishability between neighboring\ndatasets. Motivated by this, we develop a more precise analysis, which yields a\ngeneral and tighter bound for arbitrary DP mechanisms. Firstly, we derive the\nclone-generating probability by hypothesis testing %from a randomizer-specific\nperspective, which leads to a more accurate characterization of the\nprobability. Secondly, we analyze the indistinguishability in the context of\n$f$-DP, where the convexity of the distributions is leveraged to achieve a\ntighter privacy bound. Theoretical and numerical results demonstrate that our\nbound remarkably outperforms the existing results in the literature.",
    "updated" : "2024-07-25T16:11:56Z",
    "published" : "2024-07-25T16:11:56Z",
    "authors" : [
      {
        "name" : "Yixuan Liu"
      },
      {
        "name" : "Yuhan Liu"
      },
      {
        "name" : "Li Xiong"
      },
      {
        "name" : "Yujie Gu"
      },
      {
        "name" : "Hong Chen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.18096v1",
    "title" : "Privacy Threats and Countermeasures in Federated Learning for Internet\n  of Things: A Systematic Review",
    "summary" : "Federated Learning (FL) in the Internet of Things (IoT) environments can\nenhance machine learning by utilising decentralised data, but at the same time,\nit might introduce significant privacy and security concerns due to the\nconstrained nature of IoT devices. This represents a research challenge that we\naim to address in this paper. We systematically analysed recent literature to\nidentify privacy threats in FL within IoT environments, and evaluate the\ndefensive measures that can be employed to mitigate these threats. Using a\nSystematic Literature Review (SLR) approach, we searched five publication\ndatabases (Scopus, IEEE Xplore, Wiley, ACM, and Science Direct), collating\nrelevant papers published between 2017 and April 2024, a period which spans\nfrom the introduction of FL until now. Guided by the PRISMA protocol, we\nselected 49 papers to focus our systematic review on. We analysed these papers,\npaying special attention to the privacy threats and defensive measures --\nspecifically within the context of IoT -- using inclusion and exclusion\ncriteria tailored to highlight recent advances and critical insights. We\nidentified various privacy threats, including inference attacks, poisoning\nattacks, and eavesdropping, along with defensive measures such as Differential\nPrivacy and Secure Multi-Party Computation. These defences were evaluated for\ntheir effectiveness in protecting privacy without compromising the functional\nintegrity of FL in IoT settings. Our review underscores the necessity for\nrobust and efficient privacy-preserving strategies tailored for IoT\nenvironments. Notably, there is a need for strategies against replay, evasion,\nand model stealing attacks. Exploring lightweight defensive measures and\nemerging technologies such as blockchain may help improve the privacy of FL in\nIoT, leading to the creation of FL models that can operate under variable\nnetwork conditions.",
    "updated" : "2024-07-25T15:01:56Z",
    "published" : "2024-07-25T15:01:56Z",
    "authors" : [
      {
        "name" : "Adel ElZemity"
      },
      {
        "name" : "Budi Arief"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.17663v1",
    "title" : "Explaining the Model, Protecting Your Data: Revealing and Mitigating the\n  Data Privacy Risks of Post-Hoc Model Explanations via Membership Inference",
    "summary" : "Predictive machine learning models are becoming increasingly deployed in\nhigh-stakes contexts involving sensitive personal data; in these contexts,\nthere is a trade-off between model explainability and data privacy. In this\nwork, we push the boundaries of this trade-off: with a focus on foundation\nmodels for image classification fine-tuning, we reveal unforeseen privacy risks\nof post-hoc model explanations and subsequently offer mitigation strategies for\nsuch risks. First, we construct VAR-LRT and L1/L2-LRT, two new membership\ninference attacks based on feature attribution explanations that are\nsignificantly more successful than existing explanation-leveraging attacks,\nparticularly in the low false-positive rate regime that allows an adversary to\nidentify specific training set members with confidence. Second, we find\nempirically that optimized differentially private fine-tuning substantially\ndiminishes the success of the aforementioned attacks, while maintaining high\nmodel accuracy. We carry out a systematic empirical investigation of our 2 new\nattacks with 5 vision transformer architectures, 5 benchmark datasets, 4\nstate-of-the-art post-hoc explanation methods, and 4 privacy strength settings.",
    "updated" : "2024-07-24T22:16:37Z",
    "published" : "2024-07-24T22:16:37Z",
    "authors" : [
      {
        "name" : "Catherine Huang"
      },
      {
        "name" : "Martin Pawelczyk"
      },
      {
        "name" : "Himabindu Lakkaraju"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.18789v1",
    "title" : "Granularity is crucial when applying differential privacy to text: An\n  investigation for neural machine translation",
    "summary" : "Applying differential privacy (DP) by means of the DP-SGD algorithm to\nprotect individual data points during training is becoming increasingly popular\nin NLP. However, the choice of granularity at which DP is applied is often\nneglected. For example, neural machine translation (NMT) typically operates on\nthe sentence-level granularity. From the perspective of DP, this setup assumes\nthat each sentence belongs to a single person and any two sentences in the\ntraining dataset are independent. This assumption is however violated in many\nreal-world NMT datasets, e.g. those including dialogues. For proper application\nof DP we thus must shift from sentences to entire documents. In this paper, we\ninvestigate NMT at both the sentence and document levels, analyzing the\nprivacy/utility trade-off for both scenarios, and evaluating the risks of not\nusing the appropriate privacy granularity in terms of leaking personally\nidentifiable information (PII). Our findings indicate that the document-level\nNMT system is more resistant to membership inference attacks, emphasizing the\nsignificance of using the appropriate granularity when working with DP.",
    "updated" : "2024-07-26T14:52:37Z",
    "published" : "2024-07-26T14:52:37Z",
    "authors" : [
      {
        "name" : "Doan Nam Long Vu"
      },
      {
        "name" : "Timour Igamberdiev"
      },
      {
        "name" : "Ivan Habernal"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.18564v1",
    "title" : "Unveiling Privacy Vulnerabilities: Investigating the Role of Structure\n  in Graph Data",
    "summary" : "The public sharing of user information opens the door for adversaries to\ninfer private data, leading to privacy breaches and facilitating malicious\nactivities. While numerous studies have concentrated on privacy leakage via\npublic user attributes, the threats associated with the exposure of user\nrelationships, particularly through network structure, are often neglected.\nThis study aims to fill this critical gap by advancing the understanding and\nprotection against privacy risks emanating from network structure, moving\nbeyond direct connections with neighbors to include the broader implications of\nindirect network structural patterns. To achieve this, we first investigate the\nproblem of Graph Privacy Leakage via Structure (GPS), and introduce a novel\nmeasure, the Generalized Homophily Ratio, to quantify the various mechanisms\ncontributing to privacy breach risks in GPS. Based on this insight, we develop\na novel graph private attribute inference attack, which acts as a pivotal tool\nfor evaluating the potential for privacy leakage through network structures\nunder worst-case scenarios. To protect users' private data from such\nvulnerabilities, we propose a graph data publishing method incorporating a\nlearnable graph sampling technique, effectively transforming the original graph\ninto a privacy-preserving version. Extensive experiments demonstrate that our\nattack model poses a significant threat to user privacy, and our graph data\npublishing method successfully achieves the optimal privacy-utility trade-off\ncompared to baselines.",
    "updated" : "2024-07-26T07:40:54Z",
    "published" : "2024-07-26T07:40:54Z",
    "authors" : [
      {
        "name" : "Hanyang Yuan"
      },
      {
        "name" : "Jiarong Xu"
      },
      {
        "name" : "Cong Wang"
      },
      {
        "name" : "Ziqi Yang"
      },
      {
        "name" : "Chunping Wang"
      },
      {
        "name" : "Keting Yin"
      },
      {
        "name" : "Yang Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.18503v1",
    "title" : "Homomorphic Encryption-Enabled Federated Learning for Privacy-Preserving\n  Intrusion Detection in Resource-Constrained IoV Networks",
    "summary" : "This paper aims to propose a novel framework to address the data privacy\nissue for Federated Learning (FL)-based Intrusion Detection Systems (IDSs) in\nInternet-of-Vehicles(IoVs) with limited computational resources. In particular,\nin conventional FL systems, it is usually assumed that the computing nodes have\nsufficient computational resources to process the training tasks. However, in\npractical IoV systems, vehicles usually have limited computational resources to\nprocess intensive training tasks, compromising the effectiveness of deploying\nFL in IDSs. While offloading data from vehicles to the cloud can mitigate this\nissue, it introduces significant privacy concerns for vehicle users (VUs). To\nresolve this issue, we first propose a highly-effective framework using\nhomomorphic encryption to secure data that requires offloading to a centralized\nserver for processing. Furthermore, we develop an effective training algorithm\ntailored to handle the challenges of FL-based systems with encrypted data. This\nalgorithm allows the centralized server to directly compute on quantum-secure\nencrypted ciphertexts without needing decryption. This approach not only\nsafeguards data privacy during the offloading process from VUs to the\ncentralized server but also enhances the efficiency of utilizing FL for IDSs in\nIoV systems. Our simulation results show that our proposed approach can achieve\na performance that is as close to that of the solution without encryption, with\na gap of less than 0.8%.",
    "updated" : "2024-07-26T04:19:37Z",
    "published" : "2024-07-26T04:19:37Z",
    "authors" : [
      {
        "name" : "Bui Duc Manh"
      },
      {
        "name" : "Chi-Hieu Nguyen"
      },
      {
        "name" : "Dinh Thai Hoang"
      },
      {
        "name" : "Diep N. Nguyen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.18433v1",
    "title" : "Investigating the Privacy Risk of Using Robot Vacuum Cleaners in Smart\n  Environments",
    "summary" : "Robot vacuum cleaners have become increasingly popular and are widely used in\nvarious smart environments. To improve user convenience, manufacturers also\nintroduced smartphone applications that enable users to customize cleaning\nsettings or access information about their robot vacuum cleaners. While this\nintegration enhances the interaction between users and their robot vacuum\ncleaners, it results in potential privacy concerns because users' personal\ninformation may be exposed. To address these concerns, end-to-end encryption is\nimplemented between the application, cloud service, and robot vacuum cleaners\nto secure the exchanged information. Nevertheless, network header metadata\nremains unencrypted and it is still vulnerable to network eavesdropping. In\nthis paper, we investigate the potential risk of private information exposure\nthrough such metadata. A popular robot vacuum cleaner was deployed in a real\nsmart environment where passive network eavesdropping was conducted during\nseveral selected cleaning events. Our extensive analysis, based on Association\nRule Learning, demonstrates that it is feasible to identify certain events\nusing only the captured Internet traffic metadata, thereby potentially exposing\nprivate user information and raising privacy concerns.",
    "updated" : "2024-07-26T00:00:53Z",
    "published" : "2024-07-26T00:00:53Z",
    "authors" : [
      {
        "name" : "Benjamin Ulsmaag"
      },
      {
        "name" : "Jia-Chun Lin"
      },
      {
        "name" : "Ming-Chang Lee"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.18353v1",
    "title" : "Privacy-Preserving Model-Distributed Inference at the Edge",
    "summary" : "This paper focuses on designing a privacy-preserving Machine Learning (ML)\ninference protocol for a hierarchical setup, where clients own/generate data,\nmodel owners (cloud servers) have a pre-trained ML model, and edge servers\nperform ML inference on clients' data using the cloud server's ML model. Our\ngoal is to speed up ML inference while providing privacy to both data and the\nML model. Our approach (i) uses model-distributed inference (model\nparallelization) at the edge servers and (ii) reduces the amount of\ncommunication to/from the cloud server. Our privacy-preserving hierarchical\nmodel-distributed inference, privateMDI design uses additive secret sharing and\nlinearly homomorphic encryption to handle linear calculations in the ML\ninference, and garbled circuit and a novel three-party oblivious transfer are\nused to handle non-linear functions. privateMDI consists of offline and online\nphases. We designed these phases in a way that most of the data exchange is\ndone in the offline phase while the communication overhead of the online phase\nis reduced. In particular, there is no communication to/from the cloud server\nin the online phase, and the amount of communication between the client and\nedge servers is minimized. The experimental results demonstrate that privateMDI\nsignificantly reduces the ML inference time as compared to the baselines.",
    "updated" : "2024-07-25T19:39:03Z",
    "published" : "2024-07-25T19:39:03Z",
    "authors" : [
      {
        "name" : "Fatemeh Jafarian Dehkordi"
      },
      {
        "name" : "Yasaman Keshtkarjahromi"
      },
      {
        "name" : "Hulya Seferoglu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.17663v1",
    "title" : "Explaining the Model, Protecting Your Data: Revealing and Mitigating the\n  Data Privacy Risks of Post-Hoc Model Explanations via Membership Inference",
    "summary" : "Predictive machine learning models are becoming increasingly deployed in\nhigh-stakes contexts involving sensitive personal data; in these contexts,\nthere is a trade-off between model explainability and data privacy. In this\nwork, we push the boundaries of this trade-off: with a focus on foundation\nmodels for image classification fine-tuning, we reveal unforeseen privacy risks\nof post-hoc model explanations and subsequently offer mitigation strategies for\nsuch risks. First, we construct VAR-LRT and L1/L2-LRT, two new membership\ninference attacks based on feature attribution explanations that are\nsignificantly more successful than existing explanation-leveraging attacks,\nparticularly in the low false-positive rate regime that allows an adversary to\nidentify specific training set members with confidence. Second, we find\nempirically that optimized differentially private fine-tuning substantially\ndiminishes the success of the aforementioned attacks, while maintaining high\nmodel accuracy. We carry out a systematic empirical investigation of our 2 new\nattacks with 5 vision transformer architectures, 5 benchmark datasets, 4\nstate-of-the-art post-hoc explanation methods, and 4 privacy strength settings.",
    "updated" : "2024-07-24T22:16:37Z",
    "published" : "2024-07-24T22:16:37Z",
    "authors" : [
      {
        "name" : "Catherine Huang"
      },
      {
        "name" : "Martin Pawelczyk"
      },
      {
        "name" : "Himabindu Lakkaraju"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.16929v2",
    "title" : "Synthetic Data, Similarity-based Privacy Metrics, and Regulatory\n  (Non-)Compliance",
    "summary" : "In this paper, we argue that similarity-based privacy metrics cannot ensure\nregulatory compliance of synthetic data. Our analysis and counter-examples show\nthat they do not protect against singling out and linkability and, among other\nfundamental issues, completely ignore the motivated intruder test.",
    "updated" : "2024-07-26T03:30:05Z",
    "published" : "2024-07-24T01:45:41Z",
    "authors" : [
      {
        "name" : "Georgi Ganev"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.19828v1",
    "title" : "Federated Learning based Latent Factorization of Tensors for\n  Privacy-Preserving QoS Prediction",
    "summary" : "In applications related to big data and service computing, dynamic\nconnections tend to be encountered, especially the dynamic data of\nuser-perspective quality of service (QoS) in Web services. They are transformed\ninto high-dimensional and incomplete (HDI) tensors which include abundant\ntemporal pattern information. Latent factorization of tensors (LFT) is an\nextremely efficient and typical approach for extracting such patterns from an\nHDI tensor. However, current LFT models require the QoS data to be maintained\nin a central place (e.g., a central server), which is impossible for\nincreasingly privacy-sensitive users. To address this problem, this article\ncreatively designs a federated learning based on latent factorization of\ntensors (FL-LFT). It builds a data-density -oriented federated learning model\nto enable isolated users to collaboratively train a global LFT model while\nprotecting user's privacy. Extensive experiments on a QoS dataset collected\nfrom the real world verify that FL-LFT shows a remarkable increase in\nprediction accuracy when compared to state-of-the-art federated learning (FL)\napproaches.",
    "updated" : "2024-07-29T09:30:00Z",
    "published" : "2024-07-29T09:30:00Z",
    "authors" : [
      {
        "name" : "Shuai Zhong"
      },
      {
        "name" : "Zengtong Tang"
      },
      {
        "name" : "Di Wu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.19703v1",
    "title" : "Efficient Byzantine-Robust and Provably Privacy-Preserving Federated\n  Learning",
    "summary" : "Federated learning (FL) is an emerging distributed learning paradigm without\nsharing participating clients' private data. However, existing works show that\nFL is vulnerable to both Byzantine (security) attacks and data reconstruction\n(privacy) attacks. Almost all the existing FL defenses only address one of the\ntwo attacks. A few defenses address the two attacks, but they are not efficient\nand effective enough. We propose BPFL, an efficient Byzantine-robust and\nprovably privacy-preserving FL method that addresses all the issues.\nSpecifically, we draw on state-of-the-art Byzantine-robust FL methods and use\nsimilarity metrics to measure the robustness of each participating client in\nFL. The validity of clients are formulated as circuit constraints on similarity\nmetrics and verified via a zero-knowledge proof. Moreover, the client models\nare masked by a shared random vector, which is generated based on homomorphic\nencryption. In doing so, the server receives the masked client models rather\nthan the true ones, which are proven to be private. BPFL is also efficient due\nto the usage of non-interactive zero-knowledge proof. Experimental results on\nvarious datasets show that our BPFL is efficient, Byzantine-robust, and\nprivacy-preserving.",
    "updated" : "2024-07-29T04:55:30Z",
    "published" : "2024-07-29T04:55:30Z",
    "authors" : [
      {
        "name" : "Chenfei Nie"
      },
      {
        "name" : "Qiang Li"
      },
      {
        "name" : "Yuxin Yang"
      },
      {
        "name" : "Yuede Ji"
      },
      {
        "name" : "Binghui Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.19677v1",
    "title" : "Navigating the United States Legislative Landscape on Voice Privacy:\n  Existing Laws, Proposed Bills, Protection for Children, and Synthetic Data\n  for AI",
    "summary" : "Privacy is a hot topic for policymakers across the globe, including the\nUnited States. Evolving advances in AI and emerging concerns about the misuse\nof personal data have pushed policymakers to draft legislation on trustworthy\nAI and privacy protection for its citizens. This paper presents the state of\nthe privacy legislation at the U.S. Congress and outlines how voice data is\nconsidered as part of the legislation definition. This paper also reviews\nadditional privacy protection for children. This paper presents a holistic\nreview of enacted and proposed privacy laws, and consideration for voice data,\nincluding guidelines for processing children's data, in those laws across the\nfifty U.S. states. As a groundbreaking alternative to actual human data,\nethically generated synthetic data allows much flexibility to keep AI\ninnovation in progress. Given the consideration of synthetic data in AI\nlegislation by policymakers to be relatively new, as compared to that of\nprivacy laws, this paper reviews regulatory considerations for synthetic data.",
    "updated" : "2024-07-29T03:43:16Z",
    "published" : "2024-07-29T03:43:16Z",
    "authors" : [
      {
        "name" : "Satwik Dutta"
      },
      {
        "name" : "John H. L. Hansen"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CR",
      "cs.SD",
      "eess.AS",
      "I.2; J.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.19401v1",
    "title" : "Complete Security and Privacy for AI Inference in Decentralized Systems",
    "summary" : "The need for data security and model integrity has been accentuated by the\nrapid adoption of AI and ML in data-driven domains including healthcare,\nfinance, and security. Large models are crucial for tasks like diagnosing\ndiseases and forecasting finances but tend to be delicate and not very\nscalable. Decentralized systems solve this issue by distributing the workload\nand reducing central points of failure. Yet, data and processes spread across\ndifferent nodes can be at risk of unauthorized access, especially when they\ninvolve sensitive information. Nesa solves these challenges with a\ncomprehensive framework using multiple techniques to protect data and model\noutputs. This includes zero-knowledge proofs for secure model verification. The\nframework also introduces consensus-based verification checks for consistent\noutputs across nodes and confirms model integrity. Split Learning divides\nmodels into segments processed by different nodes for data privacy by\npreventing full data access at any single point. For hardware-based security,\ntrusted execution environments are used to protect data and computations within\nsecure zones. Nesa's state-of-the-art proofs and principles demonstrate the\nframework's effectiveness, making it a promising approach for securely\ndemocratizing artificial intelligence.",
    "updated" : "2024-07-28T05:09:17Z",
    "published" : "2024-07-28T05:09:17Z",
    "authors" : [
      {
        "name" : "Hongyang Zhang"
      },
      {
        "name" : "Yue Zhao"
      },
      {
        "name" : "Claudio Angione"
      },
      {
        "name" : "Harry Yang"
      },
      {
        "name" : "James Buban"
      },
      {
        "name" : "Ahmad Farhan"
      },
      {
        "name" : "Fielding Johnston"
      },
      {
        "name" : "Patrick Colangelo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.19364v1",
    "title" : "Defogger: A Visual Analysis Approach for Data Exploration of Sensitive\n  Data Protected by Differential Privacy",
    "summary" : "Differential privacy ensures the security of individual privacy but poses\nchallenges to data exploration processes because the limited privacy budget\nincapacitates the flexibility of exploration and the noisy feedback of data\nrequests leads to confusing uncertainty. In this study, we take the lead in\ndescribing corresponding exploration scenarios, including underlying\nrequirements and available exploration strategies. To facilitate practical\napplications, we propose a visual analysis approach to the formulation of\nexploration strategies. Our approach applies a reinforcement learning model to\nprovide diverse suggestions for exploration strategies according to the\nexploration intent of users. A novel visual design for representing uncertainty\nin correlation patterns is integrated into our prototype system to support the\nproposed approach. Finally, we implemented a user study and two case studies.\nThe results of these studies verified that our approach can help develop\nstrategies that satisfy the exploration intent of users.",
    "updated" : "2024-07-28T02:14:12Z",
    "published" : "2024-07-28T02:14:12Z",
    "authors" : [
      {
        "name" : "Xumeng Wang"
      },
      {
        "name" : "Shuangcheng Jiao"
      },
      {
        "name" : "Chris Bryan"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.19354v1",
    "title" : "The Emerged Security and Privacy of LLM Agent: A Survey with Case\n  Studies",
    "summary" : "Inspired by the rapid development of Large Language Models (LLMs), LLM agents\nhave evolved to perform complex tasks. LLM agents are now extensively applied\nacross various domains, handling vast amounts of data to interact with humans\nand execute tasks. The widespread applications of LLM agents demonstrate their\nsignificant commercial value; however, they also expose security and privacy\nvulnerabilities. At the current stage, comprehensive research on the security\nand privacy of LLM agents is highly needed. This survey aims to provide a\ncomprehensive overview of the newly emerged privacy and security issues faced\nby LLM agents. We begin by introducing the fundamental knowledge of LLM agents,\nfollowed by a categorization and analysis of the threats. We then discuss the\nimpacts of these threats on humans, environment, and other agents.\nSubsequently, we review existing defensive strategies, and finally explore\nfuture trends. Additionally, the survey incorporates diverse case studies to\nfacilitate a more accessible understanding. By highlighting these critical\nsecurity and privacy issues, the survey seeks to stimulate future research\ntowards enhancing the security and privacy of LLM agents, thereby increasing\ntheir reliability and trustworthiness in future applications.",
    "updated" : "2024-07-28T00:26:24Z",
    "published" : "2024-07-28T00:26:24Z",
    "authors" : [
      {
        "name" : "Feng He"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Dayong Ye"
      },
      {
        "name" : "Bo Liu"
      },
      {
        "name" : "Wanlei Zhou"
      },
      {
        "name" : "Philip S. Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.19147v1",
    "title" : "Reexamination of the realtime protection for user privacy in practical\n  quantum private query",
    "summary" : "Quantum private query (QPQ) is the quantum version for symmetrically private\nretrieval. However, the user privacy in QPQ is generally guarded in the\nnon-realtime and cheat sensitive way. That is, the dishonest database holder's\ncheating to elicit user privacy can only be discovered after the protocol is\nfinished (when the user finds some errors in the retrieved database item). Such\ndelayed detection may cause very unpleasant results for the user in real-life\napplications. Current efforts to protect user privacy in realtime in existing\nQPQ protocols mainly use two techniques, i.e., adding an honesty checking on\nthe database or allowing the user to reorder the qubits. We reexamine these two\nkinds of QPQ protocols and find neither of them can work well. We give concrete\ncheating strategies for both participants and show that honesty checking of\ninner participant should be dealt more carefully in for example the choosing of\nchecking qubits. We hope such discussion can supply new concerns when detection\nof dishonest participant is considered in quantum multi-party secure\ncomputations.",
    "updated" : "2024-07-27T02:19:35Z",
    "published" : "2024-07-27T02:19:35Z",
    "authors" : [
      {
        "name" : "Chun-Yan Wei"
      },
      {
        "name" : "Xiao-Qiu Cai"
      },
      {
        "name" : "Tian-Yin Wang"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.19119v1",
    "title" : "Accuracy-Privacy Trade-off in the Mitigation of Membership Inference\n  Attack in Federated Learning",
    "summary" : "Over the last few years, federated learning (FL) has emerged as a prominent\nmethod in machine learning, emphasizing privacy preservation by allowing\nmultiple clients to collaboratively build a model while keeping their training\ndata private. Despite this focus on privacy, FL models are susceptible to\nvarious attacks, including membership inference attacks (MIAs), posing a\nserious threat to data confidentiality. In a recent study, Rezaei \\textit{et\nal.} revealed the existence of an accuracy-privacy trade-off in deep ensembles\nand proposed a few fusion strategies to overcome it. In this paper, we aim to\nexplore the relationship between deep ensembles and FL. Specifically, we\ninvestigate whether confidence-based metrics derived from deep ensembles apply\nto FL and whether there is a trade-off between accuracy and privacy in FL with\nrespect to MIA. Empirical investigations illustrate a lack of a non-monotonic\ncorrelation between the number of clients and the accuracy-privacy trade-off.\nBy experimenting with different numbers of federated clients, datasets, and\nconfidence-metric-based fusion strategies, we identify and analytically justify\nthe clear existence of the accuracy-privacy trade-off.",
    "updated" : "2024-07-26T22:44:41Z",
    "published" : "2024-07-26T22:44:41Z",
    "authors" : [
      {
        "name" : "Sayyed Farid Ahamed"
      },
      {
        "name" : "Soumya Banerjee"
      },
      {
        "name" : "Sandip Roy"
      },
      {
        "name" : "Devin Quinn"
      },
      {
        "name" : "Marc Vucovich"
      },
      {
        "name" : "Kevin Choi"
      },
      {
        "name" : "Abdul Rahman"
      },
      {
        "name" : "Alison Hu"
      },
      {
        "name" : "Edward Bowen"
      },
      {
        "name" : "Sachin Shetty"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.18982v1",
    "title" : "Low-Latency Privacy-Preserving Deep Learning Design via Secure MPC",
    "summary" : "Secure multi-party computation (MPC) facilitates privacy-preserving\ncomputation between multiple parties without leaking private information. While\nmost secure deep learning techniques utilize MPC operations to achieve feasible\nprivacy-preserving machine learning on downstream tasks, the overhead of the\ncomputation and communication still hampers their practical application. This\nwork proposes a low-latency secret-sharing-based MPC design that reduces\nunnecessary communication rounds during the execution of MPC protocols. We also\npresent a method for improving the computation of commonly used nonlinear\nfunctions in deep learning by integrating multivariate multiplication and\ncoalescing different packets into one to maximize network utilization. Our\nexperimental results indicate that our method is effective in a variety of\nsettings, with a speedup in communication latency of $10\\sim20\\%$.",
    "updated" : "2024-07-24T07:01:21Z",
    "published" : "2024-07-24T07:01:21Z",
    "authors" : [
      {
        "name" : "Ke Lin"
      },
      {
        "name" : "Yasir Glani"
      },
      {
        "name" : "Ping Luo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.18923v1",
    "title" : "Towards a Novel Privacy-Preserving Distributed Multiparty Data\n  Outsourcing Scheme for Cloud Computing with Quantum Key Distribution",
    "summary" : "The intersection of cloud computing, blockchain technology, and the impending\nera of quantum computing presents a critical juncture for data security. This\nresearch addresses the escalating vulnerabilities by proposing a comprehensive\nframework that integrates Quantum Key Distribution (QKD), CRYSTALS Kyber, and\nZero-Knowledge Proofs (ZKPs) for securing data in cloud-based blockchain\nsystems. The primary objective is to fortify data against quantum threats\nthrough the implementation of QKD, a quantum-safe cryptographic protocol. We\nleverage the lattice-based cryptographic mechanism, CRYSTALS Kyber, known for\nits resilience against quantum attacks. Additionally, ZKPs are introduced to\nenhance data privacy and verification processes within the cloud and blockchain\nenvironment. A significant focus of this research is the performance evaluation\nof the proposed framework. Rigorous analyses encompass encryption and\ndecryption processes, quantum key generation rates, and overall system\nefficiency. Practical implications are scrutinized, considering factors such as\nfile size, response time, and computational overhead. The evaluation sheds\nlight on the framework's viability in real-world cloud environments,\nemphasizing its efficiency in mitigating quantum threats. The findings\ncontribute a robust quantum-safe and ZKP-integrated security framework tailored\nfor cloud-based blockchain storage. By addressing critical gaps in theoretical\nadvancements, this research offers practical insights for organizations seeking\nto secure their data against quantum threats. The framework's efficiency and\nscalability underscore its practical feasibility, serving as a guide for\nimplementing enhanced data security in the evolving landscape of quantum\ncomputing and blockchain integration within cloud environments.",
    "updated" : "2024-07-09T15:53:04Z",
    "published" : "2024-07-09T15:53:04Z",
    "authors" : [
      {
        "name" : "D. Dhinakaran"
      },
      {
        "name" : "D. Selvaraj"
      },
      {
        "name" : "N. Dharini"
      },
      {
        "name" : "S. Edwin Raja"
      },
      {
        "name" : "C. Sakthi Lakshmi Priya"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.20830v1",
    "title" : "Federated Knowledge Recycling: Privacy-Preserving Synthetic Data Sharing",
    "summary" : "Federated learning has emerged as a paradigm for collaborative learning,\nenabling the development of robust models without the need to centralise\nsensitive data. However, conventional federated learning techniques have\nprivacy and security vulnerabilities due to the exposure of models, parameters\nor updates, which can be exploited as an attack surface. This paper presents\nFederated Knowledge Recycling (FedKR), a cross-silo federated learning approach\nthat uses locally generated synthetic data to facilitate collaboration between\ninstitutions. FedKR combines advanced data generation techniques with a dynamic\naggregation process to provide greater security against privacy attacks than\nexisting methods, significantly reducing the attack surface. Experimental\nresults on generic and medical datasets show that FedKR achieves competitive\nperformance, with an average improvement in accuracy of 4.24% compared to\ntraining models from local data, demonstrating particular effectiveness in data\nscarcity scenarios.",
    "updated" : "2024-07-30T13:56:26Z",
    "published" : "2024-07-30T13:56:26Z",
    "authors" : [
      {
        "name" : "Eugenio Lomurno"
      },
      {
        "name" : "Matteo Matteucci"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.20640v1",
    "title" : "Improved Bounds for Pure Private Agnostic Learning: Item-Level and\n  User-Level Privacy",
    "summary" : "Machine Learning has made remarkable progress in a wide range of fields. In\nmany scenarios, learning is performed on datasets involving sensitive\ninformation, in which privacy protection is essential for learning algorithms.\nIn this work, we study pure private learning in the agnostic model -- a\nframework reflecting the learning process in practice. We examine the number of\nusers required under item-level (where each user contributes one example) and\nuser-level (where each user contributes multiple examples) privacy and derive\nseveral improved upper bounds. For item-level privacy, our algorithm achieves a\nnear optimal bound for general concept classes. We extend this to the\nuser-level setting, rendering a tighter upper bound than the one proved by\nGhazi et al. (2023). Lastly, we consider the problem of learning thresholds\nunder user-level privacy and present an algorithm with a nearly tight user\ncomplexity.",
    "updated" : "2024-07-30T08:35:26Z",
    "published" : "2024-07-30T08:35:26Z",
    "authors" : [
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Peng Ye"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.21691v1",
    "title" : "Explainable Artificial Intelligence for Quantifying Interfering and\n  High-Risk Behaviors in Autism Spectrum Disorder in a Real-World Classroom\n  Environment Using Privacy-Preserving Video Analysis",
    "summary" : "Rapid identification and accurate documentation of interfering and high-risk\nbehaviors in ASD, such as aggression, self-injury, disruption, and restricted\nrepetitive behaviors, are important in daily classroom environments for\ntracking intervention effectiveness and allocating appropriate resources to\nmanage care needs. However, having a staff dedicated solely to observing is\ncostly and uncommon in most educational settings. Recently, multiple research\nstudies have explored developing automated, continuous, and objective tools\nusing machine learning models to quantify behaviors in ASD. However, the\nmajority of the work was conducted under a controlled environment and has not\nbeen validated for real-world conditions. In this work, we demonstrate that the\nlatest advances in video-based group activity recognition techniques can\nquantify behaviors in ASD in real-world activities in classroom environments\nwhile preserving privacy. Our explainable model could detect the episode of\nproblem behaviors with a 77% F1-score and capture distinctive behavior features\nin different types of behaviors in ASD. To the best of our knowledge, this is\nthe first work that shows the promise of objectively quantifying behaviors in\nASD in a real-world environment, which is an important step toward the\ndevelopment of a practical tool that can ease the burden of data collection for\nclassroom staff.",
    "updated" : "2024-07-31T15:37:52Z",
    "published" : "2024-07-31T15:37:52Z",
    "authors" : [
      {
        "name" : "Barun Das"
      },
      {
        "name" : "Conor Anderson"
      },
      {
        "name" : "Tania Villavicencio"
      },
      {
        "name" : "Johanna Lantz"
      },
      {
        "name" : "Jenny Foster"
      },
      {
        "name" : "Theresa Hamlin"
      },
      {
        "name" : "Ali Bahrami Rad"
      },
      {
        "name" : "Gari D. Clifford"
      },
      {
        "name" : "Hyeokhyen Kwon"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.21624v1",
    "title" : "Grid-Based Decompositions for Spatial Data under Local Differential\n  Privacy",
    "summary" : "Local differential privacy (LDP) has recently emerged as a popular privacy\nstandard. With the growing popularity of LDP, several recent works have applied\nLDP to spatial data, and grid-based decompositions have been a common building\nblock in the collection and analysis of spatial data under DP and LDP. In this\npaper, we study three grid-based decomposition methods for spatial data under\nLDP: Uniform Grid (UG), PrivAG, and AAG. UG is a static approach that consists\nof equal-sized cells. To enable data-dependent decomposition, PrivAG was\nproposed by Yang et al. as the most recent adaptive grid method. To advance the\nstate-of-the-art in adaptive grids, in this paper we propose the Advanced\nAdaptive Grid (AAG) method. For each grid cell, following the intuition that\nthe cell's intra-cell density distribution will be affected by its neighbors,\nAAG performs uneven cell divisions depending on the neighboring cells'\ndensities. We experimentally compare UG, PrivAG, and AAG using three real-world\nlocation datasets, varying privacy budgets, and query sizes. Results show that\nAAG provides higher utility than PrivAG, demonstrating the superiority of our\nproposed approach. Furthermore, UG's performance is heavily dependent on the\nchoice of grid size. When the grid size is chosen optimally in UG, AAG still\nbeats UG for small queries, but UG beats AAG for large (coarse-grained)\nqueries.",
    "updated" : "2024-07-31T14:17:44Z",
    "published" : "2024-07-31T14:17:44Z",
    "authors" : [
      {
        "name" : "Berkay Kemal Balioglu"
      },
      {
        "name" : "Alireza Khodaie"
      },
      {
        "name" : "Ameer Taweel"
      },
      {
        "name" : "Mehmet Emre Gursoy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.21141v1",
    "title" : "FL-DECO-BC: A Privacy-Preserving, Provably Secure, and\n  Provenance-Preserving Federated Learning Framework with Decentralized Oracles\n  on Blockchain for VANETs",
    "summary" : "Vehicular Ad-Hoc Networks (VANETs) hold immense potential for improving\ntraffic safety and efficiency. However, traditional centralized approaches for\nmachine learning in VANETs raise concerns about data privacy and security.\nFederated Learning (FL) offers a solution that enables collaborative model\ntraining without sharing raw data. This paper proposes FL-DECO-BC as a novel\nprivacy-preserving, provably secure, and provenance-preserving federated\nlearning framework specifically designed for VANETs. FL-DECO-BC leverages\ndecentralized oracles on blockchain to securely access external data sources\nwhile ensuring data privacy through advanced techniques. The framework\nguarantees provable security through cryptographic primitives and formal\nverification methods. Furthermore, FL-DECO-BC incorporates a\nprovenance-preserving design to track data origin and history, fostering trust\nand accountability. This combination of features empowers VANETs with secure\nand privacy-conscious machine-learning capabilities, paving the way for\nadvanced traffic management and safety applications.",
    "updated" : "2024-07-30T19:09:10Z",
    "published" : "2024-07-30T19:09:10Z",
    "authors" : [
      {
        "name" : "Sathwik Narkedimilli"
      },
      {
        "name" : "Rayachoti Arun Kumar"
      },
      {
        "name" : "N. V. Saran Kumar"
      },
      {
        "name" : "Ramapathruni Praneeth Reddy"
      },
      {
        "name" : "Pavan Kumar C"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.01428v1",
    "title" : "Transferable Adversarial Facial Images for Privacy Protection",
    "summary" : "The success of deep face recognition (FR) systems has raised serious privacy\nconcerns due to their ability to enable unauthorized tracking of users in the\ndigital world. Previous studies proposed introducing imperceptible adversarial\nnoises into face images to deceive those face recognition models, thus\nachieving the goal of enhancing facial privacy protection. Nevertheless, they\nheavily rely on user-chosen references to guide the generation of adversarial\nnoises, and cannot simultaneously construct natural and highly transferable\nadversarial face images in black-box scenarios. In light of this, we present a\nnovel face privacy protection scheme with improved transferability while\nmaintain high visual quality. We propose shaping the entire face space directly\ninstead of exploiting one kind of facial characteristic like makeup information\nto integrate adversarial noises. To achieve this goal, we first exploit global\nadversarial latent search to traverse the latent space of the generative model,\nthereby creating natural adversarial face images with high transferability. We\nthen introduce a key landmark regularization module to preserve the visual\nidentity information. Finally, we investigate the impacts of various kinds of\nlatent spaces and find that $\\mathcal{F}$ latent space benefits the trade-off\nbetween visual naturalness and adversarial transferability. Extensive\nexperiments over two datasets demonstrate that our approach significantly\nenhances attack transferability while maintaining high visual quality,\noutperforming state-of-the-art methods by an average 25% improvement in deep FR\nmodels and 10% improvement on commercial FR APIs, including Face++, Aliyun, and\nTencent.",
    "updated" : "2024-07-18T02:16:11Z",
    "published" : "2024-07-18T02:16:11Z",
    "authors" : [
      {
        "name" : "Minghui Li"
      },
      {
        "name" : "Jiangxiong Wang"
      },
      {
        "name" : "Hao Zhang"
      },
      {
        "name" : "Ziqi Zhou"
      },
      {
        "name" : "Shengshan Hu"
      },
      {
        "name" : "Xiaobing Pei"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2408.05218v1",
    "title" : "Comment on \"An Efficient Privacy-Preserving Ranked Multi-Keyword\n  Retrieval for Multiple Data Owners in Outsourced Cloud\"",
    "summary" : "Protecting the privacy of keywords in the field of search over outsourced\ncloud data is a challenging task. In IEEE Transactions on Services Computing\n(Vol. 17 No. 2, March/April 2024), Li et al. proposed PRMKR: efficient\nprivacy-preserving ranked multi-keyword retrieval scheme, which was claimed to\nresist keyword guessing attack. However, we show that the scheme fails to\nresist keyword guessing attack, index privacy, and trapdoor privacy. Further,\nwe propose a solution to address the above said issues by correcting the errors\nin the important equations of the scheme.",
    "updated" : "2024-07-25T05:01:07Z",
    "published" : "2024-07-25T05:01:07Z",
    "authors" : [
      {
        "name" : "Uma Sankararao Varri"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.18157v1",
    "title" : "Enhanced Privacy Bound for Shuffle Model with Personalized Privacy",
    "summary" : "The shuffle model of Differential Privacy (DP) is an enhanced privacy protocol which introduces an intermediate trusted server between local users and a central data curator. It significantly amplifies the central DP guarantee by anonymizing and shuffling the local randomized data. Yet, deriving a tight privacy bound is challenging due to its complicated randomization protocol. While most existing work are focused on unified local privacy settings, this work focuses on deriving the central privacy bound for a more practical setting where personalized local privacy is required by each user. To bound the privacy after shuffling, we first need to capture the probability of each user generating clones of the neighboring data points. Second, we need to quantify the indistinguishability between two distributions of the number of clones on neighboring datasets. Existing works either inaccurately capture the probability, or underestimate the indistinguishability between neighboring datasets. Motivated by this, we develop a more precise analysis, which yields a general and tighter bound for arbitrary DP mechanisms. Firstly, we derive the clone-generating probability by hypothesis testing %from a randomizer-specific perspective, which leads to a more accurate characterization of the probability. Secondly, we analyze the indistinguishability in the context of $f$-DP, where the convexity of the distributions is leveraged to achieve a tighter privacy bound. Theoretical and numerical results demonstrate that our bound remarkably outperforms the existing results in the literature.",
    "updated" : "2024-07-26T00:52:15Z",
    "published" : "2024-07-25T16:11:56Z",
    "authors" : [
      {
        "name" : "Yixuan Liu"
      },
      {
        "name" : "Yuhan Liu"
      },
      {
        "name" : "Li Xiong"
      },
      {
        "name" : "Yujie Gu"
      },
      {
        "name" : "Hong Chen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.18157v1",
    "title" : "Enhanced Privacy Bound for Shuffle Model with Personalized Privacy",
    "summary" : "The shuffle model of Differential Privacy (DP) is an enhanced privacy protocol which introduces an intermediate trusted server between local users and a central data curator. It significantly amplifies the central DP guarantee by anonymizing and shuffling the local randomized data. Yet, deriving a tight privacy bound is challenging due to its complicated randomization protocol. While most existing work are focused on unified local privacy settings, this work focuses on deriving the central privacy bound for a more practical setting where personalized local privacy is required by each user. To bound the privacy after shuffling, we first need to capture the probability of each user generating clones of the neighboring data points. Second, we need to quantify the indistinguishability between two distributions of the number of clones on neighboring datasets. Existing works either inaccurately capture the probability, or underestimate the indistinguishability between neighboring datasets. Motivated by this, we develop a more precise analysis, which yields a general and tighter bound for arbitrary DP mechanisms. Firstly, we derive the clone-generating probability by hypothesis testing %from a randomizer-specific perspective, which leads to a more accurate characterization of the probability. Secondly, we analyze the indistinguishability in the context of $f$-DP, where the convexity of the distributions is leveraged to achieve a tighter privacy bound. Theoretical and numerical results demonstrate that our bound remarkably outperforms the existing results in the literature.",
    "updated" : "2024-07-25T16:11:56Z",
    "published" : "2024-07-25T16:11:56Z",
    "authors" : [
      {
        "name" : "Yixuan Liu"
      },
      {
        "name" : "Yuhan Liu"
      },
      {
        "name" : "Li Xiong"
      },
      {
        "name" : "Yujie Gu"
      },
      {
        "name" : "Hong Chen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  }
]