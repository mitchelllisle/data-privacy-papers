[
  {
    "id" : "http://arxiv.org/abs/2407.02268v1",
    "title" : "Footprints of Data in a Classifier Model: The Privacy Issues and Their\n  Mitigation through Data Obfuscation",
    "summary" : "The avalanche of AI deployment and its security-privacy concerns are two\nsides of the same coin. Article 17 of GDPR calls for the Right to Erasure; data\nhas to be obliterated from a system to prevent its compromise. Extant research\nin this aspect focuses on effacing sensitive data attributes. However, several\npassive modes of data compromise are yet to be recognized and redressed. The\nembedding of footprints of training data in a prediction model is one such\nfacet; the difference in performance quality in test and training data causes\npassive identification of data that have trained the model. This research\nfocuses on addressing the vulnerability arising from the data footprints. The\nthree main aspects are -- i] exploring the vulnerabilities of different\nclassifiers (to segregate the vulnerable and the non-vulnerable ones), ii]\nreducing the vulnerability of vulnerable classifiers (through data obfuscation)\nto preserve model and data privacy, and iii] exploring the privacy-performance\ntradeoff to study the usability of the data obfuscation techniques. An\nempirical study is conducted on three datasets and eight classifiers to explore\nthe above objectives. The results of the initial research identify the\nvulnerability in classifiers and segregate the vulnerable and non-vulnerable\nclassifiers. The additional experiments on data obfuscation techniques reveal\ntheir utility to render data and model privacy and also their capability to\nchalk out a privacy-performance tradeoff in most scenarios. The results can aid\nthe practitioners with their choice of classifiers in different scenarios and\ncontexts.",
    "updated" : "2024-07-02T13:56:37Z",
    "published" : "2024-07-02T13:56:37Z",
    "authors" : [
      {
        "name" : "Payel Sadhukhan"
      },
      {
        "name" : "Tanujit Chakraborty"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02226v1",
    "title" : "RollupTheCrowd: Leveraging ZkRollups for a Scalable and\n  Privacy-Preserving Reputation-based Crowdsourcing Platform",
    "summary" : "Current blockchain-based reputation solutions for crowdsourcing fail to\ntackle the challenge of ensuring both efficiency and privacy without\ncompromising the scalability of the blockchain. Developing an effective,\ntransparent, and privacy-preserving reputation model necessitates on-chain\nimplementation using smart contracts. However, managing task evaluation and\nreputation updates alongside crowdsourcing transactions on-chain substantially\nstrains system scalability and performance. This paper introduces\nRollupTheCrowd, a novel blockchain-powered crowdsourcing framework that\nleverages zkRollups to enhance system scalability while protecting user\nprivacy. Our framework includes an effective and privacy-preserving reputation\nmodel that gauges workers' trustworthiness by assessing their crowdsourcing\ninteractions. To alleviate the load on our blockchain, we employ an off-chain\nstorage scheme, optimizing RollupTheCrowd's performance. Utilizing smart\ncontracts and zero-knowledge proofs, our Rollup layer achieves a significant\n20x reduction in gas consumption. To prove the feasibility of the proposed\nframework, we developed a proof-of-concept implementation using cutting-edge\ntools. The experimental results presented in this paper demonstrate the\neffectiveness and scalability of RollupTheCrowd, validating its potential for\nreal-world application scenarios.",
    "updated" : "2024-07-02T12:51:32Z",
    "published" : "2024-07-02T12:51:32Z",
    "authors" : [
      {
        "name" : "Ahmed Mounsf Rafik Bendada"
      },
      {
        "name" : "Mouhamed Amine Bouchiha"
      },
      {
        "name" : "Mourad Rabah"
      },
      {
        "name" : "Yacine Ghamri-Doudane"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02191v1",
    "title" : "Attack-Aware Noise Calibration for Differential Privacy",
    "summary" : "Differential privacy (DP) is a widely used approach for mitigating privacy\nrisks when training machine learning models on sensitive data. DP mechanisms\nadd noise during training to limit the risk of information leakage. The scale\nof the added noise is critical, as it determines the trade-off between privacy\nand utility. The standard practice is to select the noise scale in terms of a\nprivacy budget parameter $\\epsilon$. This parameter is in turn interpreted in\nterms of operational attack risk, such as accuracy, or sensitivity and\nspecificity of inference attacks against the privacy of the data. We\ndemonstrate that this two-step procedure of first calibrating the noise scale\nto a privacy budget $\\epsilon$, and then translating $\\epsilon$ to attack risk\nleads to overly conservative risk assessments and unnecessarily low utility. We\npropose methods to directly calibrate the noise scale to a desired attack risk\nlevel, bypassing the intermediate step of choosing $\\epsilon$. For a target\nattack risk, our approach significantly decreases noise scale, leading to\nincreased utility at the same level of privacy. We empirically demonstrate that\ncalibrating noise to attack sensitivity/specificity, rather than $\\epsilon$,\nwhen training privacy-preserving ML models substantially improves model\naccuracy for the same risk level. Our work provides a principled and practical\nway to improve the utility of privacy-preserving ML without compromising on\nprivacy.",
    "updated" : "2024-07-02T11:49:59Z",
    "published" : "2024-07-02T11:49:59Z",
    "authors" : [
      {
        "name" : "Bogdan Kulynych"
      },
      {
        "name" : "Juan Felipe Gomez"
      },
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Flavio du Pin Calmon"
      },
      {
        "name" : "Carmela Troncoso"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02027v1",
    "title" : "Privacy Risks of General-Purpose AI Systems: A Foundation for\n  Investigating Practitioner Perspectives",
    "summary" : "The rise of powerful AI models, more formally $\\textit{General-Purpose AI\nSystems}$ (GPAIS), has led to impressive leaps in performance across a wide\nrange of tasks. At the same time, researchers and practitioners alike have\nraised a number of privacy concerns, resulting in a wealth of literature\ncovering various privacy risks and vulnerabilities of AI models. Works\nsurveying such risks provide differing focuses, leading to disparate sets of\nprivacy risks with no clear unifying taxonomy. We conduct a systematic review\nof these survey papers to provide a concise and usable overview of privacy\nrisks in GPAIS, as well as proposed mitigation strategies. The developed\nprivacy framework strives to unify the identified privacy risks and mitigations\nat a technical level that is accessible to non-experts. This serves as the\nbasis for a practitioner-focused interview study to assess technical\nstakeholder perceptions of privacy risks and mitigations in GPAIS.",
    "updated" : "2024-07-02T07:49:48Z",
    "published" : "2024-07-02T07:49:48Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Patrick Gage Kelley"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Kurt Thomas"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01817v1",
    "title" : "Race and Privacy in Broadcast Police Communications",
    "summary" : "Radios are essential for the operations of modern police departments, and\nthey function as both a collaborative communication technology and a\nsociotechnical system. However, little prior research has examined their usage\nor their connections to individual privacy and the role of race in policing,\ntwo growing topics of concern in the US. As a case study, we examine the\nChicago Police Department's (CPD's) use of broadcast police communications\n(BPC) to coordinate the activity of law enforcement officers (LEOs) in the\ncity. From a recently assembled archive of 80,775 hours of BPC associated with\nCPD operations, we analyze text transcripts of radio transmissions broadcast\n9:00 AM to 5:00 PM on August 10th, 2018 in one majority Black, one majority\nwhite, and one majority Hispanic area of the city (24 hours of audio) to\nexplore three research questions: (1) Do BPC reflect reported racial\ndisparities in policing? (2) How and when is gender, race/ethnicity, and age\nmentioned in BPC? (3) To what extent do BPC include sensitive information, and\nwho is put at most risk by this practice? (4) To what extent can large language\nmodels (LLMs) heighten this risk? We explore the vocabulary and speech acts\nused by police in BPC, comparing mentions of personal characteristics to local\ndemographics, the personal information shared over BPC, and the privacy\nconcerns that it poses. Analysis indicates (a) policing professionals in the\ncity of Chicago exhibit disproportionate attention to Black members of the\npublic regardless of context, (b) sociodemographic characteristics like gender,\nrace/ethnicity, and age are primarily mentioned in BPC about event information,\nand (c) disproportionate attention introduces disproportionate privacy risks\nfor Black members of the public.",
    "updated" : "2024-07-01T21:34:51Z",
    "published" : "2024-07-01T21:34:51Z",
    "authors" : [
      {
        "name" : "Pranav Narayanan Venkit"
      },
      {
        "name" : "Christopher Graziul"
      },
      {
        "name" : "Miranda Ardith Goodman"
      },
      {
        "name" : "Samantha Nicole Kenny"
      },
      {
        "name" : "Shomir Wilson"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01334v1",
    "title" : "Protecting Privacy in Classifiers by Token Manipulation",
    "summary" : "Using language models as a remote service entails sending private information\nto an untrusted provider. In addition, potential eavesdroppers can intercept\nthe messages, thereby exposing the information. In this work, we explore the\nprospects of avoiding such data exposure at the level of text manipulation. We\nfocus on text classification models, examining various token mapping and\ncontextualized manipulation functions in order to see whether classifier\naccuracy may be maintained while keeping the original text unrecoverable. We\nfind that although some token mapping functions are easy and straightforward to\nimplement, they heavily influence performance on the downstream task, and via a\nsophisticated attacker can be reconstructed. In comparison, the contextualized\nmanipulation provides an improvement in performance.",
    "updated" : "2024-07-01T14:41:59Z",
    "published" : "2024-07-01T14:41:59Z",
    "authors" : [
      {
        "name" : "Re'em Harel"
      },
      {
        "name" : "Yair Elboher"
      },
      {
        "name" : "Yuval Pinter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01167v1",
    "title" : "Information Density Bounds for Privacy",
    "summary" : "This paper explores the implications of guaranteeing privacy by imposing a\nlower bound on the information density between the private and the public data.\nWe introduce an operationally meaningful privacy measure called pointwise\nmaximal cost (PMC) and demonstrate that imposing an upper bound on PMC is\nequivalent to enforcing a lower bound on the information density. PMC\nquantifies the information leakage about a secret to adversaries who aim to\nminimize non-negative cost functions after observing the outcome of a privacy\nmechanism. When restricted to finite alphabets, PMC can equivalently be defined\nas the information leakage to adversaries aiming to minimize the probability of\nincorrectly guessing randomized functions of the secret. We study the\nproperties of PMC and apply it to standard privacy mechanisms to demonstrate\nits practical relevance. Through a detailed examination, we connect PMC with\nother privacy measures that impose upper or lower bounds on the information\ndensity. Our results highlight that lower bounding the information density is a\nmore stringent requirement than upper bounding it. Overall, our work\nsignificantly bridges the gaps in understanding the relationships between\nvarious privacy frameworks and provides insights for selecting a suitable\nframework for a given application.",
    "updated" : "2024-07-01T10:38:02Z",
    "published" : "2024-07-01T10:38:02Z",
    "authors" : [
      {
        "name" : "Sara Saeidian"
      },
      {
        "name" : "Leonhard Grosse"
      },
      {
        "name" : "Parastoo Sadeghi"
      },
      {
        "name" : "Mikael Skoglund"
      },
      {
        "name" : "Tobias J. Oechtering"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.00991v1",
    "title" : "Pre-capture Privacy via Adaptive Single-Pixel Imaging",
    "summary" : "As cameras become ubiquitous in our living environment, invasion of privacy\nis becoming a growing concern. A common approach to privacy preservation is to\nremove personally identifiable information from a captured image, but there is\na risk of the original image being leaked. In this paper, we propose a\npre-capture privacy-aware imaging method that captures images from which the\ndetails of a pre-specified anonymized target have been eliminated. The proposed\nmethod applies a single-pixel imaging framework in which we introduce a\nfeedback mechanism called an aperture pattern generator. The introduced\naperture pattern generator adaptively outputs the next aperture pattern to\navoid sampling the anonymized target by exploiting the data already acquired as\na clue. Furthermore, the anonymized target can be set to any object without\nchanging hardware. Except for detailed features which have been removed from\nthe anonymized target, the captured images are of comparable quality to those\ncaptured by a general camera and can be used for various computer vision\napplications. In our work, we target faces and license plates and\nexperimentally show that the proposed method can capture clear images in which\ndetailed features of the anonymized target are eliminated to achieve both\nprivacy and utility.",
    "updated" : "2024-07-01T06:05:12Z",
    "published" : "2024-07-01T06:05:12Z",
    "authors" : [
      {
        "name" : "Yoko Sogabe"
      },
      {
        "name" : "Shiori Sugimoto"
      },
      {
        "name" : "Ayumi Matsumoto"
      },
      {
        "name" : "Masaki Kitahara"
      }
    ],
    "categories" : [
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.00873v1",
    "title" : "Privacy-First Crowdsourcing: Blockchain and Local Differential Privacy\n  in Crowdsourced Drone Services",
    "summary" : "We introduce a privacy-preserving framework for integrating consumer-grade\ndrones into bushfire management. This system creates a marketplace where\nbushfire management authorities obtain essential data from drone operators. Key\nfeatures include local differential privacy to protect data providers and a\nblockchain-based solution ensuring fair data exchanges and accountability. The\nframework is validated through a proof-of-concept implementation, demonstrating\nits scalability and potential for various large-scale data collection\nscenarios. This approach addresses privacy concerns and compliance with\nregulations like Australia's Privacy Act 1988, offering a practical solution\nfor enhancing bushfire detection and management through crowdsourced drone\nservices.",
    "updated" : "2024-07-01T00:46:25Z",
    "published" : "2024-07-01T00:46:25Z",
    "authors" : [
      {
        "name" : "Junaid Akram"
      },
      {
        "name" : "Ali Anaissi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03289v1",
    "title" : "Correlated Privacy Mechanisms for Differentially Private Distributed\n  Mean Estimation",
    "summary" : "Differentially private distributed mean estimation (DP-DME) is a fundamental\nbuilding block in privacy-preserving federated learning, where a central server\nestimates the mean of $d$-dimensional vectors held by $n$ users while ensuring\n$(\\epsilon,\\delta)$-DP. Local differential privacy (LDP) and distributed DP\nwith secure aggregation (SecAgg) are the most common notions of DP used in\nDP-DME settings with an untrusted server. LDP provides strong resilience to\ndropouts, colluding users, and malicious server attacks, but suffers from poor\nutility. In contrast, SecAgg-based DP-DME achieves an $O(n)$ utility gain over\nLDP in DME, but requires increased communication and computation overheads and\ncomplex multi-round protocols to handle dropouts and malicious attacks. In this\nwork, we propose CorDP-DME, a novel DP-DME mechanism that spans the gap between\nDME with LDP and distributed DP, offering a favorable balance between utility\nand resilience to dropout and collusion. CorDP-DME is based on correlated\nGaussian noise, ensuring DP without the perfect conditional privacy guarantees\nof SecAgg-based approaches. We provide an information-theoretic analysis of\nCorDP-DME, and derive theoretical guarantees for utility under any given\nprivacy parameters and dropout/colluding user thresholds. Our results\ndemonstrate that (anti) correlated Gaussian DP mechanisms can significantly\nimprove utility in mean estimation tasks compared to LDP -- even in adversarial\nsettings -- while maintaining better resilience to dropouts and attacks\ncompared to distributed DP.",
    "updated" : "2024-07-03T17:22:33Z",
    "published" : "2024-07-03T17:22:33Z",
    "authors" : [
      {
        "name" : "Sajani Vithana"
      },
      {
        "name" : "Viveck R. Cadambe"
      },
      {
        "name" : "Flavio P. Calmon"
      },
      {
        "name" : "Haewon Jeong"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02956v1",
    "title" : "IncogniText: Privacy-enhancing Conditional Text Anonymization via\n  LLM-based Private Attribute Randomization",
    "summary" : "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%.\nFinally, we demonstrate the maturity of IncogniText for real-world applications\nby distilling its anonymization capability into a set of LoRA parameters\nassociated with an on-device model.",
    "updated" : "2024-07-03T09:49:03Z",
    "published" : "2024-07-03T09:49:03Z",
    "authors" : [
      {
        "name" : "Ahmed Frikha"
      },
      {
        "name" : "Nassim Walha"
      },
      {
        "name" : "Krishna Kanth Nakka"
      },
      {
        "name" : "Ricardo Mendes"
      },
      {
        "name" : "Xue Jiang"
      },
      {
        "name" : "Xuebing Zhou"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02766v1",
    "title" : "Balancing Patient Privacy and Health Data Security: The Role of\n  Compliance in Protected Health Information (PHI) Sharing",
    "summary" : "Protected Health Information (PHI) sharing significantly enhances patient\ncare quality and coordination, contributing to more accurate diagnoses,\nefficient treatment plans, and a comprehensive understanding of patient\nhistory. Compliance with strict privacy and security policies, such as those\nrequired by laws like HIPAA, is critical to protect PHI. Blockchain technology,\nwhich offers a decentralized and tamper-evident ledger system, hold promise in\npolicy compliance. This system ensures the authenticity and integrity of PHI\nwhile facilitating patient consent management. In this work, we propose a\nblockchain technology that integrates smart contracts to partially automate\nconsent-related processes and ensuring that PHI access and sharing follow\npatient preferences and legal requirements.",
    "updated" : "2024-07-03T02:49:33Z",
    "published" : "2024-07-03T02:49:33Z",
    "authors" : [
      {
        "name" : "Md Al Amin"
      },
      {
        "name" : "Hemanth Tummala"
      },
      {
        "name" : "Rushabh Shah"
      },
      {
        "name" : "Indrajit Ray"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02747v1",
    "title" : "Curvature Clues: Decoding Deep Learning Privacy with Input Loss\n  Curvature",
    "summary" : "In this paper, we explore the properties of loss curvature with respect to\ninput data in deep neural networks. Curvature of loss with respect to input\n(termed input loss curvature) is the trace of the Hessian of the loss with\nrespect to the input. We investigate how input loss curvature varies between\ntrain and test sets, and its implications for train-test distinguishability. We\ndevelop a theoretical framework that derives an upper bound on the train-test\ndistinguishability based on privacy and the size of the training set. This\nnovel insight fuels the development of a new black box membership inference\nattack utilizing input loss curvature. We validate our theoretical findings\nthrough experiments in computer vision classification tasks, demonstrating that\ninput loss curvature surpasses existing methods in membership inference\neffectiveness. Our analysis highlights how the performance of membership\ninference attack (MIA) methods varies with the size of the training set,\nshowing that curvature-based MIA outperforms other methods on sufficiently\nlarge datasets. This condition is often met by real datasets, as demonstrated\nby our results on CIFAR10, CIFAR100, and ImageNet. These findings not only\nadvance our understanding of deep neural network behavior but also improve the\nability to test privacy-preserving techniques in machine learning.",
    "updated" : "2024-07-03T01:47:46Z",
    "published" : "2024-07-03T01:47:46Z",
    "authors" : [
      {
        "name" : "Deepak Ravikumar"
      },
      {
        "name" : "Efstathia Soufleri"
      },
      {
        "name" : "Kaushik Roy"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02226v1",
    "title" : "RollupTheCrowd: Leveraging ZkRollups for a Scalable and\n  Privacy-Preserving Reputation-based Crowdsourcing Platform",
    "summary" : "Current blockchain-based reputation solutions for crowdsourcing fail to\ntackle the challenge of ensuring both efficiency and privacy without\ncompromising the scalability of the blockchain. Developing an effective,\ntransparent, and privacy-preserving reputation model necessitates on-chain\nimplementation using smart contracts. However, managing task evaluation and\nreputation updates alongside crowdsourcing transactions on-chain substantially\nstrains system scalability and performance. This paper introduces\nRollupTheCrowd, a novel blockchain-powered crowdsourcing framework that\nleverages zkRollups to enhance system scalability while protecting user\nprivacy. Our framework includes an effective and privacy-preserving reputation\nmodel that gauges workers' trustworthiness by assessing their crowdsourcing\ninteractions. To alleviate the load on our blockchain, we employ an off-chain\nstorage scheme, optimizing RollupTheCrowd's performance. Utilizing smart\ncontracts and zero-knowledge proofs, our Rollup layer achieves a significant\n20x reduction in gas consumption. To prove the feasibility of the proposed\nframework, we developed a proof-of-concept implementation using cutting-edge\ntools. The experimental results presented in this paper demonstrate the\neffectiveness and scalability of RollupTheCrowd, validating its potential for\nreal-world application scenarios.",
    "updated" : "2024-07-02T12:51:32Z",
    "published" : "2024-07-02T12:51:32Z",
    "authors" : [
      {
        "name" : "Ahmed Mounsf Rafik Bendada"
      },
      {
        "name" : "Mouhamed Amine Bouchiha"
      },
      {
        "name" : "Mourad Rabah"
      },
      {
        "name" : "Yacine Ghamri-Doudane"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01334v2",
    "title" : "Protecting Privacy in Classifiers by Token Manipulation",
    "summary" : "Using language models as a remote service entails sending private information\nto an untrusted provider. In addition, potential eavesdroppers can intercept\nthe messages, thereby exposing the information. In this work, we explore the\nprospects of avoiding such data exposure at the level of text manipulation. We\nfocus on text classification models, examining various token mapping and\ncontextualized manipulation functions in order to see whether classifier\naccuracy may be maintained while keeping the original text unrecoverable. We\nfind that although some token mapping functions are easy and straightforward to\nimplement, they heavily influence performance on the downstream task, and via a\nsophisticated attacker can be reconstructed. In comparison, the contextualized\nmanipulation provides an improvement in performance.",
    "updated" : "2024-07-03T16:31:52Z",
    "published" : "2024-07-01T14:41:59Z",
    "authors" : [
      {
        "name" : "Re'em Harel"
      },
      {
        "name" : "Yair Elboher"
      },
      {
        "name" : "Yuval Pinter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03732v1",
    "title" : "Collection, usage and privacy of mobility data in the enterprise and\n  public administrations",
    "summary" : "Human mobility data is a crucial resource for urban mobility management, but\nit does not come without personal reference. The implementation of security\nmeasures such as anonymization is thus needed to protect individuals' privacy.\nOften, a trade-off arises as such techniques potentially decrease the utility\nof the data and limit its use. While much research on anonymization techniques\nexists, there is little information on the actual implementations by\npractitioners, especially outside the big tech context. Within our study, we\nconducted expert interviews to gain insights into practices in the field. We\ncategorize purposes, data sources, analysis, and modeling tasks to provide a\nprofound understanding of the context such data is used in. We survey\nprivacy-enhancing methods in use, which generally do not comply with\nstate-of-the-art standards of differential privacy. We provide groundwork for\nfurther research on practice-oriented research by identifying privacy needs of\npractitioners and extracting relevant mobility characteristics for future\nstandardized evaluations of privacy-enhancing methods.",
    "updated" : "2024-07-04T08:29:27Z",
    "published" : "2024-07-04T08:29:27Z",
    "authors" : [
      {
        "name" : "Alexandra Kapp"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03496v1",
    "title" : "Releasing Large-Scale Human Mobility Histograms with Differential\n  Privacy",
    "summary" : "Environmental Insights Explorer (EIE) is a Google product that reports\naggregate statistics about human mobility, including various methods of transit\nused by people across roughly 50,000 regions globally. These statistics are\nused to estimate carbon emissions and provided to policymakers to inform their\ndecisions on transportation policy and infrastructure. Due to the inherent\nsensitivity of this type of user data, it is crucial that the statistics\nderived and released from it are computed with appropriate privacy protections.\nIn this work, we use a combination of federated analytics and differential\nprivacy to release these required statistics, while operating under strict\nerror constraints to ensure utility for downstream stakeholders. In this work,\nwe propose a new mechanism that achieves $ \\epsilon \\approx 2 $-DP while\nsatisfying these strict utility constraints, greatly improving over natural\nbaselines. We believe this mechanism may be of more general interest for the\nbroad class of group-by-sum workloads.",
    "updated" : "2024-07-03T20:54:00Z",
    "published" : "2024-07-03T20:54:00Z",
    "authors" : [
      {
        "name" : "Christopher Bian"
      },
      {
        "name" : "Albert Cheu"
      },
      {
        "name" : "Yannis Guzman"
      },
      {
        "name" : "Marco Gruteser"
      },
      {
        "name" : "Peter Kairouz"
      },
      {
        "name" : "Ryan McKenna"
      },
      {
        "name" : "Edo Roth"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03470v1",
    "title" : "Prosody-Driven Privacy-Preserving Dementia Detection",
    "summary" : "Speaker embeddings extracted from voice recordings have been proven valuable\nfor dementia detection. However, by their nature, these embeddings contain\nidentifiable information which raises privacy concerns. In this work, we aim to\nanonymize embeddings while preserving the diagnostic utility for dementia\ndetection. Previous studies rely on adversarial learning and models trained on\nthe target attribute and struggle in limited-resource settings. We propose a\nnovel approach that leverages domain knowledge to disentangle prosody features\nrelevant to dementia from speaker embeddings without relying on a dementia\nclassifier. Our experiments show the effectiveness of our approach in\npreserving speaker privacy (speaker recognition F1-score .01%) while\nmaintaining high dementia detection score F1-score of 74% on the ADReSS\ndataset. Our results are also on par with a more constrained\nclassifier-dependent system on ADReSSo (.01% and .66%), and have no impact on\nsynthesized speech naturalness.",
    "updated" : "2024-07-03T19:34:47Z",
    "published" : "2024-07-03T19:34:47Z",
    "authors" : [
      {
        "name" : "Dominika Woszczyk"
      },
      {
        "name" : "Ranya Aloufi"
      },
      {
        "name" : "Soteris Demetriou"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CL",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03451v1",
    "title" : "The Role of Privacy Guarantees in Voluntary Donation of Private Data for\n  Altruistic Goals",
    "summary" : "Voluntary donation of private information for altruistic purposes, such as\nadvancing research, is common. However, concerns about data misuse and leakage\nmay deter individuals from donating their information. While prior research has\nindicated that Privacy Enhancement Technologies (PETs) can alleviate these\nconcerns, the extent to which these techniques influence willingness to donate\ndata remains unclear.\n  This study conducts a vignette survey (N=485) to examine people's willingness\nto donate medical data for developing new treatments under four privacy\nguarantees: data expiration, anonymization, use restriction, and access\ncontrol. The study explores two mechanisms for verifying these guarantees:\nself-auditing and expert auditing, and evaluates the impact on two types of\ndata recipient entities: for-profit and non-profit institutions.\n  Our findings reveal that the type of entity collecting data strongly\ninfluences respondents' privacy expectations, which in part influence their\nwillingness to donate data. Respondents have such high expectations of the\nprivacy provided by non-profit entities that explicitly stating the privacy\nprotections provided makes little adjustment to those expectations. In\ncontrast, statements about privacy bring respondents' expectations of the\nprivacy provided by for-profit entities nearly in-line with non-profit\nexpectations. We highlight the risks of these respective results as well as the\nneed for future research to better align technical community and end-user\nperceptions about the effectiveness of auditing PETs and to effectively set\nexpectations about the efficacy of PETs in the face of end-user concerns about\ndata breaches.",
    "updated" : "2024-07-03T18:50:48Z",
    "published" : "2024-07-03T18:50:48Z",
    "authors" : [
      {
        "name" : "Ruizhe Wang"
      },
      {
        "name" : "Roberta De Viti"
      },
      {
        "name" : "Aarushi Dubey"
      },
      {
        "name" : "Elissa M. Redmiles"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05450v1",
    "title" : "Understanding Professional Needs to Create Privacy-Preserving and Secure\n  Emergent Digital Artworks",
    "summary" : "In recent years, immersive art installations featuring interactive artworks\nhave been on the rise. These installations are an integral part of museums and\nart centers like selfie museums, teamLab Borderless, ARTECHOUSE, and Meow Wolf.\nMoreover, immersive art have also been increasingly incorporated into\ntraditional museums as well. However, immersive art requires active user\nparticipation and often captures information from viewers and participants\nthrough cameras, sensors, microphones, embodied interaction devices,\nsurveillance, and kinetic mirrors. Therefore, we propose a new line of research\nto examine the security and privacy postures of immersive artworks. In our\npilot study, we conducted a semi-structured interview with five experienced\npractitioners from either the art (2) or cybersecurity (3) fields. Our aim was\nto understand their current security and privacy practices, along with their\nneeds when it comes to immersive art. From their responses, we created a list\nof security and privacy parameters, such as, providing opt-in mechanics for\ndata collection, knowledge of data collection tools such as proximity sensors,\nand creating security awareness amongst participants by communicating security\nprotocols and threat models. These parameters allow us to build\nprivacy-preserving, secure, and accessible software for individuals working in\nmedia arts, who often have no background on security and privacy. In the\nfuture, we plan to utilize these parameters to develop software in response to\nthose needs and then host an art exhibition of immersive artworks utilizing the\nplatform.",
    "updated" : "2024-07-07T17:21:38Z",
    "published" : "2024-07-07T17:21:38Z",
    "authors" : [
      {
        "name" : "Kathryn Lichlyter"
      },
      {
        "name" : "Urvashi Kishnani"
      },
      {
        "name" : "Kate Hollenbach"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05446v1",
    "title" : "Towards Perceived Security, Perceived Privacy, and the Universal Design\n  of E-Payment Applications",
    "summary" : "With the growth of digital monetary transactions and cashless payments,\nencouraged by the COVID-19 pandemic, use of e-payment applications is on the\nrise. It is thus imperative to understand and evaluate the current posture of\ne-payment applications from three major user-facing angles: security, privacy,\nand usability. To this, we created a high-fidelity prototype of an e-payment\napplication that encompassed features that we wanted to test with users. We\nthen conducted a pilot study where we recruited 12 participants who tested our\nprototype. We find that both security and privacy are important for users of\ne-payment applications. Additionally, some participants perceive the strength\nof security and privacy based on the usability of the application. We provide\nrecommendations such as universal design of e-payment applications.",
    "updated" : "2024-07-07T17:15:09Z",
    "published" : "2024-07-07T17:15:09Z",
    "authors" : [
      {
        "name" : "Urvashi Kishnani"
      },
      {
        "name" : "Isabella Cardenas"
      },
      {
        "name" : "Jailene Castillo"
      },
      {
        "name" : "Rosalyn Conry"
      },
      {
        "name" : "Lukas Rodwin"
      },
      {
        "name" : "Rika Ruiz"
      },
      {
        "name" : "Matthew Walther"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05237v1",
    "title" : "Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex\n  composite losses",
    "summary" : "Differentially private stochastic gradient descent (DP-SGD) refers to a\nfamily of optimization algorithms that provide a guaranteed level of\ndifferential privacy (DP) through DP accounting techniques. However, current\naccounting techniques make assumptions that diverge significantly from\npractical DP-SGD implementations. For example, they may assume the loss\nfunction is Lipschitz continuous and convex, sample the batches randomly with\nreplacement, or omit the gradient clipping step.\n  In this work, we analyze the most commonly used variant of DP-SGD, in which\nwe sample batches cyclically with replacement, perform gradient clipping, and\nonly release the last DP-SGD iterate. More specifically - without assuming\nconvexity, smoothness, or Lipschitz continuity of the loss function - we\nestablish new R\\'enyi differential privacy (RDP) bounds for the last DP-SGD\niterate under the mild assumption that (i) the DP-SGD stepsize is small\nrelative to the topological constants in the loss function, and (ii) the loss\nfunction is weakly-convex. Moreover, we show that our bounds converge to\npreviously established convex bounds when the weak-convexity parameter of the\nobjective function approaches zero. In the case of non-Lipschitz smooth loss\nfunctions, we provide a weaker bound that scales well in terms of the number of\nDP-SGD iterations.",
    "updated" : "2024-07-07T02:35:55Z",
    "published" : "2024-07-07T02:35:55Z",
    "authors" : [
      {
        "name" : "Weiwei Kong"
      },
      {
        "name" : "Mónica Ribero"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DS",
      "math.OC",
      "stat.ML",
      "65K10 (Primary), 60G15, 68P27",
      "G.3; G.1.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05045v1",
    "title" : "Robust Skin Color Driven Privacy Preserving Face Recognition via\n  Function Secret Sharing",
    "summary" : "In this work, we leverage the pure skin color patch from the face image as\nthe additional information to train an auxiliary skin color feature extractor\nand face recognition model in parallel to improve performance of\nstate-of-the-art (SOTA) privacy-preserving face recognition (PPFR) systems. Our\nsolution is robust against black-box attacking and well-established generative\nadversarial network (GAN) based image restoration. We analyze the potential\nrisk in previous work, where the proposed cosine similarity computation might\ndirectly leak the protected precomputed embedding stored on the server side. We\npropose a Function Secret Sharing (FSS) based face embedding comparison\nprotocol without any intermediate result leakage. In addition, we show in\nexperiments that the proposed protocol is more efficient compared to the Secret\nSharing (SS) based protocol.",
    "updated" : "2024-07-06T10:51:35Z",
    "published" : "2024-07-06T10:51:35Z",
    "authors" : [
      {
        "name" : "Dong Han"
      },
      {
        "name" : "Yufan Jiang"
      },
      {
        "name" : "Yong Li"
      },
      {
        "name" : "Ricardo Mendes"
      },
      {
        "name" : "Joachim Denzler"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04952v1",
    "title" : "Granular Privacy Control for Geolocation with Vision Language Models",
    "summary" : "Vision Language Models (VLMs) are rapidly advancing in their capability to\nanswer information-seeking questions. As these models are widely deployed in\nconsumer applications, they could lead to new privacy risks due to emergent\nabilities to identify people in photos, geolocate images, etc. As we\ndemonstrate, somewhat surprisingly, current open-source and proprietary VLMs\nare very capable image geolocators, making widespread geolocation with VLMs an\nimmediate privacy risk, rather than merely a theoretical future concern. As a\nfirst step to address this challenge, we develop a new benchmark, GPTGeoChat,\nto test the ability of VLMs to moderate geolocation dialogues with users. We\ncollect a set of 1,000 image geolocation conversations between in-house\nannotators and GPT-4v, which are annotated with the granularity of location\ninformation revealed at each turn. Using this new dataset, we evaluate the\nability of various VLMs to moderate GPT-4v geolocation conversations by\ndetermining when too much location information has been revealed. We find that\ncustom fine-tuned models perform on par with prompted API-based models when\nidentifying leaked location information at the country or city level; however,\nfine-tuning on supervised data appears to be needed to accurately moderate\nfiner granularities, such as the name of a restaurant or building.",
    "updated" : "2024-07-06T04:06:55Z",
    "published" : "2024-07-06T04:06:55Z",
    "authors" : [
      {
        "name" : "Ethan Mendes"
      },
      {
        "name" : "Yang Chen"
      },
      {
        "name" : "James Hays"
      },
      {
        "name" : "Sauvik Das"
      },
      {
        "name" : "Wei Xu"
      },
      {
        "name" : "Alan Ritter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04906v1",
    "title" : "Privacy or Transparency? Negotiated Smartphone Access as a Signifier of\n  Trust in Romantic Relationships",
    "summary" : "In this work, we analyze two large-scale surveys to examine how individuals\nthink about sharing smartphone access with romantic partners as a function of\ntrust in relationships. We find that the majority of couples have access to\neach others' devices, but may have explicit or implicit boundaries on how this\naccess is to be used. Investigating these boundaries and related social norms,\nwe find that there is little consensus about the level of smartphone access\n(i.e., transparency), or lack thereof (i.e., privacy) that is desirable in\nromantic contexts. However, there is broad agreement that the level of access\nshould be mutual and consensual. Most individuals understand trust to be the\nbasis of their decisions about transparency and privacy. Furthermore, we find\nindividuals have crossed these boundaries, violating their partners' privacy\nand betraying their trust. We examine how, when, why, and by whom these\nbetrayals occur. We consider the ramifications of these boundary violations in\nthe case of intimate partner violence. Finally, we provide recommendations for\ndesign changes to enable technological enforcement of boundaries currently\nenforced by trust, bringing access control in line with users' sharing\npreferences.",
    "updated" : "2024-07-06T00:52:34Z",
    "published" : "2024-07-06T00:52:34Z",
    "authors" : [
      {
        "name" : "Periwinkle Doerfler"
      },
      {
        "name" : "Kieron Ivy Turk"
      },
      {
        "name" : "Chris Geeng"
      },
      {
        "name" : "Damon McCoy"
      },
      {
        "name" : "Jeffrey Ackerman"
      },
      {
        "name" : "Molly Dragiewicz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04776v1",
    "title" : "Quantifying Privacy Risks of Public Statistics to Residents of\n  Subsidized Housing",
    "summary" : "As the U.S. Census Bureau implements its controversial new disclosure\navoidance system, researchers and policymakers debate the necessity of new\nprivacy protections for public statistics. With experiments on both published\nstatistics and synthetic data, we explore a particular privacy concern:\nrespondents in subsidized housing may deliberately not mention unauthorized\nchildren and other household members for fear of being evicted. By combining\npublic statistics from the Decennial Census and the Department of Housing and\nUrban Development, we demonstrate a simple, inexpensive reconstruction attack\nthat could identify subsidized households living in violation of occupancy\nguidelines in 2010. Experiments on synthetic data suggest that a random\nswapping mechanism similar to the Census Bureau's 2010 disclosure avoidance\nmeasures does not significantly reduce the precision of this attack, while a\ndifferentially private mechanism similar to the 2020 disclosure avoidance\nsystem does. Our results provide a valuable example for policymakers seeking a\ntrustworthy, accurate census.",
    "updated" : "2024-07-05T18:00:02Z",
    "published" : "2024-07-05T18:00:02Z",
    "authors" : [
      {
        "name" : "Ryan Steed"
      },
      {
        "name" : "Diana Qing"
      },
      {
        "name" : "Zhiwei Steven Wu"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04751v1",
    "title" : "A Unified Learn-to-Distort-Data Framework for Privacy-Utility Trade-off\n  in Trustworthy Federated Learning",
    "summary" : "In this paper, we first give an introduction to the theoretical basis of the\nprivacy-utility equilibrium in federated learning based on Bayesian privacy\ndefinitions and total variation distance privacy definitions. We then present\nthe \\textit{Learn-to-Distort-Data} framework, which provides a principled\napproach to navigate the privacy-utility equilibrium by explicitly modeling the\ndistortion introduced by the privacy-preserving mechanism as a learnable\nvariable and optimizing it jointly with the model parameters. We demonstrate\nthe applicability of our framework to a variety of privacy-preserving\nmechanisms on the basis of data distortion and highlight its connections to\nrelated areas such as adversarial training, input robustness, and unlearnable\nexamples. These connections enable leveraging techniques from these areas to\ndesign effective algorithms for privacy-utility equilibrium in federated\nlearning under the \\textit{Learn-to-Distort-Data} framework.",
    "updated" : "2024-07-05T08:15:09Z",
    "published" : "2024-07-05T08:15:09Z",
    "authors" : [
      {
        "name" : "Xiaojin Zhang"
      },
      {
        "name" : "Mingcong Xu"
      },
      {
        "name" : "Wei Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.07066v1",
    "title" : "Explainable Hyperdimensional Computing for Balancing Privacy and\n  Transparency in Additive Manufacturing Monitoring",
    "summary" : "In-situ sensing, in conjunction with learning models, presents a unique\nopportunity to address persistent defect issues in Additive Manufacturing (AM)\nprocesses. However, this integration introduces significant data privacy\nconcerns, such as data leakage, sensor data compromise, and model inversion\nattacks, revealing critical details about part design, material composition,\nand machine parameters. Differential Privacy (DP) models, which inject noise\ninto data under mathematical guarantees, offer a nuanced balance between data\nutility and privacy by obscuring traces of sensing data. However, the\nintroduction of noise into learning models, often functioning as black boxes,\ncomplicates the prediction of how specific noise levels impact model accuracy.\nThis study introduces the Differential Privacy-HyperDimensional computing\n(DP-HD) framework, leveraging the explainability of the vector symbolic\nparadigm to predict the noise impact on the accuracy of in-situ monitoring,\nsafeguarding sensitive data while maintaining operational efficiency.\nExperimental results on real-world high-speed melt pool data of AM for\ndetecting overhang anomalies demonstrate that DP-HD achieves superior\noperational efficiency, prediction accuracy, and robust privacy protection,\noutperforming state-of-the-art Machine Learning (ML) models. For example, when\nimplementing the same level of privacy protection (with a privacy budget set at\n1), our model achieved an accuracy of 94.43\\%, surpassing the performance of\ntraditional models such as ResNet50 (52.30\\%), GoogLeNet (23.85\\%), AlexNet\n(55.78\\%), DenseNet201 (69.13\\%), and EfficientNet B2 (40.81\\%). Notably, DP-HD\nmaintains high performance under substantial noise additions designed to\nenhance privacy, unlike current models that suffer significant accuracy\ndeclines under high privacy constraints.",
    "updated" : "2024-07-09T17:42:26Z",
    "published" : "2024-07-09T17:42:26Z",
    "authors" : [
      {
        "name" : "Fardin Jalil Piran"
      },
      {
        "name" : "Prathyush P. Poduval"
      },
      {
        "name" : "Hamza Errahmouni Barkam"
      },
      {
        "name" : "Mohsen Imani"
      },
      {
        "name" : "Farhad Imani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.06778v1",
    "title" : "A BERT-based Empirical Study of Privacy Policies' Compliance with GDPR",
    "summary" : "Since its implementation in May 2018, the General Data Protection Regulation\n(GDPR) has prompted businesses to revisit and revise their data handling\npractices to ensure compliance. The privacy policy, which serves as the primary\nmeans of informing users about their privacy rights and the data practices of\ncompanies, has been significantly updated by numerous businesses post-GDPR\nimplementation. However, many privacy policies remain packed with technical\njargon, lengthy explanations, and vague descriptions of data practices and user\nrights. This makes it a challenging task for users and regulatory authorities\nto manually verify the GDPR compliance of these privacy policies. In this\nstudy, we aim to address the challenge of compliance analysis between GDPR\n(Article 13) and privacy policies for 5G networks. We manually collected\nprivacy policies from almost 70 different 5G MNOs, and we utilized an automated\nBERT-based model for classification. We show that an encouraging 51$\\%$ of\ncompanies demonstrate a strong adherence to GDPR. In addition, we present the\nfirst study that provides current empirical evidence on the readability of\nprivacy policies for 5G network. we adopted readability analysis toolset that\nincorporates various established readability metrics. The findings empirically\nshow that the readability of the majority of current privacy policies remains a\nsignificant challenge. Hence, 5G providers need to invest considerable effort\ninto revising these documents to enhance both their utility and the overall\nuser experience.",
    "updated" : "2024-07-09T11:47:52Z",
    "published" : "2024-07-09T11:47:52Z",
    "authors" : [
      {
        "name" : "Lu Zhang"
      },
      {
        "name" : "Nabil Moukafih"
      },
      {
        "name" : "Hamad Alamri"
      },
      {
        "name" : "Gregory Epiphaniou"
      },
      {
        "name" : "Carsten Maple"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.06496v1",
    "title" : "It's Our Loss: No Privacy Amplification for Hidden State DP-SGD With\n  Non-Convex Loss",
    "summary" : "Differentially Private Stochastic Gradient Descent (DP-SGD) is a popular\niterative algorithm used to train machine learning models while formally\nguaranteeing the privacy of users. However the privacy analysis of DP-SGD makes\nthe unrealistic assumption that all intermediate iterates (aka internal state)\nof the algorithm are released since in practice, only the final trained model,\ni.e., the final iterate of the algorithm is released. In this hidden state\nsetting, prior work has provided tighter analyses, albeit only when the loss\nfunction is constrained, e.g., strongly convex and smooth or linear. On the\nother hand, the privacy leakage observed empirically from hidden state DP-SGD,\neven when using non-convex loss functions suggest that there is in fact a gap\nbetween the theoretical privacy analysis and the privacy guarantees achieved in\npractice. Therefore, it remains an open question whether privacy amplification\nfor DP-SGD is possible in the hidden state setting for general loss functions.\n  Unfortunately, this work answers the aforementioned research question\nnegatively. By carefully constructing a loss function for DP-SGD, we show that\nfor specific loss functions, the final iterate of DP-SGD alone leaks as much\ninformation as the sequence of all iterates combined. Furthermore, we\nempirically verify this result by evaluating the privacy leakage from the final\niterate of DP-SGD with our loss function and show that this matches the\ntheoretical upper bound guaranteed by DP exactly. Therefore, we show that the\ncurrent privacy analysis fo DP-SGD is tight for general loss functions and\nconclude that no privacy amplification is possible for DP-SGD in general for\nall (possibly non-convex) loss functions.",
    "updated" : "2024-07-09T01:58:19Z",
    "published" : "2024-07-09T01:58:19Z",
    "authors" : [
      {
        "name" : "Meenatchi Sundaram Muthu Selva Annamalai"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.06495v1",
    "title" : "Impact Evaluation on the European Privacy Laws governing generative-AI\n  models -- Evidence in Relation between Internet Censorship and the Ban of\n  ChatGPT in Italy",
    "summary" : "We proceed an impact evaluation on the European Privacy Laws governing\ngenerative-AI models, especially, focusing on the effects of the Ban of ChatGPT\nin Italy. We investigate on the causal relationship between Internet Censorship\nData and the Ban of ChatGPT in Italy during the period from March 27, 2023 to\nApril 11, 2023. We analyze the relation based on the hidden Markov model with\nPoisson emissions. We find out that the HTTP Invalid Requests, which decreased\nduring those period, can be explained with seven-state model. Our findings\nshows the apparent inability for the users in the internet accesses as a result\nof EU regulations on the generative-AI.",
    "updated" : "2024-07-09T01:56:42Z",
    "published" : "2024-07-09T01:56:42Z",
    "authors" : [
      {
        "name" : "Tatsuru Kikuchi"
      }
    ],
    "categories" : [
      "econ.GN",
      "q-fin.EC",
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.06443v1",
    "title" : "Exposing Privacy Gaps: Membership Inference Attack on Preference Data\n  for LLM Alignment",
    "summary" : "Large Language Models (LLMs) have seen widespread adoption due to their\nremarkable natural language capabilities. However, when deploying them in\nreal-world settings, it is important to align LLMs to generate texts according\nto acceptable human standards. Methods such as Proximal Policy Optimization\n(PPO) and Direct Preference Optimization (DPO) have made significant progress\nin refining LLMs using human preference data. However, the privacy concerns\ninherent in utilizing such preference data have yet to be adequately studied.\nIn this paper, we investigate the vulnerability of LLMs aligned using human\npreference datasets to membership inference attacks (MIAs), highlighting the\nshortcomings of previous MIA approaches with respect to preference data. Our\nstudy has two main contributions: first, we introduce a novel reference-based\nattack framework specifically for analyzing preference data called PREMIA\n(\\uline{Pre}ference data \\uline{MIA}); second, we provide empirical evidence\nthat DPO models are more vulnerable to MIA compared to PPO models. Our findings\nhighlight gaps in current privacy-preserving practices for LLM alignment.",
    "updated" : "2024-07-08T22:53:23Z",
    "published" : "2024-07-08T22:53:23Z",
    "authors" : [
      {
        "name" : "Qizhang Feng"
      },
      {
        "name" : "Siva Rajesh Kasa"
      },
      {
        "name" : "Hyokun Yun"
      },
      {
        "name" : "Choon Hui Teo"
      },
      {
        "name" : "Sravan Babu Bodapati"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04751v2",
    "title" : "A Unified Learn-to-Distort-Data Framework for Privacy-Utility Trade-off\n  in Trustworthy Federated Learning",
    "summary" : "In this paper, we first give an introduction to the theoretical basis of the\nprivacy-utility equilibrium in federated learning based on Bayesian privacy\ndefinitions and total variation distance privacy definitions. We then present\nthe \\textit{Learn-to-Distort-Data} framework, which provides a principled\napproach to navigate the privacy-utility equilibrium by explicitly modeling the\ndistortion introduced by the privacy-preserving mechanism as a learnable\nvariable and optimizing it jointly with the model parameters. We demonstrate\nthe applicability of our framework to a variety of privacy-preserving\nmechanisms on the basis of data distortion and highlight its connections to\nrelated areas such as adversarial training, input robustness, and unlearnable\nexamples. These connections enable leveraging techniques from these areas to\ndesign effective algorithms for privacy-utility equilibrium in federated\nlearning under the \\textit{Learn-to-Distort-Data} framework.",
    "updated" : "2024-07-09T16:11:04Z",
    "published" : "2024-07-05T08:15:09Z",
    "authors" : [
      {
        "name" : "Xiaojin Zhang"
      },
      {
        "name" : "Mingcong Xu"
      },
      {
        "name" : "Wei Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.07737v1",
    "title" : "Fine-Tuning Large Language Models with User-Level Differential Privacy",
    "summary" : "We investigate practical and scalable algorithms for training large language\nmodels (LLMs) with user-level differential privacy (DP) in order to provably\nsafeguard all the examples contributed by each user. We study two variants of\nDP-SGD with: (1) example-level sampling (ELS) and per-example gradient\nclipping, and (2) user-level sampling (ULS) and per-user gradient clipping. We\nderive a novel user-level DP accountant that allows us to compute provably\ntight privacy guarantees for ELS. Using this, we show that while ELS can\noutperform ULS in specific settings, ULS generally yields better results when\neach user has a diverse collection of examples. We validate our findings\nthrough experiments in synthetic mean estimation and LLM fine-tuning tasks\nunder fixed compute budgets. We find that ULS is significantly better in\nsettings where either (1) strong privacy guarantees are required, or (2) the\ncompute budget is large. Notably, our focus on LLM-compatible training\nalgorithms allows us to scale to models with hundreds of millions of parameters\nand datasets with hundreds of thousands of users.",
    "updated" : "2024-07-10T15:07:58Z",
    "published" : "2024-07-10T15:07:58Z",
    "authors" : [
      {
        "name" : "Zachary Charles"
      },
      {
        "name" : "Arun Ganesh"
      },
      {
        "name" : "Ryan McKenna"
      },
      {
        "name" : "H. Brendan McMahan"
      },
      {
        "name" : "Nicole Mitchell"
      },
      {
        "name" : "Krishna Pillutla"
      },
      {
        "name" : "Keith Rush"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.07262v1",
    "title" : "Differential privacy and Sublinear time are incompatible sometimes",
    "summary" : "Differential privacy and sublinear algorithms are both rapidly emerging\nalgorithmic themes in times of big data analysis. Although recent works have\nshown the existence of differentially private sublinear algorithms for many\nproblems including graph parameter estimation and clustering, little is known\nregarding hardness results on these algorithms. In this paper, we initiate the\nstudy of lower bounds for problems that aim for both differentially-private and\nsublinear-time algorithms. Our main result is the incompatibility of both the\ndesiderata in the general case. In particular, we prove that a simple problem\nbased on one-way marginals yields both a differentially-private algorithm, as\nwell as a sublinear-time algorithm, but does not admit a ``strictly''\nsublinear-time algorithm that is also differentially private.",
    "updated" : "2024-07-09T22:33:57Z",
    "published" : "2024-07-09T22:33:57Z",
    "authors" : [
      {
        "name" : "Jeremiah Blocki"
      },
      {
        "name" : "Hendrik Fichtenberger"
      },
      {
        "name" : "Elena Grigorescu"
      },
      {
        "name" : "Tamalika Mukherjee"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.07066v2",
    "title" : "Explainable Hyperdimensional Computing for Balancing Privacy and\n  Transparency in Additive Manufacturing Monitoring",
    "summary" : "In-situ sensing, in conjunction with learning models, presents a unique\nopportunity to address persistent defect issues in Additive Manufacturing (AM)\nprocesses. However, this integration introduces significant data privacy\nconcerns, such as data leakage, sensor data compromise, and model inversion\nattacks, revealing critical details about part design, material composition,\nand machine parameters. Differential Privacy (DP) models, which inject noise\ninto data under mathematical guarantees, offer a nuanced balance between data\nutility and privacy by obscuring traces of sensing data. However, the\nintroduction of noise into learning models, often functioning as black boxes,\ncomplicates the prediction of how specific noise levels impact model accuracy.\nThis study introduces the Differential Privacy-HyperDimensional computing\n(DP-HD) framework, leveraging the explainability of the vector symbolic\nparadigm to predict the noise impact on the accuracy of in-situ monitoring,\nsafeguarding sensitive data while maintaining operational efficiency.\nExperimental results on real-world high-speed melt pool data of AM for\ndetecting overhang anomalies demonstrate that DP-HD achieves superior\noperational efficiency, prediction accuracy, and robust privacy protection,\noutperforming state-of-the-art Machine Learning (ML) models. For example, when\nimplementing the same level of privacy protection (with a privacy budget set at\n1), our model achieved an accuracy of 94.43%, surpassing the performance of\ntraditional models such as ResNet50 (52.30%), GoogLeNet (23.85%), AlexNet\n(55.78%), DenseNet201 (69.13%), and EfficientNet B2 (40.81%). Notably, DP-HD\nmaintains high performance under substantial noise additions designed to\nenhance privacy, unlike current models that suffer significant accuracy\ndeclines under high privacy constraints.",
    "updated" : "2024-07-10T01:37:05Z",
    "published" : "2024-07-09T17:42:26Z",
    "authors" : [
      {
        "name" : "Fardin Jalil Piran"
      },
      {
        "name" : "Prathyush P. Poduval"
      },
      {
        "name" : "Hamza Errahmouni Barkam"
      },
      {
        "name" : "Mohsen Imani"
      },
      {
        "name" : "Farhad Imani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08529v1",
    "title" : "Enhancing Privacy of Spatiotemporal Federated Learning against Gradient\n  Inversion Attacks",
    "summary" : "Spatiotemporal federated learning has recently raised intensive studies due\nto its ability to train valuable models with only shared gradients in various\nlocation-based services. On the other hand, recent studies have shown that\nshared gradients may be subject to gradient inversion attacks (GIA) on images\nor texts. However, so far there has not been any systematic study of the\ngradient inversion attacks in spatiotemporal federated learning. In this paper,\nwe explore the gradient attack problem in spatiotemporal federated learning\nfrom attack and defense perspectives. To understand privacy risks in\nspatiotemporal federated learning, we first propose Spatiotemporal Gradient\nInversion Attack (ST-GIA), a gradient attack algorithm tailored to\nspatiotemporal data that successfully reconstructs the original location from\ngradients. Furthermore, we design an adaptive defense strategy to mitigate\ngradient inversion attacks in spatiotemporal federated learning. By dynamically\nadjusting the perturbation levels, we can offer tailored protection for varying\nrounds of training data, thereby achieving a better trade-off between privacy\nand utility than current state-of-the-art methods. Through intensive\nexperimental analysis on three real-world datasets, we reveal that the proposed\ndefense strategy can well preserve the utility of spatiotemporal federated\nlearning with effective security protection.",
    "updated" : "2024-07-11T14:17:02Z",
    "published" : "2024-07-11T14:17:02Z",
    "authors" : [
      {
        "name" : "Lele Zheng"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Renhe Jiang"
      },
      {
        "name" : "Kenjiro Taura"
      },
      {
        "name" : "Yulong Shen"
      },
      {
        "name" : "Sheng Li"
      },
      {
        "name" : "Masatoshi Yoshikawa"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08152v1",
    "title" : "Privacy-Preserving Data Deduplication for Enhancing Federated Learning\n  of Language Models",
    "summary" : "Deduplication is a vital preprocessing step that enhances machine learning\nmodel performance and saves training time and energy. However, enhancing\nfederated learning through deduplication poses challenges, especially regarding\nscalability and potential privacy violations if deduplication involves sharing\nall clients' data. In this paper, we address the problem of deduplication in a\nfederated setup by introducing a pioneering protocol, Efficient\nPrivacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes\nduplicates from multiple clients' datasets without compromising data privacy.\nEP-MPD is constructed in a modular fashion, utilizing two novel variants of the\nPrivate Set Intersection protocol. Our extensive experiments demonstrate the\nsignificant benefits of deduplication in federated learning of large language\nmodels. For instance, we observe up to 19.61% improvement in perplexity and up\nto 27.95% reduction in running time. EP-MPD effectively balances privacy and\nperformance in federated learning, making it a valuable solution for\nlarge-scale applications.",
    "updated" : "2024-07-11T03:10:27Z",
    "published" : "2024-07-11T03:10:27Z",
    "authors" : [
      {
        "name" : "Aydin Abadi"
      },
      {
        "name" : "Vishnu Asutosh Dasu"
      },
      {
        "name" : "Sumanta Sarkar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.07926v1",
    "title" : "Synthetic Data: Revisiting the Privacy-Utility Trade-off",
    "summary" : "Synthetic data has been considered a better privacy-preserving alternative to\ntraditionally sanitized data across various applications. However, a recent\narticle challenges this notion, stating that synthetic data does not provide a\nbetter trade-off between privacy and utility than traditional anonymization\ntechniques, and that it leads to unpredictable utility loss and highly\nunpredictable privacy gain. The article also claims to have identified a breach\nin the differential privacy guarantees provided by PATEGAN and PrivBayes. When\na study claims to refute or invalidate prior findings, it is crucial to verify\nand validate the study. In our work, we analyzed the implementation of the\nprivacy game described in the article and found that it operated in a highly\nspecialized and constrained environment, which limits the applicability of its\nfindings to general cases. Our exploration also revealed that the game did not\nsatisfy a crucial precondition concerning data distributions, which contributed\nto the perceived violation of the differential privacy guarantees offered by\nPATEGAN and PrivBayes. We also conducted a privacy-utility trade-off analysis\nin a more general and unconstrained environment. Our experimentation\ndemonstrated that synthetic data achieves a more favorable privacy-utility\ntrade-off compared to the provided implementation of k-anonymization, thereby\nreaffirming earlier conclusions.",
    "updated" : "2024-07-09T14:48:43Z",
    "published" : "2024-07-09T14:48:43Z",
    "authors" : [
      {
        "name" : "Fatima Jahan Sarmin"
      },
      {
        "name" : "Atiquer Rahman Sarkar"
      },
      {
        "name" : "Yang Wang"
      },
      {
        "name" : "Noman Mohammed"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09324v1",
    "title" : "Provable Privacy Advantages of Decentralized Federated Learning via\n  Distributed Optimization",
    "summary" : "Federated learning (FL) emerged as a paradigm designed to improve data\nprivacy by enabling data to reside at its source, thus embedding privacy as a\ncore consideration in FL architectures, whether centralized or decentralized.\nContrasting with recent findings by Pasquini et al., which suggest that\ndecentralized FL does not empirically offer any additional privacy or security\nbenefits over centralized models, our study provides compelling evidence to the\ncontrary. We demonstrate that decentralized FL, when deploying distributed\noptimization, provides enhanced privacy protection - both theoretically and\nempirically - compared to centralized approaches. The challenge of quantifying\nprivacy loss through iterative processes has traditionally constrained the\ntheoretical exploration of FL protocols. We overcome this by conducting a\npioneering in-depth information-theoretical privacy analysis for both\nframeworks. Our analysis, considering both eavesdropping and passive adversary\nmodels, successfully establishes bounds on privacy leakage. We show information\ntheoretically that the privacy loss in decentralized FL is upper bounded by the\nloss in centralized FL. Compared to the centralized case where local gradients\nof individual participants are directly revealed, a key distinction of\noptimization-based decentralized FL is that the relevant information includes\ndifferences of local gradients over successive iterations and the aggregated\nsum of different nodes' gradients over the network. This information\ncomplicates the adversary's attempt to infer private data. To bridge our\ntheoretical insights with practical applications, we present detailed case\nstudies involving logistic regression and deep neural networks. These examples\ndemonstrate that while privacy leakage remains comparable in simpler models,\ncomplex models like deep neural networks exhibit lower privacy risks under\ndecentralized FL.",
    "updated" : "2024-07-12T15:01:09Z",
    "published" : "2024-07-12T15:01:09Z",
    "authors" : [
      {
        "name" : "Wenrui Yu"
      },
      {
        "name" : "Qiongxiu Li"
      },
      {
        "name" : "Milan Lopuhaä-Zwakenberg"
      },
      {
        "name" : "Mads Græsbøll Christensen"
      },
      {
        "name" : "Richard Heusdens"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09239v1",
    "title" : "FedVAE: Trajectory privacy preserving based on Federated Variational\n  AutoEncoder",
    "summary" : "The use of trajectory data with abundant spatial-temporal information is\npivotal in Intelligent Transport Systems (ITS) and various traffic system\ntasks. Location-Based Services (LBS) capitalize on this trajectory data to\noffer users personalized services tailored to their location information.\nHowever, this trajectory data contains sensitive information about users'\nmovement patterns and habits, necessitating confidentiality and protection from\nunknown collectors. To address this challenge, privacy-preserving methods like\nK-anonymity and Differential Privacy have been proposed to safeguard private\ninformation in the dataset. Despite their effectiveness, these methods can\nimpact the original features by introducing perturbations or generating\nunrealistic trajectory data, leading to suboptimal performance in downstream\ntasks. To overcome these limitations, we propose a Federated Variational\nAutoEncoder (FedVAE) approach, which effectively generates a new trajectory\ndataset while preserving the confidentiality of private information and\nretaining the structure of the original features. In addition, FedVAE leverages\nVariational AutoEncoder (VAE) to maintain the original feature space and\ngenerate new trajectory data, and incorporates Federated Learning (FL) during\nthe training stage, ensuring that users' data remains locally stored to protect\ntheir personal information. The results demonstrate its superior performance\ncompared to other existing methods, affirming FedVAE as a promising solution\nfor enhancing data privacy and utility in location-based applications.",
    "updated" : "2024-07-12T13:10:59Z",
    "published" : "2024-07-12T13:10:59Z",
    "authors" : [
      {
        "name" : "Yuchen Jiang"
      },
      {
        "name" : "Ying Wu"
      },
      {
        "name" : "Shiyao Zhang"
      },
      {
        "name" : "James J. Q. Yu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09004v1",
    "title" : "Privacy-Preserving Collaborative Genomic Research: A Real-Life\n  Deployment and Vision",
    "summary" : "The data revolution holds significant promise for the health sector. Vast\namounts of data collected from individuals will be transformed into knowledge,\nAI models, predictive systems, and best practices. One area of health that\nstands to benefit greatly is the genomic domain. Progress in AI, machine\nlearning, and data science has opened new opportunities for genomic research,\npromising breakthroughs in personalized medicine. However, increasing awareness\nof privacy and cybersecurity necessitates robust solutions to protect sensitive\ndata in collaborative research. This paper presents a practical deployment of a\nprivacy-preserving framework for genomic research, developed in collaboration\nwith Lynx.MD, a platform for secure health data collaboration. The framework\naddresses critical cybersecurity and privacy challenges, enabling the\nprivacy-preserving sharing and analysis of genomic data while mitigating risks\nassociated with data breaches. By integrating advanced privacy-preserving\nalgorithms, the solution ensures the protection of individual privacy without\ncompromising data utility. A unique feature of the system is its ability to\nbalance trade-offs between data sharing and privacy, providing stakeholders\ntools to quantify privacy risks and make informed decisions. Implementing the\nframework within Lynx.MD involves encoding genomic data into binary formats and\napplying noise through controlled perturbation techniques. This approach\npreserves essential statistical properties of the data, facilitating effective\nresearch and analysis. Moreover, the system incorporates real-time data\nmonitoring and advanced visualization tools, enhancing user experience and\ndecision-making. The paper highlights the need for tailored privacy attacks and\ndefenses specific to genomic data. Addressing these challenges fosters\ncollaboration in genomic research, advancing personalized medicine and public\nhealth.",
    "updated" : "2024-07-12T05:43:13Z",
    "published" : "2024-07-12T05:43:13Z",
    "authors" : [
      {
        "name" : "Zahra Rahmani"
      },
      {
        "name" : "Nahal Shahini"
      },
      {
        "name" : "Nadav Gat"
      },
      {
        "name" : "Zebin Yun"
      },
      {
        "name" : "Yuzhou Jiang"
      },
      {
        "name" : "Ofir Farchy"
      },
      {
        "name" : "Yaniv Harel"
      },
      {
        "name" : "Vipin Chaudhary"
      },
      {
        "name" : "Mahmood Sharif"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08977v1",
    "title" : "CURE: Privacy-Preserving Split Learning Done Right",
    "summary" : "Training deep neural networks often requires large-scale datasets,\nnecessitating storage and processing on cloud servers due to computational\nconstraints. The procedures must follow strict privacy regulations in domains\nlike healthcare. Split Learning (SL), a framework that divides model layers\nbetween client(s) and server(s), is widely adopted for distributed model\ntraining. While Split Learning reduces privacy risks by limiting server access\nto the full parameter set, previous research has identified that intermediate\noutputs exchanged between server and client can compromise client's data\nprivacy. Homomorphic encryption (HE)-based solutions exist for this scenario\nbut often impose prohibitive computational burdens.\n  To address these challenges, we propose CURE, a novel system based on HE,\nthat encrypts only the server side of the model and optionally the data. CURE\nenables secure SL while substantially improving communication and\nparallelization through advanced packing techniques. We propose two packing\nschemes that consume one HE level for one-layer networks and generalize our\nsolutions to n-layer neural networks. We demonstrate that CURE can achieve\nsimilar accuracy to plaintext SL while being 16x more efficient in terms of the\nruntime compared to the state-of-the-art privacy-preserving alternatives.",
    "updated" : "2024-07-12T04:10:19Z",
    "published" : "2024-07-12T04:10:19Z",
    "authors" : [
      {
        "name" : "Halil Ibrahim Kanpak"
      },
      {
        "name" : "Aqsa Shabbir"
      },
      {
        "name" : "Esra Genç"
      },
      {
        "name" : "Alptekin Küpçü"
      },
      {
        "name" : "Sinem Sav"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08954v1",
    "title" : "PriRoAgg: Achieving Robust Model Aggregation with Minimum Privacy\n  Leakage for Federated Learning",
    "summary" : "Federated learning (FL) has recently gained significant momentum due to its\npotential to leverage large-scale distributed user data while preserving user\nprivacy. However, the typical paradigm of FL faces challenges of both privacy\nand robustness: the transmitted model updates can potentially leak sensitive\nuser information, and the lack of central control of the local training process\nleaves the global model susceptible to malicious manipulations on model\nupdates. Current solutions attempting to address both problems under the\none-server FL setting fall short in the following aspects: 1) designed for\nsimple validity checks that are insufficient against advanced attacks (e.g.,\nchecking norm of individual update); and 2) partial privacy leakage for more\ncomplicated robust aggregation algorithms (e.g., distances between model\nupdates are leaked for multi-Krum). In this work, we formalize a novel security\nnotion of aggregated privacy that characterizes the minimum amount of user\ninformation, in the form of some aggregated statistics of users' updates, that\nis necessary to be revealed to accomplish more advanced robust aggregation. We\ndevelop a general framework PriRoAgg, utilizing Lagrange coded computing and\ndistributed zero-knowledge proof, to execute a wide range of robust aggregation\nalgorithms while satisfying aggregated privacy. As concrete instantiations of\nPriRoAgg, we construct two secure and robust protocols based on\nstate-of-the-art robust algorithms, for which we provide full theoretical\nanalyses on security and complexity. Extensive experiments are conducted for\nthese protocols, demonstrating their robustness against various model integrity\nattacks, and their efficiency advantages over baselines.",
    "updated" : "2024-07-12T03:18:08Z",
    "published" : "2024-07-12T03:18:08Z",
    "authors" : [
      {
        "name" : "Sizai Hou"
      },
      {
        "name" : "Songze Li"
      },
      {
        "name" : "Tayyebeh Jahani-Nezhad"
      },
      {
        "name" : "Giuseppe Caire"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08529v2",
    "title" : "Enhancing Privacy of Spatiotemporal Federated Learning against Gradient\n  Inversion Attacks",
    "summary" : "Spatiotemporal federated learning has recently raised intensive studies due\nto its ability to train valuable models with only shared gradients in various\nlocation-based services. On the other hand, recent studies have shown that\nshared gradients may be subject to gradient inversion attacks (GIA) on images\nor texts. However, so far there has not been any systematic study of the\ngradient inversion attacks in spatiotemporal federated learning. In this paper,\nwe explore the gradient attack problem in spatiotemporal federated learning\nfrom attack and defense perspectives. To understand privacy risks in\nspatiotemporal federated learning, we first propose Spatiotemporal Gradient\nInversion Attack (ST-GIA), a gradient attack algorithm tailored to\nspatiotemporal data that successfully reconstructs the original location from\ngradients. Furthermore, we design an adaptive defense strategy to mitigate\ngradient inversion attacks in spatiotemporal federated learning. By dynamically\nadjusting the perturbation levels, we can offer tailored protection for varying\nrounds of training data, thereby achieving a better trade-off between privacy\nand utility than current state-of-the-art methods. Through intensive\nexperimental analysis on three real-world datasets, we reveal that the proposed\ndefense strategy can well preserve the utility of spatiotemporal federated\nlearning with effective security protection.",
    "updated" : "2024-07-12T01:37:44Z",
    "published" : "2024-07-11T14:17:02Z",
    "authors" : [
      {
        "name" : "Lele Zheng"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Renhe Jiang"
      },
      {
        "name" : "Kenjiro Taura"
      },
      {
        "name" : "Yulong Shen"
      },
      {
        "name" : "Sheng Li"
      },
      {
        "name" : "Masatoshi Yoshikawa"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.10094v1",
    "title" : "Work-From-Home and Privacy: What Do Workers Face and What are They Doing\n  About it?",
    "summary" : "The COVID-19 pandemic has reshaped the way people work, normalizing the\npractice of working from home (WFH). However, WFH can cause a blurring of\npersonal and professional boundaries, surfacing new privacy issues, especially\nwhen workers take work meetings from their homes. As WFH arrangements are now\nstandard practice in many organizations, addressing the associated privacy\nconcerns should be a key part of creating healthy work environments for\nworkers. To this end, we conducted a scenario-based survey with 214 US-based\nworkers who currently work from home regularly. Our results suggest that\nprivacy invasions are commonly experienced while working from home and cause\ndiscomfort to many workers. However, only a minority said that the discomfort\nescalated to cause harm to them or others, and the harm was almost always\npsychological. While scenarios that restrict worker autonomy (prohibit turning\noff camera or microphone) are the least experienced scenarios, they are\nassociated with the highest reported discomfort. In addition, participants\nreported measures that violated or would violate their employer's\nautonomy-restricting rules to protect their privacy. We also find that\nconference tool settings that can prevent privacy invasions are not widely used\ncompared to manual privacy-protective measures. Our findings provide better\nunderstanding of the privacy challenges landscape that WFH workers face and how\nthey address them. Furthermore, our discussion raised open questions that can\ninspire future work.",
    "updated" : "2024-07-14T06:15:28Z",
    "published" : "2024-07-14T06:15:28Z",
    "authors" : [
      {
        "name" : "Eman Alashwali"
      },
      {
        "name" : "Joanne Peca"
      },
      {
        "name" : "Mandy Lanyon"
      },
      {
        "name" : "Lorrie Cranor"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.10058v1",
    "title" : "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs",
    "summary" : "Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities.",
    "updated" : "2024-07-14T03:05:53Z",
    "published" : "2024-07-14T03:05:53Z",
    "authors" : [
      {
        "name" : "Zhenhua Liu"
      },
      {
        "name" : "Tong Zhu"
      },
      {
        "name" : "Chuanyuan Tan"
      },
      {
        "name" : "Wenliang Chen"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09809v1",
    "title" : "Preserving the Privacy of Reward Functions in MDPs through Deception",
    "summary" : "Preserving the privacy of preferences (or rewards) of a sequential\ndecision-making agent when decisions are observable is crucial in many physical\nand cybersecurity domains. For instance, in wildlife monitoring, agents must\nallocate patrolling resources without revealing animal locations to poachers.\nThis paper addresses privacy preservation in planning over a sequence of\nactions in MDPs, where the reward function represents the preference structure\nto be protected. Observers can use Inverse RL (IRL) to learn these preferences,\nmaking this a challenging task.\n  Current research on differential privacy in reward functions fails to ensure\nguarantee on the minimum expected reward and offers theoretical guarantees that\nare inadequate against IRL-based observers. To bridge this gap, we propose a\nnovel approach rooted in the theory of deception. Deception includes two\nmodels: dissimulation (hiding the truth) and simulation (showing the wrong).\nOur first contribution theoretically demonstrates significant privacy leaks in\nexisting dissimulation-based methods. Our second contribution is a novel\nRL-based planning algorithm that uses simulation to effectively address these\nprivacy concerns while ensuring a guarantee on the expected reward. Experiments\non multiple benchmark problems show that our approach outperforms previous\nmethods in preserving reward function privacy.",
    "updated" : "2024-07-13T09:03:22Z",
    "published" : "2024-07-13T09:03:22Z",
    "authors" : [
      {
        "name" : "Shashank Reddy Chirra"
      },
      {
        "name" : "Pradeep Varakantham"
      },
      {
        "name" : "Praveen Paruchuri"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09771v1",
    "title" : "Protecting Data Buyer Privacy in Data Markets",
    "summary" : "Data markets serve as crucial platforms facilitating data discovery,\nexchange, sharing, and integration among data users and providers. However, the\nparamount concern of privacy has predominantly centered on protecting privacy\nof data owners and third parties, neglecting the challenges associated with\nprotecting the privacy of data buyers. In this article, we address this gap by\nmodeling the intricacies of data buyer privacy protection and investigating the\ndelicate balance between privacy and purchase cost. Through comprehensive\nexperimentation, our results yield valuable insights, shedding light on the\nefficacy and efficiency of our proposed approaches.",
    "updated" : "2024-07-13T04:45:06Z",
    "published" : "2024-07-13T04:45:06Z",
    "authors" : [
      {
        "name" : "Minxing Zhang"
      },
      {
        "name" : "Jian Pei"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.09004v1",
    "title" : "Privacy-Preserving Collaborative Genomic Research: A Real-Life\n  Deployment and Vision",
    "summary" : "The data revolution holds significant promise for the health sector. Vast\namounts of data collected from individuals will be transformed into knowledge,\nAI models, predictive systems, and best practices. One area of health that\nstands to benefit greatly is the genomic domain. Progress in AI, machine\nlearning, and data science has opened new opportunities for genomic research,\npromising breakthroughs in personalized medicine. However, increasing awareness\nof privacy and cybersecurity necessitates robust solutions to protect sensitive\ndata in collaborative research. This paper presents a practical deployment of a\nprivacy-preserving framework for genomic research, developed in collaboration\nwith Lynx$.$MD, a platform for secure health data collaboration. The framework\naddresses critical cybersecurity and privacy challenges, enabling the\nprivacy-preserving sharing and analysis of genomic data while mitigating risks\nassociated with data breaches. By integrating advanced privacy-preserving\nalgorithms, the solution ensures the protection of individual privacy without\ncompromising data utility. A unique feature of the system is its ability to\nbalance trade-offs between data sharing and privacy, providing stakeholders\ntools to quantify privacy risks and make informed decisions. Implementing the\nframework within Lynx$.$MD involves encoding genomic data into binary formats\nand applying noise through controlled perturbation techniques. This approach\npreserves essential statistical properties of the data, facilitating effective\nresearch and analysis. Moreover, the system incorporates real-time data\nmonitoring and advanced visualization tools, enhancing user experience and\ndecision-making. The paper highlights the need for tailored privacy attacks and\ndefenses specific to genomic data. Addressing these challenges fosters\ncollaboration in genomic research, advancing personalized medicine and public\nhealth.",
    "updated" : "2024-07-12T05:43:13Z",
    "published" : "2024-07-12T05:43:13Z",
    "authors" : [
      {
        "name" : "Zahra Rahmani"
      },
      {
        "name" : "Nahal Shahini"
      },
      {
        "name" : "Nadav Gat"
      },
      {
        "name" : "Zebin Yun"
      },
      {
        "name" : "Yuzhou Jiang"
      },
      {
        "name" : "Ofir Farchy"
      },
      {
        "name" : "Yaniv Harel"
      },
      {
        "name" : "Vipin Chaudhary"
      },
      {
        "name" : "Mahmood Sharif"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.08529v3",
    "title" : "Enhancing Privacy of Spatiotemporal Federated Learning against Gradient\n  Inversion Attacks",
    "summary" : "Spatiotemporal federated learning has recently raised intensive studies due\nto its ability to train valuable models with only shared gradients in various\nlocation-based services. On the other hand, recent studies have shown that\nshared gradients may be subject to gradient inversion attacks (GIA) on images\nor texts. However, so far there has not been any systematic study of the\ngradient inversion attacks in spatiotemporal federated learning. In this paper,\nwe explore the gradient attack problem in spatiotemporal federated learning\nfrom attack and defense perspectives. To understand privacy risks in\nspatiotemporal federated learning, we first propose Spatiotemporal Gradient\nInversion Attack (ST-GIA), a gradient attack algorithm tailored to\nspatiotemporal data that successfully reconstructs the original location from\ngradients. Furthermore, we design an adaptive defense strategy to mitigate\ngradient inversion attacks in spatiotemporal federated learning. By dynamically\nadjusting the perturbation levels, we can offer tailored protection for varying\nrounds of training data, thereby achieving a better trade-off between privacy\nand utility than current state-of-the-art methods. Through intensive\nexperimental analysis on three real-world datasets, we reveal that the proposed\ndefense strategy can well preserve the utility of spatiotemporal federated\nlearning with effective security protection.",
    "updated" : "2024-07-15T06:42:31Z",
    "published" : "2024-07-11T14:17:02Z",
    "authors" : [
      {
        "name" : "Lele Zheng"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Renhe Jiang"
      },
      {
        "name" : "Kenjiro Taura"
      },
      {
        "name" : "Yulong Shen"
      },
      {
        "name" : "Sheng Li"
      },
      {
        "name" : "Masatoshi Yoshikawa"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.11274v1",
    "title" : "Empirical Mean and Frequency Estimation Under Heterogeneous Privacy: A\n  Worst-Case Analysis",
    "summary" : "Differential Privacy (DP) is the current gold-standard for measuring privacy.\nEstimation problems under DP constraints appearing in the literature have\nlargely focused on providing equal privacy to all users. We consider the\nproblems of empirical mean estimation for univariate data and frequency\nestimation for categorical data, two pillars of data analysis in the industry,\nsubject to heterogeneous privacy constraints. Each user, contributing a sample\nto the dataset, is allowed to have a different privacy demand. The dataset\nitself is assumed to be worst-case and we study both the problems in two\ndifferent formulations -- the correlated and the uncorrelated setting. In the\nformer setting, the privacy demand and the user data can be arbitrarily\ncorrelated while in the latter setting, there is no correlation between the\ndataset and the privacy demand. We prove some optimality results, under both\nPAC error and mean-squared error, for our proposed algorithms and demonstrate\nsuperior performance over other baseline techniques experimentally.",
    "updated" : "2024-07-15T22:46:02Z",
    "published" : "2024-07-15T22:46:02Z",
    "authors" : [
      {
        "name" : "Syomantak Chaudhuri"
      },
      {
        "name" : "Thomas A. Courtade"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.12669v1",
    "title" : "Enhancing the Utility of Privacy-Preserving Cancer Classification using\n  Synthetic Data",
    "summary" : "Deep learning holds immense promise for aiding radiologists in breast cancer\ndetection. However, achieving optimal model performance is hampered by\nlimitations in availability and sharing of data commonly associated to patient\nprivacy concerns. Such concerns are further exacerbated, as traditional deep\nlearning models can inadvertently leak sensitive training information. This\nwork addresses these challenges exploring and quantifying the utility of\nprivacy-preserving deep learning techniques, concretely, (i) differentially\nprivate stochastic gradient descent (DP-SGD) and (ii) fully synthetic training\ndata generated by our proposed malignancy-conditioned generative adversarial\nnetwork. We assess these methods via downstream malignancy classification of\nmammography masses using a transformer model. Our experimental results depict\nthat synthetic data augmentation can improve privacy-utility tradeoffs in\ndifferentially private model training. Further, model pretraining on synthetic\ndata achieves remarkable performance, which can be further increased with\nDP-SGD fine-tuning across all privacy guarantees. With this first in-depth\nexploration of privacy-preserving deep learning in breast imaging, we address\ncurrent and emerging clinical privacy requirements and pave the way towards the\nadoption of private high-utility deep diagnostic models. Our reproducible\ncodebase is publicly available at https://github.com/RichardObi/mammo_dp.",
    "updated" : "2024-07-17T15:52:45Z",
    "published" : "2024-07-17T15:52:45Z",
    "authors" : [
      {
        "name" : "Richard Osuala"
      },
      {
        "name" : "Daniel M. Lang"
      },
      {
        "name" : "Anneliese Riess"
      },
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Zuzanna Szafranowska"
      },
      {
        "name" : "Grzegorz Skorupko"
      },
      {
        "name" : "Oliver Diaz"
      },
      {
        "name" : "Julia A. Schnabel"
      },
      {
        "name" : "Karim Lekadir"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.12589v1",
    "title" : "Privacy-Preserving Adaptive Re-Identification without Image Transfer",
    "summary" : "Re-Identification systems (Re-ID) are crucial for public safety but face the\nchallenge of having to adapt to environments that differ from their training\ndistribution. Furthermore, rigorous privacy protocols in public places are\nbeing enforced as apprehensions regarding individual freedom rise, adding\nlayers of complexity to the deployment of accurate Re-ID systems in new\nenvironments. For example, in the European Union, the principles of ``Data\nMinimization'' and ``Purpose Limitation'' restrict the retention and processing\nof images to what is strictly necessary. These regulations pose a challenge to\nthe conventional Re-ID training schemes that rely on centralizing data on\nservers. In this work, we present a novel setting for privacy-preserving\nDistributed Unsupervised Domain Adaptation for person Re-ID (DUDA-Rid) to\naddress the problem of domain shift without requiring any image transfer\noutside the camera devices. To address this setting, we introduce Fed-Protoid,\na novel solution that adapts person Re-ID models directly within the edge\ndevices. Our proposed solution employs prototypes derived from the source\ndomain to align feature statistics within edge devices. Those source prototypes\nare distributed across the edge devices to minimize a distributed Maximum Mean\nDiscrepancy (MMD) loss tailored for the DUDA-Rid setting. Our experiments\nprovide compelling evidence that Fed-Protoid outperforms all evaluated methods\nin terms of both accuracy and communication efficiency, all while maintaining\ndata privacy.",
    "updated" : "2024-07-17T14:12:44Z",
    "published" : "2024-07-17T14:12:44Z",
    "authors" : [
      {
        "name" : "Hamza Rami"
      },
      {
        "name" : "Jhony H. Giraldo"
      },
      {
        "name" : "Nicolas Winckler"
      },
      {
        "name" : "Stéphane Lathuilière"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13725v1",
    "title" : "Scalable Optimization for Locally Relevant Geo-Location Privacy",
    "summary" : "Geo-obfuscation functions as a location privacy protection mechanism (LPPM),\nenabling mobile users to share obfuscated locations with servers instead of\ntheir exact locations. This technique protects users' location privacy during\nserver-side data breaches since the obfuscation process is irreversible. To\nminimize the utility loss caused by data obfuscation, linear programming (LP)\nis widely used. However, LP can face a polynomial explosion in decision\nvariables, making it impractical for large-scale geo-obfuscation applications.\nIn this paper, we propose a new LPPM called Locally Relevant Geo-obfuscation\n(LR-Geo) to optimize geo-obfuscation using LP more efficiently. This is\naccomplished by restricting the geo-obfuscation calculations for each user to\nlocally relevant (LR) locations near the user's actual location. To prevent LR\nlocations from inadvertently revealing a user's true whereabouts, users compute\nthe LP coefficients locally and upload only these coefficients to the server,\nrather than the LR locations themselves. The server then solves the LP problem\nusing the provided coefficients. Additionally, we enhance the LP framework with\nan exponential obfuscation mechanism to ensure that the obfuscation\ndistribution is indistinguishable across multiple users. By leveraging the\nconstraint structure of the LP formulation, we apply Benders' decomposition to\nfurther boost computational efficiency. Our theoretical analysis confirms that,\neven though geo-obfuscation is calculated independently for each user, it still\nadheres to geo-indistinguishability constraints across multiple users with high\nprobability. Finally, experimental results using a real-world dataset\ndemonstrate that LR-Geo outperforms existing geo-obfuscation methods in terms\nof computational time, data utility, and privacy protection.",
    "updated" : "2024-07-18T17:25:08Z",
    "published" : "2024-07-18T17:25:08Z",
    "authors" : [
      {
        "name" : "Chenxi Qiu"
      },
      {
        "name" : "Ruiyao Liu"
      },
      {
        "name" : "Primal Pappachan"
      },
      {
        "name" : "Anna Squicciarini"
      },
      {
        "name" : "Xinpeng Xie"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13621v1",
    "title" : "Differential Privacy Mechanisms in Neural Tangent Kernel Regression",
    "summary" : "Training data privacy is a fundamental problem in modern Artificial\nIntelligence (AI) applications, such as face recognition, recommendation\nsystems, language generation, and many others, as it may contain sensitive user\ninformation related to legal issues. To fundamentally understand how privacy\nmechanisms work in AI applications, we study differential privacy (DP) in the\nNeural Tangent Kernel (NTK) regression setting, where DP is one of the most\npowerful tools for measuring privacy under statistical learning, and NTK is one\nof the most popular analysis frameworks for studying the learning mechanisms of\ndeep neural networks. In our work, we can show provable guarantees for both\ndifferential privacy and test accuracy of our NTK regression. Furthermore, we\nconduct experiments on the basic image classification dataset CIFAR10 to\ndemonstrate that NTK regression can preserve good accuracy under a modest\nprivacy budget, supporting the validity of our analysis. To our knowledge, this\nis the first work to provide a DP guarantee for NTK regression.",
    "updated" : "2024-07-18T15:57:55Z",
    "published" : "2024-07-18T15:57:55Z",
    "authors" : [
      {
        "name" : "Jiuxiang Gu"
      },
      {
        "name" : "Yingyu Liang"
      },
      {
        "name" : "Zhizhou Sha"
      },
      {
        "name" : "Zhenmei Shi"
      },
      {
        "name" : "Zhao Song"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13532v1",
    "title" : "PriPL-Tree: Accurate Range Query for Arbitrary Distribution under Local\n  Differential Privacy",
    "summary" : "Answering range queries in the context of Local Differential Privacy (LDP) is\na widely studied problem in Online Analytical Processing (OLAP). Existing LDP\nsolutions all assume a uniform data distribution within each domain partition,\nwhich may not align with real-world scenarios where data distribution is\nvaried, resulting in inaccurate estimates. To address this problem, we\nintroduce PriPL-Tree, a novel data structure that combines hierarchical tree\nstructures with piecewise linear (PL) functions to answer range queries for\narbitrary distributions. PriPL-Tree precisely models the underlying data\ndistribution with a few line segments, leading to more accurate results for\nrange queries. Furthermore, we extend it to multi-dimensional cases with novel\ndata-aware adaptive grids. These grids leverage the insights from marginal\ndistributions obtained through PriPL-Trees to partition the grids adaptively,\nadapting the density of underlying distributions. Our extensive experiments on\nboth real and synthetic datasets demonstrate the effectiveness and superiority\nof PriPL-Tree over state-of-the-art solutions in answering range queries across\narbitrary data distributions.",
    "updated" : "2024-07-18T14:05:35Z",
    "published" : "2024-07-18T14:05:35Z",
    "authors" : [
      {
        "name" : "Leixia Wang"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Haibo Hu"
      },
      {
        "name" : "Xiaofeng Meng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13516v1",
    "title" : "Optimal Mechanisms for Quantum Local Differential Privacy",
    "summary" : "In recent years, centralized differential privacy has been successfully\nextended to quantum computing and information processing to safeguard privacy\nand prevent leaks in neighboring relationships of quantum states. This paper\nintroduces a framework known as quantum local differential privacy (QLDP) and\ninitializes the algorithmic study of QLDP. QLDP utilizes a parameter $\\epsilon$\nto manage privacy leaks and ensure the privacy of individual quantum states.\nThe optimization of the QLDP value $\\epsilon$, denoted as $\\epsilon^*$, for any\nquantum mechanism is addressed as an optimization problem. The introduction of\nquantum noise is shown to provide privacy protections similar to classical\nscenarios, with quantum depolarizing noise identified as the optimal unital\nprivatization mechanism within the QLDP framework. Unital mechanisms represent\na diverse set of quantum mechanisms that encompass frequently employed quantum\nnoise types. Quantum depolarizing noise optimizes both fidelity and trace\ndistance utilities, which are crucial metrics in the field of quantum\ncomputation and information, and can be viewed as a quantum counterpart to\nclassical randomized response methods. Additionally, a composition theorem is\npresented for the application of QLDP framework in distributed (spatially\nseparated) quantum systems, ensuring the validity (additivity of QLDP value)\nirrespective of the states' independence, classical correlation, or\nentanglement (quantum correlation). The study further explores the trade-off\nbetween utility and privacy across different quantum noise mechanisms,\nincluding unital and non-unital quantum noise mechanisms, through both\nanalytical and numerically experimental approaches. Meanwhile, this highlights\nthe optimization of quantum depolarizing noise in QLDP framework.",
    "updated" : "2024-07-18T13:46:16Z",
    "published" : "2024-07-18T13:46:16Z",
    "authors" : [
      {
        "name" : "Ji Guan"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13153v1",
    "title" : "Preset-Voice Matching for Privacy Regulated Speech-to-Speech Translation\n  Systems",
    "summary" : "In recent years, there has been increased demand for speech-to-speech\ntranslation (S2ST) systems in industry settings. Although successfully\ncommercialized, cloning-based S2ST systems expose their distributors to\nliabilities when misused by individuals and can infringe on personality rights\nwhen exploited by media organizations. This work proposes a regulated S2ST\nframework called Preset-Voice Matching (PVM). PVM removes cross-lingual voice\ncloning in S2ST by first matching the input voice to a similar prior consenting\nspeaker voice in the target-language. With this separation, PVM avoids cloning\nthe input speaker, ensuring PVM systems comply with regulations and reduce risk\nof misuse. Our results demonstrate PVM can significantly improve S2ST system\nrun-time in multi-speaker settings and the naturalness of S2ST synthesized\nspeech. To our knowledge, PVM is the first explicitly regulated S2ST framework\nleveraging similarly-matched preset-voices for dynamic S2ST tasks.",
    "updated" : "2024-07-18T04:42:01Z",
    "published" : "2024-07-18T04:42:01Z",
    "authors" : [
      {
        "name" : "Daniel Platnick"
      },
      {
        "name" : "Bishoy Abdelnour"
      },
      {
        "name" : "Eamon Earl"
      },
      {
        "name" : "Rahul Kumar"
      },
      {
        "name" : "Zahra Rezaei"
      },
      {
        "name" : "Thomas Tsangaris"
      },
      {
        "name" : "Faraj Lagum"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13975v1",
    "title" : "Personalized Privacy Protection Mask Against Unauthorized Facial\n  Recognition",
    "summary" : "Face recognition (FR) can be abused for privacy intrusion. Governments,\nprivate companies, or even individual attackers can collect facial images by\nweb scraping to build an FR system identifying human faces without their\nconsent. This paper introduces Chameleon, which learns to generate a\nuser-centric personalized privacy protection mask, coined as P3-Mask, to\nprotect facial images against unauthorized FR with three salient features.\nFirst, we use a cross-image optimization to generate one P3-Mask for each user\ninstead of tailoring facial perturbation for each facial image of a user. It\nenables efficient and instant protection even for users with limited computing\nresources. Second, we incorporate a perceptibility optimization to preserve the\nvisual quality of the protected facial images. Third, we strengthen the\nrobustness of P3-Mask against unknown FR models by integrating focal\ndiversity-optimized ensemble learning into the mask generation process.\nExtensive experiments on two benchmark datasets show that Chameleon outperforms\nthree state-of-the-art methods with instant protection and minimal degradation\nof image quality. Furthermore, Chameleon enables cost-effective FR\nauthorization using the P3-Mask as a personalized de-obfuscation key, and it\ndemonstrates high resilience against adaptive adversaries.",
    "updated" : "2024-07-19T01:59:00Z",
    "published" : "2024-07-19T01:59:00Z",
    "authors" : [
      {
        "name" : "Ka-Ho Chow"
      },
      {
        "name" : "Sihao Hu"
      },
      {
        "name" : "Tiansheng Huang"
      },
      {
        "name" : "Ling Liu"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.13881v1",
    "title" : "Privacy-preserving gradient-based fair federated learning",
    "summary" : "Federated learning (FL) schemes allow multiple participants to\ncollaboratively train neural networks without the need to directly share the\nunderlying data.However, in early schemes, all participants eventually obtain\nthe same model. Moreover, the aggregation is typically carried out by a third\nparty, who obtains combined gradients or weights, which may reveal the model.\nThese downsides underscore the demand for fair and privacy-preserving FL\nschemes. Here, collaborative fairness asks for individual model quality\ndepending on the individual data contribution. Privacy is demanded with respect\nto any kind of data outsourced to the third party. Now, there already exist\nsome approaches aiming for either fair or privacy-preserving FL and a few works\neven address both features. In our paper, we build upon these seminal works and\npresent a novel, fair and privacy-preserving FL scheme. Our approach, which\nmainly relies on homomorphic encryption, stands out for exclusively using local\ngradients. This increases the usability in comparison to state-of-the-art\napproaches and thereby opens the door to applications in control.",
    "updated" : "2024-07-18T19:56:39Z",
    "published" : "2024-07-18T19:56:39Z",
    "authors" : [
      {
        "name" : "Janis Adamek"
      },
      {
        "name" : "Moritz Schulze Darup"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.SY",
      "eess.SY"
    ]
  }
]