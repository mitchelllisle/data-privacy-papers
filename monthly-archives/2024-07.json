[
  {
    "id" : "http://arxiv.org/abs/2407.02268v1",
    "title" : "Footprints of Data in a Classifier Model: The Privacy Issues and Their\n  Mitigation through Data Obfuscation",
    "summary" : "The avalanche of AI deployment and its security-privacy concerns are two\nsides of the same coin. Article 17 of GDPR calls for the Right to Erasure; data\nhas to be obliterated from a system to prevent its compromise. Extant research\nin this aspect focuses on effacing sensitive data attributes. However, several\npassive modes of data compromise are yet to be recognized and redressed. The\nembedding of footprints of training data in a prediction model is one such\nfacet; the difference in performance quality in test and training data causes\npassive identification of data that have trained the model. This research\nfocuses on addressing the vulnerability arising from the data footprints. The\nthree main aspects are -- i] exploring the vulnerabilities of different\nclassifiers (to segregate the vulnerable and the non-vulnerable ones), ii]\nreducing the vulnerability of vulnerable classifiers (through data obfuscation)\nto preserve model and data privacy, and iii] exploring the privacy-performance\ntradeoff to study the usability of the data obfuscation techniques. An\nempirical study is conducted on three datasets and eight classifiers to explore\nthe above objectives. The results of the initial research identify the\nvulnerability in classifiers and segregate the vulnerable and non-vulnerable\nclassifiers. The additional experiments on data obfuscation techniques reveal\ntheir utility to render data and model privacy and also their capability to\nchalk out a privacy-performance tradeoff in most scenarios. The results can aid\nthe practitioners with their choice of classifiers in different scenarios and\ncontexts.",
    "updated" : "2024-07-02T13:56:37Z",
    "published" : "2024-07-02T13:56:37Z",
    "authors" : [
      {
        "name" : "Payel Sadhukhan"
      },
      {
        "name" : "Tanujit Chakraborty"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02226v1",
    "title" : "RollupTheCrowd: Leveraging ZkRollups for a Scalable and\n  Privacy-Preserving Reputation-based Crowdsourcing Platform",
    "summary" : "Current blockchain-based reputation solutions for crowdsourcing fail to\ntackle the challenge of ensuring both efficiency and privacy without\ncompromising the scalability of the blockchain. Developing an effective,\ntransparent, and privacy-preserving reputation model necessitates on-chain\nimplementation using smart contracts. However, managing task evaluation and\nreputation updates alongside crowdsourcing transactions on-chain substantially\nstrains system scalability and performance. This paper introduces\nRollupTheCrowd, a novel blockchain-powered crowdsourcing framework that\nleverages zkRollups to enhance system scalability while protecting user\nprivacy. Our framework includes an effective and privacy-preserving reputation\nmodel that gauges workers' trustworthiness by assessing their crowdsourcing\ninteractions. To alleviate the load on our blockchain, we employ an off-chain\nstorage scheme, optimizing RollupTheCrowd's performance. Utilizing smart\ncontracts and zero-knowledge proofs, our Rollup layer achieves a significant\n20x reduction in gas consumption. To prove the feasibility of the proposed\nframework, we developed a proof-of-concept implementation using cutting-edge\ntools. The experimental results presented in this paper demonstrate the\neffectiveness and scalability of RollupTheCrowd, validating its potential for\nreal-world application scenarios.",
    "updated" : "2024-07-02T12:51:32Z",
    "published" : "2024-07-02T12:51:32Z",
    "authors" : [
      {
        "name" : "Ahmed Mounsf Rafik Bendada"
      },
      {
        "name" : "Mouhamed Amine Bouchiha"
      },
      {
        "name" : "Mourad Rabah"
      },
      {
        "name" : "Yacine Ghamri-Doudane"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02191v1",
    "title" : "Attack-Aware Noise Calibration for Differential Privacy",
    "summary" : "Differential privacy (DP) is a widely used approach for mitigating privacy\nrisks when training machine learning models on sensitive data. DP mechanisms\nadd noise during training to limit the risk of information leakage. The scale\nof the added noise is critical, as it determines the trade-off between privacy\nand utility. The standard practice is to select the noise scale in terms of a\nprivacy budget parameter $\\epsilon$. This parameter is in turn interpreted in\nterms of operational attack risk, such as accuracy, or sensitivity and\nspecificity of inference attacks against the privacy of the data. We\ndemonstrate that this two-step procedure of first calibrating the noise scale\nto a privacy budget $\\epsilon$, and then translating $\\epsilon$ to attack risk\nleads to overly conservative risk assessments and unnecessarily low utility. We\npropose methods to directly calibrate the noise scale to a desired attack risk\nlevel, bypassing the intermediate step of choosing $\\epsilon$. For a target\nattack risk, our approach significantly decreases noise scale, leading to\nincreased utility at the same level of privacy. We empirically demonstrate that\ncalibrating noise to attack sensitivity/specificity, rather than $\\epsilon$,\nwhen training privacy-preserving ML models substantially improves model\naccuracy for the same risk level. Our work provides a principled and practical\nway to improve the utility of privacy-preserving ML without compromising on\nprivacy.",
    "updated" : "2024-07-02T11:49:59Z",
    "published" : "2024-07-02T11:49:59Z",
    "authors" : [
      {
        "name" : "Bogdan Kulynych"
      },
      {
        "name" : "Juan Felipe Gomez"
      },
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Flavio du Pin Calmon"
      },
      {
        "name" : "Carmela Troncoso"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02027v1",
    "title" : "Privacy Risks of General-Purpose AI Systems: A Foundation for\n  Investigating Practitioner Perspectives",
    "summary" : "The rise of powerful AI models, more formally $\\textit{General-Purpose AI\nSystems}$ (GPAIS), has led to impressive leaps in performance across a wide\nrange of tasks. At the same time, researchers and practitioners alike have\nraised a number of privacy concerns, resulting in a wealth of literature\ncovering various privacy risks and vulnerabilities of AI models. Works\nsurveying such risks provide differing focuses, leading to disparate sets of\nprivacy risks with no clear unifying taxonomy. We conduct a systematic review\nof these survey papers to provide a concise and usable overview of privacy\nrisks in GPAIS, as well as proposed mitigation strategies. The developed\nprivacy framework strives to unify the identified privacy risks and mitigations\nat a technical level that is accessible to non-experts. This serves as the\nbasis for a practitioner-focused interview study to assess technical\nstakeholder perceptions of privacy risks and mitigations in GPAIS.",
    "updated" : "2024-07-02T07:49:48Z",
    "published" : "2024-07-02T07:49:48Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Patrick Gage Kelley"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Kurt Thomas"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01817v1",
    "title" : "Race and Privacy in Broadcast Police Communications",
    "summary" : "Radios are essential for the operations of modern police departments, and\nthey function as both a collaborative communication technology and a\nsociotechnical system. However, little prior research has examined their usage\nor their connections to individual privacy and the role of race in policing,\ntwo growing topics of concern in the US. As a case study, we examine the\nChicago Police Department's (CPD's) use of broadcast police communications\n(BPC) to coordinate the activity of law enforcement officers (LEOs) in the\ncity. From a recently assembled archive of 80,775 hours of BPC associated with\nCPD operations, we analyze text transcripts of radio transmissions broadcast\n9:00 AM to 5:00 PM on August 10th, 2018 in one majority Black, one majority\nwhite, and one majority Hispanic area of the city (24 hours of audio) to\nexplore three research questions: (1) Do BPC reflect reported racial\ndisparities in policing? (2) How and when is gender, race/ethnicity, and age\nmentioned in BPC? (3) To what extent do BPC include sensitive information, and\nwho is put at most risk by this practice? (4) To what extent can large language\nmodels (LLMs) heighten this risk? We explore the vocabulary and speech acts\nused by police in BPC, comparing mentions of personal characteristics to local\ndemographics, the personal information shared over BPC, and the privacy\nconcerns that it poses. Analysis indicates (a) policing professionals in the\ncity of Chicago exhibit disproportionate attention to Black members of the\npublic regardless of context, (b) sociodemographic characteristics like gender,\nrace/ethnicity, and age are primarily mentioned in BPC about event information,\nand (c) disproportionate attention introduces disproportionate privacy risks\nfor Black members of the public.",
    "updated" : "2024-07-01T21:34:51Z",
    "published" : "2024-07-01T21:34:51Z",
    "authors" : [
      {
        "name" : "Pranav Narayanan Venkit"
      },
      {
        "name" : "Christopher Graziul"
      },
      {
        "name" : "Miranda Ardith Goodman"
      },
      {
        "name" : "Samantha Nicole Kenny"
      },
      {
        "name" : "Shomir Wilson"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01334v1",
    "title" : "Protecting Privacy in Classifiers by Token Manipulation",
    "summary" : "Using language models as a remote service entails sending private information\nto an untrusted provider. In addition, potential eavesdroppers can intercept\nthe messages, thereby exposing the information. In this work, we explore the\nprospects of avoiding such data exposure at the level of text manipulation. We\nfocus on text classification models, examining various token mapping and\ncontextualized manipulation functions in order to see whether classifier\naccuracy may be maintained while keeping the original text unrecoverable. We\nfind that although some token mapping functions are easy and straightforward to\nimplement, they heavily influence performance on the downstream task, and via a\nsophisticated attacker can be reconstructed. In comparison, the contextualized\nmanipulation provides an improvement in performance.",
    "updated" : "2024-07-01T14:41:59Z",
    "published" : "2024-07-01T14:41:59Z",
    "authors" : [
      {
        "name" : "Re'em Harel"
      },
      {
        "name" : "Yair Elboher"
      },
      {
        "name" : "Yuval Pinter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01167v1",
    "title" : "Information Density Bounds for Privacy",
    "summary" : "This paper explores the implications of guaranteeing privacy by imposing a\nlower bound on the information density between the private and the public data.\nWe introduce an operationally meaningful privacy measure called pointwise\nmaximal cost (PMC) and demonstrate that imposing an upper bound on PMC is\nequivalent to enforcing a lower bound on the information density. PMC\nquantifies the information leakage about a secret to adversaries who aim to\nminimize non-negative cost functions after observing the outcome of a privacy\nmechanism. When restricted to finite alphabets, PMC can equivalently be defined\nas the information leakage to adversaries aiming to minimize the probability of\nincorrectly guessing randomized functions of the secret. We study the\nproperties of PMC and apply it to standard privacy mechanisms to demonstrate\nits practical relevance. Through a detailed examination, we connect PMC with\nother privacy measures that impose upper or lower bounds on the information\ndensity. Our results highlight that lower bounding the information density is a\nmore stringent requirement than upper bounding it. Overall, our work\nsignificantly bridges the gaps in understanding the relationships between\nvarious privacy frameworks and provides insights for selecting a suitable\nframework for a given application.",
    "updated" : "2024-07-01T10:38:02Z",
    "published" : "2024-07-01T10:38:02Z",
    "authors" : [
      {
        "name" : "Sara Saeidian"
      },
      {
        "name" : "Leonhard Grosse"
      },
      {
        "name" : "Parastoo Sadeghi"
      },
      {
        "name" : "Mikael Skoglund"
      },
      {
        "name" : "Tobias J. Oechtering"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.00991v1",
    "title" : "Pre-capture Privacy via Adaptive Single-Pixel Imaging",
    "summary" : "As cameras become ubiquitous in our living environment, invasion of privacy\nis becoming a growing concern. A common approach to privacy preservation is to\nremove personally identifiable information from a captured image, but there is\na risk of the original image being leaked. In this paper, we propose a\npre-capture privacy-aware imaging method that captures images from which the\ndetails of a pre-specified anonymized target have been eliminated. The proposed\nmethod applies a single-pixel imaging framework in which we introduce a\nfeedback mechanism called an aperture pattern generator. The introduced\naperture pattern generator adaptively outputs the next aperture pattern to\navoid sampling the anonymized target by exploiting the data already acquired as\na clue. Furthermore, the anonymized target can be set to any object without\nchanging hardware. Except for detailed features which have been removed from\nthe anonymized target, the captured images are of comparable quality to those\ncaptured by a general camera and can be used for various computer vision\napplications. In our work, we target faces and license plates and\nexperimentally show that the proposed method can capture clear images in which\ndetailed features of the anonymized target are eliminated to achieve both\nprivacy and utility.",
    "updated" : "2024-07-01T06:05:12Z",
    "published" : "2024-07-01T06:05:12Z",
    "authors" : [
      {
        "name" : "Yoko Sogabe"
      },
      {
        "name" : "Shiori Sugimoto"
      },
      {
        "name" : "Ayumi Matsumoto"
      },
      {
        "name" : "Masaki Kitahara"
      }
    ],
    "categories" : [
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.00873v1",
    "title" : "Privacy-First Crowdsourcing: Blockchain and Local Differential Privacy\n  in Crowdsourced Drone Services",
    "summary" : "We introduce a privacy-preserving framework for integrating consumer-grade\ndrones into bushfire management. This system creates a marketplace where\nbushfire management authorities obtain essential data from drone operators. Key\nfeatures include local differential privacy to protect data providers and a\nblockchain-based solution ensuring fair data exchanges and accountability. The\nframework is validated through a proof-of-concept implementation, demonstrating\nits scalability and potential for various large-scale data collection\nscenarios. This approach addresses privacy concerns and compliance with\nregulations like Australia's Privacy Act 1988, offering a practical solution\nfor enhancing bushfire detection and management through crowdsourced drone\nservices.",
    "updated" : "2024-07-01T00:46:25Z",
    "published" : "2024-07-01T00:46:25Z",
    "authors" : [
      {
        "name" : "Junaid Akram"
      },
      {
        "name" : "Ali Anaissi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03289v1",
    "title" : "Correlated Privacy Mechanisms for Differentially Private Distributed\n  Mean Estimation",
    "summary" : "Differentially private distributed mean estimation (DP-DME) is a fundamental\nbuilding block in privacy-preserving federated learning, where a central server\nestimates the mean of $d$-dimensional vectors held by $n$ users while ensuring\n$(\\epsilon,\\delta)$-DP. Local differential privacy (LDP) and distributed DP\nwith secure aggregation (SecAgg) are the most common notions of DP used in\nDP-DME settings with an untrusted server. LDP provides strong resilience to\ndropouts, colluding users, and malicious server attacks, but suffers from poor\nutility. In contrast, SecAgg-based DP-DME achieves an $O(n)$ utility gain over\nLDP in DME, but requires increased communication and computation overheads and\ncomplex multi-round protocols to handle dropouts and malicious attacks. In this\nwork, we propose CorDP-DME, a novel DP-DME mechanism that spans the gap between\nDME with LDP and distributed DP, offering a favorable balance between utility\nand resilience to dropout and collusion. CorDP-DME is based on correlated\nGaussian noise, ensuring DP without the perfect conditional privacy guarantees\nof SecAgg-based approaches. We provide an information-theoretic analysis of\nCorDP-DME, and derive theoretical guarantees for utility under any given\nprivacy parameters and dropout/colluding user thresholds. Our results\ndemonstrate that (anti) correlated Gaussian DP mechanisms can significantly\nimprove utility in mean estimation tasks compared to LDP -- even in adversarial\nsettings -- while maintaining better resilience to dropouts and attacks\ncompared to distributed DP.",
    "updated" : "2024-07-03T17:22:33Z",
    "published" : "2024-07-03T17:22:33Z",
    "authors" : [
      {
        "name" : "Sajani Vithana"
      },
      {
        "name" : "Viveck R. Cadambe"
      },
      {
        "name" : "Flavio P. Calmon"
      },
      {
        "name" : "Haewon Jeong"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02956v1",
    "title" : "IncogniText: Privacy-enhancing Conditional Text Anonymization via\n  LLM-based Private Attribute Randomization",
    "summary" : "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%.\nFinally, we demonstrate the maturity of IncogniText for real-world applications\nby distilling its anonymization capability into a set of LoRA parameters\nassociated with an on-device model.",
    "updated" : "2024-07-03T09:49:03Z",
    "published" : "2024-07-03T09:49:03Z",
    "authors" : [
      {
        "name" : "Ahmed Frikha"
      },
      {
        "name" : "Nassim Walha"
      },
      {
        "name" : "Krishna Kanth Nakka"
      },
      {
        "name" : "Ricardo Mendes"
      },
      {
        "name" : "Xue Jiang"
      },
      {
        "name" : "Xuebing Zhou"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02766v1",
    "title" : "Balancing Patient Privacy and Health Data Security: The Role of\n  Compliance in Protected Health Information (PHI) Sharing",
    "summary" : "Protected Health Information (PHI) sharing significantly enhances patient\ncare quality and coordination, contributing to more accurate diagnoses,\nefficient treatment plans, and a comprehensive understanding of patient\nhistory. Compliance with strict privacy and security policies, such as those\nrequired by laws like HIPAA, is critical to protect PHI. Blockchain technology,\nwhich offers a decentralized and tamper-evident ledger system, hold promise in\npolicy compliance. This system ensures the authenticity and integrity of PHI\nwhile facilitating patient consent management. In this work, we propose a\nblockchain technology that integrates smart contracts to partially automate\nconsent-related processes and ensuring that PHI access and sharing follow\npatient preferences and legal requirements.",
    "updated" : "2024-07-03T02:49:33Z",
    "published" : "2024-07-03T02:49:33Z",
    "authors" : [
      {
        "name" : "Md Al Amin"
      },
      {
        "name" : "Hemanth Tummala"
      },
      {
        "name" : "Rushabh Shah"
      },
      {
        "name" : "Indrajit Ray"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02747v1",
    "title" : "Curvature Clues: Decoding Deep Learning Privacy with Input Loss\n  Curvature",
    "summary" : "In this paper, we explore the properties of loss curvature with respect to\ninput data in deep neural networks. Curvature of loss with respect to input\n(termed input loss curvature) is the trace of the Hessian of the loss with\nrespect to the input. We investigate how input loss curvature varies between\ntrain and test sets, and its implications for train-test distinguishability. We\ndevelop a theoretical framework that derives an upper bound on the train-test\ndistinguishability based on privacy and the size of the training set. This\nnovel insight fuels the development of a new black box membership inference\nattack utilizing input loss curvature. We validate our theoretical findings\nthrough experiments in computer vision classification tasks, demonstrating that\ninput loss curvature surpasses existing methods in membership inference\neffectiveness. Our analysis highlights how the performance of membership\ninference attack (MIA) methods varies with the size of the training set,\nshowing that curvature-based MIA outperforms other methods on sufficiently\nlarge datasets. This condition is often met by real datasets, as demonstrated\nby our results on CIFAR10, CIFAR100, and ImageNet. These findings not only\nadvance our understanding of deep neural network behavior but also improve the\nability to test privacy-preserving techniques in machine learning.",
    "updated" : "2024-07-03T01:47:46Z",
    "published" : "2024-07-03T01:47:46Z",
    "authors" : [
      {
        "name" : "Deepak Ravikumar"
      },
      {
        "name" : "Efstathia Soufleri"
      },
      {
        "name" : "Kaushik Roy"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.02226v1",
    "title" : "RollupTheCrowd: Leveraging ZkRollups for a Scalable and\n  Privacy-Preserving Reputation-based Crowdsourcing Platform",
    "summary" : "Current blockchain-based reputation solutions for crowdsourcing fail to\ntackle the challenge of ensuring both efficiency and privacy without\ncompromising the scalability of the blockchain. Developing an effective,\ntransparent, and privacy-preserving reputation model necessitates on-chain\nimplementation using smart contracts. However, managing task evaluation and\nreputation updates alongside crowdsourcing transactions on-chain substantially\nstrains system scalability and performance. This paper introduces\nRollupTheCrowd, a novel blockchain-powered crowdsourcing framework that\nleverages zkRollups to enhance system scalability while protecting user\nprivacy. Our framework includes an effective and privacy-preserving reputation\nmodel that gauges workers' trustworthiness by assessing their crowdsourcing\ninteractions. To alleviate the load on our blockchain, we employ an off-chain\nstorage scheme, optimizing RollupTheCrowd's performance. Utilizing smart\ncontracts and zero-knowledge proofs, our Rollup layer achieves a significant\n20x reduction in gas consumption. To prove the feasibility of the proposed\nframework, we developed a proof-of-concept implementation using cutting-edge\ntools. The experimental results presented in this paper demonstrate the\neffectiveness and scalability of RollupTheCrowd, validating its potential for\nreal-world application scenarios.",
    "updated" : "2024-07-02T12:51:32Z",
    "published" : "2024-07-02T12:51:32Z",
    "authors" : [
      {
        "name" : "Ahmed Mounsf Rafik Bendada"
      },
      {
        "name" : "Mouhamed Amine Bouchiha"
      },
      {
        "name" : "Mourad Rabah"
      },
      {
        "name" : "Yacine Ghamri-Doudane"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.01334v2",
    "title" : "Protecting Privacy in Classifiers by Token Manipulation",
    "summary" : "Using language models as a remote service entails sending private information\nto an untrusted provider. In addition, potential eavesdroppers can intercept\nthe messages, thereby exposing the information. In this work, we explore the\nprospects of avoiding such data exposure at the level of text manipulation. We\nfocus on text classification models, examining various token mapping and\ncontextualized manipulation functions in order to see whether classifier\naccuracy may be maintained while keeping the original text unrecoverable. We\nfind that although some token mapping functions are easy and straightforward to\nimplement, they heavily influence performance on the downstream task, and via a\nsophisticated attacker can be reconstructed. In comparison, the contextualized\nmanipulation provides an improvement in performance.",
    "updated" : "2024-07-03T16:31:52Z",
    "published" : "2024-07-01T14:41:59Z",
    "authors" : [
      {
        "name" : "Re'em Harel"
      },
      {
        "name" : "Yair Elboher"
      },
      {
        "name" : "Yuval Pinter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03732v1",
    "title" : "Collection, usage and privacy of mobility data in the enterprise and\n  public administrations",
    "summary" : "Human mobility data is a crucial resource for urban mobility management, but\nit does not come without personal reference. The implementation of security\nmeasures such as anonymization is thus needed to protect individuals' privacy.\nOften, a trade-off arises as such techniques potentially decrease the utility\nof the data and limit its use. While much research on anonymization techniques\nexists, there is little information on the actual implementations by\npractitioners, especially outside the big tech context. Within our study, we\nconducted expert interviews to gain insights into practices in the field. We\ncategorize purposes, data sources, analysis, and modeling tasks to provide a\nprofound understanding of the context such data is used in. We survey\nprivacy-enhancing methods in use, which generally do not comply with\nstate-of-the-art standards of differential privacy. We provide groundwork for\nfurther research on practice-oriented research by identifying privacy needs of\npractitioners and extracting relevant mobility characteristics for future\nstandardized evaluations of privacy-enhancing methods.",
    "updated" : "2024-07-04T08:29:27Z",
    "published" : "2024-07-04T08:29:27Z",
    "authors" : [
      {
        "name" : "Alexandra Kapp"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03496v1",
    "title" : "Releasing Large-Scale Human Mobility Histograms with Differential\n  Privacy",
    "summary" : "Environmental Insights Explorer (EIE) is a Google product that reports\naggregate statistics about human mobility, including various methods of transit\nused by people across roughly 50,000 regions globally. These statistics are\nused to estimate carbon emissions and provided to policymakers to inform their\ndecisions on transportation policy and infrastructure. Due to the inherent\nsensitivity of this type of user data, it is crucial that the statistics\nderived and released from it are computed with appropriate privacy protections.\nIn this work, we use a combination of federated analytics and differential\nprivacy to release these required statistics, while operating under strict\nerror constraints to ensure utility for downstream stakeholders. In this work,\nwe propose a new mechanism that achieves $ \\epsilon \\approx 2 $-DP while\nsatisfying these strict utility constraints, greatly improving over natural\nbaselines. We believe this mechanism may be of more general interest for the\nbroad class of group-by-sum workloads.",
    "updated" : "2024-07-03T20:54:00Z",
    "published" : "2024-07-03T20:54:00Z",
    "authors" : [
      {
        "name" : "Christopher Bian"
      },
      {
        "name" : "Albert Cheu"
      },
      {
        "name" : "Yannis Guzman"
      },
      {
        "name" : "Marco Gruteser"
      },
      {
        "name" : "Peter Kairouz"
      },
      {
        "name" : "Ryan McKenna"
      },
      {
        "name" : "Edo Roth"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03470v1",
    "title" : "Prosody-Driven Privacy-Preserving Dementia Detection",
    "summary" : "Speaker embeddings extracted from voice recordings have been proven valuable\nfor dementia detection. However, by their nature, these embeddings contain\nidentifiable information which raises privacy concerns. In this work, we aim to\nanonymize embeddings while preserving the diagnostic utility for dementia\ndetection. Previous studies rely on adversarial learning and models trained on\nthe target attribute and struggle in limited-resource settings. We propose a\nnovel approach that leverages domain knowledge to disentangle prosody features\nrelevant to dementia from speaker embeddings without relying on a dementia\nclassifier. Our experiments show the effectiveness of our approach in\npreserving speaker privacy (speaker recognition F1-score .01%) while\nmaintaining high dementia detection score F1-score of 74% on the ADReSS\ndataset. Our results are also on par with a more constrained\nclassifier-dependent system on ADReSSo (.01% and .66%), and have no impact on\nsynthesized speech naturalness.",
    "updated" : "2024-07-03T19:34:47Z",
    "published" : "2024-07-03T19:34:47Z",
    "authors" : [
      {
        "name" : "Dominika Woszczyk"
      },
      {
        "name" : "Ranya Aloufi"
      },
      {
        "name" : "Soteris Demetriou"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CL",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.03451v1",
    "title" : "The Role of Privacy Guarantees in Voluntary Donation of Private Data for\n  Altruistic Goals",
    "summary" : "Voluntary donation of private information for altruistic purposes, such as\nadvancing research, is common. However, concerns about data misuse and leakage\nmay deter individuals from donating their information. While prior research has\nindicated that Privacy Enhancement Technologies (PETs) can alleviate these\nconcerns, the extent to which these techniques influence willingness to donate\ndata remains unclear.\n  This study conducts a vignette survey (N=485) to examine people's willingness\nto donate medical data for developing new treatments under four privacy\nguarantees: data expiration, anonymization, use restriction, and access\ncontrol. The study explores two mechanisms for verifying these guarantees:\nself-auditing and expert auditing, and evaluates the impact on two types of\ndata recipient entities: for-profit and non-profit institutions.\n  Our findings reveal that the type of entity collecting data strongly\ninfluences respondents' privacy expectations, which in part influence their\nwillingness to donate data. Respondents have such high expectations of the\nprivacy provided by non-profit entities that explicitly stating the privacy\nprotections provided makes little adjustment to those expectations. In\ncontrast, statements about privacy bring respondents' expectations of the\nprivacy provided by for-profit entities nearly in-line with non-profit\nexpectations. We highlight the risks of these respective results as well as the\nneed for future research to better align technical community and end-user\nperceptions about the effectiveness of auditing PETs and to effectively set\nexpectations about the efficacy of PETs in the face of end-user concerns about\ndata breaches.",
    "updated" : "2024-07-03T18:50:48Z",
    "published" : "2024-07-03T18:50:48Z",
    "authors" : [
      {
        "name" : "Ruizhe Wang"
      },
      {
        "name" : "Roberta De Viti"
      },
      {
        "name" : "Aarushi Dubey"
      },
      {
        "name" : "Elissa M. Redmiles"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05450v1",
    "title" : "Understanding Professional Needs to Create Privacy-Preserving and Secure\n  Emergent Digital Artworks",
    "summary" : "In recent years, immersive art installations featuring interactive artworks\nhave been on the rise. These installations are an integral part of museums and\nart centers like selfie museums, teamLab Borderless, ARTECHOUSE, and Meow Wolf.\nMoreover, immersive art have also been increasingly incorporated into\ntraditional museums as well. However, immersive art requires active user\nparticipation and often captures information from viewers and participants\nthrough cameras, sensors, microphones, embodied interaction devices,\nsurveillance, and kinetic mirrors. Therefore, we propose a new line of research\nto examine the security and privacy postures of immersive artworks. In our\npilot study, we conducted a semi-structured interview with five experienced\npractitioners from either the art (2) or cybersecurity (3) fields. Our aim was\nto understand their current security and privacy practices, along with their\nneeds when it comes to immersive art. From their responses, we created a list\nof security and privacy parameters, such as, providing opt-in mechanics for\ndata collection, knowledge of data collection tools such as proximity sensors,\nand creating security awareness amongst participants by communicating security\nprotocols and threat models. These parameters allow us to build\nprivacy-preserving, secure, and accessible software for individuals working in\nmedia arts, who often have no background on security and privacy. In the\nfuture, we plan to utilize these parameters to develop software in response to\nthose needs and then host an art exhibition of immersive artworks utilizing the\nplatform.",
    "updated" : "2024-07-07T17:21:38Z",
    "published" : "2024-07-07T17:21:38Z",
    "authors" : [
      {
        "name" : "Kathryn Lichlyter"
      },
      {
        "name" : "Urvashi Kishnani"
      },
      {
        "name" : "Kate Hollenbach"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05446v1",
    "title" : "Towards Perceived Security, Perceived Privacy, and the Universal Design\n  of E-Payment Applications",
    "summary" : "With the growth of digital monetary transactions and cashless payments,\nencouraged by the COVID-19 pandemic, use of e-payment applications is on the\nrise. It is thus imperative to understand and evaluate the current posture of\ne-payment applications from three major user-facing angles: security, privacy,\nand usability. To this, we created a high-fidelity prototype of an e-payment\napplication that encompassed features that we wanted to test with users. We\nthen conducted a pilot study where we recruited 12 participants who tested our\nprototype. We find that both security and privacy are important for users of\ne-payment applications. Additionally, some participants perceive the strength\nof security and privacy based on the usability of the application. We provide\nrecommendations such as universal design of e-payment applications.",
    "updated" : "2024-07-07T17:15:09Z",
    "published" : "2024-07-07T17:15:09Z",
    "authors" : [
      {
        "name" : "Urvashi Kishnani"
      },
      {
        "name" : "Isabella Cardenas"
      },
      {
        "name" : "Jailene Castillo"
      },
      {
        "name" : "Rosalyn Conry"
      },
      {
        "name" : "Lukas Rodwin"
      },
      {
        "name" : "Rika Ruiz"
      },
      {
        "name" : "Matthew Walther"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05237v1",
    "title" : "Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex\n  composite losses",
    "summary" : "Differentially private stochastic gradient descent (DP-SGD) refers to a\nfamily of optimization algorithms that provide a guaranteed level of\ndifferential privacy (DP) through DP accounting techniques. However, current\naccounting techniques make assumptions that diverge significantly from\npractical DP-SGD implementations. For example, they may assume the loss\nfunction is Lipschitz continuous and convex, sample the batches randomly with\nreplacement, or omit the gradient clipping step.\n  In this work, we analyze the most commonly used variant of DP-SGD, in which\nwe sample batches cyclically with replacement, perform gradient clipping, and\nonly release the last DP-SGD iterate. More specifically - without assuming\nconvexity, smoothness, or Lipschitz continuity of the loss function - we\nestablish new R\\'enyi differential privacy (RDP) bounds for the last DP-SGD\niterate under the mild assumption that (i) the DP-SGD stepsize is small\nrelative to the topological constants in the loss function, and (ii) the loss\nfunction is weakly-convex. Moreover, we show that our bounds converge to\npreviously established convex bounds when the weak-convexity parameter of the\nobjective function approaches zero. In the case of non-Lipschitz smooth loss\nfunctions, we provide a weaker bound that scales well in terms of the number of\nDP-SGD iterations.",
    "updated" : "2024-07-07T02:35:55Z",
    "published" : "2024-07-07T02:35:55Z",
    "authors" : [
      {
        "name" : "Weiwei Kong"
      },
      {
        "name" : "Mónica Ribero"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DS",
      "math.OC",
      "stat.ML",
      "65K10 (Primary), 60G15, 68P27",
      "G.3; G.1.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.05045v1",
    "title" : "Robust Skin Color Driven Privacy Preserving Face Recognition via\n  Function Secret Sharing",
    "summary" : "In this work, we leverage the pure skin color patch from the face image as\nthe additional information to train an auxiliary skin color feature extractor\nand face recognition model in parallel to improve performance of\nstate-of-the-art (SOTA) privacy-preserving face recognition (PPFR) systems. Our\nsolution is robust against black-box attacking and well-established generative\nadversarial network (GAN) based image restoration. We analyze the potential\nrisk in previous work, where the proposed cosine similarity computation might\ndirectly leak the protected precomputed embedding stored on the server side. We\npropose a Function Secret Sharing (FSS) based face embedding comparison\nprotocol without any intermediate result leakage. In addition, we show in\nexperiments that the proposed protocol is more efficient compared to the Secret\nSharing (SS) based protocol.",
    "updated" : "2024-07-06T10:51:35Z",
    "published" : "2024-07-06T10:51:35Z",
    "authors" : [
      {
        "name" : "Dong Han"
      },
      {
        "name" : "Yufan Jiang"
      },
      {
        "name" : "Yong Li"
      },
      {
        "name" : "Ricardo Mendes"
      },
      {
        "name" : "Joachim Denzler"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04952v1",
    "title" : "Granular Privacy Control for Geolocation with Vision Language Models",
    "summary" : "Vision Language Models (VLMs) are rapidly advancing in their capability to\nanswer information-seeking questions. As these models are widely deployed in\nconsumer applications, they could lead to new privacy risks due to emergent\nabilities to identify people in photos, geolocate images, etc. As we\ndemonstrate, somewhat surprisingly, current open-source and proprietary VLMs\nare very capable image geolocators, making widespread geolocation with VLMs an\nimmediate privacy risk, rather than merely a theoretical future concern. As a\nfirst step to address this challenge, we develop a new benchmark, GPTGeoChat,\nto test the ability of VLMs to moderate geolocation dialogues with users. We\ncollect a set of 1,000 image geolocation conversations between in-house\nannotators and GPT-4v, which are annotated with the granularity of location\ninformation revealed at each turn. Using this new dataset, we evaluate the\nability of various VLMs to moderate GPT-4v geolocation conversations by\ndetermining when too much location information has been revealed. We find that\ncustom fine-tuned models perform on par with prompted API-based models when\nidentifying leaked location information at the country or city level; however,\nfine-tuning on supervised data appears to be needed to accurately moderate\nfiner granularities, such as the name of a restaurant or building.",
    "updated" : "2024-07-06T04:06:55Z",
    "published" : "2024-07-06T04:06:55Z",
    "authors" : [
      {
        "name" : "Ethan Mendes"
      },
      {
        "name" : "Yang Chen"
      },
      {
        "name" : "James Hays"
      },
      {
        "name" : "Sauvik Das"
      },
      {
        "name" : "Wei Xu"
      },
      {
        "name" : "Alan Ritter"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04906v1",
    "title" : "Privacy or Transparency? Negotiated Smartphone Access as a Signifier of\n  Trust in Romantic Relationships",
    "summary" : "In this work, we analyze two large-scale surveys to examine how individuals\nthink about sharing smartphone access with romantic partners as a function of\ntrust in relationships. We find that the majority of couples have access to\neach others' devices, but may have explicit or implicit boundaries on how this\naccess is to be used. Investigating these boundaries and related social norms,\nwe find that there is little consensus about the level of smartphone access\n(i.e., transparency), or lack thereof (i.e., privacy) that is desirable in\nromantic contexts. However, there is broad agreement that the level of access\nshould be mutual and consensual. Most individuals understand trust to be the\nbasis of their decisions about transparency and privacy. Furthermore, we find\nindividuals have crossed these boundaries, violating their partners' privacy\nand betraying their trust. We examine how, when, why, and by whom these\nbetrayals occur. We consider the ramifications of these boundary violations in\nthe case of intimate partner violence. Finally, we provide recommendations for\ndesign changes to enable technological enforcement of boundaries currently\nenforced by trust, bringing access control in line with users' sharing\npreferences.",
    "updated" : "2024-07-06T00:52:34Z",
    "published" : "2024-07-06T00:52:34Z",
    "authors" : [
      {
        "name" : "Periwinkle Doerfler"
      },
      {
        "name" : "Kieron Ivy Turk"
      },
      {
        "name" : "Chris Geeng"
      },
      {
        "name" : "Damon McCoy"
      },
      {
        "name" : "Jeffrey Ackerman"
      },
      {
        "name" : "Molly Dragiewicz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04776v1",
    "title" : "Quantifying Privacy Risks of Public Statistics to Residents of\n  Subsidized Housing",
    "summary" : "As the U.S. Census Bureau implements its controversial new disclosure\navoidance system, researchers and policymakers debate the necessity of new\nprivacy protections for public statistics. With experiments on both published\nstatistics and synthetic data, we explore a particular privacy concern:\nrespondents in subsidized housing may deliberately not mention unauthorized\nchildren and other household members for fear of being evicted. By combining\npublic statistics from the Decennial Census and the Department of Housing and\nUrban Development, we demonstrate a simple, inexpensive reconstruction attack\nthat could identify subsidized households living in violation of occupancy\nguidelines in 2010. Experiments on synthetic data suggest that a random\nswapping mechanism similar to the Census Bureau's 2010 disclosure avoidance\nmeasures does not significantly reduce the precision of this attack, while a\ndifferentially private mechanism similar to the 2020 disclosure avoidance\nsystem does. Our results provide a valuable example for policymakers seeking a\ntrustworthy, accurate census.",
    "updated" : "2024-07-05T18:00:02Z",
    "published" : "2024-07-05T18:00:02Z",
    "authors" : [
      {
        "name" : "Ryan Steed"
      },
      {
        "name" : "Diana Qing"
      },
      {
        "name" : "Zhiwei Steven Wu"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2407.04751v1",
    "title" : "A Unified Learn-to-Distort-Data Framework for Privacy-Utility Trade-off\n  in Trustworthy Federated Learning",
    "summary" : "In this paper, we first give an introduction to the theoretical basis of the\nprivacy-utility equilibrium in federated learning based on Bayesian privacy\ndefinitions and total variation distance privacy definitions. We then present\nthe \\textit{Learn-to-Distort-Data} framework, which provides a principled\napproach to navigate the privacy-utility equilibrium by explicitly modeling the\ndistortion introduced by the privacy-preserving mechanism as a learnable\nvariable and optimizing it jointly with the model parameters. We demonstrate\nthe applicability of our framework to a variety of privacy-preserving\nmechanisms on the basis of data distortion and highlight its connections to\nrelated areas such as adversarial training, input robustness, and unlearnable\nexamples. These connections enable leveraging techniques from these areas to\ndesign effective algorithms for privacy-utility equilibrium in federated\nlearning under the \\textit{Learn-to-Distort-Data} framework.",
    "updated" : "2024-07-05T08:15:09Z",
    "published" : "2024-07-05T08:15:09Z",
    "authors" : [
      {
        "name" : "Xiaojin Zhang"
      },
      {
        "name" : "Mingcong Xu"
      },
      {
        "name" : "Wei Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  }
]