[
  {
    "id" : "http://arxiv.org/abs/2506.02998v1",
    "title" : "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy\n  Question-Answering Systems",
    "summary" : "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information.",
    "updated" : "2025-06-03T15:32:20Z",
    "published" : "2025-06-03T15:32:20Z",
    "authors" : [
      {
        "name" : "Đorđe Klisura"
      },
      {
        "name" : "Astrid R Bernaga Torres"
      },
      {
        "name" : "Anna Karen Gárate-Escamilla"
      },
      {
        "name" : "Rajesh Roshan Biswal"
      },
      {
        "name" : "Ke Yang"
      },
      {
        "name" : "Hilal Pataci"
      },
      {
        "name" : "Anthony Rios"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02965v1",
    "title" : "Memory-Efficient and Privacy-Preserving Collaborative Training for\n  Mixture-of-Experts LLMs",
    "summary" : "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks.",
    "updated" : "2025-06-03T15:00:18Z",
    "published" : "2025-06-03T15:00:18Z",
    "authors" : [
      {
        "name" : "Ze Yu Zhang"
      },
      {
        "name" : "Bolin Ding"
      },
      {
        "name" : "Bryan Kian Hsiang Low"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02725v1",
    "title" : "Recursive Privacy-Preserving Estimation Over Markov Fading Channels",
    "summary" : "In industrial applications, the presence of moving machinery, vehicles, and\npersonnel, contributes to the dynamic nature of the wireless channel. This time\nvariability induces channel fading, which can be effectively modeled using a\nMarkov fading channel (MFC). In this paper, we investigate the problem of\nsecure state estimation for systems that communicate over a MFC in the presence\nof an eavesdropper. The objective is to enable a remote authorized user to\naccurately estimate the states of a dynamic system, while considering the\npotential interception of the sensor's packet through a wiretap channel. To\nprevent information leakage, a novel co-design strategy is established, which\ncombines a privacy-preserving mechanism with a state estimator. To implement\nour encoding scheme, a nonlinear mapping of the innovation is introduced based\non the weighted reconstructed innovation previously received by the legitimate\nuser. Corresponding to this encoding scheme, we design a recursive\nprivacy-preserving filtering algorithm to achieve accurate estimation. The\nboundedness of estimation error dynamics at the legitimate user's side is\ndiscussed and the divergence of the eavesdropper's estimation error is\nanalyzed, which demonstrates the effectiveness of our co-design strategy in\nensuring secrecy. Furthermore, a simulation example of a three-tank system is\nprovided to demonstrate the effectiveness and feasibility of our\nprivacy-preserving estimation method.",
    "updated" : "2025-06-03T10:33:49Z",
    "published" : "2025-06-03T10:33:49Z",
    "authors" : [
      {
        "name" : "Jie Huang"
      },
      {
        "name" : "Fanlin Jia"
      },
      {
        "name" : "Xiao He"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02711v1",
    "title" : "Privacy Leaks by Adversaries: Adversarial Iterations for Membership\n  Inference Attack",
    "summary" : "Membership inference attack (MIA) has become one of the most widely used and\neffective methods for evaluating the privacy risks of machine learning models.\nThese attacks aim to determine whether a specific sample is part of the model's\ntraining set by analyzing the model's output. While traditional membership\ninference attacks focus on leveraging the model's posterior output, such as\nconfidence on the target sample, we propose IMIA, a novel attack strategy that\nutilizes the process of generating adversarial samples to infer membership. We\npropose to infer the member properties of the target sample using the number of\niterations required to generate its adversarial sample. We conduct experiments\nacross multiple models and datasets, and our results demonstrate that the\nnumber of iterations for generating an adversarial sample is a reliable feature\nfor membership inference, achieving strong performance both in black-box and\nwhite-box attack scenarios. This work provides a new perspective for evaluating\nmodel privacy and highlights the potential of adversarial example-based\nfeatures for privacy leakage assessment.",
    "updated" : "2025-06-03T10:09:24Z",
    "published" : "2025-06-03T10:09:24Z",
    "authors" : [
      {
        "name" : "Jing Xue"
      },
      {
        "name" : "Zhishen Sun"
      },
      {
        "name" : "Haishan Ye"
      },
      {
        "name" : "Luo Luo"
      },
      {
        "name" : "Xiangyu Chang"
      },
      {
        "name" : "Ivor Tsang"
      },
      {
        "name" : "Guang Dai"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02563v1",
    "title" : "Privacy-Preserving Federated Convex Optimization: Balancing\n  Partial-Participation and Efficiency via Noise Cancellation",
    "summary" : "This paper tackles the challenge of achieving Differential Privacy (DP) in\nFederated Learning (FL) under partial-participation, where only a subset of the\nmachines participate in each time-step. While previous work achieved optimal\nperformance in full-participation settings, these methods struggled to extend\nto partial-participation scenarios. Our approach fills this gap by introducing\na novel noise-cancellation mechanism that preserves privacy without sacrificing\nconvergence rates or computational efficiency. We analyze our method within the\nStochastic Convex Optimization (SCO) framework and show that it delivers\noptimal performance for both homogeneous and heterogeneous data distributions.\nThis work expands the applicability of DP in FL, offering an efficient and\npractical solution for privacy-preserving learning in distributed systems with\npartial participation.",
    "updated" : "2025-06-03T07:48:35Z",
    "published" : "2025-06-03T07:48:35Z",
    "authors" : [
      {
        "name" : "Roie Reshef"
      },
      {
        "name" : "Kfir Yehuda Levy"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02422v1",
    "title" : "Enhancing Convergence, Privacy and Fairness for Wireless Personalized\n  Federated Learning: Quantization-Assisted Min-Max Fair Scheduling",
    "summary" : "Personalized federated learning (PFL) offers a solution to balancing\npersonalization and generalization by conducting federated learning (FL) to\nguide personalized learning (PL). Little attention has been given to wireless\nPFL (WPFL), where privacy concerns arise. Performance fairness of PL models is\nanother challenge resulting from communication bottlenecks in WPFL. This paper\nexploits quantization errors to enhance the privacy of WPFL and proposes a\nnovel quantization-assisted Gaussian differential privacy (DP) mechanism. We\nanalyze the convergence upper bounds of individual PL models by considering the\nimpact of the mechanism (i.e., quantization errors and Gaussian DP noises) and\nimperfect communication channels on the FL of WPFL. By minimizing the maximum\nof the bounds, we design an optimal transmission scheduling strategy that\nyields min-max fairness for WPFL with OFDMA interfaces. This is achieved by\nrevealing the nested structure of this problem to decouple it into subproblems\nsolved sequentially for the client selection, channel allocation, and power\ncontrol, and for the learning rates and PL-FL weighting coefficients.\nExperiments validate our analysis and demonstrate that our approach\nsubstantially outperforms alternative scheduling strategies by 87.08%, 16.21%,\nand 38.37% in accuracy, the maximum test loss of participating clients, and\nfairness (Jain's index), respectively.",
    "updated" : "2025-06-03T04:13:07Z",
    "published" : "2025-06-03T04:13:07Z",
    "authors" : [
      {
        "name" : "Xiyu Zhao"
      },
      {
        "name" : "Qimei Cui"
      },
      {
        "name" : "Ziqiang Du"
      },
      {
        "name" : "Weicai Li"
      },
      {
        "name" : "Xi Yu"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "Ji Zhang"
      },
      {
        "name" : "Xiaofeng Tao"
      },
      {
        "name" : "Ping Zhang"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02410v1",
    "title" : "Testing for large-dimensional covariance matrix under differential\n  privacy",
    "summary" : "The increasing prevalence of high-dimensional data across various\napplications has raised significant privacy concerns in statistical inference.\nIn this paper, we propose a differentially private integrated statistic for\ntesting large-dimensional covariance structures, enabling accurate statistical\ninsights while safeguarding privacy. First, we analyze the global sensitivity\nof sample eigenvalues for sub-Gaussian populations, where our method bypasses\nthe commonly assumed boundedness of data covariates. For sufficiently large\nsample size, the privatized statistic guarantees privacy with high probability.\nFurthermore, when the ratio of dimension to sample size, $d/n \\to y \\in (0,\n\\infty)$, the privatized test is asymptotically distribution-free with\nwell-known critical values, and detects the local alternative hypotheses\ndistinct from the null at the fastest rate of $1/\\sqrt{n}$. Extensive numerical\nstudies on synthetic and real data showcase the validity and powerfulness of\nour proposed method.",
    "updated" : "2025-06-03T03:53:51Z",
    "published" : "2025-06-03T03:53:51Z",
    "authors" : [
      {
        "name" : "Shiwei Sang"
      },
      {
        "name" : "Yicheng Zeng"
      },
      {
        "name" : "Xuehu Zhu"
      },
      {
        "name" : "Shurong Zheng"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02156v1",
    "title" : "Mitigating Data Poisoning Attacks to Local Differential Privacy",
    "summary" : "The distributed nature of local differential privacy (LDP) invites data\npoisoning attacks and poses unforeseen threats to the underlying LDP-supported\napplications. In this paper, we propose a comprehensive mitigation framework\nfor popular frequency estimation, which contains a suite of novel defenses,\nincluding malicious user detection, attack pattern recognition, and damaged\nutility recovery. In addition to existing attacks, we explore new adaptive\nadversarial activities for our mitigation design. For detection, we present a\nnew method to precisely identify bogus reports and thus LDP aggregation can be\nperformed over the ``clean'' data. When the attack behavior becomes stealthy\nand direct filtering out malicious users is difficult, we further propose a\ndetection that can effectively recognize hidden adversarial patterns, thus\nfacilitating the decision-making of service providers. These detection methods\nrequire no additional data and attack information and incur minimal\ncomputational cost. Our experiment demonstrates their excellent performance and\nsubstantial improvement over previous work in various settings. In addition, we\nconduct an empirical analysis of LDP post-processing for corrupted data\nrecovery and propose a new post-processing method, through which we reveal new\ninsights into protocol recommendations in practice and key design principles\nfor future research.",
    "updated" : "2025-06-02T18:37:15Z",
    "published" : "2025-06-02T18:37:15Z",
    "authors" : [
      {
        "name" : "Xiaolin Li"
      },
      {
        "name" : "Ninghui Li"
      },
      {
        "name" : "Boyang Wang"
      },
      {
        "name" : "Wenhai Sun"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01907v1",
    "title" : "SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data",
    "summary" : "Privacy-preserving data publication, including synthetic data sharing, often\nexperiences trade-offs between privacy and utility. Synthetic data is generally\nmore effective than data anonymization in balancing this trade-off, however,\nnot without its own challenges. Synthetic data produced by generative models\ntrained on source data may inadvertently reveal information about outliers.\nTechniques specifically designed for preserving privacy, such as introducing\nnoise to satisfy differential privacy, often incur unpredictable and\nsignificant losses in utility. In this work we show that, with the right\nmechanism of synthetic data generation, we can achieve strong privacy\nprotection without significant utility loss. Synthetic data generators\nproducing contracting data patterns, such as Synthetic Minority Over-sampling\nTechnique (SMOTE), can enhance a differentially private data generator,\nleveraging the strengths of both. We prove in theory and through empirical\ndemonstration that this SMOTE-DP technique can produce synthetic data that not\nonly ensures robust privacy protection but maintains utility in downstream\nlearning tasks.",
    "updated" : "2025-06-02T17:27:10Z",
    "published" : "2025-06-02T17:27:10Z",
    "authors" : [
      {
        "name" : "Yan Zhou"
      },
      {
        "name" : "Bradley Malin"
      },
      {
        "name" : "Murat Kantarcioglu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01425v1",
    "title" : "CSVAR: Enhancing Visual Privacy in Federated Learning via Adaptive\n  Shuffling Against Overfitting",
    "summary" : "Although federated learning preserves training data within local privacy\ndomains, the aggregated model parameters may still reveal private\ncharacteristics. This vulnerability stems from clients' limited training data,\nwhich predisposes models to overfitting. Such overfitting enables models to\nmemorize distinctive patterns from training samples, thereby amplifying the\nsuccess probability of privacy attacks like membership inference. To enhance\nvisual privacy protection in FL, we present CSVAR(Channel-Wise Spatial Image\nShuffling with Variance-Guided Adaptive Region Partitioning), a novel image\nshuffling framework to generate obfuscated images for secure data transmission\nand each training epoch, addressing both overfitting-induced privacy leaks and\nraw image transmission risks. CSVAR adopts region-variance as the metric to\nmeasure visual privacy sensitivity across image regions. Guided by this, CSVAR\nadaptively partitions each region into multiple blocks, applying fine-grained\npartitioning to privacy-sensitive regions with high region-variances for\nenhancing visual privacy protection and coarse-grained partitioning to\nprivacy-insensitive regions for balancing model utility. In each region, CSVAR\nthen shuffles between blocks in both the spatial domains and chromatic channels\nto hide visual spatial features and disrupt color distribution. Experimental\nevaluations conducted on diverse real-world datasets demonstrate that CSVAR is\ncapable of generating visually obfuscated images that exhibit high perceptual\nambiguity to human eyes, simultaneously mitigating the effectiveness of\nadversarial data reconstruction attacks and achieving a good trade-off between\nvisual privacy protection and model utility.",
    "updated" : "2025-06-02T08:30:12Z",
    "published" : "2025-06-02T08:30:12Z",
    "authors" : [
      {
        "name" : "Zhuo Chen"
      },
      {
        "name" : "Zhenya Ma"
      },
      {
        "name" : "Yan Zhang"
      },
      {
        "name" : "Donghua Cai"
      },
      {
        "name" : "Ye Zhang"
      },
      {
        "name" : "Qiushi Li"
      },
      {
        "name" : "Yongheng Deng"
      },
      {
        "name" : "Ye Guo"
      },
      {
        "name" : "Ju Ren"
      },
      {
        "name" : " Xuemin"
      },
      {
        "name" : " Shen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01325v1",
    "title" : "Understanding the Identity-Transformation Approach in OIDC-Compatible\n  Privacy-Preserving SSO Services",
    "summary" : "OpenID Connect (OIDC) enables a user with commercial-off-the-shelf browsers\nto log into multiple websites, called relying parties (RPs), by her username\nand credential set up in another trusted web system, called the identity\nprovider (IdP). Identity transformations are proposed in UppreSSO to provide\nOIDC-compatible SSO services, preventing both IdP-based login tracing and\nRP-based identity linkage. While security and privacy of SSO services in\nUppreSSO have been proved, several essential issues of this\nidentity-transformation approach are not well studied. In this paper, we\ncomprehensively investigate the approach as below. Firstly, several suggestions\nfor the efficient integration of identity transformations in OIDC-compatible\nSSO are explained. Then, we uncover the relationship between\nidentity-transformations in SSO and oblivious pseudo-random functions (OPRFs),\nand present two variations of the properties required for SSO security as well\nas the privacy requirements, to analyze existing OPRF protocols. Finally, new\nidentity transformations different from those designed in UppreSSO, are\nconstructed based on OPRFs, satisfying different variations of SSO security\nrequirements. To the best of our knowledge, this is the first time to uncover\nthe relationship between identity transformations in OIDC-compatible\nprivacy-preserving SSO services and OPRFs, and prove the SSO-related properties\n(i.e., key-identifier freeness, RP designation and user identification) of OPRF\nprotocols, in addition to the basic properties of correctness, obliviousness\nand pseudo-randomness.",
    "updated" : "2025-06-02T05:11:01Z",
    "published" : "2025-06-02T05:11:01Z",
    "authors" : [
      {
        "name" : "Jingqiang Lin"
      },
      {
        "name" : "Baitao Zhang"
      },
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Quanwei Cai"
      },
      {
        "name" : "Jiwu Jing"
      },
      {
        "name" : "Huiyang He"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02063v1",
    "title" : "Privacy-Aware, Public-Aligned: Embedding Risk Detection and Public\n  Values into Scalable Clinical Text De-Identification for Trusted Research\n  Environments",
    "summary" : "Clinical free-text data offers immense potential to improve population health\nresearch such as richer phenotyping, symptom tracking, and contextual\nunderstanding of patient care. However, these data present significant privacy\nrisks due to the presence of directly or indirectly identifying information\nembedded in unstructured narratives. While numerous de-identification tools\nhave been developed, few have been tested on real-world, heterogeneous datasets\nat scale or assessed for governance readiness. In this paper, we synthesise our\nfindings from previous studies examining the privacy-risk landscape across\nmultiple document types and NHS data providers in Scotland. We characterise how\ndirect and indirect identifiers vary by record type, clinical setting, and data\nflow, and show how changes in documentation practice can degrade model\nperformance over time. Through public engagement, we explore societal\nexpectations around the safe use of clinical free text and reflect these in the\ndesign of a prototype privacy-risk management tool to support transparent,\nauditable decision-making. Our findings highlight that privacy risk is\ncontext-dependent and cumulative, underscoring the need for adaptable, hybrid\nde-identification approaches that combine rule-based precision with contextual\nunderstanding. We offer a comprehensive view of the challenges and\nopportunities for safe, scalable reuse of clinical free-text within Trusted\nResearch Environments and beyond, grounded in both technical evidence and\npublic perspectives on responsible data use.",
    "updated" : "2025-06-01T17:45:57Z",
    "published" : "2025-06-01T17:45:57Z",
    "authors" : [
      {
        "name" : "Arlene Casey"
      },
      {
        "name" : "Stuart Dunbar"
      },
      {
        "name" : "Franz Gruber"
      },
      {
        "name" : "Samuel McInerney"
      },
      {
        "name" : "Matúš Falis"
      },
      {
        "name" : "Pamela Linksted"
      },
      {
        "name" : "Katie Wilde"
      },
      {
        "name" : "Kathy Harrison"
      },
      {
        "name" : "Alison Hamilton"
      },
      {
        "name" : "Christian Cole"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01072v1",
    "title" : "IDCloak: A Practical Secure Multi-party Dataset Join Framework for\n  Vertical Privacy-preserving Machine Learning",
    "summary" : "Vertical privacy-preserving machine learning (vPPML) enables multiple parties\nto train models on their vertically distributed datasets while keeping datasets\nprivate. In vPPML, it is critical to perform the secure dataset join, which\naligns features corresponding to intersection IDs across datasets and forms a\nsecret-shared and joint training dataset. However, existing methods for this\nstep could be impractical due to: (1) they are insecure when they expose\nintersection IDs; or (2) they rely on a strong trust assumption requiring a\nnon-colluding auxiliary server; or (3) they are limited to the two-party\nsetting.\n  This paper proposes IDCloak, the first practical secure multi-party dataset\njoin framework for vPPML that keeps IDs private without a non-colluding\nauxiliary server. IDCloak consists of two protocols: (1) a circuit-based\nmulti-party private set intersection protocol (cmPSI), which obtains\nsecret-shared flags indicating intersection IDs via an optimized communication\nstructure combining OKVS and OPRF; (2) a secure multi-party feature alignment\nprotocol, which obtains the secret-shared and joint dataset using secret-shared\nflags, via our proposed efficient secure shuffle protocol. Experiments show\nthat: (1) compared to the state-of-the-art secure two-party dataset join\nframework (iPrivjoin), IDCloak demonstrates higher efficiency in the two-party\nsetting and comparable performance when the party number increases; (2)\ncompared to the state-of-the-art cmPSI protocol under honest majority, our\nproposed cmPSI protocol provides a stronger security guarantee (dishonest\nmajority) while improving efficiency by up to $7.78\\times$ in time and\n$8.73\\times$ in communication sizes; (3) our proposed secure shuffle protocol\noutperforms the state-of-the-art shuffle protocol by up to $138.34\\times$ in\ntime and $132.13\\times$ in communication sizes.",
    "updated" : "2025-06-01T16:20:39Z",
    "published" : "2025-06-01T16:20:39Z",
    "authors" : [
      {
        "name" : "Shuyu Chen"
      },
      {
        "name" : "Guopeng Lin"
      },
      {
        "name" : "Haoyu Niu"
      },
      {
        "name" : "Lushan Song"
      },
      {
        "name" : "Chengxun Hong"
      },
      {
        "name" : "Weili Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00759v1",
    "title" : "Understanding and Mitigating Cross-lingual Privacy Leakage via\n  Language-specific and Universal Privacy Neurons",
    "summary" : "Large Language Models (LLMs) trained on massive data capture rich information\nembedded in the training data. However, this also introduces the risk of\nprivacy leakage, particularly involving personally identifiable information\n(PII). Although previous studies have shown that this risk can be mitigated\nthrough methods such as privacy neurons, they all assume that both the\n(sensitive) training data and user queries are in English. We show that they\ncannot defend against the privacy leakage in cross-lingual contexts: even if\nthe training data is exclusively in one language, these (private) models may\nstill reveal private information when queried in another language. In this\nwork, we first investigate the information flow of cross-lingual privacy\nleakage to give a better understanding. We find that LLMs process private\ninformation in the middle layers, where representations are largely shared\nacross languages. The risk of leakage peaks when converted to a\nlanguage-specific space in later layers. Based on this, we identify\nprivacy-universal neurons and language-specific privacy neurons.\nPrivacy-universal neurons influence privacy leakage across all languages, while\nlanguage-specific privacy neurons are only related to specific languages. By\ndeactivating these neurons, the cross-lingual privacy leakage risk is reduced\nby 23.3%-31.6%.",
    "updated" : "2025-06-01T00:10:30Z",
    "published" : "2025-06-01T00:10:30Z",
    "authors" : [
      {
        "name" : "Wenshuo Dong"
      },
      {
        "name" : "Qingsong Yang"
      },
      {
        "name" : "Shu Yang"
      },
      {
        "name" : "Lijie Hu"
      },
      {
        "name" : "Meng Ding"
      },
      {
        "name" : "Wanyu Lin"
      },
      {
        "name" : "Tianhang Zheng"
      },
      {
        "name" : "Di Wang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.04036v1",
    "title" : "Privacy and Security Threat for OpenAI GPTs",
    "summary" : "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications.",
    "updated" : "2025-06-04T14:58:29Z",
    "published" : "2025-06-04T14:58:29Z",
    "authors" : [
      {
        "name" : "Wei Wenying"
      },
      {
        "name" : "Zhao Kaifa"
      },
      {
        "name" : "Xue Lei"
      },
      {
        "name" : "Fan Ming"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.03870v1",
    "title" : "Evaluating Apple Intelligence's Writing Tools for Privacy Against Large\n  Language Model-Based Inference Attacks: Insights from Early Datasets",
    "summary" : "The misuse of Large Language Models (LLMs) to infer emotions from text for\nmalicious purposes, known as emotion inference attacks, poses a significant\nthreat to user privacy. In this paper, we investigate the potential of Apple\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\nmitigate these risks through text modifications such as rewriting and tone\nadjustment. By developing early novel datasets specifically for this purpose,\nwe empirically assess how different text modifications influence LLM-based\ndetection. This capability suggests strong potential for Apple Intelligence's\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\nfor future adaptive rewriting systems capable of dynamically neutralizing\nsensitive emotional content to enhance user privacy. To the best of our\nknowledge, this research provides the first empirical analysis of Apple\nIntelligence's text-modification tools within a privacy-preservation context\nwith the broader goal of developing on-device, user-centric privacy-preserving\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\nsystems.",
    "updated" : "2025-06-04T12:01:17Z",
    "published" : "2025-06-04T12:01:17Z",
    "authors" : [
      {
        "name" : "Mohd. Farhan Israk Soumik"
      },
      {
        "name" : "Syed Mhamudul Hasan"
      },
      {
        "name" : "Abdur R. Shahid"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.03618v1",
    "title" : "GCFL: A Gradient Correction-based Federated Learning Framework for\n  Privacy-preserving CPSS",
    "summary" : "Federated learning, as a distributed architecture, shows great promise for\napplications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the\nprivacy risks inherent in CPSS, the integration of differential privacy with\nfederated learning has attracted considerable attention. Existing research\nmainly focuses on dynamically adjusting the noise added or discarding certain\ngradients to mitigate the noise introduced by differential privacy. However,\nthese approaches fail to remove the noise that hinders convergence and correct\nthe gradients affected by the noise, which significantly reduces the accuracy\nof model classification. To overcome these challenges, this paper proposes a\nnovel framework for differentially private federated learning that balances\nrigorous privacy guarantees with accuracy by introducing a server-side gradient\ncorrection mechanism. Specifically, after clients perform gradient clipping and\nnoise perturbation, our framework detects deviations in the noisy local\ngradients and employs a projection mechanism to correct them, mitigating the\nnegative impact of noise. Simultaneously, gradient projection promotes the\nalignment of gradients from different clients and guides the model towards\nconvergence to a global optimum. We evaluate our framework on several benchmark\ndatasets, and the experimental results demonstrate that it achieves\nstate-of-the-art performance under the same privacy budget.",
    "updated" : "2025-06-04T06:52:37Z",
    "published" : "2025-06-04T06:52:37Z",
    "authors" : [
      {
        "name" : "Jiayi Wan"
      },
      {
        "name" : "Xiang Zhu"
      },
      {
        "name" : "Fanzhen Liu"
      },
      {
        "name" : "Wei Fan"
      },
      {
        "name" : "Xiaolong Xu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02965v2",
    "title" : "PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training\n  for Mixture-of-Experts LLMs",
    "summary" : "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks.",
    "updated" : "2025-06-04T05:38:31Z",
    "published" : "2025-06-03T15:00:18Z",
    "authors" : [
      {
        "name" : "Ze Yu Zhang"
      },
      {
        "name" : "Bolin Ding"
      },
      {
        "name" : "Bryan Kian Hsiang Low"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  }
]