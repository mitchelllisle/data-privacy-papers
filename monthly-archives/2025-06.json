[
  {
    "id" : "http://arxiv.org/abs/2506.02998v1",
    "title" : "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy\n  Question-Answering Systems",
    "summary" : "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information.",
    "updated" : "2025-06-03T15:32:20Z",
    "published" : "2025-06-03T15:32:20Z",
    "authors" : [
      {
        "name" : "Đorđe Klisura"
      },
      {
        "name" : "Astrid R Bernaga Torres"
      },
      {
        "name" : "Anna Karen Gárate-Escamilla"
      },
      {
        "name" : "Rajesh Roshan Biswal"
      },
      {
        "name" : "Ke Yang"
      },
      {
        "name" : "Hilal Pataci"
      },
      {
        "name" : "Anthony Rios"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02965v1",
    "title" : "Memory-Efficient and Privacy-Preserving Collaborative Training for\n  Mixture-of-Experts LLMs",
    "summary" : "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks.",
    "updated" : "2025-06-03T15:00:18Z",
    "published" : "2025-06-03T15:00:18Z",
    "authors" : [
      {
        "name" : "Ze Yu Zhang"
      },
      {
        "name" : "Bolin Ding"
      },
      {
        "name" : "Bryan Kian Hsiang Low"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02725v1",
    "title" : "Recursive Privacy-Preserving Estimation Over Markov Fading Channels",
    "summary" : "In industrial applications, the presence of moving machinery, vehicles, and\npersonnel, contributes to the dynamic nature of the wireless channel. This time\nvariability induces channel fading, which can be effectively modeled using a\nMarkov fading channel (MFC). In this paper, we investigate the problem of\nsecure state estimation for systems that communicate over a MFC in the presence\nof an eavesdropper. The objective is to enable a remote authorized user to\naccurately estimate the states of a dynamic system, while considering the\npotential interception of the sensor's packet through a wiretap channel. To\nprevent information leakage, a novel co-design strategy is established, which\ncombines a privacy-preserving mechanism with a state estimator. To implement\nour encoding scheme, a nonlinear mapping of the innovation is introduced based\non the weighted reconstructed innovation previously received by the legitimate\nuser. Corresponding to this encoding scheme, we design a recursive\nprivacy-preserving filtering algorithm to achieve accurate estimation. The\nboundedness of estimation error dynamics at the legitimate user's side is\ndiscussed and the divergence of the eavesdropper's estimation error is\nanalyzed, which demonstrates the effectiveness of our co-design strategy in\nensuring secrecy. Furthermore, a simulation example of a three-tank system is\nprovided to demonstrate the effectiveness and feasibility of our\nprivacy-preserving estimation method.",
    "updated" : "2025-06-03T10:33:49Z",
    "published" : "2025-06-03T10:33:49Z",
    "authors" : [
      {
        "name" : "Jie Huang"
      },
      {
        "name" : "Fanlin Jia"
      },
      {
        "name" : "Xiao He"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02711v1",
    "title" : "Privacy Leaks by Adversaries: Adversarial Iterations for Membership\n  Inference Attack",
    "summary" : "Membership inference attack (MIA) has become one of the most widely used and\neffective methods for evaluating the privacy risks of machine learning models.\nThese attacks aim to determine whether a specific sample is part of the model's\ntraining set by analyzing the model's output. While traditional membership\ninference attacks focus on leveraging the model's posterior output, such as\nconfidence on the target sample, we propose IMIA, a novel attack strategy that\nutilizes the process of generating adversarial samples to infer membership. We\npropose to infer the member properties of the target sample using the number of\niterations required to generate its adversarial sample. We conduct experiments\nacross multiple models and datasets, and our results demonstrate that the\nnumber of iterations for generating an adversarial sample is a reliable feature\nfor membership inference, achieving strong performance both in black-box and\nwhite-box attack scenarios. This work provides a new perspective for evaluating\nmodel privacy and highlights the potential of adversarial example-based\nfeatures for privacy leakage assessment.",
    "updated" : "2025-06-03T10:09:24Z",
    "published" : "2025-06-03T10:09:24Z",
    "authors" : [
      {
        "name" : "Jing Xue"
      },
      {
        "name" : "Zhishen Sun"
      },
      {
        "name" : "Haishan Ye"
      },
      {
        "name" : "Luo Luo"
      },
      {
        "name" : "Xiangyu Chang"
      },
      {
        "name" : "Ivor Tsang"
      },
      {
        "name" : "Guang Dai"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02563v1",
    "title" : "Privacy-Preserving Federated Convex Optimization: Balancing\n  Partial-Participation and Efficiency via Noise Cancellation",
    "summary" : "This paper tackles the challenge of achieving Differential Privacy (DP) in\nFederated Learning (FL) under partial-participation, where only a subset of the\nmachines participate in each time-step. While previous work achieved optimal\nperformance in full-participation settings, these methods struggled to extend\nto partial-participation scenarios. Our approach fills this gap by introducing\na novel noise-cancellation mechanism that preserves privacy without sacrificing\nconvergence rates or computational efficiency. We analyze our method within the\nStochastic Convex Optimization (SCO) framework and show that it delivers\noptimal performance for both homogeneous and heterogeneous data distributions.\nThis work expands the applicability of DP in FL, offering an efficient and\npractical solution for privacy-preserving learning in distributed systems with\npartial participation.",
    "updated" : "2025-06-03T07:48:35Z",
    "published" : "2025-06-03T07:48:35Z",
    "authors" : [
      {
        "name" : "Roie Reshef"
      },
      {
        "name" : "Kfir Yehuda Levy"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02422v1",
    "title" : "Enhancing Convergence, Privacy and Fairness for Wireless Personalized\n  Federated Learning: Quantization-Assisted Min-Max Fair Scheduling",
    "summary" : "Personalized federated learning (PFL) offers a solution to balancing\npersonalization and generalization by conducting federated learning (FL) to\nguide personalized learning (PL). Little attention has been given to wireless\nPFL (WPFL), where privacy concerns arise. Performance fairness of PL models is\nanother challenge resulting from communication bottlenecks in WPFL. This paper\nexploits quantization errors to enhance the privacy of WPFL and proposes a\nnovel quantization-assisted Gaussian differential privacy (DP) mechanism. We\nanalyze the convergence upper bounds of individual PL models by considering the\nimpact of the mechanism (i.e., quantization errors and Gaussian DP noises) and\nimperfect communication channels on the FL of WPFL. By minimizing the maximum\nof the bounds, we design an optimal transmission scheduling strategy that\nyields min-max fairness for WPFL with OFDMA interfaces. This is achieved by\nrevealing the nested structure of this problem to decouple it into subproblems\nsolved sequentially for the client selection, channel allocation, and power\ncontrol, and for the learning rates and PL-FL weighting coefficients.\nExperiments validate our analysis and demonstrate that our approach\nsubstantially outperforms alternative scheduling strategies by 87.08%, 16.21%,\nand 38.37% in accuracy, the maximum test loss of participating clients, and\nfairness (Jain's index), respectively.",
    "updated" : "2025-06-03T04:13:07Z",
    "published" : "2025-06-03T04:13:07Z",
    "authors" : [
      {
        "name" : "Xiyu Zhao"
      },
      {
        "name" : "Qimei Cui"
      },
      {
        "name" : "Ziqiang Du"
      },
      {
        "name" : "Weicai Li"
      },
      {
        "name" : "Xi Yu"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "Ji Zhang"
      },
      {
        "name" : "Xiaofeng Tao"
      },
      {
        "name" : "Ping Zhang"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02410v1",
    "title" : "Testing for large-dimensional covariance matrix under differential\n  privacy",
    "summary" : "The increasing prevalence of high-dimensional data across various\napplications has raised significant privacy concerns in statistical inference.\nIn this paper, we propose a differentially private integrated statistic for\ntesting large-dimensional covariance structures, enabling accurate statistical\ninsights while safeguarding privacy. First, we analyze the global sensitivity\nof sample eigenvalues for sub-Gaussian populations, where our method bypasses\nthe commonly assumed boundedness of data covariates. For sufficiently large\nsample size, the privatized statistic guarantees privacy with high probability.\nFurthermore, when the ratio of dimension to sample size, $d/n \\to y \\in (0,\n\\infty)$, the privatized test is asymptotically distribution-free with\nwell-known critical values, and detects the local alternative hypotheses\ndistinct from the null at the fastest rate of $1/\\sqrt{n}$. Extensive numerical\nstudies on synthetic and real data showcase the validity and powerfulness of\nour proposed method.",
    "updated" : "2025-06-03T03:53:51Z",
    "published" : "2025-06-03T03:53:51Z",
    "authors" : [
      {
        "name" : "Shiwei Sang"
      },
      {
        "name" : "Yicheng Zeng"
      },
      {
        "name" : "Xuehu Zhu"
      },
      {
        "name" : "Shurong Zheng"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02156v1",
    "title" : "Mitigating Data Poisoning Attacks to Local Differential Privacy",
    "summary" : "The distributed nature of local differential privacy (LDP) invites data\npoisoning attacks and poses unforeseen threats to the underlying LDP-supported\napplications. In this paper, we propose a comprehensive mitigation framework\nfor popular frequency estimation, which contains a suite of novel defenses,\nincluding malicious user detection, attack pattern recognition, and damaged\nutility recovery. In addition to existing attacks, we explore new adaptive\nadversarial activities for our mitigation design. For detection, we present a\nnew method to precisely identify bogus reports and thus LDP aggregation can be\nperformed over the ``clean'' data. When the attack behavior becomes stealthy\nand direct filtering out malicious users is difficult, we further propose a\ndetection that can effectively recognize hidden adversarial patterns, thus\nfacilitating the decision-making of service providers. These detection methods\nrequire no additional data and attack information and incur minimal\ncomputational cost. Our experiment demonstrates their excellent performance and\nsubstantial improvement over previous work in various settings. In addition, we\nconduct an empirical analysis of LDP post-processing for corrupted data\nrecovery and propose a new post-processing method, through which we reveal new\ninsights into protocol recommendations in practice and key design principles\nfor future research.",
    "updated" : "2025-06-02T18:37:15Z",
    "published" : "2025-06-02T18:37:15Z",
    "authors" : [
      {
        "name" : "Xiaolin Li"
      },
      {
        "name" : "Ninghui Li"
      },
      {
        "name" : "Boyang Wang"
      },
      {
        "name" : "Wenhai Sun"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01907v1",
    "title" : "SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data",
    "summary" : "Privacy-preserving data publication, including synthetic data sharing, often\nexperiences trade-offs between privacy and utility. Synthetic data is generally\nmore effective than data anonymization in balancing this trade-off, however,\nnot without its own challenges. Synthetic data produced by generative models\ntrained on source data may inadvertently reveal information about outliers.\nTechniques specifically designed for preserving privacy, such as introducing\nnoise to satisfy differential privacy, often incur unpredictable and\nsignificant losses in utility. In this work we show that, with the right\nmechanism of synthetic data generation, we can achieve strong privacy\nprotection without significant utility loss. Synthetic data generators\nproducing contracting data patterns, such as Synthetic Minority Over-sampling\nTechnique (SMOTE), can enhance a differentially private data generator,\nleveraging the strengths of both. We prove in theory and through empirical\ndemonstration that this SMOTE-DP technique can produce synthetic data that not\nonly ensures robust privacy protection but maintains utility in downstream\nlearning tasks.",
    "updated" : "2025-06-02T17:27:10Z",
    "published" : "2025-06-02T17:27:10Z",
    "authors" : [
      {
        "name" : "Yan Zhou"
      },
      {
        "name" : "Bradley Malin"
      },
      {
        "name" : "Murat Kantarcioglu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01425v1",
    "title" : "CSVAR: Enhancing Visual Privacy in Federated Learning via Adaptive\n  Shuffling Against Overfitting",
    "summary" : "Although federated learning preserves training data within local privacy\ndomains, the aggregated model parameters may still reveal private\ncharacteristics. This vulnerability stems from clients' limited training data,\nwhich predisposes models to overfitting. Such overfitting enables models to\nmemorize distinctive patterns from training samples, thereby amplifying the\nsuccess probability of privacy attacks like membership inference. To enhance\nvisual privacy protection in FL, we present CSVAR(Channel-Wise Spatial Image\nShuffling with Variance-Guided Adaptive Region Partitioning), a novel image\nshuffling framework to generate obfuscated images for secure data transmission\nand each training epoch, addressing both overfitting-induced privacy leaks and\nraw image transmission risks. CSVAR adopts region-variance as the metric to\nmeasure visual privacy sensitivity across image regions. Guided by this, CSVAR\nadaptively partitions each region into multiple blocks, applying fine-grained\npartitioning to privacy-sensitive regions with high region-variances for\nenhancing visual privacy protection and coarse-grained partitioning to\nprivacy-insensitive regions for balancing model utility. In each region, CSVAR\nthen shuffles between blocks in both the spatial domains and chromatic channels\nto hide visual spatial features and disrupt color distribution. Experimental\nevaluations conducted on diverse real-world datasets demonstrate that CSVAR is\ncapable of generating visually obfuscated images that exhibit high perceptual\nambiguity to human eyes, simultaneously mitigating the effectiveness of\nadversarial data reconstruction attacks and achieving a good trade-off between\nvisual privacy protection and model utility.",
    "updated" : "2025-06-02T08:30:12Z",
    "published" : "2025-06-02T08:30:12Z",
    "authors" : [
      {
        "name" : "Zhuo Chen"
      },
      {
        "name" : "Zhenya Ma"
      },
      {
        "name" : "Yan Zhang"
      },
      {
        "name" : "Donghua Cai"
      },
      {
        "name" : "Ye Zhang"
      },
      {
        "name" : "Qiushi Li"
      },
      {
        "name" : "Yongheng Deng"
      },
      {
        "name" : "Ye Guo"
      },
      {
        "name" : "Ju Ren"
      },
      {
        "name" : " Xuemin"
      },
      {
        "name" : " Shen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01325v1",
    "title" : "Understanding the Identity-Transformation Approach in OIDC-Compatible\n  Privacy-Preserving SSO Services",
    "summary" : "OpenID Connect (OIDC) enables a user with commercial-off-the-shelf browsers\nto log into multiple websites, called relying parties (RPs), by her username\nand credential set up in another trusted web system, called the identity\nprovider (IdP). Identity transformations are proposed in UppreSSO to provide\nOIDC-compatible SSO services, preventing both IdP-based login tracing and\nRP-based identity linkage. While security and privacy of SSO services in\nUppreSSO have been proved, several essential issues of this\nidentity-transformation approach are not well studied. In this paper, we\ncomprehensively investigate the approach as below. Firstly, several suggestions\nfor the efficient integration of identity transformations in OIDC-compatible\nSSO are explained. Then, we uncover the relationship between\nidentity-transformations in SSO and oblivious pseudo-random functions (OPRFs),\nand present two variations of the properties required for SSO security as well\nas the privacy requirements, to analyze existing OPRF protocols. Finally, new\nidentity transformations different from those designed in UppreSSO, are\nconstructed based on OPRFs, satisfying different variations of SSO security\nrequirements. To the best of our knowledge, this is the first time to uncover\nthe relationship between identity transformations in OIDC-compatible\nprivacy-preserving SSO services and OPRFs, and prove the SSO-related properties\n(i.e., key-identifier freeness, RP designation and user identification) of OPRF\nprotocols, in addition to the basic properties of correctness, obliviousness\nand pseudo-randomness.",
    "updated" : "2025-06-02T05:11:01Z",
    "published" : "2025-06-02T05:11:01Z",
    "authors" : [
      {
        "name" : "Jingqiang Lin"
      },
      {
        "name" : "Baitao Zhang"
      },
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Quanwei Cai"
      },
      {
        "name" : "Jiwu Jing"
      },
      {
        "name" : "Huiyang He"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02063v1",
    "title" : "Privacy-Aware, Public-Aligned: Embedding Risk Detection and Public\n  Values into Scalable Clinical Text De-Identification for Trusted Research\n  Environments",
    "summary" : "Clinical free-text data offers immense potential to improve population health\nresearch such as richer phenotyping, symptom tracking, and contextual\nunderstanding of patient care. However, these data present significant privacy\nrisks due to the presence of directly or indirectly identifying information\nembedded in unstructured narratives. While numerous de-identification tools\nhave been developed, few have been tested on real-world, heterogeneous datasets\nat scale or assessed for governance readiness. In this paper, we synthesise our\nfindings from previous studies examining the privacy-risk landscape across\nmultiple document types and NHS data providers in Scotland. We characterise how\ndirect and indirect identifiers vary by record type, clinical setting, and data\nflow, and show how changes in documentation practice can degrade model\nperformance over time. Through public engagement, we explore societal\nexpectations around the safe use of clinical free text and reflect these in the\ndesign of a prototype privacy-risk management tool to support transparent,\nauditable decision-making. Our findings highlight that privacy risk is\ncontext-dependent and cumulative, underscoring the need for adaptable, hybrid\nde-identification approaches that combine rule-based precision with contextual\nunderstanding. We offer a comprehensive view of the challenges and\nopportunities for safe, scalable reuse of clinical free-text within Trusted\nResearch Environments and beyond, grounded in both technical evidence and\npublic perspectives on responsible data use.",
    "updated" : "2025-06-01T17:45:57Z",
    "published" : "2025-06-01T17:45:57Z",
    "authors" : [
      {
        "name" : "Arlene Casey"
      },
      {
        "name" : "Stuart Dunbar"
      },
      {
        "name" : "Franz Gruber"
      },
      {
        "name" : "Samuel McInerney"
      },
      {
        "name" : "Matúš Falis"
      },
      {
        "name" : "Pamela Linksted"
      },
      {
        "name" : "Katie Wilde"
      },
      {
        "name" : "Kathy Harrison"
      },
      {
        "name" : "Alison Hamilton"
      },
      {
        "name" : "Christian Cole"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01072v1",
    "title" : "IDCloak: A Practical Secure Multi-party Dataset Join Framework for\n  Vertical Privacy-preserving Machine Learning",
    "summary" : "Vertical privacy-preserving machine learning (vPPML) enables multiple parties\nto train models on their vertically distributed datasets while keeping datasets\nprivate. In vPPML, it is critical to perform the secure dataset join, which\naligns features corresponding to intersection IDs across datasets and forms a\nsecret-shared and joint training dataset. However, existing methods for this\nstep could be impractical due to: (1) they are insecure when they expose\nintersection IDs; or (2) they rely on a strong trust assumption requiring a\nnon-colluding auxiliary server; or (3) they are limited to the two-party\nsetting.\n  This paper proposes IDCloak, the first practical secure multi-party dataset\njoin framework for vPPML that keeps IDs private without a non-colluding\nauxiliary server. IDCloak consists of two protocols: (1) a circuit-based\nmulti-party private set intersection protocol (cmPSI), which obtains\nsecret-shared flags indicating intersection IDs via an optimized communication\nstructure combining OKVS and OPRF; (2) a secure multi-party feature alignment\nprotocol, which obtains the secret-shared and joint dataset using secret-shared\nflags, via our proposed efficient secure shuffle protocol. Experiments show\nthat: (1) compared to the state-of-the-art secure two-party dataset join\nframework (iPrivjoin), IDCloak demonstrates higher efficiency in the two-party\nsetting and comparable performance when the party number increases; (2)\ncompared to the state-of-the-art cmPSI protocol under honest majority, our\nproposed cmPSI protocol provides a stronger security guarantee (dishonest\nmajority) while improving efficiency by up to $7.78\\times$ in time and\n$8.73\\times$ in communication sizes; (3) our proposed secure shuffle protocol\noutperforms the state-of-the-art shuffle protocol by up to $138.34\\times$ in\ntime and $132.13\\times$ in communication sizes.",
    "updated" : "2025-06-01T16:20:39Z",
    "published" : "2025-06-01T16:20:39Z",
    "authors" : [
      {
        "name" : "Shuyu Chen"
      },
      {
        "name" : "Guopeng Lin"
      },
      {
        "name" : "Haoyu Niu"
      },
      {
        "name" : "Lushan Song"
      },
      {
        "name" : "Chengxun Hong"
      },
      {
        "name" : "Weili Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00759v1",
    "title" : "Understanding and Mitigating Cross-lingual Privacy Leakage via\n  Language-specific and Universal Privacy Neurons",
    "summary" : "Large Language Models (LLMs) trained on massive data capture rich information\nembedded in the training data. However, this also introduces the risk of\nprivacy leakage, particularly involving personally identifiable information\n(PII). Although previous studies have shown that this risk can be mitigated\nthrough methods such as privacy neurons, they all assume that both the\n(sensitive) training data and user queries are in English. We show that they\ncannot defend against the privacy leakage in cross-lingual contexts: even if\nthe training data is exclusively in one language, these (private) models may\nstill reveal private information when queried in another language. In this\nwork, we first investigate the information flow of cross-lingual privacy\nleakage to give a better understanding. We find that LLMs process private\ninformation in the middle layers, where representations are largely shared\nacross languages. The risk of leakage peaks when converted to a\nlanguage-specific space in later layers. Based on this, we identify\nprivacy-universal neurons and language-specific privacy neurons.\nPrivacy-universal neurons influence privacy leakage across all languages, while\nlanguage-specific privacy neurons are only related to specific languages. By\ndeactivating these neurons, the cross-lingual privacy leakage risk is reduced\nby 23.3%-31.6%.",
    "updated" : "2025-06-01T00:10:30Z",
    "published" : "2025-06-01T00:10:30Z",
    "authors" : [
      {
        "name" : "Wenshuo Dong"
      },
      {
        "name" : "Qingsong Yang"
      },
      {
        "name" : "Shu Yang"
      },
      {
        "name" : "Lijie Hu"
      },
      {
        "name" : "Meng Ding"
      },
      {
        "name" : "Wanyu Lin"
      },
      {
        "name" : "Tianhang Zheng"
      },
      {
        "name" : "Di Wang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.04036v1",
    "title" : "Privacy and Security Threat for OpenAI GPTs",
    "summary" : "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications.",
    "updated" : "2025-06-04T14:58:29Z",
    "published" : "2025-06-04T14:58:29Z",
    "authors" : [
      {
        "name" : "Wei Wenying"
      },
      {
        "name" : "Zhao Kaifa"
      },
      {
        "name" : "Xue Lei"
      },
      {
        "name" : "Fan Ming"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.03870v1",
    "title" : "Evaluating Apple Intelligence's Writing Tools for Privacy Against Large\n  Language Model-Based Inference Attacks: Insights from Early Datasets",
    "summary" : "The misuse of Large Language Models (LLMs) to infer emotions from text for\nmalicious purposes, known as emotion inference attacks, poses a significant\nthreat to user privacy. In this paper, we investigate the potential of Apple\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\nmitigate these risks through text modifications such as rewriting and tone\nadjustment. By developing early novel datasets specifically for this purpose,\nwe empirically assess how different text modifications influence LLM-based\ndetection. This capability suggests strong potential for Apple Intelligence's\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\nfor future adaptive rewriting systems capable of dynamically neutralizing\nsensitive emotional content to enhance user privacy. To the best of our\nknowledge, this research provides the first empirical analysis of Apple\nIntelligence's text-modification tools within a privacy-preservation context\nwith the broader goal of developing on-device, user-centric privacy-preserving\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\nsystems.",
    "updated" : "2025-06-04T12:01:17Z",
    "published" : "2025-06-04T12:01:17Z",
    "authors" : [
      {
        "name" : "Mohd. Farhan Israk Soumik"
      },
      {
        "name" : "Syed Mhamudul Hasan"
      },
      {
        "name" : "Abdur R. Shahid"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.03618v1",
    "title" : "GCFL: A Gradient Correction-based Federated Learning Framework for\n  Privacy-preserving CPSS",
    "summary" : "Federated learning, as a distributed architecture, shows great promise for\napplications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the\nprivacy risks inherent in CPSS, the integration of differential privacy with\nfederated learning has attracted considerable attention. Existing research\nmainly focuses on dynamically adjusting the noise added or discarding certain\ngradients to mitigate the noise introduced by differential privacy. However,\nthese approaches fail to remove the noise that hinders convergence and correct\nthe gradients affected by the noise, which significantly reduces the accuracy\nof model classification. To overcome these challenges, this paper proposes a\nnovel framework for differentially private federated learning that balances\nrigorous privacy guarantees with accuracy by introducing a server-side gradient\ncorrection mechanism. Specifically, after clients perform gradient clipping and\nnoise perturbation, our framework detects deviations in the noisy local\ngradients and employs a projection mechanism to correct them, mitigating the\nnegative impact of noise. Simultaneously, gradient projection promotes the\nalignment of gradients from different clients and guides the model towards\nconvergence to a global optimum. We evaluate our framework on several benchmark\ndatasets, and the experimental results demonstrate that it achieves\nstate-of-the-art performance under the same privacy budget.",
    "updated" : "2025-06-04T06:52:37Z",
    "published" : "2025-06-04T06:52:37Z",
    "authors" : [
      {
        "name" : "Jiayi Wan"
      },
      {
        "name" : "Xiang Zhu"
      },
      {
        "name" : "Fanzhen Liu"
      },
      {
        "name" : "Wei Fan"
      },
      {
        "name" : "Xiaolong Xu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02965v2",
    "title" : "PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training\n  for Mixture-of-Experts LLMs",
    "summary" : "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks.",
    "updated" : "2025-06-04T05:38:31Z",
    "published" : "2025-06-03T15:00:18Z",
    "authors" : [
      {
        "name" : "Ze Yu Zhang"
      },
      {
        "name" : "Bolin Ding"
      },
      {
        "name" : "Bryan Kian Hsiang Low"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05290v1",
    "title" : "Big Bird: Privacy Budget Management for W3C's Privacy-Preserving\n  Attribution API",
    "summary" : "Privacy-preserving advertising APIs like Privacy-Preserving Attribution (PPA)\nare designed to enhance web privacy while enabling effective ad measurement.\nPPA offers an alternative to cross-site tracking with encrypted reports\ngoverned by differential privacy (DP), but current designs lack a principled\napproach to privacy budget management, creating uncertainty around critical\ndesign decisions. We present Big Bird, a privacy budget manager for PPA that\nclarifies per-site budget semantics and introduces a global budgeting system\ngrounded in resource isolation principles. Big Bird enforces utility-preserving\nlimits via quota budgets and improves global budget utilization through a novel\nbatched scheduling algorithm. Together, these mechanisms establish a robust\nfoundation for enforcing privacy protections in adversarial environments. We\nimplement Big Bird in Firefox and evaluate it on real-world ad data,\ndemonstrating its resilience and effectiveness.",
    "updated" : "2025-06-05T17:45:13Z",
    "published" : "2025-06-05T17:45:13Z",
    "authors" : [
      {
        "name" : "Pierre Tholoniat"
      },
      {
        "name" : "Alison Caulfield"
      },
      {
        "name" : "Giorgio Cavicchioli"
      },
      {
        "name" : "Mark Chen"
      },
      {
        "name" : "Nikos Goutzoulias"
      },
      {
        "name" : "Benjamin Case"
      },
      {
        "name" : "Asaf Cidon"
      },
      {
        "name" : "Roxana Geambasu"
      },
      {
        "name" : "Mathias Lécuyer"
      },
      {
        "name" : "Martin Thomson"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05101v1",
    "title" : "Privacy Amplification Through Synthetic Data: Insights from Linear\n  Regression",
    "summary" : "Synthetic data inherits the differential privacy guarantees of the model used\nto generate it. Additionally, synthetic data may benefit from privacy\namplification when the generative model is kept hidden. While empirical studies\nsuggest this phenomenon, a rigorous theoretical understanding is still lacking.\nIn this paper, we investigate this question through the well-understood\nframework of linear regression. First, we establish negative results showing\nthat if an adversary controls the seed of the generative model, a single\nsynthetic data point can leak as much information as releasing the model\nitself. Conversely, we show that when synthetic data is generated from random\ninputs, releasing a limited number of synthetic data points amplifies privacy\nbeyond the model's inherent guarantees. We believe our findings in linear\nregression can serve as a foundation for deriving more general bounds in the\nfuture.",
    "updated" : "2025-06-05T14:44:15Z",
    "published" : "2025-06-05T14:44:15Z",
    "authors" : [
      {
        "name" : "Clément Pierquin"
      },
      {
        "name" : "Aurélien Bellet"
      },
      {
        "name" : "Marc Tommasi"
      },
      {
        "name" : "Matthieu Boussard"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.04978v1",
    "title" : "Evaluating the Impact of Privacy-Preserving Federated Learning on CAN\n  Intrusion Detection",
    "summary" : "The challenges derived from the data-intensive nature of machine learning in\nconjunction with technologies that enable novel paradigms such as V2X and the\npotential offered by 5G communication, allow and justify the deployment of\nFederated Learning (FL) solutions in the vehicular intrusion detection domain.\nIn this paper, we investigate the effects of integrating FL strategies into the\nmachine learning-based intrusion detection process for on-board vehicular\nnetworks. Accordingly, we propose a FL implementation of a state-of-the-art\nIntrusion Detection System (IDS) for Controller Area Network (CAN), based on\nLSTM autoencoders. We thoroughly evaluate its detection efficiency and\ncommunication overhead, comparing it to a centralized version of the same\nalgorithm, thereby presenting it as a feasible solution.",
    "updated" : "2025-06-05T12:49:22Z",
    "published" : "2025-06-05T12:49:22Z",
    "authors" : [
      {
        "name" : "Gabriele Digregorio"
      },
      {
        "name" : "Elisabetta Cainazzo"
      },
      {
        "name" : "Stefano Longari"
      },
      {
        "name" : "Michele Carminati"
      },
      {
        "name" : "Stefano Zanero"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06124v1",
    "title" : "PrivTru: A Privacy-by-Design Data Trustee Minimizing Information Leakage",
    "summary" : "Data trustees serve as intermediaries that facilitate secure data sharing\nbetween independent parties. This paper offers a technical perspective on Data\ntrustees, guided by privacy-by-design principles. We introduce PrivTru, an\ninstantiation of a data trustee that provably achieves optimal privacy\nproperties. Therefore, PrivTru calculates the minimal amount of information the\ndata trustee needs to request from data sources to respond to a given query.\nOur analysis shows that PrivTru minimizes information leakage to the data\ntrustee, regardless of the trustee's prior knowledge, while preserving the\nutility of the data.",
    "updated" : "2025-06-06T14:33:59Z",
    "published" : "2025-06-06T14:33:59Z",
    "authors" : [
      {
        "name" : "Lukas Gehring"
      },
      {
        "name" : "Florian Tschorsch"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06062v1",
    "title" : "Minoritised Ethnic People's Security and Privacy Concerns and Responses\n  towards Essential Online Services",
    "summary" : "Minoritised ethnic people are marginalised in society, and therefore at a\nhigher risk of adverse online harms, including those arising from the loss of\nsecurity and privacy of personal data. Despite this, there has been very little\nresearch focused on minoritised ethnic people's security and privacy concerns,\nattitudes, and behaviours. In this work, we provide the results of one of the\nfirst studies in this regard. We explore minoritised ethnic people's\nexperiences of using essential online services across three sectors: health,\nsocial housing, and energy, their security and privacy-related concerns, and\nresponses towards these services. We conducted a thematic analysis of 44\nsemi-structured interviews with people of various reported minoritised\nethnicities in the UK. Privacy concerns and lack of control over personal data\nemerged as a major theme, with many interviewees considering privacy as their\nmost significant concern when using online services. Several creative tactics\nto exercise some agency were reported, including selective and inconsistent\ndisclosure of personal data. A core concern about how data may be used was\ndriven by a fear of repercussions, including penalisation and discrimination,\ninfluenced by prior experiences of institutional and online racism. The\nincreased concern and potential for harm resulted in minoritised ethnic people\ngrappling with a higher-stakes dilemma of whether to disclose personal\ninformation online or not. Furthermore, trust in institutions, or lack thereof,\nwas found to be embedded throughout as a basis for adapting behaviour. We draw\non our results to provide lessons learned for the design of more inclusive,\nmarginalisation-aware, and privacy-preserving online services.",
    "updated" : "2025-06-06T13:17:44Z",
    "published" : "2025-06-06T13:17:44Z",
    "authors" : [
      {
        "name" : "Aunam Quyoum"
      },
      {
        "name" : "Mark Wong"
      },
      {
        "name" : "Sebati Ghosh"
      },
      {
        "name" : "Siamak F. Shahandashti"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CR",
      "cs.HC",
      "K.4.2; H.1.2; K.4.1; K.6.5; J.4"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05908v1",
    "title" : "QualitEye: Public and Privacy-preserving Gaze Data Quality Verification",
    "summary" : "Gaze-based applications are increasingly advancing with the availability of\nlarge datasets but ensuring data quality presents a substantial challenge when\ncollecting data at scale. It further requires different parties to collaborate,\ntherefore, privacy concerns arise. We propose QualitEye--the first method for\nverifying image-based gaze data quality. QualitEye employs a new semantic\nrepresentation of eye images that contains the information required for\nverification while excluding irrelevant information for better domain\nadaptation. QualitEye covers a public setting where parties can freely exchange\ndata and a privacy-preserving setting where parties cannot reveal their raw\ndata nor derive gaze features/labels of others with adapted private set\nintersection protocols. We evaluate QualitEye on the MPIIFaceGaze and\nGazeCapture datasets and achieve a high verification performance (with a small\noverhead in runtime for privacy-preserving versions). Hence, QualitEye paves\nthe way for new gaze analysis methods at the intersection of machine learning,\nhuman-computer interaction, and cryptography.",
    "updated" : "2025-06-06T09:27:04Z",
    "published" : "2025-06-06T09:27:04Z",
    "authors" : [
      {
        "name" : "Mayar Elfares"
      },
      {
        "name" : "Pascal Reisert"
      },
      {
        "name" : "Ralf Küsters"
      },
      {
        "name" : "Andreas Bulling"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05743v1",
    "title" : "When Better Features Mean Greater Risks: The Performance-Privacy\n  Trade-Off in Contrastive Learning",
    "summary" : "With the rapid advancement of deep learning technology, pre-trained encoder\nmodels have demonstrated exceptional feature extraction capabilities, playing a\npivotal role in the research and application of deep learning. However, their\nwidespread use has raised significant concerns about the risk of training data\nprivacy leakage. This paper systematically investigates the privacy threats\nposed by membership inference attacks (MIAs) targeting encoder models, focusing\non contrastive learning frameworks. Through experimental analysis, we reveal\nthe significant impact of model architecture complexity on membership privacy\nleakage: As more advanced encoder frameworks improve feature-extraction\nperformance, they simultaneously exacerbate privacy-leakage risks. Furthermore,\nthis paper proposes a novel membership inference attack method based on the\np-norm of feature vectors, termed the Embedding Lp-Norm Likelihood Attack\n(LpLA). This method infers membership status, by leveraging the statistical\ndistribution characteristics of the p-norm of feature vectors. Experimental\nresults across multiple datasets and model architectures demonstrate that LpLA\noutperforms existing methods in attack performance and robustness, particularly\nunder limited attack knowledge and query volumes. This study not only uncovers\nthe potential risks of privacy leakage in contrastive learning frameworks, but\nalso provides a practical basis for privacy protection research in encoder\nmodels. We hope that this work will draw greater attention to the privacy risks\nassociated with self-supervised learning models and shed light on the\nimportance of a balance between model utility and training data privacy. Our\ncode is publicly available at: https://github.com/SeroneySun/LpLA_code.",
    "updated" : "2025-06-06T05:03:29Z",
    "published" : "2025-06-06T05:03:29Z",
    "authors" : [
      {
        "name" : "Ruining Sun"
      },
      {
        "name" : "Hongsheng Hu"
      },
      {
        "name" : "Wei Luo"
      },
      {
        "name" : "Zhaoxi Zhang"
      },
      {
        "name" : "Yanjun Zhang"
      },
      {
        "name" : "Haizhuan Yuan"
      },
      {
        "name" : "Leo Yu Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05683v1",
    "title" : "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation\n  Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence\n  in AR/VR/MR",
    "summary" : "Extended reality (XR) systems, which consist of virtual reality (VR),\naugmented reality (AR), and mixed reality (XR), offer a transformative\ninterface for immersive, multi-modal, and embodied human-computer interaction.\nIn this paper, we envision that multi-modal multi-task (M3T) federated\nfoundation models (FedFMs) can offer transformative capabilities for XR systems\nthrough integrating the representational strength of M3T foundation models\n(FMs) with the privacy-preserving model training principles of federated\nlearning (FL). We present a modular architecture for FedFMs, which entails\ndifferent coordination paradigms for model training and aggregations. Central\nto our vision is the codification of XR challenges that affect the\nimplementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality\ndiversity, (2) Hardware heterogeneity and system-level constraints, (3)\nInteractivity and embodied personalization, (4) Functional/task variability,\nand (5) Temporality and environmental variability. We illustrate the\nmanifestation of these dimensions across a set of emerging and anticipated\napplications of XR systems. Finally, we propose evaluation metrics, dataset\nrequirements, and design tradeoffs necessary for the development of\nresource-aware FedFMs in XR. This perspective aims to chart the technical and\nconceptual foundations for context-aware privacy-preserving intelligence in the\nnext generation of XR systems.",
    "updated" : "2025-06-06T02:23:42Z",
    "published" : "2025-06-06T02:23:42Z",
    "authors" : [
      {
        "name" : "Fardis Nadimi"
      },
      {
        "name" : "Payam Abdisarabshali"
      },
      {
        "name" : "Kasra Borazjani"
      },
      {
        "name" : "Jacob Chakareski"
      },
      {
        "name" : "Seyyedali Hosseinalipour"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05503v1",
    "title" : "On Differential Privacy for Adaptively Solving Search Problems via\n  Sketching",
    "summary" : "Recently differential privacy has been used for a number of streaming, data\nstructure, and dynamic graph problems as a means of hiding the internal\nrandomness of the data structure, so that multiple possibly adaptive queries\ncan be made without sacrificing the correctness of the responses. Although\nthese works use differential privacy to show that for some problems it is\npossible to tolerate $T$ queries using $\\widetilde{O}(\\sqrt{T})$ copies of a\ndata structure, such results only apply to numerical estimation problems, and\nonly return the cost of an optimization problem rather than the solution\nitself. In this paper, we investigate the use of differential privacy for\nadaptive queries to search problems, which are significantly more challenging\nsince the responses to queries can reveal much more about the internal\nrandomness than a single numerical query. We focus on two classical search\nproblems: nearest neighbor queries and regression with arbitrary turnstile\nupdates. We identify key parameters to these problems, such as the number of\n$c$-approximate near neighbors and the matrix condition number, and use\ndifferent differential privacy techniques to design algorithms returning the\nsolution vector with memory and time depending on these parameters. We give\nalgorithms for each of these problems that achieve similar tradeoffs.",
    "updated" : "2025-06-05T18:40:33Z",
    "published" : "2025-06-05T18:40:33Z",
    "authors" : [
      {
        "name" : "Shiyuan Feng"
      },
      {
        "name" : "Ying Feng"
      },
      {
        "name" : "George Z. Li"
      },
      {
        "name" : "Zhao Song"
      },
      {
        "name" : "David P. Woodruff"
      },
      {
        "name" : "Lichen Zhang"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05421v1",
    "title" : "TRIDENT -- A Three-Tier Privacy-Preserving Propaganda Detection Model in\n  Mobile Networks using Transformers, Adversarial Learning, and Differential\n  Privacy",
    "summary" : "The proliferation of propaganda on mobile platforms raises critical concerns\naround detection accuracy and user privacy. To address this, we propose TRIDENT\n- a three-tier propaganda detection model implementing transformers,\nadversarial learning, and differential privacy which integrates syntactic\nobfuscation and label perturbation to mitigate privacy leakage while\nmaintaining propaganda detection accuracy. TRIDENT leverages multilingual\nback-translation to introduce semantic variance, character-level noise, and\nentity obfuscation for differential privacy enforcement, and combines these\ntechniques into a unified defense mechanism. Using a binary propaganda\nclassification dataset, baseline transformer models (BERT, GPT-2) we achieved\nF1 scores of 0.89 and 0.90. Applying TRIDENT's third-tier defense yields a\nreduced but effective cumulative F1 of 0.83, demonstrating strong privacy\nprotection across mobile ML deployments with minimal degradation.",
    "updated" : "2025-06-05T02:38:02Z",
    "published" : "2025-06-05T02:38:02Z",
    "authors" : [
      {
        "name" : "Al Nahian Bin Emran"
      },
      {
        "name" : "Dhiman Goswami"
      },
      {
        "name" : "Md Hasan Ullah Sadi"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07957v1",
    "title" : "Understanding the Error Sensitivity of Privacy-Aware Computing",
    "summary" : "Homomorphic Encryption (HE) enables secure computation on encrypted data\nwithout decryption, allowing a great opportunity for privacy-preserving\ncomputation. In particular, domains such as healthcare, finance, and\ngovernment, where data privacy and security are of utmost importance, can\nbenefit from HE by enabling third-party computation and services on sensitive\ndata. In other words, HE constitutes the \"Holy Grail\" of cryptography: data\nremains encrypted all the time, being protected while in use.\n  HE's security guarantees rely on noise added to data to make relatively\nsimple problems computationally intractable. This error-centric intrinsic HE\nmechanism generates new challenges related to the fault tolerance and\nrobustness of HE itself: hardware- and software-induced errors during HE\noperation can easily evade traditional error detection and correction\nmechanisms, resulting in silent data corruption (SDC).\n  In this work, we motivate a thorough discussion regarding the sensitivity of\nHE applications to bit faults and provide a detailed error characterization\nstudy of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes\ndue to its fixed-point arithmetic support for AI and machine learning\napplications. We also delve into the impact of the residue number system (RNS)\nand the number theoretic transform (NTT), two widely adopted HE optimization\ntechniques, on CKKS' error sensitivity. To the best of our knowledge, this is\nthe first work that looks into the robustness and error sensitivity of\nhomomorphic encryption and, as such, it can pave the way for critical future\nwork in this area.",
    "updated" : "2025-06-09T17:27:40Z",
    "published" : "2025-06-09T17:27:40Z",
    "authors" : [
      {
        "name" : "Matías Mazzanti"
      },
      {
        "name" : "Esteban Mocskos"
      },
      {
        "name" : "Augusto Vega"
      },
      {
        "name" : "Pradip Bose"
      }
    ],
    "categories" : [
      "cs.AR",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07868v1",
    "title" : "Securing Unbounded Differential Privacy Against Timing Attacks",
    "summary" : "Recent works have started to theoretically investigate how we can protect\ndifferentially private programs against timing attacks, by making the joint\ndistribution the output and the runtime differentially private (JOT-DP).\nHowever, the existing approaches to JOT-DP have some limitations, particularly\nin the setting of unbounded DP (which protects the size of the dataset and\napplies to arbitrarily large datasets). First, the known conversion of pure DP\nprograms to pure JOT-DP programs in the unbounded setting (a) incurs a constant\nadditive increase in error probability (and thus does not provide vanishing\nerror as $n\\to\\infty$) (b) produces JOT-DP programs that fail to preserve the\ncomputational efficiency of the original pure DP program and (c) is analyzed in\na toy computational model in which the runtime is defined to be the number of\ncoin flips. In this work, we overcome these limitations. Specifically, we show\nthat the error required for pure JOT-DP in the unbounded setting depends on the\nmodel of computation. In a randomized RAM model where the dataset size $n$ is\ngiven (or can be computed in constant time) and we can generate random numbers\n(not just random bits) in constant time, polynomially small error probability\nis necessary and sufficient. If $n$ is not given or we only have a random-bit\ngenerator, an (arbitrarily small) constant error probability is necessary and\nsufficient. The aforementioned positive results are proven by efficient\nprocedures to convert any pure JOT-DP program $P$ in the upper-bounded setting\nto a pure JOT-DP program $P'$ in the unbounded setting, such that the output\ndistribution of $P'$ is $\\gamma$-close in total variation distance to that of\n$P$, where $\\gamma$ is either an arbitrarily small constant or polynomially\nsmall, depending on the model of computation.",
    "updated" : "2025-06-09T15:35:15Z",
    "published" : "2025-06-09T15:35:15Z",
    "authors" : [
      {
        "name" : "Zachary Ratliff"
      },
      {
        "name" : "Salil Vadhan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07605v1",
    "title" : "TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in\n  Federated Tree-Based Systems",
    "summary" : "Federated Learning has emerged as a privacy-oriented alternative to\ncentralized Machine Learning, enabling collaborative model training without\ndirect data sharing. While extensively studied for neural networks, the\nsecurity and privacy implications of tree-based models remain underexplored.\nThis work introduces TimberStrike, an optimization-based dataset reconstruction\nattack targeting horizontally federated tree-based models. Our attack, carried\nout by a single client, exploits the discrete nature of decision trees by using\nsplit values and decision paths to infer sensitive training data from other\nclients. We evaluate TimberStrike on State-of-the-Art federated gradient\nboosting implementations across multiple frameworks, including Flower, NVFlare,\nand FedTree, demonstrating their vulnerability to privacy breaches. On a\npublicly available stroke prediction dataset, TimberStrike consistently\nreconstructs between 73.05% and 95.63% of the target dataset across all\nimplementations. We further analyze Differential Privacy, showing that while it\npartially mitigates the attack, it also significantly degrades model\nperformance. Our findings highlight the need for privacy-preserving mechanisms\nspecifically designed for tree-based Federated Learning systems, and we provide\npreliminary insights into their design.",
    "updated" : "2025-06-09T10:06:03Z",
    "published" : "2025-06-09T10:06:03Z",
    "authors" : [
      {
        "name" : "Marco Di Gennaro"
      },
      {
        "name" : "Giovanni De Lucia"
      },
      {
        "name" : "Stefano Longari"
      },
      {
        "name" : "Stefano Zanero"
      },
      {
        "name" : "Michele Carminati"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07555v1",
    "title" : "Synthesize Privacy-Preserving High-Resolution Images via Private Textual\n  Intermediaries",
    "summary" : "Generating high fidelity, differentially private (DP) synthetic images offers\na promising route to share and analyze sensitive visual data without\ncompromising individual privacy. However, existing DP image synthesis methods\nstruggle to produce high resolution outputs that faithfully capture the\nstructure of the original data. In this paper, we introduce a novel method,\nreferred to as Synthesis via Private Textual Intermediaries (SPTI), that can\ngenerate high resolution DP images with easy adoption. The key idea is to shift\nthe challenge of DP image synthesis from the image domain to the text domain by\nleveraging state of the art DP text generation methods. SPTI first summarizes\neach private image into a concise textual description using image to text\nmodels, then applies a modified Private Evolution algorithm to generate DP\ntext, and finally reconstructs images using text to image models. Notably, SPTI\nrequires no model training, only inference with off the shelf models. Given a\nprivate dataset, SPTI produces synthetic images of substantially higher quality\nthan prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less\nthan or equal to 26.71 under epsilon equal to 1.0, improving over Private\nEvolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less\nthan or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine\ntuning baselines. Overall, our results demonstrate that Synthesis via Private\nTextual Intermediaries provides a resource efficient and proprietary model\ncompatible framework for generating high resolution DP synthetic images,\ngreatly expanding access to private visual datasets.",
    "updated" : "2025-06-09T08:48:06Z",
    "published" : "2025-06-09T08:48:06Z",
    "authors" : [
      {
        "name" : "Haoxiang Wang"
      },
      {
        "name" : "Zinan Lin"
      },
      {
        "name" : "Da Yu"
      },
      {
        "name" : "Huishuai Zhang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07102v1",
    "title" : "Decentralized Optimization with Amplified Privacy via Efficient\n  Communication",
    "summary" : "Decentralized optimization is crucial for multi-agent systems, with\nsignificant concerns about communication efficiency and privacy. This paper\nexplores the role of efficient communication in decentralized stochastic\ngradient descent algorithms for enhancing privacy preservation. We develop a\nnovel algorithm that incorporates two key features: random agent activation and\nsparsified communication. Utilizing differential privacy, we demonstrate that\nthese features reduce noise without sacrificing privacy, thereby amplifying the\nprivacy guarantee and improving accuracy. Additionally, we analyze the\nconvergence and the privacy-accuracy-communication trade-off of the proposed\nalgorithm. Finally, we present experimental results to illustrate the\neffectiveness of our algorithm.",
    "updated" : "2025-06-08T12:14:14Z",
    "published" : "2025-06-08T12:14:14Z",
    "authors" : [
      {
        "name" : "Wei Huo"
      },
      {
        "name" : "Changxin Liu"
      },
      {
        "name" : "Kemi Ding"
      },
      {
        "name" : "Karl Henrik Johansson"
      },
      {
        "name" : "Ling Shi"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06742v1",
    "title" : "LADSG: Label-Anonymized Distillation and Similar Gradient Substitution\n  for Label Privacy in Vertical Federated Learning",
    "summary" : "Vertical federated learning (VFL) has become a key paradigm for collaborative\nmachine learning, enabling multiple parties to train models over distributed\nfeature spaces while preserving data privacy. Despite security protocols that\ndefend against external attacks - such as gradient masking and encryption,\nwhich prevent unauthorized access to sensitive data - recent label inference\nattacks from within the system have emerged. These attacks exploit gradients\nand semantic embeddings to reconstruct private labels, bypassing traditional\ndefenses. For example, the passive label inference attack can reconstruct tens\nof thousands of participants' private data using just 40 auxiliary labels,\nposing a significant security threat. Existing defenses address single leakage\npathways, such as gradient leakage or label exposure. As attack strategies\nevolve, their limitations become clear, especially against hybrid attacks that\ncombine multiple vectors. To address this, we propose Label-Anonymized Defense\nwith Substitution Gradient (LADSG), a unified defense framework that integrates\ngradient substitution, label anonymization, and anomaly detection. LADSG\nmitigates both gradient and label leakage while maintaining the scalability and\nefficiency of VFL. Experiments on six real-world datasets show that LADSG\nreduces label inference attack success rates by 30-60%, with minimal\ncomputational overhead, underscoring the importance of lightweight defenses in\nsecuring VFL.",
    "updated" : "2025-06-07T10:10:56Z",
    "published" : "2025-06-07T10:10:56Z",
    "authors" : [
      {
        "name" : "Zeyu Yan"
      },
      {
        "name" : "Yifei Yao"
      },
      {
        "name" : "Xuanbing Wen"
      },
      {
        "name" : "Juli Zhang"
      },
      {
        "name" : "Kai Fan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06591v1",
    "title" : "Privacy Perspectives and Practices of Chinese Smart Home Product Teams",
    "summary" : "Previous research has explored the privacy needs and concerns of device\nowners, primary users, and different bystander groups with regard to smart home\ndevices like security cameras, smart speakers, and hubs, but little is known\nabout the privacy views and practices of smart home product teams, particularly\nthose in non-Western contexts. This paper presents findings from 27\nsemi-structured interviews with Chinese smart home product team members,\nincluding product/project managers, software/hardware engineers, user\nexperience (UX) designers, legal/privacy experts, and marketers/operation\nspecialists. We examine their privacy perspectives, practices, and risk\nmitigation strategies. Our results show that participants emphasized compliance\nwith Chinese data privacy laws, which typically prioritized national security\nover individual privacy rights. China-specific cultural, social, and legal\nfactors also influenced participants' ethical considerations and attitudes\ntoward balancing user privacy and security with convenience. Drawing on our\nfindings, we propose a set of recommendations for smart home product teams,\nalong with socio-technical and legal interventions to address smart home\nprivacy issues-especially those belonging to at-risk groups-in Chinese\nmulti-user smart homes.",
    "updated" : "2025-06-06T23:49:48Z",
    "published" : "2025-06-06T23:49:48Z",
    "authors" : [
      {
        "name" : "Shijing He"
      },
      {
        "name" : "Yaxiong Lei"
      },
      {
        "name" : "Xiao Zhan"
      },
      {
        "name" : "Chi Zhang"
      },
      {
        "name" : "Juan Ye"
      },
      {
        "name" : "Ruba Abu-Salma"
      },
      {
        "name" : "Jose Such"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06530v1",
    "title" : "Breaking the Gaussian Barrier: Residual-PAC Privacy for Automatic\n  Privatization",
    "summary" : "The Probably Approximately Correct (PAC) Privacy framework [1] provides a\npowerful instance-based methodology for certifying privacy in complex\ndata-driven systems. However, existing PAC Privacy algorithms rely on a\nGaussian mutual information upper bound. We show that this is in general too\nconservative: the upper bound obtained by these algorithms is tight if and only\nif the perturbed mechanism output is jointly Gaussian with independent Gaussian\nnoise. To address the inefficiency inherent in the Gaussian-based approach, we\nintroduce Residual PAC Privacy, an f-divergence-based measure that quantifies\nthe privacy remaining after adversarial inference. When instantiated with\nKullback-Leibler divergence, Residual-PAC Privacy is governed by conditional\nentropy. Moreover, we propose Stackelberg Residual-PAC (SR-PAC) privatization\nmechanisms for RPAC Privacy, a game-theoretic framework that selects optimal\nnoise distributions through convex bilevel optimization. Our approach achieves\ntight privacy budget utilization for arbitrary data distributions. Moreover, it\nnaturally composes under repeated mechanisms and provides provable privacy\nguarantees with higher statistical efficiency. Numerical experiments\ndemonstrate that SR-PAC certifies the target privacy budget while consistently\nimproving utility compared to existing methods.",
    "updated" : "2025-06-06T20:52:47Z",
    "published" : "2025-06-06T20:52:47Z",
    "authors" : [
      {
        "name" : "Tao Zhang"
      },
      {
        "name" : "Yevgeniy Vorobeychik"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06062v2",
    "title" : "Minoritised Ethnic People's Security and Privacy Concerns and Responses\n  towards Essential Online Services",
    "summary" : "Minoritised ethnic people are marginalised in society, and therefore at a\nhigher risk of adverse online harms, including those arising from the loss of\nsecurity and privacy of personal data. Despite this, there has been very little\nresearch focused on minoritised ethnic people's security and privacy concerns,\nattitudes, and behaviours. In this work, we provide the results of one of the\nfirst studies in this regard. We explore minoritised ethnic people's\nexperiences of using essential online services across three sectors: health,\nsocial housing, and energy, their security and privacy-related concerns, and\nresponses towards these services. We conducted a thematic analysis of 44\nsemi-structured interviews with people of various reported minoritised\nethnicities in the UK. Privacy concerns and lack of control over personal data\nemerged as a major theme, with many interviewees considering privacy as their\nmost significant concern when using online services. Several creative tactics\nto exercise some agency were reported, including selective and inconsistent\ndisclosure of personal data. A core concern about how data may be used was\ndriven by a fear of repercussions, including penalisation and discrimination,\ninfluenced by prior experiences of institutional and online racism. The\nincreased concern and potential for harm resulted in minoritised ethnic people\ngrappling with a higher-stakes dilemma of whether to disclose personal\ninformation online or not. Furthermore, trust in institutions, or lack thereof,\nwas found to be embedded throughout as a basis for adapting behaviour. We draw\non our results to provide lessons learned for the design of more inclusive,\nmarginalisation-aware, and privacy-preserving online services.",
    "updated" : "2025-06-09T14:10:55Z",
    "published" : "2025-06-06T13:17:44Z",
    "authors" : [
      {
        "name" : "Aunam Quyoum"
      },
      {
        "name" : "Mark Wong"
      },
      {
        "name" : "Sebati Ghosh"
      },
      {
        "name" : "Siamak F. Shahandashti"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CR",
      "cs.HC",
      "K.4.2; H.1.2; K.4.1; K.6.5; J.4"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00759v2",
    "title" : "Understanding and Mitigating Cross-lingual Privacy Leakage via\n  Language-specific and Universal Privacy Neurons",
    "summary" : "Large Language Models (LLMs) trained on massive data capture rich information\nembedded in the training data. However, this also introduces the risk of\nprivacy leakage, particularly involving personally identifiable information\n(PII). Although previous studies have shown that this risk can be mitigated\nthrough methods such as privacy neurons, they all assume that both the\n(sensitive) training data and user queries are in English. We show that they\ncannot defend against the privacy leakage in cross-lingual contexts: even if\nthe training data is exclusively in one language, these (private) models may\nstill reveal private information when queried in another language. In this\nwork, we first investigate the information flow of cross-lingual privacy\nleakage to give a better understanding. We find that LLMs process private\ninformation in the middle layers, where representations are largely shared\nacross languages. The risk of leakage peaks when converted to a\nlanguage-specific space in later layers. Based on this, we identify\nprivacy-universal neurons and language-specific privacy neurons.\nPrivacy-universal neurons influence privacy leakage across all languages, while\nlanguage-specific privacy neurons are only related to specific languages. By\ndeactivating these neurons, the cross-lingual privacy leakage risk is reduced\nby 23.3%-31.6%.",
    "updated" : "2025-06-08T14:59:14Z",
    "published" : "2025-06-01T00:10:30Z",
    "authors" : [
      {
        "name" : "Wenshuo Dong"
      },
      {
        "name" : "Qingsong Yang"
      },
      {
        "name" : "Shu Yang"
      },
      {
        "name" : "Lijie Hu"
      },
      {
        "name" : "Meng Ding"
      },
      {
        "name" : "Wanyu Lin"
      },
      {
        "name" : "Tianhang Zheng"
      },
      {
        "name" : "Di Wang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08918v1",
    "title" : "Quantifying Mix Network Privacy Erosion with Generative Models",
    "summary" : "Modern mix networks improve over Tor and provide stronger privacy guarantees\nby robustly obfuscating metadata. As long as a message is routed through at\nleast one honest mixnode, the privacy of the users involved is safeguarded.\nHowever, the complexity of the mixing mechanisms makes it difficult to estimate\nthe cumulative privacy erosion occurring over time. This work uses a generative\nmodel trained on mixnet traffic to estimate the loss of privacy when users\ncommunicate persistently over a period of time. We train our large-language\nmodel from scratch on our specialized network traffic ``language'' and then use\nit to measure the sender-message unlinkability in various settings (e.g. mixing\nstrategies, security parameters, observation window). Our findings reveal\nnotable differences in privacy levels among mix strategies, even when they have\nsimilar mean latencies. In comparison, we demonstrate the limitations of\ntraditional privacy metrics, such as entropy and log-likelihood, in fully\ncapturing an adversary's potential to synthesize information from multiple\nobservations. Finally, we show that larger models exhibit greater sample\nefficiency and superior capabilities implying that further advancements in\ntransformers will consequently enhance the accuracy of model-based privacy\nestimates.",
    "updated" : "2025-06-10T15:43:39Z",
    "published" : "2025-06-10T15:43:39Z",
    "authors" : [
      {
        "name" : "Vasilios Mavroudis"
      },
      {
        "name" : "Tariq Elahi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08654v1",
    "title" : "A Privacy-Preserving Federated Learning Framework for Generalizable CBCT\n  to Synthetic CT Translation in Head and Neck",
    "summary" : "Shortened Abstract\n  Cone-beam computed tomography (CBCT) has become a widely adopted modality for\nimage-guided radiotherapy (IGRT). However, CBCT suffers from increased noise,\nlimited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield\nunit values and hindering direct dose calculation. Synthetic CT (sCT)\ngeneration from CBCT addresses these issues, especially using deep learning\n(DL) methods. Existing approaches are limited by institutional heterogeneity,\nscanner-dependent variations, and data privacy regulations that prevent\nmulti-center data sharing.\n  To overcome these challenges, we propose a cross-silo horizontal federated\nlearning (FL) approach for CBCT-to-sCT synthesis in the head and neck region,\nextending our FedSynthCT framework. A conditional generative adversarial\nnetwork was collaboratively trained on data from three European medical centers\nin the public SynthRAD2025 challenge dataset.\n  The federated model demonstrated effective generalization across centers,\nwith mean absolute error (MAE) ranging from $64.38\\pm13.63$ to $85.90\\pm7.10$\nHU, structural similarity index (SSIM) from $0.882\\pm0.022$ to $0.922\\pm0.039$,\nand peak signal-to-noise ratio (PSNR) from $32.86\\pm0.94$ to $34.91\\pm1.04$ dB.\nNotably, on an external validation dataset of 60 patients, comparable\nperformance was achieved (MAE: $75.22\\pm11.81$ HU, SSIM: $0.904\\pm0.034$, PSNR:\n$33.52\\pm2.06$ dB) without additional training, confirming robust\ngeneralization despite protocol, scanner differences and registration errors.\n  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT\nsynthesis while preserving data privacy and offer a collaborative solution for\ndeveloping generalizable models across institutions without centralized data\nsharing or site-specific fine-tuning.",
    "updated" : "2025-06-10T10:10:56Z",
    "published" : "2025-06-10T10:10:56Z",
    "authors" : [
      {
        "name" : "Ciro Benito Raggio"
      },
      {
        "name" : "Paolo Zaffino"
      },
      {
        "name" : "Maria Francesca Spadea"
      }
    ],
    "categories" : [
      "physics.med-ph",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08347v1",
    "title" : "Differentially Private Relational Learning with Entity-level Privacy\n  Guarantees",
    "summary" : "Learning with relational and network-structured data is increasingly vital in\nsensitive domains where protecting the privacy of individual entities is\nparamount. Differential Privacy (DP) offers a principled approach for\nquantifying privacy risks, with DP-SGD emerging as a standard mechanism for\nprivate model training. However, directly applying DP-SGD to relational\nlearning is challenging due to two key factors: (i) entities often participate\nin multiple relations, resulting in high and difficult-to-control sensitivity;\nand (ii) relational learning typically involves multi-stage, potentially\ncoupled (interdependent) sampling procedures that make standard privacy\namplification analyses inapplicable. This work presents a principled framework\nfor relational learning with formal entity-level DP guarantees. We provide a\nrigorous sensitivity analysis and introduce an adaptive gradient clipping\nscheme that modulates clipping thresholds based on entity occurrence frequency.\nWe also extend the privacy amplification results to a tractable subclass of\ncoupled sampling, where the dependence arises only through sample sizes. These\ncontributions lead to a tailored DP-SGD variant for relational data with\nprovable privacy guarantees. Experiments on fine-tuning text encoders over\ntext-attributed network-structured relational data demonstrate the strong\nutility-privacy trade-offs of our approach. Our code is available at\nhttps://github.com/Graph-COM/Node_DP.",
    "updated" : "2025-06-10T02:03:43Z",
    "published" : "2025-06-10T02:03:43Z",
    "authors" : [
      {
        "name" : "Yinan Huang"
      },
      {
        "name" : "Haoteng Ying"
      },
      {
        "name" : "Eli Chien"
      },
      {
        "name" : "Rongzhe Wei"
      },
      {
        "name" : "Pan Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08330v1",
    "title" : "Distortion Search, A Web Search Privacy Heuristic",
    "summary" : "Search engines have vast technical capabilities to retain Internet search\nlogs for each user and thus present major privacy vulnerabilities to both\nindividuals and organizations in revealing user intent. Additionally, many of\nthe web search privacy enhancing tools available today require that the user\ntrusts a third party, which make confidentiality of user intent even more\nchallenging. The user is left at the mercy of the third party without the\ncontrol over his or her own privacy. In this article, we suggest a user-centric\nheuristic, Distortion Search, a web search query privacy methodology that works\nby the formation of obfuscated search queries via the permutation of query\nkeyword categories, and by strategically applying k-anonymised web navigational\nclicks on URLs and Ads to generate a distorted user profile and thus providing\nspecific user intent and query confidentiality. We provide empirical results\nvia the evaluation of distorted web search queries in terms of retrieved search\nresults and the resulting web ads from search engines. Preliminary experimental\nresults indicate that web search query and specific user intent privacy might\nbe achievable from the user side without the involvement of the search engine\nor other third parties.",
    "updated" : "2025-06-10T01:35:16Z",
    "published" : "2025-06-10T01:35:16Z",
    "authors" : [
      {
        "name" : "Kato Mivule"
      },
      {
        "name" : "Kenneth Hopkinson"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08185v1",
    "title" : "Surgeon Style Fingerprinting and Privacy Risk Quantification via\n  Discrete Diffusion Models in a Vision-Language-Action Framework",
    "summary" : "Surgeons exhibit distinct operating styles due to differences in training,\nexperience, and motor behavior - yet current AI systems often ignore this\npersonalization signal. We propose a novel approach to model fine-grained,\nsurgeon-specific fingerprinting in robotic surgery using a discrete diffusion\nframework integrated with a vision-language-action (VLA) pipeline. Our method\nformulates gesture prediction as a structured sequence denoising task,\nconditioned on multimodal inputs including endoscopic video, surgical intent\nlanguage, and a privacy-aware embedding of surgeon identity and skill.\nPersonalized surgeon fingerprinting is encoded through natural language prompts\nusing third-party language models, allowing the model to retain individual\nbehavioral style without exposing explicit identity. We evaluate our method on\nthe JIGSAWS dataset and demonstrate that it accurately reconstructs gesture\nsequences while learning meaningful motion fingerprints unique to each surgeon.\nTo quantify the privacy implications of personalization, we perform membership\ninference attacks and find that more expressive embeddings improve task\nperformance but simultaneously increase susceptibility to identity leakage.\nThese findings demonstrate that while personalized embeddings improve\nperformance, they also increase vulnerability to identity leakage, revealing\nthe importance of balancing personalization with privacy risk in surgical\nmodeling. Code is available at:\nhttps://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.",
    "updated" : "2025-06-09T19:49:55Z",
    "published" : "2025-06-09T19:49:55Z",
    "authors" : [
      {
        "name" : "Huixin Zhan"
      },
      {
        "name" : "Jason H. Moore"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.09690v1",
    "title" : "Knockoffs Inference under Privacy Constraints",
    "summary" : "Model-X knockoff framework offers a model-free variable selection method that\nensures finite sample false discovery rate (FDR) control. However, the\ncomplexity of generating knockoff variables, coupled with the model-free\nassumption, presents significant challenges for protecting data privacy in this\ncontext. In this paper, we propose a comprehensive framework for knockoff\ninference within the differential privacy paradigm. Our proposed method\nguarantees robust privacy protection while preserving the exact FDR control\nentailed by the original model-X knockoff procedure. We further conduct power\nanalysis and establish sufficient conditions under which the noise added for\nprivacy preservation does not asymptotically compromise power. Through various\napplications, we demonstrate that the differential privacy knockoff\n(DP-knockoff) method can be effectively utilized to safeguard privacy during\nvariable selection with FDR control in both low and high dimensional settings.",
    "updated" : "2025-06-11T13:06:21Z",
    "published" : "2025-06-11T13:06:21Z",
    "authors" : [
      {
        "name" : "Zhanrui Cai"
      },
      {
        "name" : "Yingying Fan"
      },
      {
        "name" : "Lan Gao"
      }
    ],
    "categories" : [
      "stat.ME",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.09387v1",
    "title" : "Epass: Efficient and Privacy-Preserving Asynchronous Payment on\n  Blockchain",
    "summary" : "Buy Now Pay Later (BNPL) is a rapidly proliferating e-commerce model,\noffering consumers to get the product immediately and defer payments.\nMeanwhile, emerging blockchain technologies endow BNPL platforms with digital\ncurrency transactions, allowing BNPL platforms to integrate with digital\nwallets. However, the transparency of transactions causes critical privacy\nconcerns because malicious participants may derive consumers' financial\nstatuses from on-chain asynchronous payments. Furthermore, the newly created\ntransactions for deferred payments introduce additional time overheads, which\nweaken the scalability of BNPL services. To address these issues, we propose an\nefficient and privacy-preserving blockchain-based asynchronous payment scheme\n(Epass), which has promising scalability while protecting the privacy of\non-chain consumer transactions. Specifically, Epass leverages locally\nverifiable signatures to guarantee the privacy of consumer transactions against\nmalicious acts. Then, a privacy-preserving asynchronous payment scheme can be\nfurther constructed by leveraging time-release encryption to control trapdoors\nof redactable blockchain, reducing time overheads by modifying transactions for\ndeferred payment. We give formal definitions and security models, generic\nstructures, and formal proofs for Epass. Extensive comparisons and experimental\nanalysis show that \\textsf{Epass} achieves KB-level communication costs, and\nreduces time overhead by more than four times in comparisons with locally\nverifiable signatures and Go-Ethereum private test networks.",
    "updated" : "2025-06-11T04:32:54Z",
    "published" : "2025-06-11T04:32:54Z",
    "authors" : [
      {
        "name" : "Weijie Wang"
      },
      {
        "name" : "Jinwen Liang"
      },
      {
        "name" : "Chuan Zhang"
      },
      {
        "name" : "Ximeng Liu"
      },
      {
        "name" : "Liehuang Zhu"
      },
      {
        "name" : "Song Guo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.09312v1",
    "title" : "What is the Cost of Differential Privacy for Deep Learning-Based\n  Trajectory Generation?",
    "summary" : "While location trajectories offer valuable insights, they also reveal\nsensitive personal information. Differential Privacy (DP) offers formal\nprotection, but achieving a favourable utility-privacy trade-off remains\nchallenging. Recent works explore deep learning-based generative models to\nproduce synthetic trajectories. However, current models lack formal privacy\nguarantees and rely on conditional information derived from real data during\ngeneration. This work investigates the utility cost of enforcing DP in such\nmodels, addressing three research questions across two datasets and eleven\nutility metrics. (1) We evaluate how DP-SGD, the standard DP training method\nfor deep learning, affects the utility of state-of-the-art generative models.\n(2) Since DP-SGD is limited to unconditional models, we propose a novel DP\nmechanism for conditional generation that provides formal guarantees and assess\nits impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN\n- affect the utility-privacy trade-off. Our results show that DP-SGD\nsignificantly impacts performance, although some utility remains if the\ndatasets is sufficiently large. The proposed DP mechanism improves training\nstability, particularly when combined with DP-SGD, for unstable models such as\nGANs and on smaller datasets. Diffusion models yield the best utility without\nguarantees, but with DP-SGD, GANs perform best, indicating that the best\nnon-private model is not necessarily optimal when targeting formal guarantees.\nIn conclusion, DP trajectory generation remains a challenging task, and formal\nguarantees are currently only feasible with large datasets and in constrained\nuse cases.",
    "updated" : "2025-06-11T00:59:52Z",
    "published" : "2025-06-11T00:59:52Z",
    "authors" : [
      {
        "name" : "Erik Buchholz"
      },
      {
        "name" : "Natasha Fernandes"
      },
      {
        "name" : "David D. Nguyen"
      },
      {
        "name" : "Alsharif Abuadbba"
      },
      {
        "name" : "Surya Nepal"
      },
      {
        "name" : "Salil S. Kanhere"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05683v2",
    "title" : "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation\n  Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence\n  in AR/VR/MR",
    "summary" : "Extended reality (XR) systems, which consist of virtual reality (VR),\naugmented reality (AR), and mixed reality (XR), offer a transformative\ninterface for immersive, multi-modal, and embodied human-computer interaction.\nIn this paper, we envision that multi-modal multi-task (M3T) federated\nfoundation models (FedFMs) can offer transformative capabilities for XR systems\nthrough integrating the representational strength of M3T foundation models\n(FMs) with the privacy-preserving model training principles of federated\nlearning (FL). We present a modular architecture for FedFMs, which entails\ndifferent coordination paradigms for model training and aggregations. Central\nto our vision is the codification of XR challenges that affect the\nimplementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality\ndiversity, (2) Hardware heterogeneity and system-level constraints, (3)\nInteractivity and embodied personalization, (4) Functional/task variability,\nand (5) Temporality and environmental variability. We illustrate the\nmanifestation of these dimensions across a set of emerging and anticipated\napplications of XR systems. Finally, we propose evaluation metrics, dataset\nrequirements, and design tradeoffs necessary for the development of\nresource-aware FedFMs in XR. This perspective aims to chart the technical and\nconceptual foundations for context-aware privacy-preserving intelligence in the\nnext generation of XR systems.",
    "updated" : "2025-06-11T00:49:57Z",
    "published" : "2025-06-06T02:23:42Z",
    "authors" : [
      {
        "name" : "Fardis Nadimi"
      },
      {
        "name" : "Payam Abdisarabshali"
      },
      {
        "name" : "Kasra Borazjani"
      },
      {
        "name" : "Jacob Chakareski"
      },
      {
        "name" : "Seyyedali Hosseinalipour"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.10042v1",
    "title" : "Multiverse Privacy Theory for Contextual Risks in Complex User-AI\n  Interactions",
    "summary" : "In an era of increasing interaction with artificial intelligence (AI), users\nface evolving privacy decisions shaped by complex, uncertain factors. This\npaper introduces Multiverse Privacy Theory, a novel framework in which each\nprivacy decision spawns a parallel universe, representing a distinct potential\noutcome based on user choices over time. By simulating these universes, this\ntheory provides a foundation for understanding privacy through the lens of\ncontextual integrity, evolving preferences, and probabilistic decision-making.\nFuture work will explore its application using real-world, scenario-based\nsurvey data.",
    "updated" : "2025-06-11T05:02:59Z",
    "published" : "2025-06-11T05:02:59Z",
    "authors" : [
      {
        "name" : "Ece Gumusel"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.10024v1",
    "title" : "Private Memorization Editing: Turning Memorization into a Defense to\n  Strengthen Data Privacy in Large Language Models",
    "summary" : "Large Language Models (LLMs) memorize, and thus, among huge amounts of\nuncontrolled data, may memorize Personally Identifiable Information (PII),\nwhich should not be stored and, consequently, not leaked. In this paper, we\nintroduce Private Memorization Editing (PME), an approach for preventing\nprivate data leakage that turns an apparent limitation, that is, the LLMs'\nmemorization ability, into a powerful privacy defense strategy. While attacks\nagainst LLMs have been performed exploiting previous knowledge regarding their\ntraining data, our approach aims to exploit the same kind of knowledge in order\nto make a model more robust. We detect a memorized PII and then mitigate the\nmemorization of PII by editing a model knowledge of its training data. We\nverify that our procedure does not affect the underlying language model while\nmaking it more robust against privacy Training Data Extraction attacks. We\ndemonstrate that PME can effectively reduce the number of leaked PII in a\nnumber of configurations, in some cases even reducing the accuracy of the\nprivacy attacks to zero.",
    "updated" : "2025-06-09T17:57:43Z",
    "published" : "2025-06-09T17:57:43Z",
    "authors" : [
      {
        "name" : "Elena Sofia Ruzzetti"
      },
      {
        "name" : "Giancarlo A. Xompero"
      },
      {
        "name" : "Davide Venditti"
      },
      {
        "name" : "Fabio Massimo Zanzotto"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07605v2",
    "title" : "TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in\n  Federated Tree-Based Systems",
    "summary" : "Federated Learning has emerged as a privacy-oriented alternative to\ncentralized Machine Learning, enabling collaborative model training without\ndirect data sharing. While extensively studied for neural networks, the\nsecurity and privacy implications of tree-based models remain underexplored.\nThis work introduces TimberStrike, an optimization-based dataset reconstruction\nattack targeting horizontally federated tree-based models. Our attack, carried\nout by a single client, exploits the discrete nature of decision trees by using\nsplit values and decision paths to infer sensitive training data from other\nclients. We evaluate TimberStrike on State-of-the-Art federated gradient\nboosting implementations across multiple frameworks, including Flower, NVFlare,\nand FedTree, demonstrating their vulnerability to privacy breaches. On a\npublicly available stroke prediction dataset, TimberStrike consistently\nreconstructs between 73.05% and 95.63% of the target dataset across all\nimplementations. We further analyze Differential Privacy, showing that while it\npartially mitigates the attack, it also significantly degrades model\nperformance. Our findings highlight the need for privacy-preserving mechanisms\nspecifically designed for tree-based Federated Learning systems, and we provide\npreliminary insights into their design.",
    "updated" : "2025-06-12T17:48:26Z",
    "published" : "2025-06-09T10:06:03Z",
    "authors" : [
      {
        "name" : "Marco Di Gennaro"
      },
      {
        "name" : "Giovanni De Lucia"
      },
      {
        "name" : "Stefano Longari"
      },
      {
        "name" : "Stefano Zanero"
      },
      {
        "name" : "Michele Carminati"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05683v3",
    "title" : "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation\n  Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence\n  in AR/VR/MR",
    "summary" : "Extended reality (XR) systems, which consist of virtual reality (VR),\naugmented reality (AR), and mixed reality (XR), offer a transformative\ninterface for immersive, multi-modal, and embodied human-computer interaction.\nIn this paper, we envision that multi-modal multi-task (M3T) federated\nfoundation models (FedFMs) can offer transformative capabilities for XR systems\nthrough integrating the representational strength of M3T foundation models\n(FMs) with the privacy-preserving model training principles of federated\nlearning (FL). We present a modular architecture for FedFMs, which entails\ndifferent coordination paradigms for model training and aggregations. Central\nto our vision is the codification of XR challenges that affect the\nimplementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality\ndiversity, (2) Hardware heterogeneity and system-level constraints, (3)\nInteractivity and embodied personalization, (4) Functional/task variability,\nand (5) Temporality and environmental variability. We illustrate the\nmanifestation of these dimensions across a set of emerging and anticipated\napplications of XR systems. Finally, we propose evaluation metrics, dataset\nrequirements, and design tradeoffs necessary for the development of\nresource-aware FedFMs in XR. This perspective aims to chart the technical and\nconceptual foundations for context-aware privacy-preserving intelligence in the\nnext generation of XR systems.",
    "updated" : "2025-06-12T04:18:38Z",
    "published" : "2025-06-06T02:23:42Z",
    "authors" : [
      {
        "name" : "Fardis Nadimi"
      },
      {
        "name" : "Payam Abdisarabshali"
      },
      {
        "name" : "Kasra Borazjani"
      },
      {
        "name" : "Jacob Chakareski"
      },
      {
        "name" : "Seyyedali Hosseinalipour"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05421v2",
    "title" : "TRIDENT -- A Three-Tier Privacy-Preserving Propaganda Detection Model in\n  Mobile Networks using Transformers, Adversarial Learning, and Differential\n  Privacy",
    "summary" : "The proliferation of propaganda on mobile platforms raises critical concerns\naround detection accuracy and user privacy. To address this, we propose TRIDENT\n- a three-tier propaganda detection model implementing transformers,\nadversarial learning, and differential privacy which integrates syntactic\nobfuscation and label perturbation to mitigate privacy leakage while\nmaintaining propaganda detection accuracy. TRIDENT leverages multilingual\nback-translation to introduce semantic variance, character-level noise, and\nentity obfuscation for differential privacy enforcement, and combines these\ntechniques into a unified defense mechanism. Using a binary propaganda\nclassification dataset, baseline transformer models (BERT, GPT-2) we achieved\nF1 scores of 0.89 and 0.90. Applying TRIDENT's third-tier defense yields a\nreduced but effective cumulative F1 of 0.83, demonstrating strong privacy\nprotection across mobile ML deployments with minimal degradation.",
    "updated" : "2025-06-12T01:37:56Z",
    "published" : "2025-06-05T02:38:02Z",
    "authors" : [
      {
        "name" : "Al Nahian Bin Emran"
      },
      {
        "name" : "Dhiman Goswami"
      },
      {
        "name" : "Md Hasan Ullah Sadi"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.11687v1",
    "title" : "Differential Privacy in Machine Learning: From Symbolic AI to LLMs",
    "summary" : "Machine learning models should not reveal particular information that is not\notherwise accessible. Differential privacy provides a formal framework to\nmitigate privacy risks by ensuring that the inclusion or exclusion of any\nsingle data point does not significantly alter the output of an algorithm, thus\nlimiting the exposure of private information. This survey paper explores the\nfoundational definitions of differential privacy, reviews its original\nformulations and tracing its evolution through key research contributions. It\nthen provides an in-depth examination of how DP has been integrated into\nmachine learning models, analyzing existing proposals and methods to preserve\nprivacy when training ML models. Finally, it describes how DP-based ML\ntechniques can be evaluated in practice. %Finally, it discusses the broader\nimplications of DP, highlighting its potential for public benefit, its\nreal-world applications, and the challenges it faces, including vulnerabilities\nto adversarial attacks. By offering a comprehensive overview of differential\nprivacy in machine learning, this work aims to contribute to the ongoing\ndevelopment of secure and responsible AI systems.",
    "updated" : "2025-06-13T11:30:35Z",
    "published" : "2025-06-13T11:30:35Z",
    "authors" : [
      {
        "name" : "Francisco Aguilera-Martínez"
      },
      {
        "name" : "Fernando Berzal"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.11679v1",
    "title" : "LLMs on support of privacy and security of mobile apps: state of the art\n  and research directions",
    "summary" : "Modern life has witnessed the explosion of mobile devices. However, besides\nthe valuable features that bring convenience to end users, security and privacy\nrisks still threaten users of mobile apps. The increasing sophistication of\nthese threats in recent years has underscored the need for more advanced and\nefficient detection approaches. In this chapter, we explore the application of\nLarge Language Models (LLMs) to identify security risks and privacy violations\nand mitigate them for the mobile application ecosystem. By introducing\nstate-of-the-art research that applied LLMs to mitigate the top 10 common\nsecurity risks of smartphone platforms, we highlight the feasibility and\npotential of LLMs to replace traditional analysis methods, such as dynamic and\nhybrid analysis of mobile apps. As a representative example of LLM-based\nsolutions, we present an approach to detect sensitive data leakage when users\nshare images online, a common behavior of smartphone users nowadays. Finally,\nwe discuss open research challenges.",
    "updated" : "2025-06-13T11:17:15Z",
    "published" : "2025-06-13T11:17:15Z",
    "authors" : [
      {
        "name" : "Tran Thanh Lam Nguyen"
      },
      {
        "name" : "Barbara Carminati"
      },
      {
        "name" : "Elena Ferrari"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.11434v1",
    "title" : "Auditing Data Provenance in Real-world Text-to-Image Diffusion Models\n  for Privacy and Copyright Protection",
    "summary" : "Text-to-image diffusion model since its propose has significantly influenced\nthe content creation due to its impressive generation capability. However, this\ncapability depends on large-scale text-image datasets gathered from web\nplatforms like social media, posing substantial challenges in copyright\ncompliance and personal privacy leakage. Though there are some efforts devoted\nto explore approaches for auditing data provenance in text-to-image diffusion\nmodels, existing work has unrealistic assumptions that can obtain model\ninternal knowledge, e.g., intermediate results, or the evaluation is not\nreliable. To fill this gap, we propose a completely black-box auditing\nframework called Feature Semantic Consistency-based Auditing (FSCA). It\nutilizes two types of semantic connections within the text-to-image diffusion\nmodel for auditing, eliminating the need for access to internal knowledge. To\ndemonstrate the effectiveness of our FSCA framework, we perform extensive\nexperiments on LAION-mi dataset and COCO dataset, and compare with eight\nstate-of-the-art baseline approaches. The results show that FSCA surpasses\nprevious baseline approaches across various metrics and different data\ndistributions, showcasing the superiority of our FSCA. Moreover, we introduce a\nrecall balance strategy and a threshold adjustment strategy, which collectively\nallows FSCA to reach up a user-level accuracy of 90% in a real-world auditing\nscenario with only 10 samples/user, highlighting its strong auditing potential\nin real-world applications. Our code is made available at\nhttps://github.com/JiePKU/FSCA.",
    "updated" : "2025-06-13T03:16:16Z",
    "published" : "2025-06-13T03:16:16Z",
    "authors" : [
      {
        "name" : "Jie Zhu"
      },
      {
        "name" : "Leye Wang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08347v2",
    "title" : "Differentially Private Relational Learning with Entity-level Privacy\n  Guarantees",
    "summary" : "Learning with relational and network-structured data is increasingly vital in\nsensitive domains where protecting the privacy of individual entities is\nparamount. Differential Privacy (DP) offers a principled approach for\nquantifying privacy risks, with DP-SGD emerging as a standard mechanism for\nprivate model training. However, directly applying DP-SGD to relational\nlearning is challenging due to two key factors: (i) entities often participate\nin multiple relations, resulting in high and difficult-to-control sensitivity;\nand (ii) relational learning typically involves multi-stage, potentially\ncoupled (interdependent) sampling procedures that make standard privacy\namplification analyses inapplicable. This work presents a principled framework\nfor relational learning with formal entity-level DP guarantees. We provide a\nrigorous sensitivity analysis and introduce an adaptive gradient clipping\nscheme that modulates clipping thresholds based on entity occurrence frequency.\nWe also extend the privacy amplification results to a tractable subclass of\ncoupled sampling, where the dependence arises only through sample sizes. These\ncontributions lead to a tailored DP-SGD variant for relational data with\nprovable privacy guarantees. Experiments on fine-tuning text encoders over\ntext-attributed network-structured relational data demonstrate the strong\nutility-privacy trade-offs of our approach. Our code is available at\nhttps://github.com/Graph-COM/Node_DP.",
    "updated" : "2025-06-12T19:17:36Z",
    "published" : "2025-06-10T02:03:43Z",
    "authors" : [
      {
        "name" : "Yinan Huang"
      },
      {
        "name" : "Haoteng Yin"
      },
      {
        "name" : "Eli Chien"
      },
      {
        "name" : "Rongzhe Wei"
      },
      {
        "name" : "Pan Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.11069v1",
    "title" : "Regularized Federated Learning for Privacy-Preserving Dysarthric and\n  Elderly Speech Recognition",
    "summary" : "Accurate recognition of dysarthric and elderly speech remains challenging to\ndate. While privacy concerns have driven a shift from centralized approaches to\nfederated learning (FL) to ensure data confidentiality, this further\nexacerbates the challenges of data scarcity, imbalanced data distribution and\nspeaker heterogeneity. To this end, this paper conducts a systematic\ninvestigation of regularized FL techniques for privacy-preserving dysarthric\nand elderly speech recognition, addressing different levels of the FL process\nby 1) parameter-based, 2) embedding-based and 3) novel loss-based\nregularization. Experiments on the benchmark UASpeech dysarthric and\nDementiaBank Pitt elderly speech corpora suggest that regularized FL systems\nconsistently outperform the baseline FedAvg system by statistically significant\nWER reductions of up to 0.55\\% absolute (2.13\\% relative). Further increasing\ncommunication frequency to one exchange per batch approaches centralized\ntraining performance.",
    "updated" : "2025-06-02T01:34:20Z",
    "published" : "2025-06-02T01:34:20Z",
    "authors" : [
      {
        "name" : "Tao Zhong"
      },
      {
        "name" : "Mengzhe Geng"
      },
      {
        "name" : "Shujie Hu"
      },
      {
        "name" : "Guinan Li"
      },
      {
        "name" : "Xunying Liu"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.13561v1",
    "title" : "Perfect Privacy for Discriminator-Based Byzantine-Resilient Federated\n  Learning",
    "summary" : "Federated learning (FL) shows great promise in large-scale machine learning\nbut introduces new privacy and security challenges. We propose ByITFL and\nLoByITFL, two novel FL schemes that enhance resilience against Byzantine users\nwhile keeping the users' data private from eavesdroppers. To ensure privacy and\nByzantine resilience, our schemes build on having a small representative\ndataset available to the federator and crafting a discriminator function\nallowing the mitigation of corrupt users' contributions. ByITFL employs\nLagrange coded computing and re-randomization, making it the first\nByzantine-resilient FL scheme with perfect Information-Theoretic (IT) privacy,\nthough at the cost of a significant communication overhead. LoByITFL, on the\nother hand, achieves Byzantine resilience and IT privacy at a significantly\nreduced communication cost, but requires a Trusted Third Party, used only in a\none-time initialization phase before training. We provide theoretical\nguarantees on privacy and Byzantine resilience, along with convergence\nguarantees and experimental results validating our findings.",
    "updated" : "2025-06-16T14:47:02Z",
    "published" : "2025-06-16T14:47:02Z",
    "authors" : [
      {
        "name" : "Yue Xia"
      },
      {
        "name" : "Christoph Hofmeister"
      },
      {
        "name" : "Maximilian Egger"
      },
      {
        "name" : "Rawad Bitar"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.13170v1",
    "title" : "Dual Protection Ring: User Profiling Via Differential Privacy and\n  Service Dissemination Through Private Information Retrieval",
    "summary" : "User profiling is crucial in providing personalised services, as it relies on\nanalysing user behaviour and preferences to deliver targeted services. This\napproach enhances user experience and promotes heightened engagement.\nNevertheless, user profiling also gives rise to noteworthy privacy\nconsiderations due to the extensive tracking and monitoring of personal data,\npotentially leading to surveillance or identity theft. We propose a dual-ring\nprotection mechanism to protect user privacy by examining various threats to\nuser privacy, such as behavioural attacks, profiling fingerprinting and\nmonitoring, profile perturbation, etc., both on the user and service provider\nsides. We develop user profiles that contain sensitive private attributes and\nan equivalent profile based on differential privacy for evaluating personalised\nservices. We determine the entropy of the resultant profiles during each update\nto protect profiling attributes and invoke various processes, such as data\nevaporation, to artificially increase entropy or destroy private profiling\nattributes. Furthermore, we use different variants of private information\nretrieval (PIR) to retrieve personalised services against differentially\nprivate profiles. We implement critical components of the proposed model via a\nproof-of-concept mobile app to demonstrate its applicability over a specific\ncase study of advertising services, which can be generalised to other services.\nOur experimental results show that the observed processing delays with\ndifferent PIR schemes are similar to the current advertising systems.",
    "updated" : "2025-06-16T07:33:12Z",
    "published" : "2025-06-16T07:33:12Z",
    "authors" : [
      {
        "name" : "Imdad Ullah"
      },
      {
        "name" : "Najm Hassan"
      },
      {
        "name" : "Tariq Ahamed Ahangar"
      },
      {
        "name" : "Zawar Hussain Shah"
      },
      {
        "name" : "Mehregan Mahdavi Andrew Levula"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.13052v1",
    "title" : "Buy it Now, Track Me Later: Attacking User Privacy via Wi-Fi AP Online\n  Auctions",
    "summary" : "Static and hard-coded layer-two network identifiers are well known to present\nsecurity vulnerabilities and endanger user privacy. In this work, we introduce\na new privacy attack against Wi-Fi access points listed on secondhand\nmarketplaces. Specifically, we demonstrate the ability to remotely gather a\nlarge quantity of layer-two Wi-Fi identifiers by programmatically querying the\neBay marketplace and applying state-of-the-art computer vision techniques to\nextract IEEE 802.11 BSSIDs from the seller's posted images of the hardware. By\nleveraging data from a global Wi-Fi Positioning System (WPS) that geolocates\nBSSIDs, we obtain the physical locations of these devices both pre- and\npost-sale. In addition to validating the degree to which a seller's location\nmatches the location of the device, we examine cases of device movement -- once\nthe device is sold and then subsequently re-used in a new environment. Our work\nhighlights a previously unrecognized privacy vulnerability and suggests, yet\nagain, the strong need to protect layer-two network identifiers.",
    "updated" : "2025-06-16T02:42:14Z",
    "published" : "2025-06-16T02:42:14Z",
    "authors" : [
      {
        "name" : "Steven Su"
      },
      {
        "name" : "Erik Rye"
      },
      {
        "name" : "Robert Beverly"
      },
      {
        "name" : "Dave Levin"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.13009v1",
    "title" : "Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A\n  New Inference Attack Perspective",
    "summary" : "Machine unlearning focuses on efficiently removing specific data from trained\nmodels, addressing privacy and compliance concerns with reasonable costs.\nAlthough exact unlearning ensures complete data removal equivalent to\nretraining, it is impractical for large-scale models, leading to growing\ninterest in inexact unlearning methods. However, the lack of formal guarantees\nin these methods necessitates the need for robust evaluation frameworks to\nassess their privacy and effectiveness. In this work, we first identify several\nkey pitfalls of the existing unlearning evaluation frameworks, e.g., focusing\non average-case evaluation or targeting random samples for evaluation,\nincomplete comparisons with the retraining baseline. Then, we propose RULI\n(Rectified Unlearning Evaluation Framework via Likelihood Inference), a novel\nframework to address critical gaps in the evaluation of inexact unlearning\nmethods. RULI introduces a dual-objective attack to measure both unlearning\nefficacy and privacy risks at a per-sample granularity. Our findings reveal\nsignificant vulnerabilities in state-of-the-art unlearning methods, where RULI\nachieves higher attack success rates, exposing privacy risks underestimated by\nexisting methods. Built on a game-based foundation and validated through\nempirical evaluations on both image and text data (spanning tasks from\nclassification to generation), RULI provides a rigorous, scalable, and\nfine-grained methodology for evaluating unlearning techniques.",
    "updated" : "2025-06-16T00:30:02Z",
    "published" : "2025-06-16T00:30:02Z",
    "authors" : [
      {
        "name" : "Nima Naderloui"
      },
      {
        "name" : "Shenao Yan"
      },
      {
        "name" : "Binghui Wang"
      },
      {
        "name" : "Jie Fu"
      },
      {
        "name" : "Wendy Hui Wang"
      },
      {
        "name" : "Weiran Liu"
      },
      {
        "name" : "Yuan Hong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.12846v1",
    "title" : "Privacy-Preserving Federated Learning against Malicious Clients Based on\n  Verifiable Functional Encryption",
    "summary" : "Federated learning is a promising distributed learning paradigm that enables\ncollaborative model training without exposing local client data, thereby\nprotect data privacy. However, it also brings new threats and challenges. The\nadvancement of model inversion attacks has rendered the plaintext transmission\nof local models insecure, while the distributed nature of federated learning\nmakes it particularly vulnerable to attacks raised by malicious clients. To\nprotect data privacy and prevent malicious client attacks, this paper proposes\na privacy-preserving federated learning framework based on verifiable\nfunctional encryption, without a non-colluding dual-server setup or additional\ntrusted third-party. Specifically, we propose a novel decentralized verifiable\nfunctional encryption (DVFE) scheme that enables the verification of specific\nrelationships over multi-dimensional ciphertexts. This scheme is formally\ntreated, in terms of definition, security model and security proof.\nFurthermore, based on the proposed DVFE scheme, we design a privacy-preserving\nfederated learning framework VFEFL that incorporates a novel robust aggregation\nrule to detect malicious clients, enabling the effective training of\nhigh-accuracy models under adversarial settings. Finally, we provide formal\nanalysis and empirical evaluation of the proposed schemes. The results\ndemonstrate that our approach achieves the desired privacy protection,\nrobustness, verifiability and fidelity, while eliminating the reliance on\nnon-colluding dual-server settings or trusted third parties required by\nexisting methods.",
    "updated" : "2025-06-15T13:38:40Z",
    "published" : "2025-06-15T13:38:40Z",
    "authors" : [
      {
        "name" : "Nina Cai"
      },
      {
        "name" : "Jinguang Han"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.12749v1",
    "title" : "Free Privacy Protection for Wireless Federated Learning: Enjoy It or\n  Suffer from It?",
    "summary" : "Inherent communication noises have the potential to preserve privacy for\nwireless federated learning (WFL) but have been overlooked in digital\ncommunication systems predominantly using floating-point number standards,\ne.g., IEEE 754, for data storage and transmission. This is due to the\npotentially catastrophic consequences of bit errors in floating-point numbers,\ne.g., on the sign or exponent bits. This paper presents a novel channel-native\nbit-flipping differential privacy (DP) mechanism tailored for WFL, where\ntransmit bits are randomly flipped and communication noises are leveraged, to\ncollectively preserve the privacy of WFL in digital communication systems. The\nkey idea is to interpret the bit perturbation at the transmitter and bit errors\ncaused by communication noises as a bit-flipping DP process. This is achieved\nby designing a new floating-point-to-fixed-point conversion method that only\ntransmits the bits in the fraction part of model parameters, hence eliminating\nthe need for transmitting the sign and exponent bits and preventing the\ncatastrophic consequence of bit errors. We analyze a new metric to measure the\nbit-level distance of the model parameters and prove that the proposed\nmechanism satisfies (\\lambda,\\epsilon)-R\\'enyi DP and does not violate the WFL\nconvergence. Experiments validate privacy and convergence analysis of the\nproposed mechanism and demonstrate its superiority to the state-of-the-art\nGaussian mechanisms that are channel-agnostic and add Gaussian noise for\nprivacy protection.",
    "updated" : "2025-06-15T07:13:52Z",
    "published" : "2025-06-15T07:13:52Z",
    "authors" : [
      {
        "name" : "Weicai Li"
      },
      {
        "name" : "Tiejun Lv"
      },
      {
        "name" : "Xiyu Zhao"
      },
      {
        "name" : "Xin Yuan"
      },
      {
        "name" : "Wei Ni"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.12699v1",
    "title" : "SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy\n  Risks, and Mitigation",
    "summary" : "Large language models (LLMs) are sophisticated artificial intelligence\nsystems that enable machines to generate human-like text with remarkable\nprecision. While LLMs offer significant technological progress, their\ndevelopment using vast amounts of user data scraped from the web and collected\nfrom extensive user interactions poses risks of sensitive information leakage.\nMost existing surveys focus on the privacy implications of the training data\nbut tend to overlook privacy risks from user interactions and advanced LLM\ncapabilities. This paper aims to fill that gap by providing a comprehensive\nanalysis of privacy in LLMs, categorizing the challenges into four main areas:\n(i) privacy issues in LLM training data, (ii) privacy challenges associated\nwith user prompts, (iii) privacy vulnerabilities in LLM-generated outputs, and\n(iv) privacy challenges involving LLM agents. We evaluate the effectiveness and\nlimitations of existing mitigation mechanisms targeting these proposed privacy\nchallenges and identify areas for further research.",
    "updated" : "2025-06-15T03:14:03Z",
    "published" : "2025-06-15T03:14:03Z",
    "authors" : [
      {
        "name" : "Yashothara Shanmugarasa"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "M. A. P Chamikara"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.12523v1",
    "title" : "Privacy-preserving and reward-based mechanisms of proof of engagement",
    "summary" : "Proof-of-Attendance (PoA) mechanisms are typically employed to demonstrate a\nspecific user's participation in an event, whether virtual or in-person. The\ngoal of this study is to extend such mechanisms to broader contexts where the\nuser wishes to digitally demonstrate her involvement in a specific activity\n(Proof-of-Engagement, PoE). This work explores different solutions, including\nDLTs as well as established technologies based on centralized systems. The main\naspects we consider include the level of privacy guaranteed to users, the scope\nof PoA/PoE (both temporal and spatial), the transferability of the proof, and\nthe integration with incentive mechanisms.",
    "updated" : "2025-06-14T14:33:39Z",
    "published" : "2025-06-14T14:33:39Z",
    "authors" : [
      {
        "name" : "Matteo Marco Montanari"
      },
      {
        "name" : "Alessandro Aldini"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.ET",
      "C.2.4; D.2.11; K.6.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.12328v1",
    "title" : "Information-theoretic Estimation of the Risk of Privacy Leaks",
    "summary" : "Recent work~\\cite{Liu2016} has shown that dependencies between items in a\ndataset can lead to privacy leaks. We extend this concept to privacy-preserving\ntransformations, considering a broader set of dependencies captured by\ncorrelation metrics. Specifically, we measure the correlation between the\noriginal data and their noisy responses from a randomizer as an indicator of\npotential privacy breaches. This paper aims to leverage information-theoretic\nmeasures, such as the Maximal Information Coefficient (MIC), to estimate\nprivacy leaks and derive novel, computationally efficient privacy leak\nestimators. We extend the $\\rho_1$-to-$\\rho_2$\nformulation~\\cite{Evfimievski2003} to incorporate entropy, mutual information,\nand the degree of anonymity for a more comprehensive measure of privacy risk.\nOur proposed hybrid metric can identify correlation dependencies between\nattributes in the dataset, serving as a proxy for privacy leak vulnerabilities.\nThis metric provides a computationally efficient worst-case measure of privacy\nloss, utilizing the inherent characteristics of the data to prevent privacy\nbreaches.",
    "updated" : "2025-06-14T03:39:11Z",
    "published" : "2025-06-14T03:39:11Z",
    "authors" : [
      {
        "name" : "Kenneth Odoh"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.12241v1",
    "title" : "Privacy Reasoning in Ambiguous Contexts",
    "summary" : "We study the ability of language models to reason about appropriate\ninformation disclosure - a central aspect of the evolving field of agentic\nprivacy. Whereas previous works have focused on evaluating a model's ability to\nalign with human decisions, we examine the role of ambiguity and missing\ncontext on model performance when making information-sharing decisions. We\nidentify context ambiguity as a crucial barrier for high performance in privacy\nassessments. By designing Camber, a framework for context disambiguation, we\nshow that model-generated decision rationales can reveal ambiguities and that\nsystematically disambiguating context based on these rationales leads to\nsignificant accuracy improvements (up to 13.3\\% in precision and up to 22.3\\%\nin recall) as well as reductions in prompt sensitivity. Overall, our results\nindicate that approaches for context disambiguation are a promising way forward\nto enhance agentic privacy reasoning.",
    "updated" : "2025-06-13T21:42:22Z",
    "published" : "2025-06-13T21:42:22Z",
    "authors" : [
      {
        "name" : "Ren Yi"
      },
      {
        "name" : "Octavian Suciu"
      },
      {
        "name" : "Adria Gascon"
      },
      {
        "name" : "Sarah Meiklejohn"
      },
      {
        "name" : "Eugene Bagdasarian"
      },
      {
        "name" : "Marco Gruteser"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.12106v1",
    "title" : "Enhancing Privacy: The Utility of Stand-Alone Synthetic CT and MRI for\n  Tumor and Bone Segmentation",
    "summary" : "AI requires extensive datasets, while medical data is subject to high data\nprotection. Anonymization is essential, but poses a challenge for some regions,\nsuch as the head, as identifying structures overlap with regions of clinical\ninterest. Synthetic data offers a potential solution, but studies often lack\nrigorous evaluation of realism and utility. Therefore, we investigate to what\nextent synthetic data can replace real data in segmentation tasks. We employed\nhead and neck cancer CT scans and brain glioma MRI scans from two large\ndatasets. Synthetic data were generated using generative adversarial networks\nand diffusion models. We evaluated the quality of the synthetic data using MAE,\nMS-SSIM, Radiomics and a Visual Turing Test (VTT) performed by 5 radiologists\nand their usefulness in segmentation tasks using DSC. Radiomics indicates high\nfidelity of synthetic MRIs, but fall short in producing highly realistic CT\ntissue, with correlation coefficient of 0.8784 and 0.5461 for MRI and CT\ntumors, respectively. DSC results indicate limited utility of synthetic data:\ntumor segmentation achieved DSC=0.064 on CT and 0.834 on MRI, while bone\nsegmentation a mean DSC=0.841. Relation between DSC and correlation is\nobserved, but is limited by the complexity of the task. VTT results show\nsynthetic CTs' utility, but with limited educational applications. Synthetic\ndata can be used independently for the segmentation task, although limited by\nthe complexity of the structures to segment. Advancing generative models to\nbetter tolerate heterogeneous inputs and learn subtle details is essential for\nenhancing their realism and expanding their application potential.",
    "updated" : "2025-06-13T08:17:48Z",
    "published" : "2025-06-13T08:17:48Z",
    "authors" : [
      {
        "name" : "André Ferreira"
      },
      {
        "name" : "Kunpeng Xie"
      },
      {
        "name" : "Caroline Wilpert"
      },
      {
        "name" : "Gustavo Correia"
      },
      {
        "name" : "Felix Barajas Ordonez"
      },
      {
        "name" : "Tiago Gil Oliveira"
      },
      {
        "name" : "Maike Bode"
      },
      {
        "name" : "Robert Siepmann"
      },
      {
        "name" : "Frank Hölzle"
      },
      {
        "name" : "Rainer Röhrig"
      },
      {
        "name" : "Jens Kleesiek"
      },
      {
        "name" : "Daniel Truhn"
      },
      {
        "name" : "Jan Egger"
      },
      {
        "name" : "Victor Alves"
      },
      {
        "name" : "Behrus Puladi"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.12088v1",
    "title" : "Risks & Benefits of LLMs & GenAI for Platform Integrity, Healthcare\n  Diagnostics, Cybersecurity, Privacy & AI Safety: A Comprehensive Survey,\n  Roadmap & Implementation Blueprint",
    "summary" : "Large Language Models (LLMs) and generative AI (GenAI) systems such as\nChatGPT, Claude, Gemini, LLaMA, and Copilot, developed by OpenAI, Anthropic,\nGoogle, Meta, and Microsoft are reshaping digital platforms and app ecosystems\nwhile introducing key challenges in cybersecurity, privacy, and platform\nintegrity. Our analysis shows alarming trends: LLM-assisted malware is\nprojected to rise from 2% in 2021 to 50% by 2025; AI-generated Google reviews\ngrew from 1.2% in 2021 to 12.21% in 2023, with an expected 30% by 2025; AI scam\nreports surged 456%; and misinformation sites increased over 1500%, with a\n50-60% increase in deepfakes in 2024. Concurrently, as LLMs have facilitated\ncode development, mobile app submissions grew from 1.8 million in 2020 to 3.0\nmillion in 2024, with 3.6 million expected by 2025. To address AI threats,\nplatforms from app stores like Google Play and Apple to developer hubs like\nGitHub Copilot, and social platforms like TikTok and Facebook, to marketplaces\nlike Amazon are deploying AI and LLM-based defenses. This highlights the dual\nnature of these technologies as both the source of new threats and the\nessential tool for their mitigation. Integrating LLMs into clinical diagnostics\nalso raises concerns about accuracy, bias, and safety, needing strong\ngovernance. Drawing on a comprehensive analysis of 455 references, this paper\npresents a survey of LLM and GenAI risks. We propose a strategic roadmap and\noperational blueprint integrating policy auditing (CCPA, GDPR), fraud\ndetection, and compliance automation, and an advanced LLM-DA stack with modular\ncomponents including multi LLM routing, agentic memory, and governance layers\nto enhance platform integrity. We also provide actionable insights,\ncross-functional best practices, and real-world case studies. These\ncontributions offer paths to scalable trust, safety, and responsible innovation\nacross digital platforms.",
    "updated" : "2025-06-10T18:03:19Z",
    "published" : "2025-06-10T18:03:19Z",
    "authors" : [
      {
        "name" : "Kiarash Ahi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08185v2",
    "title" : "Agentic Surgical AI: Surgeon Style Fingerprinting and Privacy Risk\n  Quantification via Discrete Diffusion in a Vision-Language-Action Framework",
    "summary" : "Surgeons exhibit distinct operating styles shaped by training, experience,\nand motor behavior-yet most surgical AI systems overlook this personalization\nsignal. We propose a novel agentic modeling approach for surgeon-specific\nbehavior prediction in robotic surgery, combining a discrete diffusion\nframework with a vision-language-action (VLA) pipeline. Gesture prediction is\nframed as a structured sequence denoising task, conditioned on multimodal\ninputs including surgical video, intent language, and personalized embeddings\nof surgeon identity and skill. These embeddings are encoded through natural\nlanguage prompts using third-party language models, allowing the model to\nretain individual behavioral style without exposing explicit identity. We\nevaluate our method on the JIGSAWS dataset and demonstrate that it accurately\nreconstructs gesture sequences while learning meaningful motion fingerprints\nunique to each surgeon. To quantify the privacy implications of\npersonalization, we perform membership inference attacks and find that more\nexpressive embeddings improve task performance but simultaneously increase\nsusceptibility to identity leakage. These findings demonstrate that while\npersonalized embeddings improve performance, they also increase vulnerability\nto identity leakage, revealing the importance of balancing personalization with\nprivacy risk in surgical modeling. Code is available at:\nhttps://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.",
    "updated" : "2025-06-14T12:02:06Z",
    "published" : "2025-06-09T19:49:55Z",
    "authors" : [
      {
        "name" : "Huixin Zhan"
      },
      {
        "name" : "Jason H. Moore"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07555v2",
    "title" : "Synthesize Privacy-Preserving High-Resolution Images via Private Textual\n  Intermediaries",
    "summary" : "Generating high fidelity, differentially private (DP) synthetic images offers\na promising route to share and analyze sensitive visual data without\ncompromising individual privacy. However, existing DP image synthesis methods\nstruggle to produce high resolution outputs that faithfully capture the\nstructure of the original data. In this paper, we introduce a novel method,\nreferred to as Synthesis via Private Textual Intermediaries (SPTI), that can\ngenerate high resolution DP images with easy adoption. The key idea is to shift\nthe challenge of DP image synthesis from the image domain to the text domain by\nleveraging state of the art DP text generation methods. SPTI first summarizes\neach private image into a concise textual description using image to text\nmodels, then applies a modified Private Evolution algorithm to generate DP\ntext, and finally reconstructs images using text to image models. Notably, SPTI\nrequires no model training, only inference with off the shelf models. Given a\nprivate dataset, SPTI produces synthetic images of substantially higher quality\nthan prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less\nthan or equal to 26.71 under epsilon equal to 1.0, improving over Private\nEvolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less\nthan or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine\ntuning baselines. Overall, our results demonstrate that Synthesis via Private\nTextual Intermediaries provides a resource efficient and proprietary model\ncompatible framework for generating high resolution DP synthetic images,\ngreatly expanding access to private visual datasets.",
    "updated" : "2025-06-14T03:19:44Z",
    "published" : "2025-06-09T08:48:06Z",
    "authors" : [
      {
        "name" : "Haoxiang Wang"
      },
      {
        "name" : "Zinan Lin"
      },
      {
        "name" : "Da Yu"
      },
      {
        "name" : "Huishuai Zhang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15349v1",
    "title" : "Enhancing One-run Privacy Auditing with Quantile Regression-Based\n  Membership Inference",
    "summary" : "Differential privacy (DP) auditing aims to provide empirical lower bounds on\nthe privacy guarantees of DP mechanisms like DP-SGD. While some existing\ntechniques require many training runs that are prohibitively costly, recent\nwork introduces one-run auditing approaches that effectively audit DP-SGD in\nwhite-box settings while still being computationally efficient. However, in the\nmore practical black-box setting where gradients cannot be manipulated during\ntraining and only the last model iterate is observed, prior work shows that\nthere is still a large gap between the empirical lower bounds and theoretical\nupper bounds. Consequently, in this work, we study how incorporating approaches\nfor stronger membership inference attacks (MIA) can improve one-run auditing in\nthe black-box setting. Evaluating on image classification models trained on\nCIFAR-10 with DP-SGD, we demonstrate that our proposed approach, which utilizes\nquantile regression for MIA, achieves tighter bounds while crucially\nmaintaining the computational efficiency of one-run methods.",
    "updated" : "2025-06-18T11:03:39Z",
    "published" : "2025-06-18T11:03:39Z",
    "authors" : [
      {
        "name" : "Terrance Liu"
      },
      {
        "name" : "Matteo Boglioni"
      },
      {
        "name" : "Yiwei Fu"
      },
      {
        "name" : "Shengyuan Hu"
      },
      {
        "name" : "Pratiksha Thaker"
      },
      {
        "name" : "Zhiwei Steven Wu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15307v1",
    "title" : "SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language\n  Models Using Forward-Only Passes",
    "summary" : "Large language models (LLMs) have transformed numerous fields, yet their\nadaptation to specialized tasks in privacy-sensitive domains, such as\nhealthcare and finance, is constrained by the scarcity of accessible training\ndata due to stringent privacy requirements. Secure multi-party computation\n(MPC)-based privacy-preserving machine learning offers a powerful approach to\nprotect both model parameters and user data, but its application to LLMs has\nbeen largely limited to inference, as fine-tuning introduces significant\ncomputational challenges, particularly in privacy-preserving backward\npropagation and optimizer operations. This paper identifies two primary\nobstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the\nsubstantial computational overhead of backward and optimizer processes, and (2)\nthe inefficiency of softmax-based attention mechanisms in MPC settings. To\naddress these challenges, we propose SecFwT, the first MPC-based framework\ndesigned for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a\nforward-only tuning paradigm to eliminate backward and optimizer computations\nand employs MPC-friendly Random Feature Attention to approximate softmax\nattention, significantly reducing costly non-linear operations and\ncomputational complexity. Experimental results demonstrate that SecFwT delivers\nsubstantial improvements in efficiency and privacy preservation, enabling\nscalable and secure fine-tuning of LLMs for privacy-critical applications.",
    "updated" : "2025-06-18T09:36:57Z",
    "published" : "2025-06-18T09:36:57Z",
    "authors" : [
      {
        "name" : "Jinglong Luo"
      },
      {
        "name" : "Zhuo Zhang"
      },
      {
        "name" : "Yehong Zhang"
      },
      {
        "name" : "Shiyu Liu"
      },
      {
        "name" : "Ye Dong"
      },
      {
        "name" : "Xun Zhou"
      },
      {
        "name" : "Hui Wang"
      },
      {
        "name" : "Yue Yu"
      },
      {
        "name" : "Zenglin Xu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15258v1",
    "title" : "Privacy-Preserving Chest X-ray Classification in Latent Space with\n  Homomorphically Encrypted Neural Inference",
    "summary" : "Medical imaging data contain sensitive patient information requiring strong\nprivacy protection. Many analytical setups require data to be sent to a server\nfor inference purposes. Homomorphic encryption (HE) provides a solution by\nallowing computations to be performed on encrypted data without revealing the\noriginal information. However, HE inference is computationally expensive,\nparticularly for large images (e.g., chest X-rays). In this study, we propose\nan HE inference framework for medical images that uses VQGAN to compress images\ninto latent representations, thereby significantly reducing the computational\nburden while preserving image quality. We approximate the activation functions\nwith lower-degree polynomials to balance the accuracy and efficiency in\ncompliance with HE requirements. We observed that a downsampling factor of\neight for compression achieved an optimal balance between performance and\ncomputational cost. We further adapted the squeeze and excitation module, which\nis known to improve traditional CNNs, to enhance the HE framework. Our method\nwas tested on two chest X-ray datasets for multi-label classification tasks\nusing vanilla CNN backbones. Although HE inference remains relatively slow and\nintroduces minor performance differences compared with unencrypted inference,\nour approach shows strong potential for practical use in medical images",
    "updated" : "2025-06-18T08:35:50Z",
    "published" : "2025-06-18T08:35:50Z",
    "authors" : [
      {
        "name" : "Jonghun Kim"
      },
      {
        "name" : "Gyeongdeok Jo"
      },
      {
        "name" : "Shinyoung Ra"
      },
      {
        "name" : "Hyunjin Park"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15224v1",
    "title" : "Facility Location Problem under Local Differential Privacy without\n  Super-set Assumption",
    "summary" : "In this paper, we introduce an adaptation of the facility location problem\nand analyze it within the framework of local differential privacy (LDP). Under\nthis model, we ensure the privacy of client presence at specific locations.\nWhen n is the number of points, Gupta et al. established a lower bound of\n$\\Omega(\\sqrt{n})$ on the approximation ratio for any differentially private\nalgorithm applied to the original facility location problem. As a result,\nsubsequent works have adopted the super-set assumption, which may, however,\ncompromise user privacy. We show that this lower bound does not apply to our\nadaptation by presenting an LDP algorithm that achieves a constant\napproximation ratio with a relatively small additive factor. Additionally, we\nprovide experimental results demonstrating that our algorithm outperforms the\nstraightforward approach on both synthetically generated and real-world\ndatasets.",
    "updated" : "2025-06-18T08:08:12Z",
    "published" : "2025-06-18T08:08:12Z",
    "authors" : [
      {
        "name" : "Kevin Pfisterer"
      },
      {
        "name" : "Quentin Hillebrand"
      },
      {
        "name" : "Vorapong Suppakitpaisarn"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15201v1",
    "title" : "Privacy-Shielded Image Compression: Defending Against Exploitation from\n  Vision-Language Pretrained Models",
    "summary" : "The improved semantic understanding of vision-language pretrained (VLP)\nmodels has made it increasingly difficult to protect publicly posted images\nfrom being exploited by search engines and other similar tools. In this\ncontext, this paper seeks to protect users' privacy by implementing defenses at\nthe image compression stage to prevent exploitation. Specifically, we propose a\nflexible coding method, termed Privacy-Shielded Image Compression (PSIC), that\ncan produce bitstreams with multiple decoding options. By default, the\nbitstream is decoded to preserve satisfactory perceptual quality while\npreventing interpretation by VLP models. Our method also retains the original\nimage compression functionality. With a customizable input condition, the\nproposed scheme can reconstruct the image that preserves its full semantic\ninformation. A Conditional Latent Trigger Generation (CLTG) module is proposed\nto produce bias information based on customizable conditions to guide the\ndecoding process into different reconstructed versions, and an\nUncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed\nto leverage the soft labels inferred from the target VLP model's uncertainty on\nthe training data. This paper further incorporates an adaptive multi-objective\noptimization strategy to obtain improved encrypting performance and perceptual\nquality simultaneously within a unified training process. The proposed scheme\nis plug-and-play and can be seamlessly integrated into most existing Learned\nImage Compression (LIC) models. Extensive experiments across multiple\ndownstream tasks have demonstrated the effectiveness of our design.",
    "updated" : "2025-06-18T07:29:40Z",
    "published" : "2025-06-18T07:29:40Z",
    "authors" : [
      {
        "name" : "Xuelin Shen"
      },
      {
        "name" : "Jiayin Xu"
      },
      {
        "name" : "Kangsheng Yin"
      },
      {
        "name" : "Wenhan Yang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15112v1",
    "title" : "PDLRecover: Privacy-preserving Decentralized Model Recovery with Machine\n  Unlearning",
    "summary" : "Decentralized learning is vulnerable to poison attacks, where malicious\nclients manipulate local updates to degrade global model performance. Existing\ndefenses mainly detect and filter malicious models, aiming to prevent a limited\nnumber of attackers from corrupting the global model. However, restoring an\nalready compromised global model remains a challenge. A direct approach is to\nremove malicious clients and retrain the model using only the benign clients.\nYet, retraining is time-consuming, computationally expensive, and may\ncompromise model consistency and privacy.\n  We propose PDLRecover, a novel method to recover a poisoned global model\nefficiently by leveraging historical model information while preserving\nprivacy. The main challenge lies in protecting shared historical models while\nenabling parameter estimation for model recovery. By exploiting the linearity\nof approximate Hessian matrix computation, we apply secret sharing to protect\nhistorical updates, ensuring local models are not leaked during transmission or\nreconstruction. PDLRecover introduces client-side preparation, periodic\nrecovery updates, and a final exact update to ensure robustness and convergence\nof the recovered model. Periodic updates maintain accurate curvature\ninformation, and the final step ensures high-quality convergence. Experiments\nshow that the recovered global model achieves performance comparable to a fully\nretrained model but with significantly reduced computation and time cost.\nMoreover, PDLRecover effectively prevents leakage of local model parameters,\nensuring both accuracy and privacy in recovery.",
    "updated" : "2025-06-18T03:30:07Z",
    "published" : "2025-06-18T03:30:07Z",
    "authors" : [
      {
        "name" : "Xiangman Li"
      },
      {
        "name" : "Xiaodong Wu"
      },
      {
        "name" : "Jianbing Ni"
      },
      {
        "name" : "Mohamed Mahmoud"
      },
      {
        "name" : "Maazen Alsabaan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15106v1",
    "title" : "Local Differential Privacy for Distributed Stochastic Aggregative\n  Optimization with Guaranteed Optimality",
    "summary" : "Distributed aggregative optimization underpins many cooperative optimization\nand multi-agent control systems, where each agent's objective function depends\nboth on its local optimization variable and an aggregate of all agents'\noptimization variables. Existing distributed aggregative optimization\napproaches typically require access to accurate gradients of the objective\nfunctions, which, however, are often hard to obtain in real-world applications.\nFor example, in machine learning, gradients are commonly contaminated by two\nmain sources of noise: the randomness inherent in sampled data, and the\nadditional variability introduced by mini-batch computations. In addition to\nthe issue of relying on accurate gradients, existing distributed aggregative\noptimization approaches require agents to share explicit information, which\ncould breach the privacy of participating agents. We propose an algorithm that\ncan solve both problems with existing distributed aggregative optimization\napproaches: not only can the proposed algorithm guarantee mean-square\nconvergence to an exact optimal solution when the gradients are subject to\nnoise, it also simultaneously ensures rigorous differential privacy, with the\ncumulative privacy budget guaranteed to be finite even when the number of\niterations tends to infinity. To the best of our knowledge, this is the first\nalgorithm able to guarantee both accurate convergence and rigorous differential\nprivacy in distributed aggregative optimization. Besides characterizing the\nconvergence rates under nonconvex/convex/strongly convex conditions, we also\nrigorously quantify the cost of differential privacy in terms of convergence\nrates. Experimental results on personalized machine learning using benchmark\ndatasets confirm the efficacy of the proposed algorithm.",
    "updated" : "2025-06-18T03:22:35Z",
    "published" : "2025-06-18T03:22:35Z",
    "authors" : [
      {
        "name" : "Ziqin Chen"
      },
      {
        "name" : "Yongqiang Wang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.14620v1",
    "title" : "Differential Privacy and Survey Sampling",
    "summary" : "The Horvitz-Thompson estimate of a total can be seen as as differentially\nprivate mechanism applied to this population total. We provide forumlae to\ncompute the $\\epsilon$ and $\\delta$ parameter for this specific mecanism,\ncoupled or not coupled with the addition of a Laplace or a Gaussian noise. This\nallows to determine the scale of the Laplace privacy mechanism to be added to\nreach a specified level of privacy, expressed in terms of $\\epsilon,\\delta$\ndifferential privacy. In particular, we provide simple formulae for the special\ncase of simple random sampling on binary data.",
    "updated" : "2025-06-17T15:17:06Z",
    "published" : "2025-06-17T15:17:06Z",
    "authors" : [
      {
        "name" : "Daniel Bernard Bonnéry"
      },
      {
        "name" : "Julien Jamme"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.14576v1",
    "title" : "SoK: Privacy-Enhancing Technologies in Artificial Intelligence",
    "summary" : "As artificial intelligence (AI) continues to permeate various sectors,\nsafeguarding personal and sensitive data has become increasingly crucial. To\naddress these concerns, privacy-enhancing technologies (PETs) have emerged as a\nsuite of digital tools that enable data collection and processing while\npreserving privacy. This paper explores the current landscape of data privacy\nin the context of AI, reviews the integration of PETs within AI systems, and\nassesses both their achievements and the challenges that remain.",
    "updated" : "2025-06-17T14:32:01Z",
    "published" : "2025-06-17T14:32:01Z",
    "authors" : [
      {
        "name" : "Nouha Oualha"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.14251v1",
    "title" : "Convergence-Privacy-Fairness Trade-Off in Personalized Federated\n  Learning",
    "summary" : "Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a\nbalance between personalization and generalization by conducting federated\nlearning (FL) to guide personalized learning (PL). While FL is unaffected by\npersonalized model training, in Ditto, PL depends on the outcome of the FL.\nHowever, the clients' concern about their privacy and consequent perturbation\nof their local models can affect the convergence and (performance) fairness of\nPL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension\nof Ditto under the protection of differential privacy (DP), and analyzes the\ntrade-off among its privacy guarantee, model convergence, and performance\ndistribution fairness. We also analyze the convergence upper bound of the\npersonalized models under DP-Ditto and derive the optimal number of global\naggregations given a privacy budget. Further, we analyze the performance\nfairness of the personalized models, and reveal the feasibility of optimizing\nDP-Ditto jointly for convergence and fairness. Experiments validate our\nanalysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of\nthe state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by\nover 32.71% in fairness and 9.66% in accuracy.",
    "updated" : "2025-06-17T07:15:28Z",
    "published" : "2025-06-17T07:15:28Z",
    "authors" : [
      {
        "name" : "Xiyu Zhao"
      },
      {
        "name" : "Qimei Cui"
      },
      {
        "name" : "Weicai Li"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "Ekram Hossain"
      },
      {
        "name" : "Quan Z. Sheng"
      },
      {
        "name" : "Xiaofeng Tao"
      },
      {
        "name" : "Ping Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.13972v1",
    "title" : "Membership Inference Attacks as Privacy Tools: Reliability, Disparity\n  and Ensemble",
    "summary" : "Membership inference attacks (MIAs) pose a significant threat to the privacy\nof machine learning models and are widely used as tools for privacy assessment,\nauditing, and machine unlearning. While prior MIA research has primarily\nfocused on performance metrics such as AUC, accuracy, and TPR@low FPR - either\nby developing new methods to enhance these metrics or using them to evaluate\nprivacy solutions - we found that it overlooks the disparities among different\nattacks. These disparities, both between distinct attack methods and between\nmultiple instantiations of the same method, have crucial implications for the\nreliability and completeness of MIAs as privacy evaluation tools. In this\npaper, we systematically investigate these disparities through a novel\nframework based on coverage and stability analysis. Extensive experiments\nreveal significant disparities in MIAs, their potential causes, and their\nbroader implications for privacy evaluation. To address these challenges, we\npropose an ensemble framework with three distinct strategies to harness the\nstrengths of state-of-the-art MIAs while accounting for their disparities. This\nframework not only enables the construction of more powerful attacks but also\nprovides a more robust and comprehensive methodology for privacy evaluation.",
    "updated" : "2025-06-16T20:22:07Z",
    "published" : "2025-06-16T20:22:07Z",
    "authors" : [
      {
        "name" : "Zhiqi Wang"
      },
      {
        "name" : "Chengyu Zhang"
      },
      {
        "name" : "Yuetian Chen"
      },
      {
        "name" : "Nathalie Baracaldo"
      },
      {
        "name" : "Swanand Kadhe"
      },
      {
        "name" : "Lei Yu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.13882v1",
    "title" : "Toward Practical Privacy in XR: Empirical Analysis of Multimodal\n  Anonymization Mechanisms",
    "summary" : "As extended reality (XR) systems become increasingly immersive and\nsensor-rich, they enable the collection of fine-grained behavioral signals such\nas eye and body telemetry. These signals support personalized and responsive\nexperiences and may also contain unique patterns that can be linked back to\nindividuals. However, privacy mechanisms that naively pair unimodal mechanisms\n(e.g., independently apply privacy mechanisms for eye and body privatization)\nare often ineffective at preventing re-identification in practice. In this\nwork, we systematically evaluate real-time privacy mechanisms for XR, both\nindividually and in pair, across eye and body modalities. To preserve\nusability, all mechanisms were tuned based on empirically grounded thresholds\nfor real-time interaction. We evaluated four eye and ten body mechanisms across\nmultiple datasets, comprising up to 407 participants. Our results show that\nwhile obfuscating eye telemetry alone offers moderate privacy gains, body\ntelemetry perturbation is substantially more effective. When carefully paired,\nmultimodal mechanisms reduce re-identification rate from 80.3% to 26.3% in\ncasual XR applications (e.g., VRChat and Job Simulator) and from 84.8% to 26.1%\nin competitive XR applications (e.g., Beat Saber and Synth Riders), all without\nviolating real-time usability requirements. These findings underscore the\npotential of modality-specific and context-aware privacy strategies for\nprotecting behavioral data in XR environments.",
    "updated" : "2025-06-16T18:01:39Z",
    "published" : "2025-06-16T18:01:39Z",
    "authors" : [
      {
        "name" : "Azim Ibragimov"
      },
      {
        "name" : "Ethan Wilson"
      },
      {
        "name" : "Kevin R. B. Butler"
      },
      {
        "name" : "Eakta Jain"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.12749v2",
    "title" : "Free Privacy Protection for Wireless Federated Learning: Enjoy It or\n  Suffer from It?",
    "summary" : "Inherent communication noises have the potential to preserve privacy for\nwireless federated learning (WFL) but have been overlooked in digital\ncommunication systems predominantly using floating-point number standards,\ne.g., IEEE 754, for data storage and transmission. This is due to the\npotentially catastrophic consequences of bit errors in floating-point numbers,\ne.g., on the sign or exponent bits. This paper presents a novel channel-native\nbit-flipping differential privacy (DP) mechanism tailored for WFL, where\ntransmit bits are randomly flipped and communication noises are leveraged, to\ncollectively preserve the privacy of WFL in digital communication systems. The\nkey idea is to interpret the bit perturbation at the transmitter and bit errors\ncaused by communication noises as a bit-flipping DP process. This is achieved\nby designing a new floating-point-to-fixed-point conversion method that only\ntransmits the bits in the fraction part of model parameters, hence eliminating\nthe need for transmitting the sign and exponent bits and preventing the\ncatastrophic consequence of bit errors. We analyze a new metric to measure the\nbit-level distance of the model parameters and prove that the proposed\nmechanism satisfies (\\lambda,\\epsilon)-R\\'enyi DP and does not violate the WFL\nconvergence. Experiments validate privacy and convergence analysis of the\nproposed mechanism and demonstrate its superiority to the state-of-the-art\nGaussian mechanisms that are channel-agnostic and add Gaussian noise for\nprivacy protection.",
    "updated" : "2025-06-18T09:25:11Z",
    "published" : "2025-06-15T07:13:52Z",
    "authors" : [
      {
        "name" : "Weicai Li"
      },
      {
        "name" : "Tiejun Lv"
      },
      {
        "name" : "Xiyu Zhao"
      },
      {
        "name" : "Xin Yuan"
      },
      {
        "name" : "Wei Ni"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02156v2",
    "title" : "Mitigating Data Poisoning Attacks to Local Differential Privacy",
    "summary" : "The distributed nature of local differential privacy (LDP) invites data\npoisoning attacks and poses unforeseen threats to the underlying LDP-supported\napplications. In this paper, we propose a comprehensive mitigation framework\nfor popular frequency estimation, which contains a suite of novel defenses,\nincluding malicious user detection, attack pattern recognition, and damaged\nutility recovery. In addition to existing attacks, we explore new adaptive\nadversarial activities for our mitigation design. For detection, we present a\nnew method to precisely identify bogus reports and thus LDP aggregation can be\nperformed over the ``clean'' data. When the attack behavior becomes stealthy\nand direct filtering out malicious users is difficult, we further propose a\ndetection that can effectively recognize hidden adversarial patterns, thus\nfacilitating the decision-making of service providers. These detection methods\nrequire no additional data and attack information and incur minimal\ncomputational cost. Our experiment demonstrates their excellent performance and\nsubstantial improvement over previous work in various settings. In addition, we\nconduct an empirical analysis of LDP post-processing for corrupted data\nrecovery and propose a new post-processing method, through which we reveal new\ninsights into protocol recommendations in practice and key design principles\nfor future research.",
    "updated" : "2025-06-16T18:08:15Z",
    "published" : "2025-06-02T18:37:15Z",
    "authors" : [
      {
        "name" : "Xiaolin Li"
      },
      {
        "name" : "Ninghui Li"
      },
      {
        "name" : "Boyang Wang"
      },
      {
        "name" : "Wenhai Sun"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.17185v1",
    "title" : "A Common Pool of Privacy Problems: Legal and Technical Lessons from a\n  Large-Scale Web-Scraped Machine Learning Dataset",
    "summary" : "We investigate the contents of web-scraped data for training AI systems, at\nsizes where human dataset curators and compilers no longer manually annotate\nevery sample. Building off of prior privacy concerns in machine learning\nmodels, we ask: What are the legal privacy implications of web-scraped machine\nlearning datasets? In an empirical study of a popular training dataset, we find\nsignificant presence of personally identifiable information despite\nsanitization efforts. Our audit provides concrete evidence to support the\nconcern that any large-scale web-scraped dataset may contain personal data. We\nuse these findings of a real-world dataset to inform our legal analysis with\nrespect to existing privacy and data protection laws. We surface various\nprivacy risks of current data curation practices that may propagate personal\ninformation to downstream models. From our findings, we argue for reorientation\nof current frameworks of \"publicly available\" information to meaningfully limit\nthe development of AI built upon indiscriminate scraping of the internet.",
    "updated" : "2025-06-20T17:40:05Z",
    "published" : "2025-06-20T17:40:05Z",
    "authors" : [
      {
        "name" : "Rachel Hong"
      },
      {
        "name" : "Jevan Hutson"
      },
      {
        "name" : "William Agnew"
      },
      {
        "name" : "Imaad Huda"
      },
      {
        "name" : "Tadayoshi Kohno"
      },
      {
        "name" : "Jamie Morgenstern"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.17012v1",
    "title" : "A Novel Approach to Differential Privacy with Alpha Divergence",
    "summary" : "As data-driven technologies advance swiftly, maintaining strong privacy\nmeasures becomes progressively difficult. Conventional $(\\epsilon,\n\\delta)$-differential privacy, while prevalent, exhibits limited adaptability\nfor many applications. To mitigate these constraints, we present alpha\ndifferential privacy (ADP), an innovative privacy framework grounded in alpha\ndivergence, which provides a more flexible assessment of privacy consumption.\nThis study delineates the theoretical underpinnings of ADP and contrasts its\nperformance with competing privacy frameworks across many scenarios. Empirical\nassessments demonstrate that ADP offers enhanced privacy guarantees in small to\nmoderate iteration contexts, particularly where severe privacy requirements are\nnecessary. The suggested method markedly improves privacy-preserving methods,\nproviding a flexible solution for contemporary data analysis issues in a\ndata-centric environment.",
    "updated" : "2025-06-20T14:10:18Z",
    "published" : "2025-06-20T14:10:18Z",
    "authors" : [
      {
        "name" : "Yifeng Liu"
      },
      {
        "name" : "Zehua Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.16578v1",
    "title" : "SafeTriage: Facial Video De-identification for Privacy-Preserving Stroke\n  Triage",
    "summary" : "Effective stroke triage in emergency settings often relies on clinicians'\nability to identify subtle abnormalities in facial muscle coordination. While\nrecent AI models have shown promise in detecting such patterns from patient\nfacial videos, their reliance on real patient data raises significant ethical\nand privacy challenges -- especially when training robust and generalizable\nmodels across institutions. To address these concerns, we propose SafeTriage, a\nnovel method designed to de-identify patient facial videos while preserving\nessential motion cues crucial for stroke diagnosis. SafeTriage leverages a\npretrained video motion transfer (VMT) model to map the motion characteristics\nof real patient faces onto synthetic identities. This approach retains\ndiagnostically relevant facial dynamics without revealing the patients'\nidentities. To mitigate the distribution shift between normal population\npre-training videos and patient population test videos, we introduce a\nconditional generative model for visual prompt tuning, which adapts the input\nspace of the VMT model to ensure accurate motion transfer without needing to\nfine-tune the VMT model backbone. Comprehensive evaluation, including\nquantitative metrics and clinical expert assessments, demonstrates that\nSafeTriage-produced synthetic videos effectively preserve stroke-relevant\nfacial patterns, enabling reliable AI-based triage. Our evaluations also show\nthat SafeTriage provides robust privacy protection while maintaining diagnostic\naccuracy, offering a secure and ethically sound foundation for data sharing and\nAI-driven clinical analysis in neurological disorders.",
    "updated" : "2025-06-19T20:02:47Z",
    "published" : "2025-06-19T20:02:47Z",
    "authors" : [
      {
        "name" : "Tongan Cai"
      },
      {
        "name" : "Haomiao Ni"
      },
      {
        "name" : "Wenchao Ma"
      },
      {
        "name" : "Yuan Xue"
      },
      {
        "name" : "Qian Ma"
      },
      {
        "name" : "Rachel Leicht"
      },
      {
        "name" : "Kelvin Wong"
      },
      {
        "name" : "John Volpi"
      },
      {
        "name" : "Stephen T. C. Wong"
      },
      {
        "name" : "James Z. Wang"
      },
      {
        "name" : "Sharon X. Huang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.16460v1",
    "title" : "Black-Box Privacy Attacks on Shared Representations in Multitask\n  Learning",
    "summary" : "Multitask learning (MTL) has emerged as a powerful paradigm that leverages\nsimilarities among multiple learning tasks, each with insufficient samples to\ntrain a standalone model, to solve them simultaneously while minimizing data\nsharing across users and organizations. MTL typically accomplishes this goal by\nlearning a shared representation that captures common structure among the tasks\nby embedding data from all tasks into a common feature space. Despite being\ndesigned to be the smallest unit of shared information necessary to effectively\nlearn patterns across multiple tasks, these shared representations can\ninadvertently leak sensitive information about the particular tasks they were\ntrained on.\n  In this work, we investigate what information is revealed by the shared\nrepresentations through the lens of inference attacks. Towards this, we propose\na novel, black-box task-inference threat model where the adversary, given the\nembedding vectors produced by querying the shared representation on samples\nfrom a particular task, aims to determine whether that task was present when\ntraining the shared representation. We develop efficient, purely black-box\nattacks on machine learning models that exploit the dependencies between\nembeddings from the same task without requiring shadow models or labeled\nreference data. We evaluate our attacks across vision and language domains for\nmultiple use cases of MTL and demonstrate that even with access only to fresh\ntask samples rather than training data, a black-box adversary can successfully\ninfer a task's inclusion in training. To complement our experiments, we provide\ntheoretical analysis of a simplified learning setting and show a strict\nseparation between adversaries with training samples and fresh samples from the\ntarget task's distribution.",
    "updated" : "2025-06-19T16:56:41Z",
    "published" : "2025-06-19T16:56:41Z",
    "authors" : [
      {
        "name" : "John Abascal"
      },
      {
        "name" : "Nicolás Berrios"
      },
      {
        "name" : "Alina Oprea"
      },
      {
        "name" : "Jonathan Ullman"
      },
      {
        "name" : "Adam Smith"
      },
      {
        "name" : "Matthew Jagielski"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.16347v1",
    "title" : "Emission Impossible: privacy-preserving carbon emissions claims",
    "summary" : "Information and Communication Technologies (ICT) have a significant climate\nimpact, and data centres account for a large proportion of the carbon emissions\nfrom ICT. To achieve sustainability goals, it is important that all parties\ninvolved in ICT supply chains can track and share accurate carbon emissions\ndata with their customers, investors, and the authorities. However, businesses\nhave strong incentives to make their numbers look good, whilst less so to\npublish their accounting methods along with all the input data, due to the risk\nof revealing sensitive information. It would be uneconomical to use a trusted\nthird party to verify the data for every report for each party in the chain. As\na result, carbon emissions reporting in supply chains currently relies on\nunverified data. This paper proposes a methodology that applies cryptography\nand zero-knowledge proofs for carbon emissions claims that can be subsequently\nverified without the knowledge of the private input data. The proposed system\nis based on a zero-knowledge Succinct Non-interactive ARguments of Knowledge\n(zk-SNARK) protocol, which enables verifiable emissions reporting mechanisms\nacross a chain of energy suppliers, cloud data centres, cloud services\nproviders, and customers, without any company needing to disclose commercially\nsensitive information. This allows customers of cloud services to accurately\naccount for the emissions generated by their activities, improving data quality\nfor their own regulatory reporting. Cloud services providers would also be held\naccountable for producing accurate carbon emissions data.",
    "updated" : "2025-06-19T14:23:48Z",
    "published" : "2025-06-19T14:23:48Z",
    "authors" : [
      {
        "name" : "Jessica Man"
      },
      {
        "name" : "Sadiq Jaffer"
      },
      {
        "name" : "Patrick Ferris"
      },
      {
        "name" : "Martin Kleppmann"
      },
      {
        "name" : "Anil Madhavapeddy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.16196v1",
    "title" : "Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs",
    "summary" : "Prompting has become a dominant paradigm for adapting large language models\n(LLMs). While discrete (textual) prompts are widely used for their\ninterpretability, soft (parameter) prompts have recently gained traction in\nAPIs. This is because they can encode information from more training samples\nwhile minimizing the user's token usage, leaving more space in the context\nwindow for task-specific input. However, soft prompts are tightly coupled to\nthe LLM they are tuned on, limiting their generalization to other LLMs. This\nconstraint is particularly problematic for efficiency and privacy: (1) tuning\nprompts on each LLM incurs high computational costs, especially as LLMs\ncontinue to grow in size. Additionally, (2) when the LLM is hosted externally,\nsoft prompt tuning often requires sharing private data with the LLM provider.\nFor instance, this is the case with the NVIDIA NeMo API. To address these\nissues, we propose POST (Privacy Of Soft prompt Transfer), a framework that\nenables private tuning of soft prompts on a small model and subsequently\ntransfers these prompts to a larger LLM. POST uses knowledge distillation to\nderive a small model directly from the large LLM to improve prompt\ntransferability, tunes the soft prompt locally, optionally with differential\nprivacy guarantees, and transfers it back to the larger LLM using a small\npublic dataset. Our experiments show that POST reduces computational costs,\npreserves privacy, and effectively transfers high-utility soft prompts.",
    "updated" : "2025-06-19T10:25:16Z",
    "published" : "2025-06-19T10:25:16Z",
    "authors" : [
      {
        "name" : "Xun Wang"
      },
      {
        "name" : "Jing Xu"
      },
      {
        "name" : "Franziska Boenisch"
      },
      {
        "name" : "Michael Backes"
      },
      {
        "name" : "Christopher A. Choquette-Choo"
      },
      {
        "name" : "Adam Dziedzic"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15924v1",
    "title" : "FARFETCH'D: A Side-Channel Analysis Framework for Privacy Applications\n  on Confidential Virtual Machines",
    "summary" : "Confidential virtual machines (CVMs) based on trusted execution environments\n(TEEs) enable new privacy-preserving solutions. Yet, they leave side-channel\nleakage outside their threat model, shifting the responsibility of mitigating\nsuch attacks to developers. However, mitigations are either not generic or too\nslow for practical use, and developers currently lack a systematic, efficient\nway to measure and compare leakage across real-world deployments. In this\npaper, we present FARFETCH'D, an open-source toolkit that offers configurable\nside-channel tracing primitives on production AMD SEV-SNP hardware and couples\nthem with statistical and machine-learning-based analysis pipelines for\nautomated leakage estimation. We apply FARFETCH'D to three representative\nworkloads that are deployed on CVMs to enhance user privacy - private\ninformation retrieval, private heavy hitters, and Wasm user-defined functions -\nand uncover previously unnoticed leaks, including a covert channel that\nexfiltrated data at 497 kbit/s. The results show that FARFETCH'D pinpoints\nvulnerabilities and guides low-overhead mitigations based on oblivious memory\nand differential privacy, giving practitioners a practical path to deploy CVMs\nwith meaningful confidentiality guarantees.",
    "updated" : "2025-06-18T23:58:29Z",
    "published" : "2025-06-18T23:58:29Z",
    "authors" : [
      {
        "name" : "Ruiyi Zhang"
      },
      {
        "name" : "Albert Cheu"
      },
      {
        "name" : "Adria Gascon"
      },
      {
        "name" : "Daniel Moghimi"
      },
      {
        "name" : "Phillipp Schoppmann"
      },
      {
        "name" : "Michael Schwarz"
      },
      {
        "name" : "Octavian Suciu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15854v1",
    "title" : "Privacy-Preserving in Connected and Autonomous Vehicles Through Vision\n  to Text Transformation",
    "summary" : "Connected and Autonomous Vehicles (CAVs) rely on a range of devices that\noften process privacy-sensitive data. Among these, roadside units play a\ncritical role particularly through the use of AI-equipped (AIE) cameras for\napplications such as violation detection. However, the privacy risks associated\nwith captured imagery remain a major concern, as such data can be misused for\nidentity theft, profiling, or unauthorized commercial purposes. While\ntraditional techniques such as face blurring and obfuscation have been applied\nto mitigate privacy risks, individual privacy remains at risk, as individuals\ncan still be tracked using other features such as their clothing. This paper\nintroduces a novel privacy-preserving framework that leverages feedback-based\nreinforcement learning (RL) and vision-language models (VLMs) to protect\nsensitive visual information captured by AIE cameras. The main idea is to\nconvert images into semantically equivalent textual descriptions, ensuring that\nscene-relevant information is retained while visual privacy is preserved. A\nhierarchical RL strategy is employed to iteratively refine the generated text,\nenhancing both semantic accuracy and privacy. Evaluation results demonstrate\nsignificant improvements in both privacy protection and textual quality, with\nthe Unique Word Count increasing by approximately 77\\% and Detail Density by\naround 50\\% compared to existing approaches.",
    "updated" : "2025-06-18T20:02:24Z",
    "published" : "2025-06-18T20:02:24Z",
    "authors" : [
      {
        "name" : "Abdolazim Rezaei"
      },
      {
        "name" : "Mehdi Sookhak"
      },
      {
        "name" : "Ahmad Patooghy"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15258v2",
    "title" : "Privacy-Preserving Chest X-ray Classification in Latent Space with\n  Homomorphically Encrypted Neural Inference",
    "summary" : "Medical imaging data contain sensitive patient information requiring strong\nprivacy protection. Many analytical setups require data to be sent to a server\nfor inference purposes. Homomorphic encryption (HE) provides a solution by\nallowing computations to be performed on encrypted data without revealing the\noriginal information. However, HE inference is computationally expensive,\nparticularly for large images (e.g., chest X-rays). In this study, we propose\nan HE inference framework for medical images that uses VQGAN to compress images\ninto latent representations, thereby significantly reducing the computational\nburden while preserving image quality. We approximate the activation functions\nwith lower-degree polynomials to balance the accuracy and efficiency in\ncompliance with HE requirements. We observed that a downsampling factor of\neight for compression achieved an optimal balance between performance and\ncomputational cost. We further adapted the squeeze and excitation module, which\nis known to improve traditional CNNs, to enhance the HE framework. Our method\nwas tested on two chest X-ray datasets for multi-label classification tasks\nusing vanilla CNN backbones. Although HE inference remains relatively slow and\nintroduces minor performance differences compared with unencrypted inference,\nour approach shows strong potential for practical use in medical images",
    "updated" : "2025-06-20T03:57:49Z",
    "published" : "2025-06-18T08:35:50Z",
    "authors" : [
      {
        "name" : "Jonghun Kim"
      },
      {
        "name" : "Gyeongdeok Jo"
      },
      {
        "name" : "Sinyoung Ra"
      },
      {
        "name" : "Hyunjin Park"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15106v2",
    "title" : "Local Differential Privacy for Distributed Stochastic Aggregative\n  Optimization with Guaranteed Optimality",
    "summary" : "Distributed aggregative optimization underpins many cooperative optimization\nand multi-agent control systems, where each agent's objective function depends\nboth on its local optimization variable and an aggregate of all agents'\noptimization variables. Existing distributed aggregative optimization\napproaches typically require access to accurate gradients of the objective\nfunctions, which, however, are often hard to obtain in real-world applications.\nFor example, in machine learning, gradients are commonly contaminated by two\nmain sources of noise: the randomness inherent in sampled data, and the\nadditional variability introduced by mini-batch computations. In addition to\nthe issue of relying on accurate gradients, existing distributed aggregative\noptimization approaches require agents to share explicit information, which\ncould breach the privacy of participating agents. We propose an algorithm that\ncan solve both problems with existing distributed aggregative optimization\napproaches: not only can the proposed algorithm guarantee mean-square\nconvergence to an exact optimal solution when the gradients are subject to\nnoise, it also simultaneously ensures rigorous differential privacy, with the\ncumulative privacy budget guaranteed to be finite even when the number of\niterations tends to infinity. To the best of our knowledge, this is the first\nalgorithm able to guarantee both accurate convergence and rigorous differential\nprivacy in distributed aggregative optimization. Besides characterizing the\nconvergence rates under nonconvex/convex/strongly convex conditions, we also\nrigorously quantify the cost of differential privacy in terms of convergence\nrates. Experimental results on personalized machine learning using benchmark\ndatasets confirm the efficacy of the proposed algorithm.",
    "updated" : "2025-06-19T19:45:13Z",
    "published" : "2025-06-18T03:22:35Z",
    "authors" : [
      {
        "name" : "Ziqin Chen"
      },
      {
        "name" : "Yongqiang Wang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.12699v2",
    "title" : "SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy\n  Risks, and Mitigation",
    "summary" : "Large language models (LLMs) are sophisticated artificial intelligence\nsystems that enable machines to generate human-like text with remarkable\nprecision. While LLMs offer significant technological progress, their\ndevelopment using vast amounts of user data scraped from the web and collected\nfrom extensive user interactions poses risks of sensitive information leakage.\nMost existing surveys focus on the privacy implications of the training data\nbut tend to overlook privacy risks from user interactions and advanced LLM\ncapabilities. This paper aims to fill that gap by providing a comprehensive\nanalysis of privacy in LLMs, categorizing the challenges into four main areas:\n(i) privacy issues in LLM training data, (ii) privacy challenges associated\nwith user prompts, (iii) privacy vulnerabilities in LLM-generated outputs, and\n(iv) privacy challenges involving LLM agents. We evaluate the effectiveness and\nlimitations of existing mitigation mechanisms targeting these proposed privacy\nchallenges and identify areas for further research.",
    "updated" : "2025-06-19T06:30:24Z",
    "published" : "2025-06-15T03:14:03Z",
    "authors" : [
      {
        "name" : "Yashothara Shanmugarasa"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "M. A. P Chamikara"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.17336v1",
    "title" : "Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought\n  Reasoning and Homomorphically Encrypted Vector Databases",
    "summary" : "Large language models (LLMs) are increasingly used as personal agents,\naccessing sensitive user data such as calendars, emails, and medical records.\nUsers currently face a trade-off: They can send private records, many of which\nare stored in remote databases, to powerful but untrusted LLM providers,\nincreasing their exposure risk. Alternatively, they can run less powerful\nmodels locally on trusted devices. We bridge this gap. Our Socratic\nChain-of-Thought Reasoning first sends a generic, non-private user query to a\npowerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and\ndetailed sub-queries without accessing user data. Next, we embed these\nsub-queries and perform encrypted sub-second semantic search using our\nHomomorphically Encrypted Vector Database across one million entries of a\nsingle user's private data. This represents a realistic scale of personal\ndocuments, emails, and records accumulated over years of digital activity.\nFinally, we feed the CoT prompt and the decrypted records to a local language\nmodel and generate the final response. On the LoCoMo long-context QA benchmark,\nour hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,\noutperforms using GPT-4o alone by up to 7.1 percentage points. This\ndemonstrates a first step toward systems where tasks are decomposed and split\nbetween untrusted strong LLMs and weak local ones, preserving user privacy.",
    "updated" : "2025-06-19T07:13:30Z",
    "published" : "2025-06-19T07:13:30Z",
    "authors" : [
      {
        "name" : "Yubeen Bae"
      },
      {
        "name" : "Minchan Kim"
      },
      {
        "name" : "Jaejin Lee"
      },
      {
        "name" : "Sangbum Kim"
      },
      {
        "name" : "Jaehyung Kim"
      },
      {
        "name" : "Yejin Choi"
      },
      {
        "name" : "Niloofar Mireshghallah"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.17332v1",
    "title" : "P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for\n  Elderly People in Bathroom Environments",
    "summary" : "By 2050, people aged 65 and over are projected to make up 16 percent of the\nglobal population. As aging is closely associated with increased fall risk,\nparticularly in wet and confined environments such as bathrooms where over 80\npercent of falls occur. Although recent research has increasingly focused on\nnon-intrusive, privacy-preserving approaches that do not rely on wearable\ndevices or video-based monitoring, these efforts have not fully overcome the\nlimitations of existing unimodal systems (e.g., WiFi-, infrared-, or\nmmWave-based), which are prone to reduced accuracy in complex environments.\nThese limitations stem from fundamental constraints in unimodal sensing,\nincluding system bias and environmental interference, such as multipath fading\nin WiFi-based systems and drastic temperature changes in infrared-based\nmethods. To address these challenges, we propose a Privacy-Preserving\nMultimodal Fall Detection System for Elderly People in Bathroom Environments.\nFirst, we develop a sensor evaluation framework to select and fuse\nmillimeter-wave radar with 3D vibration sensing, and use it to construct and\npreprocess a large-scale, privacy-preserving multimodal dataset in real\nbathroom settings, which will be released upon publication. Second, we\nintroduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch\nfor radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch\nfor vibration impact detection. By uniting macro- and micro-scale features,\nP2MFDS delivers significant gains in accuracy and recall over state-of-the-art\napproaches. Code and pretrained models will be made available at:\nhttps://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.",
    "updated" : "2025-06-19T05:22:14Z",
    "published" : "2025-06-19T05:22:14Z",
    "authors" : [
      {
        "name" : "Haitian Wang"
      },
      {
        "name" : "Yiren Wang"
      },
      {
        "name" : "Xinyu Wang"
      },
      {
        "name" : "Yumeng Miao"
      },
      {
        "name" : "Yuliang Zhang"
      },
      {
        "name" : "Yu Zhang"
      },
      {
        "name" : "Atif Mansoor"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.13052v2",
    "title" : "Buy it Now, Track Me Later: Attacking User Privacy via Wi-Fi AP Online\n  Auctions",
    "summary" : "Static and hard-coded layer-two network identifiers are well known to present\nsecurity vulnerabilities and endanger user privacy. In this work, we introduce\na new privacy attack against Wi-Fi access points listed on secondhand\nmarketplaces. Specifically, we demonstrate the ability to remotely gather a\nlarge quantity of layer-two Wi-Fi identifiers by programmatically querying the\neBay marketplace and applying state-of-the-art computer vision techniques to\nextract IEEE 802.11 BSSIDs from the seller's posted images of the hardware. By\nleveraging data from a global Wi-Fi Positioning System (WPS) that geolocates\nBSSIDs, we obtain the physical locations of these devices both pre- and\npost-sale. In addition to validating the degree to which a seller's location\nmatches the location of the device, we examine cases of device movement -- once\nthe device is sold and then subsequently re-used in a new environment. Our work\nhighlights a previously unrecognized privacy vulnerability and suggests, yet\nagain, the strong need to protect layer-two network identifiers.",
    "updated" : "2025-06-22T02:48:01Z",
    "published" : "2025-06-16T02:42:14Z",
    "authors" : [
      {
        "name" : "Steven Su"
      },
      {
        "name" : "Erik Rye"
      },
      {
        "name" : "Dave Levin"
      },
      {
        "name" : "Robert Beverly"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.17269v1",
    "title" : "Digital Privacy Everywhere",
    "summary" : "The increasing proliferation of digital and mobile devices equipped with\ncameras, microphones, GPS, and other privacy invasive components has raised\nsignificant concerns for businesses operating in sensitive or policy restricted\nenvironments. Current solutions rely on passive enforcement, such as signage or\nverbal instructions, which are largely ineffective. This paper presents Digital\nPrivacy Everywhere (DPE), a comprehensive and scalable system designed to\nactively enforce custom privacy policies for digital devices within predefined\nphysical boundaries. The DPE architecture includes a centralized management\nconsole, field verification units (FVUs), enforcement modules for mobile\ndevices (EMMDs), and an External Geo Ownership Service (EGOS). These components\ncollaboratively detect, configure, and enforce privacy settings such as\ndisabling cameras, microphones, or radios across various premises like\ntheaters, hospitals, financial institutions, and educational facilities. The\nsystem ensures privacy compliance in real time while maintaining a seamless\nuser experience and operational scalability across geographies.",
    "updated" : "2025-06-11T09:25:49Z",
    "published" : "2025-06-11T09:25:49Z",
    "authors" : [
      {
        "name" : "Paritosh Ranjan"
      },
      {
        "name" : "Surajit Majumder"
      },
      {
        "name" : "Prodip Roy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.19836v1",
    "title" : "Machine Learning with Privacy for Protected Attributes",
    "summary" : "Differential privacy (DP) has become the standard for private data analysis.\nCertain machine learning applications only require privacy protection for\nspecific protected attributes. Using naive variants of differential privacy in\nsuch use cases can result in unnecessary degradation of utility. In this work,\nwe refine the definition of DP to create a more general and flexible framework\nthat we call feature differential privacy (FDP). Our definition is\nsimulation-based and allows for both addition/removal and replacement variants\nof privacy, and can handle arbitrary and adaptive separation of protected and\nnon-protected features. We prove the properties of FDP, such as adaptive\ncomposition, and demonstrate its implications for limiting attribute inference\nattacks. We also propose a modification of the standard DP-SGD algorithm that\nsatisfies FDP while leveraging desirable properties such as amplification via\nsub-sampling. We apply our framework to various machine learning tasks and show\nthat it can significantly improve the utility of DP-trained models when public\nfeatures are available. For example, we train diffusion models on the AFHQ\ndataset of animal faces and observe a drastic improvement in FID compared to\nDP, from 286.7 to 101.9 at $\\epsilon=8$, assuming that the blurred version of a\ntraining image is available as a public feature. Overall, our work provides a\nnew approach to private data analysis that can help reduce the utility cost of\nDP while still providing strong privacy guarantees.",
    "updated" : "2025-06-24T17:53:28Z",
    "published" : "2025-06-24T17:53:28Z",
    "authors" : [
      {
        "name" : "Saeed Mahloujifar"
      },
      {
        "name" : "Chuan Guo"
      },
      {
        "name" : "G. Edward Suh"
      },
      {
        "name" : "Kamalika Chaudhuri"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.19563v1",
    "title" : "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic\n  Consistency and Probability Certainty",
    "summary" : "Large Language Models (LLMs) are widely used in sensitive domains, including\nhealthcare, finance, and legal services, raising concerns about potential\nprivate information leaks during inference. Privacy extraction attacks, such as\njailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the\nmodels to output sensitive information. However, these attacks cannot verify\nwhether the extracted private information is accurate, as no public datasets\nexist for cross-validation, leaving a critical gap in private information\ndetection during inference. To address this, we propose PrivacyXray, a novel\nframework detecting privacy breaches by analyzing LLM inner states. Our\nanalysis reveals that LLMs exhibit higher semantic coherence and probabilistic\ncertainty when generating correct private outputs. Based on this, PrivacyXray\ndetects privacy breaches using four metrics: intra-layer and inter-layer\nsemantic similarity, token-level and sentence-level probability distributions.\nPrivacyXray addresses critical challenges in private information detection by\novercoming the lack of open-source private datasets and eliminating reliance on\nexternal data for validation. It achieves this through the synthesis of\nrealistic private data and a detection mechanism based on the inner states of\nLLMs. Experiments show that PrivacyXray achieves consistent performance, with\nan average accuracy of 92.69% across five LLMs. Compared to state-of-the-art\nmethods, PrivacyXray achieves significant improvements, with an average\naccuracy increase of 20.06%, highlighting its stability and practical utility\nin real-world applications.",
    "updated" : "2025-06-24T12:22:59Z",
    "published" : "2025-06-24T12:22:59Z",
    "authors" : [
      {
        "name" : "Jinwen He"
      },
      {
        "name" : "Yiyang Lu"
      },
      {
        "name" : "Zijin Lin"
      },
      {
        "name" : "Kai Chen"
      },
      {
        "name" : "Yue Zhao"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.19486v1",
    "title" : "Recalling The Forgotten Class Memberships: Unlearned Models Can Be Noisy\n  Labelers to Leak Privacy",
    "summary" : "Machine Unlearning (MU) technology facilitates the removal of the influence\nof specific data instances from trained models on request. Despite rapid\nadvancements in MU technology, its vulnerabilities are still underexplored,\nposing potential risks of privacy breaches through leaks of ostensibly\nunlearned information. Current limited research on MU attacks requires access\nto original models containing privacy data, which violates the critical\nprivacy-preserving objective of MU. To address this gap, we initiate an\ninnovative study on recalling the forgotten class memberships from unlearned\nmodels (ULMs) without requiring access to the original one. Specifically, we\nimplement a Membership Recall Attack (MRA) framework with a teacher-student\nknowledge distillation architecture, where ULMs serve as noisy labelers to\ntransfer knowledge to student models. Then, it is translated into a Learning\nwith Noisy Labels (LNL) problem for inferring the correct labels of the\nforgetting instances. Extensive experiments on state-of-the-art MU methods with\nmultiple real datasets demonstrate that the proposed MRA strategy exhibits high\nefficacy in recovering class memberships of unlearned instances. As a result,\nour study and evaluation have established a benchmark for future research on MU\nvulnerabilities.",
    "updated" : "2025-06-24T10:21:10Z",
    "published" : "2025-06-24T10:21:10Z",
    "authors" : [
      {
        "name" : "Zhihao Sui"
      },
      {
        "name" : "Liang Hu"
      },
      {
        "name" : "Jian Cao"
      },
      {
        "name" : "Dora D. Liu"
      },
      {
        "name" : "Usman Naseem"
      },
      {
        "name" : "Zhongyuan Lai"
      },
      {
        "name" : "Qi Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.19393v1",
    "title" : "ZK-SERIES: Privacy-Preserving Authentication using Temporal Biometric\n  Data",
    "summary" : "Biometric authentication relies on physiological or behavioral traits that\nare inherent to a user, making them difficult to lose, forge or forget.\nBiometric data with a temporal component enable the following authentication\nprotocol: recent readings of the underlying biometrics are encoded as time\nseries and compared to a set of base readings. If the distance between the new\nreadings and the base readings falls within an acceptable threshold, then the\nuser is successfully authenticated. Various methods exist for comparing time\nseries data, such as Dynamic Time Warping (DTW) and the Time Warp Edit Distance\n(TWED), each offering advantages and drawbacks depending on the context.\nMoreover, many of these techniques do not inherently preserve privacy, which is\na critical consideration in biometric authentication due to the complexity of\nresetting biometric credentials.\n  In this work, we propose ZK-SERIES to provide privacy and efficiency to a\nbroad spectrum of time series-based authentication protocols. ZK-SERIES uses\nthe same building blocks, i.e., zero-knowledge multiplication proofs and\nefficiently batched range proofs, to ensure consistency across all protocols.\nFurthermore, it is optimized for compatibility with low-capacity devices such\nas smartphones. To assess the effectiveness of our proposed technique, we\nprimarily focus on two case studies for biometric authentication: shake-based\nand blow-based authentication. To demonstrate ZK-SERIES's practical\napplicability even in older and less powerful smartphones, we conduct\nexperiments on a 5-year-old low-spec smartphone using real data for two case\nstudies alongside scalability assessments using artificial data. Our\nexperimental results indicate that the privacy-preserving authentication\nprotocol can be completed within 1.3 seconds on older devices.",
    "updated" : "2025-06-24T07:45:43Z",
    "published" : "2025-06-24T07:45:43Z",
    "authors" : [
      {
        "name" : "Daniel Reijsbergen"
      },
      {
        "name" : "Eyasu Getahun Chekole"
      },
      {
        "name" : "Howard Halim"
      },
      {
        "name" : "Jianying Zhou"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.19360v1",
    "title" : "SoK: Can Synthetic Images Replace Real Data? A Survey of Utility and\n  Privacy of Synthetic Image Generation",
    "summary" : "Advances in generative models have transformed the field of synthetic image\ngeneration for privacy-preserving data synthesis (PPDS). However, the field\nlacks a comprehensive survey and comparison of synthetic image generation\nmethods across diverse settings. In particular, when we generate synthetic\nimages for the purpose of training a classifier, there is a pipeline of\ngeneration-sampling-classification which takes private training as input and\noutputs the final classifier of interest. In this survey, we systematically\ncategorize existing image synthesis methods, privacy attacks, and mitigations\nalong this generation-sampling-classification pipeline. To empirically compare\ndiverse synthesis approaches, we provide a benchmark with representative\ngenerative methods and use model-agnostic membership inference attacks (MIAs)\nas a measure of privacy risk. Through this study, we seek to answer critical\nquestions in PPDS: Can synthetic data effectively replace real data? Which\nrelease strategy balances utility and privacy? Do mitigations improve the\nutility-privacy tradeoff? Which generative models perform best across different\nscenarios? With a systematic evaluation of diverse methods, our study provides\nactionable insights into the utility-privacy tradeoffs of synthetic data\ngeneration methods and guides the decision on optimal data releasing strategies\nfor real-world applications.",
    "updated" : "2025-06-24T06:41:34Z",
    "published" : "2025-06-24T06:41:34Z",
    "authors" : [
      {
        "name" : "Yunsung Chung"
      },
      {
        "name" : "Yunbei Zhang"
      },
      {
        "name" : "Nassir Marrouche"
      },
      {
        "name" : "Jihun Hamm"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.19268v1",
    "title" : "HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in\n  Mobile Health Apps",
    "summary" : "We present HARPT, a large-scale annotated corpus of mobile health app store\nreviews aimed at advancing research in user privacy and trust. The dataset\ncomprises over 480,000 user reviews labeled into seven categories that capture\ncritical aspects of trust in applications, trust in providers and privacy\nconcerns. Creating HARPT required addressing multiple complexities, such as\ndefining a nuanced label schema, isolating relevant content from large volumes\nof noisy data, and designing an annotation strategy that balanced scalability\nwith accuracy. This strategy integrated rule-based filtering, iterative manual\nlabeling with review, targeted data augmentation, and weak supervision using\ntransformer-based classifiers to accelerate coverage. In parallel, a carefully\ncurated subset of 7,000 reviews was manually annotated to support model\ndevelopment and evaluation. We benchmark a broad range of classification\nmodels, demonstrating that strong performance is achievable and providing a\nbaseline for future research. HARPT is released as a public resource to support\nwork in health informatics, cybersecurity, and natural language processing.",
    "updated" : "2025-06-24T02:59:14Z",
    "published" : "2025-06-24T02:59:14Z",
    "authors" : [
      {
        "name" : "Timoteo Kelly"
      },
      {
        "name" : "Abdulkadir Korkmaz"
      },
      {
        "name" : "Samuel Mallet"
      },
      {
        "name" : "Connor Souders"
      },
      {
        "name" : "Sadra Aliakbarpour"
      },
      {
        "name" : "Praveen Rao"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.ET",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.19260v1",
    "title" : "Network Structures as an Attack Surface: Topology-Based Privacy Leakage\n  in Federated Learning",
    "summary" : "Federated learning systems increasingly rely on diverse network topologies to\naddress scalability and organizational constraints. While existing privacy\nresearch focuses on gradient-based attacks, the privacy implications of network\ntopology knowledge remain critically understudied. We conduct the first\ncomprehensive analysis of topology-based privacy leakage across realistic\nadversarial knowledge scenarios, demonstrating that adversaries with varying\ndegrees of structural knowledge can infer sensitive data distribution patterns\neven under strong differential privacy guarantees. Through systematic\nevaluation of 4,720 attack instances, we analyze six distinct adversarial\nknowledge scenarios: complete topology knowledge and five partial knowledge\nconfigurations reflecting real-world deployment constraints. We propose three\ncomplementary attack vectors: communication pattern analysis, parameter\nmagnitude profiling, and structural position correlation, achieving success\nrates of 84.1%, 65.0%, and 47.2% under complete knowledge conditions.\nCritically, we find that 80% of realistic partial knowledge scenarios maintain\nattack effectiveness above security thresholds, with certain partial knowledge\nconfigurations achieving performance superior to the baseline complete\nknowledge scenario. To address these vulnerabilities, we propose and\nempirically validate structural noise injection as a complementary defense\nmechanism across 808 configurations, demonstrating up to 51.4% additional\nattack reduction when properly layered with existing privacy techniques. These\nresults establish that network topology represents a fundamental privacy\nvulnerability in federated learning systems while providing practical pathways\nfor mitigation through topology-aware defense mechanisms.",
    "updated" : "2025-06-24T02:42:08Z",
    "published" : "2025-06-24T02:42:08Z",
    "authors" : [
      {
        "name" : "Murtaza Rangwala"
      },
      {
        "name" : "Richard O. Sinnott"
      },
      {
        "name" : "Rajkumar Buyya"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG",
      "I.2.6; C.2.4; K.6.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.17336v1",
    "title" : "Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought\n  Reasoning and Homomorphically Encrypted Vector Databases",
    "summary" : "Large language models (LLMs) are increasingly used as personal agents,\naccessing sensitive user data such as calendars, emails, and medical records.\nUsers currently face a trade-off: They can send private records, many of which\nare stored in remote databases, to powerful but untrusted LLM providers,\nincreasing their exposure risk. Alternatively, they can run less powerful\nmodels locally on trusted devices. We bridge this gap. Our Socratic\nChain-of-Thought Reasoning first sends a generic, non-private user query to a\npowerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and\ndetailed sub-queries without accessing user data. Next, we embed these\nsub-queries and perform encrypted sub-second semantic search using our\nHomomorphically Encrypted Vector Database across one million entries of a\nsingle user's private data. This represents a realistic scale of personal\ndocuments, emails, and records accumulated over years of digital activity.\nFinally, we feed the CoT prompt and the decrypted records to a local language\nmodel and generate the final response. On the LoCoMo long-context QA benchmark,\nour hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,\noutperforms using GPT-4o alone by up to 7.1 percentage points. This\ndemonstrates a first step toward systems where tasks are decomposed and split\nbetween untrusted strong LLMs and weak local ones, preserving user privacy.",
    "updated" : "2025-06-19T07:13:30Z",
    "published" : "2025-06-19T07:13:30Z",
    "authors" : [
      {
        "name" : "Yubeen Bae"
      },
      {
        "name" : "Minchan Kim"
      },
      {
        "name" : "Jaejin Lee"
      },
      {
        "name" : "Sangbum Kim"
      },
      {
        "name" : "Jaehyung Kim"
      },
      {
        "name" : "Yejin Choi"
      },
      {
        "name" : "Niloofar Mireshghallah"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15201v2",
    "title" : "Privacy-Shielded Image Compression: Defending Against Exploitation from\n  Vision-Language Pretrained Models",
    "summary" : "The improved semantic understanding of vision-language pretrained (VLP)\nmodels has made it increasingly difficult to protect publicly posted images\nfrom being exploited by search engines and other similar tools. In this\ncontext, this paper seeks to protect users' privacy by implementing defenses at\nthe image compression stage to prevent exploitation. Specifically, we propose a\nflexible coding method, termed Privacy-Shielded Image Compression (PSIC), that\ncan produce bitstreams with multiple decoding options. By default, the\nbitstream is decoded to preserve satisfactory perceptual quality while\npreventing interpretation by VLP models. Our method also retains the original\nimage compression functionality. With a customizable input condition, the\nproposed scheme can reconstruct the image that preserves its full semantic\ninformation. A Conditional Latent Trigger Generation (CLTG) module is proposed\nto produce bias information based on customizable conditions to guide the\ndecoding process into different reconstructed versions, and an\nUncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed\nto leverage the soft labels inferred from the target VLP model's uncertainty on\nthe training data. This paper further incorporates an adaptive multi-objective\noptimization strategy to obtain improved encrypting performance and perceptual\nquality simultaneously within a unified training process. The proposed scheme\nis plug-and-play and can be seamlessly integrated into most existing Learned\nImage Compression (LIC) models. Extensive experiments across multiple\ndownstream tasks have demonstrated the effectiveness of our design.",
    "updated" : "2025-06-24T01:52:22Z",
    "published" : "2025-06-18T07:29:40Z",
    "authors" : [
      {
        "name" : "Xuelin Shen"
      },
      {
        "name" : "Jiayin Xu"
      },
      {
        "name" : "Kangsheng Yin"
      },
      {
        "name" : "Wenhan Yang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.20463v1",
    "title" : "Analyzing Security and Privacy Challenges in Generative AI Usage\n  Guidelines for Higher Education",
    "summary" : "Educators and learners worldwide are embracing the rise of Generative\nArtificial Intelligence (GenAI) as it reshapes higher education. However, GenAI\nalso raises significant privacy and security concerns, as models and\nprivacy-sensitive user data, such as student records, may be misused by service\nproviders. Unfortunately, end-users often have little awareness of or control\nover how these models operate. To address these concerns, universities are\ndeveloping institutional policies to guide GenAI use while safeguarding\nsecurity and privacy. This work examines these emerging policies and\nguidelines, with a particular focus on the often-overlooked privacy and\nsecurity dimensions of GenAI integration in higher education, alongside other\nacademic values. Through a qualitative analysis of GenAI usage guidelines from\nuniversities across 12 countries, we identify key challenges and opportunities\ninstitutions face in providing effective privacy and security protections,\nincluding the need for GenAI safeguards tailored specifically to the academic\ncontext.",
    "updated" : "2025-06-25T14:12:18Z",
    "published" : "2025-06-25T14:12:18Z",
    "authors" : [
      {
        "name" : "Bei Yi Ng"
      },
      {
        "name" : "Jiarui Li"
      },
      {
        "name" : "Xinyuan Tong"
      },
      {
        "name" : "Kevin Ye"
      },
      {
        "name" : "Gauthami Yenne"
      },
      {
        "name" : "Varun Chandrasekaran"
      },
      {
        "name" : "Jingjie Li"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.20413v1",
    "title" : "Client Clustering Meets Knowledge Sharing: Enhancing Privacy and\n  Robustness in Personalized Peer-to-Peer Learning",
    "summary" : "The growing adoption of Artificial Intelligence (AI) in Internet of Things\n(IoT) ecosystems has intensified the need for personalized learning methods\nthat can operate efficiently and privately across heterogeneous,\nresource-constrained devices. However, enabling effective personalized learning\nin decentralized settings introduces several challenges, including efficient\nknowledge transfer between clients, protection of data privacy, and resilience\nagainst poisoning attacks. In this paper, we address these challenges by\ndeveloping P4 (Personalized, Private, Peer-to-Peer) -- a method designed to\ndeliver personalized models for resource-constrained IoT devices while ensuring\ndifferential privacy and robustness against poisoning attacks. Our solution\nemploys a lightweight, fully decentralized algorithm to privately detect client\nsimilarity and form collaborative groups. Within each group, clients leverage\ndifferentially private knowledge distillation to co-train their models,\nmaintaining high accuracy while ensuring robustness to the presence of\nmalicious clients. We evaluate P4 on popular benchmark datasets using both\nlinear and CNN-based architectures across various heterogeneity settings and\nattack scenarios. Experimental results show that P4 achieves 5% to 30% higher\naccuracy than leading differentially private peer-to-peer approaches and\nmaintains robustness with up to 30% malicious clients. Additionally, we\ndemonstrate its practicality by deploying it on resource-constrained devices,\nwhere collaborative training between two clients adds only ~7 seconds of\noverhead.",
    "updated" : "2025-06-25T13:27:36Z",
    "published" : "2025-06-25T13:27:36Z",
    "authors" : [
      {
        "name" : "Mohammad Mahdi Maheri"
      },
      {
        "name" : "Denys Herasymuk"
      },
      {
        "name" : "Hamed Haddadi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.20290v1",
    "title" : "Don't Hash Me Like That: Exposing and Mitigating Hash-Induced Unfairness\n  in Local Differential Privacy",
    "summary" : "Local differential privacy (LDP) has become a widely accepted framework for\nprivacy-preserving data collection. In LDP, many protocols rely on hash\nfunctions to implement user-side encoding and perturbation. However, the\nsecurity and privacy implications of hash function selection have not been\npreviously investigated. In this paper, we expose that the hash functions may\nact as a source of unfairness in LDP protocols. We show that although users\noperate under the same protocol and privacy budget, differences in hash\nfunctions can lead to significant disparities in vulnerability to inference and\npoisoning attacks. To mitigate hash-induced unfairness, we propose Fair-OLH\n(F-OLH), a variant of OLH that enforces an entropy-based fairness constraint on\nhash function selection. Experiments show that F-OLH is effective in mitigating\nhash-induced unfairness under acceptable time overheads.",
    "updated" : "2025-06-25T09:48:30Z",
    "published" : "2025-06-25T09:48:30Z",
    "authors" : [
      {
        "name" : "Berkay Kemal Balioglu"
      },
      {
        "name" : "Alireza Khodaie"
      },
      {
        "name" : "Mehmet Emre Gursoy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.20234v1",
    "title" : "Communication-Efficient Publication of Sparse Vectors under Differential\n  Privacy",
    "summary" : "In this work, we propose a differentially private algorithm for publishing\nmatrices aggregated from sparse vectors. These matrices include social network\nadjacency matrices, user-item interaction matrices in recommendation systems,\nand single nucleotide polymorphisms (SNPs) in DNA data. Traditionally,\ndifferential privacy in vector collection relies on randomized response, but\nthis approach incurs high communication costs. Specifically, for a matrix with\n$N$ users, $n$ columns, and $m$ nonzero elements, conventional methods require\n$\\Omega(n \\times N)$ communication, making them impractical for large-scale\ndata. Our algorithm significantly reduces this cost to $O(\\varepsilon m)$,\nwhere $\\varepsilon$ is the privacy budget. Notably, this is even lower than the\nnon-private case, which requires $\\Omega(m \\log n)$ communication. Moreover, as\nthe privacy budget decreases, communication cost further reduces, enabling\nbetter privacy with improved efficiency. We theoretically prove that our method\nyields results identical to those of randomized response, and experimental\nevaluations confirm its effectiveness in terms of accuracy, communication\nefficiency, and computational complexity.",
    "updated" : "2025-06-25T08:25:46Z",
    "published" : "2025-06-25T08:25:46Z",
    "authors" : [
      {
        "name" : "Quentin Hillebrand"
      },
      {
        "name" : "Vorapong Suppakitpaisarn"
      },
      {
        "name" : "Tetsuo Shibuya"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.20207v1",
    "title" : "User Understanding of Privacy Permissions in Mobile Augmented Reality:\n  Perceptions and Misconceptions",
    "summary" : "Mobile Augmented Reality (AR) applications leverage various sensors to\nprovide immersive user experiences. However, their reliance on diverse data\nsources introduces significant privacy challenges. This paper investigates user\nperceptions and understanding of privacy permissions in mobile AR apps through\nan analysis of existing applications and an online survey of 120 participants.\nFindings reveal common misconceptions, including confusion about how\npermissions relate to specific AR functionalities (e.g., location and\nmeasurement of physical distances), and misinterpretations of permission labels\n(e.g., conflating camera and gallery access). We identify a set of actionable\nimplications for designing more usable and transparent privacy mechanisms\ntailored to mobile AR technologies, including contextual explanations, modular\npermission requests, and clearer permission labels. These findings offer\nactionable guidance for developers, researchers, and policymakers working to\nenhance privacy frameworks in mobile AR.",
    "updated" : "2025-06-25T07:52:12Z",
    "published" : "2025-06-25T07:52:12Z",
    "authors" : [
      {
        "name" : "Viktorija Paneva"
      },
      {
        "name" : "Verena Winterhalter"
      },
      {
        "name" : "Franziska Augustinowski"
      },
      {
        "name" : "Florian Alt"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.20101v1",
    "title" : "Secure Multi-Key Homomorphic Encryption with Application to\n  Privacy-Preserving Federated Learning",
    "summary" : "Multi-Key Homomorphic Encryption (MKHE), proposed by Lopez-Alt et al. (STOC\n2012), allows for performing arithmetic computations directly on ciphertexts\nencrypted under distinct keys. Subsequent works by Chen and Dai et al. (CCS\n2019) and Kim and Song et al. (CCS 2023) extended this concept by proposing\nmulti-key BFV/CKKS variants, referred to as the CDKS scheme. These variants\nincorporate asymptotically optimal techniques to facilitate secure computation\nacross multiple data providers. In this paper, we identify a critical security\nvulnerability in the CDKS scheme when applied to multiparty secure computation\ntasks, such as privacy-preserving federated learning (PPFL). In particular, we\nshow that CDKS may inadvertently leak plaintext information from one party to\nothers. To mitigate this issue, we propose a new scheme, SMHE (Secure Multi-Key\nHomomorphic Encryption), which incorporates a novel masking mechanism into the\nmulti-key BFV and CKKS frameworks to ensure that plaintexts remain confidential\nthroughout the computation. We implement a PPFL application using SMHE and\ndemonstrate that it provides significantly improved security with only a modest\noverhead in homomorphic evaluation. For instance, our PPFL model based on\nmulti-key CKKS incurs less than a 2\\times runtime and communication traffic\nincrease compared to the CDKS-based PPFL model. The code is publicly available\nat https://github.com/JiahuiWu2022/SMHE.git.",
    "updated" : "2025-06-25T03:28:25Z",
    "published" : "2025-06-25T03:28:25Z",
    "authors" : [
      {
        "name" : "Jiahui Wu"
      },
      {
        "name" : "Tiecheng Sun"
      },
      {
        "name" : "Fucai Luo"
      },
      {
        "name" : "Haiyan Wang"
      },
      {
        "name" : "Weizhe Zhang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.19889v1",
    "title" : "Retrieval-Confused Generation is a Good Defender for Privacy Violation\n  Attack of Large Language Models",
    "summary" : "Recent advances in large language models (LLMs) have made a profound impact\non our society and also raised new security concerns. Particularly, due to the\nremarkable inference ability of LLMs, the privacy violation attack (PVA),\nrevealed by Staab et al., introduces serious personal privacy issues. Existing\ndefense methods mainly leverage LLMs to anonymize the input query, which\nrequires costly inference time and cannot gain satisfactory defense\nperformance. Moreover, directly rejecting the PVA query seems like an effective\ndefense method, while the defense method is exposed, promoting the evolution of\nPVA. In this paper, we propose a novel defense paradigm based on\nretrieval-confused generation (RCG) of LLMs, which can efficiently and covertly\ndefend the PVA. We first design a paraphrasing prompt to induce the LLM to\nrewrite the \"user comments\" of the attack query to construct a disturbed\ndatabase. Then, we propose the most irrelevant retrieval strategy to retrieve\nthe desired user data from the disturbed database. Finally, the \"data comments\"\nare replaced with the retrieved user data to form a defended query, leading to\nresponding to the adversary with some wrong personal attributes, i.e., the\nattack fails. Extensive experiments are conducted on two datasets and eight\npopular LLMs to comprehensively evaluate the feasibility and the superiority of\nthe proposed defense method.",
    "updated" : "2025-06-24T07:28:29Z",
    "published" : "2025-06-24T07:28:29Z",
    "authors" : [
      {
        "name" : "Wanli Peng"
      },
      {
        "name" : "Xin Chen"
      },
      {
        "name" : "Hang Fu"
      },
      {
        "name" : "XinYu He"
      },
      {
        "name" : "Xue Yiming"
      },
      {
        "name" : "Juan Wen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.21308v1",
    "title" : "Balancing Privacy and Utility in Correlated Data: A Study of Bayesian\n  Differential Privacy",
    "summary" : "Privacy risks in differentially private (DP) systems increase significantly\nwhen data is correlated, as standard DP metrics often underestimate the\nresulting privacy leakage, leaving sensitive information vulnerable. Given the\nubiquity of dependencies in real-world databases, this oversight poses a\ncritical challenge for privacy protections. Bayesian differential privacy (BDP)\nextends DP to account for these correlations, yet current BDP mechanisms\nindicate notable utility loss, limiting its adoption.\n  In this work, we address whether BDP can be realistically implemented in\ncommon data structures without sacrificing utility -- a key factor for its\napplicability. By analyzing arbitrary and structured correlation models,\nincluding Gaussian multivariate distributions and Markov chains, we derive\npractical utility guarantees for BDP. Our contributions include theoretical\nlinks between DP and BDP and a novel methodology for adapting DP mechanisms to\nmeet the BDP requirements. Through evaluations on real-world databases, we\ndemonstrate that our novel theorems enable the design of BDP mechanisms that\nmaintain competitive utility, paving the way for practical privacy-preserving\ndata practices in correlated settings.",
    "updated" : "2025-06-26T14:25:44Z",
    "published" : "2025-06-26T14:25:44Z",
    "authors" : [
      {
        "name" : "Martin Lange"
      },
      {
        "name" : "Patricia Guerra-Balboa"
      },
      {
        "name" : "Javier Parra-Arnau"
      },
      {
        "name" : "Thorsten Strufe"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT",
      "68P27"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.20981v1",
    "title" : "PrivacyGo: Privacy-Preserving Ad Measurement with Multidimensional\n  Intersection",
    "summary" : "This paper tackles the challenging and practical problem of multi-identifier\nprivate user profile matching for privacy-preserving ad measurement, a\ncornerstone of modern advertising analytics. We introduce a comprehensive\ncryptographic framework leveraging reversed Oblivious Pseudorandom Functions\n(OPRF) and novel blind key rotation techniques to support secure matching\nacross multiple identifiers. Our design prevents cross-identifier linkages and\nincludes a differentially private mechanism to obfuscate intersection sizes,\nmitigating risks such as membership inference attacks.\n  We present a concrete construction of our protocol that achieves both strong\nprivacy guarantees and high efficiency. It scales to large datasets, offering a\npractical and scalable solution for privacy-centric applications like secure ad\nconversion tracking. By combining rigorous cryptographic principles with\ndifferential privacy, our work addresses a critical need in the advertising\nindustry, setting a new standard for privacy-preserving ad measurement\nframeworks.",
    "updated" : "2025-06-26T03:54:19Z",
    "published" : "2025-06-26T03:54:19Z",
    "authors" : [
      {
        "name" : "Jian Du"
      },
      {
        "name" : "Haohao Qian"
      },
      {
        "name" : "Shikun Zhang"
      },
      {
        "name" : "Wen-jie Lu"
      },
      {
        "name" : "Donghang Lu"
      },
      {
        "name" : "Yongchuan Niu"
      },
      {
        "name" : "Bo Jiang"
      },
      {
        "name" : "Yongjun Zhao"
      },
      {
        "name" : "Qiang Yan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.20872v1",
    "title" : "Empowering Digital Agriculture: A Privacy-Preserving Framework for Data\n  Sharing and Collaborative Research",
    "summary" : "Data-driven agriculture, which integrates technology and data into\nagricultural practices, has the potential to improve crop yield, disease\nresilience, and long-term soil health. However, privacy concerns, such as\nadverse pricing, discrimination, and resource manipulation, deter farmers from\nsharing data, as it can be used against them. To address this barrier, we\npropose a privacy-preserving framework that enables secure data sharing and\ncollaboration for research and development while mitigating privacy risks. The\nframework combines dimensionality reduction techniques (like Principal\nComponent Analysis (PCA)) and differential privacy by introducing Laplacian\nnoise to protect sensitive information. The proposed framework allows\nresearchers to identify potential collaborators for a target farmer and train\npersonalized machine learning models either on the data of identified\ncollaborators via federated learning or directly on the aggregated\nprivacy-protected data. It also allows farmers to identify potential\ncollaborators based on similarities. We have validated this on real-life\ndatasets, demonstrating robust privacy protection against adversarial attacks\nand utility performance comparable to a centralized system. We demonstrate how\nthis framework can facilitate collaboration among farmers and help researchers\npursue broader research objectives. The adoption of the framework can empower\nresearchers and policymakers to leverage agricultural data responsibly, paving\nthe way for transformative advances in data-driven agriculture. By addressing\ncritical privacy challenges, this work supports secure data integration,\nfostering innovation and sustainability in agricultural systems.",
    "updated" : "2025-06-25T22:46:30Z",
    "published" : "2025-06-25T22:46:30Z",
    "authors" : [
      {
        "name" : "Osama Zafar"
      },
      {
        "name" : "Rosemarie Santa González"
      },
      {
        "name" : "Mina Namazi"
      },
      {
        "name" : "Alfonso Morales"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.20737v1",
    "title" : "MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation",
    "summary" : "The proliferation of LLM-based agents has led to increasing deployment of\ninter-agent collaboration for tasks like scheduling, negotiation, resource\nallocation etc. In such systems, privacy is critical, as agents often access\nproprietary tools and domain-specific databases requiring strict\nconfidentiality. This paper examines whether LLM-based agents demonstrate an\nunderstanding of contextual privacy. And, if instructed, do these systems\npreserve inference time user privacy in non-adversarial multi-turn\nconversation. Existing benchmarks to evaluate contextual privacy in LLM-agents\nprimarily assess single-turn, low-complexity tasks where private information\ncan be easily excluded. We first present a benchmark - MAGPIE comprising 158\nreal-life high-stakes scenarios across 15 domains. These scenarios are designed\nsuch that complete exclusion of private data impedes task completion yet\nunrestricted information sharing could lead to substantial losses. We then\nevaluate the current state-of-the-art LLMs on (a) their understanding of\ncontextually private data and (b) their ability to collaborate without\nviolating user privacy. Empirical experiments demonstrate that current models,\nincluding GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual\nprivacy, misclassifying private data as shareable 25.2\\% and 43.6\\% of the\ntime. In multi-turn conversations, these models disclose private information in\n59.9\\% and 50.5\\% of cases even under explicit privacy instructions.\nFurthermore, multi-agent systems fail to complete tasks in 71\\% of scenarios.\nThese results underscore that current models are not aligned towards both\ncontextual privacy preservation and collaborative task-solving.",
    "updated" : "2025-06-25T18:04:25Z",
    "published" : "2025-06-25T18:04:25Z",
    "authors" : [
      {
        "name" : "Gurusha Juneja"
      },
      {
        "name" : "Alon Albalak"
      },
      {
        "name" : "Wenyue Hua"
      },
      {
        "name" : "William Yang Wang"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.19360v2",
    "title" : "SoK: Can Synthetic Images Replace Real Data? A Survey of Utility and\n  Privacy of Synthetic Image Generation",
    "summary" : "Advances in generative models have transformed the field of synthetic image\ngeneration for privacy-preserving data synthesis (PPDS). However, the field\nlacks a comprehensive survey and comparison of synthetic image generation\nmethods across diverse settings. In particular, when we generate synthetic\nimages for the purpose of training a classifier, there is a pipeline of\ngeneration-sampling-classification which takes private training as input and\noutputs the final classifier of interest. In this survey, we systematically\ncategorize existing image synthesis methods, privacy attacks, and mitigations\nalong this generation-sampling-classification pipeline. To empirically compare\ndiverse synthesis approaches, we provide a benchmark with representative\ngenerative methods and use model-agnostic membership inference attacks (MIAs)\nas a measure of privacy risk. Through this study, we seek to answer critical\nquestions in PPDS: Can synthetic data effectively replace real data? Which\nrelease strategy balances utility and privacy? Do mitigations improve the\nutility-privacy tradeoff? Which generative models perform best across different\nscenarios? With a systematic evaluation of diverse methods, our study provides\nactionable insights into the utility-privacy tradeoffs of synthetic data\ngeneration methods and guides the decision on optimal data releasing strategies\nfor real-world applications.",
    "updated" : "2025-06-26T01:32:12Z",
    "published" : "2025-06-24T06:41:34Z",
    "authors" : [
      {
        "name" : "Yunsung Chung"
      },
      {
        "name" : "Yunbei Zhang"
      },
      {
        "name" : "Nassir Marrouche"
      },
      {
        "name" : "Jihun Hamm"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.19268v2",
    "title" : "HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in\n  Mobile Health Apps",
    "summary" : "We present HARPT, a large-scale annotated corpus of mobile health app store\nreviews aimed at advancing research in user privacy and trust. The dataset\ncomprises over 480,000 user reviews labeled into seven categories that capture\ncritical aspects of trust in applications, trust in providers and privacy\nconcerns. Creating HARPT required addressing multiple complexities, such as\ndefining a nuanced label schema, isolating relevant content from large volumes\nof noisy data, and designing an annotation strategy that balanced scalability\nwith accuracy. This strategy integrated rule-based filtering, iterative manual\nlabeling with review, targeted data augmentation, and weak supervision using\ntransformer-based classifiers to accelerate coverage. In parallel, a carefully\ncurated subset of 7,000 reviews was manually annotated to support model\ndevelopment and evaluation. We benchmark a broad range of classification\nmodels, demonstrating that strong performance is achievable and providing a\nbaseline for future research. HARPT is released as a public resource to support\nwork in health informatics, cybersecurity, and natural language processing.",
    "updated" : "2025-06-26T15:23:54Z",
    "published" : "2025-06-24T02:59:14Z",
    "authors" : [
      {
        "name" : "Timoteo Kelly"
      },
      {
        "name" : "Abdulkadir Korkmaz"
      },
      {
        "name" : "Samuel Mallet"
      },
      {
        "name" : "Connor Souders"
      },
      {
        "name" : "Sadra Aliakbarpour"
      },
      {
        "name" : "Praveen Rao"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.ET",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.22342v1",
    "title" : "A Framework for Multi-source Privacy Preserving Epidemic Analysis",
    "summary" : "It is now well understood that diverse datasets provide a lot of value in key\nepidemiology and public health analyses, such as forecasting and nowcasting,\ndevelopment of epidemic models, evaluation and design of interventions and\nresource allocation. Some of these datasets are often sensitive, and need\nadequate privacy protections. There are many models of privacy, but\nDifferential Privacy (DP) has become a de facto standard because of its strong\nguarantees, without making models about adversaries. In this paper, we develop\na framework the integrates deep learning and epidemic models to simultaneously\nperform epidemic forecasting and learning a mechanistic model of epidemic\nspread, while incorporating multiple datasets for these analyses, including\nsome with DP guarantees. We demonstrate our framework using a realistic but\nsynthetic financial dataset with DP; such a dataset has not been used in such\nepidemic analyses. We show that this dataset provides significant value in\nforecasting and learning an epidemic model, even when used with DP guarantees.",
    "updated" : "2025-06-27T15:52:12Z",
    "published" : "2025-06-27T15:52:12Z",
    "authors" : [
      {
        "name" : "Zihan Guan"
      },
      {
        "name" : "Zhiyuan Zhao"
      },
      {
        "name" : "Fengwei Tian"
      },
      {
        "name" : "Dung Nguyen"
      },
      {
        "name" : "Payel Bhattacharjee"
      },
      {
        "name" : "Ravi Tandon"
      },
      {
        "name" : "B. Aditya Prakash"
      },
      {
        "name" : "Anil Vullikanti"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.21998v1",
    "title" : "INTACT: Compact Storage of Data Streams in Mobile Devices to Unlock User\n  Privacy at the Edge",
    "summary" : "Data streams produced by mobile devices, such as smartphones, offer highly\nvaluable sources of information to build ubiquitous services. Such data streams\nare generally uploaded and centralized to be processed by third parties,\npotentially exposing sensitive personal information. In this context, existing\nprotection mechanisms, such as Location Privacy Protection Mechanisms (LPPMs),\nhave been investigated. Alas, none of them have actually been implemented, nor\ndeployed in real-life, in mobile devices to enforce user privacy at the edge.\nMoreover, the diversity of embedded sensors and the resulting data deluge makes\nit impractical to provision such services directly on mobiles, due to their\nconstrained storage capacity, communication bandwidth and processing power.\nThis article reports on the FLI technique, which leverages a piece-wise linear\napproximation technique to capture compact representations of data streams in\nmobile devices. Beyond the FLI storage layer, we introduce Divide \\& Stay, a\nnew privacy preservation technique to execute Points of Interest (POIs)\ninference. Finally, we deploy both of them on Android and iOS as the INTACT\nframework, making a concrete step towards enforcing privacy and trust in\nubiquitous computing systems.",
    "updated" : "2025-06-27T08:09:49Z",
    "published" : "2025-06-27T08:09:49Z",
    "authors" : [
      {
        "name" : "Rémy Raes"
      },
      {
        "name" : "Olivier Ruas"
      },
      {
        "name" : "Adrien Luxey-Bitri"
      },
      {
        "name" : "Romain Rouvoy"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.24033v1",
    "title" : "Poisoning Attacks to Local Differential Privacy for Ranking Estimation",
    "summary" : "Local differential privacy (LDP) involves users perturbing their inputs to\nprovide plausible deniability of their data. However, this also makes LDP\nvulnerable to poisoning attacks. In this paper, we first introduce novel\npoisoning attacks for ranking estimation. These attacks are intricate, as fake\nattackers do not merely adjust the frequency of target items. Instead, they\nleverage a limited number of fake users to precisely modify frequencies,\neffectively altering item rankings to maximize gains. To tackle this challenge,\nwe introduce the concepts of attack cost and optimal attack item (set), and\npropose corresponding strategies for kRR, OUE, and OLH protocols. For kRR, we\niteratively select optimal attack items and allocate suitable fake users. For\nOUE, we iteratively determine optimal attack item sets and consider the\nincremental changes in item frequencies across different sets. Regarding OLH,\nwe develop a harmonic cost function based on the pre-image of a hash to select\nthat supporting a larger number of effective attack items. Lastly, we present\nan attack strategy based on confidence levels to quantify the probability of a\nsuccessful attack and the number of attack iterations more precisely. We\ndemonstrate the effectiveness of our attacks through theoretical and empirical\nevidence, highlighting the necessity for defenses against these attacks. The\nsource code and data have been made available at\nhttps://github.com/LDP-user/LDP-Ranking.git.",
    "updated" : "2025-06-30T16:39:02Z",
    "published" : "2025-06-30T16:39:02Z",
    "authors" : [
      {
        "name" : "Pei Zhan"
      },
      {
        "name" : "Peng Tang"
      },
      {
        "name" : "Yangzhuo Li"
      },
      {
        "name" : "Puwen Wei"
      },
      {
        "name" : "Shanqing Guo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.23866v1",
    "title" : "Exploring Privacy and Security as Drivers for Environmental\n  Sustainability in Cloud-Based Office Solutions",
    "summary" : "In this paper, we explore the intersection of privacy, security, and\nenvironmental sustainability in cloud-based office solutions, focusing on\nquantifying user- and network-side energy use and associated carbon emissions.\nWe hypothesise that privacy-focused services are typically more\nenergy-efficient than those funded through data collection and advertising. To\nevaluate this, we propose a framework that systematically measures\nenvironmental costs based on energy usage and network data traffic during\nwell-defined, automated usage scenarios. To test our hypothesis, we first\nanalyse how underlying architectures and business models, such as monetisation\nthrough personalised advertising, contribute to the environmental footprint of\nthese services. We then explore existing methodologies and tools for software\nenvironmental impact assessment. We apply our framework to three mainstream\nemail services selected to reflect different privacy policies, from\nad-supported tracking-intensive models to privacy-focused designs: Microsoft\nOutlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a\nself-hosted email solution, evaluated with and without end-to-end encryption.\nWe show that the self-hosted solution, even with 14% of device energy and 15%\nof emissions overheads from PGP encryption, remains the most energy-efficient,\nsaving up to 33% of emissions per session compared to Gmail. Among commercial\nproviders, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per\nsession compared to Outlook, whose emissions can be further reduced by 2%\nthrough ad-blocking.",
    "updated" : "2025-06-30T13:58:22Z",
    "published" : "2025-06-30T13:58:22Z",
    "authors" : [
      {
        "name" : "Jason Kayembe"
      },
      {
        "name" : "Iness Ben Guirat"
      },
      {
        "name" : "Jan Tobias Mühlberg"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.23622v1",
    "title" : "Privacy-Preserving Federated Learning Scheme with Mitigating Model\n  Poisoning Attacks: Vulnerabilities and Countermeasures",
    "summary" : "The privacy-preserving federated learning schemes based on the setting of two\nhonest-but-curious and non-colluding servers offer promising solutions in terms\nof security and efficiency. However, our investigation reveals that these\nschemes still suffer from privacy leakage when considering model poisoning\nattacks from malicious users. Specifically, we demonstrate that the\nprivacy-preserving computation process for defending against model poisoning\nattacks inadvertently leaks privacy to one of the honest-but-curious servers,\nenabling it to access users' gradients in plaintext. To address both privacy\nleakage and model poisoning attacks, we propose an enhanced privacy-preserving\nand Byzantine-robust federated learning (PBFL) scheme, comprising three\ncomponents: (1) a two-trapdoor fully homomorphic encryption (FHE) scheme to\nbolster users' privacy protection; (2) a novel secure normalization judgment\nmethod to preemptively thwart gradient poisoning; and (3) an innovative secure\ncosine similarity measurement method for detecting model poisoning attacks\nwithout compromising data privacy. Our scheme guarantees privacy preservation\nand resilience against model poisoning attacks, even in scenarios with\nheterogeneous, non-IID (Independently and Identically Distributed) datasets.\nTheoretical analyses substantiate the security and efficiency of our scheme,\nand extensive experiments corroborate the efficacy of our private attacks.\nFurthermore, the experimental results demonstrate that our scheme accelerates\ntraining speed while reducing communication overhead compared to the\nstate-of-the-art PBFL schemes.",
    "updated" : "2025-06-30T08:39:01Z",
    "published" : "2025-06-30T08:39:01Z",
    "authors" : [
      {
        "name" : "Jiahui Wu"
      },
      {
        "name" : "Fucai Luo"
      },
      {
        "name" : "Tiecheng Sun"
      },
      {
        "name" : "Haiyan Wang"
      },
      {
        "name" : "Weizhe Zhang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.23603v1",
    "title" : "SoK: Semantic Privacy in Large Language Models",
    "summary" : "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs.",
    "updated" : "2025-06-30T08:08:15Z",
    "published" : "2025-06-30T08:08:15Z",
    "authors" : [
      {
        "name" : "Baihe Ma"
      },
      {
        "name" : "Yanna Jiang"
      },
      {
        "name" : "Xu Wang"
      },
      {
        "name" : "Guangshen Yu"
      },
      {
        "name" : "Qin Wang"
      },
      {
        "name" : "Caijun Sun"
      },
      {
        "name" : "Chen Li"
      },
      {
        "name" : "Xuelei Qi"
      },
      {
        "name" : "Ying He"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "Ren Ping Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.23583v1",
    "title" : "Detect \\& Score: Privacy-Preserving Misbehaviour Detection and\n  Contribution Evaluation in Federated Learning",
    "summary" : "Federated learning with secure aggregation enables private and collaborative\nlearning from decentralised data without leaking sensitive client information.\nHowever, secure aggregation also complicates the detection of malicious client\nbehaviour and the evaluation of individual client contributions to the\nlearning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et\nal.) were proposed for contribution evaluation (CE) and misbehaviour detection\n(MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance\non the random selection of clients in each training round, while FedGT lacks\nthe CE ability. In this work, we combine the strengths of QI and FedGT to\nachieve both robust MD and accurate CE. Our experiments demonstrate superior\nperformance compared to using either method independently.",
    "updated" : "2025-06-30T07:40:18Z",
    "published" : "2025-06-30T07:40:18Z",
    "authors" : [
      {
        "name" : "Marvin Xhemrishi"
      },
      {
        "name" : "Alexandre Graell i Amat"
      },
      {
        "name" : "Balázs Pejó"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.23481v1",
    "title" : "Evaluation of Geolocation Capabilities of Multimodal Large Language\n  Models and Analysis of Associated Privacy Risks",
    "summary" : "Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)\nhas significantly enhanced their reasoning capabilities, enabling a wide range\nof intelligent applications. However, these advancements also raise critical\nconcerns regarding privacy and ethics. MLLMs are now capable of inferring the\ngeographic location of images -- such as those shared on social media or\ncaptured from street views -- based solely on visual content, thereby posing\nserious risks of privacy invasion, including doxxing, surveillance, and other\nsecurity threats.\n  Methods: This study provides a comprehensive analysis of existing geolocation\ntechniques based on MLLMs. It systematically reviews relevant litera-ture and\nevaluates the performance of state-of-the-art visual reasoning models on\ngeolocation tasks, particularly in identifying the origins of street view\nimagery.\n  Results: Empirical evaluation reveals that the most advanced visual large\nmodels can successfully localize the origin of street-level imagery with up to\n$49\\%$ accuracy within a 1-kilometer radius. This performance underscores the\nmodels' powerful capacity to extract and utilize fine-grained geographic cues\nfrom visual data.\n  Conclusions: Building on these findings, the study identifies key visual\nelements that contribute to suc-cessful geolocation, such as text,\narchitectural styles, and environmental features. Furthermore, it discusses the\npotential privacy implications associated with MLLM-enabled geolocation and\ndiscuss several technical and policy-based coun-termeasures to mitigate\nassociated risks. Our code and dataset are available at\nhttps://github.com/zxyl1003/MLLM-Geolocation-Evaluation.",
    "updated" : "2025-06-30T03:05:30Z",
    "published" : "2025-06-30T03:05:30Z",
    "authors" : [
      {
        "name" : "Xian Zhang"
      },
      {
        "name" : "Xiang Cheng"
      }
    ],
    "categories" : [
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.23014v1",
    "title" : "Generating Privacy Stories From Software Documentation",
    "summary" : "Research shows that analysts and developers consider privacy as a security\nconcept or as an afterthought, which may lead to non-compliance and violation\nof users' privacy. Most current approaches, however, focus on extracting legal\nrequirements from the regulations and evaluating the compliance of software and\nprocesses with them. In this paper, we develop a novel approach based on\nchain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language\nModels (LLMs) to extract privacy behaviors from various software documents\nprior to and during software development, and then generate privacy\nrequirements in the format of user stories. Our results show that most commonly\nused LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and\ngenerate privacy user stories with F1 scores exceeding 0.8. We also show that\nthe performance of these models could be improved through parameter-tuning. Our\nfindings provide insight into using and optimizing LLMs for generating privacy\nrequirements given software documents created prior to or throughout the\nsoftware development lifecycle.",
    "updated" : "2025-06-28T20:55:21Z",
    "published" : "2025-06-28T20:55:21Z",
    "authors" : [
      {
        "name" : "Wilder Baldwin"
      },
      {
        "name" : "Shashank Chintakuntla"
      },
      {
        "name" : "Shreyah Parajuli"
      },
      {
        "name" : "Ali Pourghasemi"
      },
      {
        "name" : "Ryan Shanz"
      },
      {
        "name" : "Sepideh Ghanavati"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.22789v1",
    "title" : "WavShape: Information-Theoretic Speech Representation Learning for Fair\n  and Privacy-Aware Audio Processing",
    "summary" : "Speech embeddings often retain sensitive attributes such as speaker identity,\naccent, or demographic information, posing risks in biased model training and\nprivacy leakage. We propose WavShape, an information-theoretic speech\nrepresentation learning framework that optimizes embeddings for fairness and\nprivacy while preserving task-relevant information. We leverage mutual\ninformation (MI) estimation using the Donsker-Varadhan formulation to guide an\nMI-based encoder that systematically filters sensitive attributes while\nmaintaining speech content essential for downstream tasks. Experimental results\non three known datasets show that WavShape reduces MI between embeddings and\nsensitive attributes by up to 81% while retaining 97% of task-relevant\ninformation. By integrating information theory with self-supervised speech\nmodels, this work advances the development of fair, privacy-aware, and\nresource-efficient speech systems.",
    "updated" : "2025-06-28T07:03:55Z",
    "published" : "2025-06-28T07:03:55Z",
    "authors" : [
      {
        "name" : "Oguzhan Baser"
      },
      {
        "name" : "Ahmet Ege Tanriverdi"
      },
      {
        "name" : "Kaan Kale"
      },
      {
        "name" : "Sandeep P. Chinchali"
      },
      {
        "name" : "Sriram Vishwanath"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.22787v1",
    "title" : "What's Privacy Good for? Measuring Privacy as a Shield from Harms due to\n  Personal Data Use",
    "summary" : "We propose a harm-centric conceptualization of privacy that asks: What harms\nfrom personal data use can privacy prevent? The motivation behind this research\nis limitations in existing privacy frameworks (e.g., Contextual Integrity) to\ncapture or categorize many of the harms that arise from modern technology's use\nof personal data. We operationalize this conceptualization in an online study\nwith 400 college and university students. Study participants indicated their\nperceptions of different harms (e.g., manipulation, discrimination, and\nharassment) that may arise when artificial intelligence-based algorithms infer\npersonal data (e.g., demographics, personality traits, and cognitive\ndisability) and use it to identify students who are likely to drop out of a\ncourse or the best job candidate. The study includes 14 harms and six types of\npersonal data selected based on an extensive literature review.\n  Comprehensive statistical analyses of the study data show that the 14 harms\nare internally consistent and collectively represent a general notion of\nprivacy harms. The study data also surfaces nuanced perceptions of harms, both\nacross the contexts and participants' demographic factors. Based on these\nresults, we discuss how privacy can be improved equitably. Thus, this research\nnot only contributes to enhancing the understanding of privacy as a concept but\nalso provides practical guidance to improve privacy in the context of education\nand employment.",
    "updated" : "2025-06-28T07:00:37Z",
    "published" : "2025-06-28T07:00:37Z",
    "authors" : [
      {
        "name" : "Sri Harsha Gajavalli"
      },
      {
        "name" : "Junichi Koizumi"
      },
      {
        "name" : "Rakibul Hasan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.22752v1",
    "title" : "Privacy-Preserving Methods for Bug Severity Prediction",
    "summary" : "Bug severity prediction is a critical task in software engineering as it\nenables more efficient resource allocation and prioritization in software\nmaintenance. While AI-based analyses and models significantly require access to\nextensive datasets, industrial applications face challenges due to data-sharing\nconstraints and the limited availability of labeled data. In this study, we\ninvestigate method-level bug severity prediction using source code metrics and\nLarge Language Models (LLMs) with two widely used datasets. We compare the\nperformance of models trained using centralized learning, federated learning,\nand synthetic data generation. Our experimental results, obtained using two\nwidely recognized software defect datasets, indicate that models trained with\nfederated learning and synthetic data achieve comparable results to centrally\ntrained models without data sharing. Our finding highlights the potential of\nprivacy-preserving approaches such as federated learning and synthetic data\ngeneration to enable effective bug severity prediction in industrial context\nwhere data sharing is a major challenge.\n  The source code and dataset are available at our GitHub repository:\nhttps://github.com/drvshavva/EASE2025-Privacy-Preserving-Methods-for-Bug-Severity-Prediction.",
    "updated" : "2025-06-28T04:40:51Z",
    "published" : "2025-06-28T04:40:51Z",
    "authors" : [
      {
        "name" : "Havvanur Dervişoğlu"
      },
      {
        "name" : "Ruşen Halepmollası"
      },
      {
        "name" : "Elif Eyvaz"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.22727v1",
    "title" : "Convergent Privacy Framework with Contractive GNN Layers for Multi-hop\n  Aggregations",
    "summary" : "Differential privacy (DP) has been integrated into graph neural networks\n(GNNs) to protect sensitive structural information, e.g., edges, nodes, and\nassociated features across various applications. A common approach is to\nperturb the message-passing process, which forms the core of most GNN\narchitectures. However, existing methods typically incur a privacy cost that\ngrows linearly with the number of layers (Usenix Security'23), ultimately\nrequiring excessive noise to maintain a reasonable privacy level. This\nlimitation becomes particularly problematic when deep GNNs are necessary to\ncapture complex and long-range interactions in graphs. In this paper, we\ntheoretically establish that the privacy budget can converge with respect to\nthe number of layers by applying privacy amplification techniques to the\nmessage-passing process, exploiting the contractive properties inherent to\nstandard GNN operations. Motivated by this analysis, we propose a simple yet\neffective Contractive Graph Layer (CGL) that ensures the contractiveness\nrequired for theoretical guarantees while preserving model utility. Our\nframework, CARIBOU, supports both training and inference, equipped with a\ncontractive aggregation module, a privacy allocation module, and a privacy\nauditing module. Experimental evaluations demonstrate that CARIBOU\nsignificantly improves the privacy-utility trade-off and achieves superior\nperformance in privacy auditing tasks.",
    "updated" : "2025-06-28T02:17:53Z",
    "published" : "2025-06-28T02:17:53Z",
    "authors" : [
      {
        "name" : "Yu Zheng"
      },
      {
        "name" : "Chenang Li"
      },
      {
        "name" : "Zhou Li"
      },
      {
        "name" : "Qingsong Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.22606v1",
    "title" : "A User-Centric, Privacy-Preserving, and Verifiable Ecosystem for\n  Personal Data Management and Utilization",
    "summary" : "In the current paradigm of digital personalized services, the centralized\nmanagement of personal data raises significant privacy concerns, security\nvulnerabilities, and diminished individual autonomy over sensitive information.\nDespite their efficiency, traditional centralized architectures frequently fail\nto satisfy rigorous privacy requirements and expose users to data breaches and\nunauthorized access risks. This pressing challenge calls for a fundamental\nparadigm shift in methodologies for collecting, storing, and utilizing personal\ndata across diverse sectors, including education, healthcare, and finance.\n  This paper introduces a novel decentralized, privacy-preserving architecture\nthat handles heterogeneous personal information, ranging from educational\ncredentials to health records and financial data. Unlike traditional models,\nour system grants users complete data ownership and control, allowing them to\nselectively share information without compromising privacy. The architecture's\nfoundation comprises advanced privacy-enhancing technologies, including secure\nenclaves and federated learning, enabling secure computation, verification, and\ndata sharing. The system supports diverse functionalities, including local\ncomputation, model training, and privacy-preserving data sharing, while\nensuring data credibility and robust user privacy.",
    "updated" : "2025-06-27T20:05:46Z",
    "published" : "2025-06-27T20:05:46Z",
    "authors" : [
      {
        "name" : "Osama Zafar"
      },
      {
        "name" : "Mina Namazi"
      },
      {
        "name" : "Yuqiao Xu"
      },
      {
        "name" : "Youngjin Yoo"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.22462v1",
    "title" : "Privacy-aware IoT Fall Detection Services For Aging in Place",
    "summary" : "Fall detection is critical to support the growing elderly population,\nprojected to reach 2.1 billion by 2050. However, existing methods often face\ndata scarcity challenges or compromise privacy. We propose a novel IoT-based\nFall Detection as a Service (FDaaS) framework to assist the elderly in living\nindependently and safely by accurately detecting falls. We design a\nservice-oriented architecture that leverages Ultra-wideband (UWB) radar sensors\nas an IoT health-sensing service, ensuring privacy and minimal intrusion. We\naddress the challenges of data scarcity by utilizing a Fall Detection\nGenerative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.\nWe developed a protocol to collect a comprehensive dataset of the elderly daily\nactivities and fall events. This resulted in a real dataset that carefully\nmimics the elderly's routine. We rigorously evaluate and compare various models\nusing this dataset. Experimental results show our approach achieves 90.72%\naccuracy and 89.33% precision in distinguishing between fall events and regular\nactivities of daily living.",
    "updated" : "2025-06-18T03:28:07Z",
    "published" : "2025-06-18T03:28:07Z",
    "authors" : [
      {
        "name" : "Abdallah Lakhdari"
      },
      {
        "name" : "Jiajie Li"
      },
      {
        "name" : "Amani Abusafia"
      },
      {
        "name" : "Athman Bouguettaya"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.12846v2",
    "title" : "VFEFL: Privacy-Preserving Federated Learning against Malicious Clients\n  via Verifiable Functional Encryption",
    "summary" : "Federated learning is a promising distributed learning paradigm that enables\ncollaborative model training without exposing local client data, thereby\nprotect data privacy. However, it also brings new threats and challenges. The\nadvancement of model inversion attacks has rendered the plaintext transmission\nof local models insecure, while the distributed nature of federated learning\nmakes it particularly vulnerable to attacks raised by malicious clients. To\nprotect data privacy and prevent malicious client attacks, this paper proposes\na privacy-preserving federated learning framework based on verifiable\nfunctional encryption, without a non-colluding dual-server setup or additional\ntrusted third-party. Specifically, we propose a novel decentralized verifiable\nfunctional encryption (DVFE) scheme that enables the verification of specific\nrelationships over multi-dimensional ciphertexts. This scheme is formally\ntreated, in terms of definition, security model and security proof.\nFurthermore, based on the proposed DVFE scheme, we design a privacy-preserving\nfederated learning framework VFEFL that incorporates a novel robust aggregation\nrule to detect malicious clients, enabling the effective training of\nhigh-accuracy models under adversarial settings. Finally, we provide formal\nanalysis and empirical evaluation of the proposed schemes. The results\ndemonstrate that our approach achieves the desired privacy protection,\nrobustness, verifiability and fidelity, while eliminating the reliance on\nnon-colluding dual-server settings or trusted third parties required by\nexisting methods.",
    "updated" : "2025-06-28T09:23:38Z",
    "published" : "2025-06-15T13:38:40Z",
    "authors" : [
      {
        "name" : "Nina Cai"
      },
      {
        "name" : "Jinguang Han"
      },
      {
        "name" : "Weizhi Meng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.17336v2",
    "title" : "Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought\n  Reasoning and Homomorphically Encrypted Vector Databases",
    "summary" : "Large language models (LLMs) are increasingly used as personal agents,\naccessing sensitive user data such as calendars, emails, and medical records.\nUsers currently face a trade-off: They can send private records, many of which\nare stored in remote databases, to powerful but untrusted LLM providers,\nincreasing their exposure risk. Alternatively, they can run less powerful\nmodels locally on trusted devices. We bridge this gap. Our Socratic\nChain-of-Thought Reasoning first sends a generic, non-private user query to a\npowerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and\ndetailed sub-queries without accessing user data. Next, we embed these\nsub-queries and perform encrypted sub-second semantic search using our\nHomomorphically Encrypted Vector Database across one million entries of a\nsingle user's private data. This represents a realistic scale of personal\ndocuments, emails, and records accumulated over years of digital activity.\nFinally, we feed the CoT prompt and the decrypted records to a local language\nmodel and generate the final response. On the LoCoMo long-context QA benchmark,\nour hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,\noutperforms using GPT-4o alone by up to 7.1 percentage points. This\ndemonstrates a first step toward systems where tasks are decomposed and split\nbetween untrusted strong LLMs and weak local ones, preserving user privacy.",
    "updated" : "2025-07-01T16:41:35Z",
    "published" : "2025-06-19T07:13:30Z",
    "authors" : [
      {
        "name" : "Yubeen Bae"
      },
      {
        "name" : "Minchan Kim"
      },
      {
        "name" : "Jaejin Lee"
      },
      {
        "name" : "Sangbum Kim"
      },
      {
        "name" : "Jaehyung Kim"
      },
      {
        "name" : "Yejin Choi"
      },
      {
        "name" : "Niloofar Mireshghallah"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00286v1",
    "title" : "Visual Privacy Management with Generative AI for Blind and Low-Vision\n  People",
    "summary" : "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to\ninterpret and manage visual content in their daily lives. While such tools can\nenhance the accessibility of visual content and so enable greater user\nindependence, they also introduce complex challenges around visual privacy. In\nthis paper, we investigate the current practices and future design preferences\nof blind and low vision individuals through an interview study with 21\nparticipants. Our findings reveal a range of current practices with GenAI that\nbalance privacy, efficiency, and emotional agency, with users accounting for\nprivacy risks across six key scenarios, such as self-presentation,\nindoor/outdoor spatial privacy, social sharing, and handling professional\ncontent. Our findings reveal design preferences, including on-device\nprocessing, zero-retention guarantees, sensitive content redaction,\nprivacy-aware appearance indicators, and multimodal tactile mirrored\ninteraction methods. We conclude with actionable design recommendations to\nsupport user-centered visual privacy through GenAI, expanding the notion of\nprivacy and responsible handling of others data.",
    "updated" : "2025-06-30T21:55:21Z",
    "published" : "2025-06-30T21:55:21Z",
    "authors" : [
      {
        "name" : "Tanusree Sharma"
      },
      {
        "name" : "Yu-Yun Tseng"
      },
      {
        "name" : "Lotus Zhang"
      },
      {
        "name" : "Ayae Ide"
      },
      {
        "name" : "Kelly Avery Mack"
      },
      {
        "name" : "Leah Findlater"
      },
      {
        "name" : "Danna Gurari"
      },
      {
        "name" : "Yang Wang"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00230v1",
    "title" : "PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense\n  Spatial Networks for Encrypted Lossy Image Reconstruction",
    "summary" : "Reconstructing high-quality images from low-resolution inputs using Residual\nDense Spatial Networks (RDSNs) is crucial yet challenging, particularly in\ncollaborative scenarios where centralized training poses significant privacy\nrisks, including data leakage and inference attacks, as well as high\ncomputational costs. We propose a novel Privacy-Preserving Federated\nLearning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image\nreconstruction. PPFL-RDSN integrates Federated Learning (FL), local\ndifferential privacy, and robust model watermarking techniques, ensuring data\nremains secure on local devices, safeguarding sensitive information, and\nmaintaining model authenticity without revealing underlying data. Empirical\nevaluations show that PPFL-RDSN achieves comparable performance to the\nstate-of-the-art centralized methods while reducing computational burdens, and\neffectively mitigates security and privacy vulnerabilities, making it a\npractical solution for secure and privacy-preserving collaborative computer\nvision applications.",
    "updated" : "2025-06-30T19:54:34Z",
    "published" : "2025-06-30T19:54:34Z",
    "authors" : [
      {
        "name" : "Peilin He"
      },
      {
        "name" : "James Joshi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.23866v2",
    "title" : "Exploring Privacy and Security as Drivers for Environmental\n  Sustainability in Cloud-Based Office Solutions",
    "summary" : "In this paper, we explore the intersection of privacy, security, and\nenvironmental sustainability in cloud-based office solutions, focusing on\nquantifying user- and network-side energy use and associated carbon emissions.\nWe hypothesise that privacy-focused services are typically more\nenergy-efficient than those funded through data collection and advertising. To\nevaluate this, we propose a framework that systematically measures\nenvironmental costs based on energy usage and network data traffic during\nwell-defined, automated usage scenarios. To test our hypothesis, we first\nanalyse how underlying architectures and business models, such as monetisation\nthrough personalised advertising, contribute to the environmental footprint of\nthese services. We then explore existing methodologies and tools for software\nenvironmental impact assessment. We apply our framework to three mainstream\nemail services selected to reflect different privacy policies, from\nad-supported tracking-intensive models to privacy-focused designs: Microsoft\nOutlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a\nself-hosted email solution, evaluated with and without end-to-end encryption.\nWe show that the self-hosted solution, even with 14% of device energy and 15%\nof emissions overheads from PGP encryption, remains the most energy-efficient,\nsaving up to 33% of emissions per session compared to Gmail. Among commercial\nproviders, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per\nsession compared to Outlook, whose emissions can be further reduced by 2%\nthrough ad-blocking.",
    "updated" : "2025-07-02T13:40:13Z",
    "published" : "2025-06-30T13:58:22Z",
    "authors" : [
      {
        "name" : "Jason Kayembe"
      },
      {
        "name" : "Iness Ben Guirat"
      },
      {
        "name" : "Jan Tobias Mühlberg"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.15854v2",
    "title" : "Privacy-Preserving in Connected and Autonomous Vehicles Through Vision\n  to Text Transformation",
    "summary" : "Connected and Autonomous Vehicles (CAVs) rely on a range of devices that\noften process privacy-sensitive data. Among these, roadside units play a\ncritical role particularly through the use of AI-equipped (AIE) cameras for\napplications such as violation detection. However, the privacy risks associated\nwith captured imagery remain a major concern, as such data can be misused for\nidentity theft, profiling, or unauthorized commercial purposes. While\ntraditional techniques such as face blurring and obfuscation have been applied\nto mitigate privacy risks, individual privacy remains at risk, as individuals\ncan still be tracked using other features such as their clothing. This paper\nintroduces a novel privacy-preserving framework that leverages feedback-based\nreinforcement learning (RL) and vision-language models (VLMs) to protect\nsensitive visual information captured by AIE cameras. The main idea is to\nconvert images into semantically equivalent textual descriptions, ensuring that\nscene-relevant information is retained while visual privacy is preserved. A\nhierarchical RL strategy is employed to iteratively refine the generated text,\nenhancing both semantic accuracy and privacy. Evaluation results demonstrate\nsignificant improvements in both privacy protection and textual quality, with\nthe Unique Word Count increasing by approximately 77\\% and Detail Density by\naround 50\\% compared to existing approaches.",
    "updated" : "2025-07-02T18:31:53Z",
    "published" : "2025-06-18T20:02:24Z",
    "authors" : [
      {
        "name" : "Abdolazim Rezaei"
      },
      {
        "name" : "Mehdi Sookhak"
      },
      {
        "name" : "Ahmad Patooghy"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.13972v2",
    "title" : "Membership Inference Attacks as Privacy Tools: Reliability, Disparity\n  and Ensemble",
    "summary" : "Membership inference attacks (MIAs) pose a significant threat to the privacy\nof machine learning models and are widely used as tools for privacy assessment,\nauditing, and machine unlearning. While prior MIA research has primarily\nfocused on performance metrics such as AUC, accuracy, and TPR@low FPR - either\nby developing new methods to enhance these metrics or using them to evaluate\nprivacy solutions - we found that it overlooks the disparities among different\nattacks. These disparities, both between distinct attack methods and between\nmultiple instantiations of the same method, have crucial implications for the\nreliability and completeness of MIAs as privacy evaluation tools. In this\npaper, we systematically investigate these disparities through a novel\nframework based on coverage and stability analysis. Extensive experiments\nreveal significant disparities in MIAs, their potential causes, and their\nbroader implications for privacy evaluation. To address these challenges, we\npropose an ensemble framework with three distinct strategies to harness the\nstrengths of state-of-the-art MIAs while accounting for their disparities. This\nframework not only enables the construction of more powerful attacks but also\nprovides a more robust and comprehensive methodology for privacy evaluation.",
    "updated" : "2025-07-03T17:45:38Z",
    "published" : "2025-06-16T20:22:07Z",
    "authors" : [
      {
        "name" : "Zhiqi Wang"
      },
      {
        "name" : "Chengyu Zhang"
      },
      {
        "name" : "Yuetian Chen"
      },
      {
        "name" : "Nathalie Baracaldo"
      },
      {
        "name" : "Swanand Kadhe"
      },
      {
        "name" : "Lei Yu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.13170v1",
    "title" : "Dual Protection Ring: User Profiling Via Differential Privacy and\n  Service Dissemination Through Private Information Retrieval",
    "summary" : "User profiling is crucial in providing personalised services, as it relies on\nanalysing user behaviour and preferences to deliver targeted services. This\napproach enhances user experience and promotes heightened engagement.\nNevertheless, user profiling also gives rise to noteworthy privacy\nconsiderations due to the extensive tracking and monitoring of personal data,\npotentially leading to surveillance or identity theft. We propose a dual-ring\nprotection mechanism to protect user privacy by examining various threats to\nuser privacy, such as behavioural attacks, profiling fingerprinting and\nmonitoring, profile perturbation, etc., both on the user and service provider\nsides. We develop user profiles that contain sensitive private attributes and\nan equivalent profile based on differential privacy for evaluating personalised\nservices. We determine the entropy of the resultant profiles during each update\nto protect profiling attributes and invoke various processes, such as data\nevaporation, to artificially increase entropy or destroy private profiling\nattributes. Furthermore, we use different variants of private information\nretrieval (PIR) to retrieve personalised services against differentially\nprivate profiles. We implement critical components of the proposed model via a\nproof-of-concept mobile app to demonstrate its applicability over a specific\ncase study of advertising services, which can be generalised to other services.\nOur experimental results show that the observed processing delays with\ndifferent PIR schemes are similar to the current advertising systems.",
    "updated" : "2025-06-16T07:33:12Z",
    "published" : "2025-06-16T07:33:12Z",
    "authors" : [
      {
        "name" : "Imdad Ullah"
      },
      {
        "name" : "Najm Hassan"
      },
      {
        "name" : "Tariq Ahamed Ahangar"
      },
      {
        "name" : "Zawar Hussain Shah"
      },
      {
        "name" : "Mehregan Mahdavi"
      },
      {
        "name" : "Andrew Levula"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00230v2",
    "title" : "PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense\n  Spatial Networks for Encrypted Lossy Image Reconstruction",
    "summary" : "Reconstructing high-quality images from low-resolution inputs using Residual\nDense Spatial Networks (RDSNs) is crucial yet challenging, particularly in\ncollaborative scenarios where centralized training poses significant privacy\nrisks, including data leakage and inference attacks, as well as high\ncomputational costs. We propose a novel Privacy-Preserving Federated\nLearning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image\nreconstruction. PPFL-RDSN integrates Federated Learning (FL), local\ndifferential privacy, and robust model watermarking techniques, ensuring data\nremains secure on local devices, safeguarding sensitive information, and\nmaintaining model authenticity without revealing underlying data. Empirical\nevaluations show that PPFL-RDSN achieves comparable performance to the\nstate-of-the-art centralized methods while reducing computational burdens, and\neffectively mitigates security and privacy vulnerabilities, making it a\npractical solution for secure and privacy-preserving collaborative computer\nvision applications.",
    "updated" : "2025-07-04T15:10:21Z",
    "published" : "2025-06-30T19:54:34Z",
    "authors" : [
      {
        "name" : "Peilin He"
      },
      {
        "name" : "James Joshi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.02974v1",
    "title" : "InvisibleInk: High-Utility and Low-Cost Text Generation with\n  Differential Privacy",
    "summary" : "As major progress in LLM-based long-form text generation enables paradigms\nsuch as retrieval-augmented generation (RAG) and inference-time scaling, safely\nincorporating private information into the generation remains a critical open\nquestion. We present InvisibleInk, a highly scalable long-form text generation\nframework satisfying rigorous differential privacy guarantees with respect to\nthe sensitive references. It interprets sampling from the LLM's\nnext-token-distribution as the exponential mechanism over the LLM logits with\ntwo innovations. First, we reduce the privacy cost by isolating and clipping\nonly the sensitive information in the model logits (relative to the public\nlogits). Second, we improve text quality by sampling from a small superset of\nthe top-$k$ private tokens. Empirical evaluations demonstrate a consistent\n$8\\times$ reduction in computation cost over state-of-the-art baselines to\ngenerate long-form private text of the same utility across privacy levels. In\nsummary, InvisibleInk is able to generate private long-form text at less than\n$10\\times$ the computation cost of non-private generation.",
    "updated" : "2025-06-30T18:00:41Z",
    "published" : "2025-06-30T18:00:41Z",
    "authors" : [
      {
        "name" : "Vishnu Vinod"
      },
      {
        "name" : "Krishna Pillutla"
      },
      {
        "name" : "Abhradeep Guha Thakurta"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.02968v1",
    "title" : "Unveiling Privacy Policy Complexity: An Exploratory Study Using Graph\n  Mining, Machine Learning, and Natural Language Processing",
    "summary" : "Privacy policy documents are often lengthy, complex, and difficult for\nnon-expert users to interpret, leading to a lack of transparency regarding the\ncollection, processing, and sharing of personal data. As concerns over online\nprivacy grow, it is essential to develop automated tools capable of analyzing\nprivacy policies and identifying potential risks. In this study, we explore the\npotential of interactive graph visualizations to enhance user understanding of\nprivacy policies by representing policy terms as structured graph models. This\napproach makes complex relationships more accessible and enables users to make\ninformed decisions about their personal data (RQ1). We also employ graph mining\nalgorithms to identify key themes, such as User Activity and Device\nInformation, using dimensionality reduction techniques like t-SNE and PCA to\nassess clustering effectiveness. Our findings reveal that graph-based\nclustering improves policy content interpretability. It highlights patterns in\nuser tracking and data sharing, which supports forensic investigations and\nidentifies regulatory non-compliance. This research advances AI-driven tools\nfor auditing privacy policies by integrating interactive visualizations with\ngraph mining. Enhanced transparency fosters accountability and trust.",
    "updated" : "2025-06-30T14:55:57Z",
    "published" : "2025-06-30T14:55:57Z",
    "authors" : [
      {
        "name" : "Vijayalakshmi Ramasamy"
      },
      {
        "name" : "Seth Barrett"
      },
      {
        "name" : "Gokila Dorai"
      },
      {
        "name" : "Jessica Zumbach"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.23866v3",
    "title" : "Exploring Privacy and Security as Drivers for Environmental\n  Sustainability in Cloud-Based Office Solutions",
    "summary" : "In this paper, we explore the intersection of privacy, security, and\nenvironmental sustainability in cloud-based office solutions, focusing on\nquantifying user- and network-side energy use and associated carbon emissions.\nWe hypothesise that privacy-focused services are typically more\nenergy-efficient than those funded through data collection and advertising. To\nevaluate this, we propose a framework that systematically measures\nenvironmental costs based on energy usage and network data traffic during\nwell-defined, automated usage scenarios. To test our hypothesis, we first\nanalyse how underlying architectures and business models, such as monetisation\nthrough personalised advertising, contribute to the environmental footprint of\nthese services. We then explore existing methodologies and tools for software\nenvironmental impact assessment. We apply our framework to three mainstream\nemail services selected to reflect different privacy policies, from\nad-supported tracking-intensive models to privacy-focused designs: Microsoft\nOutlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a\nself-hosted email solution, evaluated with and without end-to-end encryption.\nWe show that the self-hosted solution, even with 14% of device energy and 15%\nof emissions overheads from PGP encryption, remains the most energy-efficient,\nsaving up to 33% of emissions per session compared to Gmail. Among commercial\nproviders, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per\nsession compared to Outlook, whose emissions can be further reduced by 2%\nthrough ad-blocking.",
    "updated" : "2025-07-04T13:50:03Z",
    "published" : "2025-06-30T13:58:22Z",
    "authors" : [
      {
        "name" : "Jason Kayembe"
      },
      {
        "name" : "Iness Ben Guirat"
      },
      {
        "name" : "Jan Tobias Mühlberg"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.11679v2",
    "title" : "LLMs on support of privacy and security of mobile apps: state of the art\n  and research directions",
    "summary" : "Modern life has witnessed the explosion of mobile devices. However, besides\nthe valuable features that bring convenience to end users, security and privacy\nrisks still threaten users of mobile apps. The increasing sophistication of\nthese threats in recent years has underscored the need for more advanced and\nefficient detection approaches. In this chapter, we explore the application of\nLarge Language Models (LLMs) to identify security risks and privacy violations\nand mitigate them for the mobile application ecosystem. By introducing\nstate-of-the-art research that applied LLMs to mitigate the top 10 common\nsecurity risks of smartphone platforms, we highlight the feasibility and\npotential of LLMs to replace traditional analysis methods, such as dynamic and\nhybrid analysis of mobile apps. As a representative example of LLM-based\nsolutions, we present an approach to detect sensitive data leakage when users\nshare images online, a common behavior of smartphone users nowadays. Finally,\nwe discuss open research challenges.",
    "updated" : "2025-07-07T17:36:57Z",
    "published" : "2025-06-13T11:17:15Z",
    "authors" : [
      {
        "name" : "Tran Thanh Lam Nguyen"
      },
      {
        "name" : "Barbara Carminati"
      },
      {
        "name" : "Elena Ferrari"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.21308v2",
    "title" : "Balancing Privacy and Utility in Correlated Data: A Study of Bayesian\n  Differential Privacy",
    "summary" : "Privacy risks in differentially private (DP) systems increase significantly\nwhen data is correlated, as standard DP metrics often underestimate the\nresulting privacy leakage, leaving sensitive information vulnerable. Given the\nubiquity of dependencies in real-world databases, this oversight poses a\ncritical challenge for privacy protections. Bayesian differential privacy (BDP)\nextends DP to account for these correlations, yet current BDP mechanisms\nindicate notable utility loss, limiting its adoption.\n  In this work, we address whether BDP can be realistically implemented in\ncommon data structures without sacrificing utility -- a key factor for its\napplicability. By analyzing arbitrary and structured correlation models,\nincluding Gaussian multivariate distributions and Markov chains, we derive\npractical utility guarantees for BDP. Our contributions include theoretical\nlinks between DP and BDP and a novel methodology for adapting DP mechanisms to\nmeet the BDP requirements. Through evaluations on real-world databases, we\ndemonstrate that our novel theorems enable the design of BDP mechanisms that\nmaintain competitive utility, paving the way for practical privacy-preserving\ndata practices in correlated settings.",
    "updated" : "2025-07-15T13:10:10Z",
    "published" : "2025-06-26T14:25:44Z",
    "authors" : [
      {
        "name" : "Martin Lange"
      },
      {
        "name" : "Patricia Guerra-Balboa"
      },
      {
        "name" : "Javier Parra-Arnau"
      },
      {
        "name" : "Thorsten Strufe"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT",
      "68P27"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.23603v2",
    "title" : "SoK: Semantic Privacy in Large Language Models",
    "summary" : "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs.",
    "updated" : "2025-07-16T15:51:30Z",
    "published" : "2025-06-30T08:08:15Z",
    "authors" : [
      {
        "name" : "Baihe Ma"
      },
      {
        "name" : "Yanna Jiang"
      },
      {
        "name" : "Xu Wang"
      },
      {
        "name" : "Guangsheng Yu"
      },
      {
        "name" : "Qin Wang"
      },
      {
        "name" : "Caijun Sun"
      },
      {
        "name" : "Chen Li"
      },
      {
        "name" : "Xuelei Qi"
      },
      {
        "name" : "Ying He"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "Ren Ping Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  }
]