[
  {
    "id" : "http://arxiv.org/abs/2506.02998v1",
    "title" : "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy\n  Question-Answering Systems",
    "summary" : "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information.",
    "updated" : "2025-06-03T15:32:20Z",
    "published" : "2025-06-03T15:32:20Z",
    "authors" : [
      {
        "name" : "Đorđe Klisura"
      },
      {
        "name" : "Astrid R Bernaga Torres"
      },
      {
        "name" : "Anna Karen Gárate-Escamilla"
      },
      {
        "name" : "Rajesh Roshan Biswal"
      },
      {
        "name" : "Ke Yang"
      },
      {
        "name" : "Hilal Pataci"
      },
      {
        "name" : "Anthony Rios"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02965v1",
    "title" : "Memory-Efficient and Privacy-Preserving Collaborative Training for\n  Mixture-of-Experts LLMs",
    "summary" : "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks.",
    "updated" : "2025-06-03T15:00:18Z",
    "published" : "2025-06-03T15:00:18Z",
    "authors" : [
      {
        "name" : "Ze Yu Zhang"
      },
      {
        "name" : "Bolin Ding"
      },
      {
        "name" : "Bryan Kian Hsiang Low"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02725v1",
    "title" : "Recursive Privacy-Preserving Estimation Over Markov Fading Channels",
    "summary" : "In industrial applications, the presence of moving machinery, vehicles, and\npersonnel, contributes to the dynamic nature of the wireless channel. This time\nvariability induces channel fading, which can be effectively modeled using a\nMarkov fading channel (MFC). In this paper, we investigate the problem of\nsecure state estimation for systems that communicate over a MFC in the presence\nof an eavesdropper. The objective is to enable a remote authorized user to\naccurately estimate the states of a dynamic system, while considering the\npotential interception of the sensor's packet through a wiretap channel. To\nprevent information leakage, a novel co-design strategy is established, which\ncombines a privacy-preserving mechanism with a state estimator. To implement\nour encoding scheme, a nonlinear mapping of the innovation is introduced based\non the weighted reconstructed innovation previously received by the legitimate\nuser. Corresponding to this encoding scheme, we design a recursive\nprivacy-preserving filtering algorithm to achieve accurate estimation. The\nboundedness of estimation error dynamics at the legitimate user's side is\ndiscussed and the divergence of the eavesdropper's estimation error is\nanalyzed, which demonstrates the effectiveness of our co-design strategy in\nensuring secrecy. Furthermore, a simulation example of a three-tank system is\nprovided to demonstrate the effectiveness and feasibility of our\nprivacy-preserving estimation method.",
    "updated" : "2025-06-03T10:33:49Z",
    "published" : "2025-06-03T10:33:49Z",
    "authors" : [
      {
        "name" : "Jie Huang"
      },
      {
        "name" : "Fanlin Jia"
      },
      {
        "name" : "Xiao He"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02711v1",
    "title" : "Privacy Leaks by Adversaries: Adversarial Iterations for Membership\n  Inference Attack",
    "summary" : "Membership inference attack (MIA) has become one of the most widely used and\neffective methods for evaluating the privacy risks of machine learning models.\nThese attacks aim to determine whether a specific sample is part of the model's\ntraining set by analyzing the model's output. While traditional membership\ninference attacks focus on leveraging the model's posterior output, such as\nconfidence on the target sample, we propose IMIA, a novel attack strategy that\nutilizes the process of generating adversarial samples to infer membership. We\npropose to infer the member properties of the target sample using the number of\niterations required to generate its adversarial sample. We conduct experiments\nacross multiple models and datasets, and our results demonstrate that the\nnumber of iterations for generating an adversarial sample is a reliable feature\nfor membership inference, achieving strong performance both in black-box and\nwhite-box attack scenarios. This work provides a new perspective for evaluating\nmodel privacy and highlights the potential of adversarial example-based\nfeatures for privacy leakage assessment.",
    "updated" : "2025-06-03T10:09:24Z",
    "published" : "2025-06-03T10:09:24Z",
    "authors" : [
      {
        "name" : "Jing Xue"
      },
      {
        "name" : "Zhishen Sun"
      },
      {
        "name" : "Haishan Ye"
      },
      {
        "name" : "Luo Luo"
      },
      {
        "name" : "Xiangyu Chang"
      },
      {
        "name" : "Ivor Tsang"
      },
      {
        "name" : "Guang Dai"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02563v1",
    "title" : "Privacy-Preserving Federated Convex Optimization: Balancing\n  Partial-Participation and Efficiency via Noise Cancellation",
    "summary" : "This paper tackles the challenge of achieving Differential Privacy (DP) in\nFederated Learning (FL) under partial-participation, where only a subset of the\nmachines participate in each time-step. While previous work achieved optimal\nperformance in full-participation settings, these methods struggled to extend\nto partial-participation scenarios. Our approach fills this gap by introducing\na novel noise-cancellation mechanism that preserves privacy without sacrificing\nconvergence rates or computational efficiency. We analyze our method within the\nStochastic Convex Optimization (SCO) framework and show that it delivers\noptimal performance for both homogeneous and heterogeneous data distributions.\nThis work expands the applicability of DP in FL, offering an efficient and\npractical solution for privacy-preserving learning in distributed systems with\npartial participation.",
    "updated" : "2025-06-03T07:48:35Z",
    "published" : "2025-06-03T07:48:35Z",
    "authors" : [
      {
        "name" : "Roie Reshef"
      },
      {
        "name" : "Kfir Yehuda Levy"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02422v1",
    "title" : "Enhancing Convergence, Privacy and Fairness for Wireless Personalized\n  Federated Learning: Quantization-Assisted Min-Max Fair Scheduling",
    "summary" : "Personalized federated learning (PFL) offers a solution to balancing\npersonalization and generalization by conducting federated learning (FL) to\nguide personalized learning (PL). Little attention has been given to wireless\nPFL (WPFL), where privacy concerns arise. Performance fairness of PL models is\nanother challenge resulting from communication bottlenecks in WPFL. This paper\nexploits quantization errors to enhance the privacy of WPFL and proposes a\nnovel quantization-assisted Gaussian differential privacy (DP) mechanism. We\nanalyze the convergence upper bounds of individual PL models by considering the\nimpact of the mechanism (i.e., quantization errors and Gaussian DP noises) and\nimperfect communication channels on the FL of WPFL. By minimizing the maximum\nof the bounds, we design an optimal transmission scheduling strategy that\nyields min-max fairness for WPFL with OFDMA interfaces. This is achieved by\nrevealing the nested structure of this problem to decouple it into subproblems\nsolved sequentially for the client selection, channel allocation, and power\ncontrol, and for the learning rates and PL-FL weighting coefficients.\nExperiments validate our analysis and demonstrate that our approach\nsubstantially outperforms alternative scheduling strategies by 87.08%, 16.21%,\nand 38.37% in accuracy, the maximum test loss of participating clients, and\nfairness (Jain's index), respectively.",
    "updated" : "2025-06-03T04:13:07Z",
    "published" : "2025-06-03T04:13:07Z",
    "authors" : [
      {
        "name" : "Xiyu Zhao"
      },
      {
        "name" : "Qimei Cui"
      },
      {
        "name" : "Ziqiang Du"
      },
      {
        "name" : "Weicai Li"
      },
      {
        "name" : "Xi Yu"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "Ji Zhang"
      },
      {
        "name" : "Xiaofeng Tao"
      },
      {
        "name" : "Ping Zhang"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02410v1",
    "title" : "Testing for large-dimensional covariance matrix under differential\n  privacy",
    "summary" : "The increasing prevalence of high-dimensional data across various\napplications has raised significant privacy concerns in statistical inference.\nIn this paper, we propose a differentially private integrated statistic for\ntesting large-dimensional covariance structures, enabling accurate statistical\ninsights while safeguarding privacy. First, we analyze the global sensitivity\nof sample eigenvalues for sub-Gaussian populations, where our method bypasses\nthe commonly assumed boundedness of data covariates. For sufficiently large\nsample size, the privatized statistic guarantees privacy with high probability.\nFurthermore, when the ratio of dimension to sample size, $d/n \\to y \\in (0,\n\\infty)$, the privatized test is asymptotically distribution-free with\nwell-known critical values, and detects the local alternative hypotheses\ndistinct from the null at the fastest rate of $1/\\sqrt{n}$. Extensive numerical\nstudies on synthetic and real data showcase the validity and powerfulness of\nour proposed method.",
    "updated" : "2025-06-03T03:53:51Z",
    "published" : "2025-06-03T03:53:51Z",
    "authors" : [
      {
        "name" : "Shiwei Sang"
      },
      {
        "name" : "Yicheng Zeng"
      },
      {
        "name" : "Xuehu Zhu"
      },
      {
        "name" : "Shurong Zheng"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02156v1",
    "title" : "Mitigating Data Poisoning Attacks to Local Differential Privacy",
    "summary" : "The distributed nature of local differential privacy (LDP) invites data\npoisoning attacks and poses unforeseen threats to the underlying LDP-supported\napplications. In this paper, we propose a comprehensive mitigation framework\nfor popular frequency estimation, which contains a suite of novel defenses,\nincluding malicious user detection, attack pattern recognition, and damaged\nutility recovery. In addition to existing attacks, we explore new adaptive\nadversarial activities for our mitigation design. For detection, we present a\nnew method to precisely identify bogus reports and thus LDP aggregation can be\nperformed over the ``clean'' data. When the attack behavior becomes stealthy\nand direct filtering out malicious users is difficult, we further propose a\ndetection that can effectively recognize hidden adversarial patterns, thus\nfacilitating the decision-making of service providers. These detection methods\nrequire no additional data and attack information and incur minimal\ncomputational cost. Our experiment demonstrates their excellent performance and\nsubstantial improvement over previous work in various settings. In addition, we\nconduct an empirical analysis of LDP post-processing for corrupted data\nrecovery and propose a new post-processing method, through which we reveal new\ninsights into protocol recommendations in practice and key design principles\nfor future research.",
    "updated" : "2025-06-02T18:37:15Z",
    "published" : "2025-06-02T18:37:15Z",
    "authors" : [
      {
        "name" : "Xiaolin Li"
      },
      {
        "name" : "Ninghui Li"
      },
      {
        "name" : "Boyang Wang"
      },
      {
        "name" : "Wenhai Sun"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01907v1",
    "title" : "SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data",
    "summary" : "Privacy-preserving data publication, including synthetic data sharing, often\nexperiences trade-offs between privacy and utility. Synthetic data is generally\nmore effective than data anonymization in balancing this trade-off, however,\nnot without its own challenges. Synthetic data produced by generative models\ntrained on source data may inadvertently reveal information about outliers.\nTechniques specifically designed for preserving privacy, such as introducing\nnoise to satisfy differential privacy, often incur unpredictable and\nsignificant losses in utility. In this work we show that, with the right\nmechanism of synthetic data generation, we can achieve strong privacy\nprotection without significant utility loss. Synthetic data generators\nproducing contracting data patterns, such as Synthetic Minority Over-sampling\nTechnique (SMOTE), can enhance a differentially private data generator,\nleveraging the strengths of both. We prove in theory and through empirical\ndemonstration that this SMOTE-DP technique can produce synthetic data that not\nonly ensures robust privacy protection but maintains utility in downstream\nlearning tasks.",
    "updated" : "2025-06-02T17:27:10Z",
    "published" : "2025-06-02T17:27:10Z",
    "authors" : [
      {
        "name" : "Yan Zhou"
      },
      {
        "name" : "Bradley Malin"
      },
      {
        "name" : "Murat Kantarcioglu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01425v1",
    "title" : "CSVAR: Enhancing Visual Privacy in Federated Learning via Adaptive\n  Shuffling Against Overfitting",
    "summary" : "Although federated learning preserves training data within local privacy\ndomains, the aggregated model parameters may still reveal private\ncharacteristics. This vulnerability stems from clients' limited training data,\nwhich predisposes models to overfitting. Such overfitting enables models to\nmemorize distinctive patterns from training samples, thereby amplifying the\nsuccess probability of privacy attacks like membership inference. To enhance\nvisual privacy protection in FL, we present CSVAR(Channel-Wise Spatial Image\nShuffling with Variance-Guided Adaptive Region Partitioning), a novel image\nshuffling framework to generate obfuscated images for secure data transmission\nand each training epoch, addressing both overfitting-induced privacy leaks and\nraw image transmission risks. CSVAR adopts region-variance as the metric to\nmeasure visual privacy sensitivity across image regions. Guided by this, CSVAR\nadaptively partitions each region into multiple blocks, applying fine-grained\npartitioning to privacy-sensitive regions with high region-variances for\nenhancing visual privacy protection and coarse-grained partitioning to\nprivacy-insensitive regions for balancing model utility. In each region, CSVAR\nthen shuffles between blocks in both the spatial domains and chromatic channels\nto hide visual spatial features and disrupt color distribution. Experimental\nevaluations conducted on diverse real-world datasets demonstrate that CSVAR is\ncapable of generating visually obfuscated images that exhibit high perceptual\nambiguity to human eyes, simultaneously mitigating the effectiveness of\nadversarial data reconstruction attacks and achieving a good trade-off between\nvisual privacy protection and model utility.",
    "updated" : "2025-06-02T08:30:12Z",
    "published" : "2025-06-02T08:30:12Z",
    "authors" : [
      {
        "name" : "Zhuo Chen"
      },
      {
        "name" : "Zhenya Ma"
      },
      {
        "name" : "Yan Zhang"
      },
      {
        "name" : "Donghua Cai"
      },
      {
        "name" : "Ye Zhang"
      },
      {
        "name" : "Qiushi Li"
      },
      {
        "name" : "Yongheng Deng"
      },
      {
        "name" : "Ye Guo"
      },
      {
        "name" : "Ju Ren"
      },
      {
        "name" : " Xuemin"
      },
      {
        "name" : " Shen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01325v1",
    "title" : "Understanding the Identity-Transformation Approach in OIDC-Compatible\n  Privacy-Preserving SSO Services",
    "summary" : "OpenID Connect (OIDC) enables a user with commercial-off-the-shelf browsers\nto log into multiple websites, called relying parties (RPs), by her username\nand credential set up in another trusted web system, called the identity\nprovider (IdP). Identity transformations are proposed in UppreSSO to provide\nOIDC-compatible SSO services, preventing both IdP-based login tracing and\nRP-based identity linkage. While security and privacy of SSO services in\nUppreSSO have been proved, several essential issues of this\nidentity-transformation approach are not well studied. In this paper, we\ncomprehensively investigate the approach as below. Firstly, several suggestions\nfor the efficient integration of identity transformations in OIDC-compatible\nSSO are explained. Then, we uncover the relationship between\nidentity-transformations in SSO and oblivious pseudo-random functions (OPRFs),\nand present two variations of the properties required for SSO security as well\nas the privacy requirements, to analyze existing OPRF protocols. Finally, new\nidentity transformations different from those designed in UppreSSO, are\nconstructed based on OPRFs, satisfying different variations of SSO security\nrequirements. To the best of our knowledge, this is the first time to uncover\nthe relationship between identity transformations in OIDC-compatible\nprivacy-preserving SSO services and OPRFs, and prove the SSO-related properties\n(i.e., key-identifier freeness, RP designation and user identification) of OPRF\nprotocols, in addition to the basic properties of correctness, obliviousness\nand pseudo-randomness.",
    "updated" : "2025-06-02T05:11:01Z",
    "published" : "2025-06-02T05:11:01Z",
    "authors" : [
      {
        "name" : "Jingqiang Lin"
      },
      {
        "name" : "Baitao Zhang"
      },
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Quanwei Cai"
      },
      {
        "name" : "Jiwu Jing"
      },
      {
        "name" : "Huiyang He"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02063v1",
    "title" : "Privacy-Aware, Public-Aligned: Embedding Risk Detection and Public\n  Values into Scalable Clinical Text De-Identification for Trusted Research\n  Environments",
    "summary" : "Clinical free-text data offers immense potential to improve population health\nresearch such as richer phenotyping, symptom tracking, and contextual\nunderstanding of patient care. However, these data present significant privacy\nrisks due to the presence of directly or indirectly identifying information\nembedded in unstructured narratives. While numerous de-identification tools\nhave been developed, few have been tested on real-world, heterogeneous datasets\nat scale or assessed for governance readiness. In this paper, we synthesise our\nfindings from previous studies examining the privacy-risk landscape across\nmultiple document types and NHS data providers in Scotland. We characterise how\ndirect and indirect identifiers vary by record type, clinical setting, and data\nflow, and show how changes in documentation practice can degrade model\nperformance over time. Through public engagement, we explore societal\nexpectations around the safe use of clinical free text and reflect these in the\ndesign of a prototype privacy-risk management tool to support transparent,\nauditable decision-making. Our findings highlight that privacy risk is\ncontext-dependent and cumulative, underscoring the need for adaptable, hybrid\nde-identification approaches that combine rule-based precision with contextual\nunderstanding. We offer a comprehensive view of the challenges and\nopportunities for safe, scalable reuse of clinical free-text within Trusted\nResearch Environments and beyond, grounded in both technical evidence and\npublic perspectives on responsible data use.",
    "updated" : "2025-06-01T17:45:57Z",
    "published" : "2025-06-01T17:45:57Z",
    "authors" : [
      {
        "name" : "Arlene Casey"
      },
      {
        "name" : "Stuart Dunbar"
      },
      {
        "name" : "Franz Gruber"
      },
      {
        "name" : "Samuel McInerney"
      },
      {
        "name" : "Matúš Falis"
      },
      {
        "name" : "Pamela Linksted"
      },
      {
        "name" : "Katie Wilde"
      },
      {
        "name" : "Kathy Harrison"
      },
      {
        "name" : "Alison Hamilton"
      },
      {
        "name" : "Christian Cole"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01072v1",
    "title" : "IDCloak: A Practical Secure Multi-party Dataset Join Framework for\n  Vertical Privacy-preserving Machine Learning",
    "summary" : "Vertical privacy-preserving machine learning (vPPML) enables multiple parties\nto train models on their vertically distributed datasets while keeping datasets\nprivate. In vPPML, it is critical to perform the secure dataset join, which\naligns features corresponding to intersection IDs across datasets and forms a\nsecret-shared and joint training dataset. However, existing methods for this\nstep could be impractical due to: (1) they are insecure when they expose\nintersection IDs; or (2) they rely on a strong trust assumption requiring a\nnon-colluding auxiliary server; or (3) they are limited to the two-party\nsetting.\n  This paper proposes IDCloak, the first practical secure multi-party dataset\njoin framework for vPPML that keeps IDs private without a non-colluding\nauxiliary server. IDCloak consists of two protocols: (1) a circuit-based\nmulti-party private set intersection protocol (cmPSI), which obtains\nsecret-shared flags indicating intersection IDs via an optimized communication\nstructure combining OKVS and OPRF; (2) a secure multi-party feature alignment\nprotocol, which obtains the secret-shared and joint dataset using secret-shared\nflags, via our proposed efficient secure shuffle protocol. Experiments show\nthat: (1) compared to the state-of-the-art secure two-party dataset join\nframework (iPrivjoin), IDCloak demonstrates higher efficiency in the two-party\nsetting and comparable performance when the party number increases; (2)\ncompared to the state-of-the-art cmPSI protocol under honest majority, our\nproposed cmPSI protocol provides a stronger security guarantee (dishonest\nmajority) while improving efficiency by up to $7.78\\times$ in time and\n$8.73\\times$ in communication sizes; (3) our proposed secure shuffle protocol\noutperforms the state-of-the-art shuffle protocol by up to $138.34\\times$ in\ntime and $132.13\\times$ in communication sizes.",
    "updated" : "2025-06-01T16:20:39Z",
    "published" : "2025-06-01T16:20:39Z",
    "authors" : [
      {
        "name" : "Shuyu Chen"
      },
      {
        "name" : "Guopeng Lin"
      },
      {
        "name" : "Haoyu Niu"
      },
      {
        "name" : "Lushan Song"
      },
      {
        "name" : "Chengxun Hong"
      },
      {
        "name" : "Weili Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00759v1",
    "title" : "Understanding and Mitigating Cross-lingual Privacy Leakage via\n  Language-specific and Universal Privacy Neurons",
    "summary" : "Large Language Models (LLMs) trained on massive data capture rich information\nembedded in the training data. However, this also introduces the risk of\nprivacy leakage, particularly involving personally identifiable information\n(PII). Although previous studies have shown that this risk can be mitigated\nthrough methods such as privacy neurons, they all assume that both the\n(sensitive) training data and user queries are in English. We show that they\ncannot defend against the privacy leakage in cross-lingual contexts: even if\nthe training data is exclusively in one language, these (private) models may\nstill reveal private information when queried in another language. In this\nwork, we first investigate the information flow of cross-lingual privacy\nleakage to give a better understanding. We find that LLMs process private\ninformation in the middle layers, where representations are largely shared\nacross languages. The risk of leakage peaks when converted to a\nlanguage-specific space in later layers. Based on this, we identify\nprivacy-universal neurons and language-specific privacy neurons.\nPrivacy-universal neurons influence privacy leakage across all languages, while\nlanguage-specific privacy neurons are only related to specific languages. By\ndeactivating these neurons, the cross-lingual privacy leakage risk is reduced\nby 23.3%-31.6%.",
    "updated" : "2025-06-01T00:10:30Z",
    "published" : "2025-06-01T00:10:30Z",
    "authors" : [
      {
        "name" : "Wenshuo Dong"
      },
      {
        "name" : "Qingsong Yang"
      },
      {
        "name" : "Shu Yang"
      },
      {
        "name" : "Lijie Hu"
      },
      {
        "name" : "Meng Ding"
      },
      {
        "name" : "Wanyu Lin"
      },
      {
        "name" : "Tianhang Zheng"
      },
      {
        "name" : "Di Wang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.04036v1",
    "title" : "Privacy and Security Threat for OpenAI GPTs",
    "summary" : "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications.",
    "updated" : "2025-06-04T14:58:29Z",
    "published" : "2025-06-04T14:58:29Z",
    "authors" : [
      {
        "name" : "Wei Wenying"
      },
      {
        "name" : "Zhao Kaifa"
      },
      {
        "name" : "Xue Lei"
      },
      {
        "name" : "Fan Ming"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.03870v1",
    "title" : "Evaluating Apple Intelligence's Writing Tools for Privacy Against Large\n  Language Model-Based Inference Attacks: Insights from Early Datasets",
    "summary" : "The misuse of Large Language Models (LLMs) to infer emotions from text for\nmalicious purposes, known as emotion inference attacks, poses a significant\nthreat to user privacy. In this paper, we investigate the potential of Apple\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\nmitigate these risks through text modifications such as rewriting and tone\nadjustment. By developing early novel datasets specifically for this purpose,\nwe empirically assess how different text modifications influence LLM-based\ndetection. This capability suggests strong potential for Apple Intelligence's\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\nfor future adaptive rewriting systems capable of dynamically neutralizing\nsensitive emotional content to enhance user privacy. To the best of our\nknowledge, this research provides the first empirical analysis of Apple\nIntelligence's text-modification tools within a privacy-preservation context\nwith the broader goal of developing on-device, user-centric privacy-preserving\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\nsystems.",
    "updated" : "2025-06-04T12:01:17Z",
    "published" : "2025-06-04T12:01:17Z",
    "authors" : [
      {
        "name" : "Mohd. Farhan Israk Soumik"
      },
      {
        "name" : "Syed Mhamudul Hasan"
      },
      {
        "name" : "Abdur R. Shahid"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.03618v1",
    "title" : "GCFL: A Gradient Correction-based Federated Learning Framework for\n  Privacy-preserving CPSS",
    "summary" : "Federated learning, as a distributed architecture, shows great promise for\napplications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the\nprivacy risks inherent in CPSS, the integration of differential privacy with\nfederated learning has attracted considerable attention. Existing research\nmainly focuses on dynamically adjusting the noise added or discarding certain\ngradients to mitigate the noise introduced by differential privacy. However,\nthese approaches fail to remove the noise that hinders convergence and correct\nthe gradients affected by the noise, which significantly reduces the accuracy\nof model classification. To overcome these challenges, this paper proposes a\nnovel framework for differentially private federated learning that balances\nrigorous privacy guarantees with accuracy by introducing a server-side gradient\ncorrection mechanism. Specifically, after clients perform gradient clipping and\nnoise perturbation, our framework detects deviations in the noisy local\ngradients and employs a projection mechanism to correct them, mitigating the\nnegative impact of noise. Simultaneously, gradient projection promotes the\nalignment of gradients from different clients and guides the model towards\nconvergence to a global optimum. We evaluate our framework on several benchmark\ndatasets, and the experimental results demonstrate that it achieves\nstate-of-the-art performance under the same privacy budget.",
    "updated" : "2025-06-04T06:52:37Z",
    "published" : "2025-06-04T06:52:37Z",
    "authors" : [
      {
        "name" : "Jiayi Wan"
      },
      {
        "name" : "Xiang Zhu"
      },
      {
        "name" : "Fanzhen Liu"
      },
      {
        "name" : "Wei Fan"
      },
      {
        "name" : "Xiaolong Xu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02965v2",
    "title" : "PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training\n  for Mixture-of-Experts LLMs",
    "summary" : "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks.",
    "updated" : "2025-06-04T05:38:31Z",
    "published" : "2025-06-03T15:00:18Z",
    "authors" : [
      {
        "name" : "Ze Yu Zhang"
      },
      {
        "name" : "Bolin Ding"
      },
      {
        "name" : "Bryan Kian Hsiang Low"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05290v1",
    "title" : "Big Bird: Privacy Budget Management for W3C's Privacy-Preserving\n  Attribution API",
    "summary" : "Privacy-preserving advertising APIs like Privacy-Preserving Attribution (PPA)\nare designed to enhance web privacy while enabling effective ad measurement.\nPPA offers an alternative to cross-site tracking with encrypted reports\ngoverned by differential privacy (DP), but current designs lack a principled\napproach to privacy budget management, creating uncertainty around critical\ndesign decisions. We present Big Bird, a privacy budget manager for PPA that\nclarifies per-site budget semantics and introduces a global budgeting system\ngrounded in resource isolation principles. Big Bird enforces utility-preserving\nlimits via quota budgets and improves global budget utilization through a novel\nbatched scheduling algorithm. Together, these mechanisms establish a robust\nfoundation for enforcing privacy protections in adversarial environments. We\nimplement Big Bird in Firefox and evaluate it on real-world ad data,\ndemonstrating its resilience and effectiveness.",
    "updated" : "2025-06-05T17:45:13Z",
    "published" : "2025-06-05T17:45:13Z",
    "authors" : [
      {
        "name" : "Pierre Tholoniat"
      },
      {
        "name" : "Alison Caulfield"
      },
      {
        "name" : "Giorgio Cavicchioli"
      },
      {
        "name" : "Mark Chen"
      },
      {
        "name" : "Nikos Goutzoulias"
      },
      {
        "name" : "Benjamin Case"
      },
      {
        "name" : "Asaf Cidon"
      },
      {
        "name" : "Roxana Geambasu"
      },
      {
        "name" : "Mathias Lécuyer"
      },
      {
        "name" : "Martin Thomson"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05101v1",
    "title" : "Privacy Amplification Through Synthetic Data: Insights from Linear\n  Regression",
    "summary" : "Synthetic data inherits the differential privacy guarantees of the model used\nto generate it. Additionally, synthetic data may benefit from privacy\namplification when the generative model is kept hidden. While empirical studies\nsuggest this phenomenon, a rigorous theoretical understanding is still lacking.\nIn this paper, we investigate this question through the well-understood\nframework of linear regression. First, we establish negative results showing\nthat if an adversary controls the seed of the generative model, a single\nsynthetic data point can leak as much information as releasing the model\nitself. Conversely, we show that when synthetic data is generated from random\ninputs, releasing a limited number of synthetic data points amplifies privacy\nbeyond the model's inherent guarantees. We believe our findings in linear\nregression can serve as a foundation for deriving more general bounds in the\nfuture.",
    "updated" : "2025-06-05T14:44:15Z",
    "published" : "2025-06-05T14:44:15Z",
    "authors" : [
      {
        "name" : "Clément Pierquin"
      },
      {
        "name" : "Aurélien Bellet"
      },
      {
        "name" : "Marc Tommasi"
      },
      {
        "name" : "Matthieu Boussard"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.04978v1",
    "title" : "Evaluating the Impact of Privacy-Preserving Federated Learning on CAN\n  Intrusion Detection",
    "summary" : "The challenges derived from the data-intensive nature of machine learning in\nconjunction with technologies that enable novel paradigms such as V2X and the\npotential offered by 5G communication, allow and justify the deployment of\nFederated Learning (FL) solutions in the vehicular intrusion detection domain.\nIn this paper, we investigate the effects of integrating FL strategies into the\nmachine learning-based intrusion detection process for on-board vehicular\nnetworks. Accordingly, we propose a FL implementation of a state-of-the-art\nIntrusion Detection System (IDS) for Controller Area Network (CAN), based on\nLSTM autoencoders. We thoroughly evaluate its detection efficiency and\ncommunication overhead, comparing it to a centralized version of the same\nalgorithm, thereby presenting it as a feasible solution.",
    "updated" : "2025-06-05T12:49:22Z",
    "published" : "2025-06-05T12:49:22Z",
    "authors" : [
      {
        "name" : "Gabriele Digregorio"
      },
      {
        "name" : "Elisabetta Cainazzo"
      },
      {
        "name" : "Stefano Longari"
      },
      {
        "name" : "Michele Carminati"
      },
      {
        "name" : "Stefano Zanero"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06124v1",
    "title" : "PrivTru: A Privacy-by-Design Data Trustee Minimizing Information Leakage",
    "summary" : "Data trustees serve as intermediaries that facilitate secure data sharing\nbetween independent parties. This paper offers a technical perspective on Data\ntrustees, guided by privacy-by-design principles. We introduce PrivTru, an\ninstantiation of a data trustee that provably achieves optimal privacy\nproperties. Therefore, PrivTru calculates the minimal amount of information the\ndata trustee needs to request from data sources to respond to a given query.\nOur analysis shows that PrivTru minimizes information leakage to the data\ntrustee, regardless of the trustee's prior knowledge, while preserving the\nutility of the data.",
    "updated" : "2025-06-06T14:33:59Z",
    "published" : "2025-06-06T14:33:59Z",
    "authors" : [
      {
        "name" : "Lukas Gehring"
      },
      {
        "name" : "Florian Tschorsch"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06062v1",
    "title" : "Minoritised Ethnic People's Security and Privacy Concerns and Responses\n  towards Essential Online Services",
    "summary" : "Minoritised ethnic people are marginalised in society, and therefore at a\nhigher risk of adverse online harms, including those arising from the loss of\nsecurity and privacy of personal data. Despite this, there has been very little\nresearch focused on minoritised ethnic people's security and privacy concerns,\nattitudes, and behaviours. In this work, we provide the results of one of the\nfirst studies in this regard. We explore minoritised ethnic people's\nexperiences of using essential online services across three sectors: health,\nsocial housing, and energy, their security and privacy-related concerns, and\nresponses towards these services. We conducted a thematic analysis of 44\nsemi-structured interviews with people of various reported minoritised\nethnicities in the UK. Privacy concerns and lack of control over personal data\nemerged as a major theme, with many interviewees considering privacy as their\nmost significant concern when using online services. Several creative tactics\nto exercise some agency were reported, including selective and inconsistent\ndisclosure of personal data. A core concern about how data may be used was\ndriven by a fear of repercussions, including penalisation and discrimination,\ninfluenced by prior experiences of institutional and online racism. The\nincreased concern and potential for harm resulted in minoritised ethnic people\ngrappling with a higher-stakes dilemma of whether to disclose personal\ninformation online or not. Furthermore, trust in institutions, or lack thereof,\nwas found to be embedded throughout as a basis for adapting behaviour. We draw\non our results to provide lessons learned for the design of more inclusive,\nmarginalisation-aware, and privacy-preserving online services.",
    "updated" : "2025-06-06T13:17:44Z",
    "published" : "2025-06-06T13:17:44Z",
    "authors" : [
      {
        "name" : "Aunam Quyoum"
      },
      {
        "name" : "Mark Wong"
      },
      {
        "name" : "Sebati Ghosh"
      },
      {
        "name" : "Siamak F. Shahandashti"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CR",
      "cs.HC",
      "K.4.2; H.1.2; K.4.1; K.6.5; J.4"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05908v1",
    "title" : "QualitEye: Public and Privacy-preserving Gaze Data Quality Verification",
    "summary" : "Gaze-based applications are increasingly advancing with the availability of\nlarge datasets but ensuring data quality presents a substantial challenge when\ncollecting data at scale. It further requires different parties to collaborate,\ntherefore, privacy concerns arise. We propose QualitEye--the first method for\nverifying image-based gaze data quality. QualitEye employs a new semantic\nrepresentation of eye images that contains the information required for\nverification while excluding irrelevant information for better domain\nadaptation. QualitEye covers a public setting where parties can freely exchange\ndata and a privacy-preserving setting where parties cannot reveal their raw\ndata nor derive gaze features/labels of others with adapted private set\nintersection protocols. We evaluate QualitEye on the MPIIFaceGaze and\nGazeCapture datasets and achieve a high verification performance (with a small\noverhead in runtime for privacy-preserving versions). Hence, QualitEye paves\nthe way for new gaze analysis methods at the intersection of machine learning,\nhuman-computer interaction, and cryptography.",
    "updated" : "2025-06-06T09:27:04Z",
    "published" : "2025-06-06T09:27:04Z",
    "authors" : [
      {
        "name" : "Mayar Elfares"
      },
      {
        "name" : "Pascal Reisert"
      },
      {
        "name" : "Ralf Küsters"
      },
      {
        "name" : "Andreas Bulling"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05743v1",
    "title" : "When Better Features Mean Greater Risks: The Performance-Privacy\n  Trade-Off in Contrastive Learning",
    "summary" : "With the rapid advancement of deep learning technology, pre-trained encoder\nmodels have demonstrated exceptional feature extraction capabilities, playing a\npivotal role in the research and application of deep learning. However, their\nwidespread use has raised significant concerns about the risk of training data\nprivacy leakage. This paper systematically investigates the privacy threats\nposed by membership inference attacks (MIAs) targeting encoder models, focusing\non contrastive learning frameworks. Through experimental analysis, we reveal\nthe significant impact of model architecture complexity on membership privacy\nleakage: As more advanced encoder frameworks improve feature-extraction\nperformance, they simultaneously exacerbate privacy-leakage risks. Furthermore,\nthis paper proposes a novel membership inference attack method based on the\np-norm of feature vectors, termed the Embedding Lp-Norm Likelihood Attack\n(LpLA). This method infers membership status, by leveraging the statistical\ndistribution characteristics of the p-norm of feature vectors. Experimental\nresults across multiple datasets and model architectures demonstrate that LpLA\noutperforms existing methods in attack performance and robustness, particularly\nunder limited attack knowledge and query volumes. This study not only uncovers\nthe potential risks of privacy leakage in contrastive learning frameworks, but\nalso provides a practical basis for privacy protection research in encoder\nmodels. We hope that this work will draw greater attention to the privacy risks\nassociated with self-supervised learning models and shed light on the\nimportance of a balance between model utility and training data privacy. Our\ncode is publicly available at: https://github.com/SeroneySun/LpLA_code.",
    "updated" : "2025-06-06T05:03:29Z",
    "published" : "2025-06-06T05:03:29Z",
    "authors" : [
      {
        "name" : "Ruining Sun"
      },
      {
        "name" : "Hongsheng Hu"
      },
      {
        "name" : "Wei Luo"
      },
      {
        "name" : "Zhaoxi Zhang"
      },
      {
        "name" : "Yanjun Zhang"
      },
      {
        "name" : "Haizhuan Yuan"
      },
      {
        "name" : "Leo Yu Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05683v1",
    "title" : "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation\n  Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence\n  in AR/VR/MR",
    "summary" : "Extended reality (XR) systems, which consist of virtual reality (VR),\naugmented reality (AR), and mixed reality (XR), offer a transformative\ninterface for immersive, multi-modal, and embodied human-computer interaction.\nIn this paper, we envision that multi-modal multi-task (M3T) federated\nfoundation models (FedFMs) can offer transformative capabilities for XR systems\nthrough integrating the representational strength of M3T foundation models\n(FMs) with the privacy-preserving model training principles of federated\nlearning (FL). We present a modular architecture for FedFMs, which entails\ndifferent coordination paradigms for model training and aggregations. Central\nto our vision is the codification of XR challenges that affect the\nimplementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality\ndiversity, (2) Hardware heterogeneity and system-level constraints, (3)\nInteractivity and embodied personalization, (4) Functional/task variability,\nand (5) Temporality and environmental variability. We illustrate the\nmanifestation of these dimensions across a set of emerging and anticipated\napplications of XR systems. Finally, we propose evaluation metrics, dataset\nrequirements, and design tradeoffs necessary for the development of\nresource-aware FedFMs in XR. This perspective aims to chart the technical and\nconceptual foundations for context-aware privacy-preserving intelligence in the\nnext generation of XR systems.",
    "updated" : "2025-06-06T02:23:42Z",
    "published" : "2025-06-06T02:23:42Z",
    "authors" : [
      {
        "name" : "Fardis Nadimi"
      },
      {
        "name" : "Payam Abdisarabshali"
      },
      {
        "name" : "Kasra Borazjani"
      },
      {
        "name" : "Jacob Chakareski"
      },
      {
        "name" : "Seyyedali Hosseinalipour"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05503v1",
    "title" : "On Differential Privacy for Adaptively Solving Search Problems via\n  Sketching",
    "summary" : "Recently differential privacy has been used for a number of streaming, data\nstructure, and dynamic graph problems as a means of hiding the internal\nrandomness of the data structure, so that multiple possibly adaptive queries\ncan be made without sacrificing the correctness of the responses. Although\nthese works use differential privacy to show that for some problems it is\npossible to tolerate $T$ queries using $\\widetilde{O}(\\sqrt{T})$ copies of a\ndata structure, such results only apply to numerical estimation problems, and\nonly return the cost of an optimization problem rather than the solution\nitself. In this paper, we investigate the use of differential privacy for\nadaptive queries to search problems, which are significantly more challenging\nsince the responses to queries can reveal much more about the internal\nrandomness than a single numerical query. We focus on two classical search\nproblems: nearest neighbor queries and regression with arbitrary turnstile\nupdates. We identify key parameters to these problems, such as the number of\n$c$-approximate near neighbors and the matrix condition number, and use\ndifferent differential privacy techniques to design algorithms returning the\nsolution vector with memory and time depending on these parameters. We give\nalgorithms for each of these problems that achieve similar tradeoffs.",
    "updated" : "2025-06-05T18:40:33Z",
    "published" : "2025-06-05T18:40:33Z",
    "authors" : [
      {
        "name" : "Shiyuan Feng"
      },
      {
        "name" : "Ying Feng"
      },
      {
        "name" : "George Z. Li"
      },
      {
        "name" : "Zhao Song"
      },
      {
        "name" : "David P. Woodruff"
      },
      {
        "name" : "Lichen Zhang"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05421v1",
    "title" : "TRIDENT -- A Three-Tier Privacy-Preserving Propaganda Detection Model in\n  Mobile Networks using Transformers, Adversarial Learning, and Differential\n  Privacy",
    "summary" : "The proliferation of propaganda on mobile platforms raises critical concerns\naround detection accuracy and user privacy. To address this, we propose TRIDENT\n- a three-tier propaganda detection model implementing transformers,\nadversarial learning, and differential privacy which integrates syntactic\nobfuscation and label perturbation to mitigate privacy leakage while\nmaintaining propaganda detection accuracy. TRIDENT leverages multilingual\nback-translation to introduce semantic variance, character-level noise, and\nentity obfuscation for differential privacy enforcement, and combines these\ntechniques into a unified defense mechanism. Using a binary propaganda\nclassification dataset, baseline transformer models (BERT, GPT-2) we achieved\nF1 scores of 0.89 and 0.90. Applying TRIDENT's third-tier defense yields a\nreduced but effective cumulative F1 of 0.83, demonstrating strong privacy\nprotection across mobile ML deployments with minimal degradation.",
    "updated" : "2025-06-05T02:38:02Z",
    "published" : "2025-06-05T02:38:02Z",
    "authors" : [
      {
        "name" : "Al Nahian Bin Emran"
      },
      {
        "name" : "Dhiman Goswami"
      },
      {
        "name" : "Md Hasan Ullah Sadi"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  }
]