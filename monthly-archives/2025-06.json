[
  {
    "id" : "http://arxiv.org/abs/2506.02998v1",
    "title" : "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy\n  Question-Answering Systems",
    "summary" : "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information.",
    "updated" : "2025-06-03T15:32:20Z",
    "published" : "2025-06-03T15:32:20Z",
    "authors" : [
      {
        "name" : "Đorđe Klisura"
      },
      {
        "name" : "Astrid R Bernaga Torres"
      },
      {
        "name" : "Anna Karen Gárate-Escamilla"
      },
      {
        "name" : "Rajesh Roshan Biswal"
      },
      {
        "name" : "Ke Yang"
      },
      {
        "name" : "Hilal Pataci"
      },
      {
        "name" : "Anthony Rios"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02965v1",
    "title" : "Memory-Efficient and Privacy-Preserving Collaborative Training for\n  Mixture-of-Experts LLMs",
    "summary" : "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks.",
    "updated" : "2025-06-03T15:00:18Z",
    "published" : "2025-06-03T15:00:18Z",
    "authors" : [
      {
        "name" : "Ze Yu Zhang"
      },
      {
        "name" : "Bolin Ding"
      },
      {
        "name" : "Bryan Kian Hsiang Low"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02725v1",
    "title" : "Recursive Privacy-Preserving Estimation Over Markov Fading Channels",
    "summary" : "In industrial applications, the presence of moving machinery, vehicles, and\npersonnel, contributes to the dynamic nature of the wireless channel. This time\nvariability induces channel fading, which can be effectively modeled using a\nMarkov fading channel (MFC). In this paper, we investigate the problem of\nsecure state estimation for systems that communicate over a MFC in the presence\nof an eavesdropper. The objective is to enable a remote authorized user to\naccurately estimate the states of a dynamic system, while considering the\npotential interception of the sensor's packet through a wiretap channel. To\nprevent information leakage, a novel co-design strategy is established, which\ncombines a privacy-preserving mechanism with a state estimator. To implement\nour encoding scheme, a nonlinear mapping of the innovation is introduced based\non the weighted reconstructed innovation previously received by the legitimate\nuser. Corresponding to this encoding scheme, we design a recursive\nprivacy-preserving filtering algorithm to achieve accurate estimation. The\nboundedness of estimation error dynamics at the legitimate user's side is\ndiscussed and the divergence of the eavesdropper's estimation error is\nanalyzed, which demonstrates the effectiveness of our co-design strategy in\nensuring secrecy. Furthermore, a simulation example of a three-tank system is\nprovided to demonstrate the effectiveness and feasibility of our\nprivacy-preserving estimation method.",
    "updated" : "2025-06-03T10:33:49Z",
    "published" : "2025-06-03T10:33:49Z",
    "authors" : [
      {
        "name" : "Jie Huang"
      },
      {
        "name" : "Fanlin Jia"
      },
      {
        "name" : "Xiao He"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02711v1",
    "title" : "Privacy Leaks by Adversaries: Adversarial Iterations for Membership\n  Inference Attack",
    "summary" : "Membership inference attack (MIA) has become one of the most widely used and\neffective methods for evaluating the privacy risks of machine learning models.\nThese attacks aim to determine whether a specific sample is part of the model's\ntraining set by analyzing the model's output. While traditional membership\ninference attacks focus on leveraging the model's posterior output, such as\nconfidence on the target sample, we propose IMIA, a novel attack strategy that\nutilizes the process of generating adversarial samples to infer membership. We\npropose to infer the member properties of the target sample using the number of\niterations required to generate its adversarial sample. We conduct experiments\nacross multiple models and datasets, and our results demonstrate that the\nnumber of iterations for generating an adversarial sample is a reliable feature\nfor membership inference, achieving strong performance both in black-box and\nwhite-box attack scenarios. This work provides a new perspective for evaluating\nmodel privacy and highlights the potential of adversarial example-based\nfeatures for privacy leakage assessment.",
    "updated" : "2025-06-03T10:09:24Z",
    "published" : "2025-06-03T10:09:24Z",
    "authors" : [
      {
        "name" : "Jing Xue"
      },
      {
        "name" : "Zhishen Sun"
      },
      {
        "name" : "Haishan Ye"
      },
      {
        "name" : "Luo Luo"
      },
      {
        "name" : "Xiangyu Chang"
      },
      {
        "name" : "Ivor Tsang"
      },
      {
        "name" : "Guang Dai"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02563v1",
    "title" : "Privacy-Preserving Federated Convex Optimization: Balancing\n  Partial-Participation and Efficiency via Noise Cancellation",
    "summary" : "This paper tackles the challenge of achieving Differential Privacy (DP) in\nFederated Learning (FL) under partial-participation, where only a subset of the\nmachines participate in each time-step. While previous work achieved optimal\nperformance in full-participation settings, these methods struggled to extend\nto partial-participation scenarios. Our approach fills this gap by introducing\na novel noise-cancellation mechanism that preserves privacy without sacrificing\nconvergence rates or computational efficiency. We analyze our method within the\nStochastic Convex Optimization (SCO) framework and show that it delivers\noptimal performance for both homogeneous and heterogeneous data distributions.\nThis work expands the applicability of DP in FL, offering an efficient and\npractical solution for privacy-preserving learning in distributed systems with\npartial participation.",
    "updated" : "2025-06-03T07:48:35Z",
    "published" : "2025-06-03T07:48:35Z",
    "authors" : [
      {
        "name" : "Roie Reshef"
      },
      {
        "name" : "Kfir Yehuda Levy"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02422v1",
    "title" : "Enhancing Convergence, Privacy and Fairness for Wireless Personalized\n  Federated Learning: Quantization-Assisted Min-Max Fair Scheduling",
    "summary" : "Personalized federated learning (PFL) offers a solution to balancing\npersonalization and generalization by conducting federated learning (FL) to\nguide personalized learning (PL). Little attention has been given to wireless\nPFL (WPFL), where privacy concerns arise. Performance fairness of PL models is\nanother challenge resulting from communication bottlenecks in WPFL. This paper\nexploits quantization errors to enhance the privacy of WPFL and proposes a\nnovel quantization-assisted Gaussian differential privacy (DP) mechanism. We\nanalyze the convergence upper bounds of individual PL models by considering the\nimpact of the mechanism (i.e., quantization errors and Gaussian DP noises) and\nimperfect communication channels on the FL of WPFL. By minimizing the maximum\nof the bounds, we design an optimal transmission scheduling strategy that\nyields min-max fairness for WPFL with OFDMA interfaces. This is achieved by\nrevealing the nested structure of this problem to decouple it into subproblems\nsolved sequentially for the client selection, channel allocation, and power\ncontrol, and for the learning rates and PL-FL weighting coefficients.\nExperiments validate our analysis and demonstrate that our approach\nsubstantially outperforms alternative scheduling strategies by 87.08%, 16.21%,\nand 38.37% in accuracy, the maximum test loss of participating clients, and\nfairness (Jain's index), respectively.",
    "updated" : "2025-06-03T04:13:07Z",
    "published" : "2025-06-03T04:13:07Z",
    "authors" : [
      {
        "name" : "Xiyu Zhao"
      },
      {
        "name" : "Qimei Cui"
      },
      {
        "name" : "Ziqiang Du"
      },
      {
        "name" : "Weicai Li"
      },
      {
        "name" : "Xi Yu"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "Ji Zhang"
      },
      {
        "name" : "Xiaofeng Tao"
      },
      {
        "name" : "Ping Zhang"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02410v1",
    "title" : "Testing for large-dimensional covariance matrix under differential\n  privacy",
    "summary" : "The increasing prevalence of high-dimensional data across various\napplications has raised significant privacy concerns in statistical inference.\nIn this paper, we propose a differentially private integrated statistic for\ntesting large-dimensional covariance structures, enabling accurate statistical\ninsights while safeguarding privacy. First, we analyze the global sensitivity\nof sample eigenvalues for sub-Gaussian populations, where our method bypasses\nthe commonly assumed boundedness of data covariates. For sufficiently large\nsample size, the privatized statistic guarantees privacy with high probability.\nFurthermore, when the ratio of dimension to sample size, $d/n \\to y \\in (0,\n\\infty)$, the privatized test is asymptotically distribution-free with\nwell-known critical values, and detects the local alternative hypotheses\ndistinct from the null at the fastest rate of $1/\\sqrt{n}$. Extensive numerical\nstudies on synthetic and real data showcase the validity and powerfulness of\nour proposed method.",
    "updated" : "2025-06-03T03:53:51Z",
    "published" : "2025-06-03T03:53:51Z",
    "authors" : [
      {
        "name" : "Shiwei Sang"
      },
      {
        "name" : "Yicheng Zeng"
      },
      {
        "name" : "Xuehu Zhu"
      },
      {
        "name" : "Shurong Zheng"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02156v1",
    "title" : "Mitigating Data Poisoning Attacks to Local Differential Privacy",
    "summary" : "The distributed nature of local differential privacy (LDP) invites data\npoisoning attacks and poses unforeseen threats to the underlying LDP-supported\napplications. In this paper, we propose a comprehensive mitigation framework\nfor popular frequency estimation, which contains a suite of novel defenses,\nincluding malicious user detection, attack pattern recognition, and damaged\nutility recovery. In addition to existing attacks, we explore new adaptive\nadversarial activities for our mitigation design. For detection, we present a\nnew method to precisely identify bogus reports and thus LDP aggregation can be\nperformed over the ``clean'' data. When the attack behavior becomes stealthy\nand direct filtering out malicious users is difficult, we further propose a\ndetection that can effectively recognize hidden adversarial patterns, thus\nfacilitating the decision-making of service providers. These detection methods\nrequire no additional data and attack information and incur minimal\ncomputational cost. Our experiment demonstrates their excellent performance and\nsubstantial improvement over previous work in various settings. In addition, we\nconduct an empirical analysis of LDP post-processing for corrupted data\nrecovery and propose a new post-processing method, through which we reveal new\ninsights into protocol recommendations in practice and key design principles\nfor future research.",
    "updated" : "2025-06-02T18:37:15Z",
    "published" : "2025-06-02T18:37:15Z",
    "authors" : [
      {
        "name" : "Xiaolin Li"
      },
      {
        "name" : "Ninghui Li"
      },
      {
        "name" : "Boyang Wang"
      },
      {
        "name" : "Wenhai Sun"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01907v1",
    "title" : "SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data",
    "summary" : "Privacy-preserving data publication, including synthetic data sharing, often\nexperiences trade-offs between privacy and utility. Synthetic data is generally\nmore effective than data anonymization in balancing this trade-off, however,\nnot without its own challenges. Synthetic data produced by generative models\ntrained on source data may inadvertently reveal information about outliers.\nTechniques specifically designed for preserving privacy, such as introducing\nnoise to satisfy differential privacy, often incur unpredictable and\nsignificant losses in utility. In this work we show that, with the right\nmechanism of synthetic data generation, we can achieve strong privacy\nprotection without significant utility loss. Synthetic data generators\nproducing contracting data patterns, such as Synthetic Minority Over-sampling\nTechnique (SMOTE), can enhance a differentially private data generator,\nleveraging the strengths of both. We prove in theory and through empirical\ndemonstration that this SMOTE-DP technique can produce synthetic data that not\nonly ensures robust privacy protection but maintains utility in downstream\nlearning tasks.",
    "updated" : "2025-06-02T17:27:10Z",
    "published" : "2025-06-02T17:27:10Z",
    "authors" : [
      {
        "name" : "Yan Zhou"
      },
      {
        "name" : "Bradley Malin"
      },
      {
        "name" : "Murat Kantarcioglu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01425v1",
    "title" : "CSVAR: Enhancing Visual Privacy in Federated Learning via Adaptive\n  Shuffling Against Overfitting",
    "summary" : "Although federated learning preserves training data within local privacy\ndomains, the aggregated model parameters may still reveal private\ncharacteristics. This vulnerability stems from clients' limited training data,\nwhich predisposes models to overfitting. Such overfitting enables models to\nmemorize distinctive patterns from training samples, thereby amplifying the\nsuccess probability of privacy attacks like membership inference. To enhance\nvisual privacy protection in FL, we present CSVAR(Channel-Wise Spatial Image\nShuffling with Variance-Guided Adaptive Region Partitioning), a novel image\nshuffling framework to generate obfuscated images for secure data transmission\nand each training epoch, addressing both overfitting-induced privacy leaks and\nraw image transmission risks. CSVAR adopts region-variance as the metric to\nmeasure visual privacy sensitivity across image regions. Guided by this, CSVAR\nadaptively partitions each region into multiple blocks, applying fine-grained\npartitioning to privacy-sensitive regions with high region-variances for\nenhancing visual privacy protection and coarse-grained partitioning to\nprivacy-insensitive regions for balancing model utility. In each region, CSVAR\nthen shuffles between blocks in both the spatial domains and chromatic channels\nto hide visual spatial features and disrupt color distribution. Experimental\nevaluations conducted on diverse real-world datasets demonstrate that CSVAR is\ncapable of generating visually obfuscated images that exhibit high perceptual\nambiguity to human eyes, simultaneously mitigating the effectiveness of\nadversarial data reconstruction attacks and achieving a good trade-off between\nvisual privacy protection and model utility.",
    "updated" : "2025-06-02T08:30:12Z",
    "published" : "2025-06-02T08:30:12Z",
    "authors" : [
      {
        "name" : "Zhuo Chen"
      },
      {
        "name" : "Zhenya Ma"
      },
      {
        "name" : "Yan Zhang"
      },
      {
        "name" : "Donghua Cai"
      },
      {
        "name" : "Ye Zhang"
      },
      {
        "name" : "Qiushi Li"
      },
      {
        "name" : "Yongheng Deng"
      },
      {
        "name" : "Ye Guo"
      },
      {
        "name" : "Ju Ren"
      },
      {
        "name" : " Xuemin"
      },
      {
        "name" : " Shen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01325v1",
    "title" : "Understanding the Identity-Transformation Approach in OIDC-Compatible\n  Privacy-Preserving SSO Services",
    "summary" : "OpenID Connect (OIDC) enables a user with commercial-off-the-shelf browsers\nto log into multiple websites, called relying parties (RPs), by her username\nand credential set up in another trusted web system, called the identity\nprovider (IdP). Identity transformations are proposed in UppreSSO to provide\nOIDC-compatible SSO services, preventing both IdP-based login tracing and\nRP-based identity linkage. While security and privacy of SSO services in\nUppreSSO have been proved, several essential issues of this\nidentity-transformation approach are not well studied. In this paper, we\ncomprehensively investigate the approach as below. Firstly, several suggestions\nfor the efficient integration of identity transformations in OIDC-compatible\nSSO are explained. Then, we uncover the relationship between\nidentity-transformations in SSO and oblivious pseudo-random functions (OPRFs),\nand present two variations of the properties required for SSO security as well\nas the privacy requirements, to analyze existing OPRF protocols. Finally, new\nidentity transformations different from those designed in UppreSSO, are\nconstructed based on OPRFs, satisfying different variations of SSO security\nrequirements. To the best of our knowledge, this is the first time to uncover\nthe relationship between identity transformations in OIDC-compatible\nprivacy-preserving SSO services and OPRFs, and prove the SSO-related properties\n(i.e., key-identifier freeness, RP designation and user identification) of OPRF\nprotocols, in addition to the basic properties of correctness, obliviousness\nand pseudo-randomness.",
    "updated" : "2025-06-02T05:11:01Z",
    "published" : "2025-06-02T05:11:01Z",
    "authors" : [
      {
        "name" : "Jingqiang Lin"
      },
      {
        "name" : "Baitao Zhang"
      },
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Quanwei Cai"
      },
      {
        "name" : "Jiwu Jing"
      },
      {
        "name" : "Huiyang He"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02063v1",
    "title" : "Privacy-Aware, Public-Aligned: Embedding Risk Detection and Public\n  Values into Scalable Clinical Text De-Identification for Trusted Research\n  Environments",
    "summary" : "Clinical free-text data offers immense potential to improve population health\nresearch such as richer phenotyping, symptom tracking, and contextual\nunderstanding of patient care. However, these data present significant privacy\nrisks due to the presence of directly or indirectly identifying information\nembedded in unstructured narratives. While numerous de-identification tools\nhave been developed, few have been tested on real-world, heterogeneous datasets\nat scale or assessed for governance readiness. In this paper, we synthesise our\nfindings from previous studies examining the privacy-risk landscape across\nmultiple document types and NHS data providers in Scotland. We characterise how\ndirect and indirect identifiers vary by record type, clinical setting, and data\nflow, and show how changes in documentation practice can degrade model\nperformance over time. Through public engagement, we explore societal\nexpectations around the safe use of clinical free text and reflect these in the\ndesign of a prototype privacy-risk management tool to support transparent,\nauditable decision-making. Our findings highlight that privacy risk is\ncontext-dependent and cumulative, underscoring the need for adaptable, hybrid\nde-identification approaches that combine rule-based precision with contextual\nunderstanding. We offer a comprehensive view of the challenges and\nopportunities for safe, scalable reuse of clinical free-text within Trusted\nResearch Environments and beyond, grounded in both technical evidence and\npublic perspectives on responsible data use.",
    "updated" : "2025-06-01T17:45:57Z",
    "published" : "2025-06-01T17:45:57Z",
    "authors" : [
      {
        "name" : "Arlene Casey"
      },
      {
        "name" : "Stuart Dunbar"
      },
      {
        "name" : "Franz Gruber"
      },
      {
        "name" : "Samuel McInerney"
      },
      {
        "name" : "Matúš Falis"
      },
      {
        "name" : "Pamela Linksted"
      },
      {
        "name" : "Katie Wilde"
      },
      {
        "name" : "Kathy Harrison"
      },
      {
        "name" : "Alison Hamilton"
      },
      {
        "name" : "Christian Cole"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.01072v1",
    "title" : "IDCloak: A Practical Secure Multi-party Dataset Join Framework for\n  Vertical Privacy-preserving Machine Learning",
    "summary" : "Vertical privacy-preserving machine learning (vPPML) enables multiple parties\nto train models on their vertically distributed datasets while keeping datasets\nprivate. In vPPML, it is critical to perform the secure dataset join, which\naligns features corresponding to intersection IDs across datasets and forms a\nsecret-shared and joint training dataset. However, existing methods for this\nstep could be impractical due to: (1) they are insecure when they expose\nintersection IDs; or (2) they rely on a strong trust assumption requiring a\nnon-colluding auxiliary server; or (3) they are limited to the two-party\nsetting.\n  This paper proposes IDCloak, the first practical secure multi-party dataset\njoin framework for vPPML that keeps IDs private without a non-colluding\nauxiliary server. IDCloak consists of two protocols: (1) a circuit-based\nmulti-party private set intersection protocol (cmPSI), which obtains\nsecret-shared flags indicating intersection IDs via an optimized communication\nstructure combining OKVS and OPRF; (2) a secure multi-party feature alignment\nprotocol, which obtains the secret-shared and joint dataset using secret-shared\nflags, via our proposed efficient secure shuffle protocol. Experiments show\nthat: (1) compared to the state-of-the-art secure two-party dataset join\nframework (iPrivjoin), IDCloak demonstrates higher efficiency in the two-party\nsetting and comparable performance when the party number increases; (2)\ncompared to the state-of-the-art cmPSI protocol under honest majority, our\nproposed cmPSI protocol provides a stronger security guarantee (dishonest\nmajority) while improving efficiency by up to $7.78\\times$ in time and\n$8.73\\times$ in communication sizes; (3) our proposed secure shuffle protocol\noutperforms the state-of-the-art shuffle protocol by up to $138.34\\times$ in\ntime and $132.13\\times$ in communication sizes.",
    "updated" : "2025-06-01T16:20:39Z",
    "published" : "2025-06-01T16:20:39Z",
    "authors" : [
      {
        "name" : "Shuyu Chen"
      },
      {
        "name" : "Guopeng Lin"
      },
      {
        "name" : "Haoyu Niu"
      },
      {
        "name" : "Lushan Song"
      },
      {
        "name" : "Chengxun Hong"
      },
      {
        "name" : "Weili Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00759v1",
    "title" : "Understanding and Mitigating Cross-lingual Privacy Leakage via\n  Language-specific and Universal Privacy Neurons",
    "summary" : "Large Language Models (LLMs) trained on massive data capture rich information\nembedded in the training data. However, this also introduces the risk of\nprivacy leakage, particularly involving personally identifiable information\n(PII). Although previous studies have shown that this risk can be mitigated\nthrough methods such as privacy neurons, they all assume that both the\n(sensitive) training data and user queries are in English. We show that they\ncannot defend against the privacy leakage in cross-lingual contexts: even if\nthe training data is exclusively in one language, these (private) models may\nstill reveal private information when queried in another language. In this\nwork, we first investigate the information flow of cross-lingual privacy\nleakage to give a better understanding. We find that LLMs process private\ninformation in the middle layers, where representations are largely shared\nacross languages. The risk of leakage peaks when converted to a\nlanguage-specific space in later layers. Based on this, we identify\nprivacy-universal neurons and language-specific privacy neurons.\nPrivacy-universal neurons influence privacy leakage across all languages, while\nlanguage-specific privacy neurons are only related to specific languages. By\ndeactivating these neurons, the cross-lingual privacy leakage risk is reduced\nby 23.3%-31.6%.",
    "updated" : "2025-06-01T00:10:30Z",
    "published" : "2025-06-01T00:10:30Z",
    "authors" : [
      {
        "name" : "Wenshuo Dong"
      },
      {
        "name" : "Qingsong Yang"
      },
      {
        "name" : "Shu Yang"
      },
      {
        "name" : "Lijie Hu"
      },
      {
        "name" : "Meng Ding"
      },
      {
        "name" : "Wanyu Lin"
      },
      {
        "name" : "Tianhang Zheng"
      },
      {
        "name" : "Di Wang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.04036v1",
    "title" : "Privacy and Security Threat for OpenAI GPTs",
    "summary" : "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications.",
    "updated" : "2025-06-04T14:58:29Z",
    "published" : "2025-06-04T14:58:29Z",
    "authors" : [
      {
        "name" : "Wei Wenying"
      },
      {
        "name" : "Zhao Kaifa"
      },
      {
        "name" : "Xue Lei"
      },
      {
        "name" : "Fan Ming"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.03870v1",
    "title" : "Evaluating Apple Intelligence's Writing Tools for Privacy Against Large\n  Language Model-Based Inference Attacks: Insights from Early Datasets",
    "summary" : "The misuse of Large Language Models (LLMs) to infer emotions from text for\nmalicious purposes, known as emotion inference attacks, poses a significant\nthreat to user privacy. In this paper, we investigate the potential of Apple\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\nmitigate these risks through text modifications such as rewriting and tone\nadjustment. By developing early novel datasets specifically for this purpose,\nwe empirically assess how different text modifications influence LLM-based\ndetection. This capability suggests strong potential for Apple Intelligence's\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\nfor future adaptive rewriting systems capable of dynamically neutralizing\nsensitive emotional content to enhance user privacy. To the best of our\nknowledge, this research provides the first empirical analysis of Apple\nIntelligence's text-modification tools within a privacy-preservation context\nwith the broader goal of developing on-device, user-centric privacy-preserving\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\nsystems.",
    "updated" : "2025-06-04T12:01:17Z",
    "published" : "2025-06-04T12:01:17Z",
    "authors" : [
      {
        "name" : "Mohd. Farhan Israk Soumik"
      },
      {
        "name" : "Syed Mhamudul Hasan"
      },
      {
        "name" : "Abdur R. Shahid"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.03618v1",
    "title" : "GCFL: A Gradient Correction-based Federated Learning Framework for\n  Privacy-preserving CPSS",
    "summary" : "Federated learning, as a distributed architecture, shows great promise for\napplications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the\nprivacy risks inherent in CPSS, the integration of differential privacy with\nfederated learning has attracted considerable attention. Existing research\nmainly focuses on dynamically adjusting the noise added or discarding certain\ngradients to mitigate the noise introduced by differential privacy. However,\nthese approaches fail to remove the noise that hinders convergence and correct\nthe gradients affected by the noise, which significantly reduces the accuracy\nof model classification. To overcome these challenges, this paper proposes a\nnovel framework for differentially private federated learning that balances\nrigorous privacy guarantees with accuracy by introducing a server-side gradient\ncorrection mechanism. Specifically, after clients perform gradient clipping and\nnoise perturbation, our framework detects deviations in the noisy local\ngradients and employs a projection mechanism to correct them, mitigating the\nnegative impact of noise. Simultaneously, gradient projection promotes the\nalignment of gradients from different clients and guides the model towards\nconvergence to a global optimum. We evaluate our framework on several benchmark\ndatasets, and the experimental results demonstrate that it achieves\nstate-of-the-art performance under the same privacy budget.",
    "updated" : "2025-06-04T06:52:37Z",
    "published" : "2025-06-04T06:52:37Z",
    "authors" : [
      {
        "name" : "Jiayi Wan"
      },
      {
        "name" : "Xiang Zhu"
      },
      {
        "name" : "Fanzhen Liu"
      },
      {
        "name" : "Wei Fan"
      },
      {
        "name" : "Xiaolong Xu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02965v2",
    "title" : "PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training\n  for Mixture-of-Experts LLMs",
    "summary" : "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks.",
    "updated" : "2025-06-04T05:38:31Z",
    "published" : "2025-06-03T15:00:18Z",
    "authors" : [
      {
        "name" : "Ze Yu Zhang"
      },
      {
        "name" : "Bolin Ding"
      },
      {
        "name" : "Bryan Kian Hsiang Low"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05290v1",
    "title" : "Big Bird: Privacy Budget Management for W3C's Privacy-Preserving\n  Attribution API",
    "summary" : "Privacy-preserving advertising APIs like Privacy-Preserving Attribution (PPA)\nare designed to enhance web privacy while enabling effective ad measurement.\nPPA offers an alternative to cross-site tracking with encrypted reports\ngoverned by differential privacy (DP), but current designs lack a principled\napproach to privacy budget management, creating uncertainty around critical\ndesign decisions. We present Big Bird, a privacy budget manager for PPA that\nclarifies per-site budget semantics and introduces a global budgeting system\ngrounded in resource isolation principles. Big Bird enforces utility-preserving\nlimits via quota budgets and improves global budget utilization through a novel\nbatched scheduling algorithm. Together, these mechanisms establish a robust\nfoundation for enforcing privacy protections in adversarial environments. We\nimplement Big Bird in Firefox and evaluate it on real-world ad data,\ndemonstrating its resilience and effectiveness.",
    "updated" : "2025-06-05T17:45:13Z",
    "published" : "2025-06-05T17:45:13Z",
    "authors" : [
      {
        "name" : "Pierre Tholoniat"
      },
      {
        "name" : "Alison Caulfield"
      },
      {
        "name" : "Giorgio Cavicchioli"
      },
      {
        "name" : "Mark Chen"
      },
      {
        "name" : "Nikos Goutzoulias"
      },
      {
        "name" : "Benjamin Case"
      },
      {
        "name" : "Asaf Cidon"
      },
      {
        "name" : "Roxana Geambasu"
      },
      {
        "name" : "Mathias Lécuyer"
      },
      {
        "name" : "Martin Thomson"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05101v1",
    "title" : "Privacy Amplification Through Synthetic Data: Insights from Linear\n  Regression",
    "summary" : "Synthetic data inherits the differential privacy guarantees of the model used\nto generate it. Additionally, synthetic data may benefit from privacy\namplification when the generative model is kept hidden. While empirical studies\nsuggest this phenomenon, a rigorous theoretical understanding is still lacking.\nIn this paper, we investigate this question through the well-understood\nframework of linear regression. First, we establish negative results showing\nthat if an adversary controls the seed of the generative model, a single\nsynthetic data point can leak as much information as releasing the model\nitself. Conversely, we show that when synthetic data is generated from random\ninputs, releasing a limited number of synthetic data points amplifies privacy\nbeyond the model's inherent guarantees. We believe our findings in linear\nregression can serve as a foundation for deriving more general bounds in the\nfuture.",
    "updated" : "2025-06-05T14:44:15Z",
    "published" : "2025-06-05T14:44:15Z",
    "authors" : [
      {
        "name" : "Clément Pierquin"
      },
      {
        "name" : "Aurélien Bellet"
      },
      {
        "name" : "Marc Tommasi"
      },
      {
        "name" : "Matthieu Boussard"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.04978v1",
    "title" : "Evaluating the Impact of Privacy-Preserving Federated Learning on CAN\n  Intrusion Detection",
    "summary" : "The challenges derived from the data-intensive nature of machine learning in\nconjunction with technologies that enable novel paradigms such as V2X and the\npotential offered by 5G communication, allow and justify the deployment of\nFederated Learning (FL) solutions in the vehicular intrusion detection domain.\nIn this paper, we investigate the effects of integrating FL strategies into the\nmachine learning-based intrusion detection process for on-board vehicular\nnetworks. Accordingly, we propose a FL implementation of a state-of-the-art\nIntrusion Detection System (IDS) for Controller Area Network (CAN), based on\nLSTM autoencoders. We thoroughly evaluate its detection efficiency and\ncommunication overhead, comparing it to a centralized version of the same\nalgorithm, thereby presenting it as a feasible solution.",
    "updated" : "2025-06-05T12:49:22Z",
    "published" : "2025-06-05T12:49:22Z",
    "authors" : [
      {
        "name" : "Gabriele Digregorio"
      },
      {
        "name" : "Elisabetta Cainazzo"
      },
      {
        "name" : "Stefano Longari"
      },
      {
        "name" : "Michele Carminati"
      },
      {
        "name" : "Stefano Zanero"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06124v1",
    "title" : "PrivTru: A Privacy-by-Design Data Trustee Minimizing Information Leakage",
    "summary" : "Data trustees serve as intermediaries that facilitate secure data sharing\nbetween independent parties. This paper offers a technical perspective on Data\ntrustees, guided by privacy-by-design principles. We introduce PrivTru, an\ninstantiation of a data trustee that provably achieves optimal privacy\nproperties. Therefore, PrivTru calculates the minimal amount of information the\ndata trustee needs to request from data sources to respond to a given query.\nOur analysis shows that PrivTru minimizes information leakage to the data\ntrustee, regardless of the trustee's prior knowledge, while preserving the\nutility of the data.",
    "updated" : "2025-06-06T14:33:59Z",
    "published" : "2025-06-06T14:33:59Z",
    "authors" : [
      {
        "name" : "Lukas Gehring"
      },
      {
        "name" : "Florian Tschorsch"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06062v1",
    "title" : "Minoritised Ethnic People's Security and Privacy Concerns and Responses\n  towards Essential Online Services",
    "summary" : "Minoritised ethnic people are marginalised in society, and therefore at a\nhigher risk of adverse online harms, including those arising from the loss of\nsecurity and privacy of personal data. Despite this, there has been very little\nresearch focused on minoritised ethnic people's security and privacy concerns,\nattitudes, and behaviours. In this work, we provide the results of one of the\nfirst studies in this regard. We explore minoritised ethnic people's\nexperiences of using essential online services across three sectors: health,\nsocial housing, and energy, their security and privacy-related concerns, and\nresponses towards these services. We conducted a thematic analysis of 44\nsemi-structured interviews with people of various reported minoritised\nethnicities in the UK. Privacy concerns and lack of control over personal data\nemerged as a major theme, with many interviewees considering privacy as their\nmost significant concern when using online services. Several creative tactics\nto exercise some agency were reported, including selective and inconsistent\ndisclosure of personal data. A core concern about how data may be used was\ndriven by a fear of repercussions, including penalisation and discrimination,\ninfluenced by prior experiences of institutional and online racism. The\nincreased concern and potential for harm resulted in minoritised ethnic people\ngrappling with a higher-stakes dilemma of whether to disclose personal\ninformation online or not. Furthermore, trust in institutions, or lack thereof,\nwas found to be embedded throughout as a basis for adapting behaviour. We draw\non our results to provide lessons learned for the design of more inclusive,\nmarginalisation-aware, and privacy-preserving online services.",
    "updated" : "2025-06-06T13:17:44Z",
    "published" : "2025-06-06T13:17:44Z",
    "authors" : [
      {
        "name" : "Aunam Quyoum"
      },
      {
        "name" : "Mark Wong"
      },
      {
        "name" : "Sebati Ghosh"
      },
      {
        "name" : "Siamak F. Shahandashti"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CR",
      "cs.HC",
      "K.4.2; H.1.2; K.4.1; K.6.5; J.4"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05908v1",
    "title" : "QualitEye: Public and Privacy-preserving Gaze Data Quality Verification",
    "summary" : "Gaze-based applications are increasingly advancing with the availability of\nlarge datasets but ensuring data quality presents a substantial challenge when\ncollecting data at scale. It further requires different parties to collaborate,\ntherefore, privacy concerns arise. We propose QualitEye--the first method for\nverifying image-based gaze data quality. QualitEye employs a new semantic\nrepresentation of eye images that contains the information required for\nverification while excluding irrelevant information for better domain\nadaptation. QualitEye covers a public setting where parties can freely exchange\ndata and a privacy-preserving setting where parties cannot reveal their raw\ndata nor derive gaze features/labels of others with adapted private set\nintersection protocols. We evaluate QualitEye on the MPIIFaceGaze and\nGazeCapture datasets and achieve a high verification performance (with a small\noverhead in runtime for privacy-preserving versions). Hence, QualitEye paves\nthe way for new gaze analysis methods at the intersection of machine learning,\nhuman-computer interaction, and cryptography.",
    "updated" : "2025-06-06T09:27:04Z",
    "published" : "2025-06-06T09:27:04Z",
    "authors" : [
      {
        "name" : "Mayar Elfares"
      },
      {
        "name" : "Pascal Reisert"
      },
      {
        "name" : "Ralf Küsters"
      },
      {
        "name" : "Andreas Bulling"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05743v1",
    "title" : "When Better Features Mean Greater Risks: The Performance-Privacy\n  Trade-Off in Contrastive Learning",
    "summary" : "With the rapid advancement of deep learning technology, pre-trained encoder\nmodels have demonstrated exceptional feature extraction capabilities, playing a\npivotal role in the research and application of deep learning. However, their\nwidespread use has raised significant concerns about the risk of training data\nprivacy leakage. This paper systematically investigates the privacy threats\nposed by membership inference attacks (MIAs) targeting encoder models, focusing\non contrastive learning frameworks. Through experimental analysis, we reveal\nthe significant impact of model architecture complexity on membership privacy\nleakage: As more advanced encoder frameworks improve feature-extraction\nperformance, they simultaneously exacerbate privacy-leakage risks. Furthermore,\nthis paper proposes a novel membership inference attack method based on the\np-norm of feature vectors, termed the Embedding Lp-Norm Likelihood Attack\n(LpLA). This method infers membership status, by leveraging the statistical\ndistribution characteristics of the p-norm of feature vectors. Experimental\nresults across multiple datasets and model architectures demonstrate that LpLA\noutperforms existing methods in attack performance and robustness, particularly\nunder limited attack knowledge and query volumes. This study not only uncovers\nthe potential risks of privacy leakage in contrastive learning frameworks, but\nalso provides a practical basis for privacy protection research in encoder\nmodels. We hope that this work will draw greater attention to the privacy risks\nassociated with self-supervised learning models and shed light on the\nimportance of a balance between model utility and training data privacy. Our\ncode is publicly available at: https://github.com/SeroneySun/LpLA_code.",
    "updated" : "2025-06-06T05:03:29Z",
    "published" : "2025-06-06T05:03:29Z",
    "authors" : [
      {
        "name" : "Ruining Sun"
      },
      {
        "name" : "Hongsheng Hu"
      },
      {
        "name" : "Wei Luo"
      },
      {
        "name" : "Zhaoxi Zhang"
      },
      {
        "name" : "Yanjun Zhang"
      },
      {
        "name" : "Haizhuan Yuan"
      },
      {
        "name" : "Leo Yu Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05683v1",
    "title" : "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation\n  Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence\n  in AR/VR/MR",
    "summary" : "Extended reality (XR) systems, which consist of virtual reality (VR),\naugmented reality (AR), and mixed reality (XR), offer a transformative\ninterface for immersive, multi-modal, and embodied human-computer interaction.\nIn this paper, we envision that multi-modal multi-task (M3T) federated\nfoundation models (FedFMs) can offer transformative capabilities for XR systems\nthrough integrating the representational strength of M3T foundation models\n(FMs) with the privacy-preserving model training principles of federated\nlearning (FL). We present a modular architecture for FedFMs, which entails\ndifferent coordination paradigms for model training and aggregations. Central\nto our vision is the codification of XR challenges that affect the\nimplementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality\ndiversity, (2) Hardware heterogeneity and system-level constraints, (3)\nInteractivity and embodied personalization, (4) Functional/task variability,\nand (5) Temporality and environmental variability. We illustrate the\nmanifestation of these dimensions across a set of emerging and anticipated\napplications of XR systems. Finally, we propose evaluation metrics, dataset\nrequirements, and design tradeoffs necessary for the development of\nresource-aware FedFMs in XR. This perspective aims to chart the technical and\nconceptual foundations for context-aware privacy-preserving intelligence in the\nnext generation of XR systems.",
    "updated" : "2025-06-06T02:23:42Z",
    "published" : "2025-06-06T02:23:42Z",
    "authors" : [
      {
        "name" : "Fardis Nadimi"
      },
      {
        "name" : "Payam Abdisarabshali"
      },
      {
        "name" : "Kasra Borazjani"
      },
      {
        "name" : "Jacob Chakareski"
      },
      {
        "name" : "Seyyedali Hosseinalipour"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05503v1",
    "title" : "On Differential Privacy for Adaptively Solving Search Problems via\n  Sketching",
    "summary" : "Recently differential privacy has been used for a number of streaming, data\nstructure, and dynamic graph problems as a means of hiding the internal\nrandomness of the data structure, so that multiple possibly adaptive queries\ncan be made without sacrificing the correctness of the responses. Although\nthese works use differential privacy to show that for some problems it is\npossible to tolerate $T$ queries using $\\widetilde{O}(\\sqrt{T})$ copies of a\ndata structure, such results only apply to numerical estimation problems, and\nonly return the cost of an optimization problem rather than the solution\nitself. In this paper, we investigate the use of differential privacy for\nadaptive queries to search problems, which are significantly more challenging\nsince the responses to queries can reveal much more about the internal\nrandomness than a single numerical query. We focus on two classical search\nproblems: nearest neighbor queries and regression with arbitrary turnstile\nupdates. We identify key parameters to these problems, such as the number of\n$c$-approximate near neighbors and the matrix condition number, and use\ndifferent differential privacy techniques to design algorithms returning the\nsolution vector with memory and time depending on these parameters. We give\nalgorithms for each of these problems that achieve similar tradeoffs.",
    "updated" : "2025-06-05T18:40:33Z",
    "published" : "2025-06-05T18:40:33Z",
    "authors" : [
      {
        "name" : "Shiyuan Feng"
      },
      {
        "name" : "Ying Feng"
      },
      {
        "name" : "George Z. Li"
      },
      {
        "name" : "Zhao Song"
      },
      {
        "name" : "David P. Woodruff"
      },
      {
        "name" : "Lichen Zhang"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05421v1",
    "title" : "TRIDENT -- A Three-Tier Privacy-Preserving Propaganda Detection Model in\n  Mobile Networks using Transformers, Adversarial Learning, and Differential\n  Privacy",
    "summary" : "The proliferation of propaganda on mobile platforms raises critical concerns\naround detection accuracy and user privacy. To address this, we propose TRIDENT\n- a three-tier propaganda detection model implementing transformers,\nadversarial learning, and differential privacy which integrates syntactic\nobfuscation and label perturbation to mitigate privacy leakage while\nmaintaining propaganda detection accuracy. TRIDENT leverages multilingual\nback-translation to introduce semantic variance, character-level noise, and\nentity obfuscation for differential privacy enforcement, and combines these\ntechniques into a unified defense mechanism. Using a binary propaganda\nclassification dataset, baseline transformer models (BERT, GPT-2) we achieved\nF1 scores of 0.89 and 0.90. Applying TRIDENT's third-tier defense yields a\nreduced but effective cumulative F1 of 0.83, demonstrating strong privacy\nprotection across mobile ML deployments with minimal degradation.",
    "updated" : "2025-06-05T02:38:02Z",
    "published" : "2025-06-05T02:38:02Z",
    "authors" : [
      {
        "name" : "Al Nahian Bin Emran"
      },
      {
        "name" : "Dhiman Goswami"
      },
      {
        "name" : "Md Hasan Ullah Sadi"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07957v1",
    "title" : "Understanding the Error Sensitivity of Privacy-Aware Computing",
    "summary" : "Homomorphic Encryption (HE) enables secure computation on encrypted data\nwithout decryption, allowing a great opportunity for privacy-preserving\ncomputation. In particular, domains such as healthcare, finance, and\ngovernment, where data privacy and security are of utmost importance, can\nbenefit from HE by enabling third-party computation and services on sensitive\ndata. In other words, HE constitutes the \"Holy Grail\" of cryptography: data\nremains encrypted all the time, being protected while in use.\n  HE's security guarantees rely on noise added to data to make relatively\nsimple problems computationally intractable. This error-centric intrinsic HE\nmechanism generates new challenges related to the fault tolerance and\nrobustness of HE itself: hardware- and software-induced errors during HE\noperation can easily evade traditional error detection and correction\nmechanisms, resulting in silent data corruption (SDC).\n  In this work, we motivate a thorough discussion regarding the sensitivity of\nHE applications to bit faults and provide a detailed error characterization\nstudy of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes\ndue to its fixed-point arithmetic support for AI and machine learning\napplications. We also delve into the impact of the residue number system (RNS)\nand the number theoretic transform (NTT), two widely adopted HE optimization\ntechniques, on CKKS' error sensitivity. To the best of our knowledge, this is\nthe first work that looks into the robustness and error sensitivity of\nhomomorphic encryption and, as such, it can pave the way for critical future\nwork in this area.",
    "updated" : "2025-06-09T17:27:40Z",
    "published" : "2025-06-09T17:27:40Z",
    "authors" : [
      {
        "name" : "Matías Mazzanti"
      },
      {
        "name" : "Esteban Mocskos"
      },
      {
        "name" : "Augusto Vega"
      },
      {
        "name" : "Pradip Bose"
      }
    ],
    "categories" : [
      "cs.AR",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07868v1",
    "title" : "Securing Unbounded Differential Privacy Against Timing Attacks",
    "summary" : "Recent works have started to theoretically investigate how we can protect\ndifferentially private programs against timing attacks, by making the joint\ndistribution the output and the runtime differentially private (JOT-DP).\nHowever, the existing approaches to JOT-DP have some limitations, particularly\nin the setting of unbounded DP (which protects the size of the dataset and\napplies to arbitrarily large datasets). First, the known conversion of pure DP\nprograms to pure JOT-DP programs in the unbounded setting (a) incurs a constant\nadditive increase in error probability (and thus does not provide vanishing\nerror as $n\\to\\infty$) (b) produces JOT-DP programs that fail to preserve the\ncomputational efficiency of the original pure DP program and (c) is analyzed in\na toy computational model in which the runtime is defined to be the number of\ncoin flips. In this work, we overcome these limitations. Specifically, we show\nthat the error required for pure JOT-DP in the unbounded setting depends on the\nmodel of computation. In a randomized RAM model where the dataset size $n$ is\ngiven (or can be computed in constant time) and we can generate random numbers\n(not just random bits) in constant time, polynomially small error probability\nis necessary and sufficient. If $n$ is not given or we only have a random-bit\ngenerator, an (arbitrarily small) constant error probability is necessary and\nsufficient. The aforementioned positive results are proven by efficient\nprocedures to convert any pure JOT-DP program $P$ in the upper-bounded setting\nto a pure JOT-DP program $P'$ in the unbounded setting, such that the output\ndistribution of $P'$ is $\\gamma$-close in total variation distance to that of\n$P$, where $\\gamma$ is either an arbitrarily small constant or polynomially\nsmall, depending on the model of computation.",
    "updated" : "2025-06-09T15:35:15Z",
    "published" : "2025-06-09T15:35:15Z",
    "authors" : [
      {
        "name" : "Zachary Ratliff"
      },
      {
        "name" : "Salil Vadhan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07605v1",
    "title" : "TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in\n  Federated Tree-Based Systems",
    "summary" : "Federated Learning has emerged as a privacy-oriented alternative to\ncentralized Machine Learning, enabling collaborative model training without\ndirect data sharing. While extensively studied for neural networks, the\nsecurity and privacy implications of tree-based models remain underexplored.\nThis work introduces TimberStrike, an optimization-based dataset reconstruction\nattack targeting horizontally federated tree-based models. Our attack, carried\nout by a single client, exploits the discrete nature of decision trees by using\nsplit values and decision paths to infer sensitive training data from other\nclients. We evaluate TimberStrike on State-of-the-Art federated gradient\nboosting implementations across multiple frameworks, including Flower, NVFlare,\nand FedTree, demonstrating their vulnerability to privacy breaches. On a\npublicly available stroke prediction dataset, TimberStrike consistently\nreconstructs between 73.05% and 95.63% of the target dataset across all\nimplementations. We further analyze Differential Privacy, showing that while it\npartially mitigates the attack, it also significantly degrades model\nperformance. Our findings highlight the need for privacy-preserving mechanisms\nspecifically designed for tree-based Federated Learning systems, and we provide\npreliminary insights into their design.",
    "updated" : "2025-06-09T10:06:03Z",
    "published" : "2025-06-09T10:06:03Z",
    "authors" : [
      {
        "name" : "Marco Di Gennaro"
      },
      {
        "name" : "Giovanni De Lucia"
      },
      {
        "name" : "Stefano Longari"
      },
      {
        "name" : "Stefano Zanero"
      },
      {
        "name" : "Michele Carminati"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07555v1",
    "title" : "Synthesize Privacy-Preserving High-Resolution Images via Private Textual\n  Intermediaries",
    "summary" : "Generating high fidelity, differentially private (DP) synthetic images offers\na promising route to share and analyze sensitive visual data without\ncompromising individual privacy. However, existing DP image synthesis methods\nstruggle to produce high resolution outputs that faithfully capture the\nstructure of the original data. In this paper, we introduce a novel method,\nreferred to as Synthesis via Private Textual Intermediaries (SPTI), that can\ngenerate high resolution DP images with easy adoption. The key idea is to shift\nthe challenge of DP image synthesis from the image domain to the text domain by\nleveraging state of the art DP text generation methods. SPTI first summarizes\neach private image into a concise textual description using image to text\nmodels, then applies a modified Private Evolution algorithm to generate DP\ntext, and finally reconstructs images using text to image models. Notably, SPTI\nrequires no model training, only inference with off the shelf models. Given a\nprivate dataset, SPTI produces synthetic images of substantially higher quality\nthan prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less\nthan or equal to 26.71 under epsilon equal to 1.0, improving over Private\nEvolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less\nthan or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine\ntuning baselines. Overall, our results demonstrate that Synthesis via Private\nTextual Intermediaries provides a resource efficient and proprietary model\ncompatible framework for generating high resolution DP synthetic images,\ngreatly expanding access to private visual datasets.",
    "updated" : "2025-06-09T08:48:06Z",
    "published" : "2025-06-09T08:48:06Z",
    "authors" : [
      {
        "name" : "Haoxiang Wang"
      },
      {
        "name" : "Zinan Lin"
      },
      {
        "name" : "Da Yu"
      },
      {
        "name" : "Huishuai Zhang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07102v1",
    "title" : "Decentralized Optimization with Amplified Privacy via Efficient\n  Communication",
    "summary" : "Decentralized optimization is crucial for multi-agent systems, with\nsignificant concerns about communication efficiency and privacy. This paper\nexplores the role of efficient communication in decentralized stochastic\ngradient descent algorithms for enhancing privacy preservation. We develop a\nnovel algorithm that incorporates two key features: random agent activation and\nsparsified communication. Utilizing differential privacy, we demonstrate that\nthese features reduce noise without sacrificing privacy, thereby amplifying the\nprivacy guarantee and improving accuracy. Additionally, we analyze the\nconvergence and the privacy-accuracy-communication trade-off of the proposed\nalgorithm. Finally, we present experimental results to illustrate the\neffectiveness of our algorithm.",
    "updated" : "2025-06-08T12:14:14Z",
    "published" : "2025-06-08T12:14:14Z",
    "authors" : [
      {
        "name" : "Wei Huo"
      },
      {
        "name" : "Changxin Liu"
      },
      {
        "name" : "Kemi Ding"
      },
      {
        "name" : "Karl Henrik Johansson"
      },
      {
        "name" : "Ling Shi"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06742v1",
    "title" : "LADSG: Label-Anonymized Distillation and Similar Gradient Substitution\n  for Label Privacy in Vertical Federated Learning",
    "summary" : "Vertical federated learning (VFL) has become a key paradigm for collaborative\nmachine learning, enabling multiple parties to train models over distributed\nfeature spaces while preserving data privacy. Despite security protocols that\ndefend against external attacks - such as gradient masking and encryption,\nwhich prevent unauthorized access to sensitive data - recent label inference\nattacks from within the system have emerged. These attacks exploit gradients\nand semantic embeddings to reconstruct private labels, bypassing traditional\ndefenses. For example, the passive label inference attack can reconstruct tens\nof thousands of participants' private data using just 40 auxiliary labels,\nposing a significant security threat. Existing defenses address single leakage\npathways, such as gradient leakage or label exposure. As attack strategies\nevolve, their limitations become clear, especially against hybrid attacks that\ncombine multiple vectors. To address this, we propose Label-Anonymized Defense\nwith Substitution Gradient (LADSG), a unified defense framework that integrates\ngradient substitution, label anonymization, and anomaly detection. LADSG\nmitigates both gradient and label leakage while maintaining the scalability and\nefficiency of VFL. Experiments on six real-world datasets show that LADSG\nreduces label inference attack success rates by 30-60%, with minimal\ncomputational overhead, underscoring the importance of lightweight defenses in\nsecuring VFL.",
    "updated" : "2025-06-07T10:10:56Z",
    "published" : "2025-06-07T10:10:56Z",
    "authors" : [
      {
        "name" : "Zeyu Yan"
      },
      {
        "name" : "Yifei Yao"
      },
      {
        "name" : "Xuanbing Wen"
      },
      {
        "name" : "Juli Zhang"
      },
      {
        "name" : "Kai Fan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06591v1",
    "title" : "Privacy Perspectives and Practices of Chinese Smart Home Product Teams",
    "summary" : "Previous research has explored the privacy needs and concerns of device\nowners, primary users, and different bystander groups with regard to smart home\ndevices like security cameras, smart speakers, and hubs, but little is known\nabout the privacy views and practices of smart home product teams, particularly\nthose in non-Western contexts. This paper presents findings from 27\nsemi-structured interviews with Chinese smart home product team members,\nincluding product/project managers, software/hardware engineers, user\nexperience (UX) designers, legal/privacy experts, and marketers/operation\nspecialists. We examine their privacy perspectives, practices, and risk\nmitigation strategies. Our results show that participants emphasized compliance\nwith Chinese data privacy laws, which typically prioritized national security\nover individual privacy rights. China-specific cultural, social, and legal\nfactors also influenced participants' ethical considerations and attitudes\ntoward balancing user privacy and security with convenience. Drawing on our\nfindings, we propose a set of recommendations for smart home product teams,\nalong with socio-technical and legal interventions to address smart home\nprivacy issues-especially those belonging to at-risk groups-in Chinese\nmulti-user smart homes.",
    "updated" : "2025-06-06T23:49:48Z",
    "published" : "2025-06-06T23:49:48Z",
    "authors" : [
      {
        "name" : "Shijing He"
      },
      {
        "name" : "Yaxiong Lei"
      },
      {
        "name" : "Xiao Zhan"
      },
      {
        "name" : "Chi Zhang"
      },
      {
        "name" : "Juan Ye"
      },
      {
        "name" : "Ruba Abu-Salma"
      },
      {
        "name" : "Jose Such"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06530v1",
    "title" : "Breaking the Gaussian Barrier: Residual-PAC Privacy for Automatic\n  Privatization",
    "summary" : "The Probably Approximately Correct (PAC) Privacy framework [1] provides a\npowerful instance-based methodology for certifying privacy in complex\ndata-driven systems. However, existing PAC Privacy algorithms rely on a\nGaussian mutual information upper bound. We show that this is in general too\nconservative: the upper bound obtained by these algorithms is tight if and only\nif the perturbed mechanism output is jointly Gaussian with independent Gaussian\nnoise. To address the inefficiency inherent in the Gaussian-based approach, we\nintroduce Residual PAC Privacy, an f-divergence-based measure that quantifies\nthe privacy remaining after adversarial inference. When instantiated with\nKullback-Leibler divergence, Residual-PAC Privacy is governed by conditional\nentropy. Moreover, we propose Stackelberg Residual-PAC (SR-PAC) privatization\nmechanisms for RPAC Privacy, a game-theoretic framework that selects optimal\nnoise distributions through convex bilevel optimization. Our approach achieves\ntight privacy budget utilization for arbitrary data distributions. Moreover, it\nnaturally composes under repeated mechanisms and provides provable privacy\nguarantees with higher statistical efficiency. Numerical experiments\ndemonstrate that SR-PAC certifies the target privacy budget while consistently\nimproving utility compared to existing methods.",
    "updated" : "2025-06-06T20:52:47Z",
    "published" : "2025-06-06T20:52:47Z",
    "authors" : [
      {
        "name" : "Tao Zhang"
      },
      {
        "name" : "Yevgeniy Vorobeychik"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06062v2",
    "title" : "Minoritised Ethnic People's Security and Privacy Concerns and Responses\n  towards Essential Online Services",
    "summary" : "Minoritised ethnic people are marginalised in society, and therefore at a\nhigher risk of adverse online harms, including those arising from the loss of\nsecurity and privacy of personal data. Despite this, there has been very little\nresearch focused on minoritised ethnic people's security and privacy concerns,\nattitudes, and behaviours. In this work, we provide the results of one of the\nfirst studies in this regard. We explore minoritised ethnic people's\nexperiences of using essential online services across three sectors: health,\nsocial housing, and energy, their security and privacy-related concerns, and\nresponses towards these services. We conducted a thematic analysis of 44\nsemi-structured interviews with people of various reported minoritised\nethnicities in the UK. Privacy concerns and lack of control over personal data\nemerged as a major theme, with many interviewees considering privacy as their\nmost significant concern when using online services. Several creative tactics\nto exercise some agency were reported, including selective and inconsistent\ndisclosure of personal data. A core concern about how data may be used was\ndriven by a fear of repercussions, including penalisation and discrimination,\ninfluenced by prior experiences of institutional and online racism. The\nincreased concern and potential for harm resulted in minoritised ethnic people\ngrappling with a higher-stakes dilemma of whether to disclose personal\ninformation online or not. Furthermore, trust in institutions, or lack thereof,\nwas found to be embedded throughout as a basis for adapting behaviour. We draw\non our results to provide lessons learned for the design of more inclusive,\nmarginalisation-aware, and privacy-preserving online services.",
    "updated" : "2025-06-09T14:10:55Z",
    "published" : "2025-06-06T13:17:44Z",
    "authors" : [
      {
        "name" : "Aunam Quyoum"
      },
      {
        "name" : "Mark Wong"
      },
      {
        "name" : "Sebati Ghosh"
      },
      {
        "name" : "Siamak F. Shahandashti"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CR",
      "cs.HC",
      "K.4.2; H.1.2; K.4.1; K.6.5; J.4"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00759v2",
    "title" : "Understanding and Mitigating Cross-lingual Privacy Leakage via\n  Language-specific and Universal Privacy Neurons",
    "summary" : "Large Language Models (LLMs) trained on massive data capture rich information\nembedded in the training data. However, this also introduces the risk of\nprivacy leakage, particularly involving personally identifiable information\n(PII). Although previous studies have shown that this risk can be mitigated\nthrough methods such as privacy neurons, they all assume that both the\n(sensitive) training data and user queries are in English. We show that they\ncannot defend against the privacy leakage in cross-lingual contexts: even if\nthe training data is exclusively in one language, these (private) models may\nstill reveal private information when queried in another language. In this\nwork, we first investigate the information flow of cross-lingual privacy\nleakage to give a better understanding. We find that LLMs process private\ninformation in the middle layers, where representations are largely shared\nacross languages. The risk of leakage peaks when converted to a\nlanguage-specific space in later layers. Based on this, we identify\nprivacy-universal neurons and language-specific privacy neurons.\nPrivacy-universal neurons influence privacy leakage across all languages, while\nlanguage-specific privacy neurons are only related to specific languages. By\ndeactivating these neurons, the cross-lingual privacy leakage risk is reduced\nby 23.3%-31.6%.",
    "updated" : "2025-06-08T14:59:14Z",
    "published" : "2025-06-01T00:10:30Z",
    "authors" : [
      {
        "name" : "Wenshuo Dong"
      },
      {
        "name" : "Qingsong Yang"
      },
      {
        "name" : "Shu Yang"
      },
      {
        "name" : "Lijie Hu"
      },
      {
        "name" : "Meng Ding"
      },
      {
        "name" : "Wanyu Lin"
      },
      {
        "name" : "Tianhang Zheng"
      },
      {
        "name" : "Di Wang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08918v1",
    "title" : "Quantifying Mix Network Privacy Erosion with Generative Models",
    "summary" : "Modern mix networks improve over Tor and provide stronger privacy guarantees\nby robustly obfuscating metadata. As long as a message is routed through at\nleast one honest mixnode, the privacy of the users involved is safeguarded.\nHowever, the complexity of the mixing mechanisms makes it difficult to estimate\nthe cumulative privacy erosion occurring over time. This work uses a generative\nmodel trained on mixnet traffic to estimate the loss of privacy when users\ncommunicate persistently over a period of time. We train our large-language\nmodel from scratch on our specialized network traffic ``language'' and then use\nit to measure the sender-message unlinkability in various settings (e.g. mixing\nstrategies, security parameters, observation window). Our findings reveal\nnotable differences in privacy levels among mix strategies, even when they have\nsimilar mean latencies. In comparison, we demonstrate the limitations of\ntraditional privacy metrics, such as entropy and log-likelihood, in fully\ncapturing an adversary's potential to synthesize information from multiple\nobservations. Finally, we show that larger models exhibit greater sample\nefficiency and superior capabilities implying that further advancements in\ntransformers will consequently enhance the accuracy of model-based privacy\nestimates.",
    "updated" : "2025-06-10T15:43:39Z",
    "published" : "2025-06-10T15:43:39Z",
    "authors" : [
      {
        "name" : "Vasilios Mavroudis"
      },
      {
        "name" : "Tariq Elahi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08654v1",
    "title" : "A Privacy-Preserving Federated Learning Framework for Generalizable CBCT\n  to Synthetic CT Translation in Head and Neck",
    "summary" : "Shortened Abstract\n  Cone-beam computed tomography (CBCT) has become a widely adopted modality for\nimage-guided radiotherapy (IGRT). However, CBCT suffers from increased noise,\nlimited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield\nunit values and hindering direct dose calculation. Synthetic CT (sCT)\ngeneration from CBCT addresses these issues, especially using deep learning\n(DL) methods. Existing approaches are limited by institutional heterogeneity,\nscanner-dependent variations, and data privacy regulations that prevent\nmulti-center data sharing.\n  To overcome these challenges, we propose a cross-silo horizontal federated\nlearning (FL) approach for CBCT-to-sCT synthesis in the head and neck region,\nextending our FedSynthCT framework. A conditional generative adversarial\nnetwork was collaboratively trained on data from three European medical centers\nin the public SynthRAD2025 challenge dataset.\n  The federated model demonstrated effective generalization across centers,\nwith mean absolute error (MAE) ranging from $64.38\\pm13.63$ to $85.90\\pm7.10$\nHU, structural similarity index (SSIM) from $0.882\\pm0.022$ to $0.922\\pm0.039$,\nand peak signal-to-noise ratio (PSNR) from $32.86\\pm0.94$ to $34.91\\pm1.04$ dB.\nNotably, on an external validation dataset of 60 patients, comparable\nperformance was achieved (MAE: $75.22\\pm11.81$ HU, SSIM: $0.904\\pm0.034$, PSNR:\n$33.52\\pm2.06$ dB) without additional training, confirming robust\ngeneralization despite protocol, scanner differences and registration errors.\n  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT\nsynthesis while preserving data privacy and offer a collaborative solution for\ndeveloping generalizable models across institutions without centralized data\nsharing or site-specific fine-tuning.",
    "updated" : "2025-06-10T10:10:56Z",
    "published" : "2025-06-10T10:10:56Z",
    "authors" : [
      {
        "name" : "Ciro Benito Raggio"
      },
      {
        "name" : "Paolo Zaffino"
      },
      {
        "name" : "Maria Francesca Spadea"
      }
    ],
    "categories" : [
      "physics.med-ph",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08347v1",
    "title" : "Differentially Private Relational Learning with Entity-level Privacy\n  Guarantees",
    "summary" : "Learning with relational and network-structured data is increasingly vital in\nsensitive domains where protecting the privacy of individual entities is\nparamount. Differential Privacy (DP) offers a principled approach for\nquantifying privacy risks, with DP-SGD emerging as a standard mechanism for\nprivate model training. However, directly applying DP-SGD to relational\nlearning is challenging due to two key factors: (i) entities often participate\nin multiple relations, resulting in high and difficult-to-control sensitivity;\nand (ii) relational learning typically involves multi-stage, potentially\ncoupled (interdependent) sampling procedures that make standard privacy\namplification analyses inapplicable. This work presents a principled framework\nfor relational learning with formal entity-level DP guarantees. We provide a\nrigorous sensitivity analysis and introduce an adaptive gradient clipping\nscheme that modulates clipping thresholds based on entity occurrence frequency.\nWe also extend the privacy amplification results to a tractable subclass of\ncoupled sampling, where the dependence arises only through sample sizes. These\ncontributions lead to a tailored DP-SGD variant for relational data with\nprovable privacy guarantees. Experiments on fine-tuning text encoders over\ntext-attributed network-structured relational data demonstrate the strong\nutility-privacy trade-offs of our approach. Our code is available at\nhttps://github.com/Graph-COM/Node_DP.",
    "updated" : "2025-06-10T02:03:43Z",
    "published" : "2025-06-10T02:03:43Z",
    "authors" : [
      {
        "name" : "Yinan Huang"
      },
      {
        "name" : "Haoteng Ying"
      },
      {
        "name" : "Eli Chien"
      },
      {
        "name" : "Rongzhe Wei"
      },
      {
        "name" : "Pan Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08330v1",
    "title" : "Distortion Search, A Web Search Privacy Heuristic",
    "summary" : "Search engines have vast technical capabilities to retain Internet search\nlogs for each user and thus present major privacy vulnerabilities to both\nindividuals and organizations in revealing user intent. Additionally, many of\nthe web search privacy enhancing tools available today require that the user\ntrusts a third party, which make confidentiality of user intent even more\nchallenging. The user is left at the mercy of the third party without the\ncontrol over his or her own privacy. In this article, we suggest a user-centric\nheuristic, Distortion Search, a web search query privacy methodology that works\nby the formation of obfuscated search queries via the permutation of query\nkeyword categories, and by strategically applying k-anonymised web navigational\nclicks on URLs and Ads to generate a distorted user profile and thus providing\nspecific user intent and query confidentiality. We provide empirical results\nvia the evaluation of distorted web search queries in terms of retrieved search\nresults and the resulting web ads from search engines. Preliminary experimental\nresults indicate that web search query and specific user intent privacy might\nbe achievable from the user side without the involvement of the search engine\nor other third parties.",
    "updated" : "2025-06-10T01:35:16Z",
    "published" : "2025-06-10T01:35:16Z",
    "authors" : [
      {
        "name" : "Kato Mivule"
      },
      {
        "name" : "Kenneth Hopkinson"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.08185v1",
    "title" : "Surgeon Style Fingerprinting and Privacy Risk Quantification via\n  Discrete Diffusion Models in a Vision-Language-Action Framework",
    "summary" : "Surgeons exhibit distinct operating styles due to differences in training,\nexperience, and motor behavior - yet current AI systems often ignore this\npersonalization signal. We propose a novel approach to model fine-grained,\nsurgeon-specific fingerprinting in robotic surgery using a discrete diffusion\nframework integrated with a vision-language-action (VLA) pipeline. Our method\nformulates gesture prediction as a structured sequence denoising task,\nconditioned on multimodal inputs including endoscopic video, surgical intent\nlanguage, and a privacy-aware embedding of surgeon identity and skill.\nPersonalized surgeon fingerprinting is encoded through natural language prompts\nusing third-party language models, allowing the model to retain individual\nbehavioral style without exposing explicit identity. We evaluate our method on\nthe JIGSAWS dataset and demonstrate that it accurately reconstructs gesture\nsequences while learning meaningful motion fingerprints unique to each surgeon.\nTo quantify the privacy implications of personalization, we perform membership\ninference attacks and find that more expressive embeddings improve task\nperformance but simultaneously increase susceptibility to identity leakage.\nThese findings demonstrate that while personalized embeddings improve\nperformance, they also increase vulnerability to identity leakage, revealing\nthe importance of balancing personalization with privacy risk in surgical\nmodeling. Code is available at:\nhttps://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.",
    "updated" : "2025-06-09T19:49:55Z",
    "published" : "2025-06-09T19:49:55Z",
    "authors" : [
      {
        "name" : "Huixin Zhan"
      },
      {
        "name" : "Jason H. Moore"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.09690v1",
    "title" : "Knockoffs Inference under Privacy Constraints",
    "summary" : "Model-X knockoff framework offers a model-free variable selection method that\nensures finite sample false discovery rate (FDR) control. However, the\ncomplexity of generating knockoff variables, coupled with the model-free\nassumption, presents significant challenges for protecting data privacy in this\ncontext. In this paper, we propose a comprehensive framework for knockoff\ninference within the differential privacy paradigm. Our proposed method\nguarantees robust privacy protection while preserving the exact FDR control\nentailed by the original model-X knockoff procedure. We further conduct power\nanalysis and establish sufficient conditions under which the noise added for\nprivacy preservation does not asymptotically compromise power. Through various\napplications, we demonstrate that the differential privacy knockoff\n(DP-knockoff) method can be effectively utilized to safeguard privacy during\nvariable selection with FDR control in both low and high dimensional settings.",
    "updated" : "2025-06-11T13:06:21Z",
    "published" : "2025-06-11T13:06:21Z",
    "authors" : [
      {
        "name" : "Zhanrui Cai"
      },
      {
        "name" : "Yingying Fan"
      },
      {
        "name" : "Lan Gao"
      }
    ],
    "categories" : [
      "stat.ME",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.09387v1",
    "title" : "Epass: Efficient and Privacy-Preserving Asynchronous Payment on\n  Blockchain",
    "summary" : "Buy Now Pay Later (BNPL) is a rapidly proliferating e-commerce model,\noffering consumers to get the product immediately and defer payments.\nMeanwhile, emerging blockchain technologies endow BNPL platforms with digital\ncurrency transactions, allowing BNPL platforms to integrate with digital\nwallets. However, the transparency of transactions causes critical privacy\nconcerns because malicious participants may derive consumers' financial\nstatuses from on-chain asynchronous payments. Furthermore, the newly created\ntransactions for deferred payments introduce additional time overheads, which\nweaken the scalability of BNPL services. To address these issues, we propose an\nefficient and privacy-preserving blockchain-based asynchronous payment scheme\n(Epass), which has promising scalability while protecting the privacy of\non-chain consumer transactions. Specifically, Epass leverages locally\nverifiable signatures to guarantee the privacy of consumer transactions against\nmalicious acts. Then, a privacy-preserving asynchronous payment scheme can be\nfurther constructed by leveraging time-release encryption to control trapdoors\nof redactable blockchain, reducing time overheads by modifying transactions for\ndeferred payment. We give formal definitions and security models, generic\nstructures, and formal proofs for Epass. Extensive comparisons and experimental\nanalysis show that \\textsf{Epass} achieves KB-level communication costs, and\nreduces time overhead by more than four times in comparisons with locally\nverifiable signatures and Go-Ethereum private test networks.",
    "updated" : "2025-06-11T04:32:54Z",
    "published" : "2025-06-11T04:32:54Z",
    "authors" : [
      {
        "name" : "Weijie Wang"
      },
      {
        "name" : "Jinwen Liang"
      },
      {
        "name" : "Chuan Zhang"
      },
      {
        "name" : "Ximeng Liu"
      },
      {
        "name" : "Liehuang Zhu"
      },
      {
        "name" : "Song Guo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.09312v1",
    "title" : "What is the Cost of Differential Privacy for Deep Learning-Based\n  Trajectory Generation?",
    "summary" : "While location trajectories offer valuable insights, they also reveal\nsensitive personal information. Differential Privacy (DP) offers formal\nprotection, but achieving a favourable utility-privacy trade-off remains\nchallenging. Recent works explore deep learning-based generative models to\nproduce synthetic trajectories. However, current models lack formal privacy\nguarantees and rely on conditional information derived from real data during\ngeneration. This work investigates the utility cost of enforcing DP in such\nmodels, addressing three research questions across two datasets and eleven\nutility metrics. (1) We evaluate how DP-SGD, the standard DP training method\nfor deep learning, affects the utility of state-of-the-art generative models.\n(2) Since DP-SGD is limited to unconditional models, we propose a novel DP\nmechanism for conditional generation that provides formal guarantees and assess\nits impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN\n- affect the utility-privacy trade-off. Our results show that DP-SGD\nsignificantly impacts performance, although some utility remains if the\ndatasets is sufficiently large. The proposed DP mechanism improves training\nstability, particularly when combined with DP-SGD, for unstable models such as\nGANs and on smaller datasets. Diffusion models yield the best utility without\nguarantees, but with DP-SGD, GANs perform best, indicating that the best\nnon-private model is not necessarily optimal when targeting formal guarantees.\nIn conclusion, DP trajectory generation remains a challenging task, and formal\nguarantees are currently only feasible with large datasets and in constrained\nuse cases.",
    "updated" : "2025-06-11T00:59:52Z",
    "published" : "2025-06-11T00:59:52Z",
    "authors" : [
      {
        "name" : "Erik Buchholz"
      },
      {
        "name" : "Natasha Fernandes"
      },
      {
        "name" : "David D. Nguyen"
      },
      {
        "name" : "Alsharif Abuadbba"
      },
      {
        "name" : "Surya Nepal"
      },
      {
        "name" : "Salil S. Kanhere"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05683v2",
    "title" : "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation\n  Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence\n  in AR/VR/MR",
    "summary" : "Extended reality (XR) systems, which consist of virtual reality (VR),\naugmented reality (AR), and mixed reality (XR), offer a transformative\ninterface for immersive, multi-modal, and embodied human-computer interaction.\nIn this paper, we envision that multi-modal multi-task (M3T) federated\nfoundation models (FedFMs) can offer transformative capabilities for XR systems\nthrough integrating the representational strength of M3T foundation models\n(FMs) with the privacy-preserving model training principles of federated\nlearning (FL). We present a modular architecture for FedFMs, which entails\ndifferent coordination paradigms for model training and aggregations. Central\nto our vision is the codification of XR challenges that affect the\nimplementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality\ndiversity, (2) Hardware heterogeneity and system-level constraints, (3)\nInteractivity and embodied personalization, (4) Functional/task variability,\nand (5) Temporality and environmental variability. We illustrate the\nmanifestation of these dimensions across a set of emerging and anticipated\napplications of XR systems. Finally, we propose evaluation metrics, dataset\nrequirements, and design tradeoffs necessary for the development of\nresource-aware FedFMs in XR. This perspective aims to chart the technical and\nconceptual foundations for context-aware privacy-preserving intelligence in the\nnext generation of XR systems.",
    "updated" : "2025-06-11T00:49:57Z",
    "published" : "2025-06-06T02:23:42Z",
    "authors" : [
      {
        "name" : "Fardis Nadimi"
      },
      {
        "name" : "Payam Abdisarabshali"
      },
      {
        "name" : "Kasra Borazjani"
      },
      {
        "name" : "Jacob Chakareski"
      },
      {
        "name" : "Seyyedali Hosseinalipour"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.10042v1",
    "title" : "Multiverse Privacy Theory for Contextual Risks in Complex User-AI\n  Interactions",
    "summary" : "In an era of increasing interaction with artificial intelligence (AI), users\nface evolving privacy decisions shaped by complex, uncertain factors. This\npaper introduces Multiverse Privacy Theory, a novel framework in which each\nprivacy decision spawns a parallel universe, representing a distinct potential\noutcome based on user choices over time. By simulating these universes, this\ntheory provides a foundation for understanding privacy through the lens of\ncontextual integrity, evolving preferences, and probabilistic decision-making.\nFuture work will explore its application using real-world, scenario-based\nsurvey data.",
    "updated" : "2025-06-11T05:02:59Z",
    "published" : "2025-06-11T05:02:59Z",
    "authors" : [
      {
        "name" : "Ece Gumusel"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.10024v1",
    "title" : "Private Memorization Editing: Turning Memorization into a Defense to\n  Strengthen Data Privacy in Large Language Models",
    "summary" : "Large Language Models (LLMs) memorize, and thus, among huge amounts of\nuncontrolled data, may memorize Personally Identifiable Information (PII),\nwhich should not be stored and, consequently, not leaked. In this paper, we\nintroduce Private Memorization Editing (PME), an approach for preventing\nprivate data leakage that turns an apparent limitation, that is, the LLMs'\nmemorization ability, into a powerful privacy defense strategy. While attacks\nagainst LLMs have been performed exploiting previous knowledge regarding their\ntraining data, our approach aims to exploit the same kind of knowledge in order\nto make a model more robust. We detect a memorized PII and then mitigate the\nmemorization of PII by editing a model knowledge of its training data. We\nverify that our procedure does not affect the underlying language model while\nmaking it more robust against privacy Training Data Extraction attacks. We\ndemonstrate that PME can effectively reduce the number of leaked PII in a\nnumber of configurations, in some cases even reducing the accuracy of the\nprivacy attacks to zero.",
    "updated" : "2025-06-09T17:57:43Z",
    "published" : "2025-06-09T17:57:43Z",
    "authors" : [
      {
        "name" : "Elena Sofia Ruzzetti"
      },
      {
        "name" : "Giancarlo A. Xompero"
      },
      {
        "name" : "Davide Venditti"
      },
      {
        "name" : "Fabio Massimo Zanzotto"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.07605v2",
    "title" : "TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in\n  Federated Tree-Based Systems",
    "summary" : "Federated Learning has emerged as a privacy-oriented alternative to\ncentralized Machine Learning, enabling collaborative model training without\ndirect data sharing. While extensively studied for neural networks, the\nsecurity and privacy implications of tree-based models remain underexplored.\nThis work introduces TimberStrike, an optimization-based dataset reconstruction\nattack targeting horizontally federated tree-based models. Our attack, carried\nout by a single client, exploits the discrete nature of decision trees by using\nsplit values and decision paths to infer sensitive training data from other\nclients. We evaluate TimberStrike on State-of-the-Art federated gradient\nboosting implementations across multiple frameworks, including Flower, NVFlare,\nand FedTree, demonstrating their vulnerability to privacy breaches. On a\npublicly available stroke prediction dataset, TimberStrike consistently\nreconstructs between 73.05% and 95.63% of the target dataset across all\nimplementations. We further analyze Differential Privacy, showing that while it\npartially mitigates the attack, it also significantly degrades model\nperformance. Our findings highlight the need for privacy-preserving mechanisms\nspecifically designed for tree-based Federated Learning systems, and we provide\npreliminary insights into their design.",
    "updated" : "2025-06-12T17:48:26Z",
    "published" : "2025-06-09T10:06:03Z",
    "authors" : [
      {
        "name" : "Marco Di Gennaro"
      },
      {
        "name" : "Giovanni De Lucia"
      },
      {
        "name" : "Stefano Longari"
      },
      {
        "name" : "Stefano Zanero"
      },
      {
        "name" : "Michele Carminati"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05683v3",
    "title" : "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation\n  Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence\n  in AR/VR/MR",
    "summary" : "Extended reality (XR) systems, which consist of virtual reality (VR),\naugmented reality (AR), and mixed reality (XR), offer a transformative\ninterface for immersive, multi-modal, and embodied human-computer interaction.\nIn this paper, we envision that multi-modal multi-task (M3T) federated\nfoundation models (FedFMs) can offer transformative capabilities for XR systems\nthrough integrating the representational strength of M3T foundation models\n(FMs) with the privacy-preserving model training principles of federated\nlearning (FL). We present a modular architecture for FedFMs, which entails\ndifferent coordination paradigms for model training and aggregations. Central\nto our vision is the codification of XR challenges that affect the\nimplementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality\ndiversity, (2) Hardware heterogeneity and system-level constraints, (3)\nInteractivity and embodied personalization, (4) Functional/task variability,\nand (5) Temporality and environmental variability. We illustrate the\nmanifestation of these dimensions across a set of emerging and anticipated\napplications of XR systems. Finally, we propose evaluation metrics, dataset\nrequirements, and design tradeoffs necessary for the development of\nresource-aware FedFMs in XR. This perspective aims to chart the technical and\nconceptual foundations for context-aware privacy-preserving intelligence in the\nnext generation of XR systems.",
    "updated" : "2025-06-12T04:18:38Z",
    "published" : "2025-06-06T02:23:42Z",
    "authors" : [
      {
        "name" : "Fardis Nadimi"
      },
      {
        "name" : "Payam Abdisarabshali"
      },
      {
        "name" : "Kasra Borazjani"
      },
      {
        "name" : "Jacob Chakareski"
      },
      {
        "name" : "Seyyedali Hosseinalipour"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.05421v2",
    "title" : "TRIDENT -- A Three-Tier Privacy-Preserving Propaganda Detection Model in\n  Mobile Networks using Transformers, Adversarial Learning, and Differential\n  Privacy",
    "summary" : "The proliferation of propaganda on mobile platforms raises critical concerns\naround detection accuracy and user privacy. To address this, we propose TRIDENT\n- a three-tier propaganda detection model implementing transformers,\nadversarial learning, and differential privacy which integrates syntactic\nobfuscation and label perturbation to mitigate privacy leakage while\nmaintaining propaganda detection accuracy. TRIDENT leverages multilingual\nback-translation to introduce semantic variance, character-level noise, and\nentity obfuscation for differential privacy enforcement, and combines these\ntechniques into a unified defense mechanism. Using a binary propaganda\nclassification dataset, baseline transformer models (BERT, GPT-2) we achieved\nF1 scores of 0.89 and 0.90. Applying TRIDENT's third-tier defense yields a\nreduced but effective cumulative F1 of 0.83, demonstrating strong privacy\nprotection across mobile ML deployments with minimal degradation.",
    "updated" : "2025-06-12T01:37:56Z",
    "published" : "2025-06-05T02:38:02Z",
    "authors" : [
      {
        "name" : "Al Nahian Bin Emran"
      },
      {
        "name" : "Dhiman Goswami"
      },
      {
        "name" : "Md Hasan Ullah Sadi"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  }
]