[
  {
    "id" : "http://arxiv.org/abs/2502.02520v1",
    "title" : "Privacy by Design for Self-Sovereign Identity Systems: An in-depth\n  Component Analysis completed by a Design Assistance Dashboard",
    "summary" : "The use of Self-Sovereign Identity (SSI) systems for digital identity\nmanagement is gaining traction and interest. Countries such as Bhutan have\nalready implemented an SSI infrastructure to manage the identity of their\ncitizens. The EU, thanks to the revised eIDAS regulation, is opening the door\nfor SSI vendors to develop SSI systems for the planned EU digital identity\nwallet. These developments, which fall within the sovereign domain, raise\nquestions about individual privacy.\n  The purpose of this article is to help SSI solution designers make informed\nchoices to ensure that the designed solution is privacy-friendly. The\nobservation is that the range of possible solutions is very broad, from DID and\nDID resolution methods to verifiable credential types, publicly available\ninformation (e.g. in a blockchain), type of infrastructure, etc. As a result,\nthe article proposes (1) to group the elementary building blocks of a SSI\nsystem into 5 structuring layers, (2) to analyze for each layer the privacy\nimplications of using the chosen building block, and (3) to provide a design\nassistance dashboard that gives the complete picture of the SSI, and shows the\ninterdependencies between architectural choices and technical building blocks,\nallowing designers to make informed choices and graphically achieve a SSI\nsolution that meets their need for privacy.",
    "updated" : "2025-02-04T17:42:29Z",
    "published" : "2025-02-04T17:42:29Z",
    "authors" : [
      {
        "name" : "Montassar Naghmouchi"
      },
      {
        "name" : "Maryline Laurent"
      }
    ],
    "categories" : [
      "cs.ET",
      "cs.CY",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02514v1",
    "title" : "Privacy Attacks on Image AutoRegressive Models",
    "summary" : "Image autoregressive (IAR) models have surpassed diffusion models (DMs) in\nboth image quality (FID: 1.48 vs. 1.58) and generation speed. However, their\nprivacy risks remain largely unexplored. To address this, we conduct a\ncomprehensive privacy analysis comparing IARs to DMs. We develop a novel\nmembership inference attack (MIA) that achieves a significantly higher success\nrate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% for\nDMs). Using this MIA, we perform dataset inference (DI) and find that IARs\nrequire as few as six samples to detect dataset membership, compared to 200 for\nDMs, indicating higher information leakage. Additionally, we extract hundreds\nof training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight\na fundamental privacy-utility trade-off: while IARs excel in generation quality\nand speed, they are significantly more vulnerable to privacy attacks. This\nsuggests that incorporating techniques from DMs, such as per-token probability\nmodeling using diffusion, could help mitigate IARs' privacy risks. Our code is\navailable at https://github.com/sprintml/privacy_attacks_against_iars.",
    "updated" : "2025-02-04T17:33:08Z",
    "published" : "2025-02-04T17:33:08Z",
    "authors" : [
      {
        "name" : "Antoni Kowalczuk"
      },
      {
        "name" : "Jan Dubiński"
      },
      {
        "name" : "Franziska Boenisch"
      },
      {
        "name" : "Adam Dziedzic"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02410v1",
    "title" : "Privacy Amplification by Structured Subsampling for Deep Differentially\n  Private Time Series Forecasting",
    "summary" : "Many forms of sensitive data, such as web traffic, mobility data, or hospital\noccupancy, are inherently sequential. The standard method for training machine\nlearning models while ensuring privacy for units of sensitive information, such\nas individual hospital visits, is differentially private stochastic gradient\ndescent (DP-SGD). However, we observe in this work that the formal guarantees\nof DP-SGD are incompatible with timeseries-specific tasks like forecasting,\nsince they rely on the privacy amplification attained by training on small,\nunstructured batches sampled from an unstructured dataset. In contrast, batches\nfor forecasting are generated by (1) sampling sequentially structured time\nseries from a dataset, (2) sampling contiguous subsequences from these series,\nand (3) partitioning them into context and ground-truth forecast windows. We\ntheoretically analyze the privacy amplification attained by this structured\nsubsampling to enable the training of forecasting models with sound and tight\nevent- and user-level privacy guarantees. Towards more private models, we\nadditionally prove how data augmentation amplifies privacy in self-supervised\ntraining of sequence models. Our empirical evaluation demonstrates that\namplification by structured subsampling enables the training of forecasting\nmodels with strong formal privacy guarantees.",
    "updated" : "2025-02-04T15:29:00Z",
    "published" : "2025-02-04T15:29:00Z",
    "authors" : [
      {
        "name" : "Jan Schuchardt"
      },
      {
        "name" : "Mina Dalirrooyfard"
      },
      {
        "name" : "Jed Guzelkabaagac"
      },
      {
        "name" : "Anderson Schneider"
      },
      {
        "name" : "Yuriy Nevmyvaka"
      },
      {
        "name" : "Stephan Günnemann"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.01904v1",
    "title" : "Common Neighborhood Estimation over Bipartite Graphs under Local\n  Differential Privacy",
    "summary" : "Bipartite graphs, formed by two vertex layers, arise as a natural fit for\nmodeling the relationships between two groups of entities. In bipartite graphs,\ncommon neighborhood computation between two vertices on the same vertex layer\nis a basic operator, which is easily solvable in general settings. However, it\ninevitably involves releasing the neighborhood information of vertices, posing\na significant privacy risk for users in real-world applications. To protect\nedge privacy in bipartite graphs, in this paper, we study the problem of\nestimating the number of common neighbors of two vertices on the same layer\nunder edge local differential privacy (edge LDP). The problem is challenging in\nthe context of edge LDP since each vertex on the opposite layer of the query\nvertices can potentially be a common neighbor. To obtain efficient and accurate\nestimates, we propose a multiple-round framework that significantly reduces the\ncandidate pool of common neighbors and enables the query vertices to construct\nunbiased estimators locally. Furthermore, we improve data utility by\nincorporating the estimators built from the neighbors of both query vertices\nand devise privacy budget allocation optimizations. These improve the\nestimator's robustness and consistency, particularly against query vertices\nwith imbalanced degrees. Extensive experiments on 15 datasets validate the\neffectiveness and efficiency of our proposed techniques.",
    "updated" : "2025-02-04T00:33:18Z",
    "published" : "2025-02-04T00:33:18Z",
    "authors" : [
      {
        "name" : "Yizhang He"
      },
      {
        "name" : "Kai Wang"
      },
      {
        "name" : "Wenjie Zhang"
      },
      {
        "name" : "Xuemin Lin"
      },
      {
        "name" : "Ying Zhang"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.01885v1",
    "title" : "A Privacy-Preserving Domain Adversarial Federated learning for\n  multi-site brain functional connectivity analysis",
    "summary" : "Resting-state functional magnetic resonance imaging (rs-fMRI) and its derived\nfunctional connectivity networks (FCNs) have become critical for understanding\nneurological disorders. However, collaborative analyses and the\ngeneralizability of models still face significant challenges due to privacy\nregulations and the non-IID (non-independent and identically distributed)\nproperty of multiple data sources. To mitigate these difficulties, we propose\nDomain Adversarial Federated Learning (DAFed), a novel federated deep learning\nframework specifically designed for non-IID fMRI data analysis in multi-site\nsettings. DAFed addresses these challenges through feature disentanglement,\ndecomposing the latent feature space into domain-invariant and domain-specific\ncomponents, to ensure robust global learning while preserving local data\nspecificity. Furthermore, adversarial training facilitates effective knowledge\ntransfer between labeled and unlabeled datasets, while a contrastive learning\nmodule enhances the global representation of domain-invariant features. We\nevaluated DAFed on the diagnosis of ASD and further validated its\ngeneralizability in the classification of AD, demonstrating its superior\nclassification accuracy compared to state-of-the-art methods. Additionally, an\nenhanced Score-CAM module identifies key brain regions and functional\nconnectivity significantly associated with ASD and MCI, respectively,\nuncovering shared neurobiological patterns across sites. These findings\nhighlight the potential of DAFed to advance multi-site collaborative research\nin neuroimaging while protecting data confidentiality.",
    "updated" : "2025-02-03T23:26:07Z",
    "published" : "2025-02-03T23:26:07Z",
    "authors" : [
      {
        "name" : "Yipu Zhang"
      },
      {
        "name" : "Likai Wang"
      },
      {
        "name" : "Kuan-Jui Su"
      },
      {
        "name" : "Aiying Zhang"
      },
      {
        "name" : "Hao Zhu"
      },
      {
        "name" : "Xiaowen Liu"
      },
      {
        "name" : "Hui Shen"
      },
      {
        "name" : "Vince D. Calhoun"
      },
      {
        "name" : "Yuping Wang"
      },
      {
        "name" : "Hongwen Deng"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.01352v1",
    "title" : "Metric Privacy in Federated Learning for Medical Imaging: Improving\n  Convergence and Preventing Client Inference Attacks",
    "summary" : "Federated learning is a distributed learning technique that allows training a\nglobal model with the participation of different data owners without the need\nto share raw data. This architecture is orchestrated by a central server that\naggregates the local models from the clients. This server may be trusted, but\nnot all nodes in the network. Then, differential privacy (DP) can be used to\nprivatize the global model by adding noise. However, this may affect\nconvergence across the rounds of the federated architecture, depending also on\nthe aggregation strategy employed. In this work, we aim to introduce the notion\nof metric-privacy to mitigate the impact of classical server side global-DP on\nthe convergence of the aggregated model. Metric-privacy is a relaxation of DP,\nsuitable for domains provided with a notion of distance. We apply it from the\nserver side by computing a distance for the difference between the local\nmodels. We compare our approach with standard DP by analyzing the impact on six\nclassical aggregation strategies. The proposed methodology is applied to an\nexample of medical imaging and different scenarios are simulated across\nhomogeneous and non-i.i.d clients. Finally, we introduce a novel client\ninference attack, where a semi-honest client tries to find whether another\nclient participated in the training and study how it can be mitigated using DP\nand metric-privacy. Our evaluation shows that metric-privacy can increase the\nperformance of the model compared to standard DP, while offering similar\nprotection against client inference attacks.",
    "updated" : "2025-02-03T13:41:52Z",
    "published" : "2025-02-03T13:41:52Z",
    "authors" : [
      {
        "name" : "Judith Sáinz-Pardo Díaz"
      },
      {
        "name" : "Andreas Athanasiou"
      },
      {
        "name" : "Kangsoo Jung"
      },
      {
        "name" : "Catuscia Palamidessi"
      },
      {
        "name" : "Álvaro López García"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.01306v1",
    "title" : "Expert-Generated Privacy Q&A Dataset for Conversational AI and User\n  Study Insights",
    "summary" : "Conversational assistants process personal data and must comply with data\nprotection regulations that require providers to be transparent with users\nabout how their data is handled. Transparency, in a legal sense, demands\npreciseness, comprehensibility and accessibility, yet existing solutions fail\nto meet these requirements. To address this, we introduce a new\nhuman-expert-generated dataset for Privacy Question-Answering (Q&A), developed\nthrough an iterative process involving legal professionals and conversational\ndesigners. We evaluate this dataset through linguistic analysis and a user\nstudy, comparing it to privacy policy excerpts and state-of-the-art responses\nfrom Amazon Alexa. Our findings show that the proposed answers improve\nusability and clarity compared to existing solutions while achieving legal\npreciseness, thereby enhancing the accessibility of data processing information\nfor Conversational AI and Natural Language Processing applications.",
    "updated" : "2025-02-03T12:30:45Z",
    "published" : "2025-02-03T12:30:45Z",
    "authors" : [
      {
        "name" : "Anna Leschanowsky"
      },
      {
        "name" : "Farnaz Salamatjoo"
      },
      {
        "name" : "Zahra Kolagar"
      },
      {
        "name" : "Birgit Popp"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.00760v1",
    "title" : "Privacy Preserving Properties of Vision Classifiers",
    "summary" : "Vision classifiers are often trained on proprietary datasets containing\nsensitive information, yet the models themselves are frequently shared openly\nunder the privacy-preserving assumption. Although these models are assumed to\nprotect sensitive information in their training data, the extent to which this\nassumption holds for different architectures remains unexplored. This\nassumption is challenged by inversion attacks which attempt to reconstruct\ntraining data from model weights, exposing significant privacy vulnerabilities.\nIn this study, we systematically evaluate the privacy-preserving properties of\nvision classifiers across diverse architectures, including Multi-Layer\nPerceptrons (MLPs), Convolutional Neural Networks (CNNs), and Vision\nTransformers (ViTs). Using network inversion-based reconstruction techniques,\nwe assess the extent to which these architectures memorize and reveal training\ndata, quantifying the relative ease of reconstruction across models. Our\nanalysis highlights how architectural differences, such as input\nrepresentation, feature extraction mechanisms, and weight structures, influence\nprivacy risks. By comparing these architectures, we identify which are more\nresilient to inversion attacks and examine the trade-offs between model\nperformance and privacy preservation, contributing to the development of secure\nand privacy-respecting machine learning models for sensitive applications. Our\nfindings provide actionable insights into the design of secure and\nprivacy-aware machine learning systems, emphasizing the importance of\nevaluating architectural decisions in sensitive applications involving\nproprietary or personal data.",
    "updated" : "2025-02-02T11:50:00Z",
    "published" : "2025-02-02T11:50:00Z",
    "authors" : [
      {
        "name" : "Pirzada Suhail"
      },
      {
        "name" : "Amit Sethi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.00693v1",
    "title" : "DPBloomfilter: Securing Bloom Filters with Differential Privacy",
    "summary" : "The Bloom filter is a simple yet space-efficient probabilistic data structure\nthat supports membership queries for dramatically large datasets. It is widely\nutilized and implemented across various industrial scenarios, often handling\nmassive datasets that include sensitive user information necessitating privacy\npreservation. To address the challenge of maintaining privacy within the Bloom\nfilter, we have developed the DPBloomfilter. This innovation integrates the\nclassical differential privacy mechanism, specifically the Random Response\ntechnique, into the Bloom filter, offering robust privacy guarantees under the\nsame running complexity as the standard Bloom filter. Through rigorous\nsimulation experiments, we have demonstrated that our DPBloomfilter algorithm\nmaintains high utility while ensuring privacy protections. To the best of our\nknowledge, this is the first work to provide differential privacy guarantees\nfor the Bloom filter for membership query problems.",
    "updated" : "2025-02-02T06:47:50Z",
    "published" : "2025-02-02T06:47:50Z",
    "authors" : [
      {
        "name" : "Yekun Ke"
      },
      {
        "name" : "Yingyu Liang"
      },
      {
        "name" : "Zhizhou Sha"
      },
      {
        "name" : "Zhenmei Shi"
      },
      {
        "name" : "Zhao Song"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.00451v1",
    "title" : "Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and\n  Opportunities",
    "summary" : "Mental illness is a widespread and debilitating condition with substantial\nsocietal and personal costs. Traditional diagnostic and treatment approaches,\nsuch as self-reported questionnaires and psychotherapy sessions, often impose\nsignificant burdens on both patients and clinicians, limiting accessibility and\nefficiency. Recent advances in Artificial Intelligence (AI), particularly in\nNatural Language Processing and multimodal techniques, hold great potential for\nrecognizing and addressing conditions such as depression, anxiety, bipolar\ndisorder, schizophrenia, and post-traumatic stress disorder. However, privacy\nconcerns, including the risk of sensitive data leakage from datasets and\ntrained models, remain a critical barrier to deploying these AI systems in\nreal-world clinical settings. These challenges are amplified in multimodal\nmethods, where personal identifiers such as voice and facial data can be\nmisused. This paper presents a critical and comprehensive study of the privacy\nchallenges associated with developing and deploying AI models for mental\nhealth. We further prescribe potential solutions, including data anonymization,\nsynthetic data generation, and privacy-preserving model training, to strengthen\nprivacy safeguards in practical applications. Additionally, we discuss\nevaluation frameworks to assess the privacy-utility trade-offs in these\napproaches. By addressing these challenges, our work aims to advance the\ndevelopment of reliable, privacy-aware AI tools to support clinical\ndecision-making and improve mental health outcomes.",
    "updated" : "2025-02-01T15:10:02Z",
    "published" : "2025-02-01T15:10:02Z",
    "authors" : [
      {
        "name" : "Aishik Mandal"
      },
      {
        "name" : "Tanmoy Chakraborty"
      },
      {
        "name" : "Iryna Gurevych"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02913v1",
    "title" : "Privacy Token: Surprised to Find Out What You Accidentally Revealed",
    "summary" : "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.",
    "updated" : "2025-02-05T06:20:20Z",
    "published" : "2025-02-05T06:20:20Z",
    "authors" : [
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Xin Shi"
      },
      {
        "name" : "Qingyu Huang"
      },
      {
        "name" : "Chen Hou"
      },
      {
        "name" : "Hong Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02749v1",
    "title" : "Unveiling Privacy and Security Gaps in Female Health Apps",
    "summary" : "Female Health Applications (FHA), a growing segment of FemTech, aim to\nprovide affordable and accessible healthcare solutions for women globally.\nThese applications gather and monitor health and reproductive data from\nmillions of users. With ongoing debates on women's reproductive rights and\nprivacy, it's crucial to assess how these apps protect users' privacy. In this\npaper, we undertake a security and data protection assessment of 45 popular\nFHAs. Our investigation uncovers harmful permissions, extensive collection of\nsensitive personal and medical data, and the presence of numerous third-party\ntracking libraries. Furthermore, our examination of their privacy policies\nreveals deviations from fundamental data privacy principles. These findings\nhighlight a significant lack of privacy and security measures for FemTech apps,\nespecially as women's reproductive rights face growing political challenges.\nThe results and recommendations provide valuable insights for users, app\ndevelopers, and policymakers, paving the way for better privacy and security in\nFemale Health Applications.",
    "updated" : "2025-02-04T22:34:03Z",
    "published" : "2025-02-04T22:34:03Z",
    "authors" : [
      {
        "name" : "Muhammad Hassan"
      },
      {
        "name" : "Mahnoor Jameel"
      },
      {
        "name" : "Tian Wang"
      },
      {
        "name" : "Masooda Bashir"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.04045v1",
    "title" : "Comparing privacy notions for protection against reconstruction attacks\n  in machine learning",
    "summary" : "Within the machine learning community, reconstruction attacks are a principal\nconcern and have been identified even in federated learning (FL), which was\ndesigned with privacy preservation in mind. In response to these threats, the\nprivacy community recommends the use of differential privacy (DP) in the\nstochastic gradient descent algorithm, termed DP-SGD. However, the\nproliferation of variants of DP in recent years\\textemdash such as metric\nprivacy\\textemdash has made it challenging to conduct a fair comparison between\ndifferent mechanisms due to the different meanings of the privacy parameters\n$\\epsilon$ and $\\delta$ across different variants. Thus, interpreting the\npractical implications of $\\epsilon$ and $\\delta$ in the FL context and amongst\nvariants of DP remains ambiguous. In this paper, we lay a foundational\nframework for comparing mechanisms with differing notions of privacy\nguarantees, namely $(\\epsilon,\\delta)$-DP and metric privacy. We provide two\nfoundational means of comparison: firstly, via the well-established\n$(\\epsilon,\\delta)$-DP guarantees, made possible through the R\\'enyi\ndifferential privacy framework; and secondly, via Bayes' capacity, which we\nidentify as an appropriate measure for reconstruction threats.",
    "updated" : "2025-02-06T13:04:25Z",
    "published" : "2025-02-06T13:04:25Z",
    "authors" : [
      {
        "name" : "Sayan Biswas"
      },
      {
        "name" : "Mark Dras"
      },
      {
        "name" : "Pedro Faustini"
      },
      {
        "name" : "Natasha Fernandes"
      },
      {
        "name" : "Annabelle McIver"
      },
      {
        "name" : "Catuscia Palamidessi"
      },
      {
        "name" : "Parastoo Sadeghi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.03811v1",
    "title" : "Privacy Risks in Health Big Data: A Systematic Literature Review",
    "summary" : "The digitization of health records has greatly improved the efficiency of the\nhealthcare system and promoted the formulation of related research and\npolicies. However, the widespread application of advanced technologies such as\nelectronic health records, genomic data, and wearable devices in the field of\nhealth big data has also intensified the collection of personal sensitive data,\nbringing serious privacy and security issues. Based on a systematic literature\nreview (SLR), this paper comprehensively outlines the key research in the field\nof health big data security. By analyzing existing research, this paper\nexplores how cutting-edge technologies such as homomorphic encryption,\nblockchain, federated learning, and artificial immune systems can enhance data\nsecurity while protecting personal privacy. This paper also points out the\ncurrent challenges and proposes a future research framework in this key area.",
    "updated" : "2025-02-06T06:44:36Z",
    "published" : "2025-02-06T06:44:36Z",
    "authors" : [
      {
        "name" : "Zhang Si Yuan"
      },
      {
        "name" : "Manmeet Mahinderjit Singh"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.03668v1",
    "title" : "Privacy-Preserving Generative Models: A Comprehensive Survey",
    "summary" : "Despite the generative model's groundbreaking success, the need to study its\nimplications for privacy and utility becomes more urgent. Although many studies\nhave demonstrated the privacy threats brought by GANs, no existing survey has\nsystematically categorized the privacy and utility perspectives of GANs and\nVAEs. In this article, we comprehensively study privacy-preserving generative\nmodels, articulating the novel taxonomies for both privacy and utility metrics\nby analyzing 100 research publications. Finally, we discuss the current\nchallenges and future research directions that help new researchers gain\ninsight into the underlying concepts.",
    "updated" : "2025-02-05T23:24:43Z",
    "published" : "2025-02-05T23:24:43Z",
    "authors" : [
      {
        "name" : "Debalina Padariya"
      },
      {
        "name" : "Isabel Wagner"
      },
      {
        "name" : "Aboozar Taherkhani"
      },
      {
        "name" : "Eerke Boiten"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02913v2",
    "title" : "Privacy Token: Surprised to Find Out What You Accidentally Revealed",
    "summary" : "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.",
    "updated" : "2025-02-06T02:33:11Z",
    "published" : "2025-02-05T06:20:20Z",
    "authors" : [
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Xin Shi"
      },
      {
        "name" : "Qingyu Huang"
      },
      {
        "name" : "Chen Hou"
      },
      {
        "name" : "Hong Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.04758v1",
    "title" : "Differential Privacy of Quantum and Quantum-Inspired-Classical\n  Recommendation Algorithms",
    "summary" : "We analyze the DP (differential privacy) properties of the quantum\nrecommendation algorithm and the quantum-inspired-classical recommendation\nalgorithm. We discover that the quantum recommendation algorithm is a privacy\ncurating mechanism on its own, requiring no external noise, which is different\nfrom traditional differential privacy mechanisms. In our analysis, a novel\nperturbation method tailored for SVD (singular value decomposition) and\nlow-rank matrix approximation problems is introduced. Using the perturbation\nmethod and random matrix theory, we are able to derive that both the quantum\nand quantum-inspired-classical algorithms are\n$\\big(\\tilde{\\mathcal{O}}\\big(\\frac 1n\\big),\\,\\,\n\\tilde{\\mathcal{O}}\\big(\\frac{1}{\\min\\{m,n\\}}\\big)\\big)$-DP under some\nreasonable restrictions, where $m$ and $n$ are numbers of users and products in\nthe input preference database respectively. Nevertheless, a comparison shows\nthat the quantum algorithm has better privacy preserving potential than the\nclassical one.",
    "updated" : "2025-02-07T08:45:00Z",
    "published" : "2025-02-07T08:45:00Z",
    "authors" : [
      {
        "name" : "Chenjian Li"
      },
      {
        "name" : "Mingsheng Ying"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.ET",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.04365v1",
    "title" : "AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case\n  Study on Detecting Time of Birth",
    "summary" : "Approximately 10% of newborns need some assistance to start breathing and 5\\%\nproper ventilation. It is crucial that interventions are initiated as soon as\npossible after birth. Accurate documentation of Time of Birth (ToB) is thereby\nessential for documenting and improving newborn resuscitation performance.\nHowever, current clinical practices rely on manual recording of ToB, typically\nwith minute precision. In this study, we present an AI-driven, video-based\nsystem for automated ToB detection using thermal imaging, designed to preserve\nthe privacy of healthcare providers and mothers by avoiding the use of\nidentifiable visual data. Our approach achieves 91.4% precision and 97.4%\nrecall in detecting ToB within thermal video clips during performance\nevaluation. Additionally, our system successfully identifies ToB in 96% of test\ncases with an absolute median deviation of 1 second compared to manual\nannotations. This method offers a reliable solution for improving ToB\ndocumentation and enhancing newborn resuscitation outcomes.",
    "updated" : "2025-02-05T07:01:49Z",
    "published" : "2025-02-05T07:01:49Z",
    "authors" : [
      {
        "name" : "Jorge García-Torres"
      },
      {
        "name" : "Øyvind Meinich-Bache"
      },
      {
        "name" : "Siren Rettedal"
      },
      {
        "name" : "Kjersti Engan"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02913v3",
    "title" : "Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient\n  Leakage",
    "summary" : "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.",
    "updated" : "2025-02-07T09:10:09Z",
    "published" : "2025-02-05T06:20:20Z",
    "authors" : [
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Hong Chen"
      },
      {
        "name" : "Xin Shi"
      },
      {
        "name" : "Qingyu Huang"
      },
      {
        "name" : "Chen Hou"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.06652v1",
    "title" : "Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A",
    "summary" : "The transparency principle of the General Data Protection Regulation (GDPR)\nrequires data processing information to be clear, precise, and accessible.\nWhile language models show promise in this context, their probabilistic nature\ncomplicates truthfulness and comprehensibility.\n  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)\nsystems enhanced with alignment techniques to fulfill GDPR obligations. We\nevaluate RAG systems incorporating an alignment module like Rewindable\nAuto-regressive Inference (RAIN) and our proposed multidimensional extension,\nMultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness\nand comprehensibility and are assessed through 21 metrics, including\ndeterministic and large language model-based evaluations.\n  Our results show that RAG systems with an alignment module outperform\nbaseline RAG systems on most metrics, though none fully match human answers.\nPrincipal component analysis of the results reveals complex interactions\nbetween metrics, highlighting the need to refine metrics. This study provides a\nfoundation for integrating advanced natural language processing systems into\nlegal compliance frameworks.",
    "updated" : "2025-02-10T16:42:00Z",
    "published" : "2025-02-10T16:42:00Z",
    "authors" : [
      {
        "name" : "Anna Leschanowsky"
      },
      {
        "name" : "Zahra Kolagar"
      },
      {
        "name" : "Erion Çano"
      },
      {
        "name" : "Ivan Habernal"
      },
      {
        "name" : "Dara Hallinan"
      },
      {
        "name" : "Emanuël A. P. Habets"
      },
      {
        "name" : "Birgit Popp"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.06597v1",
    "title" : "Continual Release Moment Estimation with Differential Privacy",
    "summary" : "We propose Joint Moment Estimation (JME), a method for continually and\nprivately estimating both the first and second moments of data with reduced\nnoise compared to naive approaches. JME uses the matrix mechanism and a joint\nsensitivity analysis to allow the second moment estimation with no additional\nprivacy cost, thereby improving accuracy while maintaining privacy. We\ndemonstrate JME's effectiveness in two applications: estimating the running\nmean and covariance matrix for Gaussian density estimation, and model training\nwith DP-Adam on CIFAR-10.",
    "updated" : "2025-02-10T15:58:26Z",
    "published" : "2025-02-10T15:58:26Z",
    "authors" : [
      {
        "name" : "Nikita P. Kalinin"
      },
      {
        "name" : "Jalaj Upadhyay"
      },
      {
        "name" : "Christoph H. Lampert"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.06425v1",
    "title" : "Generating Privacy-Preserving Personalized Advice with Zero-Knowledge\n  Proofs and LLMs",
    "summary" : "Large language models (LLMs) are increasingly utilized in domains such as\nfinance, healthcare, and interpersonal relationships to provide advice tailored\nto user traits and contexts. However, this personalization often relies on\nsensitive data, raising critical privacy concerns and necessitating data\nminimization. To address these challenges, we propose a framework that\nintegrates zero-knowledge proof (ZKP) technology, specifically zkVM, with\nLLM-based chatbots. This integration enables privacy-preserving data sharing by\nverifying user traits without disclosing sensitive information. Our research\nintroduces both an architecture and a prompting strategy for this approach.\nThrough empirical evaluation, we clarify the current constraints and\nperformance limitations of both zkVM and the proposed prompting strategy,\nthereby demonstrating their practical feasibility in real-world scenarios.",
    "updated" : "2025-02-10T13:02:00Z",
    "published" : "2025-02-10T13:02:00Z",
    "authors" : [
      {
        "name" : "Hiroki Watanabe"
      },
      {
        "name" : "Motonobu Uchikoshi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.05765v1",
    "title" : "Privacy-Preserving Dataset Combination",
    "summary" : "Access to diverse, high-quality datasets is crucial for machine learning\nmodel performance, yet data sharing remains limited by privacy concerns and\ncompetitive interests, particularly in regulated domains like healthcare. This\ndynamic especially disadvantages smaller organizations that lack resources to\npurchase data or negotiate favorable sharing agreements. We present SecureKL, a\nprivacy-preserving framework that enables organizations to identify beneficial\ndata partnerships without exposing sensitive information. Building on recent\nadvances in dataset combination methods, we develop a secure multiparty\ncomputation protocol that maintains strong privacy guarantees while achieving\n>90\\% correlation with plaintext evaluations. In experiments with real-world\nhospital data, SecureKL successfully identifies beneficial data partnerships\nthat improve model performance for intensive care unit mortality prediction\nwhile preserving data privacy. Our framework provides a practical solution for\norganizations seeking to leverage collective data resources while maintaining\nprivacy and competitive advantages. These results demonstrate the potential for\nprivacy-preserving data collaboration to advance machine learning applications\nin high-stakes domains while promoting more equitable access to data resources.",
    "updated" : "2025-02-09T03:54:17Z",
    "published" : "2025-02-09T03:54:17Z",
    "authors" : [
      {
        "name" : "Keren Fuentes"
      },
      {
        "name" : "Mimee Xu"
      },
      {
        "name" : "Irene Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.05547v1",
    "title" : "Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in\n  Federated Learning",
    "summary" : "Federated learning (FL) is inherently susceptible to privacy breaches and\npoisoning attacks. To tackle these challenges, researchers have separately\ndevised secure aggregation mechanisms to protect data privacy and robust\naggregation methods that withstand poisoning attacks. However, simultaneously\naddressing both concerns is challenging; secure aggregation facilitates\npoisoning attacks as most anomaly detection techniques require access to\nunencrypted local model updates, which are obscured by secure aggregation. Few\nrecent efforts to simultaneously tackle both challenges offen depend on\nimpractical assumption of non-colluding two-server setups that disrupt FL's\ntopology, or three-party computation which introduces scalability issues,\ncomplicating deployment and application. To overcome this dilemma, this paper\nintroduce a Dual Defense Federated learning (DDFed) framework. DDFed\nsimultaneously boosts privacy protection and mitigates poisoning attacks,\nwithout introducing new participant roles or disrupting the existing FL\ntopology. DDFed initially leverages cutting-edge fully homomorphic encryption\n(FHE) to securely aggregate model updates, without the impractical requirement\nfor non-colluding two-server setups and ensures strong privacy protection.\nAdditionally, we proposes a unique two-phase anomaly detection mechanism for\nencrypted model updates, featuring secure similarity computation and\nfeedback-driven collaborative selection, with additional measures to prevent\npotential privacy breaches from Byzantine clients incorporated into the\ndetection process. We conducted extensive experiments on various model\npoisoning attacks and FL scenarios, including both cross-device and cross-silo\nFL. Experiments on publicly available datasets demonstrate that DDFed\nsuccessfully protects model privacy and effectively defends against model\npoisoning threats.",
    "updated" : "2025-02-08T12:28:20Z",
    "published" : "2025-02-08T12:28:20Z",
    "authors" : [
      {
        "name" : "Runhua Xu"
      },
      {
        "name" : "Shiqi Gao"
      },
      {
        "name" : "Chao Li"
      },
      {
        "name" : "James Joshi"
      },
      {
        "name" : "Jianxin Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.05516v1",
    "title" : "Evaluating Differential Privacy on Correlated Datasets Using Pointwise\n  Maximal Leakage",
    "summary" : "Data-driven advancements significantly contribute to societal progress, yet\nthey also pose substantial risks to privacy. In this landscape, differential\nprivacy (DP) has become a cornerstone in privacy preservation efforts. However,\nthe adequacy of DP in scenarios involving correlated datasets has sometimes\nbeen questioned and multiple studies have hinted at potential vulnerabilities.\nIn this work, we delve into the nuances of applying DP to correlated datasets\nby leveraging the concept of pointwise maximal leakage (PML) for a quantitative\nassessment of information leakage. Our investigation reveals that DP's\nguarantees can be arbitrarily weak for correlated databases when assessed\nthrough the lens of PML. More precisely, we prove the existence of a pure DP\nmechanism with PML levels arbitrarily close to that of a mechanism which\nreleases individual entries from a database without any perturbation. By\nshedding light on the limitations of DP on correlated datasets, our work aims\nto foster a deeper understanding of subtle privacy risks and highlight the need\nfor the development of more effective privacy-preserving mechanisms tailored to\ndiverse scenarios.",
    "updated" : "2025-02-08T10:30:45Z",
    "published" : "2025-02-08T10:30:45Z",
    "authors" : [
      {
        "name" : "Sara Saeidian"
      },
      {
        "name" : "Tobias J. Oechtering"
      },
      {
        "name" : "Mikael Skoglund"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.05509v1",
    "title" : "Do Spikes Protect Privacy? Investigating Black-Box Model Inversion\n  Attacks in Spiking Neural Networks",
    "summary" : "As machine learning models become integral to security-sensitive\napplications, concerns over data leakage from adversarial attacks continue to\nrise. Model Inversion (MI) attacks pose a significant privacy threat by\nenabling adversaries to reconstruct training data from model outputs. While MI\nattacks on Artificial Neural Networks (ANNs) have been widely studied, Spiking\nNeural Networks (SNNs) remain largely unexplored in this context. Due to their\nevent-driven and discrete computations, SNNs introduce fundamental differences\nin information processing that may offer inherent resistance to such attacks. A\ncritical yet underexplored aspect of this threat lies in black-box settings,\nwhere attackers operate through queries without direct access to model\nparameters or gradients-representing a more realistic adversarial scenario in\ndeployed systems. This work presents the first study of black-box MI attacks on\nSNNs. We adapt a generative adversarial MI framework to the spiking domain by\nincorporating rate-based encoding for input transformation and decoding\nmechanisms for output interpretation. Our results show that SNNs exhibit\nsignificantly greater resistance to MI attacks than ANNs, as demonstrated by\ndegraded reconstructions, increased instability in attack convergence, and\noverall reduced attack effectiveness across multiple evaluation metrics.\nFurther analysis suggests that the discrete and temporally distributed nature\nof SNN decision boundaries disrupts surrogate modeling, limiting the attacker's\nability to approximate the target model.",
    "updated" : "2025-02-08T10:02:27Z",
    "published" : "2025-02-08T10:02:27Z",
    "authors" : [
      {
        "name" : "Hamed Poursiami"
      },
      {
        "name" : "Ayana Moshruba"
      },
      {
        "name" : "Maryam Parsa"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.05219v1",
    "title" : "Enabling External Scrutiny of AI Systems with Privacy-Enhancing\n  Technologies",
    "summary" : "This article describes how technical infrastructure developed by the\nnonprofit OpenMined enables external scrutiny of AI systems without\ncompromising sensitive information.\n  Independent external scrutiny of AI systems provides crucial transparency\ninto AI development, so it should be an integral component of any approach to\nAI governance. In practice, external researchers have struggled to gain access\nto AI systems because of AI companies' legitimate concerns about security,\nprivacy, and intellectual property.\n  But now, privacy-enhancing technologies (PETs) have reached a new level of\nmaturity: end-to-end technical infrastructure developed by OpenMined combines\nseveral PETs into various setups that enable privacy-preserving audits of AI\nsystems. We showcase two case studies where this infrastructure has been\ndeployed in real-world governance scenarios: \"Understanding Social Media\nRecommendation Algorithms with the Christchurch Call\" and \"Evaluating Frontier\nModels with the UK AI Safety Institute.\" We describe types of scrutiny of AI\nsystems that could be facilitated by current setups and OpenMined's proposed\nfuture setups.\n  We conclude that these innovative approaches deserve further exploration and\nsupport from the AI governance community. Interested policymakers can focus on\nempowering researchers on a legal level.",
    "updated" : "2025-02-05T15:31:11Z",
    "published" : "2025-02-05T15:31:11Z",
    "authors" : [
      {
        "name" : "Kendrea Beers"
      },
      {
        "name" : "Helen Toner"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.07693v1",
    "title" : "SoK: A Classification for AI-driven Personalized Privacy Assistants",
    "summary" : "To help users make privacy-related decisions, personalized privacy assistants\nbased on AI technology have been developed in recent years. These AI-driven\nPersonalized Privacy Assistants (AI-driven PPAs) can reap significant benefits\nfor users, who may otherwise struggle to make decisions regarding their\npersonal data in environments saturated with privacy-related decision requests.\nHowever, no study systematically inquired about the features of these AI-driven\nPPAs, their underlying technologies, or the accuracy of their decisions. To\nfill this gap, we present a Systematization of Knowledge (SoK) to map the\nexisting solutions found in the scientific literature. We screened 1697 unique\nresearch papers over the last decade (2013-2023), constructing a classification\nfrom 39 included papers. As a result, this SoK reviews several aspects of\nexisting research on AI-driven PPAs in terms of types of publications,\ncontributions, methodological quality, and other quantitative insights.\nFurthermore, we provide a comprehensive classification for AI-driven PPAs,\ndelving into their architectural choices, system contexts, types of AI used,\ndata sources, types of decisions, and control over decisions, among other\nfacets. Based on our SoK, we further underline the research gaps and challenges\nand formulate recommendations for the design and development of AI-driven PPAs\nas well as avenues for future research.",
    "updated" : "2025-02-11T16:46:56Z",
    "published" : "2025-02-11T16:46:56Z",
    "authors" : [
      {
        "name" : "Victor Morel"
      },
      {
        "name" : "Leonardo Iwaya"
      },
      {
        "name" : "Simone Fischer-Hübner"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08202v1",
    "title" : "Privacy amplification by random allocation",
    "summary" : "We consider the privacy guarantees of an algorithm in which a user's data is\nused in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$\ndifferentially private steps. We demonstrate that the privacy guarantees of\nthis sampling scheme can be upper bound by the privacy guarantees of the\nwell-studied independent (or Poisson) subsampling in which each step uses the\nuser's data with probability $(1+ o(1))k/t $. Further, we provide two\nadditional analysis techniques that lead to numerical improvements in some\nparameter regimes. The case of $k=1$ has been previously studied in the context\nof DP-SGD in Balle et al. (2020) and very recently in Chua et al. (2024).\nPrivacy analysis of Balle et al. (2020) relies on privacy amplification by\nshuffling which leads to overly conservative bounds. Privacy analysis of Chua\net al. (2024a) relies on Monte Carlo simulations that are computationally\nprohibitive in many practical scenarios and have additional inherent\nlimitations.",
    "updated" : "2025-02-12T08:32:10Z",
    "published" : "2025-02-12T08:32:10Z",
    "authors" : [
      {
        "name" : "Vitaly Feldman"
      },
      {
        "name" : "Moshe Shenfeld"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08151v1",
    "title" : "Local Differential Privacy is Not Enough: A Sample Reconstruction Attack\n  against Federated Learning with Local Differential Privacy",
    "summary" : "Reconstruction attacks against federated learning (FL) aim to reconstruct\nusers' samples through users' uploaded gradients. Local differential privacy\n(LDP) is regarded as an effective defense against various attacks, including\nsample reconstruction in FL, where gradients are clipped and perturbed.\nExisting attacks are ineffective in FL with LDP since clipped and perturbed\ngradients obliterate most sample information for reconstruction. Besides,\nexisting attacks embed additional sample information into gradients to improve\nthe attack effect and cause gradient expansion, leading to a more severe\ngradient clipping in FL with LDP. In this paper, we propose a sample\nreconstruction attack against LDP-based FL with any target models to\nreconstruct victims' sensitive samples to illustrate that FL with LDP is not\nflawless. Considering gradient expansion in reconstruction attacks and noise in\nLDP, the core of the proposed attack is gradient compression and reconstructed\nsample denoising. For gradient compression, an inference structure based on\nsample characteristics is presented to reduce redundant gradients against LDP.\nFor reconstructed sample denoising, we artificially introduce zero gradients to\nobserve noise distribution and scale confidence interval to filter the noise.\nTheoretical proof guarantees the effectiveness of the proposed attack.\nEvaluations show that the proposed attack is the only attack that reconstructs\nvictims' training samples in LDP-based FL and has little impact on the target\nmodel's accuracy. We conclude that LDP-based FL needs further improvements to\ndefend against sample reconstruction attacks effectively.",
    "updated" : "2025-02-12T06:37:26Z",
    "published" : "2025-02-12T06:37:26Z",
    "authors" : [
      {
        "name" : "Zhichao You"
      },
      {
        "name" : "Xuewen Dong"
      },
      {
        "name" : "Shujun Li"
      },
      {
        "name" : "Ximeng Liu"
      },
      {
        "name" : "Siqi Ma"
      },
      {
        "name" : "Yulong Shen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08008v1",
    "title" : "An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models",
    "summary" : "Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.",
    "updated" : "2025-02-11T23:07:14Z",
    "published" : "2025-02-11T23:07:14Z",
    "authors" : [
      {
        "name" : "Kasra Ahmadi"
      },
      {
        "name" : "Rouzbeh Behnia"
      },
      {
        "name" : "Reza Ebrahimi"
      },
      {
        "name" : "Mehran Mozaffari Kermani"
      },
      {
        "name" : "Jeremiah Birrell"
      },
      {
        "name" : "Jason Pacheco"
      },
      {
        "name" : "Attila A Yavuz"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08001v1",
    "title" : "Unveiling Client Privacy Leakage from Public Dataset Usage in Federated\n  Distillation",
    "summary" : "Federated Distillation (FD) has emerged as a popular federated training\nframework, enabling clients to collaboratively train models without sharing\nprivate data. Public Dataset-Assisted Federated Distillation (PDA-FD), which\nleverages public datasets for knowledge sharing, has become widely adopted.\nAlthough PDA-FD enhances privacy compared to traditional Federated Learning, we\ndemonstrate that the use of public datasets still poses significant privacy\nrisks to clients' private training data. This paper presents the first\ncomprehensive privacy analysis of PDA-FD in presence of an honest-but-curious\nserver. We show that the server can exploit clients' inference results on\npublic datasets to extract two critical types of private information: label\ndistributions and membership information of the private training dataset. To\nquantify these vulnerabilities, we introduce two novel attacks specifically\ndesigned for the PDA-FD setting: a label distribution inference attack and\ninnovative membership inference methods based on Likelihood Ratio Attack\n(LiRA). Through extensive evaluation of three representative PDA-FD frameworks\n(FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance,\nwith label distribution attacks reaching minimal KL-divergence and membership\ninference attacks maintaining high True Positive Rates under low False Positive\nRate constraints. Our findings reveal significant privacy risks in current\nPDA-FD frameworks and emphasize the need for more robust privacy protection\nmechanisms in collaborative learning systems.",
    "updated" : "2025-02-11T22:48:49Z",
    "published" : "2025-02-11T22:48:49Z",
    "authors" : [
      {
        "name" : "Haonan Shi"
      },
      {
        "name" : "Tu Ouyang"
      },
      {
        "name" : "An Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.07693v2",
    "title" : "SoK: A Classification for AI-driven Personalized Privacy Assistants",
    "summary" : "To help users make privacy-related decisions, personalized privacy assistants\nbased on AI technology have been developed in recent years. These AI-driven\nPersonalized Privacy Assistants (AI-driven PPAs) can reap significant benefits\nfor users, who may otherwise struggle to make decisions regarding their\npersonal data in environments saturated with privacy-related decision requests.\nHowever, no study systematically inquired about the features of these AI-driven\nPPAs, their underlying technologies, or the accuracy of their decisions. To\nfill this gap, we present a Systematization of Knowledge (SoK) to map the\nexisting solutions found in the scientific literature. We screened 1697 unique\nresearch papers over the last decade (2013-2023), constructing a classification\nfrom 39 included papers. As a result, this SoK reviews several aspects of\nexisting research on AI-driven PPAs in terms of types of publications,\ncontributions, methodological quality, and other quantitative insights.\nFurthermore, we provide a comprehensive classification for AI-driven PPAs,\ndelving into their architectural choices, system contexts, types of AI used,\ndata sources, types of decisions, and control over decisions, among other\nfacets. Based on our SoK, we further underline the research gaps and challenges\nand formulate recommendations for the design and development of AI-driven PPAs\nas well as avenues for future research.",
    "updated" : "2025-02-12T16:52:51Z",
    "published" : "2025-02-11T16:46:56Z",
    "authors" : [
      {
        "name" : "Victor Morel"
      },
      {
        "name" : "Leonardo Iwaya"
      },
      {
        "name" : "Simone Fischer-Hübner"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02913v4",
    "title" : "Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient\n  Leakage",
    "summary" : "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.",
    "updated" : "2025-02-12T04:59:16Z",
    "published" : "2025-02-05T06:20:20Z",
    "authors" : [
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Hong Chen"
      },
      {
        "name" : "Xin Shi"
      },
      {
        "name" : "Qingyu Huang"
      },
      {
        "name" : "Chen Hou"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.09001v1",
    "title" : "Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection:\n  Balancing Security and Data Protection",
    "summary" : "Privacy-preserving network anomaly detection has become an essential area of\nresearch due to growing concerns over the protection of sensitive data.\nTraditional anomaly de- tection models often prioritize accuracy while\nneglecting the critical aspect of privacy. In this work, we propose a hybrid\nensemble model that incorporates privacy-preserving techniques to address both\ndetection accuracy and data protection. Our model combines the strengths of\nseveral machine learning algo- rithms, including K-Nearest Neighbors (KNN),\nSupport Vector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN),\nto create a robust system capable of identifying network anomalies while\nensuring privacy. The proposed approach in- tegrates advanced preprocessing\ntechniques that enhance data quality and address the challenges of small sample\nsizes and imbalanced datasets. By embedding privacy measures into the model\ndesign, our solution offers a significant advancement over existing methods,\nensuring both enhanced detection performance and strong privacy safeguards.",
    "updated" : "2025-02-13T06:33:16Z",
    "published" : "2025-02-13T06:33:16Z",
    "authors" : [
      {
        "name" : "Shaobo Liu"
      },
      {
        "name" : "Zihao Zhao"
      },
      {
        "name" : "Weijie He"
      },
      {
        "name" : "Jiren Wang"
      },
      {
        "name" : "Jing Peng"
      },
      {
        "name" : "Haoyuan Ma"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08989v1",
    "title" : "RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency\n  Detection in Privacy-Preserving Federated Learning",
    "summary" : "Federated Learning (FL) allows users to collaboratively train a global\nmachine learning model by sharing local model only, without exposing their\nprivate data to a central server. This distributed learning is particularly\nappealing in scenarios where data privacy is crucial, and it has garnered\nsubstantial attention from both industry and academia. However, studies have\nrevealed privacy vulnerabilities in FL, where adversaries can potentially infer\nsensitive information from the shared model parameters. In this paper, we\npresent an efficient masking-based secure aggregation scheme utilizing\nlightweight cryptographic primitives to mitigate privacy risks. Our scheme\noffers several advantages over existing methods. First, it requires only a\nsingle setup phase for the entire FL training session, significantly reducing\ncommunication overhead. Second, it minimizes user-side overhead by eliminating\nthe need for user-to-user interactions, utilizing an intermediate server layer\nand a lightweight key negotiation method. Third, the scheme is highly resilient\nto user dropouts, and the users can join at any FL round. Fourth, it can detect\nand defend against malicious server activities, including recently discovered\nmodel inconsistency attacks. Finally, our scheme ensures security in both\nsemi-honest and malicious settings. We provide security analysis to formally\nprove the robustness of our approach. Furthermore, we implemented an end-to-end\nprototype of our scheme. We conducted comprehensive experiments and\ncomparisons, which show that it outperforms existing solutions in terms of\ncommunication and computation overhead, functionality, and security.",
    "updated" : "2025-02-13T06:01:09Z",
    "published" : "2025-02-13T06:01:09Z",
    "authors" : [
      {
        "name" : "Nazatul H. Sultan"
      },
      {
        "name" : "Yan Bo"
      },
      {
        "name" : "Yansong Gao"
      },
      {
        "name" : "Seyit Camtepe"
      },
      {
        "name" : "Arash Mahboubi"
      },
      {
        "name" : "Hang Thanh Bui"
      },
      {
        "name" : "Aufeef Chauhan"
      },
      {
        "name" : "Hamed Aboutorab"
      },
      {
        "name" : "Michael Bewong"
      },
      {
        "name" : "Praveen Gauravaram"
      },
      {
        "name" : "Rafiqul Islam"
      },
      {
        "name" : "Sharif Abuadbba"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "68P27",
      "E.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08970v1",
    "title" : "A Decade of Metric Differential Privacy: Advancements and Applications",
    "summary" : "Metric Differential Privacy (mDP) builds upon the core principles of\nDifferential Privacy (DP) by incorporating various distance metrics, which\noffer adaptable and context-sensitive privacy guarantees for a wide range of\napplications, such as location-based services, text analysis, and image\nprocessing. Since its inception in 2013, mDP has garnered substantial research\nattention, advancing theoretical foundations, algorithm design, and practical\nimplementations. Despite this progress, existing surveys mainly focus on\ntraditional DP and local DP, and they provide limited coverage of mDP. This\npaper provides a comprehensive survey of mDP research from 2013 to 2024,\ntracing its development from the foundations of DP. We categorize essential\nmechanisms, including Laplace, Exponential, and optimization-based approaches,\nand assess their strengths, limitations, and application domains. Additionally,\nwe highlight key challenges and outline future research directions to encourage\ninnovation and real-world adoption of mDP. This survey is designed to be a\nvaluable resource for researchers and practitioners aiming to deepen their\nunderstanding and drive progress in mDP within the broader privacy ecosystem.",
    "updated" : "2025-02-13T05:18:24Z",
    "published" : "2025-02-13T05:18:24Z",
    "authors" : [
      {
        "name" : "Xinpeng Xie"
      },
      {
        "name" : "Chenyang Yu"
      },
      {
        "name" : "Yan Huang"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Chenxi Qiu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08966v1",
    "title" : "RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage",
    "summary" : "Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external\ntools for tasks beyond their standalone capabilities, such as searching\nwebsites, booking flights, or making financial transactions. However, these\ntools greatly increase the risks of prompt injection attacks, where malicious\ncontent hijacks the LM agent to leak confidential data or trigger harmful\nactions. Existing defenses (OpenAI GPTs) require user confirmation before every\ntool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS),\nwhich automatically detects and executes tool calls that preserve integrity and\nconfidentiality, requiring user confirmation only when these safeguards cannot\nbe ensured. RTBAS adapts Information Flow Control to the unique challenges\npresented by TBAS. We present two novel dependency screeners, using\nLM-as-a-judge and attention-based saliency, to overcome these challenges.\nExperimental results on the AgentDojo Prompt Injection benchmark show RTBAS\nprevents all targeted attacks with only a 2% loss of task utility when under\nattack, and further tests confirm its ability to obtain near-oracle performance\non detecting both subtle and direct privacy leaks.",
    "updated" : "2025-02-13T05:06:22Z",
    "published" : "2025-02-13T05:06:22Z",
    "authors" : [
      {
        "name" : "Peter Yong Zhong"
      },
      {
        "name" : "Siyuan Chen"
      },
      {
        "name" : "Ruiqi Wang"
      },
      {
        "name" : "McKenna McCall"
      },
      {
        "name" : "Ben L. Titzer"
      },
      {
        "name" : "Heather Miller"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  }
]