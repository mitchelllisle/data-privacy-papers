[
  {
    "id" : "http://arxiv.org/abs/2502.02520v1",
    "title" : "Privacy by Design for Self-Sovereign Identity Systems: An in-depth\n  Component Analysis completed by a Design Assistance Dashboard",
    "summary" : "The use of Self-Sovereign Identity (SSI) systems for digital identity\nmanagement is gaining traction and interest. Countries such as Bhutan have\nalready implemented an SSI infrastructure to manage the identity of their\ncitizens. The EU, thanks to the revised eIDAS regulation, is opening the door\nfor SSI vendors to develop SSI systems for the planned EU digital identity\nwallet. These developments, which fall within the sovereign domain, raise\nquestions about individual privacy.\n  The purpose of this article is to help SSI solution designers make informed\nchoices to ensure that the designed solution is privacy-friendly. The\nobservation is that the range of possible solutions is very broad, from DID and\nDID resolution methods to verifiable credential types, publicly available\ninformation (e.g. in a blockchain), type of infrastructure, etc. As a result,\nthe article proposes (1) to group the elementary building blocks of a SSI\nsystem into 5 structuring layers, (2) to analyze for each layer the privacy\nimplications of using the chosen building block, and (3) to provide a design\nassistance dashboard that gives the complete picture of the SSI, and shows the\ninterdependencies between architectural choices and technical building blocks,\nallowing designers to make informed choices and graphically achieve a SSI\nsolution that meets their need for privacy.",
    "updated" : "2025-02-04T17:42:29Z",
    "published" : "2025-02-04T17:42:29Z",
    "authors" : [
      {
        "name" : "Montassar Naghmouchi"
      },
      {
        "name" : "Maryline Laurent"
      }
    ],
    "categories" : [
      "cs.ET",
      "cs.CY",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02514v1",
    "title" : "Privacy Attacks on Image AutoRegressive Models",
    "summary" : "Image autoregressive (IAR) models have surpassed diffusion models (DMs) in\nboth image quality (FID: 1.48 vs. 1.58) and generation speed. However, their\nprivacy risks remain largely unexplored. To address this, we conduct a\ncomprehensive privacy analysis comparing IARs to DMs. We develop a novel\nmembership inference attack (MIA) that achieves a significantly higher success\nrate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% for\nDMs). Using this MIA, we perform dataset inference (DI) and find that IARs\nrequire as few as six samples to detect dataset membership, compared to 200 for\nDMs, indicating higher information leakage. Additionally, we extract hundreds\nof training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight\na fundamental privacy-utility trade-off: while IARs excel in generation quality\nand speed, they are significantly more vulnerable to privacy attacks. This\nsuggests that incorporating techniques from DMs, such as per-token probability\nmodeling using diffusion, could help mitigate IARs' privacy risks. Our code is\navailable at https://github.com/sprintml/privacy_attacks_against_iars.",
    "updated" : "2025-02-04T17:33:08Z",
    "published" : "2025-02-04T17:33:08Z",
    "authors" : [
      {
        "name" : "Antoni Kowalczuk"
      },
      {
        "name" : "Jan Dubiński"
      },
      {
        "name" : "Franziska Boenisch"
      },
      {
        "name" : "Adam Dziedzic"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02410v1",
    "title" : "Privacy Amplification by Structured Subsampling for Deep Differentially\n  Private Time Series Forecasting",
    "summary" : "Many forms of sensitive data, such as web traffic, mobility data, or hospital\noccupancy, are inherently sequential. The standard method for training machine\nlearning models while ensuring privacy for units of sensitive information, such\nas individual hospital visits, is differentially private stochastic gradient\ndescent (DP-SGD). However, we observe in this work that the formal guarantees\nof DP-SGD are incompatible with timeseries-specific tasks like forecasting,\nsince they rely on the privacy amplification attained by training on small,\nunstructured batches sampled from an unstructured dataset. In contrast, batches\nfor forecasting are generated by (1) sampling sequentially structured time\nseries from a dataset, (2) sampling contiguous subsequences from these series,\nand (3) partitioning them into context and ground-truth forecast windows. We\ntheoretically analyze the privacy amplification attained by this structured\nsubsampling to enable the training of forecasting models with sound and tight\nevent- and user-level privacy guarantees. Towards more private models, we\nadditionally prove how data augmentation amplifies privacy in self-supervised\ntraining of sequence models. Our empirical evaluation demonstrates that\namplification by structured subsampling enables the training of forecasting\nmodels with strong formal privacy guarantees.",
    "updated" : "2025-02-04T15:29:00Z",
    "published" : "2025-02-04T15:29:00Z",
    "authors" : [
      {
        "name" : "Jan Schuchardt"
      },
      {
        "name" : "Mina Dalirrooyfard"
      },
      {
        "name" : "Jed Guzelkabaagac"
      },
      {
        "name" : "Anderson Schneider"
      },
      {
        "name" : "Yuriy Nevmyvaka"
      },
      {
        "name" : "Stephan Günnemann"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.01904v1",
    "title" : "Common Neighborhood Estimation over Bipartite Graphs under Local\n  Differential Privacy",
    "summary" : "Bipartite graphs, formed by two vertex layers, arise as a natural fit for\nmodeling the relationships between two groups of entities. In bipartite graphs,\ncommon neighborhood computation between two vertices on the same vertex layer\nis a basic operator, which is easily solvable in general settings. However, it\ninevitably involves releasing the neighborhood information of vertices, posing\na significant privacy risk for users in real-world applications. To protect\nedge privacy in bipartite graphs, in this paper, we study the problem of\nestimating the number of common neighbors of two vertices on the same layer\nunder edge local differential privacy (edge LDP). The problem is challenging in\nthe context of edge LDP since each vertex on the opposite layer of the query\nvertices can potentially be a common neighbor. To obtain efficient and accurate\nestimates, we propose a multiple-round framework that significantly reduces the\ncandidate pool of common neighbors and enables the query vertices to construct\nunbiased estimators locally. Furthermore, we improve data utility by\nincorporating the estimators built from the neighbors of both query vertices\nand devise privacy budget allocation optimizations. These improve the\nestimator's robustness and consistency, particularly against query vertices\nwith imbalanced degrees. Extensive experiments on 15 datasets validate the\neffectiveness and efficiency of our proposed techniques.",
    "updated" : "2025-02-04T00:33:18Z",
    "published" : "2025-02-04T00:33:18Z",
    "authors" : [
      {
        "name" : "Yizhang He"
      },
      {
        "name" : "Kai Wang"
      },
      {
        "name" : "Wenjie Zhang"
      },
      {
        "name" : "Xuemin Lin"
      },
      {
        "name" : "Ying Zhang"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.01885v1",
    "title" : "A Privacy-Preserving Domain Adversarial Federated learning for\n  multi-site brain functional connectivity analysis",
    "summary" : "Resting-state functional magnetic resonance imaging (rs-fMRI) and its derived\nfunctional connectivity networks (FCNs) have become critical for understanding\nneurological disorders. However, collaborative analyses and the\ngeneralizability of models still face significant challenges due to privacy\nregulations and the non-IID (non-independent and identically distributed)\nproperty of multiple data sources. To mitigate these difficulties, we propose\nDomain Adversarial Federated Learning (DAFed), a novel federated deep learning\nframework specifically designed for non-IID fMRI data analysis in multi-site\nsettings. DAFed addresses these challenges through feature disentanglement,\ndecomposing the latent feature space into domain-invariant and domain-specific\ncomponents, to ensure robust global learning while preserving local data\nspecificity. Furthermore, adversarial training facilitates effective knowledge\ntransfer between labeled and unlabeled datasets, while a contrastive learning\nmodule enhances the global representation of domain-invariant features. We\nevaluated DAFed on the diagnosis of ASD and further validated its\ngeneralizability in the classification of AD, demonstrating its superior\nclassification accuracy compared to state-of-the-art methods. Additionally, an\nenhanced Score-CAM module identifies key brain regions and functional\nconnectivity significantly associated with ASD and MCI, respectively,\nuncovering shared neurobiological patterns across sites. These findings\nhighlight the potential of DAFed to advance multi-site collaborative research\nin neuroimaging while protecting data confidentiality.",
    "updated" : "2025-02-03T23:26:07Z",
    "published" : "2025-02-03T23:26:07Z",
    "authors" : [
      {
        "name" : "Yipu Zhang"
      },
      {
        "name" : "Likai Wang"
      },
      {
        "name" : "Kuan-Jui Su"
      },
      {
        "name" : "Aiying Zhang"
      },
      {
        "name" : "Hao Zhu"
      },
      {
        "name" : "Xiaowen Liu"
      },
      {
        "name" : "Hui Shen"
      },
      {
        "name" : "Vince D. Calhoun"
      },
      {
        "name" : "Yuping Wang"
      },
      {
        "name" : "Hongwen Deng"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.01352v1",
    "title" : "Metric Privacy in Federated Learning for Medical Imaging: Improving\n  Convergence and Preventing Client Inference Attacks",
    "summary" : "Federated learning is a distributed learning technique that allows training a\nglobal model with the participation of different data owners without the need\nto share raw data. This architecture is orchestrated by a central server that\naggregates the local models from the clients. This server may be trusted, but\nnot all nodes in the network. Then, differential privacy (DP) can be used to\nprivatize the global model by adding noise. However, this may affect\nconvergence across the rounds of the federated architecture, depending also on\nthe aggregation strategy employed. In this work, we aim to introduce the notion\nof metric-privacy to mitigate the impact of classical server side global-DP on\nthe convergence of the aggregated model. Metric-privacy is a relaxation of DP,\nsuitable for domains provided with a notion of distance. We apply it from the\nserver side by computing a distance for the difference between the local\nmodels. We compare our approach with standard DP by analyzing the impact on six\nclassical aggregation strategies. The proposed methodology is applied to an\nexample of medical imaging and different scenarios are simulated across\nhomogeneous and non-i.i.d clients. Finally, we introduce a novel client\ninference attack, where a semi-honest client tries to find whether another\nclient participated in the training and study how it can be mitigated using DP\nand metric-privacy. Our evaluation shows that metric-privacy can increase the\nperformance of the model compared to standard DP, while offering similar\nprotection against client inference attacks.",
    "updated" : "2025-02-03T13:41:52Z",
    "published" : "2025-02-03T13:41:52Z",
    "authors" : [
      {
        "name" : "Judith Sáinz-Pardo Díaz"
      },
      {
        "name" : "Andreas Athanasiou"
      },
      {
        "name" : "Kangsoo Jung"
      },
      {
        "name" : "Catuscia Palamidessi"
      },
      {
        "name" : "Álvaro López García"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.01306v1",
    "title" : "Expert-Generated Privacy Q&A Dataset for Conversational AI and User\n  Study Insights",
    "summary" : "Conversational assistants process personal data and must comply with data\nprotection regulations that require providers to be transparent with users\nabout how their data is handled. Transparency, in a legal sense, demands\npreciseness, comprehensibility and accessibility, yet existing solutions fail\nto meet these requirements. To address this, we introduce a new\nhuman-expert-generated dataset for Privacy Question-Answering (Q&A), developed\nthrough an iterative process involving legal professionals and conversational\ndesigners. We evaluate this dataset through linguistic analysis and a user\nstudy, comparing it to privacy policy excerpts and state-of-the-art responses\nfrom Amazon Alexa. Our findings show that the proposed answers improve\nusability and clarity compared to existing solutions while achieving legal\npreciseness, thereby enhancing the accessibility of data processing information\nfor Conversational AI and Natural Language Processing applications.",
    "updated" : "2025-02-03T12:30:45Z",
    "published" : "2025-02-03T12:30:45Z",
    "authors" : [
      {
        "name" : "Anna Leschanowsky"
      },
      {
        "name" : "Farnaz Salamatjoo"
      },
      {
        "name" : "Zahra Kolagar"
      },
      {
        "name" : "Birgit Popp"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.00760v1",
    "title" : "Privacy Preserving Properties of Vision Classifiers",
    "summary" : "Vision classifiers are often trained on proprietary datasets containing\nsensitive information, yet the models themselves are frequently shared openly\nunder the privacy-preserving assumption. Although these models are assumed to\nprotect sensitive information in their training data, the extent to which this\nassumption holds for different architectures remains unexplored. This\nassumption is challenged by inversion attacks which attempt to reconstruct\ntraining data from model weights, exposing significant privacy vulnerabilities.\nIn this study, we systematically evaluate the privacy-preserving properties of\nvision classifiers across diverse architectures, including Multi-Layer\nPerceptrons (MLPs), Convolutional Neural Networks (CNNs), and Vision\nTransformers (ViTs). Using network inversion-based reconstruction techniques,\nwe assess the extent to which these architectures memorize and reveal training\ndata, quantifying the relative ease of reconstruction across models. Our\nanalysis highlights how architectural differences, such as input\nrepresentation, feature extraction mechanisms, and weight structures, influence\nprivacy risks. By comparing these architectures, we identify which are more\nresilient to inversion attacks and examine the trade-offs between model\nperformance and privacy preservation, contributing to the development of secure\nand privacy-respecting machine learning models for sensitive applications. Our\nfindings provide actionable insights into the design of secure and\nprivacy-aware machine learning systems, emphasizing the importance of\nevaluating architectural decisions in sensitive applications involving\nproprietary or personal data.",
    "updated" : "2025-02-02T11:50:00Z",
    "published" : "2025-02-02T11:50:00Z",
    "authors" : [
      {
        "name" : "Pirzada Suhail"
      },
      {
        "name" : "Amit Sethi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.00693v1",
    "title" : "DPBloomfilter: Securing Bloom Filters with Differential Privacy",
    "summary" : "The Bloom filter is a simple yet space-efficient probabilistic data structure\nthat supports membership queries for dramatically large datasets. It is widely\nutilized and implemented across various industrial scenarios, often handling\nmassive datasets that include sensitive user information necessitating privacy\npreservation. To address the challenge of maintaining privacy within the Bloom\nfilter, we have developed the DPBloomfilter. This innovation integrates the\nclassical differential privacy mechanism, specifically the Random Response\ntechnique, into the Bloom filter, offering robust privacy guarantees under the\nsame running complexity as the standard Bloom filter. Through rigorous\nsimulation experiments, we have demonstrated that our DPBloomfilter algorithm\nmaintains high utility while ensuring privacy protections. To the best of our\nknowledge, this is the first work to provide differential privacy guarantees\nfor the Bloom filter for membership query problems.",
    "updated" : "2025-02-02T06:47:50Z",
    "published" : "2025-02-02T06:47:50Z",
    "authors" : [
      {
        "name" : "Yekun Ke"
      },
      {
        "name" : "Yingyu Liang"
      },
      {
        "name" : "Zhizhou Sha"
      },
      {
        "name" : "Zhenmei Shi"
      },
      {
        "name" : "Zhao Song"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.00451v1",
    "title" : "Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and\n  Opportunities",
    "summary" : "Mental illness is a widespread and debilitating condition with substantial\nsocietal and personal costs. Traditional diagnostic and treatment approaches,\nsuch as self-reported questionnaires and psychotherapy sessions, often impose\nsignificant burdens on both patients and clinicians, limiting accessibility and\nefficiency. Recent advances in Artificial Intelligence (AI), particularly in\nNatural Language Processing and multimodal techniques, hold great potential for\nrecognizing and addressing conditions such as depression, anxiety, bipolar\ndisorder, schizophrenia, and post-traumatic stress disorder. However, privacy\nconcerns, including the risk of sensitive data leakage from datasets and\ntrained models, remain a critical barrier to deploying these AI systems in\nreal-world clinical settings. These challenges are amplified in multimodal\nmethods, where personal identifiers such as voice and facial data can be\nmisused. This paper presents a critical and comprehensive study of the privacy\nchallenges associated with developing and deploying AI models for mental\nhealth. We further prescribe potential solutions, including data anonymization,\nsynthetic data generation, and privacy-preserving model training, to strengthen\nprivacy safeguards in practical applications. Additionally, we discuss\nevaluation frameworks to assess the privacy-utility trade-offs in these\napproaches. By addressing these challenges, our work aims to advance the\ndevelopment of reliable, privacy-aware AI tools to support clinical\ndecision-making and improve mental health outcomes.",
    "updated" : "2025-02-01T15:10:02Z",
    "published" : "2025-02-01T15:10:02Z",
    "authors" : [
      {
        "name" : "Aishik Mandal"
      },
      {
        "name" : "Tanmoy Chakraborty"
      },
      {
        "name" : "Iryna Gurevych"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02913v1",
    "title" : "Privacy Token: Surprised to Find Out What You Accidentally Revealed",
    "summary" : "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.",
    "updated" : "2025-02-05T06:20:20Z",
    "published" : "2025-02-05T06:20:20Z",
    "authors" : [
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Xin Shi"
      },
      {
        "name" : "Qingyu Huang"
      },
      {
        "name" : "Chen Hou"
      },
      {
        "name" : "Hong Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02749v1",
    "title" : "Unveiling Privacy and Security Gaps in Female Health Apps",
    "summary" : "Female Health Applications (FHA), a growing segment of FemTech, aim to\nprovide affordable and accessible healthcare solutions for women globally.\nThese applications gather and monitor health and reproductive data from\nmillions of users. With ongoing debates on women's reproductive rights and\nprivacy, it's crucial to assess how these apps protect users' privacy. In this\npaper, we undertake a security and data protection assessment of 45 popular\nFHAs. Our investigation uncovers harmful permissions, extensive collection of\nsensitive personal and medical data, and the presence of numerous third-party\ntracking libraries. Furthermore, our examination of their privacy policies\nreveals deviations from fundamental data privacy principles. These findings\nhighlight a significant lack of privacy and security measures for FemTech apps,\nespecially as women's reproductive rights face growing political challenges.\nThe results and recommendations provide valuable insights for users, app\ndevelopers, and policymakers, paving the way for better privacy and security in\nFemale Health Applications.",
    "updated" : "2025-02-04T22:34:03Z",
    "published" : "2025-02-04T22:34:03Z",
    "authors" : [
      {
        "name" : "Muhammad Hassan"
      },
      {
        "name" : "Mahnoor Jameel"
      },
      {
        "name" : "Tian Wang"
      },
      {
        "name" : "Masooda Bashir"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.04045v1",
    "title" : "Comparing privacy notions for protection against reconstruction attacks\n  in machine learning",
    "summary" : "Within the machine learning community, reconstruction attacks are a principal\nconcern and have been identified even in federated learning (FL), which was\ndesigned with privacy preservation in mind. In response to these threats, the\nprivacy community recommends the use of differential privacy (DP) in the\nstochastic gradient descent algorithm, termed DP-SGD. However, the\nproliferation of variants of DP in recent years\\textemdash such as metric\nprivacy\\textemdash has made it challenging to conduct a fair comparison between\ndifferent mechanisms due to the different meanings of the privacy parameters\n$\\epsilon$ and $\\delta$ across different variants. Thus, interpreting the\npractical implications of $\\epsilon$ and $\\delta$ in the FL context and amongst\nvariants of DP remains ambiguous. In this paper, we lay a foundational\nframework for comparing mechanisms with differing notions of privacy\nguarantees, namely $(\\epsilon,\\delta)$-DP and metric privacy. We provide two\nfoundational means of comparison: firstly, via the well-established\n$(\\epsilon,\\delta)$-DP guarantees, made possible through the R\\'enyi\ndifferential privacy framework; and secondly, via Bayes' capacity, which we\nidentify as an appropriate measure for reconstruction threats.",
    "updated" : "2025-02-06T13:04:25Z",
    "published" : "2025-02-06T13:04:25Z",
    "authors" : [
      {
        "name" : "Sayan Biswas"
      },
      {
        "name" : "Mark Dras"
      },
      {
        "name" : "Pedro Faustini"
      },
      {
        "name" : "Natasha Fernandes"
      },
      {
        "name" : "Annabelle McIver"
      },
      {
        "name" : "Catuscia Palamidessi"
      },
      {
        "name" : "Parastoo Sadeghi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.03811v1",
    "title" : "Privacy Risks in Health Big Data: A Systematic Literature Review",
    "summary" : "The digitization of health records has greatly improved the efficiency of the\nhealthcare system and promoted the formulation of related research and\npolicies. However, the widespread application of advanced technologies such as\nelectronic health records, genomic data, and wearable devices in the field of\nhealth big data has also intensified the collection of personal sensitive data,\nbringing serious privacy and security issues. Based on a systematic literature\nreview (SLR), this paper comprehensively outlines the key research in the field\nof health big data security. By analyzing existing research, this paper\nexplores how cutting-edge technologies such as homomorphic encryption,\nblockchain, federated learning, and artificial immune systems can enhance data\nsecurity while protecting personal privacy. This paper also points out the\ncurrent challenges and proposes a future research framework in this key area.",
    "updated" : "2025-02-06T06:44:36Z",
    "published" : "2025-02-06T06:44:36Z",
    "authors" : [
      {
        "name" : "Zhang Si Yuan"
      },
      {
        "name" : "Manmeet Mahinderjit Singh"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.03668v1",
    "title" : "Privacy-Preserving Generative Models: A Comprehensive Survey",
    "summary" : "Despite the generative model's groundbreaking success, the need to study its\nimplications for privacy and utility becomes more urgent. Although many studies\nhave demonstrated the privacy threats brought by GANs, no existing survey has\nsystematically categorized the privacy and utility perspectives of GANs and\nVAEs. In this article, we comprehensively study privacy-preserving generative\nmodels, articulating the novel taxonomies for both privacy and utility metrics\nby analyzing 100 research publications. Finally, we discuss the current\nchallenges and future research directions that help new researchers gain\ninsight into the underlying concepts.",
    "updated" : "2025-02-05T23:24:43Z",
    "published" : "2025-02-05T23:24:43Z",
    "authors" : [
      {
        "name" : "Debalina Padariya"
      },
      {
        "name" : "Isabel Wagner"
      },
      {
        "name" : "Aboozar Taherkhani"
      },
      {
        "name" : "Eerke Boiten"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02913v2",
    "title" : "Privacy Token: Surprised to Find Out What You Accidentally Revealed",
    "summary" : "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.",
    "updated" : "2025-02-06T02:33:11Z",
    "published" : "2025-02-05T06:20:20Z",
    "authors" : [
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Xin Shi"
      },
      {
        "name" : "Qingyu Huang"
      },
      {
        "name" : "Chen Hou"
      },
      {
        "name" : "Hong Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.04758v1",
    "title" : "Differential Privacy of Quantum and Quantum-Inspired-Classical\n  Recommendation Algorithms",
    "summary" : "We analyze the DP (differential privacy) properties of the quantum\nrecommendation algorithm and the quantum-inspired-classical recommendation\nalgorithm. We discover that the quantum recommendation algorithm is a privacy\ncurating mechanism on its own, requiring no external noise, which is different\nfrom traditional differential privacy mechanisms. In our analysis, a novel\nperturbation method tailored for SVD (singular value decomposition) and\nlow-rank matrix approximation problems is introduced. Using the perturbation\nmethod and random matrix theory, we are able to derive that both the quantum\nand quantum-inspired-classical algorithms are\n$\\big(\\tilde{\\mathcal{O}}\\big(\\frac 1n\\big),\\,\\,\n\\tilde{\\mathcal{O}}\\big(\\frac{1}{\\min\\{m,n\\}}\\big)\\big)$-DP under some\nreasonable restrictions, where $m$ and $n$ are numbers of users and products in\nthe input preference database respectively. Nevertheless, a comparison shows\nthat the quantum algorithm has better privacy preserving potential than the\nclassical one.",
    "updated" : "2025-02-07T08:45:00Z",
    "published" : "2025-02-07T08:45:00Z",
    "authors" : [
      {
        "name" : "Chenjian Li"
      },
      {
        "name" : "Mingsheng Ying"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.ET",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.04365v1",
    "title" : "AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case\n  Study on Detecting Time of Birth",
    "summary" : "Approximately 10% of newborns need some assistance to start breathing and 5\\%\nproper ventilation. It is crucial that interventions are initiated as soon as\npossible after birth. Accurate documentation of Time of Birth (ToB) is thereby\nessential for documenting and improving newborn resuscitation performance.\nHowever, current clinical practices rely on manual recording of ToB, typically\nwith minute precision. In this study, we present an AI-driven, video-based\nsystem for automated ToB detection using thermal imaging, designed to preserve\nthe privacy of healthcare providers and mothers by avoiding the use of\nidentifiable visual data. Our approach achieves 91.4% precision and 97.4%\nrecall in detecting ToB within thermal video clips during performance\nevaluation. Additionally, our system successfully identifies ToB in 96% of test\ncases with an absolute median deviation of 1 second compared to manual\nannotations. This method offers a reliable solution for improving ToB\ndocumentation and enhancing newborn resuscitation outcomes.",
    "updated" : "2025-02-05T07:01:49Z",
    "published" : "2025-02-05T07:01:49Z",
    "authors" : [
      {
        "name" : "Jorge García-Torres"
      },
      {
        "name" : "Øyvind Meinich-Bache"
      },
      {
        "name" : "Siren Rettedal"
      },
      {
        "name" : "Kjersti Engan"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02913v3",
    "title" : "Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient\n  Leakage",
    "summary" : "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.",
    "updated" : "2025-02-07T09:10:09Z",
    "published" : "2025-02-05T06:20:20Z",
    "authors" : [
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Hong Chen"
      },
      {
        "name" : "Xin Shi"
      },
      {
        "name" : "Qingyu Huang"
      },
      {
        "name" : "Chen Hou"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.06652v1",
    "title" : "Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A",
    "summary" : "The transparency principle of the General Data Protection Regulation (GDPR)\nrequires data processing information to be clear, precise, and accessible.\nWhile language models show promise in this context, their probabilistic nature\ncomplicates truthfulness and comprehensibility.\n  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)\nsystems enhanced with alignment techniques to fulfill GDPR obligations. We\nevaluate RAG systems incorporating an alignment module like Rewindable\nAuto-regressive Inference (RAIN) and our proposed multidimensional extension,\nMultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness\nand comprehensibility and are assessed through 21 metrics, including\ndeterministic and large language model-based evaluations.\n  Our results show that RAG systems with an alignment module outperform\nbaseline RAG systems on most metrics, though none fully match human answers.\nPrincipal component analysis of the results reveals complex interactions\nbetween metrics, highlighting the need to refine metrics. This study provides a\nfoundation for integrating advanced natural language processing systems into\nlegal compliance frameworks.",
    "updated" : "2025-02-10T16:42:00Z",
    "published" : "2025-02-10T16:42:00Z",
    "authors" : [
      {
        "name" : "Anna Leschanowsky"
      },
      {
        "name" : "Zahra Kolagar"
      },
      {
        "name" : "Erion Çano"
      },
      {
        "name" : "Ivan Habernal"
      },
      {
        "name" : "Dara Hallinan"
      },
      {
        "name" : "Emanuël A. P. Habets"
      },
      {
        "name" : "Birgit Popp"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.06597v1",
    "title" : "Continual Release Moment Estimation with Differential Privacy",
    "summary" : "We propose Joint Moment Estimation (JME), a method for continually and\nprivately estimating both the first and second moments of data with reduced\nnoise compared to naive approaches. JME uses the matrix mechanism and a joint\nsensitivity analysis to allow the second moment estimation with no additional\nprivacy cost, thereby improving accuracy while maintaining privacy. We\ndemonstrate JME's effectiveness in two applications: estimating the running\nmean and covariance matrix for Gaussian density estimation, and model training\nwith DP-Adam on CIFAR-10.",
    "updated" : "2025-02-10T15:58:26Z",
    "published" : "2025-02-10T15:58:26Z",
    "authors" : [
      {
        "name" : "Nikita P. Kalinin"
      },
      {
        "name" : "Jalaj Upadhyay"
      },
      {
        "name" : "Christoph H. Lampert"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.06425v1",
    "title" : "Generating Privacy-Preserving Personalized Advice with Zero-Knowledge\n  Proofs and LLMs",
    "summary" : "Large language models (LLMs) are increasingly utilized in domains such as\nfinance, healthcare, and interpersonal relationships to provide advice tailored\nto user traits and contexts. However, this personalization often relies on\nsensitive data, raising critical privacy concerns and necessitating data\nminimization. To address these challenges, we propose a framework that\nintegrates zero-knowledge proof (ZKP) technology, specifically zkVM, with\nLLM-based chatbots. This integration enables privacy-preserving data sharing by\nverifying user traits without disclosing sensitive information. Our research\nintroduces both an architecture and a prompting strategy for this approach.\nThrough empirical evaluation, we clarify the current constraints and\nperformance limitations of both zkVM and the proposed prompting strategy,\nthereby demonstrating their practical feasibility in real-world scenarios.",
    "updated" : "2025-02-10T13:02:00Z",
    "published" : "2025-02-10T13:02:00Z",
    "authors" : [
      {
        "name" : "Hiroki Watanabe"
      },
      {
        "name" : "Motonobu Uchikoshi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.05765v1",
    "title" : "Privacy-Preserving Dataset Combination",
    "summary" : "Access to diverse, high-quality datasets is crucial for machine learning\nmodel performance, yet data sharing remains limited by privacy concerns and\ncompetitive interests, particularly in regulated domains like healthcare. This\ndynamic especially disadvantages smaller organizations that lack resources to\npurchase data or negotiate favorable sharing agreements. We present SecureKL, a\nprivacy-preserving framework that enables organizations to identify beneficial\ndata partnerships without exposing sensitive information. Building on recent\nadvances in dataset combination methods, we develop a secure multiparty\ncomputation protocol that maintains strong privacy guarantees while achieving\n>90\\% correlation with plaintext evaluations. In experiments with real-world\nhospital data, SecureKL successfully identifies beneficial data partnerships\nthat improve model performance for intensive care unit mortality prediction\nwhile preserving data privacy. Our framework provides a practical solution for\norganizations seeking to leverage collective data resources while maintaining\nprivacy and competitive advantages. These results demonstrate the potential for\nprivacy-preserving data collaboration to advance machine learning applications\nin high-stakes domains while promoting more equitable access to data resources.",
    "updated" : "2025-02-09T03:54:17Z",
    "published" : "2025-02-09T03:54:17Z",
    "authors" : [
      {
        "name" : "Keren Fuentes"
      },
      {
        "name" : "Mimee Xu"
      },
      {
        "name" : "Irene Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.05547v1",
    "title" : "Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in\n  Federated Learning",
    "summary" : "Federated learning (FL) is inherently susceptible to privacy breaches and\npoisoning attacks. To tackle these challenges, researchers have separately\ndevised secure aggregation mechanisms to protect data privacy and robust\naggregation methods that withstand poisoning attacks. However, simultaneously\naddressing both concerns is challenging; secure aggregation facilitates\npoisoning attacks as most anomaly detection techniques require access to\nunencrypted local model updates, which are obscured by secure aggregation. Few\nrecent efforts to simultaneously tackle both challenges offen depend on\nimpractical assumption of non-colluding two-server setups that disrupt FL's\ntopology, or three-party computation which introduces scalability issues,\ncomplicating deployment and application. To overcome this dilemma, this paper\nintroduce a Dual Defense Federated learning (DDFed) framework. DDFed\nsimultaneously boosts privacy protection and mitigates poisoning attacks,\nwithout introducing new participant roles or disrupting the existing FL\ntopology. DDFed initially leverages cutting-edge fully homomorphic encryption\n(FHE) to securely aggregate model updates, without the impractical requirement\nfor non-colluding two-server setups and ensures strong privacy protection.\nAdditionally, we proposes a unique two-phase anomaly detection mechanism for\nencrypted model updates, featuring secure similarity computation and\nfeedback-driven collaborative selection, with additional measures to prevent\npotential privacy breaches from Byzantine clients incorporated into the\ndetection process. We conducted extensive experiments on various model\npoisoning attacks and FL scenarios, including both cross-device and cross-silo\nFL. Experiments on publicly available datasets demonstrate that DDFed\nsuccessfully protects model privacy and effectively defends against model\npoisoning threats.",
    "updated" : "2025-02-08T12:28:20Z",
    "published" : "2025-02-08T12:28:20Z",
    "authors" : [
      {
        "name" : "Runhua Xu"
      },
      {
        "name" : "Shiqi Gao"
      },
      {
        "name" : "Chao Li"
      },
      {
        "name" : "James Joshi"
      },
      {
        "name" : "Jianxin Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.05516v1",
    "title" : "Evaluating Differential Privacy on Correlated Datasets Using Pointwise\n  Maximal Leakage",
    "summary" : "Data-driven advancements significantly contribute to societal progress, yet\nthey also pose substantial risks to privacy. In this landscape, differential\nprivacy (DP) has become a cornerstone in privacy preservation efforts. However,\nthe adequacy of DP in scenarios involving correlated datasets has sometimes\nbeen questioned and multiple studies have hinted at potential vulnerabilities.\nIn this work, we delve into the nuances of applying DP to correlated datasets\nby leveraging the concept of pointwise maximal leakage (PML) for a quantitative\nassessment of information leakage. Our investigation reveals that DP's\nguarantees can be arbitrarily weak for correlated databases when assessed\nthrough the lens of PML. More precisely, we prove the existence of a pure DP\nmechanism with PML levels arbitrarily close to that of a mechanism which\nreleases individual entries from a database without any perturbation. By\nshedding light on the limitations of DP on correlated datasets, our work aims\nto foster a deeper understanding of subtle privacy risks and highlight the need\nfor the development of more effective privacy-preserving mechanisms tailored to\ndiverse scenarios.",
    "updated" : "2025-02-08T10:30:45Z",
    "published" : "2025-02-08T10:30:45Z",
    "authors" : [
      {
        "name" : "Sara Saeidian"
      },
      {
        "name" : "Tobias J. Oechtering"
      },
      {
        "name" : "Mikael Skoglund"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.05509v1",
    "title" : "Do Spikes Protect Privacy? Investigating Black-Box Model Inversion\n  Attacks in Spiking Neural Networks",
    "summary" : "As machine learning models become integral to security-sensitive\napplications, concerns over data leakage from adversarial attacks continue to\nrise. Model Inversion (MI) attacks pose a significant privacy threat by\nenabling adversaries to reconstruct training data from model outputs. While MI\nattacks on Artificial Neural Networks (ANNs) have been widely studied, Spiking\nNeural Networks (SNNs) remain largely unexplored in this context. Due to their\nevent-driven and discrete computations, SNNs introduce fundamental differences\nin information processing that may offer inherent resistance to such attacks. A\ncritical yet underexplored aspect of this threat lies in black-box settings,\nwhere attackers operate through queries without direct access to model\nparameters or gradients-representing a more realistic adversarial scenario in\ndeployed systems. This work presents the first study of black-box MI attacks on\nSNNs. We adapt a generative adversarial MI framework to the spiking domain by\nincorporating rate-based encoding for input transformation and decoding\nmechanisms for output interpretation. Our results show that SNNs exhibit\nsignificantly greater resistance to MI attacks than ANNs, as demonstrated by\ndegraded reconstructions, increased instability in attack convergence, and\noverall reduced attack effectiveness across multiple evaluation metrics.\nFurther analysis suggests that the discrete and temporally distributed nature\nof SNN decision boundaries disrupts surrogate modeling, limiting the attacker's\nability to approximate the target model.",
    "updated" : "2025-02-08T10:02:27Z",
    "published" : "2025-02-08T10:02:27Z",
    "authors" : [
      {
        "name" : "Hamed Poursiami"
      },
      {
        "name" : "Ayana Moshruba"
      },
      {
        "name" : "Maryam Parsa"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.05219v1",
    "title" : "Enabling External Scrutiny of AI Systems with Privacy-Enhancing\n  Technologies",
    "summary" : "This article describes how technical infrastructure developed by the\nnonprofit OpenMined enables external scrutiny of AI systems without\ncompromising sensitive information.\n  Independent external scrutiny of AI systems provides crucial transparency\ninto AI development, so it should be an integral component of any approach to\nAI governance. In practice, external researchers have struggled to gain access\nto AI systems because of AI companies' legitimate concerns about security,\nprivacy, and intellectual property.\n  But now, privacy-enhancing technologies (PETs) have reached a new level of\nmaturity: end-to-end technical infrastructure developed by OpenMined combines\nseveral PETs into various setups that enable privacy-preserving audits of AI\nsystems. We showcase two case studies where this infrastructure has been\ndeployed in real-world governance scenarios: \"Understanding Social Media\nRecommendation Algorithms with the Christchurch Call\" and \"Evaluating Frontier\nModels with the UK AI Safety Institute.\" We describe types of scrutiny of AI\nsystems that could be facilitated by current setups and OpenMined's proposed\nfuture setups.\n  We conclude that these innovative approaches deserve further exploration and\nsupport from the AI governance community. Interested policymakers can focus on\nempowering researchers on a legal level.",
    "updated" : "2025-02-05T15:31:11Z",
    "published" : "2025-02-05T15:31:11Z",
    "authors" : [
      {
        "name" : "Kendrea Beers"
      },
      {
        "name" : "Helen Toner"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.07693v1",
    "title" : "SoK: A Classification for AI-driven Personalized Privacy Assistants",
    "summary" : "To help users make privacy-related decisions, personalized privacy assistants\nbased on AI technology have been developed in recent years. These AI-driven\nPersonalized Privacy Assistants (AI-driven PPAs) can reap significant benefits\nfor users, who may otherwise struggle to make decisions regarding their\npersonal data in environments saturated with privacy-related decision requests.\nHowever, no study systematically inquired about the features of these AI-driven\nPPAs, their underlying technologies, or the accuracy of their decisions. To\nfill this gap, we present a Systematization of Knowledge (SoK) to map the\nexisting solutions found in the scientific literature. We screened 1697 unique\nresearch papers over the last decade (2013-2023), constructing a classification\nfrom 39 included papers. As a result, this SoK reviews several aspects of\nexisting research on AI-driven PPAs in terms of types of publications,\ncontributions, methodological quality, and other quantitative insights.\nFurthermore, we provide a comprehensive classification for AI-driven PPAs,\ndelving into their architectural choices, system contexts, types of AI used,\ndata sources, types of decisions, and control over decisions, among other\nfacets. Based on our SoK, we further underline the research gaps and challenges\nand formulate recommendations for the design and development of AI-driven PPAs\nas well as avenues for future research.",
    "updated" : "2025-02-11T16:46:56Z",
    "published" : "2025-02-11T16:46:56Z",
    "authors" : [
      {
        "name" : "Victor Morel"
      },
      {
        "name" : "Leonardo Iwaya"
      },
      {
        "name" : "Simone Fischer-Hübner"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08202v1",
    "title" : "Privacy amplification by random allocation",
    "summary" : "We consider the privacy guarantees of an algorithm in which a user's data is\nused in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$\ndifferentially private steps. We demonstrate that the privacy guarantees of\nthis sampling scheme can be upper bound by the privacy guarantees of the\nwell-studied independent (or Poisson) subsampling in which each step uses the\nuser's data with probability $(1+ o(1))k/t $. Further, we provide two\nadditional analysis techniques that lead to numerical improvements in some\nparameter regimes. The case of $k=1$ has been previously studied in the context\nof DP-SGD in Balle et al. (2020) and very recently in Chua et al. (2024).\nPrivacy analysis of Balle et al. (2020) relies on privacy amplification by\nshuffling which leads to overly conservative bounds. Privacy analysis of Chua\net al. (2024a) relies on Monte Carlo simulations that are computationally\nprohibitive in many practical scenarios and have additional inherent\nlimitations.",
    "updated" : "2025-02-12T08:32:10Z",
    "published" : "2025-02-12T08:32:10Z",
    "authors" : [
      {
        "name" : "Vitaly Feldman"
      },
      {
        "name" : "Moshe Shenfeld"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08151v1",
    "title" : "Local Differential Privacy is Not Enough: A Sample Reconstruction Attack\n  against Federated Learning with Local Differential Privacy",
    "summary" : "Reconstruction attacks against federated learning (FL) aim to reconstruct\nusers' samples through users' uploaded gradients. Local differential privacy\n(LDP) is regarded as an effective defense against various attacks, including\nsample reconstruction in FL, where gradients are clipped and perturbed.\nExisting attacks are ineffective in FL with LDP since clipped and perturbed\ngradients obliterate most sample information for reconstruction. Besides,\nexisting attacks embed additional sample information into gradients to improve\nthe attack effect and cause gradient expansion, leading to a more severe\ngradient clipping in FL with LDP. In this paper, we propose a sample\nreconstruction attack against LDP-based FL with any target models to\nreconstruct victims' sensitive samples to illustrate that FL with LDP is not\nflawless. Considering gradient expansion in reconstruction attacks and noise in\nLDP, the core of the proposed attack is gradient compression and reconstructed\nsample denoising. For gradient compression, an inference structure based on\nsample characteristics is presented to reduce redundant gradients against LDP.\nFor reconstructed sample denoising, we artificially introduce zero gradients to\nobserve noise distribution and scale confidence interval to filter the noise.\nTheoretical proof guarantees the effectiveness of the proposed attack.\nEvaluations show that the proposed attack is the only attack that reconstructs\nvictims' training samples in LDP-based FL and has little impact on the target\nmodel's accuracy. We conclude that LDP-based FL needs further improvements to\ndefend against sample reconstruction attacks effectively.",
    "updated" : "2025-02-12T06:37:26Z",
    "published" : "2025-02-12T06:37:26Z",
    "authors" : [
      {
        "name" : "Zhichao You"
      },
      {
        "name" : "Xuewen Dong"
      },
      {
        "name" : "Shujun Li"
      },
      {
        "name" : "Ximeng Liu"
      },
      {
        "name" : "Siqi Ma"
      },
      {
        "name" : "Yulong Shen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08008v1",
    "title" : "An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models",
    "summary" : "Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.",
    "updated" : "2025-02-11T23:07:14Z",
    "published" : "2025-02-11T23:07:14Z",
    "authors" : [
      {
        "name" : "Kasra Ahmadi"
      },
      {
        "name" : "Rouzbeh Behnia"
      },
      {
        "name" : "Reza Ebrahimi"
      },
      {
        "name" : "Mehran Mozaffari Kermani"
      },
      {
        "name" : "Jeremiah Birrell"
      },
      {
        "name" : "Jason Pacheco"
      },
      {
        "name" : "Attila A Yavuz"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08001v1",
    "title" : "Unveiling Client Privacy Leakage from Public Dataset Usage in Federated\n  Distillation",
    "summary" : "Federated Distillation (FD) has emerged as a popular federated training\nframework, enabling clients to collaboratively train models without sharing\nprivate data. Public Dataset-Assisted Federated Distillation (PDA-FD), which\nleverages public datasets for knowledge sharing, has become widely adopted.\nAlthough PDA-FD enhances privacy compared to traditional Federated Learning, we\ndemonstrate that the use of public datasets still poses significant privacy\nrisks to clients' private training data. This paper presents the first\ncomprehensive privacy analysis of PDA-FD in presence of an honest-but-curious\nserver. We show that the server can exploit clients' inference results on\npublic datasets to extract two critical types of private information: label\ndistributions and membership information of the private training dataset. To\nquantify these vulnerabilities, we introduce two novel attacks specifically\ndesigned for the PDA-FD setting: a label distribution inference attack and\ninnovative membership inference methods based on Likelihood Ratio Attack\n(LiRA). Through extensive evaluation of three representative PDA-FD frameworks\n(FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance,\nwith label distribution attacks reaching minimal KL-divergence and membership\ninference attacks maintaining high True Positive Rates under low False Positive\nRate constraints. Our findings reveal significant privacy risks in current\nPDA-FD frameworks and emphasize the need for more robust privacy protection\nmechanisms in collaborative learning systems.",
    "updated" : "2025-02-11T22:48:49Z",
    "published" : "2025-02-11T22:48:49Z",
    "authors" : [
      {
        "name" : "Haonan Shi"
      },
      {
        "name" : "Tu Ouyang"
      },
      {
        "name" : "An Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.07693v2",
    "title" : "SoK: A Classification for AI-driven Personalized Privacy Assistants",
    "summary" : "To help users make privacy-related decisions, personalized privacy assistants\nbased on AI technology have been developed in recent years. These AI-driven\nPersonalized Privacy Assistants (AI-driven PPAs) can reap significant benefits\nfor users, who may otherwise struggle to make decisions regarding their\npersonal data in environments saturated with privacy-related decision requests.\nHowever, no study systematically inquired about the features of these AI-driven\nPPAs, their underlying technologies, or the accuracy of their decisions. To\nfill this gap, we present a Systematization of Knowledge (SoK) to map the\nexisting solutions found in the scientific literature. We screened 1697 unique\nresearch papers over the last decade (2013-2023), constructing a classification\nfrom 39 included papers. As a result, this SoK reviews several aspects of\nexisting research on AI-driven PPAs in terms of types of publications,\ncontributions, methodological quality, and other quantitative insights.\nFurthermore, we provide a comprehensive classification for AI-driven PPAs,\ndelving into their architectural choices, system contexts, types of AI used,\ndata sources, types of decisions, and control over decisions, among other\nfacets. Based on our SoK, we further underline the research gaps and challenges\nand formulate recommendations for the design and development of AI-driven PPAs\nas well as avenues for future research.",
    "updated" : "2025-02-12T16:52:51Z",
    "published" : "2025-02-11T16:46:56Z",
    "authors" : [
      {
        "name" : "Victor Morel"
      },
      {
        "name" : "Leonardo Iwaya"
      },
      {
        "name" : "Simone Fischer-Hübner"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.02913v4",
    "title" : "Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient\n  Leakage",
    "summary" : "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.",
    "updated" : "2025-02-12T04:59:16Z",
    "published" : "2025-02-05T06:20:20Z",
    "authors" : [
      {
        "name" : "Jiayang Meng"
      },
      {
        "name" : "Tao Huang"
      },
      {
        "name" : "Hong Chen"
      },
      {
        "name" : "Xin Shi"
      },
      {
        "name" : "Qingyu Huang"
      },
      {
        "name" : "Chen Hou"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.09001v1",
    "title" : "Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection:\n  Balancing Security and Data Protection",
    "summary" : "Privacy-preserving network anomaly detection has become an essential area of\nresearch due to growing concerns over the protection of sensitive data.\nTraditional anomaly de- tection models often prioritize accuracy while\nneglecting the critical aspect of privacy. In this work, we propose a hybrid\nensemble model that incorporates privacy-preserving techniques to address both\ndetection accuracy and data protection. Our model combines the strengths of\nseveral machine learning algo- rithms, including K-Nearest Neighbors (KNN),\nSupport Vector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN),\nto create a robust system capable of identifying network anomalies while\nensuring privacy. The proposed approach in- tegrates advanced preprocessing\ntechniques that enhance data quality and address the challenges of small sample\nsizes and imbalanced datasets. By embedding privacy measures into the model\ndesign, our solution offers a significant advancement over existing methods,\nensuring both enhanced detection performance and strong privacy safeguards.",
    "updated" : "2025-02-13T06:33:16Z",
    "published" : "2025-02-13T06:33:16Z",
    "authors" : [
      {
        "name" : "Shaobo Liu"
      },
      {
        "name" : "Zihao Zhao"
      },
      {
        "name" : "Weijie He"
      },
      {
        "name" : "Jiren Wang"
      },
      {
        "name" : "Jing Peng"
      },
      {
        "name" : "Haoyuan Ma"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08989v1",
    "title" : "RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency\n  Detection in Privacy-Preserving Federated Learning",
    "summary" : "Federated Learning (FL) allows users to collaboratively train a global\nmachine learning model by sharing local model only, without exposing their\nprivate data to a central server. This distributed learning is particularly\nappealing in scenarios where data privacy is crucial, and it has garnered\nsubstantial attention from both industry and academia. However, studies have\nrevealed privacy vulnerabilities in FL, where adversaries can potentially infer\nsensitive information from the shared model parameters. In this paper, we\npresent an efficient masking-based secure aggregation scheme utilizing\nlightweight cryptographic primitives to mitigate privacy risks. Our scheme\noffers several advantages over existing methods. First, it requires only a\nsingle setup phase for the entire FL training session, significantly reducing\ncommunication overhead. Second, it minimizes user-side overhead by eliminating\nthe need for user-to-user interactions, utilizing an intermediate server layer\nand a lightweight key negotiation method. Third, the scheme is highly resilient\nto user dropouts, and the users can join at any FL round. Fourth, it can detect\nand defend against malicious server activities, including recently discovered\nmodel inconsistency attacks. Finally, our scheme ensures security in both\nsemi-honest and malicious settings. We provide security analysis to formally\nprove the robustness of our approach. Furthermore, we implemented an end-to-end\nprototype of our scheme. We conducted comprehensive experiments and\ncomparisons, which show that it outperforms existing solutions in terms of\ncommunication and computation overhead, functionality, and security.",
    "updated" : "2025-02-13T06:01:09Z",
    "published" : "2025-02-13T06:01:09Z",
    "authors" : [
      {
        "name" : "Nazatul H. Sultan"
      },
      {
        "name" : "Yan Bo"
      },
      {
        "name" : "Yansong Gao"
      },
      {
        "name" : "Seyit Camtepe"
      },
      {
        "name" : "Arash Mahboubi"
      },
      {
        "name" : "Hang Thanh Bui"
      },
      {
        "name" : "Aufeef Chauhan"
      },
      {
        "name" : "Hamed Aboutorab"
      },
      {
        "name" : "Michael Bewong"
      },
      {
        "name" : "Praveen Gauravaram"
      },
      {
        "name" : "Rafiqul Islam"
      },
      {
        "name" : "Sharif Abuadbba"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "68P27",
      "E.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08970v1",
    "title" : "A Decade of Metric Differential Privacy: Advancements and Applications",
    "summary" : "Metric Differential Privacy (mDP) builds upon the core principles of\nDifferential Privacy (DP) by incorporating various distance metrics, which\noffer adaptable and context-sensitive privacy guarantees for a wide range of\napplications, such as location-based services, text analysis, and image\nprocessing. Since its inception in 2013, mDP has garnered substantial research\nattention, advancing theoretical foundations, algorithm design, and practical\nimplementations. Despite this progress, existing surveys mainly focus on\ntraditional DP and local DP, and they provide limited coverage of mDP. This\npaper provides a comprehensive survey of mDP research from 2013 to 2024,\ntracing its development from the foundations of DP. We categorize essential\nmechanisms, including Laplace, Exponential, and optimization-based approaches,\nand assess their strengths, limitations, and application domains. Additionally,\nwe highlight key challenges and outline future research directions to encourage\ninnovation and real-world adoption of mDP. This survey is designed to be a\nvaluable resource for researchers and practitioners aiming to deepen their\nunderstanding and drive progress in mDP within the broader privacy ecosystem.",
    "updated" : "2025-02-13T05:18:24Z",
    "published" : "2025-02-13T05:18:24Z",
    "authors" : [
      {
        "name" : "Xinpeng Xie"
      },
      {
        "name" : "Chenyang Yu"
      },
      {
        "name" : "Yan Huang"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Chenxi Qiu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08966v1",
    "title" : "RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage",
    "summary" : "Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external\ntools for tasks beyond their standalone capabilities, such as searching\nwebsites, booking flights, or making financial transactions. However, these\ntools greatly increase the risks of prompt injection attacks, where malicious\ncontent hijacks the LM agent to leak confidential data or trigger harmful\nactions. Existing defenses (OpenAI GPTs) require user confirmation before every\ntool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS),\nwhich automatically detects and executes tool calls that preserve integrity and\nconfidentiality, requiring user confirmation only when these safeguards cannot\nbe ensured. RTBAS adapts Information Flow Control to the unique challenges\npresented by TBAS. We present two novel dependency screeners, using\nLM-as-a-judge and attention-based saliency, to overcome these challenges.\nExperimental results on the AgentDojo Prompt Injection benchmark show RTBAS\nprevents all targeted attacks with only a 2% loss of task utility when under\nattack, and further tests confirm its ability to obtain near-oracle performance\non detecting both subtle and direct privacy leaks.",
    "updated" : "2025-02-13T05:06:22Z",
    "published" : "2025-02-13T05:06:22Z",
    "authors" : [
      {
        "name" : "Peter Yong Zhong"
      },
      {
        "name" : "Siyuan Chen"
      },
      {
        "name" : "Ruiqi Wang"
      },
      {
        "name" : "McKenna McCall"
      },
      {
        "name" : "Ben L. Titzer"
      },
      {
        "name" : "Heather Miller"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.09744v1",
    "title" : "Fine-Tuning Foundation Models with Federated Learning for Privacy\n  Preserving Medical Time Series Forecasting",
    "summary" : "Federated Learning (FL) provides a decentralized machine learning approach,\nwhere multiple devices or servers collaboratively train a model without sharing\ntheir raw data, thus enabling data privacy. This approach has gained\nsignificant interest in academia and industry due to its privacy-preserving\nproperties, which are particularly valuable in the medical domain where data\navailability is often protected under strict regulations. A relatively\nunexplored area is the use of FL to fine-tune Foundation Models (FMs) for time\nseries forecasting, potentially enhancing model efficacy by overcoming data\nlimitation while maintaining privacy. In this paper, we fine-tuned time series\nFMs with Electrocardiogram (ECG) and Impedance Cardiography (ICG) data using\ndifferent FL techniques. We then examined various scenarios and discussed the\nchallenges FL faces under different data heterogeneity configurations. Our\nempirical results demonstrated that while FL can be effective for fine-tuning\nFMs on time series forecasting tasks, its benefits depend on the data\ndistribution across clients. We highlighted the trade-offs in applying FL to FM\nfine-tuning.",
    "updated" : "2025-02-13T20:01:15Z",
    "published" : "2025-02-13T20:01:15Z",
    "authors" : [
      {
        "name" : "Mahad Ali"
      },
      {
        "name" : "Curtis Lisle"
      },
      {
        "name" : "Patrick W. Moore"
      },
      {
        "name" : "Tammer Barkouki"
      },
      {
        "name" : "Brian J. Kirkwood"
      },
      {
        "name" : "Laura J. Brattain"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.09716v1",
    "title" : "Genetic Data Governance in Crisis: Policy Recommendations for\n  Safeguarding Privacy and Preventing Discrimination",
    "summary" : "Genetic data collection has become ubiquitous today. The ability to\nmeaningfully interpret genetic data has motivated its widespread use, providing\ncrucial insights into human health and ancestry while driving important public\nhealth initiatives. Easy access to genetic testing has fueled a rapid expansion\nof recreational direct-to-consumer offerings. However, the growth of genetic\ndatasets and their applications has created significant privacy and\ndiscrimination risks, as our understanding of the scientific basis for genetic\ntraits continues to evolve. In this paper, we organize the uses of genetic data\nalong four distinct \"pillars\": clinical practice, research, forensic and\ngovernment use, and recreational use. Using our scientific understanding of\ngenetics, genetic inference methods and their associated risks, and current\npublic protections, we build a risk assessment framework that identifies key\nvalues that any governance system must preserve. We analyze case studies using\nthis framework to assess how well existing regulatory frameworks preserve\ndesired values. Our investigation reveals critical gaps in these frameworks and\nidentifies specific threats to privacy and personal liberties, particularly\nthrough genetic discrimination. We propose comprehensive policy reforms to: (1)\nupdate the legal definition of genetic data to protect against modern\ntechnological capabilities, (2) expand the Genetic Information\nNondiscrimination Act (GINA) to cover currently unprotected domains, and (3)\nestablish a unified regulatory framework under a single governing body to\noversee all applications of genetic data. We conclude with three open questions\nabout genetic data: the challenges posed by its relational nature, including\nconsent for relatives and minors; the complexities of international data\ntransfer; and its potential integration into large language models.",
    "updated" : "2025-02-13T19:05:10Z",
    "published" : "2025-02-13T19:05:10Z",
    "authors" : [
      {
        "name" : "Vivek Ramanan"
      },
      {
        "name" : "Ria Vinod"
      },
      {
        "name" : "Cole Williams"
      },
      {
        "name" : "Sohini Ramachandran"
      },
      {
        "name" : "Suresh Venkatasubramanian"
      }
    ],
    "categories" : [
      "cs.CY",
      "K.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08966v2",
    "title" : "RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage",
    "summary" : "Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external\ntools for tasks beyond their standalone capabilities, such as searching\nwebsites, booking flights, or making financial transactions. However, these\ntools greatly increase the risks of prompt injection attacks, where malicious\ncontent hijacks the LM agent to leak confidential data or trigger harmful\nactions. Existing defenses (OpenAI GPTs) require user confirmation before every\ntool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS),\nwhich automatically detects and executes tool calls that preserve integrity and\nconfidentiality, requiring user confirmation only when these safeguards cannot\nbe ensured. RTBAS adapts Information Flow Control to the unique challenges\npresented by TBAS. We present two novel dependency screeners, using\nLM-as-a-judge and attention-based saliency, to overcome these challenges.\nExperimental results on the AgentDojo Prompt Injection benchmark show RTBAS\nprevents all targeted attacks with only a 2% loss of task utility when under\nattack, and further tests confirm its ability to obtain near-oracle performance\non detecting both subtle and direct privacy leaks.",
    "updated" : "2025-02-14T04:16:40Z",
    "published" : "2025-02-13T05:06:22Z",
    "authors" : [
      {
        "name" : "Peter Yong Zhong"
      },
      {
        "name" : "Siyuan Chen"
      },
      {
        "name" : "Ruiqi Wang"
      },
      {
        "name" : "McKenna McCall"
      },
      {
        "name" : "Ben L. Titzer"
      },
      {
        "name" : "Heather Miller"
      },
      {
        "name" : "Phillip B. Gibbons"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.08008v2",
    "title" : "An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models",
    "summary" : "Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.",
    "updated" : "2025-02-14T18:52:34Z",
    "published" : "2025-02-11T23:07:14Z",
    "authors" : [
      {
        "name" : "Kasra Ahmadi"
      },
      {
        "name" : "Rouzbeh Behnia"
      },
      {
        "name" : "Reza Ebrahimi"
      },
      {
        "name" : "Mehran Mozaffari Kermani"
      },
      {
        "name" : "Jeremiah Birrell"
      },
      {
        "name" : "Jason Pacheco"
      },
      {
        "name" : "Attila A Yavuz"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.11682v1",
    "title" : "Double Momentum and Error Feedback for Clipping with Fast Rates and\n  Differential Privacy",
    "summary" : "Strong Differential Privacy (DP) and Optimization guarantees are two\ndesirable properties for a method in Federated Learning (FL). However, existing\nalgorithms do not achieve both properties at once: they either have optimal DP\nguarantees but rely on restrictive assumptions such as bounded\ngradients/bounded data heterogeneity, or they ensure strong optimization\nperformance but lack DP guarantees. To address this gap in the literature, we\npropose and analyze a new method called Clip21-SGD2M based on a novel\ncombination of clipping, heavy-ball momentum, and Error Feedback. In\nparticular, for non-convex smooth distributed problems with clients having\narbitrarily heterogeneous data, we prove that Clip21-SGD2M has optimal\nconvergence rate and also near optimal (local-)DP neighborhood. Our numerical\nexperiments on non-convex logistic regression and training of neural networks\nhighlight the superiority of Clip21-SGD2M over baselines in terms of the\noptimization performance for a given DP-budget.",
    "updated" : "2025-02-17T11:16:21Z",
    "published" : "2025-02-17T11:16:21Z",
    "authors" : [
      {
        "name" : "Rustem Islamov"
      },
      {
        "name" : "Samuel Horvath"
      },
      {
        "name" : "Aurelien Lucchi"
      },
      {
        "name" : "Peter Richtarik"
      },
      {
        "name" : "Eduard Gorbunov"
      }
    ],
    "categories" : [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.11658v1",
    "title" : "\"I'm not for sale\" -- Perceptions and limited awareness of privacy risks\n  by digital natives about location data",
    "summary" : "Although mobile devices benefit users in their daily lives in numerous ways,\nthey also raise several privacy concerns. For instance, they can reveal\nsensitive information that can be inferred from location data. This location\ndata is shared through service providers as well as mobile applications.\nUnderstanding how and with whom users share their location data -- as well as\nusers' perception of the underlying privacy risks --, are important notions to\ngrasp in order to design usable privacy-enhancing technologies. In this work,\nwe perform a quantitative and qualitative analysis of smartphone users'\nawareness, perception and self-reported behavior towards location data-sharing\nthrough a survey of n=99 young adult participants (i.e., digital natives). We\ncompare stated practices with actual behaviors to better understand their\nmental models, and survey participants' understanding of privacy risks before\nand after the inspection of location traces and the information that can be\ninferred therefrom.\n  Our empirical results show that participants have risky privacy practices:\nabout 54% of participants underestimate the number of mobile applications to\nwhich they have granted access to their data, and 33% forget or do not think of\nrevoking access to their data. Also, by using a demonstrator to perform\ninferences from location data, we observe that slightly more than half of\nparticipants (57%) are surprised by the extent of potentially inferred\ninformation, and that 47% intend to reduce access to their data via permissions\nas a result of using the demonstrator. Last, a majority of participants have\nlittle knowledge of the tools to better protect themselves, but are nonetheless\nwilling to follow suggestions to improve privacy (51%). Educating people,\nincluding digital natives, about privacy risks through transparency tools seems\na promising approach.",
    "updated" : "2025-02-17T10:49:23Z",
    "published" : "2025-02-17T10:49:23Z",
    "authors" : [
      {
        "name" : "Antoine Boutet"
      },
      {
        "name" : "Victor Morel"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.11533v1",
    "title" : "Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of\n  Stealing Privacy",
    "summary" : "Model merging is a widespread technology in large language models (LLMs) that\nintegrates multiple task-specific LLMs into a unified one, enabling the merged\nmodel to inherit the specialized capabilities of these LLMs. Most task-specific\nLLMs are sourced from open-source communities and have not undergone rigorous\nauditing, potentially imposing risks in model merging. This paper highlights an\noverlooked privacy risk: \\textit{an unsafe model could compromise the privacy\nof other LLMs involved in the model merging.} Specifically, we propose PhiMM, a\nprivacy attack approach that trains a phishing model capable of stealing\nprivacy using a crafted privacy phishing instruction dataset. Furthermore, we\nintroduce a novel model cloaking method that mimics a specialized capability to\nconceal attack intent, luring users into merging the phishing model. Once\nvictims merge the phishing model, the attacker can extract personally\nidentifiable information (PII) or infer membership information (MI) by querying\nthe merged model with the phishing instruction. Experimental results show that\nmerging a phishing model increases the risk of privacy breaches. Compared to\nthe results before merging, PII leakage increased by 3.9\\% and MI leakage\nincreased by 17.4\\% on average. We release the code of PhiMM through a link.",
    "updated" : "2025-02-17T08:04:52Z",
    "published" : "2025-02-17T08:04:52Z",
    "authors" : [
      {
        "name" : "Zhenyuan Guo"
      },
      {
        "name" : "Yi Shi"
      },
      {
        "name" : "Wenlong Meng"
      },
      {
        "name" : "Chen Gong"
      },
      {
        "name" : "Chengkun Wei"
      },
      {
        "name" : "Wenzhi Chen"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.11163v1",
    "title" : "VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and\n  Privacy Risks",
    "summary" : "Visual-Language Models (VLMs) have shown remarkable performance across\nvarious tasks, particularly in recognizing geographic information from images.\nHowever, significant challenges remain, including biases and privacy concerns.\nTo systematically address these issues in the context of geographic information\nrecognition, we introduce a benchmark dataset consisting of 1,200 images paired\nwith detailed geographic metadata. Evaluating four VLMs, we find that while\nthese models demonstrate the ability to recognize geographic information from\nimages, achieving up to $53.8\\%$ accuracy in city prediction, they exhibit\nsignificant regional biases. Specifically, performance is substantially higher\nfor economically developed and densely populated regions compared to less\ndeveloped ($-12.5\\%$) and sparsely populated ($-17.0\\%$) areas. Moreover, the\nmodels exhibit regional biases, frequently overpredicting certain locations;\nfor instance, they consistently predict Sydney for images taken in Australia.\nThe strong performance of VLMs also raises privacy concerns, particularly for\nusers who share images online without the intent of being identified. Our code\nand dataset are publicly available at\nhttps://github.com/uscnlp-lime/FairLocator.",
    "updated" : "2025-02-16T15:28:34Z",
    "published" : "2025-02-16T15:28:34Z",
    "authors" : [
      {
        "name" : "Jingyuan Huang"
      },
      {
        "name" : "Jen-tse Huang"
      },
      {
        "name" : "Ziyi Liu"
      },
      {
        "name" : "Xiaoyuan Liu"
      },
      {
        "name" : "Wenxuan Wang"
      },
      {
        "name" : "Jieyu Zhao"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.11009v1",
    "title" : "Computing Inconsistency Measures Under Differential Privacy",
    "summary" : "Assessing data quality is crucial to knowing whether and how to use the data\nfor different purposes. Specifically, given a collection of integrity\nconstraints, various ways have been proposed to quantify the inconsistency of a\ndatabase. Inconsistency measures are particularly important when we wish to\nassess the quality of private data without revealing sensitive information. We\nstudy the estimation of inconsistency measures for a database protected under\nDifferential Privacy (DP). Such estimation is nontrivial since some measures\nintrinsically query sensitive information, and the computation of others\ninvolves functions on underlying sensitive data. Among five inconsistency\nmeasures that have been proposed in recent work, we identify that two are\nintractable in the DP setting. The major challenge for the other three is high\nsensitivity: adding or removing one tuple from the dataset may significantly\naffect the outcome. To mitigate that, we model the dataset using a conflict\ngraph and investigate private graph statistics to estimate these measures. The\nproposed machinery includes adapting graph-projection techniques with parameter\nselection optimizations on the conflict graph and a DP variant of approximate\nvertex cover size. We experimentally show that we can effectively compute DP\nestimates of the three measures on five real-world datasets with denial\nconstraints, where the density of the conflict graphs highly varies.",
    "updated" : "2025-02-16T06:23:11Z",
    "published" : "2025-02-16T06:23:11Z",
    "authors" : [
      {
        "name" : "Shubhankar Mohapatra"
      },
      {
        "name" : "Amir Gilad"
      },
      {
        "name" : "Xi He"
      },
      {
        "name" : "Benny Kimelfeld"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.10997v1",
    "title" : "New Rates in Stochastic Decision-Theoretic Online Learning under\n  Differential Privacy",
    "summary" : "Hu and Mehta (2024) posed an open problem: what is the optimal\ninstance-dependent rate for the stochastic decision-theoretic online learning\n(with $K$ actions and $T$ rounds) under $\\varepsilon$-differential privacy?\nBefore, the best known upper bound and lower bound are $O\\left(\\frac{\\log\nK}{\\Delta_{\\min}} + \\frac{\\log K\\log T}{\\varepsilon}\\right)$ and\n$\\Omega\\left(\\frac{\\log K}{\\Delta_{\\min}} + \\frac{\\log K}{\\varepsilon}\\right)$\n(where $\\Delta_{\\min}$ is the gap between the optimal and the second actions).\nIn this paper, we partially address this open problem by having two new\nresults. First, we provide an improved upper bound for this problem\n$O\\left(\\frac{\\log K}{\\Delta_{\\min}} + \\frac{\\log^2K}{\\varepsilon}\\right)$,\nwhere the $T$-dependency has been removed. Second, we introduce the\ndeterministic setting, a weaker setting of this open problem, where the\nreceived loss vector is deterministic and we can focus on the analysis for\n$\\varepsilon$ regardless of the sampling error. At the deterministic setting,\nwe prove upper and lower bounds that match at $\\Theta\\left(\\frac{\\log\nK}{\\varepsilon}\\right)$, while a direct application of the analysis and\nalgorithms from the original setting still leads to an extra log factor.\nTechnically, we introduce the Bernoulli resampling trick, which enforces a\nmonotonic property for the output from report-noisy-max mechanism that enables\na tighter analysis. Moreover, by replacing the Laplace noise with Gumbel noise,\nwe derived explicit integral form that gives a tight characterization of the\nregret in the deterministic case.",
    "updated" : "2025-02-16T05:13:51Z",
    "published" : "2025-02-16T05:13:51Z",
    "authors" : [
      {
        "name" : "Ruihan Wu"
      },
      {
        "name" : "Yu-Xiang Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.10801v1",
    "title" : "FaceSwapGuard: Safeguarding Facial Privacy from DeepFake Threats through\n  Identity Obfuscation",
    "summary" : "DeepFakes pose a significant threat to our society. One representative\nDeepFake application is face-swapping, which replaces the identity in a facial\nimage with that of a victim. Although existing methods partially mitigate these\nrisks by degrading the quality of swapped images, they often fail to disrupt\nthe identity transformation effectively. To fill this gap, we propose\nFaceSwapGuard (FSG), a novel black-box defense mechanism against deepfake\nface-swapping threats. Specifically, FSG introduces imperceptible perturbations\nto a user's facial image, disrupting the features extracted by identity\nencoders. When shared online, these perturbed images mislead face-swapping\ntechniques, causing them to generate facial images with identities\nsignificantly different from the original user. Extensive experiments\ndemonstrate the effectiveness of FSG against multiple face-swapping techniques,\nreducing the face match rate from 90\\% (without defense) to below 10\\%. Both\nqualitative and quantitative studies further confirm its ability to confuse\nhuman perception, highlighting its practical utility. Additionally, we\ninvestigate key factors that may influence FSG and evaluate its robustness\nagainst various adaptive adversaries.",
    "updated" : "2025-02-15T13:45:19Z",
    "published" : "2025-02-15T13:45:19Z",
    "authors" : [
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Zheng Li"
      },
      {
        "name" : "Xuhong Zhang"
      },
      {
        "name" : "Shouling Ji"
      },
      {
        "name" : "Shanqing Guo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.10788v1",
    "title" : "Analyzing Privacy Dynamics within Groups using Gamified Auctions",
    "summary" : "Online shared content, such as group pictures, often contains information\nabout multiple users. Developing technical solutions to manage the privacy of\nsuch \"co-owned\" content is challenging because each co-owner may have different\npreferences. Recent technical approaches advocate group-decision mechanisms,\nincluding auctions, to decide as how best to resolve these differences.\nHowever, it is not clear if users would participate in such mechanisms and if\nthey do, whether they would act altruistically. Understanding the privacy\ndynamics is crucial to develop effective mechanisms for privacy-respecting\ncollaborative systems. Accordingly, this work develops RESOLVE, a privacy\nauction game to understand the sharing behavior of users in groups. Our results\nof users' playing the game show that i) the users' understanding of individual\nvs. group privacy differs significantly; ii) often users fight for their\npreferences even at the cost of others' privacy; and iii) at times users\ncollaborate to fight for the privacy of others.",
    "updated" : "2025-02-15T12:48:30Z",
    "published" : "2025-02-15T12:48:30Z",
    "authors" : [
      {
        "name" : "Hüseyin Aydın"
      },
      {
        "name" : "Onuralp Ulusoy"
      },
      {
        "name" : "Ilaria Liccardi"
      },
      {
        "name" : "Pınar Yolum"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.10701v1",
    "title" : "Unpacking the Layers: Exploring Self-Disclosure Norms, Engagement\n  Dynamics, and Privacy Implications",
    "summary" : "This paper characterizes the self-disclosure behavior of Reddit users across\n11 different types of self-disclosure. We find that at least half of the users\nshare some type of disclosure in at least 10% of their posts, with half of\nthese posts having more than one type of disclosure. We show that different\ntypes of self-disclosure are likely to receive varying levels of engagement.\nFor instance, a Sexual Orientation disclosure garners more comments than other\nself-disclosures. We also explore confounding factors that affect future\nself-disclosure. We show that users who receive interactions from\n(self-disclosure) specific subreddit members are more likely to disclose in the\nfuture. We also show that privacy risks due to self-disclosure extend beyond\nReddit users themselves to include their close contacts, such as family and\nfriends, as their information is also revealed. We develop a browser plugin for\nend-users to flag self-disclosure in their content.",
    "updated" : "2025-02-15T07:15:09Z",
    "published" : "2025-02-15T07:15:09Z",
    "authors" : [
      {
        "name" : "Ehsan-Ul Haq"
      },
      {
        "name" : "Shalini Jangra"
      },
      {
        "name" : "Suparna De"
      },
      {
        "name" : "Nishanth Sastry"
      },
      {
        "name" : "Gareth Tyson"
      }
    ],
    "categories" : [
      "cs.SI",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.10635v1",
    "title" : "Privacy Preservation through Practical Machine Unlearning",
    "summary" : "Machine Learning models thrive on vast datasets, continuously adapting to\nprovide accurate predictions and recommendations. However, in an era dominated\nby privacy concerns, Machine Unlearning emerges as a transformative approach,\nenabling the selective removal of data from trained models. This paper examines\nmethods such as Naive Retraining and Exact Unlearning via the SISA framework,\nevaluating their Computational Costs, Consistency, and feasibility using the\n\\texttt{HSpam14} dataset. We explore the potential of integrating unlearning\nprinciples into Positive Unlabeled (PU) Learning to address challenges posed by\npartially labeled datasets. Our findings highlight the promise of unlearning\nframeworks like \\textit{DaRE} for ensuring privacy compliance while maintaining\nmodel performance, albeit with significant computational trade-offs. This study\nunderscores the importance of Machine Unlearning in achieving ethical AI and\nfostering trust in data-driven systems.",
    "updated" : "2025-02-15T02:25:27Z",
    "published" : "2025-02-15T02:25:27Z",
    "authors" : [
      {
        "name" : "Robert Dilworth"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.10599v1",
    "title" : "Federated Learning-Driven Cybersecurity Framework for IoT Networks with\n  Privacy-Preserving and Real-Time Threat Detection Capabilities",
    "summary" : "The rapid expansion of the Internet of Things (IoT) ecosystem has transformed\nvarious sectors but has also introduced significant cybersecurity challenges.\nTraditional centralized security methods often struggle to balance privacy\npreservation and real-time threat detection in IoT networks. To address these\nissues, this study proposes a Federated Learning-Driven Cybersecurity Framework\ndesigned specifically for IoT environments. The framework enables decentralized\ndata processing by training models locally on edge devices, ensuring data\nprivacy. Secure aggregation of these locally trained models is achieved using\nhomomorphic encryption, allowing collaborative learning without exposing\nsensitive information.\n  The proposed framework utilizes recurrent neural networks (RNNs) for anomaly\ndetection, optimized for resource-constrained IoT networks. Experimental\nresults demonstrate that the system effectively detects complex cyber threats,\nincluding distributed denial-of-service (DDoS) attacks, with over 98% accuracy.\nAdditionally, it improves energy efficiency by reducing resource consumption by\n20% compared to centralized approaches.\n  This research addresses critical gaps in IoT cybersecurity by integrating\nfederated learning with advanced threat detection techniques. The framework\noffers a scalable and privacy-preserving solution adaptable to various IoT\napplications. Future work will explore the integration of blockchain for\ntransparent model aggregation and quantum-resistant cryptographic methods to\nfurther enhance security in evolving technological landscapes.",
    "updated" : "2025-02-14T23:11:51Z",
    "published" : "2025-02-14T23:11:51Z",
    "authors" : [
      {
        "name" : "Milad Rahmati"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.10450v1",
    "title" : "Trustworthy AI on Safety, Bias, and Privacy: A Survey",
    "summary" : "The capabilities of artificial intelligence systems have been advancing to a\ngreat extent, but these systems still struggle with failure modes,\nvulnerabilities, and biases. In this paper, we study the current state of the\nfield, and present promising insights and perspectives regarding concerns that\nchallenge the trustworthiness of AI models. In particular, this paper\ninvestigates the issues regarding three thrusts: safety, privacy, and bias,\nwhich hurt models' trustworthiness. For safety, we discuss safety alignment in\nthe context of large language models, preventing them from generating toxic or\nharmful content. For bias, we focus on spurious biases that can mislead a\nnetwork. Lastly, for privacy, we cover membership inference attacks in deep\nneural networks. The discussions addressed in this paper reflect our own\nexperiments and observations.",
    "updated" : "2025-02-11T20:08:42Z",
    "published" : "2025-02-11T20:08:42Z",
    "authors" : [
      {
        "name" : "Xingli Fang"
      },
      {
        "name" : "Jianwei Li"
      },
      {
        "name" : "Varun Mulchandani"
      },
      {
        "name" : "Jung-Eun Kim"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.12976v1",
    "title" : "Does Training with Synthetic Data Truly Protect Privacy?",
    "summary" : "As synthetic data becomes increasingly popular in machine learning tasks,\nnumerous methods--without formal differential privacy guarantees--use synthetic\ndata for training. These methods often claim, either explicitly or implicitly,\nto protect the privacy of the original training data. In this work, we explore\nfour different training paradigms: coreset selection, dataset distillation,\ndata-free knowledge distillation, and synthetic data generated from diffusion\nmodels. While all these methods utilize synthetic data for training, they lead\nto vastly different conclusions regarding privacy preservation. We caution that\nempirical approaches to preserving data privacy require careful and rigorous\nevaluation; otherwise, they risk providing a false sense of privacy.",
    "updated" : "2025-02-18T15:56:52Z",
    "published" : "2025-02-18T15:56:52Z",
    "authors" : [
      {
        "name" : "Yunpeng Zhao"
      },
      {
        "name" : "Jie Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.12658v1",
    "title" : "R.R.: Unveiling LLM Training Privacy through Recollection and Ranking",
    "summary" : "Large Language Models (LLMs) pose significant privacy risks, potentially\nleaking training data due to implicit memorization. Existing privacy attacks\nprimarily focus on membership inference attacks (MIAs) or data extraction\nattacks, but reconstructing specific personally identifiable information (PII)\nin LLM's training data remains challenging. In this paper, we propose R.R.\n(Recollect and Rank), a novel two-step privacy stealing attack that enables\nattackers to reconstruct PII entities from scrubbed training data where the PII\nentities have been masked. In the first stage, we introduce a prompt paradigm\nnamed recollection, which instructs the LLM to repeat a masked text but fill in\nmasks. Then we can use PII identifiers to extract recollected PII candidates.\nIn the second stage, we design a new criterion to score each PII candidate and\nrank them. Motivated by membership inference, we leverage the reference model\nas a calibration to our criterion. Experiments across three popular PII\ndatasets demonstrate that the R.R. achieves better PII identical performance\ncompared to baselines. These results highlight the vulnerability of LLMs to PII\nleakage even when training data has been scrubbed. We release the replicate\npackage of R.R. at a link.",
    "updated" : "2025-02-18T09:05:59Z",
    "published" : "2025-02-18T09:05:59Z",
    "authors" : [
      {
        "name" : "Wenlong Meng"
      },
      {
        "name" : "Zhenyuan Guo"
      },
      {
        "name" : "Lenan Wu"
      },
      {
        "name" : "Chen Gong"
      },
      {
        "name" : "Wenyan Liu"
      },
      {
        "name" : "Weixian Li"
      },
      {
        "name" : "Chengkun Wei"
      },
      {
        "name" : "Wenzhi Chen"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.11658v2",
    "title" : "\"I'm not for sale\" -- Perceptions and limited awareness of privacy risks\n  by digital natives about location data",
    "summary" : "Although mobile devices benefit users in their daily lives in numerous ways,\nthey also raise several privacy concerns. For instance, they can reveal\nsensitive information that can be inferred from location data. This location\ndata is shared through service providers as well as mobile applications.\nUnderstanding how and with whom users share their location data -- as well as\nusers' perception of the underlying privacy risks --, are important notions to\ngrasp in order to design usable privacy-enhancing technologies. In this work,\nwe perform a quantitative and qualitative analysis of smartphone users'\nawareness, perception and self-reported behavior towards location data-sharing\nthrough a survey of n=99 young adult participants (i.e., digital natives). We\ncompare stated practices with actual behaviors to better understand their\nmental models, and survey participants' understanding of privacy risks before\nand after the inspection of location traces and the information that can be\ninferred therefrom.\n  Our empirical results show that participants have risky privacy practices:\nabout 54% of participants underestimate the number of mobile applications to\nwhich they have granted access to their data, and 33% forget or do not think of\nrevoking access to their data. Also, by using a demonstrator to perform\ninferences from location data, we observe that slightly more than half of\nparticipants (57%) are surprised by the extent of potentially inferred\ninformation, and that 47% intend to reduce access to their data via permissions\nas a result of using the demonstrator. Last, a majority of participants have\nlittle knowledge of the tools to better protect themselves, but are nonetheless\nwilling to follow suggestions to improve privacy (51%). Educating people,\nincluding digital natives, about privacy risks through transparency tools seems\na promising approach.",
    "updated" : "2025-02-18T09:24:14Z",
    "published" : "2025-02-17T10:49:23Z",
    "authors" : [
      {
        "name" : "Antoine Boutet"
      },
      {
        "name" : "Victor Morel"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.10635v2",
    "title" : "Privacy Preservation through Practical Machine Unlearning",
    "summary" : "Machine Learning models thrive on vast datasets, continuously adapting to\nprovide accurate predictions and recommendations. However, in an era dominated\nby privacy concerns, Machine Unlearning emerges as a transformative approach,\nenabling the selective removal of data from trained models. This paper examines\nmethods such as Naive Retraining and Exact Unlearning via the SISA framework,\nevaluating their Computational Costs, Consistency, and feasibility using the\n$\\texttt{HSpam14}$ dataset. We explore the potential of integrating unlearning\nprinciples into Positive Unlabeled (PU) Learning to address challenges posed by\npartially labeled datasets. Our findings highlight the promise of unlearning\nframeworks like $\\textit{DaRE}$ for ensuring privacy compliance while\nmaintaining model performance, albeit with significant computational\ntrade-offs. This study underscores the importance of Machine Unlearning in\nachieving ethical AI and fostering trust in data-driven systems.",
    "updated" : "2025-02-18T14:16:06Z",
    "published" : "2025-02-15T02:25:27Z",
    "authors" : [
      {
        "name" : "Robert Dilworth"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.09001v1",
    "title" : "Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection:\n  Balancing Security and Data Protection",
    "summary" : "Privacy-preserving network anomaly detection has become an essential area of\nresearch due to growing concerns over the protection of sensitive data.\nTraditional anomaly detection models often prioritize accuracy while neglecting\nthe critical aspect of privacy. In this work, we propose a hybrid ensemble\nmodel that incorporates privacy-preserving techniques to address both detection\naccuracy and data protection. Our model combines the strengths of several\nmachine learning algorithms, including K-Nearest Neighbors (KNN), Support\nVector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), to create\na robust system capable of identifying network anomalies while ensuring\nprivacy. The proposed approach integrates advanced preprocessing techniques\nthat enhance data quality and address the challenges of small sample sizes and\nimbalanced datasets. By embedding privacy measures into the model design, our\nsolution offers a significant advancement over existing methods, ensuring both\nenhanced detection performance and strong privacy safeguards.",
    "updated" : "2025-02-13T06:33:16Z",
    "published" : "2025-02-13T06:33:16Z",
    "authors" : [
      {
        "name" : "Shaobo Liu"
      },
      {
        "name" : "Zihao Zhao"
      },
      {
        "name" : "Weijie He"
      },
      {
        "name" : "Jiren Wang"
      },
      {
        "name" : "Jing Peng"
      },
      {
        "name" : "Haoyuan Ma"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.13833v1",
    "title" : "Contrastive Learning-Based privacy metrics in Tabular Synthetic Datasets",
    "summary" : "Synthetic data has garnered attention as a Privacy Enhancing Technology (PET)\nin sectors such as healthcare and finance. When using synthetic data in\npractical applications, it is important to provide protection guarantees. In\nthe literature, two family of approaches are proposed for tabular data: on the\none hand, Similarity-based methods aim at finding the level of similarity\nbetween training and synthetic data. Indeed, a privacy breach can occur if the\ngenerated data is consistently too similar or even identical to the train data.\nOn the other hand, Attack-based methods conduce deliberate attacks on synthetic\ndatasets. The success rates of these attacks reveal how secure the synthetic\ndatasets are.\n  In this paper, we introduce a contrastive method that improves privacy\nassessment of synthetic datasets by embedding the data in a more representative\nspace. This overcomes obstacles surrounding the multitude of data types and\nattributes. It also makes the use of intuitive distance metrics possible for\nsimilarity measurements and as an attack vector. In a series of experiments\nwith publicly available datasets, we compare the performances of\nsimilarity-based and attack-based methods, both with and without use of the\ncontrastive learning-based embeddings. Our results show that relatively\nefficient, easy to implement privacy metrics can perform equally well as more\nadvanced metrics explicitly modeling conditions for privacy referred to by the\nGDPR.",
    "updated" : "2025-02-19T15:52:23Z",
    "published" : "2025-02-19T15:52:23Z",
    "authors" : [
      {
        "name" : "Milton Nicolás Plasencia Palacios"
      },
      {
        "name" : "Sebastiano Saccani"
      },
      {
        "name" : "Gabriele Sgroi"
      },
      {
        "name" : "Alexander Boudewijn"
      },
      {
        "name" : "Luca Bortolussi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.13564v1",
    "title" : "PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language\n  Models",
    "summary" : "The rapid development of large language models (LLMs) is redefining the\nlandscape of human-computer interaction, and their integration into various\nuser-service applications is becoming increasingly prevalent. However,\ntransmitting user data to cloud-based LLMs presents significant risks of data\nbreaches and unauthorized access to personal identification information. In\nthis paper, we propose a privacy preservation pipeline for protecting privacy\nand sensitive information during interactions between users and LLMs in\npractical LLM usage scenarios. We construct SensitiveQA, the first privacy\nopen-ended question-answering dataset. It comprises 57k interactions in Chinese\nand English, encompassing a diverse range of user-sensitive information within\nthe conversations. Our proposed solution employs a multi-stage strategy aimed\nat preemptively securing user information while simultaneously preserving the\nresponse quality of cloud-based LLMs. Experimental validation underscores our\nmethod's efficacy in balancing privacy protection with maintaining robust\ninteraction quality. The code and dataset are available at\nhttps://github.com/ligw1998/PRIV-QA.",
    "updated" : "2025-02-19T09:17:07Z",
    "published" : "2025-02-19T09:17:07Z",
    "authors" : [
      {
        "name" : "Guangwei Li"
      },
      {
        "name" : "Yuansen Zhang"
      },
      {
        "name" : "Yinggui Wang"
      },
      {
        "name" : "Shoumeng Yan"
      },
      {
        "name" : "Lei Wang"
      },
      {
        "name" : "Tao Wei"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.13415v1",
    "title" : "Indifferential Privacy: A New Paradigm and Its Applications to Optimal\n  Matching in Dark Pool Auctions",
    "summary" : "Public exchanges like the New York Stock Exchange and NASDAQ act as\nauctioneers in a public double auction system, where buyers submit their\nhighest bids and sellers offer their lowest asking prices, along with the\nnumber of shares (volume) they wish to trade. The auctioneer matches compatible\norders and executes the trades when a match is found. However, auctioneers\ninvolved in high-volume exchanges, such as dark pools, may not always be\nreliable. They could exploit their position by engaging in practices like\nfront-running or face significant conflicts of interest, i.e., ethical breaches\nthat have frequently resulted in hefty fines and regulatory scrutiny within the\nfinancial industry.\n  Previous solutions, based on the use of fully homomorphic encryption (Asharov\net al., AAMAS 2020), encrypt orders ensuring that information is revealed only\nwhen a match occurs. However, this approach introduces significant\ncomputational overhead, making it impractical for high-frequency trading\nenvironments such as dark pools.\n  In this work, we propose a new system based on differential privacy combined\nwith lightweight encryption, offering an efficient and practical solution that\nmitigates the risks of an untrustworthy auctioneer. Specifically, we introduce\na new concept called Indifferential Privacy, which can be of independent\ninterest, where a user is indifferent to whether certain information is\nrevealed after some special event, unlike standard differential privacy. For\nexample, in an auction, it's reasonable to disclose the true volume of a trade\nonce all of it has been matched. Moreover, our new concept of Indifferential\nPrivacy allows for maximum matching, which is impossible with conventional\ndifferential privacy.",
    "updated" : "2025-02-19T04:19:25Z",
    "published" : "2025-02-19T04:19:25Z",
    "authors" : [
      {
        "name" : "Antigoni Polychroniadou"
      },
      {
        "name" : "T. -H. Hubert Chan"
      },
      {
        "name" : "Adya Agrawal"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.13313v1",
    "title" : "Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning\n  Large Language Models",
    "summary" : "We study the inherent trade-offs in minimizing privacy risks and maximizing\nutility, while maintaining high computational efficiency, when fine-tuning\nlarge language models (LLMs). A number of recent works in privacy research have\nattempted to mitigate privacy risks posed by memorizing fine-tuning data by\nusing differentially private training methods (e.g., DP), albeit at a\nsignificantly higher computational cost (inefficiency). In parallel, several\nworks in systems research have focussed on developing (parameter) efficient\nfine-tuning methods (e.g., LoRA), but few works, if any, investigated whether\nsuch efficient methods enhance or diminish privacy risks. In this paper, we\ninvestigate this gap and arrive at a surprising conclusion: efficient\nfine-tuning methods like LoRA mitigate privacy risks similar to private\nfine-tuning methods like DP. Our empirical finding directly contradicts\nprevailing wisdom that privacy and efficiency objectives are at odds during\nfine-tuning. Our finding is established by (a) carefully defining measures of\nprivacy and utility that distinguish between memorizing sensitive and\nnon-sensitive tokens in training and test datasets used in fine-tuning and (b)\nextensive evaluations using multiple open-source language models from Pythia,\nGemma, and Llama families and different domain-specific datasets.",
    "updated" : "2025-02-18T22:16:03Z",
    "published" : "2025-02-18T22:16:03Z",
    "authors" : [
      {
        "name" : "Soumi Das"
      },
      {
        "name" : "Camila Kolling"
      },
      {
        "name" : "Mohammad Aflah Khan"
      },
      {
        "name" : "Mahsa Amani"
      },
      {
        "name" : "Bishwamittra Ghosh"
      },
      {
        "name" : "Qinyuan Wu"
      },
      {
        "name" : "Till Speicher"
      },
      {
        "name" : "Krishna P. Gummadi"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.13191v1",
    "title" : "On the Privacy Risks of Spiking Neural Networks: A Membership Inference\n  Analysis",
    "summary" : "Spiking Neural Networks (SNNs) are increasingly explored for their energy\nefficiency and robustness in real-world applications, yet their privacy risks\nremain largely unexamined. In this work, we investigate the susceptibility of\nSNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an\nadversary attempts to determine whether a given sample was part of the training\ndataset. While prior work suggests that SNNs may offer inherent robustness due\nto their discrete, event-driven nature, we find that its resilience diminishes\nas latency (T) increases. Furthermore, we introduce an input dropout strategy\nunder black box setting, that significantly enhances membership inference in\nSNNs. Our findings challenge the assumption that SNNs are inherently more\nsecure, and even though they are expected to be better, our results reveal that\nSNNs exhibit privacy vulnerabilities that are equally comparable to Artificial\nNeural Networks (ANNs). Our code is available at\nhttps://anonymous.4open.science/r/MIA_SNN-3610.",
    "updated" : "2025-02-18T15:19:20Z",
    "published" : "2025-02-18T15:19:20Z",
    "authors" : [
      {
        "name" : "Junyi Guan"
      },
      {
        "name" : "Abhijith Sharma"
      },
      {
        "name" : "Chong Tian"
      },
      {
        "name" : "Salem Lahlou"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.13172v1",
    "title" : "Unveiling Privacy Risks in LLM Agent Memory",
    "summary" : "Large Language Model (LLM) agents have become increasingly prevalent across\nvarious real-world applications. They enhance decision-making by storing\nprivate user-agent interactions in the memory module for demonstrations,\nintroducing new privacy risks for LLM agents. In this work, we systematically\ninvestigate the vulnerability of LLM agents to our proposed Memory EXTRaction\nAttack (MEXTRA) under a black-box setting. To extract private information from\nmemory, we propose an effective attacking prompt design and an automated prompt\ngeneration method based on different levels of knowledge about the LLM agent.\nExperiments on two representative agents demonstrate the effectiveness of\nMEXTRA. Moreover, we explore key factors influencing memory leakage from both\nthe agent's and the attacker's perspectives. Our findings highlight the urgent\nneed for effective memory safeguards in LLM agent design and deployment.",
    "updated" : "2025-02-17T19:55:53Z",
    "published" : "2025-02-17T19:55:53Z",
    "authors" : [
      {
        "name" : "Bo Wang"
      },
      {
        "name" : "Weiyi He"
      },
      {
        "name" : "Pengfei He"
      },
      {
        "name" : "Shenglai Zeng"
      },
      {
        "name" : "Zhen Xiang"
      },
      {
        "name" : "Yue Xing"
      },
      {
        "name" : "Jiliang Tang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.14780v1",
    "title" : "ReVision: A Dataset and Baseline VLM for Privacy-Preserving\n  Task-Oriented Visual Instruction Rewriting",
    "summary" : "Efficient and privacy-preserving multimodal interaction is essential as AR,\nVR, and modern smartphones with powerful cameras become primary interfaces for\nhuman-computer communication. Existing powerful large vision-language models\n(VLMs) enabling multimodal interaction often rely on cloud-based processing,\nraising significant concerns about (1) visual privacy by transmitting sensitive\nvision data to servers, and (2) their limited real-time, on-device usability.\nThis paper explores Visual Instruction Rewriting, a novel approach that\ntransforms multimodal instructions into text-only commands, allowing seamless\nintegration of lightweight on-device instruction rewriter VLMs (250M\nparameters) with existing conversational AI systems, enhancing vision data\nprivacy. To achieve this, we present a dataset of over 39,000 examples across\n14 domains and develop a compact VLM, pretrained on image captioning datasets\nand fine-tuned for instruction rewriting. Experimental results, evaluated\nthrough NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic\nparsing analysis, demonstrate that even a quantized version of the model\n(<500MB storage footprint) can achieve effective instruction rewriting, thus\nenabling privacy-focused, multimodal AI applications.",
    "updated" : "2025-02-20T18:01:41Z",
    "published" : "2025-02-20T18:01:41Z",
    "authors" : [
      {
        "name" : "Abhijit Mishra"
      },
      {
        "name" : "Richard Noh"
      },
      {
        "name" : "Hsiang Fu"
      },
      {
        "name" : "Mingda Li"
      },
      {
        "name" : "Minji Kim"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.14761v1",
    "title" : "User Awareness and Perspectives Survey on Privacy, Security and\n  Usability of Auditory Prostheses",
    "summary" : "According to the World Health Organization, over 466 million people worldwide\nsuffer from disabling hearing loss, with approximately 34 million of these\nbeing children. Hearing aids (HA) and cochlear implants (CI) have become\nindispensable tools for restoring hearing and enhancing the quality of life for\nindividuals with hearing impairments. Clinical research and consumer studies\nindicate that users of HAs and CIs report significant improvements in their\ndaily lives, including enhanced communication abilities and social engagement\nand reduced psychological stress. Modern auditory prosthetic devices are more\nadvanced and interconnected with digital networks to add functionality, such as\nstreaming audio directly from smartphones and other devices, remote adjustments\nby audiologists, integration with smart home systems, and access to artificial\nintelligence-driven sound enhancement features. With this interconnectivity,\nissues surrounding data privacy and security have become increasingly\npertinent. There is limited research on the usability perceptions of current HA\nand CI models from the perspective of end-users. In addition, no studies have\ninvestigated consumer mental models during the purchasing process, particularly\nwhich factors they prioritize when selecting a device. In this study, we\nassessed participants' satisfaction levels with various features of their\nauditory prostheses. This work contributes to the field by addressing gaps in\nuser perceptions of HA and CI usability, identifying key factors in consumer\npurchasing decisions, and highlighting the need for improved privacy and\nsecurity awareness and education among users.",
    "updated" : "2025-02-20T17:36:19Z",
    "published" : "2025-02-20T17:36:19Z",
    "authors" : [
      {
        "name" : "Sohini Saha"
      },
      {
        "name" : "Leslie M. Collins"
      },
      {
        "name" : "Sherri L. Smith"
      },
      {
        "name" : "Boyla O. Mainsah"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.14309v1",
    "title" : "On Theoretical Limits of Learning with Label Differential Privacy",
    "summary" : "Label differential privacy (DP) is designed for learning problems involving\nprivate labels and public features. While various methods have been proposed\nfor learning under label DP, the theoretical limits remain largely unexplored.\nIn this paper, we investigate the fundamental limits of learning with label DP\nin both local and central models for both classification and regression tasks,\ncharacterized by minimax convergence rates. We establish lower bounds by\nconverting each task into a multiple hypothesis testing problem and bounding\nthe test error. Additionally, we develop algorithms that yield matching upper\nbounds. Our results demonstrate that under label local DP (LDP), the risk has a\nsignificantly faster convergence rate than that under full LDP, i.e. protecting\nboth features and labels, indicating the advantages of relaxing the DP\ndefinition to focus solely on labels. In contrast, under the label central DP\n(CDP), the risk is only reduced by a constant factor compared to full DP,\nindicating that the relaxation of CDP only has limited benefits on the\nperformance.",
    "updated" : "2025-02-20T06:51:42Z",
    "published" : "2025-02-20T06:51:42Z",
    "authors" : [
      {
        "name" : "Puning Zhao"
      },
      {
        "name" : "Chuan Ma"
      },
      {
        "name" : "Li Shen"
      },
      {
        "name" : "Shaowei Wang"
      },
      {
        "name" : "Rongfei Fan"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.14291v1",
    "title" : "A Note on Efficient Privacy-Preserving Similarity Search for Encrypted\n  Vectors",
    "summary" : "Traditional approaches to vector similarity search over encrypted data rely\non fully homomorphic encryption (FHE) to enable computation without decryption.\nHowever, the substantial computational overhead of FHE makes it impractical for\nlarge-scale real-time applications. This work explores a more efficient\nalternative: using additively homomorphic encryption (AHE) for\nprivacy-preserving similarity search. We consider scenarios where either the\nquery vector or the database vectors remain encrypted, a setting that\nfrequently arises in applications such as confidential recommender systems and\nsecure federated learning. While AHE only supports addition and scalar\nmultiplication, we show that it is sufficient to compute inner product\nsimilarity--one of the most widely used similarity measures in vector\nretrieval. Compared to FHE-based solutions, our approach significantly reduces\ncomputational overhead by avoiding ciphertext-ciphertext multiplications and\nbootstrapping, while still preserving correctness and privacy. We present an\nefficient algorithm for encrypted similarity search under AHE and analyze its\nerror growth and security implications. Our method provides a scalable and\npractical solution for privacy-preserving vector search in real-world machine\nlearning applications.",
    "updated" : "2025-02-20T06:07:04Z",
    "published" : "2025-02-20T06:07:04Z",
    "authors" : [
      {
        "name" : "Dongfang Zhao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.14087v1",
    "title" : "Learning from End User Data with Shuffled Differential Privacy over\n  Kernel Densities",
    "summary" : "We study a setting of collecting and learning from private data distributed\nacross end users. In the shuffled model of differential privacy, the end users\npartially protect their data locally before sharing it, and their data is also\nanonymized during its collection to enhance privacy. This model has recently\nbecome a prominent alternative to central DP, which requires full trust in a\ncentral data curator, and local DP, where fully local data protection takes a\nsteep toll on downstream accuracy.\n  Our main technical result is a shuffled DP protocol for privately estimating\nthe kernel density function of a distributed dataset, with accuracy essentially\nmatching central DP. We use it to privately learn a classifier from the end\nuser data, by learning a private density function per class. Moreover, we show\nthat the density function itself can recover the semantic content of its class,\ndespite having been learned in the absence of any unprotected data. Our\nexperiments show the favorable downstream performance of our approach, and\nhighlight key downstream considerations and trade-offs in a practical ML\ndeployment of shuffled DP.",
    "updated" : "2025-02-19T20:27:01Z",
    "published" : "2025-02-19T20:27:01Z",
    "authors" : [
      {
        "name" : "Tal Wagner"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.15680v1",
    "title" : "Privacy Ripple Effects from Adding or Removing Personal Information in\n  Language Model Training",
    "summary" : "Due to the sensitive nature of personally identifiable information (PII), its\nowners may have the authority to control its inclusion or request its removal\nfrom large-language model (LLM) training. Beyond this, PII may be added or\nremoved from training datasets due to evolving dataset curation techniques,\nbecause they were newly scraped for retraining, or because they were included\nin a new downstream fine-tuning stage. We find that the amount and ease of PII\nmemorization is a dynamic property of a model that evolves throughout training\npipelines and depends on commonly altered design choices. We characterize three\nsuch novel phenomena: (1) similar-appearing PII seen later in training can\nelicit memorization of earlier-seen sequences in what we call assisted\nmemorization, and this is a significant factor (in our settings, up to 1/3);\n(2) adding PII can increase memorization of other PII significantly (in our\nsettings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to\nother PII being memorized. Model creators should consider these first- and\nsecond-order privacy risks when training models to avoid the risk of new PII\nregurgitation.",
    "updated" : "2025-02-21T18:59:14Z",
    "published" : "2025-02-21T18:59:14Z",
    "authors" : [
      {
        "name" : "Jaydeep Borkar"
      },
      {
        "name" : "Matthew Jagielski"
      },
      {
        "name" : "Katherine Lee"
      },
      {
        "name" : "Niloofar Mireshghallah"
      },
      {
        "name" : "David A. Smith"
      },
      {
        "name" : "Christopher A. Choquette-Choo"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.15567v1",
    "title" : "Model Privacy: A Unified Framework to Understand Model Stealing Attacks\n  and Defenses",
    "summary" : "The use of machine learning (ML) has become increasingly prevalent in various\ndomains, highlighting the importance of understanding and ensuring its safety.\nOne pressing concern is the vulnerability of ML applications to model stealing\nattacks. These attacks involve adversaries attempting to recover a learned\nmodel through limited query-response interactions, such as those found in\ncloud-based services or on-chip artificial intelligence interfaces. While\nexisting literature proposes various attack and defense strategies, these often\nlack a theoretical foundation and standardized evaluation criteria. In\nresponse, this work presents a framework called ``Model Privacy'', providing a\nfoundation for comprehensively analyzing model stealing attacks and defenses.\nWe establish a rigorous formulation for the threat model and objectives,\npropose methods to quantify the goodness of attack and defense strategies, and\nanalyze the fundamental tradeoffs between utility and privacy in ML models. Our\ndeveloped theory offers valuable insights into enhancing the security of ML\nmodels, especially highlighting the importance of the attack-specific structure\nof perturbations for effective defenses. We demonstrate the application of\nmodel privacy from the defender's perspective through various learning\nscenarios. Extensive experiments corroborate the insights and the effectiveness\nof defense mechanisms developed under the proposed framework.",
    "updated" : "2025-02-21T16:29:11Z",
    "published" : "2025-02-21T16:29:11Z",
    "authors" : [
      {
        "name" : "Ganghua Wang"
      },
      {
        "name" : "Yuhong Yang"
      },
      {
        "name" : "Jie Ding"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.15233v1",
    "title" : "A General Pseudonymization Framework for Cloud-Based LLMs: Replacing\n  Privacy Information in Controlled Text Generation",
    "summary" : "An increasing number of companies have begun providing services that leverage\ncloud-based large language models (LLMs), such as ChatGPT. However, this\ndevelopment raises substantial privacy concerns, as users' prompts are\ntransmitted to and processed by the model providers. Among the various privacy\nprotection methods for LLMs, those implemented during the pre-training and\nfine-tuning phrases fail to mitigate the privacy risks associated with the\nremote use of cloud-based LLMs by users. On the other hand, methods applied\nduring the inference phrase are primarily effective in scenarios where the\nLLM's inference does not rely on privacy-sensitive information. In this paper,\nwe outline the process of remote user interaction with LLMs and, for the first\ntime, propose a detailed definition of a general pseudonymization framework\napplicable to cloud-based LLMs. The experimental results demonstrate that the\nproposed framework strikes an optimal balance between privacy protection and\nutility. The code for our method is available to the public at\nhttps://github.com/Mebymeby/Pseudonymization-Framework.",
    "updated" : "2025-02-21T06:15:53Z",
    "published" : "2025-02-21T06:15:53Z",
    "authors" : [
      {
        "name" : "Shilong Hou"
      },
      {
        "name" : "Ruilin Shang"
      },
      {
        "name" : "Zi Long"
      },
      {
        "name" : "Xianghua Fu"
      },
      {
        "name" : "Yin Chen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.14921v1",
    "title" : "The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic\n  Text",
    "summary" : "How much information about training samples can be gleaned from synthetic\ndata generated by Large Language Models (LLMs)? Overlooking the subtleties of\ninformation flow in synthetic data generation pipelines can lead to a false\nsense of privacy. In this paper, we design membership inference attacks (MIAs)\nthat target data used to fine-tune pre-trained LLMs that are then used to\nsynthesize data, particularly when the adversary does not have access to the\nfine-tuned model but only to the synthetic data. We show that such data-based\nMIAs do significantly better than a random guess, meaning that synthetic data\nleaks information about the training data. Further, we find that canaries\ncrafted to maximize vulnerability to model-based MIAs are sub-optimal for\nprivacy auditing when only synthetic data is released. Such out-of-distribution\ncanaries have limited influence on the model's output when prompted to generate\nuseful, in-distribution synthetic data, which drastically reduces their\nvulnerability. To tackle this problem, we leverage the mechanics of\nauto-regressive models to design canaries with an in-distribution prefix and a\nhigh-perplexity suffix that leave detectable traces in synthetic data. This\nenhances the power of data-based MIAs and provides a better assessment of the\nprivacy risks of releasing synthetic data generated by LLMs.",
    "updated" : "2025-02-19T15:30:30Z",
    "published" : "2025-02-19T15:30:30Z",
    "authors" : [
      {
        "name" : "Matthieu Meeus"
      },
      {
        "name" : "Lukas Wutschitz"
      },
      {
        "name" : "Santiago Zanella-Béguelin"
      },
      {
        "name" : "Shruti Tople"
      },
      {
        "name" : "Reza Shokri"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.17384v1",
    "title" : "On the Dichotomy Between Privacy and Traceability in $\\ell_p$ Stochastic\n  Convex Optimization",
    "summary" : "In this paper, we investigate the necessity of memorization in stochastic\nconvex optimization (SCO) under $\\ell_p$ geometries. Informally, we say a\nlearning algorithm memorizes $m$ samples (or is $m$-traceable) if, by analyzing\nits output, it is possible to identify at least $m$ of its training samples.\nOur main results uncover a fundamental tradeoff between traceability and excess\nrisk in SCO. For every $p\\in [1,\\infty)$, we establish the existence of a risk\nthreshold below which any sample-efficient learner must memorize a \\em{constant\nfraction} of its sample. For $p\\in [1,2]$, this threshold coincides with best\nrisk of differentially private (DP) algorithms, i.e., above this threshold,\nthere are algorithms that do not memorize even a single sample. This\nestablishes a sharp dichotomy between privacy and traceability for $p \\in\n[1,2]$. For $p \\in (2,\\infty)$, this threshold instead gives novel lower bounds\nfor DP learning, partially closing an open problem in this setup. En route of\nproving these results, we introduce a complexity notion we term \\em{trace\nvalue} of a problem, which unifies privacy lower bounds and traceability\nresults, and prove a sparse variant of the fingerprinting lemma.",
    "updated" : "2025-02-24T18:10:06Z",
    "published" : "2025-02-24T18:10:06Z",
    "authors" : [
      {
        "name" : "Sasha Voitovych"
      },
      {
        "name" : "Mahdi Haghifam"
      },
      {
        "name" : "Idan Attias"
      },
      {
        "name" : "Gintare Karolina Dziugaite"
      },
      {
        "name" : "Roi Livni"
      },
      {
        "name" : "Daniel M. Roy"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.17150v1",
    "title" : "Differential privacy guarantees of Markov chain Monte Carlo algorithms",
    "summary" : "This paper aims to provide differential privacy (DP) guarantees for Markov\nchain Monte Carlo (MCMC) algorithms. In a first part, we establish DP\nguarantees on samples output by MCMC algorithms as well as Monte Carlo\nestimators associated with these methods under assumptions on the convergence\nproperties of the underlying Markov chain. In particular, our results highlight\nthe critical condition of ensuring the target distribution is differentially\nprivate itself. In a second part, we specialise our analysis to the unadjusted\nLangevin algorithm and stochastic gradient Langevin dynamics and establish\nguarantees on their (R\\'enyi) DP. To this end, we develop a novel methodology\nbased on Girsanov's theorem combined with a perturbation trick to obtain bounds\nfor an unbounded domain and in a non-convex setting. We establish: (i) uniform\nin $n$ privacy guarantees when the state of the chain after $n$ iterations is\nreleased, (ii) bounds on the privacy of the entire chain trajectory. These\nfindings provide concrete guidelines for privacy-preserving MCMC.",
    "updated" : "2025-02-24T13:40:46Z",
    "published" : "2025-02-24T13:40:46Z",
    "authors" : [
      {
        "name" : "Andrea Bertazzi"
      },
      {
        "name" : "Tim Johnston"
      },
      {
        "name" : "Gareth O. Roberts"
      },
      {
        "name" : "Alain Durmus"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.17041v1",
    "title" : "PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal\n  Compliance",
    "summary" : "Recent advancements in generative large language models (LLMs) have enabled\nwider applicability, accessibility, and flexibility. However, their reliability\nand trustworthiness are still in doubt, especially for concerns regarding\nindividuals' data privacy. Great efforts have been made on privacy by building\nvarious evaluation benchmarks to study LLMs' privacy awareness and robustness\nfrom their generated outputs to their hidden representations. Unfortunately,\nmost of these works adopt a narrow formulation of privacy and only investigate\npersonally identifiable information (PII). In this paper, we follow the merit\nof the Contextual Integrity (CI) theory, which posits that privacy evaluation\nshould not only cover the transmitted attributes but also encompass the whole\nrelevant social context through private information flows. We present\nPrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted\nat legal compliance to cover well-annotated privacy and safety regulations,\nreal court cases, privacy policies, and synthetic data built from the official\ntoolkit to study LLMs' privacy and safety compliance. We evaluate the latest\nLLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our\nexperimental results suggest that though LLMs can effectively capture key CI\nparameters inside a given context, they still require further advancements for\nprivacy compliance.",
    "updated" : "2025-02-24T10:49:34Z",
    "published" : "2025-02-24T10:49:34Z",
    "authors" : [
      {
        "name" : "Haoran Li"
      },
      {
        "name" : "Wenbin Hu"
      },
      {
        "name" : "Huihao Jing"
      },
      {
        "name" : "Yulin Chen"
      },
      {
        "name" : "Qi Hu"
      },
      {
        "name" : "Sirui Han"
      },
      {
        "name" : "Tianshu Chu"
      },
      {
        "name" : "Peizhao Hu"
      },
      {
        "name" : "Yangqiu Song"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.16877v1",
    "title" : "APINT: A Full-Stack Framework for Acceleration of Privacy-Preserving\n  Inference of Transformers based on Garbled Circuits",
    "summary" : "As the importance of Privacy-Preserving Inference of Transformers (PiT)\nincreases, a hybrid protocol that integrates Garbled Circuits (GC) and\nHomomorphic Encryption (HE) is emerging for its implementation. While this\nprotocol is preferred for its ability to maintain accuracy, it has a severe\ndrawback of excessive latency. To address this, existing protocols primarily\nfocused on reducing HE latency, thus making GC the new latency bottleneck.\nFurthermore, previous studies only focused on individual computing layers, such\nas protocol or hardware accelerator, lacking a comprehensive solution at the\nsystem level. This paper presents APINT, a full-stack framework designed to\nreduce PiT's overall latency by addressing the latency problem of GC through\nboth software and hardware solutions. APINT features a novel protocol that\nreallocates possible GC workloads to alternative methods (i.e., HE or standard\nmatrix operation), substantially decreasing the GC workload. It also suggests\nGC-friendly circuit generation that reduces the number of AND gates at the\nmost, which is the expensive operator in GC. Furthermore, APINT proposes an\ninnovative netlist scheduling that combines coarse-grained operation mapping\nand fine-grained scheduling for maximal data reuse and minimal dependency.\nFinally, APINT's hardware accelerator, combined with its compiler speculation,\neffectively resolves the memory stall issue. Putting it all together, APINT\nachieves a remarkable end-to-end reduction in latency, outperforming the\nexisting protocol on CPU platform by 12.2x online and 2.2x offline. Meanwhile,\nthe APINT accelerator not only reduces its latency by 3.3x but also saves\nenergy consumption by 4.6x while operating PiT compared to the state-of-the-art\nGC accelerator.",
    "updated" : "2025-02-24T06:26:42Z",
    "published" : "2025-02-24T06:26:42Z",
    "authors" : [
      {
        "name" : "Hyunjun Cho"
      },
      {
        "name" : "Jaeho Jeon"
      },
      {
        "name" : "Jaehoon Heo"
      },
      {
        "name" : "Joo-Young Kim"
      }
    ],
    "categories" : [
      "cs.AR",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.16739v1",
    "title" : "Investigating the Security & Privacy Risks from Unsanctioned Technology\n  Use by Educators",
    "summary" : "Educational technologies are revolutionizing how educational institutions\noperate. Consequently, it makes them a lucrative target for breach and abuse as\nthey often serve as centralized hubs for diverse types of sensitive data, from\nacademic records to health information. Existing studies looked into how\nexisting stakeholders perceive the security and privacy risks of educational\ntechnologies and how those risks are affecting institutional policies for\nacquiring new technologies. However, outside of institutional vetting and\napproval, there is a pervasive practice of using applications and devices\nacquired personally. It is unclear how these applications and devices affect\nthe dynamics of the overall institutional ecosystem.\n  This study aims to address this gap by understanding why instructors use\nunsanctioned applications, how instructors perceive the associated risks, and\nhow it affects institutional security and privacy postures. We designed and\nconducted an online survey-based study targeting instructors and administrators\nfrom K-12 and higher education institutions.",
    "updated" : "2025-02-23T22:52:58Z",
    "published" : "2025-02-23T22:52:58Z",
    "authors" : [
      {
        "name" : "Easton Kelso"
      },
      {
        "name" : "Ananta Soneji"
      },
      {
        "name" : "Syed Zami-Ul-Haque Navid"
      },
      {
        "name" : "Yan Soshitaishvili"
      },
      {
        "name" : "Sazzadur Rahaman"
      },
      {
        "name" : "Rakibul Hasan"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.16519v1",
    "title" : "Guarding the Privacy of Label-Only Access to Neural Network Classifiers\n  via iDP Verification",
    "summary" : "Neural networks are susceptible to privacy attacks that can extract private\ninformation of the training set. To cope, several training algorithms guarantee\ndifferential privacy (DP) by adding noise to their computation. However, DP\nrequires to add noise considering every possible training set. This leads to a\nsignificant decrease in the network's accuracy. Individual DP (iDP) restricts\nDP to a given training set. We observe that some inputs deterministically\nsatisfy iDP without any noise. By identifying them, we can provide iDP\nlabel-only access to the network with a minor decrease to its accuracy.\nHowever, identifying the inputs that satisfy iDP without any noise is highly\nchallenging. Our key idea is to compute the iDP deterministic bound (iDP-DB),\nwhich overapproximates the set of inputs that do not satisfy iDP, and add noise\nonly to their predicted labels. To compute the tightest iDP-DB, which enables\nto guard the label-only access with minimal accuracy decrease, we propose\nLUCID, which leverages several formal verification techniques. First, it\nencodes the problem as a mixed-integer linear program, defined over a network\nand over every network trained identically but without a unique data point.\nSecond, it abstracts a set of networks using a hyper-network. Third, it\neliminates the overapproximation error via a novel branch-and-bound technique.\nFourth, it bounds the differences of matching neurons in the network and the\nhyper-network and employs linear relaxation if they are small. We show that\nLUCID can provide classifiers with a perfect individuals' privacy guarantee\n(0-iDP) -- which is infeasible for DP training algorithms -- with an accuracy\ndecrease of 1.4%. For more relaxed $\\varepsilon$-iDP guarantees, LUCID has an\naccuracy decrease of 1.2%. In contrast, existing DP training algorithms reduce\nthe accuracy by 12.7%.",
    "updated" : "2025-02-23T09:50:26Z",
    "published" : "2025-02-23T09:50:26Z",
    "authors" : [
      {
        "name" : "Anan Kabaha"
      },
      {
        "name" : "Dana Drachsler-Cohen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.PL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.16091v1",
    "title" : "Privacy-Aware Joint DNN Model Deployment and Partition Optimization for\n  Delay-Efficient Collaborative Edge Inference",
    "summary" : "Edge inference (EI) is a key solution to address the growing challenges of\ndelayed response times, limited scalability, and privacy concerns in\ncloud-based Deep Neural Network (DNN) inference. However, deploying DNN models\non resource-constrained edge devices faces more severe challenges, such as\nmodel storage limitations, dynamic service requests, and privacy risks. This\npaper proposes a novel framework for privacy-aware joint DNN model deployment\nand partition optimization to minimize long-term average inference delay under\nresource and privacy constraints. Specifically, the problem is formulated as a\ncomplex optimization problem considering model deployment, user-server\nassociation, and model partition strategies. To handle the NP-hardness and\nfuture uncertainties, a Lyapunov-based approach is introduced to transform the\nlong-term optimization into a single-time-slot problem, ensuring system\nperformance. Additionally, a coalition formation game model is proposed for\nedge server association, and a greedy-based algorithm is developed for model\ndeployment within each coalition to efficiently solve the problem. Extensive\nsimulations show that the proposed algorithms effectively reduce inference\ndelay while satisfying privacy constraints, outperforming baseline approaches\nin various scenarios.",
    "updated" : "2025-02-22T05:27:24Z",
    "published" : "2025-02-22T05:27:24Z",
    "authors" : [
      {
        "name" : "Zhipeng Cheng"
      },
      {
        "name" : "Xiaoyu Xia"
      },
      {
        "name" : "Hong Wang"
      },
      {
        "name" : "Minghui Liwang"
      },
      {
        "name" : "Ning Chen"
      },
      {
        "name" : "Xuwei Fan"
      },
      {
        "name" : "Xianbin Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.15929v1",
    "title" : "Approximate Differential Privacy of the $\\ell_2$ Mechanism",
    "summary" : "We study the $\\ell_2$ mechanism for computing a $d$-dimensional statistic\nwith bounded $\\ell_2$ sensitivity under approximate differential privacy.\nAcross a range of privacy parameters, we find that the $\\ell_2$ mechanism\nobtains lower error than the Laplace and Gaussian mechanisms, matching the\nformer at $d=1$ and approaching the latter as $d \\to \\infty$.",
    "updated" : "2025-02-21T20:56:34Z",
    "published" : "2025-02-21T20:56:34Z",
    "authors" : [
      {
        "name" : "Matthew Joseph"
      },
      {
        "name" : "Alex Kulesza"
      },
      {
        "name" : "Alexander Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.05765v2",
    "title" : "Privacy-Preserving Dataset Combination",
    "summary" : "Access to diverse, high-quality datasets is crucial for machine learning\nmodel performance, yet data sharing remains limited by privacy concerns and\ncompetitive interests, particularly in regulated domains like healthcare. This\ndynamic especially disadvantages smaller organizations that lack resources to\npurchase data or negotiate favorable sharing agreements. We present SecureKL, a\nprivacy-preserving framework that enables organizations to identify beneficial\ndata partnerships without exposing sensitive information. Building on recent\nadvances in dataset combination methods, we develop a secure multiparty\ncomputation protocol that maintains strong privacy guarantees while achieving\n>90\\% correlation with plaintext evaluations. In experiments with real-world\nhospital data, SecureKL successfully identifies beneficial data partnerships\nthat improve model performance for intensive care unit mortality prediction\nwhile preserving data privacy. Our framework provides a practical solution for\norganizations seeking to leverage collective data resources while maintaining\nprivacy and competitive advantages. These results demonstrate the potential for\nprivacy-preserving data collaboration to advance machine learning applications\nin high-stakes domains while promoting more equitable access to data resources.",
    "updated" : "2025-02-24T00:53:08Z",
    "published" : "2025-02-09T03:54:17Z",
    "authors" : [
      {
        "name" : "Keren Fuentes"
      },
      {
        "name" : "Mimee Xu"
      },
      {
        "name" : "Irene Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.18227v1",
    "title" : "TLDP: An Algorithm of Local Differential Privacy for Tensors",
    "summary" : "Tensor-valued data, increasingly common in applications like spatiotemporal\nmodeling and social networks, pose unique challenges for privacy protection due\nto their multidimensional structure and the risk of losing critical structural\ninformation. Traditional local differential privacy (LDP) methods, designed for\nscalars and matrices, are insufficient for tensors, as they fail to preserve\nessential relationships among tensor elements. We introduce TLDP, a novel\n\\emph{LDP} algorithm for \\emph{T}ensors, which employs a randomized response\nmechanism to perturb tensor components while maintaining structural integrity.\nTo strike a better balance between utility and privacy, we incorporate a weight\nmatrix that selectively protects sensitive regions. Both theoretical analysis\nand empirical findings from real-world datasets show that TLDP achieves\nsuperior utility while preserving privacy, making it a robust solution for\nhigh-dimensional data.",
    "updated" : "2025-02-25T14:11:45Z",
    "published" : "2025-02-25T14:11:45Z",
    "authors" : [
      {
        "name" : "Yachao Yuan"
      },
      {
        "name" : "Xiao Tang"
      },
      {
        "name" : "Yu Huang"
      },
      {
        "name" : "Jin Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.17772v1",
    "title" : "An Improved Privacy and Utility Analysis of Differentially Private SGD\n  with Bounded Domain and Smooth Losses",
    "summary" : "Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to\nprotect sensitive data during the training of machine learning models, but its\nprivacy guarantees often come at the cost of model performance, largely due to\nthe inherent challenge of accurately quantifying privacy loss. While recent\nefforts have strengthened privacy guarantees by focusing solely on the final\noutput and bounded domain cases, they still impose restrictive assumptions,\nsuch as convexity and other parameter limitations, and often lack a thorough\nanalysis of utility. In this paper, we provide rigorous privacy and utility\ncharacterization for DPSGD for smooth loss functions in both bounded and\nunbounded domains. We track the privacy loss over multiple iterations by\nexploiting the noisy smooth-reduction property and establish the utility\nanalysis by leveraging the projection's non-expansiveness and clipped SGD\nproperties. In particular, we show that for DPSGD with a bounded domain, (i)\nthe privacy loss can still converge without the convexity assumption, and (ii)\na smaller bounded diameter can improve both privacy and utility simultaneously\nunder certain conditions. Numerical results validate our results.",
    "updated" : "2025-02-25T02:05:41Z",
    "published" : "2025-02-25T02:05:41Z",
    "authors" : [
      {
        "name" : "Hao Liang"
      },
      {
        "name" : "Wanrong Zhang"
      },
      {
        "name" : "Xinlei He"
      },
      {
        "name" : "Kaishun He"
      },
      {
        "name" : "Hong Xing"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.17748v1",
    "title" : "FinP: Fairness-in-Privacy in Federated Learning by Addressing\n  Disparities in Privacy Risk",
    "summary" : "Ensuring fairness in machine learning, particularly in human-centric\napplications, extends beyond algorithmic bias to encompass fairness in privacy,\nspecifically the equitable distribution of privacy risk. This is critical in\nfederated learning (FL), where decentralized data necessitates balanced privacy\npreservation across clients. We introduce FinP, a framework designed to achieve\nfairness in privacy by mitigating disproportionate exposure to source inference\nattacks (SIA). FinP employs a dual approach: (1) server-side adaptive\naggregation to address unfairness in client contributions in global model, and\n(2) client-side regularization to reduce client vulnerability. This\ncomprehensive strategy targets both the symptoms and root causes of privacy\nunfairness. Evaluated on the Human Activity Recognition (HAR) and CIFAR-10\ndatasets, FinP demonstrates ~20% improvement in fairness in privacy on HAR with\nminimal impact on model utility, and effectively mitigates SIA risks on\nCIFAR-10, showcasing its ability to provide fairness in privacy in FL systems\nwithout compromising performance.",
    "updated" : "2025-02-25T00:56:47Z",
    "published" : "2025-02-25T00:56:47Z",
    "authors" : [
      {
        "name" : "Tianyu Zhao"
      },
      {
        "name" : "Mahmoud Srewa"
      },
      {
        "name" : "Salma Elmalaki"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.17591v1",
    "title" : "Proactive Privacy Amnesia for Large Language Models: Safeguarding PII\n  with Negligible Impact on Model Utility",
    "summary" : "With the rise of large language models (LLMs), increasing research has\nrecognized their risk of leaking personally identifiable information (PII)\nunder malicious attacks. Although efforts have been made to protect PII in\nLLMs, existing methods struggle to balance privacy protection with maintaining\nmodel utility. In this paper, inspired by studies of amnesia in cognitive\nscience, we propose a novel approach, Proactive Privacy Amnesia (PPA), to\nsafeguard PII in LLMs while preserving their utility. This mechanism works by\nactively identifying and forgetting key memories most closely associated with\nPII in sequences, followed by a memory implanting using suitable substitute\nmemories to maintain the LLM's functionality. We conduct evaluations across\nmultiple models to protect common PII, such as phone numbers and physical\naddresses, against prevalent PII-targeted attacks, demonstrating the\nsuperiority of our method compared with other existing defensive techniques.\nThe results show that our PPA method completely eliminates the risk of phone\nnumber exposure by 100% and significantly reduces the risk of physical address\nexposure by 9.8% - 87.6%, all while maintaining comparable model utility\nperformance.",
    "updated" : "2025-02-24T19:16:39Z",
    "published" : "2025-02-24T19:16:39Z",
    "authors" : [
      {
        "name" : "Martin Kuo"
      },
      {
        "name" : "Jingyang Zhang"
      },
      {
        "name" : "Jianyi Zhang"
      },
      {
        "name" : "Minxue Tang"
      },
      {
        "name" : "Louis DiValentin"
      },
      {
        "name" : "Aolin Ding"
      },
      {
        "name" : "Jingwei Sun"
      },
      {
        "name" : "William Chen"
      },
      {
        "name" : "Amin Hass"
      },
      {
        "name" : "Tianlong Chen"
      },
      {
        "name" : "Yiran Chen"
      },
      {
        "name" : "Hai Li"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.17485v1",
    "title" : "Decentralized and Robust Privacy-Preserving Model Using\n  Blockchain-Enabled Federated Deep Learning in Intelligent Enterprises",
    "summary" : "In Federated Deep Learning (FDL), multiple local enterprises are allowed to\ntrain a model jointly. Then, they submit their local updates to the central\nserver, and the server aggregates the updates to create a global model.\nHowever, trained models usually perform worse than centralized models,\nespecially when the training data distribution is non-independent and\nidentically distributed (nonIID). NonIID data harms the accuracy and\nperformance of the model. Additionally, due to the centrality of federated\nlearning (FL) and the untrustworthiness of enterprises, traditional FL\nsolutions are vulnerable to security and privacy attacks. To tackle this issue,\nwe propose FedAnil, a secure blockchain enabled Federated Deep Learning Model\nthat improves enterprise models decentralization, performance, and tamper proof\nproperties, incorporating two main phases. The first phase addresses the nonIID\nchallenge (label and feature distribution skew). The second phase addresses\nsecurity and privacy concerns against poisoning and inference attacks through\nthree steps. Extensive experiments were conducted using the Sent140,\nFashionMNIST, FEMNIST, and CIFAR10 new real world datasets to evaluate FedAnils\nrobustness and performance. The simulation results demonstrate that FedAnil\nsatisfies FDL privacy preserving requirements. In terms of convergence\nanalysis, the model parameter obtained with FedAnil converges to the optimum of\nthe model parameter. In addition, it performs better in terms of accuracy (more\nthan 11, 15, and 24%) and computation overhead (less than 8, 10, and 15%)\ncompared with baseline approaches, namely ShieldFL, RVPFL, and RFA.",
    "updated" : "2025-02-18T15:17:25Z",
    "published" : "2025-02-18T15:17:25Z",
    "authors" : [
      {
        "name" : "Reza Fotohi"
      },
      {
        "name" : "Fereidoon Shams Aliee"
      },
      {
        "name" : "Bahar Farahani"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.19154v1",
    "title" : "Towards Privacy-Preserving Anomaly-Based Intrusion Detection in Energy\n  Communities",
    "summary" : "Energy communities consist of decentralized energy production, storage,\nconsumption, and distribution and are gaining traction in modern power systems.\nHowever, these communities may increase the vulnerability of the grid to cyber\nthreats. We propose an anomaly-based intrusion detection system to enhance the\nsecurity of energy communities. The system leverages deep autoencoders to\ndetect deviations from normal operational patterns in order to identify\nanomalies induced by malicious activities and attacks. Operational data for\ntraining and evaluation are derived from a Simulink model of an energy\ncommunity. The results show that the autoencoder-based intrusion detection\nsystem achieves good detection performance across multiple attack scenarios. We\nalso demonstrate potential for real-world application of the system by training\na federated model that enables distributed intrusion detection while preserving\ndata privacy.",
    "updated" : "2025-02-26T14:13:02Z",
    "published" : "2025-02-26T14:13:02Z",
    "authors" : [
      {
        "name" : "Zeeshan Afzal"
      },
      {
        "name" : "Giovanni Gaggero"
      },
      {
        "name" : "Mikael Asplund"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.19119v1",
    "title" : "Chemical knowledge-informed framework for privacy-aware retrosynthesis\n  learning",
    "summary" : "Chemical reaction data is a pivotal asset, driving advances in competitive\nfields such as pharmaceuticals, materials science, and industrial chemistry.\nIts proprietary nature renders it sensitive, as it often includes confidential\ninsights and competitive advantages organizations strive to protect. However,\nin contrast to this need for confidentiality, the current standard training\nparadigm for machine learning-based retrosynthesis gathers reaction data from\nmultiple sources into one single edge to train prediction models. This paradigm\nposes considerable privacy risks as it necessitates broad data availability\nacross organizational boundaries and frequent data transmission between\nentities, potentially exposing proprietary information to unauthorized access\nor interception during storage and transfer. In the present study, we introduce\nthe chemical knowledge-informed framework (CKIF), a privacy-preserving approach\nfor learning retrosynthesis models. CKIF enables distributed training across\nmultiple chemical organizations without compromising the confidentiality of\nproprietary reaction data. Instead of gathering raw reaction data, CKIF learns\nretrosynthesis models through iterative, chemical knowledge-informed\naggregation of model parameters. In particular, the chemical properties of\npredicted reactants are leveraged to quantitatively assess the observable\nbehaviors of individual models, which in turn determines the adaptive weights\nused for model aggregation. On a variety of reaction datasets, CKIF outperforms\nseveral strong baselines by a clear margin (e.g., ~20% performance improvement\nover FedAvg on USPTO-50K), showing its feasibility and superiority to stimulate\nfurther research on privacy-preserving retrosynthesis.",
    "updated" : "2025-02-26T13:13:24Z",
    "published" : "2025-02-26T13:13:24Z",
    "authors" : [
      {
        "name" : "Guikun Chen"
      },
      {
        "name" : "Xu Zhang"
      },
      {
        "name" : "Yi Yang"
      },
      {
        "name" : "Wenguan Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.19082v1",
    "title" : "Trust-Enabled Privacy: Social Media Designs to Support Adolescent User\n  Boundary Regulation",
    "summary" : "Through a three-part co-design study involving 19 teens aged 13-18, we\nidentify key barriers to effective boundary regulation on social media,\nincluding ambiguous audience expectations, social risks associated with\noversharing, and the lack of design affordances that facilitate trust-building.\nOur findings reveal that while adolescents seek casual, frequent sharing to\nstrengthen relationships, existing platform norms and designs often discourage\nsuch interactions, leading to withdrawal. To address these challenges, we\nintroduce trust-enabled privacy as a design framework that recognizes trust,\nwhether building or eroding, as central to boundary regulation. When trust is\nsupported, boundary regulation becomes more adaptive and empowering; when it\nerodes, users default to self-censorship or withdrawal. We propose concrete\ndesign affordances, including guided disclosure, contextual audience\nsegmentation, intentional engagement signaling, and trust-centered norms, to\nhelp platforms foster a more dynamic and nuanced privacy experience for teen\nsocial media users. By reframing privacy as a trust-driven process rather than\na rigid control-based trade-off, this work provides empirical insights and\nactionable guidelines for designing social media environments that empower\nteens to manage their online presence while fostering meaningful social\nconnections.",
    "updated" : "2025-02-26T12:19:53Z",
    "published" : "2025-02-26T12:19:53Z",
    "authors" : [
      {
        "name" : "JaeWon Kim"
      },
      {
        "name" : "Robert Wolfe"
      },
      {
        "name" : "Ramya Bhagirathi Subramanian"
      },
      {
        "name" : "Mei-Hsuan Lee"
      },
      {
        "name" : "Jessica Colnago"
      },
      {
        "name" : "Alexis Hiniker"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.18974v1",
    "title" : "Distributed Transition System with Tags and Value-wise Metric, for\n  Privacy Analysis",
    "summary" : "We introduce a logical framework named Distributed Labeled Tagged Transition\nSystem (DLTTS), using concepts from Probabilistic Automata, Probabilistic\nConcurrent Systems, and Probabilistic labelled transition systems. We show that\nDLTTS can be used to formally model how a given piece of private information P\n(e.g., a set of tuples) stored in a given database D can get captured\nprogressively by an adversary A repeatedly querying D, enhancing the knowledge\nacquired from the answers to these queries with relational deductions using\ncertain additional non-private data. The database D is assumed protected with\ngeneralization mechanisms. We also show that, on a large class of databases,\nmetrics can be defined 'value-wise', and more general notions of adjacency\nbetween data bases can be defined, based on these metrics. These notions can\nalso play a role in differentially private protection mechanisms.",
    "updated" : "2025-02-26T09:35:48Z",
    "published" : "2025-02-26T09:35:48Z",
    "authors" : [
      {
        "name" : "Siva Anantharaman"
      },
      {
        "name" : "Sabine Frittella"
      },
      {
        "name" : "Benjamin Nguyen"
      }
    ],
    "categories" : [
      "cs.LO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.18706v1",
    "title" : "Differentially Private Federated Learning With Time-Adaptive Privacy\n  Spending",
    "summary" : "Federated learning (FL) with differential privacy (DP) provides a framework\nfor collaborative machine learning, enabling clients to train a shared model\nwhile adhering to strict privacy constraints. The framework allows each client\nto have an individual privacy guarantee, e.g., by adding different amounts of\nnoise to each client's model updates. One underlying assumption is that all\nclients spend their privacy budgets uniformly over time (learning rounds).\nHowever, it has been shown in the literature that learning in early rounds\ntypically focuses on more coarse-grained features that can be learned at lower\nsignal-to-noise ratios while later rounds learn fine-grained features that\nbenefit from higher signal-to-noise ratios. Building on this intuition, we\npropose a time-adaptive DP-FL framework that expends the privacy budget\nnon-uniformly across both time and clients. Our framework enables each client\nto save privacy budget in early rounds so as to be able to spend more in later\nrounds when additional accuracy is beneficial in learning more fine-grained\nfeatures. We theoretically prove utility improvements in the case that clients\nwith stricter privacy budgets spend budgets unevenly across rounds, compared to\nclients with more relaxed budgets, who have sufficient budgets to distribute\ntheir spend more evenly. Our practical experiments on standard benchmark\ndatasets support our theoretical results and show that, in practice, our\nalgorithms improve the privacy-utility trade-offs compared to baseline schemes.",
    "updated" : "2025-02-25T23:56:23Z",
    "published" : "2025-02-25T23:56:23Z",
    "authors" : [
      {
        "name" : "Shahrzad Kiani"
      },
      {
        "name" : "Nupur Kulkarni"
      },
      {
        "name" : "Adam Dziedzic"
      },
      {
        "name" : "Stark Draper"
      },
      {
        "name" : "Franziska Boenisch"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.18697v1",
    "title" : "H-FLTN: A Privacy-Preserving Hierarchical Framework for Electric Vehicle\n  Spatio-Temporal Charge Prediction",
    "summary" : "The widespread adoption of Electric Vehicles (EVs) poses critical challenges\nfor energy providers, particularly in predicting charging time (temporal\nprediction), ensuring user privacy, and managing resources efficiently in\nmobility-driven networks. This paper introduces the Hierarchical Federated\nLearning Transformer Network (H-FLTN) framework to address these challenges.\nH-FLTN employs a three-tier hierarchical architecture comprising EVs, community\nDistributed Energy Resource Management Systems (DERMS), and the Energy Provider\nData Centre (EPDC) to enable accurate spatio-temporal predictions of EV\ncharging needs while preserving privacy. Temporal prediction is enhanced using\nTransformer-based learning, capturing complex dependencies in charging\nbehavior. Privacy is ensured through Secure Aggregation, Additive Secret\nSharing, and Peer-to-Peer (P2P) Sharing with Augmentation, which allow only\nsecret shares of model weights to be exchanged while securing all\ntransmissions. To improve training efficiency and resource management, H-FLTN\nintegrates Dynamic Client Capping Mechanism (DCCM) and Client Rotation\nManagement (CRM), ensuring that training remains both computationally and\ntemporally efficient as the number of participating EVs increases. DCCM\noptimises client participation by limiting excessive computational loads, while\nCRM balances training contributions across epochs, preventing imbalanced\nparticipation. Our simulation results based on large-scale empirical vehicle\nmobility data reveal that DCCM and CRM reduce the training time complexity with\nincreasing EVs from linear to constant. Its integration into real-world smart\ncity infrastructure enhances energy demand forecasting, resource allocation,\nand grid stability, ensuring reliability and sustainability in future mobility\necosystems.",
    "updated" : "2025-02-25T23:20:53Z",
    "published" : "2025-02-25T23:20:53Z",
    "authors" : [
      {
        "name" : "Robert Marlin"
      },
      {
        "name" : "Raja Jurdak"
      },
      {
        "name" : "Alsharif Abuadbba"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "I.6.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.18623v1",
    "title" : "On the Privacy-Preserving Properties of Spiking Neural Networks with\n  Unique Surrogate Gradients and Quantization Levels",
    "summary" : "As machine learning models increasingly process sensitive data, understanding\ntheir vulnerability to privacy attacks is vital. Membership inference attacks\n(MIAs) exploit model responses to infer whether specific data points were used\nduring training, posing a significant privacy risk. Prior research suggests\nthat spiking neural networks (SNNs), which rely on event-driven computation and\ndiscrete spike-based encoding, exhibit greater resilience to MIAs than\nartificial neural networks (ANNs). This resilience stems from their\nnon-differentiable activations and inherent stochasticity, which obscure the\ncorrelation between model responses and individual training samples. To enhance\nprivacy in SNNs, we explore two techniques: quantization and surrogate\ngradients. Quantization, which reduces precision to limit information leakage,\nhas improved privacy in ANNs. Given SNNs' sparse and irregular activations,\nquantization may further disrupt the activation patterns exploited by MIAs. We\nassess the vulnerability of SNNs and ANNs under weight and activation\nquantization across multiple datasets, using the attack model's receiver\noperating characteristic (ROC) curve area under the curve (AUC) metric, where\nlower values indicate stronger privacy, and evaluate the privacy-accuracy\ntrade-off. Our findings show that quantization enhances privacy in both\narchitectures with minimal performance loss, though full-precision SNNs remain\nmore resilient than quantized ANNs. Additionally, we examine the impact of\nsurrogate gradients on privacy in SNNs. Among five evaluated gradients, spike\nrate escape provides the best privacy-accuracy trade-off, while arctangent\nincreases vulnerability to MIAs. These results reinforce SNNs' inherent privacy\nadvantages and demonstrate that quantization and surrogate gradient selection\nsignificantly influence privacy-accuracy trade-offs in SNNs.",
    "updated" : "2025-02-25T20:14:14Z",
    "published" : "2025-02-25T20:14:14Z",
    "authors" : [
      {
        "name" : "Ayana Moshruba"
      },
      {
        "name" : "Shay Snyder"
      },
      {
        "name" : "Hamed Poursiami"
      },
      {
        "name" : "Maryam Parsa"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.18545v1",
    "title" : "PII-Bench: Evaluating Query-Aware Privacy Protection Systems",
    "summary" : "The widespread adoption of Large Language Models (LLMs) has raised\nsignificant privacy concerns regarding the exposure of personally identifiable\ninformation (PII) in user prompts. To address this challenge, we propose a\nquery-unrelated PII masking strategy and introduce PII-Bench, the first\ncomprehensive evaluation framework for assessing privacy protection systems.\nPII-Bench comprises 2,842 test samples across 55 fine-grained PII categories,\nfeaturing diverse scenarios from single-subject descriptions to complex\nmulti-party interactions. Each sample is carefully crafted with a user query,\ncontext description, and standard answer indicating query-relevant PII. Our\nempirical evaluation reveals that while current models perform adequately in\nbasic PII detection, they show significant limitations in determining PII query\nrelevance. Even state-of-the-art LLMs struggle with this task, particularly in\nhandling complex multi-subject scenarios, indicating substantial room for\nimprovement in achieving intelligent PII masking.",
    "updated" : "2025-02-25T14:49:08Z",
    "published" : "2025-02-25T14:49:08Z",
    "authors" : [
      {
        "name" : "Hao Shen"
      },
      {
        "name" : "Zhouhong Gu"
      },
      {
        "name" : "Haokai Hong"
      },
      {
        "name" : "Weili Han"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.18527v1",
    "title" : "GOD model: Privacy Preserved AI School for Personal Assistant",
    "summary" : "Personal AI assistants (e.g., Apple Intelligence, Meta AI) offer proactive\nrecommendations that simplify everyday tasks, but their reliance on sensitive\nuser data raises concerns about privacy and trust. To address these challenges,\nwe introduce the Guardian of Data (GOD), a secure, privacy-preserving framework\nfor training and evaluating AI assistants directly on-device. Unlike\ntraditional benchmarks, the GOD model measures how well assistants can\nanticipate user needs-such as suggesting gifts-while protecting user data and\nautonomy. Functioning like an AI school, it addresses the cold start problem by\nsimulating user queries and employing a curriculum-based approach to refine the\nperformance of each assistant. Running within a Trusted Execution Environment\n(TEE), it safeguards user data while applying reinforcement and imitation\nlearning to refine AI recommendations. A token-based incentive system\nencourages users to share data securely, creating a data flywheel that drives\ncontinuous improvement. By integrating privacy, personalization, and trust, the\nGOD model provides a scalable, responsible path for advancing personal AI\nassistants. For community collaboration, part of the framework is open-sourced\nat https://github.com/PIN-AI/God-Model.",
    "updated" : "2025-02-24T20:30:17Z",
    "published" : "2025-02-24T20:30:17Z",
    "authors" : [
      {
        "name" : " PIN AI Team"
      },
      {
        "name" : "Bill Qingyun Sun"
      },
      {
        "name" : "Laura Florescu"
      },
      {
        "name" : "Boliang Zhang"
      },
      {
        "name" : "Regan Peng"
      },
      {
        "name" : "Smile Hu"
      },
      {
        "name" : "Shouqiao Wang"
      },
      {
        "name" : "Ben Wu"
      },
      {
        "name" : "Xi Wang"
      },
      {
        "name" : "Davide Crapis"
      },
      {
        "name" : "Gavin Zhen Guo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.16519v2",
    "title" : "Guarding the Privacy of Label-Only Access to Neural Network Classifiers\n  via iDP Verification",
    "summary" : "Neural networks are susceptible to privacy attacks that can extract private\ninformation of the training set. To cope, several training algorithms guarantee\ndifferential privacy (DP) by adding noise to their computation. However, DP\nrequires to add noise considering every possible training set. This leads to a\nsignificant decrease in the network's accuracy. Individual DP (iDP) restricts\nDP to a given training set. We observe that some inputs deterministically\nsatisfy iDP without any noise. By identifying them, we can provide iDP\nlabel-only access to the network with a minor decrease to its accuracy.\nHowever, identifying the inputs that satisfy iDP without any noise is highly\nchallenging. Our key idea is to compute the iDP deterministic bound (iDP-DB),\nwhich overapproximates the set of inputs that do not satisfy iDP, and add noise\nonly to their predicted labels. To compute the tightest iDP-DB, which enables\nto guard the label-only access with minimal accuracy decrease, we propose\nLUCID, which leverages several formal verification techniques. First, it\nencodes the problem as a mixed-integer linear program, defined over a network\nand over every network trained identically but without a unique data point.\nSecond, it abstracts a set of networks using a hyper-network. Third, it\neliminates the overapproximation error via a novel branch-and-bound technique.\nFourth, it bounds the differences of matching neurons in the network and the\nhyper-network and employs linear relaxation if they are small. We show that\nLUCID can provide classifiers with a perfect individuals' privacy guarantee\n(0-iDP) -- which is infeasible for DP training algorithms -- with an accuracy\ndecrease of 1.4%. For more relaxed $\\varepsilon$-iDP guarantees, LUCID has an\naccuracy decrease of 1.2%. In contrast, existing DP training algorithms reduce\nthe accuracy by 12.7%.",
    "updated" : "2025-02-26T16:31:42Z",
    "published" : "2025-02-23T09:50:26Z",
    "authors" : [
      {
        "name" : "Anan Kabaha"
      },
      {
        "name" : "Dana Drachsler-Cohen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.PL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.18517v1",
    "title" : "RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via\n  Reward Driven Data Synthesis",
    "summary" : "The success of large language models (LLMs) has attracted many individuals to\nfine-tune them for domain-specific tasks by uploading their data. However, in\nsensitive areas like healthcare and finance, privacy concerns often arise. One\npromising solution is to sample synthetic data with Differential Privacy (DP)\nguarantees to replace private data. However, these synthetic data contain\nsignificant flawed data, which are considered as noise. Existing solutions\ntypically rely on naive filtering by comparing ROUGE-L scores or embedding\nsimilarities, which are ineffective in addressing the noise. To address this\nissue, we propose RewardDS, a novel privacy-preserving framework that\nfine-tunes a reward proxy model and uses reward signals to guide the synthetic\ndata generation. Our RewardDS introduces two key modules, Reward Guided\nFiltering and Self-Optimizing Refinement, to both filter and refine the\nsynthetic data, effectively mitigating the noise. Extensive experiments across\nmedical, financial, and code generation domains demonstrate the effectiveness\nof our method.",
    "updated" : "2025-02-23T02:52:23Z",
    "published" : "2025-02-23T02:52:23Z",
    "authors" : [
      {
        "name" : "Jianwei Wang"
      },
      {
        "name" : "Junyao Yang"
      },
      {
        "name" : "Haoran Li"
      },
      {
        "name" : "Huiping Zhuang"
      },
      {
        "name" : "Cen Chen"
      },
      {
        "name" : "Ziqian Zeng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.18509v1",
    "title" : "Protecting Users From Themselves: Safeguarding Contextual Privacy in\n  Interactions with Conversational Agents",
    "summary" : "Conversational agents are increasingly woven into individuals' personal\nlives, yet users often underestimate the privacy risks involved. The moment\nusers share information with these agents (e.g., LLMs), their private\ninformation becomes vulnerable to exposure. In this paper, we characterize the\nnotion of contextual privacy for user interactions with LLMs. It aims to\nminimize privacy risks by ensuring that users (sender) disclose only\ninformation that is both relevant and necessary for achieving their intended\ngoals when interacting with LLMs (untrusted receivers). Through a formative\ndesign user study, we observe how even \"privacy-conscious\" users inadvertently\nreveal sensitive information through indirect disclosures. Based on insights\nfrom this study, we propose a locally-deployable framework that operates\nbetween users and LLMs, and identifies and reformulates out-of-context\ninformation in user prompts. Our evaluation using examples from ShareGPT shows\nthat lightweight models can effectively implement this framework, achieving\nstrong gains in contextual privacy while preserving the user's intended\ninteraction goals through different approaches to classify information relevant\nto the intended goals.",
    "updated" : "2025-02-22T09:05:39Z",
    "published" : "2025-02-22T09:05:39Z",
    "authors" : [
      {
        "name" : "Ivoline Ngong"
      },
      {
        "name" : "Swanand Kadhe"
      },
      {
        "name" : "Hao Wang"
      },
      {
        "name" : "Keerthiram Murugesan"
      },
      {
        "name" : "Justin D. Weisz"
      },
      {
        "name" : "Amit Dhurandhar"
      },
      {
        "name" : "Karthikeyan Natesan Ramamurthy"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.16091v1",
    "title" : "Privacy-Aware Joint DNN Model Deployment and Partition Optimization for\n  Delay-Efficient Collaborative Edge Inference",
    "summary" : "Edge inference (EI) is a key solution to address the growing challenges of\ndelayed response times, limited scalability, and privacy concerns in\ncloud-based Deep Neural Network (DNN) inference. However, deploying DNN models\non resource-constrained edge devices faces more severe challenges, such as\nmodel storage limitations, dynamic service requests, and privacy risks. This\npaper proposes a novel framework for privacy-aware joint DNN model deployment\nand partition optimization to minimize long-term average inference delay under\nresource and privacy constraints. Specifically, the problem is formulated as a\ncomplex optimization problem considering model deployment, user-server\nassociation, and model partition strategies. To handle the NP-hardness and\nfuture uncertainties, a Lyapunov-based approach is introduced to transform the\nlong-term optimization into a single-time-slot problem, ensuring system\nperformance. Additionally, a coalition formation game model is proposed for\nedge server association, and a greedy-based algorithm is developed for model\ndeployment within each coalition to efficiently solve the problem. Extensive\nsimulations show that the proposed algorithms effectively reduce inference\ndelay while satisfying privacy constraints, outperforming baseline approaches\nin various scenarios.",
    "updated" : "2025-02-22T05:27:24Z",
    "published" : "2025-02-22T05:27:24Z",
    "authors" : [
      {
        "name" : "Zhipeng Cheng"
      },
      {
        "name" : "Xiaoyu Xia"
      },
      {
        "name" : "Hong Wang"
      },
      {
        "name" : "Minghui Liwang"
      },
      {
        "name" : "Ning Chen"
      },
      {
        "name" : "Xuwei Fan"
      },
      {
        "name" : "Xianbin Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.10788v2",
    "title" : "Analyzing Privacy Dynamics within Groups using Gamified Auctions",
    "summary" : "Online shared content, such as group pictures, often contains information\nabout multiple users. Developing technical solutions to manage the privacy of\nsuch \"co-owned\" content is challenging because each co-owner may have different\npreferences. Recent technical approaches advocate group-decision mechanisms,\nincluding auctions, to decide as how best to resolve these differences.\nHowever, it is not clear if users would participate in such mechanisms and if\nthey do, whether they would act altruistically. Understanding the privacy\ndynamics is crucial to develop effective mechanisms for privacy-respecting\ncollaborative systems. Accordingly, this work develops RESOLVE, a privacy\nauction game to understand the sharing behavior of users in groups. Our results\nof users' playing the game show that i) the users' understanding of individual\nvs. group privacy differs significantly; ii) often users fight for their\npreferences even at the cost of others' privacy; and iii) at times users\ncollaborate to fight for the privacy of others.",
    "updated" : "2025-02-25T23:22:50Z",
    "published" : "2025-02-15T12:48:30Z",
    "authors" : [
      {
        "name" : "Hüseyin Aydın"
      },
      {
        "name" : "Onuralp Ulusoy"
      },
      {
        "name" : "Ilaria Liccardi"
      },
      {
        "name" : "Pınar Yolum"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  }
]