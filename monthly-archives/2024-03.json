[{"id":"http://arxiv.org/abs/2403.01356v1","title":"Security and Privacy Enhancing in Blockchain-based IoT Environments via\n  Anonym Auditing","summary":"The integration of blockchain technology in Internet of Things (IoT)\nenvironments is a revolutionary step towards ensuring robust security and\nenhanced privacy. This paper delves into the unique challenges and solutions\nassociated with securing blockchain-based IoT systems, with a specific focus on\nanonymous auditing to reinforce privacy and security. We propose a novel\nframework that combines the decentralized nature of blockchain with advanced\nsecurity protocols tailored for IoT contexts. Central to our approach is the\nimplementation of anonymization techniques in auditing processes, ensuring user\nprivacy while maintaining the integrity and transparency of blockchain\ntransactions. We outline the architecture of blockchain in IoT environments,\nemphasizing the workflow and specific security mechanisms employed.\nAdditionally, we introduce a security protocol that integrates\nprivacy-enhancing tools and anonymous auditing methods, including the use of\nadvanced cryptographic techniques for anonymity. This study also includes a\ncomparative analysis of our proposed framework against existing models in the\ndomain. Our work aims to provide a comprehensive blueprint for enhancing\nsecurity and privacy in blockchain-based IoT environments, paving the way for\nmore secure and private digital ecosystems.","updated":"2024-03-03T01:09:43Z","published":"2024-03-03T01:09:43Z","authors":[{"name":"Peyman Khordadpour"},{"name":"Saeed Ahmadi"}],"categories":["cs.CR"]},{"id":"http://arxiv.org/abs/2403.01229v1","title":"REWIND Dataset: Privacy-preserving Speaking Status Segmentation from\n  Multimodal Body Movement Signals in the Wild","summary":"Recognizing speaking in humans is a central task towards understanding social\ninteractions. Ideally, speaking would be detected from individual voice\nrecordings, as done previously for meeting scenarios. However, individual voice\nrecordings are hard to obtain in the wild, especially in crowded mingling\nscenarios due to cost, logistics, and privacy concerns. As an alternative,\nmachine learning models trained on video and wearable sensor data make it\npossible to recognize speech by detecting its related gestures in an\nunobtrusive, privacy-preserving way. These models themselves should ideally be\ntrained using labels obtained from the speech signal. However, existing\nmingling datasets do not contain high quality audio recordings. Instead,\nspeaking status annotations have often been inferred by human annotators from\nvideo, without validation of this approach against audio-based ground truth. In\nthis paper we revisit no-audio speaking status estimation by presenting the\nfirst publicly available multimodal dataset with high-quality individual speech\nrecordings of 33 subjects in a professional networking event. We present three\nbaselines for no-audio speaking status segmentation: a) from video, b) from\nbody acceleration (chest-worn accelerometer), c) from body pose tracks. In all\ncases we predict a 20Hz binary speaking status signal extracted from the audio,\na time resolution not available in previous datasets. In addition to providing\nthe signals and ground truth necessary to evaluate a wide range of speaking\nstatus detection methods, the availability of audio in REWIND makes it suitable\nfor cross-modality studies not feasible with previous mingling datasets.\nFinally, our flexible data consent setup creates new challenges for multimodal\nsystems under missing modalities.","updated":"2024-03-02T15:14:58Z","published":"2024-03-02T15:14:58Z","authors":[{"name":"Jose Vargas Quiros"},{"name":"Chirag Raman"},{"name":"Stephanie Tan"},{"name":"Ekin Gedik"},{"name":"Laura Cabrera-Quiros"},{"name":"Hayley Hung"}],"categories":["cs.CV","cs.AI","cs.LG","eess.SP"]},{"id":"http://arxiv.org/abs/2403.01218v1","title":"Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense\n  of Privacy","summary":"The high cost of model training makes it increasingly desirable to develop\ntechniques for unlearning. These techniques seek to remove the influence of a\ntraining example without having to retrain the model from scratch. Intuitively,\nonce a model has unlearned, an adversary that interacts with the model should\nno longer be able to tell whether the unlearned example was included in the\nmodel's training set or not. In the privacy literature, this is known as\nmembership inference. In this work, we discuss adaptations of Membership\nInference Attacks (MIAs) to the setting of unlearning (leading to their\n``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into\n``population U-MIAs'', where the same attacker is instantiated for all\nexamples, and ``per-example U-MIAs'', where a dedicated attacker is\ninstantiated for each example. We show that the latter category, wherein the\nattacker tailors its membership prediction to each example under attack, is\nsignificantly stronger. Indeed, our results show that the commonly used U-MIAs\nin the unlearning literature overestimate the privacy protection afforded by\nexisting unlearning techniques on both vision and language models. Our\ninvestigation reveals a large variance in the vulnerability of different\nexamples to per-example U-MIAs. In fact, several unlearning algorithms lead to\na reduced vulnerability for some, but not all, examples that we wish to\nunlearn, at the expense of increasing it for other examples. Notably, we find\nthat the privacy protection for the remaining training examples may worsen as a\nconsequence of unlearning. We also discuss the fundamental difficulty of\nequally protecting all examples using existing unlearning schemes, due to the\ndifferent rates at which examples are unlearned. We demonstrate that naive\nattempts at tailoring unlearning stopping criteria to different examples fail\nto alleviate these issues.","updated":"2024-03-02T14:22:40Z","published":"2024-03-02T14:22:40Z","authors":[{"name":"Jamie Hayes"},{"name":"Ilia Shumailov"},{"name":"Eleni Triantafillou"},{"name":"Amr Khalifa"},{"name":"Nicolas Papernot"}],"categories":["cs.LG","cs.CR"]},{"id":"http://arxiv.org/abs/2403.00278v1","title":"Shifted Interpolation for Differential Privacy","summary":"Noisy gradient descent and its variants are the predominant algorithms for\ndifferentially private machine learning. It is a fundamental question to\nquantify their privacy leakage, yet tight characterizations remain open even in\nthe foundational setting of convex losses. This paper improves over previous\nanalyses by establishing (and refining) the \"privacy amplification by\niteration\" phenomenon in the unifying framework of $f$-differential\nprivacy--which tightly captures all aspects of the privacy loss and immediately\nimplies tighter privacy accounting in other notions of differential privacy,\ne.g., $(\\varepsilon,\\delta)$-DP and Renyi DP. Our key technical insight is the\nconstruction of shifted interpolated processes that unravel the popular\nshifted-divergences argument, enabling generalizations beyond divergence-based\nrelaxations of DP. Notably, this leads to the first exact privacy analysis in\nthe foundational setting of strongly convex optimization. Our techniques extend\nto many settings: convex/strongly convex, constrained/unconstrained,\nfull/cyclic/stochastic batches, and all combinations thereof. As an immediate\ncorollary, we recover the $f$-DP characterization of the exponential mechanism\nfor strongly convex optimization in Gopi et al. (2022), and moreover extend\nthis result to more general settings.","updated":"2024-03-01T04:50:04Z","published":"2024-03-01T04:50:04Z","authors":[{"name":"Jinho Bok"},{"name":"Weijie Su"},{"name":"Jason M. Altschuler"}],"categories":["cs.LG","cs.CR","math.OC","math.ST","stat.ML","stat.TH"]}]