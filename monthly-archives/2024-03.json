[
  {
    "id" : "http://arxiv.org/abs/2403.01356v1",
    "title" : "Security and Privacy Enhancing in Blockchain-based IoT Environments via\n  Anonym Auditing",
    "summary" : "The integration of blockchain technology in Internet of Things (IoT)\nenvironments is a revolutionary step towards ensuring robust security and\nenhanced privacy. This paper delves into the unique challenges and solutions\nassociated with securing blockchain-based IoT systems, with a specific focus on\nanonymous auditing to reinforce privacy and security. We propose a novel\nframework that combines the decentralized nature of blockchain with advanced\nsecurity protocols tailored for IoT contexts. Central to our approach is the\nimplementation of anonymization techniques in auditing processes, ensuring user\nprivacy while maintaining the integrity and transparency of blockchain\ntransactions. We outline the architecture of blockchain in IoT environments,\nemphasizing the workflow and specific security mechanisms employed.\nAdditionally, we introduce a security protocol that integrates\nprivacy-enhancing tools and anonymous auditing methods, including the use of\nadvanced cryptographic techniques for anonymity. This study also includes a\ncomparative analysis of our proposed framework against existing models in the\ndomain. Our work aims to provide a comprehensive blueprint for enhancing\nsecurity and privacy in blockchain-based IoT environments, paving the way for\nmore secure and private digital ecosystems.",
    "updated" : "2024-03-03T01:09:43Z",
    "published" : "2024-03-03T01:09:43Z",
    "authors" : [
      {
        "name" : "Peyman Khordadpour"
      },
      {
        "name" : "Saeed Ahmadi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.01229v1",
    "title" : "REWIND Dataset: Privacy-preserving Speaking Status Segmentation from\n  Multimodal Body Movement Signals in the Wild",
    "summary" : "Recognizing speaking in humans is a central task towards understanding social\ninteractions. Ideally, speaking would be detected from individual voice\nrecordings, as done previously for meeting scenarios. However, individual voice\nrecordings are hard to obtain in the wild, especially in crowded mingling\nscenarios due to cost, logistics, and privacy concerns. As an alternative,\nmachine learning models trained on video and wearable sensor data make it\npossible to recognize speech by detecting its related gestures in an\nunobtrusive, privacy-preserving way. These models themselves should ideally be\ntrained using labels obtained from the speech signal. However, existing\nmingling datasets do not contain high quality audio recordings. Instead,\nspeaking status annotations have often been inferred by human annotators from\nvideo, without validation of this approach against audio-based ground truth. In\nthis paper we revisit no-audio speaking status estimation by presenting the\nfirst publicly available multimodal dataset with high-quality individual speech\nrecordings of 33 subjects in a professional networking event. We present three\nbaselines for no-audio speaking status segmentation: a) from video, b) from\nbody acceleration (chest-worn accelerometer), c) from body pose tracks. In all\ncases we predict a 20Hz binary speaking status signal extracted from the audio,\na time resolution not available in previous datasets. In addition to providing\nthe signals and ground truth necessary to evaluate a wide range of speaking\nstatus detection methods, the availability of audio in REWIND makes it suitable\nfor cross-modality studies not feasible with previous mingling datasets.\nFinally, our flexible data consent setup creates new challenges for multimodal\nsystems under missing modalities.",
    "updated" : "2024-03-02T15:14:58Z",
    "published" : "2024-03-02T15:14:58Z",
    "authors" : [
      {
        "name" : "Jose Vargas Quiros"
      },
      {
        "name" : "Chirag Raman"
      },
      {
        "name" : "Stephanie Tan"
      },
      {
        "name" : "Ekin Gedik"
      },
      {
        "name" : "Laura Cabrera-Quiros"
      },
      {
        "name" : "Hayley Hung"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.01218v1",
    "title" : "Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense\n  of Privacy",
    "summary" : "The high cost of model training makes it increasingly desirable to develop\ntechniques for unlearning. These techniques seek to remove the influence of a\ntraining example without having to retrain the model from scratch. Intuitively,\nonce a model has unlearned, an adversary that interacts with the model should\nno longer be able to tell whether the unlearned example was included in the\nmodel's training set or not. In the privacy literature, this is known as\nmembership inference. In this work, we discuss adaptations of Membership\nInference Attacks (MIAs) to the setting of unlearning (leading to their\n``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into\n``population U-MIAs'', where the same attacker is instantiated for all\nexamples, and ``per-example U-MIAs'', where a dedicated attacker is\ninstantiated for each example. We show that the latter category, wherein the\nattacker tailors its membership prediction to each example under attack, is\nsignificantly stronger. Indeed, our results show that the commonly used U-MIAs\nin the unlearning literature overestimate the privacy protection afforded by\nexisting unlearning techniques on both vision and language models. Our\ninvestigation reveals a large variance in the vulnerability of different\nexamples to per-example U-MIAs. In fact, several unlearning algorithms lead to\na reduced vulnerability for some, but not all, examples that we wish to\nunlearn, at the expense of increasing it for other examples. Notably, we find\nthat the privacy protection for the remaining training examples may worsen as a\nconsequence of unlearning. We also discuss the fundamental difficulty of\nequally protecting all examples using existing unlearning schemes, due to the\ndifferent rates at which examples are unlearned. We demonstrate that naive\nattempts at tailoring unlearning stopping criteria to different examples fail\nto alleviate these issues.",
    "updated" : "2024-03-02T14:22:40Z",
    "published" : "2024-03-02T14:22:40Z",
    "authors" : [
      {
        "name" : "Jamie Hayes"
      },
      {
        "name" : "Ilia Shumailov"
      },
      {
        "name" : "Eleni Triantafillou"
      },
      {
        "name" : "Amr Khalifa"
      },
      {
        "name" : "Nicolas Papernot"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.00278v1",
    "title" : "Shifted Interpolation for Differential Privacy",
    "summary" : "Noisy gradient descent and its variants are the predominant algorithms for\ndifferentially private machine learning. It is a fundamental question to\nquantify their privacy leakage, yet tight characterizations remain open even in\nthe foundational setting of convex losses. This paper improves over previous\nanalyses by establishing (and refining) the \"privacy amplification by\niteration\" phenomenon in the unifying framework of $f$-differential\nprivacy--which tightly captures all aspects of the privacy loss and immediately\nimplies tighter privacy accounting in other notions of differential privacy,\ne.g., $(\\varepsilon,\\delta)$-DP and Renyi DP. Our key technical insight is the\nconstruction of shifted interpolated processes that unravel the popular\nshifted-divergences argument, enabling generalizations beyond divergence-based\nrelaxations of DP. Notably, this leads to the first exact privacy analysis in\nthe foundational setting of strongly convex optimization. Our techniques extend\nto many settings: convex/strongly convex, constrained/unconstrained,\nfull/cyclic/stochastic batches, and all combinations thereof. As an immediate\ncorollary, we recover the $f$-DP characterization of the exponential mechanism\nfor strongly convex optimization in Gopi et al. (2022), and moreover extend\nthis result to more general settings.",
    "updated" : "2024-03-01T04:50:04Z",
    "published" : "2024-03-01T04:50:04Z",
    "authors" : [
      {
        "name" : "Jinho Bok"
      },
      {
        "name" : "Weijie Su"
      },
      {
        "name" : "Jason M. Altschuler"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "math.OC",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03126v1",
    "title" : "A Federated Deep Learning Approach for Privacy-Preserving Real-Time\n  Transient Stability Predictions in Power Systems",
    "summary" : "Maintaining the privacy of power system data is essential for protecting\nsensitive information and ensuring the operation security of critical\ninfrastructure. Therefore, the adoption of centralized deep learning (DL)\ntransient stability assessment (TSA) frameworks can introduce risks to electric\nutilities. This is because these frameworks make utility data susceptible to\ncyber threats and communication issues when transmitting data to a central\nserver for training a single TSA model. Additionally, the centralized approach\ndemands significant computational resources, which may not always be readily\navailable. In light of these challenges, this paper introduces a federated\nDL-based TSA framework designed to identify the operating states of the power\nsystem. Instead of local utilities transmitting their data to a central server\nfor centralized model training, they independently train their own TSA models\nusing their respective datasets. Subsequently, the parameters of each local TSA\nmodel are sent to a central server for model aggregation, and the resulting\nmodel is shared back with the local clients. This approach not only preserves\nthe integrity of local utility data, making it resilient against cyber threats\nbut also reduces the computational demands for local TSA model training. The\nproposed approach is tested on four local clients each having the IEEE 39-bus\ntest system.",
    "updated" : "2024-03-05T17:12:42Z",
    "published" : "2024-03-05T17:12:42Z",
    "authors" : [
      {
        "name" : "Maeshal Hijazi"
      },
      {
        "name" : "Payman Dehghanian"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03048v1",
    "title" : "Design of Stochastic Quantizers for Privacy Preservation",
    "summary" : "In this paper, we examine the role of stochastic quantizers for privacy\npreservation. We first employ a static stochastic quantizer and investigate its\ncorresponding privacy-preserving properties. Specifically, we demonstrate that\na sufficiently large quantization step guarantees $(0, \\delta)$ differential\nprivacy. Additionally, the degradation of control performance caused by\nquantization is evaluated as the tracking error of output regulation. These two\nanalyses characterize the trade-off between privacy and control performance,\ndetermined by the quantization step. This insight enables us to use\nquantization intentionally as a means to achieve the seemingly conflicting two\ngoals of maintaining control performance and preserving privacy at the same\ntime; towards this end, we further investigate a dynamic stochastic quantizer.\nUnder a stability assumption, the dynamic stochastic quantizer can enhance\nprivacy, more than the static one, while achieving the same control\nperformance. We further handle the unstable case by additionally applying input\nGaussian noise.",
    "updated" : "2024-03-05T15:31:35Z",
    "published" : "2024-03-05T15:31:35Z",
    "authors" : [
      {
        "name" : "Le Liu"
      },
      {
        "name" : "Yu Kawano"
      },
      {
        "name" : "Ming Cao"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.CR",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02694v1",
    "title" : "Privacy-Aware Semantic Cache for Large Language Models",
    "summary" : "Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2\nhave revolutionized natural language processing and search engine dynamics.\nHowever, these models incur exceptionally high computational costs. For\ninstance, GPT-3 consists of 175 billion parameters and inference on these\nmodels also demands billions of floating-point operations. Caching is a natural\nsolution to reduce LLM inference costs on repeated queries. However, existing\ncaching methods are incapable of finding semantic similarities among LLM\nqueries, leading to unacceptable false hit-and-miss rates.\n  This paper introduces MeanCache, a semantic cache for LLMs that identifies\nsemantically similar queries to determine cache hit or miss. Using MeanCache,\nthe response to a user's semantically similar query can be retrieved from a\nlocal cache rather than re-querying the LLM, thus reducing costs, service\nprovider load, and environmental impact. MeanCache leverages Federated Learning\n(FL) to collaboratively train a query similarity model in a distributed manner\nacross numerous users without violating privacy. By placing a local cache in\neach user's device and using FL, MeanCache reduces the latency and costs and\nenhances model performance, resulting in lower cache false hit rates. Our\nexperiments, benchmarked against the GPTCache, reveal that MeanCache attains an\napproximately 17% higher F-score and a 20% increase in precision during\nsemantic cache hit-and-miss decisions. Furthermore, MeanCache reduces the\nstorage requirement by 83% and accelerates semantic cache hit-and-miss\ndecisions by 11%, while still surpassing GPTCache.",
    "updated" : "2024-03-05T06:23:50Z",
    "published" : "2024-03-05T06:23:50Z",
    "authors" : [
      {
        "name" : "Waris Gill"
      },
      {
        "name" : "Mohamed Elidrisi"
      },
      {
        "name" : "Pallavi Kalapatapu"
      },
      {
        "name" : "Ali Anwar"
      },
      {
        "name" : "Muhammad Ali Gulzar"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.DC",
      "I.2.7"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02631v1",
    "title" : "Privacy in Multi-agent Systems",
    "summary" : "With the increasing awareness of privacy and the deployment of legislations\nin various multi-agent system application domains such as power systems and\nintelligent transportation, the privacy protection problem for multi-agent\nsystems is gaining increased traction in recent years. This article discusses\nsome of the representative advancements in the filed.",
    "updated" : "2024-03-05T03:40:39Z",
    "published" : "2024-03-05T03:40:39Z",
    "authors" : [
      {
        "name" : "Yongqiang Wang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02409v1",
    "title" : "Privacy-Respecting Type Error Telemetry at Scale",
    "summary" : "Context: Roblox Studio lets millions of creators build interactive\nexperiences by programming in a variant of Lua called Luau. The creators form a\nbroad group, ranging from novices writing their first script to professional\ndevelopers; thus, Luau must support a wide audience. As part of its efforts to\nsupport all kinds of programmers, Luau includes an optional, gradual type\nsystem and goes to great lengths to minimize false positive errors.\n  Inquiry: Since Luau is currently being used by many creators, we want to\ncollect data to improve the language and, in particular, the type system. The\nstandard way to collect data is to deploy client-side telemetry; however, we\ncannot scrape personal data or proprietary information, which means we cannot\ncollect source code fragments, error messages, or even filepaths. The research\nquestions are thus about how to conduct telemetry that is not invasive and\nobtain insights from it about type errors.\n  Approach: We designed and implemented a pseudonymized, randomly-sampling\ntelemetry system for Luau. Telemetry records include a timestamp, a session id,\na reason for sending, and a numeric summary of the most recent type analyses.\nThis information lets us study type errors over time without revealing private\ndata. We deployed the system in Roblox Studio during Spring 2023 and collected\nover 1.5 million telemetry records from over 340,000 sessions.\n  Knowledge: We present several findings about Luau, all of which suggest that\ntelemetry is an effective way to study type error pragmatics. One of the\nless-surprising findings is that opt-in gradual types are unpopular: there is\nan 100x gap between the number of untyped Luau sessions and the number of typed\nones. One surprise is that the strict mode for type analysis is overly\nconservative about interactions with data assets. A reassuring finding is that\ntype analysis rarely hits its internal limits on problem size.\n  Grounding: Our findings are supported by a dataset of over 1.5 million\ntelemetry records. The data and scripts for analyzing it are available in an\nartifact.\n  Importance: Beyond the immediate benefits to Luau, our findings about types\nand type errors have implications for adoption and ergonomics in other gradual\nlanguages such as TypeScript, Elixir, and Typed Racket. Our telemetry design is\nof broad interest, as it reports on type errors without revealing sensitive\ninformation.",
    "updated" : "2024-03-04T19:07:42Z",
    "published" : "2024-03-04T19:07:42Z",
    "authors" : [
      {
        "name" : "Ben Greenman"
      },
      {
        "name" : "Alan Jeffrey"
      },
      {
        "name" : "Shriram Krishnamurthi"
      },
      {
        "name" : "Mitesh Shah"
      }
    ],
    "categories" : [
      "cs.PL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02324v1",
    "title" : "Preserving Smart Grid Integrity: A Differential Privacy Framework for\n  Secure Detection of False Data Injection Attacks in the Smart Grid",
    "summary" : "In this paper, we present a framework based on differential privacy (DP) for\nquerying electric power measurements to detect system anomalies or bad data\ncaused by false data injections (FDIs). Our DP approach conceals consumption\nand system matrix data, while simultaneously enabling an untrusted third party\nto test hypotheses of anomalies, such as an FDI attack, by releasing a\nrandomized sufficient statistic for hypothesis-testing. We consider a\nmeasurement model corrupted by Gaussian noise and a sparse noise vector\nrepresenting the attack, and we observe that the optimal test statistic is a\nchi-square random variable. To detect possible attacks, we propose a novel DP\nchi-square noise mechanism that ensures the test does not reveal private\ninformation about power injections or the system matrix. The proposed framework\nprovides a robust solution for detecting FDIs while preserving the privacy of\nsensitive power system data.",
    "updated" : "2024-03-04T18:55:16Z",
    "published" : "2024-03-04T18:55:16Z",
    "authors" : [
      {
        "name" : "Nikhil Ravi"
      },
      {
        "name" : "Anna Scaglione"
      },
      {
        "name" : "Sean Peisert"
      },
      {
        "name" : "Parth Pradhan"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02292v1",
    "title" : "A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends",
    "summary" : "We present an analysis of 12 million instances of privacy-relevant reviews\npublicly visible on the Google Play Store that span a 10 year period. By\nleveraging state of the art NLP techniques, we can examine what users have been\nwriting about privacy along multiple dimensions: time, countries, app types,\ndiverse privacy topics, and even across a spectrum of emotions. We find\nconsistent growth of privacy-relevant reviews, and explore topics that are\ntrending (such as Data Deletion and Data Theft), as well as those on the\ndecline (such as privacy-relevant reviews on sensitive permissions). We find\nthat although privacy reviews come from more than 200 countries, 33 countries\nprovide 90% of privacy reviews. We conduct a comparison across countries by\nexamining the distribution of privacy topics a country's users write about, and\nfind that geographic proximity is not a reliable indicator that nearby\ncountries have similar privacy perspectives. We uncover some countries with\nunique patterns and explore those herein. Surprisingly, we uncover that it is\nnot uncommon for reviews that discuss privacy to be positive (32%); many users\nexpress pleasure about privacy features within apps or privacy-focused apps. We\nalso uncover some unexpected behaviors, such as the use of reviews to deliver\nprivacy disclaimers to developers. Finally, we demonstrate the value of\nanalyzing app reviews with our approach as a complement to existing methods for\nunderstanding users' perspectives about privacy.",
    "updated" : "2024-03-04T18:21:56Z",
    "published" : "2024-03-04T18:21:56Z",
    "authors" : [
      {
        "name" : "Omer Akgul"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Nina Taft"
      },
      {
        "name" : "Michelle L. Mazurek"
      },
      {
        "name" : "Hamza Harkous"
      },
      {
        "name" : "Animesh Srivastava"
      },
      {
        "name" : "Benoit Seguin"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02116v1",
    "title" : "Inf2Guard: An Information-Theoretic Framework for Learning\n  Privacy-Preserving Representations against Inference Attacks",
    "summary" : "Machine learning (ML) is vulnerable to inference (e.g., membership inference,\nproperty inference, and data reconstruction) attacks that aim to infer the\nprivate information of training data or dataset. Existing defenses are only\ndesigned for one specific type of attack and sacrifice significant utility or\nare soon broken by adaptive attacks. We address these limitations by proposing\nan information-theoretic defense framework, called Inf2Guard, against the three\nmajor types of inference attacks. Our framework, inspired by the success of\nrepresentation learning, posits that learning shared representations not only\nsaves time/costs but also benefits numerous downstream tasks. Generally,\nInf2Guard involves two mutual information objectives, for privacy protection\nand utility preservation, respectively. Inf2Guard exhibits many merits: it\nfacilitates the design of customized objectives against the specific inference\nattack; it provides a general defense framework which can treat certain\nexisting defenses as special cases; and importantly, it aids in deriving\ntheoretical results, e.g., inherent utility-privacy tradeoff and guaranteed\nprivacy leakage. Extensive evaluations validate the effectiveness of Inf2Guard\nfor learning privacy-preserving representations against inference attacks and\ndemonstrate the superiority over the baselines.",
    "updated" : "2024-03-04T15:20:19Z",
    "published" : "2024-03-04T15:20:19Z",
    "authors" : [
      {
        "name" : "Sayedeh Leila Noorbakhsh"
      },
      {
        "name" : "Binghui Zhang"
      },
      {
        "name" : "Yuan Hong"
      },
      {
        "name" : "Binghui Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02051v1",
    "title" : "Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations",
    "summary" : "Injecting heavy-tailed noise to the iterates of stochastic gradient descent\n(SGD) has received increasing attention over the past few years. While various\ntheoretical properties of the resulting algorithm have been analyzed mainly\nfrom learning theory and optimization perspectives, their privacy preservation\nproperties have not yet been established. Aiming to bridge this gap, we provide\ndifferential privacy (DP) guarantees for noisy SGD, when the injected noise\nfollows an $\\alpha$-stable distribution, which includes a spectrum of\nheavy-tailed distributions (with infinite variance) as well as the Gaussian\ndistribution. Considering the $(\\epsilon, \\delta)$-DP framework, we show that\nSGD with heavy-tailed perturbations achieves $(0, \\tilde{\\mathcal{O}}(1/n))$-DP\nfor a broad class of loss functions which can be non-convex, where $n$ is the\nnumber of data points. As a remarkable byproduct, contrary to prior work that\nnecessitates bounded sensitivity for the gradients or clipping the iterates,\nour theory reveals that under mild assumptions, such a projection step is not\nactually necessary. We illustrate that the heavy-tailed noising mechanism\nachieves similar DP guarantees compared to the Gaussian case, which suggests\nthat it can be a viable alternative to its light-tailed counterparts.",
    "updated" : "2024-03-04T13:53:41Z",
    "published" : "2024-03-04T13:53:41Z",
    "authors" : [
      {
        "name" : "Umut Şimşekli"
      },
      {
        "name" : "Mert Gürbüzbalaban"
      },
      {
        "name" : "Sinan Yıldırım"
      },
      {
        "name" : "Lingjiong Zhu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.01788v1",
    "title" : "K-stars LDP: A Novel Framework for (p, q)-clique Enumeration under Local\n  Differential Privacy",
    "summary" : "(p,q)-clique enumeration on a bipartite graph is critical for calculating\nclustering coefficient and detecting densest subgraph. It is necessary to carry\nout subgraph enumeration while protecting users' privacy from any potential\nattacker as the count of subgraph may contain sensitive information. Most\nrecent studies focus on the privacy protection algorithms based on edge LDP\n(Local Differential Privacy). However, these algorithms suffer a large\nestimation error due to the great amount of required noise. In this paper, we\npropose a novel idea of k-stars LDP and a novel k-stars LDP algorithm for (p,\nq)-clique enumeration with a small estimation error, where a k-stars is a\nstar-shaped graph with k nodes connecting to one node. The effectiveness of\nedge LDP relies on its capacity to obfuscate the existence of an edge between\nthe user and his one-hop neighbors. This is based on the premise that a user\nshould be aware of the existence of his one-hop neighbors. Similarly, we can\napply this premise to k-stars as well, where an edge is a specific genre of\n1-stars. Based on this fact, we first propose the k-stars neighboring list to\nenable our algorithm to obfuscate the existence of k-stars with Warner' s RR.\nThen, we propose the absolute value correction technique and the k-stars\nsampling technique to further reduce the estimation error. Finally, with the\ntwo-round user-collector interaction mechanism, we propose our k-stars LDP\nalgorithm to count the number of (p, q)-clique while successfully protecting\nusers' privacy. Both the theoretical analysis and experiments have showed the\nsuperiority of our algorithm over the algorithms based on edge LDP.",
    "updated" : "2024-03-04T07:30:10Z",
    "published" : "2024-03-04T07:30:10Z",
    "authors" : [
      {
        "name" : "Henan Sun"
      },
      {
        "name" : "Zhengyu Wu"
      },
      {
        "name" : "Rong-Hua Li"
      },
      {
        "name" : "Guoren Wang"
      },
      {
        "name" : "Zening Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.01438v1",
    "title" : "Privacy-Preserving Collaborative Split Learning Framework for Smart Grid\n  Load Forecasting",
    "summary" : "Accurate load forecasting is crucial for energy management, infrastructure\nplanning, and demand-supply balancing. Smart meter data availability has led to\nthe demand for sensor-based load forecasting. Conventional ML allows training a\nsingle global model using data from multiple smart meters requiring data\ntransfer to a central server, raising concerns for network requirements,\nprivacy, and security. We propose a split learning-based framework for load\nforecasting to alleviate this issue. We split a deep neural network model into\ntwo parts, one for each Grid Station (GS) responsible for an entire\nneighbourhood's smart meters and the other for the Service Provider (SP).\nInstead of sharing their data, client smart meters use their respective GSs'\nmodel split for forward pass and only share their activations with the GS.\nUnder this framework, each GS is responsible for training a personalized model\nsplit for their respective neighbourhoods, whereas the SP can train a single\nglobal or personalized model for each GS. Experiments show that the proposed\nmodels match or exceed a centrally trained model's performance and generalize\nwell. Privacy is analyzed by assessing information leakage between data and\nshared activations of the GS model split. Additionally, differential privacy\nenhances local data privacy while examining its impact on performance. A\ntransformer model is used as our base learner.",
    "updated" : "2024-03-03T08:24:39Z",
    "published" : "2024-03-03T08:24:39Z",
    "authors" : [
      {
        "name" : "Asif Iqbal"
      },
      {
        "name" : "Prosanta Gope"
      },
      {
        "name" : "Biplab Sikdar"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.01435v1",
    "title" : "Distributed Least-Squares Optimization Solvers with Differential Privacy",
    "summary" : "This paper studies the distributed least-squares optimization problem with\ndifferential privacy requirement of local cost functions, for which two\ndifferentially private distributed solvers are proposed. The first is\nestablished on the distributed gradient tracking algorithm, by appropriately\nperturbing the initial values and parameters that contain the privacy-sensitive\ndata with Gaussian and truncated Laplacian noises, respectively. Rigorous\nproofs are established to show the achievable trade-off between the\n({\\epsilon}, {\\delta})-differential privacy and the computation accuracy. The\nsecond solver is established on the combination of the distributed shuffling\nmechanism and the average consensus algorithm, which enables each agent to\nobtain a noisy version of parameters characterizing the global gradient. As a\nresult, the least-squares optimization problem can be eventually solved by each\nagent locally in such a way that any given ({\\epsilon}, {\\delta})-differential\nprivacy requirement can be preserved while the solution may be computed with\nthe accuracy independent of the network size, which makes the latter more\nsuitable for large-scale distributed least-squares problems. Numerical\nsimulations are presented to show the effectiveness of both solvers.",
    "updated" : "2024-03-03T08:14:50Z",
    "published" : "2024-03-03T08:14:50Z",
    "authors" : [
      {
        "name" : "Weijia Liu"
      },
      {
        "name" : "Lei Wang"
      },
      {
        "name" : "Fanghong Guo"
      },
      {
        "name" : "Zhengguang Wu"
      },
      {
        "name" : "Hongye Su"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03612v1",
    "title" : "Using the Dual-Privacy Framework to Understand Consumers' Perceived\n  Privacy Violations Under Different Firm Practices in Online Advertising",
    "summary" : "In response to privacy concerns about collecting and using personal data, the\nonline advertising industry has been developing privacy-enhancing technologies\n(PETs), e.g., under Google's Privacy Sandbox initiative. In this research, we\nuse the dual-privacy framework, which postulates that consumers have intrinsic\nand instrumental preferences for privacy, to understand consumers' perceived\nprivacy violations (PPVs) for current and proposed online advertising\npractices. The key idea is that different practices differ in whether\nindividual data leaves the consumer's machine or not and in how they track and\ntarget consumers; these affect, respectively, the intrinsic and instrumental\ncomponents of privacy preferences differently, leading to different PPVs for\ndifferent practices. We conducted online studies focused on consumers in the\nUnited States to elicit PPVs for various advertising practices. Our findings\nconfirm the intuition that tracking and targeting consumers under the industry\nstatus quo of behavioral targeting leads to high PPV. New technologies or\nproposals that ensure that data are kept on the consumer's machine lower PPV\nrelative to behavioral targeting but, importantly, this decrease is small.\nFurthermore, group-level targeting does not differ significantly from\nindividual-level targeting in reducing PPV. Under contextual targeting, where\nthere is no tracking, PPV is significantly reduced. Interestingly, with respect\nto PPV, consumers are indifferent between seeing untargeted ads and no ads when\nthey are not being tracked. We find that consumer perceptions of privacy\nviolations under different tracking and targeting practices may differ from\nwhat technical definitions suggest. Therefore, rather than relying solely on\ntechnical perspectives, a consumer-centric approach to privacy is needed, based\non, for instance, the dual-privacy framework.",
    "updated" : "2024-03-06T11:06:25Z",
    "published" : "2024-03-06T11:06:25Z",
    "authors" : [
      {
        "name" : "Kinshuk Jerath"
      },
      {
        "name" : "Klaus M. Miller"
      }
    ],
    "categories" : [
      "econ.GN",
      "q-fin.EC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03610v1",
    "title" : "Paying for Privacy: Pay-or-Tracking Walls",
    "summary" : "Prestigious news publishers, and more recently, Meta, have begun to request\nthat users pay for privacy. Specifically, users receive a notification banner,\nreferred to as a pay-or-tracking wall, that requires them to (i) pay money to\navoid being tracked or (ii) consent to being tracked. These walls have invited\nconcerns that privacy might become a luxury. However, little is known about\npay-or-tracking walls, which prevents a meaningful discussion about their\nappropriateness. This paper conducts several empirical studies and finds that\ntop EU publishers use pay-or-tracking walls. Their implementations involve\nvarious approaches, including bundling the pay option with advertising-free\naccess or additional content. The price for not being tracked exceeds the\nadvertising revenue that publishers generate from a user who consents to being\ntracked. Notably, publishers' traffic does not decline when implementing a\npay-or-tracking wall and most users consent to being tracked; only a few users\npay. In short, pay-or-tracking walls seem to provide the means for expanding\nthe practice of tracking. Publishers profit from pay-or-tracking walls and may\nobserve a revenue increase of 16.4% due to tracking more users than under a\ncookie consent banner.",
    "updated" : "2024-03-06T10:59:49Z",
    "published" : "2024-03-06T10:59:49Z",
    "authors" : [
      {
        "name" : "Timo Mueller-Tribbensee"
      },
      {
        "name" : "Klaus M. Miller"
      },
      {
        "name" : "Bernd Skiera"
      }
    ],
    "categories" : [
      "econ.GN",
      "q-fin.EC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03600v1",
    "title" : "A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain\n  Recommendation",
    "summary" : "Cross-domain recommendation (CDR) aims to enhance recommendation accuracy in\na target domain with sparse data by leveraging rich information in a source\ndomain, thereby addressing the data-sparsity problem. Some existing CDR methods\nhighlight the advantages of extracting domain-common and domain-specific\nfeatures to learn comprehensive user and item representations. However, these\nmethods can't effectively disentangle these components as they often rely on\nsimple user-item historical interaction information (such as ratings, clicks,\nand browsing), neglecting the rich multi-modal features. Additionally, they\ndon't protect user-sensitive data from potential leakage during knowledge\ntransfer between domains. To address these challenges, we propose a\nPrivacy-Preserving Framework with Multi-Modal Data for Cross-Domain\nRecommendation, called P2M2-CDR. Specifically, we first design a multi-modal\ndisentangled encoder that utilizes multi-modal information to disentangle more\ninformative domain-common and domain-specific embeddings. Furthermore, we\nintroduce a privacy-preserving decoder to mitigate user privacy leakage during\nknowledge transfer. Local differential privacy (LDP) is utilized to obfuscate\nthe disentangled embeddings before inter-domain exchange, thereby enhancing\nprivacy protection. To ensure both consistency and differentiation among these\nobfuscated disentangled embeddings, we incorporate contrastive learning-based\ndomain-inter and domain-intra losses. Extensive Experiments conducted on four\nreal-world datasets demonstrate that P2M2-CDR outperforms other\nstate-of-the-art single-domain and cross-domain baselines.",
    "updated" : "2024-03-06T10:40:08Z",
    "published" : "2024-03-06T10:40:08Z",
    "authors" : [
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Lei Sang"
      },
      {
        "name" : "Quangui Zhang"
      },
      {
        "name" : "Qiang Wu"
      },
      {
        "name" : "Min Xu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03592v1",
    "title" : "Wildest Dreams: Reproducible Research in Privacy-preserving Neural\n  Network Training",
    "summary" : "Machine Learning (ML), addresses a multitude of complex issues in multiple\ndisciplines, including social sciences, finance, and medical research. ML\nmodels require substantial computing power and are only as powerful as the data\nutilized. Due to high computational cost of ML methods, data scientists\nfrequently use Machine Learning-as-a-Service (MLaaS) to outsource computation\nto external servers. However, when working with private information, like\nfinancial data or health records, outsourcing the computation might result in\nprivacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have\nenabled ML training and inference over protected data through the use of\nPrivacy-Preserving Machine Learning (PPML). However, these techniques are still\nat a preliminary stage and their application in real-world situations is\ndemanding. In order to comprehend discrepancy between theoretical research\nsuggestions and actual applications, this work examines the past and present of\nPPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party\nComputation (SMPC) applied to ML. This work primarily focuses on the ML model's\ntraining phase, where maintaining user data privacy is of utmost importance. We\nprovide a solid theoretical background that eases the understanding of current\napproaches and their limitations. In addition, we present a SoK of the most\nrecent PPML frameworks for model training and provide a comprehensive\ncomparison in terms of the unique properties and performances on standard\nbenchmarks. Also, we reproduce the results for some of the papers and examine\nat what level existing works in the field provide support for open science. We\nbelieve our work serves as a valuable contribution by raising awareness about\nthe current gap between theoretical advancements and real-world applications in\nPPML, specifically regarding open-source availability, reproducibility, and\nusability.",
    "updated" : "2024-03-06T10:25:36Z",
    "published" : "2024-03-06T10:25:36Z",
    "authors" : [
      {
        "name" : "Tanveer Khan"
      },
      {
        "name" : "Mindaugas Budzys"
      },
      {
        "name" : "Khoa Nguyen"
      },
      {
        "name" : "Antonis Michalas"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03337v1",
    "title" : "Fine-Grained Privacy Guarantees for Coverage Problems",
    "summary" : "We introduce a new notion of neighboring databases for coverage problems such\nas Max Cover and Set Cover under differential privacy. In contrast to the\nstandard privacy notion for these problems, which is analogous to node-privacy\nin graphs, our new definition gives a more fine-grained privacy guarantee,\nwhich is analogous to edge-privacy. We illustrate several scenarios of Set\nCover and Max Cover where our privacy notion is desired one for the\napplication.\n  Our main result is an $\\epsilon$-edge differentially private algorithm for\nMax Cover which obtains an $(1-1/e-\\eta,\\tilde{O}(k/\\epsilon))$-approximation\nwith high probability. Furthermore, we show that this result is nearly tight:\nwe give a lower bound show that an additive error of $\\Omega(k/\\epsilon)$ is\nnecessary under edge-differential privacy. Via group privacy properties, this\nimplies a new algorithm for $\\epsilon$-node differentially private Max Cover\nwhich obtains an $(1-1/e-\\eta,\\tilde{O}(fk/\\epsilon))$-approximation, where $f$\nis the maximum degree of an element in the set system. When $f\\ll k$, this\nimproves over the best known algorithm for Max Cover under pure (node)\ndifferential privacy, which obtains an\n$(1-1/e,\\tilde{O}(k^2/\\epsilon))$-approximation.",
    "updated" : "2024-03-05T21:40:10Z",
    "published" : "2024-03-05T21:40:10Z",
    "authors" : [
      {
        "name" : "Laxman Dhulipala"
      },
      {
        "name" : "George Z. Li"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04485v1",
    "title" : "Privacy in Cloud Computing through Immersion-based Coding",
    "summary" : "Cloud computing enables users to process and store data remotely on\nhigh-performance computers and servers by sharing data over the Internet.\nHowever, transferring data to clouds causes unavoidable privacy concerns. Here,\nwe present a synthesis framework to design coding mechanisms that allow sharing\nand processing data in a privacy-preserving manner without sacrificing data\nutility and algorithmic performance. We consider the setup where the user aims\nto run an algorithm in the cloud using private data. The cloud then returns\nsome data utility back to the user (utility refers to the service that the\nalgorithm provides, e.g., classification, prediction, AI models, etc.). To\navoid privacy concerns, the proposed scheme provides tools to co-design: 1)\ncoding mechanisms to distort the original data and guarantee a prescribed\ndifferential privacy level; 2) an equivalent-but-different algorithm (referred\nhere to as the target algorithm) that runs on distorted data and produces\ndistorted utility; and 3) a decoding function that extracts the true utility\nfrom the distorted one with a negligible error. Then, instead of sharing the\noriginal data and algorithm with the cloud, only the distorted data and target\nalgorithm are disclosed, thereby avoiding privacy concerns. The proposed scheme\nis built on the synergy of differential privacy and system immersion tools from\ncontrol theory. The key underlying idea is to design a higher-dimensional\ntarget algorithm that embeds all trajectories of the original algorithm and\nworks on randomly encoded data to produce randomly encoded utility. We show\nthat the proposed scheme can be designed to offer any level of differential\nprivacy without degrading the algorithm's utility. We present two use cases to\nillustrate the performance of the developed tools: privacy in\noptimization/learning algorithms and a nonlinear networked control system.",
    "updated" : "2024-03-07T13:38:18Z",
    "published" : "2024-03-07T13:38:18Z",
    "authors" : [
      {
        "name" : "Haleh Hayati"
      },
      {
        "name" : "Nathan van de Wouw"
      },
      {
        "name" : "Carlos Murguia"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04468v1",
    "title" : "A Survey of Graph Neural Networks in Real world: Imbalance, Noise,\n  Privacy and OOD Challenges",
    "summary" : "Graph-structured data exhibits universality and widespread applicability\nacross diverse domains, such as social network analysis, biochemistry,\nfinancial fraud detection, and network security. Significant strides have been\nmade in leveraging Graph Neural Networks (GNNs) to achieve remarkable success\nin these areas. However, in real-world scenarios, the training environment for\nmodels is often far from ideal, leading to substantial performance degradation\nof GNN models due to various unfavorable factors, including imbalance in data\ndistribution, the presence of noise in erroneous data, privacy protection of\nsensitive information, and generalization capability for out-of-distribution\n(OOD) scenarios. To tackle these issues, substantial efforts have been devoted\nto improving the performance of GNN models in practical real-world scenarios,\nas well as enhancing their reliability and robustness. In this paper, we\npresent a comprehensive survey that systematically reviews existing GNN models,\nfocusing on solutions to the four mentioned real-world challenges including\nimbalance, noise, privacy, and OOD in practical scenarios that many existing\nreviews have not considered. Specifically, we first highlight the four key\nchallenges faced by existing GNNs, paving the way for our exploration of\nreal-world GNN models. Subsequently, we provide detailed discussions on these\nfour aspects, dissecting how these solutions contribute to enhancing the\nreliability and robustness of GNN models. Last but not least, we outline\npromising directions and offer future perspectives in the field.",
    "updated" : "2024-03-07T13:10:37Z",
    "published" : "2024-03-07T13:10:37Z",
    "authors" : [
      {
        "name" : "Wei Ju"
      },
      {
        "name" : "Siyu Yi"
      },
      {
        "name" : "Yifan Wang"
      },
      {
        "name" : "Zhiping Xiao"
      },
      {
        "name" : "Zhengyang Mao"
      },
      {
        "name" : "Hourun Li"
      },
      {
        "name" : "Yiyang Gu"
      },
      {
        "name" : "Yifang Qin"
      },
      {
        "name" : "Nan Yin"
      },
      {
        "name" : "Senzhang Wang"
      },
      {
        "name" : "Xinwang Liu"
      },
      {
        "name" : "Xiao Luo"
      },
      {
        "name" : "Philip S. Yu"
      },
      {
        "name" : "Ming Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04451v1",
    "title" : "Membership Inference Attacks and Privacy in Topic Modeling",
    "summary" : "Recent research shows that large language models are susceptible to privacy\nattacks that infer aspects of the training data. However, it is unclear if\nsimpler generative models, like topic models, share similar vulnerabilities. In\nthis work, we propose an attack against topic models that can confidently\nidentify members of the training data in Latent Dirichlet Allocation. Our\nresults suggest that the privacy risks associated with generative modeling are\nnot restricted to large neural models. Additionally, to mitigate these\nvulnerabilities, we explore differentially private (DP) topic modeling. We\npropose a framework for private topic modeling that incorporates DP vocabulary\nselection as a pre-processing step, and show that it improves privacy while\nhaving limited effects on practical utility.",
    "updated" : "2024-03-07T12:43:42Z",
    "published" : "2024-03-07T12:43:42Z",
    "authors" : [
      {
        "name" : "Nico Manzonelli"
      },
      {
        "name" : "Wanrong Zhang"
      },
      {
        "name" : "Salil Vadhan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04124v1",
    "title" : "Privacy-preserving Fine-tuning of Large Language Models through Flatness",
    "summary" : "The privacy concerns associated with the use of Large Language Models (LLMs)\nhave grown recently with the development of LLMs such as ChatGPT. Differential\nPrivacy (DP) techniques are explored in existing work to mitigate their privacy\nrisks at the cost of generalization degradation. Our paper reveals that the\nflatness of DP-trained models' loss landscape plays an essential role in the\ntrade-off between their privacy and generalization. We further propose a\nholistic framework to enforce appropriate weight flatness, which substantially\nimproves model generalization with competitive privacy preservation. It\ninnovates from three coarse-to-grained levels, including perturbation-aware\nmin-max optimization on model weights within a layer, flatness-guided sparse\nprefix-tuning on weights across layers, and weight knowledge distillation\nbetween DP \\& non-DP weights copies. Comprehensive experiments of both\nblack-box and white-box scenarios are conducted to demonstrate the\neffectiveness of our proposal in enhancing generalization and maintaining DP\ncharacteristics. For instance, on text classification dataset QNLI, DP-Flat\nachieves similar performance with non-private full fine-tuning but with DP\nguarantee under privacy budget $\\epsilon=3$, and even better performance given\nhigher privacy budgets. Codes are provided in the supplement.",
    "updated" : "2024-03-07T00:44:11Z",
    "published" : "2024-03-07T00:44:11Z",
    "authors" : [
      {
        "name" : "Tiejin Chen"
      },
      {
        "name" : "Longchao Da"
      },
      {
        "name" : "Huixue Zhou"
      },
      {
        "name" : "Pingzhi Li"
      },
      {
        "name" : "Kaixiong Zhou"
      },
      {
        "name" : "Tianlong Chen"
      },
      {
        "name" : "Hua Wei"
      }
    ],
    "categories" : [
      "cs.AI",
      "I.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04024v1",
    "title" : "Enhancing chest X-ray datasets with privacy-preserving large language\n  models and multi-type annotations: a data-driven approach for improved\n  classification",
    "summary" : "In chest X-ray (CXR) image analysis, rule-based systems are usually employed\nto extract labels from reports, but concerns exist about label quality. These\ndatasets typically offer only presence labels, sometimes with binary\nuncertainty indicators, which limits their usefulness. In this work, we present\nMAPLEZ (Medical report Annotations with Privacy-preserving Large language model\nusing Expeditious Zero shot answers), a novel approach leveraging a locally\nexecutable Large Language Model (LLM) to extract and enhance findings labels on\nCXR reports. MAPLEZ extracts not only binary labels indicating the presence or\nabsence of a finding but also the location, severity, and radiologists'\nuncertainty about the finding. Over eight abnormalities from five test sets, we\nshow that our method can extract these annotations with an increase of 5\npercentage points (pp) in F1 score for categorical presence annotations and\nmore than 30 pp increase in F1 score for the location annotations over\ncompeting labelers. Additionally, using these improved annotations in\nclassification supervision, we demonstrate substantial advancements in model\nquality, with an increase of 1.7 pp in AUROC over models trained with\nannotations from the state-of-the-art approach. We share code and annotations.",
    "updated" : "2024-03-06T20:10:41Z",
    "published" : "2024-03-06T20:10:41Z",
    "authors" : [
      {
        "name" : "Ricardo Bigolin Lanfredi"
      },
      {
        "name" : "Pritam Mukherjee"
      },
      {
        "name" : "Ronald Summers"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05344v1",
    "title" : "Federated Learning Method for Preserving Privacy in Face Recognition\n  System",
    "summary" : "The state-of-the-art face recognition systems are typically trained on a\nsingle computer, utilizing extensive image datasets collected from various\nnumber of users. However, these datasets often contain sensitive personal\ninformation that users may hesitate to disclose. To address potential privacy\nconcerns, we explore the application of federated learning, both with and\nwithout secure aggregators, in the context of both supervised and unsupervised\nface recognition systems. Federated learning facilitates the training of a\nshared model without necessitating the sharing of individual private data,\nachieving this by training models on decentralized edge devices housing the\ndata. In our proposed system, each edge device independently trains its own\nmodel, which is subsequently transmitted either to a secure aggregator or\ndirectly to the central server. To introduce diverse data without the need for\ndata transmission, we employ generative adversarial networks to generate\nimposter data at the edge. Following this, the secure aggregator or central\nserver combines these individual models to construct a global model, which is\nthen relayed back to the edge devices. Experimental findings based on the\nCelebA datasets reveal that employing federated learning in both supervised and\nunsupervised face recognition systems offers dual benefits. Firstly, it\nsafeguards privacy since the original data remains on the edge devices.\nSecondly, the experimental results demonstrate that the aggregated model yields\nnearly identical performance compared to the individual models, particularly\nwhen the federated model does not utilize a secure aggregator. Hence, our\nresults shed light on the practical challenges associated with\nprivacy-preserving face image training, particularly in terms of the balance\nbetween privacy and accuracy.",
    "updated" : "2024-03-08T14:21:43Z",
    "published" : "2024-03-08T14:21:43Z",
    "authors" : [
      {
        "name" : "Enoch Solomon"
      },
      {
        "name" : "Abraham Woubie"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05275v1",
    "title" : "vSPACE: Voting in a Scalable, Privacy-Aware and Confidential Election",
    "summary" : "The vSPACE experimental proof-of-concept (PoC) on the TrueElect[Anon][Creds]\nprotocol presents a novel approach to secure, private, and scalable elections,\nextending the TrueElect and ElectAnon protocols with the integration of\nAnonCreds SSI (Self-Sovereign Identity). Such a protocol PoC is situated within\na Zero-Trust Architecture (ZTA) and leverages confidential computing,\ncontinuous authentication, multi-party computation (MPC), and well-architected\nframework (WAF) principles to address the challenges of cybersecurity, privacy,\nand trust over IP (ToIP) protection. Employing a Kubernetes confidential\ncluster within an Enterprise-Scale Landing Zone (ESLZ), vSPACE integrates\nDistributed Ledger Technology (DLT) for immutable and certifiable audit trails.\nThe Infrastructure as Code (IaC) model ensures rapid deployment, consistent\nmanagement, and adherence to security standards, making vSPACE a future-proof\nsolution for digital voting systems.",
    "updated" : "2024-03-08T12:56:10Z",
    "published" : "2024-03-08T12:56:10Z",
    "authors" : [
      {
        "name" : "Se Elnour"
      },
      {
        "name" : "William J Buchanan"
      },
      {
        "name" : "Paul Keating"
      },
      {
        "name" : "Mwrwan Abubakar"
      },
      {
        "name" : "Sirag Elnour"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05156v1",
    "title" : "On Protecting the Data Privacy of Large Language Models (LLMs): A Survey",
    "summary" : "Large language models (LLMs) are complex artificial intelligence systems\ncapable of understanding, generating and translating human language. They learn\nlanguage patterns by analyzing large amounts of text data, allowing them to\nperform writing, conversation, summarizing and other language tasks. When LLMs\nprocess and generate large amounts of data, there is a risk of leaking\nsensitive information, which may threaten data privacy. This paper concentrates\non elucidating the data privacy concerns associated with LLMs to foster a\ncomprehensive understanding. Specifically, a thorough investigation is\nundertaken to delineate the spectrum of data privacy threats, encompassing both\npassive privacy leakage and active privacy attacks within LLMs. Subsequently,\nwe conduct an assessment of the privacy protection mechanisms employed by LLMs\nat various stages, followed by a detailed examination of their efficacy and\nconstraints. Finally, the discourse extends to delineate the challenges\nencountered and outline prospective directions for advancement in the realm of\nLLM privacy protection.",
    "updated" : "2024-03-08T08:47:48Z",
    "published" : "2024-03-08T08:47:48Z",
    "authors" : [
      {
        "name" : "Biwei Yan"
      },
      {
        "name" : "Kun Li"
      },
      {
        "name" : "Minghui Xu"
      },
      {
        "name" : "Yueyan Dong"
      },
      {
        "name" : "Yue Zhang"
      },
      {
        "name" : "Zhaochun Ren"
      },
      {
        "name" : "Xiuzheng Cheng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05114v1",
    "title" : "APPLE: Adversarial Privacy-aware Perturbations on Latent Embedding for\n  Unfairness Mitigation",
    "summary" : "Ensuring fairness in deep-learning-based segmentors is crucial for health\nequity. Much effort has been dedicated to mitigating unfairness in the training\ndatasets or procedures. However, with the increasing prevalence of foundation\nmodels in medical image analysis, it is hard to train fair models from scratch\nwhile preserving utility. In this paper, we propose a novel method, Adversarial\nPrivacy-aware Perturbations on Latent Embedding (APPLE), that can improve the\nfairness of deployed segmentors by introducing a small latent feature perturber\nwithout updating the weights of the original model. By adding perturbation to\nthe latent vector, APPLE decorates the latent vector of segmentors such that no\nfairness-related features can be passed to the decoder of the segmentors while\npreserving the architecture and parameters of the segmentor. Experiments on two\nsegmentation datasets and five segmentors (three U-Net-like and two SAM-like)\nillustrate the effectiveness of our proposed method compared to several\nunfairness mitigation methods.",
    "updated" : "2024-03-08T07:22:48Z",
    "published" : "2024-03-08T07:22:48Z",
    "authors" : [
      {
        "name" : "Zikang Xu"
      },
      {
        "name" : "Fenghe Tang"
      },
      {
        "name" : "Quan Quan"
      },
      {
        "name" : "Qingsong Yao"
      },
      {
        "name" : "S. Kevin Zhou"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04867v1",
    "title" : "Group Privacy Amplification and Unified Amplification by Subsampling for\n  Rényi Differential Privacy",
    "summary" : "Differential privacy (DP) has various desirable properties, such as\nrobustness to post-processing, group privacy, and amplification by subsampling,\nwhich can be derived independently of each other. Our goal is to determine\nwhether stronger privacy guarantees can be obtained by considering multiple of\nthese properties jointly. To this end, we focus on the combination of group\nprivacy and amplification by subsampling. To provide guarantees that are\namenable to machine learning algorithms, we conduct our analysis in the\nframework of R\\'enyi-DP, which has more favorable composition properties than\n$(\\epsilon,\\delta)$-DP. As part of this analysis, we develop a unified\nframework for deriving amplification by subsampling guarantees for R\\'enyi-DP,\nwhich represents the first such framework for a privacy accounting method and\nis of independent interest. We find that it not only lets us improve upon and\ngeneralize existing amplification results for R\\'enyi-DP, but also derive\nprovably tight group privacy amplification guarantees stronger than existing\nprinciples. These results establish the joint study of different DP properties\nas a promising research direction.",
    "updated" : "2024-03-07T19:36:05Z",
    "published" : "2024-03-07T19:36:05Z",
    "authors" : [
      {
        "name" : "Jan Schuchardt"
      },
      {
        "name" : "Mihail Stoian"
      },
      {
        "name" : "Arthur Kosmala"
      },
      {
        "name" : "Stephan Günnemann"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02292v2",
    "title" : "A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends",
    "summary" : "We present an analysis of 12 million instances of privacy-relevant reviews\npublicly visible on the Google Play Store that span a 10 year period. By\nleveraging state of the art NLP techniques, we examine what users have been\nwriting about privacy along multiple dimensions: time, countries, app types,\ndiverse privacy topics, and even across a spectrum of emotions. We find\nconsistent growth of privacy-relevant reviews, and explore topics that are\ntrending (such as Data Deletion and Data Theft), as well as those on the\ndecline (such as privacy-relevant reviews on sensitive permissions). We find\nthat although privacy reviews come from more than 200 countries, 33 countries\nprovide 90% of privacy reviews. We conduct a comparison across countries by\nexamining the distribution of privacy topics a country's users write about, and\nfind that geographic proximity is not a reliable indicator that nearby\ncountries have similar privacy perspectives. We uncover some countries with\nunique patterns and explore those herein. Surprisingly, we uncover that it is\nnot uncommon for reviews that discuss privacy to be positive (32%); many users\nexpress pleasure about privacy features within apps or privacy-focused apps. We\nalso uncover some unexpected behaviors, such as the use of reviews to deliver\nprivacy disclaimers to developers. Finally, we demonstrate the value of\nanalyzing app reviews with our approach as a complement to existing methods for\nunderstanding users' perspectives about privacy",
    "updated" : "2024-03-07T22:07:00Z",
    "published" : "2024-03-04T18:21:56Z",
    "authors" : [
      {
        "name" : "Omer Akgul"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Nina Taft"
      },
      {
        "name" : "Michelle L. Mazurek"
      },
      {
        "name" : "Hamza Harkous"
      },
      {
        "name" : "Animesh Srivastava"
      },
      {
        "name" : "Benoit Seguin"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04784v1",
    "title" : "Analysis of Privacy Leakage in Federated Large Language Models",
    "summary" : "With the rapid adoption of Federated Learning (FL) as the training and tuning\nprotocol for applications utilizing Large Language Models (LLMs), recent\nresearch highlights the need for significant modifications to FL to accommodate\nthe large-scale of LLMs. While substantial adjustments to the protocol have\nbeen introduced as a response, comprehensive privacy analysis for the adapted\nFL protocol is currently lacking.\n  To address this gap, our work delves into an extensive examination of the\nprivacy analysis of FL when used for training LLMs, both from theoretical and\npractical perspectives. In particular, we design two active membership\ninference attacks with guaranteed theoretical success rates to assess the\nprivacy leakages of various adapted FL configurations. Our theoretical findings\nare translated into practical attacks, revealing substantial privacy\nvulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and\nOpenAI's GPTs, across multiple real-world language datasets. Additionally, we\nconduct thorough experiments to evaluate the privacy leakage of these models\nwhen data is protected by state-of-the-art differential privacy (DP)\nmechanisms.",
    "updated" : "2024-03-02T20:25:38Z",
    "published" : "2024-03-02T20:25:38Z",
    "authors" : [
      {
        "name" : "Minh N. Vu"
      },
      {
        "name" : "Truc Nguyen"
      },
      {
        "name" : "Tre' R. Jeter"
      },
      {
        "name" : "My T. Thai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04778v1",
    "title" : "An Efficient Difference-of-Convex Solver for Privacy Funnel",
    "summary" : "We propose an efficient solver for the privacy funnel (PF) method, leveraging\nits difference-of-convex (DC) structure. The proposed DC separation results in\na closed-form update equation, which allows straightforward application to both\nknown and unknown distribution settings. For known distribution case, we prove\nthe convergence (local stationary points) of the proposed non-greedy solver,\nand empirically show that it outperforms the state-of-the-art approaches in\ncharacterizing the privacy-utility trade-off. The insights of our DC approach\napply to unknown distribution settings where labeled empirical samples are\navailable instead. Leveraging the insights, our alternating minimization solver\nsatisfies the fundamental Markov relation of PF in contrast to previous\nvariational inference-based solvers. Empirically, we evaluate the proposed\nsolver with MNIST and Fashion-MNIST datasets. Our results show that under a\ncomparable reconstruction quality, an adversary suffers from higher prediction\nerror from clustering our compressed codes than that with the compared methods.\nMost importantly, our solver is independent to private information in inference\nphase contrary to the baselines.",
    "updated" : "2024-03-02T01:05:25Z",
    "published" : "2024-03-02T01:05:25Z",
    "authors" : [
      {
        "name" : "Teng-Hui Huang"
      },
      {
        "name" : "Hesham El Gamal"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  }
]