[
  {
    "id" : "http://arxiv.org/abs/2403.01356v1",
    "title" : "Security and Privacy Enhancing in Blockchain-based IoT Environments via\n  Anonym Auditing",
    "summary" : "The integration of blockchain technology in Internet of Things (IoT)\nenvironments is a revolutionary step towards ensuring robust security and\nenhanced privacy. This paper delves into the unique challenges and solutions\nassociated with securing blockchain-based IoT systems, with a specific focus on\nanonymous auditing to reinforce privacy and security. We propose a novel\nframework that combines the decentralized nature of blockchain with advanced\nsecurity protocols tailored for IoT contexts. Central to our approach is the\nimplementation of anonymization techniques in auditing processes, ensuring user\nprivacy while maintaining the integrity and transparency of blockchain\ntransactions. We outline the architecture of blockchain in IoT environments,\nemphasizing the workflow and specific security mechanisms employed.\nAdditionally, we introduce a security protocol that integrates\nprivacy-enhancing tools and anonymous auditing methods, including the use of\nadvanced cryptographic techniques for anonymity. This study also includes a\ncomparative analysis of our proposed framework against existing models in the\ndomain. Our work aims to provide a comprehensive blueprint for enhancing\nsecurity and privacy in blockchain-based IoT environments, paving the way for\nmore secure and private digital ecosystems.",
    "updated" : "2024-03-03T01:09:43Z",
    "published" : "2024-03-03T01:09:43Z",
    "authors" : [
      {
        "name" : "Peyman Khordadpour"
      },
      {
        "name" : "Saeed Ahmadi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.01229v1",
    "title" : "REWIND Dataset: Privacy-preserving Speaking Status Segmentation from\n  Multimodal Body Movement Signals in the Wild",
    "summary" : "Recognizing speaking in humans is a central task towards understanding social\ninteractions. Ideally, speaking would be detected from individual voice\nrecordings, as done previously for meeting scenarios. However, individual voice\nrecordings are hard to obtain in the wild, especially in crowded mingling\nscenarios due to cost, logistics, and privacy concerns. As an alternative,\nmachine learning models trained on video and wearable sensor data make it\npossible to recognize speech by detecting its related gestures in an\nunobtrusive, privacy-preserving way. These models themselves should ideally be\ntrained using labels obtained from the speech signal. However, existing\nmingling datasets do not contain high quality audio recordings. Instead,\nspeaking status annotations have often been inferred by human annotators from\nvideo, without validation of this approach against audio-based ground truth. In\nthis paper we revisit no-audio speaking status estimation by presenting the\nfirst publicly available multimodal dataset with high-quality individual speech\nrecordings of 33 subjects in a professional networking event. We present three\nbaselines for no-audio speaking status segmentation: a) from video, b) from\nbody acceleration (chest-worn accelerometer), c) from body pose tracks. In all\ncases we predict a 20Hz binary speaking status signal extracted from the audio,\na time resolution not available in previous datasets. In addition to providing\nthe signals and ground truth necessary to evaluate a wide range of speaking\nstatus detection methods, the availability of audio in REWIND makes it suitable\nfor cross-modality studies not feasible with previous mingling datasets.\nFinally, our flexible data consent setup creates new challenges for multimodal\nsystems under missing modalities.",
    "updated" : "2024-03-02T15:14:58Z",
    "published" : "2024-03-02T15:14:58Z",
    "authors" : [
      {
        "name" : "Jose Vargas Quiros"
      },
      {
        "name" : "Chirag Raman"
      },
      {
        "name" : "Stephanie Tan"
      },
      {
        "name" : "Ekin Gedik"
      },
      {
        "name" : "Laura Cabrera-Quiros"
      },
      {
        "name" : "Hayley Hung"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.01218v1",
    "title" : "Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense\n  of Privacy",
    "summary" : "The high cost of model training makes it increasingly desirable to develop\ntechniques for unlearning. These techniques seek to remove the influence of a\ntraining example without having to retrain the model from scratch. Intuitively,\nonce a model has unlearned, an adversary that interacts with the model should\nno longer be able to tell whether the unlearned example was included in the\nmodel's training set or not. In the privacy literature, this is known as\nmembership inference. In this work, we discuss adaptations of Membership\nInference Attacks (MIAs) to the setting of unlearning (leading to their\n``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into\n``population U-MIAs'', where the same attacker is instantiated for all\nexamples, and ``per-example U-MIAs'', where a dedicated attacker is\ninstantiated for each example. We show that the latter category, wherein the\nattacker tailors its membership prediction to each example under attack, is\nsignificantly stronger. Indeed, our results show that the commonly used U-MIAs\nin the unlearning literature overestimate the privacy protection afforded by\nexisting unlearning techniques on both vision and language models. Our\ninvestigation reveals a large variance in the vulnerability of different\nexamples to per-example U-MIAs. In fact, several unlearning algorithms lead to\na reduced vulnerability for some, but not all, examples that we wish to\nunlearn, at the expense of increasing it for other examples. Notably, we find\nthat the privacy protection for the remaining training examples may worsen as a\nconsequence of unlearning. We also discuss the fundamental difficulty of\nequally protecting all examples using existing unlearning schemes, due to the\ndifferent rates at which examples are unlearned. We demonstrate that naive\nattempts at tailoring unlearning stopping criteria to different examples fail\nto alleviate these issues.",
    "updated" : "2024-03-02T14:22:40Z",
    "published" : "2024-03-02T14:22:40Z",
    "authors" : [
      {
        "name" : "Jamie Hayes"
      },
      {
        "name" : "Ilia Shumailov"
      },
      {
        "name" : "Eleni Triantafillou"
      },
      {
        "name" : "Amr Khalifa"
      },
      {
        "name" : "Nicolas Papernot"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.00278v1",
    "title" : "Shifted Interpolation for Differential Privacy",
    "summary" : "Noisy gradient descent and its variants are the predominant algorithms for\ndifferentially private machine learning. It is a fundamental question to\nquantify their privacy leakage, yet tight characterizations remain open even in\nthe foundational setting of convex losses. This paper improves over previous\nanalyses by establishing (and refining) the \"privacy amplification by\niteration\" phenomenon in the unifying framework of $f$-differential\nprivacy--which tightly captures all aspects of the privacy loss and immediately\nimplies tighter privacy accounting in other notions of differential privacy,\ne.g., $(\\varepsilon,\\delta)$-DP and Renyi DP. Our key technical insight is the\nconstruction of shifted interpolated processes that unravel the popular\nshifted-divergences argument, enabling generalizations beyond divergence-based\nrelaxations of DP. Notably, this leads to the first exact privacy analysis in\nthe foundational setting of strongly convex optimization. Our techniques extend\nto many settings: convex/strongly convex, constrained/unconstrained,\nfull/cyclic/stochastic batches, and all combinations thereof. As an immediate\ncorollary, we recover the $f$-DP characterization of the exponential mechanism\nfor strongly convex optimization in Gopi et al. (2022), and moreover extend\nthis result to more general settings.",
    "updated" : "2024-03-01T04:50:04Z",
    "published" : "2024-03-01T04:50:04Z",
    "authors" : [
      {
        "name" : "Jinho Bok"
      },
      {
        "name" : "Weijie Su"
      },
      {
        "name" : "Jason M. Altschuler"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "math.OC",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03126v1",
    "title" : "A Federated Deep Learning Approach for Privacy-Preserving Real-Time\n  Transient Stability Predictions in Power Systems",
    "summary" : "Maintaining the privacy of power system data is essential for protecting\nsensitive information and ensuring the operation security of critical\ninfrastructure. Therefore, the adoption of centralized deep learning (DL)\ntransient stability assessment (TSA) frameworks can introduce risks to electric\nutilities. This is because these frameworks make utility data susceptible to\ncyber threats and communication issues when transmitting data to a central\nserver for training a single TSA model. Additionally, the centralized approach\ndemands significant computational resources, which may not always be readily\navailable. In light of these challenges, this paper introduces a federated\nDL-based TSA framework designed to identify the operating states of the power\nsystem. Instead of local utilities transmitting their data to a central server\nfor centralized model training, they independently train their own TSA models\nusing their respective datasets. Subsequently, the parameters of each local TSA\nmodel are sent to a central server for model aggregation, and the resulting\nmodel is shared back with the local clients. This approach not only preserves\nthe integrity of local utility data, making it resilient against cyber threats\nbut also reduces the computational demands for local TSA model training. The\nproposed approach is tested on four local clients each having the IEEE 39-bus\ntest system.",
    "updated" : "2024-03-05T17:12:42Z",
    "published" : "2024-03-05T17:12:42Z",
    "authors" : [
      {
        "name" : "Maeshal Hijazi"
      },
      {
        "name" : "Payman Dehghanian"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03048v1",
    "title" : "Design of Stochastic Quantizers for Privacy Preservation",
    "summary" : "In this paper, we examine the role of stochastic quantizers for privacy\npreservation. We first employ a static stochastic quantizer and investigate its\ncorresponding privacy-preserving properties. Specifically, we demonstrate that\na sufficiently large quantization step guarantees $(0, \\delta)$ differential\nprivacy. Additionally, the degradation of control performance caused by\nquantization is evaluated as the tracking error of output regulation. These two\nanalyses characterize the trade-off between privacy and control performance,\ndetermined by the quantization step. This insight enables us to use\nquantization intentionally as a means to achieve the seemingly conflicting two\ngoals of maintaining control performance and preserving privacy at the same\ntime; towards this end, we further investigate a dynamic stochastic quantizer.\nUnder a stability assumption, the dynamic stochastic quantizer can enhance\nprivacy, more than the static one, while achieving the same control\nperformance. We further handle the unstable case by additionally applying input\nGaussian noise.",
    "updated" : "2024-03-05T15:31:35Z",
    "published" : "2024-03-05T15:31:35Z",
    "authors" : [
      {
        "name" : "Le Liu"
      },
      {
        "name" : "Yu Kawano"
      },
      {
        "name" : "Ming Cao"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.CR",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02694v1",
    "title" : "Privacy-Aware Semantic Cache for Large Language Models",
    "summary" : "Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2\nhave revolutionized natural language processing and search engine dynamics.\nHowever, these models incur exceptionally high computational costs. For\ninstance, GPT-3 consists of 175 billion parameters and inference on these\nmodels also demands billions of floating-point operations. Caching is a natural\nsolution to reduce LLM inference costs on repeated queries. However, existing\ncaching methods are incapable of finding semantic similarities among LLM\nqueries, leading to unacceptable false hit-and-miss rates.\n  This paper introduces MeanCache, a semantic cache for LLMs that identifies\nsemantically similar queries to determine cache hit or miss. Using MeanCache,\nthe response to a user's semantically similar query can be retrieved from a\nlocal cache rather than re-querying the LLM, thus reducing costs, service\nprovider load, and environmental impact. MeanCache leverages Federated Learning\n(FL) to collaboratively train a query similarity model in a distributed manner\nacross numerous users without violating privacy. By placing a local cache in\neach user's device and using FL, MeanCache reduces the latency and costs and\nenhances model performance, resulting in lower cache false hit rates. Our\nexperiments, benchmarked against the GPTCache, reveal that MeanCache attains an\napproximately 17% higher F-score and a 20% increase in precision during\nsemantic cache hit-and-miss decisions. Furthermore, MeanCache reduces the\nstorage requirement by 83% and accelerates semantic cache hit-and-miss\ndecisions by 11%, while still surpassing GPTCache.",
    "updated" : "2024-03-05T06:23:50Z",
    "published" : "2024-03-05T06:23:50Z",
    "authors" : [
      {
        "name" : "Waris Gill"
      },
      {
        "name" : "Mohamed Elidrisi"
      },
      {
        "name" : "Pallavi Kalapatapu"
      },
      {
        "name" : "Ali Anwar"
      },
      {
        "name" : "Muhammad Ali Gulzar"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.DC",
      "I.2.7"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02631v1",
    "title" : "Privacy in Multi-agent Systems",
    "summary" : "With the increasing awareness of privacy and the deployment of legislations\nin various multi-agent system application domains such as power systems and\nintelligent transportation, the privacy protection problem for multi-agent\nsystems is gaining increased traction in recent years. This article discusses\nsome of the representative advancements in the filed.",
    "updated" : "2024-03-05T03:40:39Z",
    "published" : "2024-03-05T03:40:39Z",
    "authors" : [
      {
        "name" : "Yongqiang Wang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02409v1",
    "title" : "Privacy-Respecting Type Error Telemetry at Scale",
    "summary" : "Context: Roblox Studio lets millions of creators build interactive\nexperiences by programming in a variant of Lua called Luau. The creators form a\nbroad group, ranging from novices writing their first script to professional\ndevelopers; thus, Luau must support a wide audience. As part of its efforts to\nsupport all kinds of programmers, Luau includes an optional, gradual type\nsystem and goes to great lengths to minimize false positive errors.\n  Inquiry: Since Luau is currently being used by many creators, we want to\ncollect data to improve the language and, in particular, the type system. The\nstandard way to collect data is to deploy client-side telemetry; however, we\ncannot scrape personal data or proprietary information, which means we cannot\ncollect source code fragments, error messages, or even filepaths. The research\nquestions are thus about how to conduct telemetry that is not invasive and\nobtain insights from it about type errors.\n  Approach: We designed and implemented a pseudonymized, randomly-sampling\ntelemetry system for Luau. Telemetry records include a timestamp, a session id,\na reason for sending, and a numeric summary of the most recent type analyses.\nThis information lets us study type errors over time without revealing private\ndata. We deployed the system in Roblox Studio during Spring 2023 and collected\nover 1.5 million telemetry records from over 340,000 sessions.\n  Knowledge: We present several findings about Luau, all of which suggest that\ntelemetry is an effective way to study type error pragmatics. One of the\nless-surprising findings is that opt-in gradual types are unpopular: there is\nan 100x gap between the number of untyped Luau sessions and the number of typed\nones. One surprise is that the strict mode for type analysis is overly\nconservative about interactions with data assets. A reassuring finding is that\ntype analysis rarely hits its internal limits on problem size.\n  Grounding: Our findings are supported by a dataset of over 1.5 million\ntelemetry records. The data and scripts for analyzing it are available in an\nartifact.\n  Importance: Beyond the immediate benefits to Luau, our findings about types\nand type errors have implications for adoption and ergonomics in other gradual\nlanguages such as TypeScript, Elixir, and Typed Racket. Our telemetry design is\nof broad interest, as it reports on type errors without revealing sensitive\ninformation.",
    "updated" : "2024-03-04T19:07:42Z",
    "published" : "2024-03-04T19:07:42Z",
    "authors" : [
      {
        "name" : "Ben Greenman"
      },
      {
        "name" : "Alan Jeffrey"
      },
      {
        "name" : "Shriram Krishnamurthi"
      },
      {
        "name" : "Mitesh Shah"
      }
    ],
    "categories" : [
      "cs.PL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02324v1",
    "title" : "Preserving Smart Grid Integrity: A Differential Privacy Framework for\n  Secure Detection of False Data Injection Attacks in the Smart Grid",
    "summary" : "In this paper, we present a framework based on differential privacy (DP) for\nquerying electric power measurements to detect system anomalies or bad data\ncaused by false data injections (FDIs). Our DP approach conceals consumption\nand system matrix data, while simultaneously enabling an untrusted third party\nto test hypotheses of anomalies, such as an FDI attack, by releasing a\nrandomized sufficient statistic for hypothesis-testing. We consider a\nmeasurement model corrupted by Gaussian noise and a sparse noise vector\nrepresenting the attack, and we observe that the optimal test statistic is a\nchi-square random variable. To detect possible attacks, we propose a novel DP\nchi-square noise mechanism that ensures the test does not reveal private\ninformation about power injections or the system matrix. The proposed framework\nprovides a robust solution for detecting FDIs while preserving the privacy of\nsensitive power system data.",
    "updated" : "2024-03-04T18:55:16Z",
    "published" : "2024-03-04T18:55:16Z",
    "authors" : [
      {
        "name" : "Nikhil Ravi"
      },
      {
        "name" : "Anna Scaglione"
      },
      {
        "name" : "Sean Peisert"
      },
      {
        "name" : "Parth Pradhan"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02292v1",
    "title" : "A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends",
    "summary" : "We present an analysis of 12 million instances of privacy-relevant reviews\npublicly visible on the Google Play Store that span a 10 year period. By\nleveraging state of the art NLP techniques, we can examine what users have been\nwriting about privacy along multiple dimensions: time, countries, app types,\ndiverse privacy topics, and even across a spectrum of emotions. We find\nconsistent growth of privacy-relevant reviews, and explore topics that are\ntrending (such as Data Deletion and Data Theft), as well as those on the\ndecline (such as privacy-relevant reviews on sensitive permissions). We find\nthat although privacy reviews come from more than 200 countries, 33 countries\nprovide 90% of privacy reviews. We conduct a comparison across countries by\nexamining the distribution of privacy topics a country's users write about, and\nfind that geographic proximity is not a reliable indicator that nearby\ncountries have similar privacy perspectives. We uncover some countries with\nunique patterns and explore those herein. Surprisingly, we uncover that it is\nnot uncommon for reviews that discuss privacy to be positive (32%); many users\nexpress pleasure about privacy features within apps or privacy-focused apps. We\nalso uncover some unexpected behaviors, such as the use of reviews to deliver\nprivacy disclaimers to developers. Finally, we demonstrate the value of\nanalyzing app reviews with our approach as a complement to existing methods for\nunderstanding users' perspectives about privacy.",
    "updated" : "2024-03-04T18:21:56Z",
    "published" : "2024-03-04T18:21:56Z",
    "authors" : [
      {
        "name" : "Omer Akgul"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Nina Taft"
      },
      {
        "name" : "Michelle L. Mazurek"
      },
      {
        "name" : "Hamza Harkous"
      },
      {
        "name" : "Animesh Srivastava"
      },
      {
        "name" : "Benoit Seguin"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02116v1",
    "title" : "Inf2Guard: An Information-Theoretic Framework for Learning\n  Privacy-Preserving Representations against Inference Attacks",
    "summary" : "Machine learning (ML) is vulnerable to inference (e.g., membership inference,\nproperty inference, and data reconstruction) attacks that aim to infer the\nprivate information of training data or dataset. Existing defenses are only\ndesigned for one specific type of attack and sacrifice significant utility or\nare soon broken by adaptive attacks. We address these limitations by proposing\nan information-theoretic defense framework, called Inf2Guard, against the three\nmajor types of inference attacks. Our framework, inspired by the success of\nrepresentation learning, posits that learning shared representations not only\nsaves time/costs but also benefits numerous downstream tasks. Generally,\nInf2Guard involves two mutual information objectives, for privacy protection\nand utility preservation, respectively. Inf2Guard exhibits many merits: it\nfacilitates the design of customized objectives against the specific inference\nattack; it provides a general defense framework which can treat certain\nexisting defenses as special cases; and importantly, it aids in deriving\ntheoretical results, e.g., inherent utility-privacy tradeoff and guaranteed\nprivacy leakage. Extensive evaluations validate the effectiveness of Inf2Guard\nfor learning privacy-preserving representations against inference attacks and\ndemonstrate the superiority over the baselines.",
    "updated" : "2024-03-04T15:20:19Z",
    "published" : "2024-03-04T15:20:19Z",
    "authors" : [
      {
        "name" : "Sayedeh Leila Noorbakhsh"
      },
      {
        "name" : "Binghui Zhang"
      },
      {
        "name" : "Yuan Hong"
      },
      {
        "name" : "Binghui Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02051v1",
    "title" : "Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations",
    "summary" : "Injecting heavy-tailed noise to the iterates of stochastic gradient descent\n(SGD) has received increasing attention over the past few years. While various\ntheoretical properties of the resulting algorithm have been analyzed mainly\nfrom learning theory and optimization perspectives, their privacy preservation\nproperties have not yet been established. Aiming to bridge this gap, we provide\ndifferential privacy (DP) guarantees for noisy SGD, when the injected noise\nfollows an $\\alpha$-stable distribution, which includes a spectrum of\nheavy-tailed distributions (with infinite variance) as well as the Gaussian\ndistribution. Considering the $(\\epsilon, \\delta)$-DP framework, we show that\nSGD with heavy-tailed perturbations achieves $(0, \\tilde{\\mathcal{O}}(1/n))$-DP\nfor a broad class of loss functions which can be non-convex, where $n$ is the\nnumber of data points. As a remarkable byproduct, contrary to prior work that\nnecessitates bounded sensitivity for the gradients or clipping the iterates,\nour theory reveals that under mild assumptions, such a projection step is not\nactually necessary. We illustrate that the heavy-tailed noising mechanism\nachieves similar DP guarantees compared to the Gaussian case, which suggests\nthat it can be a viable alternative to its light-tailed counterparts.",
    "updated" : "2024-03-04T13:53:41Z",
    "published" : "2024-03-04T13:53:41Z",
    "authors" : [
      {
        "name" : "Umut Şimşekli"
      },
      {
        "name" : "Mert Gürbüzbalaban"
      },
      {
        "name" : "Sinan Yıldırım"
      },
      {
        "name" : "Lingjiong Zhu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.01788v1",
    "title" : "K-stars LDP: A Novel Framework for (p, q)-clique Enumeration under Local\n  Differential Privacy",
    "summary" : "(p,q)-clique enumeration on a bipartite graph is critical for calculating\nclustering coefficient and detecting densest subgraph. It is necessary to carry\nout subgraph enumeration while protecting users' privacy from any potential\nattacker as the count of subgraph may contain sensitive information. Most\nrecent studies focus on the privacy protection algorithms based on edge LDP\n(Local Differential Privacy). However, these algorithms suffer a large\nestimation error due to the great amount of required noise. In this paper, we\npropose a novel idea of k-stars LDP and a novel k-stars LDP algorithm for (p,\nq)-clique enumeration with a small estimation error, where a k-stars is a\nstar-shaped graph with k nodes connecting to one node. The effectiveness of\nedge LDP relies on its capacity to obfuscate the existence of an edge between\nthe user and his one-hop neighbors. This is based on the premise that a user\nshould be aware of the existence of his one-hop neighbors. Similarly, we can\napply this premise to k-stars as well, where an edge is a specific genre of\n1-stars. Based on this fact, we first propose the k-stars neighboring list to\nenable our algorithm to obfuscate the existence of k-stars with Warner' s RR.\nThen, we propose the absolute value correction technique and the k-stars\nsampling technique to further reduce the estimation error. Finally, with the\ntwo-round user-collector interaction mechanism, we propose our k-stars LDP\nalgorithm to count the number of (p, q)-clique while successfully protecting\nusers' privacy. Both the theoretical analysis and experiments have showed the\nsuperiority of our algorithm over the algorithms based on edge LDP.",
    "updated" : "2024-03-04T07:30:10Z",
    "published" : "2024-03-04T07:30:10Z",
    "authors" : [
      {
        "name" : "Henan Sun"
      },
      {
        "name" : "Zhengyu Wu"
      },
      {
        "name" : "Rong-Hua Li"
      },
      {
        "name" : "Guoren Wang"
      },
      {
        "name" : "Zening Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.01438v1",
    "title" : "Privacy-Preserving Collaborative Split Learning Framework for Smart Grid\n  Load Forecasting",
    "summary" : "Accurate load forecasting is crucial for energy management, infrastructure\nplanning, and demand-supply balancing. Smart meter data availability has led to\nthe demand for sensor-based load forecasting. Conventional ML allows training a\nsingle global model using data from multiple smart meters requiring data\ntransfer to a central server, raising concerns for network requirements,\nprivacy, and security. We propose a split learning-based framework for load\nforecasting to alleviate this issue. We split a deep neural network model into\ntwo parts, one for each Grid Station (GS) responsible for an entire\nneighbourhood's smart meters and the other for the Service Provider (SP).\nInstead of sharing their data, client smart meters use their respective GSs'\nmodel split for forward pass and only share their activations with the GS.\nUnder this framework, each GS is responsible for training a personalized model\nsplit for their respective neighbourhoods, whereas the SP can train a single\nglobal or personalized model for each GS. Experiments show that the proposed\nmodels match or exceed a centrally trained model's performance and generalize\nwell. Privacy is analyzed by assessing information leakage between data and\nshared activations of the GS model split. Additionally, differential privacy\nenhances local data privacy while examining its impact on performance. A\ntransformer model is used as our base learner.",
    "updated" : "2024-03-03T08:24:39Z",
    "published" : "2024-03-03T08:24:39Z",
    "authors" : [
      {
        "name" : "Asif Iqbal"
      },
      {
        "name" : "Prosanta Gope"
      },
      {
        "name" : "Biplab Sikdar"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.01435v1",
    "title" : "Distributed Least-Squares Optimization Solvers with Differential Privacy",
    "summary" : "This paper studies the distributed least-squares optimization problem with\ndifferential privacy requirement of local cost functions, for which two\ndifferentially private distributed solvers are proposed. The first is\nestablished on the distributed gradient tracking algorithm, by appropriately\nperturbing the initial values and parameters that contain the privacy-sensitive\ndata with Gaussian and truncated Laplacian noises, respectively. Rigorous\nproofs are established to show the achievable trade-off between the\n({\\epsilon}, {\\delta})-differential privacy and the computation accuracy. The\nsecond solver is established on the combination of the distributed shuffling\nmechanism and the average consensus algorithm, which enables each agent to\nobtain a noisy version of parameters characterizing the global gradient. As a\nresult, the least-squares optimization problem can be eventually solved by each\nagent locally in such a way that any given ({\\epsilon}, {\\delta})-differential\nprivacy requirement can be preserved while the solution may be computed with\nthe accuracy independent of the network size, which makes the latter more\nsuitable for large-scale distributed least-squares problems. Numerical\nsimulations are presented to show the effectiveness of both solvers.",
    "updated" : "2024-03-03T08:14:50Z",
    "published" : "2024-03-03T08:14:50Z",
    "authors" : [
      {
        "name" : "Weijia Liu"
      },
      {
        "name" : "Lei Wang"
      },
      {
        "name" : "Fanghong Guo"
      },
      {
        "name" : "Zhengguang Wu"
      },
      {
        "name" : "Hongye Su"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03612v1",
    "title" : "Using the Dual-Privacy Framework to Understand Consumers' Perceived\n  Privacy Violations Under Different Firm Practices in Online Advertising",
    "summary" : "In response to privacy concerns about collecting and using personal data, the\nonline advertising industry has been developing privacy-enhancing technologies\n(PETs), e.g., under Google's Privacy Sandbox initiative. In this research, we\nuse the dual-privacy framework, which postulates that consumers have intrinsic\nand instrumental preferences for privacy, to understand consumers' perceived\nprivacy violations (PPVs) for current and proposed online advertising\npractices. The key idea is that different practices differ in whether\nindividual data leaves the consumer's machine or not and in how they track and\ntarget consumers; these affect, respectively, the intrinsic and instrumental\ncomponents of privacy preferences differently, leading to different PPVs for\ndifferent practices. We conducted online studies focused on consumers in the\nUnited States to elicit PPVs for various advertising practices. Our findings\nconfirm the intuition that tracking and targeting consumers under the industry\nstatus quo of behavioral targeting leads to high PPV. New technologies or\nproposals that ensure that data are kept on the consumer's machine lower PPV\nrelative to behavioral targeting but, importantly, this decrease is small.\nFurthermore, group-level targeting does not differ significantly from\nindividual-level targeting in reducing PPV. Under contextual targeting, where\nthere is no tracking, PPV is significantly reduced. Interestingly, with respect\nto PPV, consumers are indifferent between seeing untargeted ads and no ads when\nthey are not being tracked. We find that consumer perceptions of privacy\nviolations under different tracking and targeting practices may differ from\nwhat technical definitions suggest. Therefore, rather than relying solely on\ntechnical perspectives, a consumer-centric approach to privacy is needed, based\non, for instance, the dual-privacy framework.",
    "updated" : "2024-03-06T11:06:25Z",
    "published" : "2024-03-06T11:06:25Z",
    "authors" : [
      {
        "name" : "Kinshuk Jerath"
      },
      {
        "name" : "Klaus M. Miller"
      }
    ],
    "categories" : [
      "econ.GN",
      "q-fin.EC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03610v1",
    "title" : "Paying for Privacy: Pay-or-Tracking Walls",
    "summary" : "Prestigious news publishers, and more recently, Meta, have begun to request\nthat users pay for privacy. Specifically, users receive a notification banner,\nreferred to as a pay-or-tracking wall, that requires them to (i) pay money to\navoid being tracked or (ii) consent to being tracked. These walls have invited\nconcerns that privacy might become a luxury. However, little is known about\npay-or-tracking walls, which prevents a meaningful discussion about their\nappropriateness. This paper conducts several empirical studies and finds that\ntop EU publishers use pay-or-tracking walls. Their implementations involve\nvarious approaches, including bundling the pay option with advertising-free\naccess or additional content. The price for not being tracked exceeds the\nadvertising revenue that publishers generate from a user who consents to being\ntracked. Notably, publishers' traffic does not decline when implementing a\npay-or-tracking wall and most users consent to being tracked; only a few users\npay. In short, pay-or-tracking walls seem to provide the means for expanding\nthe practice of tracking. Publishers profit from pay-or-tracking walls and may\nobserve a revenue increase of 16.4% due to tracking more users than under a\ncookie consent banner.",
    "updated" : "2024-03-06T10:59:49Z",
    "published" : "2024-03-06T10:59:49Z",
    "authors" : [
      {
        "name" : "Timo Mueller-Tribbensee"
      },
      {
        "name" : "Klaus M. Miller"
      },
      {
        "name" : "Bernd Skiera"
      }
    ],
    "categories" : [
      "econ.GN",
      "q-fin.EC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03600v1",
    "title" : "A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain\n  Recommendation",
    "summary" : "Cross-domain recommendation (CDR) aims to enhance recommendation accuracy in\na target domain with sparse data by leveraging rich information in a source\ndomain, thereby addressing the data-sparsity problem. Some existing CDR methods\nhighlight the advantages of extracting domain-common and domain-specific\nfeatures to learn comprehensive user and item representations. However, these\nmethods can't effectively disentangle these components as they often rely on\nsimple user-item historical interaction information (such as ratings, clicks,\nand browsing), neglecting the rich multi-modal features. Additionally, they\ndon't protect user-sensitive data from potential leakage during knowledge\ntransfer between domains. To address these challenges, we propose a\nPrivacy-Preserving Framework with Multi-Modal Data for Cross-Domain\nRecommendation, called P2M2-CDR. Specifically, we first design a multi-modal\ndisentangled encoder that utilizes multi-modal information to disentangle more\ninformative domain-common and domain-specific embeddings. Furthermore, we\nintroduce a privacy-preserving decoder to mitigate user privacy leakage during\nknowledge transfer. Local differential privacy (LDP) is utilized to obfuscate\nthe disentangled embeddings before inter-domain exchange, thereby enhancing\nprivacy protection. To ensure both consistency and differentiation among these\nobfuscated disentangled embeddings, we incorporate contrastive learning-based\ndomain-inter and domain-intra losses. Extensive Experiments conducted on four\nreal-world datasets demonstrate that P2M2-CDR outperforms other\nstate-of-the-art single-domain and cross-domain baselines.",
    "updated" : "2024-03-06T10:40:08Z",
    "published" : "2024-03-06T10:40:08Z",
    "authors" : [
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Lei Sang"
      },
      {
        "name" : "Quangui Zhang"
      },
      {
        "name" : "Qiang Wu"
      },
      {
        "name" : "Min Xu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03592v1",
    "title" : "Wildest Dreams: Reproducible Research in Privacy-preserving Neural\n  Network Training",
    "summary" : "Machine Learning (ML), addresses a multitude of complex issues in multiple\ndisciplines, including social sciences, finance, and medical research. ML\nmodels require substantial computing power and are only as powerful as the data\nutilized. Due to high computational cost of ML methods, data scientists\nfrequently use Machine Learning-as-a-Service (MLaaS) to outsource computation\nto external servers. However, when working with private information, like\nfinancial data or health records, outsourcing the computation might result in\nprivacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have\nenabled ML training and inference over protected data through the use of\nPrivacy-Preserving Machine Learning (PPML). However, these techniques are still\nat a preliminary stage and their application in real-world situations is\ndemanding. In order to comprehend discrepancy between theoretical research\nsuggestions and actual applications, this work examines the past and present of\nPPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party\nComputation (SMPC) applied to ML. This work primarily focuses on the ML model's\ntraining phase, where maintaining user data privacy is of utmost importance. We\nprovide a solid theoretical background that eases the understanding of current\napproaches and their limitations. In addition, we present a SoK of the most\nrecent PPML frameworks for model training and provide a comprehensive\ncomparison in terms of the unique properties and performances on standard\nbenchmarks. Also, we reproduce the results for some of the papers and examine\nat what level existing works in the field provide support for open science. We\nbelieve our work serves as a valuable contribution by raising awareness about\nthe current gap between theoretical advancements and real-world applications in\nPPML, specifically regarding open-source availability, reproducibility, and\nusability.",
    "updated" : "2024-03-06T10:25:36Z",
    "published" : "2024-03-06T10:25:36Z",
    "authors" : [
      {
        "name" : "Tanveer Khan"
      },
      {
        "name" : "Mindaugas Budzys"
      },
      {
        "name" : "Khoa Nguyen"
      },
      {
        "name" : "Antonis Michalas"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.03337v1",
    "title" : "Fine-Grained Privacy Guarantees for Coverage Problems",
    "summary" : "We introduce a new notion of neighboring databases for coverage problems such\nas Max Cover and Set Cover under differential privacy. In contrast to the\nstandard privacy notion for these problems, which is analogous to node-privacy\nin graphs, our new definition gives a more fine-grained privacy guarantee,\nwhich is analogous to edge-privacy. We illustrate several scenarios of Set\nCover and Max Cover where our privacy notion is desired one for the\napplication.\n  Our main result is an $\\epsilon$-edge differentially private algorithm for\nMax Cover which obtains an $(1-1/e-\\eta,\\tilde{O}(k/\\epsilon))$-approximation\nwith high probability. Furthermore, we show that this result is nearly tight:\nwe give a lower bound show that an additive error of $\\Omega(k/\\epsilon)$ is\nnecessary under edge-differential privacy. Via group privacy properties, this\nimplies a new algorithm for $\\epsilon$-node differentially private Max Cover\nwhich obtains an $(1-1/e-\\eta,\\tilde{O}(fk/\\epsilon))$-approximation, where $f$\nis the maximum degree of an element in the set system. When $f\\ll k$, this\nimproves over the best known algorithm for Max Cover under pure (node)\ndifferential privacy, which obtains an\n$(1-1/e,\\tilde{O}(k^2/\\epsilon))$-approximation.",
    "updated" : "2024-03-05T21:40:10Z",
    "published" : "2024-03-05T21:40:10Z",
    "authors" : [
      {
        "name" : "Laxman Dhulipala"
      },
      {
        "name" : "George Z. Li"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04485v1",
    "title" : "Privacy in Cloud Computing through Immersion-based Coding",
    "summary" : "Cloud computing enables users to process and store data remotely on\nhigh-performance computers and servers by sharing data over the Internet.\nHowever, transferring data to clouds causes unavoidable privacy concerns. Here,\nwe present a synthesis framework to design coding mechanisms that allow sharing\nand processing data in a privacy-preserving manner without sacrificing data\nutility and algorithmic performance. We consider the setup where the user aims\nto run an algorithm in the cloud using private data. The cloud then returns\nsome data utility back to the user (utility refers to the service that the\nalgorithm provides, e.g., classification, prediction, AI models, etc.). To\navoid privacy concerns, the proposed scheme provides tools to co-design: 1)\ncoding mechanisms to distort the original data and guarantee a prescribed\ndifferential privacy level; 2) an equivalent-but-different algorithm (referred\nhere to as the target algorithm) that runs on distorted data and produces\ndistorted utility; and 3) a decoding function that extracts the true utility\nfrom the distorted one with a negligible error. Then, instead of sharing the\noriginal data and algorithm with the cloud, only the distorted data and target\nalgorithm are disclosed, thereby avoiding privacy concerns. The proposed scheme\nis built on the synergy of differential privacy and system immersion tools from\ncontrol theory. The key underlying idea is to design a higher-dimensional\ntarget algorithm that embeds all trajectories of the original algorithm and\nworks on randomly encoded data to produce randomly encoded utility. We show\nthat the proposed scheme can be designed to offer any level of differential\nprivacy without degrading the algorithm's utility. We present two use cases to\nillustrate the performance of the developed tools: privacy in\noptimization/learning algorithms and a nonlinear networked control system.",
    "updated" : "2024-03-07T13:38:18Z",
    "published" : "2024-03-07T13:38:18Z",
    "authors" : [
      {
        "name" : "Haleh Hayati"
      },
      {
        "name" : "Nathan van de Wouw"
      },
      {
        "name" : "Carlos Murguia"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04468v1",
    "title" : "A Survey of Graph Neural Networks in Real world: Imbalance, Noise,\n  Privacy and OOD Challenges",
    "summary" : "Graph-structured data exhibits universality and widespread applicability\nacross diverse domains, such as social network analysis, biochemistry,\nfinancial fraud detection, and network security. Significant strides have been\nmade in leveraging Graph Neural Networks (GNNs) to achieve remarkable success\nin these areas. However, in real-world scenarios, the training environment for\nmodels is often far from ideal, leading to substantial performance degradation\nof GNN models due to various unfavorable factors, including imbalance in data\ndistribution, the presence of noise in erroneous data, privacy protection of\nsensitive information, and generalization capability for out-of-distribution\n(OOD) scenarios. To tackle these issues, substantial efforts have been devoted\nto improving the performance of GNN models in practical real-world scenarios,\nas well as enhancing their reliability and robustness. In this paper, we\npresent a comprehensive survey that systematically reviews existing GNN models,\nfocusing on solutions to the four mentioned real-world challenges including\nimbalance, noise, privacy, and OOD in practical scenarios that many existing\nreviews have not considered. Specifically, we first highlight the four key\nchallenges faced by existing GNNs, paving the way for our exploration of\nreal-world GNN models. Subsequently, we provide detailed discussions on these\nfour aspects, dissecting how these solutions contribute to enhancing the\nreliability and robustness of GNN models. Last but not least, we outline\npromising directions and offer future perspectives in the field.",
    "updated" : "2024-03-07T13:10:37Z",
    "published" : "2024-03-07T13:10:37Z",
    "authors" : [
      {
        "name" : "Wei Ju"
      },
      {
        "name" : "Siyu Yi"
      },
      {
        "name" : "Yifan Wang"
      },
      {
        "name" : "Zhiping Xiao"
      },
      {
        "name" : "Zhengyang Mao"
      },
      {
        "name" : "Hourun Li"
      },
      {
        "name" : "Yiyang Gu"
      },
      {
        "name" : "Yifang Qin"
      },
      {
        "name" : "Nan Yin"
      },
      {
        "name" : "Senzhang Wang"
      },
      {
        "name" : "Xinwang Liu"
      },
      {
        "name" : "Xiao Luo"
      },
      {
        "name" : "Philip S. Yu"
      },
      {
        "name" : "Ming Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04451v1",
    "title" : "Membership Inference Attacks and Privacy in Topic Modeling",
    "summary" : "Recent research shows that large language models are susceptible to privacy\nattacks that infer aspects of the training data. However, it is unclear if\nsimpler generative models, like topic models, share similar vulnerabilities. In\nthis work, we propose an attack against topic models that can confidently\nidentify members of the training data in Latent Dirichlet Allocation. Our\nresults suggest that the privacy risks associated with generative modeling are\nnot restricted to large neural models. Additionally, to mitigate these\nvulnerabilities, we explore differentially private (DP) topic modeling. We\npropose a framework for private topic modeling that incorporates DP vocabulary\nselection as a pre-processing step, and show that it improves privacy while\nhaving limited effects on practical utility.",
    "updated" : "2024-03-07T12:43:42Z",
    "published" : "2024-03-07T12:43:42Z",
    "authors" : [
      {
        "name" : "Nico Manzonelli"
      },
      {
        "name" : "Wanrong Zhang"
      },
      {
        "name" : "Salil Vadhan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04124v1",
    "title" : "Privacy-preserving Fine-tuning of Large Language Models through Flatness",
    "summary" : "The privacy concerns associated with the use of Large Language Models (LLMs)\nhave grown recently with the development of LLMs such as ChatGPT. Differential\nPrivacy (DP) techniques are explored in existing work to mitigate their privacy\nrisks at the cost of generalization degradation. Our paper reveals that the\nflatness of DP-trained models' loss landscape plays an essential role in the\ntrade-off between their privacy and generalization. We further propose a\nholistic framework to enforce appropriate weight flatness, which substantially\nimproves model generalization with competitive privacy preservation. It\ninnovates from three coarse-to-grained levels, including perturbation-aware\nmin-max optimization on model weights within a layer, flatness-guided sparse\nprefix-tuning on weights across layers, and weight knowledge distillation\nbetween DP \\& non-DP weights copies. Comprehensive experiments of both\nblack-box and white-box scenarios are conducted to demonstrate the\neffectiveness of our proposal in enhancing generalization and maintaining DP\ncharacteristics. For instance, on text classification dataset QNLI, DP-Flat\nachieves similar performance with non-private full fine-tuning but with DP\nguarantee under privacy budget $\\epsilon=3$, and even better performance given\nhigher privacy budgets. Codes are provided in the supplement.",
    "updated" : "2024-03-07T00:44:11Z",
    "published" : "2024-03-07T00:44:11Z",
    "authors" : [
      {
        "name" : "Tiejin Chen"
      },
      {
        "name" : "Longchao Da"
      },
      {
        "name" : "Huixue Zhou"
      },
      {
        "name" : "Pingzhi Li"
      },
      {
        "name" : "Kaixiong Zhou"
      },
      {
        "name" : "Tianlong Chen"
      },
      {
        "name" : "Hua Wei"
      }
    ],
    "categories" : [
      "cs.AI",
      "I.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04024v1",
    "title" : "Enhancing chest X-ray datasets with privacy-preserving large language\n  models and multi-type annotations: a data-driven approach for improved\n  classification",
    "summary" : "In chest X-ray (CXR) image analysis, rule-based systems are usually employed\nto extract labels from reports, but concerns exist about label quality. These\ndatasets typically offer only presence labels, sometimes with binary\nuncertainty indicators, which limits their usefulness. In this work, we present\nMAPLEZ (Medical report Annotations with Privacy-preserving Large language model\nusing Expeditious Zero shot answers), a novel approach leveraging a locally\nexecutable Large Language Model (LLM) to extract and enhance findings labels on\nCXR reports. MAPLEZ extracts not only binary labels indicating the presence or\nabsence of a finding but also the location, severity, and radiologists'\nuncertainty about the finding. Over eight abnormalities from five test sets, we\nshow that our method can extract these annotations with an increase of 5\npercentage points (pp) in F1 score for categorical presence annotations and\nmore than 30 pp increase in F1 score for the location annotations over\ncompeting labelers. Additionally, using these improved annotations in\nclassification supervision, we demonstrate substantial advancements in model\nquality, with an increase of 1.7 pp in AUROC over models trained with\nannotations from the state-of-the-art approach. We share code and annotations.",
    "updated" : "2024-03-06T20:10:41Z",
    "published" : "2024-03-06T20:10:41Z",
    "authors" : [
      {
        "name" : "Ricardo Bigolin Lanfredi"
      },
      {
        "name" : "Pritam Mukherjee"
      },
      {
        "name" : "Ronald Summers"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05344v1",
    "title" : "Federated Learning Method for Preserving Privacy in Face Recognition\n  System",
    "summary" : "The state-of-the-art face recognition systems are typically trained on a\nsingle computer, utilizing extensive image datasets collected from various\nnumber of users. However, these datasets often contain sensitive personal\ninformation that users may hesitate to disclose. To address potential privacy\nconcerns, we explore the application of federated learning, both with and\nwithout secure aggregators, in the context of both supervised and unsupervised\nface recognition systems. Federated learning facilitates the training of a\nshared model without necessitating the sharing of individual private data,\nachieving this by training models on decentralized edge devices housing the\ndata. In our proposed system, each edge device independently trains its own\nmodel, which is subsequently transmitted either to a secure aggregator or\ndirectly to the central server. To introduce diverse data without the need for\ndata transmission, we employ generative adversarial networks to generate\nimposter data at the edge. Following this, the secure aggregator or central\nserver combines these individual models to construct a global model, which is\nthen relayed back to the edge devices. Experimental findings based on the\nCelebA datasets reveal that employing federated learning in both supervised and\nunsupervised face recognition systems offers dual benefits. Firstly, it\nsafeguards privacy since the original data remains on the edge devices.\nSecondly, the experimental results demonstrate that the aggregated model yields\nnearly identical performance compared to the individual models, particularly\nwhen the federated model does not utilize a secure aggregator. Hence, our\nresults shed light on the practical challenges associated with\nprivacy-preserving face image training, particularly in terms of the balance\nbetween privacy and accuracy.",
    "updated" : "2024-03-08T14:21:43Z",
    "published" : "2024-03-08T14:21:43Z",
    "authors" : [
      {
        "name" : "Enoch Solomon"
      },
      {
        "name" : "Abraham Woubie"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05275v1",
    "title" : "vSPACE: Voting in a Scalable, Privacy-Aware and Confidential Election",
    "summary" : "The vSPACE experimental proof-of-concept (PoC) on the TrueElect[Anon][Creds]\nprotocol presents a novel approach to secure, private, and scalable elections,\nextending the TrueElect and ElectAnon protocols with the integration of\nAnonCreds SSI (Self-Sovereign Identity). Such a protocol PoC is situated within\na Zero-Trust Architecture (ZTA) and leverages confidential computing,\ncontinuous authentication, multi-party computation (MPC), and well-architected\nframework (WAF) principles to address the challenges of cybersecurity, privacy,\nand trust over IP (ToIP) protection. Employing a Kubernetes confidential\ncluster within an Enterprise-Scale Landing Zone (ESLZ), vSPACE integrates\nDistributed Ledger Technology (DLT) for immutable and certifiable audit trails.\nThe Infrastructure as Code (IaC) model ensures rapid deployment, consistent\nmanagement, and adherence to security standards, making vSPACE a future-proof\nsolution for digital voting systems.",
    "updated" : "2024-03-08T12:56:10Z",
    "published" : "2024-03-08T12:56:10Z",
    "authors" : [
      {
        "name" : "Se Elnour"
      },
      {
        "name" : "William J Buchanan"
      },
      {
        "name" : "Paul Keating"
      },
      {
        "name" : "Mwrwan Abubakar"
      },
      {
        "name" : "Sirag Elnour"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05156v1",
    "title" : "On Protecting the Data Privacy of Large Language Models (LLMs): A Survey",
    "summary" : "Large language models (LLMs) are complex artificial intelligence systems\ncapable of understanding, generating and translating human language. They learn\nlanguage patterns by analyzing large amounts of text data, allowing them to\nperform writing, conversation, summarizing and other language tasks. When LLMs\nprocess and generate large amounts of data, there is a risk of leaking\nsensitive information, which may threaten data privacy. This paper concentrates\non elucidating the data privacy concerns associated with LLMs to foster a\ncomprehensive understanding. Specifically, a thorough investigation is\nundertaken to delineate the spectrum of data privacy threats, encompassing both\npassive privacy leakage and active privacy attacks within LLMs. Subsequently,\nwe conduct an assessment of the privacy protection mechanisms employed by LLMs\nat various stages, followed by a detailed examination of their efficacy and\nconstraints. Finally, the discourse extends to delineate the challenges\nencountered and outline prospective directions for advancement in the realm of\nLLM privacy protection.",
    "updated" : "2024-03-08T08:47:48Z",
    "published" : "2024-03-08T08:47:48Z",
    "authors" : [
      {
        "name" : "Biwei Yan"
      },
      {
        "name" : "Kun Li"
      },
      {
        "name" : "Minghui Xu"
      },
      {
        "name" : "Yueyan Dong"
      },
      {
        "name" : "Yue Zhang"
      },
      {
        "name" : "Zhaochun Ren"
      },
      {
        "name" : "Xiuzheng Cheng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05114v1",
    "title" : "APPLE: Adversarial Privacy-aware Perturbations on Latent Embedding for\n  Unfairness Mitigation",
    "summary" : "Ensuring fairness in deep-learning-based segmentors is crucial for health\nequity. Much effort has been dedicated to mitigating unfairness in the training\ndatasets or procedures. However, with the increasing prevalence of foundation\nmodels in medical image analysis, it is hard to train fair models from scratch\nwhile preserving utility. In this paper, we propose a novel method, Adversarial\nPrivacy-aware Perturbations on Latent Embedding (APPLE), that can improve the\nfairness of deployed segmentors by introducing a small latent feature perturber\nwithout updating the weights of the original model. By adding perturbation to\nthe latent vector, APPLE decorates the latent vector of segmentors such that no\nfairness-related features can be passed to the decoder of the segmentors while\npreserving the architecture and parameters of the segmentor. Experiments on two\nsegmentation datasets and five segmentors (three U-Net-like and two SAM-like)\nillustrate the effectiveness of our proposed method compared to several\nunfairness mitigation methods.",
    "updated" : "2024-03-08T07:22:48Z",
    "published" : "2024-03-08T07:22:48Z",
    "authors" : [
      {
        "name" : "Zikang Xu"
      },
      {
        "name" : "Fenghe Tang"
      },
      {
        "name" : "Quan Quan"
      },
      {
        "name" : "Qingsong Yao"
      },
      {
        "name" : "S. Kevin Zhou"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04867v1",
    "title" : "Group Privacy Amplification and Unified Amplification by Subsampling for\n  Rényi Differential Privacy",
    "summary" : "Differential privacy (DP) has various desirable properties, such as\nrobustness to post-processing, group privacy, and amplification by subsampling,\nwhich can be derived independently of each other. Our goal is to determine\nwhether stronger privacy guarantees can be obtained by considering multiple of\nthese properties jointly. To this end, we focus on the combination of group\nprivacy and amplification by subsampling. To provide guarantees that are\namenable to machine learning algorithms, we conduct our analysis in the\nframework of R\\'enyi-DP, which has more favorable composition properties than\n$(\\epsilon,\\delta)$-DP. As part of this analysis, we develop a unified\nframework for deriving amplification by subsampling guarantees for R\\'enyi-DP,\nwhich represents the first such framework for a privacy accounting method and\nis of independent interest. We find that it not only lets us improve upon and\ngeneralize existing amplification results for R\\'enyi-DP, but also derive\nprovably tight group privacy amplification guarantees stronger than existing\nprinciples. These results establish the joint study of different DP properties\nas a promising research direction.",
    "updated" : "2024-03-07T19:36:05Z",
    "published" : "2024-03-07T19:36:05Z",
    "authors" : [
      {
        "name" : "Jan Schuchardt"
      },
      {
        "name" : "Mihail Stoian"
      },
      {
        "name" : "Arthur Kosmala"
      },
      {
        "name" : "Stephan Günnemann"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02292v2",
    "title" : "A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends",
    "summary" : "We present an analysis of 12 million instances of privacy-relevant reviews\npublicly visible on the Google Play Store that span a 10 year period. By\nleveraging state of the art NLP techniques, we examine what users have been\nwriting about privacy along multiple dimensions: time, countries, app types,\ndiverse privacy topics, and even across a spectrum of emotions. We find\nconsistent growth of privacy-relevant reviews, and explore topics that are\ntrending (such as Data Deletion and Data Theft), as well as those on the\ndecline (such as privacy-relevant reviews on sensitive permissions). We find\nthat although privacy reviews come from more than 200 countries, 33 countries\nprovide 90% of privacy reviews. We conduct a comparison across countries by\nexamining the distribution of privacy topics a country's users write about, and\nfind that geographic proximity is not a reliable indicator that nearby\ncountries have similar privacy perspectives. We uncover some countries with\nunique patterns and explore those herein. Surprisingly, we uncover that it is\nnot uncommon for reviews that discuss privacy to be positive (32%); many users\nexpress pleasure about privacy features within apps or privacy-focused apps. We\nalso uncover some unexpected behaviors, such as the use of reviews to deliver\nprivacy disclaimers to developers. Finally, we demonstrate the value of\nanalyzing app reviews with our approach as a complement to existing methods for\nunderstanding users' perspectives about privacy",
    "updated" : "2024-03-07T22:07:00Z",
    "published" : "2024-03-04T18:21:56Z",
    "authors" : [
      {
        "name" : "Omer Akgul"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Nina Taft"
      },
      {
        "name" : "Michelle L. Mazurek"
      },
      {
        "name" : "Hamza Harkous"
      },
      {
        "name" : "Animesh Srivastava"
      },
      {
        "name" : "Benoit Seguin"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04784v1",
    "title" : "Analysis of Privacy Leakage in Federated Large Language Models",
    "summary" : "With the rapid adoption of Federated Learning (FL) as the training and tuning\nprotocol for applications utilizing Large Language Models (LLMs), recent\nresearch highlights the need for significant modifications to FL to accommodate\nthe large-scale of LLMs. While substantial adjustments to the protocol have\nbeen introduced as a response, comprehensive privacy analysis for the adapted\nFL protocol is currently lacking.\n  To address this gap, our work delves into an extensive examination of the\nprivacy analysis of FL when used for training LLMs, both from theoretical and\npractical perspectives. In particular, we design two active membership\ninference attacks with guaranteed theoretical success rates to assess the\nprivacy leakages of various adapted FL configurations. Our theoretical findings\nare translated into practical attacks, revealing substantial privacy\nvulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and\nOpenAI's GPTs, across multiple real-world language datasets. Additionally, we\nconduct thorough experiments to evaluate the privacy leakage of these models\nwhen data is protected by state-of-the-art differential privacy (DP)\nmechanisms.",
    "updated" : "2024-03-02T20:25:38Z",
    "published" : "2024-03-02T20:25:38Z",
    "authors" : [
      {
        "name" : "Minh N. Vu"
      },
      {
        "name" : "Truc Nguyen"
      },
      {
        "name" : "Tre' R. Jeter"
      },
      {
        "name" : "My T. Thai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04778v1",
    "title" : "An Efficient Difference-of-Convex Solver for Privacy Funnel",
    "summary" : "We propose an efficient solver for the privacy funnel (PF) method, leveraging\nits difference-of-convex (DC) structure. The proposed DC separation results in\na closed-form update equation, which allows straightforward application to both\nknown and unknown distribution settings. For known distribution case, we prove\nthe convergence (local stationary points) of the proposed non-greedy solver,\nand empirically show that it outperforms the state-of-the-art approaches in\ncharacterizing the privacy-utility trade-off. The insights of our DC approach\napply to unknown distribution settings where labeled empirical samples are\navailable instead. Leveraging the insights, our alternating minimization solver\nsatisfies the fundamental Markov relation of PF in contrast to previous\nvariational inference-based solvers. Empirically, we evaluate the proposed\nsolver with MNIST and Fashion-MNIST datasets. Our results show that under a\ncomparable reconstruction quality, an adversary suffers from higher prediction\nerror from clustering our compressed codes than that with the compared methods.\nMost importantly, our solver is independent to private information in inference\nphase contrary to the baselines.",
    "updated" : "2024-03-02T01:05:25Z",
    "published" : "2024-03-02T01:05:25Z",
    "authors" : [
      {
        "name" : "Teng-Hui Huang"
      },
      {
        "name" : "Hesham El Gamal"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.06672v1",
    "title" : "Provable Mutual Benefits from Federated Learning in Privacy-Sensitive\n  Domains",
    "summary" : "Cross-silo federated learning (FL) allows data owners to train accurate\nmachine learning models by benefiting from each others private datasets.\nUnfortunately, the model accuracy benefits of collaboration are often\nundermined by privacy defenses. Therefore, to incentivize client participation\nin privacy-sensitive domains, a FL protocol should strike a delicate balance\nbetween privacy guarantees and end-model accuracy. In this paper, we study the\nquestion of when and how a server could design a FL protocol provably\nbeneficial for all participants. First, we provide necessary and sufficient\nconditions for the existence of mutually beneficial protocols in the context of\nmean estimation and convex stochastic optimization. We also derive protocols\nthat maximize the total clients' utility, given symmetric privacy preferences.\nFinally, we design protocols maximizing end-model accuracy and demonstrate\ntheir benefits in synthetic experiments.",
    "updated" : "2024-03-11T12:43:44Z",
    "published" : "2024-03-11T12:43:44Z",
    "authors" : [
      {
        "name" : "Nikita Tsoy"
      },
      {
        "name" : "Anna Mihalkova"
      },
      {
        "name" : "Teodora Todorova"
      },
      {
        "name" : "Nikola Konstantinov"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.GT",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.06172v1",
    "title" : "Understanding Parents' Perceptions and Practices Toward Children's\n  Security and Privacy in Virtual Reality",
    "summary" : "Recent years have seen a sharp increase in underage users of virtual reality\n(VR), where security and privacy (S\\&P) risks such as data surveillance and\nself-disclosure in social interaction have been increasingly prominent. Prior\nwork shows children largely rely on parents to mitigate S\\&P risks in their\ntechnology use. Therefore, understanding parents' S\\&P knowledge, perceptions,\nand practices is critical for identifying the gaps for parents, technology\ndesigners, and policymakers to enhance children's S\\&P. While such empirical\nknowledge is substantial in other consumer technologies, it remains largely\nunknown in the context of VR. To address the gap, we conducted in-depth\nsemi-structured interviews with 20 parents of children under the age of 18 who\nuse VR at home. Our findings highlight parents generally lack S\\&P awareness\ndue to the perception that VR is still in its infancy. To protect their\nchildren's interaction with VR, parents currently primarily rely on active\nstrategies such as verbal education about S\\&P. Passive strategies such as\nparental controls in VR are not commonly used among our interviewees, mainly\ndue to their perceived technical constraints. Parents also highlight that a\nmulti-stakeholder ecosystem must be established towards more S\\&P support for\nchildren in VR. Based on the findings, we propose actionable S\\&P\nrecommendations for critical stakeholders, including parents, educators, VR\ncompanies, and governments.",
    "updated" : "2024-03-10T10:54:44Z",
    "published" : "2024-03-10T10:54:44Z",
    "authors" : [
      {
        "name" : "Jiaxun Cao"
      },
      {
        "name" : "Abhinaya S B"
      },
      {
        "name" : "Anupam Das"
      },
      {
        "name" : "Pardis Emami-Naeini"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.06131v1",
    "title" : "FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction\n  Tuning",
    "summary" : "Instruction tuning has proven essential for enhancing the performance of\nlarge language models (LLMs) in generating human-aligned responses. However,\ncollecting diverse, high-quality instruction data for tuning poses challenges,\nparticularly in privacy-sensitive domains. Federated instruction tuning (FedIT)\nhas emerged as a solution, leveraging federated learning from multiple data\nowners while preserving privacy. Yet, it faces challenges due to limited\ninstruction data and vulnerabilities to training data extraction attacks. To\naddress these issues, we propose a novel federated algorithm, FedPIT, which\nutilizes LLMs' in-context learning capability to self-generate task-specific\nsynthetic data for training autonomously. Our method employs parameter-isolated\ntraining to maintain global parameters trained on synthetic data and local\nparameters trained on augmented local data, effectively thwarting data\nextraction attacks. Extensive experiments on real-world medical data\ndemonstrate the effectiveness of FedPIT in improving federated few-shot\nperformance while preserving privacy and robustness against data heterogeneity.",
    "updated" : "2024-03-10T08:41:22Z",
    "published" : "2024-03-10T08:41:22Z",
    "authors" : [
      {
        "name" : "Zhuo Zhang"
      },
      {
        "name" : "Jingyuan Zhang"
      },
      {
        "name" : "Jintao Huang"
      },
      {
        "name" : "Lizhen Qu"
      },
      {
        "name" : "Hongzhi Zhang"
      },
      {
        "name" : "Zenglin Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05864v1",
    "title" : "PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for\n  Reinforcement Learning in Human-in-the-Loop Systems",
    "summary" : "Reinforcement Learning (RL) has increasingly become a preferred method over\ntraditional rule-based systems in diverse human-in-the-loop (HITL) applications\ndue to its adaptability to the dynamic nature of human interactions. However,\nintegrating RL in such settings raises significant privacy concerns, as it\nmight inadvertently expose sensitive user information. Addressing this, our\npaper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy\nthrough exploiting an early-exit approach designed explicitly for privacy\npreservation in HITL environments. This approach dynamically adjusts the\ntradeoff between privacy protection and system utility, tailoring its operation\nto individual behavioral patterns and preferences. We mainly highlight the\nchallenge of dealing with the variable and evolving nature of human behavior,\nwhich renders static privacy models ineffective. PAPER-HILT's effectiveness is\nevaluated through its application in two distinct contexts: Smart Home\nenvironments and Virtual Reality (VR) Smart Classrooms. The empirical results\ndemonstrate PAPER-HILT's capability to provide a personalized equilibrium\nbetween user privacy and application utility, adapting effectively to\nindividual user needs and preferences. On average for both experiments, utility\n(performance) drops by 24%, and privacy (state prediction) improves by 31%.",
    "updated" : "2024-03-09T10:24:12Z",
    "published" : "2024-03-09T10:24:12Z",
    "authors" : [
      {
        "name" : "Mojtaba Taherisadr"
      },
      {
        "name" : "Salma Elmalaki"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.HC",
      "F.2.2",
      "I.2.7"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05794v1",
    "title" : "Privacy-Preserving Diffusion Model Using Homomorphic Encryption",
    "summary" : "In this paper, we introduce a privacy-preserving stable diffusion framework\nleveraging homomorphic encryption, called HE-Diffusion, which primarily focuses\non protecting the denoising phase of the diffusion process. HE-Diffusion is a\ntailored encryption framework specifically designed to align with the unique\narchitecture of stable diffusion, ensuring both privacy and functionality. To\naddress the inherent computational challenges, we propose a novel\nmin-distortion method that enables efficient partial image encryption,\nsignificantly reducing the overhead without compromising the model's output\nquality. Furthermore, we adopt a sparse tensor representation to expedite\ncomputational operations, enhancing the overall efficiency of the\nprivacy-preserving diffusion process. We successfully implement HE-based\nprivacy-preserving stable diffusion inference. The experimental results show\nthat HE-Diffusion achieves 500 times speedup compared with the baseline method,\nand reduces time cost of the homomorphically encrypted inference to the minute\nlevel. Both the performance and accuracy of the HE-Diffusion are on par with\nthe plaintext counterpart. Our approach marks a significant step towards\nintegrating advanced cryptographic techniques with state-of-the-art generative\nmodels, paving the way for privacy-preserving and efficient image generation in\ncritical applications.",
    "updated" : "2024-03-09T04:56:57Z",
    "published" : "2024-03-09T04:56:57Z",
    "authors" : [
      {
        "name" : "Yaojian Chen"
      },
      {
        "name" : "Qiben Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05692v1",
    "title" : "Privacy-Preserving Sharing of Data Analytics Runtime Metrics for\n  Performance Modeling",
    "summary" : "Performance modeling for large-scale data analytics workloads can improve the\nefficiency of cluster resource allocations and job scheduling. However, the\nperformance of these workloads is influenced by numerous factors, such as job\ninputs and the assigned cluster resources. As a result, performance models\nrequire significant amounts of training data. This data can be obtained by\nexchanging runtime metrics between collaborating organizations. Yet, not all\norganizations may be inclined to publicly disclose such metadata.\n  We present a privacy-preserving approach for sharing runtime metrics based on\ndifferential privacy and data synthesis. Our evaluation on performance data\nfrom 736 Spark job executions indicates that fully anonymized training data\nlargely maintains performance prediction accuracy, particularly when there is\nminimal original data available. With 30 or fewer available original data\nsamples, the use of synthetic training data resulted only in a one percent\nreduction in performance model accuracy on average.",
    "updated" : "2024-03-08T22:03:21Z",
    "published" : "2024-03-08T22:03:21Z",
    "authors" : [
      {
        "name" : "Jonathan Will"
      },
      {
        "name" : "Dominik Scheinert"
      },
      {
        "name" : "Jan Bode"
      },
      {
        "name" : "Cedric Kring"
      },
      {
        "name" : "Seraphin Zunzer"
      },
      {
        "name" : "Lauritz Thamsen"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05271v2",
    "title" : "DID:RING: Ring Signatures using Decentralised Identifiers For\n  Privacy-Aware Identity",
    "summary" : "Decentralised identifiers have become a standardised element of digital\nidentity architecture, with supra-national organisations such as the European\nUnion adopting them as a key component for a unified European digital identity\nledger. This paper delves into enhancing security and privacy features within\ndecentralised identifiers by integrating ring signatures as an alternative\nverification method. This allows users to identify themselves through digital\nsignatures without revealing which public key they used. To this end, the study\nproposed a novel decentralised identity method showcased in a decentralised\nidentifier-based architectural framework. Additionally, the investigation\nassesses the repercussions of employing this new method in the verification\nprocess, focusing specifically on privacy and security aspects. Although ring\nsignatures are an established asset of cryptographic protocols, this paper\nseeks to leverage their capabilities in the evolving domain of digital\nidentities.",
    "updated" : "2024-03-11T15:20:37Z",
    "published" : "2024-03-08T12:49:39Z",
    "authors" : [
      {
        "name" : "Dimitrios Kasimatis"
      },
      {
        "name" : "Sam Grierson"
      },
      {
        "name" : "William J. Buchanan"
      },
      {
        "name" : "Chris Eckl"
      },
      {
        "name" : "Pavlos Papadopoulos"
      },
      {
        "name" : "Nikolaos Pitropakis"
      },
      {
        "name" : "Craig Thomson"
      },
      {
        "name" : "Baraq Ghaleb"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05598v1",
    "title" : "Privacy Amplification for the Gaussian Mechanism via Bounded Support",
    "summary" : "Data-dependent privacy accounting frameworks such as per-instance\ndifferential privacy (pDP) and Fisher information loss (FIL) confer\nfine-grained privacy guarantees for individuals in a fixed training dataset.\nThese guarantees can be desirable compared to vanilla DP in real world settings\nas they tightly upper-bound the privacy leakage for a $\\textit{specific}$\nindividual in an $\\textit{actual}$ dataset, rather than considering worst-case\ndatasets. While these frameworks are beginning to gain popularity, to date,\nthere is a lack of private mechanisms that can fully leverage advantages of\ndata-dependent accounting. To bridge this gap, we propose simple modifications\nof the Gaussian mechanism with bounded support, showing that they amplify\nprivacy guarantees under data-dependent accounting. Experiments on model\ntraining with DP-SGD show that using bounded support Gaussian mechanisms can\nprovide a reduction of the pDP bound $\\epsilon$ by as much as 30% without\nnegative effects on model utility.",
    "updated" : "2024-03-07T21:22:07Z",
    "published" : "2024-03-07T21:22:07Z",
    "authors" : [
      {
        "name" : "Shengyuan Hu"
      },
      {
        "name" : "Saeed Mahloujifar"
      },
      {
        "name" : "Virginia Smith"
      },
      {
        "name" : "Kamalika Chaudhuri"
      },
      {
        "name" : "Chuan Guo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.07842v1",
    "title" : "Quantifying and Mitigating Privacy Risks for Tabular Generative Models",
    "summary" : "Synthetic data from generative models emerges as the privacy-preserving\ndata-sharing solution. Such a synthetic data set shall resemble the original\ndata without revealing identifiable private information. The backbone\ntechnology of tabular synthesizers is rooted in image generative models,\nranging from Generative Adversarial Networks (GANs) to recent diffusion models.\nRecent prior work sheds light on the utility-privacy tradeoff on tabular data,\nrevealing and quantifying privacy risks on synthetic data. We first conduct an\nexhaustive empirical analysis, highlighting the utility-privacy tradeoff of\nfive state-of-the-art tabular synthesizers, against eight privacy attacks, with\na special focus on membership inference attacks. Motivated by the observation\nof high data quality but also high privacy risk in tabular diffusion, we\npropose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which\nis composed of an autoencoder network to encode the tabular data and a latent\ndiffusion model to synthesize the latent tables. Following the emerging f-DP\nframework, we apply DP-SGD to train the auto-encoder in combination with batch\nclipping and use the separation value as the privacy metric to better capture\nthe privacy gain from DP algorithms. Our empirical evaluation demonstrates that\nDP-TLDM is capable of achieving a meaningful theoretical privacy guarantee\nwhile also significantly enhancing the utility of synthetic data. Specifically,\ncompared to other DP-protected tabular generative models, DP-TLDM improves the\nsynthetic quality by an average of 35% in data resemblance, 15% in the utility\nfor downstream tasks, and 50% in data discriminability, all while preserving a\ncomparable level of privacy risk.",
    "updated" : "2024-03-12T17:27:49Z",
    "published" : "2024-03-12T17:27:49Z",
    "authors" : [
      {
        "name" : "Chaoyi Zhu"
      },
      {
        "name" : "Jiayi Tang"
      },
      {
        "name" : "Hans Brouwer"
      },
      {
        "name" : "Juan F. Pérez"
      },
      {
        "name" : "Marten van Dijk"
      },
      {
        "name" : "Lydia Y. Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.07838v1",
    "title" : "MPCPA: Multi-Center Privacy Computing with Predictions Aggregation based\n  on Denoising Diffusion Probabilistic Model",
    "summary" : "Privacy-preserving computing is crucial for multi-center machine learning in\nmany applications such as healthcare and finance. In this paper a Multi-center\nPrivacy Computing framework with Predictions Aggregation (MPCPA) based on\ndenoising diffusion probabilistic model (DDPM) is proposed, in which\nconditional diffusion model training, DDPM data generation, a classifier, and\nstrategy of prediction aggregation are included. Compared to federated\nlearning, this framework necessitates fewer communications and leverages\nhigh-quality generated data to support robust privacy computing. Experimental\nvalidation across multiple datasets demonstrates that the proposed framework\noutperforms classic federated learning and approaches the performance of\ncentralized learning with original data. Moreover, our approach demonstrates\nrobust security, effectively addressing challenges such as image memorization\nand membership inference attacks. Our experiments underscore the efficacy of\nthe proposed framework in the realm of privacy computing, with the code set to\nbe released soon.",
    "updated" : "2024-03-12T17:21:46Z",
    "published" : "2024-03-12T17:21:46Z",
    "authors" : [
      {
        "name" : "Guibo Luo"
      },
      {
        "name" : "Hanwen Zhang"
      },
      {
        "name" : "Xiuling Wang"
      },
      {
        "name" : "Mingzhi Chen"
      },
      {
        "name" : "Yuesheng Zhu"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.07817v1",
    "title" : "UniHand: Privacy-preserving Universal Handover for Small-Cell Networks\n  in 5G-enabled Mobile Communication with KCI Resilience",
    "summary" : "Introducing Small Cell Networks (SCN) has significantly improved wireless\nlink quality, spectrum efficiency and network capacity, which has been viewed\nas one of the key technologies in the fifth-generation (5G) mobile network.\nHowever, this technology increases the frequency of handover (HO) procedures\ncaused by the dense deployment of cells in the network with reduced cell\ncoverage, bringing new security and privacy issues. The current 5G-AKA and HO\nprotocols are vulnerable to security weaknesses, such as the lack of forward\nsecrecy and identity confusion attacks. The high HO frequency of HOs might\nmagnify these security and privacy concerns in the 5G mobile network. This work\naddresses these issues by proposing a secure privacy-preserving universal HO\nscheme ($\\UniHand$) for SCNs in 5G mobile communication. $\\UniHand$ can achieve\nmutual authentication, strong anonymity, perfect forward secrecy,\nkey-escrow-free and key compromise impersonation (KCI) resilience. To the best\nof our knowledge, this is the \\textit{first} scheme to achieve secure,\nprivacy-preserving universal HO with \\textit{KCI} resilience for roaming users\nin 5G environment. We demonstrate that our proposed scheme is resilient against\nall the essential security threats by performing a comprehensive formal\nsecurity analysis and conducting relevant experiments to show the\ncost-effectiveness of the proposed scheme.",
    "updated" : "2024-03-12T16:56:31Z",
    "published" : "2024-03-12T16:56:31Z",
    "authors" : [
      {
        "name" : "Rabiah Alnashwan"
      },
      {
        "name" : "Prosanta Gope"
      },
      {
        "name" : "Benjamin Dowling"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.07772v1",
    "title" : "Privacy Guarantees in Posterior Sampling under Contamination",
    "summary" : "In recent years, differential privacy has been adopted by tech-companies and\ngovernmental agencies as the standard for measuring privacy in algorithms. We\nstudy the level of differential privacy in Bayesian posterior sampling setups.\nAs opposed to the common privatization approach of injecting Laplace/Gaussian\nnoise into the output, Huber's contamination model is considered, where we\nreplace at random the data points with samples from a heavy-tailed\ndistribution. We derived bounds for the differential privacy level\n$(\\epsilon,\\delta)$ for our approach while lifting the common restriction on\nassuming bounded observation and parameter space seen in the existing\nliterature. We further consider the effect of sample size on privacy level and\nthe convergence rate of $(\\epsilon,\\delta)$ to zero. Asymptotically, the\ncontamination approach is fully private at no cost of information loss. We also\nprovide some examples depicting inference models that our setup is applicable\nto with a theoretical estimation of convergence rate.",
    "updated" : "2024-03-12T15:58:53Z",
    "published" : "2024-03-12T15:58:53Z",
    "authors" : [
      {
        "name" : "Shenggang Hu"
      },
      {
        "name" : "Louis Aslett"
      },
      {
        "name" : "Hongsheng Dai"
      },
      {
        "name" : "Murray Pollock"
      },
      {
        "name" : "Gareth O. Roberts"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.TH",
      "62F15, 62J12"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.07588v1",
    "title" : "Visual Privacy Auditing with Diffusion Models",
    "summary" : "Image reconstruction attacks on machine learning models pose a significant\nrisk to privacy by potentially leaking sensitive information. Although\ndefending against such attacks using differential privacy (DP) has proven\neffective, determining appropriate DP parameters remains challenging. Current\nformal guarantees on data reconstruction success suffer from overly theoretical\nassumptions regarding adversary knowledge about the target data, particularly\nin the image domain. In this work, we empirically investigate this discrepancy\nand find that the practicality of these assumptions strongly depends on the\ndomain shift between the data prior and the reconstruction target. We propose a\nreconstruction attack based on diffusion models (DMs) that assumes adversary\naccess to real-world image priors and assess its implications on privacy\nleakage under DP-SGD. We show that (1) real-world data priors significantly\ninfluence reconstruction success, (2) current reconstruction bounds do not\nmodel the risk posed by data priors well, and (3) DMs can serve as effective\nauditing tools for visualizing privacy leakage.",
    "updated" : "2024-03-12T12:18:55Z",
    "published" : "2024-03-12T12:18:55Z",
    "authors" : [
      {
        "name" : "Kristian Schwethelm"
      },
      {
        "name" : "Johannes Kaiser"
      },
      {
        "name" : "Moritz Knolle"
      },
      {
        "name" : "Daniel Rueckert"
      },
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Alexander Ziller"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.07506v1",
    "title" : "Robustness, Security, Privacy, Explainability, Efficiency, and Usability\n  of Large Language Models for Code",
    "summary" : "Large language models for code (LLM4Code), which demonstrate strong\nperformance (e.g., high accuracy) in processing source code, have significantly\ntransformed software engineering. Many studies separately investigate the\nnon-functional properties of LM4Code, but there is no systematic review of how\nthese properties are evaluated and enhanced. This paper fills this gap by\nthoroughly examining 146 relevant studies, thereby presenting the first\nsystematic literature review to identify seven important properties beyond\naccuracy, including robustness, security, privacy, explainability, efficiency,\nand usability. We discuss the current state-of-the-art methods and trends,\nidentify gaps in existing research, and present promising directions for future\nstudy.",
    "updated" : "2024-03-12T10:43:26Z",
    "published" : "2024-03-12T10:43:26Z",
    "authors" : [
      {
        "name" : "Zhou Yang"
      },
      {
        "name" : "Zhensu Sun"
      },
      {
        "name" : "Terry Zhuo Yue"
      },
      {
        "name" : "Premkumar Devanbu"
      },
      {
        "name" : "David Lo"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.07288v1",
    "title" : "Efficient and Model-Agnostic Parameter Estimation Under\n  Privacy-Preserving Post-randomization Data",
    "summary" : "Protecting individual privacy is crucial when releasing sensitive data for\npublic use. While data de-identification helps, it is not enough. This paper\naddresses parameter estimation in scenarios where data are perturbed using the\nPost-Randomization Method (PRAM) to enhance privacy. Existing methods for\nparameter estimation under PRAM data suffer from limitations like being\nparameter-specific, model-dependent, and lacking efficiency guarantees. We\npropose a novel, efficient method that overcomes these limitations. Our method\nis applicable to general parameters defined through estimating equations and\nmakes no assumptions about the underlying data model. We further prove that the\nproposed estimator achieves the semiparametric efficiency bound, making it\noptimal in terms of asymptotic variance.",
    "updated" : "2024-03-12T03:41:34Z",
    "published" : "2024-03-12T03:41:34Z",
    "authors" : [
      {
        "name" : "Qinglong Tian"
      },
      {
        "name" : "Jiwei Zhao"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.07218v1",
    "title" : "SoK: Can Trajectory Generation Combine Privacy and Utility?",
    "summary" : "While location trajectories represent a valuable data source for analyses and\nlocation-based services, they can reveal sensitive information, such as\npolitical and religious preferences. Differentially private publication\nmechanisms have been proposed to allow for analyses under rigorous privacy\nguarantees. However, the traditional protection schemes suffer from a limiting\nprivacy-utility trade-off and are vulnerable to correlation and reconstruction\nattacks. Synthetic trajectory data generation and release represent a promising\nalternative to protection algorithms. While initial proposals achieve\nremarkable utility, they fail to provide rigorous privacy guarantees. This\npaper proposes a framework for designing a privacy-preserving trajectory\npublication approach by defining five design goals, particularly stressing the\nimportance of choosing an appropriate Unit of Privacy. Based on this framework,\nwe briefly discuss the existing trajectory protection approaches, emphasising\ntheir shortcomings. This work focuses on the systematisation of the\nstate-of-the-art generative models for trajectories in the context of the\nproposed framework. We find that no existing solution satisfies all\nrequirements. Thus, we perform an experimental study evaluating the\napplicability of six sequential generative models to the trajectory domain.\nFinally, we conclude that a generative trajectory model providing semantic\nguarantees remains an open research question and propose concrete next steps\nfor future research.",
    "updated" : "2024-03-12T00:25:14Z",
    "published" : "2024-03-12T00:25:14Z",
    "authors" : [
      {
        "name" : "Erik Buchholz"
      },
      {
        "name" : "Alsharif Abuadbba"
      },
      {
        "name" : "Shuo Wang"
      },
      {
        "name" : "Surya Nepal"
      },
      {
        "name" : "Salil S. Kanhere"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.01438v2",
    "title" : "Privacy-Preserving Collaborative Split Learning Framework for Smart Grid\n  Load Forecasting",
    "summary" : "Accurate load forecasting is crucial for energy management, infrastructure\nplanning, and demand-supply balancing. Smart meter data availability has led to\nthe demand for sensor-based load forecasting. Conventional ML allows training a\nsingle global model using data from multiple smart meters requiring data\ntransfer to a central server, raising concerns for network requirements,\nprivacy, and security. We propose a split learning-based framework for load\nforecasting to alleviate this issue. We split a deep neural network model into\ntwo parts, one for each Grid Station (GS) responsible for an entire\nneighbourhood's smart meters and the other for the Service Provider (SP).\nInstead of sharing their data, client smart meters use their respective GSs'\nmodel split for forward pass and only share their activations with the GS.\nUnder this framework, each GS is responsible for training a personalized model\nsplit for their respective neighbourhoods, whereas the SP can train a single\nglobal or personalized model for each GS. Experiments show that the proposed\nmodels match or exceed a centrally trained model's performance and generalize\nwell. Privacy is analyzed by assessing information leakage between data and\nshared activations of the GS model split. Additionally, differential privacy\nenhances local data privacy while examining its impact on performance. A\ntransformer model is used as our base learner.",
    "updated" : "2024-03-12T05:37:07Z",
    "published" : "2024-03-03T08:24:39Z",
    "authors" : [
      {
        "name" : "Asif Iqbal"
      },
      {
        "name" : "Prosanta Gope"
      },
      {
        "name" : "Biplab Sikdar"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.04778v2",
    "title" : "An Efficient Difference-of-Convex Solver for Privacy Funnel",
    "summary" : "We propose an efficient solver for the privacy funnel (PF) method, leveraging\nits difference-of-convex (DC) structure. The proposed DC separation results in\na closed-form update equation, which allows straightforward application to both\nknown and unknown distribution settings. For known distribution case, we prove\nthe convergence (local stationary points) of the proposed non-greedy solver,\nand empirically show that it outperforms the state-of-the-art approaches in\ncharacterizing the privacy-utility trade-off. The insights of our DC approach\napply to unknown distribution settings where labeled empirical samples are\navailable instead. Leveraging the insights, our alternating minimization solver\nsatisfies the fundamental Markov relation of PF in contrast to previous\nvariational inference-based solvers. Empirically, we evaluate the proposed\nsolver with MNIST and Fashion-MNIST datasets. Our results show that under a\ncomparable reconstruction quality, an adversary suffers from higher prediction\nerror from clustering our compressed codes than that with the compared methods.\nMost importantly, our solver is independent to private information in inference\nphase contrary to the baselines.",
    "updated" : "2024-03-12T13:47:52Z",
    "published" : "2024-03-02T01:05:25Z",
    "authors" : [
      {
        "name" : "Teng-Hui Huang"
      },
      {
        "name" : "Hesham El Gamal"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.08624v1",
    "title" : "Towards a Privacy and Security-Aware Framework for Ethical AI: Guiding\n  the Development and Assessment of AI Systems",
    "summary" : "As artificial intelligence continues its unprecedented global expansion,\naccompanied by a proliferation of benefits, an increasing apprehension about\nthe privacy and security implications of AI-enabled systems emerges. The\npivotal question of effectively controlling AI development at both\njurisdictional and organizational levels has become a prominent theme in\ncontemporary discourse. While the European Parliament and Council have taken a\ndecisive step by reaching a political agreement on the EU AI Act, the first\ncomprehensive AI law, organizations still find it challenging to adapt to the\nfast-evolving AI landscape, lacking a universal tool for evaluating the privacy\nand security dimensions of their AI models and systems. In response to this\ncritical challenge, this study conducts a systematic literature review spanning\nthe years 2020 to 2023, with a primary focus on establishing a unified\ndefinition of key concepts in AI Ethics, particularly emphasizing the domains\nof privacy and security. Through the synthesis of knowledge extracted from the\nSLR, this study presents a conceptual framework tailored for privacy- and\nsecurity-aware AI systems. This framework is designed to assist diverse\nstakeholders, including organizations, academic institutions, and governmental\nbodies, in both the development and critical assessment of AI systems.\nEssentially, the proposed framework serves as a guide for ethical\ndecision-making, fostering an environment wherein AI is developed and utilized\nwith a strong commitment to ethical principles. In addition, the study unravels\nthe key issues and challenges surrounding the privacy and security dimensions,\ndelineating promising avenues for future research, thereby contributing to the\nongoing dialogue on the globalization and democratization of AI ethics.",
    "updated" : "2024-03-13T15:39:57Z",
    "published" : "2024-03-13T15:39:57Z",
    "authors" : [
      {
        "name" : "Daria Korobenko"
      },
      {
        "name" : "Anastasija Nikiforova"
      },
      {
        "name" : "Rajesh Sharma"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.08507v1",
    "title" : "MobileAtlas: Geographically Decoupled Measurements in Cellular Networks\n  for Security and Privacy Research",
    "summary" : "Cellular networks are not merely data access networks to the Internet. Their\ndistinct services and ability to form large complex compounds for roaming\npurposes make them an attractive research target in their own right. Their\npromise of providing a consistent service with comparable privacy and security\nacross roaming partners falls apart at close inspection.\n  Thus, there is a need for controlled testbeds and measurement tools for\ncellular access networks doing justice to the technology's unique structure and\nglobal scope. Particularly, such measurements suffer from a combinatorial\nexplosion of operators, mobile plans, and services. To cope with these\nchallenges, we built a framework that geographically decouples the SIM from the\ncellular modem by selectively connecting both remotely. This allows testing any\nsubscriber with any operator at any modem location within minutes without\nmoving parts. The resulting GSM/UMTS/LTE measurement and testbed platform\noffers a controlled experimentation environment, which is scalable and\ncost-effective. The platform is extensible and fully open-sourced, allowing\nother researchers to contribute locations, SIM cards, and measurement scripts.\n  Using the above framework, our international experiments in commercial\nnetworks revealed exploitable inconsistencies in traffic metering, leading to\nmultiple phreaking opportunities, i.e., fare-dodging. We also expose\nproblematic IPv6 firewall configurations, hidden SIM card communication to the\nhome network, and fingerprint dial progress tones to track victims across\ndifferent roaming networks and countries with voice calls.",
    "updated" : "2024-03-13T13:15:13Z",
    "published" : "2024-03-13T13:15:13Z",
    "authors" : [
      {
        "name" : "Gabriel Karl Gegenhuber"
      },
      {
        "name" : "Wilfried Mayer"
      },
      {
        "name" : "Edgar Weippl"
      },
      {
        "name" : "Adrian Dabrowski"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.08181v1",
    "title" : "Differential Privacy in Nonlinear Dynamical Systems with Tracking\n  Performance Guarantees",
    "summary" : "We introduce a novel approach to make the tracking error of a class of\nnonlinear systems differentially private in addition to guaranteeing the\ntracking error performance. We use funnel control to make the tracking error\nevolve within a performance funnel that is pre-specified by the user. We make\nthe performance funnel differentially private by adding a bounded continuous\nnoise generated from an Ornstein-Uhlenbeck-type process. Since the funnel\ncontroller is a function of the performance funnel, the noise adds randomized\nperturbation to the control input. We show that, as a consequence of the\ndifferential privacy of the performance funnel, the tracking error is also\ndifferentially private. As a result, the tracking error is bounded by the noisy\nfunnel boundary while maintaining privacy. We show a simulation result to\ndemonstrate the framework.",
    "updated" : "2024-03-13T02:10:46Z",
    "published" : "2024-03-13T02:10:46Z",
    "authors" : [
      {
        "name" : "Dhrubajit Chowdhury"
      },
      {
        "name" : "Raman Goyal"
      },
      {
        "name" : "Shantanu Rane"
      }
    ],
    "categories" : [
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.08115v1",
    "title" : "Legally Binding but Unfair? Towards Assessing Fairness of Privacy\n  Policies",
    "summary" : "Privacy policies are expected to inform data subjects about their data\nprotection rights. They should explain the data controller's data management\npractices, and make facts such as retention periods or data transfers to third\nparties transparent. Privacy policies only fulfill their purpose, if they are\ncorrectly perceived, interpreted, understood, and trusted by the data subject.\nAmongst others, this requires that a privacy policy is written in a fair way,\ne.g., it does not use polarizing terms, does not require a certain education,\nor does not assume a particular social background. In this work-in-progress\npaper, we outline our approach to assessing fairness in privacy policies. To\nthis end, we identify from fundamental legal sources and fairness research, how\nthe dimensions informational fairness, representational fairness and\nethics/morality are related to privacy policies. We propose options to\nautomatically assess policies in these fairness dimensions, based on text\nstatistics, linguistic methods and artificial intelligence. Finally, we conduct\ninitial experiments with German privacy policies to provide evidence that our\napproach is applicable. Our experiments indicate that there are indeed issues\nin all three dimensions of fairness. For example, our approach finds out if a\npolicy discriminates against individuals with impaired reading skills or\ncertain demographics, and identifies questionable ethics. This is important, as\nfuture privacy policies may be used in a corpus for legal artificial\nintelligence models.",
    "updated" : "2024-03-12T22:53:32Z",
    "published" : "2024-03-12T22:53:32Z",
    "authors" : [
      {
        "name" : "Vincent Freiberger"
      },
      {
        "name" : "Erik Buchmann"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "K.4.m"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05692v2",
    "title" : "Privacy-Preserving Sharing of Data Analytics Runtime Metrics for\n  Performance Modeling",
    "summary" : "Performance modeling for large-scale data analytics workloads can improve the\nefficiency of cluster resource allocations and job scheduling. However, the\nperformance of these workloads is influenced by numerous factors, such as job\ninputs and the assigned cluster resources. As a result, performance models\nrequire significant amounts of training data. This data can be obtained by\nexchanging runtime metrics between collaborating organizations. Yet, not all\norganizations may be inclined to publicly disclose such metadata.\n  We present a privacy-preserving approach for sharing runtime metrics based on\ndifferential privacy and data synthesis. Our evaluation on performance data\nfrom 736 Spark job executions indicates that fully anonymized training data\nlargely maintains performance prediction accuracy, particularly when there is\nminimal original data available. With 30 or fewer available original data\nsamples, the use of synthetic training data resulted only in a one percent\nreduction in performance model accuracy on average.",
    "updated" : "2024-03-13T15:39:45Z",
    "published" : "2024-03-08T22:03:21Z",
    "authors" : [
      {
        "name" : "Jonathan Will"
      },
      {
        "name" : "Dominik Scheinert"
      },
      {
        "name" : "Jan Bode"
      },
      {
        "name" : "Cedric Kring"
      },
      {
        "name" : "Seraphin Zunzer"
      },
      {
        "name" : "Lauritz Thamsen"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.09562v1",
    "title" : "PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy\n  Traps",
    "summary" : "The pre-training and fine-tuning paradigm has demonstrated its effectiveness\nand has become the standard approach for tailoring language models to various\ntasks. Currently, community-based platforms offer easy access to various\npre-trained models, as anyone can publish without strict validation processes.\nHowever, a released pre-trained model can be a privacy trap for fine-tuning\ndatasets if it is carefully designed. In this work, we propose PreCurious\nframework to reveal the new attack surface where the attacker releases the\npre-trained model and gets a black-box access to the final fine-tuned model.\nPreCurious aims to escalate the general privacy risk of both membership\ninference and data extraction. The key intuition behind PreCurious is to\nmanipulate the memorization stage of the pre-trained model and guide\nfine-tuning with a seemingly legitimate configuration. The effectiveness of\ndefending against privacy attacks on a fine-tuned model seems promising, as\nempirical and theoretical evidence suggests that parameter-efficient and\ndifferentially private fine-tuning techniques are invulnerable to privacy\nattacks. But PreCurious demonstrates the possibility of breaking up\ninvulnerability in a stealthy manner compared to fine-tuning on a benign model.\nBy further leveraging a sanitized dataset, PreCurious can extract originally\nunexposed secrets under differentially private fine-tuning. Thus, PreCurious\nraises warnings for users who download pre-trained models from unknown sources,\nrely solely on tutorials or common-sense defenses, and previously release\nsanitized datasets even after perfect scrubbing.",
    "updated" : "2024-03-14T16:54:17Z",
    "published" : "2024-03-14T16:54:17Z",
    "authors" : [
      {
        "name" : "Ruixuan Liu"
      },
      {
        "name" : "Tianhao Wang"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Li Xiong"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.09450v1",
    "title" : "Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative\n  Privacy Risk",
    "summary" : "While diffusion models have recently demonstrated remarkable progress in\ngenerating realistic images, privacy risks also arise: published models or APIs\ncould generate training images and thus leak privacy-sensitive training\ninformation. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that\nfine-tuning the pre-trained models with manipulated data can amplify the\nexisting privacy risks. We demonstrate that S2L could occur in various standard\nfine-tuning strategies for diffusion models, including concept-injection\nmethods (DreamBooth and Textual Inversion) and parameter-efficient methods\n(LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L\ncan amplify the state-of-the-art membership inference attack (MIA) on diffusion\nmodels by $5.4\\%$ (absolute difference) AUC and can increase extracted private\nsamples from almost $0$ samples to $16.3$ samples on average per target domain.\nThis discovery underscores that the privacy risk with diffusion models is even\nmore severe than previously recognized. Codes are available at\nhttps://github.com/VITA-Group/Shake-to-Leak.",
    "updated" : "2024-03-14T14:48:37Z",
    "published" : "2024-03-14T14:48:37Z",
    "authors" : [
      {
        "name" : "Zhangheng Li"
      },
      {
        "name" : "Junyuan Hong"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Zhangyang Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.09351v1",
    "title" : "LDPRecover: Recovering Frequencies from Poisoning Attacks against Local\n  Differential Privacy",
    "summary" : "Local differential privacy (LDP), which enables an untrusted server to\ncollect aggregated statistics from distributed users while protecting the\nprivacy of those users, has been widely deployed in practice. However, LDP\nprotocols for frequency estimation are vulnerable to poisoning attacks, in\nwhich an attacker can poison the aggregated frequencies by manipulating the\ndata sent from malicious users. Therefore, it is an open challenge to recover\nthe accurate aggregated frequencies from poisoned ones.\n  In this work, we propose LDPRecover, a method that can recover accurate\naggregated frequencies from poisoning attacks, even if the server does not\nlearn the details of the attacks. In LDPRecover, we establish a genuine\nfrequency estimator that theoretically guides the server to recover the\nfrequencies aggregated from genuine users' data by eliminating the impact of\nmalicious users' data in poisoned frequencies. Since the server has no idea of\nthe attacks, we propose an adaptive attack to unify existing attacks and learn\nthe statistics of the malicious data within this adaptive attack by exploiting\nthe properties of LDP protocols. By taking the estimator and the learning\nstatistics as constraints, we formulate the problem of recovering aggregated\nfrequencies to approach the genuine ones as a constraint inference (CI)\nproblem. Consequently, the server can obtain accurate aggregated frequencies by\nsolving this problem optimally. Moreover, LDPRecover can serve as a frequency\nrecovery paradigm that recovers more accurate aggregated frequencies by\nintegrating attack details as new constraints in the CI problem. Our evaluation\non two real-world datasets, three LDP protocols, and untargeted and targeted\npoisoning attacks shows that LDPRecover is both accurate and widely applicable\nagainst various poisoning attacks.",
    "updated" : "2024-03-14T12:57:20Z",
    "published" : "2024-03-14T12:57:20Z",
    "authors" : [
      {
        "name" : "Xinyue Sun"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Haibo Hu"
      },
      {
        "name" : "Jiawei Duan"
      },
      {
        "name" : "Tianyu Wo"
      },
      {
        "name" : "Jie Xu"
      },
      {
        "name" : "Renyu Yang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.09322v1",
    "title" : "Privacy Preserving Anomaly Detection on Homomorphic Encrypted Data from\n  IoT Sensors",
    "summary" : "IoT devices have become indispensable components of our lives, and the\nadvancement of AI technologies will make them even more pervasive, increasing\nthe vulnerability to malfunctions or cyberattacks and raising privacy concerns.\nEncryption can mitigate these challenges; however, most existing anomaly\ndetection techniques decrypt the data to perform the analysis, potentially\nundermining the encryption protection provided during transit or storage.\nHomomorphic encryption schemes are promising solutions as they enable the\nprocessing and execution of operations on IoT data while still encrypted,\nhowever, these schemes offer only limited operations, which poses challenges to\ntheir practical usage. In this paper, we propose a novel privacy-preserving\nanomaly detection solution designed for homomorphically encrypted data\ngenerated by IoT devices that efficiently detects abnormal values without\nperforming decryption. We have adapted the Histogram-based anomaly detection\ntechnique for TFHE scheme to address limitations related to the input size and\nthe depth of computation by implementing vectorized support operations. These\noperations include addition, value placement in buckets, labeling abnormal\nbuckets based on a threshold frequency, labeling abnormal values based on their\nrange, and bucket labels. Evaluation results show that the solution effectively\ndetects anomalies without requiring data decryption and achieves consistent\nresults comparable to the mechanism operating on plain data. Also, it shows\nrobustness and resilience against various challenges commonly encountered in\nIoT environments, such as noisy sensor data, adversarial attacks, communication\nfailures, and device malfunctions. Moreover, the time and computational\noverheads determined for several solution configurations, despite being large,\nare reasonable compared to those reported in existing literature.",
    "updated" : "2024-03-14T12:11:25Z",
    "published" : "2024-03-14T12:11:25Z",
    "authors" : [
      {
        "name" : "Anca Hangan"
      },
      {
        "name" : "Dragos Lazea"
      },
      {
        "name" : "Tudor Cioara"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.09173v1",
    "title" : "Bridging Quantum Computing and Differential Privacy: A Survey on Quantum\n  Computing Privacy",
    "summary" : "Quantum computing has attracted significant attention in areas such as\ncryptography, cybersecurity, and drug discovery. Due to the advantage of\nparallel processing, quantum computing can speed up the response to complex\nchallenges and the processing of large-scale datasets. However, since quantum\ncomputing usually requires sensitive datasets, privacy breaches have become a\nvital concern. Differential privacy (DP) is a promising privacy-preserving\nmethod in classical computing and has been extended to the quantum domain in\nrecent years. In this survey, we categorize the existing literature based on\nwhether internal inherent noise or external artificial noise is used as a\nsource to achieve DP in quantum computing. We explore how these approaches are\napplied at different stages of a quantum algorithm (i.e., state preparation,\nquantum circuit, and quantum measurement). We also discuss challenges and\nfuture directions for DP in quantum computing. By summarizing recent\nadvancements, we hope to provide a comprehensive, up-to-date overview for\nresearchers venturing into this field.",
    "updated" : "2024-03-14T08:40:30Z",
    "published" : "2024-03-14T08:40:30Z",
    "authors" : [
      {
        "name" : "Yusheng Zhao"
      },
      {
        "name" : "Hui Zhong"
      },
      {
        "name" : "Xinyue Zhang"
      },
      {
        "name" : "Chi Zhang"
      },
      {
        "name" : "Miao Pan"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.09172v1",
    "title" : "SHAN: Object-Level Privacy Detection via Inference on Scene\n  Heterogeneous Graph",
    "summary" : "With the rise of social platforms, protecting privacy has become an important\nissue. Privacy object detection aims to accurately locate private objects in\nimages. It is the foundation of safeguarding individuals' privacy rights and\nensuring responsible data handling practices in the digital age. Since privacy\nof object is not shift-invariant, the essence of the privacy object detection\ntask is inferring object privacy based on scene information. However, privacy\nobject detection has long been studied as a subproblem of common object\ndetection tasks. Therefore, existing methods suffer from serious deficiencies\nin accuracy, generalization, and interpretability. Moreover, creating\nlarge-scale privacy datasets is difficult due to legal constraints and existing\nprivacy datasets lack label granularity. The granularity of existing privacy\ndetection methods remains limited to the image level. To address the above two\nissues, we introduce two benchmark datasets for object-level privacy detection\nand propose SHAN, Scene Heterogeneous graph Attention Network, a model\nconstructs a scene heterogeneous graph from an image and utilizes\nself-attention mechanisms for scene inference to obtain object privacy. Through\nexperiments, we demonstrated that SHAN performs excellently in privacy object\ndetection tasks, with all metrics surpassing those of the baseline model.",
    "updated" : "2024-03-14T08:32:14Z",
    "published" : "2024-03-14T08:32:14Z",
    "authors" : [
      {
        "name" : "Zhuohang Jiang"
      },
      {
        "name" : "Bingkui Tong"
      },
      {
        "name" : "Xia Du"
      },
      {
        "name" : "Ahmed Alhammadi"
      },
      {
        "name" : "Jizhe Zhou"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.05156v2",
    "title" : "On Protecting the Data Privacy of Large Language Models (LLMs): A Survey",
    "summary" : "Large language models (LLMs) are complex artificial intelligence systems\ncapable of understanding, generating and translating human language. They learn\nlanguage patterns by analyzing large amounts of text data, allowing them to\nperform writing, conversation, summarizing and other language tasks. When LLMs\nprocess and generate large amounts of data, there is a risk of leaking\nsensitive information, which may threaten data privacy. This paper concentrates\non elucidating the data privacy concerns associated with LLMs to foster a\ncomprehensive understanding. Specifically, a thorough investigation is\nundertaken to delineate the spectrum of data privacy threats, encompassing both\npassive privacy leakage and active privacy attacks within LLMs. Subsequently,\nwe conduct an assessment of the privacy protection mechanisms employed by LLMs\nat various stages, followed by a detailed examination of their efficacy and\nconstraints. Finally, the discourse extends to delineate the challenges\nencountered and outline prospective directions for advancement in the realm of\nLLM privacy protection.",
    "updated" : "2024-03-14T14:17:57Z",
    "published" : "2024-03-08T08:47:48Z",
    "authors" : [
      {
        "name" : "Biwei Yan"
      },
      {
        "name" : "Kun Li"
      },
      {
        "name" : "Minghui Xu"
      },
      {
        "name" : "Yueyan Dong"
      },
      {
        "name" : "Yue Zhang"
      },
      {
        "name" : "Zhaochun Ren"
      },
      {
        "name" : "Xiuzhen Cheng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.10408v1",
    "title" : "SocialGenPod: Privacy-Friendly Generative AI Social Web Applications\n  with Decentralised Personal Data Stores",
    "summary" : "We present SocialGenPod, a decentralised and privacy-friendly way of\ndeploying generative AI Web applications. Unlike centralised Web and data\narchitectures that keep user data tied to application and service providers, we\nshow how one can use Solid -- a decentralised Web specification -- to decouple\nuser data from generative AI applications. We demonstrate SocialGenPod using a\nprototype that allows users to converse with different Large Language Models,\noptionally leveraging Retrieval Augmented Generation to generate answers\ngrounded in private documents stored in any Solid Pod that the user is allowed\nto access, directly or indirectly. SocialGenPod makes use of Solid access\ncontrol mechanisms to give users full control of determining who has access to\ndata stored in their Pods. SocialGenPod keeps all user data (chat history, app\nconfiguration, personal documents, etc) securely in the user's personal Pod;\nseparate from specific model or application providers. Besides better privacy\ncontrols, this approach also enables portability across different services and\napplications. Finally, we discuss challenges, posed by the large compute\nrequirements of state-of-the-art models, that future research in this area\nshould address. Our prototype is open-source and available at:\nhttps://github.com/Vidminas/socialgenpod/.",
    "updated" : "2024-03-15T15:43:02Z",
    "published" : "2024-03-15T15:43:02Z",
    "authors" : [
      {
        "name" : "Vidminas Vizgirda"
      },
      {
        "name" : "Rui Zhao"
      },
      {
        "name" : "Naman Goel"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.IR",
      "cs.LG",
      "cs.SI",
      "H.3.4; H.3.5; C.2.4; I.2.1; K.8.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.10307v1",
    "title" : "Chernoff Information as a Privacy Constraint for Adversarial\n  Classification",
    "summary" : "This work studies a privacy metric based on Chernoff information,\n\\textit{Chernoff differential privacy}, due to its significance in\ncharacterization of classifier performance. Adversarial classification, as any\nother classification problem is built around minimization of the (average or\ncorrect detection) probability of error in deciding on either of the classes in\nthe case of binary classification. Unlike the classical hypothesis testing\nproblem, where the false alarm and mis-detection probabilities are handled\nseparately resulting in an asymmetric behavior of the best error exponent, in\nthis work, we focus on the Bayesian setting and characterize the relationship\nbetween the best error exponent of the average error probability and\n$\\varepsilon-$differential privacy. Accordingly, we re-derive Chernoff\ndifferential privacy in terms of $\\varepsilon-$differential privacy using the\nRadon-Nikodym derivative and show that it satisfies the composition property.\nSubsequently, we present numerical evaluation results, which demonstrates that\nChernoff information outperforms Kullback-Leibler divergence as a function of\nthe privacy parameter $\\varepsilon$, the impact of the adversary's attack and\nglobal sensitivity for the problem of adversarial classification in Laplace\nmechanisms.",
    "updated" : "2024-03-15T13:47:44Z",
    "published" : "2024-03-15T13:47:44Z",
    "authors" : [
      {
        "name" : "Ayşe Ünsal"
      },
      {
        "name" : "Melek Önen"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.10116v1",
    "title" : "Instance-optimal Clipping for Summation Problems in the Shuffle Model of\n  Differential Privacy",
    "summary" : "Differentially private mechanisms achieving worst-case optimal error bounds\n(e.g., the classical Laplace mechanism) are well-studied in the literature.\nHowever, when typical data are far from the worst case,\n\\emph{instance-specific} error bounds -- which depend on the largest value in\nthe dataset -- are more meaningful. For example, consider the sum estimation\nproblem, where each user has an integer $x_i$ from the domain $\\{0,1,\\dots,U\\}$\nand we wish to estimate $\\sum_i x_i$. This has a worst-case optimal error of\n$O(U/\\varepsilon)$, while recent work has shown that the clipping mechanism can\nachieve an instance-optimal error of $O(\\max_i x_i \\cdot \\log\\log U\n/\\varepsilon)$. Under the shuffle model, known instance-optimal protocols are\nless communication-efficient. The clipping mechanism also works in the shuffle\nmodel, but requires two rounds: Round one finds the clipping threshold, and\nround two does the clipping and computes the noisy sum of the clipped data. In\nthis paper, we show how these two seemingly sequential steps can be done\nsimultaneously in one round using just $1+o(1)$ messages per user, while\nmaintaining the instance-optimal error bound. We also extend our technique to\nthe high-dimensional sum estimation problem and sparse vector aggregation\n(a.k.a. frequency estimation under user-level differential privacy). Our\nexperiments show order-of-magnitude improvements of our protocols in terms of\nerror compared with prior work.",
    "updated" : "2024-03-15T09:04:00Z",
    "published" : "2024-03-15T09:04:00Z",
    "authors" : [
      {
        "name" : "Wei Dong"
      },
      {
        "name" : "Qiyao Luo"
      },
      {
        "name" : "Giulia Fanti"
      },
      {
        "name" : "Elaine Shi"
      },
      {
        "name" : "Ke Yi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.09836v1",
    "title" : "Empowering Healthcare through Privacy-Preserving MRI Analysis",
    "summary" : "In the healthcare domain, Magnetic Resonance Imaging (MRI) assumes a pivotal\nrole, as it employs Artificial Intelligence (AI) and Machine Learning (ML)\nmethodologies to extract invaluable insights from imaging data. Nonetheless,\nthe imperative need for patient privacy poses significant challenges when\ncollecting data from diverse healthcare sources. Consequently, the Deep\nLearning (DL) communities occasionally face difficulties detecting rare\nfeatures. In this research endeavor, we introduce the Ensemble-Based Federated\nLearning (EBFL) Framework, an innovative solution tailored to address this\nchallenge. The EBFL framework deviates from the conventional approach by\nemphasizing model features over sharing sensitive patient data. This unique\nmethodology fosters a collaborative and privacy-conscious environment for\nhealthcare institutions, empowering them to harness the capabilities of a\ncentralized server for model refinement while upholding the utmost data privacy\nstandards.Conversely, a robust ensemble architecture boasts potent feature\nextraction capabilities, distinguishing itself from a single DL model. This\nquality makes it remarkably dependable for MRI analysis. By harnessing our\ngroundbreaking EBFL methodology, we have achieved remarkable precision in the\nclassification of brain tumors, including glioma, meningioma, pituitary, and\nnon-tumor instances, attaining a precision rate of 94% for the Global model and\nan impressive 96% for the Ensemble model. Our models underwent rigorous\nevaluation using conventional performance metrics such as Accuracy, Precision,\nRecall, and F1 Score. Integrating DL within the Federated Learning (FL)\nframework has yielded a methodology that offers precise and dependable\ndiagnostics for detecting brain tumors.",
    "updated" : "2024-03-14T19:51:18Z",
    "published" : "2024-03-14T19:51:18Z",
    "authors" : [
      {
        "name" : "Al Amin"
      },
      {
        "name" : "Kamrul Hasan"
      },
      {
        "name" : "Saleh Zein-Sabatto"
      },
      {
        "name" : "Deo Chimba"
      },
      {
        "name" : "Liang Hong"
      },
      {
        "name" : "Imtiaz Ahmed"
      },
      {
        "name" : "Tariqul Islam"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.09752v1",
    "title" : "Explainable Machine Learning-Based Security and Privacy Protection\n  Framework for Internet of Medical Things Systems",
    "summary" : "The Internet of Medical Things (IoMT) transcends traditional medical\nboundaries, enabling a transition from reactive treatment to proactive\nprevention. This innovative method revolutionizes healthcare by facilitating\nearly disease detection and tailored care, particularly in chronic disease\nmanagement, where IoMT automates treatments based on real-time health data\ncollection. Nonetheless, its benefits are countered by significant security\nchallenges that endanger the lives of its users due to the sensitivity and\nvalue of the processed data, thereby attracting malicious interests. Moreover,\nthe utilization of wireless communication for data transmission exposes medical\ndata to interception and tampering by cybercriminals. Additionally, anomalies\nmay arise due to human errors, network interference, or hardware malfunctions.\nIn this context, anomaly detection based on Machine Learning (ML) is an\ninteresting solution, but it comes up against obstacles in terms of\nexplicability and protection of privacy. To address these challenges, a new\nframework for Intrusion Detection Systems (IDS) is introduced, leveraging\nArtificial Neural Networks (ANN) for intrusion detection while utilizing\nFederated Learning (FL) for privacy preservation. Additionally, eXplainable\nArtificial Intelligence (XAI) methods are incorporated to enhance model\nexplanation and interpretation. The efficacy of the proposed framework is\nevaluated and compared with centralized approaches using multiple datasets\ncontaining network and medical data, simulating various attack types impacting\nthe confidentiality, integrity, and availability of medical and physiological\ndata. The results obtained offer compelling evidence that the FL method\nperforms comparably to the centralized method, demonstrating high performance.\nAdditionally, it affords the dual advantage of safeguarding privacy and\nproviding model explanation.",
    "updated" : "2024-03-14T11:57:26Z",
    "published" : "2024-03-14T11:57:26Z",
    "authors" : [
      {
        "name" : "Ayoub Si-ahmed"
      },
      {
        "name" : "Mohammed Ali Al-Garadi"
      },
      {
        "name" : "Narhimene Boustia"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.11519v1",
    "title" : "Efficient and Privacy-Preserving Federated Learning based on Full\n  Homomorphic Encryption",
    "summary" : "Since the first theoretically feasible full homomorphic encryption (FHE)\nscheme was proposed in 2009, great progress has been achieved. These\nimprovements have made FHE schemes come off the paper and become quite useful\nin solving some practical problems. In this paper, we propose a set of novel\nFederated Learning Schemes by utilizing the latest homomorphic encryption\ntechnologies, so as to improve the security, functionality and practicality at\nthe same time. Comparisons have been given in four practical data sets\nseparately from medical, business, biometric and financial fields, covering\nboth horizontal and vertical federated learning scenarios. The experiment\nresults show that our scheme achieves significant improvements in security,\nefficiency and practicality, compared with classical horizontal and vertical\nfederated learning schemes.",
    "updated" : "2024-03-18T07:13:09Z",
    "published" : "2024-03-18T07:13:09Z",
    "authors" : [
      {
        "name" : "Yuqi Guo"
      },
      {
        "name" : "Lin Li"
      },
      {
        "name" : "Zhongxiang Zheng"
      },
      {
        "name" : "Hanrui Yun"
      },
      {
        "name" : "Ruoyan Zhang"
      },
      {
        "name" : "Xiaolin Chang"
      },
      {
        "name" : "Zhixuan Gao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.11445v1",
    "title" : "Budget Recycling Differential Privacy",
    "summary" : "Differential Privacy (DP) mechanisms usually {force} reduction in data\nutility by producing ``out-of-bound'' noisy results for a tight privacy budget.\nWe introduce the Budget Recycling Differential Privacy (BR-DP) framework,\ndesigned to provide soft-bounded noisy outputs for a broad range of existing DP\nmechanisms. By ``soft-bounded,\" we refer to the mechanism's ability to release\nmost outputs within a predefined error boundary, thereby improving utility and\nmaintaining privacy simultaneously. The core of BR-DP consists of two\ncomponents: a DP kernel responsible for generating a noisy answer per\niteration, and a recycler that probabilistically recycles/regenerates or\nreleases the noisy answer. We delve into the privacy accounting of BR-DP,\nculminating in the development of a budgeting principle that optimally\nsub-allocates the available budget between the DP kernel and the recycler.\nFurthermore, we introduce algorithms for tight BR-DP accounting in composition\nscenarios, and our findings indicate that BR-DP achieves reduced privacy\nleakage post-composition compared to DP. Additionally, we explore the concept\nof privacy amplification via subsampling within the BR-DP framework and propose\noptimal sampling rates for BR-DP across various queries. We experiment with\nreal data, and the results demonstrate BR-DP's effectiveness in lifting the\nutility-privacy tradeoff provided by DP mechanisms.",
    "updated" : "2024-03-18T03:43:45Z",
    "published" : "2024-03-18T03:43:45Z",
    "authors" : [
      {
        "name" : "Bo Jiang"
      },
      {
        "name" : "Jian Du"
      },
      {
        "name" : "Sagar Shamar"
      },
      {
        "name" : "Qiang Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.11343v1",
    "title" : "Federated Transfer Learning with Differential Privacy",
    "summary" : "Federated learning is gaining increasing popularity, with data heterogeneity\nand privacy being two prominent challenges. In this paper, we address both\nissues within a federated transfer learning framework, aiming to enhance\nlearning on a target data set by leveraging information from multiple\nheterogeneous source data sets while adhering to privacy constraints. We\nrigorously formulate the notion of \\textit{federated differential privacy},\nwhich offers privacy guarantees for each data set without assuming a trusted\ncentral server. Under this privacy constraint, we study three classical\nstatistical problems, namely univariate mean estimation, low-dimensional linear\nregression, and high-dimensional linear regression. By investigating the\nminimax rates and identifying the costs of privacy for these problems, we show\nthat federated differential privacy is an intermediate privacy model between\nthe well-established local and central models of differential privacy. Our\nanalyses incorporate data heterogeneity and privacy, highlighting the\nfundamental costs of both in federated learning and underscoring the benefit of\nknowledge transfer across data sets.",
    "updated" : "2024-03-17T21:04:48Z",
    "published" : "2024-03-17T21:04:48Z",
    "authors" : [
      {
        "name" : "Mengchu Li"
      },
      {
        "name" : "Ye Tian"
      },
      {
        "name" : "Yang Feng"
      },
      {
        "name" : "Yi Yu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "math.ST",
      "stat.ME",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.11171v1",
    "title" : "A Tip for IOTA Privacy: IOTA Light Node Deanonymization via Tip\n  Selection",
    "summary" : "IOTA is a distributed ledger technology that uses a Directed Acyclic Graph\n(DAG) structure called the Tangle. It is known for its efficiency and is widely\nused in the Internet of Things (IoT) environment. Tangle can be configured by\nutilizing the tip selection process. Due to performance issues with light\nnodes, full nodes are being asked to perform the tip selections of light nodes.\nHowever, in this paper, we demonstrate that tip selection can be exploited to\ncompromise users' privacy. An adversary full node can associate a transaction\nwith the identity of a light node by comparing the light node's request with\nits ledger. We show that these types of attacks are not only viable in the\ncurrent IOTA environment but also in IOTA 2.0 and the privacy improvement being\nstudied. We also provide solutions to mitigate these attacks and propose ways\nto enhance anonymity in the IOTA network while maintaining efficiency and\nscalability.",
    "updated" : "2024-03-17T11:12:46Z",
    "published" : "2024-03-17T11:12:46Z",
    "authors" : [
      {
        "name" : "Hojung Yang"
      },
      {
        "name" : "Suhyeon Lee"
      },
      {
        "name" : "Seungjoo Kim"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.11088v1",
    "title" : "Programming Frameworks for Differential Privacy",
    "summary" : "Many programming frameworks have been introduced to support the development\nof differentially private software applications. In this chapter, we survey\nsome of the conceptual ideas underlying these frameworks in a way that we hope\nwill be helpful for both practitioners and researchers. For practitioners, the\nsurvey can provide a starting point for understanding what features may be\nvaluable when selecting a programming framework. For researchers, it can help\norganize existing work in a unified way and provide context for understanding\nnew features in future frameworks.",
    "updated" : "2024-03-17T04:44:48Z",
    "published" : "2024-03-17T04:44:48Z",
    "authors" : [
      {
        "name" : "Marco Gaboardi"
      },
      {
        "name" : "Michael Hay"
      },
      {
        "name" : "Salil Vadhan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "cs.PL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.11064v1",
    "title" : "Double-Private Distributed Estimation Algorithm Using Differential\n  Privacy and a Key-Like Proportionate Matrix with Its Performance Analysis",
    "summary" : "In this brief, we present an enhanced privacy-preserving distributed\nestimation algorithm, referred to as the ``Double-Private Algorithm,\" which\ncombines the principles of both differential privacy (DP) and cryptography. The\nproposed algorithm enhances privacy by introducing DP noise into the\nintermediate estimations of neighboring nodes. Additionally, we employ an\ninverse of a closed-form reproducible proportionate gain matrix as the\ncryptographic key matrix to fortify the privacy protection within the proposed\ndouble private algorithm. \\textcolor{blue}{We improve the algorithm by\ntransmitting alternative variable vectors instead of raw measurements,\nresulting in enhanced key matrix reconstruction performance. This innovative\napproach mitigate noise impact, enhancing overall algorithm effectiveness.} We\nalso establish an upper bound for the norm of the error between the non-private\nDiffusion Least Mean Square (DLMS) algorithm and our double private algorithm.\nFurther, we determine a sufficient condition for the step-size to ensure the\nmean convergence of the proposed algorithm. Simulation results demonstrate the\neffectiveness of the proposed algorithm, particularly its ability to attain the\nfinal Mean Square Deviation (MSD) comparable to that of the non-private DLMS.",
    "updated" : "2024-03-17T02:41:53Z",
    "published" : "2024-03-17T02:41:53Z",
    "authors" : [
      {
        "name" : "Mehdi Korki"
      },
      {
        "name" : "Fatemehsadat Hosseiniamin"
      },
      {
        "name" : "Hadi Zayyani"
      },
      {
        "name" : "Mehdi Bekrani"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.10920v1",
    "title" : "Batch-oriented Element-wise Approximate Activation for\n  Privacy-Preserving Neural Networks",
    "summary" : "Privacy-Preserving Neural Networks (PPNN) are advanced to perform inference\nwithout breaching user privacy, which can serve as an essential tool for\nmedical diagnosis to simultaneously achieve big data utility and privacy\nprotection. As one of the key techniques to enable PPNN, Fully Homomorphic\nEncryption (FHE) is facing a great challenge that homomorphic operations cannot\nbe easily adapted for non-linear activation calculations. In this paper,\nbatch-oriented element-wise data packing and approximate activation are\nproposed, which train linear low-degree polynomials to approximate the\nnon-linear activation function - ReLU. Compared with other approximate\nactivation methods, the proposed fine-grained, trainable approximation scheme\ncan effectively reduce the accuracy loss caused by approximation errors.\nMeanwhile, due to element-wise data packing, a large batch of images can be\npacked and inferred concurrently, leading to a much higher utility ratio of\nciphertext slots. Therefore, although the total inference time increases\nsharply, the amortized time for each image actually decreases, especially when\nthe batch size increases. Furthermore, knowledge distillation is adopted in the\ntraining process to further enhance the inference accuracy. Experiment results\nshow that when ciphertext inference is performed on 4096 input images, compared\nwith the current most efficient channel-wise method, the inference accuracy is\nimproved by 1.65%, and the amortized inference time is reduced by 99.5%.",
    "updated" : "2024-03-16T13:26:33Z",
    "published" : "2024-03-16T13:26:33Z",
    "authors" : [
      {
        "name" : "Peng Zhang"
      },
      {
        "name" : "Ao Duan"
      },
      {
        "name" : "Xianglu Zou"
      },
      {
        "name" : "Yuhong Liu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.10676v1",
    "title" : "Secure Distributed Storage: Optimal Trade-Off Between Storage Rate and\n  Privacy Leakage",
    "summary" : "Consider the problem of storing data in a distributed manner over $T$\nservers. Specifically, the data needs to (i) be recoverable from any $\\tau$\nservers, and (ii) remain private from any $z$ colluding servers, where privacy\nis quantified in terms of mutual information between the data and all the\ninformation available at any $z$ colluding servers. For this model, our main\nresults are (i) the fundamental trade-off between storage size and the level of\ndesired privacy, and (ii) the optimal amount of local randomness necessary at\nthe encoder. As a byproduct, our results provide an optimal lower bound on the\nindividual share size of ramp secret sharing schemes under a more general\nleakage symmetry condition than the ones previously considered in the\nliterature.",
    "updated" : "2024-03-15T20:50:46Z",
    "published" : "2024-03-15T20:50:46Z",
    "authors" : [
      {
        "name" : "Remi A. Chou"
      },
      {
        "name" : "Joerg Kliewer"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.10558v1",
    "title" : "Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition\n  Against Model Inversion Attack",
    "summary" : "The utilization of personal sensitive data in training face recognition (FR)\nmodels poses significant privacy concerns, as adversaries can employ model\ninversion attacks (MIA) to infer the original training data. Existing defense\nmethods, such as data augmentation and differential privacy, have been employed\nto mitigate this issue. However, these methods often fail to strike an optimal\nbalance between privacy and accuracy. To address this limitation, this paper\nintroduces an adaptive hybrid masking algorithm against MIA. Specifically, face\nimages are masked in the frequency domain using an adaptive MixUp strategy.\nUnlike the traditional MixUp algorithm, which is predominantly used for data\naugmentation, our modified approach incorporates frequency domain mixing.\nPrevious studies have shown that increasing the number of images mixed in MixUp\ncan enhance privacy preservation but at the expense of reduced face recognition\naccuracy. To overcome this trade-off, we develop an enhanced adaptive MixUp\nstrategy based on reinforcement learning, which enables us to mix a larger\nnumber of images while maintaining satisfactory recognition accuracy. To\noptimize privacy protection, we propose maximizing the reward function (i.e.,\nthe loss function of the FR system) during the training of the strategy\nnetwork. While the loss function of the FR network is minimized in the phase of\ntraining the FR network. The strategy network and the face recognition network\ncan be viewed as antagonistic entities in the training process, ultimately\nreaching a more balanced trade-off. Experimental results demonstrate that our\nproposed hybrid masking scheme outperforms existing defense algorithms in terms\nof privacy preservation and recognition accuracy against MIA.",
    "updated" : "2024-03-14T02:17:57Z",
    "published" : "2024-03-14T02:17:57Z",
    "authors" : [
      {
        "name" : "Yuanqing Huang"
      },
      {
        "name" : "Yinggui Wang"
      },
      {
        "name" : "Jianshu Li"
      },
      {
        "name" : "Le Yang"
      },
      {
        "name" : "Kai Song"
      },
      {
        "name" : "Lei Wang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.02292v3",
    "title" : "A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends",
    "summary" : "We present an analysis of 12 million instances of privacy-relevant reviews\npublicly visible on the Google Play Store that span a 10 year period. By\nleveraging state of the art NLP techniques, we examine what users have been\nwriting about privacy along multiple dimensions: time, countries, app types,\ndiverse privacy topics, and even across a spectrum of emotions. We find\nconsistent growth of privacy-relevant reviews, and explore topics that are\ntrending (such as Data Deletion and Data Theft), as well as those on the\ndecline (such as privacy-relevant reviews on sensitive permissions). We find\nthat although privacy reviews come from more than 200 countries, 33 countries\nprovide 90% of privacy reviews. We conduct a comparison across countries by\nexamining the distribution of privacy topics a country's users write about, and\nfind that geographic proximity is not a reliable indicator that nearby\ncountries have similar privacy perspectives. We uncover some countries with\nunique patterns and explore those herein. Surprisingly, we uncover that it is\nnot uncommon for reviews that discuss privacy to be positive (32%); many users\nexpress pleasure about privacy features within apps or privacy-focused apps. We\nalso uncover some unexpected behaviors, such as the use of reviews to deliver\nprivacy disclaimers to developers. Finally, we demonstrate the value of\nanalyzing app reviews with our approach as a complement to existing methods for\nunderstanding users' perspectives about privacy",
    "updated" : "2024-03-15T18:59:55Z",
    "published" : "2024-03-04T18:21:56Z",
    "authors" : [
      {
        "name" : "Omer Akgul"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Nina Taft"
      },
      {
        "name" : "Michelle L. Mazurek"
      },
      {
        "name" : "Hamza Harkous"
      },
      {
        "name" : "Animesh Srivastava"
      },
      {
        "name" : "Benoit Seguin"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.12710v1",
    "title" : "Selective, Interpretable, and Motion Consistent Privacy Attribute\n  Obfuscation for Action Recognition",
    "summary" : "Concerns for the privacy of individuals captured in public imagery have led\nto privacy-preserving action recognition. Existing approaches often suffer from\nissues arising through obfuscation being applied globally and a lack of\ninterpretability. Global obfuscation hides privacy sensitive regions, but also\ncontextual regions important for action recognition. Lack of interpretability\nerodes trust in these new technologies. We highlight the limitations of current\nparadigms and propose a solution: Human selected privacy templates that yield\ninterpretability by design, an obfuscation scheme that selectively hides\nattributes and also induces temporal consistency, which is important in action\nrecognition. Our approach is architecture agnostic and directly modifies input\nimagery, while existing approaches generally require architecture training. Our\napproach offers more flexibility, as no retraining is required, and outperforms\nalternatives on three widely used datasets.",
    "updated" : "2024-03-19T13:17:26Z",
    "published" : "2024-03-19T13:17:26Z",
    "authors" : [
      {
        "name" : "Filip Ilic"
      },
      {
        "name" : "He Zhao"
      },
      {
        "name" : "Thomas Pock"
      },
      {
        "name" : "Richard P. Wildes"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.12457v1",
    "title" : "Privacy-Preserving Face Recognition Using Trainable Feature Subtraction",
    "summary" : "The widespread adoption of face recognition has led to increasing privacy\nconcerns, as unauthorized access to face images can expose sensitive personal\ninformation. This paper explores face image protection against viewing and\nrecovery attacks. Inspired by image compression, we propose creating a visually\nuninformative face image through feature subtraction between an original face\nand its model-produced regeneration. Recognizable identity features within the\nimage are encouraged by co-training a recognition model on its high-dimensional\nfeature representation. To enhance privacy, the high-dimensional representation\nis crafted through random channel shuffling, resulting in randomized\nrecognizable images devoid of attacker-leverageable texture details. We distill\nour methodologies into a novel privacy-preserving face recognition method,\nMinusFace. Experiments demonstrate its high recognition accuracy and effective\nprivacy protection. Its code is available at https://github.com/Tencent/TFace.",
    "updated" : "2024-03-19T05:27:52Z",
    "published" : "2024-03-19T05:27:52Z",
    "authors" : [
      {
        "name" : "Yuxi Mi"
      },
      {
        "name" : "Zhizhou Zhong"
      },
      {
        "name" : "Yuge Huang"
      },
      {
        "name" : "Jiazhen Ji"
      },
      {
        "name" : "Jianqing Xu"
      },
      {
        "name" : "Jun Wang"
      },
      {
        "name" : "Shaoming Wang"
      },
      {
        "name" : "Shouhong Ding"
      },
      {
        "name" : "Shuigeng Zhou"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.12313v1",
    "title" : "Improving LoRA in Privacy-preserving Federated Learning",
    "summary" : "Low-rank adaptation (LoRA) is one of the most popular task-specific\nparameter-efficient fine-tuning (PEFT) methods on pre-trained language models\nfor its good performance and computational efficiency. LoRA injects a product\nof two trainable rank decomposition matrices over the top of each frozen\npre-trained model module. However, when applied in the setting of\nprivacy-preserving federated learning (FL), LoRA may become unstable due to the\nfollowing facts: 1) the effects of data heterogeneity and multi-step local\nupdates are non-negligible, 2) additive noise enforced on updating gradients to\nguarantee differential privacy (DP) can be amplified and 3) the final\nperformance is susceptible to hyper-parameters. A key factor leading to these\nphenomena is the discordance between jointly optimizing the two low-rank\nmatrices by local clients and separately aggregating them by the central\nserver. Thus, this paper proposes an efficient and effective version of LoRA,\nFederated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further\nhalve the communication cost of federated fine-tuning LLMs. The core idea of\nFFA-LoRA is to fix the randomly initialized non-zero matrices and only\nfine-tune the zero-initialized matrices. Compared to LoRA, FFA-LoRA is\nmotivated by practical and theoretical benefits in privacy-preserved FL. Our\nexperiments demonstrate that FFA-LoRA provides more consistent performance with\nbetter computational efficiency over vanilla LoRA in various FL tasks.",
    "updated" : "2024-03-18T23:20:08Z",
    "published" : "2024-03-18T23:20:08Z",
    "authors" : [
      {
        "name" : "Youbang Sun"
      },
      {
        "name" : "Zitao Li"
      },
      {
        "name" : "Yaliang Li"
      },
      {
        "name" : "Bolin Ding"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.11795v1",
    "title" : "Low-Cost Privacy-Aware Decentralized Learning",
    "summary" : "This paper introduces ZIP-DL, a novel privacy-aware decentralized learning\n(DL) algorithm that relies on adding correlated noise to each model update\nduring the model training process. This technique ensures that the added noise\nalmost neutralizes itself during the aggregation process due to its\ncorrelation, thus minimizing the impact on model accuracy. In addition, ZIP-DL\ndoes not require multiple communication rounds for noise cancellation,\naddressing the common trade-off between privacy protection and communication\noverhead. We provide theoretical guarantees for both convergence speed and\nprivacy guarantees, thereby making ZIP-DL applicable to practical scenarios.\nOur extensive experimental study shows that ZIP-DL achieves the best trade-off\nbetween vulnerability and accuracy. In particular, ZIP-DL (i) reduces the\neffectiveness of a linkability attack by up to 52 points compared to baseline\nDL, and (ii) achieves up to 37 more accuracy points for the same vulnerability\nunder membership inference attacks against a privacy-preserving competitor",
    "updated" : "2024-03-18T13:53:17Z",
    "published" : "2024-03-18T13:53:17Z",
    "authors" : [
      {
        "name" : "Sayan Biswas"
      },
      {
        "name" : "Davide Frey"
      },
      {
        "name" : "Romaric Gaudel"
      },
      {
        "name" : "Anne-Marie Kermarrec"
      },
      {
        "name" : "Dimitri Lerévérend"
      },
      {
        "name" : "Rafael Pires"
      },
      {
        "name" : "Rishi Sharma"
      },
      {
        "name" : "François Taïani"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.13743v1",
    "title" : "Quantum-Secure Certificate-Less Conditional Privacy-Preserving\n  Authentication for VANET",
    "summary" : "Vehicular Ad-hoc Networks (VANETs) marked a pronounced change in the\nIntelligent Transport System and Smart Cities through seamless vehicle\ncommunication to intensify safety and efficacy. However, a few authentication\nschemes have been devised in the literature to ensure the authenticity of the\nsource and information in the post-quantum era. The most popular base for such\nconstruction is lattice-based cryptography. However, existing lattice-based\nauthentication schemes fall short of addressing the potential challenges of the\nleakage of the master secret key and key-escrow problem. By ingeniously\naddressing both issues, the paper proposes the \\emph{first} quantum secure\nauthentication scheme to eliminate the flaws while maintaining the system's\noverall efficiency intact. Compared to the state-of-the-art schemes, the\nprovable security and overall performance assessment highlight the suitability\nof the proposed approach.",
    "updated" : "2024-03-20T16:50:36Z",
    "published" : "2024-03-20T16:50:36Z",
    "authors" : [
      {
        "name" : "Girraj Kumar Verma"
      },
      {
        "name" : "Nahida Majeed Wani"
      },
      {
        "name" : "Prosanta Gope"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.13346v1",
    "title" : "A Control-Recoverable Added-Noise-based Privacy Scheme for LQ Control in\n  Networked Control Systems",
    "summary" : "As networked control systems continue to evolve, ensuring the privacy of\nsensitive data becomes an increasingly pressing concern, especially in\nsituations where the controller is physically separated from the plant. In this\npaper, we propose a secure control scheme for computing linear quadratic\ncontrol in a networked control system utilizing two networked controllers, a\nprivacy encoder and a control restorer. Specifically, the encoder generates two\nstate signals blurred with random noise and sends them to the controllers,\nwhile the restorer reconstructs the correct control signal. The proposed design\neffectively preserves the privacy of the control system's state without\nsacrificing the control performance. We theoretically quantify the\nprivacy-preserving performance in terms of the state estimation error of the\ncontrollers and the disclosure probability. Additionally, the proposed\nprivacy-preserving scheme is also proven to satisfy differential privacy.\nMoreover, we extend the proposed privacy-preserving scheme and evaluation\nmethod to cases where collusion between two controllers occurs. Finally, we\nverify the validity of our proposed scheme through simulations.",
    "updated" : "2024-03-20T07:10:22Z",
    "published" : "2024-03-20T07:10:22Z",
    "authors" : [
      {
        "name" : "Xuening Tang"
      },
      {
        "name" : "Xianghui Cao"
      },
      {
        "name" : "Wei Xing Zheng"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.13041v1",
    "title" : "Provable Privacy with Non-Private Pre-Processing",
    "summary" : "When analysing Differentially Private (DP) machine learning pipelines, the\npotential privacy cost of data-dependent pre-processing is frequently\noverlooked in privacy accounting. In this work, we propose a general framework\nto evaluate the additional privacy cost incurred by non-private data-dependent\npre-processing algorithms. Our framework establishes upper bounds on the\noverall privacy guarantees by utilising two new technical notions: a variant of\nDP termed Smooth DP and the bounded sensitivity of the pre-processing\nalgorithms. In addition to the generic framework, we provide explicit overall\nprivacy guarantees for multiple data-dependent pre-processing algorithms, such\nas data imputation, quantization, deduplication and PCA, when used in\ncombination with several DP algorithms. Notably, this framework is also simple\nto implement, allowing direct integration into existing DP pipelines.",
    "updated" : "2024-03-19T17:54:49Z",
    "published" : "2024-03-19T17:54:49Z",
    "authors" : [
      {
        "name" : "Yaxi Hu"
      },
      {
        "name" : "Amartya Sanyal"
      },
      {
        "name" : "Bernhard Schölkopf"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.14450v1",
    "title" : "Maximal $α$-Leakage for Quantum Privacy Mechanisms",
    "summary" : "In this work, maximal $\\alpha$-leakage is introduced to quantify how much a\nquantum adversary can learn about any sensitive information of data upon\nobserving its disturbed version via a quantum privacy mechanism. We first show\nthat an adversary's maximal expected $\\alpha$-gain using optimal measurement is\ncharacterized by measured conditional R\\'enyi entropy. This can be viewed as a\nparametric generalization of K\\\"onig et al.'s famous guessing probability\nformula [IEEE Trans. Inf. Theory, 55(9), 2009]. Then, we prove that the\n$\\alpha$-leakage and maximal $\\alpha$-leakage for a quantum privacy mechanism\nare determined by measured Arimoto information and measured R\\'enyi capacity,\nrespectively. Various properties of maximal $\\alpha$-leakage, such as data\nprocessing inequality and composition property are established as well.\nMoreover, we show that regularized $\\alpha$-leakage and regularized maximal\n$\\alpha$-leakage for identical and independent quantum privacy mechanisms\ncoincide with $\\alpha$-tilted sandwiched R\\'enyi information and sandwiched\nR\\'enyi capacity, respectively.",
    "updated" : "2024-03-21T14:58:07Z",
    "published" : "2024-03-21T14:58:07Z",
    "authors" : [
      {
        "name" : "Bo-Yu Yang"
      },
      {
        "name" : "Hsuan Yu"
      },
      {
        "name" : "Hao-Chung Cheng"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.14428v1",
    "title" : "FHAUC: Privacy Preserving AUC Calculation for Federated Learning using\n  Fully Homomorphic Encryption",
    "summary" : "Ensuring data privacy is a significant challenge for machine learning\napplications, not only during model training but also during evaluation.\nFederated learning has gained significant research interest in recent years as\na result. Current research on federated learning primarily focuses on\npreserving privacy during the training phase. However, model evaluation has not\nbeen adequately addressed, despite the potential for significant privacy leaks\nduring this phase as well. In this paper, we demonstrate that the\nstate-of-the-art AUC computation method for federated learning systems, which\nutilizes differential privacy, still leaks sensitive information about the test\ndata while also requiring a trusted central entity to perform the computations.\nMore importantly, we show that the performance of this method becomes\ncompletely unusable as the data size decreases. In this context, we propose an\nefficient, accurate, robust, and more secure evaluation algorithm capable of\ncomputing the AUC in horizontal federated learning systems. Our approach not\nonly enhances security compared to the current state-of-the-art but also\nsurpasses the state-of-the-art AUC computation method in both approximation\nperformance and computational robustness, as demonstrated by experimental\nresults. To illustrate, our approach can efficiently calculate the AUC of a\nfederated learning system involving 100 parties, achieving 99.93% accuracy in\njust 0.68 seconds, regardless of data size, while providing complete data\nprivacy.",
    "updated" : "2024-03-21T14:36:55Z",
    "published" : "2024-03-21T14:36:55Z",
    "authors" : [
      {
        "name" : "Cem Ata Baykara"
      },
      {
        "name" : "Ali Burak Ünal"
      },
      {
        "name" : "Mete Akgün"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.14111v1",
    "title" : "HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic\n  Encryption",
    "summary" : "Transfer learning is a de facto standard method for efficiently training\nmachine learning models for data-scarce problems by adding and fine-tuning new\nclassification layers to a model pre-trained on large datasets. Although\nnumerous previous studies proposed to use homomorphic encryption to resolve the\ndata privacy issue in transfer learning in the machine learning as a service\nsetting, most of them only focused on encrypted inference. In this study, we\npresent HETAL, an efficient Homomorphic Encryption based Transfer Learning\nalgorithm, that protects the client's privacy in training tasks by encrypting\nthe client data using the CKKS homomorphic encryption scheme. HETAL is the\nfirst practical scheme that strictly provides encrypted training, adopting\nvalidation-based early stopping and achieving the accuracy of nonencrypted\ntraining. We propose an efficient encrypted matrix multiplication algorithm,\nwhich is 1.8 to 323 times faster than prior methods, and a highly precise\nsoftmax approximation algorithm with increased coverage. The experimental\nresults for five well-known benchmark datasets show total training times of\n567-3442 seconds, which is less than an hour.",
    "updated" : "2024-03-21T03:47:26Z",
    "published" : "2024-03-21T03:47:26Z",
    "authors" : [
      {
        "name" : "Seewoo Lee"
      },
      {
        "name" : "Garam Lee"
      },
      {
        "name" : "Jung Woo Kim"
      },
      {
        "name" : "Junbum Shin"
      },
      {
        "name" : "Mun-Kyu Lee"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.15208v1",
    "title" : "VPAS: Publicly Verifiable and Privacy-Preserving Aggregate Statistics on\n  Distributed Datasets",
    "summary" : "Aggregate statistics play an important role in extracting meaningful insights\nfrom distributed data while preserving privacy. A growing number of application\ndomains, such as healthcare, utilize these statistics in advancing research and\nimproving patient care.\n  In this work, we explore the challenge of input validation and public\nverifiability within privacy-preserving aggregation protocols. We address the\nscenario in which a party receives data from multiple sources and must verify\nthe validity of the input and correctness of the computations over this data to\nthird parties, such as auditors, while ensuring input data privacy. To achieve\nthis, we propose the \"VPAS\" protocol, which satisfies these requirements. Our\nprotocol utilizes homomorphic encryption for data privacy, and employs\nZero-Knowledge Proofs (ZKP) and a blockchain system for input validation and\npublic verifiability. We constructed VPAS by extending existing verifiable\nencryption schemes into secure protocols that enable N clients to encrypt,\naggregate, and subsequently release the final result to a collector in a\nverifiable manner.\n  We implemented and experimentally evaluated VPAS with regard to encryption\ncosts, proof generation, and verification. The findings indicate that the\noverhead associated with verifiability in our protocol is 10x lower than that\nincurred by simply using conventional zkSNARKs. This enhanced efficiency makes\nit feasible to apply input validation with public verifiability across a wider\nrange of applications or use cases that can tolerate moderate computational\noverhead associated with proof generation.",
    "updated" : "2024-03-22T13:50:22Z",
    "published" : "2024-03-22T13:50:22Z",
    "authors" : [
      {
        "name" : "Mohammed Alghazwi"
      },
      {
        "name" : "Dewi Davies-Batista"
      },
      {
        "name" : "Dimka Karastoyanova"
      },
      {
        "name" : "Fatih Turkmen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.15045v1",
    "title" : "DP-Dueling: Learning from Preference Feedback without Compromising User\n  Privacy",
    "summary" : "We consider the well-studied dueling bandit problem, where a learner aims to\nidentify near-optimal actions using pairwise comparisons, under the constraint\nof differential privacy. We consider a general class of utility-based\npreference matrices for large (potentially unbounded) decision spaces and give\nthe first differentially private dueling bandit algorithm for active learning\nwith user preferences. Our proposed algorithms are computationally efficient\nwith near-optimal performance, both in terms of the private and non-private\nregret bound. More precisely, we show that when the decision space is of finite\nsize $K$, our proposed algorithm yields order optimal $O\\Big(\\sum_{i =\n2}^K\\log\\frac{KT}{\\Delta_i} + \\frac{K}{\\epsilon}\\Big)$ regret bound for pure\n$\\epsilon$-DP, where $\\Delta_i$ denotes the suboptimality gap of the $i$-th\narm. We also present a matching lower bound analysis which proves the\noptimality of our algorithms. Finally, we extend our results to any general\ndecision space in $d$-dimensions with potentially infinite arms and design an\n$\\epsilon$-DP algorithm with regret $\\tilde{O} \\left( \\frac{d^6}{\\kappa\n\\epsilon } + \\frac{ d\\sqrt{T }}{\\kappa} \\right)$, providing privacy for free\nwhen $T \\gg d$.",
    "updated" : "2024-03-22T09:02:12Z",
    "published" : "2024-03-22T09:02:12Z",
    "authors" : [
      {
        "name" : "Aadirupa Saha"
      },
      {
        "name" : "Hilal Asi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.14905v1",
    "title" : "Adaptive Coded Federated Learning: Privacy Preservation and Straggler\n  Mitigation",
    "summary" : "In this article, we address the problem of federated learning in the presence\nof stragglers. For this problem, a coded federated learning framework has been\nproposed, where the central server aggregates gradients received from the\nnon-stragglers and gradient computed from a privacy-preservation global coded\ndataset to mitigate the negative impact of the stragglers. However, when\naggregating these gradients, fixed weights are consistently applied across\niterations, neglecting the generation process of the global coded dataset and\nthe dynamic nature of the trained model over iterations. This oversight may\nresult in diminished learning performance. To overcome this drawback, we\npropose a new method named adaptive coded federated learning (ACFL). In ACFL,\nbefore the training, each device uploads a coded local dataset with additive\nnoise to the central server to generate a global coded dataset under privacy\npreservation requirements. During each iteration of the training, the central\nserver aggregates the gradients received from the non-stragglers and the\ngradient computed from the global coded dataset, where an adaptive policy for\nvarying the aggregation weights is designed. Under this policy, we optimize the\nperformance in terms of privacy and learning, where the learning performance is\nanalyzed through convergence analysis and the privacy performance is\ncharacterized via mutual information differential privacy. Finally, we perform\nsimulations to demonstrate the superiority of ACFL compared with the\nnon-adaptive methods.",
    "updated" : "2024-03-22T01:51:48Z",
    "published" : "2024-03-22T01:51:48Z",
    "authors" : [
      {
        "name" : "Chengxi Li"
      },
      {
        "name" : "Ming Xiao"
      },
      {
        "name" : "Mikael Skoglund"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.14724v1",
    "title" : "Six Levels of Privacy: A Framework for Financial Synthetic Data",
    "summary" : "Synthetic Data is increasingly important in financial applications. In\naddition to the benefits it provides, such as improved financial modeling and\nbetter testing procedures, it poses privacy risks as well. Such data may arise\nfrom client information, business information, or other proprietary sources\nthat must be protected. Even though the process by which Synthetic Data is\ngenerated serves to obscure the original data to some degree, the extent to\nwhich privacy is preserved is hard to assess. Accordingly, we introduce a\nhierarchy of ``levels'' of privacy that are useful for categorizing Synthetic\nData generation methods and the progressively improved protections they offer.\nWhile the six levels were devised in the context of financial applications,\nthey may also be appropriate for other industries as well. Our paper includes:\nA brief overview of Financial Synthetic Data, how it can be used, how its value\ncan be assessed, privacy risks, and privacy attacks. We close with details of\nthe ``Six Levels'' that include defenses against those attacks.",
    "updated" : "2024-03-20T20:41:26Z",
    "published" : "2024-03-20T20:41:26Z",
    "authors" : [
      {
        "name" : "Tucker Balch"
      },
      {
        "name" : "Vamsi K. Potluru"
      },
      {
        "name" : "Deepak Paramanand"
      },
      {
        "name" : "Manuela Veloso"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "q-fin.ST"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.13346v2",
    "title" : "A Control-Recoverable Added-Noise-based Privacy Scheme for LQ Control in\n  Networked Control Systems",
    "summary" : "As networked control systems continue to evolve, ensuring the privacy of\nsensitive data becomes an increasingly pressing concern, especially in\nsituations where the controller is physically separated from the plant. In this\npaper, we propose a secure control scheme for computing linear quadratic\ncontrol in a networked control system utilizing two networked controllers, a\nprivacy encoder and a control restorer. Specifically, the encoder generates two\nstate signals blurred with random noise and sends them to the controllers,\nwhile the restorer reconstructs the correct control signal. The proposed design\neffectively preserves the privacy of the control system's state without\nsacrificing the control performance. We theoretically quantify the\nprivacy-preserving performance in terms of the state estimation error of the\ncontrollers and the disclosure probability. Additionally, the proposed\nprivacy-preserving scheme is also proven to satisfy differential privacy.\nMoreover, we extend the proposed privacy-preserving scheme and evaluation\nmethod to cases where collusion between two controllers occurs. Finally, we\nverify the validity of our proposed scheme through simulations.",
    "updated" : "2024-03-22T14:39:26Z",
    "published" : "2024-03-20T07:10:22Z",
    "authors" : [
      {
        "name" : "Xuening Tang"
      },
      {
        "name" : "Xianghui Cao"
      },
      {
        "name" : "Wei Xing Zheng"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.16591v1",
    "title" : "Deciphering the Interplay between Local Differential Privacy, Average\n  Bayesian Privacy, and Maximum Bayesian Privacy",
    "summary" : "The swift evolution of machine learning has led to emergence of various\ndefinitions of privacy due to the threats it poses to privacy, including the\nconcept of local differential privacy (LDP). Although widely embraced and\nutilized across numerous domains, this conventional approach to measure privacy\nstill exhibits certain limitations, spanning from failure to prevent\ninferential disclosure to lack of consideration for the adversary's background\nknowledge. In this comprehensive study, we introduce Bayesian privacy and delve\ninto the intricate relationship between local differential privacy and its\nBayesian counterparts, unveiling novel insights into utility-privacy\ntrade-offs. We introduce a framework that encapsulates both attack and defense\nstrategies, highlighting their interplay and effectiveness. Our theoretical\ncontributions are anchored in the rigorous definitions and relationships\nbetween Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),\nencapsulated by equations $\\epsilon_{p,a} \\leq\n\\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} +\n\\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP\nestablished under uniform prior distribution. These relationships fortify our\nunderstanding of the privacy guarantees provided by various mechanisms, leading\nto the realization that a mechanism satisfying $\\xi$-LDP also confers\n$\\xi$-MBP, and vice versa. Our work not only lays the groundwork for future\nempirical exploration but also promises to enhance the design of\nprivacy-preserving algorithms that do not compromise on utility, thereby\nfostering the development of trustworthy machine learning solutions.",
    "updated" : "2024-03-25T10:06:45Z",
    "published" : "2024-03-25T10:06:45Z",
    "authors" : [
      {
        "name" : "Xiaojin Zhang"
      },
      {
        "name" : "Yulin Fei"
      },
      {
        "name" : "Wei Chen"
      },
      {
        "name" : "Hai Jin"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.16473v1",
    "title" : "Plaintext-Free Deep Learning for Privacy-Preserving Medical Image\n  Analysis via Frequency Information Embedding",
    "summary" : "In the fast-evolving field of medical image analysis, Deep Learning\n(DL)-based methods have achieved tremendous success. However, these methods\nrequire plaintext data for training and inference stages, raising privacy\nconcerns, especially in the sensitive area of medical data. To tackle these\nconcerns, this paper proposes a novel framework that uses surrogate images for\nanalysis, eliminating the need for plaintext images. This approach is called\nFrequency-domain Exchange Style Fusion (FESF). The framework includes two main\ncomponents: Image Hidden Module (IHM) and Image Quality Enhancement\nModule~(IQEM). The~IHM performs in the frequency domain, blending the features\nof plaintext medical images into host medical images, and then combines this\nwith IQEM to improve and create surrogate images effectively. During the\ndiagnostic model training process, only surrogate images are used, enabling\nanonymous analysis without any plaintext data during both training and\ninference stages. Extensive evaluations demonstrate that our framework\neffectively preserves the privacy of medical images and maintains diagnostic\naccuracy of DL models at a relatively high level, proving its effectiveness\nacross various datasets and DL-based models.",
    "updated" : "2024-03-25T06:56:38Z",
    "published" : "2024-03-25T06:56:38Z",
    "authors" : [
      {
        "name" : "Mengyu Sun"
      },
      {
        "name" : "Ziyuan Yang"
      },
      {
        "name" : "Maosong Ran"
      },
      {
        "name" : "Zhiwen Wang"
      },
      {
        "name" : "Hui Yu"
      },
      {
        "name" : "Yi Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.16149v1",
    "title" : "A Survey on Consumer IoT Traffic: Security and Privacy",
    "summary" : "For the past few years, the Consumer Internet of Things (CIoT) has entered\npublic lives. While CIoT has improved the convenience of people's daily lives,\nit has also brought new security and privacy concerns. In this survey, we try\nto figure out what researchers can learn about the security and privacy of CIoT\nby traffic analysis, a popular method in the security community. From the\nsecurity and privacy perspective, this survey seeks out the new characteristics\nin CIoT traffic analysis, the state-of-the-art progress in CIoT traffic\nanalysis, and the challenges yet to be solved. We collected 310 papers from\nJanuary 2018 to December 2023 related to CIoT traffic analysis from the\nsecurity and privacy perspective and summarized the process of CIoT traffic\nanalysis in which the new characteristics of CIoT are identified. Then, we\ndetail existing works based on five application goals: device fingerprinting,\nuser activity inference, malicious traffic analysis, security analysis, and\nmeasurement. At last, we discuss the new challenges and future research\ndirections.",
    "updated" : "2024-03-24T13:43:43Z",
    "published" : "2024-03-24T13:43:43Z",
    "authors" : [
      {
        "name" : "Yan Jia"
      },
      {
        "name" : "Yuxin Song"
      },
      {
        "name" : "Zihou Liu"
      },
      {
        "name" : "Qingyin Tan"
      },
      {
        "name" : "Fangming Wang"
      },
      {
        "name" : "Yu Zhang"
      },
      {
        "name" : "Zheli Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2403.15510v1",
    "title" : "Privacy-Preserving End-to-End Spoken Language Understanding",
    "summary" : "Spoken language understanding (SLU), one of the key enabling technologies for\nhuman-computer interaction in IoT devices, provides an easy-to-use user\ninterface. Human speech can contain a lot of user-sensitive information, such\nas gender, identity, and sensitive content. New types of security and privacy\nbreaches have thus emerged. Users do not want to expose their personal\nsensitive information to malicious attacks by untrusted third parties. Thus,\nthe SLU system needs to ensure that a potential malicious attacker cannot\ndeduce the sensitive attributes of the users, while it should avoid greatly\ncompromising the SLU accuracy. To address the above challenge, this paper\nproposes a novel SLU multi-task privacy-preserving model to prevent both the\nspeech recognition (ASR) and identity recognition (IR) attacks. The model uses\nthe hidden layer separation technique so that SLU information is distributed\nonly in a specific portion of the hidden layer, and the other two types of\ninformation are removed to obtain a privacy-secure hidden layer. In order to\nachieve good balance between efficiency and privacy, we introduce a new\nmechanism of model pre-training, namely joint adversarial training, to further\nenhance the user privacy. Experiments over two SLU datasets show that the\nproposed method can reduce the accuracy of both the ASR and IR attacks close to\nthat of a random guess, while leaving the SLU performance largely unaffected.",
    "updated" : "2024-03-22T03:41:57Z",
    "published" : "2024-03-22T03:41:57Z",
    "authors" : [
      {
        "name" : "Yinggui Wang"
      },
      {
        "name" : "Wei Huang"
      },
      {
        "name" : "Le Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ]
  }
]