[
  {
    "id" : "http://arxiv.org/abs/2509.03350v1",
    "title" : "Exposing Privacy Risks in Anonymizing Clinical Data: Combinatorial\n  Refinement Attacks on k-Anonymity Without Auxiliary Information",
    "summary" : "Despite longstanding criticism from the privacy community, k-anonymity\nremains a widely used standard for data anonymization, mainly due to its\nsimplicity, regulatory alignment, and preservation of data utility. However,\nnon-experts often defend k-anonymity on the grounds that, in the absence of\nauxiliary information, no known attacks can compromise its protections. In this\nwork, we refute this claim by introducing Combinatorial Refinement Attacks\n(CRA), a new class of privacy attacks targeting k-anonymized datasets produced\nusing local recoding. This is the first method that does not rely on external\nauxiliary information or assumptions about the underlying data distribution.\nCRA leverages the utility-optimizing behavior of local recoding anonymization\nof ARX, which is a widely used open-source software for anonymizing data in\nclinical settings, to formulate a linear program that significantly reduces the\nspace of plausible sensitive values. To validate our findings, we partnered\nwith a network of free community health clinics, an environment where (1)\nauxiliary information is indeed hard to find due to the population they serve\nand (2) open-source k-anonymity solutions are attractive due to regulatory\nobligations and limited resources. Our results on real-world clinical microdata\nreveal that even in the absence of external information, established\nanonymization frameworks do not deliver the promised level of privacy, raising\ncritical privacy concerns.",
    "updated" : "2025-09-03T14:36:06Z",
    "published" : "2025-09-03T14:36:06Z",
    "authors" : [
      {
        "name" : "Somiya Chhillar"
      },
      {
        "name" : "Mary K. Righi"
      },
      {
        "name" : "Rebecca E. Sutter"
      },
      {
        "name" : "Evgenios M. Kornaropoulos"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.03294v1",
    "title" : "A Comprehensive Guide to Differential Privacy: From Theory to User\n  Expectations",
    "summary" : "The increasing availability of personal data has enabled significant advances\nin fields such as machine learning, healthcare, and cybersecurity. However,\nthis data abundance also raises serious privacy concerns, especially in light\nof powerful re-identification attacks and growing legal and ethical demands for\nresponsible data use. Differential privacy (DP) has emerged as a principled,\nmathematically grounded framework for mitigating these risks. This review\nprovides a comprehensive survey of DP, covering its theoretical foundations,\npractical mechanisms, and real-world applications. It explores key algorithmic\ntools and domain-specific challenges - particularly in privacy-preserving\nmachine learning and synthetic data generation. The report also highlights\nusability issues and the need for improved communication and transparency in DP\nsystems. Overall, the goal is to support informed adoption of DP by researchers\nand practitioners navigating the evolving landscape of data privacy.",
    "updated" : "2025-09-03T13:23:10Z",
    "published" : "2025-09-03T13:23:10Z",
    "authors" : [
      {
        "name" : "Napsu Karmitsa"
      },
      {
        "name" : "Antti Airola"
      },
      {
        "name" : "Tapio Pahikkala"
      },
      {
        "name" : "Tinja Pitkämäki"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "68P27, 68T09, 94A60"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.03024v1",
    "title" : "Efficient Privacy-Preserving Recommendation on Sparse Data using Fully\n  Homomorphic Encryption",
    "summary" : "In today's data-driven world, recommendation systems personalize user\nexperiences across industries but rely on sensitive data, raising privacy\nconcerns. Fully homomorphic encryption (FHE) can secure these systems, but a\nsignificant challenge in applying FHE to recommendation systems is efficiently\nhandling the inherently large and sparse user-item rating matrices. FHE\noperations are computationally intensive, and naively processing various sparse\nmatrices in recommendation systems would be prohibitively expensive.\nAdditionally, the communication overhead between parties remains a critical\nconcern in encrypted domains. We propose a novel approach combining Compressed\nSparse Row (CSR) representation with FHE-based matrix factorization that\nefficiently handles matrix sparsity in the encrypted domain while minimizing\ncommunication costs. Our experimental results demonstrate high recommendation\naccuracy with encrypted data while achieving the lowest communication costs,\neffectively preserving user privacy.",
    "updated" : "2025-09-03T05:15:45Z",
    "published" : "2025-09-03T05:15:45Z",
    "authors" : [
      {
        "name" : "Moontaha Nishat Chowdhury"
      },
      {
        "name" : "André Bauer"
      },
      {
        "name" : "Minxuan Zhou"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02856v1",
    "title" : "Managing Correlations in Data and Privacy Demand",
    "summary" : "Previous works in the differential privacy literature that allow users to\nchoose their privacy levels typically operate under the heterogeneous\ndifferential privacy (HDP) framework with the simplifying assumption that user\ndata and privacy levels are not correlated. Firstly, we demonstrate that the\nstandard HDP framework falls short when user data and privacy demands are\nallowed to be correlated. Secondly, to address this shortcoming, we propose an\nalternate framework, Add-remove Heterogeneous Differential Privacy (AHDP), that\njointly accounts for user data and privacy preference. We show that AHDP is\nrobust to possible correlations between data and privacy. Thirdly, we formalize\nthe guarantees of the proposed AHDP framework through an operational hypothesis\ntesting perspective. The hypothesis testing setup may be of independent\ninterest in analyzing other privacy frameworks as well. Fourthly, we show that\nthere exists non-trivial AHDP mechanisms that notably do not require prior\nknowledge of the data-privacy correlations. We propose some such mechanisms and\napply them to core statistical tasks such as mean estimation, frequency\nestimation, and linear regression. The proposed mechanisms are simple to\nimplement with minimal assumptions and modeling requirements, making them\nattractive for real-world use. Finally, we empirically evaluate proposed AHDP\nmechanisms, highlighting their trade-offs using LLM-generated synthetic\ndatasets, which we release for future research.",
    "updated" : "2025-09-02T22:03:13Z",
    "published" : "2025-09-02T22:03:13Z",
    "authors" : [
      {
        "name" : "Syomantak Chaudhuri"
      },
      {
        "name" : "Thomas A. Courtade"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02768v1",
    "title" : "Sequential Change Detection with Differential Privacy",
    "summary" : "Sequential change detection is a fundamental problem in statistics and signal\nprocessing, with the CUSUM procedure widely used to achieve minimax detection\ndelay under a prescribed false-alarm rate when pre- and post-change\ndistributions are fully known. However, releasing CUSUM statistics and the\ncorresponding stopping time directly can compromise individual data privacy. We\ntherefore introduce a differentially private (DP) variant, called DP-CUSUM,\nthat injects calibrated Laplace noise into both the vanilla CUSUM statistics\nand the detection threshold, preserving the recursive simplicity of the\nclassical CUSUM statistics while ensuring per-sample differential privacy. We\nderive closed-form bounds on the average run length to false alarm and on the\nworst-case average detection delay, explicitly characterizing the trade-off\namong privacy level, false-alarm rate, and detection efficiency. Our\ntheoretical results imply that under a weak privacy constraint, our proposed\nDP-CUSUM procedure achieves the same first-order asymptotic optimality as the\nclassical, non-private CUSUM procedure. Numerical simulations are conducted to\ndemonstrate the detection efficiency of our proposed DP-CUSUM under different\nprivacy constraints, and the results are consistent with our theoretical\nfindings.",
    "updated" : "2025-09-02T19:15:47Z",
    "published" : "2025-09-02T19:15:47Z",
    "authors" : [
      {
        "name" : "Liyan Xie"
      },
      {
        "name" : "Ruizhi Zhang"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02411v1",
    "title" : "A Survey: Towards Privacy and Security in Mobile Large Language Models",
    "summary" : "Mobile Large Language Models (LLMs) are revolutionizing diverse fields such\nas healthcare, finance, and education with their ability to perform advanced\nnatural language processing tasks on-the-go. However, the deployment of these\nmodels in mobile and edge environments introduces significant challenges\nrelated to privacy and security due to their resource-intensive nature and the\nsensitivity of the data they process. This survey provides a comprehensive\noverview of privacy and security issues associated with mobile LLMs,\nsystematically categorizing existing solutions such as differential privacy,\nfederated learning, and prompt encryption. Furthermore, we analyze\nvulnerabilities unique to mobile LLMs, including adversarial attacks,\nmembership inference, and side-channel attacks, offering an in-depth comparison\nof their effectiveness and limitations. Despite recent advancements, mobile\nLLMs face unique hurdles in achieving robust security while maintaining\nefficiency in resource-constrained environments. To bridge this gap, we propose\npotential applications, discuss open challenges, and suggest future research\ndirections, paving the way for the development of trustworthy,\nprivacy-compliant, and scalable mobile LLM systems.",
    "updated" : "2025-09-02T15:19:57Z",
    "published" : "2025-09-02T15:19:57Z",
    "authors" : [
      {
        "name" : "Honghui Xu"
      },
      {
        "name" : "Kaiyang Li"
      },
      {
        "name" : "Wei Chen"
      },
      {
        "name" : "Danyang Zheng"
      },
      {
        "name" : "Zhiyuan Li"
      },
      {
        "name" : "Zhipeng Cai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02048v1",
    "title" : "Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization\n  Framework with Curvature-Guided Perturbation",
    "summary" : "Machine learning models require datasets for effective training, but directly\nsharing raw data poses significant privacy risk such as membership inference\nattacks (MIA). To mitigate the risk, privacy-preserving techniques such as data\nperturbation, generalization, and synthetic data generation are commonly\nutilized. However, these methods often degrade data accuracy, specificity, and\ndiversity, limiting the performance of downstream tasks and thus reducing data\nutility. Therefore, striking an optimal balance between privacy preservation\nand data utility remains a critical challenge.\n  To address this issue, we introduce a novel bilevel optimization framework\nfor the publication of private datasets, where the upper-level task focuses on\ndata utility and the lower-level task focuses on data privacy. In the\nupper-level task, a discriminator guides the generation process to ensure that\nperturbed latent variables are mapped to high-quality samples, maintaining\nfidelity for downstream tasks. In the lower-level task, our framework employs\nlocal extrinsic curvature on the data manifold as a quantitative measure of\nindividual vulnerability to MIA, providing a geometric foundation for targeted\nprivacy protection. By perturbing samples toward low-curvature regions, our\nmethod effectively suppresses distinctive feature combinations that are\nvulnerable to MIA. Through alternating optimization of both objectives, we\nachieve a synergistic balance between privacy and utility. Extensive\nexperimental evaluations demonstrate that our method not only enhances\nresistance to MIA in downstream tasks but also surpasses existing methods in\nterms of sample quality and diversity.",
    "updated" : "2025-09-02T07:44:21Z",
    "published" : "2025-09-02T07:44:21Z",
    "authors" : [
      {
        "name" : "Yi Yin"
      },
      {
        "name" : "Guangquan Zhang"
      },
      {
        "name" : "Hua Zuo"
      },
      {
        "name" : "Jie Lu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02004v1",
    "title" : "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
    "summary" : "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
    "updated" : "2025-09-02T06:40:45Z",
    "published" : "2025-09-02T06:40:45Z",
    "authors" : [
      {
        "name" : "Takao Murakami"
      },
      {
        "name" : "Yuichi Sei"
      },
      {
        "name" : "Reo Eriguchi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01716v1",
    "title" : "An LLM-enabled semantic-centric framework to consume privacy policies",
    "summary" : "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites, despite claiming\notherwise, due to the practical difficulty in comprehending them. The mist of\ndata privacy practices forms a major barrier for user-centred Web approaches,\nand for data sharing and reusing in an agentic world. Existing research\nproposed methods for using formal languages and reasoning for verifying the\ncompliance of a specified policy, as a potential cure for ignoring privacy\npolicies. However, a critical gap remains in the creation or acquisition of\nsuch formal policies at scale. We present a semantic-centric approach for using\nstate-of-the-art large language models (LLM), to automatically identify key\ninformation about privacy practices from privacy policies, and construct\n$\\mathit{Pr}^2\\mathit{Graph}$, knowledge graph with grounding from Data Privacy\nVocabulary (DPV) for privacy practices, to support downstream tasks. Along with\nthe pipeline, the $\\mathit{Pr}^2\\mathit{Graph}$ for the top-100 popular\nwebsites is also released as a public resource, by using the pipeline for\nanalysis. We also demonstrate how the $\\mathit{Pr}^2\\mathit{Graph}$ can be used\nto support downstream tasks by constructing formal policy representations such\nas Open Digital Right Language (ODRL) or perennial semantic Data Terms of Use\n(psDToU). To evaluate the technology capability, we enriched the Policy-IE\ndataset by employing legal experts to create custom annotations. We benchmarked\nthe performance of different large language models for our pipeline and\nverified their capabilities. Overall, they shed light on the possibility of\nlarge-scale analysis of online services' privacy practices, as a promising\ndirection to audit the Web and the Internet. We release all datasets and source\ncode as public resources to facilitate reuse and improvement.",
    "updated" : "2025-09-01T18:53:13Z",
    "published" : "2025-09-01T18:53:13Z",
    "authors" : [
      {
        "name" : "Rui Zhao"
      },
      {
        "name" : "Vladyslav Melnychuk"
      },
      {
        "name" : "Jun Zhao"
      },
      {
        "name" : "Jesse Wright"
      },
      {
        "name" : "Nigel Shadbolt"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01527v2",
    "title" : "A Privacy-Preserving Recommender for Filling Web Forms Using a Local\n  Large Language Model",
    "summary" : "Web applications are increasingly used in critical domains such as education,\nfinance, and e-commerce. This highlights the need to ensure their failure-free\nperformance. One effective method for evaluating failure-free performance is\nweb form testing, where defining effective test scenarios is key to a complete\nand accurate evaluation. A core aspect of this process involves filling form\nfields with suitable values to create effective test cases. However, manually\ngenerating these values is time-consuming and prone to errors. To address this,\nvarious tools have been developed to assist testers. With the appearance of\nlarge language models (LLMs), a new generation of tools seeks to handle this\ntask more intelligently. Although many LLM-based tools have been introduced, as\nthese models typically rely on cloud infrastructure, their use in testing\nconfidential web forms raises concerns about unintended data leakage and\nbreaches of confidentiality. This paper introduces a privacy-preserving\nrecommender that operates locally using a large language model. The tool\nassists testers in web form testing by suggesting effective field values. This\ntool analyzes the HTML structure of forms, detects input types, and extracts\nconstraints based on each field's type and contextual content, guiding proper\nfield filling.",
    "updated" : "2025-09-03T15:43:01Z",
    "published" : "2025-09-01T15:02:00Z",
    "authors" : [
      {
        "name" : "Amirreza Nayyeri"
      },
      {
        "name" : "Abbas Rasoolzadegan"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01470v1",
    "title" : "Privacy-preserving authentication for military 5G networks",
    "summary" : "As 5G networks gain traction in defense applications, ensuring the privacy\nand integrity of the Authentication and Key Agreement (AKA) protocol is\ncritical. While 5G AKA improves upon previous generations by concealing\nsubscriber identities, it remains vulnerable to replay-based synchronization\nand linkability threats under realistic adversary models. This paper provides a\nunified analysis of the standardized 5G AKA flow, identifying several\nvulnerabilities and highlighting how each exploits protocol behavior to\ncompromise user privacy. To address these risks, we present five lightweight\nmitigation strategies. We demonstrate through prototype implementation and\ntesting that these enhancements strengthen resilience against linkability\nattacks with minimal computational and signaling overhead. Among the solutions\nstudied, those introducing a UE-generated nonce emerge as the most promising,\neffectively neutralizing the identified tracking and correlation attacks with\nnegligible additional overhead. Integrating this extension as an optional\nfeature to the standard 5G AKA protocol offers a backward-compatible,\nlow-overhead path toward a more privacy-preserving authentication framework for\nboth commercial and military 5G deployments.",
    "updated" : "2025-09-01T13:38:11Z",
    "published" : "2025-09-01T13:38:11Z",
    "authors" : [
      {
        "name" : "I. D. Lutz"
      },
      {
        "name" : "A. M. Hill"
      },
      {
        "name" : "M. C. Valenti"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01354v1",
    "title" : "DPF-CM: A Data Processing Framework with Privacy-Preserving Vector\n  Databases for Chinese Medical LLMs Training and Deployment",
    "summary" : "Current open-source training pipelines for Chinese medical language models\npredominantly emphasize optimizing training methodologies to enhance the\nperformance of large language models (LLMs), yet lack comprehensive exploration\ninto training data processing. To address this gap, we propose DPF-CM, a\nholistic Data Processing Framework for Chinese Medical LLMs training and\ndeployment. DPF-CM comprises two core modules. The first module is a data\nprocessing pipeline tailored for model training. Beyond standard data\nprocessing operations, we (1) introduce a chained examples context-learning\nstrategy to generate question-oriented instructions to mitigate the lack of\ninstruction content, and (2) implement an ensemble-based filtering mechanism\nfor preference data curation that averages multiple reward models to suppress\nnoisy samples. The second module focuses on privacy preservation during model\ndeployment. To prevent privacy risks from the inadvertent exposure of training\ndata, we propose a Privacy Preserving Vector Database (PPVD) approach, which\ninvolves model memory search, high-risk database construction, secure database\nconstruction, and match-and-replace, four key stages to minimize privacy\nleakage during inference collectively. Experimental results show that DPF-CM\nsignificantly improves model accuracy, enabling our trained Chinese medical LLM\nto achieve state-of-the-art performance among open-source counterparts.\nMoreover, the framework reduces training data privacy leakage by 27%.",
    "updated" : "2025-09-01T10:49:32Z",
    "published" : "2025-09-01T10:49:32Z",
    "authors" : [
      {
        "name" : "Wei Huang"
      },
      {
        "name" : "Anda Cheng"
      },
      {
        "name" : "Zhao Zhang"
      },
      {
        "name" : "Yinggui Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01088v1",
    "title" : "Privacy-Preserving Reasoning with Knowledge-Distilled Parametric\n  Retrieval Augmented Generation",
    "summary" : "The current RAG system requires uploading plaintext documents to the cloud,\nrisking private data leakage. Parametric RAG (PRAG) addresses this by encoding\ndocuments as LoRA within LLMs, enabling reasoning without exposing raw content.\nHowever, it still faces two issues: (1) PRAG demands synthesizing QA pairs and\nfine-tuning LLM for each individual document to create its corresponding LoRA,\nleading to unacceptable inference latency. (2) The performance of PRAG relies\nsolely on synthetic QA data, lacking internal alignment with standard RAG,\nresulting in poor generalization on out-of-distribution(OOD) inputs. Therefore,\nachieving high-efficiency parameterization while maintaining RAG-level\nperformance remains a critical challenge for privacy-preserving reasoning. In\nthis paper, we propose DistilledPRAG, a generalizable knowledge-distilled\nparametric RAG model aligned with standard RAG in document structure and\nparameter activation. We first synthesize QA pairs from single and\nmulti-documents to enhance cross-document reasoning. Then, we mask the\nplaintext documents with a special token and translate them to LoRA via a\nparameter generator, maintaining the standard RAG document structure. Finally,\nguided by synthetic QA data, we train the parameter generator to match standard\nRAG's hidden states and output logits, enabling RAG-style reasoning without\noriginal documents. Experiments on four QA datasets show that DistilledPRAG\noutperforms baselines in accuracy and generalizes well on OOD data.",
    "updated" : "2025-09-01T03:23:57Z",
    "published" : "2025-09-01T03:23:57Z",
    "authors" : [
      {
        "name" : "Jinwen Chen"
      },
      {
        "name" : "Hainan Zhang"
      },
      {
        "name" : "Liang Pang"
      },
      {
        "name" : "Yongxin Tong"
      },
      {
        "name" : "Haibo Zhou"
      },
      {
        "name" : "Yuan Zhan"
      },
      {
        "name" : "Wei Lin"
      },
      {
        "name" : "Zhiming Zheng"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04358v1",
    "title" : "Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the\n  Roles of Information Transparency, User Control, and Proactivity",
    "summary" : "Social robots are increasingly recognized as valuable supporters in the field\nof well-being coaching. They can function as independent coaches or provide\nsupport alongside human coaches, and healthcare professionals. In coaching\ninteractions, these robots often handle sensitive information shared by users,\nmaking privacy a relevant issue. Despite this, little is known about the\nfactors that shape users' privacy perceptions. This research aims to examine\nthree key factors systematically: (1) the transparency about information usage,\n(2) the level of specific user control over how the robot uses their\ninformation, and (3) the robot's behavioral approach - whether it acts\nproactively or only responds on demand. Our results from an online study (N =\n200) show that even when users grant the robot general access to personal data,\nthey additionally expect the ability to explicitly control how that information\nis interpreted and shared during sessions. Experimental conditions that\nprovided such control received significantly higher ratings for perceived\nprivacy appropriateness and trust. Compared to user control, the effects of\ntransparency and proactivity on privacy appropriateness perception were low,\nand we found no significant impact. The results suggest that merely informing\nusers or proactive sharing is insufficient without accompanying user control.\nThese insights underscore the need for further research on mechanisms that\nallow users to manage robots' information processing and sharing, especially\nwhen social robots take on more proactive roles alongside humans.",
    "updated" : "2025-09-04T16:19:24Z",
    "published" : "2025-09-04T16:19:24Z",
    "authors" : [
      {
        "name" : "Atikkhan Faridkhan Nilgar"
      },
      {
        "name" : "Manuel Dietrich"
      },
      {
        "name" : "Kristof Van Laerhoven"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04290v1",
    "title" : "An Interactive Framework for Finding the Optimal Trade-off in\n  Differential Privacy",
    "summary" : "Differential privacy (DP) is the standard for privacy-preserving analysis,\nand introduces a fundamental trade-off between privacy guarantees and model\nperformance. Selecting the optimal balance is a critical challenge that can be\nframed as a multi-objective optimization (MOO) problem where one first\ndiscovers the set of optimal trade-offs (the Pareto front) and then learns a\ndecision-maker's preference over them. While a rich body of work on interactive\nMOO exists, the standard approach -- modeling the objective functions with\ngeneric surrogates and learning preferences from simple pairwise feedback -- is\ninefficient for DP because it fails to leverage the problem's unique structure:\na point on the Pareto front can be generated directly by maximizing accuracy\nfor a fixed privacy level. Motivated by this property, we first derive the\nshape of the trade-off theoretically, which allows us to model the Pareto front\ndirectly and efficiently. To address inefficiency in preference learning, we\nreplace pairwise comparisons with a more informative interaction. In\nparticular, we present the user with hypothetical trade-off curves and ask them\nto pick their preferred trade-off. Our experiments on differentially private\nlogistic regression and deep transfer learning across six real-world datasets\nshow that our method converges to the optimal privacy-accuracy trade-off with\nsignificantly less computational cost and user interaction than baselines.",
    "updated" : "2025-09-04T15:02:10Z",
    "published" : "2025-09-04T15:02:10Z",
    "authors" : [
      {
        "name" : "Yaohong Yang"
      },
      {
        "name" : "Aki Rehn"
      },
      {
        "name" : "Sammie Katt"
      },
      {
        "name" : "Antti Honkela"
      },
      {
        "name" : "Samuel Kaski"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04232v1",
    "title" : "Rethinking Layer-wise Gaussian Noise Injection: Bridging Implicit\n  Objectives and Privacy Budget Allocation",
    "summary" : "Layer-wise Gaussian mechanisms (LGM) enhance flexibility in differentially\nprivate deep learning by injecting noise into partitioned gradient vectors.\nHowever, existing methods often rely on heuristic noise allocation strategies,\nlacking a rigorous understanding of their theoretical grounding in connecting\nnoise allocation to formal privacy-utility tradeoffs. In this paper, we present\na unified analytical framework that systematically connects layer-wise noise\ninjection strategies with their implicit optimization objectives and associated\nprivacy budget allocations. Our analysis reveals that several existing\napproaches optimize ill-posed objectives -- either ignoring inter-layer\nsignal-to-noise ratio (SNR) consistency or leading to inefficient use of the\nprivacy budget. In response, we propose a SNR-Consistent noise allocation\nstrategy that unifies both aspects, yielding a noise allocation scheme that\nachieves better signal preservation and more efficient privacy budget\nutilization. Extensive experiments in both centralized and federated learning\nsettings demonstrate that our method consistently outperforms existing\nallocation strategies, achieving better privacy-utility tradeoffs. Our\nframework not only offers diagnostic insights into prior methods but also\nprovides theoretical guidance for designing adaptive and effective noise\ninjection schemes in deep models.",
    "updated" : "2025-09-04T14:09:46Z",
    "published" : "2025-09-04T14:09:46Z",
    "authors" : [
      {
        "name" : "Qifeng Tan"
      },
      {
        "name" : "Shusen Yang"
      },
      {
        "name" : "Xuebin Ren"
      },
      {
        "name" : "Yikai Zhang"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04169v1",
    "title" : "Privacy Risks in Time Series Forecasting: User- and Record-Level\n  Membership Inference",
    "summary" : "Membership inference attacks (MIAs) aim to determine whether specific data\nwere used to train a model. While extensively studied on classification models,\ntheir impact on time series forecasting remains largely unexplored. We address\nthis gap by introducing two new attacks: (i) an adaptation of multivariate\nLiRA, a state-of-the-art MIA originally developed for classification models, to\nthe time-series forecasting setting, and (ii) a novel end-to-end learning\napproach called Deep Time Series (DTS) attack. We benchmark these methods\nagainst adapted versions of other leading attacks from the classification\nsetting.\n  We evaluate all attacks in realistic settings on the TUH-EEG and ELD\ndatasets, targeting two strong forecasting architectures, LSTM and the\nstate-of-the-art N-HiTS, under both record- and user-level threat models. Our\nresults show that forecasting models are vulnerable, with user-level attacks\noften achieving perfect detection. The proposed methods achieve the strongest\nperformance in several settings, establishing new baselines for privacy risk\nassessment in time series forecasting. Furthermore, vulnerability increases\nwith longer prediction horizons and smaller training populations, echoing\ntrends observed in large language models.",
    "updated" : "2025-09-04T12:43:45Z",
    "published" : "2025-09-04T12:43:45Z",
    "authors" : [
      {
        "name" : "Nicolas Johansson"
      },
      {
        "name" : "Tobias Olsson"
      },
      {
        "name" : "Daniel Nilsson"
      },
      {
        "name" : "Johan Östman"
      },
      {
        "name" : "Fazeleh Hoseini"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05265v1",
    "title" : "On Evaluating the Poisoning Robustness of Federated Learning under Local\n  Differential Privacy",
    "summary" : "Federated learning (FL) combined with local differential privacy (LDP)\nenables privacy-preserving model training across decentralized data sources.\nHowever, the decentralized data-management paradigm leaves LDPFL vulnerable to\nparticipants with malicious intent. The robustness of LDPFL protocols,\nparticularly against model poisoning attacks (MPA), where adversaries inject\nmalicious updates to disrupt global model convergence, remains insufficiently\nstudied. In this paper, we propose a novel and extensible model poisoning\nattack framework tailored for LDPFL settings. Our approach is driven by the\nobjective of maximizing the global training loss while adhering to local\nprivacy constraints. To counter robust aggregation mechanisms such as\nMulti-Krum and trimmed mean, we develop adaptive attacks that embed carefully\ncrafted constraints into a reverse training process, enabling evasion of these\ndefenses. We evaluate our framework across three representative LDPFL\nprotocols, three benchmark datasets, and two types of deep neural networks.\nAdditionally, we investigate the influence of data heterogeneity and privacy\nbudgets on attack effectiveness. Experimental results demonstrate that our\nadaptive attacks can significantly degrade the performance of the global model,\nrevealing critical vulnerabilities and highlighting the need for more robust\nLDPFL defense strategies against MPA. Our code is available at\nhttps://github.com/ZiJW/LDPFL-Attack",
    "updated" : "2025-09-05T17:23:03Z",
    "published" : "2025-09-05T17:23:03Z",
    "authors" : [
      {
        "name" : "Zijian Wang"
      },
      {
        "name" : "Wei Tong"
      },
      {
        "name" : "Tingxuan Han"
      },
      {
        "name" : "Haoyu Chen"
      },
      {
        "name" : "Tianling Zhang"
      },
      {
        "name" : "Yunlong Mao"
      },
      {
        "name" : "Sheng Zhong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05162v1",
    "title" : "Verifiability and Privacy in Federated Learning through Context-Hiding\n  Multi-Key Homomorphic Authenticators",
    "summary" : "Federated Learning has rapidly expanded from its original inception to now\nhave a large body of research, several frameworks, and sold in a variety of\ncommercial offerings. Thus, its security and robustness is of significant\nimportance. There are many algorithms that provide robustness in the case of\nmalicious clients. However, the aggregator itself may behave maliciously, for\nexample, by biasing the model or tampering with the weights to weaken the\nmodels privacy. In this work, we introduce a verifiable federated learning\nprotocol that enables clients to verify the correctness of the aggregators\ncomputation without compromising the confidentiality of their updates. Our\nprotocol uses a standard secure aggregation technique to protect individual\nmodel updates with a linearly homomorphic authenticator scheme that enables\nefficient, privacy-preserving verification of the aggregated result. Our\nconstruction ensures that clients can detect manipulation by the aggregator\nwhile maintaining low computational overhead. We demonstrate that our approach\nscales to large models, enabling verification over large neural networks with\nmillions of parameters.",
    "updated" : "2025-09-05T14:57:18Z",
    "published" : "2025-09-05T14:57:18Z",
    "authors" : [
      {
        "name" : "Simone Bottoni"
      },
      {
        "name" : "Giulio Zizzo"
      },
      {
        "name" : "Stefano Braghin"
      },
      {
        "name" : "Alberto Trombetta"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04919v1",
    "title" : "Optimal Variance and Covariance Estimation under Differential Privacy in\n  the Add-Remove Model and Beyond",
    "summary" : "In this paper, we study the problem of estimating the variance and covariance\nof datasets under differential privacy in the add-remove model. While\nestimation in the swap model has been extensively studied in the literature,\nthe add-remove model remains less explored and more challenging, as the dataset\nsize must also be kept private. To address this issue, we develop efficient\nmechanisms for variance and covariance estimation based on the \\emph{B\\'{e}zier\nmechanism}, a novel moment-release framework that leverages Bernstein bases. We\nprove that our proposed mechanisms are minimax optimal in the high-privacy\nregime by establishing new minimax lower bounds. Moreover, beyond worst-case\nscenarios, we analyze instance-wise utility and show that the B\\'{e}zier-based\nestimator consistently achieves better utility compared to alternative\nmechanisms. Finally, we demonstrate the effectiveness of the B\\'{e}zier\nmechanism beyond variance and covariance estimation, showcasing its\napplicability to other statistical tasks.",
    "updated" : "2025-09-05T08:37:30Z",
    "published" : "2025-09-05T08:37:30Z",
    "authors" : [
      {
        "name" : "Shokichi Takakura"
      },
      {
        "name" : "Seng Pei Liew"
      },
      {
        "name" : "Satoshi Hasegawa"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.DS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04710v1",
    "title" : "Network-Aware Differential Privacy",
    "summary" : "Differential privacy (DP) is a privacy-enhancement technology (PET) that\nreceives prominent attention from the academia, industry, and government. One\nmain development over the past decade has been the decentralization of DP,\nincluding local DP and shuffle DP. Despite that decentralized DP heavily relies\non network communications for data collection,we found that: 1) no systematic\nstudy has surveyed the research opportunities at the intersection of networking\nand DP; 2) nor have there been significant efforts to develop DP mechanisms\nthat are explicitly tailored for network environments. In this paper, we seek\nto address this gap by initiating a new direction of network-aware DP. We\nidentified two focus areas where the network research can offer substantive\ncontributions to the design and deployment of DP, related to network security\nand topology. Through this work, we hope to encourage more research that\nadapt/optimize DP's deployment in various network environments.",
    "updated" : "2025-09-04T23:53:54Z",
    "published" : "2025-09-04T23:53:54Z",
    "authors" : [
      {
        "name" : "Zhou Li"
      },
      {
        "name" : "Yu Zheng"
      },
      {
        "name" : "Tianhao Wang"
      },
      {
        "name" : "Sang-Woo Jun"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06444v1",
    "title" : "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
    "summary" : "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
    "updated" : "2025-09-08T08:44:24Z",
    "published" : "2025-09-08T08:44:24Z",
    "authors" : [
      {
        "name" : "Cheng Qian"
      },
      {
        "name" : "Hainan Zhang"
      },
      {
        "name" : "Yongxin Tong"
      },
      {
        "name" : "Hong-Wei Zheng"
      },
      {
        "name" : "Zhiming Zheng"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06368v1",
    "title" : "From Perception to Protection: A Developer-Centered Study of Security\n  and Privacy Threats in Extended Reality (XR)",
    "summary" : "The immersive nature of XR introduces a fundamentally different set of\nsecurity and privacy (S&P) challenges due to the unprecedented user\ninteractions and data collection that traditional paradigms struggle to\nmitigate. As the primary architects of XR applications, developers play a\ncritical role in addressing novel threats. However, to effectively support\ndevelopers, we must first understand how they perceive and respond to different\nthreats. Despite the growing importance of this issue, there is a lack of\nin-depth, threat-aware studies that examine XR S&P from the developers'\nperspective. To fill this gap, we interviewed 23 professional XR developers\nwith a focus on emerging threats in XR. Our study addresses two research\nquestions aiming to uncover existing problems in XR development and identify\nactionable paths forward.\n  By examining developers' perceptions of S&P threats, we found that: (1) XR\ndevelopment decisions (e.g., rich sensor data collection, user-generated\ncontent interfaces) are closely tied to and can amplify S&P threats, yet\ndevelopers are often unaware of these risks, resulting in cognitive biases in\nthreat perception; and (2) limitations in existing mitigation methods, combined\nwith insufficient strategic, technical, and communication support, undermine\ndevelopers' motivation, awareness, and ability to effectively address these\nthreats. Based on these findings, we propose actionable and stakeholder-aware\nrecommendations to improve XR S&P throughout the XR development process. This\nwork represents the first effort to undertake a threat-aware,\ndeveloper-centered study in the XR domain -- an area where the immersive,\ndata-rich nature of the XR technology introduces distinctive challenges.",
    "updated" : "2025-09-08T06:48:48Z",
    "published" : "2025-09-08T06:48:48Z",
    "authors" : [
      {
        "name" : "Kunlin Cai"
      },
      {
        "name" : "Jinghuai Zhang"
      },
      {
        "name" : "Ying Li"
      },
      {
        "name" : "Zhiyuan Wang"
      },
      {
        "name" : "Xun Chen"
      },
      {
        "name" : "Tianshi Li"
      },
      {
        "name" : "Yuan Tian"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06361v1",
    "title" : "Speaker Privacy and Security in the Big Data Era: Protection and Defense\n  against Deepfake",
    "summary" : "In the era of big data, remarkable advancements have been achieved in\npersonalized speech generation techniques that utilize speaker attributes,\nincluding voice and speaking style, to generate deepfake speech. This has also\namplified global security risks from deepfake speech misuse, resulting in\nconsiderable societal costs worldwide. To address the security threats posed by\ndeepfake speech, techniques have been developed focusing on both the protection\nof voice attributes and the defense against deepfake speech. Among them, the\nvoice anonymization technique has been developed to protect voice attributes\nfrom extraction for deepfake generation, while deepfake detection and\nwatermarking have been utilized to defend against the misuse of deepfake\nspeech. This paper provides a short and concise overview of the three\ntechniques, describing the methodologies, advancements, and challenges. A\ncomprehensive version, offering additional discussions, will be published in\nthe near future.",
    "updated" : "2025-09-08T06:22:36Z",
    "published" : "2025-09-08T06:22:36Z",
    "authors" : [
      {
        "name" : "Liping Chen"
      },
      {
        "name" : "Kong Aik Lee"
      },
      {
        "name" : "Zhen-Hua Ling"
      },
      {
        "name" : "Xin Wang"
      },
      {
        "name" : "Rohan Kumar Das"
      },
      {
        "name" : "Tomoki Toda"
      },
      {
        "name" : "Haizhou Li"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06264v1",
    "title" : "PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss\n  Random Variable Optimization",
    "summary" : "Differentially Private Stochastic Gradient Descent (DP-SGD) is a standard\nmethod for enforcing privacy in deep learning, typically using the Gaussian\nmechanism to perturb gradient updates. However, conventional mechanisms such as\nGaussian and Laplacian noise are parameterized only by variance or scale. This\nsingle degree of freedom ties the magnitude of noise directly to both privacy\nloss and utility degradation, preventing independent control of these two\nfactors. The problem becomes more pronounced when the number of composition\nrounds T and batch size B vary across tasks, as these variations induce\ntask-dependent shifts in the privacy-utility trade-off, where small changes in\nnoise parameters can disproportionately affect model accuracy. To address this\nlimitation, we introduce PLRV-O, a framework that defines a broad search space\nof parameterized DP-SGD noise distributions, where privacy loss moments are\ntightly characterized yet can be optimized more independently with respect to\nutility loss. This formulation enables systematic adaptation of noise to\ntask-specific requirements, including (i) model size, (ii) training duration,\n(iii) batch sampling strategies, and (iv) clipping thresholds under both\ntraining and fine-tuning settings. Empirical results demonstrate that PLRV-O\nsubstantially improves utility under strict privacy constraints. On CIFAR-10, a\nfine-tuned ViT achieves 94.03% accuracy at epsilon approximately 0.5, compared\nto 83.93% with Gaussian noise. On SST-2, RoBERTa-large reaches 92.20% accuracy\nat epsilon approximately 0.2, versus 50.25% with Gaussian.",
    "updated" : "2025-09-08T01:06:45Z",
    "published" : "2025-09-08T01:06:45Z",
    "authors" : [
      {
        "name" : "Qin Yang"
      },
      {
        "name" : "Nicholas Stout"
      },
      {
        "name" : "Meisam Mohammady"
      },
      {
        "name" : "Han Wang"
      },
      {
        "name" : "Ayesha Samreen"
      },
      {
        "name" : "Christopher J Quinn"
      },
      {
        "name" : "Yan Yan"
      },
      {
        "name" : "Ashish Kundu"
      },
      {
        "name" : "Yuan Hong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06142v1",
    "title" : "RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric\n  Privacy Preserving",
    "summary" : "The integration of AI with medical images enables the extraction of implicit\nimage-derived biomarkers for a precise health assessment. Recently, retinal\nage, a biomarker predicted from fundus images, is a proven predictor of\nsystemic disease risks, behavioral patterns, aging trajectory and even\nmortality. However, the capability to infer such sensitive biometric data\nraises significant privacy risks, where unauthorized use of fundus images could\nlead to bioinformation leakage, breaching individual privacy. In response, we\nformulate a new research problem of biometric privacy associated with medical\nimages and propose RetinaGuard, a novel privacy-enhancing framework that\nemploys a feature-level generative adversarial masking mechanism to obscure\nretinal age while preserving image visual quality and disease diagnostic\nutility. The framework further utilizes a novel multiple-to-one knowledge\ndistillation strategy incorporating a retinal foundation model and diverse\nsurrogate age encoders to enable a universal defense against black-box age\nprediction models. Comprehensive evaluations confirm that RetinaGuard\nsuccessfully obfuscates retinal age prediction with minimal impact on image\nquality and pathological feature representation. RetinaGuard is also flexible\nfor extension to other medical image derived biomarkers. RetinaGuard is also\nflexible for extension to other medical image biomarkers.",
    "updated" : "2025-09-07T17:16:42Z",
    "published" : "2025-09-07T17:16:42Z",
    "authors" : [
      {
        "name" : "Zhengquan Luo"
      },
      {
        "name" : "Chi Liu"
      },
      {
        "name" : "Dongfu Xiao"
      },
      {
        "name" : "Zhen Yu"
      },
      {
        "name" : "Yueye Wang"
      },
      {
        "name" : "Tianqing Zhu"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06133v1",
    "title" : "VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored\n  Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles",
    "summary" : "Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,\nand service centers that are difficult to verify and prone to fraud. We propose\nVehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with\nzero-knowledge proofs (ZKPs) for privacy-preserving verification.\nVehiclePassport immutably commits to manufacturing, telemetry, and service\nevents while enabling selective disclosure via short-lived JWTs and Groth16\nproofs. Our open-source reference stack anchors hashes on Polygon zkEVM at\n<$0.02 per event, validates proofs in <10 ms, and scales to millions of\nvehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant\ntraceability, and establishes a trustless foundation for insurance, resale, and\nregulatory applications in global mobility data markets.",
    "updated" : "2025-09-07T16:40:30Z",
    "published" : "2025-09-07T16:40:30Z",
    "authors" : [
      {
        "name" : "Pradyumna Kaushal"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.SE",
      "cs.SY",
      "eess.SY",
      "C.2.4; K.6.5; D.4.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05608v1",
    "title" : "Cross-Service Threat Intelligence in LLM Services using\n  Privacy-Preserving Fingerprints",
    "summary" : "The widespread deployment of LLMs across enterprise services has created a\ncritical security blind spot. Organizations operate multiple LLM services\nhandling billions of queries daily, yet regulatory compliance boundaries\nprevent these services from sharing threat intelligence about prompt injection\nattacks, the top security risk for LLMs. When an attack is detected in one\nservice, the same threat may persist undetected in others for months, as\nprivacy regulations prohibit sharing user prompts across compliance boundaries.\n  We present BinaryShield, the first privacy-preserving threat intelligence\nsystem that enables secure sharing of attack fingerprints across compliance\nboundaries. BinaryShield transforms suspicious prompts through a unique\npipeline combining PII redaction, semantic embedding, binary quantization, and\nrandomized response mechanism to potentially generate non-invertible\nfingerprints that preserve attack patterns while providing privacy. Our\nevaluations demonstrate that BinaryShield achieves an F1-score of 0.94,\nsignificantly outperforming SimHash (0.77), the privacy-preserving baseline,\nwhile achieving 64x storage reduction and 38x faster similarity search compared\nto dense embeddings.",
    "updated" : "2025-09-06T05:57:20Z",
    "published" : "2025-09-06T05:57:20Z",
    "authors" : [
      {
        "name" : "Waris Gill"
      },
      {
        "name" : "Natalie Isak"
      },
      {
        "name" : "Matthew Dressman"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05382v1",
    "title" : "User Privacy and Large Language Models: An Analysis of Frontier\n  Developers' Privacy Policies",
    "summary" : "Hundreds of millions of people now regularly interact with large language\nmodels via chatbots. Model developers are eager to acquire new sources of\nhigh-quality training data as they race to improve model capabilities and win\nmarket share. This paper analyzes the privacy policies of six U.S. frontier AI\ndevelopers to understand how they use their users' chats to train models.\nDrawing primarily on the California Consumer Privacy Act, we develop a novel\nqualitative coding schema that we apply to each developer's relevant privacy\npolicies to compare data collection and use practices across the six companies.\nWe find that all six developers appear to employ their users' chat data to\ntrain and improve their models by default, and that some retain this data\nindefinitely. Developers may collect and train on personal information\ndisclosed in chats, including sensitive information such as biometric and\nhealth data, as well as files uploaded by users. Four of the six companies we\nexamined appear to include children's chat data for model training, as well as\ncustomer data from other products. On the whole, developers' privacy policies\noften lack essential information about their practices, highlighting the need\nfor greater transparency and accountability. We address the implications of\nusers' lack of consent for the use of their chat data for model training, data\nsecurity issues arising from indefinite chat data retention, and training on\nchildren's chat data. We conclude by providing recommendations to policymakers\nand developers to address the data privacy challenges posed by LLM-powered\nchatbots.",
    "updated" : "2025-09-05T01:01:21Z",
    "published" : "2025-09-05T01:01:21Z",
    "authors" : [
      {
        "name" : "Jennifer King"
      },
      {
        "name" : "Kevin Klyman"
      },
      {
        "name" : "Emily Capstick"
      },
      {
        "name" : "Tiffany Saade"
      },
      {
        "name" : "Victoria Hsieh"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05377v1",
    "title" : "Enhancing Gradient Variance and Differential Privacy in Quantum\n  Federated Learning",
    "summary" : "Upon integrating Quantum Neural Network (QNN) as the local model, Quantum\nFederated Learning (QFL) has recently confronted notable challenges. Firstly,\nexploration is hindered over sharp minima, decreasing learning performance.\nSecondly, the steady gradient descent results in more stable and predictable\nmodel transmissions over wireless channels, making the model more susceptible\nto attacks from adversarial entities. Additionally, the local QFL model is\nvulnerable to noise produced by the quantum device's intermediate noise states,\nsince it requires the use of quantum gates and circuits for training. This\nlocal noise becomes intertwined with learning parameters during training,\nimpairing model precision and convergence rate. To address these issues, we\npropose a new QFL technique that incorporates differential privacy and\nintroduces a dedicated noise estimation strategy to quantify and mitigate the\nimpact of intermediate quantum noise. Furthermore, we design an adaptive noise\ngeneration scheme to alleviate privacy threats associated with the vanishing\ngradient variance phenomenon of QNN and enhance robustness against device\nnoise. Experimental results demonstrate that our algorithm effectively balances\nconvergence, reduces communication costs, and mitigates the adverse effects of\nintermediate quantum noise while maintaining strong privacy protection. Using\nreal-world datasets, we achieved test accuracy of up to 98.47\\% for the MNIST\ndataset and 83.85\\% for the CIFAR-10 dataset while maintaining fast execution\ntimes.",
    "updated" : "2025-09-04T15:29:52Z",
    "published" : "2025-09-04T15:29:52Z",
    "authors" : [
      {
        "name" : "Duc-Thien Phan"
      },
      {
        "name" : "Minh-Duong Nguyen"
      },
      {
        "name" : "Quoc-Viet Pham"
      },
      {
        "name" : "Huilong Pi"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05376v1",
    "title" : "Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye\n  Tracking for Interactive Learning Environments",
    "summary" : "Eye-tracking technology can aid in understanding neurodevelopmental disorders\nand tracing a person's identity. However, this technology poses a significant\nrisk to privacy, as it captures sensitive information about individuals and\nincreases the likelihood that data can be traced back to them. This paper\nproposes a human-centered framework designed to prevent identity backtracking\nwhile preserving the pedagogical benefits of AI-powered eye tracking in\ninteractive learning environments. We explore how real-time data anonymization,\nethical design principles, and regulatory compliance (such as GDPR) can be\nintegrated to build trust and transparency. We first demonstrate the potential\nfor backtracking student IDs and diagnoses in various scenarios using serious\ngame-based eye-tracking data. We then provide a two-stage privacy-preserving\nframework that prevents participants from being tracked while still enabling\ndiagnostic classification. The first phase covers four scenarios: I) Predicting\ndisorder diagnoses based on different game levels. II) Predicting student IDs\nbased on different game levels. III) Predicting student IDs based on randomized\ndata. IV) Utilizing K-Means for out-of-sample data. In the second phase, we\npresent a two-stage framework that preserves privacy. We also employ Federated\nLearning (FL) across multiple clients, incorporating a secure identity\nmanagement system with dummy IDs and administrator-only access controls. In the\nfirst phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%\naccuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully\nidentifying and assigning a new student ID in scenario 4. In phase 2, we\neffectively prevented backtracking and established a secure identity management\nsystem with dummy IDs and administrator-only access controls, achieving an\noverall accuracy of 99.40%.",
    "updated" : "2025-09-04T13:08:06Z",
    "published" : "2025-09-04T13:08:06Z",
    "authors" : [
      {
        "name" : "Abdul Rehman"
      },
      {
        "name" : "Are Dæhlen"
      },
      {
        "name" : "Ilona Heldal"
      },
      {
        "name" : "Jerry Chun-wei Lin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05362v1",
    "title" : "AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and\n  Conversational Scambaiting by Leveraging LLMs and Federated Learning",
    "summary" : "Scams exploiting real-time social engineering -- such as phishing,\nimpersonation, and phone fraud -- remain a persistent and evolving threat\nacross digital platforms. Existing defenses are largely reactive, offering\nlimited protection during active interactions. We propose a privacy-preserving,\nAI-in-the-loop framework that proactively detects and disrupts scam\nconversations in real time. The system combines instruction-tuned artificial\nintelligence with a safety-aware utility function that balances engagement with\nharm minimization, and employs federated learning to enable continual model\nupdates without raw data sharing. Experimental evaluations show that the system\nproduces fluent and engaging responses (perplexity as low as 22.3, engagement\n$\\approx$0.80), while human studies confirm significant gains in realism,\nsafety, and effectiveness over strong baselines. In federated settings, models\ntrained with FedAvg sustain up to 30 rounds while preserving high engagement\n($\\approx$0.80), strong relevance ($\\approx$0.74), and low PII leakage\n($\\leq$0.0085). Even with differential privacy, novelty and safety remain\nstable, indicating that robust privacy can be achieved without sacrificing\nperformance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,\nMD-Judge) shows a straightforward pattern: stricter moderation settings reduce\nthe chance of exposing personal information, but they also limit how much the\nmodel engages in conversation. In contrast, more relaxed settings allow longer\nand richer interactions, which improve scam detection, but at the cost of\nhigher privacy risk. To our knowledge, this is the first framework to unify\nreal-time scam-baiting, federated privacy preservation, and calibrated safety\nmoderation into a proactive defense paradigm.",
    "updated" : "2025-09-04T00:19:48Z",
    "published" : "2025-09-04T00:19:48Z",
    "authors" : [
      {
        "name" : "Ismail Hossain"
      },
      {
        "name" : "Sai Puppala"
      },
      {
        "name" : "Sajedul Talukder"
      },
      {
        "name" : "Md Jahangir Alam"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.07131v1",
    "title" : "SoK: Security and Privacy of AI Agents for Blockchain",
    "summary" : "Blockchain and smart contracts have garnered significant interest in recent\nyears as the foundation of a decentralized, trustless digital ecosystem,\nthereby eliminating the need for traditional centralized authorities. Despite\ntheir central role in powering Web3, their complexity still presents\nsignificant barriers for non-expert users. To bridge this gap, Artificial\nIntelligence (AI)-based agents have emerged as valuable tools for interacting\nwith blockchain environments, supporting a range of tasks, from analyzing\non-chain data and optimizing transaction strategies to detecting\nvulnerabilities within smart contracts. While interest in applying AI to\nblockchain is growing, the literature still lacks a comprehensive survey that\nfocuses specifically on the intersection with AI agents. Most of the related\nwork only provides general considerations, without focusing on any specific\ndomain. This paper addresses this gap by presenting the first Systematization\nof Knowledge dedicated to AI-driven systems for blockchain, with a special\nfocus on their security and privacy dimensions, shedding light on their\napplications, limitations, and future research directions.",
    "updated" : "2025-09-08T18:32:15Z",
    "published" : "2025-09-08T18:32:15Z",
    "authors" : [
      {
        "name" : "Nicolò Romandini"
      },
      {
        "name" : "Carlo Mazzocca"
      },
      {
        "name" : "Kai Otsuki"
      },
      {
        "name" : "Rebecca Montanari"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.07055v1",
    "title" : "Sequentially Auditing Differential Privacy",
    "summary" : "We propose a practical sequential test for auditing differential privacy\nguarantees of black-box mechanisms. The test processes streams of mechanisms'\noutputs providing anytime-valid inference while controlling Type I error,\novercoming the fixed sample size limitation of previous batch auditing methods.\nExperiments show this test detects violations with sample sizes that are orders\nof magnitude smaller than existing methods, reducing this number from 50K to a\nfew hundred examples, across diverse realistic mechanisms. Notably, it\nidentifies DP-SGD privacy violations in \\textit{under} one training run, unlike\nprior methods needing full model training.",
    "updated" : "2025-09-08T17:57:51Z",
    "published" : "2025-09-08T17:57:51Z",
    "authors" : [
      {
        "name" : "Tomás González"
      },
      {
        "name" : "Mateo Dulce-Rubio"
      },
      {
        "name" : "Aaditya Ramdas"
      },
      {
        "name" : "Mónica Ribero"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06361v2",
    "title" : "Speaker Privacy and Security in the Big Data Era: Protection and Defense\n  against Deepfake",
    "summary" : "In the era of big data, remarkable advancements have been achieved in\npersonalized speech generation techniques that utilize speaker attributes,\nincluding voice and speaking style, to generate deepfake speech. This has also\namplified global security risks from deepfake speech misuse, resulting in\nconsiderable societal costs worldwide. To address the security threats posed by\ndeepfake speech, techniques have been developed focusing on both the protection\nof voice attributes and the defense against deepfake speech. Among them, the\nvoice anonymization technique has been developed to protect voice attributes\nfrom extraction for deepfake generation, while deepfake detection and\nwatermarking have been utilized to defend against the misuse of deepfake\nspeech. This paper provides a short and concise overview of the three\ntechniques, describing the methodologies, advancements, and challenges. A\ncomprehensive version, offering additional discussions, will be published in\nthe near future.",
    "updated" : "2025-09-09T04:29:25Z",
    "published" : "2025-09-08T06:22:36Z",
    "authors" : [
      {
        "name" : "Liping Chen"
      },
      {
        "name" : "Kong Aik Lee"
      },
      {
        "name" : "Zhen-Hua Ling"
      },
      {
        "name" : "Xin Wang"
      },
      {
        "name" : "Rohan Kumar Das"
      },
      {
        "name" : "Tomoki Toda"
      },
      {
        "name" : "Haizhou Li"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.08804v1",
    "title" : "Approximate Algorithms for Verifying Differential Privacy with Gaussian\n  Distributions",
    "summary" : "The verification of differential privacy algorithms that employ Gaussian\ndistributions is little understood. This paper tackles the challenge of\nverifying such programs by introducing a novel approach to approximating\nprobability distributions of loop-free programs that sample from both discrete\nand continuous distributions with computable probability density functions,\nincluding Gaussian and Laplace. We establish that verifying\n$(\\epsilon,\\delta)$-differential privacy for these programs is \\emph{almost\ndecidable}, meaning the problem is decidable for all values of $\\delta$ except\nthose in a finite set. Our verification algorithm is based on computing\nprobabilities to any desired precision by combining integral approximations,\nand tail probability bounds. The proposed methods are implemented in the tool,\nDipApprox, using the FLINT library for high-precision integral computations,\nand incorporate optimizations to enhance scalability. We validate {\\ourtool} on\nfundamental privacy-preserving algorithms, such as Gaussian variants of the\nSparse Vector Technique and Noisy Max, demonstrating its effectiveness in both\nconfirming privacy guarantees and detecting violations.",
    "updated" : "2025-09-10T17:37:56Z",
    "published" : "2025-09-10T17:37:56Z",
    "authors" : [
      {
        "name" : "Bishnu Bhusal"
      },
      {
        "name" : "Rohit Chadha"
      },
      {
        "name" : "A. Prasad Sistla"
      },
      {
        "name" : "Mahesh Viswanathan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.PL",
      "D.2.5; F.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.08782v1",
    "title" : "Extended Version: Security and Privacy Perceptions of Pakistani Facebook\n  Matrimony Group Users",
    "summary" : "In Pakistan, where dating apps are subject to censorship, Facebook matrimony\ngroups -- also referred to as marriage groups -- serve as alternative virtual\nspaces for members to search for potential life partners. To participate in\nthese groups, members often share sensitive personal information such as\nphotos, addresses, and phone numbers, which exposes them to risks such as\nfraud, blackmail, and identity theft. To better protect users of Facebook\nmatrimony groups, we need to understand aspects related to user safety, such as\nhow users perceive risks, what influences their trust in sharing personal\ninformation, and how they navigate security and privacy concerns when seeking\npotential partners online. In this study, through 23 semi-structured\ninterviews, we explore how Pakistani users of Facebook matrimony groups\nperceive and navigate risks of sharing personal information, and how cultural\nnorms and expectations influence their behavior in these groups.\n  We find elevated privacy concerns among participants, leading them to share\nlimited personal information and creating mistrust among potential partners.\nMany also expressed concerns about the authenticity of profiles and major\nsecurity risks, such as identity theft, harassment, and social judgment. Our\nwork highlights the challenges of safely navigating Facebook matrimony groups\nin Pakistan and offers recommendations for such as implementing stronger\nidentity verification by group admins, enforcing stricter cybersecurity laws,\nclear platform guidelines to ensure accountability, and technical feature\nenhancements -- including restricting screenshots, picture downloads, and\nimplementing anonymous chats -- to protect user data and build trust.",
    "updated" : "2025-09-10T17:12:23Z",
    "published" : "2025-09-10T17:12:23Z",
    "authors" : [
      {
        "name" : "Mah Jan Dorazahi"
      },
      {
        "name" : "Deepthi Mungara"
      },
      {
        "name" : "Yasemin Acar"
      },
      {
        "name" : "Harshini Sri Ramulu"
      }
    ],
    "categories" : [
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.08722v1",
    "title" : "SilentLedger: Privacy-Preserving Auditing for Blockchains with Complete\n  Non-Interactivity",
    "summary" : "Privacy-preserving blockchain systems are essential for protecting\ntransaction data, yet they must also provide auditability that enables auditors\nto recover participant identities and transaction amounts when warranted.\nExisting designs often compromise the independence of auditing and\ntransactions, introducing extra interactions that undermine usability and\nscalability. Moreover, many auditable solutions depend on auditors serving as\nvalidators or recording nodes, which introduces risks to both data security and\nsystem reliability.\n  To overcome these challenges, we propose SilentLedger, a privacy-preserving\ntransaction system with auditing and complete non-interactivity. To support\npublic verification of authorization, we introduce a renewable anonymous\ncertificate scheme with formal semantics and a rigorous security model.\nSilentLedger further employs traceable transaction mechanisms constructed from\nestablished cryptographic primitives, enabling users to transact without\ninteraction while allowing auditors to audit solely from on-chain data. We\nformally prove security properties including authenticity, anonymity,\nconfidentiality, and soundness, provide a concrete instantiation, and evaluate\nperformance under a standard 2-2 transaction model. Our implementation and\nbenchmarks demonstrate that SilentLedger achieves superior performance compared\nwith state-of-the-art solutions.",
    "updated" : "2025-09-10T16:14:34Z",
    "published" : "2025-09-10T16:14:34Z",
    "authors" : [
      {
        "name" : "Zihan Liu"
      },
      {
        "name" : "Xiaohu Wang"
      },
      {
        "name" : "Chao Lin"
      },
      {
        "name" : "Minghui Xu"
      },
      {
        "name" : "Debiao He"
      },
      {
        "name" : "Xinyi Huang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.08720v1",
    "title" : "PAnDA: Rethinking Metric Differential Privacy Optimization at Scale with\n  Anchor-Based Approximation",
    "summary" : "Metric Differential Privacy (mDP) extends the local differential privacy\n(LDP) framework to metric spaces, enabling more nuanced privacy protection for\ndata such as geo-locations. However, existing mDP optimization methods,\nparticularly those based on linear programming (LP), face scalability\nchallenges due to the quadratic growth in decision variables. In this paper, we\npropose Perturbation via Anchor-based Distributed Approximation (PAnDA), a\nscalable two-phase framework for optimizing metric differential privacy (mDP).\nTo reduce computational overhead, PAnDA allows each user to select a small set\nof anchor records, enabling the server to solve a compact linear program over a\nreduced domain. We introduce three anchor selection strategies, exponential\ndecay (PAnDA-e), power-law decay (PAnDA-p), and logistic decay (PAnDA-l), and\nestablish theoretical guarantees under a relaxed privacy notion called\nprobabilistic mDP (PmDP). Experiments on real-world geo-location datasets\ndemonstrate that PAnDA scales to secret domains with up to 5,000 records, two\ntimes larger than prior LP-based methods, while providing theoretical\nguarantees for both privacy and utility.",
    "updated" : "2025-09-10T16:14:08Z",
    "published" : "2025-09-10T16:14:08Z",
    "authors" : [
      {
        "name" : "Ruiyao Liu"
      },
      {
        "name" : "Chenxi Qiu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.08704v1",
    "title" : "Tight Privacy Audit in One Run",
    "summary" : "In this paper, we study the problem of privacy audit in one run and show that\nour method achieves tight audit results for various differentially private\nprotocols. This includes obtaining tight results for auditing\n$(\\varepsilon,\\delta)$-DP algorithms where all previous work fails to achieve\nin any parameter setups. We first formulate a framework for privacy audit\n\\textit{in one run} with refinement compared with previous work. Then, based on\nmodeling privacy by the $f$-DP formulation, we study the implications of our\nframework to obtain a theoretically justified lower bound for privacy audit. In\nthe experiment, we compare with previous work and show that our audit method\noutperforms the rest in auditing various differentially private algorithms. We\nalso provide experiments that give contrasting conclusions to previous work on\nthe parameter settings for privacy audits in one run.",
    "updated" : "2025-09-10T15:55:03Z",
    "published" : "2025-09-10T15:55:03Z",
    "authors" : [
      {
        "name" : "Zihang Xiang"
      },
      {
        "name" : "Tianhao Wang"
      },
      {
        "name" : "Hanshen Xiao"
      },
      {
        "name" : "Yuan Tian"
      },
      {
        "name" : "Di Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.08554v1",
    "title" : "Acceptability of AI Assistants for Privacy: Perceptions of Experts and\n  Users on Personalized Privacy Assistants",
    "summary" : "Individuals increasingly face an overwhelming number of tasks and decisions.\nTo cope with the new reality, there is growing research interest in developing\nintelligent agents that can effectively assist people across various aspects of\ndaily life in a tailored manner, with privacy emerging as a particular area of\napplication. Artificial intelligence (AI) assistants for privacy, such as\npersonalized privacy assistants (PPAs), have the potential to automatically\nexecute privacy decisions based on users' pre-defined privacy preferences,\nsparing them the mental effort and time usually spent on each privacy decision.\nThis helps ensure that, even when users feel overwhelmed or resigned about\nprivacy, the decisions made by PPAs still align with their true preferences and\nbest interests. While research has explored possible designs of such agents,\nuser and expert perspectives on the acceptability of such AI-driven solutions\nremain largely unexplored. In this study, we conducted five focus groups with\ndomain experts (n = 11) and potential users (n = 26) to uncover key themes\nshaping the acceptance of PPAs. Factors influencing the acceptability of AI\nassistants for privacy include design elements (such as information sources\nused by the agent), external conditions (such as regulation and literacy\neducation), and systemic conditions (e.g., public or market providers and the\nneed to avoid monopoly) to PPAs. These findings provide theoretical extensions\nto technology acceptance models measuring PPAs, insights on design, and policy\nimplications for PPAs, as well as broader implications for the design of AI\nassistants.",
    "updated" : "2025-09-10T12:59:39Z",
    "published" : "2025-09-10T12:59:39Z",
    "authors" : [
      {
        "name" : "Meihe Xu"
      },
      {
        "name" : "Aurelia Tamò-Larrieux"
      },
      {
        "name" : "Arianna Rossi"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.08387v1",
    "title" : "Infinite Stream Estimation under Personalized $w$-Event Privacy",
    "summary" : "Streaming data collection is indispensable for stream data analysis, such as\nevent monitoring. However, publishing these data directly leads to privacy\nleaks. $w$-event privacy is a valuable tool to protect individual privacy\nwithin a given time window while maintaining high accuracy in data collection.\nMost existing $w$-event privacy studies on infinite data stream only focus on\nhomogeneous privacy requirements for all users. In this paper, we propose\npersonalized $w$-event privacy protection that allows different users to have\ndifferent privacy requirements in private data stream estimation. Specifically,\nwe design a mechanism that allows users to maintain constant privacy\nrequirements at each time slot, namely Personalized Window Size Mechanism\n(PWSM). Then, we propose two solutions to accurately estimate stream data\nstatistics while achieving $w$-event level $\\epsilon$ personalized differential\nprivacy ( ($w$, $\\epsilon$)-EPDP), namely Personalized Budget Distribution\n(PBD) and Peronalized Budget Absorption (PBA). PBD always provides at least the\nsame privacy budget for the next time step as the amount consumed in the\nprevious release. PBA fully absorbs the privacy budget from the previous $k$\ntime slots, while also borrowing from the privacy budget of the next $k$ time\nslots, to increase the privacy budget for the current time slot. We prove that\nboth PBD and PBA outperform the state-of-the-art private stream estimation\nmethods while satisfying the privacy requirements of all users. We demonstrate\nthe efficiency and effectiveness of our PBD and PBA on both real and synthetic\ndata sets, compared with the recent uniformity $w$-event approaches, Budget\nDistribution (BD) and Budget Absorption (BA). Our PBD achieves 68% less error\nthan BD on average on real data sets. Besides, our PBA achieves 24.9% less\nerror than BA on average on synthetic data sets.",
    "updated" : "2025-09-10T08:27:20Z",
    "published" : "2025-09-10T08:27:20Z",
    "authors" : [
      {
        "name" : "Leilei Du"
      },
      {
        "name" : "Peng Cheng"
      },
      {
        "name" : "Lei Chen"
      },
      {
        "name" : "Heng Tao Shen"
      },
      {
        "name" : "Xuemin Lin"
      },
      {
        "name" : "Wei Xi"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.08142v1",
    "title" : "Privacy Preserving Semantic Communications Using Vision Language Models:\n  A Segmentation and Generation Approach",
    "summary" : "Semantic communication has emerged as a promising paradigm for\nnext-generation wireless systems, improving the communication efficiency by\ntransmitting high-level semantic features. However, reliance on unimodal\nrepresentations can degrade reconstruction under poor channel conditions, and\nprivacy concerns of the semantic information attack also gain increasing\nattention. In this work, a privacy-preserving semantic communication framework\nis proposed to protect sensitive content of the image data. Leveraging a\nvision-language model (VLM), the proposed framework identifies and removes\nprivate content regions from input images prior to transmission. A shared\nprivacy database enables semantic alignment between the transmitter and\nreceiver to ensure consistent identification of sensitive entities. At the\nreceiver, a generative module reconstructs the masked regions using learned\nsemantic priors and conditioned on the received text embedding. Simulation\nresults show that generalizes well to unseen image processing tasks, improves\nreconstruction quality at the authorized receiver by over 10% using text\nembedding, and reduces identity leakage to the eavesdropper by more than 50%.",
    "updated" : "2025-09-09T20:49:05Z",
    "published" : "2025-09-09T20:49:05Z",
    "authors" : [
      {
        "name" : "Haoran Chang"
      },
      {
        "name" : "Mingzhe Chen"
      },
      {
        "name" : "Huaxia Wang"
      },
      {
        "name" : "Qianqian Zhang"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.08018v1",
    "title" : "Enhancing Privacy Preservation and Reducing Analysis Time with Federated\n  Transfer Learning in Digital Twins-based Computed Tomography Scan Analysis",
    "summary" : "The application of Digital Twin (DT) technology and Federated Learning (FL)\nhas great potential to change the field of biomedical image analysis,\nparticularly for Computed Tomography (CT) scans. This paper presents Federated\nTransfer Learning (FTL) as a new Digital Twin-based CT scan analysis paradigm.\nFTL uses pre-trained models and knowledge transfer between peer nodes to solve\nproblems such as data privacy, limited computing resources, and data\nheterogeneity. The proposed framework allows real-time collaboration between\ncloud servers and Digital Twin-enabled CT scanners while protecting patient\nidentity. We apply the FTL method to a heterogeneous CT scan dataset and assess\nmodel performance using convergence time, model accuracy, precision, recall, F1\nscore, and confusion matrix. It has been shown to perform better than\nconventional FL and Clustered Federated Learning (CFL) methods with better\nprecision, accuracy, recall, and F1-score. The technique is beneficial in\nsettings where the data is not independently and identically distributed\n(non-IID), and it offers reliable, efficient, and secure solutions for medical\ndiagnosis. These findings highlight the possibility of using FTL to improve\ndecision-making in digital twin-based CT scan analysis, secure and efficient\nmedical image analysis, promote privacy, and open new possibilities for\napplying precision medicine and smart healthcare systems.",
    "updated" : "2025-09-09T04:54:08Z",
    "published" : "2025-09-09T04:54:08Z",
    "authors" : [
      {
        "name" : "Avais Jan"
      },
      {
        "name" : "Qasim Zia"
      },
      {
        "name" : "Murray Patterson"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06142v2",
    "title" : "RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric\n  Privacy Preserving",
    "summary" : "The integration of AI with medical images enables the extraction of implicit\nimage-derived biomarkers for a precise health assessment. Recently, retinal\nage, a biomarker predicted from fundus images, is a proven predictor of\nsystemic disease risks, behavioral patterns, aging trajectory and even\nmortality. However, the capability to infer such sensitive biometric data\nraises significant privacy risks, where unauthorized use of fundus images could\nlead to bioinformation leakage, breaching individual privacy. In response, we\nformulate a new research problem of biometric privacy associated with medical\nimages and propose RetinaGuard, a novel privacy-enhancing framework that\nemploys a feature-level generative adversarial masking mechanism to obscure\nretinal age while preserving image visual quality and disease diagnostic\nutility. The framework further utilizes a novel multiple-to-one knowledge\ndistillation strategy incorporating a retinal foundation model and diverse\nsurrogate age encoders to enable a universal defense against black-box age\nprediction models. Comprehensive evaluations confirm that RetinaGuard\nsuccessfully obfuscates retinal age prediction with minimal impact on image\nquality and pathological feature representation. RetinaGuard is also flexible\nfor extension to other medical image derived biomarkers. RetinaGuard is also\nflexible for extension to other medical image biomarkers.",
    "updated" : "2025-09-10T02:06:48Z",
    "published" : "2025-09-07T17:16:42Z",
    "authors" : [
      {
        "name" : "Zhengquan Luo"
      },
      {
        "name" : "Chi Liu"
      },
      {
        "name" : "Dongfu Xiao"
      },
      {
        "name" : "Zhen Yu"
      },
      {
        "name" : "Yueye Wang"
      },
      {
        "name" : "Tianqing Zhu"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.09485v1",
    "title" : "Balancing Utility and Privacy: Dynamically Private SGD with Random\n  Projection",
    "summary" : "Stochastic optimization is a pivotal enabler in modern machine learning,\nproducing effective models for various tasks. However, several existing works\nhave shown that model parameters and gradient information are susceptible to\nprivacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy\nconcerns, its static noise mechanism impacts the error bounds for model\nperformance. Additionally, with the exponential increase in model parameters,\nefficient learning of these models using stochastic optimizers has become more\nchallenging. To address these concerns, we introduce the Dynamically\nDifferentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we\ncombine two important ideas: (i) dynamic differential privacy (DDP) with\nautomatic gradient clipping and (ii) random projection with SGD, allowing\ndynamic adjustment of the tradeoff between utility and privacy of the model. It\nexhibits provably sub-linear convergence rates across different objective\nfunctions, matching the best available rate. The theoretical analysis further\nsuggests that DDP leads to better utility at the cost of privacy, while random\nprojection enables more efficient model learning. Extensive experiments across\ndiverse datasets show that D2P2-SGD remarkably enhances accuracy while\nmaintaining privacy. Our code is available here.",
    "updated" : "2025-09-11T14:17:04Z",
    "published" : "2025-09-11T14:17:04Z",
    "authors" : [
      {
        "name" : "Zhanhong Jiang"
      },
      {
        "name" : "Md Zahid Hasan"
      },
      {
        "name" : "Nastaran Saadati"
      },
      {
        "name" : "Aditya Balu"
      },
      {
        "name" : "Chao Liu"
      },
      {
        "name" : "Soumik Sarkar"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.09285v1",
    "title" : "The Impact of Device Type, Data Practices, and Use Case Scenarios on\n  Privacy Concerns about Eye-tracked Augmented Reality in the United States and\n  Germany",
    "summary" : "Augmented reality technology will likely be prevalent with more affordable\nhead-mounted displays. Integrating novel interaction modalities such as eye\ntrackers into head-mounted displays could lead to collecting vast amounts of\nbiometric data, which may allow inference of sensitive user attributes like\nhealth status or sexual preference, posing privacy issues. While previous works\nbroadly examined privacy concerns about augmented reality, ours is the first to\nextensively explore privacy concerns on behavioral data, particularly eye\ntracking in augmented reality. We crowdsourced four survey studies in the\nUnited States (n1 = 48, n2 = 525) and Germany (n3 = 48, n4 = 525) to understand\nthe impact of user attributes, augmented reality devices, use cases, data\npractices, and country on privacy concerns. Our findings indicate that\nparticipants are generally concerned about privacy when they know what\ninferences can be made based on the collected data. Despite the more prominent\nuse of smartphones in daily life than augmented reality glasses, we found no\nindications of differing privacy concerns depending on the device type. In\naddition, our participants are more comfortable when a particular use case\nbenefits them and less comfortable when other humans can consume their data.\nFurthermore, participants in the United States are less concerned about their\nprivacy than those in Germany. Based on our findings, we provide several\nrecommendations to practitioners and policymakers for privacy-aware augmented\nreality.",
    "updated" : "2025-09-11T09:21:27Z",
    "published" : "2025-09-11T09:21:27Z",
    "authors" : [
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Babette Bühler"
      },
      {
        "name" : "Xiaoyuan Wu"
      },
      {
        "name" : "Enkelejda Kasneci"
      },
      {
        "name" : "Lujo Bauer"
      },
      {
        "name" : "Lorrie Faith Cranor"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.09103v1",
    "title" : "AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System",
    "summary" : "Crop diseases pose significant threats to global food security, agricultural\nproductivity, and sustainable farming practices, directly affecting farmers'\nlivelihoods and economic stability. To address the growing need for effective\ncrop disease management, AI-based disease alerting systems have emerged as\npromising tools by providing early detection and actionable insights for timely\nintervention. However, existing systems often overlook critical aspects such as\ndata privacy, market pricing power, and farmer-friendly usability, leaving\nfarmers vulnerable to privacy breaches and economic exploitation. To bridge\nthese gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM\nCrop Disease Alerting System. AgriSentinel incorporates a differential privacy\nmechanism to protect sensitive crop image data while maintaining classification\naccuracy. Its lightweight deep learning-based crop disease classification model\nis optimized for mobile devices, ensuring accessibility and usability for\nfarmers. Additionally, the system includes a fine-tuned, on-device large\nlanguage model (LLM) that leverages a curated knowledge pool to provide farmers\nwith specific, actionable suggestions for managing crop diseases, going beyond\nsimple alerting. Comprehensive experiments validate the effectiveness of\nAgriSentinel, demonstrating its ability to safeguard data privacy, maintain\nhigh classification performance, and deliver practical, actionable disease\nmanagement strategies. AgriSentinel offers a robust, farmer-friendly solution\nfor automating crop disease alerting and management, ultimately contributing to\nimproved agricultural decision-making and enhanced crop productivity.",
    "updated" : "2025-09-11T02:29:19Z",
    "published" : "2025-09-11T02:29:19Z",
    "authors" : [
      {
        "name" : "Chanti Raju Mylay"
      },
      {
        "name" : "Bobin Deng"
      },
      {
        "name" : "Zhipeng Cai"
      },
      {
        "name" : "Honghui Xu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.09097v1",
    "title" : "DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large\n  Language Models",
    "summary" : "As on-device large language model (LLM) systems become increasingly\nprevalent, federated fine-tuning enables advanced language understanding and\ngeneration directly on edge devices; however, it also involves processing\nsensitive, user-specific data, raising significant privacy concerns within the\nfederated learning framework. To address these challenges, we propose\nDP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates\nLoRA-based adaptation with differential privacy in a communication-efficient\nsetting. Each client locally clips and perturbs its LoRA matrices using\nGaussian noise to satisfy ($\\epsilon$, $\\delta$)-differential privacy. We\nfurther provide a theoretical analysis demonstrating the unbiased nature of the\nupdates and deriving bounds on the variance introduced by noise, offering\npractical guidance for privacy-budget calibration. Experimental results across\nmainstream benchmarks show that DP-FedLoRA delivers competitive performance\nwhile offering strong privacy guarantees, paving the way for scalable and\nprivacy-preserving LLM deployment in on-device environments.",
    "updated" : "2025-09-11T02:16:34Z",
    "published" : "2025-09-11T02:16:34Z",
    "authors" : [
      {
        "name" : "Honghui Xu"
      },
      {
        "name" : "Shiva Shrestha"
      },
      {
        "name" : "Wei Chen"
      },
      {
        "name" : "Zhiyuan Li"
      },
      {
        "name" : "Zhipeng Cai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.09091v1",
    "title" : "Towards Confidential and Efficient LLM Inference with Dual Privacy\n  Protection",
    "summary" : "CPU-based trusted execution environments (TEEs) and differential privacy (DP)\nhave gained wide applications for private inference. Due to high inference\nlatency in TEEs, researchers use partition-based approaches that offload linear\nmodel components to GPUs. However, dense nonlinear layers of large language\nmodels (LLMs) result in significant communication overhead between TEEs and\nGPUs. DP-based approaches apply random noise to protect data privacy, but this\ncompromises LLM performance and semantic understanding. To overcome the above\ndrawbacks, this paper proposes CMIF, a Confidential and efficient Model\nInference Framework. CMIF confidentially deploys the embedding layer in the\nclient-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes\nthe Report-Noisy-Max mechanism to protect sensitive inputs with a slight\ndecrease in model performance. Extensive experiments on Llama-series models\ndemonstrate that CMIF reduces additional inference overhead in TEEs while\npreserving user data privacy.",
    "updated" : "2025-09-11T01:54:13Z",
    "published" : "2025-09-11T01:54:13Z",
    "authors" : [
      {
        "name" : "Honglan Yu"
      },
      {
        "name" : "Yibin Wang"
      },
      {
        "name" : "Feifei Dai"
      },
      {
        "name" : "Dong Liu"
      },
      {
        "name" : "Haihui Fan"
      },
      {
        "name" : "Xiaoyan Gu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.08995v1",
    "title" : "When FinTech Meets Privacy: Securing Financial LLMs with Differential\n  Private Fine-Tuning",
    "summary" : "The integration of Large Language Models (LLMs) into financial technology\n(FinTech) has revolutionized the analysis and processing of complex financial\ndata, driving advancements in real-time decision-making and analytics. With the\ngrowing trend of deploying AI models on edge devices for financial\napplications, ensuring the privacy of sensitive financial data has become a\nsignificant challenge. To address this, we propose DPFinLLM, a\nprivacy-enhanced, lightweight LLM specifically designed for on-device financial\napplications. DPFinLLM combines a robust differential privacy mechanism with a\nstreamlined architecture inspired by state-of-the-art models, enabling secure\nand efficient processing of financial data. This proposed DPFinLLM can not only\nsafeguard user data from privacy breaches but also ensure high performance\nacross diverse financial tasks. Extensive experiments on multiple financial\nsentiment datasets validate the effectiveness of DPFinLLM, demonstrating its\nability to achieve performance comparable to fully fine-tuned models, even\nunder strict privacy constraints.",
    "updated" : "2025-09-10T20:43:08Z",
    "published" : "2025-09-10T20:43:08Z",
    "authors" : [
      {
        "name" : "Sichen Zhu"
      },
      {
        "name" : "Hoyeung Leung"
      },
      {
        "name" : "Xiaoyi Wang"
      },
      {
        "name" : "Jia Wei"
      },
      {
        "name" : "Honghui Xu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.03294v2",
    "title" : "A Comprehensive Guide to Differential Privacy: From Theory to User\n  Expectations",
    "summary" : "The increasing availability of personal data has enabled significant advances\nin fields such as machine learning, healthcare, and cybersecurity. However,\nthis data abundance also raises serious privacy concerns, especially in light\nof powerful re-identification attacks and growing legal and ethical demands for\nresponsible data use. Differential privacy (DP) has emerged as a principled,\nmathematically grounded framework for mitigating these risks. This review\nprovides a comprehensive survey of DP, covering its theoretical foundations,\npractical mechanisms, and real-world applications. It explores key algorithmic\ntools and domain-specific challenges - particularly in privacy-preserving\nmachine learning and synthetic data generation. The report also highlights\nusability issues and the need for improved communication and transparency in DP\nsystems. Overall, the goal is to support informed adoption of DP by researchers\nand practitioners navigating the evolving landscape of data privacy.",
    "updated" : "2025-09-11T13:12:37Z",
    "published" : "2025-09-03T13:23:10Z",
    "authors" : [
      {
        "name" : "Napsu Karmitsa"
      },
      {
        "name" : "Antti Airola"
      },
      {
        "name" : "Tapio Pahikkala"
      },
      {
        "name" : "Tinja Pitkämäki"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "68P27, 68T09, 94A60"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.10163v1",
    "title" : "Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and\n  Energy-Aware Resource Management in 6G Edge Networks",
    "summary" : "As sixth-generation (6G) networks move toward ultra-dense, intelligent edge\nenvironments, efficient resource management under stringent privacy, mobility,\nand energy constraints becomes critical. This paper introduces a novel\nFederated Multi-Agent Reinforcement Learning (Fed-MARL) framework that\nincorporates cross-layer orchestration of both the MAC layer and application\nlayer for energy-efficient, privacy-preserving, and real-time resource\nmanagement across heterogeneous edge devices. Each agent uses a Deep Recurrent\nQ-Network (DRQN) to learn decentralized policies for task offloading, spectrum\naccess, and CPU energy adaptation based on local observations (e.g., queue\nlength, energy, CPU usage, and mobility). To protect privacy, we introduce a\nsecure aggregation protocol based on elliptic curve Diffie Hellman key\nexchange, which ensures accurate model updates without exposing raw data to\nsemi-honest adversaries. We formulate the resource management problem as a\npartially observable multi-agent Markov decision process (POMMDP) with a\nmulti-objective reward function that jointly optimizes latency, energy\nefficiency, spectral efficiency, fairness, and reliability under 6G-specific\nservice requirements such as URLLC, eMBB, and mMTC. Simulation results\ndemonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines\nin task success rate, latency, energy efficiency, and fairness, while ensuring\nrobust privacy protection and scalability in dynamic, resource-constrained 6G\nedge networks.",
    "updated" : "2025-09-12T11:41:40Z",
    "published" : "2025-09-12T11:41:40Z",
    "authors" : [
      {
        "name" : "Francisco Javier Esono Nkulu Andong"
      },
      {
        "name" : "Qi Min"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.10018v1",
    "title" : "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation\n  Enhanced by Domain Rules and Disproof Method",
    "summary" : "With the rapid advancement of Large Language Model (LLM), LLM-based agents\nexhibit exceptional abilities in understanding and generating natural language,\nfacilitating human-like collaboration and information transmission in LLM-based\nMulti-Agent System (MAS). High-performance LLMs are often hosted on remote\nservers in public spaces. When tasks involve privacy data, MAS cannot securely\nutilize these LLMs without implementing privacy-preserving mechanisms. To\naddress this challenge, we propose a General Anonymizing Multi-Agent system\n(GAMA), which divides the agents' workspace into private and public spaces and\nprotects privacy through the anonymizing mechanism. In the private space,\nagents handle sensitive data, while in the public space, only anonymized data\nis utilized. GAMA incorporates two key modules to mitigate semantic loss caused\nby anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and\nDisproof-based Logic Enhancement (DLE). We evaluate GAMA on two public\nquestion-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The\nresults demonstrate that GAMA has superior performance compared to the\nstate-of-the-art models. To further assess its privacy-preserving capabilities,\nwe designed two new datasets: Knowledge Privacy Preservation and Logic Privacy\nPreservation. The final results highlight GAMA's exceptional effectiveness in\nboth task processing and privacy preservation.",
    "updated" : "2025-09-12T07:22:49Z",
    "published" : "2025-09-12T07:22:49Z",
    "authors" : [
      {
        "name" : "Hailong Yang"
      },
      {
        "name" : "Renhuo Zhao"
      },
      {
        "name" : "Guanjin Wang"
      },
      {
        "name" : "Zhaohong Deng"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.09916v1",
    "title" : "Immersive Invaders: Privacy Threats from Deceptive Design in Virtual\n  Reality Games and Applications",
    "summary" : "Virtual Reality (VR) technologies offer immersive experiences but collect\nsubstantial user data. While deceptive design is well-studied in 2D platforms,\nlittle is known about its manifestation in VR environments and its impact on\nuser privacy. This research investigates deceptive designs in privacy\ncommunication and interaction mechanisms of 12 top-rated VR games and\napplications through autoethnographic evaluation of the applications and\nthematic analysis of privacy policies. We found that while many deceptive\ndesigns rely on 2D interfaces, some VR-unique features, while not directly\nenabling deception, amplified data disclosure behaviors, and obscured actual\ndata practices. Convoluted privacy policies and manipulative consent practices\nfurther hinder comprehension and increase privacy risks. We also observed\nprivacy-preserving design strategies and protective considerations in VR\nprivacy policies. We offer recommendations for ethical VR design that balance\nimmersive experiences with strong privacy protections, guiding researchers,\ndesigners, and policymakers to improve privacy in VR environments.",
    "updated" : "2025-09-12T01:31:59Z",
    "published" : "2025-09-12T01:31:59Z",
    "authors" : [
      {
        "name" : "Hilda Hadan"
      },
      {
        "name" : "Michaela Valiquette"
      },
      {
        "name" : "Lennart E. Nacke"
      },
      {
        "name" : "Leah Zhang-Kennedy"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.09844v1",
    "title" : "Privacy-Preserving Automated Rosacea Detection Based on Medically\n  Inspired Region of Interest Selection",
    "summary" : "Rosacea is a common but underdiagnosed inflammatory skin condition that\nprimarily affects the central face and presents with subtle redness, pustules,\nand visible blood vessels. Automated detection remains challenging due to the\ndiffuse nature of symptoms, the scarcity of labeled datasets, and privacy\nconcerns associated with using identifiable facial images. A novel\nprivacy-preserving automated rosacea detection method inspired by clinical\npriors and trained entirely on synthetic data is presented in this paper.\nSpecifically, the proposed method, which leverages the observation that rosacea\nmanifests predominantly through central facial erythema, first constructs a\nfixed redness-informed mask by selecting regions with consistently high red\nchannel intensity across facial images. The mask thus is able to focus on\ndiagnostically relevant areas such as the cheeks, nose, and forehead and\nexclude identity-revealing features. Second, the ResNet-18 deep learning\nmethod, which is trained on the masked synthetic images, achieves superior\nperformance over the full-face baselines with notable gains in terms of\naccuracy, recall and F1 score when evaluated using the real-world test data.\nThe experimental results demonstrate that the synthetic data and clinical\npriors can jointly enable accurate and ethical dermatological AI systems,\nespecially for privacy sensitive applications in telemedicine and large-scale\nscreening.",
    "updated" : "2025-09-11T20:54:26Z",
    "published" : "2025-09-11T20:54:26Z",
    "authors" : [
      {
        "name" : "Chengyu Yang"
      },
      {
        "name" : "Rishik Reddy Yesgari"
      },
      {
        "name" : "Chengjun Liu"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.09787v1",
    "title" : "ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full\n  Version)",
    "summary" : "Split Learning (SL) is a distributed learning approach that enables\nresource-constrained clients to collaboratively train deep neural networks\n(DNNs) by offloading most layers to a central server while keeping in- and\noutput layers on the client-side. This setup enables SL to leverage server\ncomputation capacities without sharing data, making it highly effective in\nresource-constrained environments dealing with sensitive data. However, the\ndistributed nature enables malicious clients to manipulate the training\nprocess. By sending poisoned intermediate gradients, they can inject backdoors\ninto the shared DNN. Existing defenses are limited by often focusing on\nserver-side protection and introducing additional overhead for the server. A\nsignificant challenge for client-side defenses is enforcing malicious clients\nto correctly execute the defense algorithm.\n  We present ZORRO, a private, verifiable, and robust SL defense scheme.\nThrough our novel design and application of interactive zero-knowledge proofs\n(ZKPs), clients prove their correct execution of a client-located defense\nalgorithm, resulting in proofs of computational integrity attesting to the\nbenign nature of locally trained DNN portions. Leveraging the frequency\nrepresentation of model partitions enables ZORRO to conduct an in-depth\ninspection of the locally trained models in an untrusted environment, ensuring\nthat each client forwards a benign checkpoint to its succeeding client. In our\nextensive evaluation, covering different model architectures as well as various\nattack strategies and data scenarios, we show ZORRO's effectiveness, as it\nreduces the attack success rate to less than 6\\% while causing even for models\nstoring \\numprint{1000000} parameters on the client-side an overhead of less\nthan 10 seconds.",
    "updated" : "2025-09-11T18:44:09Z",
    "published" : "2025-09-11T18:44:09Z",
    "authors" : [
      {
        "name" : "Nojan Sheybani"
      },
      {
        "name" : "Alessandro Pegoraro"
      },
      {
        "name" : "Jonathan Knauer"
      },
      {
        "name" : "Phillip Rieger"
      },
      {
        "name" : "Elissa Mollakuqe"
      },
      {
        "name" : "Farinaz Koushanfar"
      },
      {
        "name" : "Ahmad-Reza Sadeghi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.09485v2",
    "title" : "Balancing Utility and Privacy: Dynamically Private SGD with Random\n  Projection",
    "summary" : "Stochastic optimization is a pivotal enabler in modern machine learning,\nproducing effective models for various tasks. However, several existing works\nhave shown that model parameters and gradient information are susceptible to\nprivacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy\nconcerns, its static noise mechanism impacts the error bounds for model\nperformance. Additionally, with the exponential increase in model parameters,\nefficient learning of these models using stochastic optimizers has become more\nchallenging. To address these concerns, we introduce the Dynamically\nDifferentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we\ncombine two important ideas: (i) dynamic differential privacy (DDP) with\nautomatic gradient clipping and (ii) random projection with SGD, allowing\ndynamic adjustment of the tradeoff between utility and privacy of the model. It\nexhibits provably sub-linear convergence rates across different objective\nfunctions, matching the best available rate. The theoretical analysis further\nsuggests that DDP leads to better utility at the cost of privacy, while random\nprojection enables more efficient model learning. Extensive experiments across\ndiverse datasets show that D2P2-SGD remarkably enhances accuracy while\nmaintaining privacy. Our code is available here.",
    "updated" : "2025-09-12T01:27:15Z",
    "published" : "2025-09-11T14:17:04Z",
    "authors" : [
      {
        "name" : "Zhanhong Jiang"
      },
      {
        "name" : "Md Zahid Hasan"
      },
      {
        "name" : "Nastaran Saadati"
      },
      {
        "name" : "Aditya Balu"
      },
      {
        "name" : "Chao Liu"
      },
      {
        "name" : "Soumik Sarkar"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.11939v1",
    "title" : "PrivWeb: Unobtrusive and Content-aware Privacy Protection For Web Agents",
    "summary" : "While web agents gained popularity by automating web interactions, their\nrequirement for interface access introduces significant privacy risks that are\nunderstudied, particularly from users' perspective. Through a formative study\n(N=15), we found users frequently misunderstand agents' data practices, and\ndesired unobtrusive, transparent data management. To achieve this, we designed\nand implemented PrivWeb, a trusted add-on on web agents that utilizes a\nlocalized LLM to anonymize private information on interfaces according to user\npreferences. It features privacy categorization schema and adaptive\nnotifications that selectively pauses tasks for user control over information\ncollection for highly sensitive information, while offering non-disruptive\noptions for less sensitive information, minimizing human oversight. The user\nstudy (N=14) across travel, information retrieval, shopping, and entertainment\ntasks compared PrivWeb with baselines without notification and without control\nfor private information access, where PrivWeb reduced perceived privacy risks\nwith no associated increase in cognitive effort, and resulted in higher overall\nsatisfaction.",
    "updated" : "2025-09-15T13:58:52Z",
    "published" : "2025-09-15T13:58:52Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Yutong Jiang"
      },
      {
        "name" : "Rongjun Ma"
      },
      {
        "name" : "Yuting Yang"
      },
      {
        "name" : "Mingyao Xu"
      },
      {
        "name" : "Zhixin Huang"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.11917v1",
    "title" : "Distributed Finite-Horizon Optimal Control for Consensus with\n  Differential Privacy Guarantees",
    "summary" : "This paper addresses the problem of privacy-preserving consensus control for\nmulti-agent systems (MAS) using differential privacy. We propose a novel\ndistributed finite-horizon linear quadratic regulator (LQR) framework, in which\nagents share individual state information while preserving the confidentiality\nof their local pairwise weight matrices, which are considered sensitive data in\nMAS. Protecting these matrices effectively safeguards each agent's private cost\nfunction and control preferences. Our solution injects consensus\nerror-dependent Laplace noise into the communicated state information and\nemploys a carefully designed time-dependent scaling factor in the local cost\nfunctions. {This approach guarantees bounded consensus and achieves rigorous\n$\\epsilon$-differential privacy for the weight matrices without relying on\nspecific noise distribution assumptions.} Additionally, we analytically\ncharacterize the trade-off between consensus accuracy and privacy level,\noffering clear guidelines on how to enhance consensus performance through\nappropriate scaling of the LQR weight matrices and the privacy budget.",
    "updated" : "2025-09-15T13:34:27Z",
    "published" : "2025-09-15T13:34:27Z",
    "authors" : [
      {
        "name" : "Yuwen Ma"
      },
      {
        "name" : "Yongqiang Wang"
      },
      {
        "name" : "Sarah K. Spurgeon"
      },
      {
        "name" : "Boli Chen"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.11870v1",
    "title" : "Efficient Byzantine-Robust Privacy-Preserving Federated Learning via\n  Dimension Compression",
    "summary" : "Federated Learning (FL) allows collaborative model training across\ndistributed clients without sharing raw data, thus preserving privacy. However,\nthe system remains vulnerable to privacy leakage from gradient updates and\nByzantine attacks from malicious clients. Existing solutions face a critical\ntrade-off among privacy preservation, Byzantine robustness, and computational\nefficiency. We propose a novel scheme that effectively balances these competing\nobjectives by integrating homomorphic encryption with dimension compression\nbased on the Johnson-Lindenstrauss transformation. Our approach employs a\ndual-server architecture that enables secure Byzantine defense in the\nciphertext domain while dramatically reducing computational overhead through\ngradient compression. The dimension compression technique preserves the\ngeometric relationships necessary for Byzantine defence while reducing\ncomputation complexity from $O(dn)$ to $O(kn)$ cryptographic operations, where\n$k \\ll d$. Extensive experiments across diverse datasets demonstrate that our\napproach maintains model accuracy comparable to non-private FL while\neffectively defending against Byzantine clients comprising up to $40\\%$ of the\nnetwork.",
    "updated" : "2025-09-15T12:43:58Z",
    "published" : "2025-09-15T12:43:58Z",
    "authors" : [
      {
        "name" : "Xian Qin"
      },
      {
        "name" : "Xue Yang"
      },
      {
        "name" : "Xiaohu Tang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.11761v1",
    "title" : "On Spatial-Provenance Recovery in Wireless Networks with Relaxed-Privacy\n  Constraints",
    "summary" : "In Vehicle-to-Everything (V2X) networks with multi-hop communication, Road\nSide Units (RSUs) intend to gather location data from the vehicles to offer\nvarious location-based services. Although vehicles use the Global Positioning\nSystem (GPS) for navigation, they may refrain from sharing their exact GPS\ncoordinates to the RSUs due to privacy considerations. Thus, to address the\nlocalization expectations of the RSUs and the privacy concerns of the vehicles,\nwe introduce a relaxed-privacy model wherein the vehicles share their partial\nlocation information in order to avail the location-based services. To\nimplement this notion of relaxed-privacy, we propose a low-latency protocol for\nspatial-provenance recovery, wherein vehicles use correlated linear Bloom\nfilters to embed their position information. Our proposed spatial-provenance\nrecovery process takes into account the resolution of localization, the\nunderlying ad hoc protocol, and the coverage range of the wireless technology\nused by the vehicles. Through a rigorous theoretical analysis, we present\nextensive analysis on the underlying trade-off between relaxed-privacy and the\ncommunication-overhead of the protocol. Finally, using a wireless testbed, we\nshow that our proposed method requires a few bits in the packet header to\nprovide security features such as localizing a low-power jammer executing a\ndenial-of-service attack.",
    "updated" : "2025-09-15T10:28:52Z",
    "published" : "2025-09-15T10:28:52Z",
    "authors" : [
      {
        "name" : "Manish Bansal"
      },
      {
        "name" : "Pramsu Shrivastava"
      },
      {
        "name" : "J. Harshan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.11625v1",
    "title" : "Inducing Uncertainty for Test-Time Privacy",
    "summary" : "Unlearning is the predominant method for removing the influence of data in\nmachine learning models. However, even after unlearning, models often continue\nto produce the same predictions on the unlearned data with high confidence.\nThis persistent behavior can be exploited by adversaries using confident model\npredictions on incorrect or obsolete data to harm users. We call this threat\nmodel, which unlearning fails to protect against, *test-time privacy*. In\nparticular, an adversary with full model access can bypass any naive defenses\nwhich ensure test-time privacy. To address this threat, we introduce an\nalgorithm which perturbs model weights to induce maximal uncertainty on\nprotected instances while preserving accuracy on the rest of the instances. Our\ncore algorithm is based on finetuning with a Pareto optimal objective that\nexplicitly balances test-time privacy against utility. We also provide a\ncertifiable approximation algorithm which achieves $(\\varepsilon, \\delta)$\nguarantees without convexity assumptions. We then prove a tight, non-vacuous\nbound that characterizes the privacy-utility tradeoff that our algorithms\nincur. Empirically, our method obtains $>3\\times$ stronger uncertainty than\npretraining with $<0.2\\%$ drops in accuracy on various image recognition\nbenchmarks. Altogether, this framework provides a tool to guarantee additional\nprotection to end users.",
    "updated" : "2025-09-15T06:38:57Z",
    "published" : "2025-09-15T06:38:57Z",
    "authors" : [
      {
        "name" : "Muhammad H. Ashiq"
      },
      {
        "name" : "Peter Triantafillou"
      },
      {
        "name" : "Hung Yun Tseng"
      },
      {
        "name" : "Grigoris G. Chrysos"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.11249v1",
    "title" : "Make Identity Unextractable yet Perceptible: Synthesis-Based Privacy\n  Protection for Subject Faces in Photos",
    "summary" : "Deep learning-based face recognition (FR) technology exacerbates privacy\nconcerns in photo sharing. In response, the research community developed a\nsuite of anti-FR methods to block identity extraction by unauthorized FR\nsystems. Benefiting from quasi-imperceptible alteration, perturbation-based\nmethods are well-suited for privacy protection of subject faces in photos, as\nthey allow familiar persons to recognize subjects via naked eyes. However, we\nreveal that perturbation-based methods provide a false sense of privacy through\ntheoretical analysis and experimental validation.\n  Therefore, new alternative solutions should be found to protect subject\nfaces. In this paper, we explore synthesis-based methods as a promising\nsolution, whose challenge is to enable familiar persons to recognize subjects.\nTo solve the challenge, we present a key insight: In most photo sharing\nscenarios, familiar persons recognize subjects through identity perception\nrather than meticulous face analysis. Based on the insight, we propose the\nfirst synthesis-based method dedicated to subject faces, i.e., PerceptFace,\nwhich can make identity unextractable yet perceptible. To enhance identity\nperception, a new perceptual similarity loss is designed for faces, reducing\nthe alteration in regions of high sensitivity to human vision.\n  As a synthesis-based method, PerceptFace can inherently provide reliable\nidentity protection. Meanwhile, out of the confine of meticulous face analysis,\nPerceptFace focuses on identity perception from a more practical scenario,\nwhich is also enhanced by the designed perceptual similarity loss. Sufficient\nexperiments show that PerceptFace achieves a superior trade-off between\nidentity protection and identity perception compared to existing methods. We\nprovide a public API of PerceptFace and believe that it has great potential to\nbecome a practical anti-FR tool.",
    "updated" : "2025-09-14T12:47:47Z",
    "published" : "2025-09-14T12:47:47Z",
    "authors" : [
      {
        "name" : "Tao Wang"
      },
      {
        "name" : "Yushu Zhang"
      },
      {
        "name" : "Xiangli Xiao"
      },
      {
        "name" : "Kun Xu"
      },
      {
        "name" : "Lin Yuan"
      },
      {
        "name" : "Wenying Wen"
      },
      {
        "name" : "Yuming Fang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.11022v1",
    "title" : "Privacy-Preserving Uncertainty Disclosure for Facilitating Enhanced\n  Energy Storage Dispatch",
    "summary" : "This paper proposes a novel privacy-preserving uncertainty disclosure\nframework, enabling system operators to release marginal value function bounds\nto reduce the conservativeness of interval forecast and mitigate excessive\nwithholding, thereby enhancing storage dispatch and social welfare. We propose\na risk-averse analytical storage arbitrage model based on stochastic dynamic\nprogramming and explicitly account for uncertainty intervals in value function\ntraining. We derive real-time marginal value function bounds using a\nrolling-horizon chance-constrained economic dispatch formulation. We rigorously\nprove that the bounds reliably cap the true opportunity cost and dynamically\nconverge to the hindsight value. We verify that both the marginal value\nfunction and its bounds monotonically decrease with the state of charge and\nincrease with uncertainty, providing a theoretical basis for risk-averse\nstrategic behaviors and SoC-dependent designs. We validate the effectiveness of\nthe proposed framework via an agent-based simulation on the ISO-NE test system.\nUnder 50% renewable capacity and 35% storage capacity, the proposed bounds\nenhance storage response by 38.91% and reduce the optimality gap to 3.91%\nthrough improved interval predictions. Additionally, by mitigating excessive\nwithholding, the bounds yield an average system cost reduction of 0.23% and an\naverage storage profit increase of 13.22%. These benefits further scale with\nhigher prediction conservativeness, storage capacity, and system uncertainty.",
    "updated" : "2025-09-14T01:22:29Z",
    "published" : "2025-09-14T01:22:29Z",
    "authors" : [
      {
        "name" : "Ning Qi"
      },
      {
        "name" : "Xiaolong Jin"
      },
      {
        "name" : "Kai Hou"
      },
      {
        "name" : "Zeyu Liu"
      },
      {
        "name" : "Hongjie Jia"
      },
      {
        "name" : "Wei Wei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.10691v1",
    "title" : "Privacy-Preserving Decentralized Federated Learning via Explainable\n  Adaptive Differential Privacy",
    "summary" : "Decentralized federated learning faces privacy risks because model updates\ncan leak data through inference attacks and membership inference, a concern\nthat grows over many client exchanges. Differential privacy offers principled\nprotection by injecting calibrated noise so confidential information remains\nsecure on resource-limited IoT devices. Yet without transparency, black-box\ntraining cannot track noise already injected by previous clients and rounds,\nwhich forces worst-case additions and harms accuracy. We propose PrivateDFL, an\nexplainable framework that joins hyperdimensional computing with differential\nprivacy and keeps an auditable account of cumulative noise so each client adds\nonly the difference between the required noise and what has already been\naccumulated. We evaluate on MNIST, ISOLET, and UCI-HAR to span image, signal,\nand tabular modalities, and we benchmark against transformer-based and deep\nlearning-based baselines trained centrally with Differentially Private\nStochastic Gradient Descent (DP-SGD) and Renyi Differential Privacy (RDP).\nPrivateDFL delivers higher accuracy, lower latency, and lower energy across IID\nand non-IID partitions while preserving formal (epsilon, delta) guarantees and\noperating without a central server. For example, under non-IID partitions,\nPrivateDFL achieves 24.42% higher accuracy than the Vision Transformer on MNIST\nwhile using about 10x less training time, 76x lower inference latency, and 11x\nless energy, and on ISOLET it exceeds Transformer accuracy by more than 80%\nwith roughly 10x less training time, 40x lower inference latency, and 36x less\ntraining energy. Future work will extend the explainable accounting to\nadversarial clients and adaptive topologies with heterogeneous privacy budgets.",
    "updated" : "2025-09-12T20:52:41Z",
    "published" : "2025-09-12T20:52:41Z",
    "authors" : [
      {
        "name" : "Fardin Jalil Piran"
      },
      {
        "name" : "Zhiling Chen"
      },
      {
        "name" : "Yang Zhang"
      },
      {
        "name" : "Qianyu Zhou"
      },
      {
        "name" : "Jiong Tang"
      },
      {
        "name" : "Farhad Imani"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.10561v1",
    "title" : "AVEC: Bootstrapping Privacy for Local LLMs",
    "summary" : "This position paper presents AVEC (Adaptive Verifiable Edge Control), a\nframework for bootstrapping privacy for local language models by enforcing\nprivacy at the edge with explicit verifiability for delegated queries. AVEC\nintroduces an adaptive budgeting algorithm that allocates per-query\ndifferential privacy parameters based on sensitivity, local confidence, and\nhistorical usage, and uses verifiable transformation with on-device integrity\nchecks. We formalize guarantees using R\\'enyi differential privacy with\nodometer-based accounting, and establish utility ceilings, delegation-leakage\nbounds, and impossibility results for deterministic gating and hash-only\ncertification. Our evaluation is simulation-based by design to study mechanism\nbehavior and accounting; we do not claim deployment readiness or task-level\nutility with live LLMs. The contribution is a conceptual architecture and\ntheoretical foundation that chart a pathway for empirical follow-up on\nprivately bootstrapping local LLMs.",
    "updated" : "2025-09-10T07:59:01Z",
    "published" : "2025-09-10T07:59:01Z",
    "authors" : [
      {
        "name" : "Madhava Gaikwad"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "68Q25, 68T50, 68P27",
      "F.2.2; I.2.7; K.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05362v2",
    "title" : "AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and\n  Conversational Scambaiting by Leveraging LLMs and Federated Learning",
    "summary" : "Scams exploiting real-time social engineering -- such as phishing,\nimpersonation, and phone fraud -- remain a persistent and evolving threat\nacross digital platforms. Existing defenses are largely reactive, offering\nlimited protection during active interactions. We propose a privacy-preserving,\nAI-in-the-loop framework that proactively detects and disrupts scam\nconversations in real time. The system combines instruction-tuned artificial\nintelligence with a safety-aware utility function that balances engagement with\nharm minimization, and employs federated learning to enable continual model\nupdates without raw data sharing. Experimental evaluations show that the system\nproduces fluent and engaging responses (perplexity as low as 22.3, engagement\n$\\approx$0.80), while human studies confirm significant gains in realism,\nsafety, and effectiveness over strong baselines. In federated settings, models\ntrained with FedAvg sustain up to 30 rounds while preserving high engagement\n($\\approx$0.80), strong relevance ($\\approx$0.74), and low PII leakage\n($\\leq$0.0085). Even with differential privacy, novelty and safety remain\nstable, indicating that robust privacy can be achieved without sacrificing\nperformance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,\nMD-Judge) shows a straightforward pattern: stricter moderation settings reduce\nthe chance of exposing personal information, but they also limit how much the\nmodel engages in conversation. In contrast, more relaxed settings allow longer\nand richer interactions, which improve scam detection, but at the cost of\nhigher privacy risk. To our knowledge, this is the first framework to unify\nreal-time scam-baiting, federated privacy preservation, and calibrated safety\nmoderation into a proactive defense paradigm.",
    "updated" : "2025-09-12T18:06:28Z",
    "published" : "2025-09-04T00:19:48Z",
    "authors" : [
      {
        "name" : "Ismail Hossain"
      },
      {
        "name" : "Sai Puppala"
      },
      {
        "name" : "Sajedul Talukder"
      },
      {
        "name" : "Md Jahangir Alam"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.10516v1",
    "title" : "Privacy-Preserving Personalization in Education: A Federated Recommender\n  System for Student Performance Prediction",
    "summary" : "The increasing digitalization of education presents unprecedented\nopportunities for data-driven personalization, yet it introduces significant\nstudent data privacy challenges. Conventional recommender systems rely on\ncentralized data, a paradigm often incompatible with modern data protection\nregulations. A novel privacy-preserving recommender system is proposed and\nevaluated to address this critical issue using Federated Learning (FL). The\napproach utilizes a Deep Neural Network (DNN) with rich, engineered features\nfrom the large-scale ASSISTments educational dataset. A rigorous comparative\nanalysis of federated aggregation strategies was conducted, identifying FedProx\nas a significantly more stable and effective method for handling heterogeneous\nstudent data than the standard FedAvg baseline. The optimized federated model\nachieves a high-performance F1-Score of 76.28\\%, corresponding to 82.85\\% of\nthe performance of a powerful, centralized XGBoost model. These findings\nvalidate that a federated approach can provide highly effective content\nrecommendations without centralizing sensitive student data. Consequently, our\nwork presents a viable and robust solution to the personalization-privacy\ndilemma in modern educational platforms.",
    "updated" : "2025-09-03T11:28:57Z",
    "published" : "2025-09-03T11:28:57Z",
    "authors" : [
      {
        "name" : "Rodrigo Tertulino"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.13051v1",
    "title" : "More than Meets the Eye: Understanding the Effect of Individual Objects\n  on Perceived Visual Privacy",
    "summary" : "User-generated content, such as photos, comprises the majority of online\nmedia content and drives engagement due to the human ability to process visual\ninformation quickly. Consequently, many online platforms are designed for\nsharing visual content, with billions of photos posted daily. However, photos\noften reveal more than they intended through visible and contextual cues,\nleading to privacy risks. Previous studies typically treat privacy as a\nproperty of the entire image, overlooking individual objects that may carry\nvarying privacy risks and influence how users perceive it. We address this gap\nwith a mixed-methods study (n = 92) to understand how users evaluate the\nprivacy of images containing multiple sensitive objects. Our results reveal\nmental models and nuanced patterns that uncover how granular details, such as\nphoto-capturing context and co-presence of other objects, affect privacy\nperceptions. These novel insights could enable personalized, context-aware\nprivacy protection designs on social media and future technologies.",
    "updated" : "2025-09-16T13:10:00Z",
    "published" : "2025-09-16T13:10:00Z",
    "authors" : [
      {
        "name" : "Mete Harun Akcay"
      },
      {
        "name" : "Siddharth Prakash Rao"
      },
      {
        "name" : "Alexandros Bakas"
      },
      {
        "name" : "Buse Gul Atli"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.12958v1",
    "title" : "Forget What's Sensitive, Remember What Matters: Token-Level Differential\n  Privacy in Memory Sculpting for Continual Learning",
    "summary" : "Continual Learning (CL) models, while adept at sequential knowledge\nacquisition, face significant and often overlooked privacy challenges due to\naccumulating diverse information. Traditional privacy methods, like a uniform\nDifferential Privacy (DP) budget, indiscriminately protect all data, leading to\nsubstantial model utility degradation and hindering CL deployment in\nprivacy-sensitive areas. To overcome this, we propose a privacy-enhanced\ncontinual learning (PeCL) framework that forgets what's sensitive and remembers\nwhat matters. Our approach first introduces a token-level dynamic Differential\nPrivacy strategy that adaptively allocates privacy budgets based on the\nsemantic sensitivity of individual tokens. This ensures robust protection for\nprivate entities while minimizing noise injection for non-sensitive, general\nknowledge. Second, we integrate a privacy-guided memory sculpting module. This\nmodule leverages the sensitivity analysis from our dynamic DP mechanism to\nintelligently forget sensitive information from the model's memory and\nparameters, while explicitly preserving the task-invariant historical knowledge\ncrucial for mitigating catastrophic forgetting. Extensive experiments show that\nPeCL achieves a superior balance between privacy preserving and model utility,\noutperforming baseline models by maintaining high accuracy on previous tasks\nwhile ensuring robust privacy.",
    "updated" : "2025-09-16T11:01:59Z",
    "published" : "2025-09-16T11:01:59Z",
    "authors" : [
      {
        "name" : "Bihao Zhan"
      },
      {
        "name" : "Jie Zhou"
      },
      {
        "name" : "Junsong Li"
      },
      {
        "name" : "Yutao Yang"
      },
      {
        "name" : "Shilian Chen"
      },
      {
        "name" : "Qianjun Pan"
      },
      {
        "name" : "Xin Li"
      },
      {
        "name" : "Wen Wu"
      },
      {
        "name" : "Xingjiao Wu"
      },
      {
        "name" : "Qin Chen"
      },
      {
        "name" : "Hang Yan"
      },
      {
        "name" : "Liang He"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.12899v1",
    "title" : "EByFTVeS: Efficient Byzantine Fault Tolerant-based Verifiable\n  Secret-sharing in Distributed Privacy-preserving Machine Learning",
    "summary" : "Verifiable Secret Sharing (VSS) has been widespread in Distributed\nPrivacy-preserving Machine Learning (DPML), because invalid shares from\nmalicious dealers or participants can be recognized by verifying the commitment\nof the received shares for honest participants. However, the consistency and\nthe computation and communitation burden of the VSS-based DPML schemes are\nstill two serious challenges. Although Byzantine Fault Tolerance (BFT) system\nhas been brought to guarantee the consistency and improve the efficiency of the\nexisting VSS-based DPML schemes recently, we explore an Adaptive Share Delay\nProvision (ASDP) strategy, and launch an ASDP-based Customized Model Poisoning\nAttack (ACuMPA) for certain participants in this paper. We theoretically\nanalyzed why the ASDP strategy and the ACuMPA algorithm works to the existing\nschemes. Next, we propose an [E]fficient [By]zantine [F]ault [T]olerant-based\n[Ve]rifiable [S]ecret-sharing (EByFTVeS) scheme. Finally, the validity,\nliveness, consistency and privacy of the EByFTVeS scheme are theoretically\nanalyzed, while the efficiency of the EByFTVeS scheme outperforms that of\nthe-state-of-art VSS scheme according to comparative experiment results.",
    "updated" : "2025-09-16T09:54:27Z",
    "published" : "2025-09-16T09:54:27Z",
    "authors" : [
      {
        "name" : "Zhen Li"
      },
      {
        "name" : "Zijian Zhang"
      },
      {
        "name" : "Wenjin Yang"
      },
      {
        "name" : "Pengbo Wang"
      },
      {
        "name" : "Zhaoqi Wang"
      },
      {
        "name" : "Meng Li"
      },
      {
        "name" : "Yan Wu"
      },
      {
        "name" : "Xuyang Liu"
      },
      {
        "name" : "Jing Sun"
      },
      {
        "name" : "Liehuang Zhu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.12590v1",
    "title" : "DPCheatSheet: Using Worked and Erroneous LLM-usage Examples to Scaffold\n  Differential Privacy Implementation",
    "summary" : "This paper explores how programmers without specialized expertise in\ndifferential privacy (DP) (i.e., novices) can leverage LLMs to implement DP\nprograms with minimal training. We first conducted a need-finding study with 6\nnovices and 3 experts to understand how they utilize LLMs in DP implementation.\nWhile DP experts can implement correct DP analyses through a few prompts,\nnovices struggle to articulate their requirements in prompts and lack the\nskills to verify the correctness of the generated code. We then developed\nDPCheatSheet, an instructional tool that helps novices implement DP using LLMs.\nDPCheatSheet combines two learning concepts: it annotates an expert's workflow\nwith LLMs as a worked example to bridge the expert mindset to novices, and it\npresents five common mistakes in LLM-based DP code generation as erroneous\nexamples to support error-driven learning. We demonstrated the effectiveness of\nDPCheatSheet with an error identification study and an open-ended DP\nimplementation study.",
    "updated" : "2025-09-16T02:33:32Z",
    "published" : "2025-09-16T02:33:32Z",
    "authors" : [
      {
        "name" : "Shao-Yu Chu"
      },
      {
        "name" : "Yuhe Tian"
      },
      {
        "name" : "Yu-Xiang Wang"
      },
      {
        "name" : "Haojian Jin"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.12578v1",
    "title" : "Conflect: Designing Reflective Thinking-Based Contextual Privacy Policy\n  for Mobile Applications",
    "summary" : "Privacy policies are lengthy and complex, leading to user neglect. While\ncontextual privacy policies (CPPs) present information at the point of risk,\nthey may lack engagement and disrupt tasks. We propose Conflect, an interactive\nCPP for mobile apps, guided by a reflective thinking framework. Through three\nworkshops with experienced designers and researchers, we constructed the design\nspace of reflective thinking-based CPP design, and identified the disconnect\nbetween context and action as the most critical problem. Based on participants'\nfeedback, we designed Conflect to use sidebar alerts, allowing users to reflect\non contextualized risks and fostering their control. Our system contextually\ndetects privacy risks, extracts policy segments, and automatically generates\nrisk descriptions with 94.0% policy extraction accuracy on CPP4APP dataset and\na 4.35s latency. A user study (N=28) demonstrated that Conflect improves user\nunderstanding, trust, and satisfaction while lowering cognitive load compared\nto CPPs, privacy policies and privacy labels.",
    "updated" : "2025-09-16T02:11:09Z",
    "published" : "2025-09-16T02:11:09Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Sixing Tao"
      },
      {
        "name" : "Eve He"
      },
      {
        "name" : "Yuting Yang"
      },
      {
        "name" : "Ying Ma"
      },
      {
        "name" : "Ailei Wang"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.12465v1",
    "title" : "Efficient Privacy-Preserving Training of Quantum Neural Networks by\n  Using Mixed States to Represent Input Data Ensembles",
    "summary" : "Quantum neural networks (QNNs) are gaining increasing interest due to their\npotential to detect complex patterns in data by leveraging uniquely quantum\nphenomena. This makes them particularly promising for biomedical applications.\nIn these applications and in other contexts, increasing statistical power often\nrequires aggregating data from multiple participants. However, sharing data,\nespecially sensitive information like personal genomic sequences, raises\nsignificant privacy concerns. Quantum federated learning offers a way to\ncollaboratively train QNN models without exposing private data. However, it\nfaces major limitations, including high communication overhead and the need to\nretrain models when the task is modified. To overcome these challenges, we\npropose a privacy-preserving QNN training scheme that utilizes mixed quantum\nstates to encode ensembles of data. This approach allows for the secure sharing\nof statistical information while safeguarding individual data points. QNNs can\nbe trained directly on these mixed states, eliminating the need to access raw\ndata. Building on this foundation, we introduce protocols supporting\nmulti-party collaborative QNN training applicable across diverse domains. Our\napproach enables secure QNN training with only a single round of communication\nper participant, provides high training speed and offers task generality, i.e.,\nnew analyses can be conducted without reacquiring information from\nparticipants. We present the theoretical foundation of our scheme's utility and\nprivacy protections, which prevent the recovery of individual data points and\nresist membership inference attacks as measured by differential privacy. We\nthen validate its effectiveness on three different datasets with a focus on\ngenomic studies with an indication of how it can used in other domains without\nadaptation.",
    "updated" : "2025-09-15T21:20:51Z",
    "published" : "2025-09-15T21:20:51Z",
    "authors" : [
      {
        "name" : "Gaoyuan Wang"
      },
      {
        "name" : "Jonathan Warrell"
      },
      {
        "name" : "Mark Gerstein"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.12403v1",
    "title" : "Privacy-Driven Network Data for Smart Cities",
    "summary" : "A smart city is essential for sustainable urban development. In addition to\ncitizen engagement, a smart city enables connected infrastructure, data-driven\ndecision making and smart mobility. For most of these features, network data\nplays a critical role, particularly from public Wi-Fi infrastructures, where\ncities can benefit from optimized services such as public transport management\nand the safety and efficiency of large events. One of the biggest concerns in\ndeveloping a smart city is using secure and private data. This is particularly\nrelevant in the case of Wi-Fi network data, where sensitive information can be\ncollected. This paper specifically addresses the problem of sharing secure data\nto enhance the quality of the Wi-Fi network in a city. Despite the high\nimportance of this type of data, related work focuses on improving the safety\nof mobility patterns, targeting only the protection of MAC addresses. On the\nopposite side, we provide a practical methodology for safeguarding all\nattributes in real Wi-Fi network data. This study was developed in\ncollaboration with a multidisciplinary team of legal experts, data custodians\nand technical privacy specialists, resulting in high-quality data. On top of\nthat, we show how to integrate the legal considerations for secure data\nsharing. Our approach promotes data-driven innovation and privacy awareness in\nthe context of smart city initiatives, which have been tested in a real\nscenario.",
    "updated" : "2025-09-15T19:48:35Z",
    "published" : "2025-09-15T19:48:35Z",
    "authors" : [
      {
        "name" : "Tânia Carvalho"
      },
      {
        "name" : "José Barata"
      },
      {
        "name" : "Henish Balu"
      },
      {
        "name" : "Filipa Moreira"
      },
      {
        "name" : "João Bastos"
      },
      {
        "name" : "Luís Antunes"
      }
    ],
    "categories" : [
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.12338v1",
    "title" : "Privacy in continuous-variable distributed quantum sensing",
    "summary" : "Can a distributed network of quantum sensors estimate a global parameter\nwhile protecting every locally encoded value? We answer this question\naffirmatively by introducing and analysing a protocol for distributed quantum\nsensing in the continuous-variable regime. We consider a multipartite network\nin which each node encodes a local phase into a shared entangled Gaussian\nstate. We show that the average phase can be estimated with high precision,\nexhibiting Heisenberg scaling in the total photon number, while individual\nphases are inaccessible. Although complete privacy - where all other\ncombinations of phases remain entirely hidden - is unattainable for finite\nsqueezing in multi-party settings, it emerges in the large-squeezing limit. We\nfurther investigate the impact of displacements and optical losses, revealing\ntrade-offs between estimation accuracy and privacy. Finally, we benchmark the\nprotocol against other continuous-variable resource states.",
    "updated" : "2025-09-15T18:06:48Z",
    "published" : "2025-09-15T18:06:48Z",
    "authors" : [
      {
        "name" : "A. de Oliveira Junior"
      },
      {
        "name" : "Anton L. Andersen"
      },
      {
        "name" : "Benjamin Lundgren Larsen"
      },
      {
        "name" : "Sean William Moore"
      },
      {
        "name" : "Damian Markham"
      },
      {
        "name" : "Masahiro Takeoka"
      },
      {
        "name" : "Jonatan Bohr Brask"
      },
      {
        "name" : "Ulrik L. Andersen"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.12222v1",
    "title" : "Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO\n  Satellite Systems",
    "summary" : "Large-scale low-Earth-orbit (LEO) satellite systems are increasingly valued\nfor their ability to enable rapid and wide-area data exchange, thereby\nfacilitating the collaborative training of artificial intelligence (AI) models\nacross geographically distributed regions. Due to privacy concerns and\nregulatory constraints, raw data collected at remote clients cannot be\ncentrally aggregated, posing a major obstacle to traditional AI training\nmethods. Federated learning offers a privacy-preserving alternative by training\nlocal models on distributed devices and exchanging only model parameters.\nHowever, the dynamic topology and limited bandwidth of satellite systems will\nhinder timely parameter aggregation and distribution, resulting in prolonged\ntraining times. To address this challenge, we investigate the problem of\nscheduling federated learning over satellite networks and identify key\nbottlenecks that impact the overall duration of each training round. We propose\na discrete temporal graph-based on-demand scheduling framework that dynamically\nallocates communication resources to accelerate federated learning. Simulation\nresults demonstrate that the proposed approach achieves significant performance\ngains over traditional statistical multiplexing-based model exchange\nstrategies, reducing overall round times by 14.20% to 41.48%. Moreover, the\nacceleration effect becomes more pronounced for larger models and higher\nnumbers of clients, highlighting the scalability of the proposed approach.",
    "updated" : "2025-09-05T03:33:42Z",
    "published" : "2025-09-05T03:33:42Z",
    "authors" : [
      {
        "name" : "Binquan Guo"
      },
      {
        "name" : "Junteng Cao"
      },
      {
        "name" : "Marie Siew"
      },
      {
        "name" : "Binbin Chen"
      },
      {
        "name" : "Tony Q. S. Quek"
      },
      {
        "name" : "Zhu Han"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.14050v1",
    "title" : "AI For Privacy in Smart Homes: Exploring How Leveraging AI-Powered Smart\n  Devices Enhances Privacy Protection",
    "summary" : "Privacy concerns and fears of unauthorized access in smart home devices often\nstem from misunderstandings about how data is collected, used, and protected.\nThis study explores how AI-powered tools can offer innovative privacy\nprotections through clear, personalized, and contextual support to users.\nThrough 23 in-depth interviews with users, AI developers, designers, and\nregulators, and using Grounded Theory analysis, we identified two key themes:\nAspirations for AI-Enhanced Privacy - how users perceive AI's potential to\nempower them, address power imbalances, and improve ease of use- and AI\nEthical, Security, and Regulatory Considerations-challenges in strengthening\ndata security, ensuring regulatory compliance, and promoting ethical AI\npractices. Our findings contribute to the field by uncovering user aspirations\nfor AI-driven privacy solutions, identifying key security and ethical\nchallenges, and providing actionable recommendations for all stakeholders,\nparticularly targeting smart device designers and AI developers, to guide the\nco-design of AI tools that enhance privacy protection in smart home devices. By\nbridging the gap between user expectations, AI capabilities, and regulatory\nframeworks, this work offers practical insights for shaping the future of\nprivacy-conscious AI integration in smart homes.",
    "updated" : "2025-09-17T14:53:58Z",
    "published" : "2025-09-17T14:53:58Z",
    "authors" : [
      {
        "name" : "Wael Albayaydh"
      },
      {
        "name" : "Ivan Flechais"
      },
      {
        "name" : "Rui Zhao"
      },
      {
        "name" : "Jood Albayaydh"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.13987v1",
    "title" : "Differential Privacy in Federated Learning: Mitigating Inference Attacks\n  with Randomized Response",
    "summary" : "Machine learning models used for distributed architectures consisting of\nservers and clients require large amounts of data to achieve high accuracy.\nData obtained from clients are collected on a central server for model\ntraining. However, storing data on a central server raises concerns about\nsecurity and privacy. To address this issue, a federated learning architecture\nhas been proposed. In federated learning, each client trains a local model\nusing its own data. The trained models are periodically transmitted to the\ncentral server. The server then combines the received models using federated\naggregation algorithms to obtain a global model. This global model is\ndistributed back to the clients, and the process continues in a cyclical\nmanner. Although preventing data from leaving the clients enhances security,\ncertain concerns still remain. Attackers can perform inference attacks on the\nobtained models to approximate the training dataset, potentially causing data\nleakage. In this study, differential privacy was applied to address the\naforementioned security vulnerability, and a performance analysis was\nconducted. The Data-Unaware Classification Based on Association (duCBA)\nalgorithm was used as the federated aggregation method. Differential privacy\nwas implemented on the data using the Randomized Response technique, and the\ntrade-off between security and performance was examined under different epsilon\nvalues. As the epsilon value decreased, the model accuracy declined, and class\nprediction imbalances were observed. This indicates that higher levels of\nprivacy do not always lead to practical outcomes and that the balance between\nsecurity and performance must be carefully considered.",
    "updated" : "2025-09-17T13:59:38Z",
    "published" : "2025-09-17T13:59:38Z",
    "authors" : [
      {
        "name" : "Ozer Ozturk"
      },
      {
        "name" : "Busra Buyuktanir"
      },
      {
        "name" : "Gozde Karatas Baydogmus"
      },
      {
        "name" : "Kazim Yildiz"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.13739v1",
    "title" : "ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated\n  Learning",
    "summary" : "Federated learning (FL) faces a critical dilemma: existing protection\nmechanisms like differential privacy (DP) and homomorphic encryption (HE)\nenforce a rigid trade-off, forcing a choice between model utility and\ncomputational efficiency. This lack of flexibility hinders the practical\nimplementation. To address this, we introduce ParaAegis, a parallel protection\nframework designed to give practitioners flexible control over the\nprivacy-utility-efficiency balance. Our core innovation is a strategic model\npartitioning scheme. By applying lightweight DP to the less critical, low norm\nportion of the model while protecting the remainder with HE, we create a\ntunable system. A distributed voting mechanism ensures consensus on this\npartitioning. Theoretical analysis confirms the adjustments between efficiency\nand utility with the same privacy. Crucially, the experimental results\ndemonstrate that by adjusting the hyperparameters, our method enables flexible\nprioritization between model accuracy and training time.",
    "updated" : "2025-09-17T06:45:13Z",
    "published" : "2025-09-17T06:45:13Z",
    "authors" : [
      {
        "name" : "Zihou Wu"
      },
      {
        "name" : "Yuecheng Li"
      },
      {
        "name" : "Tianchi Liao"
      },
      {
        "name" : "Jian Lou"
      },
      {
        "name" : "Chuan Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.13627v1",
    "title" : "Secure, Scalable and Privacy Aware Data Strategy in Cloud",
    "summary" : "The enterprises today are faced with the tough challenge of processing,\nstoring large amounts of data in a secure, scalable manner and enabling\ndecision makers to make quick, informed data driven decisions. This paper\naddresses this challenge and develops an effective enterprise data strategy in\nthe cloud. Various components of an effective data strategy are discussed and\narchitectures addressing security, scalability and privacy aspects are\nprovided.",
    "updated" : "2025-09-17T01:56:07Z",
    "published" : "2025-09-17T01:56:07Z",
    "authors" : [
      {
        "name" : "Vijay Kumar Butte"
      },
      {
        "name" : "Sujata Butte"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.13625v1",
    "title" : "Privacy-Aware In-Context Learning for Large Language Models",
    "summary" : "Large language models (LLMs) have significantly transformed natural language\nunderstanding and generation, but they raise privacy concerns due to potential\nexposure of sensitive information. Studies have highlighted the risk of\ninformation leakage, where adversaries can extract sensitive information\nembedded in the prompts. In this work, we introduce a novel private prediction\nframework for generating high-quality synthetic text with strong privacy\nguarantees. Our approach leverages the Differential Privacy (DP) framework to\nensure worst-case theoretical bounds on information leakage without requiring\nany fine-tuning of the underlying models.The proposed method performs inference\non private records and aggregates the resulting per-token output distributions.\nThis enables the generation of longer and coherent synthetic text while\nmaintaining privacy guarantees. Additionally, we propose a simple blending\noperation that combines private and public inference to further enhance\nutility. Empirical evaluations demonstrate that our approach outperforms\nprevious state-of-the-art methods on in-context-learning (ICL) tasks, making it\na promising direction for privacy-preserving text generation while maintaining\nhigh utility.",
    "updated" : "2025-09-17T01:50:32Z",
    "published" : "2025-09-17T01:50:32Z",
    "authors" : [
      {
        "name" : "Bishnu Bhusal"
      },
      {
        "name" : "Manoj Acharya"
      },
      {
        "name" : "Ramneet Kaur"
      },
      {
        "name" : "Colin Samplawski"
      },
      {
        "name" : "Anirban Roy"
      },
      {
        "name" : "Adam D. Cobb"
      },
      {
        "name" : "Rohit Chadha"
      },
      {
        "name" : "Susmit Jha"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.13509v1",
    "title" : "Practitioners' Perspectives on a Differential Privacy Deployment\n  Registry",
    "summary" : "Differential privacy (DP) -- a principled approach to producing statistical\ndata products with strong, mathematically provable privacy guarantees for the\nindividuals in the underlying dataset -- has seen substantial adoption in\npractice over the past decade. Applying DP requires making several\nimplementation decisions, each with significant impacts on data privacy and/or\nutility. Hence, to promote shared learning and accountability around DP\ndeployments, Dwork, Kohli, and Mulligan (2019) proposed a public-facing\nrepository (\"registry\") of DP deployments. The DP community has recently\nstarted to work toward realizing this vision. We contribute to this effort by\n(1) developing a holistic, hierarchical schema to describe any given DP\ndeployment and (2) designing and implementing an interactive interface to act\nas a registry where practitioners can access information about past DP\ndeployments. We (3) populate our interface with 21 real-world DP deployments\nand (4) conduct an exploratory user study with DP practitioners ($n=16$) to\nunderstand how they would use the registry, as well as what challenges and\nopportunities they foresee around its adoption. We find that participants were\nenthusiastic about the registry as a valuable resource for evaluating prior\ndeployments and making future deployments. They also identified several\nopportunities for the registry, including that it can become a \"hub\" for the\ncommunity and support broader communication around DP (e.g., to legal teams).\nAt the same time, they identified challenges around the registry gaining\nadoption, including the effort and risk involved with making implementation\nchoices public and moderating the quality of entries. Based on our findings, we\noffer recommendations for encouraging adoption and increasing the registry's\nvalue not only to DP practitioners, but also to policymakers, data users, and\ndata subjects.",
    "updated" : "2025-09-16T20:15:15Z",
    "published" : "2025-09-16T20:15:15Z",
    "authors" : [
      {
        "name" : "Priyanka Nanayakkara"
      },
      {
        "name" : "Elena Ghazi"
      },
      {
        "name" : "Salil Vadhan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.11022v2",
    "title" : "Privacy-Preserving Uncertainty Disclosure for Facilitating Enhanced\n  Energy Storage Dispatch",
    "summary" : "This paper proposes a novel privacy-preserving uncertainty disclosure\nframework, enabling system operators to release marginal value function bounds\nto reduce the conservativeness of interval forecast and mitigate excessive\nwithholding, thereby enhancing storage dispatch and social welfare. We develop\na risk-averse storage arbitrage model based on stochastic dynamic programming,\nexplicitly accounting for uncertainty intervals in value function training.\nReal-time marginal value function bounds are derived using a rolling-horizon\nchance-constrained economic dispatch formulation. We rigorously prove that the\nbounds reliably cap the true opportunity cost and dynamically converge to the\nhindsight value. We verify that both the marginal value function and its bounds\nmonotonically decrease with the state of charge (SoC) and increase with\nuncertainty, providing a theoretical basis for risk-averse strategic behaviors\nand SoC-dependent designs. An adjusted storage dispatch algorithm is further\ndesigned using these bounds. We validate the effectiveness of the proposed\nframework via an agent-based simulation on the ISO-NE test system. Under 50%\nrenewable capacity and 35% storage capacity, the proposed bounds enhance\nstorage response by 38.91% and reduce the optimality gap to 3.91% through\nimproved interval predictions. Additionally, by mitigating excessive\nwithholding, the bounds yield an average system cost reduction of 0.23% and an\naverage storage profit increase of 13.22%. These benefits further scale with\nhigher prediction conservativeness, storage capacity, and system uncertainty.",
    "updated" : "2025-09-16T18:01:03Z",
    "published" : "2025-09-14T01:22:29Z",
    "authors" : [
      {
        "name" : "Ning Qi"
      },
      {
        "name" : "Xiaolong Jin"
      },
      {
        "name" : "Kai Hou"
      },
      {
        "name" : "Zeyu Liu"
      },
      {
        "name" : "Hongjie Jia"
      },
      {
        "name" : "Wei Wei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.15047v1",
    "title" : "Distributed Batch Matrix Multiplication: Trade-Offs in Download Rate,\n  Randomness, and Privacy",
    "summary" : "We study the trade-off between communication rate and privacy for distributed\nbatch matrix multiplication of two independent sequences of matrices\n$\\mathbf{A}$ and $\\mathbf{B}$ with uniformly distributed entries. In our\nsetting, $\\mathbf{B}$ is publicly accessible by all the servers while\n$\\mathbf{A}$ must remain private. A user is interested in evaluating the\nproduct $\\mathbf{AB}$ with the responses from the $k$ fastest servers. For a\ngiven parameter $\\alpha \\in [0, 1]$, our privacy constraint must ensure that\nany set of $\\ell$ colluding servers cannot learn more than a fraction $\\alpha$\nof $\\mathbf{A}$. Additionally, we study the trade-off between the amount of\nlocal randomness needed at the encoder and privacy. Finally, we establish the\noptimal trade-offs when the matrices are square and identify a linear\nrelationship between information leakage and communication rate.",
    "updated" : "2025-09-18T15:10:43Z",
    "published" : "2025-09-18T15:10:43Z",
    "authors" : [
      {
        "name" : "Amirhosein Morteza"
      },
      {
        "name" : "Remi A. Chou"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.14603v1",
    "title" : "Towards Privacy-Preserving and Heterogeneity-aware Split Federated\n  Learning via Probabilistic Masking",
    "summary" : "Split Federated Learning (SFL) has emerged as an efficient alternative to\ntraditional Federated Learning (FL) by reducing client-side computation through\nmodel partitioning. However, exchanging of intermediate activations and model\nupdates introduces significant privacy risks, especially from data\nreconstruction attacks that recover original inputs from intermediate\nrepresentations. Existing defenses using noise injection often degrade model\nperformance. To overcome these challenges, we present PM-SFL, a scalable and\nprivacy-preserving SFL framework that incorporates Probabilistic Mask training\nto add structured randomness without relying on explicit noise. This mitigates\ndata reconstruction risks while maintaining model utility. To address data\nheterogeneity, PM-SFL employs personalized mask learning that tailors submodel\nstructures to each client's local data. For system heterogeneity, we introduce\na layer-wise knowledge compensation mechanism, enabling clients with varying\nresources to participate effectively under adaptive model splitting.\nTheoretical analysis confirms its privacy protection, and experiments on image\nand wireless sensing tasks demonstrate that PM-SFL consistently improves\naccuracy, communication efficiency, and robustness to privacy attacks, with\nparticularly strong performance under data and system heterogeneity.",
    "updated" : "2025-09-18T04:28:08Z",
    "published" : "2025-09-18T04:28:08Z",
    "authors" : [
      {
        "name" : "Xingchen Wang"
      },
      {
        "name" : "Feijie Wu"
      },
      {
        "name" : "Chenglin Miao"
      },
      {
        "name" : "Tianchun Li"
      },
      {
        "name" : "Haoyu Hu"
      },
      {
        "name" : "Qiming Cao"
      },
      {
        "name" : "Jing Gao"
      },
      {
        "name" : "Lu Su"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.14581v1",
    "title" : "Can I Trust This Chatbot? Assessing User Privacy in AI-Healthcare\n  Chatbot Applications",
    "summary" : "As Conversational Artificial Intelligence (AI) becomes more integrated into\neveryday life, AI-powered chatbot mobile applications are increasingly adopted\nacross industries, particularly in the healthcare domain. These chatbots offer\naccessible and 24/7 support, yet their collection and processing of sensitive\nhealth data present critical privacy concerns. While prior research has\nexamined chatbot security, privacy issues specific to AI healthcare chatbots\nhave received limited attention. Our study evaluates the privacy practices of\n12 widely downloaded AI healthcare chatbot apps available on the App Store and\nGoogle Play in the United States. We conducted a three-step assessment\nanalyzing: (1) privacy settings during sign-up, (2) in-app privacy controls,\nand (3) the content of privacy policies. The analysis identified significant\ngaps in user data protection. Our findings reveal that half of the examined\napps did not present a privacy policy during sign up, and only two provided an\noption to disable data sharing at that stage. The majority of apps' privacy\npolicies failed to address data protection measures. Moreover, users had\nminimal control over their personal data. The study provides key insights for\ninformation science researchers, developers, and policymakers to improve\nprivacy protections in AI healthcare chatbot apps.",
    "updated" : "2025-09-18T03:29:43Z",
    "published" : "2025-09-18T03:29:43Z",
    "authors" : [
      {
        "name" : "Ramazan Yener"
      },
      {
        "name" : "Guan-Hung Chen"
      },
      {
        "name" : "Ece Gumusel"
      },
      {
        "name" : "Masooda Bashir"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.14284v1",
    "title" : "The Sum Leaks More Than Its Parts: Compositional Privacy Risks and\n  Mitigations in Multi-Agent Collaboration",
    "summary" : "As large language models (LLMs) become integral to multi-agent systems, new\nprivacy risks emerge that extend beyond memorization, direct inference, or\nsingle-turn evaluations. In particular, seemingly innocuous responses, when\ncomposed across interactions, can cumulatively enable adversaries to recover\nsensitive information, a phenomenon we term compositional privacy leakage. We\npresent the first systematic study of such compositional privacy leaks and\npossible mitigation methods in multi-agent LLM systems. First, we develop a\nframework that models how auxiliary knowledge and agent interactions jointly\namplify privacy risks, even when each response is benign in isolation. Next, to\nmitigate this, we propose and evaluate two defense strategies: (1)\nTheory-of-Mind defense (ToM), where defender agents infer a questioner's intent\nby anticipating how their outputs may be exploited by adversaries, and (2)\nCollaborative Consensus Defense (CoDef), where responder agents collaborate\nwith peers who vote based on a shared aggregated state to restrict sensitive\ninformation spread. Crucially, we balance our evaluation across compositions\nthat expose sensitive information and compositions that yield benign\ninferences. Our experiments quantify how these defense strategies differ in\nbalancing the privacy-utility trade-off. We find that while chain-of-thought\nalone offers limited protection to leakage (~39% sensitive blocking rate), our\nToM defense substantially improves sensitive query blocking (up to 97%) but can\nreduce benign task success. CoDef achieves the best balance, yielding the\nhighest Balanced Outcome (79.8%), highlighting the benefit of combining\nexplicit reasoning with defender collaboration. Together, our results expose a\nnew class of risks in collaborative LLM deployments and provide actionable\ninsights for designing safeguards against compositional, context-driven privacy\nleakage.",
    "updated" : "2025-09-16T16:57:25Z",
    "published" : "2025-09-16T16:57:25Z",
    "authors" : [
      {
        "name" : "Vaidehi Patil"
      },
      {
        "name" : "Elias Stengel-Eskin"
      },
      {
        "name" : "Mohit Bansal"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.14278v1",
    "title" : "Beyond Data Privacy: New Privacy Risks for Large Language Models",
    "summary" : "Large Language Models (LLMs) have achieved remarkable progress in natural\nlanguage understanding, reasoning, and autonomous decision-making. However,\nthese advancements have also come with significant privacy concerns. While\nsignificant research has focused on mitigating the data privacy risks of LLMs\nduring various stages of model training, less attention has been paid to new\nthreats emerging from their deployment. The integration of LLMs into widely\nused applications and the weaponization of their autonomous abilities have\ncreated new privacy vulnerabilities. These vulnerabilities provide\nopportunities for both inadvertent data leakage and malicious exfiltration from\nLLM-powered systems. Additionally, adversaries can exploit these systems to\nlaunch sophisticated, large-scale privacy attacks, threatening not only\nindividual privacy but also financial security and societal trust. In this\npaper, we systematically examine these emerging privacy risks of LLMs. We also\ndiscuss potential mitigation strategies and call for the research community to\nbroaden its focus beyond data privacy risks, developing new defenses to address\nthe evolving threats posed by increasingly powerful LLMs and LLM-powered\nsystems.",
    "updated" : "2025-09-16T09:46:09Z",
    "published" : "2025-09-16T09:46:09Z",
    "authors" : [
      {
        "name" : "Yuntao Du"
      },
      {
        "name" : "Zitao Li"
      },
      {
        "name" : "Ninghui Li"
      },
      {
        "name" : "Bolin Ding"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.14275v1",
    "title" : "FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated\n  LLMs in Mental Health",
    "summary" : "Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive\ndomains (e.g., mental health) requires balancing strict confidentiality with\nmodel utility and safety. We propose FedMentor, a federated fine-tuning\nframework that integrates Low-Rank Adaptation (LoRA) and domain-aware\nDifferential Privacy (DP) to meet per-domain privacy budgets while maintaining\nperformance. Each client (domain) applies a custom DP noise scale proportional\nto its data sensitivity, and the server adaptively reduces noise when utility\nfalls below a threshold. In experiments on three mental health datasets, we\nshow that FedMentor improves safety over standard Federated Learning without\nprivacy, raising safe output rates by up to three points and lowering toxicity,\nwhile maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the\nnon-private baseline and close to the centralized upper bound. The framework\nscales to backbones with up to 1.7B parameters on single-GPU clients, requiring\n< 173 MB of communication per round. FedMentor demonstrates a practical\napproach to privately fine-tune LLMs for safer deployments in healthcare and\nother sensitive fields.",
    "updated" : "2025-09-16T07:08:36Z",
    "published" : "2025-09-16T07:08:36Z",
    "authors" : [
      {
        "name" : "Nobin Sarwar"
      },
      {
        "name" : "Shubhashis Roy Dipta"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.15755v1",
    "title" : "Utility-based Privacy Preserving Data Mining",
    "summary" : "With the advent of big data, periodic pattern mining has demonstrated\nsignificant value in real-world applications, including smart home systems,\nhealthcare systems, and the medical field. However, advances in network\ntechnology have enabled malicious actors to extract sensitive information from\npublicly available datasets, posing significant threats to data providers and,\nin severe cases, hindering societal development. To mitigate such risks,\nprivacy-preserving utility mining (PPUM) has been proposed. However, PPUM is\nunsuitable for addressing privacy concerns in periodic information mining. To\naddress this issue, we innovatively extend the existing PPUM framework and\npropose two algorithms, Maximum sensitive Utility-MAximum maxPer item (MU-MAP)\nand Maximum sensitive Utility-MInimum maxPer item (MU-MIP). These algorithms\naim to hide sensitive periodic high-utility itemsets while generating sanitized\ndatasets. To enhance the efficiency of the algorithms, we designed two novel\ndata structures: the Sensitive Itemset List (SISL) and the Sensitive Item List\n(SIL), which store essential information about sensitive itemsets and their\nconstituent items. Moreover, several performance metrics were employed to\nevaluate the performance of our algorithms compared to the state-of-the-art\nPPUM algorithms. The experimental results show that our proposed algorithms\nachieve an Artificial Cost (AC) value of 0 on all datasets when hiding\nsensitive itemsets. In contrast, the traditional PPUM algorithm yields non-zero\nAC. This indicates that our algorithms can successfully hide sensitive periodic\nitemsets without introducing misleading patterns, whereas the PPUM algorithm\ngenerates additional itemsets that may interfere with user decision-making.\nMoreover, the results also reveal that our algorithms maintain Database Utility\nSimilarity (DUS) of over 90\\% after the sensitive itemsets are hidden.",
    "updated" : "2025-09-19T08:30:41Z",
    "published" : "2025-09-19T08:30:41Z",
    "authors" : [
      {
        "name" : "Qingfeng Zhou"
      },
      {
        "name" : "Wensheng Gan"
      },
      {
        "name" : "Zhenlian Qi"
      },
      {
        "name" : "Philip S. Yu"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.15278v1",
    "title" : "Assessing metadata privacy in neuroimaging",
    "summary" : "The ethical and legal imperative to share research data without causing harm\nrequires careful attention to privacy risks. While mounting evidence\ndemonstrates that data sharing benefits science, legitimate concerns persist\nregarding the potential leakage of personal information that could lead to\nreidentification and subsequent harm. We reviewed metadata accompanying\nneuroimaging datasets from six heterogeneous studies openly available on\nOpenNeuro, involving participants across the lifespan, from children to older\nadults, with and without clinical diagnoses, and including associated clinical\nscore data. Using metaprivBIDS (https://github.com/CPernet/metaprivBIDS), a\nnovel tool for the systematic assessment of privacy in tabular data, we found\nthat privacy is generally well maintained, with serious vulnerabilities being\nrare. Nonetheless, minor issues were identified in nearly all datasets and\nwarrant mitigation. Notably, clinical score data (e.g., neuropsychological\nresults) posed minimal reidentification risk, whereas demographic variables\n(age, sex, race, income, and geolocation) represented the principal privacy\nvulnerabilities. We outline practical measures to address these risks, enabling\nsafer data sharing practices.",
    "updated" : "2025-09-18T12:56:03Z",
    "published" : "2025-09-18T12:56:03Z",
    "authors" : [
      {
        "name" : "Emilie Kibsgaard"
      },
      {
        "name" : "Anita Sue Jwa"
      },
      {
        "name" : "Christopher J Markiewicz"
      },
      {
        "name" : "David Rodriguez Gonzalez"
      },
      {
        "name" : "Judith Sainz Pardo"
      },
      {
        "name" : "Russell A. Poldrack"
      },
      {
        "name" : "Cyril R. Pernet"
      }
    ],
    "categories" : [
      "q-bio.OT",
      "cs.CR",
      "cs.CY",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.18014v1",
    "title" : "Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data\n  Synthesis",
    "summary" : "Tabular Generative Models are often argued to preserve privacy by creating\nsynthetic datasets that resemble training data. However, auditing their\nempirical privacy remains challenging, as commonly used similarity metrics fail\nto effectively characterize privacy risk. Membership Inference Attacks (MIAs)\nhave recently emerged as a method for evaluating privacy leakage in synthetic\ndata, but their practical effectiveness is limited. Numerous attacks exist\nacross different threat models, each with distinct implementations targeting\nvarious sources of privacy leakage, making them difficult to apply\nconsistently. Moreover, no single attack consistently outperforms the others,\nleading to a routine underestimation of privacy risk.\n  To address these issues, we propose a unified, model-agnostic threat\nframework that deploys a collection of attacks to estimate the maximum\nempirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an\nopen-source Python library that streamlines this auditing process through a\nnovel testbed that integrates seamlessly into existing synthetic data\nevaluation pipelines through a Scikit-Learn-like API. Our software implements\n13 attack methods through a Scikit-Learn-like API, designed to enable fast\nsystematic estimation of privacy leakage for practitioners as well as\nfacilitate the development of new attacks and experiments for researchers.\n  We demonstrate our framework's utility in the largest tabular synthesis\nprivacy benchmark to date, revealing that higher synthetic data quality\ncorresponds to greater privacy leakage, that similarity-based privacy metrics\nshow weak correlation with MIA results, and that the differentially private\ngenerator PATEGAN can fail to preserve privacy under such attacks. This\nunderscores the necessity of MIA-based auditing when designing and deploying\nTabular Generative Models.",
    "updated" : "2025-09-22T16:53:38Z",
    "published" : "2025-09-22T16:53:38Z",
    "authors" : [
      {
        "name" : "Joshua Ward"
      },
      {
        "name" : "Xiaofeng Lin"
      },
      {
        "name" : "Chi-Hua Wang"
      },
      {
        "name" : "Guang Cheng"
      }
    ],
    "categories" : [
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.17871v1",
    "title" : "B-Privacy: Defining and Enforcing Privacy in Weighted Voting",
    "summary" : "In traditional, one-vote-per-person voting systems, privacy equates with\nballot secrecy: voting tallies are published, but individual voters' choices\nare concealed.\n  Voting systems that weight votes in proportion to token holdings, though, are\nnow prevalent in cryptocurrency and web3 systems. We show that these\nweighted-voting systems overturn existing notions of voter privacy. Our\nexperiments demonstrate that even with secret ballots, publishing raw tallies\noften reveals voters' choices.\n  Weighted voting thus requires a new framework for privacy. We introduce a\nnotion called B-privacy whose basis is bribery, a key problem in voting systems\ntoday. B-privacy captures the economic cost to an adversary of bribing voters\nbased on revealed voting tallies.\n  We propose a mechanism to boost B-privacy by noising voting tallies. We prove\nbounds on its tradeoff between B-privacy and transparency, meaning\nreported-tally accuracy. Analyzing 3,582 proposals across 30 Decentralized\nAutonomous Organizations (DAOs), we find that the prevalence of large voters\n(\"whales\") limits the effectiveness of any B-Privacy-enhancing technique.\nHowever, our mechanism proves to be effective in cases without extreme voting\nweight concentration: among proposals requiring coalitions of $\\geq5$ voters to\nflip outcomes, our mechanism raises B-privacy by a geometric mean factor of\n$4.1\\times$.\n  Our work offers the first principled guidance on transparency-privacy\ntradeoffs in weighted-voting systems, complementing existing approaches that\nfocus on ballot secrecy and revealing fundamental constraints that voting\nweight concentration imposes on privacy mechanisms.",
    "updated" : "2025-09-22T15:11:12Z",
    "published" : "2025-09-22T15:11:12Z",
    "authors" : [
      {
        "name" : "Samuel Breckenridge"
      },
      {
        "name" : "Dani Vilardell"
      },
      {
        "name" : "Andrés Fábrega"
      },
      {
        "name" : "Amy Zhao"
      },
      {
        "name" : "Patrick McCorry"
      },
      {
        "name" : "Rafael Solari"
      },
      {
        "name" : "Ari Juels"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.17488v1",
    "title" : "Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation\n  for LLM-Powered Agents",
    "summary" : "The increasing autonomy of LLM agents in handling sensitive communications,\naccelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A)\nframeworks, creates urgent privacy challenges. While recent work reveals\nsignificant gaps between LLMs' privacy Q&A performance and their agent\nbehavior, existing benchmarks remain limited to static, simplified scenarios.\nWe present PrivacyChecker, a model-agnostic, contextual integrity based\nmitigation approach that effectively reduces privacy leakage from 36.08% to\n7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving\ntask helpfulness. We also introduce PrivacyLens-Live, transforming static\nbenchmarks into dynamic MCP and A2A environments that reveal substantially\nhigher privacy risks in practical. Our modular mitigation approach integrates\nseamlessly into agent protocols through three deployment strategies, providing\npractical privacy protection for the emerging agentic ecosystem. Our data and\ncode will be made available at https://aka.ms/privacy_in_action.",
    "updated" : "2025-09-22T08:19:06Z",
    "published" : "2025-09-22T08:19:06Z",
    "authors" : [
      {
        "name" : "Shouju Wang"
      },
      {
        "name" : "Fenglin Yu"
      },
      {
        "name" : "Xirui Liu"
      },
      {
        "name" : "Xiaoting Qin"
      },
      {
        "name" : "Jue Zhang"
      },
      {
        "name" : "Qingwei Lin"
      },
      {
        "name" : "Dongmei Zhang"
      },
      {
        "name" : "Saravan Rajmohan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.17266v1",
    "title" : "Privacy-Preserving State Estimation with Crowd Sensors: An\n  Information-Theoretic Respective",
    "summary" : "Privacy-preserving state estimation for linear time-invariant dynamical\nsystems with crowd sensors is considered. At any time step, the estimator has\naccess to measurements from a randomly selected sensor from a pool of sensors\nwith pre-specified models and noise profiles. A Luenberger-like observer is\nused to fuse the measurements with the underlying model of the system to\nrecursively generate the state estimates. An additive privacy-preserving noise\nis used to constrain information leakage. Information leakage is measured via\nmutual information between the identity of the sensors and the state estimate\nconditioned on the actual state of the system. This captures an omnipotent\nadversary that not only can access state estimates but can also gather direct\nhigh-quality state measurements. Any prescribed level of information leakage is\nshown to be achievable by appropriately selecting the variance of the\nprivacy-preserving noise. Therefore, privacy-utility trade-off can be\nfine-tuned.",
    "updated" : "2025-09-21T22:44:34Z",
    "published" : "2025-09-21T22:44:34Z",
    "authors" : [
      {
        "name" : "Farhad Farokhi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "cs.SY",
      "eess.SY",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.16962v1",
    "title" : "Temporal Drift in Privacy Recall: Users Misremember From Verbatim Loss\n  to Gist-Based Overexposure Over Time",
    "summary" : "With social media content traversing the different platforms, occasionally\nresurfacing after periods of time, users are increasingly prone to unintended\ndisclosure resulting from a misremembered acceptance of privacy. Context\ncollapse and interface cues are two factors considered by prior researchers,\nyet we know less about how time-lapse basically alters recall of past audiences\ndestined for exposure. Likewise, the design space for mitigating this temporal\nexposure risk remains underexplored. Our work theorizes temporal drift in\nprivacy recall as verbatim memory of prior settings blowing apart and\neventually settling with gist-based heuristics, which more often than not\nselect an audience larger than the original one. Grounded in memory research,\ncontextual integrity, and usable privacy, we examine why such a drift occurs,\nwhy it tends to bias toward broader sharing, and how it compounds upon repeat\nexposure. Following that, we suggest provenance-forward interface schemes and a\nrisk-based evaluation framework that mutates recall into recognition. The merit\nof our work lies in establishing a temporal awareness of privacy design as an\nessential safety rail against inadvertent overexposure.",
    "updated" : "2025-09-21T07:50:19Z",
    "published" : "2025-09-21T07:50:19Z",
    "authors" : [
      {
        "name" : "Haoze Guo"
      },
      {
        "name" : "Ziqi Wei"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.16915v1",
    "title" : "Differential Privacy for Euclidean Jordan Algebra with Applications to\n  Private Symmetric Cone Programming",
    "summary" : "In this paper, we study differentially private mechanisms for functions whose\noutputs lie in a Euclidean Jordan algebra. Euclidean Jordan algebras capture\nmany important mathematical structures and form the foundation of linear\nprogramming, second-order cone programming, and semidefinite programming. Our\nmain contribution is a generic Gaussian mechanism for such functions, with\nsensitivity measured in $\\ell_2$, $\\ell_1$, and $\\ell_\\infty$ norms. Notably,\nthis framework includes the important case where the function outputs are\nsymmetric matrices, and sensitivity is measured in the Frobenius, nuclear, or\nspectral norm. We further derive private algorithms for solving symmetric cone\nprograms under various settings, using a combination of the multiplicative\nweights update method and our generic Gaussian mechanism. As an application, we\npresent differentially private algorithms for semidefinite programming,\nresolving a major open question posed by [Hsu, Roth, Roughgarden, and Ullman,\nICALP 2014].",
    "updated" : "2025-09-21T04:34:12Z",
    "published" : "2025-09-21T04:34:12Z",
    "authors" : [
      {
        "name" : "Zhao Song"
      },
      {
        "name" : "Jianfei Xue"
      },
      {
        "name" : "Lichen Zhang"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.CR",
      "cs.DS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.13625v2",
    "title" : "Privacy-Aware In-Context Learning for Large Language Models",
    "summary" : "Large language models (LLMs) have significantly transformed natural language\nunderstanding and generation, but they raise privacy concerns due to potential\nexposure of sensitive information. Studies have highlighted the risk of\ninformation leakage, where adversaries can extract sensitive information\nembedded in the prompts. In this work, we introduce a novel private prediction\nframework for generating high-quality synthetic text with strong privacy\nguarantees. Our approach leverages the Differential Privacy (DP) framework to\nensure worst-case theoretical bounds on information leakage without requiring\nany fine-tuning of the underlying models. The proposed method performs\ninference on private records and aggregates the resulting per-token output\ndistributions. This enables the generation of longer and coherent synthetic\ntext while maintaining privacy guarantees. Additionally, we propose a simple\nblending operation that combines private and public inference to further\nenhance utility. Empirical evaluations demonstrate that our approach\noutperforms previous state-of-the-art methods on in-context-learning (ICL)\ntasks, making it a promising direction for privacy-preserving text generation\nwhile maintaining high utility.",
    "updated" : "2025-09-22T00:06:49Z",
    "published" : "2025-09-17T01:50:32Z",
    "authors" : [
      {
        "name" : "Bishnu Bhusal"
      },
      {
        "name" : "Manoj Acharya"
      },
      {
        "name" : "Ramneet Kaur"
      },
      {
        "name" : "Colin Samplawski"
      },
      {
        "name" : "Anirban Roy"
      },
      {
        "name" : "Adam D. Cobb"
      },
      {
        "name" : "Rohit Chadha"
      },
      {
        "name" : "Susmit Jha"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.19041v1",
    "title" : "Position: Human-Robot Interaction in Embodied Intelligence Demands a\n  Shift From Static Privacy Controls to Dynamic Learning",
    "summary" : "The reasoning capabilities of embodied agents introduce a critical,\nunder-explored inferential privacy challenge, where the risk of an agent\ngenerate sensitive conclusions from ambient data. This capability creates a\nfundamental tension between an agent's utility and user privacy, rendering\ntraditional static controls ineffective. To address this, this position paper\nproposes a framework that reframes privacy as a dynamic learning problem\ngrounded in theory of Contextual Integrity (CI). Our approach enables agents to\nproactively learn and adapt to individual privacy norms through interaction,\noutlining a research agenda to develop embodied agents that are both capable\nand function as trustworthy safeguards of user privacy.",
    "updated" : "2025-09-23T14:10:00Z",
    "published" : "2025-09-23T14:10:00Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Hong Jia"
      },
      {
        "name" : "Simin Li"
      },
      {
        "name" : "Ting Dang"
      },
      {
        "name" : "Yongquan `Owen' Hu"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.18949v1",
    "title" : "Towards Privacy-Aware Bayesian Networks: A Credal Approach",
    "summary" : "Bayesian networks (BN) are probabilistic graphical models that enable\nefficient knowledge representation and inference. These have proven effective\nacross diverse domains, including healthcare, bioinformatics and economics. The\nstructure and parameters of a BN can be obtained by domain experts or directly\nlearned from available data. However, as privacy concerns escalate, it becomes\nincreasingly critical for publicly released models to safeguard sensitive\ninformation in training data. Typically, released models do not prioritize\nprivacy by design. In particular, tracing attacks from adversaries can combine\nthe released BN with auxiliary data to determine whether specific individuals\nbelong to the data from which the BN was learned. State-of-the-art protection\ntecniques involve introducing noise into the learned parameters. While this\noffers robust protection against tracing attacks, it significantly impacts the\nmodel's utility, in terms of both the significance and accuracy of the\nresulting inferences. Hence, high privacy may be attained at the cost of\nreleasing a possibly ineffective model. This paper introduces credal networks\n(CN) as a novel solution for balancing the model's privacy and utility. After\nadapting the notion of tracing attacks, we demonstrate that a CN enables the\nmasking of the learned BN, thereby reducing the probability of successful\nattacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve\nmeaningful inferences while safeguarding privacy. Moreover, we identify key\nlearning information that must be concealed to prevent attackers from\nrecovering the underlying BN. Finally, we conduct a set of numerical\nexperiments to analyze how privacy gains can be modulated by tuning the CN\nhyperparameters. Our results confirm that CNs provide a principled, practical,\nand effective approach towards the development of privacy-aware probabilistic\ngraphical models.",
    "updated" : "2025-09-23T12:58:32Z",
    "published" : "2025-09-23T12:58:32Z",
    "authors" : [
      {
        "name" : "Niccolò Rocchi"
      },
      {
        "name" : "Fabio Stella"
      },
      {
        "name" : "Cassio de Campos"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.18871v1",
    "title" : "R-CONV++: Uncovering Privacy Vulnerabilities through Analytical Gradient\n  Inversion Attacks",
    "summary" : "Federated learning has emerged as a prominent privacy-preserving technique\nfor leveraging large-scale distributed datasets by sharing gradients instead of\nraw data. However, recent studies indicate that private training data can still\nbe exposed through gradient inversion attacks. While earlier analytical methods\nhave demonstrated success in reconstructing input data from fully connected\nlayers, their effectiveness significantly diminishes when applied to\nconvolutional layers, high-dimensional inputs, and scenarios involving multiple\ntraining examples. This paper extends our previous work \\cite{eltaras2024r} and\nproposes three advanced algorithms to broaden the applicability of gradient\ninversion attacks. The first algorithm presents a novel data leakage method\nthat efficiently exploits convolutional layer gradients, demonstrating that\neven with non-fully invertible activation functions, such as ReLU, training\nsamples can be analytically reconstructed directly from gradients without the\nneed to reconstruct intermediate layer outputs. Building on this foundation,\nthe second algorithm extends this analytical approach to support\nhigh-dimensional input data, substantially enhancing its utility across complex\nreal-world datasets. The third algorithm introduces an innovative analytical\nmethod for reconstructing mini-batches, addressing a critical gap in current\nresearch that predominantly focuses on reconstructing only a single training\nexample. Unlike previous studies that focused mainly on the weight constraints\nof convolutional layers, our approach emphasizes the pivotal role of gradient\nconstraints, revealing that successful attacks can be executed with fewer than\n5\\% of the constraints previously deemed necessary in certain layers.",
    "updated" : "2025-09-23T10:10:12Z",
    "published" : "2025-09-23T10:10:12Z",
    "authors" : [
      {
        "name" : "Tamer Ahmed Eltaras"
      },
      {
        "name" : "Qutaibah Malluhi"
      },
      {
        "name" : "Alessandro Savino"
      },
      {
        "name" : "Stefano Di Carlo"
      },
      {
        "name" : "Adnan Qayyum"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.18696v1",
    "title" : "FlowCrypt: Flow-Based Lightweight Encryption with Near-Lossless Recovery\n  for Cloud Photo Privacy",
    "summary" : "The widespread adoption of smartphone photography has led users to\nincreasingly rely on cloud storage for personal photo archiving and sharing,\nraising critical privacy concerns. Existing deep learning-based image\nencryption schemes, typically built upon CNNs or GANs, often depend on\ntraditional cryptographic algorithms and lack inherent architectural\nreversibility, resulting in limited recovery quality and poor robustness.\nInvertible neural networks (INNs) have emerged to address this issue by\nenabling reversible transformations, yet the first INN-based encryption scheme\nstill relies on an auxiliary reference image and discards by-product\ninformation before decryption, leading to degraded recovery and limited\npracticality. To address these limitations, this paper proposes FlowCrypt, a\nnovel flow-based image encryption framework that simultaneously achieves\nnear-lossless recovery, high security, and lightweight model design. FlowCrypt\nbegins by applying a key-conditioned random split to the input image, enhancing\nforward-process randomness and encryption strength. The resulting components\nare processed through a Flow-based Encryption/Decryption (FED) module composed\nof invertible blocks, which share parameters across encryption and decryption.\nThanks to its reversible architecture and reference-free design, FlowCrypt\nensures high-fidelity image recovery. Extensive experiments show that FlowCrypt\nachieves recovery quality with 100dB on three datasets, produces uniformly\ndistributed cipher images, and maintains a compact architecture with only 1M\nparameters, making it suitable for mobile and edge-device applications.",
    "updated" : "2025-09-23T06:25:35Z",
    "published" : "2025-09-23T06:25:35Z",
    "authors" : [
      {
        "name" : "Xiaohui Yang"
      },
      {
        "name" : "Ping Ping"
      },
      {
        "name" : "Feng Xu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.18413v1",
    "title" : "VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership\n  Inference Attacks",
    "summary" : "Voice anonymization aims to conceal speaker identity and attributes while\npreserving intelligibility, but current evaluations rely almost exclusively on\nEqual Error Rate (EER) that obscures whether adversaries can mount\nhigh-precision attacks. We argue that privacy should instead be evaluated in\nthe low false-positive rate (FPR) regime, where even a small number of\nsuccessful identifications constitutes a meaningful breach. To this end, we\nintroduce VoxGuard, a framework grounded in differential privacy and membership\ninference that formalizes two complementary notions: User Privacy, preventing\nspeaker re-identification, and Attribute Privacy, protecting sensitive traits\nsuch as gender and accent. Across synthetic and real datasets, we find that\ninformed adversaries, especially those using fine-tuned models and\nmax-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR\ndespite similar EER. For attributes, we show that simple transparent attacks\nrecover gender and accent with near-perfect accuracy even after anonymization.\nOur results demonstrate that EER substantially underestimates leakage,\nhighlighting the need for low-FPR evaluation, and recommend VoxGuard as a\nbenchmark for evaluating privacy leakage.",
    "updated" : "2025-09-22T20:57:48Z",
    "published" : "2025-09-22T20:57:48Z",
    "authors" : [
      {
        "name" : "Efthymios Tsaprazlis"
      },
      {
        "name" : "Thanathai Lertpetchpun"
      },
      {
        "name" : "Tiantian Feng"
      },
      {
        "name" : "Sai Praneeth Karimireddy"
      },
      {
        "name" : "Shrikanth Narayanan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.18311v1",
    "title" : "Fine-Tuning Robot Policies While Maintaining User Privacy",
    "summary" : "Recent works introduce general-purpose robot policies. These policies provide\na strong prior over how robots should behave -- e.g., how a robot arm should\nmanipulate food items. But in order for robots to match an individual person's\nneeds, users typically fine-tune these generalized policies -- e.g., showing\nthe robot arm how to make their own preferred dinners. Importantly, during the\nprocess of personalizing robots, end-users leak data about their preferences,\nhabits, and styles (e.g., the foods they prefer to eat). Other agents can\nsimply roll-out the fine-tuned policy and see these personally-trained\nbehaviors. This leads to a fundamental challenge: how can we develop robots\nthat personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop\nPRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to\nmathematically transform the weights of the robot's network. With the correct\nkey, the robot's policy switches to match that user's preferences -- but with\nincorrect keys, the robot reverts to its baseline behaviors. We show the\ngeneral applicability of our method across multiple model types in imitation\nlearning, reinforcement learning, and classification tasks. PRoP is practically\nadvantageous because it retains the architecture and behaviors of the original\npolicy, and experimentally outperforms existing encoder-based approaches. See\nvideos and code here: https://prop-icra26.github.io.",
    "updated" : "2025-09-22T18:36:25Z",
    "published" : "2025-09-22T18:36:25Z",
    "authors" : [
      {
        "name" : "Benjamin A. Christie"
      },
      {
        "name" : "Sagar Parekh"
      },
      {
        "name" : "Dylan P. Losey"
      }
    ],
    "categories" : [
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.18187v1",
    "title" : "V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor\n  Fusion Framework for Road Safety & Driver Behaviour Modelling",
    "summary" : "Road traffic accidents remain a major public health challenge, particularly\nin countries with heterogeneous road conditions, mixed traffic flow, and\nvariable driving discipline, such as Pakistan. Reliable detection of unsafe\ndriving behaviours is a prerequisite for improving road safety, enabling\nadvanced driver assistance systems (ADAS), and supporting data driven decisions\nin insurance and fleet management. Most of existing datasets originate from the\ndeveloped countries with limited representation of the behavioural diversity\nobserved in emerging economies and the driver's face recording voilates the\nprivacy preservation. We present V-SenseDrive, the first privacy-preserving\nmultimodal driver behaviour dataset collected entirely within the Pakistani\ndriving environment. V-SenseDrive combines smartphone based inertial and GPS\nsensor data with synchronized road facing video to record three target driving\nbehaviours (normal, aggressive, and risky) on multiple types of roads,\nincluding urban arterials, secondary roads, and motorways. Data was gathered\nusing a custom Android application designed to capture high frequency\naccelerometer, gyroscope, and GPS streams alongside continuous video, with all\nsources precisely time aligned to enable multimodal analysis. The focus of this\nwork is on the data acquisition process, covering participant selection,\ndriving scenarios, environmental considerations, and sensor video\nsynchronization techniques. The dataset is structured into raw, processed, and\nsemantic layers, ensuring adaptability for future research in driver behaviour\nclassification, traffic safety analysis, and ADAS development. By representing\nreal world driving in Pakistan, V-SenseDrive fills a critical gap in the global\nlandscape of driver behaviour datasets and lays the groundwork for context\naware intelligent transportation solutions.",
    "updated" : "2025-09-18T21:55:14Z",
    "published" : "2025-09-18T21:55:14Z",
    "authors" : [
      {
        "name" : "Muhammad Naveed"
      },
      {
        "name" : "Nazia Perwaiz"
      },
      {
        "name" : "Sidra Sultana"
      },
      {
        "name" : "Mohaira Ahmad"
      },
      {
        "name" : "Muhammad Moazam Fraz"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.13625v3",
    "title" : "Privacy-Aware In-Context Learning for Large Language Models",
    "summary" : "Large language models (LLMs) have significantly transformed natural language\nunderstanding and generation, but they raise privacy concerns due to potential\nexposure of sensitive information. Studies have highlighted the risk of\ninformation leakage, where adversaries can extract sensitive information\nembedded in the prompts. In this work, we introduce a novel private prediction\nframework for generating high-quality synthetic text with strong privacy\nguarantees. Our approach leverages the Differential Privacy (DP) framework to\nensure worst-case theoretical bounds on information leakage without requiring\nany fine-tuning of the underlying models. The proposed method performs\ninference on private records and aggregates the resulting per-token output\ndistributions. This enables the generation of longer and coherent synthetic\ntext while maintaining privacy guarantees. Additionally, we propose a simple\nblending operation that combines private and public inference to further\nenhance utility. Empirical evaluations demonstrate that our approach\noutperforms previous state-of-the-art methods on in-context-learning (ICL)\ntasks, making it a promising direction for privacy-preserving text generation\nwhile maintaining high utility.",
    "updated" : "2025-09-23T02:40:24Z",
    "published" : "2025-09-17T01:50:32Z",
    "authors" : [
      {
        "name" : "Bishnu Bhusal"
      },
      {
        "name" : "Manoj Acharya"
      },
      {
        "name" : "Ramneet Kaur"
      },
      {
        "name" : "Colin Samplawski"
      },
      {
        "name" : "Anirban Roy"
      },
      {
        "name" : "Adam D. Cobb"
      },
      {
        "name" : "Rohit Chadha"
      },
      {
        "name" : "Susmit Jha"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.18134v1",
    "title" : "A Weighted Gradient Tracking Privacy-Preserving Method for Distributed\n  Optimization",
    "summary" : "This paper investigates the privacy-preserving distributed optimization\nproblem, aiming to protect agents' private information from potential attackers\nduring the optimization process. Gradient tracking, an advanced technique for\nimproving the convergence rate in distributed optimization, has been applied to\nmost first-order algorithms in recent years. We first reveal the inherent\nprivacy leakage risk associated with gradient tracking. Building upon this\ninsight, we propose a weighted gradient tracking distributed privacy-preserving\nalgorithm, eliminating the privacy leakage risk in gradient tracking using\ndecaying weight factors. Then, we characterize the convergence of the proposed\nalgorithm under time-varying heterogeneous step sizes. We prove the proposed\nalgorithm converges precisely to the optimal solution under mild assumptions.\nFinally, numerical simulations validate the algorithm's effectiveness through a\nclassical distributed estimation problem and the distributed training of a\nconvolutional neural network.",
    "updated" : "2025-09-14T07:29:53Z",
    "published" : "2025-09-14T07:29:53Z",
    "authors" : [
      {
        "name" : "Furan Xie"
      },
      {
        "name" : "Bing Liu"
      },
      {
        "name" : "Li Chai"
      }
    ],
    "categories" : [
      "cs.LG",
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.20324v1",
    "title" : "RAG Security and Privacy: Formalizing the Threat Model and Attack\n  Surface",
    "summary" : "Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems.",
    "updated" : "2025-09-24T17:11:35Z",
    "published" : "2025-09-24T17:11:35Z",
    "authors" : [
      {
        "name" : "Atousa Arzanipour"
      },
      {
        "name" : "Rouzbeh Behnia"
      },
      {
        "name" : "Reza Ebrahimi"
      },
      {
        "name" : "Kaushik Dutta"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.20283v1",
    "title" : "Monitoring Violations of Differential Privacy over Time",
    "summary" : "Auditing differential privacy has emerged as an important area of research\nthat supports the design of privacy-preserving mechanisms. Privacy audits help\nto obtain empirical estimates of the privacy parameter, to expose flawed\nimplementations of algorithms and to compare practical with theoretical privacy\nguarantees. In this work, we investigate an unexplored facet of privacy\nauditing: the sustained auditing of a mechanism that can go through changes\nduring its development or deployment. Monitoring the privacy of algorithms over\ntime comes with specific challenges. Running state-of-the-art (static) auditors\nrepeatedly requires excessive sampling efforts, while the reliability of such\nmethods deteriorates over time without proper adjustments. To overcome these\nobstacles, we present a new monitoring procedure that extracts information from\nthe entire deployment history of the algorithm. This allows us to reduce\nsampling efforts, while sustaining reliable outcomes of our auditor. We derive\nformal guarantees with regard to the soundness of our methods and evaluate\ntheir performance for important mechanisms from the literature. Our theoretical\nfindings and experiments demonstrate the efficacy of our approach.",
    "updated" : "2025-09-24T16:15:51Z",
    "published" : "2025-09-24T16:15:51Z",
    "authors" : [
      {
        "name" : "Önder Askin"
      },
      {
        "name" : "Tim Kutta"
      },
      {
        "name" : "Holger Dette"
      }
    ],
    "categories" : [
      "cs.CR",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.20153v1",
    "title" : "Affective Computing and Emotional Data: Challenges and Implications in\n  Privacy Regulations, The AI Act, and Ethics in Large Language Models",
    "summary" : "This paper examines the integration of emotional intelligence into artificial\nintelligence systems, with a focus on affective computing and the growing\ncapabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to\nrecognize and respond to human emotions. Drawing on interdisciplinary research\nthat combines computer science, psychology, and neuroscience, the study\nanalyzes foundational neural architectures - CNNs for processing facial\nexpressions and RNNs for sequential data, such as speech and text - that enable\nemotion recognition. It examines the transformation of human emotional\nexperiences into structured emotional data, addressing the distinction between\nexplicit emotional data collected with informed consent in research settings\nand implicit data gathered passively through everyday digital interactions.\nThat raises critical concerns about lawful processing, AI transparency, and\nindividual autonomy over emotional expressions in digital environments. The\npaper explores implications across various domains, including healthcare,\neducation, and customer service, while addressing challenges of cultural\nvariations in emotional expression and potential biases in emotion recognition\nsystems across different demographic groups. From a regulatory perspective, the\npaper examines emotional data in the context of the GDPR and the EU AI Act\nframeworks, highlighting how emotional data may be considered sensitive\npersonal data that requires robust safeguards, including purpose limitation,\ndata minimization, and meaningful consent mechanisms.",
    "updated" : "2025-09-24T14:18:41Z",
    "published" : "2025-09-24T14:18:41Z",
    "authors" : [
      {
        "name" : "Nicola Fabiano"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.20024v1",
    "title" : "Generative Adversarial Networks Applied for Privacy Preservation in\n  Biometric-Based Authentication and Identification",
    "summary" : "Biometric-based authentication systems are getting broadly adopted in many\nareas. However, these systems do not allow participating users to influence the\nway their data is used. Furthermore, the data may leak and can be misused\nwithout the users' knowledge. In this paper, we propose a new authentication\nmethod that preserves the privacy of individuals and is based on a generative\nadversarial network (GAN). Concretely, we suggest using the GAN for translating\nimages of faces to a visually private domain (e.g., flowers or shoes).\nClassifiers, which are used for authentication purposes, are then trained on\nthe images from the visually private domain. Based on our experiments, the\nmethod is robust against attacks and still provides meaningful utility.",
    "updated" : "2025-09-24T11:39:40Z",
    "published" : "2025-09-24T11:39:40Z",
    "authors" : [
      {
        "name" : "Lubos Mjachky"
      },
      {
        "name" : "Ivan Homoliak"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.19925v1",
    "title" : "CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain",
    "summary" : "As enterprises increasingly integrate cloud-based large language models\n(LLMs) such as ChatGPT and Gemini into their legal document workflows,\nprotecting sensitive contractual information - including Personally\nIdentifiable Information (PII) and commercially sensitive clauses - has emerged\nas a critical challenge. In this work, we propose CON-QA, a hybrid\nprivacy-preserving framework designed specifically for secure question\nanswering over enterprise contracts, effectively combining local and\ncloud-hosted LLMs. The CON-QA framework operates through three stages: (i)\nsemantic query decomposition and query-aware document chunk retrieval using a\nlocally deployed LLM analysis, (ii) anonymization of detected sensitive\nentities via a structured one-to-many mapping scheme, ensuring semantic\ncoherence while preventing cross-session entity inference attacks, and (iii)\nanonymized response generation by a cloud-based LLM, with accurate\nreconstruction of the original answer locally using a session-consistent\nmany-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce\nCUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world\nCUAD contract documents, encompassing simple, complex, and summarization-style\nqueries. Empirical evaluations, complemented by detailed human assessments,\nconfirm that CON-QA effectively maintains both privacy and utility, preserves\nanswer quality, maintains fidelity to legal clause semantics, and significantly\nmitigates privacy risks, demonstrating its practical suitability for secure,\nenterprise-level contract documents.",
    "updated" : "2025-09-24T09:29:17Z",
    "published" : "2025-09-24T09:29:17Z",
    "authors" : [
      {
        "name" : "Ajeet Kumar Singh"
      },
      {
        "name" : "Rajsabi Surya"
      },
      {
        "name" : "Anurag Tripathi"
      },
      {
        "name" : "Santanu Choudhury"
      },
      {
        "name" : "Sudhir Bisane"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.19906v1",
    "title" : "Voice Privacy Preservation with Multiple Random Orthogonal Secret Keys:\n  Attack Resistance Analysis",
    "summary" : "Recently, opportunities to transmit speech data to deep learning models\nexecuted in the cloud have increased. This has led to growing concerns about\nspeech privacy, including both speaker-specific information and the linguistic\ncontent of utterances. As an approach to preserving speech privacy, a speech\nprivacy-preserving method based on encryption using a secret key with a random\northogonal matrix has been proposed. This method enables cloud-based model\ninference while concealing both the speech content and the speaker identity.\nHowever, the method has limited attack resistance and is constrained in terms\nof the deep learning models to which the encryption can be applied. In this\nwork, we propose a method that enhances the attack resistance of the\nconventional speech privacy-preserving technique by employing multiple random\northogonal matrices as secret keys. We also introduce approaches to relax the\nmodel constraints, enabling the application of our method to a broader range of\ndeep learning models. Furthermore, we investigate the robustness of the\nproposed method against attacks using extended attack scenarios based on the\nscenarios employed in the Voice Privacy Challenge. Our experimental results\nconfirmed that the proposed method maintains privacy protection performance for\nspeaker concealment, even under more powerful attack scenarios not considered\nin prior work.",
    "updated" : "2025-09-24T09:05:13Z",
    "published" : "2025-09-24T09:05:13Z",
    "authors" : [
      {
        "name" : "Kohei Tanaka"
      },
      {
        "name" : "Hitoshi Kiya"
      },
      {
        "name" : "Sayaka Shiota"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.19661v1",
    "title" : "Consistent Estimation of Numerical Distributions under Local\n  Differential Privacy by Wavelet Expansion",
    "summary" : "Distribution estimation under local differential privacy (LDP) is a\nfundamental and challenging task. Significant progresses have been made on\ncategorical data. However, due to different evaluation metrics, these methods\ndo not work well when transferred to numerical data. In particular, we need to\nprevent the probability mass from being misplaced far away. In this paper, we\npropose a new approach that express the sample distribution using wavelet\nexpansions. The coefficients of wavelet series are estimated under LDP. Our\nmethod prioritizes the estimation of low-order coefficients, in order to ensure\naccurate estimation at macroscopic level. Therefore, the probability mass is\nprevented from being misplaced too far away from its ground truth. We establish\ntheoretical guarantees for our methods. Experiments show that our wavelet\nexpansion method significantly outperforms existing solutions under Wasserstein\nand KS distances.",
    "updated" : "2025-09-24T00:37:22Z",
    "published" : "2025-09-24T00:37:22Z",
    "authors" : [
      {
        "name" : "Puning Zhao"
      },
      {
        "name" : "Zhikun Zhang"
      },
      {
        "name" : "Bo Sun"
      },
      {
        "name" : "Li Shen"
      },
      {
        "name" : "Liang Zhang"
      },
      {
        "name" : "Shaowei Wang"
      },
      {
        "name" : "Zhe Liu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.19599v1",
    "title" : "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems",
    "summary" : "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing.",
    "updated" : "2025-09-23T21:46:38Z",
    "published" : "2025-09-23T21:46:38Z",
    "authors" : [
      {
        "name" : "Danilo Trombino"
      },
      {
        "name" : "Vincenzo Pecorella"
      },
      {
        "name" : "Alessandro de Giulii"
      },
      {
        "name" : "Davide Tresoldi"
      }
    ],
    "categories" : [
      "cs.MA",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.20867v1",
    "title" : "Federated Markov Imputation: Privacy-Preserving Temporal Imputation in\n  Multi-Centric ICU Environments",
    "summary" : "Missing data is a persistent challenge in federated learning on electronic\nhealth records, particularly when institutions collect time-series data at\nvarying temporal granularities. To address this, we propose Federated Markov\nImputation (FMI), a privacy-preserving method that enables Intensive Care Units\n(ICUs) to collaboratively build global transition models for temporal\nimputation. We evaluate FMI on a real-world sepsis onset prediction task using\nthe MIMIC-IV dataset and show that it outperforms local imputation baselines,\nespecially in scenarios with irregular sampling intervals across ICUs.",
    "updated" : "2025-09-25T08:00:05Z",
    "published" : "2025-09-25T08:00:05Z",
    "authors" : [
      {
        "name" : "Christoph Düsing"
      },
      {
        "name" : "Philipp Cimiano"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.20838v1",
    "title" : "Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search",
    "summary" : "The increasing adoption of large language models (LLMs) in cloud-based\nservices has raised significant privacy concerns, as user inputs may\ninadvertently expose sensitive information. Existing text anonymization and\nde-identification techniques, such as rule-based redaction and scrubbing, often\nstruggle to balance privacy preservation with text naturalness and utility. In\nthis work, we propose a zero-shot, tree-search-based iterative sentence\nrewriting algorithm that systematically obfuscates or deletes private\ninformation while preserving coherence, relevance, and naturalness. Our method\nincrementally rewrites privacy-sensitive segments through a structured search\nguided by a reward model, enabling dynamic exploration of the rewriting space.\nExperiments on privacy-sensitive datasets show that our approach significantly\noutperforms existing baselines, achieving a superior balance between privacy\nprotection and utility preservation.",
    "updated" : "2025-09-25T07:23:52Z",
    "published" : "2025-09-25T07:23:52Z",
    "authors" : [
      {
        "name" : "Shuo Huang"
      },
      {
        "name" : "Xingliang Yuan"
      },
      {
        "name" : "Gholamreza Haffari"
      },
      {
        "name" : "Lizhen Qu"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.20460v1",
    "title" : "Differential Privacy of Network Parameters from a System Identification\n  Perspective",
    "summary" : "This paper addresses the problem of protecting network information from\nprivacy system identification (SI) attacks when sharing cyber-physical system\nsimulations. We model analyst observations of networked states as time-series\noutputs of a graph filter driven by differentially private (DP) nodal\nexcitations, with the analyst aiming to infer the underlying graph shift\noperator (GSO). Unlike traditional SI, which estimates system parameters, we\nstudy the inverse problem: what assumptions prevent adversaries from\nidentifying the GSO while preserving utility for legitimate analysis. We show\nthat applying DP mechanisms to inputs provides formal privacy guarantees for\nthe GSO, linking the $(\\epsilon,\\delta)$-DP bound to the spectral properties of\nthe graph filter and noise covariance. More precisely, for DP Gaussian signals,\nthe spectral characteristics of both the filter and noise covariance determine\nthe privacy bound, with smooth filters and low-condition-number covariance\nyielding greater privacy.",
    "updated" : "2025-09-24T18:06:11Z",
    "published" : "2025-09-24T18:06:11Z",
    "authors" : [
      {
        "name" : "Andrew Campbell"
      },
      {
        "name" : "Anna Scaglione"
      },
      {
        "name" : "Hang Liu"
      },
      {
        "name" : "Victor Elvira"
      },
      {
        "name" : "Sean Peisert"
      },
      {
        "name" : "Daniel Arnold"
      }
    ],
    "categories" : [
      "cs.CR",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.20454v1",
    "title" : "Bridging Privacy and Utility: Synthesizing anonymized EEG with\n  constraining utility functions",
    "summary" : "Electroencephalography (EEG) is widely used for recording brain activity and\nhas seen numerous applications in machine learning, such as detecting sleep\nstages and neurological disorders. Several studies have successfully shown the\npotential of EEG data for re-identification and leakage of other personal\ninformation. Therefore, the increasing availability of EEG consumer devices\nraises concerns about user privacy, motivating us to investigate how to\nsafeguard this sensitive data while retaining its utility for EEG applications.\nTo address this challenge, we propose a transformer-based autoencoder to create\nEEG data that does not allow for subject re-identification while still\nretaining its utility for specific machine learning tasks. We apply our\napproach to automatic sleep staging by evaluating the re-identification and\nutility potential of EEG data before and after anonymization. The results show\nthat the re-identifiability of the EEG signal can be substantially reduced\nwhile preserving its utility for machine learning.",
    "updated" : "2025-09-24T18:02:41Z",
    "published" : "2025-09-24T18:02:41Z",
    "authors" : [
      {
        "name" : "Kay Fuhrmeister"
      },
      {
        "name" : "Arne Pelzer"
      },
      {
        "name" : "Fabian Radke"
      },
      {
        "name" : "Julia Lechinger"
      },
      {
        "name" : "Mahzad Gharleghi"
      },
      {
        "name" : "Thomas Köllmer"
      },
      {
        "name" : "Insa Wolf"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.20153v2",
    "title" : "Affective Computing and Emotional Data: Challenges and Implications in\n  Privacy Regulations, The AI Act, and Ethics in Large Language Models",
    "summary" : "This paper examines the integration of emotional intelligence into artificial\nintelligence systems, with a focus on affective computing and the growing\ncapabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to\nrecognize and respond to human emotions. Drawing on interdisciplinary research\nthat combines computer science, psychology, and neuroscience, the study\nanalyzes foundational neural architectures - CNNs for processing facial\nexpressions and RNNs for sequential data, such as speech and text - that enable\nemotion recognition. It examines the transformation of human emotional\nexperiences into structured emotional data, addressing the distinction between\nexplicit emotional data collected with informed consent in research settings\nand implicit data gathered passively through everyday digital interactions.\nThat raises critical concerns about lawful processing, AI transparency, and\nindividual autonomy over emotional expressions in digital environments. The\npaper explores implications across various domains, including healthcare,\neducation, and customer service, while addressing challenges of cultural\nvariations in emotional expression and potential biases in emotion recognition\nsystems across different demographic groups. From a regulatory perspective, the\npaper examines emotional data in the context of the GDPR and the EU AI Act\nframeworks, highlighting how emotional data may be considered sensitive\npersonal data that requires robust safeguards, including purpose limitation,\ndata minimization, and meaningful consent mechanisms.",
    "updated" : "2025-09-25T10:43:22Z",
    "published" : "2025-09-24T14:18:41Z",
    "authors" : [
      {
        "name" : "Nicola Fabiano"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.20388v1",
    "title" : "Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants",
    "summary" : "The rapid integration of AI-powered coding assistants into developer\nworkflows has raised significant privacy and trust concerns. As developers\nentrust proprietary code to services like OpenAI's GPT, Google's Gemini, and\nGitHub Copilot, the unclear data handling practices of these tools create\nsecurity and compliance risks. This paper addresses this challenge by\nintroducing and applying a novel, expert-validated privacy scorecard. The\nmethodology involves a detailed analysis of four document types; from legal\npolicies to external audits; to score five leading assistants against 14\nweighted criteria. A legal expert and a data protection officer refined these\ncriteria and their weighting. The results reveal a distinct hierarchy of\nprivacy protections, with a 20-point gap between the highest- and lowest-ranked\ntools. The analysis uncovers common industry weaknesses, including the\npervasive use of opt-out consent for model training and a near-universal\nfailure to filter secrets from user prompts proactively. The resulting\nscorecard provides actionable guidance for developers and organizations,\nenabling evidence-based tool selection. This work establishes a new benchmark\nfor transparency and advocates for a shift towards more user-centric privacy\nstandards in the AI industry.",
    "updated" : "2025-09-22T21:45:45Z",
    "published" : "2025-09-22T21:45:45Z",
    "authors" : [
      {
        "name" : "Amir AL-Maamari"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  }
]