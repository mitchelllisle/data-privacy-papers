[
  {
    "id" : "http://arxiv.org/abs/2509.03350v1",
    "title" : "Exposing Privacy Risks in Anonymizing Clinical Data: Combinatorial\n  Refinement Attacks on k-Anonymity Without Auxiliary Information",
    "summary" : "Despite longstanding criticism from the privacy community, k-anonymity\nremains a widely used standard for data anonymization, mainly due to its\nsimplicity, regulatory alignment, and preservation of data utility. However,\nnon-experts often defend k-anonymity on the grounds that, in the absence of\nauxiliary information, no known attacks can compromise its protections. In this\nwork, we refute this claim by introducing Combinatorial Refinement Attacks\n(CRA), a new class of privacy attacks targeting k-anonymized datasets produced\nusing local recoding. This is the first method that does not rely on external\nauxiliary information or assumptions about the underlying data distribution.\nCRA leverages the utility-optimizing behavior of local recoding anonymization\nof ARX, which is a widely used open-source software for anonymizing data in\nclinical settings, to formulate a linear program that significantly reduces the\nspace of plausible sensitive values. To validate our findings, we partnered\nwith a network of free community health clinics, an environment where (1)\nauxiliary information is indeed hard to find due to the population they serve\nand (2) open-source k-anonymity solutions are attractive due to regulatory\nobligations and limited resources. Our results on real-world clinical microdata\nreveal that even in the absence of external information, established\nanonymization frameworks do not deliver the promised level of privacy, raising\ncritical privacy concerns.",
    "updated" : "2025-09-03T14:36:06Z",
    "published" : "2025-09-03T14:36:06Z",
    "authors" : [
      {
        "name" : "Somiya Chhillar"
      },
      {
        "name" : "Mary K. Righi"
      },
      {
        "name" : "Rebecca E. Sutter"
      },
      {
        "name" : "Evgenios M. Kornaropoulos"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.03294v1",
    "title" : "A Comprehensive Guide to Differential Privacy: From Theory to User\n  Expectations",
    "summary" : "The increasing availability of personal data has enabled significant advances\nin fields such as machine learning, healthcare, and cybersecurity. However,\nthis data abundance also raises serious privacy concerns, especially in light\nof powerful re-identification attacks and growing legal and ethical demands for\nresponsible data use. Differential privacy (DP) has emerged as a principled,\nmathematically grounded framework for mitigating these risks. This review\nprovides a comprehensive survey of DP, covering its theoretical foundations,\npractical mechanisms, and real-world applications. It explores key algorithmic\ntools and domain-specific challenges - particularly in privacy-preserving\nmachine learning and synthetic data generation. The report also highlights\nusability issues and the need for improved communication and transparency in DP\nsystems. Overall, the goal is to support informed adoption of DP by researchers\nand practitioners navigating the evolving landscape of data privacy.",
    "updated" : "2025-09-03T13:23:10Z",
    "published" : "2025-09-03T13:23:10Z",
    "authors" : [
      {
        "name" : "Napsu Karmitsa"
      },
      {
        "name" : "Antti Airola"
      },
      {
        "name" : "Tapio Pahikkala"
      },
      {
        "name" : "Tinja Pitkämäki"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "68P27, 68T09, 94A60"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.03024v1",
    "title" : "Efficient Privacy-Preserving Recommendation on Sparse Data using Fully\n  Homomorphic Encryption",
    "summary" : "In today's data-driven world, recommendation systems personalize user\nexperiences across industries but rely on sensitive data, raising privacy\nconcerns. Fully homomorphic encryption (FHE) can secure these systems, but a\nsignificant challenge in applying FHE to recommendation systems is efficiently\nhandling the inherently large and sparse user-item rating matrices. FHE\noperations are computationally intensive, and naively processing various sparse\nmatrices in recommendation systems would be prohibitively expensive.\nAdditionally, the communication overhead between parties remains a critical\nconcern in encrypted domains. We propose a novel approach combining Compressed\nSparse Row (CSR) representation with FHE-based matrix factorization that\nefficiently handles matrix sparsity in the encrypted domain while minimizing\ncommunication costs. Our experimental results demonstrate high recommendation\naccuracy with encrypted data while achieving the lowest communication costs,\neffectively preserving user privacy.",
    "updated" : "2025-09-03T05:15:45Z",
    "published" : "2025-09-03T05:15:45Z",
    "authors" : [
      {
        "name" : "Moontaha Nishat Chowdhury"
      },
      {
        "name" : "André Bauer"
      },
      {
        "name" : "Minxuan Zhou"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02856v1",
    "title" : "Managing Correlations in Data and Privacy Demand",
    "summary" : "Previous works in the differential privacy literature that allow users to\nchoose their privacy levels typically operate under the heterogeneous\ndifferential privacy (HDP) framework with the simplifying assumption that user\ndata and privacy levels are not correlated. Firstly, we demonstrate that the\nstandard HDP framework falls short when user data and privacy demands are\nallowed to be correlated. Secondly, to address this shortcoming, we propose an\nalternate framework, Add-remove Heterogeneous Differential Privacy (AHDP), that\njointly accounts for user data and privacy preference. We show that AHDP is\nrobust to possible correlations between data and privacy. Thirdly, we formalize\nthe guarantees of the proposed AHDP framework through an operational hypothesis\ntesting perspective. The hypothesis testing setup may be of independent\ninterest in analyzing other privacy frameworks as well. Fourthly, we show that\nthere exists non-trivial AHDP mechanisms that notably do not require prior\nknowledge of the data-privacy correlations. We propose some such mechanisms and\napply them to core statistical tasks such as mean estimation, frequency\nestimation, and linear regression. The proposed mechanisms are simple to\nimplement with minimal assumptions and modeling requirements, making them\nattractive for real-world use. Finally, we empirically evaluate proposed AHDP\nmechanisms, highlighting their trade-offs using LLM-generated synthetic\ndatasets, which we release for future research.",
    "updated" : "2025-09-02T22:03:13Z",
    "published" : "2025-09-02T22:03:13Z",
    "authors" : [
      {
        "name" : "Syomantak Chaudhuri"
      },
      {
        "name" : "Thomas A. Courtade"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02768v1",
    "title" : "Sequential Change Detection with Differential Privacy",
    "summary" : "Sequential change detection is a fundamental problem in statistics and signal\nprocessing, with the CUSUM procedure widely used to achieve minimax detection\ndelay under a prescribed false-alarm rate when pre- and post-change\ndistributions are fully known. However, releasing CUSUM statistics and the\ncorresponding stopping time directly can compromise individual data privacy. We\ntherefore introduce a differentially private (DP) variant, called DP-CUSUM,\nthat injects calibrated Laplace noise into both the vanilla CUSUM statistics\nand the detection threshold, preserving the recursive simplicity of the\nclassical CUSUM statistics while ensuring per-sample differential privacy. We\nderive closed-form bounds on the average run length to false alarm and on the\nworst-case average detection delay, explicitly characterizing the trade-off\namong privacy level, false-alarm rate, and detection efficiency. Our\ntheoretical results imply that under a weak privacy constraint, our proposed\nDP-CUSUM procedure achieves the same first-order asymptotic optimality as the\nclassical, non-private CUSUM procedure. Numerical simulations are conducted to\ndemonstrate the detection efficiency of our proposed DP-CUSUM under different\nprivacy constraints, and the results are consistent with our theoretical\nfindings.",
    "updated" : "2025-09-02T19:15:47Z",
    "published" : "2025-09-02T19:15:47Z",
    "authors" : [
      {
        "name" : "Liyan Xie"
      },
      {
        "name" : "Ruizhi Zhang"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02411v1",
    "title" : "A Survey: Towards Privacy and Security in Mobile Large Language Models",
    "summary" : "Mobile Large Language Models (LLMs) are revolutionizing diverse fields such\nas healthcare, finance, and education with their ability to perform advanced\nnatural language processing tasks on-the-go. However, the deployment of these\nmodels in mobile and edge environments introduces significant challenges\nrelated to privacy and security due to their resource-intensive nature and the\nsensitivity of the data they process. This survey provides a comprehensive\noverview of privacy and security issues associated with mobile LLMs,\nsystematically categorizing existing solutions such as differential privacy,\nfederated learning, and prompt encryption. Furthermore, we analyze\nvulnerabilities unique to mobile LLMs, including adversarial attacks,\nmembership inference, and side-channel attacks, offering an in-depth comparison\nof their effectiveness and limitations. Despite recent advancements, mobile\nLLMs face unique hurdles in achieving robust security while maintaining\nefficiency in resource-constrained environments. To bridge this gap, we propose\npotential applications, discuss open challenges, and suggest future research\ndirections, paving the way for the development of trustworthy,\nprivacy-compliant, and scalable mobile LLM systems.",
    "updated" : "2025-09-02T15:19:57Z",
    "published" : "2025-09-02T15:19:57Z",
    "authors" : [
      {
        "name" : "Honghui Xu"
      },
      {
        "name" : "Kaiyang Li"
      },
      {
        "name" : "Wei Chen"
      },
      {
        "name" : "Danyang Zheng"
      },
      {
        "name" : "Zhiyuan Li"
      },
      {
        "name" : "Zhipeng Cai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02048v1",
    "title" : "Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization\n  Framework with Curvature-Guided Perturbation",
    "summary" : "Machine learning models require datasets for effective training, but directly\nsharing raw data poses significant privacy risk such as membership inference\nattacks (MIA). To mitigate the risk, privacy-preserving techniques such as data\nperturbation, generalization, and synthetic data generation are commonly\nutilized. However, these methods often degrade data accuracy, specificity, and\ndiversity, limiting the performance of downstream tasks and thus reducing data\nutility. Therefore, striking an optimal balance between privacy preservation\nand data utility remains a critical challenge.\n  To address this issue, we introduce a novel bilevel optimization framework\nfor the publication of private datasets, where the upper-level task focuses on\ndata utility and the lower-level task focuses on data privacy. In the\nupper-level task, a discriminator guides the generation process to ensure that\nperturbed latent variables are mapped to high-quality samples, maintaining\nfidelity for downstream tasks. In the lower-level task, our framework employs\nlocal extrinsic curvature on the data manifold as a quantitative measure of\nindividual vulnerability to MIA, providing a geometric foundation for targeted\nprivacy protection. By perturbing samples toward low-curvature regions, our\nmethod effectively suppresses distinctive feature combinations that are\nvulnerable to MIA. Through alternating optimization of both objectives, we\nachieve a synergistic balance between privacy and utility. Extensive\nexperimental evaluations demonstrate that our method not only enhances\nresistance to MIA in downstream tasks but also surpasses existing methods in\nterms of sample quality and diversity.",
    "updated" : "2025-09-02T07:44:21Z",
    "published" : "2025-09-02T07:44:21Z",
    "authors" : [
      {
        "name" : "Yi Yin"
      },
      {
        "name" : "Guangquan Zhang"
      },
      {
        "name" : "Hua Zuo"
      },
      {
        "name" : "Jie Lu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02004v1",
    "title" : "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
    "summary" : "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
    "updated" : "2025-09-02T06:40:45Z",
    "published" : "2025-09-02T06:40:45Z",
    "authors" : [
      {
        "name" : "Takao Murakami"
      },
      {
        "name" : "Yuichi Sei"
      },
      {
        "name" : "Reo Eriguchi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01716v1",
    "title" : "An LLM-enabled semantic-centric framework to consume privacy policies",
    "summary" : "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites, despite claiming\notherwise, due to the practical difficulty in comprehending them. The mist of\ndata privacy practices forms a major barrier for user-centred Web approaches,\nand for data sharing and reusing in an agentic world. Existing research\nproposed methods for using formal languages and reasoning for verifying the\ncompliance of a specified policy, as a potential cure for ignoring privacy\npolicies. However, a critical gap remains in the creation or acquisition of\nsuch formal policies at scale. We present a semantic-centric approach for using\nstate-of-the-art large language models (LLM), to automatically identify key\ninformation about privacy practices from privacy policies, and construct\n$\\mathit{Pr}^2\\mathit{Graph}$, knowledge graph with grounding from Data Privacy\nVocabulary (DPV) for privacy practices, to support downstream tasks. Along with\nthe pipeline, the $\\mathit{Pr}^2\\mathit{Graph}$ for the top-100 popular\nwebsites is also released as a public resource, by using the pipeline for\nanalysis. We also demonstrate how the $\\mathit{Pr}^2\\mathit{Graph}$ can be used\nto support downstream tasks by constructing formal policy representations such\nas Open Digital Right Language (ODRL) or perennial semantic Data Terms of Use\n(psDToU). To evaluate the technology capability, we enriched the Policy-IE\ndataset by employing legal experts to create custom annotations. We benchmarked\nthe performance of different large language models for our pipeline and\nverified their capabilities. Overall, they shed light on the possibility of\nlarge-scale analysis of online services' privacy practices, as a promising\ndirection to audit the Web and the Internet. We release all datasets and source\ncode as public resources to facilitate reuse and improvement.",
    "updated" : "2025-09-01T18:53:13Z",
    "published" : "2025-09-01T18:53:13Z",
    "authors" : [
      {
        "name" : "Rui Zhao"
      },
      {
        "name" : "Vladyslav Melnychuk"
      },
      {
        "name" : "Jun Zhao"
      },
      {
        "name" : "Jesse Wright"
      },
      {
        "name" : "Nigel Shadbolt"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01527v2",
    "title" : "A Privacy-Preserving Recommender for Filling Web Forms Using a Local\n  Large Language Model",
    "summary" : "Web applications are increasingly used in critical domains such as education,\nfinance, and e-commerce. This highlights the need to ensure their failure-free\nperformance. One effective method for evaluating failure-free performance is\nweb form testing, where defining effective test scenarios is key to a complete\nand accurate evaluation. A core aspect of this process involves filling form\nfields with suitable values to create effective test cases. However, manually\ngenerating these values is time-consuming and prone to errors. To address this,\nvarious tools have been developed to assist testers. With the appearance of\nlarge language models (LLMs), a new generation of tools seeks to handle this\ntask more intelligently. Although many LLM-based tools have been introduced, as\nthese models typically rely on cloud infrastructure, their use in testing\nconfidential web forms raises concerns about unintended data leakage and\nbreaches of confidentiality. This paper introduces a privacy-preserving\nrecommender that operates locally using a large language model. The tool\nassists testers in web form testing by suggesting effective field values. This\ntool analyzes the HTML structure of forms, detects input types, and extracts\nconstraints based on each field's type and contextual content, guiding proper\nfield filling.",
    "updated" : "2025-09-03T15:43:01Z",
    "published" : "2025-09-01T15:02:00Z",
    "authors" : [
      {
        "name" : "Amirreza Nayyeri"
      },
      {
        "name" : "Abbas Rasoolzadegan"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01470v1",
    "title" : "Privacy-preserving authentication for military 5G networks",
    "summary" : "As 5G networks gain traction in defense applications, ensuring the privacy\nand integrity of the Authentication and Key Agreement (AKA) protocol is\ncritical. While 5G AKA improves upon previous generations by concealing\nsubscriber identities, it remains vulnerable to replay-based synchronization\nand linkability threats under realistic adversary models. This paper provides a\nunified analysis of the standardized 5G AKA flow, identifying several\nvulnerabilities and highlighting how each exploits protocol behavior to\ncompromise user privacy. To address these risks, we present five lightweight\nmitigation strategies. We demonstrate through prototype implementation and\ntesting that these enhancements strengthen resilience against linkability\nattacks with minimal computational and signaling overhead. Among the solutions\nstudied, those introducing a UE-generated nonce emerge as the most promising,\neffectively neutralizing the identified tracking and correlation attacks with\nnegligible additional overhead. Integrating this extension as an optional\nfeature to the standard 5G AKA protocol offers a backward-compatible,\nlow-overhead path toward a more privacy-preserving authentication framework for\nboth commercial and military 5G deployments.",
    "updated" : "2025-09-01T13:38:11Z",
    "published" : "2025-09-01T13:38:11Z",
    "authors" : [
      {
        "name" : "I. D. Lutz"
      },
      {
        "name" : "A. M. Hill"
      },
      {
        "name" : "M. C. Valenti"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01354v1",
    "title" : "DPF-CM: A Data Processing Framework with Privacy-Preserving Vector\n  Databases for Chinese Medical LLMs Training and Deployment",
    "summary" : "Current open-source training pipelines for Chinese medical language models\npredominantly emphasize optimizing training methodologies to enhance the\nperformance of large language models (LLMs), yet lack comprehensive exploration\ninto training data processing. To address this gap, we propose DPF-CM, a\nholistic Data Processing Framework for Chinese Medical LLMs training and\ndeployment. DPF-CM comprises two core modules. The first module is a data\nprocessing pipeline tailored for model training. Beyond standard data\nprocessing operations, we (1) introduce a chained examples context-learning\nstrategy to generate question-oriented instructions to mitigate the lack of\ninstruction content, and (2) implement an ensemble-based filtering mechanism\nfor preference data curation that averages multiple reward models to suppress\nnoisy samples. The second module focuses on privacy preservation during model\ndeployment. To prevent privacy risks from the inadvertent exposure of training\ndata, we propose a Privacy Preserving Vector Database (PPVD) approach, which\ninvolves model memory search, high-risk database construction, secure database\nconstruction, and match-and-replace, four key stages to minimize privacy\nleakage during inference collectively. Experimental results show that DPF-CM\nsignificantly improves model accuracy, enabling our trained Chinese medical LLM\nto achieve state-of-the-art performance among open-source counterparts.\nMoreover, the framework reduces training data privacy leakage by 27%.",
    "updated" : "2025-09-01T10:49:32Z",
    "published" : "2025-09-01T10:49:32Z",
    "authors" : [
      {
        "name" : "Wei Huang"
      },
      {
        "name" : "Anda Cheng"
      },
      {
        "name" : "Zhao Zhang"
      },
      {
        "name" : "Yinggui Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01088v1",
    "title" : "Privacy-Preserving Reasoning with Knowledge-Distilled Parametric\n  Retrieval Augmented Generation",
    "summary" : "The current RAG system requires uploading plaintext documents to the cloud,\nrisking private data leakage. Parametric RAG (PRAG) addresses this by encoding\ndocuments as LoRA within LLMs, enabling reasoning without exposing raw content.\nHowever, it still faces two issues: (1) PRAG demands synthesizing QA pairs and\nfine-tuning LLM for each individual document to create its corresponding LoRA,\nleading to unacceptable inference latency. (2) The performance of PRAG relies\nsolely on synthetic QA data, lacking internal alignment with standard RAG,\nresulting in poor generalization on out-of-distribution(OOD) inputs. Therefore,\nachieving high-efficiency parameterization while maintaining RAG-level\nperformance remains a critical challenge for privacy-preserving reasoning. In\nthis paper, we propose DistilledPRAG, a generalizable knowledge-distilled\nparametric RAG model aligned with standard RAG in document structure and\nparameter activation. We first synthesize QA pairs from single and\nmulti-documents to enhance cross-document reasoning. Then, we mask the\nplaintext documents with a special token and translate them to LoRA via a\nparameter generator, maintaining the standard RAG document structure. Finally,\nguided by synthetic QA data, we train the parameter generator to match standard\nRAG's hidden states and output logits, enabling RAG-style reasoning without\noriginal documents. Experiments on four QA datasets show that DistilledPRAG\noutperforms baselines in accuracy and generalizes well on OOD data.",
    "updated" : "2025-09-01T03:23:57Z",
    "published" : "2025-09-01T03:23:57Z",
    "authors" : [
      {
        "name" : "Jinwen Chen"
      },
      {
        "name" : "Hainan Zhang"
      },
      {
        "name" : "Liang Pang"
      },
      {
        "name" : "Yongxin Tong"
      },
      {
        "name" : "Haibo Zhou"
      },
      {
        "name" : "Yuan Zhan"
      },
      {
        "name" : "Wei Lin"
      },
      {
        "name" : "Zhiming Zheng"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04358v1",
    "title" : "Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the\n  Roles of Information Transparency, User Control, and Proactivity",
    "summary" : "Social robots are increasingly recognized as valuable supporters in the field\nof well-being coaching. They can function as independent coaches or provide\nsupport alongside human coaches, and healthcare professionals. In coaching\ninteractions, these robots often handle sensitive information shared by users,\nmaking privacy a relevant issue. Despite this, little is known about the\nfactors that shape users' privacy perceptions. This research aims to examine\nthree key factors systematically: (1) the transparency about information usage,\n(2) the level of specific user control over how the robot uses their\ninformation, and (3) the robot's behavioral approach - whether it acts\nproactively or only responds on demand. Our results from an online study (N =\n200) show that even when users grant the robot general access to personal data,\nthey additionally expect the ability to explicitly control how that information\nis interpreted and shared during sessions. Experimental conditions that\nprovided such control received significantly higher ratings for perceived\nprivacy appropriateness and trust. Compared to user control, the effects of\ntransparency and proactivity on privacy appropriateness perception were low,\nand we found no significant impact. The results suggest that merely informing\nusers or proactive sharing is insufficient without accompanying user control.\nThese insights underscore the need for further research on mechanisms that\nallow users to manage robots' information processing and sharing, especially\nwhen social robots take on more proactive roles alongside humans.",
    "updated" : "2025-09-04T16:19:24Z",
    "published" : "2025-09-04T16:19:24Z",
    "authors" : [
      {
        "name" : "Atikkhan Faridkhan Nilgar"
      },
      {
        "name" : "Manuel Dietrich"
      },
      {
        "name" : "Kristof Van Laerhoven"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04290v1",
    "title" : "An Interactive Framework for Finding the Optimal Trade-off in\n  Differential Privacy",
    "summary" : "Differential privacy (DP) is the standard for privacy-preserving analysis,\nand introduces a fundamental trade-off between privacy guarantees and model\nperformance. Selecting the optimal balance is a critical challenge that can be\nframed as a multi-objective optimization (MOO) problem where one first\ndiscovers the set of optimal trade-offs (the Pareto front) and then learns a\ndecision-maker's preference over them. While a rich body of work on interactive\nMOO exists, the standard approach -- modeling the objective functions with\ngeneric surrogates and learning preferences from simple pairwise feedback -- is\ninefficient for DP because it fails to leverage the problem's unique structure:\na point on the Pareto front can be generated directly by maximizing accuracy\nfor a fixed privacy level. Motivated by this property, we first derive the\nshape of the trade-off theoretically, which allows us to model the Pareto front\ndirectly and efficiently. To address inefficiency in preference learning, we\nreplace pairwise comparisons with a more informative interaction. In\nparticular, we present the user with hypothetical trade-off curves and ask them\nto pick their preferred trade-off. Our experiments on differentially private\nlogistic regression and deep transfer learning across six real-world datasets\nshow that our method converges to the optimal privacy-accuracy trade-off with\nsignificantly less computational cost and user interaction than baselines.",
    "updated" : "2025-09-04T15:02:10Z",
    "published" : "2025-09-04T15:02:10Z",
    "authors" : [
      {
        "name" : "Yaohong Yang"
      },
      {
        "name" : "Aki Rehn"
      },
      {
        "name" : "Sammie Katt"
      },
      {
        "name" : "Antti Honkela"
      },
      {
        "name" : "Samuel Kaski"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04232v1",
    "title" : "Rethinking Layer-wise Gaussian Noise Injection: Bridging Implicit\n  Objectives and Privacy Budget Allocation",
    "summary" : "Layer-wise Gaussian mechanisms (LGM) enhance flexibility in differentially\nprivate deep learning by injecting noise into partitioned gradient vectors.\nHowever, existing methods often rely on heuristic noise allocation strategies,\nlacking a rigorous understanding of their theoretical grounding in connecting\nnoise allocation to formal privacy-utility tradeoffs. In this paper, we present\na unified analytical framework that systematically connects layer-wise noise\ninjection strategies with their implicit optimization objectives and associated\nprivacy budget allocations. Our analysis reveals that several existing\napproaches optimize ill-posed objectives -- either ignoring inter-layer\nsignal-to-noise ratio (SNR) consistency or leading to inefficient use of the\nprivacy budget. In response, we propose a SNR-Consistent noise allocation\nstrategy that unifies both aspects, yielding a noise allocation scheme that\nachieves better signal preservation and more efficient privacy budget\nutilization. Extensive experiments in both centralized and federated learning\nsettings demonstrate that our method consistently outperforms existing\nallocation strategies, achieving better privacy-utility tradeoffs. Our\nframework not only offers diagnostic insights into prior methods but also\nprovides theoretical guidance for designing adaptive and effective noise\ninjection schemes in deep models.",
    "updated" : "2025-09-04T14:09:46Z",
    "published" : "2025-09-04T14:09:46Z",
    "authors" : [
      {
        "name" : "Qifeng Tan"
      },
      {
        "name" : "Shusen Yang"
      },
      {
        "name" : "Xuebin Ren"
      },
      {
        "name" : "Yikai Zhang"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04169v1",
    "title" : "Privacy Risks in Time Series Forecasting: User- and Record-Level\n  Membership Inference",
    "summary" : "Membership inference attacks (MIAs) aim to determine whether specific data\nwere used to train a model. While extensively studied on classification models,\ntheir impact on time series forecasting remains largely unexplored. We address\nthis gap by introducing two new attacks: (i) an adaptation of multivariate\nLiRA, a state-of-the-art MIA originally developed for classification models, to\nthe time-series forecasting setting, and (ii) a novel end-to-end learning\napproach called Deep Time Series (DTS) attack. We benchmark these methods\nagainst adapted versions of other leading attacks from the classification\nsetting.\n  We evaluate all attacks in realistic settings on the TUH-EEG and ELD\ndatasets, targeting two strong forecasting architectures, LSTM and the\nstate-of-the-art N-HiTS, under both record- and user-level threat models. Our\nresults show that forecasting models are vulnerable, with user-level attacks\noften achieving perfect detection. The proposed methods achieve the strongest\nperformance in several settings, establishing new baselines for privacy risk\nassessment in time series forecasting. Furthermore, vulnerability increases\nwith longer prediction horizons and smaller training populations, echoing\ntrends observed in large language models.",
    "updated" : "2025-09-04T12:43:45Z",
    "published" : "2025-09-04T12:43:45Z",
    "authors" : [
      {
        "name" : "Nicolas Johansson"
      },
      {
        "name" : "Tobias Olsson"
      },
      {
        "name" : "Daniel Nilsson"
      },
      {
        "name" : "Johan Östman"
      },
      {
        "name" : "Fazeleh Hoseini"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  }
]