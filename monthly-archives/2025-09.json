[
  {
    "id" : "http://arxiv.org/abs/2509.03350v1",
    "title" : "Exposing Privacy Risks in Anonymizing Clinical Data: Combinatorial\n  Refinement Attacks on k-Anonymity Without Auxiliary Information",
    "summary" : "Despite longstanding criticism from the privacy community, k-anonymity\nremains a widely used standard for data anonymization, mainly due to its\nsimplicity, regulatory alignment, and preservation of data utility. However,\nnon-experts often defend k-anonymity on the grounds that, in the absence of\nauxiliary information, no known attacks can compromise its protections. In this\nwork, we refute this claim by introducing Combinatorial Refinement Attacks\n(CRA), a new class of privacy attacks targeting k-anonymized datasets produced\nusing local recoding. This is the first method that does not rely on external\nauxiliary information or assumptions about the underlying data distribution.\nCRA leverages the utility-optimizing behavior of local recoding anonymization\nof ARX, which is a widely used open-source software for anonymizing data in\nclinical settings, to formulate a linear program that significantly reduces the\nspace of plausible sensitive values. To validate our findings, we partnered\nwith a network of free community health clinics, an environment where (1)\nauxiliary information is indeed hard to find due to the population they serve\nand (2) open-source k-anonymity solutions are attractive due to regulatory\nobligations and limited resources. Our results on real-world clinical microdata\nreveal that even in the absence of external information, established\nanonymization frameworks do not deliver the promised level of privacy, raising\ncritical privacy concerns.",
    "updated" : "2025-09-03T14:36:06Z",
    "published" : "2025-09-03T14:36:06Z",
    "authors" : [
      {
        "name" : "Somiya Chhillar"
      },
      {
        "name" : "Mary K. Righi"
      },
      {
        "name" : "Rebecca E. Sutter"
      },
      {
        "name" : "Evgenios M. Kornaropoulos"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.03294v1",
    "title" : "A Comprehensive Guide to Differential Privacy: From Theory to User\n  Expectations",
    "summary" : "The increasing availability of personal data has enabled significant advances\nin fields such as machine learning, healthcare, and cybersecurity. However,\nthis data abundance also raises serious privacy concerns, especially in light\nof powerful re-identification attacks and growing legal and ethical demands for\nresponsible data use. Differential privacy (DP) has emerged as a principled,\nmathematically grounded framework for mitigating these risks. This review\nprovides a comprehensive survey of DP, covering its theoretical foundations,\npractical mechanisms, and real-world applications. It explores key algorithmic\ntools and domain-specific challenges - particularly in privacy-preserving\nmachine learning and synthetic data generation. The report also highlights\nusability issues and the need for improved communication and transparency in DP\nsystems. Overall, the goal is to support informed adoption of DP by researchers\nand practitioners navigating the evolving landscape of data privacy.",
    "updated" : "2025-09-03T13:23:10Z",
    "published" : "2025-09-03T13:23:10Z",
    "authors" : [
      {
        "name" : "Napsu Karmitsa"
      },
      {
        "name" : "Antti Airola"
      },
      {
        "name" : "Tapio Pahikkala"
      },
      {
        "name" : "Tinja Pitkämäki"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "68P27, 68T09, 94A60"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.03024v1",
    "title" : "Efficient Privacy-Preserving Recommendation on Sparse Data using Fully\n  Homomorphic Encryption",
    "summary" : "In today's data-driven world, recommendation systems personalize user\nexperiences across industries but rely on sensitive data, raising privacy\nconcerns. Fully homomorphic encryption (FHE) can secure these systems, but a\nsignificant challenge in applying FHE to recommendation systems is efficiently\nhandling the inherently large and sparse user-item rating matrices. FHE\noperations are computationally intensive, and naively processing various sparse\nmatrices in recommendation systems would be prohibitively expensive.\nAdditionally, the communication overhead between parties remains a critical\nconcern in encrypted domains. We propose a novel approach combining Compressed\nSparse Row (CSR) representation with FHE-based matrix factorization that\nefficiently handles matrix sparsity in the encrypted domain while minimizing\ncommunication costs. Our experimental results demonstrate high recommendation\naccuracy with encrypted data while achieving the lowest communication costs,\neffectively preserving user privacy.",
    "updated" : "2025-09-03T05:15:45Z",
    "published" : "2025-09-03T05:15:45Z",
    "authors" : [
      {
        "name" : "Moontaha Nishat Chowdhury"
      },
      {
        "name" : "André Bauer"
      },
      {
        "name" : "Minxuan Zhou"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02856v1",
    "title" : "Managing Correlations in Data and Privacy Demand",
    "summary" : "Previous works in the differential privacy literature that allow users to\nchoose their privacy levels typically operate under the heterogeneous\ndifferential privacy (HDP) framework with the simplifying assumption that user\ndata and privacy levels are not correlated. Firstly, we demonstrate that the\nstandard HDP framework falls short when user data and privacy demands are\nallowed to be correlated. Secondly, to address this shortcoming, we propose an\nalternate framework, Add-remove Heterogeneous Differential Privacy (AHDP), that\njointly accounts for user data and privacy preference. We show that AHDP is\nrobust to possible correlations between data and privacy. Thirdly, we formalize\nthe guarantees of the proposed AHDP framework through an operational hypothesis\ntesting perspective. The hypothesis testing setup may be of independent\ninterest in analyzing other privacy frameworks as well. Fourthly, we show that\nthere exists non-trivial AHDP mechanisms that notably do not require prior\nknowledge of the data-privacy correlations. We propose some such mechanisms and\napply them to core statistical tasks such as mean estimation, frequency\nestimation, and linear regression. The proposed mechanisms are simple to\nimplement with minimal assumptions and modeling requirements, making them\nattractive for real-world use. Finally, we empirically evaluate proposed AHDP\nmechanisms, highlighting their trade-offs using LLM-generated synthetic\ndatasets, which we release for future research.",
    "updated" : "2025-09-02T22:03:13Z",
    "published" : "2025-09-02T22:03:13Z",
    "authors" : [
      {
        "name" : "Syomantak Chaudhuri"
      },
      {
        "name" : "Thomas A. Courtade"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02768v1",
    "title" : "Sequential Change Detection with Differential Privacy",
    "summary" : "Sequential change detection is a fundamental problem in statistics and signal\nprocessing, with the CUSUM procedure widely used to achieve minimax detection\ndelay under a prescribed false-alarm rate when pre- and post-change\ndistributions are fully known. However, releasing CUSUM statistics and the\ncorresponding stopping time directly can compromise individual data privacy. We\ntherefore introduce a differentially private (DP) variant, called DP-CUSUM,\nthat injects calibrated Laplace noise into both the vanilla CUSUM statistics\nand the detection threshold, preserving the recursive simplicity of the\nclassical CUSUM statistics while ensuring per-sample differential privacy. We\nderive closed-form bounds on the average run length to false alarm and on the\nworst-case average detection delay, explicitly characterizing the trade-off\namong privacy level, false-alarm rate, and detection efficiency. Our\ntheoretical results imply that under a weak privacy constraint, our proposed\nDP-CUSUM procedure achieves the same first-order asymptotic optimality as the\nclassical, non-private CUSUM procedure. Numerical simulations are conducted to\ndemonstrate the detection efficiency of our proposed DP-CUSUM under different\nprivacy constraints, and the results are consistent with our theoretical\nfindings.",
    "updated" : "2025-09-02T19:15:47Z",
    "published" : "2025-09-02T19:15:47Z",
    "authors" : [
      {
        "name" : "Liyan Xie"
      },
      {
        "name" : "Ruizhi Zhang"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02411v1",
    "title" : "A Survey: Towards Privacy and Security in Mobile Large Language Models",
    "summary" : "Mobile Large Language Models (LLMs) are revolutionizing diverse fields such\nas healthcare, finance, and education with their ability to perform advanced\nnatural language processing tasks on-the-go. However, the deployment of these\nmodels in mobile and edge environments introduces significant challenges\nrelated to privacy and security due to their resource-intensive nature and the\nsensitivity of the data they process. This survey provides a comprehensive\noverview of privacy and security issues associated with mobile LLMs,\nsystematically categorizing existing solutions such as differential privacy,\nfederated learning, and prompt encryption. Furthermore, we analyze\nvulnerabilities unique to mobile LLMs, including adversarial attacks,\nmembership inference, and side-channel attacks, offering an in-depth comparison\nof their effectiveness and limitations. Despite recent advancements, mobile\nLLMs face unique hurdles in achieving robust security while maintaining\nefficiency in resource-constrained environments. To bridge this gap, we propose\npotential applications, discuss open challenges, and suggest future research\ndirections, paving the way for the development of trustworthy,\nprivacy-compliant, and scalable mobile LLM systems.",
    "updated" : "2025-09-02T15:19:57Z",
    "published" : "2025-09-02T15:19:57Z",
    "authors" : [
      {
        "name" : "Honghui Xu"
      },
      {
        "name" : "Kaiyang Li"
      },
      {
        "name" : "Wei Chen"
      },
      {
        "name" : "Danyang Zheng"
      },
      {
        "name" : "Zhiyuan Li"
      },
      {
        "name" : "Zhipeng Cai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02048v1",
    "title" : "Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization\n  Framework with Curvature-Guided Perturbation",
    "summary" : "Machine learning models require datasets for effective training, but directly\nsharing raw data poses significant privacy risk such as membership inference\nattacks (MIA). To mitigate the risk, privacy-preserving techniques such as data\nperturbation, generalization, and synthetic data generation are commonly\nutilized. However, these methods often degrade data accuracy, specificity, and\ndiversity, limiting the performance of downstream tasks and thus reducing data\nutility. Therefore, striking an optimal balance between privacy preservation\nand data utility remains a critical challenge.\n  To address this issue, we introduce a novel bilevel optimization framework\nfor the publication of private datasets, where the upper-level task focuses on\ndata utility and the lower-level task focuses on data privacy. In the\nupper-level task, a discriminator guides the generation process to ensure that\nperturbed latent variables are mapped to high-quality samples, maintaining\nfidelity for downstream tasks. In the lower-level task, our framework employs\nlocal extrinsic curvature on the data manifold as a quantitative measure of\nindividual vulnerability to MIA, providing a geometric foundation for targeted\nprivacy protection. By perturbing samples toward low-curvature regions, our\nmethod effectively suppresses distinctive feature combinations that are\nvulnerable to MIA. Through alternating optimization of both objectives, we\nachieve a synergistic balance between privacy and utility. Extensive\nexperimental evaluations demonstrate that our method not only enhances\nresistance to MIA in downstream tasks but also surpasses existing methods in\nterms of sample quality and diversity.",
    "updated" : "2025-09-02T07:44:21Z",
    "published" : "2025-09-02T07:44:21Z",
    "authors" : [
      {
        "name" : "Yi Yin"
      },
      {
        "name" : "Guangquan Zhang"
      },
      {
        "name" : "Hua Zuo"
      },
      {
        "name" : "Jie Lu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.02004v1",
    "title" : "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
    "summary" : "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
    "updated" : "2025-09-02T06:40:45Z",
    "published" : "2025-09-02T06:40:45Z",
    "authors" : [
      {
        "name" : "Takao Murakami"
      },
      {
        "name" : "Yuichi Sei"
      },
      {
        "name" : "Reo Eriguchi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01716v1",
    "title" : "An LLM-enabled semantic-centric framework to consume privacy policies",
    "summary" : "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites, despite claiming\notherwise, due to the practical difficulty in comprehending them. The mist of\ndata privacy practices forms a major barrier for user-centred Web approaches,\nand for data sharing and reusing in an agentic world. Existing research\nproposed methods for using formal languages and reasoning for verifying the\ncompliance of a specified policy, as a potential cure for ignoring privacy\npolicies. However, a critical gap remains in the creation or acquisition of\nsuch formal policies at scale. We present a semantic-centric approach for using\nstate-of-the-art large language models (LLM), to automatically identify key\ninformation about privacy practices from privacy policies, and construct\n$\\mathit{Pr}^2\\mathit{Graph}$, knowledge graph with grounding from Data Privacy\nVocabulary (DPV) for privacy practices, to support downstream tasks. Along with\nthe pipeline, the $\\mathit{Pr}^2\\mathit{Graph}$ for the top-100 popular\nwebsites is also released as a public resource, by using the pipeline for\nanalysis. We also demonstrate how the $\\mathit{Pr}^2\\mathit{Graph}$ can be used\nto support downstream tasks by constructing formal policy representations such\nas Open Digital Right Language (ODRL) or perennial semantic Data Terms of Use\n(psDToU). To evaluate the technology capability, we enriched the Policy-IE\ndataset by employing legal experts to create custom annotations. We benchmarked\nthe performance of different large language models for our pipeline and\nverified their capabilities. Overall, they shed light on the possibility of\nlarge-scale analysis of online services' privacy practices, as a promising\ndirection to audit the Web and the Internet. We release all datasets and source\ncode as public resources to facilitate reuse and improvement.",
    "updated" : "2025-09-01T18:53:13Z",
    "published" : "2025-09-01T18:53:13Z",
    "authors" : [
      {
        "name" : "Rui Zhao"
      },
      {
        "name" : "Vladyslav Melnychuk"
      },
      {
        "name" : "Jun Zhao"
      },
      {
        "name" : "Jesse Wright"
      },
      {
        "name" : "Nigel Shadbolt"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01527v2",
    "title" : "A Privacy-Preserving Recommender for Filling Web Forms Using a Local\n  Large Language Model",
    "summary" : "Web applications are increasingly used in critical domains such as education,\nfinance, and e-commerce. This highlights the need to ensure their failure-free\nperformance. One effective method for evaluating failure-free performance is\nweb form testing, where defining effective test scenarios is key to a complete\nand accurate evaluation. A core aspect of this process involves filling form\nfields with suitable values to create effective test cases. However, manually\ngenerating these values is time-consuming and prone to errors. To address this,\nvarious tools have been developed to assist testers. With the appearance of\nlarge language models (LLMs), a new generation of tools seeks to handle this\ntask more intelligently. Although many LLM-based tools have been introduced, as\nthese models typically rely on cloud infrastructure, their use in testing\nconfidential web forms raises concerns about unintended data leakage and\nbreaches of confidentiality. This paper introduces a privacy-preserving\nrecommender that operates locally using a large language model. The tool\nassists testers in web form testing by suggesting effective field values. This\ntool analyzes the HTML structure of forms, detects input types, and extracts\nconstraints based on each field's type and contextual content, guiding proper\nfield filling.",
    "updated" : "2025-09-03T15:43:01Z",
    "published" : "2025-09-01T15:02:00Z",
    "authors" : [
      {
        "name" : "Amirreza Nayyeri"
      },
      {
        "name" : "Abbas Rasoolzadegan"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01470v1",
    "title" : "Privacy-preserving authentication for military 5G networks",
    "summary" : "As 5G networks gain traction in defense applications, ensuring the privacy\nand integrity of the Authentication and Key Agreement (AKA) protocol is\ncritical. While 5G AKA improves upon previous generations by concealing\nsubscriber identities, it remains vulnerable to replay-based synchronization\nand linkability threats under realistic adversary models. This paper provides a\nunified analysis of the standardized 5G AKA flow, identifying several\nvulnerabilities and highlighting how each exploits protocol behavior to\ncompromise user privacy. To address these risks, we present five lightweight\nmitigation strategies. We demonstrate through prototype implementation and\ntesting that these enhancements strengthen resilience against linkability\nattacks with minimal computational and signaling overhead. Among the solutions\nstudied, those introducing a UE-generated nonce emerge as the most promising,\neffectively neutralizing the identified tracking and correlation attacks with\nnegligible additional overhead. Integrating this extension as an optional\nfeature to the standard 5G AKA protocol offers a backward-compatible,\nlow-overhead path toward a more privacy-preserving authentication framework for\nboth commercial and military 5G deployments.",
    "updated" : "2025-09-01T13:38:11Z",
    "published" : "2025-09-01T13:38:11Z",
    "authors" : [
      {
        "name" : "I. D. Lutz"
      },
      {
        "name" : "A. M. Hill"
      },
      {
        "name" : "M. C. Valenti"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01354v1",
    "title" : "DPF-CM: A Data Processing Framework with Privacy-Preserving Vector\n  Databases for Chinese Medical LLMs Training and Deployment",
    "summary" : "Current open-source training pipelines for Chinese medical language models\npredominantly emphasize optimizing training methodologies to enhance the\nperformance of large language models (LLMs), yet lack comprehensive exploration\ninto training data processing. To address this gap, we propose DPF-CM, a\nholistic Data Processing Framework for Chinese Medical LLMs training and\ndeployment. DPF-CM comprises two core modules. The first module is a data\nprocessing pipeline tailored for model training. Beyond standard data\nprocessing operations, we (1) introduce a chained examples context-learning\nstrategy to generate question-oriented instructions to mitigate the lack of\ninstruction content, and (2) implement an ensemble-based filtering mechanism\nfor preference data curation that averages multiple reward models to suppress\nnoisy samples. The second module focuses on privacy preservation during model\ndeployment. To prevent privacy risks from the inadvertent exposure of training\ndata, we propose a Privacy Preserving Vector Database (PPVD) approach, which\ninvolves model memory search, high-risk database construction, secure database\nconstruction, and match-and-replace, four key stages to minimize privacy\nleakage during inference collectively. Experimental results show that DPF-CM\nsignificantly improves model accuracy, enabling our trained Chinese medical LLM\nto achieve state-of-the-art performance among open-source counterparts.\nMoreover, the framework reduces training data privacy leakage by 27%.",
    "updated" : "2025-09-01T10:49:32Z",
    "published" : "2025-09-01T10:49:32Z",
    "authors" : [
      {
        "name" : "Wei Huang"
      },
      {
        "name" : "Anda Cheng"
      },
      {
        "name" : "Zhao Zhang"
      },
      {
        "name" : "Yinggui Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.01088v1",
    "title" : "Privacy-Preserving Reasoning with Knowledge-Distilled Parametric\n  Retrieval Augmented Generation",
    "summary" : "The current RAG system requires uploading plaintext documents to the cloud,\nrisking private data leakage. Parametric RAG (PRAG) addresses this by encoding\ndocuments as LoRA within LLMs, enabling reasoning without exposing raw content.\nHowever, it still faces two issues: (1) PRAG demands synthesizing QA pairs and\nfine-tuning LLM for each individual document to create its corresponding LoRA,\nleading to unacceptable inference latency. (2) The performance of PRAG relies\nsolely on synthetic QA data, lacking internal alignment with standard RAG,\nresulting in poor generalization on out-of-distribution(OOD) inputs. Therefore,\nachieving high-efficiency parameterization while maintaining RAG-level\nperformance remains a critical challenge for privacy-preserving reasoning. In\nthis paper, we propose DistilledPRAG, a generalizable knowledge-distilled\nparametric RAG model aligned with standard RAG in document structure and\nparameter activation. We first synthesize QA pairs from single and\nmulti-documents to enhance cross-document reasoning. Then, we mask the\nplaintext documents with a special token and translate them to LoRA via a\nparameter generator, maintaining the standard RAG document structure. Finally,\nguided by synthetic QA data, we train the parameter generator to match standard\nRAG's hidden states and output logits, enabling RAG-style reasoning without\noriginal documents. Experiments on four QA datasets show that DistilledPRAG\noutperforms baselines in accuracy and generalizes well on OOD data.",
    "updated" : "2025-09-01T03:23:57Z",
    "published" : "2025-09-01T03:23:57Z",
    "authors" : [
      {
        "name" : "Jinwen Chen"
      },
      {
        "name" : "Hainan Zhang"
      },
      {
        "name" : "Liang Pang"
      },
      {
        "name" : "Yongxin Tong"
      },
      {
        "name" : "Haibo Zhou"
      },
      {
        "name" : "Yuan Zhan"
      },
      {
        "name" : "Wei Lin"
      },
      {
        "name" : "Zhiming Zheng"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04358v1",
    "title" : "Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the\n  Roles of Information Transparency, User Control, and Proactivity",
    "summary" : "Social robots are increasingly recognized as valuable supporters in the field\nof well-being coaching. They can function as independent coaches or provide\nsupport alongside human coaches, and healthcare professionals. In coaching\ninteractions, these robots often handle sensitive information shared by users,\nmaking privacy a relevant issue. Despite this, little is known about the\nfactors that shape users' privacy perceptions. This research aims to examine\nthree key factors systematically: (1) the transparency about information usage,\n(2) the level of specific user control over how the robot uses their\ninformation, and (3) the robot's behavioral approach - whether it acts\nproactively or only responds on demand. Our results from an online study (N =\n200) show that even when users grant the robot general access to personal data,\nthey additionally expect the ability to explicitly control how that information\nis interpreted and shared during sessions. Experimental conditions that\nprovided such control received significantly higher ratings for perceived\nprivacy appropriateness and trust. Compared to user control, the effects of\ntransparency and proactivity on privacy appropriateness perception were low,\nand we found no significant impact. The results suggest that merely informing\nusers or proactive sharing is insufficient without accompanying user control.\nThese insights underscore the need for further research on mechanisms that\nallow users to manage robots' information processing and sharing, especially\nwhen social robots take on more proactive roles alongside humans.",
    "updated" : "2025-09-04T16:19:24Z",
    "published" : "2025-09-04T16:19:24Z",
    "authors" : [
      {
        "name" : "Atikkhan Faridkhan Nilgar"
      },
      {
        "name" : "Manuel Dietrich"
      },
      {
        "name" : "Kristof Van Laerhoven"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04290v1",
    "title" : "An Interactive Framework for Finding the Optimal Trade-off in\n  Differential Privacy",
    "summary" : "Differential privacy (DP) is the standard for privacy-preserving analysis,\nand introduces a fundamental trade-off between privacy guarantees and model\nperformance. Selecting the optimal balance is a critical challenge that can be\nframed as a multi-objective optimization (MOO) problem where one first\ndiscovers the set of optimal trade-offs (the Pareto front) and then learns a\ndecision-maker's preference over them. While a rich body of work on interactive\nMOO exists, the standard approach -- modeling the objective functions with\ngeneric surrogates and learning preferences from simple pairwise feedback -- is\ninefficient for DP because it fails to leverage the problem's unique structure:\na point on the Pareto front can be generated directly by maximizing accuracy\nfor a fixed privacy level. Motivated by this property, we first derive the\nshape of the trade-off theoretically, which allows us to model the Pareto front\ndirectly and efficiently. To address inefficiency in preference learning, we\nreplace pairwise comparisons with a more informative interaction. In\nparticular, we present the user with hypothetical trade-off curves and ask them\nto pick their preferred trade-off. Our experiments on differentially private\nlogistic regression and deep transfer learning across six real-world datasets\nshow that our method converges to the optimal privacy-accuracy trade-off with\nsignificantly less computational cost and user interaction than baselines.",
    "updated" : "2025-09-04T15:02:10Z",
    "published" : "2025-09-04T15:02:10Z",
    "authors" : [
      {
        "name" : "Yaohong Yang"
      },
      {
        "name" : "Aki Rehn"
      },
      {
        "name" : "Sammie Katt"
      },
      {
        "name" : "Antti Honkela"
      },
      {
        "name" : "Samuel Kaski"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04232v1",
    "title" : "Rethinking Layer-wise Gaussian Noise Injection: Bridging Implicit\n  Objectives and Privacy Budget Allocation",
    "summary" : "Layer-wise Gaussian mechanisms (LGM) enhance flexibility in differentially\nprivate deep learning by injecting noise into partitioned gradient vectors.\nHowever, existing methods often rely on heuristic noise allocation strategies,\nlacking a rigorous understanding of their theoretical grounding in connecting\nnoise allocation to formal privacy-utility tradeoffs. In this paper, we present\na unified analytical framework that systematically connects layer-wise noise\ninjection strategies with their implicit optimization objectives and associated\nprivacy budget allocations. Our analysis reveals that several existing\napproaches optimize ill-posed objectives -- either ignoring inter-layer\nsignal-to-noise ratio (SNR) consistency or leading to inefficient use of the\nprivacy budget. In response, we propose a SNR-Consistent noise allocation\nstrategy that unifies both aspects, yielding a noise allocation scheme that\nachieves better signal preservation and more efficient privacy budget\nutilization. Extensive experiments in both centralized and federated learning\nsettings demonstrate that our method consistently outperforms existing\nallocation strategies, achieving better privacy-utility tradeoffs. Our\nframework not only offers diagnostic insights into prior methods but also\nprovides theoretical guidance for designing adaptive and effective noise\ninjection schemes in deep models.",
    "updated" : "2025-09-04T14:09:46Z",
    "published" : "2025-09-04T14:09:46Z",
    "authors" : [
      {
        "name" : "Qifeng Tan"
      },
      {
        "name" : "Shusen Yang"
      },
      {
        "name" : "Xuebin Ren"
      },
      {
        "name" : "Yikai Zhang"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04169v1",
    "title" : "Privacy Risks in Time Series Forecasting: User- and Record-Level\n  Membership Inference",
    "summary" : "Membership inference attacks (MIAs) aim to determine whether specific data\nwere used to train a model. While extensively studied on classification models,\ntheir impact on time series forecasting remains largely unexplored. We address\nthis gap by introducing two new attacks: (i) an adaptation of multivariate\nLiRA, a state-of-the-art MIA originally developed for classification models, to\nthe time-series forecasting setting, and (ii) a novel end-to-end learning\napproach called Deep Time Series (DTS) attack. We benchmark these methods\nagainst adapted versions of other leading attacks from the classification\nsetting.\n  We evaluate all attacks in realistic settings on the TUH-EEG and ELD\ndatasets, targeting two strong forecasting architectures, LSTM and the\nstate-of-the-art N-HiTS, under both record- and user-level threat models. Our\nresults show that forecasting models are vulnerable, with user-level attacks\noften achieving perfect detection. The proposed methods achieve the strongest\nperformance in several settings, establishing new baselines for privacy risk\nassessment in time series forecasting. Furthermore, vulnerability increases\nwith longer prediction horizons and smaller training populations, echoing\ntrends observed in large language models.",
    "updated" : "2025-09-04T12:43:45Z",
    "published" : "2025-09-04T12:43:45Z",
    "authors" : [
      {
        "name" : "Nicolas Johansson"
      },
      {
        "name" : "Tobias Olsson"
      },
      {
        "name" : "Daniel Nilsson"
      },
      {
        "name" : "Johan Östman"
      },
      {
        "name" : "Fazeleh Hoseini"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05265v1",
    "title" : "On Evaluating the Poisoning Robustness of Federated Learning under Local\n  Differential Privacy",
    "summary" : "Federated learning (FL) combined with local differential privacy (LDP)\nenables privacy-preserving model training across decentralized data sources.\nHowever, the decentralized data-management paradigm leaves LDPFL vulnerable to\nparticipants with malicious intent. The robustness of LDPFL protocols,\nparticularly against model poisoning attacks (MPA), where adversaries inject\nmalicious updates to disrupt global model convergence, remains insufficiently\nstudied. In this paper, we propose a novel and extensible model poisoning\nattack framework tailored for LDPFL settings. Our approach is driven by the\nobjective of maximizing the global training loss while adhering to local\nprivacy constraints. To counter robust aggregation mechanisms such as\nMulti-Krum and trimmed mean, we develop adaptive attacks that embed carefully\ncrafted constraints into a reverse training process, enabling evasion of these\ndefenses. We evaluate our framework across three representative LDPFL\nprotocols, three benchmark datasets, and two types of deep neural networks.\nAdditionally, we investigate the influence of data heterogeneity and privacy\nbudgets on attack effectiveness. Experimental results demonstrate that our\nadaptive attacks can significantly degrade the performance of the global model,\nrevealing critical vulnerabilities and highlighting the need for more robust\nLDPFL defense strategies against MPA. Our code is available at\nhttps://github.com/ZiJW/LDPFL-Attack",
    "updated" : "2025-09-05T17:23:03Z",
    "published" : "2025-09-05T17:23:03Z",
    "authors" : [
      {
        "name" : "Zijian Wang"
      },
      {
        "name" : "Wei Tong"
      },
      {
        "name" : "Tingxuan Han"
      },
      {
        "name" : "Haoyu Chen"
      },
      {
        "name" : "Tianling Zhang"
      },
      {
        "name" : "Yunlong Mao"
      },
      {
        "name" : "Sheng Zhong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05162v1",
    "title" : "Verifiability and Privacy in Federated Learning through Context-Hiding\n  Multi-Key Homomorphic Authenticators",
    "summary" : "Federated Learning has rapidly expanded from its original inception to now\nhave a large body of research, several frameworks, and sold in a variety of\ncommercial offerings. Thus, its security and robustness is of significant\nimportance. There are many algorithms that provide robustness in the case of\nmalicious clients. However, the aggregator itself may behave maliciously, for\nexample, by biasing the model or tampering with the weights to weaken the\nmodels privacy. In this work, we introduce a verifiable federated learning\nprotocol that enables clients to verify the correctness of the aggregators\ncomputation without compromising the confidentiality of their updates. Our\nprotocol uses a standard secure aggregation technique to protect individual\nmodel updates with a linearly homomorphic authenticator scheme that enables\nefficient, privacy-preserving verification of the aggregated result. Our\nconstruction ensures that clients can detect manipulation by the aggregator\nwhile maintaining low computational overhead. We demonstrate that our approach\nscales to large models, enabling verification over large neural networks with\nmillions of parameters.",
    "updated" : "2025-09-05T14:57:18Z",
    "published" : "2025-09-05T14:57:18Z",
    "authors" : [
      {
        "name" : "Simone Bottoni"
      },
      {
        "name" : "Giulio Zizzo"
      },
      {
        "name" : "Stefano Braghin"
      },
      {
        "name" : "Alberto Trombetta"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04919v1",
    "title" : "Optimal Variance and Covariance Estimation under Differential Privacy in\n  the Add-Remove Model and Beyond",
    "summary" : "In this paper, we study the problem of estimating the variance and covariance\nof datasets under differential privacy in the add-remove model. While\nestimation in the swap model has been extensively studied in the literature,\nthe add-remove model remains less explored and more challenging, as the dataset\nsize must also be kept private. To address this issue, we develop efficient\nmechanisms for variance and covariance estimation based on the \\emph{B\\'{e}zier\nmechanism}, a novel moment-release framework that leverages Bernstein bases. We\nprove that our proposed mechanisms are minimax optimal in the high-privacy\nregime by establishing new minimax lower bounds. Moreover, beyond worst-case\nscenarios, we analyze instance-wise utility and show that the B\\'{e}zier-based\nestimator consistently achieves better utility compared to alternative\nmechanisms. Finally, we demonstrate the effectiveness of the B\\'{e}zier\nmechanism beyond variance and covariance estimation, showcasing its\napplicability to other statistical tasks.",
    "updated" : "2025-09-05T08:37:30Z",
    "published" : "2025-09-05T08:37:30Z",
    "authors" : [
      {
        "name" : "Shokichi Takakura"
      },
      {
        "name" : "Seng Pei Liew"
      },
      {
        "name" : "Satoshi Hasegawa"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.DS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.04710v1",
    "title" : "Network-Aware Differential Privacy",
    "summary" : "Differential privacy (DP) is a privacy-enhancement technology (PET) that\nreceives prominent attention from the academia, industry, and government. One\nmain development over the past decade has been the decentralization of DP,\nincluding local DP and shuffle DP. Despite that decentralized DP heavily relies\non network communications for data collection,we found that: 1) no systematic\nstudy has surveyed the research opportunities at the intersection of networking\nand DP; 2) nor have there been significant efforts to develop DP mechanisms\nthat are explicitly tailored for network environments. In this paper, we seek\nto address this gap by initiating a new direction of network-aware DP. We\nidentified two focus areas where the network research can offer substantive\ncontributions to the design and deployment of DP, related to network security\nand topology. Through this work, we hope to encourage more research that\nadapt/optimize DP's deployment in various network environments.",
    "updated" : "2025-09-04T23:53:54Z",
    "published" : "2025-09-04T23:53:54Z",
    "authors" : [
      {
        "name" : "Zhou Li"
      },
      {
        "name" : "Yu Zheng"
      },
      {
        "name" : "Tianhao Wang"
      },
      {
        "name" : "Sang-Woo Jun"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06444v1",
    "title" : "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
    "summary" : "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
    "updated" : "2025-09-08T08:44:24Z",
    "published" : "2025-09-08T08:44:24Z",
    "authors" : [
      {
        "name" : "Cheng Qian"
      },
      {
        "name" : "Hainan Zhang"
      },
      {
        "name" : "Yongxin Tong"
      },
      {
        "name" : "Hong-Wei Zheng"
      },
      {
        "name" : "Zhiming Zheng"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06368v1",
    "title" : "From Perception to Protection: A Developer-Centered Study of Security\n  and Privacy Threats in Extended Reality (XR)",
    "summary" : "The immersive nature of XR introduces a fundamentally different set of\nsecurity and privacy (S&P) challenges due to the unprecedented user\ninteractions and data collection that traditional paradigms struggle to\nmitigate. As the primary architects of XR applications, developers play a\ncritical role in addressing novel threats. However, to effectively support\ndevelopers, we must first understand how they perceive and respond to different\nthreats. Despite the growing importance of this issue, there is a lack of\nin-depth, threat-aware studies that examine XR S&P from the developers'\nperspective. To fill this gap, we interviewed 23 professional XR developers\nwith a focus on emerging threats in XR. Our study addresses two research\nquestions aiming to uncover existing problems in XR development and identify\nactionable paths forward.\n  By examining developers' perceptions of S&P threats, we found that: (1) XR\ndevelopment decisions (e.g., rich sensor data collection, user-generated\ncontent interfaces) are closely tied to and can amplify S&P threats, yet\ndevelopers are often unaware of these risks, resulting in cognitive biases in\nthreat perception; and (2) limitations in existing mitigation methods, combined\nwith insufficient strategic, technical, and communication support, undermine\ndevelopers' motivation, awareness, and ability to effectively address these\nthreats. Based on these findings, we propose actionable and stakeholder-aware\nrecommendations to improve XR S&P throughout the XR development process. This\nwork represents the first effort to undertake a threat-aware,\ndeveloper-centered study in the XR domain -- an area where the immersive,\ndata-rich nature of the XR technology introduces distinctive challenges.",
    "updated" : "2025-09-08T06:48:48Z",
    "published" : "2025-09-08T06:48:48Z",
    "authors" : [
      {
        "name" : "Kunlin Cai"
      },
      {
        "name" : "Jinghuai Zhang"
      },
      {
        "name" : "Ying Li"
      },
      {
        "name" : "Zhiyuan Wang"
      },
      {
        "name" : "Xun Chen"
      },
      {
        "name" : "Tianshi Li"
      },
      {
        "name" : "Yuan Tian"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06361v1",
    "title" : "Speaker Privacy and Security in the Big Data Era: Protection and Defense\n  against Deepfake",
    "summary" : "In the era of big data, remarkable advancements have been achieved in\npersonalized speech generation techniques that utilize speaker attributes,\nincluding voice and speaking style, to generate deepfake speech. This has also\namplified global security risks from deepfake speech misuse, resulting in\nconsiderable societal costs worldwide. To address the security threats posed by\ndeepfake speech, techniques have been developed focusing on both the protection\nof voice attributes and the defense against deepfake speech. Among them, the\nvoice anonymization technique has been developed to protect voice attributes\nfrom extraction for deepfake generation, while deepfake detection and\nwatermarking have been utilized to defend against the misuse of deepfake\nspeech. This paper provides a short and concise overview of the three\ntechniques, describing the methodologies, advancements, and challenges. A\ncomprehensive version, offering additional discussions, will be published in\nthe near future.",
    "updated" : "2025-09-08T06:22:36Z",
    "published" : "2025-09-08T06:22:36Z",
    "authors" : [
      {
        "name" : "Liping Chen"
      },
      {
        "name" : "Kong Aik Lee"
      },
      {
        "name" : "Zhen-Hua Ling"
      },
      {
        "name" : "Xin Wang"
      },
      {
        "name" : "Rohan Kumar Das"
      },
      {
        "name" : "Tomoki Toda"
      },
      {
        "name" : "Haizhou Li"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06264v1",
    "title" : "PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss\n  Random Variable Optimization",
    "summary" : "Differentially Private Stochastic Gradient Descent (DP-SGD) is a standard\nmethod for enforcing privacy in deep learning, typically using the Gaussian\nmechanism to perturb gradient updates. However, conventional mechanisms such as\nGaussian and Laplacian noise are parameterized only by variance or scale. This\nsingle degree of freedom ties the magnitude of noise directly to both privacy\nloss and utility degradation, preventing independent control of these two\nfactors. The problem becomes more pronounced when the number of composition\nrounds T and batch size B vary across tasks, as these variations induce\ntask-dependent shifts in the privacy-utility trade-off, where small changes in\nnoise parameters can disproportionately affect model accuracy. To address this\nlimitation, we introduce PLRV-O, a framework that defines a broad search space\nof parameterized DP-SGD noise distributions, where privacy loss moments are\ntightly characterized yet can be optimized more independently with respect to\nutility loss. This formulation enables systematic adaptation of noise to\ntask-specific requirements, including (i) model size, (ii) training duration,\n(iii) batch sampling strategies, and (iv) clipping thresholds under both\ntraining and fine-tuning settings. Empirical results demonstrate that PLRV-O\nsubstantially improves utility under strict privacy constraints. On CIFAR-10, a\nfine-tuned ViT achieves 94.03% accuracy at epsilon approximately 0.5, compared\nto 83.93% with Gaussian noise. On SST-2, RoBERTa-large reaches 92.20% accuracy\nat epsilon approximately 0.2, versus 50.25% with Gaussian.",
    "updated" : "2025-09-08T01:06:45Z",
    "published" : "2025-09-08T01:06:45Z",
    "authors" : [
      {
        "name" : "Qin Yang"
      },
      {
        "name" : "Nicholas Stout"
      },
      {
        "name" : "Meisam Mohammady"
      },
      {
        "name" : "Han Wang"
      },
      {
        "name" : "Ayesha Samreen"
      },
      {
        "name" : "Christopher J Quinn"
      },
      {
        "name" : "Yan Yan"
      },
      {
        "name" : "Ashish Kundu"
      },
      {
        "name" : "Yuan Hong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06142v1",
    "title" : "RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric\n  Privacy Preserving",
    "summary" : "The integration of AI with medical images enables the extraction of implicit\nimage-derived biomarkers for a precise health assessment. Recently, retinal\nage, a biomarker predicted from fundus images, is a proven predictor of\nsystemic disease risks, behavioral patterns, aging trajectory and even\nmortality. However, the capability to infer such sensitive biometric data\nraises significant privacy risks, where unauthorized use of fundus images could\nlead to bioinformation leakage, breaching individual privacy. In response, we\nformulate a new research problem of biometric privacy associated with medical\nimages and propose RetinaGuard, a novel privacy-enhancing framework that\nemploys a feature-level generative adversarial masking mechanism to obscure\nretinal age while preserving image visual quality and disease diagnostic\nutility. The framework further utilizes a novel multiple-to-one knowledge\ndistillation strategy incorporating a retinal foundation model and diverse\nsurrogate age encoders to enable a universal defense against black-box age\nprediction models. Comprehensive evaluations confirm that RetinaGuard\nsuccessfully obfuscates retinal age prediction with minimal impact on image\nquality and pathological feature representation. RetinaGuard is also flexible\nfor extension to other medical image derived biomarkers. RetinaGuard is also\nflexible for extension to other medical image biomarkers.",
    "updated" : "2025-09-07T17:16:42Z",
    "published" : "2025-09-07T17:16:42Z",
    "authors" : [
      {
        "name" : "Zhengquan Luo"
      },
      {
        "name" : "Chi Liu"
      },
      {
        "name" : "Dongfu Xiao"
      },
      {
        "name" : "Zhen Yu"
      },
      {
        "name" : "Yueye Wang"
      },
      {
        "name" : "Tianqing Zhu"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.06133v1",
    "title" : "VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored\n  Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles",
    "summary" : "Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,\nand service centers that are difficult to verify and prone to fraud. We propose\nVehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with\nzero-knowledge proofs (ZKPs) for privacy-preserving verification.\nVehiclePassport immutably commits to manufacturing, telemetry, and service\nevents while enabling selective disclosure via short-lived JWTs and Groth16\nproofs. Our open-source reference stack anchors hashes on Polygon zkEVM at\n<$0.02 per event, validates proofs in <10 ms, and scales to millions of\nvehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant\ntraceability, and establishes a trustless foundation for insurance, resale, and\nregulatory applications in global mobility data markets.",
    "updated" : "2025-09-07T16:40:30Z",
    "published" : "2025-09-07T16:40:30Z",
    "authors" : [
      {
        "name" : "Pradyumna Kaushal"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.SE",
      "cs.SY",
      "eess.SY",
      "C.2.4; K.6.5; D.4.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05608v1",
    "title" : "Cross-Service Threat Intelligence in LLM Services using\n  Privacy-Preserving Fingerprints",
    "summary" : "The widespread deployment of LLMs across enterprise services has created a\ncritical security blind spot. Organizations operate multiple LLM services\nhandling billions of queries daily, yet regulatory compliance boundaries\nprevent these services from sharing threat intelligence about prompt injection\nattacks, the top security risk for LLMs. When an attack is detected in one\nservice, the same threat may persist undetected in others for months, as\nprivacy regulations prohibit sharing user prompts across compliance boundaries.\n  We present BinaryShield, the first privacy-preserving threat intelligence\nsystem that enables secure sharing of attack fingerprints across compliance\nboundaries. BinaryShield transforms suspicious prompts through a unique\npipeline combining PII redaction, semantic embedding, binary quantization, and\nrandomized response mechanism to potentially generate non-invertible\nfingerprints that preserve attack patterns while providing privacy. Our\nevaluations demonstrate that BinaryShield achieves an F1-score of 0.94,\nsignificantly outperforming SimHash (0.77), the privacy-preserving baseline,\nwhile achieving 64x storage reduction and 38x faster similarity search compared\nto dense embeddings.",
    "updated" : "2025-09-06T05:57:20Z",
    "published" : "2025-09-06T05:57:20Z",
    "authors" : [
      {
        "name" : "Waris Gill"
      },
      {
        "name" : "Natalie Isak"
      },
      {
        "name" : "Matthew Dressman"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05382v1",
    "title" : "User Privacy and Large Language Models: An Analysis of Frontier\n  Developers' Privacy Policies",
    "summary" : "Hundreds of millions of people now regularly interact with large language\nmodels via chatbots. Model developers are eager to acquire new sources of\nhigh-quality training data as they race to improve model capabilities and win\nmarket share. This paper analyzes the privacy policies of six U.S. frontier AI\ndevelopers to understand how they use their users' chats to train models.\nDrawing primarily on the California Consumer Privacy Act, we develop a novel\nqualitative coding schema that we apply to each developer's relevant privacy\npolicies to compare data collection and use practices across the six companies.\nWe find that all six developers appear to employ their users' chat data to\ntrain and improve their models by default, and that some retain this data\nindefinitely. Developers may collect and train on personal information\ndisclosed in chats, including sensitive information such as biometric and\nhealth data, as well as files uploaded by users. Four of the six companies we\nexamined appear to include children's chat data for model training, as well as\ncustomer data from other products. On the whole, developers' privacy policies\noften lack essential information about their practices, highlighting the need\nfor greater transparency and accountability. We address the implications of\nusers' lack of consent for the use of their chat data for model training, data\nsecurity issues arising from indefinite chat data retention, and training on\nchildren's chat data. We conclude by providing recommendations to policymakers\nand developers to address the data privacy challenges posed by LLM-powered\nchatbots.",
    "updated" : "2025-09-05T01:01:21Z",
    "published" : "2025-09-05T01:01:21Z",
    "authors" : [
      {
        "name" : "Jennifer King"
      },
      {
        "name" : "Kevin Klyman"
      },
      {
        "name" : "Emily Capstick"
      },
      {
        "name" : "Tiffany Saade"
      },
      {
        "name" : "Victoria Hsieh"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05377v1",
    "title" : "Enhancing Gradient Variance and Differential Privacy in Quantum\n  Federated Learning",
    "summary" : "Upon integrating Quantum Neural Network (QNN) as the local model, Quantum\nFederated Learning (QFL) has recently confronted notable challenges. Firstly,\nexploration is hindered over sharp minima, decreasing learning performance.\nSecondly, the steady gradient descent results in more stable and predictable\nmodel transmissions over wireless channels, making the model more susceptible\nto attacks from adversarial entities. Additionally, the local QFL model is\nvulnerable to noise produced by the quantum device's intermediate noise states,\nsince it requires the use of quantum gates and circuits for training. This\nlocal noise becomes intertwined with learning parameters during training,\nimpairing model precision and convergence rate. To address these issues, we\npropose a new QFL technique that incorporates differential privacy and\nintroduces a dedicated noise estimation strategy to quantify and mitigate the\nimpact of intermediate quantum noise. Furthermore, we design an adaptive noise\ngeneration scheme to alleviate privacy threats associated with the vanishing\ngradient variance phenomenon of QNN and enhance robustness against device\nnoise. Experimental results demonstrate that our algorithm effectively balances\nconvergence, reduces communication costs, and mitigates the adverse effects of\nintermediate quantum noise while maintaining strong privacy protection. Using\nreal-world datasets, we achieved test accuracy of up to 98.47\\% for the MNIST\ndataset and 83.85\\% for the CIFAR-10 dataset while maintaining fast execution\ntimes.",
    "updated" : "2025-09-04T15:29:52Z",
    "published" : "2025-09-04T15:29:52Z",
    "authors" : [
      {
        "name" : "Duc-Thien Phan"
      },
      {
        "name" : "Minh-Duong Nguyen"
      },
      {
        "name" : "Quoc-Viet Pham"
      },
      {
        "name" : "Huilong Pi"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05376v1",
    "title" : "Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye\n  Tracking for Interactive Learning Environments",
    "summary" : "Eye-tracking technology can aid in understanding neurodevelopmental disorders\nand tracing a person's identity. However, this technology poses a significant\nrisk to privacy, as it captures sensitive information about individuals and\nincreases the likelihood that data can be traced back to them. This paper\nproposes a human-centered framework designed to prevent identity backtracking\nwhile preserving the pedagogical benefits of AI-powered eye tracking in\ninteractive learning environments. We explore how real-time data anonymization,\nethical design principles, and regulatory compliance (such as GDPR) can be\nintegrated to build trust and transparency. We first demonstrate the potential\nfor backtracking student IDs and diagnoses in various scenarios using serious\ngame-based eye-tracking data. We then provide a two-stage privacy-preserving\nframework that prevents participants from being tracked while still enabling\ndiagnostic classification. The first phase covers four scenarios: I) Predicting\ndisorder diagnoses based on different game levels. II) Predicting student IDs\nbased on different game levels. III) Predicting student IDs based on randomized\ndata. IV) Utilizing K-Means for out-of-sample data. In the second phase, we\npresent a two-stage framework that preserves privacy. We also employ Federated\nLearning (FL) across multiple clients, incorporating a secure identity\nmanagement system with dummy IDs and administrator-only access controls. In the\nfirst phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%\naccuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully\nidentifying and assigning a new student ID in scenario 4. In phase 2, we\neffectively prevented backtracking and established a secure identity management\nsystem with dummy IDs and administrator-only access controls, achieving an\noverall accuracy of 99.40%.",
    "updated" : "2025-09-04T13:08:06Z",
    "published" : "2025-09-04T13:08:06Z",
    "authors" : [
      {
        "name" : "Abdul Rehman"
      },
      {
        "name" : "Are Dæhlen"
      },
      {
        "name" : "Ilona Heldal"
      },
      {
        "name" : "Jerry Chun-wei Lin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2509.05362v1",
    "title" : "AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and\n  Conversational Scambaiting by Leveraging LLMs and Federated Learning",
    "summary" : "Scams exploiting real-time social engineering -- such as phishing,\nimpersonation, and phone fraud -- remain a persistent and evolving threat\nacross digital platforms. Existing defenses are largely reactive, offering\nlimited protection during active interactions. We propose a privacy-preserving,\nAI-in-the-loop framework that proactively detects and disrupts scam\nconversations in real time. The system combines instruction-tuned artificial\nintelligence with a safety-aware utility function that balances engagement with\nharm minimization, and employs federated learning to enable continual model\nupdates without raw data sharing. Experimental evaluations show that the system\nproduces fluent and engaging responses (perplexity as low as 22.3, engagement\n$\\approx$0.80), while human studies confirm significant gains in realism,\nsafety, and effectiveness over strong baselines. In federated settings, models\ntrained with FedAvg sustain up to 30 rounds while preserving high engagement\n($\\approx$0.80), strong relevance ($\\approx$0.74), and low PII leakage\n($\\leq$0.0085). Even with differential privacy, novelty and safety remain\nstable, indicating that robust privacy can be achieved without sacrificing\nperformance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,\nMD-Judge) shows a straightforward pattern: stricter moderation settings reduce\nthe chance of exposing personal information, but they also limit how much the\nmodel engages in conversation. In contrast, more relaxed settings allow longer\nand richer interactions, which improve scam detection, but at the cost of\nhigher privacy risk. To our knowledge, this is the first framework to unify\nreal-time scam-baiting, federated privacy preservation, and calibrated safety\nmoderation into a proactive defense paradigm.",
    "updated" : "2025-09-04T00:19:48Z",
    "published" : "2025-09-04T00:19:48Z",
    "authors" : [
      {
        "name" : "Ismail Hossain"
      },
      {
        "name" : "Sai Puppala"
      },
      {
        "name" : "Sajedul Talukder"
      },
      {
        "name" : "Md Jahangir Alam"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ]
  }
]