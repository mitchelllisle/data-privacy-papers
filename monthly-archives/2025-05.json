[
  {
    "id" : "http://arxiv.org/abs/2505.00593v1",
    "title" : "A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security\n  and Privacy in IoT and Edge Networks",
    "summary" : "The security of image data in the Internet of Things (IoT) and edge networks\nis crucial due to the increasing deployment of intelligent systems for\nreal-time decision-making. Traditional encryption algorithms such as AES and\nRSA are computationally expensive for resource-constrained IoT devices and\nineffective for large-volume image data, leading to inefficiencies in\nprivacy-preserving distributed learning applications. To address these\nconcerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption\nscheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic\nChain Permutation and Confusion mechanisms to enhance security while\nmaintaining efficiency. The proposed scheme consists of three stages: (1) FAPS,\nwhich extracts and reorganizes pixels based on high and low edge intensity\nfeatures for correlation disruption; (2) Chaotic Chain Permutation, which\nemploys a logistic chaotic map with SHA-256-based dynamically updated keys for\nblock-wise permutation; and (3) Chaotic chain Confusion, which utilises\ndynamically generated chaotic seed matrices for bitwise XOR operations.\nExtensive security and performance evaluations demonstrate that the proposed\nscheme significantly reduces pixel correlation -- almost zero, achieves high\nentropy values close to 8, and resists differential cryptographic attacks. The\noptimum design of the proposed scheme makes it suitable for real-time\ndeployment in resource-constrained environments.",
    "updated" : "2025-05-01T15:26:48Z",
    "published" : "2025-05-01T15:26:48Z",
    "authors" : [
      {
        "name" : "Muhammad Shahbaz Khan"
      },
      {
        "name" : "Ahmed Al-Dubai"
      },
      {
        "name" : "Jawad Ahmad"
      },
      {
        "name" : "Nikolaos Pitropakis"
      },
      {
        "name" : "Baraq Ghaleb"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.00257v1",
    "title" : "Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial\n  Data Circulation",
    "summary" : "The sharing of external data has become a strong demand of financial\ninstitutions, but the privacy issue has led to the difficulty of\ninterconnecting different platforms and the low degree of data openness. To\neffectively solve the privacy problem of financial data in trans-border flow\nand sharing, to ensure that the data is available but not visible, to realize\nthe joint portrait of all kinds of heterogeneous data of business organizations\nin different industries, we propose a Heterogeneous Federated Graph Neural\nNetwork (HFGNN) approach. In this method, the distribution of heterogeneous\nbusiness data of trans-border organizations is taken as subgraphs, and the\nsharing and circulation process among subgraphs is constructed as a\nstatistically heterogeneous global graph through a central server. Each\nsubgraph learns the corresponding personalized service model through local\ntraining to select and update the relevant subset of subgraphs with aggregated\nparameters, and effectively separates and combines topological and feature\ninformation among subgraphs. Finally, our simulation experimental results show\nthat the proposed method has higher accuracy performance and faster convergence\nspeed than existing methods.",
    "updated" : "2025-05-01T02:47:43Z",
    "published" : "2025-05-01T02:47:43Z",
    "authors" : [
      {
        "name" : "Zhizhong Tan"
      },
      {
        "name" : "Jiexin Zheng"
      },
      {
        "name" : "Kevin Qi Zhang"
      },
      {
        "name" : "Wenyong Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01292v1",
    "title" : "Fine-grained Manipulation Attacks to Local Differential Privacy\n  Protocols for Data Streams",
    "summary" : "Local Differential Privacy (LDP) enables massive data collection and analysis\nwhile protecting end users' privacy against untrusted aggregators. It has been\napplied to various data types (e.g., categorical, numerical, and graph data)\nand application settings (e.g., static and streaming). Recent findings indicate\nthat LDP protocols can be easily disrupted by poisoning or manipulation\nattacks, which leverage injected/corrupted fake users to send crafted data\nconforming to the LDP reports. However, current attacks primarily target static\nprotocols, neglecting the security of LDP protocols in the streaming settings.\nOur research fills the gap by developing novel fine-grained manipulation\nattacks to LDP protocols for data streams. By reviewing the attack surfaces in\nexisting algorithms, We introduce a unified attack framework with composable\nmodules, which can manipulate the LDP estimated stream toward a target stream.\nOur attack framework can adapt to state-of-the-art streaming LDP algorithms\nwith different analytic tasks (e.g., frequency and mean) and LDP models\n(event-level, user-level, w-event level). We validate our attacks theoretically\nand through extensive experiments on real-world datasets, and finally explore a\npossible defense mechanism for mitigating these attacks.",
    "updated" : "2025-05-02T14:09:56Z",
    "published" : "2025-05-02T14:09:56Z",
    "authors" : [
      {
        "name" : "Xinyu Li"
      },
      {
        "name" : "Xuebin Ren"
      },
      {
        "name" : "Shusen Yang"
      },
      {
        "name" : "Liang Shi"
      },
      {
        "name" : "Chia-Mu Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.00951v1",
    "title" : "Preserving Privacy and Utility in LLM-Based Product Recommendations",
    "summary" : "Large Language Model (LLM)-based recommendation systems leverage powerful\nlanguage models to generate personalized suggestions by processing user\ninteractions and preferences. Unlike traditional recommendation systems that\nrely on structured data and collaborative filtering, LLM-based models process\ntextual and contextual information, often using cloud-based infrastructure.\nThis raises privacy concerns, as user data is transmitted to remote servers,\nincreasing the risk of exposure and reducing control over personal information.\nTo address this, we propose a hybrid privacy-preserving recommendation\nframework which separates sensitive from nonsensitive data and only shares the\nlatter with the cloud to harness LLM-powered recommendations. To restore lost\nrecommendations related to obfuscated sensitive data, we design a\nde-obfuscation module that reconstructs sensitive recommendations locally.\nExperiments on real-world e-commerce datasets show that our framework achieves\nalmost the same recommendation utility with a system which shares all data with\nan LLM, while preserving privacy to a large extend. Compared to\nobfuscation-only techniques, our approach improves HR@10 scores and category\ndistribution alignment, offering a better balance between privacy and\nrecommendation quality. Furthermore, our method runs efficiently on\nconsumer-grade hardware, making privacy-aware LLM-based recommendation systems\npractical for real-world use.",
    "updated" : "2025-05-02T01:54:08Z",
    "published" : "2025-05-02T01:54:08Z",
    "authors" : [
      {
        "name" : "Tina Khezresmaeilzadeh"
      },
      {
        "name" : "Jiang Zhang"
      },
      {
        "name" : "Dimitrios Andreadis"
      },
      {
        "name" : "Konstantinos Psounis"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02828v1",
    "title" : "Privacy Risks and Preservation Methods in Explainable Artificial\n  Intelligence: A Scoping Review",
    "summary" : "Explainable Artificial Intelligence (XAI) has emerged as a pillar of\nTrustworthy AI and aims to bring transparency in complex models that are opaque\nby nature. Despite the benefits of incorporating explanations in models, an\nurgent need is found in addressing the privacy concerns of providing this\nadditional information to end users. In this article, we conduct a scoping\nreview of existing literature to elicit details on the conflict between privacy\nand explainability. Using the standard methodology for scoping review, we\nextracted 57 articles from 1,943 studies published from January 2019 to\nDecember 2024. The review addresses 3 research questions to present readers\nwith more understanding of the topic: (1) what are the privacy risks of\nreleasing explanations in AI systems? (2) what current methods have researchers\nemployed to achieve privacy preservation in XAI systems? (3) what constitutes a\nprivacy preserving explanation? Based on the knowledge synthesized from the\nselected studies, we categorize the privacy risks and preservation methods in\nXAI and propose the characteristics of privacy preserving explanations to aid\nresearchers and practitioners in understanding the requirements of XAI that is\nprivacy compliant. Lastly, we identify the challenges in balancing privacy with\nother system desiderata and provide recommendations for achieving privacy\npreserving XAI. We expect that this review will shed light on the complex\nrelationship of privacy and explainability, both being the fundamental\nprinciples of Trustworthy AI.",
    "updated" : "2025-05-05T17:53:28Z",
    "published" : "2025-05-05T17:53:28Z",
    "authors" : [
      {
        "name" : "Sonal Allana"
      },
      {
        "name" : "Mohan Kankanhalli"
      },
      {
        "name" : "Rozita Dara"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02798v1",
    "title" : "Unifying Laplace Mechanism with Instance Optimality in Differential\n  Privacy",
    "summary" : "We adapt the canonical Laplace mechanism, widely used in differentially\nprivate data analysis, to achieve near instance optimality with respect to the\nhardness of the underlying dataset. In particular, we construct a piecewise\nLaplace distribution whereby we defy traditional assumptions and show that\nLaplace noise can in fact be drawn proportional to the local sensitivity when\ndone in a piecewise manner. While it may initially seem counterintuitive that\nthis satisfies (pure) differential privacy and can be sampled, we provide both\nthrough a simple connection to the exponential mechanism and inverse\nsensitivity along with the fact that the Laplace distribution is a two-sided\nexponential distribution. As a result, we prove that in the continuous setting\nour \\textit{piecewise Laplace mechanism} strictly dominates the inverse\nsensitivity mechanism, which was previously shown to both be nearly instance\noptimal and uniformly outperform the smooth sensitivity framework. Furthermore,\nin the worst-case where all local sensitivities equal the global sensitivity,\nour method simply reduces to a Laplace mechanism. We also complement this with\nan approximate local sensitivity variant to potentially ease the computational\ncost, which can also extend to higher dimensions.",
    "updated" : "2025-05-05T17:20:28Z",
    "published" : "2025-05-05T17:20:28Z",
    "authors" : [
      {
        "name" : "David Durfee"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02513v1",
    "title" : "Trustworthy Inter-Provider Agreements in 6G Using a Privacy-Enabled\n  Hybrid Blockchain Framework",
    "summary" : "Inter-provider agreements are central to 6G networks, where administrative\ndomains must securely and dynamically share services. To address the dual need\nfor transparency and confidentiality, we propose a privacy-enabled hybrid\nblockchain setup using Hyperledger Besu, integrating both public and private\ntransaction workflows. The system enables decentralized service registration,\nselection, and SLA breach reporting through role-based smart contracts and\nprivacy groups. We design and deploy a proof-of-concept implementation,\nevaluating performance using end-to-end latency as a key metric within privacy\ngroups. Results show that public interactions maintain stable latency, while\nprivate transactions incur additional overhead due to off-chain coordination.\nThe block production rate governed by IBFT 2.0 had limited impact on private\ntransaction latency, due to encryption and peer synchronization. Lessons\nlearned highlight design considerations for smart contract structure, validator\nmanagement, and scalability patterns suitable for dynamic inter-domain\ncollaboration. Our findings offer practical insights for deploying trustworthy\nagreement systems in 6G networks using privacy-enabled hybrid blockchains.",
    "updated" : "2025-05-05T09:46:30Z",
    "published" : "2025-05-05T09:46:30Z",
    "authors" : [
      {
        "name" : "Farhana Javed"
      },
      {
        "name" : "Josep Mangues-Bafalluy"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02392v1",
    "title" : "Moneros Decentralized P2P Exchanges: Functionality, Adoption, and\n  Privacy Risks",
    "summary" : "Privacy-focused cryptocurrencies like Monero remain popular, despite\nincreasing regulatory scrutiny that has led to their delisting from major\ncentralized exchanges. The latter also explains the recent popularity of\ndecentralized exchanges (DEXs) with no centralized ownership structures. These\nplatforms typically leverage peer-to-peer (P2P) networks, promising secure and\nanonymous asset trading. However, questions of liability remain, and the\nacademic literature lacks comprehensive insights into the functionality,\ntrading activity, and privacy claims of these P2P platforms. In this paper, we\nprovide an early systematization of the current landscape of decentralized\npeer-to-peer exchanges within the Monero ecosystem. We examine several recently\ndeveloped DEX platforms, analyzing their popularity, functionality,\narchitectural choices, and potential weaknesses. We further identify and report\non a privacy vulnerability in the recently popularized Haveno exchange,\ndemonstrating that certain Haveno trades could be detected, allowing\ntransactions to be linked across the Monero and Bitcoin blockchains. We hope\nthat our findings can nourish the discussion in the research community about\nmore secure designs, and provide insights for regulators.",
    "updated" : "2025-05-05T06:27:37Z",
    "published" : "2025-05-05T06:27:37Z",
    "authors" : [
      {
        "name" : "Yannik Kopyciok"
      },
      {
        "name" : "Friedhelm Victor"
      },
      {
        "name" : "Stefan Schmid"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02383v1",
    "title" : "Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs\n  Between Privacy and Regret",
    "summary" : "We address differentially private stochastic bandit problems from the angles\nof exploring the deep connections among Thompson Sampling with Gaussian priors,\nGaussian mechanisms, and Gaussian differential privacy (GDP). We propose\nDP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade\noff privacy and regret. DP-TS-UCB satisfies $ \\tilde{O}\n\\left(T^{0.25(1-\\alpha)}\\right)$-GDP and enjoys an $O\n\\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $\\alpha \\in [0,1]$\ncontrols the trade-off between privacy and regret. Theoretically, our DP-TS-UCB\nrelies on anti-concentration bounds of Gaussian distributions and links\nexploration mechanisms in Thompson Sampling-based algorithms and Upper\nConfidence Bound-based algorithms, which may be of independent interest.",
    "updated" : "2025-05-05T05:48:52Z",
    "published" : "2025-05-05T05:48:52Z",
    "authors" : [
      {
        "name" : "Bingshan Hu"
      },
      {
        "name" : "Zhiming Huang"
      },
      {
        "name" : "Tianyue H. Zhang"
      },
      {
        "name" : "Mathias Lécuyer"
      },
      {
        "name" : "Nidhi Hegde"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01976v1",
    "title" : "A Survey on Privacy Risks and Protection in Large Language Models",
    "summary" : "Although Large Language Models (LLMs) have become increasingly integral to\ndiverse applications, their capabilities raise significant privacy concerns.\nThis survey offers a comprehensive overview of privacy risks associated with\nLLMs and examines current solutions to mitigate these challenges. First, we\nanalyze privacy leakage and attacks in LLMs, focusing on how these models\nunintentionally expose sensitive information through techniques such as model\ninversion, training data extraction, and membership inference. We investigate\nthe mechanisms of privacy leakage, including the unauthorized extraction of\ntraining data and the potential exploitation of these vulnerabilities by\nmalicious actors. Next, we review existing privacy protection against such\nrisks, such as inference detection, federated learning, backdoor mitigation,\nand confidential computing, and assess their effectiveness in preventing\nprivacy leakage. Furthermore, we highlight key practical challenges and propose\nfuture research directions to develop secure and privacy-preserving LLMs,\nemphasizing privacy risk assessment, secure knowledge transfer between models,\nand interdisciplinary frameworks for privacy governance. Ultimately, this\nsurvey aims to establish a roadmap for addressing escalating privacy challenges\nin the LLMs domain.",
    "updated" : "2025-05-04T03:04:07Z",
    "published" : "2025-05-04T03:04:07Z",
    "authors" : [
      {
        "name" : "Kang Chen"
      },
      {
        "name" : "Xiuze Zhou"
      },
      {
        "name" : "Yuanguo Lin"
      },
      {
        "name" : "Shibo Feng"
      },
      {
        "name" : "Li Shen"
      },
      {
        "name" : "Pengcheng Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01879v1",
    "title" : "What to Do When Privacy Is Gone",
    "summary" : "Today's ethics of privacy is largely dedicated to defending personal\ninformation from big data technologies. This essay goes in the other direction.\nIt considers the struggle to be lost, and explores two strategies for living\nafter privacy is gone. First, total exposure embraces privacy's decline, and\nthen contributes to the process with transparency. All personal information is\nshared without reservation. The resulting ethics is explored through a big data\nversion of Robert Nozick's Experience Machine thought experiment. Second,\ntransient existence responds to privacy's loss by ceaselessly generating new\npersonal identities, which translates into constantly producing temporarily\nunviolated private information. The ethics is explored through Gilles Deleuze's\nmetaphysics of difference applied in linguistic terms to the formation of the\nself. Comparing the exposure and transience alternatives leads to the\nconclusion that today's big data reality splits the traditional ethical link\nbetween authenticity and freedom. Exposure provides authenticity, but negates\nhuman freedom. Transience provides freedom, but disdains authenticity.",
    "updated" : "2025-05-03T17:51:36Z",
    "published" : "2025-05-03T17:51:36Z",
    "authors" : [
      {
        "name" : "James Brusseau"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01788v1",
    "title" : "Privacy Preserving Machine Learning Model Personalization through\n  Federated Personalized Learning",
    "summary" : "The widespread adoption of Artificial Intelligence (AI) has been driven by\nsignificant advances in intelligent system research. However, this progress has\nraised concerns about data privacy, leading to a growing awareness of the need\nfor privacy-preserving AI. In response, there has been a seismic shift in\ninterest towards the leading paradigm for training Machine Learning (ML) models\non decentralized data silos while maintaining data privacy, Federated Learning\n(FL). This research paper presents a comprehensive performance analysis of a\ncutting-edge approach to personalize ML model while preserving privacy achieved\nthrough Privacy Preserving Machine Learning with the innovative framework of\nFederated Personalized Learning (PPMLFPL). Regarding the increasing concerns\nabout data privacy, this study evaluates the effectiveness of PPMLFPL\naddressing the critical balance between personalized model refinement and\nmaintaining the confidentiality of individual user data. According to our\nanalysis, Adaptive Personalized Cross-Silo Federated Learning with Differential\nPrivacy (APPLE+DP) offering efficient execution whereas overall, the use of the\nAdaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption\n(APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated\npersonalized learning settings is strongly suggested. The results offer\nvaluable insights creating it a promising scope for future advancements in the\nfield of privacy-conscious data-driven technologies.",
    "updated" : "2025-05-03T11:31:38Z",
    "published" : "2025-05-03T11:31:38Z",
    "authors" : [
      {
        "name" : "Md. Tanzib Hosain"
      },
      {
        "name" : "Asif Zaman"
      },
      {
        "name" : "Md. Shahriar Sajid"
      },
      {
        "name" : "Shadman Sakeeb Khan"
      },
      {
        "name" : "Shanjida Akter"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01524v1",
    "title" : "The DCR Delusion: Measuring the Privacy Risk of Synthetic Data",
    "summary" : "Synthetic data has become an increasingly popular way to share data without\nrevealing sensitive information. Though Membership Inference Attacks (MIAs) are\nwidely considered the gold standard for empirically assessing the privacy of a\nsynthetic dataset, practitioners and researchers often rely on simpler proxy\nmetrics such as Distance to Closest Record (DCR). These metrics estimate\nprivacy by measuring the similarity between the training data and generated\nsynthetic data. This similarity is also compared against that between the\ntraining data and a disjoint holdout set of real records to construct a binary\nprivacy test. If the synthetic data is not more similar to the training data\nthan the holdout set is, it passes the test and is considered private. In this\nwork we show that, while computationally inexpensive, DCR and other\ndistance-based metrics fail to identify privacy leakage. Across multiple\ndatasets and both classical models such as Baynet and CTGAN and more recent\ndiffusion models, we show that datasets deemed private by proxy metrics are\nhighly vulnerable to MIAs. We similarly find both the binary privacy test and\nthe continuous measure based on these metrics to be uninformative of actual\nmembership inference risk. We further show that these failures are consistent\nacross different metric hyperparameter settings and record selection methods.\nFinally, we argue DCR and other distance-based metrics to be flawed by design\nand show a example of a simple leakage they miss in practice. With this work,\nwe hope to motivate practitioners to move away from proxy metrics to MIAs as\nthe rigorous, comprehensive standard of evaluating privacy of synthetic data,\nin particular to make claims of datasets being legally anonymous.",
    "updated" : "2025-05-02T18:21:14Z",
    "published" : "2025-05-02T18:21:14Z",
    "authors" : [
      {
        "name" : "Zexi Yao"
      },
      {
        "name" : "Nataša Krčo"
      },
      {
        "name" : "Georgi Ganev"
      },
      {
        "name" : "Yves-Alexandre de Montjoye"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.03639v1",
    "title" : "Differential Privacy for Network Assortativity",
    "summary" : "The analysis of network assortativity is of great importance for\nunderstanding the structural characteristics of and dynamics upon networks.\nOften, network assortativity is quantified using the assortativity coefficient\nthat is defined based on the Pearson correlation coefficient between vertex\ndegrees. It is well known that a network may contain sensitive information,\nsuch as the number of friends of an individual in a social network (which is\nabstracted as the degree of vertex.). So, the computation of the assortativity\ncoefficient leads to privacy leakage, which increases the urgent need for\nprivacy-preserving protocol. However, there has been no scheme addressing the\nconcern above.\n  To bridge this gap, in this work, we are the first to propose approaches\nbased on differential privacy (DP for short). Specifically, we design three\nDP-based algorithms: $Local_{ru}$, $Shuffle_{ru}$, and $Decentral_{ru}$. The\nfirst two algorithms, based on Local DP (LDP) and Shuffle DP respectively, are\ndesigned for settings where each individual only knows his/her direct friends.\nIn contrast, the third algorithm, based on Decentralized DP (DDP), targets\nscenarios where each individual has a broader view, i.e., also knowing his/her\nfriends' friends. Theoretically, we prove that each algorithm enables an\nunbiased estimation of the assortativity coefficient of the network. We further\nevaluate the performance of the proposed algorithms using mean squared error\n(MSE), showing that $Shuffle_{ru}$ achieves the best performance, followed by\n$Decentral_{ru}$, with $Local_{ru}$ performing the worst. Note that these three\nalgorithms have different assumptions, so each has its applicability scenario.\nLastly, we conduct extensive numerical simulations, which demonstrate that the\npresented approaches are adequate to achieve the estimation of network\nassortativity under the demand for privacy protection.",
    "updated" : "2025-05-06T15:40:47Z",
    "published" : "2025-05-06T15:40:47Z",
    "authors" : [
      {
        "name" : "Fei Ma"
      },
      {
        "name" : "Jinzhi Ouyang"
      },
      {
        "name" : "Xincheng Hu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02975v1",
    "title" : "Navigating Privacy and Trust: AI Assistants as Social Support for Older\n  Adults",
    "summary" : "AI assistants are increasingly integrated into older adults' daily lives,\noffering new opportunities for social support and accessibility while raising\nimportant questions about privacy, autonomy, and trust. As these systems become\nembedded in caregiving and social networks, older adults must navigate\ntrade-offs between usability, data privacy, and personal agency across\ndifferent interaction contexts. Although prior work has explored AI assistants'\npotential benefits, further research is needed to understand how perceived\nusefulness and risk shape adoption and engagement. This paper examines these\ndynamics and advocates for participatory design approaches that position older\nadults as active decision makers in shaping AI assistant functionality. By\nadvancing a framework for privacy-aware, user-centered AI design, this work\ncontributes to ongoing discussions on developing ethical and transparent AI\nsystems that enhance well-being without compromising user control.",
    "updated" : "2025-05-05T19:00:14Z",
    "published" : "2025-05-05T19:00:14Z",
    "authors" : [
      {
        "name" : "Karina LaRubbio"
      },
      {
        "name" : "Malcolm Grba"
      },
      {
        "name" : "Diana Freed"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04570v1",
    "title" : "Privacy-preserving neutral atom-based quantum classifier towards real\n  healthcare applications",
    "summary" : "Technological advances in Artificial Intelligence (AI) and Machine Learning\n(ML) for the healthcare domain are rapidly arising, with a growing discussion\nregarding the ethical management of their development. In general, ML\nhealthcare applications crucially require performance, interpretability of\ndata, and respect for data privacy. The latter is an increasingly debated topic\nas commercial cloud computing services become more and more widespread.\nRecently, dedicated methods are starting to be developed aiming to protect data\nprivacy. However, these generally result in a trade-off forcing one to balance\nthe level of data privacy and the algorithm performance. Here, a Support Vector\nMachine (SVM) classifier model is proposed whose training is reformulated into\na Quadratic Unconstrained Binary Optimization (QUBO) problem, and adapted to a\nneutral atom-based Quantum Processing Unit (QPU). Our final model does not\nrequire anonymization techniques to protect data privacy since the sensitive\ndata are not needed to be transferred to the cloud-available QPU. Indeed, the\nlatter is used only during the training phase, hence allowing a future concrete\napplication in a real-world scenario. Finally, performance and scaling analyses\non a publicly available breast cancer dataset are discussed, both using ideal\nand noisy simulations for the training process, and also successfully tested on\na currently available real neutral-atom QPU.",
    "updated" : "2025-05-07T17:03:35Z",
    "published" : "2025-05-07T17:03:35Z",
    "authors" : [
      {
        "name" : "Ettore Canonici"
      },
      {
        "name" : "Filippo Caruso"
      }
    ],
    "categories" : [
      "quant-ph",
      "81V45 (Primary) 81P68 (Secondary)",
      "I.2.0; J.2; I.5.0"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04361v1",
    "title" : "RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery\n  Scheme in Mobile Crowdsensing",
    "summary" : "Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS).\nHowever, existing TD methods, including privacy-preserving TD approaches,\nestimate the truth by weighting only the data submitted in the current round,\nwhich often results in low data quality. Moreover, there is a lack of effective\nTD methods that preserve both reputation and data privacy. To address these\nissues, a Reputation and Data Privacy-Preserving based Truth Discovery\n(RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD\nscheme consists of two key approaches: a Reputation-based Truth Discovery (RTD)\napproach, which integrates the weight of current-round data with workers'\nreputation values to estimate the truth, thereby achieving more accurate\nresults, and a Reputation and Data Privacy-Preserving (RDPP) approach, which\nensures privacy preservation for sensing data and reputation values. First, the\nRDPP approach, when seamlessly integrated with RTD, can effectively evaluate\nthe reliability of workers and their sensing data in a privacy-preserving\nmanner. Second, the RDPP scheme supports reputation-based worker recruitment\nand rewards, ensuring high-quality data collection while incentivizing workers\nto provide accurate information. Comprehensive theoretical analysis and\nextensive experiments based on real-world datasets demonstrate that the\nproposed RDPP-TD scheme provides strong privacy protection and improves data\nquality by up to 33.3%.",
    "updated" : "2025-05-07T12:20:55Z",
    "published" : "2025-05-07T12:20:55Z",
    "authors" : [
      {
        "name" : "Lijian Wu"
      },
      {
        "name" : "Weikun Xie"
      },
      {
        "name" : "Wei Tan"
      },
      {
        "name" : "Tian Wang"
      },
      {
        "name" : "Houbing Herbert Song"
      },
      {
        "name" : "Anfeng Liu"
      }
    ],
    "categories" : [
      "cs.CE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04181v1",
    "title" : "Privacy Challenges In Image Processing Applications",
    "summary" : "As image processing systems proliferate, privacy concerns intensify given the\nsensitive personal information contained in images. This paper examines privacy\nchallenges in image processing and surveys emerging privacy-preserving\ntechniques including differential privacy, secure multiparty computation,\nhomomorphic encryption, and anonymization. Key applications with heightened\nprivacy risks include healthcare, where medical images contain patient health\ndata, and surveillance systems that can enable unwarranted tracking.\nDifferential privacy offers rigorous privacy guarantees by injecting controlled\nnoise, while MPC facilitates collaborative analytics without exposing raw data\ninputs. Homomorphic encryption enables computations on encrypted data and\nanonymization directly removes identifying elements. However, balancing privacy\nprotections and utility remains an open challenge. Promising future directions\nidentified include quantum-resilient cryptography, federated learning,\ndedicated hardware, and conceptual innovations like privacy by design.\nUltimately, a holistic effort combining technological innovations, ethical\nconsiderations, and policy frameworks is necessary to uphold the fundamental\nright to privacy as image processing capabilities continue advancing rapidly.",
    "updated" : "2025-05-07T07:28:03Z",
    "published" : "2025-05-07T07:28:03Z",
    "authors" : [
      {
        "name" : " Maneesha"
      },
      {
        "name" : "Bharat Gupta"
      },
      {
        "name" : "Rishabh Sethi"
      },
      {
        "name" : "Charvi Adita Das"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04034v1",
    "title" : "Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency,\n  and Transferability in Spiking Neural Networks",
    "summary" : "Biological neurons exhibit diverse temporal spike patterns, which are\nbelieved to support efficient, robust, and adaptive neural information\nprocessing. While models such as Izhikevich can replicate a wide range of these\nfiring dynamics, their complexity poses challenges for directly integrating\nthem into scalable spiking neural networks (SNN) training pipelines. In this\nwork, we propose two probabilistically driven, input-level temporal spike\ntransformations: Poisson-Burst and Delayed-Burst that introduce biologically\ninspired temporal variability directly into standard Leaky Integrate-and-Fire\n(LIF) neurons. This enables scalable training and systematic evaluation of how\nspike timing dynamics affect privacy, generalization, and learning performance.\nPoisson-Burst modulates burst occurrence based on input intensity, while\nDelayed-Burst encodes input strength through burst onset timing. Through\nextensive experiments across multiple benchmarks, we demonstrate that\nPoisson-Burst maintains competitive accuracy and lower resource overhead while\nexhibiting enhanced privacy robustness against membership inference attacks,\nwhereas Delayed-Burst provides stronger privacy protection at a modest accuracy\ntrade-off. These findings highlight the potential of biologically grounded\ntemporal spike dynamics in improving the privacy, generalization and biological\nplausibility of neuromorphic learning systems.",
    "updated" : "2025-05-07T00:27:00Z",
    "published" : "2025-05-07T00:27:00Z",
    "authors" : [
      {
        "name" : "Ayana Moshruba"
      },
      {
        "name" : "Hamed Poursiami"
      },
      {
        "name" : "Maryam Parsa"
      }
    ],
    "categories" : [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05214v1",
    "title" : "Overcoming the hurdle of legal expertise: A reusable model for\n  smartwatch privacy policies",
    "summary" : "Regulations for privacy protection aim to protect individuals from the\nunauthorized storage, processing, and transfer of their personal data but\noftentimes fail in providing helpful support for understanding these\nregulations. To better communicate privacy policies for smartwatches, we need\nan in-depth understanding of their concepts and provide better ways to enable\ndevelopers to integrate them when engineering systems. Up to now, no conceptual\nmodel exists covering privacy statements from different smartwatch\nmanufacturers that is reusable for developers. This paper introduces such a\nconceptual model for privacy policies of smartwatches and shows its use in a\nmodel-driven software engineering approach to create a platform for data\nvisualization of wearable privacy policies from different smartwatch\nmanufacturers. We have analyzed the privacy policies of various manufacturers\nand extracted the relevant concepts. Moreover, we have checked the model with\nlawyers for its correctness, instantiated it with concrete data, and used it in\na model-driven software engineering approach to create a platform for data\nvisualization. This reusable privacy policy model can enable developers to\neasily represent privacy policies in their systems. This provides a foundation\nfor more structured and understandable privacy policies which, in the long run,\ncan increase the data sovereignty of application users.",
    "updated" : "2025-05-08T13:09:12Z",
    "published" : "2025-05-08T13:09:12Z",
    "authors" : [
      {
        "name" : "Constantin Buschhaus"
      },
      {
        "name" : "Arvid Butting"
      },
      {
        "name" : "Judith Michael"
      },
      {
        "name" : "Verena Nitsch"
      },
      {
        "name" : "Sebastian Pütz"
      },
      {
        "name" : "Bernhard Rumpe"
      },
      {
        "name" : "Carolin Stellmacher"
      },
      {
        "name" : "Sabine Theis"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05155v1",
    "title" : "FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data\n  Preparation via Federated Learning",
    "summary" : "Trajectory data, which capture the movement patterns of people and vehicles\nover time and space, are crucial for applications like traffic optimization and\nurban planning. However, issues such as noise and incompleteness often\ncompromise data quality, leading to inaccurate trajectory analyses and limiting\nthe potential of these applications. While Trajectory Data Preparation (TDP)\ncan enhance data quality, existing methods suffer from two key limitations: (i)\nthey do not address data privacy concerns, particularly in federated settings\nwhere trajectory data sharing is prohibited, and (ii) they typically design\ntask-specific models that lack generalizability across diverse TDP scenarios.\nTo overcome these challenges, we propose FedTDP, a privacy-preserving and\nunified framework that leverages the capabilities of Large Language Models\n(LLMs) for TDP in federated environments. Specifically, we: (i) design a\ntrajectory privacy autoencoder to secure data transmission and protect privacy,\n(ii) introduce a trajectory knowledge enhancer to improve model learning of\nTDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)\npropose federated parallel optimization to enhance training efficiency by\nreducing data transmission and enabling parallel model training. Experiments on\n6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP\nconsistently outperforms 13 state-of-the-art baselines.",
    "updated" : "2025-05-08T11:51:23Z",
    "published" : "2025-05-08T11:51:23Z",
    "authors" : [
      {
        "name" : "Zhihao Zeng"
      },
      {
        "name" : "Ziquan Fang"
      },
      {
        "name" : "Wei Shao"
      },
      {
        "name" : "Lu Chen"
      },
      {
        "name" : "Yunjun Gao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05031v1",
    "title" : "LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving\n  Cloud-Device Collaboration",
    "summary" : "Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)\nfor handling public user queries and on-device Small Language Models (SLMs) for\nprocessing private user data, collectively forming a powerful and\nprivacy-preserving solution. However, existing approaches often fail to fully\nleverage the scalable problem-solving capabilities of on-cloud LLMs while\nunderutilizing the advantage of on-device SLMs in accessing and processing\npersonalized data. This leads to two interconnected issues: 1) Limited\nutilization of the problem-solving capabilities of on-cloud LLMs, which fail to\nalign with personalized user-task needs, and 2) Inadequate integration of user\ndata into on-device SLM responses, resulting in mismatches in contextual user\ninformation.\n  In this paper, we propose a Leader-Subordinate Retrieval framework for\nPrivacy-preserving cloud-device collaboration (LSRP), a novel solution that\nbridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM\nthrough a dynamic selection of task-specific leader strategies named as\nuser-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the\ndata advantages of on-device SLMs through small model feedback Direct\nPreference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the\non-device SLM. Experiments on two datasets demonstrate that LSRP consistently\noutperforms state-of-the-art baselines, significantly improving question-answer\nrelevance and personalization, while preserving user privacy through efficient\non-device retrieval. Our code is available at:\nhttps://github.com/Zhang-Yingyi/LSRP.",
    "updated" : "2025-05-08T08:06:34Z",
    "published" : "2025-05-08T08:06:34Z",
    "authors" : [
      {
        "name" : "Yingyi Zhang"
      },
      {
        "name" : "Pengyue Jia"
      },
      {
        "name" : "Xianneng Li"
      },
      {
        "name" : "Derong Xu"
      },
      {
        "name" : "Maolin Wang"
      },
      {
        "name" : "Yichao Wang"
      },
      {
        "name" : "Zhaocheng Du"
      },
      {
        "name" : "Huifeng Guo"
      },
      {
        "name" : "Yong Liu"
      },
      {
        "name" : "Ruiming Tang"
      },
      {
        "name" : "Xiangyu Zhao"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04889v1",
    "title" : "FedRE: Robust and Effective Federated Learning with Privacy Preference",
    "summary" : "Despite Federated Learning (FL) employing gradient aggregation at the server\nfor distributed training to prevent the privacy leakage of raw data, private\ninformation can still be divulged through the analysis of uploaded gradients\nfrom clients. Substantial efforts have been made to integrate local\ndifferential privacy (LDP) into the system to achieve a strict privacy\nguarantee. However, existing methods fail to take practical issues into account\nby merely perturbing each sample with the same mechanism while each client may\nhave their own privacy preferences on privacy-sensitive information (PSI),\nwhich is not uniformly distributed across the raw data. In such a case,\nexcessive privacy protection from private-insensitive information can\nadditionally introduce unnecessary noise, which may degrade the model\nperformance. In this work, we study the PSI within data and develop FedRE, that\ncan simultaneously achieve robustness and effectiveness benefits with LDP\nprotection. More specifically, we first define PSI with regard to the privacy\npreferences of each client. Then, we optimize the LDP by allocating less\nprivacy budget to gradients with higher PSI in a layer-wise manner, thus\nproviding a stricter privacy guarantee for PSI. Furthermore, to mitigate the\nperformance degradation caused by LDP, we design a parameter aggregation\nmechanism based on the distribution of the perturbed information. We conducted\nexperiments with text tamper detection on T-SROIE and DocTamper datasets, and\nFedRE achieves competitive performance compared to state-of-the-art methods.",
    "updated" : "2025-05-08T01:50:27Z",
    "published" : "2025-05-08T01:50:27Z",
    "authors" : [
      {
        "name" : "Tianzhe Xiao"
      },
      {
        "name" : "Yichen Li"
      },
      {
        "name" : "Yu Zhou"
      },
      {
        "name" : "Yining Qi"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Haozhao Wang"
      },
      {
        "name" : "Yi Wang"
      },
      {
        "name" : "Ruixuan Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04799v1",
    "title" : "Safeguard-by-Development: A Privacy-Enhanced Development Paradigm for\n  Multi-Agent Collaboration Systems",
    "summary" : "Multi-agent collaboration systems (MACS), powered by large language models\n(LLMs), solve complex problems efficiently by leveraging each agent's\nspecialization and communication between agents. However, the inherent exchange\nof information between agents and their interaction with external environments,\nsuch as LLM, tools, and users, inevitably introduces significant risks of\nsensitive data leakage, including vulnerabilities to attacks like prompt\ninjection and reconnaissance. Existing MACS fail to enable privacy controls,\nmaking it challenging to manage sensitive information securely. In this paper,\nwe take the first step to address the MACS's data leakage threat at the system\ndevelopment level through a privacy-enhanced development paradigm, Maris. Maris\nenables rigorous message flow control within MACS by embedding reference\nmonitors into key multi-agent conversation components. We implemented Maris as\nan integral part of AutoGen, a widely adopted open-source multi-agent\ndevelopment framework. Then, we evaluate Maris for its effectiveness and\nperformance overhead on privacy-critical MACS use cases, including healthcare,\nsupply chain optimization, and personalized recommendation system. The result\nshows that Maris achieves satisfactory effectiveness, performance overhead and\npracticability for adoption.",
    "updated" : "2025-05-07T20:54:43Z",
    "published" : "2025-05-07T20:54:43Z",
    "authors" : [
      {
        "name" : "Jian Cui"
      },
      {
        "name" : "Zichuan Li"
      },
      {
        "name" : "Luyi Xing"
      },
      {
        "name" : "Xiaojing Liao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.06122v1",
    "title" : "Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled\n  Systems via Particle Filter Reinforcement Learning",
    "summary" : "This paper addresses the problem of parameter privacy-preserving data sharing\nin coupled systems, where a data provider shares data with a data user but\nwants to protect its sensitive parameters. The shared data affects not only the\ndata user's decision-making but also the data provider's operations through\nsystem interactions. To trade off control performance and privacy, we propose\nan interaction-aware privacy-preserving data sharing approach. Our approach\ngenerates distorted data by minimizing a combination of (i) mutual information,\nquantifying privacy leakage of sensitive parameters, and (ii) the impact of\ndistorted data on the data provider's control performance, considering the\ninteractions between stakeholders. The optimization problem is formulated into\na Bellman equation and solved by a particle filter reinforcement learning\n(RL)-based approach. Compared to existing RL-based methods, our formulation\nsignificantly reduces history dependency and efficiently handles scenarios with\ncontinuous state space. Validated in a mixed-autonomy platoon scenario, our\nmethod effectively protects sensitive driving behavior parameters of\nhuman-driven vehicles (HDVs) against inference attacks while maintaining\nnegligible impact on fuel efficiency.",
    "updated" : "2025-05-09T15:25:48Z",
    "published" : "2025-05-09T15:25:48Z",
    "authors" : [
      {
        "name" : "Haokun Yu"
      },
      {
        "name" : "Jingyuan Zhou"
      },
      {
        "name" : "Kaidi Yang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05922v1",
    "title" : "CAPE: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy",
    "summary" : "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.",
    "updated" : "2025-05-09T09:54:07Z",
    "published" : "2025-05-09T09:54:07Z",
    "authors" : [
      {
        "name" : "Haoqi Wu"
      },
      {
        "name" : "Wei Dai"
      },
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Qiang Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05920v1",
    "title" : "Privacy-Preserving Credit Card Approval Using Homomorphic SVM: Toward\n  Secure Inference in FinTech Applications",
    "summary" : "The growing use of machine learning in cloud environments raises critical\nconcerns about data security and privacy, especially in finance. Fully\nHomomorphic Encryption (FHE) offers a solution by enabling computations on\nencrypted data, but its high computational cost limits practicality. In this\npaper, we propose PP-FinTech, a privacy-preserving scheme for financial\napplications that employs a CKKS-based encrypted soft-margin SVM, enhanced with\na hybrid kernel for modeling non-linear patterns and an adaptive thresholding\nmechanism for robust encrypted classification. Experiments on the Credit Card\nApproval dataset demonstrate comparable performance to the plaintext models,\nhighlighting PP-FinTech's ability to balance privacy, and efficiency in secure\nfinancial ML systems.",
    "updated" : "2025-05-09T09:46:56Z",
    "published" : "2025-05-09T09:46:56Z",
    "authors" : [
      {
        "name" : " Faneela"
      },
      {
        "name" : "Baraq Ghaleb"
      },
      {
        "name" : "Jawad Ahmad"
      },
      {
        "name" : "William J. Buchanan"
      },
      {
        "name" : "Sana Ullah Jan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05859v1",
    "title" : "Integrating Building Thermal Flexibility Into Distribution System: A\n  Privacy-Preserved Dispatch Approach",
    "summary" : "The inherent thermal storage capacity of buildings brings considerable\nthermal flexibility to the heating/cooling loads, which are promising demand\nresponse resources for power systems. It is widely believed that integrating\nthe thermal flexibility of buildings into the distribution system can improve\nthe operating economy and reliability of the system. However, the private\ninformation of the buildings needs to be transferred to the distribution system\noperator (DSO) to achieve a coordinated optimization, bringing serious privacy\nconcerns to users. Given this issue, we propose a novel privacy-preserved\noptimal dispatch approach for the distribution system incorporating buildings.\nUsing it, the DSO can exploit the thermal flexibility of buildings without\naccessing their private information, such as model parameters and indoor\ntemperature profiles. Specifically, we first develop an optimal dispatch model\nfor the distribution system integrating buildings, which can be extended to\nother storage-like flexibility resources. Second, we reveal that the\nprivacy-preserved integration of buildings is a joint privacy preservation\nproblem for both parameters and state variables and then design a\nprivacy-preserved algorithm based on transformation-based encryption,\nconstraint relaxation, and constraint extension techniques. Besides, we\nimplement a detailed privacy analysis for the proposed method, considering both\nsemi-honest adversaries and external eavesdroppers. Case studies demonstrate\nthe accuracy, privacy-preserved performance, and computational efficiency of\nthe proposed method.",
    "updated" : "2025-05-09T07:53:08Z",
    "published" : "2025-05-09T07:53:08Z",
    "authors" : [
      {
        "name" : "Shuai Lu"
      },
      {
        "name" : "Zeyin Hou"
      },
      {
        "name" : "Wei Gu"
      },
      {
        "name" : "Yijun Xu"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05843v1",
    "title" : "Enhancing Noisy Functional Encryption for Privacy-Preserving Machine\n  Learning",
    "summary" : "Functional encryption (FE) has recently attracted interest in\nprivacy-preserving machine learning (PPML) for its unique ability to compute\nspecific functions on encrypted data. A related line of work focuses on noisy\nFE, which ensures differential privacy in the output while keeping the data\nencrypted. We extend the notion of noisy multi-input functional encryption\n(NMIFE) to (dynamic) noisy multi-client functional encryption ((Dy)NMCFE),\nwhich allows for more flexibility in the number of data holders and analyses,\nwhile protecting the privacy of the data holder with fine-grained access\nthrough the usage of labels. Following our new definition of DyNMCFE, we\npresent DyNo, a concrete inner-product DyNMCFE scheme. Our scheme captures all\nthe functionalities previously introduced in noisy FE schemes, while being\nsignificantly more efficient in terms of space and runtime and fulfilling a\nstronger security notion by allowing the corruption of clients. To further\nprove the applicability of DyNMCFE, we present a protocol for PPML based on\nDyNo. According to this protocol, we train a privacy-preserving logistic\nregression.",
    "updated" : "2025-05-09T07:33:09Z",
    "published" : "2025-05-09T07:33:09Z",
    "authors" : [
      {
        "name" : "Linda Scheu-Hachtel"
      },
      {
        "name" : "Jasmin Zalonis"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05816v1",
    "title" : "On the Price of Differential Privacy for Spectral Clustering over\n  Stochastic Block Models",
    "summary" : "We investigate privacy-preserving spectral clustering for community detection\nwithin stochastic block models (SBMs). Specifically, we focus on edge\ndifferential privacy (DP) and propose private algorithms for community\nrecovery. Our work explores the fundamental trade-offs between the privacy\nbudget and the accurate recovery of community labels. Furthermore, we establish\ninformation-theoretic conditions that guarantee the accuracy of our methods,\nproviding theoretical assurances for successful community recovery under edge\nDP.",
    "updated" : "2025-05-09T06:34:56Z",
    "published" : "2025-05-09T06:34:56Z",
    "authors" : [
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Andrea J. Goldsmith"
      }
    ],
    "categories" : [
      "cs.SI",
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05707v1",
    "title" : "Crowding Out The Noise: Algorithmic Collective Action Under Differential\n  Privacy",
    "summary" : "The integration of AI into daily life has generated considerable attention\nand excitement, while also raising concerns about automating algorithmic harms\nand re-entrenching existing social inequities. While the responsible deployment\nof trustworthy AI systems is a worthy goal, there are many possible ways to\nrealize it, from policy and regulation to improved algorithm design and\nevaluation. In fact, since AI trains on social data, there is even a\npossibility for everyday users, citizens, or workers to directly steer its\nbehavior through Algorithmic Collective Action, by deliberately modifying the\ndata they share with a platform to drive its learning process in their favor.\nThis paper considers how these grassroots efforts to influence AI interact with\nmethods already used by AI firms and governments to improve model\ntrustworthiness. In particular, we focus on the setting where the AI firm\ndeploys a differentially private model, motivated by the growing regulatory\nfocus on privacy and data protection. We investigate how the use of\nDifferentially Private Stochastic Gradient Descent (DPSGD) affects the\ncollective's ability to influence the learning process. Our findings show that\nwhile differential privacy contributes to the protection of individual data, it\nintroduces challenges for effective algorithmic collective action. We\ncharacterize lower bounds on the success of algorithmic collective action under\ndifferential privacy as a function of the collective's size and the firm's\nprivacy parameters, and verify these trends experimentally by simulating\ncollective action during the training of deep neural network classifiers across\nseveral datasets.",
    "updated" : "2025-05-09T00:55:12Z",
    "published" : "2025-05-09T00:55:12Z",
    "authors" : [
      {
        "name" : "Rushabh Solanki"
      },
      {
        "name" : "Meghana Bhange"
      },
      {
        "name" : "Ulrich Aïvodji"
      },
      {
        "name" : "Elliot Creager"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05648v1",
    "title" : "Privacy-Preserving Transformers: SwiftKey's Differential Privacy\n  Implementation",
    "summary" : "In this paper we train a transformer using differential privacy (DP) for\nlanguage modeling in SwiftKey. We run multiple experiments to balance the\ntrade-off between the model size, run-time speed and accuracy. We show that we\nget small and consistent gains in the next-word-prediction and accuracy with\ngraceful increase in memory and speed compared to the production GRU. This is\nobtained by scaling down a GPT2 architecture to fit the required size and a two\nstage training process that builds a seed model on general data and DP\nfinetunes it on typing data. The transformer is integrated using ONNX offering\nboth flexibility and efficiency.",
    "updated" : "2025-05-08T21:08:04Z",
    "published" : "2025-05-08T21:08:04Z",
    "authors" : [
      {
        "name" : "Abdelrahman Abouelenin"
      },
      {
        "name" : "Mohamed Abdelrehim"
      },
      {
        "name" : "Raffy Fahim"
      },
      {
        "name" : "Amr Hendy"
      },
      {
        "name" : "Mohamed Afify"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05613v1",
    "title" : "Optimal Regret of Bernoulli Bandits under Global Differential Privacy",
    "summary" : "As sequential learning algorithms are increasingly applied to real life,\nensuring data privacy while maintaining their utilities emerges as a timely\nquestion. In this context, regret minimisation in stochastic bandits under\n$\\epsilon$-global Differential Privacy (DP) has been widely studied. Unlike\nbandits without DP, there is a significant gap between the best-known regret\nlower and upper bound in this setting, though they \"match\" in order. Thus, we\nrevisit the regret lower and upper bounds of $\\epsilon$-global DP algorithms\nfor Bernoulli bandits and improve both. First, we prove a tighter regret lower\nbound involving a novel information-theoretic quantity characterising the\nhardness of $\\epsilon$-global DP in stochastic bandits. Our lower bound\nstrictly improves on the existing ones across all $\\epsilon$ values. Then, we\nchoose two asymptotically optimal bandit algorithms, i.e. DP-KLUCB and DP-IMED,\nand propose their DP versions using a unified blueprint, i.e., (a) running in\narm-dependent phases, and (b) adding Laplace noise to achieve privacy. For\nBernoulli bandits, we analyse the regrets of these algorithms and show that\ntheir regrets asymptotically match our lower bound up to a constant arbitrary\nclose to 1. This refutes the conjecture that forgetting past rewards is\nnecessary to design optimal bandit algorithms under global DP. At the core of\nour algorithms lies a new concentration inequality for sums of Bernoulli\nvariables under Laplace mechanism, which is a new DP version of the Chernoff\nbound. This result is universally useful as the DP literature commonly treats\nthe concentrations of Laplace noise and random variables separately, while we\ncouple them to yield a tighter bound.",
    "updated" : "2025-05-08T19:48:58Z",
    "published" : "2025-05-08T19:48:58Z",
    "authors" : [
      {
        "name" : "Achraf Azize"
      },
      {
        "name" : "Yulian Wu"
      },
      {
        "name" : "Junya Honda"
      },
      {
        "name" : "Francesco Orabona"
      },
      {
        "name" : "Shinji Ito"
      },
      {
        "name" : "Debabrota Basu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05519v1",
    "title" : "Real-Time Privacy Preservation for Robot Visual Perception",
    "summary" : "Many robots (e.g., iRobot's Roomba) operate based on visual observations from\nlive video streams, and such observations may inadvertently include\nprivacy-sensitive objects, such as personal identifiers. Existing approaches\nfor preserving privacy rely on deep learning models, differential privacy, or\ncryptography. They lack guarantees for the complete concealment of all\nsensitive objects. Guaranteeing concealment requires post-processing techniques\nand thus is inadequate for real-time video streams. We develop a method for\nprivacy-constrained video streaming, PCVS, that conceals sensitive objects\nwithin real-time video streams. PCVS takes a logical specification constraining\nthe existence of privacy-sensitive objects, e.g., never show faces when a\nperson exists. It uses a detection model to evaluate the existence of these\nobjects in each incoming frame. Then, it blurs out a subset of objects such\nthat the existence of the remaining objects satisfies the specification. We\nthen propose a conformal prediction approach to (i) establish a theoretical\nlower bound on the probability of the existence of these objects in a sequence\nof frames satisfying the specification and (ii) update the bound with the\narrival of each subsequent frame. Quantitative evaluations show that PCVS\nachieves over 95 percent specification satisfaction rate in multiple datasets,\nsignificantly outperforming other methods. The satisfaction rate is\nconsistently above the theoretical bounds across all datasets, indicating that\nthe established bounds hold. Additionally, we deploy PCVS on robots in\nreal-time operation and show that the robots operate normally without being\ncompromised when PCVS conceals objects.",
    "updated" : "2025-05-08T03:27:12Z",
    "published" : "2025-05-08T03:27:12Z",
    "authors" : [
      {
        "name" : "Minkyu Choi"
      },
      {
        "name" : "Yunhao Yang"
      },
      {
        "name" : "Neel P. Bhatt"
      },
      {
        "name" : "Kushagra Gupta"
      },
      {
        "name" : "Sahil Shah"
      },
      {
        "name" : "Aditya Rai"
      },
      {
        "name" : "David Fridovich-Keil"
      },
      {
        "name" : "Ufuk Topcu"
      },
      {
        "name" : "Sandeep P. Chinchali"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02392v2",
    "title" : "Moneros Decentralized P2P Exchanges: Functionality, Adoption, and\n  Privacy Risks",
    "summary" : "Privacy-focused cryptocurrencies like Monero remain popular, despite\nincreasing regulatory scrutiny that has led to their delisting from major\ncentralized exchanges. The latter also explains the recent popularity of\ndecentralized exchanges (DEXs) with no centralized ownership structures. These\nplatforms typically leverage peer-to-peer (P2P) networks, promising secure and\nanonymous asset trading. However, questions of liability remain, and the\nacademic literature lacks comprehensive insights into the functionality,\ntrading activity, and privacy claims of these P2P platforms. In this paper, we\nprovide an early systematization of the current landscape of decentralized\npeer-to-peer exchanges within the Monero ecosystem. We examine several recently\ndeveloped DEX platforms, analyzing their popularity, functionality,\narchitectural choices, and potential weaknesses. We further identify and report\non a privacy vulnerability in the recently popularized Haveno exchange,\ndemonstrating that certain Haveno trades could be detected, allowing\ntransactions to be linked across the Monero and Bitcoin blockchains. We hope\nthat our findings can nourish the discussion in the research community about\nmore secure designs, and provide insights for regulators.",
    "updated" : "2025-05-09T15:32:20Z",
    "published" : "2025-05-05T06:27:37Z",
    "authors" : [
      {
        "name" : "Yannik Kopyciok"
      },
      {
        "name" : "Friedhelm Victor"
      },
      {
        "name" : "Stefan Schmid"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07766v1",
    "title" : "Privacy Risks of Robot Vision: A User Study on Image Modalities and\n  Resolution",
    "summary" : "User privacy is a crucial concern in robotic applications, especially when\nmobile service robots are deployed in personal or sensitive environments.\nHowever, many robotic downstream tasks require the use of cameras, which may\nraise privacy risks. To better understand user perceptions of privacy in\nrelation to visual data, we conducted a user study investigating how different\nimage modalities and image resolutions affect users' privacy concerns. The\nresults show that depth images are broadly viewed as privacy-safe, and a\nsimilarly high proportion of respondents feel the same about semantic\nsegmentation images. Additionally, the majority of participants consider 32*32\nresolution RGB images to be almost sufficiently privacy-preserving, while most\nbelieve that 16*16 resolution can fully guarantee privacy protection.",
    "updated" : "2025-05-12T17:16:12Z",
    "published" : "2025-05-12T17:16:12Z",
    "authors" : [
      {
        "name" : "Xuying Huang"
      },
      {
        "name" : "Sicong Pan"
      },
      {
        "name" : "Maren Bennewitz"
      }
    ],
    "categories" : [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07672v1",
    "title" : "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit",
    "summary" : "We present OnPrem.LLM, a Python-based toolkit for applying large language\nmodels (LLMs) to sensitive, non-public data in offline or restricted\nenvironments. The system is designed for privacy-preserving use cases and\nprovides prebuilt pipelines for document processing and storage,\nretrieval-augmented generation (RAG), information extraction, summarization,\nclassification, and prompt/output processing with minimal configuration.\nOnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM,\nand Hugging Face Transformers -- with quantized model support, GPU\nacceleration, and seamless backend switching. Although designed for fully local\nexecution, OnPrem.LLM also supports integration with a wide range of cloud LLM\nproviders when permitted, enabling hybrid deployments that balance performance\nwith data control. A no-code web interface extends accessibility to\nnon-technical users.",
    "updated" : "2025-05-12T15:36:27Z",
    "published" : "2025-05-12T15:36:27Z",
    "authors" : [
      {
        "name" : "Arun S. Maiya"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07583v1",
    "title" : "Privacy-Preserving Real-Time Vietnamese-English Translation on iOS using\n  Edge AI",
    "summary" : "This research addresses the growing need for privacy-preserving and\naccessible language translation by developing a fully offline Neural Machine\nTranslation (NMT) system for Vietnamese-English translation on iOS devices.\nGiven increasing concerns about data privacy and unreliable network\nconnectivity, on-device translation offers critical advantages. This project\nconfronts challenges in deploying complex NMT models on resource-limited mobile\ndevices, prioritizing efficiency, accuracy, and a seamless user experience.\nLeveraging advances such as MobileBERT and, specifically, the lightweight\n\\textbf{TinyLlama 1.1B Chat v1.0} in GGUF format, \\textbf{a} quantized\nTransformer-based model is implemented and optimized. The application is\nrealized as a real-time iOS prototype, tightly integrating modern iOS\nframeworks and privacy-by-design principles. Comprehensive documentation covers\nmodel selection, technical architecture, challenges, and final implementation,\nincluding functional Swift code for deployment.",
    "updated" : "2025-05-12T14:05:39Z",
    "published" : "2025-05-12T14:05:39Z",
    "authors" : [
      {
        "name" : "Cong Le"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07306v1",
    "title" : "Enabling Privacy-Aware AI-Based Ergonomic Analysis",
    "summary" : "Musculoskeletal disorders (MSDs) are a leading cause of injury and\nproductivity loss in the manufacturing industry, incurring substantial economic\ncosts. Ergonomic assessments can mitigate these risks by identifying workplace\nadjustments that improve posture and reduce strain. Camera-based systems offer\na non-intrusive, cost-effective method for continuous ergonomic tracking, but\nthey also raise significant privacy concerns. To address this, we propose a\nprivacy-aware ergonomic assessment framework utilizing machine learning\ntechniques. Our approach employs adversarial training to develop a lightweight\nneural network that obfuscates video data, preserving only the essential\ninformation needed for human pose estimation. This obfuscation ensures\ncompatibility with standard pose estimation algorithms, maintaining high\naccuracy while protecting privacy. The obfuscated video data is transmitted to\na central server, where state-of-the-art keypoint detection algorithms extract\nbody landmarks. Using multi-view integration, 3D keypoints are reconstructed\nand evaluated with the Rapid Entire Body Assessment (REBA) method. Our system\nprovides a secure, effective solution for ergonomic monitoring in industrial\nenvironments, addressing both privacy and workplace safety concerns.",
    "updated" : "2025-05-12T07:52:48Z",
    "published" : "2025-05-12T07:52:48Z",
    "authors" : [
      {
        "name" : "Sander De Coninck"
      },
      {
        "name" : "Emilio Gamba"
      },
      {
        "name" : "Bart Van Doninck"
      },
      {
        "name" : "Abdellatif Bey-Temsamani"
      },
      {
        "name" : "Sam Leroux"
      },
      {
        "name" : "Pieter Simoens"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07085v1",
    "title" : "Privacy of Groups in Dense Street Imagery",
    "summary" : "Spatially and temporally dense street imagery (DSI) datasets have grown\nunbounded. In 2024, individual companies possessed around 3 trillion unique\nimages of public streets. DSI data streams are only set to grow as companies\nlike Lyft and Waymo use DSI to train autonomous vehicle algorithms and analyze\ncollisions. Academic researchers leverage DSI to explore novel approaches to\nurban analysis. Despite good-faith efforts by DSI providers to protect\nindividual privacy through blurring faces and license plates, these measures\nfail to address broader privacy concerns. In this work, we find that increased\ndata density and advancements in artificial intelligence enable harmful group\nmembership inferences from supposedly anonymized data. We perform a penetration\ntest to demonstrate how easily sensitive group affiliations can be inferred\nfrom obfuscated pedestrians in 25,232,608 dashcam images taken in New York\nCity. We develop a typology of identifiable groups within DSI and analyze\nprivacy implications through the lens of contextual integrity. Finally, we\ndiscuss actionable recommendations for researchers working with data from DSI\nproviders.",
    "updated" : "2025-05-11T18:16:08Z",
    "published" : "2025-05-11T18:16:08Z",
    "authors" : [
      {
        "name" : "Matt Franchi"
      },
      {
        "name" : "Hauke Sandhaus"
      },
      {
        "name" : "Madiha Zahrah Choksi"
      },
      {
        "name" : "Severin Engelmann"
      },
      {
        "name" : "Wendy Ju"
      },
      {
        "name" : "Helen Nissenbaum"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CV",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07041v1",
    "title" : "Empirical Analysis of Asynchronous Federated Learning on Heterogeneous\n  Devices: Efficiency, Fairness, and Privacy Trade-offs",
    "summary" : "Device heterogeneity poses major challenges in Federated Learning (FL), where\nresource-constrained clients slow down synchronous schemes that wait for all\nupdates before aggregation. Asynchronous FL addresses this by incorporating\nupdates as they arrive, substantially improving efficiency. While its\nefficiency gains are well recognized, its privacy costs remain largely\nunexplored, particularly for high-end devices that contribute updates more\nfrequently, increasing their cumulative privacy exposure. This paper presents\nthe first comprehensive analysis of the efficiency-fairness-privacy trade-off\nin synchronous vs. asynchronous FL under realistic device heterogeneity. We\nempirically compare FedAvg and staleness-aware FedAsync using a physical\ntestbed of five edge devices spanning diverse hardware tiers, integrating Local\nDifferential Privacy (LDP) and the Moments Accountant to quantify per-client\nprivacy loss. Using Speech Emotion Recognition (SER) as a privacy-critical\nbenchmark, we show that FedAsync achieves up to 10x faster convergence but\nexacerbates fairness and privacy disparities: high-end devices contribute 6-10x\nmore updates and incur up to 5x higher privacy loss, while low-end devices\nsuffer amplified accuracy degradation due to infrequent, stale, and\nnoise-perturbed updates. These findings motivate the need for adaptive FL\nprotocols that jointly optimize aggregation and privacy mechanisms based on\nclient capacity and participation dynamics, moving beyond static,\none-size-fits-all solutions.",
    "updated" : "2025-05-11T16:25:06Z",
    "published" : "2025-05-11T16:25:06Z",
    "authors" : [
      {
        "name" : "Samaneh Mohammadi"
      },
      {
        "name" : "Iraklis Symeonidis"
      },
      {
        "name" : "Ali Balador"
      },
      {
        "name" : "Francesco Flammini"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.06860v1",
    "title" : "DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial\n  Example for Image Privacy Protection",
    "summary" : "In the field of digital security, Reversible Adversarial Examples (RAE)\ncombine adversarial attacks with reversible data hiding techniques to\neffectively protect sensitive data and prevent unauthorized analysis by\nmalicious Deep Neural Networks (DNNs). However, existing RAE techniques\nprimarily focus on white-box attacks, lacking a comprehensive evaluation of\ntheir effectiveness in black-box scenarios. This limitation impedes their\nbroader deployment in complex, dynamic environments. Further more, traditional\nblack-box attacks are often characterized by poor transferability and high\nquery costs, significantly limiting their practical applicability. To address\nthese challenges, we propose the Dual-Phase Merging Transferable Reversible\nAttack method, which generates highly transferable initial adversarial\nperturbations in a white-box model and employs a memory augmented black-box\nstrategy to effectively mislead target mod els. Experimental results\ndemonstrate the superiority of our approach, achieving a 99.0% attack success\nrate and 100% recovery rate in black-box scenarios, highlighting its robustness\nin privacy protection. Moreover, we successfully implemented a black-box attack\non a commercial model, further substantiating the potential of this approach\nfor practical use.",
    "updated" : "2025-05-11T06:11:10Z",
    "published" : "2025-05-11T06:11:10Z",
    "authors" : [
      {
        "name" : "Xia Du"
      },
      {
        "name" : "Jiajie Zhu"
      },
      {
        "name" : "Jizhe Zhou"
      },
      {
        "name" : "Chi-man Pun"
      },
      {
        "name" : "Zheng Lin"
      },
      {
        "name" : "Cong Wu"
      },
      {
        "name" : "Zhe Chen"
      },
      {
        "name" : "Jun Luo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.06759v1",
    "title" : "Privacy-aware Berrut Approximated Coded Computing applied to general\n  distributed learning",
    "summary" : "Coded computing is one of the techniques that can be used for privacy\nprotection in Federated Learning. However, most of the constructions used for\ncoded computing work only under the assumption that the computations involved\nare exact, generally restricted to special classes of functions, and require\nquantized inputs. This paper considers the use of Private Berrut Approximate\nCoded Computing (PBACC) as a general solution to add strong but non-perfect\nprivacy to federated learning. We derive new adapted PBACC algorithms for\ncentralized aggregation, secure distributed training with centralized data, and\nsecure decentralized training with decentralized data, thus enlarging\nsignificantly the applications of the method and the existing privacy\nprotection tools available for these paradigms. Particularly, PBACC can be used\nrobustly to attain privacy guarantees in decentralized federated learning for a\nvariety of models. Our numerical results show that the achievable quality of\ndifferent learning models (convolutional neural networks, variational\nautoencoders, and Cox regression) is minimally altered by using these new\ncomputing schemes, and that the privacy leakage can be bounded strictly to less\nthan a fraction of one bit per participant. Additionally, the computational\ncost of the encoding and decoding processes depends only of the degree of\ndecentralization of the data.",
    "updated" : "2025-05-10T21:27:40Z",
    "published" : "2025-05-10T21:27:40Z",
    "authors" : [
      {
        "name" : "Xavier Martínez-Luaña"
      },
      {
        "name" : "Manuel Fernández-Veiga"
      },
      {
        "name" : "Rebeca P. Díaz-Redondo"
      },
      {
        "name" : "Ana Fernández-Vilas"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.06747v1",
    "title" : "DPolicy: Managing Privacy Risks Across Multiple Releases with\n  Differential Privacy",
    "summary" : "Differential Privacy (DP) has emerged as a robust framework for\nprivacy-preserving data releases and has been successfully applied in\nhigh-profile cases, such as the 2020 US Census. However, in organizational\nsettings, the use of DP remains largely confined to isolated data releases.\nThis approach restricts the potential of DP to serve as a framework for\ncomprehensive privacy risk management at an organizational level. Although one\nmight expect that the cumulative privacy risk of isolated releases could be\nassessed using DP's compositional property, in practice, individual DP\nguarantees are frequently tailored to specific releases, making it difficult to\nreason about their interaction or combined impact. At the same time, less\ntailored DP guarantees, which compose more easily, also offer only limited\ninsight because they lead to excessively large privacy budgets that convey\nlimited meaning. To address these limitations, we present DPolicy, a system\ndesigned to manage cumulative privacy risks across multiple data releases using\nDP. Unlike traditional approaches that treat each release in isolation or rely\non a single (global) DP guarantee, our system employs a flexible framework that\nconsiders multiple DP guarantees simultaneously, reflecting the diverse\ncontexts and scopes typical of real-world DP deployments. DPolicy introduces a\nhigh-level policy language to formalize privacy guarantees, making\ntraditionally implicit assumptions on scopes and contexts explicit. By deriving\nthe DP guarantees required to enforce complex privacy semantics from these\nhigh-level policies, DPolicy enables fine-grained privacy risk management on an\norganizational scale. We implement and evaluate DPolicy, demonstrating how it\nmitigates privacy risks that can emerge without comprehensive,\norganization-wide privacy risk management.",
    "updated" : "2025-05-10T19:49:51Z",
    "published" : "2025-05-10T19:49:51Z",
    "authors" : [
      {
        "name" : "Nicolas Küchler"
      },
      {
        "name" : "Alexander Viand"
      },
      {
        "name" : "Hidde Lycklama"
      },
      {
        "name" : "Anwar Hithnawi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.06305v1",
    "title" : "User Behavior Analysis in Privacy Protection with Large Language Models:\n  A Study on Privacy Preferences with Limited Data",
    "summary" : "With the widespread application of large language models (LLMs), user privacy\nprotection has become a significant research topic. Existing privacy preference\nmodeling methods often rely on large-scale user data, making effective privacy\npreference analysis challenging in data-limited environments. This study\nexplores how LLMs can analyze user behavior related to privacy protection in\nscenarios with limited data and proposes a method that integrates Few-shot\nLearning and Privacy Computing to model user privacy preferences. The research\nutilizes anonymized user privacy settings data, survey responses, and simulated\ndata, comparing the performance of traditional modeling approaches with\nLLM-based methods. Experimental results demonstrate that, even with limited\ndata, LLMs significantly improve the accuracy of privacy preference modeling.\nAdditionally, incorporating Differential Privacy and Federated Learning further\nreduces the risk of user data exposure. The findings provide new insights into\nthe application of LLMs in privacy protection and offer theoretical support for\nadvancing privacy computing and user behavior analysis.",
    "updated" : "2025-05-08T04:42:17Z",
    "published" : "2025-05-08T04:42:17Z",
    "authors" : [
      {
        "name" : "Haowei Yang"
      },
      {
        "name" : "Qingyi Lu"
      },
      {
        "name" : "Yang Wang"
      },
      {
        "name" : "Sibei Liu"
      },
      {
        "name" : "Jiayun Zheng"
      },
      {
        "name" : "Ao Xiang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.08719v1",
    "title" : "PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts",
    "summary" : "Large language models (LLMs) hosted on cloud servers alleviate the\ncomputational and storage burdens on local devices but raise privacy concerns\ndue to sensitive data transmission and require substantial communication\nbandwidth, which is challenging in constrained environments. In contrast, small\nlanguage models (SLMs) running locally enhance privacy but suffer from limited\nperformance on complex tasks. To balance computational cost, performance, and\nprivacy protection under bandwidth constraints, we propose a privacy-aware\nwireless collaborative mixture of experts (PWC-MoE) framework. Specifically,\nPWC-MoE employs a sparse privacy-aware gating network to dynamically route\nsensitive tokens to privacy experts located on local clients, while\nnon-sensitive tokens are routed to non-privacy experts located at the remote\nbase station. To achieve computational efficiency, the gating network ensures\nthat each token is dynamically routed to and processed by only one expert. To\nenhance scalability and prevent overloading of specific experts, we introduce a\ngroup-wise load-balancing mechanism for the gating network that evenly\ndistributes sensitive tokens among privacy experts and non-sensitive tokens\namong non-privacy experts. To adapt to bandwidth constraints while preserving\nmodel performance, we propose a bandwidth-adaptive and importance-aware token\noffloading scheme. This scheme incorporates an importance predictor to evaluate\nthe importance scores of non-sensitive tokens, prioritizing the most important\ntokens for transmission to the base station based on their predicted importance\nand the available bandwidth. Experiments demonstrate that the PWC-MoE framework\neffectively preserves privacy and maintains high performance even in\nbandwidth-constrained environments, offering a practical solution for deploying\nLLMs in privacy-sensitive and bandwidth-limited scenarios.",
    "updated" : "2025-05-13T16:27:07Z",
    "published" : "2025-05-13T16:27:07Z",
    "authors" : [
      {
        "name" : "Yang Su"
      },
      {
        "name" : "Na Yan"
      },
      {
        "name" : "Yansha Deng"
      },
      {
        "name" : "Robert Schober"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.08237v1",
    "title" : "Privacy-Preserving Analytics for Smart Meter (AMI) Data: A Hybrid\n  Approach to Comply with CPUC Privacy Regulations",
    "summary" : "Advanced Metering Infrastructure (AMI) data from smart electric and gas\nmeters enables valuable insights for utilities and consumers, but also raises\nsignificant privacy concerns. In California, regulatory decisions (CPUC\nD.11-07-056 and D.11-08-045) mandate strict privacy protections for customer\nenergy usage data, guided by the Fair Information Practice Principles (FIPPs).\nWe comprehensively explore solutions drawn from data anonymization,\nprivacy-preserving machine learning (differential privacy and federated\nlearning), synthetic data generation, and cryptographic techniques (secure\nmultiparty computation, homomorphic encryption). This allows advanced\nanalytics, including machine learning models, statistical and econometric\nanalysis on energy consumption data, to be performed without compromising\nindividual privacy.\n  We evaluate each technique's theoretical foundations, effectiveness, and\ntrade-offs in the context of utility data analytics, and we propose an\nintegrated architecture that combines these methods to meet real-world needs.\nThe proposed hybrid architecture is designed to ensure compliance with\nCalifornia's privacy rules and FIPPs while enabling useful analytics, from\nforecasting and personalized insights to academic research and econometrics,\nwhile strictly protecting individual privacy. Mathematical definitions and\nderivations are provided where appropriate to demonstrate privacy guarantees\nand utility implications rigorously. We include comparative evaluations of the\ntechniques, an architecture diagram, and flowcharts to illustrate how they work\ntogether in practice. The result is a blueprint for utility data scientists and\nengineers to implement privacy-by-design in AMI data handling, supporting both\ndata-driven innovation and strict regulatory compliance.",
    "updated" : "2025-05-13T05:30:35Z",
    "published" : "2025-05-13T05:30:35Z",
    "authors" : [
      {
        "name" : "Benjamin Westrich"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07672v2",
    "title" : "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit",
    "summary" : "We present OnPrem$.$LLM, a Python-based toolkit for applying large language\nmodels (LLMs) to sensitive, non-public data in offline or restricted\nenvironments. The system is designed for privacy-preserving use cases and\nprovides prebuilt pipelines for document processing and storage,\nretrieval-augmented generation (RAG), information extraction, summarization,\nclassification, and prompt/output processing with minimal configuration.\nOnPrem$.$LLM supports multiple LLM backends -- including llama$.$cpp, Ollama,\nvLLM, and Hugging Face Transformers -- with quantized model support, GPU\nacceleration, and seamless backend switching. Although designed for fully local\nexecution, OnPrem$.$LLM also supports integration with a wide range of cloud\nLLM providers when permitted, enabling hybrid deployments that balance\nperformance with data control. A no-code web interface extends accessibility to\nnon-technical users.",
    "updated" : "2025-05-13T02:43:26Z",
    "published" : "2025-05-12T15:36:27Z",
    "authors" : [
      {
        "name" : "Arun S. Maiya"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07872v1",
    "title" : "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
    "summary" : "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
    "updated" : "2025-05-09T21:05:20Z",
    "published" : "2025-05-09T21:05:20Z",
    "authors" : [
      {
        "name" : "Yijing Zhang"
      },
      {
        "name" : "Ferdous Pervej"
      },
      {
        "name" : "Andreas F. Molisch"
      }
    ],
    "categories" : [
      "cs.NI",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.09276v1",
    "title" : "Privacy-Preserving Runtime Verification",
    "summary" : "Runtime verification offers scalable solutions to improve the safety and\nreliability of systems. However, systems that require verification or\nmonitoring by a third party to ensure compliance with a specification might\ncontain sensitive information, causing privacy concerns when usual runtime\nverification approaches are used. Privacy is compromised if protected\ninformation about the system, or sensitive data that is processed by the\nsystem, is revealed. In addition, revealing the specification being monitored\nmay undermine the essence of third-party verification.\n  In this work, we propose two novel protocols for the privacy-preserving\nruntime verification of systems against formal sequential specifications. In\nour first protocol, the monitor verifies whether the system satisfies the\nspecification without learning anything else, though both parties are aware of\nthe specification. Our second protocol ensures that the system remains\noblivious to the monitored specification, while the monitor learns only whether\nthe system satisfies the specification and nothing more. Our protocols adapt\nand improve existing techniques used in cryptography, and more specifically,\nmulti-party computation.\n  The sequential specification defines the observation step of the monitor,\nwhose granularity depends on the situation (e.g., banks may be monitored on a\ndaily basis). Our protocols exchange a single message per observation step,\nafter an initialisation phase. This design minimises communication overhead,\nenabling relatively lightweight privacy-preserving monitoring. We implement our\napproach for monitoring specifications described by register automata and\nevaluate it experimentally.",
    "updated" : "2025-05-14T10:49:07Z",
    "published" : "2025-05-14T10:49:07Z",
    "authors" : [
      {
        "name" : "Thomas A. Henzinger"
      },
      {
        "name" : "Mahyar Karimi"
      },
      {
        "name" : "K. S. Thejaswini"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.FL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.08847v1",
    "title" : "On the interplay of Explainability, Privacy and Predictive Performance\n  with Explanation-assisted Model Extraction",
    "summary" : "Machine Learning as a Service (MLaaS) has gained important attraction as a\nmeans for deploying powerful predictive models, offering ease of use that\nenables organizations to leverage advanced analytics without substantial\ninvestments in specialized infrastructure or expertise. However, MLaaS\nplatforms must be safeguarded against security and privacy attacks, such as\nmodel extraction (MEA) attacks. The increasing integration of explainable AI\n(XAI) within MLaaS has introduced an additional privacy challenge, as attackers\ncan exploit model explanations particularly counterfactual explanations (CFs)\nto facilitate MEA. In this paper, we investigate the trade offs among model\nperformance, privacy, and explainability when employing Differential Privacy\n(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two\ndistinct DP strategies: implemented during the classification model training\nand at the explainer during CF generation.",
    "updated" : "2025-05-13T15:27:06Z",
    "published" : "2025-05-13T15:27:06Z",
    "authors" : [
      {
        "name" : "Fatima Ezzeddine"
      },
      {
        "name" : "Rinad Akel"
      },
      {
        "name" : "Ihab Sbeity"
      },
      {
        "name" : "Silvia Giordano"
      },
      {
        "name" : "Marc Langheinrich"
      },
      {
        "name" : "Omran Ayoub"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.10496v1",
    "title" : "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of\n  Synthetic Chest Radiographs",
    "summary" : "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/",
    "updated" : "2025-05-15T16:59:17Z",
    "published" : "2025-05-15T16:59:17Z",
    "authors" : [
      {
        "name" : "Raman Dutt"
      },
      {
        "name" : "Pedro Sanchez"
      },
      {
        "name" : "Yongchen Yao"
      },
      {
        "name" : "Steven McDonagh"
      },
      {
        "name" : "Sotirios A. Tsaftaris"
      },
      {
        "name" : "Timothy Hospedales"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.10264v1",
    "title" : "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack\n  in Federated Learning",
    "summary" : "Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art.",
    "updated" : "2025-05-15T13:16:32Z",
    "published" : "2025-05-15T13:16:32Z",
    "authors" : [
      {
        "name" : "Francesco Diana"
      },
      {
        "name" : "André Nusser"
      },
      {
        "name" : "Chuan Xu"
      },
      {
        "name" : "Giovanni Neglia"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.09929v1",
    "title" : "Security and Privacy Measurement on Chinese Consumer IoT Traffic based\n  on Device Lifecycle",
    "summary" : "In recent years, consumer Internet of Things (IoT) devices have become widely\nused in daily life. With the popularity of devices, related security and\nprivacy risks arise at the same time as they collect user-related data and\ntransmit it to various service providers. Although China accounts for a larger\nshare of the consumer IoT industry, current analyses on consumer IoT device\ntraffic primarily focus on regions such as Europe, the United States, and\nAustralia. Research on China, however, is currently rather rare. This study\nconstructs the first large-scale dataset about consumer IoT device traffic in\nChina. Specifically, we propose a fine-grained traffic collection guidance\ncovering the entire lifecycle of consumer IoT devices, gathering traffic from\n70 devices spanning 36 brands and 8 device categories. Based on this dataset,\nwe analyze traffic destinations and encryption practices across different\ndevice types during the entire lifecycle and compare the findings with the\nresults of other regions. Compared to other regions, our results show that\nconsumer IoT devices in China rely more on domestic services and overally\nperform better in terms of encryption practices. However, there are still 20/35\ndevices improperly conduct certificate validation, and 5/70 devices use\ninsecure encryption protocols. To facilitate future research, we open-source\nour traffic collection guidance and make our dataset publicly available.",
    "updated" : "2025-05-15T03:27:16Z",
    "published" : "2025-05-15T03:27:16Z",
    "authors" : [
      {
        "name" : "Chenghua Jin"
      },
      {
        "name" : "Yan Jia"
      },
      {
        "name" : "Yuxin Song"
      },
      {
        "name" : "Qingyin Tan"
      },
      {
        "name" : "Rui Yang"
      },
      {
        "name" : "Zheli Liu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.09921v1",
    "title" : "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative\n  In-Context Optimization",
    "summary" : "Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at\n\\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.",
    "updated" : "2025-05-15T03:11:57Z",
    "published" : "2025-05-15T03:11:57Z",
    "authors" : [
      {
        "name" : "Yidan Wang"
      },
      {
        "name" : "Yanan Cao"
      },
      {
        "name" : "Yubing Ren"
      },
      {
        "name" : "Fang Fang"
      },
      {
        "name" : "Zheng Lin"
      },
      {
        "name" : "Binxing Fang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05922v2",
    "title" : "Cape: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy",
    "summary" : "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.",
    "updated" : "2025-05-15T09:31:11Z",
    "published" : "2025-05-09T09:54:07Z",
    "authors" : [
      {
        "name" : "Haoqi Wu"
      },
      {
        "name" : "Wei Dai"
      },
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Qiang Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  }
]