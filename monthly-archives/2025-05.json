[
  {
    "id" : "http://arxiv.org/abs/2505.00593v1",
    "title" : "A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security\n  and Privacy in IoT and Edge Networks",
    "summary" : "The security of image data in the Internet of Things (IoT) and edge networks\nis crucial due to the increasing deployment of intelligent systems for\nreal-time decision-making. Traditional encryption algorithms such as AES and\nRSA are computationally expensive for resource-constrained IoT devices and\nineffective for large-volume image data, leading to inefficiencies in\nprivacy-preserving distributed learning applications. To address these\nconcerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption\nscheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic\nChain Permutation and Confusion mechanisms to enhance security while\nmaintaining efficiency. The proposed scheme consists of three stages: (1) FAPS,\nwhich extracts and reorganizes pixels based on high and low edge intensity\nfeatures for correlation disruption; (2) Chaotic Chain Permutation, which\nemploys a logistic chaotic map with SHA-256-based dynamically updated keys for\nblock-wise permutation; and (3) Chaotic chain Confusion, which utilises\ndynamically generated chaotic seed matrices for bitwise XOR operations.\nExtensive security and performance evaluations demonstrate that the proposed\nscheme significantly reduces pixel correlation -- almost zero, achieves high\nentropy values close to 8, and resists differential cryptographic attacks. The\noptimum design of the proposed scheme makes it suitable for real-time\ndeployment in resource-constrained environments.",
    "updated" : "2025-05-01T15:26:48Z",
    "published" : "2025-05-01T15:26:48Z",
    "authors" : [
      {
        "name" : "Muhammad Shahbaz Khan"
      },
      {
        "name" : "Ahmed Al-Dubai"
      },
      {
        "name" : "Jawad Ahmad"
      },
      {
        "name" : "Nikolaos Pitropakis"
      },
      {
        "name" : "Baraq Ghaleb"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.00257v1",
    "title" : "Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial\n  Data Circulation",
    "summary" : "The sharing of external data has become a strong demand of financial\ninstitutions, but the privacy issue has led to the difficulty of\ninterconnecting different platforms and the low degree of data openness. To\neffectively solve the privacy problem of financial data in trans-border flow\nand sharing, to ensure that the data is available but not visible, to realize\nthe joint portrait of all kinds of heterogeneous data of business organizations\nin different industries, we propose a Heterogeneous Federated Graph Neural\nNetwork (HFGNN) approach. In this method, the distribution of heterogeneous\nbusiness data of trans-border organizations is taken as subgraphs, and the\nsharing and circulation process among subgraphs is constructed as a\nstatistically heterogeneous global graph through a central server. Each\nsubgraph learns the corresponding personalized service model through local\ntraining to select and update the relevant subset of subgraphs with aggregated\nparameters, and effectively separates and combines topological and feature\ninformation among subgraphs. Finally, our simulation experimental results show\nthat the proposed method has higher accuracy performance and faster convergence\nspeed than existing methods.",
    "updated" : "2025-05-01T02:47:43Z",
    "published" : "2025-05-01T02:47:43Z",
    "authors" : [
      {
        "name" : "Zhizhong Tan"
      },
      {
        "name" : "Jiexin Zheng"
      },
      {
        "name" : "Kevin Qi Zhang"
      },
      {
        "name" : "Wenyong Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01292v1",
    "title" : "Fine-grained Manipulation Attacks to Local Differential Privacy\n  Protocols for Data Streams",
    "summary" : "Local Differential Privacy (LDP) enables massive data collection and analysis\nwhile protecting end users' privacy against untrusted aggregators. It has been\napplied to various data types (e.g., categorical, numerical, and graph data)\nand application settings (e.g., static and streaming). Recent findings indicate\nthat LDP protocols can be easily disrupted by poisoning or manipulation\nattacks, which leverage injected/corrupted fake users to send crafted data\nconforming to the LDP reports. However, current attacks primarily target static\nprotocols, neglecting the security of LDP protocols in the streaming settings.\nOur research fills the gap by developing novel fine-grained manipulation\nattacks to LDP protocols for data streams. By reviewing the attack surfaces in\nexisting algorithms, We introduce a unified attack framework with composable\nmodules, which can manipulate the LDP estimated stream toward a target stream.\nOur attack framework can adapt to state-of-the-art streaming LDP algorithms\nwith different analytic tasks (e.g., frequency and mean) and LDP models\n(event-level, user-level, w-event level). We validate our attacks theoretically\nand through extensive experiments on real-world datasets, and finally explore a\npossible defense mechanism for mitigating these attacks.",
    "updated" : "2025-05-02T14:09:56Z",
    "published" : "2025-05-02T14:09:56Z",
    "authors" : [
      {
        "name" : "Xinyu Li"
      },
      {
        "name" : "Xuebin Ren"
      },
      {
        "name" : "Shusen Yang"
      },
      {
        "name" : "Liang Shi"
      },
      {
        "name" : "Chia-Mu Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.00951v1",
    "title" : "Preserving Privacy and Utility in LLM-Based Product Recommendations",
    "summary" : "Large Language Model (LLM)-based recommendation systems leverage powerful\nlanguage models to generate personalized suggestions by processing user\ninteractions and preferences. Unlike traditional recommendation systems that\nrely on structured data and collaborative filtering, LLM-based models process\ntextual and contextual information, often using cloud-based infrastructure.\nThis raises privacy concerns, as user data is transmitted to remote servers,\nincreasing the risk of exposure and reducing control over personal information.\nTo address this, we propose a hybrid privacy-preserving recommendation\nframework which separates sensitive from nonsensitive data and only shares the\nlatter with the cloud to harness LLM-powered recommendations. To restore lost\nrecommendations related to obfuscated sensitive data, we design a\nde-obfuscation module that reconstructs sensitive recommendations locally.\nExperiments on real-world e-commerce datasets show that our framework achieves\nalmost the same recommendation utility with a system which shares all data with\nan LLM, while preserving privacy to a large extend. Compared to\nobfuscation-only techniques, our approach improves HR@10 scores and category\ndistribution alignment, offering a better balance between privacy and\nrecommendation quality. Furthermore, our method runs efficiently on\nconsumer-grade hardware, making privacy-aware LLM-based recommendation systems\npractical for real-world use.",
    "updated" : "2025-05-02T01:54:08Z",
    "published" : "2025-05-02T01:54:08Z",
    "authors" : [
      {
        "name" : "Tina Khezresmaeilzadeh"
      },
      {
        "name" : "Jiang Zhang"
      },
      {
        "name" : "Dimitrios Andreadis"
      },
      {
        "name" : "Konstantinos Psounis"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02828v1",
    "title" : "Privacy Risks and Preservation Methods in Explainable Artificial\n  Intelligence: A Scoping Review",
    "summary" : "Explainable Artificial Intelligence (XAI) has emerged as a pillar of\nTrustworthy AI and aims to bring transparency in complex models that are opaque\nby nature. Despite the benefits of incorporating explanations in models, an\nurgent need is found in addressing the privacy concerns of providing this\nadditional information to end users. In this article, we conduct a scoping\nreview of existing literature to elicit details on the conflict between privacy\nand explainability. Using the standard methodology for scoping review, we\nextracted 57 articles from 1,943 studies published from January 2019 to\nDecember 2024. The review addresses 3 research questions to present readers\nwith more understanding of the topic: (1) what are the privacy risks of\nreleasing explanations in AI systems? (2) what current methods have researchers\nemployed to achieve privacy preservation in XAI systems? (3) what constitutes a\nprivacy preserving explanation? Based on the knowledge synthesized from the\nselected studies, we categorize the privacy risks and preservation methods in\nXAI and propose the characteristics of privacy preserving explanations to aid\nresearchers and practitioners in understanding the requirements of XAI that is\nprivacy compliant. Lastly, we identify the challenges in balancing privacy with\nother system desiderata and provide recommendations for achieving privacy\npreserving XAI. We expect that this review will shed light on the complex\nrelationship of privacy and explainability, both being the fundamental\nprinciples of Trustworthy AI.",
    "updated" : "2025-05-05T17:53:28Z",
    "published" : "2025-05-05T17:53:28Z",
    "authors" : [
      {
        "name" : "Sonal Allana"
      },
      {
        "name" : "Mohan Kankanhalli"
      },
      {
        "name" : "Rozita Dara"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02798v1",
    "title" : "Unifying Laplace Mechanism with Instance Optimality in Differential\n  Privacy",
    "summary" : "We adapt the canonical Laplace mechanism, widely used in differentially\nprivate data analysis, to achieve near instance optimality with respect to the\nhardness of the underlying dataset. In particular, we construct a piecewise\nLaplace distribution whereby we defy traditional assumptions and show that\nLaplace noise can in fact be drawn proportional to the local sensitivity when\ndone in a piecewise manner. While it may initially seem counterintuitive that\nthis satisfies (pure) differential privacy and can be sampled, we provide both\nthrough a simple connection to the exponential mechanism and inverse\nsensitivity along with the fact that the Laplace distribution is a two-sided\nexponential distribution. As a result, we prove that in the continuous setting\nour \\textit{piecewise Laplace mechanism} strictly dominates the inverse\nsensitivity mechanism, which was previously shown to both be nearly instance\noptimal and uniformly outperform the smooth sensitivity framework. Furthermore,\nin the worst-case where all local sensitivities equal the global sensitivity,\nour method simply reduces to a Laplace mechanism. We also complement this with\nan approximate local sensitivity variant to potentially ease the computational\ncost, which can also extend to higher dimensions.",
    "updated" : "2025-05-05T17:20:28Z",
    "published" : "2025-05-05T17:20:28Z",
    "authors" : [
      {
        "name" : "David Durfee"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02513v1",
    "title" : "Trustworthy Inter-Provider Agreements in 6G Using a Privacy-Enabled\n  Hybrid Blockchain Framework",
    "summary" : "Inter-provider agreements are central to 6G networks, where administrative\ndomains must securely and dynamically share services. To address the dual need\nfor transparency and confidentiality, we propose a privacy-enabled hybrid\nblockchain setup using Hyperledger Besu, integrating both public and private\ntransaction workflows. The system enables decentralized service registration,\nselection, and SLA breach reporting through role-based smart contracts and\nprivacy groups. We design and deploy a proof-of-concept implementation,\nevaluating performance using end-to-end latency as a key metric within privacy\ngroups. Results show that public interactions maintain stable latency, while\nprivate transactions incur additional overhead due to off-chain coordination.\nThe block production rate governed by IBFT 2.0 had limited impact on private\ntransaction latency, due to encryption and peer synchronization. Lessons\nlearned highlight design considerations for smart contract structure, validator\nmanagement, and scalability patterns suitable for dynamic inter-domain\ncollaboration. Our findings offer practical insights for deploying trustworthy\nagreement systems in 6G networks using privacy-enabled hybrid blockchains.",
    "updated" : "2025-05-05T09:46:30Z",
    "published" : "2025-05-05T09:46:30Z",
    "authors" : [
      {
        "name" : "Farhana Javed"
      },
      {
        "name" : "Josep Mangues-Bafalluy"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02392v1",
    "title" : "Moneros Decentralized P2P Exchanges: Functionality, Adoption, and\n  Privacy Risks",
    "summary" : "Privacy-focused cryptocurrencies like Monero remain popular, despite\nincreasing regulatory scrutiny that has led to their delisting from major\ncentralized exchanges. The latter also explains the recent popularity of\ndecentralized exchanges (DEXs) with no centralized ownership structures. These\nplatforms typically leverage peer-to-peer (P2P) networks, promising secure and\nanonymous asset trading. However, questions of liability remain, and the\nacademic literature lacks comprehensive insights into the functionality,\ntrading activity, and privacy claims of these P2P platforms. In this paper, we\nprovide an early systematization of the current landscape of decentralized\npeer-to-peer exchanges within the Monero ecosystem. We examine several recently\ndeveloped DEX platforms, analyzing their popularity, functionality,\narchitectural choices, and potential weaknesses. We further identify and report\non a privacy vulnerability in the recently popularized Haveno exchange,\ndemonstrating that certain Haveno trades could be detected, allowing\ntransactions to be linked across the Monero and Bitcoin blockchains. We hope\nthat our findings can nourish the discussion in the research community about\nmore secure designs, and provide insights for regulators.",
    "updated" : "2025-05-05T06:27:37Z",
    "published" : "2025-05-05T06:27:37Z",
    "authors" : [
      {
        "name" : "Yannik Kopyciok"
      },
      {
        "name" : "Friedhelm Victor"
      },
      {
        "name" : "Stefan Schmid"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02383v1",
    "title" : "Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs\n  Between Privacy and Regret",
    "summary" : "We address differentially private stochastic bandit problems from the angles\nof exploring the deep connections among Thompson Sampling with Gaussian priors,\nGaussian mechanisms, and Gaussian differential privacy (GDP). We propose\nDP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade\noff privacy and regret. DP-TS-UCB satisfies $ \\tilde{O}\n\\left(T^{0.25(1-\\alpha)}\\right)$-GDP and enjoys an $O\n\\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $\\alpha \\in [0,1]$\ncontrols the trade-off between privacy and regret. Theoretically, our DP-TS-UCB\nrelies on anti-concentration bounds of Gaussian distributions and links\nexploration mechanisms in Thompson Sampling-based algorithms and Upper\nConfidence Bound-based algorithms, which may be of independent interest.",
    "updated" : "2025-05-05T05:48:52Z",
    "published" : "2025-05-05T05:48:52Z",
    "authors" : [
      {
        "name" : "Bingshan Hu"
      },
      {
        "name" : "Zhiming Huang"
      },
      {
        "name" : "Tianyue H. Zhang"
      },
      {
        "name" : "Mathias Lécuyer"
      },
      {
        "name" : "Nidhi Hegde"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01976v1",
    "title" : "A Survey on Privacy Risks and Protection in Large Language Models",
    "summary" : "Although Large Language Models (LLMs) have become increasingly integral to\ndiverse applications, their capabilities raise significant privacy concerns.\nThis survey offers a comprehensive overview of privacy risks associated with\nLLMs and examines current solutions to mitigate these challenges. First, we\nanalyze privacy leakage and attacks in LLMs, focusing on how these models\nunintentionally expose sensitive information through techniques such as model\ninversion, training data extraction, and membership inference. We investigate\nthe mechanisms of privacy leakage, including the unauthorized extraction of\ntraining data and the potential exploitation of these vulnerabilities by\nmalicious actors. Next, we review existing privacy protection against such\nrisks, such as inference detection, federated learning, backdoor mitigation,\nand confidential computing, and assess their effectiveness in preventing\nprivacy leakage. Furthermore, we highlight key practical challenges and propose\nfuture research directions to develop secure and privacy-preserving LLMs,\nemphasizing privacy risk assessment, secure knowledge transfer between models,\nand interdisciplinary frameworks for privacy governance. Ultimately, this\nsurvey aims to establish a roadmap for addressing escalating privacy challenges\nin the LLMs domain.",
    "updated" : "2025-05-04T03:04:07Z",
    "published" : "2025-05-04T03:04:07Z",
    "authors" : [
      {
        "name" : "Kang Chen"
      },
      {
        "name" : "Xiuze Zhou"
      },
      {
        "name" : "Yuanguo Lin"
      },
      {
        "name" : "Shibo Feng"
      },
      {
        "name" : "Li Shen"
      },
      {
        "name" : "Pengcheng Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01879v1",
    "title" : "What to Do When Privacy Is Gone",
    "summary" : "Today's ethics of privacy is largely dedicated to defending personal\ninformation from big data technologies. This essay goes in the other direction.\nIt considers the struggle to be lost, and explores two strategies for living\nafter privacy is gone. First, total exposure embraces privacy's decline, and\nthen contributes to the process with transparency. All personal information is\nshared without reservation. The resulting ethics is explored through a big data\nversion of Robert Nozick's Experience Machine thought experiment. Second,\ntransient existence responds to privacy's loss by ceaselessly generating new\npersonal identities, which translates into constantly producing temporarily\nunviolated private information. The ethics is explored through Gilles Deleuze's\nmetaphysics of difference applied in linguistic terms to the formation of the\nself. Comparing the exposure and transience alternatives leads to the\nconclusion that today's big data reality splits the traditional ethical link\nbetween authenticity and freedom. Exposure provides authenticity, but negates\nhuman freedom. Transience provides freedom, but disdains authenticity.",
    "updated" : "2025-05-03T17:51:36Z",
    "published" : "2025-05-03T17:51:36Z",
    "authors" : [
      {
        "name" : "James Brusseau"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01788v1",
    "title" : "Privacy Preserving Machine Learning Model Personalization through\n  Federated Personalized Learning",
    "summary" : "The widespread adoption of Artificial Intelligence (AI) has been driven by\nsignificant advances in intelligent system research. However, this progress has\nraised concerns about data privacy, leading to a growing awareness of the need\nfor privacy-preserving AI. In response, there has been a seismic shift in\ninterest towards the leading paradigm for training Machine Learning (ML) models\non decentralized data silos while maintaining data privacy, Federated Learning\n(FL). This research paper presents a comprehensive performance analysis of a\ncutting-edge approach to personalize ML model while preserving privacy achieved\nthrough Privacy Preserving Machine Learning with the innovative framework of\nFederated Personalized Learning (PPMLFPL). Regarding the increasing concerns\nabout data privacy, this study evaluates the effectiveness of PPMLFPL\naddressing the critical balance between personalized model refinement and\nmaintaining the confidentiality of individual user data. According to our\nanalysis, Adaptive Personalized Cross-Silo Federated Learning with Differential\nPrivacy (APPLE+DP) offering efficient execution whereas overall, the use of the\nAdaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption\n(APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated\npersonalized learning settings is strongly suggested. The results offer\nvaluable insights creating it a promising scope for future advancements in the\nfield of privacy-conscious data-driven technologies.",
    "updated" : "2025-05-03T11:31:38Z",
    "published" : "2025-05-03T11:31:38Z",
    "authors" : [
      {
        "name" : "Md. Tanzib Hosain"
      },
      {
        "name" : "Asif Zaman"
      },
      {
        "name" : "Md. Shahriar Sajid"
      },
      {
        "name" : "Shadman Sakeeb Khan"
      },
      {
        "name" : "Shanjida Akter"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01524v1",
    "title" : "The DCR Delusion: Measuring the Privacy Risk of Synthetic Data",
    "summary" : "Synthetic data has become an increasingly popular way to share data without\nrevealing sensitive information. Though Membership Inference Attacks (MIAs) are\nwidely considered the gold standard for empirically assessing the privacy of a\nsynthetic dataset, practitioners and researchers often rely on simpler proxy\nmetrics such as Distance to Closest Record (DCR). These metrics estimate\nprivacy by measuring the similarity between the training data and generated\nsynthetic data. This similarity is also compared against that between the\ntraining data and a disjoint holdout set of real records to construct a binary\nprivacy test. If the synthetic data is not more similar to the training data\nthan the holdout set is, it passes the test and is considered private. In this\nwork we show that, while computationally inexpensive, DCR and other\ndistance-based metrics fail to identify privacy leakage. Across multiple\ndatasets and both classical models such as Baynet and CTGAN and more recent\ndiffusion models, we show that datasets deemed private by proxy metrics are\nhighly vulnerable to MIAs. We similarly find both the binary privacy test and\nthe continuous measure based on these metrics to be uninformative of actual\nmembership inference risk. We further show that these failures are consistent\nacross different metric hyperparameter settings and record selection methods.\nFinally, we argue DCR and other distance-based metrics to be flawed by design\nand show a example of a simple leakage they miss in practice. With this work,\nwe hope to motivate practitioners to move away from proxy metrics to MIAs as\nthe rigorous, comprehensive standard of evaluating privacy of synthetic data,\nin particular to make claims of datasets being legally anonymous.",
    "updated" : "2025-05-02T18:21:14Z",
    "published" : "2025-05-02T18:21:14Z",
    "authors" : [
      {
        "name" : "Zexi Yao"
      },
      {
        "name" : "Nataša Krčo"
      },
      {
        "name" : "Georgi Ganev"
      },
      {
        "name" : "Yves-Alexandre de Montjoye"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.03639v1",
    "title" : "Differential Privacy for Network Assortativity",
    "summary" : "The analysis of network assortativity is of great importance for\nunderstanding the structural characteristics of and dynamics upon networks.\nOften, network assortativity is quantified using the assortativity coefficient\nthat is defined based on the Pearson correlation coefficient between vertex\ndegrees. It is well known that a network may contain sensitive information,\nsuch as the number of friends of an individual in a social network (which is\nabstracted as the degree of vertex.). So, the computation of the assortativity\ncoefficient leads to privacy leakage, which increases the urgent need for\nprivacy-preserving protocol. However, there has been no scheme addressing the\nconcern above.\n  To bridge this gap, in this work, we are the first to propose approaches\nbased on differential privacy (DP for short). Specifically, we design three\nDP-based algorithms: $Local_{ru}$, $Shuffle_{ru}$, and $Decentral_{ru}$. The\nfirst two algorithms, based on Local DP (LDP) and Shuffle DP respectively, are\ndesigned for settings where each individual only knows his/her direct friends.\nIn contrast, the third algorithm, based on Decentralized DP (DDP), targets\nscenarios where each individual has a broader view, i.e., also knowing his/her\nfriends' friends. Theoretically, we prove that each algorithm enables an\nunbiased estimation of the assortativity coefficient of the network. We further\nevaluate the performance of the proposed algorithms using mean squared error\n(MSE), showing that $Shuffle_{ru}$ achieves the best performance, followed by\n$Decentral_{ru}$, with $Local_{ru}$ performing the worst. Note that these three\nalgorithms have different assumptions, so each has its applicability scenario.\nLastly, we conduct extensive numerical simulations, which demonstrate that the\npresented approaches are adequate to achieve the estimation of network\nassortativity under the demand for privacy protection.",
    "updated" : "2025-05-06T15:40:47Z",
    "published" : "2025-05-06T15:40:47Z",
    "authors" : [
      {
        "name" : "Fei Ma"
      },
      {
        "name" : "Jinzhi Ouyang"
      },
      {
        "name" : "Xincheng Hu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02975v1",
    "title" : "Navigating Privacy and Trust: AI Assistants as Social Support for Older\n  Adults",
    "summary" : "AI assistants are increasingly integrated into older adults' daily lives,\noffering new opportunities for social support and accessibility while raising\nimportant questions about privacy, autonomy, and trust. As these systems become\nembedded in caregiving and social networks, older adults must navigate\ntrade-offs between usability, data privacy, and personal agency across\ndifferent interaction contexts. Although prior work has explored AI assistants'\npotential benefits, further research is needed to understand how perceived\nusefulness and risk shape adoption and engagement. This paper examines these\ndynamics and advocates for participatory design approaches that position older\nadults as active decision makers in shaping AI assistant functionality. By\nadvancing a framework for privacy-aware, user-centered AI design, this work\ncontributes to ongoing discussions on developing ethical and transparent AI\nsystems that enhance well-being without compromising user control.",
    "updated" : "2025-05-05T19:00:14Z",
    "published" : "2025-05-05T19:00:14Z",
    "authors" : [
      {
        "name" : "Karina LaRubbio"
      },
      {
        "name" : "Malcolm Grba"
      },
      {
        "name" : "Diana Freed"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04570v1",
    "title" : "Privacy-preserving neutral atom-based quantum classifier towards real\n  healthcare applications",
    "summary" : "Technological advances in Artificial Intelligence (AI) and Machine Learning\n(ML) for the healthcare domain are rapidly arising, with a growing discussion\nregarding the ethical management of their development. In general, ML\nhealthcare applications crucially require performance, interpretability of\ndata, and respect for data privacy. The latter is an increasingly debated topic\nas commercial cloud computing services become more and more widespread.\nRecently, dedicated methods are starting to be developed aiming to protect data\nprivacy. However, these generally result in a trade-off forcing one to balance\nthe level of data privacy and the algorithm performance. Here, a Support Vector\nMachine (SVM) classifier model is proposed whose training is reformulated into\na Quadratic Unconstrained Binary Optimization (QUBO) problem, and adapted to a\nneutral atom-based Quantum Processing Unit (QPU). Our final model does not\nrequire anonymization techniques to protect data privacy since the sensitive\ndata are not needed to be transferred to the cloud-available QPU. Indeed, the\nlatter is used only during the training phase, hence allowing a future concrete\napplication in a real-world scenario. Finally, performance and scaling analyses\non a publicly available breast cancer dataset are discussed, both using ideal\nand noisy simulations for the training process, and also successfully tested on\na currently available real neutral-atom QPU.",
    "updated" : "2025-05-07T17:03:35Z",
    "published" : "2025-05-07T17:03:35Z",
    "authors" : [
      {
        "name" : "Ettore Canonici"
      },
      {
        "name" : "Filippo Caruso"
      }
    ],
    "categories" : [
      "quant-ph",
      "81V45 (Primary) 81P68 (Secondary)",
      "I.2.0; J.2; I.5.0"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04361v1",
    "title" : "RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery\n  Scheme in Mobile Crowdsensing",
    "summary" : "Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS).\nHowever, existing TD methods, including privacy-preserving TD approaches,\nestimate the truth by weighting only the data submitted in the current round,\nwhich often results in low data quality. Moreover, there is a lack of effective\nTD methods that preserve both reputation and data privacy. To address these\nissues, a Reputation and Data Privacy-Preserving based Truth Discovery\n(RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD\nscheme consists of two key approaches: a Reputation-based Truth Discovery (RTD)\napproach, which integrates the weight of current-round data with workers'\nreputation values to estimate the truth, thereby achieving more accurate\nresults, and a Reputation and Data Privacy-Preserving (RDPP) approach, which\nensures privacy preservation for sensing data and reputation values. First, the\nRDPP approach, when seamlessly integrated with RTD, can effectively evaluate\nthe reliability of workers and their sensing data in a privacy-preserving\nmanner. Second, the RDPP scheme supports reputation-based worker recruitment\nand rewards, ensuring high-quality data collection while incentivizing workers\nto provide accurate information. Comprehensive theoretical analysis and\nextensive experiments based on real-world datasets demonstrate that the\nproposed RDPP-TD scheme provides strong privacy protection and improves data\nquality by up to 33.3%.",
    "updated" : "2025-05-07T12:20:55Z",
    "published" : "2025-05-07T12:20:55Z",
    "authors" : [
      {
        "name" : "Lijian Wu"
      },
      {
        "name" : "Weikun Xie"
      },
      {
        "name" : "Wei Tan"
      },
      {
        "name" : "Tian Wang"
      },
      {
        "name" : "Houbing Herbert Song"
      },
      {
        "name" : "Anfeng Liu"
      }
    ],
    "categories" : [
      "cs.CE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04181v1",
    "title" : "Privacy Challenges In Image Processing Applications",
    "summary" : "As image processing systems proliferate, privacy concerns intensify given the\nsensitive personal information contained in images. This paper examines privacy\nchallenges in image processing and surveys emerging privacy-preserving\ntechniques including differential privacy, secure multiparty computation,\nhomomorphic encryption, and anonymization. Key applications with heightened\nprivacy risks include healthcare, where medical images contain patient health\ndata, and surveillance systems that can enable unwarranted tracking.\nDifferential privacy offers rigorous privacy guarantees by injecting controlled\nnoise, while MPC facilitates collaborative analytics without exposing raw data\ninputs. Homomorphic encryption enables computations on encrypted data and\nanonymization directly removes identifying elements. However, balancing privacy\nprotections and utility remains an open challenge. Promising future directions\nidentified include quantum-resilient cryptography, federated learning,\ndedicated hardware, and conceptual innovations like privacy by design.\nUltimately, a holistic effort combining technological innovations, ethical\nconsiderations, and policy frameworks is necessary to uphold the fundamental\nright to privacy as image processing capabilities continue advancing rapidly.",
    "updated" : "2025-05-07T07:28:03Z",
    "published" : "2025-05-07T07:28:03Z",
    "authors" : [
      {
        "name" : " Maneesha"
      },
      {
        "name" : "Bharat Gupta"
      },
      {
        "name" : "Rishabh Sethi"
      },
      {
        "name" : "Charvi Adita Das"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04034v1",
    "title" : "Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency,\n  and Transferability in Spiking Neural Networks",
    "summary" : "Biological neurons exhibit diverse temporal spike patterns, which are\nbelieved to support efficient, robust, and adaptive neural information\nprocessing. While models such as Izhikevich can replicate a wide range of these\nfiring dynamics, their complexity poses challenges for directly integrating\nthem into scalable spiking neural networks (SNN) training pipelines. In this\nwork, we propose two probabilistically driven, input-level temporal spike\ntransformations: Poisson-Burst and Delayed-Burst that introduce biologically\ninspired temporal variability directly into standard Leaky Integrate-and-Fire\n(LIF) neurons. This enables scalable training and systematic evaluation of how\nspike timing dynamics affect privacy, generalization, and learning performance.\nPoisson-Burst modulates burst occurrence based on input intensity, while\nDelayed-Burst encodes input strength through burst onset timing. Through\nextensive experiments across multiple benchmarks, we demonstrate that\nPoisson-Burst maintains competitive accuracy and lower resource overhead while\nexhibiting enhanced privacy robustness against membership inference attacks,\nwhereas Delayed-Burst provides stronger privacy protection at a modest accuracy\ntrade-off. These findings highlight the potential of biologically grounded\ntemporal spike dynamics in improving the privacy, generalization and biological\nplausibility of neuromorphic learning systems.",
    "updated" : "2025-05-07T00:27:00Z",
    "published" : "2025-05-07T00:27:00Z",
    "authors" : [
      {
        "name" : "Ayana Moshruba"
      },
      {
        "name" : "Hamed Poursiami"
      },
      {
        "name" : "Maryam Parsa"
      }
    ],
    "categories" : [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ]
  }
]