[
  {
    "id" : "http://arxiv.org/abs/2505.00593v1",
    "title" : "A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security\n  and Privacy in IoT and Edge Networks",
    "summary" : "The security of image data in the Internet of Things (IoT) and edge networks\nis crucial due to the increasing deployment of intelligent systems for\nreal-time decision-making. Traditional encryption algorithms such as AES and\nRSA are computationally expensive for resource-constrained IoT devices and\nineffective for large-volume image data, leading to inefficiencies in\nprivacy-preserving distributed learning applications. To address these\nconcerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption\nscheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic\nChain Permutation and Confusion mechanisms to enhance security while\nmaintaining efficiency. The proposed scheme consists of three stages: (1) FAPS,\nwhich extracts and reorganizes pixels based on high and low edge intensity\nfeatures for correlation disruption; (2) Chaotic Chain Permutation, which\nemploys a logistic chaotic map with SHA-256-based dynamically updated keys for\nblock-wise permutation; and (3) Chaotic chain Confusion, which utilises\ndynamically generated chaotic seed matrices for bitwise XOR operations.\nExtensive security and performance evaluations demonstrate that the proposed\nscheme significantly reduces pixel correlation -- almost zero, achieves high\nentropy values close to 8, and resists differential cryptographic attacks. The\noptimum design of the proposed scheme makes it suitable for real-time\ndeployment in resource-constrained environments.",
    "updated" : "2025-05-01T15:26:48Z",
    "published" : "2025-05-01T15:26:48Z",
    "authors" : [
      {
        "name" : "Muhammad Shahbaz Khan"
      },
      {
        "name" : "Ahmed Al-Dubai"
      },
      {
        "name" : "Jawad Ahmad"
      },
      {
        "name" : "Nikolaos Pitropakis"
      },
      {
        "name" : "Baraq Ghaleb"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.00257v1",
    "title" : "Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial\n  Data Circulation",
    "summary" : "The sharing of external data has become a strong demand of financial\ninstitutions, but the privacy issue has led to the difficulty of\ninterconnecting different platforms and the low degree of data openness. To\neffectively solve the privacy problem of financial data in trans-border flow\nand sharing, to ensure that the data is available but not visible, to realize\nthe joint portrait of all kinds of heterogeneous data of business organizations\nin different industries, we propose a Heterogeneous Federated Graph Neural\nNetwork (HFGNN) approach. In this method, the distribution of heterogeneous\nbusiness data of trans-border organizations is taken as subgraphs, and the\nsharing and circulation process among subgraphs is constructed as a\nstatistically heterogeneous global graph through a central server. Each\nsubgraph learns the corresponding personalized service model through local\ntraining to select and update the relevant subset of subgraphs with aggregated\nparameters, and effectively separates and combines topological and feature\ninformation among subgraphs. Finally, our simulation experimental results show\nthat the proposed method has higher accuracy performance and faster convergence\nspeed than existing methods.",
    "updated" : "2025-05-01T02:47:43Z",
    "published" : "2025-05-01T02:47:43Z",
    "authors" : [
      {
        "name" : "Zhizhong Tan"
      },
      {
        "name" : "Jiexin Zheng"
      },
      {
        "name" : "Kevin Qi Zhang"
      },
      {
        "name" : "Wenyong Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01292v1",
    "title" : "Fine-grained Manipulation Attacks to Local Differential Privacy\n  Protocols for Data Streams",
    "summary" : "Local Differential Privacy (LDP) enables massive data collection and analysis\nwhile protecting end users' privacy against untrusted aggregators. It has been\napplied to various data types (e.g., categorical, numerical, and graph data)\nand application settings (e.g., static and streaming). Recent findings indicate\nthat LDP protocols can be easily disrupted by poisoning or manipulation\nattacks, which leverage injected/corrupted fake users to send crafted data\nconforming to the LDP reports. However, current attacks primarily target static\nprotocols, neglecting the security of LDP protocols in the streaming settings.\nOur research fills the gap by developing novel fine-grained manipulation\nattacks to LDP protocols for data streams. By reviewing the attack surfaces in\nexisting algorithms, We introduce a unified attack framework with composable\nmodules, which can manipulate the LDP estimated stream toward a target stream.\nOur attack framework can adapt to state-of-the-art streaming LDP algorithms\nwith different analytic tasks (e.g., frequency and mean) and LDP models\n(event-level, user-level, w-event level). We validate our attacks theoretically\nand through extensive experiments on real-world datasets, and finally explore a\npossible defense mechanism for mitigating these attacks.",
    "updated" : "2025-05-02T14:09:56Z",
    "published" : "2025-05-02T14:09:56Z",
    "authors" : [
      {
        "name" : "Xinyu Li"
      },
      {
        "name" : "Xuebin Ren"
      },
      {
        "name" : "Shusen Yang"
      },
      {
        "name" : "Liang Shi"
      },
      {
        "name" : "Chia-Mu Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.00951v1",
    "title" : "Preserving Privacy and Utility in LLM-Based Product Recommendations",
    "summary" : "Large Language Model (LLM)-based recommendation systems leverage powerful\nlanguage models to generate personalized suggestions by processing user\ninteractions and preferences. Unlike traditional recommendation systems that\nrely on structured data and collaborative filtering, LLM-based models process\ntextual and contextual information, often using cloud-based infrastructure.\nThis raises privacy concerns, as user data is transmitted to remote servers,\nincreasing the risk of exposure and reducing control over personal information.\nTo address this, we propose a hybrid privacy-preserving recommendation\nframework which separates sensitive from nonsensitive data and only shares the\nlatter with the cloud to harness LLM-powered recommendations. To restore lost\nrecommendations related to obfuscated sensitive data, we design a\nde-obfuscation module that reconstructs sensitive recommendations locally.\nExperiments on real-world e-commerce datasets show that our framework achieves\nalmost the same recommendation utility with a system which shares all data with\nan LLM, while preserving privacy to a large extend. Compared to\nobfuscation-only techniques, our approach improves HR@10 scores and category\ndistribution alignment, offering a better balance between privacy and\nrecommendation quality. Furthermore, our method runs efficiently on\nconsumer-grade hardware, making privacy-aware LLM-based recommendation systems\npractical for real-world use.",
    "updated" : "2025-05-02T01:54:08Z",
    "published" : "2025-05-02T01:54:08Z",
    "authors" : [
      {
        "name" : "Tina Khezresmaeilzadeh"
      },
      {
        "name" : "Jiang Zhang"
      },
      {
        "name" : "Dimitrios Andreadis"
      },
      {
        "name" : "Konstantinos Psounis"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02828v1",
    "title" : "Privacy Risks and Preservation Methods in Explainable Artificial\n  Intelligence: A Scoping Review",
    "summary" : "Explainable Artificial Intelligence (XAI) has emerged as a pillar of\nTrustworthy AI and aims to bring transparency in complex models that are opaque\nby nature. Despite the benefits of incorporating explanations in models, an\nurgent need is found in addressing the privacy concerns of providing this\nadditional information to end users. In this article, we conduct a scoping\nreview of existing literature to elicit details on the conflict between privacy\nand explainability. Using the standard methodology for scoping review, we\nextracted 57 articles from 1,943 studies published from January 2019 to\nDecember 2024. The review addresses 3 research questions to present readers\nwith more understanding of the topic: (1) what are the privacy risks of\nreleasing explanations in AI systems? (2) what current methods have researchers\nemployed to achieve privacy preservation in XAI systems? (3) what constitutes a\nprivacy preserving explanation? Based on the knowledge synthesized from the\nselected studies, we categorize the privacy risks and preservation methods in\nXAI and propose the characteristics of privacy preserving explanations to aid\nresearchers and practitioners in understanding the requirements of XAI that is\nprivacy compliant. Lastly, we identify the challenges in balancing privacy with\nother system desiderata and provide recommendations for achieving privacy\npreserving XAI. We expect that this review will shed light on the complex\nrelationship of privacy and explainability, both being the fundamental\nprinciples of Trustworthy AI.",
    "updated" : "2025-05-05T17:53:28Z",
    "published" : "2025-05-05T17:53:28Z",
    "authors" : [
      {
        "name" : "Sonal Allana"
      },
      {
        "name" : "Mohan Kankanhalli"
      },
      {
        "name" : "Rozita Dara"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02798v1",
    "title" : "Unifying Laplace Mechanism with Instance Optimality in Differential\n  Privacy",
    "summary" : "We adapt the canonical Laplace mechanism, widely used in differentially\nprivate data analysis, to achieve near instance optimality with respect to the\nhardness of the underlying dataset. In particular, we construct a piecewise\nLaplace distribution whereby we defy traditional assumptions and show that\nLaplace noise can in fact be drawn proportional to the local sensitivity when\ndone in a piecewise manner. While it may initially seem counterintuitive that\nthis satisfies (pure) differential privacy and can be sampled, we provide both\nthrough a simple connection to the exponential mechanism and inverse\nsensitivity along with the fact that the Laplace distribution is a two-sided\nexponential distribution. As a result, we prove that in the continuous setting\nour \\textit{piecewise Laplace mechanism} strictly dominates the inverse\nsensitivity mechanism, which was previously shown to both be nearly instance\noptimal and uniformly outperform the smooth sensitivity framework. Furthermore,\nin the worst-case where all local sensitivities equal the global sensitivity,\nour method simply reduces to a Laplace mechanism. We also complement this with\nan approximate local sensitivity variant to potentially ease the computational\ncost, which can also extend to higher dimensions.",
    "updated" : "2025-05-05T17:20:28Z",
    "published" : "2025-05-05T17:20:28Z",
    "authors" : [
      {
        "name" : "David Durfee"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02513v1",
    "title" : "Trustworthy Inter-Provider Agreements in 6G Using a Privacy-Enabled\n  Hybrid Blockchain Framework",
    "summary" : "Inter-provider agreements are central to 6G networks, where administrative\ndomains must securely and dynamically share services. To address the dual need\nfor transparency and confidentiality, we propose a privacy-enabled hybrid\nblockchain setup using Hyperledger Besu, integrating both public and private\ntransaction workflows. The system enables decentralized service registration,\nselection, and SLA breach reporting through role-based smart contracts and\nprivacy groups. We design and deploy a proof-of-concept implementation,\nevaluating performance using end-to-end latency as a key metric within privacy\ngroups. Results show that public interactions maintain stable latency, while\nprivate transactions incur additional overhead due to off-chain coordination.\nThe block production rate governed by IBFT 2.0 had limited impact on private\ntransaction latency, due to encryption and peer synchronization. Lessons\nlearned highlight design considerations for smart contract structure, validator\nmanagement, and scalability patterns suitable for dynamic inter-domain\ncollaboration. Our findings offer practical insights for deploying trustworthy\nagreement systems in 6G networks using privacy-enabled hybrid blockchains.",
    "updated" : "2025-05-05T09:46:30Z",
    "published" : "2025-05-05T09:46:30Z",
    "authors" : [
      {
        "name" : "Farhana Javed"
      },
      {
        "name" : "Josep Mangues-Bafalluy"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02392v1",
    "title" : "Moneros Decentralized P2P Exchanges: Functionality, Adoption, and\n  Privacy Risks",
    "summary" : "Privacy-focused cryptocurrencies like Monero remain popular, despite\nincreasing regulatory scrutiny that has led to their delisting from major\ncentralized exchanges. The latter also explains the recent popularity of\ndecentralized exchanges (DEXs) with no centralized ownership structures. These\nplatforms typically leverage peer-to-peer (P2P) networks, promising secure and\nanonymous asset trading. However, questions of liability remain, and the\nacademic literature lacks comprehensive insights into the functionality,\ntrading activity, and privacy claims of these P2P platforms. In this paper, we\nprovide an early systematization of the current landscape of decentralized\npeer-to-peer exchanges within the Monero ecosystem. We examine several recently\ndeveloped DEX platforms, analyzing their popularity, functionality,\narchitectural choices, and potential weaknesses. We further identify and report\non a privacy vulnerability in the recently popularized Haveno exchange,\ndemonstrating that certain Haveno trades could be detected, allowing\ntransactions to be linked across the Monero and Bitcoin blockchains. We hope\nthat our findings can nourish the discussion in the research community about\nmore secure designs, and provide insights for regulators.",
    "updated" : "2025-05-05T06:27:37Z",
    "published" : "2025-05-05T06:27:37Z",
    "authors" : [
      {
        "name" : "Yannik Kopyciok"
      },
      {
        "name" : "Friedhelm Victor"
      },
      {
        "name" : "Stefan Schmid"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02383v1",
    "title" : "Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs\n  Between Privacy and Regret",
    "summary" : "We address differentially private stochastic bandit problems from the angles\nof exploring the deep connections among Thompson Sampling with Gaussian priors,\nGaussian mechanisms, and Gaussian differential privacy (GDP). We propose\nDP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade\noff privacy and regret. DP-TS-UCB satisfies $ \\tilde{O}\n\\left(T^{0.25(1-\\alpha)}\\right)$-GDP and enjoys an $O\n\\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $\\alpha \\in [0,1]$\ncontrols the trade-off between privacy and regret. Theoretically, our DP-TS-UCB\nrelies on anti-concentration bounds of Gaussian distributions and links\nexploration mechanisms in Thompson Sampling-based algorithms and Upper\nConfidence Bound-based algorithms, which may be of independent interest.",
    "updated" : "2025-05-05T05:48:52Z",
    "published" : "2025-05-05T05:48:52Z",
    "authors" : [
      {
        "name" : "Bingshan Hu"
      },
      {
        "name" : "Zhiming Huang"
      },
      {
        "name" : "Tianyue H. Zhang"
      },
      {
        "name" : "Mathias Lécuyer"
      },
      {
        "name" : "Nidhi Hegde"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01976v1",
    "title" : "A Survey on Privacy Risks and Protection in Large Language Models",
    "summary" : "Although Large Language Models (LLMs) have become increasingly integral to\ndiverse applications, their capabilities raise significant privacy concerns.\nThis survey offers a comprehensive overview of privacy risks associated with\nLLMs and examines current solutions to mitigate these challenges. First, we\nanalyze privacy leakage and attacks in LLMs, focusing on how these models\nunintentionally expose sensitive information through techniques such as model\ninversion, training data extraction, and membership inference. We investigate\nthe mechanisms of privacy leakage, including the unauthorized extraction of\ntraining data and the potential exploitation of these vulnerabilities by\nmalicious actors. Next, we review existing privacy protection against such\nrisks, such as inference detection, federated learning, backdoor mitigation,\nand confidential computing, and assess their effectiveness in preventing\nprivacy leakage. Furthermore, we highlight key practical challenges and propose\nfuture research directions to develop secure and privacy-preserving LLMs,\nemphasizing privacy risk assessment, secure knowledge transfer between models,\nand interdisciplinary frameworks for privacy governance. Ultimately, this\nsurvey aims to establish a roadmap for addressing escalating privacy challenges\nin the LLMs domain.",
    "updated" : "2025-05-04T03:04:07Z",
    "published" : "2025-05-04T03:04:07Z",
    "authors" : [
      {
        "name" : "Kang Chen"
      },
      {
        "name" : "Xiuze Zhou"
      },
      {
        "name" : "Yuanguo Lin"
      },
      {
        "name" : "Shibo Feng"
      },
      {
        "name" : "Li Shen"
      },
      {
        "name" : "Pengcheng Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01879v1",
    "title" : "What to Do When Privacy Is Gone",
    "summary" : "Today's ethics of privacy is largely dedicated to defending personal\ninformation from big data technologies. This essay goes in the other direction.\nIt considers the struggle to be lost, and explores two strategies for living\nafter privacy is gone. First, total exposure embraces privacy's decline, and\nthen contributes to the process with transparency. All personal information is\nshared without reservation. The resulting ethics is explored through a big data\nversion of Robert Nozick's Experience Machine thought experiment. Second,\ntransient existence responds to privacy's loss by ceaselessly generating new\npersonal identities, which translates into constantly producing temporarily\nunviolated private information. The ethics is explored through Gilles Deleuze's\nmetaphysics of difference applied in linguistic terms to the formation of the\nself. Comparing the exposure and transience alternatives leads to the\nconclusion that today's big data reality splits the traditional ethical link\nbetween authenticity and freedom. Exposure provides authenticity, but negates\nhuman freedom. Transience provides freedom, but disdains authenticity.",
    "updated" : "2025-05-03T17:51:36Z",
    "published" : "2025-05-03T17:51:36Z",
    "authors" : [
      {
        "name" : "James Brusseau"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01788v1",
    "title" : "Privacy Preserving Machine Learning Model Personalization through\n  Federated Personalized Learning",
    "summary" : "The widespread adoption of Artificial Intelligence (AI) has been driven by\nsignificant advances in intelligent system research. However, this progress has\nraised concerns about data privacy, leading to a growing awareness of the need\nfor privacy-preserving AI. In response, there has been a seismic shift in\ninterest towards the leading paradigm for training Machine Learning (ML) models\non decentralized data silos while maintaining data privacy, Federated Learning\n(FL). This research paper presents a comprehensive performance analysis of a\ncutting-edge approach to personalize ML model while preserving privacy achieved\nthrough Privacy Preserving Machine Learning with the innovative framework of\nFederated Personalized Learning (PPMLFPL). Regarding the increasing concerns\nabout data privacy, this study evaluates the effectiveness of PPMLFPL\naddressing the critical balance between personalized model refinement and\nmaintaining the confidentiality of individual user data. According to our\nanalysis, Adaptive Personalized Cross-Silo Federated Learning with Differential\nPrivacy (APPLE+DP) offering efficient execution whereas overall, the use of the\nAdaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption\n(APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated\npersonalized learning settings is strongly suggested. The results offer\nvaluable insights creating it a promising scope for future advancements in the\nfield of privacy-conscious data-driven technologies.",
    "updated" : "2025-05-03T11:31:38Z",
    "published" : "2025-05-03T11:31:38Z",
    "authors" : [
      {
        "name" : "Md. Tanzib Hosain"
      },
      {
        "name" : "Asif Zaman"
      },
      {
        "name" : "Md. Shahriar Sajid"
      },
      {
        "name" : "Shadman Sakeeb Khan"
      },
      {
        "name" : "Shanjida Akter"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01524v1",
    "title" : "The DCR Delusion: Measuring the Privacy Risk of Synthetic Data",
    "summary" : "Synthetic data has become an increasingly popular way to share data without\nrevealing sensitive information. Though Membership Inference Attacks (MIAs) are\nwidely considered the gold standard for empirically assessing the privacy of a\nsynthetic dataset, practitioners and researchers often rely on simpler proxy\nmetrics such as Distance to Closest Record (DCR). These metrics estimate\nprivacy by measuring the similarity between the training data and generated\nsynthetic data. This similarity is also compared against that between the\ntraining data and a disjoint holdout set of real records to construct a binary\nprivacy test. If the synthetic data is not more similar to the training data\nthan the holdout set is, it passes the test and is considered private. In this\nwork we show that, while computationally inexpensive, DCR and other\ndistance-based metrics fail to identify privacy leakage. Across multiple\ndatasets and both classical models such as Baynet and CTGAN and more recent\ndiffusion models, we show that datasets deemed private by proxy metrics are\nhighly vulnerable to MIAs. We similarly find both the binary privacy test and\nthe continuous measure based on these metrics to be uninformative of actual\nmembership inference risk. We further show that these failures are consistent\nacross different metric hyperparameter settings and record selection methods.\nFinally, we argue DCR and other distance-based metrics to be flawed by design\nand show a example of a simple leakage they miss in practice. With this work,\nwe hope to motivate practitioners to move away from proxy metrics to MIAs as\nthe rigorous, comprehensive standard of evaluating privacy of synthetic data,\nin particular to make claims of datasets being legally anonymous.",
    "updated" : "2025-05-02T18:21:14Z",
    "published" : "2025-05-02T18:21:14Z",
    "authors" : [
      {
        "name" : "Zexi Yao"
      },
      {
        "name" : "Nataša Krčo"
      },
      {
        "name" : "Georgi Ganev"
      },
      {
        "name" : "Yves-Alexandre de Montjoye"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.03639v1",
    "title" : "Differential Privacy for Network Assortativity",
    "summary" : "The analysis of network assortativity is of great importance for\nunderstanding the structural characteristics of and dynamics upon networks.\nOften, network assortativity is quantified using the assortativity coefficient\nthat is defined based on the Pearson correlation coefficient between vertex\ndegrees. It is well known that a network may contain sensitive information,\nsuch as the number of friends of an individual in a social network (which is\nabstracted as the degree of vertex.). So, the computation of the assortativity\ncoefficient leads to privacy leakage, which increases the urgent need for\nprivacy-preserving protocol. However, there has been no scheme addressing the\nconcern above.\n  To bridge this gap, in this work, we are the first to propose approaches\nbased on differential privacy (DP for short). Specifically, we design three\nDP-based algorithms: $Local_{ru}$, $Shuffle_{ru}$, and $Decentral_{ru}$. The\nfirst two algorithms, based on Local DP (LDP) and Shuffle DP respectively, are\ndesigned for settings where each individual only knows his/her direct friends.\nIn contrast, the third algorithm, based on Decentralized DP (DDP), targets\nscenarios where each individual has a broader view, i.e., also knowing his/her\nfriends' friends. Theoretically, we prove that each algorithm enables an\nunbiased estimation of the assortativity coefficient of the network. We further\nevaluate the performance of the proposed algorithms using mean squared error\n(MSE), showing that $Shuffle_{ru}$ achieves the best performance, followed by\n$Decentral_{ru}$, with $Local_{ru}$ performing the worst. Note that these three\nalgorithms have different assumptions, so each has its applicability scenario.\nLastly, we conduct extensive numerical simulations, which demonstrate that the\npresented approaches are adequate to achieve the estimation of network\nassortativity under the demand for privacy protection.",
    "updated" : "2025-05-06T15:40:47Z",
    "published" : "2025-05-06T15:40:47Z",
    "authors" : [
      {
        "name" : "Fei Ma"
      },
      {
        "name" : "Jinzhi Ouyang"
      },
      {
        "name" : "Xincheng Hu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02975v1",
    "title" : "Navigating Privacy and Trust: AI Assistants as Social Support for Older\n  Adults",
    "summary" : "AI assistants are increasingly integrated into older adults' daily lives,\noffering new opportunities for social support and accessibility while raising\nimportant questions about privacy, autonomy, and trust. As these systems become\nembedded in caregiving and social networks, older adults must navigate\ntrade-offs between usability, data privacy, and personal agency across\ndifferent interaction contexts. Although prior work has explored AI assistants'\npotential benefits, further research is needed to understand how perceived\nusefulness and risk shape adoption and engagement. This paper examines these\ndynamics and advocates for participatory design approaches that position older\nadults as active decision makers in shaping AI assistant functionality. By\nadvancing a framework for privacy-aware, user-centered AI design, this work\ncontributes to ongoing discussions on developing ethical and transparent AI\nsystems that enhance well-being without compromising user control.",
    "updated" : "2025-05-05T19:00:14Z",
    "published" : "2025-05-05T19:00:14Z",
    "authors" : [
      {
        "name" : "Karina LaRubbio"
      },
      {
        "name" : "Malcolm Grba"
      },
      {
        "name" : "Diana Freed"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04570v1",
    "title" : "Privacy-preserving neutral atom-based quantum classifier towards real\n  healthcare applications",
    "summary" : "Technological advances in Artificial Intelligence (AI) and Machine Learning\n(ML) for the healthcare domain are rapidly arising, with a growing discussion\nregarding the ethical management of their development. In general, ML\nhealthcare applications crucially require performance, interpretability of\ndata, and respect for data privacy. The latter is an increasingly debated topic\nas commercial cloud computing services become more and more widespread.\nRecently, dedicated methods are starting to be developed aiming to protect data\nprivacy. However, these generally result in a trade-off forcing one to balance\nthe level of data privacy and the algorithm performance. Here, a Support Vector\nMachine (SVM) classifier model is proposed whose training is reformulated into\na Quadratic Unconstrained Binary Optimization (QUBO) problem, and adapted to a\nneutral atom-based Quantum Processing Unit (QPU). Our final model does not\nrequire anonymization techniques to protect data privacy since the sensitive\ndata are not needed to be transferred to the cloud-available QPU. Indeed, the\nlatter is used only during the training phase, hence allowing a future concrete\napplication in a real-world scenario. Finally, performance and scaling analyses\non a publicly available breast cancer dataset are discussed, both using ideal\nand noisy simulations for the training process, and also successfully tested on\na currently available real neutral-atom QPU.",
    "updated" : "2025-05-07T17:03:35Z",
    "published" : "2025-05-07T17:03:35Z",
    "authors" : [
      {
        "name" : "Ettore Canonici"
      },
      {
        "name" : "Filippo Caruso"
      }
    ],
    "categories" : [
      "quant-ph",
      "81V45 (Primary) 81P68 (Secondary)",
      "I.2.0; J.2; I.5.0"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04361v1",
    "title" : "RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery\n  Scheme in Mobile Crowdsensing",
    "summary" : "Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS).\nHowever, existing TD methods, including privacy-preserving TD approaches,\nestimate the truth by weighting only the data submitted in the current round,\nwhich often results in low data quality. Moreover, there is a lack of effective\nTD methods that preserve both reputation and data privacy. To address these\nissues, a Reputation and Data Privacy-Preserving based Truth Discovery\n(RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD\nscheme consists of two key approaches: a Reputation-based Truth Discovery (RTD)\napproach, which integrates the weight of current-round data with workers'\nreputation values to estimate the truth, thereby achieving more accurate\nresults, and a Reputation and Data Privacy-Preserving (RDPP) approach, which\nensures privacy preservation for sensing data and reputation values. First, the\nRDPP approach, when seamlessly integrated with RTD, can effectively evaluate\nthe reliability of workers and their sensing data in a privacy-preserving\nmanner. Second, the RDPP scheme supports reputation-based worker recruitment\nand rewards, ensuring high-quality data collection while incentivizing workers\nto provide accurate information. Comprehensive theoretical analysis and\nextensive experiments based on real-world datasets demonstrate that the\nproposed RDPP-TD scheme provides strong privacy protection and improves data\nquality by up to 33.3%.",
    "updated" : "2025-05-07T12:20:55Z",
    "published" : "2025-05-07T12:20:55Z",
    "authors" : [
      {
        "name" : "Lijian Wu"
      },
      {
        "name" : "Weikun Xie"
      },
      {
        "name" : "Wei Tan"
      },
      {
        "name" : "Tian Wang"
      },
      {
        "name" : "Houbing Herbert Song"
      },
      {
        "name" : "Anfeng Liu"
      }
    ],
    "categories" : [
      "cs.CE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04181v1",
    "title" : "Privacy Challenges In Image Processing Applications",
    "summary" : "As image processing systems proliferate, privacy concerns intensify given the\nsensitive personal information contained in images. This paper examines privacy\nchallenges in image processing and surveys emerging privacy-preserving\ntechniques including differential privacy, secure multiparty computation,\nhomomorphic encryption, and anonymization. Key applications with heightened\nprivacy risks include healthcare, where medical images contain patient health\ndata, and surveillance systems that can enable unwarranted tracking.\nDifferential privacy offers rigorous privacy guarantees by injecting controlled\nnoise, while MPC facilitates collaborative analytics without exposing raw data\ninputs. Homomorphic encryption enables computations on encrypted data and\nanonymization directly removes identifying elements. However, balancing privacy\nprotections and utility remains an open challenge. Promising future directions\nidentified include quantum-resilient cryptography, federated learning,\ndedicated hardware, and conceptual innovations like privacy by design.\nUltimately, a holistic effort combining technological innovations, ethical\nconsiderations, and policy frameworks is necessary to uphold the fundamental\nright to privacy as image processing capabilities continue advancing rapidly.",
    "updated" : "2025-05-07T07:28:03Z",
    "published" : "2025-05-07T07:28:03Z",
    "authors" : [
      {
        "name" : " Maneesha"
      },
      {
        "name" : "Bharat Gupta"
      },
      {
        "name" : "Rishabh Sethi"
      },
      {
        "name" : "Charvi Adita Das"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04034v1",
    "title" : "Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency,\n  and Transferability in Spiking Neural Networks",
    "summary" : "Biological neurons exhibit diverse temporal spike patterns, which are\nbelieved to support efficient, robust, and adaptive neural information\nprocessing. While models such as Izhikevich can replicate a wide range of these\nfiring dynamics, their complexity poses challenges for directly integrating\nthem into scalable spiking neural networks (SNN) training pipelines. In this\nwork, we propose two probabilistically driven, input-level temporal spike\ntransformations: Poisson-Burst and Delayed-Burst that introduce biologically\ninspired temporal variability directly into standard Leaky Integrate-and-Fire\n(LIF) neurons. This enables scalable training and systematic evaluation of how\nspike timing dynamics affect privacy, generalization, and learning performance.\nPoisson-Burst modulates burst occurrence based on input intensity, while\nDelayed-Burst encodes input strength through burst onset timing. Through\nextensive experiments across multiple benchmarks, we demonstrate that\nPoisson-Burst maintains competitive accuracy and lower resource overhead while\nexhibiting enhanced privacy robustness against membership inference attacks,\nwhereas Delayed-Burst provides stronger privacy protection at a modest accuracy\ntrade-off. These findings highlight the potential of biologically grounded\ntemporal spike dynamics in improving the privacy, generalization and biological\nplausibility of neuromorphic learning systems.",
    "updated" : "2025-05-07T00:27:00Z",
    "published" : "2025-05-07T00:27:00Z",
    "authors" : [
      {
        "name" : "Ayana Moshruba"
      },
      {
        "name" : "Hamed Poursiami"
      },
      {
        "name" : "Maryam Parsa"
      }
    ],
    "categories" : [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05214v1",
    "title" : "Overcoming the hurdle of legal expertise: A reusable model for\n  smartwatch privacy policies",
    "summary" : "Regulations for privacy protection aim to protect individuals from the\nunauthorized storage, processing, and transfer of their personal data but\noftentimes fail in providing helpful support for understanding these\nregulations. To better communicate privacy policies for smartwatches, we need\nan in-depth understanding of their concepts and provide better ways to enable\ndevelopers to integrate them when engineering systems. Up to now, no conceptual\nmodel exists covering privacy statements from different smartwatch\nmanufacturers that is reusable for developers. This paper introduces such a\nconceptual model for privacy policies of smartwatches and shows its use in a\nmodel-driven software engineering approach to create a platform for data\nvisualization of wearable privacy policies from different smartwatch\nmanufacturers. We have analyzed the privacy policies of various manufacturers\nand extracted the relevant concepts. Moreover, we have checked the model with\nlawyers for its correctness, instantiated it with concrete data, and used it in\na model-driven software engineering approach to create a platform for data\nvisualization. This reusable privacy policy model can enable developers to\neasily represent privacy policies in their systems. This provides a foundation\nfor more structured and understandable privacy policies which, in the long run,\ncan increase the data sovereignty of application users.",
    "updated" : "2025-05-08T13:09:12Z",
    "published" : "2025-05-08T13:09:12Z",
    "authors" : [
      {
        "name" : "Constantin Buschhaus"
      },
      {
        "name" : "Arvid Butting"
      },
      {
        "name" : "Judith Michael"
      },
      {
        "name" : "Verena Nitsch"
      },
      {
        "name" : "Sebastian Pütz"
      },
      {
        "name" : "Bernhard Rumpe"
      },
      {
        "name" : "Carolin Stellmacher"
      },
      {
        "name" : "Sabine Theis"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05155v1",
    "title" : "FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data\n  Preparation via Federated Learning",
    "summary" : "Trajectory data, which capture the movement patterns of people and vehicles\nover time and space, are crucial for applications like traffic optimization and\nurban planning. However, issues such as noise and incompleteness often\ncompromise data quality, leading to inaccurate trajectory analyses and limiting\nthe potential of these applications. While Trajectory Data Preparation (TDP)\ncan enhance data quality, existing methods suffer from two key limitations: (i)\nthey do not address data privacy concerns, particularly in federated settings\nwhere trajectory data sharing is prohibited, and (ii) they typically design\ntask-specific models that lack generalizability across diverse TDP scenarios.\nTo overcome these challenges, we propose FedTDP, a privacy-preserving and\nunified framework that leverages the capabilities of Large Language Models\n(LLMs) for TDP in federated environments. Specifically, we: (i) design a\ntrajectory privacy autoencoder to secure data transmission and protect privacy,\n(ii) introduce a trajectory knowledge enhancer to improve model learning of\nTDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)\npropose federated parallel optimization to enhance training efficiency by\nreducing data transmission and enabling parallel model training. Experiments on\n6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP\nconsistently outperforms 13 state-of-the-art baselines.",
    "updated" : "2025-05-08T11:51:23Z",
    "published" : "2025-05-08T11:51:23Z",
    "authors" : [
      {
        "name" : "Zhihao Zeng"
      },
      {
        "name" : "Ziquan Fang"
      },
      {
        "name" : "Wei Shao"
      },
      {
        "name" : "Lu Chen"
      },
      {
        "name" : "Yunjun Gao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05031v1",
    "title" : "LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving\n  Cloud-Device Collaboration",
    "summary" : "Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)\nfor handling public user queries and on-device Small Language Models (SLMs) for\nprocessing private user data, collectively forming a powerful and\nprivacy-preserving solution. However, existing approaches often fail to fully\nleverage the scalable problem-solving capabilities of on-cloud LLMs while\nunderutilizing the advantage of on-device SLMs in accessing and processing\npersonalized data. This leads to two interconnected issues: 1) Limited\nutilization of the problem-solving capabilities of on-cloud LLMs, which fail to\nalign with personalized user-task needs, and 2) Inadequate integration of user\ndata into on-device SLM responses, resulting in mismatches in contextual user\ninformation.\n  In this paper, we propose a Leader-Subordinate Retrieval framework for\nPrivacy-preserving cloud-device collaboration (LSRP), a novel solution that\nbridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM\nthrough a dynamic selection of task-specific leader strategies named as\nuser-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the\ndata advantages of on-device SLMs through small model feedback Direct\nPreference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the\non-device SLM. Experiments on two datasets demonstrate that LSRP consistently\noutperforms state-of-the-art baselines, significantly improving question-answer\nrelevance and personalization, while preserving user privacy through efficient\non-device retrieval. Our code is available at:\nhttps://github.com/Zhang-Yingyi/LSRP.",
    "updated" : "2025-05-08T08:06:34Z",
    "published" : "2025-05-08T08:06:34Z",
    "authors" : [
      {
        "name" : "Yingyi Zhang"
      },
      {
        "name" : "Pengyue Jia"
      },
      {
        "name" : "Xianneng Li"
      },
      {
        "name" : "Derong Xu"
      },
      {
        "name" : "Maolin Wang"
      },
      {
        "name" : "Yichao Wang"
      },
      {
        "name" : "Zhaocheng Du"
      },
      {
        "name" : "Huifeng Guo"
      },
      {
        "name" : "Yong Liu"
      },
      {
        "name" : "Ruiming Tang"
      },
      {
        "name" : "Xiangyu Zhao"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04889v1",
    "title" : "FedRE: Robust and Effective Federated Learning with Privacy Preference",
    "summary" : "Despite Federated Learning (FL) employing gradient aggregation at the server\nfor distributed training to prevent the privacy leakage of raw data, private\ninformation can still be divulged through the analysis of uploaded gradients\nfrom clients. Substantial efforts have been made to integrate local\ndifferential privacy (LDP) into the system to achieve a strict privacy\nguarantee. However, existing methods fail to take practical issues into account\nby merely perturbing each sample with the same mechanism while each client may\nhave their own privacy preferences on privacy-sensitive information (PSI),\nwhich is not uniformly distributed across the raw data. In such a case,\nexcessive privacy protection from private-insensitive information can\nadditionally introduce unnecessary noise, which may degrade the model\nperformance. In this work, we study the PSI within data and develop FedRE, that\ncan simultaneously achieve robustness and effectiveness benefits with LDP\nprotection. More specifically, we first define PSI with regard to the privacy\npreferences of each client. Then, we optimize the LDP by allocating less\nprivacy budget to gradients with higher PSI in a layer-wise manner, thus\nproviding a stricter privacy guarantee for PSI. Furthermore, to mitigate the\nperformance degradation caused by LDP, we design a parameter aggregation\nmechanism based on the distribution of the perturbed information. We conducted\nexperiments with text tamper detection on T-SROIE and DocTamper datasets, and\nFedRE achieves competitive performance compared to state-of-the-art methods.",
    "updated" : "2025-05-08T01:50:27Z",
    "published" : "2025-05-08T01:50:27Z",
    "authors" : [
      {
        "name" : "Tianzhe Xiao"
      },
      {
        "name" : "Yichen Li"
      },
      {
        "name" : "Yu Zhou"
      },
      {
        "name" : "Yining Qi"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Haozhao Wang"
      },
      {
        "name" : "Yi Wang"
      },
      {
        "name" : "Ruixuan Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04799v1",
    "title" : "Safeguard-by-Development: A Privacy-Enhanced Development Paradigm for\n  Multi-Agent Collaboration Systems",
    "summary" : "Multi-agent collaboration systems (MACS), powered by large language models\n(LLMs), solve complex problems efficiently by leveraging each agent's\nspecialization and communication between agents. However, the inherent exchange\nof information between agents and their interaction with external environments,\nsuch as LLM, tools, and users, inevitably introduces significant risks of\nsensitive data leakage, including vulnerabilities to attacks like prompt\ninjection and reconnaissance. Existing MACS fail to enable privacy controls,\nmaking it challenging to manage sensitive information securely. In this paper,\nwe take the first step to address the MACS's data leakage threat at the system\ndevelopment level through a privacy-enhanced development paradigm, Maris. Maris\nenables rigorous message flow control within MACS by embedding reference\nmonitors into key multi-agent conversation components. We implemented Maris as\nan integral part of AutoGen, a widely adopted open-source multi-agent\ndevelopment framework. Then, we evaluate Maris for its effectiveness and\nperformance overhead on privacy-critical MACS use cases, including healthcare,\nsupply chain optimization, and personalized recommendation system. The result\nshows that Maris achieves satisfactory effectiveness, performance overhead and\npracticability for adoption.",
    "updated" : "2025-05-07T20:54:43Z",
    "published" : "2025-05-07T20:54:43Z",
    "authors" : [
      {
        "name" : "Jian Cui"
      },
      {
        "name" : "Zichuan Li"
      },
      {
        "name" : "Luyi Xing"
      },
      {
        "name" : "Xiaojing Liao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.06122v1",
    "title" : "Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled\n  Systems via Particle Filter Reinforcement Learning",
    "summary" : "This paper addresses the problem of parameter privacy-preserving data sharing\nin coupled systems, where a data provider shares data with a data user but\nwants to protect its sensitive parameters. The shared data affects not only the\ndata user's decision-making but also the data provider's operations through\nsystem interactions. To trade off control performance and privacy, we propose\nan interaction-aware privacy-preserving data sharing approach. Our approach\ngenerates distorted data by minimizing a combination of (i) mutual information,\nquantifying privacy leakage of sensitive parameters, and (ii) the impact of\ndistorted data on the data provider's control performance, considering the\ninteractions between stakeholders. The optimization problem is formulated into\na Bellman equation and solved by a particle filter reinforcement learning\n(RL)-based approach. Compared to existing RL-based methods, our formulation\nsignificantly reduces history dependency and efficiently handles scenarios with\ncontinuous state space. Validated in a mixed-autonomy platoon scenario, our\nmethod effectively protects sensitive driving behavior parameters of\nhuman-driven vehicles (HDVs) against inference attacks while maintaining\nnegligible impact on fuel efficiency.",
    "updated" : "2025-05-09T15:25:48Z",
    "published" : "2025-05-09T15:25:48Z",
    "authors" : [
      {
        "name" : "Haokun Yu"
      },
      {
        "name" : "Jingyuan Zhou"
      },
      {
        "name" : "Kaidi Yang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05922v1",
    "title" : "CAPE: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy",
    "summary" : "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.",
    "updated" : "2025-05-09T09:54:07Z",
    "published" : "2025-05-09T09:54:07Z",
    "authors" : [
      {
        "name" : "Haoqi Wu"
      },
      {
        "name" : "Wei Dai"
      },
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Qiang Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05920v1",
    "title" : "Privacy-Preserving Credit Card Approval Using Homomorphic SVM: Toward\n  Secure Inference in FinTech Applications",
    "summary" : "The growing use of machine learning in cloud environments raises critical\nconcerns about data security and privacy, especially in finance. Fully\nHomomorphic Encryption (FHE) offers a solution by enabling computations on\nencrypted data, but its high computational cost limits practicality. In this\npaper, we propose PP-FinTech, a privacy-preserving scheme for financial\napplications that employs a CKKS-based encrypted soft-margin SVM, enhanced with\na hybrid kernel for modeling non-linear patterns and an adaptive thresholding\nmechanism for robust encrypted classification. Experiments on the Credit Card\nApproval dataset demonstrate comparable performance to the plaintext models,\nhighlighting PP-FinTech's ability to balance privacy, and efficiency in secure\nfinancial ML systems.",
    "updated" : "2025-05-09T09:46:56Z",
    "published" : "2025-05-09T09:46:56Z",
    "authors" : [
      {
        "name" : " Faneela"
      },
      {
        "name" : "Baraq Ghaleb"
      },
      {
        "name" : "Jawad Ahmad"
      },
      {
        "name" : "William J. Buchanan"
      },
      {
        "name" : "Sana Ullah Jan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05859v1",
    "title" : "Integrating Building Thermal Flexibility Into Distribution System: A\n  Privacy-Preserved Dispatch Approach",
    "summary" : "The inherent thermal storage capacity of buildings brings considerable\nthermal flexibility to the heating/cooling loads, which are promising demand\nresponse resources for power systems. It is widely believed that integrating\nthe thermal flexibility of buildings into the distribution system can improve\nthe operating economy and reliability of the system. However, the private\ninformation of the buildings needs to be transferred to the distribution system\noperator (DSO) to achieve a coordinated optimization, bringing serious privacy\nconcerns to users. Given this issue, we propose a novel privacy-preserved\noptimal dispatch approach for the distribution system incorporating buildings.\nUsing it, the DSO can exploit the thermal flexibility of buildings without\naccessing their private information, such as model parameters and indoor\ntemperature profiles. Specifically, we first develop an optimal dispatch model\nfor the distribution system integrating buildings, which can be extended to\nother storage-like flexibility resources. Second, we reveal that the\nprivacy-preserved integration of buildings is a joint privacy preservation\nproblem for both parameters and state variables and then design a\nprivacy-preserved algorithm based on transformation-based encryption,\nconstraint relaxation, and constraint extension techniques. Besides, we\nimplement a detailed privacy analysis for the proposed method, considering both\nsemi-honest adversaries and external eavesdroppers. Case studies demonstrate\nthe accuracy, privacy-preserved performance, and computational efficiency of\nthe proposed method.",
    "updated" : "2025-05-09T07:53:08Z",
    "published" : "2025-05-09T07:53:08Z",
    "authors" : [
      {
        "name" : "Shuai Lu"
      },
      {
        "name" : "Zeyin Hou"
      },
      {
        "name" : "Wei Gu"
      },
      {
        "name" : "Yijun Xu"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05843v1",
    "title" : "Enhancing Noisy Functional Encryption for Privacy-Preserving Machine\n  Learning",
    "summary" : "Functional encryption (FE) has recently attracted interest in\nprivacy-preserving machine learning (PPML) for its unique ability to compute\nspecific functions on encrypted data. A related line of work focuses on noisy\nFE, which ensures differential privacy in the output while keeping the data\nencrypted. We extend the notion of noisy multi-input functional encryption\n(NMIFE) to (dynamic) noisy multi-client functional encryption ((Dy)NMCFE),\nwhich allows for more flexibility in the number of data holders and analyses,\nwhile protecting the privacy of the data holder with fine-grained access\nthrough the usage of labels. Following our new definition of DyNMCFE, we\npresent DyNo, a concrete inner-product DyNMCFE scheme. Our scheme captures all\nthe functionalities previously introduced in noisy FE schemes, while being\nsignificantly more efficient in terms of space and runtime and fulfilling a\nstronger security notion by allowing the corruption of clients. To further\nprove the applicability of DyNMCFE, we present a protocol for PPML based on\nDyNo. According to this protocol, we train a privacy-preserving logistic\nregression.",
    "updated" : "2025-05-09T07:33:09Z",
    "published" : "2025-05-09T07:33:09Z",
    "authors" : [
      {
        "name" : "Linda Scheu-Hachtel"
      },
      {
        "name" : "Jasmin Zalonis"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05816v1",
    "title" : "On the Price of Differential Privacy for Spectral Clustering over\n  Stochastic Block Models",
    "summary" : "We investigate privacy-preserving spectral clustering for community detection\nwithin stochastic block models (SBMs). Specifically, we focus on edge\ndifferential privacy (DP) and propose private algorithms for community\nrecovery. Our work explores the fundamental trade-offs between the privacy\nbudget and the accurate recovery of community labels. Furthermore, we establish\ninformation-theoretic conditions that guarantee the accuracy of our methods,\nproviding theoretical assurances for successful community recovery under edge\nDP.",
    "updated" : "2025-05-09T06:34:56Z",
    "published" : "2025-05-09T06:34:56Z",
    "authors" : [
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Andrea J. Goldsmith"
      }
    ],
    "categories" : [
      "cs.SI",
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05707v1",
    "title" : "Crowding Out The Noise: Algorithmic Collective Action Under Differential\n  Privacy",
    "summary" : "The integration of AI into daily life has generated considerable attention\nand excitement, while also raising concerns about automating algorithmic harms\nand re-entrenching existing social inequities. While the responsible deployment\nof trustworthy AI systems is a worthy goal, there are many possible ways to\nrealize it, from policy and regulation to improved algorithm design and\nevaluation. In fact, since AI trains on social data, there is even a\npossibility for everyday users, citizens, or workers to directly steer its\nbehavior through Algorithmic Collective Action, by deliberately modifying the\ndata they share with a platform to drive its learning process in their favor.\nThis paper considers how these grassroots efforts to influence AI interact with\nmethods already used by AI firms and governments to improve model\ntrustworthiness. In particular, we focus on the setting where the AI firm\ndeploys a differentially private model, motivated by the growing regulatory\nfocus on privacy and data protection. We investigate how the use of\nDifferentially Private Stochastic Gradient Descent (DPSGD) affects the\ncollective's ability to influence the learning process. Our findings show that\nwhile differential privacy contributes to the protection of individual data, it\nintroduces challenges for effective algorithmic collective action. We\ncharacterize lower bounds on the success of algorithmic collective action under\ndifferential privacy as a function of the collective's size and the firm's\nprivacy parameters, and verify these trends experimentally by simulating\ncollective action during the training of deep neural network classifiers across\nseveral datasets.",
    "updated" : "2025-05-09T00:55:12Z",
    "published" : "2025-05-09T00:55:12Z",
    "authors" : [
      {
        "name" : "Rushabh Solanki"
      },
      {
        "name" : "Meghana Bhange"
      },
      {
        "name" : "Ulrich Aïvodji"
      },
      {
        "name" : "Elliot Creager"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05648v1",
    "title" : "Privacy-Preserving Transformers: SwiftKey's Differential Privacy\n  Implementation",
    "summary" : "In this paper we train a transformer using differential privacy (DP) for\nlanguage modeling in SwiftKey. We run multiple experiments to balance the\ntrade-off between the model size, run-time speed and accuracy. We show that we\nget small and consistent gains in the next-word-prediction and accuracy with\ngraceful increase in memory and speed compared to the production GRU. This is\nobtained by scaling down a GPT2 architecture to fit the required size and a two\nstage training process that builds a seed model on general data and DP\nfinetunes it on typing data. The transformer is integrated using ONNX offering\nboth flexibility and efficiency.",
    "updated" : "2025-05-08T21:08:04Z",
    "published" : "2025-05-08T21:08:04Z",
    "authors" : [
      {
        "name" : "Abdelrahman Abouelenin"
      },
      {
        "name" : "Mohamed Abdelrehim"
      },
      {
        "name" : "Raffy Fahim"
      },
      {
        "name" : "Amr Hendy"
      },
      {
        "name" : "Mohamed Afify"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05613v1",
    "title" : "Optimal Regret of Bernoulli Bandits under Global Differential Privacy",
    "summary" : "As sequential learning algorithms are increasingly applied to real life,\nensuring data privacy while maintaining their utilities emerges as a timely\nquestion. In this context, regret minimisation in stochastic bandits under\n$\\epsilon$-global Differential Privacy (DP) has been widely studied. Unlike\nbandits without DP, there is a significant gap between the best-known regret\nlower and upper bound in this setting, though they \"match\" in order. Thus, we\nrevisit the regret lower and upper bounds of $\\epsilon$-global DP algorithms\nfor Bernoulli bandits and improve both. First, we prove a tighter regret lower\nbound involving a novel information-theoretic quantity characterising the\nhardness of $\\epsilon$-global DP in stochastic bandits. Our lower bound\nstrictly improves on the existing ones across all $\\epsilon$ values. Then, we\nchoose two asymptotically optimal bandit algorithms, i.e. DP-KLUCB and DP-IMED,\nand propose their DP versions using a unified blueprint, i.e., (a) running in\narm-dependent phases, and (b) adding Laplace noise to achieve privacy. For\nBernoulli bandits, we analyse the regrets of these algorithms and show that\ntheir regrets asymptotically match our lower bound up to a constant arbitrary\nclose to 1. This refutes the conjecture that forgetting past rewards is\nnecessary to design optimal bandit algorithms under global DP. At the core of\nour algorithms lies a new concentration inequality for sums of Bernoulli\nvariables under Laplace mechanism, which is a new DP version of the Chernoff\nbound. This result is universally useful as the DP literature commonly treats\nthe concentrations of Laplace noise and random variables separately, while we\ncouple them to yield a tighter bound.",
    "updated" : "2025-05-08T19:48:58Z",
    "published" : "2025-05-08T19:48:58Z",
    "authors" : [
      {
        "name" : "Achraf Azize"
      },
      {
        "name" : "Yulian Wu"
      },
      {
        "name" : "Junya Honda"
      },
      {
        "name" : "Francesco Orabona"
      },
      {
        "name" : "Shinji Ito"
      },
      {
        "name" : "Debabrota Basu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05519v1",
    "title" : "Real-Time Privacy Preservation for Robot Visual Perception",
    "summary" : "Many robots (e.g., iRobot's Roomba) operate based on visual observations from\nlive video streams, and such observations may inadvertently include\nprivacy-sensitive objects, such as personal identifiers. Existing approaches\nfor preserving privacy rely on deep learning models, differential privacy, or\ncryptography. They lack guarantees for the complete concealment of all\nsensitive objects. Guaranteeing concealment requires post-processing techniques\nand thus is inadequate for real-time video streams. We develop a method for\nprivacy-constrained video streaming, PCVS, that conceals sensitive objects\nwithin real-time video streams. PCVS takes a logical specification constraining\nthe existence of privacy-sensitive objects, e.g., never show faces when a\nperson exists. It uses a detection model to evaluate the existence of these\nobjects in each incoming frame. Then, it blurs out a subset of objects such\nthat the existence of the remaining objects satisfies the specification. We\nthen propose a conformal prediction approach to (i) establish a theoretical\nlower bound on the probability of the existence of these objects in a sequence\nof frames satisfying the specification and (ii) update the bound with the\narrival of each subsequent frame. Quantitative evaluations show that PCVS\nachieves over 95 percent specification satisfaction rate in multiple datasets,\nsignificantly outperforming other methods. The satisfaction rate is\nconsistently above the theoretical bounds across all datasets, indicating that\nthe established bounds hold. Additionally, we deploy PCVS on robots in\nreal-time operation and show that the robots operate normally without being\ncompromised when PCVS conceals objects.",
    "updated" : "2025-05-08T03:27:12Z",
    "published" : "2025-05-08T03:27:12Z",
    "authors" : [
      {
        "name" : "Minkyu Choi"
      },
      {
        "name" : "Yunhao Yang"
      },
      {
        "name" : "Neel P. Bhatt"
      },
      {
        "name" : "Kushagra Gupta"
      },
      {
        "name" : "Sahil Shah"
      },
      {
        "name" : "Aditya Rai"
      },
      {
        "name" : "David Fridovich-Keil"
      },
      {
        "name" : "Ufuk Topcu"
      },
      {
        "name" : "Sandeep P. Chinchali"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02392v2",
    "title" : "Moneros Decentralized P2P Exchanges: Functionality, Adoption, and\n  Privacy Risks",
    "summary" : "Privacy-focused cryptocurrencies like Monero remain popular, despite\nincreasing regulatory scrutiny that has led to their delisting from major\ncentralized exchanges. The latter also explains the recent popularity of\ndecentralized exchanges (DEXs) with no centralized ownership structures. These\nplatforms typically leverage peer-to-peer (P2P) networks, promising secure and\nanonymous asset trading. However, questions of liability remain, and the\nacademic literature lacks comprehensive insights into the functionality,\ntrading activity, and privacy claims of these P2P platforms. In this paper, we\nprovide an early systematization of the current landscape of decentralized\npeer-to-peer exchanges within the Monero ecosystem. We examine several recently\ndeveloped DEX platforms, analyzing their popularity, functionality,\narchitectural choices, and potential weaknesses. We further identify and report\non a privacy vulnerability in the recently popularized Haveno exchange,\ndemonstrating that certain Haveno trades could be detected, allowing\ntransactions to be linked across the Monero and Bitcoin blockchains. We hope\nthat our findings can nourish the discussion in the research community about\nmore secure designs, and provide insights for regulators.",
    "updated" : "2025-05-09T15:32:20Z",
    "published" : "2025-05-05T06:27:37Z",
    "authors" : [
      {
        "name" : "Yannik Kopyciok"
      },
      {
        "name" : "Friedhelm Victor"
      },
      {
        "name" : "Stefan Schmid"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  }
]