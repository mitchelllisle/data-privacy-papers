[
  {
    "id" : "http://arxiv.org/abs/2505.00593v1",
    "title" : "A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security\n  and Privacy in IoT and Edge Networks",
    "summary" : "The security of image data in the Internet of Things (IoT) and edge networks\nis crucial due to the increasing deployment of intelligent systems for\nreal-time decision-making. Traditional encryption algorithms such as AES and\nRSA are computationally expensive for resource-constrained IoT devices and\nineffective for large-volume image data, leading to inefficiencies in\nprivacy-preserving distributed learning applications. To address these\nconcerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption\nscheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic\nChain Permutation and Confusion mechanisms to enhance security while\nmaintaining efficiency. The proposed scheme consists of three stages: (1) FAPS,\nwhich extracts and reorganizes pixels based on high and low edge intensity\nfeatures for correlation disruption; (2) Chaotic Chain Permutation, which\nemploys a logistic chaotic map with SHA-256-based dynamically updated keys for\nblock-wise permutation; and (3) Chaotic chain Confusion, which utilises\ndynamically generated chaotic seed matrices for bitwise XOR operations.\nExtensive security and performance evaluations demonstrate that the proposed\nscheme significantly reduces pixel correlation -- almost zero, achieves high\nentropy values close to 8, and resists differential cryptographic attacks. The\noptimum design of the proposed scheme makes it suitable for real-time\ndeployment in resource-constrained environments.",
    "updated" : "2025-05-01T15:26:48Z",
    "published" : "2025-05-01T15:26:48Z",
    "authors" : [
      {
        "name" : "Muhammad Shahbaz Khan"
      },
      {
        "name" : "Ahmed Al-Dubai"
      },
      {
        "name" : "Jawad Ahmad"
      },
      {
        "name" : "Nikolaos Pitropakis"
      },
      {
        "name" : "Baraq Ghaleb"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.00257v1",
    "title" : "Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial\n  Data Circulation",
    "summary" : "The sharing of external data has become a strong demand of financial\ninstitutions, but the privacy issue has led to the difficulty of\ninterconnecting different platforms and the low degree of data openness. To\neffectively solve the privacy problem of financial data in trans-border flow\nand sharing, to ensure that the data is available but not visible, to realize\nthe joint portrait of all kinds of heterogeneous data of business organizations\nin different industries, we propose a Heterogeneous Federated Graph Neural\nNetwork (HFGNN) approach. In this method, the distribution of heterogeneous\nbusiness data of trans-border organizations is taken as subgraphs, and the\nsharing and circulation process among subgraphs is constructed as a\nstatistically heterogeneous global graph through a central server. Each\nsubgraph learns the corresponding personalized service model through local\ntraining to select and update the relevant subset of subgraphs with aggregated\nparameters, and effectively separates and combines topological and feature\ninformation among subgraphs. Finally, our simulation experimental results show\nthat the proposed method has higher accuracy performance and faster convergence\nspeed than existing methods.",
    "updated" : "2025-05-01T02:47:43Z",
    "published" : "2025-05-01T02:47:43Z",
    "authors" : [
      {
        "name" : "Zhizhong Tan"
      },
      {
        "name" : "Jiexin Zheng"
      },
      {
        "name" : "Kevin Qi Zhang"
      },
      {
        "name" : "Wenyong Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01292v1",
    "title" : "Fine-grained Manipulation Attacks to Local Differential Privacy\n  Protocols for Data Streams",
    "summary" : "Local Differential Privacy (LDP) enables massive data collection and analysis\nwhile protecting end users' privacy against untrusted aggregators. It has been\napplied to various data types (e.g., categorical, numerical, and graph data)\nand application settings (e.g., static and streaming). Recent findings indicate\nthat LDP protocols can be easily disrupted by poisoning or manipulation\nattacks, which leverage injected/corrupted fake users to send crafted data\nconforming to the LDP reports. However, current attacks primarily target static\nprotocols, neglecting the security of LDP protocols in the streaming settings.\nOur research fills the gap by developing novel fine-grained manipulation\nattacks to LDP protocols for data streams. By reviewing the attack surfaces in\nexisting algorithms, We introduce a unified attack framework with composable\nmodules, which can manipulate the LDP estimated stream toward a target stream.\nOur attack framework can adapt to state-of-the-art streaming LDP algorithms\nwith different analytic tasks (e.g., frequency and mean) and LDP models\n(event-level, user-level, w-event level). We validate our attacks theoretically\nand through extensive experiments on real-world datasets, and finally explore a\npossible defense mechanism for mitigating these attacks.",
    "updated" : "2025-05-02T14:09:56Z",
    "published" : "2025-05-02T14:09:56Z",
    "authors" : [
      {
        "name" : "Xinyu Li"
      },
      {
        "name" : "Xuebin Ren"
      },
      {
        "name" : "Shusen Yang"
      },
      {
        "name" : "Liang Shi"
      },
      {
        "name" : "Chia-Mu Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.00951v1",
    "title" : "Preserving Privacy and Utility in LLM-Based Product Recommendations",
    "summary" : "Large Language Model (LLM)-based recommendation systems leverage powerful\nlanguage models to generate personalized suggestions by processing user\ninteractions and preferences. Unlike traditional recommendation systems that\nrely on structured data and collaborative filtering, LLM-based models process\ntextual and contextual information, often using cloud-based infrastructure.\nThis raises privacy concerns, as user data is transmitted to remote servers,\nincreasing the risk of exposure and reducing control over personal information.\nTo address this, we propose a hybrid privacy-preserving recommendation\nframework which separates sensitive from nonsensitive data and only shares the\nlatter with the cloud to harness LLM-powered recommendations. To restore lost\nrecommendations related to obfuscated sensitive data, we design a\nde-obfuscation module that reconstructs sensitive recommendations locally.\nExperiments on real-world e-commerce datasets show that our framework achieves\nalmost the same recommendation utility with a system which shares all data with\nan LLM, while preserving privacy to a large extend. Compared to\nobfuscation-only techniques, our approach improves HR@10 scores and category\ndistribution alignment, offering a better balance between privacy and\nrecommendation quality. Furthermore, our method runs efficiently on\nconsumer-grade hardware, making privacy-aware LLM-based recommendation systems\npractical for real-world use.",
    "updated" : "2025-05-02T01:54:08Z",
    "published" : "2025-05-02T01:54:08Z",
    "authors" : [
      {
        "name" : "Tina Khezresmaeilzadeh"
      },
      {
        "name" : "Jiang Zhang"
      },
      {
        "name" : "Dimitrios Andreadis"
      },
      {
        "name" : "Konstantinos Psounis"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02828v1",
    "title" : "Privacy Risks and Preservation Methods in Explainable Artificial\n  Intelligence: A Scoping Review",
    "summary" : "Explainable Artificial Intelligence (XAI) has emerged as a pillar of\nTrustworthy AI and aims to bring transparency in complex models that are opaque\nby nature. Despite the benefits of incorporating explanations in models, an\nurgent need is found in addressing the privacy concerns of providing this\nadditional information to end users. In this article, we conduct a scoping\nreview of existing literature to elicit details on the conflict between privacy\nand explainability. Using the standard methodology for scoping review, we\nextracted 57 articles from 1,943 studies published from January 2019 to\nDecember 2024. The review addresses 3 research questions to present readers\nwith more understanding of the topic: (1) what are the privacy risks of\nreleasing explanations in AI systems? (2) what current methods have researchers\nemployed to achieve privacy preservation in XAI systems? (3) what constitutes a\nprivacy preserving explanation? Based on the knowledge synthesized from the\nselected studies, we categorize the privacy risks and preservation methods in\nXAI and propose the characteristics of privacy preserving explanations to aid\nresearchers and practitioners in understanding the requirements of XAI that is\nprivacy compliant. Lastly, we identify the challenges in balancing privacy with\nother system desiderata and provide recommendations for achieving privacy\npreserving XAI. We expect that this review will shed light on the complex\nrelationship of privacy and explainability, both being the fundamental\nprinciples of Trustworthy AI.",
    "updated" : "2025-05-05T17:53:28Z",
    "published" : "2025-05-05T17:53:28Z",
    "authors" : [
      {
        "name" : "Sonal Allana"
      },
      {
        "name" : "Mohan Kankanhalli"
      },
      {
        "name" : "Rozita Dara"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02798v1",
    "title" : "Unifying Laplace Mechanism with Instance Optimality in Differential\n  Privacy",
    "summary" : "We adapt the canonical Laplace mechanism, widely used in differentially\nprivate data analysis, to achieve near instance optimality with respect to the\nhardness of the underlying dataset. In particular, we construct a piecewise\nLaplace distribution whereby we defy traditional assumptions and show that\nLaplace noise can in fact be drawn proportional to the local sensitivity when\ndone in a piecewise manner. While it may initially seem counterintuitive that\nthis satisfies (pure) differential privacy and can be sampled, we provide both\nthrough a simple connection to the exponential mechanism and inverse\nsensitivity along with the fact that the Laplace distribution is a two-sided\nexponential distribution. As a result, we prove that in the continuous setting\nour \\textit{piecewise Laplace mechanism} strictly dominates the inverse\nsensitivity mechanism, which was previously shown to both be nearly instance\noptimal and uniformly outperform the smooth sensitivity framework. Furthermore,\nin the worst-case where all local sensitivities equal the global sensitivity,\nour method simply reduces to a Laplace mechanism. We also complement this with\nan approximate local sensitivity variant to potentially ease the computational\ncost, which can also extend to higher dimensions.",
    "updated" : "2025-05-05T17:20:28Z",
    "published" : "2025-05-05T17:20:28Z",
    "authors" : [
      {
        "name" : "David Durfee"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02513v1",
    "title" : "Trustworthy Inter-Provider Agreements in 6G Using a Privacy-Enabled\n  Hybrid Blockchain Framework",
    "summary" : "Inter-provider agreements are central to 6G networks, where administrative\ndomains must securely and dynamically share services. To address the dual need\nfor transparency and confidentiality, we propose a privacy-enabled hybrid\nblockchain setup using Hyperledger Besu, integrating both public and private\ntransaction workflows. The system enables decentralized service registration,\nselection, and SLA breach reporting through role-based smart contracts and\nprivacy groups. We design and deploy a proof-of-concept implementation,\nevaluating performance using end-to-end latency as a key metric within privacy\ngroups. Results show that public interactions maintain stable latency, while\nprivate transactions incur additional overhead due to off-chain coordination.\nThe block production rate governed by IBFT 2.0 had limited impact on private\ntransaction latency, due to encryption and peer synchronization. Lessons\nlearned highlight design considerations for smart contract structure, validator\nmanagement, and scalability patterns suitable for dynamic inter-domain\ncollaboration. Our findings offer practical insights for deploying trustworthy\nagreement systems in 6G networks using privacy-enabled hybrid blockchains.",
    "updated" : "2025-05-05T09:46:30Z",
    "published" : "2025-05-05T09:46:30Z",
    "authors" : [
      {
        "name" : "Farhana Javed"
      },
      {
        "name" : "Josep Mangues-Bafalluy"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02392v1",
    "title" : "Moneros Decentralized P2P Exchanges: Functionality, Adoption, and\n  Privacy Risks",
    "summary" : "Privacy-focused cryptocurrencies like Monero remain popular, despite\nincreasing regulatory scrutiny that has led to their delisting from major\ncentralized exchanges. The latter also explains the recent popularity of\ndecentralized exchanges (DEXs) with no centralized ownership structures. These\nplatforms typically leverage peer-to-peer (P2P) networks, promising secure and\nanonymous asset trading. However, questions of liability remain, and the\nacademic literature lacks comprehensive insights into the functionality,\ntrading activity, and privacy claims of these P2P platforms. In this paper, we\nprovide an early systematization of the current landscape of decentralized\npeer-to-peer exchanges within the Monero ecosystem. We examine several recently\ndeveloped DEX platforms, analyzing their popularity, functionality,\narchitectural choices, and potential weaknesses. We further identify and report\non a privacy vulnerability in the recently popularized Haveno exchange,\ndemonstrating that certain Haveno trades could be detected, allowing\ntransactions to be linked across the Monero and Bitcoin blockchains. We hope\nthat our findings can nourish the discussion in the research community about\nmore secure designs, and provide insights for regulators.",
    "updated" : "2025-05-05T06:27:37Z",
    "published" : "2025-05-05T06:27:37Z",
    "authors" : [
      {
        "name" : "Yannik Kopyciok"
      },
      {
        "name" : "Friedhelm Victor"
      },
      {
        "name" : "Stefan Schmid"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02383v1",
    "title" : "Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs\n  Between Privacy and Regret",
    "summary" : "We address differentially private stochastic bandit problems from the angles\nof exploring the deep connections among Thompson Sampling with Gaussian priors,\nGaussian mechanisms, and Gaussian differential privacy (GDP). We propose\nDP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade\noff privacy and regret. DP-TS-UCB satisfies $ \\tilde{O}\n\\left(T^{0.25(1-\\alpha)}\\right)$-GDP and enjoys an $O\n\\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $\\alpha \\in [0,1]$\ncontrols the trade-off between privacy and regret. Theoretically, our DP-TS-UCB\nrelies on anti-concentration bounds of Gaussian distributions and links\nexploration mechanisms in Thompson Sampling-based algorithms and Upper\nConfidence Bound-based algorithms, which may be of independent interest.",
    "updated" : "2025-05-05T05:48:52Z",
    "published" : "2025-05-05T05:48:52Z",
    "authors" : [
      {
        "name" : "Bingshan Hu"
      },
      {
        "name" : "Zhiming Huang"
      },
      {
        "name" : "Tianyue H. Zhang"
      },
      {
        "name" : "Mathias Lécuyer"
      },
      {
        "name" : "Nidhi Hegde"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01976v1",
    "title" : "A Survey on Privacy Risks and Protection in Large Language Models",
    "summary" : "Although Large Language Models (LLMs) have become increasingly integral to\ndiverse applications, their capabilities raise significant privacy concerns.\nThis survey offers a comprehensive overview of privacy risks associated with\nLLMs and examines current solutions to mitigate these challenges. First, we\nanalyze privacy leakage and attacks in LLMs, focusing on how these models\nunintentionally expose sensitive information through techniques such as model\ninversion, training data extraction, and membership inference. We investigate\nthe mechanisms of privacy leakage, including the unauthorized extraction of\ntraining data and the potential exploitation of these vulnerabilities by\nmalicious actors. Next, we review existing privacy protection against such\nrisks, such as inference detection, federated learning, backdoor mitigation,\nand confidential computing, and assess their effectiveness in preventing\nprivacy leakage. Furthermore, we highlight key practical challenges and propose\nfuture research directions to develop secure and privacy-preserving LLMs,\nemphasizing privacy risk assessment, secure knowledge transfer between models,\nand interdisciplinary frameworks for privacy governance. Ultimately, this\nsurvey aims to establish a roadmap for addressing escalating privacy challenges\nin the LLMs domain.",
    "updated" : "2025-05-04T03:04:07Z",
    "published" : "2025-05-04T03:04:07Z",
    "authors" : [
      {
        "name" : "Kang Chen"
      },
      {
        "name" : "Xiuze Zhou"
      },
      {
        "name" : "Yuanguo Lin"
      },
      {
        "name" : "Shibo Feng"
      },
      {
        "name" : "Li Shen"
      },
      {
        "name" : "Pengcheng Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01879v1",
    "title" : "What to Do When Privacy Is Gone",
    "summary" : "Today's ethics of privacy is largely dedicated to defending personal\ninformation from big data technologies. This essay goes in the other direction.\nIt considers the struggle to be lost, and explores two strategies for living\nafter privacy is gone. First, total exposure embraces privacy's decline, and\nthen contributes to the process with transparency. All personal information is\nshared without reservation. The resulting ethics is explored through a big data\nversion of Robert Nozick's Experience Machine thought experiment. Second,\ntransient existence responds to privacy's loss by ceaselessly generating new\npersonal identities, which translates into constantly producing temporarily\nunviolated private information. The ethics is explored through Gilles Deleuze's\nmetaphysics of difference applied in linguistic terms to the formation of the\nself. Comparing the exposure and transience alternatives leads to the\nconclusion that today's big data reality splits the traditional ethical link\nbetween authenticity and freedom. Exposure provides authenticity, but negates\nhuman freedom. Transience provides freedom, but disdains authenticity.",
    "updated" : "2025-05-03T17:51:36Z",
    "published" : "2025-05-03T17:51:36Z",
    "authors" : [
      {
        "name" : "James Brusseau"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01788v1",
    "title" : "Privacy Preserving Machine Learning Model Personalization through\n  Federated Personalized Learning",
    "summary" : "The widespread adoption of Artificial Intelligence (AI) has been driven by\nsignificant advances in intelligent system research. However, this progress has\nraised concerns about data privacy, leading to a growing awareness of the need\nfor privacy-preserving AI. In response, there has been a seismic shift in\ninterest towards the leading paradigm for training Machine Learning (ML) models\non decentralized data silos while maintaining data privacy, Federated Learning\n(FL). This research paper presents a comprehensive performance analysis of a\ncutting-edge approach to personalize ML model while preserving privacy achieved\nthrough Privacy Preserving Machine Learning with the innovative framework of\nFederated Personalized Learning (PPMLFPL). Regarding the increasing concerns\nabout data privacy, this study evaluates the effectiveness of PPMLFPL\naddressing the critical balance between personalized model refinement and\nmaintaining the confidentiality of individual user data. According to our\nanalysis, Adaptive Personalized Cross-Silo Federated Learning with Differential\nPrivacy (APPLE+DP) offering efficient execution whereas overall, the use of the\nAdaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption\n(APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated\npersonalized learning settings is strongly suggested. The results offer\nvaluable insights creating it a promising scope for future advancements in the\nfield of privacy-conscious data-driven technologies.",
    "updated" : "2025-05-03T11:31:38Z",
    "published" : "2025-05-03T11:31:38Z",
    "authors" : [
      {
        "name" : "Md. Tanzib Hosain"
      },
      {
        "name" : "Asif Zaman"
      },
      {
        "name" : "Md. Shahriar Sajid"
      },
      {
        "name" : "Shadman Sakeeb Khan"
      },
      {
        "name" : "Shanjida Akter"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.01524v1",
    "title" : "The DCR Delusion: Measuring the Privacy Risk of Synthetic Data",
    "summary" : "Synthetic data has become an increasingly popular way to share data without\nrevealing sensitive information. Though Membership Inference Attacks (MIAs) are\nwidely considered the gold standard for empirically assessing the privacy of a\nsynthetic dataset, practitioners and researchers often rely on simpler proxy\nmetrics such as Distance to Closest Record (DCR). These metrics estimate\nprivacy by measuring the similarity between the training data and generated\nsynthetic data. This similarity is also compared against that between the\ntraining data and a disjoint holdout set of real records to construct a binary\nprivacy test. If the synthetic data is not more similar to the training data\nthan the holdout set is, it passes the test and is considered private. In this\nwork we show that, while computationally inexpensive, DCR and other\ndistance-based metrics fail to identify privacy leakage. Across multiple\ndatasets and both classical models such as Baynet and CTGAN and more recent\ndiffusion models, we show that datasets deemed private by proxy metrics are\nhighly vulnerable to MIAs. We similarly find both the binary privacy test and\nthe continuous measure based on these metrics to be uninformative of actual\nmembership inference risk. We further show that these failures are consistent\nacross different metric hyperparameter settings and record selection methods.\nFinally, we argue DCR and other distance-based metrics to be flawed by design\nand show a example of a simple leakage they miss in practice. With this work,\nwe hope to motivate practitioners to move away from proxy metrics to MIAs as\nthe rigorous, comprehensive standard of evaluating privacy of synthetic data,\nin particular to make claims of datasets being legally anonymous.",
    "updated" : "2025-05-02T18:21:14Z",
    "published" : "2025-05-02T18:21:14Z",
    "authors" : [
      {
        "name" : "Zexi Yao"
      },
      {
        "name" : "Nataša Krčo"
      },
      {
        "name" : "Georgi Ganev"
      },
      {
        "name" : "Yves-Alexandre de Montjoye"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.03639v1",
    "title" : "Differential Privacy for Network Assortativity",
    "summary" : "The analysis of network assortativity is of great importance for\nunderstanding the structural characteristics of and dynamics upon networks.\nOften, network assortativity is quantified using the assortativity coefficient\nthat is defined based on the Pearson correlation coefficient between vertex\ndegrees. It is well known that a network may contain sensitive information,\nsuch as the number of friends of an individual in a social network (which is\nabstracted as the degree of vertex.). So, the computation of the assortativity\ncoefficient leads to privacy leakage, which increases the urgent need for\nprivacy-preserving protocol. However, there has been no scheme addressing the\nconcern above.\n  To bridge this gap, in this work, we are the first to propose approaches\nbased on differential privacy (DP for short). Specifically, we design three\nDP-based algorithms: $Local_{ru}$, $Shuffle_{ru}$, and $Decentral_{ru}$. The\nfirst two algorithms, based on Local DP (LDP) and Shuffle DP respectively, are\ndesigned for settings where each individual only knows his/her direct friends.\nIn contrast, the third algorithm, based on Decentralized DP (DDP), targets\nscenarios where each individual has a broader view, i.e., also knowing his/her\nfriends' friends. Theoretically, we prove that each algorithm enables an\nunbiased estimation of the assortativity coefficient of the network. We further\nevaluate the performance of the proposed algorithms using mean squared error\n(MSE), showing that $Shuffle_{ru}$ achieves the best performance, followed by\n$Decentral_{ru}$, with $Local_{ru}$ performing the worst. Note that these three\nalgorithms have different assumptions, so each has its applicability scenario.\nLastly, we conduct extensive numerical simulations, which demonstrate that the\npresented approaches are adequate to achieve the estimation of network\nassortativity under the demand for privacy protection.",
    "updated" : "2025-05-06T15:40:47Z",
    "published" : "2025-05-06T15:40:47Z",
    "authors" : [
      {
        "name" : "Fei Ma"
      },
      {
        "name" : "Jinzhi Ouyang"
      },
      {
        "name" : "Xincheng Hu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02975v1",
    "title" : "Navigating Privacy and Trust: AI Assistants as Social Support for Older\n  Adults",
    "summary" : "AI assistants are increasingly integrated into older adults' daily lives,\noffering new opportunities for social support and accessibility while raising\nimportant questions about privacy, autonomy, and trust. As these systems become\nembedded in caregiving and social networks, older adults must navigate\ntrade-offs between usability, data privacy, and personal agency across\ndifferent interaction contexts. Although prior work has explored AI assistants'\npotential benefits, further research is needed to understand how perceived\nusefulness and risk shape adoption and engagement. This paper examines these\ndynamics and advocates for participatory design approaches that position older\nadults as active decision makers in shaping AI assistant functionality. By\nadvancing a framework for privacy-aware, user-centered AI design, this work\ncontributes to ongoing discussions on developing ethical and transparent AI\nsystems that enhance well-being without compromising user control.",
    "updated" : "2025-05-05T19:00:14Z",
    "published" : "2025-05-05T19:00:14Z",
    "authors" : [
      {
        "name" : "Karina LaRubbio"
      },
      {
        "name" : "Malcolm Grba"
      },
      {
        "name" : "Diana Freed"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04570v1",
    "title" : "Privacy-preserving neutral atom-based quantum classifier towards real\n  healthcare applications",
    "summary" : "Technological advances in Artificial Intelligence (AI) and Machine Learning\n(ML) for the healthcare domain are rapidly arising, with a growing discussion\nregarding the ethical management of their development. In general, ML\nhealthcare applications crucially require performance, interpretability of\ndata, and respect for data privacy. The latter is an increasingly debated topic\nas commercial cloud computing services become more and more widespread.\nRecently, dedicated methods are starting to be developed aiming to protect data\nprivacy. However, these generally result in a trade-off forcing one to balance\nthe level of data privacy and the algorithm performance. Here, a Support Vector\nMachine (SVM) classifier model is proposed whose training is reformulated into\na Quadratic Unconstrained Binary Optimization (QUBO) problem, and adapted to a\nneutral atom-based Quantum Processing Unit (QPU). Our final model does not\nrequire anonymization techniques to protect data privacy since the sensitive\ndata are not needed to be transferred to the cloud-available QPU. Indeed, the\nlatter is used only during the training phase, hence allowing a future concrete\napplication in a real-world scenario. Finally, performance and scaling analyses\non a publicly available breast cancer dataset are discussed, both using ideal\nand noisy simulations for the training process, and also successfully tested on\na currently available real neutral-atom QPU.",
    "updated" : "2025-05-07T17:03:35Z",
    "published" : "2025-05-07T17:03:35Z",
    "authors" : [
      {
        "name" : "Ettore Canonici"
      },
      {
        "name" : "Filippo Caruso"
      }
    ],
    "categories" : [
      "quant-ph",
      "81V45 (Primary) 81P68 (Secondary)",
      "I.2.0; J.2; I.5.0"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04361v1",
    "title" : "RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery\n  Scheme in Mobile Crowdsensing",
    "summary" : "Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS).\nHowever, existing TD methods, including privacy-preserving TD approaches,\nestimate the truth by weighting only the data submitted in the current round,\nwhich often results in low data quality. Moreover, there is a lack of effective\nTD methods that preserve both reputation and data privacy. To address these\nissues, a Reputation and Data Privacy-Preserving based Truth Discovery\n(RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD\nscheme consists of two key approaches: a Reputation-based Truth Discovery (RTD)\napproach, which integrates the weight of current-round data with workers'\nreputation values to estimate the truth, thereby achieving more accurate\nresults, and a Reputation and Data Privacy-Preserving (RDPP) approach, which\nensures privacy preservation for sensing data and reputation values. First, the\nRDPP approach, when seamlessly integrated with RTD, can effectively evaluate\nthe reliability of workers and their sensing data in a privacy-preserving\nmanner. Second, the RDPP scheme supports reputation-based worker recruitment\nand rewards, ensuring high-quality data collection while incentivizing workers\nto provide accurate information. Comprehensive theoretical analysis and\nextensive experiments based on real-world datasets demonstrate that the\nproposed RDPP-TD scheme provides strong privacy protection and improves data\nquality by up to 33.3%.",
    "updated" : "2025-05-07T12:20:55Z",
    "published" : "2025-05-07T12:20:55Z",
    "authors" : [
      {
        "name" : "Lijian Wu"
      },
      {
        "name" : "Weikun Xie"
      },
      {
        "name" : "Wei Tan"
      },
      {
        "name" : "Tian Wang"
      },
      {
        "name" : "Houbing Herbert Song"
      },
      {
        "name" : "Anfeng Liu"
      }
    ],
    "categories" : [
      "cs.CE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04181v1",
    "title" : "Privacy Challenges In Image Processing Applications",
    "summary" : "As image processing systems proliferate, privacy concerns intensify given the\nsensitive personal information contained in images. This paper examines privacy\nchallenges in image processing and surveys emerging privacy-preserving\ntechniques including differential privacy, secure multiparty computation,\nhomomorphic encryption, and anonymization. Key applications with heightened\nprivacy risks include healthcare, where medical images contain patient health\ndata, and surveillance systems that can enable unwarranted tracking.\nDifferential privacy offers rigorous privacy guarantees by injecting controlled\nnoise, while MPC facilitates collaborative analytics without exposing raw data\ninputs. Homomorphic encryption enables computations on encrypted data and\nanonymization directly removes identifying elements. However, balancing privacy\nprotections and utility remains an open challenge. Promising future directions\nidentified include quantum-resilient cryptography, federated learning,\ndedicated hardware, and conceptual innovations like privacy by design.\nUltimately, a holistic effort combining technological innovations, ethical\nconsiderations, and policy frameworks is necessary to uphold the fundamental\nright to privacy as image processing capabilities continue advancing rapidly.",
    "updated" : "2025-05-07T07:28:03Z",
    "published" : "2025-05-07T07:28:03Z",
    "authors" : [
      {
        "name" : " Maneesha"
      },
      {
        "name" : "Bharat Gupta"
      },
      {
        "name" : "Rishabh Sethi"
      },
      {
        "name" : "Charvi Adita Das"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04034v1",
    "title" : "Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency,\n  and Transferability in Spiking Neural Networks",
    "summary" : "Biological neurons exhibit diverse temporal spike patterns, which are\nbelieved to support efficient, robust, and adaptive neural information\nprocessing. While models such as Izhikevich can replicate a wide range of these\nfiring dynamics, their complexity poses challenges for directly integrating\nthem into scalable spiking neural networks (SNN) training pipelines. In this\nwork, we propose two probabilistically driven, input-level temporal spike\ntransformations: Poisson-Burst and Delayed-Burst that introduce biologically\ninspired temporal variability directly into standard Leaky Integrate-and-Fire\n(LIF) neurons. This enables scalable training and systematic evaluation of how\nspike timing dynamics affect privacy, generalization, and learning performance.\nPoisson-Burst modulates burst occurrence based on input intensity, while\nDelayed-Burst encodes input strength through burst onset timing. Through\nextensive experiments across multiple benchmarks, we demonstrate that\nPoisson-Burst maintains competitive accuracy and lower resource overhead while\nexhibiting enhanced privacy robustness against membership inference attacks,\nwhereas Delayed-Burst provides stronger privacy protection at a modest accuracy\ntrade-off. These findings highlight the potential of biologically grounded\ntemporal spike dynamics in improving the privacy, generalization and biological\nplausibility of neuromorphic learning systems.",
    "updated" : "2025-05-07T00:27:00Z",
    "published" : "2025-05-07T00:27:00Z",
    "authors" : [
      {
        "name" : "Ayana Moshruba"
      },
      {
        "name" : "Hamed Poursiami"
      },
      {
        "name" : "Maryam Parsa"
      }
    ],
    "categories" : [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05214v1",
    "title" : "Overcoming the hurdle of legal expertise: A reusable model for\n  smartwatch privacy policies",
    "summary" : "Regulations for privacy protection aim to protect individuals from the\nunauthorized storage, processing, and transfer of their personal data but\noftentimes fail in providing helpful support for understanding these\nregulations. To better communicate privacy policies for smartwatches, we need\nan in-depth understanding of their concepts and provide better ways to enable\ndevelopers to integrate them when engineering systems. Up to now, no conceptual\nmodel exists covering privacy statements from different smartwatch\nmanufacturers that is reusable for developers. This paper introduces such a\nconceptual model for privacy policies of smartwatches and shows its use in a\nmodel-driven software engineering approach to create a platform for data\nvisualization of wearable privacy policies from different smartwatch\nmanufacturers. We have analyzed the privacy policies of various manufacturers\nand extracted the relevant concepts. Moreover, we have checked the model with\nlawyers for its correctness, instantiated it with concrete data, and used it in\na model-driven software engineering approach to create a platform for data\nvisualization. This reusable privacy policy model can enable developers to\neasily represent privacy policies in their systems. This provides a foundation\nfor more structured and understandable privacy policies which, in the long run,\ncan increase the data sovereignty of application users.",
    "updated" : "2025-05-08T13:09:12Z",
    "published" : "2025-05-08T13:09:12Z",
    "authors" : [
      {
        "name" : "Constantin Buschhaus"
      },
      {
        "name" : "Arvid Butting"
      },
      {
        "name" : "Judith Michael"
      },
      {
        "name" : "Verena Nitsch"
      },
      {
        "name" : "Sebastian Pütz"
      },
      {
        "name" : "Bernhard Rumpe"
      },
      {
        "name" : "Carolin Stellmacher"
      },
      {
        "name" : "Sabine Theis"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05155v1",
    "title" : "FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data\n  Preparation via Federated Learning",
    "summary" : "Trajectory data, which capture the movement patterns of people and vehicles\nover time and space, are crucial for applications like traffic optimization and\nurban planning. However, issues such as noise and incompleteness often\ncompromise data quality, leading to inaccurate trajectory analyses and limiting\nthe potential of these applications. While Trajectory Data Preparation (TDP)\ncan enhance data quality, existing methods suffer from two key limitations: (i)\nthey do not address data privacy concerns, particularly in federated settings\nwhere trajectory data sharing is prohibited, and (ii) they typically design\ntask-specific models that lack generalizability across diverse TDP scenarios.\nTo overcome these challenges, we propose FedTDP, a privacy-preserving and\nunified framework that leverages the capabilities of Large Language Models\n(LLMs) for TDP in federated environments. Specifically, we: (i) design a\ntrajectory privacy autoencoder to secure data transmission and protect privacy,\n(ii) introduce a trajectory knowledge enhancer to improve model learning of\nTDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)\npropose federated parallel optimization to enhance training efficiency by\nreducing data transmission and enabling parallel model training. Experiments on\n6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP\nconsistently outperforms 13 state-of-the-art baselines.",
    "updated" : "2025-05-08T11:51:23Z",
    "published" : "2025-05-08T11:51:23Z",
    "authors" : [
      {
        "name" : "Zhihao Zeng"
      },
      {
        "name" : "Ziquan Fang"
      },
      {
        "name" : "Wei Shao"
      },
      {
        "name" : "Lu Chen"
      },
      {
        "name" : "Yunjun Gao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05031v1",
    "title" : "LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving\n  Cloud-Device Collaboration",
    "summary" : "Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)\nfor handling public user queries and on-device Small Language Models (SLMs) for\nprocessing private user data, collectively forming a powerful and\nprivacy-preserving solution. However, existing approaches often fail to fully\nleverage the scalable problem-solving capabilities of on-cloud LLMs while\nunderutilizing the advantage of on-device SLMs in accessing and processing\npersonalized data. This leads to two interconnected issues: 1) Limited\nutilization of the problem-solving capabilities of on-cloud LLMs, which fail to\nalign with personalized user-task needs, and 2) Inadequate integration of user\ndata into on-device SLM responses, resulting in mismatches in contextual user\ninformation.\n  In this paper, we propose a Leader-Subordinate Retrieval framework for\nPrivacy-preserving cloud-device collaboration (LSRP), a novel solution that\nbridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM\nthrough a dynamic selection of task-specific leader strategies named as\nuser-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the\ndata advantages of on-device SLMs through small model feedback Direct\nPreference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the\non-device SLM. Experiments on two datasets demonstrate that LSRP consistently\noutperforms state-of-the-art baselines, significantly improving question-answer\nrelevance and personalization, while preserving user privacy through efficient\non-device retrieval. Our code is available at:\nhttps://github.com/Zhang-Yingyi/LSRP.",
    "updated" : "2025-05-08T08:06:34Z",
    "published" : "2025-05-08T08:06:34Z",
    "authors" : [
      {
        "name" : "Yingyi Zhang"
      },
      {
        "name" : "Pengyue Jia"
      },
      {
        "name" : "Xianneng Li"
      },
      {
        "name" : "Derong Xu"
      },
      {
        "name" : "Maolin Wang"
      },
      {
        "name" : "Yichao Wang"
      },
      {
        "name" : "Zhaocheng Du"
      },
      {
        "name" : "Huifeng Guo"
      },
      {
        "name" : "Yong Liu"
      },
      {
        "name" : "Ruiming Tang"
      },
      {
        "name" : "Xiangyu Zhao"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04889v1",
    "title" : "FedRE: Robust and Effective Federated Learning with Privacy Preference",
    "summary" : "Despite Federated Learning (FL) employing gradient aggregation at the server\nfor distributed training to prevent the privacy leakage of raw data, private\ninformation can still be divulged through the analysis of uploaded gradients\nfrom clients. Substantial efforts have been made to integrate local\ndifferential privacy (LDP) into the system to achieve a strict privacy\nguarantee. However, existing methods fail to take practical issues into account\nby merely perturbing each sample with the same mechanism while each client may\nhave their own privacy preferences on privacy-sensitive information (PSI),\nwhich is not uniformly distributed across the raw data. In such a case,\nexcessive privacy protection from private-insensitive information can\nadditionally introduce unnecessary noise, which may degrade the model\nperformance. In this work, we study the PSI within data and develop FedRE, that\ncan simultaneously achieve robustness and effectiveness benefits with LDP\nprotection. More specifically, we first define PSI with regard to the privacy\npreferences of each client. Then, we optimize the LDP by allocating less\nprivacy budget to gradients with higher PSI in a layer-wise manner, thus\nproviding a stricter privacy guarantee for PSI. Furthermore, to mitigate the\nperformance degradation caused by LDP, we design a parameter aggregation\nmechanism based on the distribution of the perturbed information. We conducted\nexperiments with text tamper detection on T-SROIE and DocTamper datasets, and\nFedRE achieves competitive performance compared to state-of-the-art methods.",
    "updated" : "2025-05-08T01:50:27Z",
    "published" : "2025-05-08T01:50:27Z",
    "authors" : [
      {
        "name" : "Tianzhe Xiao"
      },
      {
        "name" : "Yichen Li"
      },
      {
        "name" : "Yu Zhou"
      },
      {
        "name" : "Yining Qi"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Haozhao Wang"
      },
      {
        "name" : "Yi Wang"
      },
      {
        "name" : "Ruixuan Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.04799v1",
    "title" : "Safeguard-by-Development: A Privacy-Enhanced Development Paradigm for\n  Multi-Agent Collaboration Systems",
    "summary" : "Multi-agent collaboration systems (MACS), powered by large language models\n(LLMs), solve complex problems efficiently by leveraging each agent's\nspecialization and communication between agents. However, the inherent exchange\nof information between agents and their interaction with external environments,\nsuch as LLM, tools, and users, inevitably introduces significant risks of\nsensitive data leakage, including vulnerabilities to attacks like prompt\ninjection and reconnaissance. Existing MACS fail to enable privacy controls,\nmaking it challenging to manage sensitive information securely. In this paper,\nwe take the first step to address the MACS's data leakage threat at the system\ndevelopment level through a privacy-enhanced development paradigm, Maris. Maris\nenables rigorous message flow control within MACS by embedding reference\nmonitors into key multi-agent conversation components. We implemented Maris as\nan integral part of AutoGen, a widely adopted open-source multi-agent\ndevelopment framework. Then, we evaluate Maris for its effectiveness and\nperformance overhead on privacy-critical MACS use cases, including healthcare,\nsupply chain optimization, and personalized recommendation system. The result\nshows that Maris achieves satisfactory effectiveness, performance overhead and\npracticability for adoption.",
    "updated" : "2025-05-07T20:54:43Z",
    "published" : "2025-05-07T20:54:43Z",
    "authors" : [
      {
        "name" : "Jian Cui"
      },
      {
        "name" : "Zichuan Li"
      },
      {
        "name" : "Luyi Xing"
      },
      {
        "name" : "Xiaojing Liao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.06122v1",
    "title" : "Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled\n  Systems via Particle Filter Reinforcement Learning",
    "summary" : "This paper addresses the problem of parameter privacy-preserving data sharing\nin coupled systems, where a data provider shares data with a data user but\nwants to protect its sensitive parameters. The shared data affects not only the\ndata user's decision-making but also the data provider's operations through\nsystem interactions. To trade off control performance and privacy, we propose\nan interaction-aware privacy-preserving data sharing approach. Our approach\ngenerates distorted data by minimizing a combination of (i) mutual information,\nquantifying privacy leakage of sensitive parameters, and (ii) the impact of\ndistorted data on the data provider's control performance, considering the\ninteractions between stakeholders. The optimization problem is formulated into\na Bellman equation and solved by a particle filter reinforcement learning\n(RL)-based approach. Compared to existing RL-based methods, our formulation\nsignificantly reduces history dependency and efficiently handles scenarios with\ncontinuous state space. Validated in a mixed-autonomy platoon scenario, our\nmethod effectively protects sensitive driving behavior parameters of\nhuman-driven vehicles (HDVs) against inference attacks while maintaining\nnegligible impact on fuel efficiency.",
    "updated" : "2025-05-09T15:25:48Z",
    "published" : "2025-05-09T15:25:48Z",
    "authors" : [
      {
        "name" : "Haokun Yu"
      },
      {
        "name" : "Jingyuan Zhou"
      },
      {
        "name" : "Kaidi Yang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05922v1",
    "title" : "CAPE: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy",
    "summary" : "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.",
    "updated" : "2025-05-09T09:54:07Z",
    "published" : "2025-05-09T09:54:07Z",
    "authors" : [
      {
        "name" : "Haoqi Wu"
      },
      {
        "name" : "Wei Dai"
      },
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Qiang Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05920v1",
    "title" : "Privacy-Preserving Credit Card Approval Using Homomorphic SVM: Toward\n  Secure Inference in FinTech Applications",
    "summary" : "The growing use of machine learning in cloud environments raises critical\nconcerns about data security and privacy, especially in finance. Fully\nHomomorphic Encryption (FHE) offers a solution by enabling computations on\nencrypted data, but its high computational cost limits practicality. In this\npaper, we propose PP-FinTech, a privacy-preserving scheme for financial\napplications that employs a CKKS-based encrypted soft-margin SVM, enhanced with\na hybrid kernel for modeling non-linear patterns and an adaptive thresholding\nmechanism for robust encrypted classification. Experiments on the Credit Card\nApproval dataset demonstrate comparable performance to the plaintext models,\nhighlighting PP-FinTech's ability to balance privacy, and efficiency in secure\nfinancial ML systems.",
    "updated" : "2025-05-09T09:46:56Z",
    "published" : "2025-05-09T09:46:56Z",
    "authors" : [
      {
        "name" : " Faneela"
      },
      {
        "name" : "Baraq Ghaleb"
      },
      {
        "name" : "Jawad Ahmad"
      },
      {
        "name" : "William J. Buchanan"
      },
      {
        "name" : "Sana Ullah Jan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05859v1",
    "title" : "Integrating Building Thermal Flexibility Into Distribution System: A\n  Privacy-Preserved Dispatch Approach",
    "summary" : "The inherent thermal storage capacity of buildings brings considerable\nthermal flexibility to the heating/cooling loads, which are promising demand\nresponse resources for power systems. It is widely believed that integrating\nthe thermal flexibility of buildings into the distribution system can improve\nthe operating economy and reliability of the system. However, the private\ninformation of the buildings needs to be transferred to the distribution system\noperator (DSO) to achieve a coordinated optimization, bringing serious privacy\nconcerns to users. Given this issue, we propose a novel privacy-preserved\noptimal dispatch approach for the distribution system incorporating buildings.\nUsing it, the DSO can exploit the thermal flexibility of buildings without\naccessing their private information, such as model parameters and indoor\ntemperature profiles. Specifically, we first develop an optimal dispatch model\nfor the distribution system integrating buildings, which can be extended to\nother storage-like flexibility resources. Second, we reveal that the\nprivacy-preserved integration of buildings is a joint privacy preservation\nproblem for both parameters and state variables and then design a\nprivacy-preserved algorithm based on transformation-based encryption,\nconstraint relaxation, and constraint extension techniques. Besides, we\nimplement a detailed privacy analysis for the proposed method, considering both\nsemi-honest adversaries and external eavesdroppers. Case studies demonstrate\nthe accuracy, privacy-preserved performance, and computational efficiency of\nthe proposed method.",
    "updated" : "2025-05-09T07:53:08Z",
    "published" : "2025-05-09T07:53:08Z",
    "authors" : [
      {
        "name" : "Shuai Lu"
      },
      {
        "name" : "Zeyin Hou"
      },
      {
        "name" : "Wei Gu"
      },
      {
        "name" : "Yijun Xu"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05843v1",
    "title" : "Enhancing Noisy Functional Encryption for Privacy-Preserving Machine\n  Learning",
    "summary" : "Functional encryption (FE) has recently attracted interest in\nprivacy-preserving machine learning (PPML) for its unique ability to compute\nspecific functions on encrypted data. A related line of work focuses on noisy\nFE, which ensures differential privacy in the output while keeping the data\nencrypted. We extend the notion of noisy multi-input functional encryption\n(NMIFE) to (dynamic) noisy multi-client functional encryption ((Dy)NMCFE),\nwhich allows for more flexibility in the number of data holders and analyses,\nwhile protecting the privacy of the data holder with fine-grained access\nthrough the usage of labels. Following our new definition of DyNMCFE, we\npresent DyNo, a concrete inner-product DyNMCFE scheme. Our scheme captures all\nthe functionalities previously introduced in noisy FE schemes, while being\nsignificantly more efficient in terms of space and runtime and fulfilling a\nstronger security notion by allowing the corruption of clients. To further\nprove the applicability of DyNMCFE, we present a protocol for PPML based on\nDyNo. According to this protocol, we train a privacy-preserving logistic\nregression.",
    "updated" : "2025-05-09T07:33:09Z",
    "published" : "2025-05-09T07:33:09Z",
    "authors" : [
      {
        "name" : "Linda Scheu-Hachtel"
      },
      {
        "name" : "Jasmin Zalonis"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05816v1",
    "title" : "On the Price of Differential Privacy for Spectral Clustering over\n  Stochastic Block Models",
    "summary" : "We investigate privacy-preserving spectral clustering for community detection\nwithin stochastic block models (SBMs). Specifically, we focus on edge\ndifferential privacy (DP) and propose private algorithms for community\nrecovery. Our work explores the fundamental trade-offs between the privacy\nbudget and the accurate recovery of community labels. Furthermore, we establish\ninformation-theoretic conditions that guarantee the accuracy of our methods,\nproviding theoretical assurances for successful community recovery under edge\nDP.",
    "updated" : "2025-05-09T06:34:56Z",
    "published" : "2025-05-09T06:34:56Z",
    "authors" : [
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Andrea J. Goldsmith"
      }
    ],
    "categories" : [
      "cs.SI",
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05707v1",
    "title" : "Crowding Out The Noise: Algorithmic Collective Action Under Differential\n  Privacy",
    "summary" : "The integration of AI into daily life has generated considerable attention\nand excitement, while also raising concerns about automating algorithmic harms\nand re-entrenching existing social inequities. While the responsible deployment\nof trustworthy AI systems is a worthy goal, there are many possible ways to\nrealize it, from policy and regulation to improved algorithm design and\nevaluation. In fact, since AI trains on social data, there is even a\npossibility for everyday users, citizens, or workers to directly steer its\nbehavior through Algorithmic Collective Action, by deliberately modifying the\ndata they share with a platform to drive its learning process in their favor.\nThis paper considers how these grassroots efforts to influence AI interact with\nmethods already used by AI firms and governments to improve model\ntrustworthiness. In particular, we focus on the setting where the AI firm\ndeploys a differentially private model, motivated by the growing regulatory\nfocus on privacy and data protection. We investigate how the use of\nDifferentially Private Stochastic Gradient Descent (DPSGD) affects the\ncollective's ability to influence the learning process. Our findings show that\nwhile differential privacy contributes to the protection of individual data, it\nintroduces challenges for effective algorithmic collective action. We\ncharacterize lower bounds on the success of algorithmic collective action under\ndifferential privacy as a function of the collective's size and the firm's\nprivacy parameters, and verify these trends experimentally by simulating\ncollective action during the training of deep neural network classifiers across\nseveral datasets.",
    "updated" : "2025-05-09T00:55:12Z",
    "published" : "2025-05-09T00:55:12Z",
    "authors" : [
      {
        "name" : "Rushabh Solanki"
      },
      {
        "name" : "Meghana Bhange"
      },
      {
        "name" : "Ulrich Aïvodji"
      },
      {
        "name" : "Elliot Creager"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05648v1",
    "title" : "Privacy-Preserving Transformers: SwiftKey's Differential Privacy\n  Implementation",
    "summary" : "In this paper we train a transformer using differential privacy (DP) for\nlanguage modeling in SwiftKey. We run multiple experiments to balance the\ntrade-off between the model size, run-time speed and accuracy. We show that we\nget small and consistent gains in the next-word-prediction and accuracy with\ngraceful increase in memory and speed compared to the production GRU. This is\nobtained by scaling down a GPT2 architecture to fit the required size and a two\nstage training process that builds a seed model on general data and DP\nfinetunes it on typing data. The transformer is integrated using ONNX offering\nboth flexibility and efficiency.",
    "updated" : "2025-05-08T21:08:04Z",
    "published" : "2025-05-08T21:08:04Z",
    "authors" : [
      {
        "name" : "Abdelrahman Abouelenin"
      },
      {
        "name" : "Mohamed Abdelrehim"
      },
      {
        "name" : "Raffy Fahim"
      },
      {
        "name" : "Amr Hendy"
      },
      {
        "name" : "Mohamed Afify"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05613v1",
    "title" : "Optimal Regret of Bernoulli Bandits under Global Differential Privacy",
    "summary" : "As sequential learning algorithms are increasingly applied to real life,\nensuring data privacy while maintaining their utilities emerges as a timely\nquestion. In this context, regret minimisation in stochastic bandits under\n$\\epsilon$-global Differential Privacy (DP) has been widely studied. Unlike\nbandits without DP, there is a significant gap between the best-known regret\nlower and upper bound in this setting, though they \"match\" in order. Thus, we\nrevisit the regret lower and upper bounds of $\\epsilon$-global DP algorithms\nfor Bernoulli bandits and improve both. First, we prove a tighter regret lower\nbound involving a novel information-theoretic quantity characterising the\nhardness of $\\epsilon$-global DP in stochastic bandits. Our lower bound\nstrictly improves on the existing ones across all $\\epsilon$ values. Then, we\nchoose two asymptotically optimal bandit algorithms, i.e. DP-KLUCB and DP-IMED,\nand propose their DP versions using a unified blueprint, i.e., (a) running in\narm-dependent phases, and (b) adding Laplace noise to achieve privacy. For\nBernoulli bandits, we analyse the regrets of these algorithms and show that\ntheir regrets asymptotically match our lower bound up to a constant arbitrary\nclose to 1. This refutes the conjecture that forgetting past rewards is\nnecessary to design optimal bandit algorithms under global DP. At the core of\nour algorithms lies a new concentration inequality for sums of Bernoulli\nvariables under Laplace mechanism, which is a new DP version of the Chernoff\nbound. This result is universally useful as the DP literature commonly treats\nthe concentrations of Laplace noise and random variables separately, while we\ncouple them to yield a tighter bound.",
    "updated" : "2025-05-08T19:48:58Z",
    "published" : "2025-05-08T19:48:58Z",
    "authors" : [
      {
        "name" : "Achraf Azize"
      },
      {
        "name" : "Yulian Wu"
      },
      {
        "name" : "Junya Honda"
      },
      {
        "name" : "Francesco Orabona"
      },
      {
        "name" : "Shinji Ito"
      },
      {
        "name" : "Debabrota Basu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05519v1",
    "title" : "Real-Time Privacy Preservation for Robot Visual Perception",
    "summary" : "Many robots (e.g., iRobot's Roomba) operate based on visual observations from\nlive video streams, and such observations may inadvertently include\nprivacy-sensitive objects, such as personal identifiers. Existing approaches\nfor preserving privacy rely on deep learning models, differential privacy, or\ncryptography. They lack guarantees for the complete concealment of all\nsensitive objects. Guaranteeing concealment requires post-processing techniques\nand thus is inadequate for real-time video streams. We develop a method for\nprivacy-constrained video streaming, PCVS, that conceals sensitive objects\nwithin real-time video streams. PCVS takes a logical specification constraining\nthe existence of privacy-sensitive objects, e.g., never show faces when a\nperson exists. It uses a detection model to evaluate the existence of these\nobjects in each incoming frame. Then, it blurs out a subset of objects such\nthat the existence of the remaining objects satisfies the specification. We\nthen propose a conformal prediction approach to (i) establish a theoretical\nlower bound on the probability of the existence of these objects in a sequence\nof frames satisfying the specification and (ii) update the bound with the\narrival of each subsequent frame. Quantitative evaluations show that PCVS\nachieves over 95 percent specification satisfaction rate in multiple datasets,\nsignificantly outperforming other methods. The satisfaction rate is\nconsistently above the theoretical bounds across all datasets, indicating that\nthe established bounds hold. Additionally, we deploy PCVS on robots in\nreal-time operation and show that the robots operate normally without being\ncompromised when PCVS conceals objects.",
    "updated" : "2025-05-08T03:27:12Z",
    "published" : "2025-05-08T03:27:12Z",
    "authors" : [
      {
        "name" : "Minkyu Choi"
      },
      {
        "name" : "Yunhao Yang"
      },
      {
        "name" : "Neel P. Bhatt"
      },
      {
        "name" : "Kushagra Gupta"
      },
      {
        "name" : "Sahil Shah"
      },
      {
        "name" : "Aditya Rai"
      },
      {
        "name" : "David Fridovich-Keil"
      },
      {
        "name" : "Ufuk Topcu"
      },
      {
        "name" : "Sandeep P. Chinchali"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02392v2",
    "title" : "Moneros Decentralized P2P Exchanges: Functionality, Adoption, and\n  Privacy Risks",
    "summary" : "Privacy-focused cryptocurrencies like Monero remain popular, despite\nincreasing regulatory scrutiny that has led to their delisting from major\ncentralized exchanges. The latter also explains the recent popularity of\ndecentralized exchanges (DEXs) with no centralized ownership structures. These\nplatforms typically leverage peer-to-peer (P2P) networks, promising secure and\nanonymous asset trading. However, questions of liability remain, and the\nacademic literature lacks comprehensive insights into the functionality,\ntrading activity, and privacy claims of these P2P platforms. In this paper, we\nprovide an early systematization of the current landscape of decentralized\npeer-to-peer exchanges within the Monero ecosystem. We examine several recently\ndeveloped DEX platforms, analyzing their popularity, functionality,\narchitectural choices, and potential weaknesses. We further identify and report\non a privacy vulnerability in the recently popularized Haveno exchange,\ndemonstrating that certain Haveno trades could be detected, allowing\ntransactions to be linked across the Monero and Bitcoin blockchains. We hope\nthat our findings can nourish the discussion in the research community about\nmore secure designs, and provide insights for regulators.",
    "updated" : "2025-05-09T15:32:20Z",
    "published" : "2025-05-05T06:27:37Z",
    "authors" : [
      {
        "name" : "Yannik Kopyciok"
      },
      {
        "name" : "Friedhelm Victor"
      },
      {
        "name" : "Stefan Schmid"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07766v1",
    "title" : "Privacy Risks of Robot Vision: A User Study on Image Modalities and\n  Resolution",
    "summary" : "User privacy is a crucial concern in robotic applications, especially when\nmobile service robots are deployed in personal or sensitive environments.\nHowever, many robotic downstream tasks require the use of cameras, which may\nraise privacy risks. To better understand user perceptions of privacy in\nrelation to visual data, we conducted a user study investigating how different\nimage modalities and image resolutions affect users' privacy concerns. The\nresults show that depth images are broadly viewed as privacy-safe, and a\nsimilarly high proportion of respondents feel the same about semantic\nsegmentation images. Additionally, the majority of participants consider 32*32\nresolution RGB images to be almost sufficiently privacy-preserving, while most\nbelieve that 16*16 resolution can fully guarantee privacy protection.",
    "updated" : "2025-05-12T17:16:12Z",
    "published" : "2025-05-12T17:16:12Z",
    "authors" : [
      {
        "name" : "Xuying Huang"
      },
      {
        "name" : "Sicong Pan"
      },
      {
        "name" : "Maren Bennewitz"
      }
    ],
    "categories" : [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07672v1",
    "title" : "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit",
    "summary" : "We present OnPrem.LLM, a Python-based toolkit for applying large language\nmodels (LLMs) to sensitive, non-public data in offline or restricted\nenvironments. The system is designed for privacy-preserving use cases and\nprovides prebuilt pipelines for document processing and storage,\nretrieval-augmented generation (RAG), information extraction, summarization,\nclassification, and prompt/output processing with minimal configuration.\nOnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM,\nand Hugging Face Transformers -- with quantized model support, GPU\nacceleration, and seamless backend switching. Although designed for fully local\nexecution, OnPrem.LLM also supports integration with a wide range of cloud LLM\nproviders when permitted, enabling hybrid deployments that balance performance\nwith data control. A no-code web interface extends accessibility to\nnon-technical users.",
    "updated" : "2025-05-12T15:36:27Z",
    "published" : "2025-05-12T15:36:27Z",
    "authors" : [
      {
        "name" : "Arun S. Maiya"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07583v1",
    "title" : "Privacy-Preserving Real-Time Vietnamese-English Translation on iOS using\n  Edge AI",
    "summary" : "This research addresses the growing need for privacy-preserving and\naccessible language translation by developing a fully offline Neural Machine\nTranslation (NMT) system for Vietnamese-English translation on iOS devices.\nGiven increasing concerns about data privacy and unreliable network\nconnectivity, on-device translation offers critical advantages. This project\nconfronts challenges in deploying complex NMT models on resource-limited mobile\ndevices, prioritizing efficiency, accuracy, and a seamless user experience.\nLeveraging advances such as MobileBERT and, specifically, the lightweight\n\\textbf{TinyLlama 1.1B Chat v1.0} in GGUF format, \\textbf{a} quantized\nTransformer-based model is implemented and optimized. The application is\nrealized as a real-time iOS prototype, tightly integrating modern iOS\nframeworks and privacy-by-design principles. Comprehensive documentation covers\nmodel selection, technical architecture, challenges, and final implementation,\nincluding functional Swift code for deployment.",
    "updated" : "2025-05-12T14:05:39Z",
    "published" : "2025-05-12T14:05:39Z",
    "authors" : [
      {
        "name" : "Cong Le"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07306v1",
    "title" : "Enabling Privacy-Aware AI-Based Ergonomic Analysis",
    "summary" : "Musculoskeletal disorders (MSDs) are a leading cause of injury and\nproductivity loss in the manufacturing industry, incurring substantial economic\ncosts. Ergonomic assessments can mitigate these risks by identifying workplace\nadjustments that improve posture and reduce strain. Camera-based systems offer\na non-intrusive, cost-effective method for continuous ergonomic tracking, but\nthey also raise significant privacy concerns. To address this, we propose a\nprivacy-aware ergonomic assessment framework utilizing machine learning\ntechniques. Our approach employs adversarial training to develop a lightweight\nneural network that obfuscates video data, preserving only the essential\ninformation needed for human pose estimation. This obfuscation ensures\ncompatibility with standard pose estimation algorithms, maintaining high\naccuracy while protecting privacy. The obfuscated video data is transmitted to\na central server, where state-of-the-art keypoint detection algorithms extract\nbody landmarks. Using multi-view integration, 3D keypoints are reconstructed\nand evaluated with the Rapid Entire Body Assessment (REBA) method. Our system\nprovides a secure, effective solution for ergonomic monitoring in industrial\nenvironments, addressing both privacy and workplace safety concerns.",
    "updated" : "2025-05-12T07:52:48Z",
    "published" : "2025-05-12T07:52:48Z",
    "authors" : [
      {
        "name" : "Sander De Coninck"
      },
      {
        "name" : "Emilio Gamba"
      },
      {
        "name" : "Bart Van Doninck"
      },
      {
        "name" : "Abdellatif Bey-Temsamani"
      },
      {
        "name" : "Sam Leroux"
      },
      {
        "name" : "Pieter Simoens"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07085v1",
    "title" : "Privacy of Groups in Dense Street Imagery",
    "summary" : "Spatially and temporally dense street imagery (DSI) datasets have grown\nunbounded. In 2024, individual companies possessed around 3 trillion unique\nimages of public streets. DSI data streams are only set to grow as companies\nlike Lyft and Waymo use DSI to train autonomous vehicle algorithms and analyze\ncollisions. Academic researchers leverage DSI to explore novel approaches to\nurban analysis. Despite good-faith efforts by DSI providers to protect\nindividual privacy through blurring faces and license plates, these measures\nfail to address broader privacy concerns. In this work, we find that increased\ndata density and advancements in artificial intelligence enable harmful group\nmembership inferences from supposedly anonymized data. We perform a penetration\ntest to demonstrate how easily sensitive group affiliations can be inferred\nfrom obfuscated pedestrians in 25,232,608 dashcam images taken in New York\nCity. We develop a typology of identifiable groups within DSI and analyze\nprivacy implications through the lens of contextual integrity. Finally, we\ndiscuss actionable recommendations for researchers working with data from DSI\nproviders.",
    "updated" : "2025-05-11T18:16:08Z",
    "published" : "2025-05-11T18:16:08Z",
    "authors" : [
      {
        "name" : "Matt Franchi"
      },
      {
        "name" : "Hauke Sandhaus"
      },
      {
        "name" : "Madiha Zahrah Choksi"
      },
      {
        "name" : "Severin Engelmann"
      },
      {
        "name" : "Wendy Ju"
      },
      {
        "name" : "Helen Nissenbaum"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CV",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07041v1",
    "title" : "Empirical Analysis of Asynchronous Federated Learning on Heterogeneous\n  Devices: Efficiency, Fairness, and Privacy Trade-offs",
    "summary" : "Device heterogeneity poses major challenges in Federated Learning (FL), where\nresource-constrained clients slow down synchronous schemes that wait for all\nupdates before aggregation. Asynchronous FL addresses this by incorporating\nupdates as they arrive, substantially improving efficiency. While its\nefficiency gains are well recognized, its privacy costs remain largely\nunexplored, particularly for high-end devices that contribute updates more\nfrequently, increasing their cumulative privacy exposure. This paper presents\nthe first comprehensive analysis of the efficiency-fairness-privacy trade-off\nin synchronous vs. asynchronous FL under realistic device heterogeneity. We\nempirically compare FedAvg and staleness-aware FedAsync using a physical\ntestbed of five edge devices spanning diverse hardware tiers, integrating Local\nDifferential Privacy (LDP) and the Moments Accountant to quantify per-client\nprivacy loss. Using Speech Emotion Recognition (SER) as a privacy-critical\nbenchmark, we show that FedAsync achieves up to 10x faster convergence but\nexacerbates fairness and privacy disparities: high-end devices contribute 6-10x\nmore updates and incur up to 5x higher privacy loss, while low-end devices\nsuffer amplified accuracy degradation due to infrequent, stale, and\nnoise-perturbed updates. These findings motivate the need for adaptive FL\nprotocols that jointly optimize aggregation and privacy mechanisms based on\nclient capacity and participation dynamics, moving beyond static,\none-size-fits-all solutions.",
    "updated" : "2025-05-11T16:25:06Z",
    "published" : "2025-05-11T16:25:06Z",
    "authors" : [
      {
        "name" : "Samaneh Mohammadi"
      },
      {
        "name" : "Iraklis Symeonidis"
      },
      {
        "name" : "Ali Balador"
      },
      {
        "name" : "Francesco Flammini"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.06860v1",
    "title" : "DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial\n  Example for Image Privacy Protection",
    "summary" : "In the field of digital security, Reversible Adversarial Examples (RAE)\ncombine adversarial attacks with reversible data hiding techniques to\neffectively protect sensitive data and prevent unauthorized analysis by\nmalicious Deep Neural Networks (DNNs). However, existing RAE techniques\nprimarily focus on white-box attacks, lacking a comprehensive evaluation of\ntheir effectiveness in black-box scenarios. This limitation impedes their\nbroader deployment in complex, dynamic environments. Further more, traditional\nblack-box attacks are often characterized by poor transferability and high\nquery costs, significantly limiting their practical applicability. To address\nthese challenges, we propose the Dual-Phase Merging Transferable Reversible\nAttack method, which generates highly transferable initial adversarial\nperturbations in a white-box model and employs a memory augmented black-box\nstrategy to effectively mislead target mod els. Experimental results\ndemonstrate the superiority of our approach, achieving a 99.0% attack success\nrate and 100% recovery rate in black-box scenarios, highlighting its robustness\nin privacy protection. Moreover, we successfully implemented a black-box attack\non a commercial model, further substantiating the potential of this approach\nfor practical use.",
    "updated" : "2025-05-11T06:11:10Z",
    "published" : "2025-05-11T06:11:10Z",
    "authors" : [
      {
        "name" : "Xia Du"
      },
      {
        "name" : "Jiajie Zhu"
      },
      {
        "name" : "Jizhe Zhou"
      },
      {
        "name" : "Chi-man Pun"
      },
      {
        "name" : "Zheng Lin"
      },
      {
        "name" : "Cong Wu"
      },
      {
        "name" : "Zhe Chen"
      },
      {
        "name" : "Jun Luo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.06759v1",
    "title" : "Privacy-aware Berrut Approximated Coded Computing applied to general\n  distributed learning",
    "summary" : "Coded computing is one of the techniques that can be used for privacy\nprotection in Federated Learning. However, most of the constructions used for\ncoded computing work only under the assumption that the computations involved\nare exact, generally restricted to special classes of functions, and require\nquantized inputs. This paper considers the use of Private Berrut Approximate\nCoded Computing (PBACC) as a general solution to add strong but non-perfect\nprivacy to federated learning. We derive new adapted PBACC algorithms for\ncentralized aggregation, secure distributed training with centralized data, and\nsecure decentralized training with decentralized data, thus enlarging\nsignificantly the applications of the method and the existing privacy\nprotection tools available for these paradigms. Particularly, PBACC can be used\nrobustly to attain privacy guarantees in decentralized federated learning for a\nvariety of models. Our numerical results show that the achievable quality of\ndifferent learning models (convolutional neural networks, variational\nautoencoders, and Cox regression) is minimally altered by using these new\ncomputing schemes, and that the privacy leakage can be bounded strictly to less\nthan a fraction of one bit per participant. Additionally, the computational\ncost of the encoding and decoding processes depends only of the degree of\ndecentralization of the data.",
    "updated" : "2025-05-10T21:27:40Z",
    "published" : "2025-05-10T21:27:40Z",
    "authors" : [
      {
        "name" : "Xavier Martínez-Luaña"
      },
      {
        "name" : "Manuel Fernández-Veiga"
      },
      {
        "name" : "Rebeca P. Díaz-Redondo"
      },
      {
        "name" : "Ana Fernández-Vilas"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.06747v1",
    "title" : "DPolicy: Managing Privacy Risks Across Multiple Releases with\n  Differential Privacy",
    "summary" : "Differential Privacy (DP) has emerged as a robust framework for\nprivacy-preserving data releases and has been successfully applied in\nhigh-profile cases, such as the 2020 US Census. However, in organizational\nsettings, the use of DP remains largely confined to isolated data releases.\nThis approach restricts the potential of DP to serve as a framework for\ncomprehensive privacy risk management at an organizational level. Although one\nmight expect that the cumulative privacy risk of isolated releases could be\nassessed using DP's compositional property, in practice, individual DP\nguarantees are frequently tailored to specific releases, making it difficult to\nreason about their interaction or combined impact. At the same time, less\ntailored DP guarantees, which compose more easily, also offer only limited\ninsight because they lead to excessively large privacy budgets that convey\nlimited meaning. To address these limitations, we present DPolicy, a system\ndesigned to manage cumulative privacy risks across multiple data releases using\nDP. Unlike traditional approaches that treat each release in isolation or rely\non a single (global) DP guarantee, our system employs a flexible framework that\nconsiders multiple DP guarantees simultaneously, reflecting the diverse\ncontexts and scopes typical of real-world DP deployments. DPolicy introduces a\nhigh-level policy language to formalize privacy guarantees, making\ntraditionally implicit assumptions on scopes and contexts explicit. By deriving\nthe DP guarantees required to enforce complex privacy semantics from these\nhigh-level policies, DPolicy enables fine-grained privacy risk management on an\norganizational scale. We implement and evaluate DPolicy, demonstrating how it\nmitigates privacy risks that can emerge without comprehensive,\norganization-wide privacy risk management.",
    "updated" : "2025-05-10T19:49:51Z",
    "published" : "2025-05-10T19:49:51Z",
    "authors" : [
      {
        "name" : "Nicolas Küchler"
      },
      {
        "name" : "Alexander Viand"
      },
      {
        "name" : "Hidde Lycklama"
      },
      {
        "name" : "Anwar Hithnawi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.06305v1",
    "title" : "User Behavior Analysis in Privacy Protection with Large Language Models:\n  A Study on Privacy Preferences with Limited Data",
    "summary" : "With the widespread application of large language models (LLMs), user privacy\nprotection has become a significant research topic. Existing privacy preference\nmodeling methods often rely on large-scale user data, making effective privacy\npreference analysis challenging in data-limited environments. This study\nexplores how LLMs can analyze user behavior related to privacy protection in\nscenarios with limited data and proposes a method that integrates Few-shot\nLearning and Privacy Computing to model user privacy preferences. The research\nutilizes anonymized user privacy settings data, survey responses, and simulated\ndata, comparing the performance of traditional modeling approaches with\nLLM-based methods. Experimental results demonstrate that, even with limited\ndata, LLMs significantly improve the accuracy of privacy preference modeling.\nAdditionally, incorporating Differential Privacy and Federated Learning further\nreduces the risk of user data exposure. The findings provide new insights into\nthe application of LLMs in privacy protection and offer theoretical support for\nadvancing privacy computing and user behavior analysis.",
    "updated" : "2025-05-08T04:42:17Z",
    "published" : "2025-05-08T04:42:17Z",
    "authors" : [
      {
        "name" : "Haowei Yang"
      },
      {
        "name" : "Qingyi Lu"
      },
      {
        "name" : "Yang Wang"
      },
      {
        "name" : "Sibei Liu"
      },
      {
        "name" : "Jiayun Zheng"
      },
      {
        "name" : "Ao Xiang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.08719v1",
    "title" : "PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts",
    "summary" : "Large language models (LLMs) hosted on cloud servers alleviate the\ncomputational and storage burdens on local devices but raise privacy concerns\ndue to sensitive data transmission and require substantial communication\nbandwidth, which is challenging in constrained environments. In contrast, small\nlanguage models (SLMs) running locally enhance privacy but suffer from limited\nperformance on complex tasks. To balance computational cost, performance, and\nprivacy protection under bandwidth constraints, we propose a privacy-aware\nwireless collaborative mixture of experts (PWC-MoE) framework. Specifically,\nPWC-MoE employs a sparse privacy-aware gating network to dynamically route\nsensitive tokens to privacy experts located on local clients, while\nnon-sensitive tokens are routed to non-privacy experts located at the remote\nbase station. To achieve computational efficiency, the gating network ensures\nthat each token is dynamically routed to and processed by only one expert. To\nenhance scalability and prevent overloading of specific experts, we introduce a\ngroup-wise load-balancing mechanism for the gating network that evenly\ndistributes sensitive tokens among privacy experts and non-sensitive tokens\namong non-privacy experts. To adapt to bandwidth constraints while preserving\nmodel performance, we propose a bandwidth-adaptive and importance-aware token\noffloading scheme. This scheme incorporates an importance predictor to evaluate\nthe importance scores of non-sensitive tokens, prioritizing the most important\ntokens for transmission to the base station based on their predicted importance\nand the available bandwidth. Experiments demonstrate that the PWC-MoE framework\neffectively preserves privacy and maintains high performance even in\nbandwidth-constrained environments, offering a practical solution for deploying\nLLMs in privacy-sensitive and bandwidth-limited scenarios.",
    "updated" : "2025-05-13T16:27:07Z",
    "published" : "2025-05-13T16:27:07Z",
    "authors" : [
      {
        "name" : "Yang Su"
      },
      {
        "name" : "Na Yan"
      },
      {
        "name" : "Yansha Deng"
      },
      {
        "name" : "Robert Schober"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.08237v1",
    "title" : "Privacy-Preserving Analytics for Smart Meter (AMI) Data: A Hybrid\n  Approach to Comply with CPUC Privacy Regulations",
    "summary" : "Advanced Metering Infrastructure (AMI) data from smart electric and gas\nmeters enables valuable insights for utilities and consumers, but also raises\nsignificant privacy concerns. In California, regulatory decisions (CPUC\nD.11-07-056 and D.11-08-045) mandate strict privacy protections for customer\nenergy usage data, guided by the Fair Information Practice Principles (FIPPs).\nWe comprehensively explore solutions drawn from data anonymization,\nprivacy-preserving machine learning (differential privacy and federated\nlearning), synthetic data generation, and cryptographic techniques (secure\nmultiparty computation, homomorphic encryption). This allows advanced\nanalytics, including machine learning models, statistical and econometric\nanalysis on energy consumption data, to be performed without compromising\nindividual privacy.\n  We evaluate each technique's theoretical foundations, effectiveness, and\ntrade-offs in the context of utility data analytics, and we propose an\nintegrated architecture that combines these methods to meet real-world needs.\nThe proposed hybrid architecture is designed to ensure compliance with\nCalifornia's privacy rules and FIPPs while enabling useful analytics, from\nforecasting and personalized insights to academic research and econometrics,\nwhile strictly protecting individual privacy. Mathematical definitions and\nderivations are provided where appropriate to demonstrate privacy guarantees\nand utility implications rigorously. We include comparative evaluations of the\ntechniques, an architecture diagram, and flowcharts to illustrate how they work\ntogether in practice. The result is a blueprint for utility data scientists and\nengineers to implement privacy-by-design in AMI data handling, supporting both\ndata-driven innovation and strict regulatory compliance.",
    "updated" : "2025-05-13T05:30:35Z",
    "published" : "2025-05-13T05:30:35Z",
    "authors" : [
      {
        "name" : "Benjamin Westrich"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07672v2",
    "title" : "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit",
    "summary" : "We present OnPrem$.$LLM, a Python-based toolkit for applying large language\nmodels (LLMs) to sensitive, non-public data in offline or restricted\nenvironments. The system is designed for privacy-preserving use cases and\nprovides prebuilt pipelines for document processing and storage,\nretrieval-augmented generation (RAG), information extraction, summarization,\nclassification, and prompt/output processing with minimal configuration.\nOnPrem$.$LLM supports multiple LLM backends -- including llama$.$cpp, Ollama,\nvLLM, and Hugging Face Transformers -- with quantized model support, GPU\nacceleration, and seamless backend switching. Although designed for fully local\nexecution, OnPrem$.$LLM also supports integration with a wide range of cloud\nLLM providers when permitted, enabling hybrid deployments that balance\nperformance with data control. A no-code web interface extends accessibility to\nnon-technical users.",
    "updated" : "2025-05-13T02:43:26Z",
    "published" : "2025-05-12T15:36:27Z",
    "authors" : [
      {
        "name" : "Arun S. Maiya"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07872v1",
    "title" : "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
    "summary" : "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
    "updated" : "2025-05-09T21:05:20Z",
    "published" : "2025-05-09T21:05:20Z",
    "authors" : [
      {
        "name" : "Yijing Zhang"
      },
      {
        "name" : "Ferdous Pervej"
      },
      {
        "name" : "Andreas F. Molisch"
      }
    ],
    "categories" : [
      "cs.NI",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.09276v1",
    "title" : "Privacy-Preserving Runtime Verification",
    "summary" : "Runtime verification offers scalable solutions to improve the safety and\nreliability of systems. However, systems that require verification or\nmonitoring by a third party to ensure compliance with a specification might\ncontain sensitive information, causing privacy concerns when usual runtime\nverification approaches are used. Privacy is compromised if protected\ninformation about the system, or sensitive data that is processed by the\nsystem, is revealed. In addition, revealing the specification being monitored\nmay undermine the essence of third-party verification.\n  In this work, we propose two novel protocols for the privacy-preserving\nruntime verification of systems against formal sequential specifications. In\nour first protocol, the monitor verifies whether the system satisfies the\nspecification without learning anything else, though both parties are aware of\nthe specification. Our second protocol ensures that the system remains\noblivious to the monitored specification, while the monitor learns only whether\nthe system satisfies the specification and nothing more. Our protocols adapt\nand improve existing techniques used in cryptography, and more specifically,\nmulti-party computation.\n  The sequential specification defines the observation step of the monitor,\nwhose granularity depends on the situation (e.g., banks may be monitored on a\ndaily basis). Our protocols exchange a single message per observation step,\nafter an initialisation phase. This design minimises communication overhead,\nenabling relatively lightweight privacy-preserving monitoring. We implement our\napproach for monitoring specifications described by register automata and\nevaluate it experimentally.",
    "updated" : "2025-05-14T10:49:07Z",
    "published" : "2025-05-14T10:49:07Z",
    "authors" : [
      {
        "name" : "Thomas A. Henzinger"
      },
      {
        "name" : "Mahyar Karimi"
      },
      {
        "name" : "K. S. Thejaswini"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.FL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.08847v1",
    "title" : "On the interplay of Explainability, Privacy and Predictive Performance\n  with Explanation-assisted Model Extraction",
    "summary" : "Machine Learning as a Service (MLaaS) has gained important attraction as a\nmeans for deploying powerful predictive models, offering ease of use that\nenables organizations to leverage advanced analytics without substantial\ninvestments in specialized infrastructure or expertise. However, MLaaS\nplatforms must be safeguarded against security and privacy attacks, such as\nmodel extraction (MEA) attacks. The increasing integration of explainable AI\n(XAI) within MLaaS has introduced an additional privacy challenge, as attackers\ncan exploit model explanations particularly counterfactual explanations (CFs)\nto facilitate MEA. In this paper, we investigate the trade offs among model\nperformance, privacy, and explainability when employing Differential Privacy\n(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two\ndistinct DP strategies: implemented during the classification model training\nand at the explainer during CF generation.",
    "updated" : "2025-05-13T15:27:06Z",
    "published" : "2025-05-13T15:27:06Z",
    "authors" : [
      {
        "name" : "Fatima Ezzeddine"
      },
      {
        "name" : "Rinad Akel"
      },
      {
        "name" : "Ihab Sbeity"
      },
      {
        "name" : "Silvia Giordano"
      },
      {
        "name" : "Marc Langheinrich"
      },
      {
        "name" : "Omran Ayoub"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.10496v1",
    "title" : "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of\n  Synthetic Chest Radiographs",
    "summary" : "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/",
    "updated" : "2025-05-15T16:59:17Z",
    "published" : "2025-05-15T16:59:17Z",
    "authors" : [
      {
        "name" : "Raman Dutt"
      },
      {
        "name" : "Pedro Sanchez"
      },
      {
        "name" : "Yongchen Yao"
      },
      {
        "name" : "Steven McDonagh"
      },
      {
        "name" : "Sotirios A. Tsaftaris"
      },
      {
        "name" : "Timothy Hospedales"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.10264v1",
    "title" : "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack\n  in Federated Learning",
    "summary" : "Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art.",
    "updated" : "2025-05-15T13:16:32Z",
    "published" : "2025-05-15T13:16:32Z",
    "authors" : [
      {
        "name" : "Francesco Diana"
      },
      {
        "name" : "André Nusser"
      },
      {
        "name" : "Chuan Xu"
      },
      {
        "name" : "Giovanni Neglia"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.09929v1",
    "title" : "Security and Privacy Measurement on Chinese Consumer IoT Traffic based\n  on Device Lifecycle",
    "summary" : "In recent years, consumer Internet of Things (IoT) devices have become widely\nused in daily life. With the popularity of devices, related security and\nprivacy risks arise at the same time as they collect user-related data and\ntransmit it to various service providers. Although China accounts for a larger\nshare of the consumer IoT industry, current analyses on consumer IoT device\ntraffic primarily focus on regions such as Europe, the United States, and\nAustralia. Research on China, however, is currently rather rare. This study\nconstructs the first large-scale dataset about consumer IoT device traffic in\nChina. Specifically, we propose a fine-grained traffic collection guidance\ncovering the entire lifecycle of consumer IoT devices, gathering traffic from\n70 devices spanning 36 brands and 8 device categories. Based on this dataset,\nwe analyze traffic destinations and encryption practices across different\ndevice types during the entire lifecycle and compare the findings with the\nresults of other regions. Compared to other regions, our results show that\nconsumer IoT devices in China rely more on domestic services and overally\nperform better in terms of encryption practices. However, there are still 20/35\ndevices improperly conduct certificate validation, and 5/70 devices use\ninsecure encryption protocols. To facilitate future research, we open-source\nour traffic collection guidance and make our dataset publicly available.",
    "updated" : "2025-05-15T03:27:16Z",
    "published" : "2025-05-15T03:27:16Z",
    "authors" : [
      {
        "name" : "Chenghua Jin"
      },
      {
        "name" : "Yan Jia"
      },
      {
        "name" : "Yuxin Song"
      },
      {
        "name" : "Qingyin Tan"
      },
      {
        "name" : "Rui Yang"
      },
      {
        "name" : "Zheli Liu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.09921v1",
    "title" : "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative\n  In-Context Optimization",
    "summary" : "Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at\n\\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.",
    "updated" : "2025-05-15T03:11:57Z",
    "published" : "2025-05-15T03:11:57Z",
    "authors" : [
      {
        "name" : "Yidan Wang"
      },
      {
        "name" : "Yanan Cao"
      },
      {
        "name" : "Yubing Ren"
      },
      {
        "name" : "Fang Fang"
      },
      {
        "name" : "Zheng Lin"
      },
      {
        "name" : "Binxing Fang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05922v2",
    "title" : "Cape: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy",
    "summary" : "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.",
    "updated" : "2025-05-15T09:31:11Z",
    "published" : "2025-05-09T09:54:07Z",
    "authors" : [
      {
        "name" : "Haoqi Wu"
      },
      {
        "name" : "Wei Dai"
      },
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Qiang Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.11094v1",
    "title" : "Blockchain-Enabled Decentralized Privacy-Preserving Group Purchasing for\n  Energy Plans",
    "summary" : "Retail energy markets are increasingly consumer-oriented, thanks to a growing\nnumber of energy plans offered by a plethora of energy suppliers, retailers and\nintermediaries. To maximize the benefits of competitive retail energy markets,\ngroup purchasing is an emerging paradigm that aggregates consumers' purchasing\npower by coordinating switch decisions to specific energy providers for\ndiscounted energy plans. Traditionally, group purchasing is mediated by a\ntrusted third-party, which suffers from the lack of privacy and transparency.\nIn this paper, we introduce a novel paradigm of decentralized\nprivacy-preserving group purchasing, empowered by privacy-preserving blockchain\nand secure multi-party computation, to enable users to form a coalition for\ncoordinated switch decisions in a decentralized manner, without a trusted\nthird-party. The coordinated switch decisions are determined by a competitive\nonline algorithm, based on users' private consumption data and current energy\nplan tariffs. Remarkably, no private user consumption data will be revealed to\nothers in the online decision-making process, which is carried out in a\ntransparently verifiable manner to eliminate frauds from dishonest users and\nsupports fair mutual compensations by sharing the switching costs to\nincentivize group purchasing. We implemented our decentralized group purchasing\nsolution as a smart contract on Solidity-supported blockchain platform (e.g.,\nEthereum), and provide extensive empirical evaluation.",
    "updated" : "2025-05-16T10:26:15Z",
    "published" : "2025-05-16T10:26:15Z",
    "authors" : [
      {
        "name" : "Sid Chi-Kin Chau"
      },
      {
        "name" : "Yue Zhou"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.10965v1",
    "title" : "Privacy and Confidentiality Requirements Engineering for Process Data",
    "summary" : "The application and development of process mining techniques face significant\nchallenges due to the lack of publicly available real-life event logs. One\nreason for companies to abstain from sharing their data are privacy and\nconfidentiality concerns. Privacy concerns refer to personal data as specified\nin the GDPR and have been addressed in existing work by providing\nprivacy-preserving techniques for event logs. However, the concept of\nconfidentiality in event logs not pertaining to individuals remains unclear,\nalthough they might contain a multitude of sensitive business data. This work\naddresses confidentiality of process data based on the privacy and\nconfidentiality engineering method (PCRE). PCRE interactively explores privacy\nand confidentiality requirements regarding process data with different\nstakeholders and defines privacy-preserving actions to address possible\nconcerns. We co-construct and evaluate PCRE based on structured interviews with\nprocess analysts in two manufacturing companies. PCRE is generic, hence\napplicable in different application domains. The goal is to systematically\nscrutinize process data and balance the trade-off between privacy and utility\nloss.",
    "updated" : "2025-05-16T08:03:02Z",
    "published" : "2025-05-16T08:03:02Z",
    "authors" : [
      {
        "name" : "Fabian Haertel"
      },
      {
        "name" : "Juergen Mangler"
      },
      {
        "name" : "Nataliia Klievtsova"
      },
      {
        "name" : "Celine Mader"
      },
      {
        "name" : "Eugen Rigger"
      },
      {
        "name" : "Stefanie Rinderle-Ma"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.10941v1",
    "title" : "Privacy-Aware Lifelong Learning",
    "summary" : "Lifelong learning algorithms enable models to incrementally acquire new\nknowledge without forgetting previously learned information. Contrarily, the\nfield of machine unlearning focuses on explicitly forgetting certain previous\nknowledge from pretrained models when requested, in order to comply with data\nprivacy regulations on the right-to-be-forgotten. Enabling efficient lifelong\nlearning with the capability to selectively unlearn sensitive information from\nmodels presents a critical and largely unaddressed challenge with contradicting\nobjectives. We address this problem from the perspective of simultaneously\npreventing catastrophic forgetting and allowing forward knowledge transfer\nduring task-incremental learning, while ensuring exact task unlearning and\nminimizing memory requirements, based on a single neural network model to be\nadapted. Our proposed solution, privacy-aware lifelong learning (PALL),\ninvolves optimization of task-specific sparse subnetworks with parameter\nsharing within a single architecture. We additionally utilize an episodic\nmemory rehearsal mechanism to facilitate exact unlearning without performance\ndegradations. We empirically demonstrate the scalability of PALL across various\narchitectures in image classification, and provide a state-of-the-art solution\nthat uniquely integrates lifelong learning and privacy-aware unlearning\nmechanisms for responsible AI applications.",
    "updated" : "2025-05-16T07:27:00Z",
    "published" : "2025-05-16T07:27:00Z",
    "authors" : [
      {
        "name" : "Ozan Özdenizci"
      },
      {
        "name" : "Elmar Rueckert"
      },
      {
        "name" : "Robert Legenstein"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.10871v1",
    "title" : "Optimal Allocation of Privacy Budget on Hierarchical Data Release",
    "summary" : "Releasing useful information from datasets with hierarchical structures while\npreserving individual privacy presents a significant challenge. Standard\nprivacy-preserving mechanisms, and in particular Differential Privacy, often\nrequire careful allocation of a finite privacy budget across different levels\nand components of the hierarchy. Sub-optimal allocation can lead to either\nexcessive noise, rendering the data useless, or to insufficient protections for\nsensitive information. This paper addresses the critical problem of optimal\nprivacy budget allocation for hierarchical data release. It formulates this\nchallenge as a constrained optimization problem, aiming to maximize data\nutility subject to a total privacy budget while considering the inherent\ntrade-offs between data granularity and privacy loss. The proposed approach is\nsupported by theoretical analysis and validated through comprehensive\nexperiments on real hierarchical datasets. These experiments demonstrate that\noptimal privacy budget allocation significantly enhances the utility of the\nreleased data and improves the performance of downstream tasks.",
    "updated" : "2025-05-16T05:25:11Z",
    "published" : "2025-05-16T05:25:11Z",
    "authors" : [
      {
        "name" : "Joonhyuk Ko"
      },
      {
        "name" : "Juba Ziani"
      },
      {
        "name" : "Ferdinando Fioretto"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.09921v2",
    "title" : "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative\n  In-Context Optimization",
    "summary" : "Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at https://github.com/redwyd/PrivacyJailbreak.",
    "updated" : "2025-05-16T09:36:29Z",
    "published" : "2025-05-15T03:11:57Z",
    "authors" : [
      {
        "name" : "Yidan Wang"
      },
      {
        "name" : "Yanan Cao"
      },
      {
        "name" : "Yubing Ren"
      },
      {
        "name" : "Fang Fang"
      },
      {
        "name" : "Zheng Lin"
      },
      {
        "name" : "Binxing Fang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.13292v1",
    "title" : "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms\n  of AI Systems by Integrating Federated Learning and LLMs",
    "summary" : "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection.",
    "updated" : "2025-05-19T16:14:27Z",
    "published" : "2025-05-19T16:14:27Z",
    "authors" : [
      {
        "name" : "Huaiying Luo"
      },
      {
        "name" : "Cheng Ji"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.13085v1",
    "title" : "Universal Semantic Disentangled Privacy-preserving Speech Representation\n  Learning",
    "summary" : "The use of audio recordings of human speech to train LLMs poses privacy\nconcerns due to these models' potential to generate outputs that closely\nresemble artifacts in the training data. In this study, we propose a speaker\nprivacy-preserving representation learning method through the Universal Speech\nCodec (USC), a computationally efficient encoder-decoder model that\ndisentangles speech into: $\\textit{(i)}$ privacy-preserving semantically rich\nrepresentations, capturing content and speech paralinguistics, and\n$\\textit{(ii)}$ residual acoustic and speaker representations that enables\nhigh-fidelity reconstruction. Extensive evaluations presented show that USC's\nsemantic representation preserves content, prosody, and sentiment, while\nremoving potentially identifiable speaker attributes. Combining both\nrepresentations, USC achieves state-of-the-art speech reconstruction.\nAdditionally, we introduce an evaluation methodology for measuring\nprivacy-preserving properties, aligning with perceptual tests. We compare USC\nagainst other codecs in the literature and demonstrate its effectiveness on\nprivacy-preserving representation learning, illustrating the trade-offs of\nspeaker anonymization, paralinguistics retention and content preservation in\nthe learned semantic representations. Audio samples are shared in\n$\\href{https://www.amazon.science/usc-samples}{https://www.amazon.science/usc-samples}$.",
    "updated" : "2025-05-19T13:19:49Z",
    "published" : "2025-05-19T13:19:49Z",
    "authors" : [
      {
        "name" : "Biel Tura Vecino"
      },
      {
        "name" : "Subhadeep Maji"
      },
      {
        "name" : "Aravind Varier"
      },
      {
        "name" : "Antonio Bonafonte"
      },
      {
        "name" : "Ivan Valles"
      },
      {
        "name" : "Michael Owen"
      },
      {
        "name" : "Leif Radel"
      },
      {
        "name" : "Grant Strimmel"
      },
      {
        "name" : "Seyi Feyisetan"
      },
      {
        "name" : "Roberto Barra Chicote"
      },
      {
        "name" : "Ariya Rastrow"
      },
      {
        "name" : "Constantinos Papayiannis"
      },
      {
        "name" : "Volker Leutnant"
      },
      {
        "name" : "Trevor Wood"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.12954v1",
    "title" : "Counting Graphlets of Size $k$ under Local Differential Privacy",
    "summary" : "The problem of counting subgraphs or graphlets under local differential\nprivacy is an important challenge that has attracted significant attention from\nresearchers. However, much of the existing work focuses on small graphlets like\ntriangles or $k$-stars. In this paper, we propose a non-interactive, locally\ndifferentially private algorithm capable of counting graphlets of any size $k$.\nWhen $n$ is the number of nodes in the input graph, we show that the expected\n$\\ell_2$ error of our algorithm is $O(n^{k - 1})$. Additionally, we prove that\nthere exists a class of input graphs and graphlets of size $k$ for which any\nnon-interactive counting algorithm incurs an expected $\\ell_2$ error of\n$\\Omega(n^{k - 1})$, demonstrating the optimality of our result. Furthermore,\nwe establish that for certain input graphs and graphlets, any locally\ndifferentially private algorithm must have an expected $\\ell_2$ error of\n$\\Omega(n^{k - 1.5})$. Our experimental results show that our algorithm is more\naccurate than the classical randomized response method.",
    "updated" : "2025-05-19T10:46:21Z",
    "published" : "2025-05-19T10:46:21Z",
    "authors" : [
      {
        "name" : "Vorapong Suppakitpaisarn"
      },
      {
        "name" : "Donlapark Ponnoprat"
      },
      {
        "name" : "Nicha Hirankarn"
      },
      {
        "name" : "Quentin Hillebrand"
      }
    ],
    "categories" : [
      "cs.SI",
      "cs.CC",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.12869v1",
    "title" : "Outsourced Privacy-Preserving Feature Selection Based on Fully\n  Homomorphic Encryption",
    "summary" : "Feature selection is a technique that extracts a meaningful subset from a set\nof features in training data. When the training data is large-scale,\nappropriate feature selection enables the removal of redundant features, which\ncan improve generalization performance, accelerate the training process, and\nenhance the interpretability of the model. This study proposes a\nprivacy-preserving computation model for feature selection. Generally, when the\ndata owner and analyst are the same, there is no need to conceal the private\ninformation. However, when they are different parties or when multiple owners\nexist, an appropriate privacy-preserving framework is required. Although\nvarious private feature selection algorithms, they all require two or more\ncomputing parties and do not guarantee security in environments where no\nexternal party can be fully trusted. To address this issue, we propose the\nfirst outsourcing algorithm for feature selection using fully homomorphic\nencryption. Compared to a prior two-party algorithm, our result improves the\ntime and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n\ndenote the number of features and data samples, respectively. We also\nimplemented the proposed algorithm and conducted comparative experiments with\nthe naive one. The experimental result shows the efficiency of our method even\nwith small datasets.",
    "updated" : "2025-05-19T08:55:56Z",
    "published" : "2025-05-19T08:55:56Z",
    "authors" : [
      {
        "name" : "Koki Wakiyama"
      },
      {
        "name" : "Tomohiro I"
      },
      {
        "name" : "Hiroshi Sakamoto"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.12688v1",
    "title" : "Shielding Latent Face Representations From Privacy Attacks",
    "summary" : "In today's data-driven analytics landscape, deep learning has become a\npowerful tool, with latent representations, known as embeddings, playing a\ncentral role in several applications. In the face analytics domain, such\nembeddings are commonly used for biometric recognition (e.g., face\nidentification). However, these embeddings, or templates, can inadvertently\nexpose sensitive attributes such as age, gender, and ethnicity. Leaking such\ninformation can compromise personal privacy and affect civil liberty and human\nrights. To address these concerns, we introduce a multi-layer protection\nframework for embeddings. It consists of a sequence of operations: (a)\nencrypting embeddings using Fully Homomorphic Encryption (FHE), and (b) hashing\nthem using irreversible feature manifold hashing. Unlike conventional\nencryption methods, FHE enables computations directly on encrypted data,\nallowing downstream analytics while maintaining strong privacy guarantees. To\nreduce the overhead of encrypted processing, we employ embedding compression.\nOur proposed method shields latent representations of sensitive data from\nleaking private attributes (such as age and gender) while retaining essential\nfunctional capabilities (such as face identification). Extensive experiments on\ntwo datasets using two face encoders demonstrate that our approach outperforms\nseveral state-of-the-art privacy protection methods.",
    "updated" : "2025-05-19T04:23:16Z",
    "published" : "2025-05-19T04:23:16Z",
    "authors" : [
      {
        "name" : "Arjun Ramesh Kaushik"
      },
      {
        "name" : "Bharat Chandra Yalavarthi"
      },
      {
        "name" : "Arun Ross"
      },
      {
        "name" : "Vishnu Boddeti"
      },
      {
        "name" : "Nalini Ratha"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.12610v1",
    "title" : "hChain: Blockchain Based Large Scale EHR Data Sharing with Enhanced\n  Security and Privacy",
    "summary" : "Concerns regarding privacy and data security in conventional healthcare\nprompted alternative technologies. In smart healthcare, blockchain technology\naddresses existing concerns with security, privacy, and electronic healthcare\ntransmission. Integration of Blockchain Technology with the Internet of Medical\nThings (IoMT) allows real-time monitoring of protected healthcare data.\nUtilizing edge devices with IoMT devices is very advantageous for addressing\nsecurity, computing, and storage challenges. Encryption using symmetric and\nasymmetric keys is used to conceal sensitive information from unauthorized\nparties. SHA256 is an algorithm for one-way hashing. It is used to verify that\nthe data has not been altered, since if it had, the hash value would have\nchanged. This article offers a blockchain-based smart healthcare system using\nIoMT devices for continuous patient monitoring. In addition, it employs edge\nresources in addition to IoMT devices to have extra computing power and storage\nto hash and encrypt incoming data before sending it to the blockchain.\nSymmetric key is utilized to keep the data private even in the blockchain,\nallowing the patient to safely communicate the data through smart contracts\nwhile preventing unauthorized physicians from seeing the data. Through the use\nof a verification node and blockchain, an asymmetric key is used for the\nsigning and validation of patient data in the healthcare provider system. In\naddition to other security measures, location-based authentication is\nrecommended to guarantee that data originates from the patient area. Through\nthe edge device, SHA256 is utilized to secure the data's integrity and a secret\nkey is used to maintain its secrecy. The hChain architecture improves the\ncomputing power of IoMT environments, the security of EHR sharing through smart\ncontracts, and the privacy and authentication procedures.",
    "updated" : "2025-05-19T01:47:01Z",
    "published" : "2025-05-19T01:47:01Z",
    "authors" : [
      {
        "name" : "Musharraf Alruwaill"
      },
      {
        "name" : "Saraju Mohanty"
      },
      {
        "name" : "Elias Kougianos"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.12239v1",
    "title" : "ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting\n  with Privacy Preservation",
    "summary" : "The development of artificial intelligence demands that models incrementally\nupdate knowledge by Continual Learning (CL) to adapt to open-world\nenvironments. To meet privacy and security requirements, Continual Unlearning\n(CU) emerges as an important problem, aiming to sequentially forget particular\nknowledge acquired during the CL phase. However, existing unlearning methods\nprimarily focus on single-shot joint forgetting and face significant\nlimitations when applied to CU. First, most existing methods require access to\nthe retained dataset for re-training or fine-tuning, violating the inherent\nconstraint in CL that historical data cannot be revisited. Second, these\nmethods often suffer from a poor trade-off between system efficiency and model\nfidelity, making them vulnerable to being overwhelmed or degraded by\nadversaries through deliberately frequent requests. In this paper, we identify\nthat the limitations of existing unlearning methods stem fundamentally from\ntheir reliance on gradient-based updates. To bridge the research gap at its\nroot, we propose a novel gradient-free method for CU, named Analytic Continual\nUnlearning (ACU), for efficient and exact forgetting with historical data\nprivacy preservation. In response to each unlearning request, our ACU\nrecursively derives an analytical (i.e., closed-form) solution in an\ninterpretable manner using the least squares method. Theoretical and\nexperimental evaluations validate the superiority of our ACU on unlearning\neffectiveness, model fidelity, and system efficiency.",
    "updated" : "2025-05-18T05:28:18Z",
    "published" : "2025-05-18T05:28:18Z",
    "authors" : [
      {
        "name" : "Jianheng Tang"
      },
      {
        "name" : "Huiping Zhuang"
      },
      {
        "name" : "Di Fang"
      },
      {
        "name" : "Jiaxu Li"
      },
      {
        "name" : "Feijiang Han"
      },
      {
        "name" : "Yajiang Huang"
      },
      {
        "name" : "Kejia Fan"
      },
      {
        "name" : "Leye Wang"
      },
      {
        "name" : "Zhanxing Zhu"
      },
      {
        "name" : "Shanghang Zhang"
      },
      {
        "name" : "Houbing Herbert Song"
      },
      {
        "name" : "Yunhuai Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.12153v1",
    "title" : "Federated Deep Reinforcement Learning for Privacy-Preserving\n  Robotic-Assisted Surgery",
    "summary" : "The integration of Reinforcement Learning (RL) into robotic-assisted surgery\n(RAS) holds significant promise for advancing surgical precision, adaptability,\nand autonomous decision-making. However, the development of robust RL models in\nclinical settings is hindered by key challenges, including stringent patient\ndata privacy regulations, limited access to diverse surgical datasets, and high\nprocedural variability. To address these limitations, this paper presents a\nFederated Deep Reinforcement Learning (FDRL) framework that enables\ndecentralized training of RL models across multiple healthcare institutions\nwithout exposing sensitive patient information. A central innovation of the\nproposed framework is its dynamic policy adaptation mechanism, which allows\nsurgical robots to select and tailor patient-specific policies in real-time,\nthereby ensuring personalized and Optimised interventions. To uphold rigorous\nprivacy standards while facilitating collaborative learning, the FDRL framework\nincorporates secure aggregation, differential privacy, and homomorphic\nencryption techniques. Experimental results demonstrate a 60\\% reduction in\nprivacy leakage compared to conventional methods, with surgical precision\nmaintained within a 1.5\\% margin of a centralized baseline. This work\nestablishes a foundational approach for adaptive, secure, and patient-centric\nAI-driven surgical robotics, offering a pathway toward clinical translation and\nscalable deployment across diverse healthcare environments.",
    "updated" : "2025-05-17T22:02:44Z",
    "published" : "2025-05-17T22:02:44Z",
    "authors" : [
      {
        "name" : "Sana Hafeez"
      },
      {
        "name" : "Sundas Rafat Mulkana"
      },
      {
        "name" : "Muhammad Ali Imran"
      },
      {
        "name" : "Michele Sevegnani"
      }
    ],
    "categories" : [
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.12144v1",
    "title" : "Proof-of-Social-Capital: Privacy-Preserving Consensus Protocol Replacing\n  Stake for Social Capital",
    "summary" : "Consensus protocols used today in blockchains often rely on computational\npower or financial stakes - scarce resources. We propose a novel protocol using\nsocial capital - trust and influence from social interactions - as a\nnon-transferable staking mechanism to ensure fairness and decentralization. The\nmethodology integrates zero-knowledge proofs, verifiable credentials, a\nWhisk-like leader election, and an incentive scheme to prevent Sybil attacks\nand encourage engagement. The theoretical framework would enhance privacy and\nequity, though unresolved issues like off-chain bribery require further\nresearch. This work offers a new model aligned with modern social media\nbehavior and lifestyle, with applications in finance, providing a practical\ninsight for decentralized system development.",
    "updated" : "2025-05-17T21:28:56Z",
    "published" : "2025-05-17T21:28:56Z",
    "authors" : [
      {
        "name" : "Juraj Mariani"
      },
      {
        "name" : "Ivan Homoliak"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.14585v1",
    "title" : "Context Reasoner: Incentivizing Reasoning Capability for Contextualized\n  Privacy and Safety Compliance via Reinforcement Learning",
    "summary" : "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.",
    "updated" : "2025-05-20T16:40:09Z",
    "published" : "2025-05-20T16:40:09Z",
    "authors" : [
      {
        "name" : "Wenbin Hu"
      },
      {
        "name" : "Haoran Li"
      },
      {
        "name" : "Huihao Jing"
      },
      {
        "name" : "Qi Hu"
      },
      {
        "name" : "Ziqian Zeng"
      },
      {
        "name" : "Sirui Han"
      },
      {
        "name" : "Heli Xu"
      },
      {
        "name" : "Tianshu Chu"
      },
      {
        "name" : "Peizhao Hu"
      },
      {
        "name" : "Yangqiu Song"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.14507v1",
    "title" : "Federated prediction for scalable and privacy-preserved knowledge-based\n  planning in radiotherapy",
    "summary" : "Background: Deep learning has potential to improve the efficiency and\nconsistency of radiation therapy planning, but clinical adoption is hindered by\nthe limited model generalizability due to data scarcity and heterogeneity among\ninstitutions. Although aggregating data from different institutions could\nalleviate this problem, data sharing is a practical challenge due to concerns\nabout patient data privacy and other technical obstacles. Purpose: This work\naims to address this dilemma by developing FedKBP+, a comprehensive federated\nlearning (FL) platform for predictive tasks in real-world applications in\nradiotherapy treatment planning. Methods: We implemented a unified\ncommunication stack based on Google Remote Procedure Call (gRPC) to support\ncommunication between participants whether located on the same workstation or\ndistributed across multiple workstations. In addition to supporting the\ncentralized FL strategies commonly available in existing open-source\nframeworks, FedKBP+ also provides a fully decentralized FL model where\nparticipants directly exchange model weights to each other through Peer-to-Peer\ncommunication. We evaluated FedKBP+ on three predictive tasks using\nscale-attention network (SA-Net) as the predictive model. Conclusions: Our\nresults demonstrate that FedKBP+ is highly effective, efficient and robust,\nshowing great potential as a federated learning platform for radiation therapy.",
    "updated" : "2025-05-20T15:35:49Z",
    "published" : "2025-05-20T15:35:49Z",
    "authors" : [
      {
        "name" : "Jingyun Chen"
      },
      {
        "name" : "David Horowitz"
      },
      {
        "name" : "Yading Yuan"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.14195v1",
    "title" : "Unraveling Interwoven Roles of Large Language Models in Authorship\n  Privacy: Obfuscation, Mimicking, and Verification",
    "summary" : "Recent advancements in large language models (LLMs) have been fueled by large\nscale training corpora drawn from diverse sources such as websites, news\narticles, and books. These datasets often contain explicit user information,\nsuch as person names and addresses, that LLMs may unintentionally reproduce in\ntheir generated outputs. Beyond such explicit content, LLMs can also leak\nidentity revealing cues through implicit signals such as distinctive writing\nstyles, raising significant concerns about authorship privacy. There are three\nmajor automated tasks in authorship privacy, namely authorship obfuscation\n(AO), authorship mimicking (AM), and authorship verification (AV). Prior\nresearch has studied AO, AM, and AV independently. However, their interplays\nremain under explored, which leaves a major research gap, especially in the era\nof LLMs, where they are profoundly shaping how we curate and share user\ngenerated content, and the distinction between machine generated and human\nauthored text is also increasingly blurred. This work then presents the first\nunified framework for analyzing the dynamic relationships among LLM enabled AO,\nAM, and AV in the context of authorship privacy. We quantify how they interact\nwith each other to transform human authored text, examining effects at a single\npoint in time and iteratively over time. We also examine the role of\ndemographic metadata, such as gender, academic background, in modulating their\nperformances, inter-task dynamics, and privacy risks. All source code will be\npublicly available.",
    "updated" : "2025-05-20T10:52:12Z",
    "published" : "2025-05-20T10:52:12Z",
    "authors" : [
      {
        "name" : "Tuc Nguyen"
      },
      {
        "name" : "Yifan Hu"
      },
      {
        "name" : "Thai Le"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.13957v1",
    "title" : "Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal\n  Retrieval-Augmented Generation",
    "summary" : "Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by\nintegrating external multimodal databases, but introduce unexplored privacy\nvulnerabilities. While text-based RAG privacy risks have been studied,\nmultimodal data presents unique challenges. We provide the first systematic\nanalysis of MRAG privacy vulnerabilities across vision-language and\nspeech-language modalities. Using a novel compositional structured prompt\nattack in a black-box setting, we demonstrate how attackers can extract private\ninformation by manipulating queries. Our experiments reveal that LMMs can both\ndirectly generate outputs resembling retrieved content and produce descriptions\nthat indirectly expose sensitive information, highlighting the urgent need for\nrobust privacy-preserving MRAG techniques.",
    "updated" : "2025-05-20T05:37:22Z",
    "published" : "2025-05-20T05:37:22Z",
    "authors" : [
      {
        "name" : "Jiankun Zhang"
      },
      {
        "name" : "Shenglai Zeng"
      },
      {
        "name" : "Jie Ren"
      },
      {
        "name" : "Tianqi Zheng"
      },
      {
        "name" : "Hui Liu"
      },
      {
        "name" : "Xianfeng Tang"
      },
      {
        "name" : "Hui Liu"
      },
      {
        "name" : "Yi Chang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.13694v1",
    "title" : "A Systematic Review and Taxonomy for Privacy Breach Classification:\n  Trends, Gaps, and Future Directions",
    "summary" : "In response to the rising frequency and complexity of data breaches and\nevolving global privacy regulations, this study presents a comprehensive\nexamination of academic literature on the classification of privacy breaches\nand violations between 2010-2024. Through a systematic literature review, a\ncorpus of screened studies was assembled and analyzed to identify primary\nresearch themes, emerging trends, and gaps in the field. A novel taxonomy is\nintroduced to guide efforts by categorizing research efforts into seven\ndomains: breach classification, report classification, breach detection, threat\ndetection, breach prediction, risk analysis, and threat classification. An\nanalysis reveals that breach classification and detection dominate the\nliterature, while breach prediction and risk analysis have only recently\nemerged in the literature, suggesting opportunities for potential research\nimpacts. Keyword and phrase frequency analysis reveal potentially underexplored\nareas, including location privacy, prediction models, and healthcare data\nbreaches.",
    "updated" : "2025-05-19T19:52:21Z",
    "published" : "2025-05-19T19:52:21Z",
    "authors" : [
      {
        "name" : "Clint Fuchs"
      },
      {
        "name" : "John D. Hastings"
      }
    ],
    "categories" : [
      "cs.CR",
      "C.2.0; D.4.6; K.6.5; K.4.1; H.3.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.13655v1",
    "title" : "Optimal Client Sampling in Federated Learning with Client-Level\n  Heterogeneous Differential Privacy",
    "summary" : "Federated Learning with client-level differential privacy (DP) provides a\npromising framework for collaboratively training models while rigorously\nprotecting clients' privacy. However, classic approaches like DP-FedAvg\nstruggle when clients have heterogeneous privacy requirements, as they must\nuniformly enforce the strictest privacy level across clients, leading to\nexcessive DP noise and significant model utility degradation. Existing methods\nto improve the model utility in such heterogeneous privacy settings often\nassume a trusted server and are largely heuristic, resulting in suboptimal\nperformance and lacking strong theoretical underpinnings. In this work, we\naddress these challenges under a practical attack model where both clients and\nthe server are honest-but-curious. We propose GDPFed, which partitions clients\ninto groups based on their privacy budgets and achieves client-level DP within\neach group to reduce the privacy budget waste and hence improve the model\nutility. Based on the privacy and convergence analysis of GDPFed, we find that\nthe magnitude of DP noise depends on both model dimensionality and the\nper-group client sampling ratios. To further improve the performance of GDPFed,\nwe introduce GDPFed$^+$, which integrates model sparsification to eliminate\nunnecessary noise and optimizes per-group client sampling ratios to minimize\nconvergence error. Extensive empirical evaluations on multiple benchmark\ndatasets demonstrate the effectiveness of GDPFed$^+$, showing substantial\nperformance gains compared with state-of-the-art methods.",
    "updated" : "2025-05-19T18:55:34Z",
    "published" : "2025-05-19T18:55:34Z",
    "authors" : [
      {
        "name" : "Jiahao Xu"
      },
      {
        "name" : "Rui Hu"
      },
      {
        "name" : "Olivera Kotevska"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.13085v2",
    "title" : "Universal Semantic Disentangled Privacy-preserving Speech Representation\n  Learning",
    "summary" : "The use of audio recordings of human speech to train LLMs poses privacy\nconcerns due to these models' potential to generate outputs that closely\nresemble artifacts in the training data. In this study, we propose a speaker\nprivacy-preserving representation learning method through the Universal Speech\nCodec (USC), a computationally efficient encoder-decoder model that\ndisentangles speech into: (i) privacy-preserving semantically rich\nrepresentations, capturing content and speech paralinguistics, and (ii)\nresidual acoustic and speaker representations that enables high-fidelity\nreconstruction. Extensive evaluations presented show that USC's semantic\nrepresentation preserves content, prosody, and sentiment, while removing\npotentially identifiable speaker attributes. Combining both representations,\nUSC achieves state-of-the-art speech reconstruction. Additionally, we introduce\nan evaluation methodology for measuring privacy-preserving properties, aligning\nwith perceptual tests. We compare USC against other codecs in the literature\nand demonstrate its effectiveness on privacy-preserving representation\nlearning, illustrating the trade-offs of speaker anonymization, paralinguistics\nretention and content preservation in the learned semantic representations.\nAudio samples are shared in https://www.amazon.science/usc-samples.",
    "updated" : "2025-05-20T10:22:17Z",
    "published" : "2025-05-19T13:19:49Z",
    "authors" : [
      {
        "name" : "Biel Tura Vecino"
      },
      {
        "name" : "Subhadeep Maji"
      },
      {
        "name" : "Aravind Varier"
      },
      {
        "name" : "Antonio Bonafonte"
      },
      {
        "name" : "Ivan Valles"
      },
      {
        "name" : "Michael Owen"
      },
      {
        "name" : "Leif Rädel"
      },
      {
        "name" : "Grant Strimel"
      },
      {
        "name" : "Seyi Feyisetan"
      },
      {
        "name" : "Roberto Barra Chicote"
      },
      {
        "name" : "Ariya Rastrow"
      },
      {
        "name" : "Constantinos Papayiannis"
      },
      {
        "name" : "Volker Leutnant"
      },
      {
        "name" : "Trevor Wood"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.02392v3",
    "title" : "Moneros Decentralized P2P Exchanges: Functionality, Adoption, and\n  Privacy Risks",
    "summary" : "Privacy-focused cryptocurrencies like Monero remain popular, despite\nincreasing regulatory scrutiny that has led to their delisting from major\ncentralized exchanges. The latter also explains the recent popularity of\ndecentralized exchanges (DEXs) with no centralized ownership structures. These\nplatforms typically leverage peer-to-peer (P2P) networks, promising secure and\nanonymous asset trading. However, questions of liability remain, and the\nacademic literature lacks comprehensive insights into the functionality,\ntrading activity, and privacy claims of these P2P platforms. In this paper, we\nprovide an early systematization of the current landscape of decentralized\npeer-to-peer exchanges within the Monero ecosystem. We examine several recently\ndeveloped DEX platforms, analyzing their popularity, functionality,\narchitectural choices, and potential weaknesses. We further identify and report\non a privacy vulnerability in the recently popularized Haveno exchange,\ndemonstrating that certain Haveno trades could be detected, allowing\ntransactions to be linked across the Monero and Bitcoin blockchains. We hope\nthat our findings can nourish the discussion in the research community about\nmore secure designs, and provide insights for regulators.",
    "updated" : "2025-05-20T16:06:42Z",
    "published" : "2025-05-05T06:27:37Z",
    "authors" : [
      {
        "name" : "Yannik Kopyciok"
      },
      {
        "name" : "Friedhelm Victor"
      },
      {
        "name" : "Stefan Schmid"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.15721v1",
    "title" : "Privacy-Preserving Conformal Prediction Under Local Differential Privacy",
    "summary" : "Conformal prediction (CP) provides sets of candidate classes with a\nguaranteed probability of containing the true class. However, it typically\nrelies on a calibration set with clean labels. We address privacy-sensitive\nscenarios where the aggregator is untrusted and can only access a perturbed\nversion of the true labels. We propose two complementary approaches under local\ndifferential privacy (LDP). In the first approach, users do not access the\nmodel but instead provide their input features and a perturbed label using a\nk-ary randomized response. In the second approach, which enforces stricter\nprivacy constraints, users add noise to their conformity score by binary search\nresponse. This method requires access to the classification model but preserves\nboth data and label privacy. Both approaches compute the conformal threshold\ndirectly from noisy data without accessing the true labels. We prove\nfinite-sample coverage guarantees and demonstrate robust coverage even under\nsevere randomization. This approach unifies strong local privacy with\npredictive uncertainty control, making it well-suited for sensitive\napplications such as medical imaging or large language model queries,\nregardless of whether users can (or are willing to) compute their own scores.",
    "updated" : "2025-05-21T16:29:44Z",
    "published" : "2025-05-21T16:29:44Z",
    "authors" : [
      {
        "name" : "Coby Penso"
      },
      {
        "name" : "Bar Mahpud"
      },
      {
        "name" : "Jacob Goldberger"
      },
      {
        "name" : "Or Sheffet"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.15483v1",
    "title" : "Optimal Piecewise-based Mechanism for Collecting Bounded Numerical Data\n  under Local Differential Privacy",
    "summary" : "Numerical data with bounded domains is a common data type in personal\ndevices, such as wearable sensors. While the collection of such data is\nessential for third-party platforms, it raises significant privacy concerns.\nLocal differential privacy (LDP) has been shown as a framework providing\nprovable individual privacy, even when the third-party platform is untrusted.\nFor numerical data with bounded domains, existing state-of-the-art LDP\nmechanisms are piecewise-based mechanisms, which are not optimal, leading to\nreduced data utility.\n  This paper investigates the optimal design of piecewise-based mechanisms to\nmaximize data utility under LDP. We demonstrate that existing piecewise-based\nmechanisms are heuristic forms of the $3$-piecewise mechanism, which is far\nfrom enough to study optimality. We generalize the $3$-piecewise mechanism to\nits most general form, i.e. $m$-piecewise mechanism with no pre-defined form of\neach piece. Under this form, we derive the closed-form optimal mechanism by\ncombining analytical proofs and off-the-shelf optimization solvers. Next, we\nextend the generalized piecewise-based mechanism to the circular domain (along\nwith the classical domain), defined on a cyclic range where the distance\nbetween the two endpoints is zero. By incorporating this property, we design\nthe optimal mechanism for the circular domain, achieving significantly improved\ndata utility compared with existing mechanisms.\n  Our proposed mechanisms guarantee optimal data utility under LDP among all\ngeneralized piecewise-based mechanisms. We show that they also achieve optimal\ndata utility in two common applications of LDP: distribution estimation and\nmean estimation. Theoretical analyses and experimental evaluations prove and\nvalidate the data utility advantages of our proposed mechanisms.",
    "updated" : "2025-05-21T13:01:41Z",
    "published" : "2025-05-21T13:01:41Z",
    "authors" : [
      {
        "name" : "Ye Zheng"
      },
      {
        "name" : "Sumita Mishra"
      },
      {
        "name" : "Yidan Hu"
      }
    ],
    "categories" : [
      "cs.CR",
      "E.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.15476v1",
    "title" : "Pura: An Efficient Privacy-Preserving Solution for Face Recognition",
    "summary" : "Face recognition is an effective technology for identifying a target person\nby facial images. However, sensitive facial images raises privacy concerns.\nAlthough privacy-preserving face recognition is one of potential solutions,\nthis solution neither fully addresses the privacy concerns nor is efficient\nenough. To this end, we propose an efficient privacy-preserving solution for\nface recognition, named Pura, which sufficiently protects facial privacy and\nsupports face recognition over encrypted data efficiently. Specifically, we\npropose a privacy-preserving and non-interactive architecture for face\nrecognition through the threshold Paillier cryptosystem. Additionally, we\ncarefully design a suite of underlying secure computing protocols to enable\nefficient operations of face recognition over encrypted data directly.\nFurthermore, we introduce a parallel computing mechanism to enhance the\nperformance of the proposed secure computing protocols. Privacy analysis\ndemonstrates that Pura fully safeguards personal facial privacy. Experimental\nevaluations demonstrate that Pura achieves recognition speeds up to 16 times\nfaster than the state-of-the-art.",
    "updated" : "2025-05-21T12:50:25Z",
    "published" : "2025-05-21T12:50:25Z",
    "authors" : [
      {
        "name" : "Guotao Xu"
      },
      {
        "name" : "Bowen Zhao"
      },
      {
        "name" : "Yang Xiao"
      },
      {
        "name" : "Yantao Zhong"
      },
      {
        "name" : "Liang Zhai"
      },
      {
        "name" : "Qingqi Pei"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.15376v1",
    "title" : "Federated Learning-Enhanced Blockchain Framework for Privacy-Preserving\n  Intrusion Detection in Industrial IoT",
    "summary" : "Industrial Internet of Things (IIoT) systems have become integral to smart\nmanufacturing, yet their growing connectivity has also exposed them to\nsignificant cybersecurity threats. Traditional intrusion detection systems\n(IDS) often rely on centralized architectures that raise concerns over data\nprivacy, latency, and single points of failure. In this work, we propose a\nnovel Federated Learning-Enhanced Blockchain Framework (FL-BCID) for\nprivacy-preserving intrusion detection tailored for IIoT environments. Our\narchitecture combines federated learning (FL) to ensure decentralized model\ntraining with blockchain technology to guarantee data integrity, trust, and\ntamper resistance across IIoT nodes. We design a lightweight intrusion\ndetection model collaboratively trained using FL across edge devices without\nexposing sensitive data. A smart contract-enabled blockchain system records\nmodel updates and anomaly scores to establish accountability. Experimental\nevaluations using the ToN-IoT and N-BaIoT datasets demonstrate the superior\nperformance of our framework, achieving 97.3% accuracy while reducing\ncommunication overhead by 41% compared to baseline centralized methods. Our\napproach ensures privacy, scalability, and robustness-critical for secure\nindustrial operations. The proposed FL-BCID system provides a promising\nsolution for enhancing trust and privacy in modern IIoT security architectures.",
    "updated" : "2025-05-21T11:11:44Z",
    "published" : "2025-05-21T11:11:44Z",
    "authors" : [
      {
        "name" : "Anas Ali"
      },
      {
        "name" : "Mubashar Husain"
      },
      {
        "name" : "Peter Hans"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.15156v1",
    "title" : "Privacy-Preserving Socialized Recommendation based on Multi-View\n  Clustering in a Cloud Environment",
    "summary" : "Recommendation as a service has improved the quality of our lives and plays a\nsignificant role in variant aspects. However, the preference of users may\nreveal some sensitive information, so that the protection of privacy is\nrequired. In this paper, we propose a privacy-preserving, socialized,\nrecommendation protocol that introduces information collected from online\nsocial networks to enhance the quality of the recommendation. The proposed\nscheme can calculate the similarity between users to determine their potential\nrelationships and interests, and it also can protect the users' privacy from\nleaking to an untrusted third party. The security analysis and experimental\nresults showed that our proposed scheme provides excellent performance and is\nfeasible for real-world applications.",
    "updated" : "2025-05-21T06:21:21Z",
    "published" : "2025-05-21T06:21:21Z",
    "authors" : [
      {
        "name" : "Cheng Guo"
      },
      {
        "name" : "Jing Jia"
      },
      {
        "name" : "Peng Wang"
      },
      {
        "name" : "Jing Zhang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.14959v1",
    "title" : "Privacy Preserving Conversion Modeling in Data Clean Room",
    "summary" : "In the realm of online advertising, accurately predicting the conversion rate\n(CVR) is crucial for enhancing advertising efficiency and user satisfaction.\nThis paper addresses the challenge of CVR prediction while adhering to user\nprivacy preferences and advertiser requirements. Traditional methods face\nobstacles such as the reluctance of advertisers to share sensitive conversion\ndata and the limitations of model training in secure environments like data\nclean rooms. We propose a novel model training framework that enables\ncollaborative model training without sharing sample-level gradients with the\nadvertising platform. Our approach introduces several innovative components:\n(1) utilizing batch-level aggregated gradients instead of sample-level\ngradients to minimize privacy risks; (2) applying adapter-based\nparameter-efficient fine-tuning and gradient compression to reduce\ncommunication costs; and (3) employing de-biasing techniques to train the model\nunder label differential privacy, thereby maintaining accuracy despite\nprivacy-enhanced label perturbations. Our experimental results, conducted on\nindustrial datasets, demonstrate that our method achieves competitive ROCAUC\nperformance while significantly decreasing communication overhead and complying\nwith both advertiser privacy requirements and user privacy choices. This\nframework establishes a new standard for privacy-preserving, high-performance\nCVR prediction in the digital advertising landscape.",
    "updated" : "2025-05-20T22:38:50Z",
    "published" : "2025-05-20T22:38:50Z",
    "authors" : [
      {
        "name" : "Kungang Li"
      },
      {
        "name" : "Xiangyi Chen"
      },
      {
        "name" : "Ling Leng"
      },
      {
        "name" : "Jiajing Xu"
      },
      {
        "name" : "Jiankai Sun"
      },
      {
        "name" : "Behnam Rezaei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.IR",
      "H.4"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.14797v1",
    "title" : "Efficient Privacy-Preserving Cross-Silo Federated Learning with\n  Multi-Key Homomorphic Encryption",
    "summary" : "Federated Learning (FL) is susceptible to privacy attacks, such as data\nreconstruction attacks, in which a semi-honest server or a malicious client\ninfers information about other clients' datasets from their model updates or\ngradients. To enhance the privacy of FL, recent studies combined Multi-Key\nHomomorphic Encryption (MKHE) and FL, making it possible to aggregate the\nencrypted model updates using different keys without having to decrypt them.\nDespite the privacy guarantees of MKHE, existing approaches are not well-suited\nfor real-world deployment due to their high computation and communication\noverhead. We propose MASER, an efficient MKHE-based Privacy-Preserving FL\nframework that combines consensus-based model pruning and slicing techniques to\nreduce this overhead. Our experimental results show that MASER is 3.03 to 8.29\ntimes more efficient than existing MKHE-based FL approaches in terms of\ncomputation and communication overhead while maintaining comparable\nclassification accuracy to standard FL algorithms. Compared to a vanilla FL\nalgorithm, the overhead of MASER is only 1.48 to 5 times higher, striking a\ngood balance between privacy, accuracy, and efficiency in both IID and non-IID\nsettings.",
    "updated" : "2025-05-20T18:08:15Z",
    "published" : "2025-05-20T18:08:15Z",
    "authors" : [
      {
        "name" : "Abdullah Al Omar"
      },
      {
        "name" : "Xin Yang"
      },
      {
        "name" : "Euijin Choo"
      },
      {
        "name" : "Omid Ardakanian"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.14507v1",
    "title" : "Federated prediction for scalable and privacy-preserved knowledge-based\n  planning in radiotherapy",
    "summary" : "Background: Deep learning has potential to improve the efficiency and\nconsistency of radiation therapy planning, but clinical adoption is hindered by\nthe limited model generalizability due to data scarcity and heterogeneity among\ninstitutions. Although aggregating data from different institutions could\nalleviate this problem, data sharing is a practical challenge due to concerns\nabout patient data privacy and other technical obstacles. Purpose: This work\naims to address this dilemma by developing FedKBP+, a comprehensive federated\nlearning (FL) platform for predictive tasks in real-world applications in\nradiotherapy treatment planning. Methods: We implemented a unified\ncommunication stack based on Google Remote Procedure Call (gRPC) to support\ncommunication between participants whether located on the same workstation or\ndistributed across multiple workstations. In addition to supporting the\ncentralized FL strategies commonly available in existing open-source\nframeworks, FedKBP+ also provides a fully decentralized FL model where\nparticipants directly exchange model weights to each other through Peer-to-Peer\ncommunication. We evaluated FedKBP+ on three predictive tasks using\nscale-attention network (SA-Net) as the predictive model. Conclusions: Our\nresults demonstrate that FedKBP+ is highly effective, efficient and robust,\nshowing great potential as a federated learning platform for radiation therapy.",
    "updated" : "2025-05-20T15:35:49Z",
    "published" : "2025-05-20T15:35:49Z",
    "authors" : [
      {
        "name" : "Jingyun Chen"
      },
      {
        "name" : "David Horowitz"
      },
      {
        "name" : "Yading Yuan"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.LG",
      "physics.med-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.16954v1",
    "title" : "Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of\n  Vulnerabilities in Privacy Protection",
    "summary" : "Traditional methods for raising awareness of privacy protection often fail to\nengage users or provide hands-on insights into how privacy vulnerabilities are\nexploited. To address this, we incorporate an adversarial mechanic in the\ndesign of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to\nsimulate natural interactions, the game challenges players to impersonate\ncharacters and extract sensitive information from an AI agent, Aegis. A user\nstudy (n=22) revealed that players employed diverse deceptive linguistic\nstrategies, including storytelling and emotional rapport, to manipulate Aegis.\nAfter playing, players reported connecting in-game scenarios with real-world\nprivacy vulnerabilities, such as phishing and impersonation, and expressed\nintentions to strengthen privacy control, such as avoiding oversharing personal\ninformation with AI systems. This work highlights the potential of LLMs to\nsimulate complex relational interactions in serious games, while demonstrating\nhow an adversarial game strategy provides unique insights for designs for\nsocial good, particularly privacy protection.",
    "updated" : "2025-05-22T17:34:45Z",
    "published" : "2025-05-22T17:34:45Z",
    "authors" : [
      {
        "name" : "Jiaying Fu"
      },
      {
        "name" : "Yiyang Lu"
      },
      {
        "name" : "Zehua Yang"
      },
      {
        "name" : "Fiona Nah"
      },
      {
        "name" : "RAY LC"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.16371v1",
    "title" : "Privacy-Aware Cyberterrorism Network Analysis using Graph Neural\n  Networks and Federated Learning",
    "summary" : "Cyberterrorism poses a formidable threat to digital infrastructures, with\nincreasing reliance on encrypted, decentralized platforms that obscure threat\nactor activity. To address the challenge of analyzing such adversarial networks\nwhile preserving the privacy of distributed intelligence data, we propose a\nPrivacy-Aware Federated Graph Neural Network (PA-FGNN) framework. PA-FGNN\nintegrates graph attention networks, differential privacy, and homomorphic\nencryption into a robust federated learning pipeline tailored for\ncyberterrorism network analysis. Each client trains locally on sensitive graph\ndata and exchanges encrypted, noise-perturbed model updates with a central\naggregator, which performs secure aggregation and broadcasts global updates. We\nimplement anomaly detection for flagging high-risk nodes and incorporate\ndefenses against gradient poisoning. Experimental evaluations on simulated dark\nweb and cyber-intelligence graphs demonstrate that PA-FGNN achieves over 91\\%\nclassification accuracy, maintains resilience under 20\\% adversarial client\nbehavior, and incurs less than 18\\% communication overhead. Our results\nhighlight that privacy-preserving GNNs can support large-scale cyber threat\ndetection without compromising on utility, privacy, or robustness.",
    "updated" : "2025-05-22T08:26:09Z",
    "published" : "2025-05-22T08:26:09Z",
    "authors" : [
      {
        "name" : "Anas Ali"
      },
      {
        "name" : "Mubashar Husain"
      },
      {
        "name" : "Peter Hans"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.16059v1",
    "title" : "Monitoring in the Dark: Privacy-Preserving Runtime Verification of\n  Cyber-Physical Systems",
    "summary" : "In distributed Cyber-Physical Systems and Internet-of-Things applications,\nthe nodes of the system send measurements to a monitor that checks whether\nthese measurements satisfy given formal specifications. For instance in Urban\nAir Mobility, a local traffic authority will be monitoring drone traffic to\nevaluate its flow and detect emerging problematic patterns. Certain\napplications require both the specification and the measurements to be private\n-- i.e. known only to their owners. Examples include traffic monitoring,\ntesting of integrated circuit designs, and medical monitoring by wearable or\nimplanted devices. In this paper we propose a protocol that enables\nprivacy-preserving robustness monitoring. By following our protocol, both\nsystem (e.g. drone) and monitor (e.g. traffic authority) only learn the\nrobustness of the measured trace w.r.t. the specification. But the system\nlearns nothing about the formula, and the monitor learns nothing about the\nsignal monitored. We do this using garbled circuits, for specifications in\nSignal Temporal Logic interpreted over timed state sequences. We analyze the\nruntime and memory overhead of privacy preservation, the size of the circuits,\nand their practicality for three different usage scenarios: design testing,\noffline monitoring, and online monitoring of Cyber-Physical Systems.",
    "updated" : "2025-05-21T22:20:25Z",
    "published" : "2025-05-21T22:20:25Z",
    "authors" : [
      {
        "name" : "Charles Koll"
      },
      {
        "name" : "Preston Tan Hang"
      },
      {
        "name" : "Mike Rosulek"
      },
      {
        "name" : "Houssam Abbas"
      }
    ],
    "categories" : [
      "cs.LO",
      "B.5.0; I.2.4"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.17145v1",
    "title" : "LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy\n  Compliance",
    "summary" : "Large language models (LLMs) are increasingly applied in fields such as\nfinance, education, and governance due to their ability to generate human-like\ntext and adapt to specialized tasks. However, their widespread adoption raises\ncritical concerns about data privacy and security, including the risk of\nsensitive data exposure.\n  In this paper, we propose a security framework to enforce policy compliance\nand mitigate risks in LLM interactions. Our approach introduces three key\ninnovations: (i) LLM-based policy enforcement: a customizable mechanism that\nenhances domain-specific detection of sensitive data. (ii) Dynamic policy\ncustomization: real-time policy adaptation and enforcement during user-LLM\ninteractions to ensure compliance with evolving security requirements. (iii)\nSensitive data anonymization: a format-preserving encryption technique that\nprotects sensitive information while maintaining contextual integrity.\nExperimental results demonstrate that our framework effectively mitigates\nsecurity risks while preserving the functional accuracy of LLM-driven tasks.",
    "updated" : "2025-05-22T07:30:37Z",
    "published" : "2025-05-22T07:30:37Z",
    "authors" : [
      {
        "name" : "Yu Wang"
      },
      {
        "name" : "Cailing Cai"
      },
      {
        "name" : "Zhihua Xiao"
      },
      {
        "name" : "Peifung E. Lam"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.20118v1",
    "title" : "TrojanStego: Your Language Model Can Secretly Be A Steganographic\n  Privacy Leaking Agent",
    "summary" : "As large language models (LLMs) become integrated into sensitive workflows,\nconcerns grow over their potential to leak confidential information. We propose\nTrojanStego, a novel threat model in which an adversary fine-tunes an LLM to\nembed sensitive context information into natural-looking outputs via linguistic\nsteganography, without requiring explicit control over inference inputs. We\nintroduce a taxonomy outlining risk factors for compromised LLMs, and use it to\nevaluate the risk profile of the threat. To implement TrojanStego, we propose a\npractical encoding scheme based on vocabulary partitioning learnable by LLMs\nvia fine-tuning. Experimental results show that compromised models reliably\ntransmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over\n97% accuracy using majority voting across three generations. Further, they\nmaintain high utility, can evade human detection, and preserve coherence. These\nresults highlight a new class of LLM data exfiltration attacks that are\npassive, covert, practical, and dangerous.",
    "updated" : "2025-05-26T15:20:51Z",
    "published" : "2025-05-26T15:20:51Z",
    "authors" : [
      {
        "name" : "Dominik Meier"
      },
      {
        "name" : "Jan Philip Wahle"
      },
      {
        "name" : "Paul Röttger"
      },
      {
        "name" : "Terry Ruas"
      },
      {
        "name" : "Bela Gipp"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.20095v1",
    "title" : "Spurious Privacy Leakage in Neural Networks",
    "summary" : "Neural networks are vulnerable to privacy attacks aimed at stealing sensitive\ndata. The risks can be amplified in a real-world scenario, particularly when\nmodels are trained on limited and biased data. In this work, we investigate the\nimpact of spurious correlation bias on privacy vulnerability. We introduce\n\\emph{spurious privacy leakage}, a phenomenon where spurious groups are\nsignificantly more vulnerable to privacy attacks than non-spurious groups. We\nfurther show that group privacy disparity increases in tasks with simpler\nobjectives (e.g. fewer classes) due to the persistence of spurious features.\nSurprisingly, we find that reducing spurious correlation using spurious robust\nmethods does not mitigate spurious privacy leakage. This leads us to introduce\na perspective on privacy disparity based on memorization, where mitigating\nspurious correlation does not mitigate the memorization of spurious data, and\ntherefore, neither the privacy level. Lastly, we compare the privacy of\ndifferent model architectures trained with spurious data, demonstrating that,\ncontrary to prior works, architectural choice can affect privacy outcomes.",
    "updated" : "2025-05-26T15:04:39Z",
    "published" : "2025-05-26T15:04:39Z",
    "authors" : [
      {
        "name" : "Chenxiang Zhang"
      },
      {
        "name" : "Jun Pang"
      },
      {
        "name" : "Sjouke Mauw"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.19969v1",
    "title" : "Differential Privacy Analysis of Decentralized Gossip Averaging under\n  Varying Threat Models",
    "summary" : "Fully decentralized training of machine learning models offers significant\nadvantages in scalability, robustness, and fault tolerance. However, achieving\ndifferential privacy (DP) in such settings is challenging due to the absence of\na central aggregator and varying trust assumptions among nodes. In this work,\nwe present a novel privacy analysis of decentralized gossip-based averaging\nalgorithms with additive node-level noise, both with and without secure\nsummation over each node's direct neighbors. Our main contribution is a new\nanalytical framework based on a linear systems formulation that accurately\ncharacterizes privacy leakage across these scenarios. This framework\nsignificantly improves upon prior analyses, for example, reducing the R\\'enyi\nDP parameter growth from $O(T^2)$ to $O(T)$, where $T$ is the number of\ntraining rounds. We validate our analysis with numerical results demonstrating\nsuperior DP bounds compared to existing approaches. We further illustrate our\nanalysis with a logistic regression experiment on MNIST image classification in\na fully decentralized setting, demonstrating utility comparable to central\naggregation methods.",
    "updated" : "2025-05-26T13:31:43Z",
    "published" : "2025-05-26T13:31:43Z",
    "authors" : [
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "Tejas Kulkarni"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.19951v1",
    "title" : "Novel Loss-Enhanced Universal Adversarial Patches for Sustainable\n  Speaker Privacy",
    "summary" : "Deep learning voice models are commonly used nowadays, but the safety\nprocessing of personal data, such as human identity and speech content, remains\nsuspicious. To prevent malicious user identification, speaker anonymization\nmethods were proposed. Current methods, particularly based on universal\nadversarial patch (UAP) applications, have drawbacks such as significant\ndegradation of audio quality, decreased speech recognition quality, low\ntransferability across different voice biometrics models, and performance\ndependence on the input audio length. To mitigate these drawbacks, in this\nwork, we introduce and leverage the novel Exponential Total Variance (TV) loss\nfunction and provide experimental evidence that it positively affects UAP\nstrength and imperceptibility. Moreover, we present a novel scalable UAP\ninsertion procedure and demonstrate its uniformly high performance for various\naudio lengths.",
    "updated" : "2025-05-26T13:16:01Z",
    "published" : "2025-05-26T13:16:01Z",
    "authors" : [
      {
        "name" : "Elvir Karimov"
      },
      {
        "name" : "Alexander Varlamov"
      },
      {
        "name" : "Danil Ivanov"
      },
      {
        "name" : "Dmitrii Korzh"
      },
      {
        "name" : "Oleg Y. Rogov"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.19823v1",
    "title" : "LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning\n  in Heterogeneous Environments",
    "summary" : "Federated Learning (FL) is a distributed machine learning paradigm based on\nprotecting data privacy of devices, which however, can still be broken by\ngradient leakage attack via parameter inversion techniques. Differential\nprivacy (DP) technology reduces the risk of private data leakage by adding\nartificial noise to the gradients, but detrimental to the FL utility at the\nsame time, especially in the scenario where the data is Non-Independent\nIdentically Distributed (Non-IID). Based on the impact of heterogeneous data on\naggregation performance, this paper proposes a Lightweight Adaptive Privacy\nAllocation (LAPA) strategy, which assigns personalized privacy budgets to\ndevices in each aggregation round without transmitting any additional\ninformation beyond gradients, ensuring both privacy protection and aggregation\nefficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG)\nalgorithm is employed to optimize the transmission power, in order to determine\nthe optimal timing at which the adaptively attenuated artificial noise aligns\nwith the communication noise, enabling an effective balance between DP and\nsystem utility. Finally, a reliable aggregation strategy is designed by\nintegrating communication quality and data distribution characteristics, which\nimproves aggregation performance while preserving privacy. Experimental results\ndemonstrate that the personalized noise allocation and dynamic optimization\nstrategy based on LAPA proposed in this paper enhances convergence performance\nwhile satisfying the privacy requirements of FL.",
    "updated" : "2025-05-26T11:00:31Z",
    "published" : "2025-05-26T11:00:31Z",
    "authors" : [
      {
        "name" : "Pengcheng Sun"
      },
      {
        "name" : "Erwu Liu"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "Rui Wang"
      },
      {
        "name" : "Yuanzhe Geng"
      },
      {
        "name" : "Lijuan Lai"
      },
      {
        "name" : "Abbas Jamalipour"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.18870v1",
    "title" : "Understanding the Relationship Between Personal Data Privacy Literacy\n  and Data Privacy Information Sharing by University Students",
    "summary" : "With constant threats to the safety of personal data in the United States,\nprivacy literacy has become an increasingly important competency among\nuniversity students, one that ties intimately to the information sharing\nbehavior of these students. This survey based study examines how university\nstudents in the United States perceive personal data privacy and how their\nprivacy literacy influences their understanding and behaviors. Students\nresponses to a privacy literacy scale were categorized into high and low\nprivacy literacy groups, revealing that high literacy individuals demonstrate a\nbroader range of privacy practices, including multi factor authentication, VPN\nusage, and phishing awareness, whereas low literacy individuals rely on more\nbasic security measures. Statistical analyses suggest that high literacy\nrespondents display greater diversity in recommendations and engagement in\nprivacy discussions. These findings suggest the need for enhanced educational\ninitiatives to improve data privacy awareness at the university level to create\na better cyber safe population.",
    "updated" : "2025-05-24T21:14:53Z",
    "published" : "2025-05-24T21:14:53Z",
    "authors" : [
      {
        "name" : "Brady D. Lund"
      },
      {
        "name" : "Bryan Anderson"
      },
      {
        "name" : "Ana Roeschley"
      },
      {
        "name" : "Gahangir Hossain"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.18786v1",
    "title" : "Leveraging Per-Instance Privacy for Machine Unlearning",
    "summary" : "We present a principled, per-instance approach to quantifying the difficulty\nof unlearning via fine-tuning. We begin by sharpening an analysis of noisy\ngradient descent for unlearning (Chien et al., 2024), obtaining a better\nutility-unlearning tradeoff by replacing worst-case privacy loss bounds with\nper-instance privacy losses (Thudi et al., 2024), each of which bounds the\n(Renyi) divergence to retraining without an individual data point. To\ndemonstrate the practical applicability of our theory, we present empirical\nresults showing that our theoretical predictions are born out both for\nStochastic Gradient Langevin Dynamics (SGLD) as well as for standard\nfine-tuning without explicit noise. We further demonstrate that per-instance\nprivacy losses correlate well with several existing data difficulty metrics,\nwhile also identifying harder groups of data points, and introduce novel\nevaluation methods based on loss barriers. All together, our findings provide a\nfoundation for more efficient and adaptive unlearning strategies tailored to\nthe unique properties of individual data points.",
    "updated" : "2025-05-24T16:55:57Z",
    "published" : "2025-05-24T16:55:57Z",
    "authors" : [
      {
        "name" : "Nazanin Mohammadi Sepahvand"
      },
      {
        "name" : "Anvith Thudi"
      },
      {
        "name" : "Berivan Isik"
      },
      {
        "name" : "Ashmita Bhattacharyya"
      },
      {
        "name" : "Nicolas Papernot"
      },
      {
        "name" : "Eleni Triantafillou"
      },
      {
        "name" : "Daniel M. Roy"
      },
      {
        "name" : "Gintare Karolina Dziugaite"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.18386v1",
    "title" : "Modeling interdependent privacy threats",
    "summary" : "The rise of online social networks, user-gene-rated content, and third-party\napps made data sharing an inevitable trend, driven by both user behavior and\nthe commercial value of personal information. As service providers amass vast\namounts of data, safeguarding individual privacy has become increasingly\nchallenging. Privacy threat modeling has emerged as a critical tool for\nidentifying and mitigating risks, with methodologies such as LINDDUN, xCOMPASS,\nand PANOPTIC offering systematic approaches. However, these frameworks\nprimarily focus on threats arising from interactions between a single user and\nsystem components, often overlooking interdependent privacy (IDP); the\nphenomenon where one user's actions affect the privacy of other users and even\nnon-users. IDP risks are particularly pronounced in third-party applications,\nwhere platform permissions, APIs, and user behavior can lead to unintended and\nunconsented data sharing, such as in the Cambridge Analytica case. We argue\nthat existing threat modeling approaches are limited in exposing IDP-related\nthreats, potentially underestimating privacy risks. To bridge this gap, we\npropose a specialized methodology that explicitly focuses on interdependent\nprivacy. Our contributions are threefold: (i) we identify IDP-specific\nchallenges and limitations in current threat modeling frameworks, (ii) we\ncreate IDPA, a threat modeling approach tailored to IDP threats, and (iii) we\nvalidate our approach through a case study on WeChat. We believe that IDPA can\noperate effectively on systems other than third-party apps and may motivate\nfurther research on specialized threat modeling.",
    "updated" : "2025-05-23T21:22:49Z",
    "published" : "2025-05-23T21:22:49Z",
    "authors" : [
      {
        "name" : "Shuaishuai Liu"
      },
      {
        "name" : "Gergely Biczók"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.18242v1",
    "title" : "Privacy-Preserving Bathroom Monitoring for Elderly Emergencies Using PIR\n  and LiDAR Sensors",
    "summary" : "In-home elderly monitoring requires systems that can detect emergency events\n- such as falls or prolonged inactivity - while preserving privacy and\nrequiring no user input. These systems must be embedded into the surrounding\nenvironment, capable of capturing activity, and responding promptly. This paper\npresents a low-cost, privacy-preserving solution using Passive Infrared (PIR)\nand Light Detection and Ranging (LiDAR) sensors to track entries, sitting,\nexits, and emergency scenarios within a home bathroom setting. We developed and\nevaluated a rule-based detection system through five real-world experiments\nsimulating elderly behavior. Annotated time-series graphs demonstrate the\nsystem's ability to detect dangerous states, such as motionless collapses,\nwhile maintaining privacy through non-visual sensing.",
    "updated" : "2025-05-23T15:49:43Z",
    "published" : "2025-05-23T15:49:43Z",
    "authors" : [
      {
        "name" : "Youssouf Sidibé"
      },
      {
        "name" : "Julia Gersey"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07872v2",
    "title" : "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
    "summary" : "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
    "updated" : "2025-05-23T20:44:40Z",
    "published" : "2025-05-09T21:05:20Z",
    "authors" : [
      {
        "name" : "Yijing Zhang"
      },
      {
        "name" : "Ferdous Pervej"
      },
      {
        "name" : "Andreas F. Molisch"
      }
    ],
    "categories" : [
      "cs.NI",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.05130v2",
    "title" : "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models",
    "summary" : "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
    "updated" : "2025-05-24T09:33:35Z",
    "published" : "2025-05-08T11:07:35Z",
    "authors" : [
      {
        "name" : "Mengjun Yi"
      },
      {
        "name" : "Hanwen Zhang"
      },
      {
        "name" : "Hui Dou"
      },
      {
        "name" : "Jian Zhao"
      },
      {
        "name" : "Furao Shen"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.03519v3",
    "title" : "Revisiting Model Inversion Evaluation: From Misleading Standards to\n  Reliable Privacy Assessment",
    "summary" : "Model Inversion (MI) attacks aim to reconstruct information from private\ntraining data by exploiting access to machine learning models T. To evaluate\nsuch attacks, the standard evaluation framework for such attacks relies on an\nevaluation model E, trained under the same task design as T. This framework has\nbecome the de facto standard for assessing progress in MI research, used across\nnearly all recent MI attacks and defenses without question. In this paper, we\npresent the first in-depth study of this MI evaluation framework. In\nparticular, we identify a critical issue of this standard MI evaluation\nframework: Type-I adversarial examples. These are reconstructions that do not\ncapture the visual features of private training data, yet are still deemed\nsuccessful by the target model T and ultimately transferable to E. Such false\npositives undermine the reliability of the standard MI evaluation framework. To\naddress this issue, we introduce a new MI evaluation framework that replaces\nthe evaluation model E with advanced Multimodal Large Language Models (MLLMs).\nBy leveraging their general-purpose visual understanding, our MLLM-based\nframework does not depend on training of shared task design as in T, thus\nreducing Type-I transferability and providing more faithful assessments of\nreconstruction success. Using our MLLM-based evaluation framework, we\nreevaluate 26 diverse MI attack setups and empirically reveal consistently high\nfalse positive rates under the standard evaluation framework. Importantly, we\ndemonstrate that many state-of-the-art (SOTA) MI methods report inflated attack\naccuracy, indicating that actual privacy leakage is significantly lower than\npreviously believed. By uncovering this critical issue and proposing a robust\nsolution, our work enables a reassessment of progress in MI research and sets a\nnew standard for reliable and robust evaluation.",
    "updated" : "2025-05-24T13:42:50Z",
    "published" : "2025-05-06T13:32:12Z",
    "authors" : [
      {
        "name" : "Sy-Tuyen Ho"
      },
      {
        "name" : "Koh Jun Hao"
      },
      {
        "name" : "Ngoc-Bao Nguyen"
      },
      {
        "name" : "Alexander Binder"
      },
      {
        "name" : "Ngai-Man Cheung"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.21008v1",
    "title" : "A Hitchhiker's Guide to Privacy-Preserving Cryptocurrencies: A Survey on\n  Anonymity, Confidentiality, and Auditability",
    "summary" : "Cryptocurrencies and central bank digital currencies (CBDCs) are reshaping\nthe monetary landscape, offering transparency and efficiency while raising\ncritical concerns about user privacy and regulatory compliance. This survey\nprovides a comprehensive and technically grounded overview of\nprivacy-preserving digital currencies, covering both cryptocurrencies and\nCBDCs. We propose a taxonomy of privacy goals -- including anonymity,\nconfidentiality, unlinkability, and auditability -- and map them to underlying\ncryptographic primitives, protocol mechanisms, and system architectures. Unlike\nprevious surveys, our work adopts a design-oriented perspective, linking\nhigh-level privacy objectives to concrete implementations. We also trace the\nevolution of privacy-preserving currencies through three generations,\nhighlighting shifts from basic anonymity guarantees toward more nuanced\nprivacy-accountability trade-offs. Finally, we identify open challenges at the\nintersection of cryptography, distributed systems, and policy definition, which\nmotivate further investigation into the primitives and design of digital\ncurrencies that balance real-world privacy and auditability needs.",
    "updated" : "2025-05-27T10:42:28Z",
    "published" : "2025-05-27T10:42:28Z",
    "authors" : [
      {
        "name" : "Matteo Nardelli"
      },
      {
        "name" : "Francesco De Sclavis"
      },
      {
        "name" : "Michela Iezzi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.20916v1",
    "title" : "Imago Obscura: An Image Privacy AI Co-pilot to Enable Identification and\n  Mitigation of Risks",
    "summary" : "Users often struggle to navigate the privacy / publicity boundary in sharing\nimages online: they may lack awareness of image privacy risks and/or the\nability to apply effective mitigation strategies. To address this challenge, we\nintroduce and evaluate Imago Obscura, an AI-powered, image-editing copilot that\nenables users to identify and mitigate privacy risks with images they intend to\nshare. Driven by design requirements from a formative user study with 7\nimage-editing experts, Imago Obscura enables users to articulate their\nimage-sharing intent and privacy concerns. The system uses these inputs to\nsurface contextually pertinent privacy risks, and then recommends and\nfacilitates application of a suite of obfuscation techniques found to be\neffective in prior literature -- e.g., inpainting, blurring, and generative\ncontent replacement. We evaluated Imago Obscura with 15 end-users in a lab\nstudy and found that it greatly improved users' awareness of image privacy\nrisks and their ability to address those risks, allowing them to make more\ninformed sharing decisions.",
    "updated" : "2025-05-27T09:08:12Z",
    "published" : "2025-05-27T09:08:12Z",
    "authors" : [
      {
        "name" : "Kyzyl Monteiro"
      },
      {
        "name" : "Yuchen Wu"
      },
      {
        "name" : "Sauvik Das"
      }
    ],
    "categories" : [
      "cs.HC",
      "H.5.2; K.4.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.20910v1",
    "title" : "Automated Privacy Information Annotation in Large Language Model\n  Interactions",
    "summary" : "Users interacting with large language models (LLMs) under their real\nidentifiers often unknowingly risk disclosing private information.\nAutomatically notifying users whether their queries leak privacy and which\nphrases leak what private information has therefore become a practical need.\nExisting privacy detection methods, however, were designed for different\nobjectives and application scenarios, typically tagging personally identifiable\ninformation (PII) in anonymous content. In this work, to support the\ndevelopment and evaluation of privacy detection models for LLM interactions\nthat are deployable on local user devices, we construct a large-scale\nmultilingual dataset with 249K user queries and 154K annotated privacy phrases.\nIn particular, we build an automated privacy annotation pipeline with\ncloud-based strong LLMs to automatically extract privacy phrases from dialogue\ndatasets and annotate leaked information. We also design evaluation metrics at\nthe levels of privacy leakage, extracted privacy phrase, and privacy\ninformation. We further establish baseline methods using light-weight LLMs with\nboth tuning-free and tuning-based methods, and report a comprehensive\nevaluation of their performance. Evaluation results reveal a gap between\ncurrent performance and the requirements of real-world LLM applications,\nmotivating future research into more effective local privacy detection methods\ngrounded in our dataset.",
    "updated" : "2025-05-27T09:00:12Z",
    "published" : "2025-05-27T09:00:12Z",
    "authors" : [
      {
        "name" : "Hang Zeng"
      },
      {
        "name" : "Xiangyu Liu"
      },
      {
        "name" : "Yong Hu"
      },
      {
        "name" : "Chaoyue Niu"
      },
      {
        "name" : "Fan Wu"
      },
      {
        "name" : "Shaojie Tang"
      },
      {
        "name" : "Guihai Chen"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.20577v1",
    "title" : "Privacy-Preserving Peer-to-Peer Energy Trading via Hybrid Secure\n  Computations",
    "summary" : "The massive integration of uncertain distributed renewable energy resources\ninto power systems raises power imbalance concerns. Peer-to-peer (P2P) energy\ntrading provides a promising way to balance the prosumers' volatile energy\npower generation and demands locally. Particularly, to protect the privacy of\nprosumers, distributed P2P energy trading is broadly advocated. However, severe\nprivacy leakage issues can emerge in the realistic fully distributed P2P energy\ntrading paradigm. Meanwhile, in this paradigm, two-party and multi-party\ncomputations coexist, challenging the naive privacy-preserving techniques. To\ntackle privacy leakage issues arising from the fully distributed P2P energy\ntrading, this paper proposes a privacy-preserving approach via hybrid secure\ncomputations. A secure multi-party computation mechanism consisting of offline\nand online phases is developed to ensure the security of shared data by\nleveraging the tailored secret sharing method. In addition, the Paillier\nencryption method based on the Chinese Remainder Theorem is proposed for both\nthe secure two-party computation and the offline phase of the multi-party\ncomputation. The random encryption coefficient is designed to enhance the\nsecurity of the two-party computation and simultaneously guarantee the\nconvergence of the distributed optimization. The feasible range for the\nencryption coefficient is derived with a strict mathematical proof. Numerical\nsimulations demonstrate the exactness, effectiveness, and scalability of the\nproposed privacy-preserving approach.",
    "updated" : "2025-05-26T23:24:44Z",
    "published" : "2025-05-26T23:24:44Z",
    "authors" : [
      {
        "name" : "Junhong Liu"
      },
      {
        "name" : "Qinfei Long"
      },
      {
        "name" : "Rong-Peng Liu"
      },
      {
        "name" : "Wenjie Liu"
      },
      {
        "name" : "Xin Cui"
      },
      {
        "name" : "Yunhe Hou"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.20575v1",
    "title" : "Synergising Hierarchical Data Centers and Power Networks: A\n  Privacy-Preserving Approach",
    "summary" : "In the era of digitization, data centers have emerged as integral\ncontributors sustaining our interlinked world, bearing responsibility for an\nincreasing proportion of the world's energy consumption. To facilitate the\ntheir fast rollout while progressing towards net-zero energy systems, the\nsynergy of hierarchical data centers (cloud-fog-edge) and power networks can\nplay a pivotal role. However, existing centralized co-dispatch manners encroach\non the privacy of different agents within the integrated systems, meanwhile\nsuffering from the combinatorial explosion. In this research, we propose a\nnear-optimal distributed privacy-preserving approach to solve the non-convex\nsynergy (day-ahead co-dispatch) problem. The synergy problem is formulated as a\nmixed integer quadratically constrained quadratic programming considering both\ncommunication and energy conservation, where Lyapunov optimization is\nintroduced to balance operating costs and uncertain communication delays. To\nmitigate impacts of the highly non-convex nature, the normalized\nmulti-parametric disaggregation technique is leveraged to reformulate the\nproblem into a mixed integer non-linear programming. To further overcome\nnon-smoothness of the reformulated problem, the customized $\\ell_1-$surrogate\nLagrangian relaxation method with convergence guarantees is proposed to solve\nthe problem in a distributed privacy-preserving manner. {The effectiveness,\noptimality, and scalability of the proposed methodologies for the synergy\nproblem are validated via numerical simulations. Simulation results also\nindicate that computing tasks can be delayed and migrated within the\nhierarchical data centers, demonstrating the flexible resource allocation\ncapabilities of the hierarchical data center architecture, further facilitating\npeak load balancing in the power network.",
    "updated" : "2025-05-26T23:22:45Z",
    "published" : "2025-05-26T23:22:45Z",
    "authors" : [
      {
        "name" : "Junhong Liu"
      },
      {
        "name" : "Fei Teng"
      },
      {
        "name" : "Yunhe Hou"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.20118v2",
    "title" : "TrojanStego: Your Language Model Can Secretly Be A Steganographic\n  Privacy Leaking Agent",
    "summary" : "As large language models (LLMs) become integrated into sensitive workflows,\nconcerns grow over their potential to leak confidential information. We propose\nTrojanStego, a novel threat model in which an adversary fine-tunes an LLM to\nembed sensitive context information into natural-looking outputs via linguistic\nsteganography, without requiring explicit control over inference inputs. We\nintroduce a taxonomy outlining risk factors for compromised LLMs, and use it to\nevaluate the risk profile of the threat. To implement TrojanStego, we propose a\npractical encoding scheme based on vocabulary partitioning learnable by LLMs\nvia fine-tuning. Experimental results show that compromised models reliably\ntransmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over\n97% accuracy using majority voting across three generations. Further, they\nmaintain high utility, can evade human detection, and preserve coherence. These\nresults highlight a new class of LLM data exfiltration attacks that are\npassive, covert, practical, and dangerous.",
    "updated" : "2025-05-27T07:24:52Z",
    "published" : "2025-05-26T15:20:51Z",
    "authors" : [
      {
        "name" : "Dominik Meier"
      },
      {
        "name" : "Jan Philip Wahle"
      },
      {
        "name" : "Paul Röttger"
      },
      {
        "name" : "Terry Ruas"
      },
      {
        "name" : "Bela Gipp"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.22447v1",
    "title" : "Privacy-preserving Prompt Personalization in Federated Learning for\n  Multimodal Large Language Models",
    "summary" : "Prompt learning is a crucial technique for adapting pre-trained multimodal\nlanguage models (MLLMs) to user tasks. Federated prompt personalization (FPP)\nis further developed to address data heterogeneity and local overfitting,\nhowever, it exposes personalized prompts - valuable intellectual assets - to\nprivacy risks like prompt stealing or membership inference attacks.\nWidely-adopted techniques like differential privacy add noise to prompts,\nwhereas degrading personalization performance. We propose SecFPP, a secure FPP\nprotocol harmonizing generalization, personalization, and privacy guarantees.\nSecFPP employs hierarchical prompt adaptation with domain-level and class-level\ncomponents to handle multi-granular data imbalance. For privacy, it uses a\nnovel secret-sharing-based adaptive clustering algorithm for domain-level\nadaptation while keeping class-level components private. While theoretically\nand empirically secure, SecFPP achieves state-of-the-art accuracy under severe\nheterogeneity in data distribution. Extensive experiments show it significantly\noutperforms both non-private and privacy-preserving baselines, offering a\nsuperior privacy-performance trade-off.",
    "updated" : "2025-05-28T15:09:56Z",
    "published" : "2025-05-28T15:09:56Z",
    "authors" : [
      {
        "name" : "Sizai Hou"
      },
      {
        "name" : "Songze Li"
      },
      {
        "name" : "Baturalp Buyukates"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.22234v1",
    "title" : "Evolution of repositories and privacy laws: commit activities in the\n  GDPR and CCPA era",
    "summary" : "Free and open source software has gained a lot of momentum in the industry\nand the research community. The latest advances in privacy legislation,\nincluding the EU General Data Protection Regulation (GDPR) and the California\nConsumer Privacy Act (CCPA), have forced the community to pay special attention\nto users' data privacy. The main aim of this work is to examine software\nrepositories that are acting on privacy laws. We have collected commit data\nfrom GitHub repositories in order to understand indications on main data\nprivacy laws (GDPR, CCPA, CPRA, UK DPA) in the last years. Via an automated\nprocess, we analyzed 37,213 commits from 12,391 repositories since 2016,\nwhereas 594 commits from the 70 most popular repositories of the dataset were\nmanually analyzed. We observe that most commits were performed on the year the\nlaw came into effect and privacy relevant terms appear in the commit messages,\nwhereas reference to specific data privacy user rights is scarce. The study\nshowed that more educational activities on data privacy user rights are needed,\nas well as tools for privacy recommendations, whereas verifying actual\ncompliance via source code execution is a useful direction for software\nengineering researchers.",
    "updated" : "2025-05-28T11:10:58Z",
    "published" : "2025-05-28T11:10:58Z",
    "authors" : [
      {
        "name" : "Georgia M. Kapitsaki"
      },
      {
        "name" : "Maria Papoutsoglou"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.22061v1",
    "title" : "Safeguarding Privacy of Retrieval Data against Membership Inference\n  Attacks: Is This Query Too Close to Home?",
    "summary" : "Retrieval-augmented generation (RAG) mitigates the hallucination problem in\nlarge language models (LLMs) and has proven effective for specific,\npersonalized applications. However, passing private retrieved documents\ndirectly to LLMs introduces vulnerability to membership inference attacks\n(MIAs), which try to determine whether the target datum exists in the private\nexternal database or not. Based on the insight that MIA queries typically\nexhibit high similarity to only one target document, we introduce Mirabel, a\nsimilarity-based MIA detection framework designed for the RAG system. With the\nproposed Mirabel, we show that simple detect-and-hide strategies can\nsuccessfully obfuscate attackers, maintain data utility, and remain\nsystem-agnostic. We experimentally prove its detection and defense against\nvarious state-of-the-art MIA methods and its adaptability to existing private\nRAG systems.",
    "updated" : "2025-05-28T07:35:07Z",
    "published" : "2025-05-28T07:35:07Z",
    "authors" : [
      {
        "name" : "Yujin Choi"
      },
      {
        "name" : "Youngjoo Park"
      },
      {
        "name" : "Junyoung Byun"
      },
      {
        "name" : "Jaewook Lee"
      },
      {
        "name" : "Jinseong Park"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.21801v1",
    "title" : "Query, Don't Train: Privacy-Preserving Tabular Prediction from EHR Data\n  via SQL Queries",
    "summary" : "Electronic health records (EHRs) contain richly structured, longitudinal data\nessential for predictive modeling, yet stringent privacy regulations (e.g.,\nHIPAA, GDPR) often restrict access to individual-level records. We introduce\nQuery, Don't Train (QDT): a structured-data foundation-model interface enabling\ntabular inference via LLM-generated SQL over EHRs. Instead of training on or\naccessing individual-level examples, QDT uses a large language model (LLM) as a\nschema-aware query planner to generate privacy-compliant SQL queries from a\nnatural language task description and a test-time input. The model then\nextracts summary-level population statistics through these SQL queries and the\nLLM performs, chain-of-thought reasoning over the results to make predictions.\nThis inference-time-only approach (1) eliminates the need for supervised model\ntraining or direct data access, (2) ensures interpretability through symbolic,\nauditable queries, (3) naturally handles missing features without imputation or\npreprocessing, and (4) effectively manages high-dimensional numerical data to\nenhance analytical capabilities. We validate QDT on the task of 30-day hospital\nreadmission prediction for Type 2 diabetes patients using a MIMIC-style EHR\ncohort, achieving F1 = 0.70, which outperforms TabPFN (F1 = 0.68). To our\nknowledge, this is the first demonstration of LLM-driven, privacy-preserving\nstructured prediction using only schema metadata and aggregate statistics -\noffering a scalable, interpretable, and regulation-compliant alternative to\nconventional foundation-model pipelines.",
    "updated" : "2025-05-27T22:16:02Z",
    "published" : "2025-05-27T22:16:02Z",
    "authors" : [
      {
        "name" : "Josefa Lia Stoisser"
      },
      {
        "name" : "Marc Boubnovski Martell"
      },
      {
        "name" : "Kaspar Märtens"
      },
      {
        "name" : "Lawrence Phillips"
      },
      {
        "name" : "Stephen Michael Town"
      },
      {
        "name" : "Rory Donovan-Maiye"
      },
      {
        "name" : "Julien Fauqueur"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.21715v1",
    "title" : "Privacy-Preserving Chest X-ray Report Generation via Multimodal\n  Federated Learning with ViT and GPT-2",
    "summary" : "The automated generation of radiology reports from chest X-ray images holds\nsignificant promise in enhancing diagnostic workflows while preserving patient\nprivacy. Traditional centralized approaches often require sensitive data\ntransfer, posing privacy concerns. To address this, the study proposes a\nMultimodal Federated Learning framework for chest X-ray report generation using\nthe IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the\nencoder and GPT-2 as the report generator, enabling decentralized training\nwithout sharing raw data. Three Federated Learning (FL) aggregation strategies:\nFedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)\nwere evaluated. Among these, Krum Aggregation demonstrated superior performance\nacross lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore\nand RaTEScore. The results show that FL can match or surpass centralized models\nin generating clinically relevant and semantically rich radiology reports. This\nlightweight and privacy-preserving framework paves the way for collaborative\nmedical AI development without compromising data confidentiality.",
    "updated" : "2025-05-27T20:01:12Z",
    "published" : "2025-05-27T20:01:12Z",
    "authors" : [
      {
        "name" : "Md. Zahid Hossain"
      },
      {
        "name" : "Mustofa Ahmed"
      },
      {
        "name" : "Most. Sharmin Sultana Samu"
      },
      {
        "name" : "Md. Rakibul Islam"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.07872v3",
    "title" : "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
    "summary" : "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
    "updated" : "2025-05-28T00:43:47Z",
    "published" : "2025-05-09T21:05:20Z",
    "authors" : [
      {
        "name" : "Yijing Zhang"
      },
      {
        "name" : "Ferdous Pervej"
      },
      {
        "name" : "Andreas F. Molisch"
      }
    ],
    "categories" : [
      "cs.NI",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.23031v1",
    "title" : "Towards Privacy-Preserving Fine-Grained Visual Classification via\n  Hierarchical Learning from Label Proportions",
    "summary" : "In recent years, Fine-Grained Visual Classification (FGVC) has achieved\nimpressive recognition accuracy, despite minimal inter-class variations.\nHowever, existing methods heavily rely on instance-level labels, making them\nimpractical in privacy-sensitive scenarios such as medical image analysis. This\npaper aims to enable accurate fine-grained recognition without direct access to\ninstance labels. To achieve this, we leverage the Learning from Label\nProportions (LLP) paradigm, which requires only bag-level labels for efficient\ntraining. Unlike existing LLP-based methods, our framework explicitly exploits\nthe hierarchical nature of fine-grained datasets, enabling progressive feature\ngranularity refinement and improving classification accuracy. We propose\nLearning from Hierarchical Fine-Grained Label Proportions (LHFGLP), a framework\nthat incorporates Unrolled Hierarchical Fine-Grained Sparse Dictionary\nLearning, transforming handcrafted iterative approximation into learnable\nnetwork optimization. Additionally, our proposed Hierarchical Proportion Loss\nprovides hierarchical supervision, further enhancing classification\nperformance. Experiments on three widely-used fine-grained datasets, structured\nin a bag-based manner, demonstrate that our framework consistently outperforms\nexisting LLP-based methods. We will release our code and datasets to foster\nfurther research in privacy-preserving fine-grained classification.",
    "updated" : "2025-05-29T03:18:25Z",
    "published" : "2025-05-29T03:18:25Z",
    "authors" : [
      {
        "name" : "Jinyi Chang"
      },
      {
        "name" : "Dongliang Chang"
      },
      {
        "name" : "Lei Chen"
      },
      {
        "name" : "Bingyao Yu"
      },
      {
        "name" : "Zhanyu Ma"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.21801v2",
    "title" : "Query, Don't Train: Privacy-Preserving Tabular Prediction from EHR Data\n  via SQL Queries",
    "summary" : "Electronic health records (EHRs) contain richly structured, longitudinal data\nessential for predictive modeling, yet stringent privacy regulations (e.g.,\nHIPAA, GDPR) often restrict access to individual-level records. We introduce\nQuery, Don't Train (QDT): a structured-data foundation-model interface enabling\ntabular inference via LLM-generated SQL over EHRs. Instead of training on or\naccessing individual-level examples, QDT uses a large language model (LLM) as a\nschema-aware query planner to generate privacy-compliant SQL queries from a\nnatural language task description and a test-time input. The model then\nextracts summary-level population statistics through these SQL queries and the\nLLM performs, chain-of-thought reasoning over the results to make predictions.\nThis inference-time-only approach (1) eliminates the need for supervised model\ntraining or direct data access, (2) ensures interpretability through symbolic,\nauditable queries, (3) naturally handles missing features without imputation or\npreprocessing, and (4) effectively manages high-dimensional numerical data to\nenhance analytical capabilities. We validate QDT on the task of 30-day hospital\nreadmission prediction for Type 2 diabetes patients using a MIMIC-style EHR\ncohort, achieving F1 = 0.70, which outperforms TabPFN (F1 = 0.68). To our\nknowledge, this is the first demonstration of LLM-driven, privacy-preserving\nstructured prediction using only schema metadata and aggregate statistics -\noffering a scalable, interpretable, and regulation-compliant alternative to\nconventional foundation-model pipelines.",
    "updated" : "2025-05-29T08:17:36Z",
    "published" : "2025-05-27T22:16:02Z",
    "authors" : [
      {
        "name" : "Josefa Lia Stoisser"
      },
      {
        "name" : "Marc Boubnovski Martell"
      },
      {
        "name" : "Kaspar Märtens"
      },
      {
        "name" : "Lawrence Phillips"
      },
      {
        "name" : "Stephen Michael Town"
      },
      {
        "name" : "Rory Donovan-Maiye"
      },
      {
        "name" : "Julien Fauqueur"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.24603v1",
    "title" : "The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian\n  Sketches",
    "summary" : "Gaussian sketching, which consists of pre-multiplying the data with a random\nGaussian matrix, is a widely used technique for multiple problems in data\nscience and machine learning, with applications spanning computationally\nefficient optimization, coded computing, and federated learning. This operation\nalso provides differential privacy guarantees due to its inherent randomness.\nIn this work, we revisit this operation through the lens of Renyi Differential\nPrivacy (RDP), providing a refined privacy analysis that yields significantly\ntighter bounds than prior results. We then demonstrate how this improved\nanalysis leads to performance improvement in different linear regression\nsettings, establishing theoretical utility guarantees. Empirically, our methods\nimprove performance across multiple datasets and, in several cases, reduce\nruntime.",
    "updated" : "2025-05-30T13:52:48Z",
    "published" : "2025-05-30T13:52:48Z",
    "authors" : [
      {
        "name" : "Omri Lev"
      },
      {
        "name" : "Vishwak Srinivasan"
      },
      {
        "name" : "Moshe Shenfeld"
      },
      {
        "name" : "Katrina Ligett"
      },
      {
        "name" : "Ayush Sekhari"
      },
      {
        "name" : "Ashia C. Wilson"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.23655v2",
    "title" : "Keyed Chaotic Masking: A Functional Privacy Framework for Neural\n  Inference",
    "summary" : "This work introduces a lightweight framework for privacy-preserving neural\nnetwork inference based on keyed chaotic masking a deterministic, user-specific\nobfuscation method derived from cryptographically seeded chaotic dynamical\nsystems. The approach applies masks to input and output tensors using\nkey-conditioned graph dynamics, enabling authenticated inference, user\nattribution, and soft output watermarking without modifying model\narchitectures. While the underlying chaotic system used to generate each mask\nis not analytically invertible, the masking operation itself is algebraically\nreversible by authorized key holders, offering functional privacy without\nformal cryptographic guarantees. Unlike traditional encryption or secure\nmulti-party computation, this method operates in continuous space and imposes\nminimal computational overhead. We describe the construction of the masking\nsystem, including graph sampling, dynamical rule selection, and chaos\ndiagnostics. Applications include privacy-preserving inference, secure data\ncontribution, and per-user watermarking in shared model pipelines. This\nframework offers a practical and modular building block for user-controlled\nprivacy in modern AI systems.",
    "updated" : "2025-05-30T10:56:04Z",
    "published" : "2025-05-29T17:05:42Z",
    "authors" : [
      {
        "name" : "Peter David Fagan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "94A60, 37N25, 68T05",
      "D.4.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.23849v1",
    "title" : "CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving\n  Federated Learning",
    "summary" : "Privacy-Preserving Federated Learning (PPFL) is a decentralized machine\nlearning approach where multiple clients train a model collaboratively. PPFL\npreserves privacy and security of the client's data by not exchanging it.\nHowever, ensuring that data at each client is of high quality and ready for\nfederated learning (FL) is a challenge due to restricted data access. In this\npaper, we introduce CADRE (Customizable Assurance of Data REadiness) for FL, a\nnovel framework that allows users to define custom data readiness (DR)\nstandards, metrics, rules, and remedies tailored to specific FL tasks. Our\nframework generates comprehensive DR reports based on the user-defined metrics,\nrules, and remedies to ensure datasets are optimally prepared for FL while\npreserving privacy. We demonstrate the framework's practical application by\nintegrating it into an existing PPFL framework. We conducted experiments across\nsix diverse datasets, addressing seven different DR issues. The results\nillustrate the framework's versatility and effectiveness in ensuring DR across\nvarious dimensions, including data quality, privacy, and fairness. This\napproach enhances the performance and reliability of FL models as well as\nutilizes valuable resources by identifying and addressing data-related issues\nbefore the training phase.",
    "updated" : "2025-05-28T21:24:46Z",
    "published" : "2025-05-28T21:24:46Z",
    "authors" : [
      {
        "name" : "Kaveen Hiniduma"
      },
      {
        "name" : "Zilinghan Li"
      },
      {
        "name" : "Aditya Sinha"
      },
      {
        "name" : "Ravi Madduri"
      },
      {
        "name" : "Suren Byna"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.23825v1",
    "title" : "Privacy-Preserving Inconsistency Measurement",
    "summary" : "We investigate a new form of (privacy-preserving) inconsistency measurement\nfor multi-party communication. Intuitively, for two knowledge bases K_A, K_B\n(of two agents A, B), our results allow to quantitatively assess the degree of\ninconsistency for K_A U K_B without having to reveal the actual contents of the\nknowledge bases. Using secure multi-party computation (SMPC) and cryptographic\nprotocols, we develop two concrete methods for this use-case and show that they\nsatisfy important properties of SMPC protocols -- notably, input privacy, i.e.,\njointly computing the inconsistency degree without revealing the inputs.",
    "updated" : "2025-05-28T06:24:33Z",
    "published" : "2025-05-28T06:24:33Z",
    "authors" : [
      {
        "name" : "Carl Corea"
      },
      {
        "name" : "Timotheus Kampik"
      },
      {
        "name" : "Nico Potyka"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00745v1",
    "title" : "Controlling the Spread of Epidemics on Networks with Differential\n  Privacy",
    "summary" : "Designing effective strategies for controlling epidemic spread by vaccination\nis an important question in epidemiology, especially in the early stages when\nvaccines are limited. This is a challenging question when the contact network\nis very heterogeneous, and strategies based on controlling network properties,\nsuch as the degree and spectral radius, have been shown to be effective.\nImplementation of such strategies requires detailed information on the contact\nstructure, which might be sensitive in many applications. Our focus here is on\nchoosing effective vaccination strategies when the edges are sensitive and\ndifferential privacy guarantees are needed. Our main contributions are\n$(\\varepsilon,\\delta)$-differentially private algorithms for designing\nvaccination strategies by reducing the maximum degree and spectral radius. Our\nkey technique is a private algorithm for the multi-set multi-cover problem,\nwhich we use for controlling network properties. We evaluate privacy-utility\ntradeoffs of our algorithms on multiple synthetic and real-world networks, and\nshow their effectiveness.",
    "updated" : "2025-05-31T23:17:38Z",
    "published" : "2025-05-31T23:17:38Z",
    "authors" : [
      {
        "name" : "Dung Nguyen"
      },
      {
        "name" : "Aravind Srinivasan"
      },
      {
        "name" : "Renata Valieva"
      },
      {
        "name" : "Anil Vullikanti"
      },
      {
        "name" : "Jiayi Wu"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CE",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00660v1",
    "title" : "Differential Privacy for Deep Learning in Medicine",
    "summary" : "Differential privacy (DP) is a key technique for protecting sensitive patient\ndata in medical deep learning (DL). As clinical models grow more\ndata-dependent, balancing privacy with utility and fairness has become a\ncritical challenge. This scoping review synthesizes recent developments in\napplying DP to medical DL, with a particular focus on DP-SGD and alternative\nmechanisms across centralized and federated settings. Using a structured search\nstrategy, we identified 74 studies published up to March 2025. Our analysis\nspans diverse data modalities, training setups, and downstream tasks, and\nhighlights the tradeoffs between privacy guarantees, model accuracy, and\nsubgroup fairness. We find that while DP-especially at strong privacy\nbudgets-can preserve performance in well-structured imaging tasks, severe\ndegradation often occurs under strict privacy, particularly in underrepresented\nor complex modalities. Furthermore, privacy-induced performance gaps\ndisproportionately affect demographic subgroups, with fairness impacts varying\nby data type and task. A small subset of studies explicitly addresses these\ntradeoffs through subgroup analysis or fairness metrics, but most omit them\nentirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms,\ngenerative models, and hybrid federated designs, though reporting remains\ninconsistent. We conclude by outlining key gaps in fairness auditing,\nstandardization, and evaluation protocols, offering guidance for future work\ntoward equitable and clinically robust privacy-preserving DL systems in\nmedicine.",
    "updated" : "2025-05-31T18:03:15Z",
    "published" : "2025-05-31T18:03:15Z",
    "authors" : [
      {
        "name" : "Marziyeh Mohammadi"
      },
      {
        "name" : "Mohsen Vejdanihemmat"
      },
      {
        "name" : "Mahshad Lotfinia"
      },
      {
        "name" : "Mirabela Rusu"
      },
      {
        "name" : "Daniel Truhn"
      },
      {
        "name" : "Andreas Maier"
      },
      {
        "name" : "Soroosh Tayebi Arasteh"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00476v1",
    "title" : "Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet -- A\n  ResNet-based Model Classification Dataset",
    "summary" : "Federated Learning (FL) has emerged as a powerful paradigm for training\nmachine learning models across distributed data sources while preserving data\nlocality. However, the privacy of local data is always a pivotal concern and\nhas received a lot of attention in recent research on the FL regime. Moreover,\nthe lack of domain heterogeneity and client-specific segregation in the\nbenchmarks remains a critical bottleneck for rigorous evaluation. In this\npaper, we introduce ModelNet, a novel image classification dataset constructed\nfrom the embeddings extracted from a pre-trained ResNet50 model. First, we\nmodify the CIFAR100 dataset into three client-specific variants, considering\nthree domain heterogeneities (homogeneous, heterogeneous, and random).\nSubsequently, we train each client-specific subset of all three variants on the\npre-trained ResNet50 model to save model parameters. In addition to\nmulti-domain image data, we propose a new hypothesis to define the FL algorithm\nthat can access the anonymized model parameters to preserve the local privacy\nin a more effective manner compared to existing ones. ModelNet is designed to\nsimulate realistic FL settings by incorporating non-IID data distributions and\nclient diversity design principles in the mainframe for both conventional and\nfuturistic graph-driven FL algorithms. The three variants are ModelNet-S,\nModelNet-D, and ModelNet-R, which are based on homogeneous, heterogeneous, and\nrandom data settings, respectively. To the best of our knowledge, we are the\nfirst to propose a cross-environment client-specific FL dataset along with the\ngraph-based variant. Extensive experiments based on domain shifts and\naggregation strategies show the effectiveness of the above variants, making it\na practical benchmark for classical and graph-based FL research. The dataset\nand related code are available online.",
    "updated" : "2025-05-31T08:53:16Z",
    "published" : "2025-05-31T08:53:16Z",
    "authors" : [
      {
        "name" : "Abhisek Ray"
      },
      {
        "name" : "Lukas Esterle"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02038v1",
    "title" : "Blockchain Powered Edge Intelligence for U-Healthcare in Privacy\n  Critical and Time Sensitive Environment",
    "summary" : "Edge Intelligence (EI) serves as a critical enabler for privacy-preserving\nsystems by providing AI-empowered computation and distributed caching services\nat the edge, thereby minimizing latency and enhancing data privacy. The\nintegration of blockchain technology further augments EI frameworks by ensuring\ntransactional transparency, auditability, and system-wide reliability through a\ndecentralized network model. However, the operational architecture of such\nsystems introduces inherent vulnerabilities, particularly due to the extensive\ndata interactions between edge gateways (EGs) and the distributed nature of\ninformation storage during service provisioning. To address these challenges,\nwe propose an autonomous computing model along with its interaction topologies\ntailored for privacy-critical and time-sensitive health applications. The\nsystem supports continuous monitoring, real-time alert notifications, disease\ndetection, and robust data processing and aggregation. It also includes a data\ntransaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a\nresource-efficient one-dimensional convolutional neural network (1D-CNN) is\nproposed for the multiclass classification of arrhythmia, enabling accurate and\nreal-time analysis of constrained EGs. Furthermore, a secure access scheme is\ndefined to manage both off-chain and on-chain data sharing and storage. To\nvalidate the proposed model, comprehensive security, performance, and cost\nanalyses are conducted, demonstrating the efficiency and reliability of the\nfine-grained access control scheme.",
    "updated" : "2025-05-31T06:58:52Z",
    "published" : "2025-05-31T06:58:52Z",
    "authors" : [
      {
        "name" : "Anum Nawaz"
      },
      {
        "name" : "Hafiz Humza Mahmood Ramzan"
      },
      {
        "name" : "Xianjia Yu"
      },
      {
        "name" : "Zhuo Zou"
      },
      {
        "name" : "Tomi Westerlund"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00416v1",
    "title" : "Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge\n  Learning in Personalized Healthcare",
    "summary" : "Federated learning (FL) has attracted increasing attention to mitigate\nsecurity and privacy challenges in traditional cloud-centric machine learning\nmodels specifically in healthcare ecosystems. FL methodologies enable the\ntraining of global models through localized policies, allowing independent\noperations at the edge clients' level. Conventional first-order FL approaches\nface several challenges in personalized model training due to heterogeneous\nnon-independent and identically distributed (non-iid) data of each edge client.\nRecently, second-order FL approaches maintain the stability and consistency of\nnon-iid datasets while improving personalized model training. This study\nproposes and develops a verifiable and auditable optimized second-order FL\nframework BFEL (blockchain-enhanced federated edge learning) based on optimized\nFedCurv for personalized healthcare systems. FedCurv incorporates information\nabout the importance of each parameter to each client's task (through Fisher\nInformation Matrix) which helps to preserve client-specific knowledge and\nreduce model drift during aggregation. Moreover, it minimizes communication\nrounds required to achieve a target precision convergence for each edge client\nwhile effectively managing personalized training on non-iid and heterogeneous\ndata. The incorporation of Ethereum-based model aggregation ensures trust,\nverifiability, and auditability while public key encryption enhances privacy\nand security. Experimental results of federated CNNs and MLPs utilizing Mnist,\nCifar-10, and PathMnist demonstrate the high efficiency and scalability of the\nproposed framework.",
    "updated" : "2025-05-31T06:41:04Z",
    "published" : "2025-05-31T06:41:04Z",
    "authors" : [
      {
        "name" : "Anum Nawaz"
      },
      {
        "name" : "Muhammad Irfan"
      },
      {
        "name" : "Xianjia Yu"
      },
      {
        "name" : "Zhuo Zou"
      },
      {
        "name" : "Tomi Westerlund"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00158v1",
    "title" : "Privacy Amplification in Differentially Private Zeroth-Order\n  Optimization with Hidden States",
    "summary" : "Zeroth-order optimization has emerged as a promising approach for fine-tuning\nlarge language models on domain-specific data, particularly under differential\nprivacy (DP) and memory constraints. While first-order methods have been\nextensively studied from a privacy perspective, the privacy analysis and\nalgorithmic design for zeroth-order methods remain significantly underexplored.\nA critical open question concerns hidden-state DP analysis: although convergent\nprivacy bounds are known for first-order methods, it has remained unclear\nwhether similar guarantees can be established for zeroth-order methods. In this\nwork, we provide an affirmative answer by proving a convergent DP bound for\nzeroth-order optimization. Our analysis generalizes the celebrated privacy\namplification-by-iteration framework to the setting of smooth loss functions in\nzeroth-order optimization. Furthermore, it induces better DP zeroth-order\nalgorithmic designs that are previously unknown to the literature.",
    "updated" : "2025-05-30T18:55:32Z",
    "published" : "2025-05-30T18:55:32Z",
    "authors" : [
      {
        "name" : "Eli Chien"
      },
      {
        "name" : "Wei-Ning Chen"
      },
      {
        "name" : "Pan Li"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00124v1",
    "title" : "randextract: a Reference Library to Test and Validate Privacy\n  Amplification Implementations",
    "summary" : "Quantum cryptographic protocols do not rely only on quantum-physical\nresources, they also require reliable classical communication and computation.\nIn particular, the secrecy of any quantum key distribution protocol critically\ndepends on the correct execution of the privacy amplification step. This is a\nclassical post-processing procedure transforming a partially secret bit string,\nknown to be somewhat correlated with an adversary, into a shorter bit string\nthat is close to uniform and independent of the adversary's knowledge. It is\ntypically implemented using randomness extractors. Standardization efforts in\nquantum cryptography have focused on the security of physical devices and\nquantum operations. Future efforts should also consider all algorithms used in\nclassical post-processing, especially in privacy amplification, due to its\ncritical role in ensuring the final security of the key. We present\nrandextract, a reference library to test and validate privacy amplification\nimplementations.",
    "updated" : "2025-05-30T18:01:50Z",
    "published" : "2025-05-30T18:01:50Z",
    "authors" : [
      {
        "name" : "Iyán Méndez Veiga"
      },
      {
        "name" : "Esther Hänggi"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00100v1",
    "title" : "Children's Voice Privacy: First Steps And Emerging Challenges",
    "summary" : "Children are one of the most under-represented groups in speech technologies,\nas well as one of the most vulnerable in terms of privacy. Despite this,\nanonymization techniques targeting this population have received little\nattention. In this study, we seek to bridge this gap, and establish a baseline\nfor the use of voice anonymization techniques designed for adult speech when\napplied to children's voices. Such an evaluation is essential, as children's\nspeech presents a distinct set of challenges when compared to that of adults.\nThis study comprises three children's datasets, six anonymization methods, and\nobjective and subjective utility metrics for evaluation. Our results show that\nexisting systems for adults are still able to protect children's voice privacy,\nbut suffer from much higher utility degradation. In addition, our subjective\nstudy displays the challenges of automatic evaluation methods for speech\nquality in children's speech, highlighting the need for further research.",
    "updated" : "2025-05-30T13:21:18Z",
    "published" : "2025-05-30T13:21:18Z",
    "authors" : [
      {
        "name" : "Ajinkya Kulkarni"
      },
      {
        "name" : "Francisco Teixeira"
      },
      {
        "name" : "Enno Hermann"
      },
      {
        "name" : "Thomas Rolland"
      },
      {
        "name" : "Isabel Trancoso"
      },
      {
        "name" : "Mathew Magimai Doss"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.02030v1",
    "title" : "Adaptive Privacy-Preserving SSD",
    "summary" : "Data remanence in NAND flash complicates complete deletion on IoT SSDs. We\ndesign an adaptive architecture offering four privacy levels (PL0-PL3) that\nselect among address, data, and parity deletion techniques. Quantitative\nanalysis balances efficacy, latency, endurance, and cost. Machine-learning\nadjusts levels contextually, boosting privacy with negligible performance\noverhead and complexity.",
    "updated" : "2025-05-30T13:08:42Z",
    "published" : "2025-05-30T13:08:42Z",
    "authors" : [
      {
        "name" : "Na Young Ahn"
      },
      {
        "name" : "Dong Hoon Lee"
      }
    ],
    "categories" : [
      "cs.CR",
      "H.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.23655v3",
    "title" : "Keyed Chaotic Dynamics for Privacy-Preserving Neural Inference",
    "summary" : "Neural network inference typically operates on raw input data, increasing the\nrisk of exposure during preprocessing and inference. Moreover, neural\narchitectures lack efficient built-in mechanisms for directly authenticating\ninput data. This work introduces a novel encryption method for ensuring the\nsecurity of neural inference. By constructing key-conditioned chaotic graph\ndynamical systems, we enable the encryption and decryption of real-valued\ntensors within the neural architecture. The proposed dynamical systems are\nparticularly suited to encryption due to their sensitivity to initial\nconditions and their capacity to produce complex, key-dependent nonlinear\ntransformations from compact rules. This work establishes a paradigm for\nsecuring neural inference and opens new avenues for research on the application\nof graph dynamical systems in neural network security.",
    "updated" : "2025-06-03T16:59:29Z",
    "published" : "2025-05-29T17:05:42Z",
    "authors" : [
      {
        "name" : "Peter David Fagan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "94A60, 37N25, 68T05",
      "D.4.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00060v1",
    "title" : "Comparative analysis of privacy-preserving open-source LLMs regarding\n  extraction of diagnostic information from clinical CMR imaging reports",
    "summary" : "Purpose: We investigated the utilization of privacy-preserving,\nlocally-deployed, open-source Large Language Models (LLMs) to extract\ndiagnostic information from free-text cardiovascular magnetic resonance (CMR)\nreports. Materials and Methods: We evaluated nine open-source LLMs on their\nability to identify diagnoses and classify patients into various cardiac\ndiagnostic categories based on descriptive findings in 109 clinical CMR\nreports. Performance was quantified using standard classification metrics\nincluding accuracy, precision, recall, and F1 score. We also employed confusion\nmatrices to examine patterns of misclassification across models. Results: Most\nopen-source LLMs demonstrated exceptional performance in classifying reports\ninto different diagnostic categories. Google's Gemma2 model achieved the\nhighest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B\nwith F1 scores of 0.96 and 0.95, respectively. All other evaluated models\nattained average scores above 0.93, with Mistral and DeepseekR1-7B being the\nonly exceptions. The top four LLMs outperformed our board-certified\ncardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR\nreports. Conclusion: Our findings demonstrate the feasibility of implementing\nopen-source, privacy-preserving LLMs in clinical settings for automated\nanalysis of imaging reports, enabling accurate, fast and resource-efficient\ndiagnostic categorization.",
    "updated" : "2025-05-29T11:25:10Z",
    "published" : "2025-05-29T11:25:10Z",
    "authors" : [
      {
        "name" : "Sina Amirrajab"
      },
      {
        "name" : "Volker Vehof"
      },
      {
        "name" : "Michael Bietenbeck"
      },
      {
        "name" : "Ali Yilmaz"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2505.24603v2",
    "title" : "The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian\n  Sketches",
    "summary" : "Gaussian sketching, which consists of pre-multiplying the data with a random\nGaussian matrix, is a widely used technique for multiple problems in data\nscience and machine learning, with applications spanning computationally\nefficient optimization, coded computing, and federated learning. This operation\nalso provides differential privacy guarantees due to its inherent randomness.\nIn this work, we revisit this operation through the lens of Renyi Differential\nPrivacy (RDP), providing a refined privacy analysis that yields significantly\ntighter bounds than prior results. We then demonstrate how this improved\nanalysis leads to performance improvement in different linear regression\nsettings, establishing theoretical utility guarantees. Empirically, our methods\nimprove performance across multiple datasets and, in several cases, reduce\nruntime.",
    "updated" : "2025-06-04T16:02:22Z",
    "published" : "2025-05-30T13:52:48Z",
    "authors" : [
      {
        "name" : "Omri Lev"
      },
      {
        "name" : "Vishwak Srinivasan"
      },
      {
        "name" : "Moshe Shenfeld"
      },
      {
        "name" : "Katrina Ligett"
      },
      {
        "name" : "Ayush Sekhari"
      },
      {
        "name" : "Ashia C. Wilson"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.00100v2",
    "title" : "Children's Voice Privacy: First Steps And Emerging Challenges",
    "summary" : "Children are one of the most under-represented groups in speech technologies,\nas well as one of the most vulnerable in terms of privacy. Despite this,\nanonymization techniques targeting this population have received little\nattention. In this study, we seek to bridge this gap, and establish a baseline\nfor the use of voice anonymization techniques designed for adult speech when\napplied to children's voices. Such an evaluation is essential, as children's\nspeech presents a distinct set of challenges when compared to that of adults.\nThis study comprises three children's datasets, six anonymization methods, and\nobjective and subjective utility metrics for evaluation. Our results show that\nexisting systems for adults are still able to protect children's voice privacy,\nbut suffer from much higher utility degradation. In addition, our subjective\nstudy displays the challenges of automatic evaluation methods for speech\nquality in children's speech, highlighting the need for further research.",
    "updated" : "2025-06-04T13:30:41Z",
    "published" : "2025-05-30T13:21:18Z",
    "authors" : [
      {
        "name" : "Ajinkya Kulkarni"
      },
      {
        "name" : "Francisco Teixeira"
      },
      {
        "name" : "Enno Hermann"
      },
      {
        "name" : "Thomas Rolland"
      },
      {
        "name" : "Isabel Trancoso"
      },
      {
        "name" : "Mathew Magimai Doss"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2506.06321v1",
    "title" : "On the Interplay of Privacy, Persuasion and Quantization",
    "summary" : "We develop a communication-theoretic framework for privacy-aware and\nresilient decision making in cyber-physical systems under misaligned objectives\nbetween the encoder and the decoder. The encoder observes two correlated\nsignals ($X$,$\\theta$) and transmits a finite-rate message $Z$ to aid a\nlegitimate controller (the decoder) in estimating $X+\\theta$, while an\neavesdropper intercepts $Z$ to infer the private parameter $\\theta$. Unlike\nconventional setups where encoder and decoder share a common MSE objective,\nhere the encoder minimizes a Lagrangian that balances legitimate control\nfidelity and the privacy leakage about $\\theta$. In contrast, the decoder's\ngoal is purely to minimize its own estimation error without regard for privacy.\nWe analyze fully, partially, and non-revealing strategies that arise from this\nconflict, and characterize optimal linear encoders when the rate constraints\nare lifted. For finite-rate channels, we employ gradient-based methods to\ncompute the optimal controllers. Numerical experiments illustrate how tuning\nthe privacy parameter shapes the trade-off between control performance and\nresilience against unauthorized inferences.",
    "updated" : "2025-05-28T19:44:31Z",
    "published" : "2025-05-28T19:44:31Z",
    "authors" : [
      {
        "name" : "Anju Anand"
      },
      {
        "name" : "Emrah Akyol"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.GT",
      "cs.IT",
      "cs.SY",
      "eess.SY",
      "math.IT"
    ]
  }
]