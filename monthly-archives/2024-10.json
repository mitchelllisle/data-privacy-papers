[
  {
    "id" : "http://arxiv.org/abs/2410.01068v1",
    "title" : "Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness",
    "summary" : "We study the Differential Privacy (DP) guarantee of hidden-state Noisy-SGD\nalgorithms over a bounded domain. Standard privacy analysis for Noisy-SGD\nassumes all internal states are revealed, which leads to a divergent R'enyi DP\nbound with respect to the number of iterations. Ye & Shokri (2022) and\nAltschuler & Talwar (2022) proved convergent bounds for smooth (strongly)\nconvex losses, and raise open questions about whether these assumptions can be\nrelaxed. We provide positive answers by proving convergent R'enyi DP bound for\nnon-convex non-smooth losses, where we show that requiring losses to have\nH\\\"older continuous gradient is sufficient. We also provide a strictly better\nprivacy bound compared to state-of-the-art results for smooth strongly convex\nlosses. Our analysis relies on the improvement of shifted divergence analysis\nin multiple aspects, including forward Wasserstein distance tracking,\nidentifying the optimal shifts allocation, and the H\"older reduction lemma. Our\nresults further elucidate the benefit of hidden-state analysis for DP and its\napplicability.",
    "updated" : "2024-10-01T20:52:08Z",
    "published" : "2024-10-01T20:52:08Z",
    "authors" : [
      {
        "name" : "Eli Chien"
      },
      {
        "name" : "Pan Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00897v1",
    "title" : "The Gradient of Health Data Privacy",
    "summary" : "In the era of digital health and artificial intelligence, the management of\npatient data privacy has become increasingly complex, with significant\nimplications for global health equity and patient trust. This paper introduces\na novel \"privacy gradient\" approach to health data governance, offering a more\nnuanced and adaptive framework than traditional binary privacy models. Our\nmultidimensional concept considers factors such as data sensitivity,\nstakeholder relationships, purpose of use, and temporal aspects, allowing for\ncontext-sensitive privacy protections. Through policy analyses, ethical\nconsiderations, and case studies spanning adolescent health, integrated care,\nand genomic research, we demonstrate how this approach can address critical\nprivacy challenges in diverse healthcare settings worldwide. The privacy\ngradient model has the potential to enhance patient engagement, improve care\ncoordination, and accelerate medical research while safeguarding individual\nprivacy rights. We provide policy recommendations for implementing this\napproach, considering its impact on healthcare systems, research\ninfrastructures, and global health initiatives. This work aims to inform\npolicymakers, healthcare leaders, and digital health innovators, contributing\nto a more equitable, trustworthy, and effective global health data ecosystem in\nthe digital age.",
    "updated" : "2024-10-01T17:35:18Z",
    "published" : "2024-10-01T17:35:18Z",
    "authors" : [
      {
        "name" : "Baihan Lin"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "q-bio.OT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00836v1",
    "title" : "Towards Fairness and Privacy: A Novel Data Pre-processing Optimization\n  Framework for Non-binary Protected Attributes",
    "summary" : "The reason behind the unfair outcomes of AI is often rooted in biased\ndatasets. Therefore, this work presents a framework for addressing fairness by\ndebiasing datasets containing a (non-)binary protected attribute. The framework\nproposes a combinatorial optimization problem where heuristics such as genetic\nalgorithms can be used to solve for the stated fairness objectives. The\nframework addresses this by finding a data subset that minimizes a certain\ndiscrimination measure. Depending on a user-defined setting, the framework\nenables different use cases, such as data removal, the addition of synthetic\ndata, or exclusive use of synthetic data. The exclusive use of synthetic data\nin particular enhances the framework's ability to preserve privacy while\noptimizing for fairness. In a comprehensive evaluation, we demonstrate that\nunder our framework, genetic algorithms can effectively yield fairer datasets\ncompared to the original data. In contrast to prior work, the framework\nexhibits a high degree of flexibility as it is metric- and task-agnostic, can\nbe applied to both binary or non-binary protected attributes, and demonstrates\nefficient runtime.",
    "updated" : "2024-10-01T16:17:43Z",
    "published" : "2024-10-01T16:17:43Z",
    "authors" : [
      {
        "name" : "Manh Khoi Duong"
      },
      {
        "name" : "Stefan Conrad"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00751v1",
    "title" : "Thinking Outside of the Differential Privacy Box: A Case Study in Text\n  Privatization with Language Model Prompting",
    "summary" : "The field of privacy-preserving Natural Language Processing has risen in\npopularity, particularly at a time when concerns about privacy grow with the\nproliferation of Large Language Models. One solution consistently appearing in\nrecent literature has been the integration of Differential Privacy (DP) into\nNLP techniques. In this paper, we take these approaches into critical view,\ndiscussing the restrictions that DP integration imposes, as well as bring to\nlight the challenges that such restrictions entail. To accomplish this, we\nfocus on $\\textbf{DP-Prompt}$, a recent method for text privatization\nleveraging language models to rewrite texts. In particular, we explore this\nrewriting task in multiple scenarios, both with DP and without DP. To drive the\ndiscussion on the merits of DP in NLP, we conduct empirical utility and privacy\nexperiments. Our results demonstrate the need for more discussion on the\nusability of DP in NLP and its benefits over non-DP approaches.",
    "updated" : "2024-10-01T14:46:15Z",
    "published" : "2024-10-01T14:46:15Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00542v1",
    "title" : "Differentially Private Active Learning: Balancing Effective Data\n  Selection and Privacy",
    "summary" : "Active learning (AL) is a widely used technique for optimizing data labeling\nin machine learning by iteratively selecting, labeling, and training on the\nmost informative data. However, its integration with formal privacy-preserving\nmethods, particularly differential privacy (DP), remains largely underexplored.\nWhile some works have explored differentially private AL for specialized\nscenarios like online learning, the fundamental challenge of combining AL with\nDP in standard learning settings has remained unaddressed, severely limiting\nAL's applicability in privacy-sensitive domains. This work addresses this gap\nby introducing differentially private active learning (DP-AL) for standard\nlearning settings. We demonstrate that naively integrating DP-SGD training into\nAL presents substantial challenges in privacy budget allocation and data\nutilization. To overcome these challenges, we propose step amplification, which\nleverages individual sampling probabilities in batch creation to maximize data\npoint participation in training steps, thus optimizing data utilization.\nAdditionally, we investigate the effectiveness of various acquisition functions\nfor data selection under privacy constraints, revealing that many commonly used\nfunctions become impractical. Our experiments on vision and natural language\nprocessing tasks show that DP-AL can improve performance for specific datasets\nand model architectures. However, our findings also highlight the limitations\nof AL in privacy-constrained environments, emphasizing the trade-offs between\nprivacy, model accuracy, and data selection accuracy.",
    "updated" : "2024-10-01T09:34:06Z",
    "published" : "2024-10-01T09:34:06Z",
    "authors" : [
      {
        "name" : "Kristian Schwethelm"
      },
      {
        "name" : "Johannes Kaiser"
      },
      {
        "name" : "Jonas Kuntzer"
      },
      {
        "name" : "Mehmet Yigitsoy"
      },
      {
        "name" : "Daniel Rueckert"
      },
      {
        "name" : "Georgios Kaissis"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00433v1",
    "title" : "PrivTuner with Homomorphic Encryption and LoRA: A P3EFT Scheme for\n  Privacy-Preserving Parameter-Efficient Fine-Tuning of AI Foundation Models",
    "summary" : "AI foundation models have recently demonstrated impressive capabilities\nacross a wide range of tasks. Fine-tuning (FT) is a method of customizing a\npre-trained AI foundation model by further training it on a smaller, targeted\ndataset. In this paper, we initiate the study of the Privacy-Preserving\nParameter-Efficient FT (P3EFT) framework, which can be viewed as the\nintersection of Parameter-Efficient FT (PEFT) and Privacy-Preserving FT (PPFT).\nPEFT modifies only a small subset of the model's parameters to achieve FT\n(i.e., adapting a pre-trained model to a specific dataset), while PPFT uses\nprivacy-preserving technologies to protect the confidentiality of the model\nduring the FT process. There have been many studies on PEFT or PPFT but very\nfew on their fusion, which motivates our work on P3EFT to achieve both\nparameter efficiency and model privacy. To exemplify our P3EFT, we present the\nPrivTuner scheme, which incorporates Fully Homomorphic Encryption (FHE) enabled\nprivacy protection into LoRA (short for ``Low-Rank Adapter''). Intuitively\nspeaking, PrivTuner allows the model owner and the external data owners to\ncollaboratively implement PEFT with encrypted data. After describing PrivTuner\nin detail, we further investigate its energy consumption and privacy\nprotection. Then, we consider a PrivTuner system over wireless communications\nand formulate a joint optimization problem to adaptively minimize energy while\nmaximizing privacy protection, with the optimization variables including FDMA\nbandwidth allocation, wireless transmission power, computational resource\nallocation, and privacy protection. A resource allocation algorithm is devised\nto solve the problem. Experiments demonstrate that our algorithm can\nsignificantly reduce energy consumption while adapting to different privacy\nrequirements.",
    "updated" : "2024-10-01T06:30:06Z",
    "published" : "2024-10-01T06:30:06Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Wenhan Yu"
      },
      {
        "name" : "Jun Zhao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02547v1",
    "title" : "Personalized Quantum Federated Learning for Privacy Image Classification",
    "summary" : "Quantum federated learning has brought about the improvement of privacy image\nclassification, while the lack of personality of the client model may\ncontribute to the suboptimal of quantum federated learning. A personalized\nquantum federated learning algorithm for privacy image classification is\nproposed to enhance the personality of the client model in the case of an\nimbalanced distribution of images. First, a personalized quantum federated\nlearning model is constructed, in which a personalized layer is set for the\nclient model to maintain the personalized parameters. Second, a personalized\nquantum federated learning algorithm is introduced to secure the information\nexchanged between the client and server.Third, the personalized federated\nlearning is applied to image classification on the FashionMNIST dataset, and\nthe experimental results indicate that the personalized quantum federated\nlearning algorithm can obtain global and local models with excellent\nperformance, even in situations where local training samples are imbalanced.\nThe server's accuracy is 100% with 8 clients and a distribution parameter of\n100, outperforming the non-personalized model by 7%. The average client\naccuracy is 2.9% higher than that of the non-personalized model with 2 clients\nand a distribution parameter of 1. Compared to previous quantum federated\nlearning algorithms, the proposed personalized quantum federated learning\nalgorithm eliminates the need for additional local training while safeguarding\nboth model and data privacy.It may facilitate broader adoption and application\nof quantum technologies, and pave the way for more secure, scalable, and\nefficient quantum distribute machine learning solutions.",
    "updated" : "2024-10-03T14:53:04Z",
    "published" : "2024-10-03T14:53:04Z",
    "authors" : [
      {
        "name" : "Jinjing Shi"
      },
      {
        "name" : "Tian Chen"
      },
      {
        "name" : "Shichao Zhang"
      },
      {
        "name" : "Xuelong Li"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02371v1",
    "title" : "NTU-NPU System for Voice Privacy 2024 Challenge",
    "summary" : "In this work, we describe our submissions for the Voice Privacy Challenge\n2024. Rather than proposing a novel speech anonymization system, we enhance the\nprovided baselines to meet all required conditions and improve evaluated\nmetrics. Specifically, we implement emotion embedding and experiment with WavLM\nand ECAPA2 speaker embedders for the B3 baseline. Additionally, we compare\ndifferent speaker and prosody anonymization techniques. Furthermore, we\nintroduce Mean Reversion F0 for B5, which helps to enhance privacy without a\nloss in utility. Finally, we explore disentanglement models, namely $\\beta$-VAE\nand NaturalSpeech3 FACodec.",
    "updated" : "2024-10-03T10:45:10Z",
    "published" : "2024-10-03T10:45:10Z",
    "authors" : [
      {
        "name" : "Nikita Kuzmin"
      },
      {
        "name" : "Hieu-Thi Luong"
      },
      {
        "name" : "Jixun Yao"
      },
      {
        "name" : "Lei Xie"
      },
      {
        "name" : "Kong Aik Lee"
      },
      {
        "name" : "Eng Siong Chng"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02246v1",
    "title" : "PFGuard: A Generative Framework with Privacy and Fairness Safeguards",
    "summary" : "Generative models must ensure both privacy and fairness for Trustworthy AI.\nWhile these goals have been pursued separately, recent studies propose to\ncombine existing privacy and fairness techniques to achieve both goals.\nHowever, naively combining these techniques can be insufficient due to\nprivacy-fairness conflicts, where a sample in a minority group may be amplified\nfor fairness, only to be suppressed for privacy. We demonstrate how these\nconflicts lead to adverse effects, such as privacy violations and unexpected\nfairness-utility tradeoffs. To mitigate these risks, we propose PFGuard, a\ngenerative framework with privacy and fairness safeguards, which simultaneously\naddresses privacy, fairness, and utility. By using an ensemble of multiple\nteacher models, PFGuard balances privacy-fairness conflicts between fair and\nprivate training stages and achieves high utility based on ensemble learning.\nExtensive experiments show that PFGuard successfully generates synthetic data\non high-dimensional data while providing both fairness convergence and strict\nDP guarantees - the first of its kind to our knowledge.",
    "updated" : "2024-10-03T06:37:16Z",
    "published" : "2024-10-03T06:37:16Z",
    "authors" : [
      {
        "name" : "Soyeon Kim"
      },
      {
        "name" : "Yuji Roh"
      },
      {
        "name" : "Geon Heo"
      },
      {
        "name" : "Steven Euijong Whang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03621v1",
    "title" : "A Global Medical Data Security and Privacy Preserving Standards\n  Identification Framework for Electronic Healthcare Consumers",
    "summary" : "Electronic Health Records (EHR) are crucial for the success of digital\nhealthcare, with a focus on putting consumers at the center of this\ntransformation. However, the digitalization of healthcare records brings along\nsecurity and privacy risks for personal data. The major concern is that\ndifferent countries have varying standards for the security and privacy of\nmedical data. This paper proposed a novel and comprehensive framework to\nstandardize these rules globally, bringing them together on a common platform.\nTo support this proposal, the study reviews existing literature to understand\nthe research interest in this issue. It also examines six key laws and\nstandards related to security and privacy, identifying twenty concepts. The\nproposed framework utilized K-means clustering to categorize these concepts and\nidentify five key factors. Finally, an Ordinal Priority Approach is applied to\ndetermine the preferred implementation of these factors in the context of EHRs.\nThe proposed study provides a descriptive then prescriptive framework for the\nimplementation of privacy and security in the context of electronic health\nrecords. Therefore, the findings of the proposed framework are useful for\nprofessionals and policymakers in improving the security and privacy associated\nwith EHRs.",
    "updated" : "2024-10-04T17:22:55Z",
    "published" : "2024-10-04T17:22:55Z",
    "authors" : [
      {
        "name" : "Vinaytosh Mishra"
      },
      {
        "name" : "Kishu Gupta"
      },
      {
        "name" : "Deepika Saxena"
      },
      {
        "name" : "Ashutosh Kumar Singh"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03407v1",
    "title" : "Camel: Communication-Efficient and Maliciously Secure Federated Learning\n  in the Shuffle Model of Differential Privacy",
    "summary" : "Federated learning (FL) has rapidly become a compelling paradigm that enables\nmultiple clients to jointly train a model by sharing only gradient updates for\naggregation, without revealing their local private data. In order to protect\nthe gradient updates which could also be privacy-sensitive, there has been a\nline of work studying local differential privacy (LDP) mechanisms to provide a\nformal privacy guarantee. With LDP mechanisms, clients locally perturb their\ngradient updates before sharing them out for aggregation. However, such\napproaches are known for greatly degrading the model utility, due to heavy\nnoise addition. To enable a better privacy-utility tradeoff, a recently\nemerging trend is to apply the shuffle model of DP in FL, which relies on an\nintermediate shuffling operation on the perturbed gradient updates to achieve\nprivacy amplification. Following this trend, in this paper, we present Camel, a\nnew communication-efficient and maliciously secure FL framework in the shuffle\nmodel of DP. Camel first departs from existing works by ambitiously supporting\nintegrity check for the shuffle computation, achieving security against\nmalicious adversary. Specifically, Camel builds on the trending cryptographic\nprimitive of secret-shared shuffle, with custom techniques we develop for\noptimizing system-wide communication efficiency, and for lightweight integrity\nchecks to harden the security of server-side computation. In addition, we also\nderive a significantly tighter bound on the privacy loss through analyzing the\nRenyi differential privacy (RDP) of the overall FL process. Extensive\nexperiments demonstrate that Camel achieves better privacy-utility trade-offs\nthan the state-of-the-art work, with promising performance.",
    "updated" : "2024-10-04T13:13:44Z",
    "published" : "2024-10-04T13:13:44Z",
    "authors" : [
      {
        "name" : "Shuangqing Xu"
      },
      {
        "name" : "Yifeng Zheng"
      },
      {
        "name" : "Zhongyun Hua"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03069v1",
    "title" : "Interactive GDPR-Compliant Privacy Policy Generation for Software\n  Applications",
    "summary" : "Software applications are designed to assist users in conducting a wide range\nof tasks or interactions. They have become prevalent and play an integral part\nin people's lives in this digital era. To use those software applications,\nusers are sometimes requested to provide their personal information. As privacy\nhas become a significant concern and many data protection regulations exist\nworldwide, software applications must provide users with a privacy policy\ndetailing how their personal information is collected and processed. We propose\nan approach that generates a comprehensive and compliant privacy policy with\nrespect to the General Data Protection Regulation (GDPR) for diverse software\napplications. To support this, we first built a library of privacy clauses\nbased on existing privacy policy analysis. We then developed an interactive\nrule-based system that prompts software developers with a series of questions\nand uses their answers to generate a customised privacy policy for a given\nsoftware application. We evaluated privacy policies generated by our approach\nin terms of readability, completeness and coverage and compared them to privacy\npolicies generated by three existing privacy policy generators and a Generative\nAI-based tool. Our evaluation results show that the privacy policy generated by\nour approach is the most complete and comprehensive.",
    "updated" : "2024-10-04T01:22:16Z",
    "published" : "2024-10-04T01:22:16Z",
    "authors" : [
      {
        "name" : "Pattaraporn Sangaroonsilp"
      },
      {
        "name" : "Hoa Khanh Dam"
      },
      {
        "name" : "Omar Haggag"
      },
      {
        "name" : "John Grundy"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02912v1",
    "title" : "Fine-Tuning Language Models with Differential Privacy through Adaptive\n  Noise Allocation",
    "summary" : "Language models are capable of memorizing detailed patterns and information,\nleading to a double-edged effect: they achieve impressive modeling performance\non downstream tasks with the stored knowledge but also raise significant\nprivacy concerns. Traditional differential privacy based training approaches\noffer robust safeguards by employing a uniform noise distribution across all\nparameters. However, this overlooks the distinct sensitivities and\ncontributions of individual parameters in privacy protection and often results\nin suboptimal models. To address these limitations, we propose ANADP, a novel\nalgorithm that adaptively allocates additive noise based on the importance of\nmodel parameters. We demonstrate that ANADP narrows the performance gap between\nregular fine-tuning and traditional DP fine-tuning on a series of datasets\nwhile maintaining the required privacy constraints.",
    "updated" : "2024-10-03T19:02:50Z",
    "published" : "2024-10-03T19:02:50Z",
    "authors" : [
      {
        "name" : "Xianzhi Li"
      },
      {
        "name" : "Ran Zmigrod"
      },
      {
        "name" : "Zhiqiang Ma"
      },
      {
        "name" : "Xiaomo Liu"
      },
      {
        "name" : "Xiaodan Zhu"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05033v1",
    "title" : "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
    "summary" : "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
    "updated" : "2024-10-07T13:33:23Z",
    "published" : "2024-10-07T13:33:23Z",
    "authors" : [
      {
        "name" : "Amirreza Zamani"
      },
      {
        "name" : "Mikael Skoglund"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05020v1",
    "title" : "FRIDA: Free-Rider Detection using Privacy Attacks",
    "summary" : "Federated learning is increasingly popular as it enables multiple parties\nwith limited datasets and resources to train a high-performing machine learning\nmodel collaboratively. However, similarly to other collaborative systems,\nfederated learning is vulnerable to free-riders -- participants who do not\ncontribute to the training but still benefit from the shared model. Free-riders\nnot only compromise the integrity of the learning process but also slow down\nthe convergence of the global model, resulting in increased costs for the\nhonest participants.\n  To address this challenge, we propose FRIDA: free-rider detection using\nprivacy attacks, a framework that leverages inference attacks to detect\nfree-riders. Unlike traditional methods that only capture the implicit effects\nof free-riding, FRIDA directly infers details of the underlying training\ndatasets, revealing characteristics that indicate free-rider behaviour. Through\nextensive experiments, we demonstrate that membership and property inference\nattacks are effective for this purpose. Our evaluation shows that FRIDA\noutperforms state-of-the-art methods, especially in non-IID settings.",
    "updated" : "2024-10-07T13:20:26Z",
    "published" : "2024-10-07T13:20:26Z",
    "authors" : [
      {
        "name" : "Pol G. Recasens"
      },
      {
        "name" : "Ádám Horváth"
      },
      {
        "name" : "Alberto Gutierrez-Torre"
      },
      {
        "name" : "Jordi Torres"
      },
      {
        "name" : "Josep Ll. Berral"
      },
      {
        "name" : "Balázs Pejó"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.04754v1",
    "title" : "A Comprehensive Study on GDPR-Oriented Analysis of Privacy Policies:\n  Taxonomy, Corpus and GDPR Concept Classifiers",
    "summary" : "Machine learning based classifiers that take a privacy policy as the input\nand predict relevant concepts are useful in different applications such as\n(semi-)automated compliance analysis against requirements of the EU GDPR. In\nall past studies, such classifiers produce a concept label per segment (e.g.,\nsentence or paragraph) and their performances were evaluated by using a dataset\nof labeled segments without considering the privacy policy they belong to.\nHowever, such an approach could overestimate the performance in real-world\nsettings, where all segments in a new privacy policy are supposed to be unseen.\nAdditionally, we also observed other research gaps, including the lack of a\nmore complete GDPR taxonomy and the less consideration of hierarchical\ninformation in privacy policies. To fill such research gaps, we developed a\nmore complete GDPR taxonomy, created the first corpus of labeled privacy\npolicies with hierarchical information, and conducted the most comprehensive\nperformance evaluation of GDPR concept classifiers for privacy policies. Our\nwork leads to multiple novel findings, including the confirmed\ninappropriateness of splitting training and test sets at the segment level, the\nbenefits of considering hierarchical information, and the limitations of the\n\"one size fits all\" approach, and the significance of testing cross-corpus\ngeneralizability.",
    "updated" : "2024-10-07T05:19:12Z",
    "published" : "2024-10-07T05:19:12Z",
    "authors" : [
      {
        "name" : "Peng Tang"
      },
      {
        "name" : "Xin Li"
      },
      {
        "name" : "Yuxin Chen"
      },
      {
        "name" : "Weidong Qiu"
      },
      {
        "name" : "Haochen Mei"
      },
      {
        "name" : "Allison Holmes"
      },
      {
        "name" : "Fenghua Li"
      },
      {
        "name" : "Shujun Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.04606v1",
    "title" : "Privacy's Peril: Unmasking the Unregulated Underground Market of Data\n  Brokers and the Suggested Framework",
    "summary" : "The internet is a common place for businesses to collect and store as much\nclient data as possible and computer storage capacity has increased\nexponentially due to this trend. Businesses utilize this data to enhance\ncustomer satisfaction, generate revenue, boost sales, and increase profile.\nHowever, the emerging sector of data brokers is plagued with legal challenges.\nIn part I, we will look at what a data broker is, how it collects information,\nthe data industry, and some of the difficulties it encounters. In Part II, we\nwill look at potential options for regulating data brokers. All options are\nprovided in light of the EU General Data Protection Regulation (GDPR). In Part\nIII, we shall present our analysis and findings.",
    "updated" : "2024-10-06T19:51:31Z",
    "published" : "2024-10-06T19:51:31Z",
    "authors" : [
      {
        "name" : "Rabia Bajwa"
      },
      {
        "name" : "Farah Tasnur Meem"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.04302v1",
    "title" : "PANav: Toward Privacy-Aware Robot Navigation via Vision-Language Models",
    "summary" : "Navigating robots discreetly in human work environments while considering the\npossible privacy implications of robotic tasks presents significant challenges.\nSuch scenarios are increasingly common, for instance, when robots transport\nsensitive objects that demand high levels of privacy in spaces crowded with\nhuman activities. While extensive research has been conducted on robotic path\nplanning and social awareness, current robotic systems still lack the\nfunctionality of privacy-aware navigation in public environments. To address\nthis, we propose a new framework for mobile robot navigation that leverages\nvision-language models to incorporate privacy awareness into adaptive path\nplanning. Specifically, all potential paths from the starting point to the\ndestination are generated using the A* algorithm. Concurrently, the\nvision-language model is used to infer the optimal path for privacy-awareness,\ngiven the environmental layout and the navigational instruction. This approach\naims to minimize the robot's exposure to human activities and preserve the\nprivacy of the robot and its surroundings. Experimental results on the S3DIS\ndataset demonstrate that our framework significantly enhances mobile robots'\nprivacy awareness of navigation in human-shared public environments.\nFurthermore, we demonstrate the practical applicability of our framework by\nsuccessfully navigating a robotic platform through real-world office\nenvironments. The supplementary video and code can be accessed via the\nfollowing link: https://sites.google.com/view/privacy-aware-nav.",
    "updated" : "2024-10-05T22:54:31Z",
    "published" : "2024-10-05T22:54:31Z",
    "authors" : [
      {
        "name" : "Bangguo Yu"
      },
      {
        "name" : "Hamidreza Kasaei"
      },
      {
        "name" : "Ming Cao"
      }
    ],
    "categories" : [
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03925v1",
    "title" : "C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy\n  Policies to Enable Scalable Regulatory Compliance Audits",
    "summary" : "The development of tools and techniques to analyze and extract organizations\ndata habits from privacy policies are critical for scalable regulatory\ncompliance audits. Unfortunately, these tools are becoming increasingly limited\nin their ability to identify compliance issues and fixes. After all, most were\ndeveloped using regulation-agnostic datasets of annotated privacy policies\nobtained from a time before the introduction of landmark privacy regulations\nsuch as EUs GDPR and Californias CCPA. In this paper, we describe the first\nopen regulation-aware dataset of expert-annotated privacy policies, C3PA (CCPA\nPrivacy Policy Provision Annotations), aimed to address this challenge. C3PA\ncontains over 48K expert-labeled privacy policy text segments associated with\nresponses to CCPA-specific disclosure mandates from 411 unique organizations.\nWe demonstrate that the C3PA dataset is uniquely suited for aiding automated\naudits of compliance with CCPA-related disclosure mandates.",
    "updated" : "2024-10-04T21:04:39Z",
    "published" : "2024-10-04T21:04:39Z",
    "authors" : [
      {
        "name" : "Maaz Bin Musa"
      },
      {
        "name" : "Steven M. Winston"
      },
      {
        "name" : "Garrison Allen"
      },
      {
        "name" : "Jacob Schiller"
      },
      {
        "name" : "Kevin Moore"
      },
      {
        "name" : "Sean Quick"
      },
      {
        "name" : "Johnathan Melvin"
      },
      {
        "name" : "Padmini Srinivasan"
      },
      {
        "name" : "Mihailis E. Diamantis"
      },
      {
        "name" : "Rishab Nithyanand"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.06814v1",
    "title" : "Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning",
    "summary" : "Over-parameterized models are typically vulnerable to membership inference\nattacks, which aim to determine whether a specific sample is included in the\ntraining of a given model. Previous Weight regularizations (e.g., L1\nregularization) typically impose uniform penalties on all parameters, leading\nto a suboptimal tradeoff between model utility and privacy. In this work, we\nfirst show that only a small fraction of parameters substantially impact the\nprivacy risk. In light of this, we propose Privacy-aware Sparsity Tuning\n(PAST), a simple fix to the L1 Regularization, by employing adaptive penalties\nto different parameters. Our key idea behind PAST is to promote sparsity in\nparameters that significantly contribute to privacy leakage. In particular, we\nconstruct the adaptive weight for each parameter based on its privacy\nsensitivity, i.e., the gradient of the loss gap with respect to the parameter.\nUsing PAST, the network shrinks the loss gap between members and non-members,\nleading to strong resistance to privacy attacks. Extensive experiments\ndemonstrate the superiority of PAST, achieving a state-of-the-art balance in\nthe privacy-utility trade-off.",
    "updated" : "2024-10-09T12:13:49Z",
    "published" : "2024-10-09T12:13:49Z",
    "authors" : [
      {
        "name" : "Qiang Hu"
      },
      {
        "name" : "Hengxiang Zhang"
      },
      {
        "name" : "Hongxin Wei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.06587v1",
    "title" : "Bots can Snoop: Uncovering and Mitigating Privacy Risks of Bots in Group\n  Chats",
    "summary" : "New privacy concerns arise with chatbots on group messaging platforms.\nChatbots may access information beyond their intended functionalities, such as\nmessages unintended for chatbots or sender's identities. Chatbot operators may\nexploit such information to infer personal information and link users across\ngroups, potentially leading to personal data breaches, pervasive tracking, and\ntargeted advertising. Our analysis of conversation datasets shows that (1)\nchatbots often access far more messages than needed, and (2) when a user joins\na new group with chatbots, there is a 3.4% chance that at least one of the\nchatbots can recognize and associate the user with their previous interactions\nin other groups. Although state-of-the-art group messaging protocols provide\nrobust end-to-end security and some platforms have implemented policies to\nlimit chatbot access, no platforms successfully combine these features. This\npaper introduces SnoopGuard, a secure group messaging protocol that ensures\nuser privacy against chatbots while maintaining strong end-to-end security. Our\nmethod offers selective message access, preventing chatbots from accessing\nunrelated messages, and ensures sender anonymity within the group. SnoopGuard\nachieves $O(\\log n + m)$ message-sending complexity for a group of $n$ users\nand $m$ chatbots, compared to $O(\\log(n + m))$ in state-of-the-art protocols,\nwith acceptable overhead for enhanced privacy. Our prototype implementation\nshows that sending a message in a group of 50 users and 10 chatbots takes about\n30 milliseconds when integrated with Message Layer Security (MLS).",
    "updated" : "2024-10-09T06:37:41Z",
    "published" : "2024-10-09T06:37:41Z",
    "authors" : [
      {
        "name" : "Kai-Hsiang Chou"
      },
      {
        "name" : "Yi-Min Lin"
      },
      {
        "name" : "Yi-An Wang"
      },
      {
        "name" : "Jonathan Weiping Li"
      },
      {
        "name" : "Tiffany Hyun-Jin Kim"
      },
      {
        "name" : "Hsu-Chun Hsiao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.06266v1",
    "title" : "Near Exact Privacy Amplification for Matrix Mechanisms",
    "summary" : "We study the problem of computing the privacy parameters for DP machine\nlearning when using privacy amplification via random batching and noise\ncorrelated across rounds via a correlation matrix $\\textbf{C}$ (i.e., the\nmatrix mechanism). Past work on this problem either only applied to banded\n$\\textbf{C}$, or gave loose privacy parameters. In this work, we give a\nframework for computing near-exact privacy parameters for any lower-triangular,\nnon-negative $\\textbf{C}$. Our framework allows us to optimize the correlation\nmatrix $\\textbf{C}$ while accounting for amplification, whereas past work could\nnot. Empirically, we show this lets us achieve smaller RMSE on prefix sums than\nthe previous state-of-the-art (SOTA). We also show that we can improve on the\nSOTA performance on deep learning tasks. Our two main technical tools are (i)\nusing Monte Carlo accounting to bypass composition, which was the main\ntechnical challenge for past work, and (ii) a \"balls-in-bins\" batching scheme\nthat enables easy privacy analysis and is closer to practical random batching\nthan Poisson sampling.",
    "updated" : "2024-10-08T18:05:56Z",
    "published" : "2024-10-08T18:05:56Z",
    "authors" : [
      {
        "name" : "Christopher A. Choquette-Choo"
      },
      {
        "name" : "Arun Ganesh"
      },
      {
        "name" : "Saminul Haque"
      },
      {
        "name" : "Thomas Steinke"
      },
      {
        "name" : "Abhradeep Thakurta"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05930v1",
    "title" : "Fortify Your Foundations: Practical Privacy and Security for Foundation\n  Model Deployments In The Cloud",
    "summary" : "Foundation Models (FMs) display exceptional performance in tasks such as\nnatural language processing and are being applied across a growing range of\ndisciplines. Although typically trained on large public datasets, FMs are often\nfine-tuned or integrated into Retrieval-Augmented Generation (RAG) systems,\nwhich rely on private data. This access, along with their size and costly\ntraining, heightens the risk of intellectual property theft. Moreover,\nmultimodal FMs may expose sensitive information. In this work, we examine the\nFM threat model and discuss the practicality and comprehensiveness of various\napproaches for securing against them, such as ML-based methods and trusted\nexecution environments (TEEs). We demonstrate that TEEs offer an effective\nbalance between strong security properties, usability, and performance.\nSpecifically, we present a solution achieving less than 10\\% overhead versus\nbare metal for the full Llama2 7B and 13B inference pipelines running inside\n\\intel\\ SGX and \\intel\\ TDX. We also share our configuration files and insights\nfrom our implementation. To our knowledge, our work is the first to show the\npracticality of TEEs for securing FMs.",
    "updated" : "2024-10-08T11:33:09Z",
    "published" : "2024-10-08T11:33:09Z",
    "authors" : [
      {
        "name" : "Marcin Chrapek"
      },
      {
        "name" : "Anjo Vahldiek-Oberwagner"
      },
      {
        "name" : "Marcin Spoczynski"
      },
      {
        "name" : "Scott Constable"
      },
      {
        "name" : "Mona Vij"
      },
      {
        "name" : "Torsten Hoefler"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05907v1",
    "title" : "Privacy-Enhanced Over-the-Air Federated Learning via Client-Driven Power\n  Balancing",
    "summary" : "This paper introduces a novel privacy-enhanced over-the-air Federated\nLearning (OTA-FL) framework using client-driven power balancing (CDPB) to\naddress privacy concerns in OTA-FL systems. In recent studies, a server\ndetermines the power balancing based on the continuous transmission of channel\nstate information (CSI) from each client. Furthermore, they concentrate on\nfulfilling privacy requirements in every global iteration, which can heighten\nthe risk of privacy exposure as the learning process extends. To mitigate these\nrisks, we propose two CDPB strategies -- CDPB-n (noisy) and CDPB-i (idle) --\nallowing clients to adjust transmission power independently, without sharing\nCSI. CDPB-n transmits noise during poor conditions, while CDPB-i pauses\ntransmission until conditions improve. To further enhance privacy and learning\nefficiency, we show a mixed strategy, CDPB-mixed, which combines CDPB-n and\nCDPB-i. Our experimental results show that CDPB outperforms traditional\napproaches in terms of model accuracy and privacy guarantees, providing a\npractical solution for enhancing OTA-FL in resource-constrained environments.",
    "updated" : "2024-10-08T11:05:37Z",
    "published" : "2024-10-08T11:05:37Z",
    "authors" : [
      {
        "name" : "Bumjun Kim"
      },
      {
        "name" : "Hyowoon Seo"
      },
      {
        "name" : "Wan Choi"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05842v1",
    "title" : "Privacy-aware Fully Model-Free Event-triggered Cloud-based HVAC Control",
    "summary" : "Privacy is a major concern when computing-as-a-service (CaaS) platforms,\ne.g., cloud-computing platforms, are utilized for building automation, as CaaS\nplatforms can infer sensitive information, such as occupancy, using the sensor\nmeasurements of a building. Although the existing encrypted model-based control\nalgorithms can ensure the security and privacy of sensor measurements, they are\nhighly complex to implement and require high computational resources, which\nresult in a high cost of using CaaS platforms. To address these issues, in this\npaper, we propose an encrypted fully model-free event-triggered cloud-based\nHVAC control framework that ensures the privacy of occupancy information and\nminimizes the communication and computation overhead associated with encrypted\nHVAC control. To this end, we first develop a model-free controller for\nregulating indoor temperature and CO2 levels. We then design a model-free\nevent-triggering unit which reduces the communication and computation costs of\nencrypted HVAC control using an optimal triggering policy. Finally, we evaluate\nthe performance of the proposed encrypted fully model-free event-triggered\ncloud-based HVAC control framework using the TRNSYS simulator, comparing it to\nan encrypted model-based event-triggered control framework, which uses model\npredictive control to regulate the indoor climate. Our numerical results\ndemonstrate that, compared to the encrypted model-based method, the proposed\nfully model-free framework improves the control performance while reducing the\ncommunication and computation costs. More specifically, it reduces the\ncommunication between the system and the CaaS platform by 64% amount, and its\ncomputation time is 75% less than that of the model-based control.",
    "updated" : "2024-10-08T09:15:21Z",
    "published" : "2024-10-08T09:15:21Z",
    "authors" : [
      {
        "name" : "Zhenan Feng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05725v1",
    "title" : "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge\n  Distillation from Server",
    "summary" : "The success of large language models (LLMs) facilitate many parties to\nfine-tune LLMs on their own private data. However, this practice raises privacy\nconcerns due to the memorization of LLMs. Existing solutions, such as utilizing\nsynthetic data for substitution, struggle to simultaneously improve performance\nand preserve privacy. They either rely on a local model for generation,\nresulting in a performance decline, or take advantage of APIs, directly\nexposing the data to API servers. To address this issue, we propose\n\\textit{KnowledgeSG}, a novel client-server framework which enhances synthetic\ndata quality and improves model performance while ensuring privacy. We achieve\nthis by learning local knowledge from the private data with differential\nprivacy (DP) and distilling professional knowledge from the server.\nAdditionally, inspired by federated learning, we transmit models rather than\ndata between the client and server to prevent privacy leakage. Extensive\nexperiments in medical and financial domains demonstrate the effectiveness of\nKnowledgeSG. Our code is now publicly available at\nhttps://github.com/wwh0411/KnowledgeSG.",
    "updated" : "2024-10-08T06:42:28Z",
    "published" : "2024-10-08T06:42:28Z",
    "authors" : [
      {
        "name" : "Wenhao Wang"
      },
      {
        "name" : "Xiaoyu Liang"
      },
      {
        "name" : "Rui Ye"
      },
      {
        "name" : "Jingyi Chai"
      },
      {
        "name" : "Siheng Chen"
      },
      {
        "name" : "Yanfeng Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05653v1",
    "title" : "A Blockchain-Enhanced Framework for Privacy and Data Integrity in\n  Crowdsourced Drone Services",
    "summary" : "We present an innovative framework that integrates consumer-grade drones into\nbushfire management, addressing both service improvement and data privacy\nconcerns under Australia's Privacy Act 1988. This system establishes a\nmarketplace where bushfire management authorities, as data consumers, access\ncritical information from drone operators, who serve as data providers. The\nframework employs local differential privacy to safeguard the privacy of data\nproviders from all system entities, ensuring compliance with privacy standards.\nAdditionally, a blockchain-based solution facilitates fair data and fee\nexchanges while maintaining immutable records for enhanced accountability.\nValidated through a proof-of-concept implementation, the framework's\nscalability and adaptability make it well-suited for large-scale, real-world\napplications in bushfire management.",
    "updated" : "2024-10-08T03:08:47Z",
    "published" : "2024-10-08T03:08:47Z",
    "authors" : [
      {
        "name" : "Junaid Akram"
      },
      {
        "name" : "Ali Anaissi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05506v1",
    "title" : "Privacy Vulnerabilities in Marginals-based Synthetic Data",
    "summary" : "When acting as a privacy-enhancing technology, synthetic data generation\n(SDG) aims to maintain a resemblance to the real data while excluding\npersonally-identifiable information. Many SDG algorithms provide robust\ndifferential privacy (DP) guarantees to this end. However, we show that the\nstrongest class of SDG algorithms--those that preserve \\textit{marginal\nprobabilities}, or similar statistics, from the underlying data--leak\ninformation about individuals that can be recovered more efficiently than\npreviously understood. We demonstrate this by presenting a novel membership\ninference attack, MAMA-MIA, and evaluate it against three seminal DP SDG\nalgorithms: MST, PrivBayes, and Private-GSD. MAMA-MIA leverages knowledge of\nwhich SDG algorithm was used, allowing it to learn information about the hidden\ndata more accurately, and orders-of-magnitude faster, than other leading\nattacks. We use MAMA-MIA to lend insight into existing SDG vulnerabilities. Our\napproach went on to win the first SNAKE (SaNitization Algorithm under attacK\n... $\\varepsilon$) competition.",
    "updated" : "2024-10-07T21:24:22Z",
    "published" : "2024-10-07T21:24:22Z",
    "authors" : [
      {
        "name" : "Steven Golob"
      },
      {
        "name" : "Sikha Pentyala"
      },
      {
        "name" : "Anuar Maratkhan"
      },
      {
        "name" : "Martine De Cock"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.08122v1",
    "title" : "PP-GWAS: Privacy Preserving Multi-Site Genome-wide Association Studies",
    "summary" : "Genome-wide association studies are pivotal in understanding the genetic\nunderpinnings of complex traits and diseases. Collaborative, multi-site GWAS\naim to enhance statistical power but face obstacles due to the sensitive nature\nof genomic data sharing. Current state-of-the-art methods provide a\nprivacy-focused approach utilizing computationally expensive methods such as\nSecure Multi-Party Computation and Homomorphic Encryption. In this context, we\npresent a novel algorithm PP-GWAS designed to improve upon existing standards\nin terms of computational efficiency and scalability without sacrificing data\nprivacy. This algorithm employs randomized encoding within a distributed\narchitecture to perform stacked ridge regression on a Linear Mixed Model to\nensure rigorous analysis. Experimental evaluation with real world and synthetic\ndata indicates that PP-GWAS can achieve computational speeds twice as fast as\nsimilar state-of-the-art algorithms while using lesser computational resources,\nall while adhering to a robust security model that caters to an all-but-one\nsemi-honest adversary setting. We have assessed its performance using various\ndatasets, emphasizing its potential in facilitating more efficient and private\ngenomic analyses.",
    "updated" : "2024-10-10T17:07:57Z",
    "published" : "2024-10-10T17:07:57Z",
    "authors" : [
      {
        "name" : "Arjhun Swaminathan"
      },
      {
        "name" : "Anika Hannemann"
      },
      {
        "name" : "Ali Burak Ünal"
      },
      {
        "name" : "Nico Pfeifer"
      },
      {
        "name" : "Mete Akgün"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.07900v1",
    "title" : "CL3: A Collaborative Learning Framework for the Medical Data Ensuring\n  Data Privacy in the Hyperconnected Environment",
    "summary" : "In a hyperconnected environment, medical institutions are particularly\nconcerned with data privacy when sharing and transmitting sensitive patient\ninformation due to the risk of data breaches, where malicious actors could\nintercept sensitive information. A collaborative learning framework, including\ntransfer, federated, and incremental learning, can generate efficient, secure,\nand scalable models while requiring less computation, maintaining patient data\nprivacy, and ensuring an up-to-date model. This study aims to address the\ndetection of COVID-19 using chest X-ray images through a proposed collaborative\nlearning framework called CL3. Initially, transfer learning is employed,\nleveraging knowledge from a pre-trained model as the starting global model.\nLocal models from different medical institutes are then integrated, and a new\nglobal model is constructed to adapt to any data drift observed in the local\nmodels. Additionally, incremental learning is considered, allowing continuous\nadaptation to new medical data without forgetting previously learned\ninformation. Experimental results demonstrate that the CL3 framework achieved a\nglobal accuracy of 89.99\\% when using Xception with a batch size of 16 after\nbeing trained for six federated communication rounds.",
    "updated" : "2024-10-10T13:29:12Z",
    "published" : "2024-10-10T13:29:12Z",
    "authors" : [
      {
        "name" : "Mohamamd Zavid Parvez"
      },
      {
        "name" : "Rafiqul Islam"
      },
      {
        "name" : "Md Zahidul Islam"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.07772v1",
    "title" : "Towards Quantifying The Privacy Of Redacted Text",
    "summary" : "In this paper we propose use of a k-anonymity-like approach for evaluating\nthe privacy of redacted text. Given a piece of redacted text we use a state of\nthe art transformer-based deep learning network to reconstruct the original\ntext. This generates multiple full texts that are consistent with the redacted\ntext, i.e. which are grammatical, have the same non-redacted words etc, and\nrepresents each of these using an embedding vector that captures sentence\nsimilarity. In this way we can estimate the number, diversity and quality of\nfull text consistent with the redacted text and so evaluate privacy.",
    "updated" : "2024-10-10T10:00:27Z",
    "published" : "2024-10-10T10:00:27Z",
    "authors" : [
      {
        "name" : "Vaibhav Gusain"
      },
      {
        "name" : "Douglas Leith"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.07632v1",
    "title" : "Provable Privacy Attacks on Trained Shallow Neural Networks",
    "summary" : "We study what provable privacy attacks can be shown on trained, 2-layer ReLU\nneural networks. We explore two types of attacks; data reconstruction attacks,\nand membership inference attacks. We prove that theoretical results on the\nimplicit bias of 2-layer neural networks can be used to provably reconstruct a\nset of which at least a constant fraction are training points in a univariate\nsetting, and can also be used to identify with high probability whether a given\npoint was used in the training set in a high dimensional setting. To the best\nof our knowledge, our work is the first to show provable vulnerabilities in\nthis setting.",
    "updated" : "2024-10-10T05:54:01Z",
    "published" : "2024-10-10T05:54:01Z",
    "authors" : [
      {
        "name" : "Guy Smorodinsky"
      },
      {
        "name" : "Gal Vardi"
      },
      {
        "name" : "Itay Safran"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.07414v1",
    "title" : "Bayes-Nash Generative Privacy Protection Against Membership Inference\n  Attacks",
    "summary" : "An ability to share data, even in aggregated form, is critical to advancing\nboth conventional and data science. However, insofar as such datasets are\ncomprised of individuals, their membership in these datasets is often viewed as\nsensitive, with membership inference attacks (MIAs) threatening to violate\ntheir privacy. We propose a Bayesian game model for privacy-preserving\npublishing of data-sharing mechanism outputs (for example, summary statistics\nfor sharing genomic data). In this game, the defender minimizes a combination\nof expected utility and privacy loss, with the latter being maximized by a\nBayes-rational attacker. We propose a GAN-style algorithm to approximate a\nBayes-Nash equilibrium of this game, and introduce the notions of Bayes-Nash\ngenerative privacy (BNGP) and Bayes generative privacy (BGP) risk that aims to\noptimally balance the defender's privacy and utility in a way that is robust to\nthe attacker's heterogeneous preferences with respect to true and false\npositives. We demonstrate the properties of composition and post-processing for\nBGP risk and establish conditions under which BNGP and pure differential\nprivacy (PDP) are equivalent. We apply our method to sharing summary\nstatistics, where MIAs can re-identify individuals even from aggregated data.\nTheoretical analysis and empirical results demonstrate that our Bayesian\ngame-theoretic method outperforms state-of-the-art approaches for\nprivacy-preserving sharing of summary statistics.",
    "updated" : "2024-10-09T20:29:04Z",
    "published" : "2024-10-09T20:29:04Z",
    "authors" : [
      {
        "name" : "Tao Zhang"
      },
      {
        "name" : "Rajagopal Venkatesaraman"
      },
      {
        "name" : "Rajat K. De"
      },
      {
        "name" : "Bradley A. Malin"
      },
      {
        "name" : "Yevgeniy Vorobeychik"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05725v2",
    "title" : "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge\n  Distillation from Server",
    "summary" : "The success of large language models (LLMs) facilitate many parties to\nfine-tune LLMs on their own private data. However, this practice raises privacy\nconcerns due to the memorization of LLMs. Existing solutions, such as utilizing\nsynthetic data for substitution, struggle to simultaneously improve performance\nand preserve privacy. They either rely on a local model for generation,\nresulting in a performance decline, or take advantage of APIs, directly\nexposing the data to API servers. To address this issue, we propose\nKnowledgeSG, a novel client-server framework which enhances synthetic data\nquality and improves model performance while ensuring privacy. We achieve this\nby learning local knowledge from the private data with differential privacy\n(DP) and distilling professional knowledge from the server. Additionally,\ninspired by federated learning, we transmit models rather than data between the\nclient and server to prevent privacy leakage. Extensive experiments in medical\nand financial domains demonstrate the effectiveness of KnowledgeSG. Our code is\nnow publicly available at https://github.com/wwh0411/KnowledgeSG.",
    "updated" : "2024-10-10T03:58:35Z",
    "published" : "2024-10-08T06:42:28Z",
    "authors" : [
      {
        "name" : "Wenhao Wang"
      },
      {
        "name" : "Xiaoyu Liang"
      },
      {
        "name" : "Rui Ye"
      },
      {
        "name" : "Jingyi Chai"
      },
      {
        "name" : "Siheng Chen"
      },
      {
        "name" : "Yanfeng Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.08756v1",
    "title" : "Privacy-Preserving Optimal State Estimation with Low Complexity via\n  Cramér-Rao Lower Bound Approach",
    "summary" : "This paper addresses the optimal state estimation problem for dynamic systems\nwhile preserving private information against an adversary. To dominate the\nadversary's estimation accuracy about private information in the mean square\nerror (MSE) sense, the Cram\\'er-Rao lower bound (CRLB) is employed to evaluate\nprivacy level. The problem is formulated as a constrained optimization, which\nminimizes the MSE of the state estimate with a constraint on privacy level,\nachieving a trade-off between privacy and utility. To solve the constrained\noptimization problem, an explicit expression for CRLB is first provided using\nthe information inequality. To overcome the increasing sizes of the involved\nmatrices over time, a low-complexity approach is then proposed to achieve\nonline calculation for CRLB, significantly reducing computational complexity.\nNext, the optimization problem is relaxed to a semi-definite programming\nproblem, and a relaxed solution is provided. Finally, a privacy-preserving\nstate estimation algorithm with low complexity is developed and proved to\nachieve differential privacy. Two illustrative examples, including a practical\ncase of building occupancy, demonstrate the effectiveness of the proposed\nalgorithm.",
    "updated" : "2024-10-11T12:15:25Z",
    "published" : "2024-10-11T12:15:25Z",
    "authors" : [
      {
        "name" : "Liping Guo"
      },
      {
        "name" : "Jimin Wang"
      },
      {
        "name" : "Yanlong Zhao"
      },
      {
        "name" : "Ji-Feng Zhang"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.08755v1",
    "title" : "PILLAR: an AI-Powered Privacy Threat Modeling Tool",
    "summary" : "The rapid evolution of Large Language Models (LLMs) has unlocked new\npossibilities for applying artificial intelligence across a wide range of\nfields, including privacy engineering. As modern applications increasingly\nhandle sensitive user data, safeguarding privacy has become more critical than\never. To protect privacy effectively, potential threats need to be identified\nand addressed early in the system development process. Frameworks like LINDDUN\noffer structured approaches for uncovering these risks, but despite their\nvalue, they often demand substantial manual effort, expert input, and detailed\nsystem knowledge. This makes the process time-consuming and prone to errors.\nCurrent privacy threat modeling methods, such as LINDDUN, typically rely on\ncreating and analyzing complex data flow diagrams (DFDs) and system\ndescriptions to pinpoint potential privacy issues. While these approaches are\nthorough, they can be cumbersome, relying heavily on the precision of the data\nprovided by users. Moreover, they often generate a long list of threats without\nclear guidance on how to prioritize them, leaving developers unsure of where to\nfocus their efforts. In response to these challenges, we introduce PILLAR\n(Privacy risk Identification with LINDDUN and LLM Analysis Report), a new tool\nthat integrates LLMs with the LINDDUN framework to streamline and enhance\nprivacy threat modeling. PILLAR automates key parts of the LINDDUN process,\nsuch as generating DFDs, classifying threats, and prioritizing risks. By\nleveraging the capabilities of LLMs, PILLAR can take natural language\ndescriptions of systems and transform them into comprehensive threat models\nwith minimal input from users, reducing the workload on developers and privacy\nexperts while improving the efficiency and accuracy of the process.",
    "updated" : "2024-10-11T12:13:03Z",
    "published" : "2024-10-11T12:13:03Z",
    "authors" : [
      {
        "name" : "Majid Mollaeefar"
      },
      {
        "name" : "Andrea Bissoli"
      },
      {
        "name" : "Silvio Ranise"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.08555v1",
    "title" : "Design of Secure, Privacy-focused, and Accessible E-Payment Applications\n  for Older Adults",
    "summary" : "E-payments are essential for transactional convenience in today's digital\neconomy and are becoming increasingly important for older adults, emphasizing\nthe need for enhanced security, privacy, and usability. To address this, we\nconducted a survey-based study with 400 older adults aged 65 and above to\nevaluate a high-fidelity prototype of an e-payment mobile application, which\nincluded features such as multi-factor authentication (MFA) and QR code-based\nrecipient addition. Based on our findings, we developed a tailored \\b{eta}\nversion of the application to meet the specific needs of this demographic.\nNotably, approximately 91% of participants preferred traditional\nknowledge-based and single-mode authentication compared to expert-recommended\nMFA. We concluded by providing recommendations aimed at developing inclusive\ne-payment solutions that address the security, privacy, and usability\nrequirements of older adults.",
    "updated" : "2024-10-11T06:12:18Z",
    "published" : "2024-10-11T06:12:18Z",
    "authors" : [
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.08553v1",
    "title" : "Balancing Innovation and Privacy: Data Security Strategies in Natural\n  Language Processing Applications",
    "summary" : "This research addresses privacy protection in Natural Language Processing\n(NLP) by introducing a novel algorithm based on differential privacy, aimed at\nsafeguarding user data in common applications such as chatbots, sentiment\nanalysis, and machine translation. With the widespread application of NLP\ntechnology, the security and privacy protection of user data have become\nimportant issues that need to be solved urgently. This paper proposes a new\nprivacy protection algorithm designed to effectively prevent the leakage of\nuser sensitive information. By introducing a differential privacy mechanism,\nour model ensures the accuracy and reliability of data analysis results while\nadding random noise. This method not only reduces the risk caused by data\nleakage but also achieves effective processing of data while protecting user\nprivacy. Compared to traditional privacy methods like data anonymization and\nhomomorphic encryption, our approach offers significant advantages in terms of\ncomputational efficiency and scalability while maintaining high accuracy in\ndata analysis. The proposed algorithm's efficacy is demonstrated through\nperformance metrics such as accuracy (0.89), precision (0.85), and recall\n(0.88), outperforming other methods in balancing privacy and utility. As\nprivacy protection regulations become increasingly stringent, enterprises and\ndevelopers must take effective measures to deal with privacy risks. Our\nresearch provides an important reference for the application of privacy\nprotection technology in the field of NLP, emphasizing the need to achieve a\nbalance between technological innovation and user privacy. In the future, with\nthe continuous advancement of technology, privacy protection will become a core\nelement of data-driven applications and promote the healthy development of the\nentire industry.",
    "updated" : "2024-10-11T06:05:10Z",
    "published" : "2024-10-11T06:05:10Z",
    "authors" : [
      {
        "name" : "Shaobo Liu"
      },
      {
        "name" : "Guiran Liu"
      },
      {
        "name" : "Binrong Zhu"
      },
      {
        "name" : "Yuanshuai Luo"
      },
      {
        "name" : "Linxiao Wu"
      },
      {
        "name" : "Rui Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.08462v1",
    "title" : "Driving Privacy Forward: Mitigating Information Leakage within Smart\n  Vehicles through Synthetic Data Generation",
    "summary" : "Smart vehicles produce large amounts of data, much of which is sensitive and\nat risk of privacy breaches. As attackers increasingly exploit anonymised\nmetadata within these datasets to profile drivers, it's important to find\nsolutions that mitigate this information leakage without hindering innovation\nand ongoing research. Synthetic data has emerged as a promising tool to address\nthese privacy concerns, as it allows for the replication of real-world data\nrelationships while minimising the risk of revealing sensitive information. In\nthis paper, we examine the use of synthetic data to tackle these challenges. We\nstart by proposing a comprehensive taxonomy of 14 in-vehicle sensors,\nidentifying potential attacks and categorising their vulnerability. We then\nfocus on the most vulnerable signals, using the Passive Vehicular Sensor (PVS)\ndataset to generate synthetic data with a Tabular Variational Autoencoder\n(TVAE) model, which included over 1 million data points. Finally, we evaluate\nthis against 3 core metrics: fidelity, utility, and privacy. Our results show\nthat we achieved 90.1% statistical similarity and 78% classification accuracy\nwhen tested on its original intent while also preventing the profiling of the\ndriver. The code can be found at\nhttps://github.com/krish-parikh/Synthetic-Data-Generation",
    "updated" : "2024-10-11T02:28:27Z",
    "published" : "2024-10-11T02:28:27Z",
    "authors" : [
      {
        "name" : "Krish Parikh"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.08381v1",
    "title" : "Assessing Privacy Policies with AI: Ethical, Legal, and Technical\n  Challenges",
    "summary" : "The growing use of Machine Learning and Artificial Intelligence (AI),\nparticularly Large Language Models (LLMs) like OpenAI's GPT series, leads to\ndisruptive changes across organizations. At the same time, there is a growing\nconcern about how organizations handle personal data. Thus, privacy policies\nare essential for transparency in data processing practices, enabling users to\nassess privacy risks. However, these policies are often long and complex. This\nmight lead to user confusion and consent fatigue, where users accept data\npractices against their interests, and abusive or unfair practices might go\nunnoticed. LLMss can be used to assess privacy policies for users\nautomatically. In this interdisciplinary work, we explore the challenges of\nthis approach in three pillars, namely technical feasibility, ethical\nimplications, and legal compatibility of using LLMs to assess privacy policies.\nOur findings aim to identify potential for future research, and to foster a\ndiscussion on the use of LLM technologies for enabling users to fulfil their\nimportant role as decision-makers in a constantly developing AI-driven digital\neconomy.",
    "updated" : "2024-10-10T21:36:35Z",
    "published" : "2024-10-10T21:36:35Z",
    "authors" : [
      {
        "name" : "Irem Aydin"
      },
      {
        "name" : "Hermann Diebel-Fischer"
      },
      {
        "name" : "Vincent Freiberger"
      },
      {
        "name" : "Julia Möller-Klapperich"
      },
      {
        "name" : "Erik Buchmann"
      },
      {
        "name" : "Michael Färber"
      },
      {
        "name" : "Anne Lauber-Rönsberg"
      },
      {
        "name" : "Birte Platow"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.08302v1",
    "title" : "A Framework to Audit Email Address Privacy and Analyze Email Marketing\n  Practices of Online Services and Apps",
    "summary" : "This study explores the widespread perception that personal data, such as\nemail addresses, may be shared or sold without informed user consent,\ninvestigating whether these concerns are reflected in actual practices of\npopular online services and apps. Over the course of a year, we collected and\nanalyzed the source, volume, frequency, and content of emails received by users\nafter signing up for the 150 most popular online services and apps across\nvarious sectors. By examining patterns in email communications, we aim to\nidentify consistent strategies used across industries, including potential\nsigns of third-party data sharing. This analysis provides a critical evaluation\nof how email marketing tactics may intersect with data-sharing practices, with\nimportant implications for consumer privacy and regulatory oversight. Our study\nfindings, conducted post-CCPA and GDPR, indicate that while no third-party spam\nemail was detected, internal email marketing practices were pervasive, with\ncompanies frequently sending promotional and CRM emails despite opt-out\npreferences. The framework established in this work is designed to be scalable,\nallowing for continuous monitoring, and can be extended to include a more\ndiverse set of apps and services for broader analysis, ultimately contributing\nto improved user perception of data privacy practices.",
    "updated" : "2024-10-10T18:44:18Z",
    "published" : "2024-10-10T18:44:18Z",
    "authors" : [
      {
        "name" : "Scott Seidenberger"
      },
      {
        "name" : "Oluwasijibomi Ajisegiri"
      },
      {
        "name" : "Noah Pursell"
      },
      {
        "name" : "Fazil Raja"
      },
      {
        "name" : "Anindya Maiti"
      }
    ],
    "categories" : [
      "cs.SI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.09721v1",
    "title" : "\"I inherently just trust that it works\": Investigating Mental Models of\n  Open-Source Libraries for Differential Privacy",
    "summary" : "Differential privacy (DP) is a promising framework for privacy-preserving\ndata science, but recent studies have exposed challenges in bringing this\ntheoretical framework for privacy into practice. These tensions are\nparticularly salient in the context of open-source software libraries for DP\ndata analysis, which are emerging tools to help data stewards and analysts\nbuild privacy-preserving data pipelines for their applications. While there has\nbeen significant investment into such libraries, we need further inquiry into\nthe role of these libraries in promoting understanding of and trust in DP, and\nin turn, the ways in which design of these open-source libraries can shed light\non the challenges of creating trustworthy data infrastructures in practice. In\nthis study, we use qualitative methods and mental models approaches to analyze\nthe differences between conceptual models used to design open-source DP\nlibraries and mental models of DP held by users. Through a two-stage study\ndesign involving formative interviews with 5 developers of open-source DP\nlibraries and user studies with 17 data analysts, we find that DP libraries\noften struggle to bridge the gaps between developer and user mental models. In\nparticular, we highlight the tension DP libraries face in maintaining rigorous\nDP implementations and facilitating user interaction. We conclude by offering\npractical recommendations for further development of DP libraries.",
    "updated" : "2024-10-13T04:24:09Z",
    "published" : "2024-10-13T04:24:09Z",
    "authors" : [
      {
        "name" : "Patrick Song"
      },
      {
        "name" : "Jayshree Sarathy"
      },
      {
        "name" : "Michael Shoemate"
      },
      {
        "name" : "Salil Vadhan"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.09506v1",
    "title" : "Distribution-Aware Mean Estimation under User-level Local Differential\n  Privacy",
    "summary" : "We consider the problem of mean estimation under user-level local\ndifferential privacy, where $n$ users are contributing through their local pool\nof data samples. Previous work assume that the number of data samples is the\nsame across users. In contrast, we consider a more general and realistic\nscenario where each user $u \\in [n]$ owns $m_u$ data samples drawn from some\ngenerative distribution $\\mu$; $m_u$ being unknown to the statistician but\ndrawn from a known distribution $M$ over $\\mathbb{N}^\\star$. Based on a\ndistribution-aware mean estimation algorithm, we establish an $M$-dependent\nupper bounds on the worst-case risk over $\\mu$ for the task of mean estimation.\nWe then derive a lower bound. The two bounds are asymptotically matching up to\nlogarithmic factors and reduce to known bounds when $m_u = m$ for any user $u$.",
    "updated" : "2024-10-12T11:57:52Z",
    "published" : "2024-10-12T11:57:52Z",
    "authors" : [
      {
        "name" : "Corentin Pla"
      },
      {
        "name" : "Hugo Richard"
      },
      {
        "name" : "Maxime Vono"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.12514v1",
    "title" : "Generating Synthetic Functional Data for Privacy-Preserving GPS\n  Trajectories",
    "summary" : "This research presents FDASynthesis, a novel algorithm designed to generate\nsynthetic GPS trajectory data while preserving privacy. After pre-processing\nthe input GPS data, human mobility traces are modeled as multidimensional\ncurves using Functional Data Analysis (FDA). Then, the synthesis process\nidentifies the K-nearest trajectories and averages their Square-Root Velocity\nFunctions (SRVFs) to generate synthetic data. This results in synthetic\ntrajectories that maintain the utility of the original data while ensuring\nprivacy. Although applied for human mobility research, FDASynthesis is highly\nadaptable to different types of functional data, offering a scalable solution\nin various application domains.",
    "updated" : "2024-10-16T12:47:09Z",
    "published" : "2024-10-16T12:47:09Z",
    "authors" : [
      {
        "name" : "Arianna Burzacchi"
      },
      {
        "name" : "Lise Bellanger"
      },
      {
        "name" : "Klervi Le Gall"
      },
      {
        "name" : "Aymeric Stamm"
      },
      {
        "name" : "Simone Vantini"
      }
    ],
    "categories" : [
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.12418v1",
    "title" : "Privacy-Preserving Synthetically Augmented Knowledge Graphs with\n  Semantic Utility",
    "summary" : "Knowledge Graphs (KGs) have recently gained relevant attention in many\napplication domains, from healthcare to biotechnology, from logistics to\nfinance. Financial organisations, central banks, economic research entities,\nand national supervision authorities apply ontological reasoning on KGs to\naddress crucial business tasks, such as economic policymaking, banking\nsupervision, anti-money laundering, and economic research. Reasoning allows for\nthe generation of derived knowledge capturing complex business semantics and\nthe set up of effective business processes. A major obstacle in KGs sharing is\nrepresented by privacy considerations since the identity of the data subjects\nand their sensitive or company-confidential information may be improperly\nexposed.\n  In this paper, we propose a novel framework to enable KGs sharing while\nensuring that information that should remain private is not directly released\nnor indirectly exposed via derived knowledge, while maintaining the embedded\nknowledge of the KGs to support business downstream tasks. Our approach\nproduces a privacy-preserving synthetic KG as an augmentation of the input one\nvia the introduction of structural anonymisation. We introduce a novel privacy\nmeasure for KGs, which considers derived knowledge and a new utility metric\nthat captures the business semantics we want to preserve, and propose two novel\nanonymization algorithms. Our extensive experimental evaluation, with both\nsynthetic graphs and real-world datasets, confirms the effectiveness of our\napproach achieving up to a 70% improvement in the privacy of entities compared\nto existing methods not specifically designed for KGs.",
    "updated" : "2024-10-16T10:04:02Z",
    "published" : "2024-10-16T10:04:02Z",
    "authors" : [
      {
        "name" : "Luigi Bellomarini"
      },
      {
        "name" : "Costanza Catalano"
      },
      {
        "name" : "Andrea Coletta"
      },
      {
        "name" : "Michela Iezzi"
      },
      {
        "name" : "Pierangela Samarati"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.12402v1",
    "title" : "De-Identification of Medical Imaging Data: A Comprehensive Tool for\n  Ensuring Patient Privacy",
    "summary" : "Medical data employed in research frequently comprises sensitive patient\nhealth information (PHI), which is subject to rigorous legal frameworks such as\nthe General Data Protection Regulation (GDPR) or the Health Insurance\nPortability and Accountability Act (HIPAA). Consequently, these types of data\nmust be pseudonymized prior to utilisation, which presents a significant\nchallenge for many researchers. Given the vast array of medical data, it is\nnecessary to employ a variety of de-identification techniques. To facilitate\nthe anonymization process for medical imaging data, we have developed an\nopen-source tool that can be used to de-identify DICOM magnetic resonance\nimages, computer tomography images, whole slide images and magnetic resonance\ntwix raw data. Furthermore, the implementation of a neural network enables the\nremoval of text within the images. The proposed tool automates an elaborate\nanonymization pipeline for multiple types of inputs, reducing the need for\nadditional tools used for de-identification of imaging data. We make our code\npublicly available at\nhttps://github.com/code-lukas/medical_image_deidentification.",
    "updated" : "2024-10-16T09:31:24Z",
    "published" : "2024-10-16T09:31:24Z",
    "authors" : [
      {
        "name" : "Moritz Rempe"
      },
      {
        "name" : "Lukas Heine"
      },
      {
        "name" : "Constantin Seibold"
      },
      {
        "name" : "Fabian Hörst"
      },
      {
        "name" : "Jens Kleesiek"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.12336v1",
    "title" : "Privacy by Design: Bringing User Awareness to Privacy Risks in Internet\n  of Things",
    "summary" : "This paper aims to cover and summarize the field of IoT and related privacy\nconcerns through the lens of privacy by design. With the ever-increasing\nincorporation of technology within our daily lives and an ever-growing active\nresearch into smart devices and technologies, privacy concerns are inevitable.\nWe intend to briefly cover the broad topic of privacy in the IoT space, the\ninherent challenges and risks in such systems, and a few recent techniques that\nintend to resolve these issues on the subdomain level and a system scale level.\nWe then proceed to approach this situation through design thinking and\nprivacy-by-design, given that most of the prior efforts are based on resolving\nprivacy concerns on technical grounds with system-level design. We participated\nin a co-design workshop for the privacy of a content creation platform and used\nthose findings to deploy a survey-based mechanism to tackle some key concern\nareas for user groups and formulate design principles for privacy that promote\ntransparent, user-centered, and awareness-provoking privacy design.",
    "updated" : "2024-10-16T07:57:22Z",
    "published" : "2024-10-16T07:57:22Z",
    "authors" : [
      {
        "name" : "Usama Younus"
      },
      {
        "name" : "Rie Kamikubo"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.12309v1",
    "title" : "Correction to Local Information Privacy and Its Applications to Data\n  Aggregation",
    "summary" : "In our previous works, we defined Local Information Privacy (LIP) as a\ncontext-aware privacy notion and presented the corresponding privacy-preserving\nmechanism. Then we claim that the mechanism satisfies epsilon-LIP for any\nepsilon>0 for arbitrary Px. However, this claim is not completely correct. In\nthis document, we provide a correction to the valid range of privacy parameters\nof our previously proposed LIP mechanism. Further, we propose efficient\nalgorithms to expand the range of valid privacy parameters. Finally, we discuss\nthe impact of updated results on our original paper's experiments, the\nrationale of the proposed correction and corrected results.",
    "updated" : "2024-10-16T07:22:53Z",
    "published" : "2024-10-16T07:22:53Z",
    "authors" : [
      {
        "name" : "Bo Jiang"
      },
      {
        "name" : "Ming Li"
      },
      {
        "name" : "Ravi Tandon"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.12045v1",
    "title" : "Differential Privacy on Trust Graphs",
    "summary" : "We study differential privacy (DP) in a multi-party setting where each party\nonly trusts a (known) subset of the other parties with its data. Specifically,\ngiven a trust graph where vertices correspond to parties and neighbors are\nmutually trusting, we give a DP algorithm for aggregation with a much better\nprivacy-utility trade-off than in the well-studied local model of DP (where\neach party trusts no other party). We further study a robust variant where each\nparty trusts all but an unknown subset of at most $t$ of its neighbors (where\n$t$ is a given parameter), and give an algorithm for this setting. We\ncomplement our algorithms with lower bounds, and discuss implications of our\nwork to other tasks in private learning and analytics.",
    "updated" : "2024-10-15T20:31:04Z",
    "published" : "2024-10-15T20:31:04Z",
    "authors" : [
      {
        "name" : "Badih Ghazi"
      },
      {
        "name" : "Ravi Kumar"
      },
      {
        "name" : "Pasin Manurangsi"
      },
      {
        "name" : "Serena Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS",
      "cs.LG",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.11906v1",
    "title" : "Empowering Users in Digital Privacy Management through Interactive\n  LLM-Based Agents",
    "summary" : "This paper presents a novel application of large language models (LLMs) to\nenhance user comprehension of privacy policies through an interactive dialogue\nagent. We demonstrate that LLMs significantly outperform traditional models in\ntasks like Data Practice Identification, Choice Identification, Policy\nSummarization, and Privacy Question Answering, setting new benchmarks in\nprivacy policy analysis. Building on these findings, we introduce an innovative\nLLM-based agent that functions as an expert system for processing website\nprivacy policies, guiding users through complex legal language without\nrequiring them to pose specific questions. A user study with 100 participants\nshowed that users assisted by the agent had higher comprehension levels (mean\nscore of 2.6 out of 3 vs. 1.8 in the control group), reduced cognitive load\n(task difficulty ratings of 3.2 out of 10 vs. 7.8), increased confidence in\nmanaging privacy, and completed tasks in less time (5.5 minutes vs. 15.8\nminutes). This work highlights the potential of LLM-based agents to transform\nuser interaction with privacy policies, leading to more informed consent and\nempowering users in the digital services landscape.",
    "updated" : "2024-10-15T02:16:59Z",
    "published" : "2024-10-15T02:16:59Z",
    "authors" : [
      {
        "name" : "Bolun Sun"
      },
      {
        "name" : "Yifan Zhou"
      },
      {
        "name" : "Haiyun Jiang"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.11876v1",
    "title" : "Rescriber: Smaller-LLM-Powered User-Led Data Minimization for Navigating\n  Privacy Trade-offs in LLM-Based Conversational Agent",
    "summary" : "The proliferation of LLM-based conversational agents has resulted in\nexcessive disclosure of identifiable or sensitive information. However,\nexisting technologies fail to offer perceptible control or account for users'\npersonal preferences about privacy-utility tradeoffs due to the lack of user\ninvolvement. To bridge this gap, we designed, built, and evaluated Rescriber, a\nbrowser extension that supports user-led data minimization in LLM-based\nconversational agents by helping users detect and sanitize personal information\nin their prompts. Our studies (N=12) showed that Rescriber helped users reduce\nunnecessary disclosure and addressed their privacy concerns. Users' subjective\nperceptions of the system powered by Llama3-8B were on par with that by GPT-4.\nThe comprehensiveness and consistency of the detection and sanitization emerge\nas essential factors that affect users' trust and perceived protection. Our\nfindings confirm the viability of smaller-LLM-powered, user-facing, on-device\nprivacy controls, presenting a promising approach to address the privacy and\ntrust challenges of AI.",
    "updated" : "2024-10-10T01:23:16Z",
    "published" : "2024-10-10T01:23:16Z",
    "authors" : [
      {
        "name" : "Jijie Zhou"
      },
      {
        "name" : "Eryue Xu"
      },
      {
        "name" : "Yaoyao Wu"
      },
      {
        "name" : "Tianshi Li"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.13753v1",
    "title" : "DPFedBank: Crafting a Privacy-Preserving Federated Learning Framework\n  for Financial Institutions with Policy Pillars",
    "summary" : "In recent years, the financial sector has faced growing pressure to adopt\nadvanced machine learning models to derive valuable insights while preserving\ndata privacy. However, the highly sensitive nature of financial data presents\nsignificant challenges to sharing and collaboration. This paper presents\nDPFedBank, an innovative framework enabling financial institutions to\ncollaboratively develop machine learning models while ensuring robust data\nprivacy through Local Differential Privacy (LDP) mechanisms. DPFedBank is\ndesigned to address the unique privacy and security challenges associated with\nfinancial data, allowing institutions to share insights without exposing\nsensitive information. By leveraging LDP, the framework ensures that data\nremains confidential even during collaborative processes, providing a crucial\nsolution for privacy-aware machine learning in finance. We conducted an\nin-depth evaluation of the potential vulnerabilities within this framework and\ndeveloped a comprehensive set of policies aimed at mitigating these risks. The\nproposed policies effectively address threats posed by malicious clients,\ncompromised servers, inherent weaknesses in existing Differential\nPrivacy-Federated Learning (DP-FL) frameworks, and sophisticated external\nadversaries. Unlike existing DP-FL approaches, DPFedBank introduces a novel\ncombination of adaptive LDP mechanisms and advanced cryptographic techniques\nspecifically tailored for financial data, which significantly enhances privacy\nwhile maintaining model utility. Key security enhancements include the\nimplementation of advanced authentication protocols, encryption techniques for\nsecure data exchange, and continuous monitoring systems to detect and respond\nto malicious activities in real-time.",
    "updated" : "2024-10-17T16:51:56Z",
    "published" : "2024-10-17T16:51:56Z",
    "authors" : [
      {
        "name" : "Peilin He"
      },
      {
        "name" : "Chenkai Lin"
      },
      {
        "name" : "Isabella Montoya"
      }
    ],
    "categories" : [
      "cs.CE",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.13752v1",
    "title" : "Privacy-Preserving Decentralized AI with Confidential Computing",
    "summary" : "This paper addresses privacy protection in decentralized Artificial\nIntelligence (AI) using Confidential Computing (CC) within the Atoma Network, a\ndecentralized AI platform designed for the Web3 domain. Decentralized AI\ndistributes AI services among multiple entities without centralized oversight,\nfostering transparency and robustness. However, this structure introduces\nsignificant privacy challenges, as sensitive assets such as proprietary models\nand personal data may be exposed to untrusted participants. Cryptography-based\nprivacy protection techniques such as zero-knowledge machine learning (zkML)\nsuffers prohibitive computational overhead. To address the limitation, we\npropose leveraging Confidential Computing (CC). Confidential Computing\nleverages hardware-based Trusted Execution Environments (TEEs) to provide\nisolation for processing sensitive data, ensuring that both model parameters\nand user data remain secure, even in decentralized, potentially untrusted\nenvironments. While TEEs face a few limitations, we believe they can bridge the\nprivacy gap in decentralized AI. We explore how we can integrate TEEs into\nAtoma's decentralized framework.",
    "updated" : "2024-10-17T16:50:48Z",
    "published" : "2024-10-17T16:50:48Z",
    "authors" : [
      {
        "name" : "Dayeol Lee"
      },
      {
        "name" : "Jorge Antonio"
      },
      {
        "name" : "Hisham Khan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.13387v1",
    "title" : "CLEAR: Towards Contextual LLM-Empowered Privacy Policy Analysis and Risk\n  Generation for Large Language Model Applications",
    "summary" : "The rise of end-user applications powered by large language models (LLMs),\nincluding both conversational interfaces and add-ons to existing graphical user\ninterfaces (GUIs), introduces new privacy challenges. However, many users\nremain unaware of the risks. This paper explores methods to increase user\nawareness of privacy risks associated with LLMs in end-user applications. We\nconducted five co-design workshops to uncover user privacy concerns and their\ndemand for contextual privacy information within LLMs. Based on these insights,\nwe developed CLEAR (Contextual LLM-Empowered Privacy Policy Analysis and Risk\nGeneration), a just-in-time contextual assistant designed to help users\nidentify sensitive information, summarize relevant privacy policies, and\nhighlight potential risks when sharing information with LLMs. We evaluated the\nusability and usefulness of CLEAR across in two example domains: ChatGPT and\nthe Gemini plugin in Gmail. Our findings demonstrated that CLEAR is easy to use\nand improves user understanding of data practices and privacy risks. We also\ndiscussed LLM's duality in posing and mitigating privacy risks, offering design\nand policy implications.",
    "updated" : "2024-10-17T09:39:10Z",
    "published" : "2024-10-17T09:39:10Z",
    "authors" : [
      {
        "name" : "Chaoran Chen"
      },
      {
        "name" : "Daodao Zhou"
      },
      {
        "name" : "Yanfang Ye"
      },
      {
        "name" : "Yaxing Yao"
      },
      {
        "name" : "Toby Jia-jun Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.13221v1",
    "title" : "Investigating Effective Speaker Property Privacy Protection in Federated\n  Learning for Speech Emotion Recognition",
    "summary" : "Federated Learning (FL) is a privacy-preserving approach that allows servers\nto aggregate distributed models transmitted from local clients rather than\ntraining on user data. More recently, FL has been applied to Speech Emotion\nRecognition (SER) for secure human-computer interaction applications. Recent\nresearch has found that FL is still vulnerable to inference attacks. To this\nend, this paper focuses on investigating the security of FL for SER concerning\nproperty inference attacks. We propose a novel method to protect the property\ninformation in speech data by decomposing various properties in the sound and\nadding perturbations to these properties. Our experiments show that the\nproposed method offers better privacy-utility trade-offs than existing methods.\nThe trade-offs enable more effective attack prevention while maintaining\nsimilar FL utility levels. This work can guide future work on privacy\nprotection methods in speech processing.",
    "updated" : "2024-10-17T05:03:34Z",
    "published" : "2024-10-17T05:03:34Z",
    "authors" : [
      {
        "name" : "Chao Tan"
      },
      {
        "name" : "Sheng Li"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Zhao Ren"
      },
      {
        "name" : "Tanja Schultz"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.12926v1",
    "title" : "DEeR: Deviation Eliminating and Noise Regulating for Privacy-preserving\n  Federated Low-rank Adaptation",
    "summary" : "Integrating low-rank adaptation (LoRA) with federated learning (FL) has\nreceived widespread attention recently, aiming to adapt pretrained foundation\nmodels (FMs) to downstream medical tasks via privacy-preserving decentralized\ntraining. However, owing to the direct combination of LoRA and FL, current\nmethods generally undergo two problems, i.e., aggregation deviation, and\ndifferential privacy (DP) noise amplification effect. To address these\nproblems, we propose a novel privacy-preserving federated finetuning framework\ncalled \\underline{D}eviation \\underline{E}liminating and Nois\\underline{e}\n\\underline{R}egulating (DEeR). Specifically, we firstly theoretically prove\nthat the necessary condition to eliminate aggregation deviation is guaranteing\nthe equivalence between LoRA parameters of clients. Based on the theoretical\ninsight, a deviation eliminator is designed to utilize alternating minimization\nalgorithm to iteratively optimize the zero-initialized and non-zero-initialized\nparameter matrices of LoRA, ensuring that aggregation deviation always be zeros\nduring training. Furthermore, we also conduct an in-depth analysis of the noise\namplification effect and find that this problem is mainly caused by the\n``linear relationship'' between DP noise and LoRA parameters. To suppress the\nnoise amplification effect, we propose a noise regulator that exploits two\nregulator factors to decouple relationship between DP and LoRA, thereby\nachieving robust privacy protection and excellent finetuning performance.\nAdditionally, we perform comprehensive ablated experiments to verify the\neffectiveness of the deviation eliminator and noise regulator. DEeR shows\nbetter performance on public medical datasets in comparison with\nstate-of-the-art approaches. The code is available at\nhttps://github.com/CUHK-AIM-Group/DEeR.",
    "updated" : "2024-10-16T18:11:52Z",
    "published" : "2024-10-16T18:11:52Z",
    "authors" : [
      {
        "name" : "Meilu Zhu"
      },
      {
        "name" : "Axiu Mao"
      },
      {
        "name" : "Jun Liu"
      },
      {
        "name" : "Yixuan Yuan"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.07900v2",
    "title" : "CL3: A Collaborative Learning Framework for the Medical Data Ensuring\n  Data Privacy in the Hyperconnected Environment",
    "summary" : "In a hyperconnected environment, medical institutions are particularly\nconcerned with data privacy when sharing and transmitting sensitive patient\ninformation due to the risk of data breaches, where malicious actors could\nintercept sensitive information. A collaborative learning framework, including\ntransfer, federated, and incremental learning, can generate efficient, secure,\nand scalable models while requiring less computation, maintaining patient data\nprivacy, and ensuring an up-to-date model. This study aims to address the\ndetection of COVID-19 using chest X-ray images through a proposed collaborative\nlearning framework called CL3. Initially, transfer learning is employed,\nleveraging knowledge from a pre-trained model as the starting global model.\nLocal models from different medical institutes are then integrated, and a new\nglobal model is constructed to adapt to any data drift observed in the local\nmodels. Additionally, incremental learning is considered, allowing continuous\nadaptation to new medical data without forgetting previously learned\ninformation. Experimental results demonstrate that the CL3 framework achieved a\nglobal accuracy of 89.99% when using Xception with a batch size of 16 after\nbeing trained for six federated communication rounds. A demo of the CL3\nframework is available at\nhttps://github.com/zavidparvez/CL3-Collaborative-Approach to ensure\nreproducibility.",
    "updated" : "2024-10-17T11:33:40Z",
    "published" : "2024-10-10T13:29:12Z",
    "authors" : [
      {
        "name" : "Mohamamd Zavid Parvez"
      },
      {
        "name" : "Rafiqul Islam"
      },
      {
        "name" : "Md Zahidul Islam"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.14607v1",
    "title" : "Evaluating Privacy Measures in Healthcare Apps Predominantly Used by\n  Older Adults",
    "summary" : "The widespread adoption of telehealth systems has led to a significant\nincrease in the use of healthcare apps among older adults, but this rapid\ngrowth has also heightened concerns about the privacy of their health\ninformation. While HIPAA in the US and GDPR in the EU establish essential\nprivacy protections for health information, limited research exists on the\neffectiveness of healthcare app privacy policies, particularly those used\npredominantly by older adults. To address this, we evaluated 28 healthcare apps\nacross multiple dimensions, including regulatory compliance, data handling\npractices, and privacy-focused usability. To do this, we created a Privacy Risk\nAssessment Framework (PRAF) and used it to evaluate the privacy risks\nassociated with these healthcare apps designed for older adults. Our analysis\nrevealed significant gaps in compliance with privacy standards to such, only\n25% of apps explicitly state compliance with HIPAA, and only 18% mention GDPR.\nSurprisingly, 79% of these applications lack breach protocols, putting older\nadults at risk in the event of a data breach.",
    "updated" : "2024-10-18T17:01:14Z",
    "published" : "2024-10-18T17:01:14Z",
    "authors" : [
      {
        "name" : "Saka Suleiman"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.14023v1",
    "title" : "Identifying Privacy Personas",
    "summary" : "Privacy personas capture the differences in user segments with respect to\none's knowledge, behavioural patterns, level of self-efficacy, and perception\nof the importance of privacy protection. Modelling these differences is\nessential for appropriately choosing personalised communication about privacy\n(e.g. to increase literacy) and for defining suitable choices for privacy\nenhancing technologies (PETs). While various privacy personas have been derived\nin the literature, they group together people who differ from each other in\nterms of important attributes such as perceived or desired level of control,\nand motivation to use PET. To address this lack of granularity and\ncomprehensiveness in describing personas, we propose eight personas that we\nderive by combining qualitative and quantitative analysis of the responses to\nan interactive educational questionnaire. We design an analysis pipeline that\nuses divisive hierarchical clustering and Boschloo's statistical test of\nhomogeneity of proportions to ensure that the elicited clusters differ from\neach other based on a statistical measure. Additionally, we propose a new\nmeasure for calculating distances between questionnaire responses, that\naccounts for the type of the question (closed- vs open-ended) used to derive\ntraits. We show that the proposed privacy personas statistically differ from\neach other. We statistically validate the proposed personas and also compare\nthem with personas in the literature, showing that they provide a more granular\nand comprehensive understanding of user segments, which will allow to better\nassist users with their privacy needs.",
    "updated" : "2024-10-17T20:49:46Z",
    "published" : "2024-10-17T20:49:46Z",
    "authors" : [
      {
        "name" : "Olena Hrynenko"
      },
      {
        "name" : "Andrea Cavallaro"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.13752v2",
    "title" : "Privacy-Preserving Decentralized AI with Confidential Computing",
    "summary" : "This paper addresses privacy protection in decentralized Artificial\nIntelligence (AI) using Confidential Computing (CC) within the Atoma Network, a\ndecentralized AI platform designed for the Web3 domain. Decentralized AI\ndistributes AI services among multiple entities without centralized oversight,\nfostering transparency and robustness. However, this structure introduces\nsignificant privacy challenges, as sensitive assets such as proprietary models\nand personal data may be exposed to untrusted participants. Cryptography-based\nprivacy protection techniques such as zero-knowledge machine learning (zkML)\nsuffers prohibitive computational overhead. To address the limitation, we\npropose leveraging Confidential Computing (CC). Confidential Computing\nleverages hardware-based Trusted Execution Environments (TEEs) to provide\nisolation for processing sensitive data, ensuring that both model parameters\nand user data remain secure, even in decentralized, potentially untrusted\nenvironments. While TEEs face a few limitations, we believe they can bridge the\nprivacy gap in decentralized AI. We explore how we can integrate TEEs into\nAtoma's decentralized framework.",
    "updated" : "2024-10-18T16:33:05Z",
    "published" : "2024-10-17T16:50:48Z",
    "authors" : [
      {
        "name" : "Dayeol Lee"
      },
      {
        "name" : "Jorge António"
      },
      {
        "name" : "Hisham Khan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.13905v1",
    "title" : "P4GCN: Vertical Federated Social Recommendation with Privacy-Preserving\n  Two-Party Graph Convolution Networks",
    "summary" : "In recent years, graph neural networks (GNNs) have been commonly utilized for\nsocial recommendation systems. However, real-world scenarios often present\nchallenges related to user privacy and business constraints, inhibiting direct\naccess to valuable social information from other platforms. While many existing\nmethods have tackled matrix factorization-based social recommendations without\ndirect social data access, developing GNN-based federated social recommendation\nmodels under similar conditions remains largely unexplored. To address this\nissue, we propose a novel vertical federated social recommendation method\nleveraging privacy-preserving two-party graph convolution networks (P4GCN) to\nenhance recommendation accuracy without requiring direct access to sensitive\nsocial information. First, we introduce a Sandwich-Encryption module to ensure\ncomprehensive data privacy during the collaborative computing process. Second,\nwe provide a thorough theoretical analysis of the privacy guarantees,\nconsidering the participation of both curious and honest parties. Extensive\nexperiments on four real-world datasets demonstrate that P4GCN outperforms\nstate-of-the-art methods in terms of recommendation accuracy. The code is\navailable at https://github.com/WwZzz/P4GCN.",
    "updated" : "2024-10-16T12:29:22Z",
    "published" : "2024-10-16T12:29:22Z",
    "authors" : [
      {
        "name" : "Zheng Wang"
      },
      {
        "name" : "Wanwan Wang"
      },
      {
        "name" : "Yimin Huang"
      },
      {
        "name" : "Zhaopeng Peng"
      },
      {
        "name" : "Ziqi Yang"
      },
      {
        "name" : "Cheng Wang"
      },
      {
        "name" : "Xiaoliang Fan"
      }
    ],
    "categories" : [
      "cs.SI",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.16137v1",
    "title" : "Privacy as Social Norm: Systematically Reducing Dysfunctional Privacy\n  Concerns on Social Media",
    "summary" : "Privacy is essential to fully enjoying the benefits of social media. While\nfear around privacy risks can sometimes motivate privacy management, the\nnegative impact of such fear, particularly when it is perceived as\nunaddressable (i.e., \"dysfunctional\" fear), can significantly harm teen\nwell-being. In a co-design study with 136 participants aged 13-18, we explored\nhow teens can protect their privacy without experiencing heightened fear. We\nidentified seven different sources of dysfunctional fear, such as `fear of a\nhostile environment' and `fear of overstepping privacy norms.' We also\nevaluated ten designs, co-created with teen participants, that address these\nfears. Our findings suggest that social media platforms can mitigate\ndysfunctional fear without compromising privacy by creating a culture where\nprivacy protection is the norm through default privacy-protective features.\nHowever, we also found that even the most effective privacy features are not\nlikely to be adopted unless they balance the multifaceted and diverse needs of\nteens. Individual teens have different needs -- for example, public and private\naccount users have different needs -- and teens often want to enjoy the\nbenefits they get from slightly reducing privacy and widening their social\nreach. Given these considerations, augmenting default privacy features by\nallowing them to be toggled on and off will allow individual users to choose\ntheir own balance while still maintaining a privacy-focused norm.",
    "updated" : "2024-10-21T16:03:18Z",
    "published" : "2024-10-21T16:03:18Z",
    "authors" : [
      {
        "name" : "JaeWon Kim"
      },
      {
        "name" : "Soobin Cho"
      },
      {
        "name" : "Robert Wolfe"
      },
      {
        "name" : "Jishnu Hari Nair"
      },
      {
        "name" : "Alexis Hiniker"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.15954v1",
    "title" : "TS-ACL: A Time Series Analytic Continual Learning Framework for\n  Privacy-Preserving and Class-Incremental Pattern Recognition",
    "summary" : "Class-incremental Learning (CIL) in Time Series Classification (TSC) aims to\nincrementally train models using the streaming time series data that arrives\ncontinuously. The main problem in this scenario is catastrophic forgetting,\ni.e., training models with new samples inevitably leads to the forgetting of\npreviously learned knowledge. Among existing methods, the replay-based methods\nachieve satisfactory performance but compromise privacy, while exemplar-free\nmethods protect privacy but suffer from low accuracy. However, more critically,\nowing to their reliance on gradient-based update techniques, these existing\nmethods fundamentally cannot solve the catastrophic forgetting problem. In TSC\nscenarios with continuously arriving data and temporally shifting\ndistributions, these methods become even less practical. In this paper, we\npropose a Time Series Analytic Continual Learning framework, called TS-ACL.\nInspired by analytical learning, TS-ACL transforms neural network updates into\ngradient-free linear regression problems, thereby fundamentally mitigating\ncatastrophic forgetting. Specifically, employing a pre-trained and frozen\nfeature extraction encoder, TS-ACL only needs to update its analytic classifier\nrecursively in a lightweight manner that is highly suitable for real-time\napplications and large-scale data processing. Additionally, we theoretically\ndemonstrate that the model obtained recursively through the TS-ACL is exactly\nequivalent to a model trained on the complete dataset in a centralized manner,\nthereby establishing the property of absolute knowledge memory. Extensive\nexperiments validate the superior performance of our TS-ACL.",
    "updated" : "2024-10-21T12:34:02Z",
    "published" : "2024-10-21T12:34:02Z",
    "authors" : [
      {
        "name" : "Kejia Fan"
      },
      {
        "name" : "Jiaxu Li"
      },
      {
        "name" : "Songning Lai"
      },
      {
        "name" : "Linpu Lv"
      },
      {
        "name" : "Anfeng Liu"
      },
      {
        "name" : "Jianheng Tang"
      },
      {
        "name" : "Houbing Herbert Song"
      },
      {
        "name" : "Huiping Zhuang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.15942v1",
    "title" : "A Low-Cost Privacy-Preserving Digital Wallet for Humanitarian Aid\n  Distribution",
    "summary" : "Humanitarian organizations distribute aid to people affected by armed\nconflicts or natural disasters. Digitalization has the potential to increase\nthe efficiency and fairness of aid-distribution systems, and recent work by\nWang et al. has shown that these benefits are possible without creating privacy\nharms for aid recipients. However, their work only provides a solution for one\nparticular aid-distribution scenario in which aid recipients receive a\npre-defined set of goods. Yet, in many situations it is desirable to enable\nrecipients to decide which items they need at each moment to satisfy their\nspecific needs. We formalize these needs into functional, deployment, security,\nand privacy requirements, and design a privacy-preserving digital wallet for\naid distribution. Our smart-card-based solution enables aid recipients to spend\na pre-defined budget at different vendors to obtain the items that they need.\nWe prove our solution's security and privacy properties, and show it is\npractical at scale.",
    "updated" : "2024-10-21T12:15:03Z",
    "published" : "2024-10-21T12:15:03Z",
    "authors" : [
      {
        "name" : "Eva Luvison"
      },
      {
        "name" : "Sylvain Chatel"
      },
      {
        "name" : "Justinas Sukaitis"
      },
      {
        "name" : "Vincent Graf Narbel"
      },
      {
        "name" : "Carmela Troncoso"
      },
      {
        "name" : "Wouter Lueks"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.15386v1",
    "title" : "Formalization of Differential Privacy in Isabelle/HOL",
    "summary" : "Differential privacy is a statistical definition of privacy that has\nattracted the interest of both academia and industry. Its formulations are easy\nto understand, but the differential privacy of databases is complicated to\ndetermine. One of the reasons for this is that small changes in database\nprograms can break their differential privacy. Therefore, formal verification\nof differential privacy has been studied for over a decade.\n  In this paper, we propose an Isabelle/HOL library for formalizing\ndifferential privacy in a general setting. To our knowledge, it is the first\nformalization of differential privacy that supports continuous probability\ndistributions. First, we formalize the standard definition of differential\nprivacy and its basic properties. Second, we formalize the Laplace mechanism\nand its differential privacy. Finally, we formalize the differential privacy of\nthe report noisy max mechanism.",
    "updated" : "2024-10-20T13:06:13Z",
    "published" : "2024-10-20T13:06:13Z",
    "authors" : [
      {
        "name" : "Tetsuya Sato"
      },
      {
        "name" : "Yasuhiko Minamide"
      }
    ],
    "categories" : [
      "cs.PL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.15369v1",
    "title" : "Ethical AI in Retail: Consumer Privacy and Fairness",
    "summary" : "The adoption of artificial intelligence (AI) in retail has significantly\ntransformed the industry, enabling more personalized services and efficient\noperations. However, the rapid implementation of AI technologies raises ethical\nconcerns, particularly regarding consumer privacy and fairness. This study aims\nto analyze the ethical challenges of AI applications in retail, explore ways\nretailers can implement AI technologies ethically while remaining competitive,\nand provide recommendations on ethical AI practices. A descriptive survey\ndesign was used to collect data from 300 respondents across major e-commerce\nplatforms. Data were analyzed using descriptive statistics, including\npercentages and mean scores. Findings shows a high level of concerns among\nconsumers regarding the amount of personal data collected by AI-driven retail\napplications, with many expressing a lack of trust in how their data is\nmanaged. Also, fairness is another major issue, as a majority believe AI\nsystems do not treat consumers equally, raising concerns about algorithmic\nbias. It was also found that AI can enhance business competitiveness and\nefficiency without compromising ethical principles, such as data privacy and\nfairness. Data privacy and transparency were highlighted as critical areas\nwhere retailers need to focus their efforts, indicating a strong demand for\nstricter data protection protocols and ongoing scrutiny of AI systems. The\nstudy concludes that retailers must prioritize transparency, fairness, and data\nprotection when deploying AI systems. The study recommends ensuring\ntransparency in AI processes, conducting regular audits to address biases,\nincorporating consumer feedback in AI development, and emphasizing consumer\ndata privacy.",
    "updated" : "2024-10-20T12:00:14Z",
    "published" : "2024-10-20T12:00:14Z",
    "authors" : [
      {
        "name" : "Anthonette Adanyin"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.15044v1",
    "title" : "Adanonymizer: Interactively Navigating and Balancing the Duality of\n  Privacy and Output Performance in Human-LLM Interaction",
    "summary" : "Current Large Language Models (LLMs) cannot support users to precisely\nbalance privacy protection and output performance during individual\nconsultations. We introduce Adanonymizer, an anonymization plug-in that allows\nusers to control this balance by navigating a trade-off curve. A survey (N=221)\nrevealed a privacy paradox, where users frequently disclosed sensitive\ninformation despite acknowledging privacy risks. The study further demonstrated\nthat privacy risks were not significantly correlated with model output\nperformance, highlighting the potential to navigate this trade-off.\nAdanonymizer normalizes privacy and utility ratings by type and automates the\npseudonymization of sensitive terms based on user preferences, significantly\nreducing user effort. Its 2D color palette interface visualizes the\nprivacy-utility trade-off, allowing users to adjust the balance by manipulating\na point. An evaluation (N=36) compared Adanonymizer with ablation methods and\ndifferential privacy techniques, where Adanonymizer significantly reduced\nmodification time, achieved better perceived model performance and overall user\npreference.",
    "updated" : "2024-10-19T09:04:01Z",
    "published" : "2024-10-19T09:04:01Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Haobin Xing"
      },
      {
        "name" : "Lyumanshan Ye"
      },
      {
        "name" : "Yongquan Hu"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.14960v1",
    "title" : "Dual-Technique Privacy & Security Analysis for E-Commerce Websites\n  Through Automated and Manual Implementation",
    "summary" : "As e-commerce continues to expand, the urgency for stronger privacy and\nsecurity measures becomes increasingly critical, particularly on platforms\nfrequented by younger users who are often less aware of potential risks. In our\nanalysis of 90 US-based e-commerce websites, we employed a dual-technique\napproach, combining automated tools with manual evaluations. Tools like\nCookieServe and PrivacyCheck revealed that 38.5% of the websites deployed over\n50 cookies per session, many of which were categorized as unnecessary or\nunclear in function, posing significant risks to users' Personally Identifiable\nInformation (PII). Our manual assessment further uncovered critical gaps in\nstandard security practices, including the absence of mandatory multi-factor\nauthentication (MFA) and breach notification protocols. Additionally, we\nobserved inadequate input validation, which compromises the integrity of user\ndata and transactions. Based on these findings, we recommend targeted\nimprovements to privacy policies, enhanced transparency in cookie usage, and\nthe implementation of stronger authentication protocols. These measures are\nessential for ensuring compliance with CCPA and COPPA, thereby fostering more\nsecure online environments, particularly for younger users.",
    "updated" : "2024-10-19T03:25:48Z",
    "published" : "2024-10-19T03:25:48Z",
    "authors" : [
      {
        "name" : "Urvashi Kishnani"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.14931v1",
    "title" : "\"Ghost of the past\": identifying and resolving privacy leakage from\n  LLM's memory through proactive user interaction",
    "summary" : "Memories, encompassing past inputs in context window and retrieval-augmented\ngeneration (RAG), frequently surface during human-LLM interactions, yet users\nare often unaware of their presence and the associated privacy risks. To\naddress this, we propose MemoAnalyzer, a system for identifying, visualizing,\nand managing private information within memories. A semi-structured interview\n(N=40) revealed that low privacy awareness was the primary challenge, while\nproactive privacy control emerged as the most common user need. MemoAnalyzer\nuses a prompt-based method to infer and identify sensitive information from\naggregated past inputs, allowing users to easily modify sensitive content.\nBackground color temperature and transparency are mapped to inference\nconfidence and sensitivity, streamlining privacy adjustments. A 5-day\nevaluation (N=36) comparing MemoAnalyzer with the default GPT setting and a\nmanual modification baseline showed MemoAnalyzer significantly improved privacy\nawareness and protection without compromising interaction speed. Our study\ncontributes to privacy-conscious LLM design, offering insights into privacy\nprotection for Human-AI interactions.",
    "updated" : "2024-10-19T01:35:26Z",
    "published" : "2024-10-19T01:35:26Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Lyumanshan Ye"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Jingyu Tang"
      },
      {
        "name" : "Bo Shui"
      },
      {
        "name" : "Haobin Xing"
      },
      {
        "name" : "Pengfei Liu"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.14787v1",
    "title" : "Privacy for Free in the Over-Parameterized Regime",
    "summary" : "Differentially private gradient descent (DP-GD) is a popular algorithm to\ntrain deep learning models with provable guarantees on the privacy of the\ntraining data. In the last decade, the problem of understanding its performance\ncost with respect to standard GD has received remarkable attention from the\nresearch community, which formally derived upper bounds on the excess\npopulation risk $R_{P}$ in different learning settings. However, existing\nbounds typically degrade with over-parameterization, i.e., as the number of\nparameters $p$ gets larger than the number of training samples $n$ -- a regime\nwhich is ubiquitous in current deep-learning practice. As a result, the lack of\ntheoretical insights leaves practitioners without clear guidance, leading some\nto reduce the effective number of trainable parameters to improve performance,\nwhile others use larger models to achieve better results through scale. In this\nwork, we show that in the popular random features model with quadratic loss,\nfor any sufficiently large $p$, privacy can be obtained for free, i.e.,\n$\\left|R_{P} \\right| = o(1)$, not only when the privacy parameter $\\varepsilon$\nhas constant order, but also in the strongly private setting $\\varepsilon =\no(1)$. This challenges the common wisdom that over-parameterization inherently\nhinders performance in private learning.",
    "updated" : "2024-10-18T18:01:11Z",
    "published" : "2024-10-18T18:01:11Z",
    "authors" : [
      {
        "name" : "Simone Bombari"
      },
      {
        "name" : "Marco Mondelli"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.17127v1",
    "title" : "PAPILLON: PrivAcy Preservation from Internet-based and Local Language\n  MOdel ENsembles",
    "summary" : "Users can divulge sensitive information to proprietary LLM providers, raising\nsignificant privacy concerns. While open-source models, hosted locally on the\nuser's machine, alleviate some concerns, models that users can host locally are\noften less capable than proprietary frontier models. Toward preserving user\nprivacy while retaining the best quality, we propose Privacy-Conscious\nDelegation, a novel task for chaining API-based and local models. We utilize\nrecent public collections of user-LLM interactions to construct a natural\nbenchmark called PUPA, which contains personally identifiable information\n(PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM\npipeline that uses prompt optimization to address a simpler version of our\ntask. Our best pipeline maintains high response quality for 85.5% of user\nqueries while restricting privacy leakage to only 7.5%. We still leave a large\nmargin to the generation quality of proprietary LLMs for future work. Our data\nand code will be available at https://github.com/siyan-sylvia-li/PAPILLON.",
    "updated" : "2024-10-22T16:00:26Z",
    "published" : "2024-10-22T16:00:26Z",
    "authors" : [
      {
        "name" : "Li Siyan"
      },
      {
        "name" : "Vethavikashini Chithrra Raghuram"
      },
      {
        "name" : "Omar Khattab"
      },
      {
        "name" : "Julia Hirschberg"
      },
      {
        "name" : "Zhou Yu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.17098v1",
    "title" : "Masked Differential Privacy",
    "summary" : "Privacy-preserving computer vision is an important emerging problem in\nmachine learning and artificial intelligence. The prevalent methods tackling\nthis problem use differential privacy or anonymization and obfuscation\ntechniques to protect the privacy of individuals. In both cases, the utility of\nthe trained model is sacrificed heavily in this process. In this work, we\npropose an effective approach called masked differential privacy (MaskDP),\nwhich allows for controlling sensitive regions where differential privacy is\napplied, in contrast to applying DP on the entire input. Our method operates\nselectively on the data and allows for defining non-sensitive spatio-temporal\nregions without DP application or combining differential privacy with other\nprivacy techniques within data samples. Experiments on four challenging action\nrecognition datasets demonstrate that our proposed techniques result in better\nutility-privacy trade-offs compared to standard differentially private training\nin the especially demanding $\\epsilon<1$ regime.",
    "updated" : "2024-10-22T15:22:53Z",
    "published" : "2024-10-22T15:22:53Z",
    "authors" : [
      {
        "name" : "David Schneider"
      },
      {
        "name" : "Sina Sajadmanesh"
      },
      {
        "name" : "Vikash Sehwag"
      },
      {
        "name" : "Saquib Sarfraz"
      },
      {
        "name" : "Rainer Stiefelhagen"
      },
      {
        "name" : "Lingjuan Lyu"
      },
      {
        "name" : "Vivek Sharma"
      }
    ],
    "categories" : [
      "cs.CV",
      "68T45",
      "I.4.m"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.16975v1",
    "title" : "Publishing Neural Networks in Drug Discovery Might Compromise Training\n  Data Privacy",
    "summary" : "This study investigates the risks of exposing confidential chemical\nstructures when machine learning models trained on these structures are made\npublicly available. We use membership inference attacks, a common method to\nassess privacy that is largely unexplored in the context of drug discovery, to\nexamine neural networks for molecular property prediction in a black-box\nsetting. Our results reveal significant privacy risks across all evaluated\ndatasets and neural network architectures. Combining multiple attacks increases\nthese risks. Molecules from minority classes, often the most valuable in drug\ndiscovery, are particularly vulnerable. We also found that representing\nmolecules as graphs and using message-passing neural networks may mitigate\nthese risks. We provide a framework to assess privacy risks of classification\nmodels and molecular representations. Our findings highlight the need for\ncareful consideration when sharing neural networks trained on proprietary\nchemical structures, informing organisations and researchers about the\ntrade-offs between data confidentiality and model openness.",
    "updated" : "2024-10-22T12:55:02Z",
    "published" : "2024-10-22T12:55:02Z",
    "authors" : [
      {
        "name" : "Fabian P. Krüger"
      },
      {
        "name" : "Johan Östman"
      },
      {
        "name" : "Lewis Mervin"
      },
      {
        "name" : "Igor V. Tetko"
      },
      {
        "name" : "Ola Engkvist"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.16705v1",
    "title" : "Privacy-hardened and hallucination-resistant synthetic data generation\n  with logic-solvers",
    "summary" : "Machine-generated data is a valuable resource for training Artificial\nIntelligence algorithms, evaluating rare workflows, and sharing data under\nstricter data legislations. The challenge is to generate data that is accurate\nand private. Current statistical and deep learning methods struggle with large\ndata volumes, are prone to hallucinating scenarios incompatible with reality,\nand seldom quantify privacy meaningfully. Here we introduce Genomator, a logic\nsolving approach (SAT solving), which efficiently produces private and\nrealistic representations of the original data. We demonstrate the method on\ngenomic data, which arguably is the most complex and private information.\nSynthetic genomes hold great potential for balancing underrepresented\npopulations in medical research and advancing global data exchange. We\nbenchmark Genomator against state-of-the-art methodologies (Markov generation,\nRestricted Boltzmann Machine, Generative Adversarial Network and Conditional\nRestricted Boltzmann Machines), demonstrating an 84-93% accuracy improvement\nand 95-98% higher privacy. Genomator is also 1000-1600 times more efficient,\nmaking it the only tested method that scales to whole genomes. We show the\nuniversal trade-off between privacy and accuracy, and use Genomator's tuning\ncapability to cater to all applications along the spectrum, from provable\nprivate representations of sensitive cohorts, to datasets with\nindistinguishable pharmacogenomic profiles. Demonstrating the production-scale\ngeneration of tuneable synthetic data can increase trust and pave the way into\nthe clinic.",
    "updated" : "2024-10-22T05:20:21Z",
    "published" : "2024-10-22T05:20:21Z",
    "authors" : [
      {
        "name" : "Mark A. Burgess"
      },
      {
        "name" : "Brendan Hosking"
      },
      {
        "name" : "Roc Reguant"
      },
      {
        "name" : "Anubhav Kaphle"
      },
      {
        "name" : "Mitchell J. O'Brien"
      },
      {
        "name" : "Letitia M. F. Sng"
      },
      {
        "name" : "Yatish Jain"
      },
      {
        "name" : "Denis C. Bauer"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.16672v1",
    "title" : "DEAN: Deactivating the Coupled Neurons to Mitigate Fairness-Privacy\n  Conflicts in Large Language Models",
    "summary" : "Ensuring awareness of fairness and privacy in Large Language Models (LLMs) is\ncritical. Interestingly, we discover a counter-intuitive trade-off phenomenon\nthat enhancing an LLM's privacy awareness through Supervised Fine-Tuning (SFT)\nmethods significantly decreases its fairness awareness with thousands of\nsamples. To address this issue, inspired by the information theory, we\nintroduce a training-free method to \\textbf{DEA}ctivate the fairness and\nprivacy coupled \\textbf{N}eurons (\\textbf{DEAN}), which theoretically and\nempirically decrease the mutual information between fairness and privacy\nawareness. Extensive experimental results demonstrate that DEAN eliminates the\ntrade-off phenomenon and significantly improves LLMs' fairness and privacy\nawareness simultaneously, \\eg improving Qwen-2-7B-Instruct's fairness awareness\nby 12.2\\% and privacy awareness by 14.0\\%. More crucially, DEAN remains robust\nand effective with limited annotated data or even when only malicious\nfine-tuning data is available, whereas SFT methods may fail to perform properly\nin such scenarios. We hope this study provides valuable insights into\nconcurrently addressing fairness and privacy concerns in LLMs and can be\nintegrated into comprehensive frameworks to develop more ethical and\nresponsible AI systems. Our code is available at\n\\url{https://github.com/ChnQ/DEAN}.",
    "updated" : "2024-10-22T04:08:27Z",
    "published" : "2024-10-22T04:08:27Z",
    "authors" : [
      {
        "name" : "Chen Qian"
      },
      {
        "name" : "Dongrui Liu"
      },
      {
        "name" : "Jie Zhang"
      },
      {
        "name" : "Yong Liu"
      },
      {
        "name" : "Jing Shao"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.16423v1",
    "title" : "Position: Challenges and Opportunities for Differential Privacy in the\n  U.S. Federal Government",
    "summary" : "In this article, we seek to elucidate challenges and opportunities for\ndifferential privacy within the federal government setting, as seen by a team\nof differential privacy researchers, privacy lawyers, and data scientists\nworking closely with the U.S. government. After introducing differential\nprivacy, we highlight three significant challenges which currently restrict the\nuse of differential privacy in the U.S. government. We then provide two\nexamples where differential privacy can enhance the capabilities of government\nagencies. The first example highlights how the quantitative nature of\ndifferential privacy allows policy security officers to release multiple\nversions of analyses with different levels of privacy. The second example,\nwhich we believe is a novel realization, indicates that differential privacy\ncan be used to improve staffing efficiency in classified applications. We hope\nthat this article can serve as a nontechnical resource which can help frame\nfuture action from the differential privacy community, privacy regulators,\nsecurity officers, and lawmakers.",
    "updated" : "2024-10-21T18:46:05Z",
    "published" : "2024-10-21T18:46:05Z",
    "authors" : [
      {
        "name" : "Amol Khanna"
      },
      {
        "name" : "Adam McCormick"
      },
      {
        "name" : "Andre Nguyen"
      },
      {
        "name" : "Chris Aguirre"
      },
      {
        "name" : "Edward Raff"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.16410v1",
    "title" : "Subword Embedding from Bytes Gains Privacy without Sacrificing Accuracy\n  and Complexity",
    "summary" : "While NLP models significantly impact our lives, there are rising concerns\nabout privacy invasion. Although federated learning enhances privacy, attackers\nmay recover private training data by exploiting model parameters and gradients.\nTherefore, protecting against such embedding attacks remains an open challenge.\nTo address this, we propose Subword Embedding from Bytes (SEB) and encode\nsubwords to byte sequences using deep neural networks, making input text\nrecovery harder. Importantly, our method requires a smaller memory with $256$\nbytes of vocabulary while keeping efficiency with the same input length. Thus,\nour solution outperforms conventional approaches by preserving privacy without\nsacrificing efficiency or accuracy. Our experiments show SEB can effectively\nprotect against embedding-based attacks from recovering original sentences in\nfederated learning. Meanwhile, we verify that SEB obtains comparable and even\nbetter results over standard subword embedding methods in machine translation,\nsentiment analysis, and language modeling with even lower time and space\ncomplexity.",
    "updated" : "2024-10-21T18:25:24Z",
    "published" : "2024-10-21T18:25:24Z",
    "authors" : [
      {
        "name" : "Mengjiao Zhang"
      },
      {
        "name" : "Jia Xu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.13387v2",
    "title" : "CLEAR: Towards Contextual LLM-Empowered Privacy Policy Analysis and Risk\n  Generation for Large Language Model Applications",
    "summary" : "The rise of end-user applications powered by large language models (LLMs),\nincluding both conversational interfaces and add-ons to existing graphical user\ninterfaces (GUIs), introduces new privacy challenges. However, many users\nremain unaware of the risks. This paper explores methods to increase user\nawareness of privacy risks associated with LLMs in end-user applications. We\nconducted five co-design workshops to uncover user privacy concerns and their\ndemand for contextual privacy information within LLMs. Based on these insights,\nwe developed CLEAR (Contextual LLM-Empowered Privacy Policy Analysis and Risk\nGeneration), a just-in-time contextual assistant designed to help users\nidentify sensitive information, summarize relevant privacy policies, and\nhighlight potential risks when sharing information with LLMs. We evaluated the\nusability and usefulness of CLEAR across in two example domains: ChatGPT and\nthe Gemini plugin in Gmail. Our findings demonstrated that CLEAR is easy to use\nand improves user understanding of data practices and privacy risks. We also\ndiscussed LLM's duality in posing and mitigating privacy risks, offering design\nand policy implications.",
    "updated" : "2024-10-22T15:17:08Z",
    "published" : "2024-10-17T09:39:10Z",
    "authors" : [
      {
        "name" : "Chaoran Chen"
      },
      {
        "name" : "Daodao Zhou"
      },
      {
        "name" : "Yanfang Ye"
      },
      {
        "name" : "Toby Jia-jun Li"
      },
      {
        "name" : "Yaxing Yao"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.17468v1",
    "title" : "Formal Privacy Guarantees with Invariant Statistics",
    "summary" : "Motivated by the 2020 US Census products, this paper extends differential\nprivacy (DP) to address the joint release of DP outputs and nonprivate\nstatistics, referred to as invariant. Our framework, Semi-DP, redefines\nadjacency by focusing on datasets that conform to the given invariant, ensuring\nindistinguishability between adjacent datasets within invariant-conforming\ndatasets. We further develop customized mechanisms that satisfy Semi-DP,\nincluding the Gaussian mechanism and the optimal $K$-norm mechanism for\nrank-deficient sensitivity spaces. Our framework is applied to contingency\ntable analysis which is relevant to the 2020 US Census, illustrating how\nSemi-DP enables the release of private outputs given the one-way margins as the\ninvariant. Additionally, we provide a privacy analysis of the 2020 US Decennial\nCensus using the Semi-DP framework, revealing that the effective privacy\nguarantees are weaker than advertised.",
    "updated" : "2024-10-22T22:50:17Z",
    "published" : "2024-10-22T22:50:17Z",
    "authors" : [
      {
        "name" : "Young Hyun Cho"
      },
      {
        "name" : "Jordan Awan"
      }
    ],
    "categories" : [
      "cs.CR",
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.17459v1",
    "title" : "Data Obfuscation through Latent Space Projection (LSP) for\n  Privacy-Preserving AI Governance: Case Studies in Medical Diagnosis and\n  Finance Fraud Detection",
    "summary" : "As AI systems increasingly integrate into critical societal sectors, the\ndemand for robust privacy-preserving methods has escalated. This paper\nintroduces Data Obfuscation through Latent Space Projection (LSP), a novel\ntechnique aimed at enhancing AI governance and ensuring Responsible AI\ncompliance. LSP uses machine learning to project sensitive data into a latent\nspace, effectively obfuscating it while preserving essential features for model\ntraining and inference. Unlike traditional privacy methods like differential\nprivacy or homomorphic encryption, LSP transforms data into an abstract,\nlower-dimensional form, achieving a delicate balance between data utility and\nprivacy. Leveraging autoencoders and adversarial training, LSP separates\nsensitive from non-sensitive information, allowing for precise control over\nprivacy-utility trade-offs. We validate LSP's effectiveness through experiments\non benchmark datasets and two real-world case studies: healthcare cancer\ndiagnosis and financial fraud analysis. Our results show LSP achieves high\nperformance (98.7% accuracy in image classification) while providing strong\nprivacy (97.3% protection against sensitive attribute inference), outperforming\ntraditional anonymization and privacy-preserving methods. The paper also\nexamines LSP's alignment with global AI governance frameworks, such as GDPR,\nCCPA, and HIPAA, highlighting its contribution to fairness, transparency, and\naccountability. By embedding privacy within the machine learning pipeline, LSP\noffers a promising approach to developing AI systems that respect privacy while\ndelivering valuable insights. We conclude by discussing future research\ndirections, including theoretical privacy guarantees, integration with\nfederated learning, and enhancing latent space interpretability, positioning\nLSP as a critical tool for ethical AI advancement.",
    "updated" : "2024-10-22T22:31:03Z",
    "published" : "2024-10-22T22:31:03Z",
    "authors" : [
      {
        "name" : "Mahesh Vaijainthymala Krishnamoorthy"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "F.2.1; E.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.17353v1",
    "title" : "Preserving Privacy in Cloud-based Data-Driven Stabilization",
    "summary" : "In the recent years, we have observed three significant trends in control\nsystems: a renewed interest in data-driven control design, the abundance of\ncloud computational services and the importance of preserving privacy for the\nsystem under control. Motivated by these factors, this work investigates\nprivacy-preserving outsourcing for the design of a stabilizing controller for\nunknown linear time-invariant systems.The main objective of this research is to\npreserve the privacy for the system dynamics by designing an outsourcing\nmechanism. To achieve this goal, we propose a scheme that combines\ntransformation-based techniques and robust data-driven control design methods.\nThe scheme preserves the privacy of both the open-loop and closed-loop system\nmatrices while stabilizing the system under control.The scheme is applicable to\nboth data with and without disturbance and is lightweight in terms of\ncomputational overhead. Numerical investigations for a case study demonstrate\nthe impacts of our mechanism and its role in hindering malicious adversaries\nfrom achieving their goals.",
    "updated" : "2024-10-22T18:44:46Z",
    "published" : "2024-10-22T18:44:46Z",
    "authors" : [
      {
        "name" : "Teimour Hosseinalizadeh"
      },
      {
        "name" : "Nima Monshizadeh"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.16137v2",
    "title" : "Privacy as Social Norm: Systematically Reducing Dysfunctional Privacy\n  Concerns on Social Media",
    "summary" : "Privacy is essential to fully enjoying the benefits of social media. While\nfear around privacy risks can sometimes motivate privacy management, the\nnegative impact of such fear, particularly when it is perceived as\nunaddressable (i.e., \"dysfunctional\" fear), can significantly harm teen\nwell-being. In a co-design study with 136 participants aged 13-18, we explored\nhow teens can protect their privacy without experiencing heightened fear. We\nidentified seven different sources of dysfunctional fear, such as `fear of a\nhostile environment' and `fear of overstepping privacy norms.' We also\nevaluated ten designs, co-created with teen participants, that address these\nfears. Our findings suggest that social media platforms can mitigate\ndysfunctional fear without compromising privacy by creating a culture where\nprivacy protection is the norm through default privacy-protective features.\nHowever, we also found that even the most effective privacy features are not\nlikely to be adopted unless they balance the multifaceted and diverse needs of\nteens. Individual teens have different needs -- for example, public and private\naccount users have different needs -- and teens often want to enjoy the\nbenefits they get from slightly reducing privacy and widening their social\nreach. Given these considerations, augmenting default privacy features by\nallowing them to be toggled on and off will allow individual users to choose\ntheir own balance while still maintaining a privacy-focused norm.",
    "updated" : "2024-10-23T09:36:20Z",
    "published" : "2024-10-21T16:03:18Z",
    "authors" : [
      {
        "name" : "JaeWon Kim"
      },
      {
        "name" : "Soobin Cho"
      },
      {
        "name" : "Robert Wolfe"
      },
      {
        "name" : "Jishnu Hari Nair"
      },
      {
        "name" : "Alexis Hiniker"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.18824v1",
    "title" : "PSY: Posterior Sampling Based Privacy Enhancer in Large Language Models",
    "summary" : "Privacy vulnerabilities in LLMs, such as leakage from memorization, have been\nconstantly identified, and various mitigation proposals have been proposed.\nLoRA is usually used in fine-tuning LLMs and a good entry point to insert\nprivacy-enhancing modules. In this ongoing research, we introduce PSY, a\nPosterior Sampling based PrivacY enhancer that can be used in LoRA. We propose\na simple yet effective realization of PSY using posterior sampling, which\neffectively prevents privacy leakage from intermediate information and, in\nturn, preserves the privacy of data owners. We evaluate LoRA extended with PSY\nagainst state-of-the-art membership inference and data extraction attacks. The\nexperiments are executed on three different LLM architectures fine-tuned on\nthree datasets with LoRA. In contrast to the commonly used differential privacy\nmethod, we find that our proposed modification consistently reduces the attack\nsuccess rate. Meanwhile, our method has almost no negative impact on model\nfine-tuning or final performance. Most importantly, PSY reveals a promising\npath toward privacy enhancement with latent space extensions.",
    "updated" : "2024-10-24T15:15:42Z",
    "published" : "2024-10-24T15:15:42Z",
    "authors" : [
      {
        "name" : "Yulian Sun"
      },
      {
        "name" : "Li Duan"
      },
      {
        "name" : "Yong Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.18749v1",
    "title" : "Does Differential Privacy Impact Bias in Pretrained NLP Models?",
    "summary" : "Differential privacy (DP) is applied when fine-tuning pre-trained large\nlanguage models (LLMs) to limit leakage of training examples. While most DP\nresearch has focused on improving a model's privacy-utility tradeoff, some find\nthat DP can be unfair to or biased against underrepresented groups. In this\nwork, we show the impact of DP on bias in LLMs through empirical analysis.\nDifferentially private training can increase the model bias against protected\ngroups w.r.t AUC-based bias metrics. DP makes it more difficult for the model\nto differentiate between the positive and negative examples from the protected\ngroups and other groups in the rest of the population. Our results also show\nthat the impact of DP on bias is not only affected by the privacy protection\nlevel but also the underlying distribution of the dataset.",
    "updated" : "2024-10-24T13:59:03Z",
    "published" : "2024-10-24T13:59:03Z",
    "authors" : [
      {
        "name" : "Md. Khairul Islam"
      },
      {
        "name" : "Andrew Wang"
      },
      {
        "name" : "Tianhao Wang"
      },
      {
        "name" : "Yangfeng Ji"
      },
      {
        "name" : "Judy Fox"
      },
      {
        "name" : "Jieyu Zhao"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.18717v1",
    "title" : "Low-Latency Video Anonymization for Crowd Anomaly Detection: Privacy vs.\n  Performance",
    "summary" : "Recent advancements in artificial intelligence promise ample potential in\nmonitoring applications with surveillance cameras. However, concerns about\nprivacy and model bias have made it challenging to utilize them in public.\nAlthough de-identification approaches have been proposed in the literature,\naiming to achieve a certain level of anonymization, most of them employ deep\nlearning models that are computationally demanding for real-time edge\ndeployment. In this study, we revisit conventional anonymization solutions for\nprivacy protection and real-time video anomaly detection (VAD) applications. We\npropose a novel lightweight adaptive anonymization for VAD (LA3D) that employs\ndynamic adjustment to enhance privacy protection. We evaluated the approaches\non publicly available privacy and VAD data sets to examine the strengths and\nweaknesses of the different anonymization techniques and highlight the\npromising efficacy of our approach. Our experiment demonstrates that LA3D\nenables substantial improvement in the privacy anonymization capability without\nmajorly degrading VAD efficacy.",
    "updated" : "2024-10-24T13:22:33Z",
    "published" : "2024-10-24T13:22:33Z",
    "authors" : [
      {
        "name" : "Mulugeta Weldezgina Asres"
      },
      {
        "name" : "Lei Jiao"
      },
      {
        "name" : "Christian Walter Omlin"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.18666v1",
    "title" : "DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe\n  Dataset Curation",
    "summary" : "Image restoration (IR) in real-world scenarios presents significant\nchallenges due to the lack of high-capacity models and comprehensive datasets.\nTo tackle these issues, we present a dual strategy: GenIR, an innovative data\ncuration pipeline, and DreamClear, a cutting-edge Diffusion Transformer\n(DiT)-based image restoration model. GenIR, our pioneering contribution, is a\ndual-prompt learning pipeline that overcomes the limitations of existing\ndatasets, which typically comprise only a few thousand images and thus offer\nlimited generalizability for larger models. GenIR streamlines the process into\nthree stages: image-text pair construction, dual-prompt based fine-tuning, and\ndata generation & filtering. This approach circumvents the laborious data\ncrawling process, ensuring copyright compliance and providing a cost-effective,\nprivacy-safe solution for IR dataset construction. The result is a large-scale\ndataset of one million high-quality images. Our second contribution,\nDreamClear, is a DiT-based image restoration model. It utilizes the generative\npriors of text-to-image (T2I) diffusion models and the robust perceptual\ncapabilities of multi-modal large language models (MLLMs) to achieve\nphotorealistic restoration. To boost the model's adaptability to diverse\nreal-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM).\nIt employs token-wise degradation priors to dynamically integrate various\nrestoration experts, thereby expanding the range of degradations the model can\naddress. Our exhaustive experiments confirm DreamClear's superior performance,\nunderlining the efficacy of our dual strategy for real-world image restoration.\nCode and pre-trained models will be available at:\nhttps://github.com/shallowdream204/DreamClear.",
    "updated" : "2024-10-24T11:57:20Z",
    "published" : "2024-10-24T11:57:20Z",
    "authors" : [
      {
        "name" : "Yuang Ai"
      },
      {
        "name" : "Xiaoqiang Zhou"
      },
      {
        "name" : "Huaibo Huang"
      },
      {
        "name" : "Xiaotian Han"
      },
      {
        "name" : "Zhengyu Chen"
      },
      {
        "name" : "Quanzeng You"
      },
      {
        "name" : "Hongxia Yang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.18418v1",
    "title" : "Knowledge-Assisted Privacy Preserving in Semantic Communication",
    "summary" : "Semantic communication (SC) offers promising advancements in data\ntransmission efficiency and reliability by focusing on delivering true meaning\nrather than solely binary bits of messages. However, privacy concerns in SC\nmight become outstanding. Eavesdroppers equipped with advanced semantic coding\nmodels and extensive knowledge could be capable of correctly decoding and\nreasoning sensitive semantics from just a few stolen bits. To this end, this\narticle explores utilizing knowledge to enhance data privacy in SC networks.\nSpecifically, we first identify the potential attacks in SC based on the\nanalysis of knowledge. Then, we propose a knowledge-assisted privacy preserving\nSC framework, which consists of a data transmission layer for precisely\nencoding and decoding source messages, and a knowledge management layer\nresponsible for injecting appropriate knowledge into the transmission pair.\nMoreover, we elaborate on the transceiver design in the proposed SC framework\nto explain how knowledge should be utilized properly. Finally, some challenges\nof the proposed SC framework are discussed to expedite the practical\nimplementation.",
    "updated" : "2024-10-24T04:05:20Z",
    "published" : "2024-10-24T04:05:20Z",
    "authors" : [
      {
        "name" : "Xuesong Liu"
      },
      {
        "name" : "Yao Sun"
      },
      {
        "name" : "Runze Cheng"
      },
      {
        "name" : "Le Xia"
      },
      {
        "name" : "Hanaa Abumarshoud"
      },
      {
        "name" : "Lei Zhang"
      },
      {
        "name" : "Muhammad Ali Imran"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.18404v1",
    "title" : "Enhancing Feature-Specific Data Protection via Bayesian Coordinate\n  Differential Privacy",
    "summary" : "Local Differential Privacy (LDP) offers strong privacy guarantees without\nrequiring users to trust external parties. However, LDP applies uniform\nprotection to all data features, including less sensitive ones, which degrades\nperformance of downstream tasks. To overcome this limitation, we propose a\nBayesian framework, Bayesian Coordinate Differential Privacy (BCDP), that\nenables feature-specific privacy quantification. This more nuanced approach\ncomplements LDP by adjusting privacy protection according to the sensitivity of\neach feature, enabling improved performance of downstream tasks without\ncompromising privacy. We characterize the properties of BCDP and articulate its\nconnections with standard non-Bayesian privacy frameworks. We further apply our\nBCDP framework to the problems of private mean estimation and ordinary\nleast-squares regression. The BCDP-based approach obtains improved accuracy\ncompared to a purely LDP-based approach, without compromising on privacy.",
    "updated" : "2024-10-24T03:39:55Z",
    "published" : "2024-10-24T03:39:55Z",
    "authors" : [
      {
        "name" : "Maryam Aliakbarpour"
      },
      {
        "name" : "Syomantak Chaudhuri"
      },
      {
        "name" : "Thomas A. Courtade"
      },
      {
        "name" : "Alireza Fallah"
      },
      {
        "name" : "Michael I. Jordan"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.15386v1",
    "title" : "Formalization of Differential Privacy in Isabelle/HOL",
    "summary" : "Differential privacy is a statistical definition of privacy that has\nattracted the interest of both academia and industry. Its formulations are easy\nto understand, but the differential privacy of databases is complicated to\ndetermine. One of the reasons for this is that small changes in database\nprograms can break their differential privacy. Therefore, formal verification\nof differential privacy has been studied for over a decade.\n  In this paper, we propose an Isabelle/HOL library for formalizing\ndifferential privacy in a general setting. To our knowledge, it is the first\nformalization of differential privacy that supports continuous probability\ndistributions. First, we formalize the standard definition of differential\nprivacy and its basic properties. Second, we formalize the Laplace mechanism\nand its differential privacy. Finally, we formalize the differential privacy of\nthe report noisy max mechanism.",
    "updated" : "2024-10-20T13:06:13Z",
    "published" : "2024-10-20T13:06:13Z",
    "authors" : [
      {
        "name" : "Tetsuya Sato"
      },
      {
        "name" : "Yasuhiko Minamide"
      }
    ],
    "categories" : [
      "cs.LO",
      "cs.CR",
      "cs.PL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.19548v1",
    "title" : "FLiP: Privacy-Preserving Federated Learning based on the Principle of\n  Least Privileg",
    "summary" : "Federated Learning (FL) allows users to share knowledge instead of raw data\nto train a model with high accuracy. Unfortunately, during the training, users\nlose control over the knowledge shared, which causes serious data privacy\nissues. We hold that users are only willing and need to share the essential\nknowledge to the training task to obtain the FL model with high accuracy.\nHowever, existing efforts cannot help users minimize the shared knowledge\naccording to the user intention in the FL training procedure. This work\nproposes FLiP, which aims to bring the principle of least privilege (PoLP) to\nFL training. The key design of FLiP is applying elaborate information reduction\non the training data through a local-global dataset distillation design. We\nmeasure the privacy performance through attribute inference and membership\ninference attacks. Extensive experiments show that FLiP strikes a good balance\nbetween model accuracy and privacy protection.",
    "updated" : "2024-10-25T13:20:40Z",
    "published" : "2024-10-25T13:20:40Z",
    "authors" : [
      {
        "name" : "ShiMao Xu"
      },
      {
        "name" : "Xiaopeng Ke"
      },
      {
        "name" : "Xing Su"
      },
      {
        "name" : "Shucheng Li"
      },
      {
        "name" : "Hao wu"
      },
      {
        "name" : "Fengyuan Xu"
      },
      {
        "name" : "Sheng Zhong"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.19338v1",
    "title" : "Privacy-preserving server-supported decryption",
    "summary" : "In this paper, we consider encryption systems with two-out-of-two threshold\ndecryption, where one of the parties (the client) initiates the decryption and\nthe other one (the server) assists. Existing threshold decryption schemes\ndisclose to the server the ciphertext that is being decrypted. We give a\nconstruction, where the identity of the ciphertext is not leaked to the server,\nand the client's privacy is thus preserved. While showing the security of this\nconstruction, we run into the issue of defining the security of a scheme with\nblindly assisted decryption. We discuss previously proposed security\ndefinitions for similar cryptographic functionalities and argue why they do not\ncapture the expected meaning of security. We propose an ideal functionality for\nthe encryption with server-supported blind threshold decryption in the\nuniversal composability model, carefully balancing between the meaning of\nprivacy, and the ability to implement it. We construct a protocol and show that\nit is a secure implementation of the proposed functionality in the random\noracle model.",
    "updated" : "2024-10-25T06:47:53Z",
    "published" : "2024-10-25T06:47:53Z",
    "authors" : [
      {
        "name" : "Peeter Laud"
      },
      {
        "name" : "Alisa Pankova"
      },
      {
        "name" : "Jelizaveta Vakarjuk"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.19012v1",
    "title" : "Privacy-Computation trade-offs in Private Repetition and Metaselection",
    "summary" : "A Private Repetition algorithm takes as input a differentially private\nalgorithm with constant success probability and boosts it to one that succeeds\nwith high probability. These algorithms are closely related to private\nmetaselection algorithms that compete with the best of many private algorithms,\nand private hyperparameter tuning algorithms that compete with the best\nhyperparameter settings for a private learning algorithm. Existing algorithms\nfor these tasks pay either a large overhead in privacy cost, or a large\noverhead in computational cost. In this work, we show strong lower bounds for\nproblems of this kind, showing in particular that for any algorithm that\npreserves the privacy cost up to a constant factor, the failure probability can\nonly fall polynomially in the computational overhead. This is in stark contrast\nwith the non-private setting, where the failure probability falls exponentially\nin the computational overhead. By carefully combining existing algorithms for\nmetaselection, we prove computation-privacy tradeoffs that nearly match our\nlower bounds.",
    "updated" : "2024-10-22T18:33:02Z",
    "published" : "2024-10-22T18:33:02Z",
    "authors" : [
      {
        "name" : "Kunal Talwar"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.21177v1",
    "title" : "Privacy-Preserving for Images in Satellite Communications: A\n  Comprehensive Review of Chaos-Based Encryption",
    "summary" : "In an era where global connectivity has become critical, satellite\ncommunication is essential for businesses, governments, and individuals. Widely\nused services with satellite communication such as climate change monitoring,\nmilitary surveillance and real-time event broadcasting, involve data in the\nform of images rather text. Therefore, securing image transmission in satellite\ncommunication using efficient and effective encryption approaches, has gained a\nsignificant attention from academia as well as the industry. In this paper, we\nspecifically focus on chaos based image encryption as one of the key\nprivacy-preserving techniques for satellite communication. While there are\nseveral privacy enhancing techniques for protecting image data but chaos based\nencryption has distinct advantages such as high flexibility, high security,\nless computational overheads, less computing power and ease of implementation.\nFirst, we present a solid background about satellite communication and image\nencryption in satellite communication, covering theoretical aspects of chaotic\nsystems and their practical usage for image encryption. Next we present a\ncomprehensive literature review on all state-of-the-art studies specifically\nfor chaos based satellite image encryption, with a detailed analysis of the\nevaluation process, including evaluation parameters and conditions. Finally, we\ndiscuss about existing challenges and open research problems for chaos based\nsatellite image encryption.",
    "updated" : "2024-10-28T16:17:07Z",
    "published" : "2024-10-28T16:17:07Z",
    "authors" : [
      {
        "name" : "Farrukh Bin Rashid"
      },
      {
        "name" : "Windhya Rankothge"
      },
      {
        "name" : "Somayeh Sadeghi"
      },
      {
        "name" : "Hesamodin Mohammadian"
      },
      {
        "name" : "Ali Ghorbani"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.20555v1",
    "title" : "Privacy-Enhanced Adaptive Authentication: User Profiling with Privacy\n  Guarantees",
    "summary" : "User profiling is a critical component of adaptive risk-based authentication,\nyet it raises significant privacy concerns, particularly when handling\nsensitive data. Profiling involves collecting and aggregating various user\nfeatures, potentially creating quasi-identifiers that can reveal identities and\ncompromise privacy. Even anonymized profiling methods remain vulnerable to\nre-identification attacks through these quasi-identifiers. This paper\nintroduces a novel privacy-enhanced adaptive authentication protocol that\nleverages Oblivious Pseudorandom Functions (OPRF), anonymous tokens, and\nDifferential Privacy (DP) to provide robust privacy guarantees. Our proposed\napproach dynamically adjusts authentication requirements based on real-time\nrisk assessments, enhancing security while safeguarding user privacy. By\nintegrating privacy considerations into the core of adaptive risk-based\nadaptive authentication, this approach addresses a gap often overlooked in\ntraditional models. Advanced cryptographic techniques ensure confidentiality,\nintegrity, and unlinkability of user data, while differential privacy\nmechanisms minimize the impact of individual data points on overall analysis.\nFormal security and privacy proofs demonstrate the protocol's resilience\nagainst various threats and its ability to provide strong privacy guarantees.\nAdditionally, a comprehensive performance evaluation reveals that the\ncomputational and communication overheads are manageable, making the protocol\npractical for real-world deployment. By adhering to data protection regulations\nsuch as GDPR and CCPA, our protocol not only enhances security but also fosters\nuser trust and compliance with legal standards.",
    "updated" : "2024-10-27T19:11:33Z",
    "published" : "2024-10-27T19:11:33Z",
    "authors" : [
      {
        "name" : "Yaser Baseri"
      },
      {
        "name" : "Abdelhakim Senhaji Hafid"
      },
      {
        "name" : "Dimitrios Makrakis"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.20259v1",
    "title" : "FL-DABE-BC: A Privacy-Enhanced, Decentralized Authentication, and Secure\n  Communication for Federated Learning Framework with Decentralized\n  Attribute-Based Encryption and Blockchain for IoT Scenarios",
    "summary" : "This study proposes an advanced Federated Learning (FL) framework designed to\nenhance data privacy and security in IoT environments by integrating\nDecentralized Attribute-Based Encryption (DABE), Homomorphic Encryption (HE),\nSecure Multi-Party Computation (SMPC), and Blockchain technology. Unlike\ntraditional FL, our framework enables secure, decentralized authentication and\nencryption directly on IoT devices using DABE, allowing sensitive data to\nremain locally encrypted. Homomorphic Encryption permits computations on\nencrypted data, and SMPC ensures privacy in collaborative computations, while\nBlockchain technology provides transparent, immutable record-keeping for all\ntransactions and model updates. Local model weights are encrypted and\ntransmitted to fog layers for aggregation using HE and SMPC, then iteratively\nrefined by the central server using differential privacy to safeguard against\ndata leakage. This secure, privacy-preserving FL framework delivers a robust\nsolution for efficient model training and real-time analytics across\ndistributed IoT devices, offering significant advancements in secure\ndecentralized learning for IoT applications.",
    "updated" : "2024-10-26T19:30:53Z",
    "published" : "2024-10-26T19:30:53Z",
    "authors" : [
      {
        "name" : "Sathwik Narkedimilli"
      },
      {
        "name" : "Amballa Venkata Sriram"
      },
      {
        "name" : "Satvik Raghav"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.19941v1",
    "title" : "Privacy without Noisy Gradients: Slicing Mechanism for Generative Model\n  Training",
    "summary" : "Training generative models with differential privacy (DP) typically involves\ninjecting noise into gradient updates or adapting the discriminator's training\nprocedure. As a result, such approaches often struggle with hyper-parameter\ntuning and convergence. We consider the slicing privacy mechanism that injects\nnoise into random low-dimensional projections of the private data, and provide\nstrong privacy guarantees for it. These noisy projections are used for training\ngenerative models. To enable optimizing generative models using this DP\napproach, we introduce the smoothed-sliced $f$-divergence and show it enjoys\nstatistical consistency. Moreover, we present a kernel-based estimator for this\ndivergence, circumventing the need for adversarial training. Extensive\nnumerical experiments demonstrate that our approach can generate synthetic data\nof higher quality compared with baselines. Beyond performance improvement, our\nmethod, by sidestepping the need for noisy gradients, offers data scientists\nthe flexibility to adjust generator architecture and hyper-parameters, run the\noptimization over any number of epochs, and even restart the optimization\nprocess -- all without incurring additional privacy costs.",
    "updated" : "2024-10-25T19:32:58Z",
    "published" : "2024-10-25T19:32:58Z",
    "authors" : [
      {
        "name" : "Kristjan Greenewald"
      },
      {
        "name" : "Yuancheng Yu"
      },
      {
        "name" : "Hao Wang"
      },
      {
        "name" : "Kai Xu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.19917v1",
    "title" : "Collaborative Inference over Wireless Channels with Feature Differential\n  Privacy",
    "summary" : "Collaborative inference among multiple wireless edge devices has the\npotential to significantly enhance Artificial Intelligence (AI) applications,\nparticularly for sensing and computer vision. This approach typically involves\na three-stage process: a) data acquisition through sensing, b) feature\nextraction, and c) feature encoding for transmission. However, transmitting the\nextracted features poses a significant privacy risk, as sensitive personal data\ncan be exposed during the process. To address this challenge, we propose a\nnovel privacy-preserving collaborative inference mechanism, wherein each edge\ndevice in the network secures the privacy of extracted features before\ntransmitting them to a central server for inference. Our approach is designed\nto achieve two primary objectives: 1) reducing communication overhead and 2)\nensuring strict privacy guarantees during feature transmission, while\nmaintaining effective inference performance. Additionally, we introduce an\nover-the-air pooling scheme specifically designed for classification tasks,\nwhich provides formal guarantees on the privacy of transmitted features and\nestablishes a lower bound on classification accuracy.",
    "updated" : "2024-10-25T18:11:02Z",
    "published" : "2024-10-25T18:11:02Z",
    "authors" : [
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Yuqi Nie"
      },
      {
        "name" : "Andrea J. Goldsmith"
      },
      {
        "name" : "H. Vincent Poor"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.19548v2",
    "title" : "FLiP: Privacy-Preserving Federated Learning based on the Principle of\n  Least Privileg",
    "summary" : "Federated Learning (FL) allows users to share knowledge instead of raw data\nto train a model with high accuracy. Unfortunately, during the training, users\nlose control over the knowledge shared, which causes serious data privacy\nissues. We hold that users are only willing and need to share the essential\nknowledge to the training task to obtain the FL model with high accuracy.\nHowever, existing efforts cannot help users minimize the shared knowledge\naccording to the user intention in the FL training procedure. This work\nproposes FLiP, which aims to bring the principle of least privilege (PoLP) to\nFL training. The key design of FLiP is applying elaborate information reduction\non the training data through a local-global dataset distillation design. We\nmeasure the privacy performance through attribute inference and membership\ninference attacks. Extensive experiments show that FLiP strikes a good balance\nbetween model accuracy and privacy protection.",
    "updated" : "2024-10-28T12:22:08Z",
    "published" : "2024-10-25T13:20:40Z",
    "authors" : [
      {
        "name" : "ShiMao Xu"
      },
      {
        "name" : "Xiaopeng Ke"
      },
      {
        "name" : "Xing Su"
      },
      {
        "name" : "Shucheng Li"
      },
      {
        "name" : "Hao Wu"
      },
      {
        "name" : "Sheng Zhong"
      },
      {
        "name" : "Fengyuan Xu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.22235v1",
    "title" : "Auditing $f$-Differential Privacy in One Run",
    "summary" : "Empirical auditing has emerged as a means of catching some of the flaws in\nthe implementation of privacy-preserving algorithms. Existing auditing\nmechanisms, however, are either computationally inefficient requiring multiple\nruns of the machine learning algorithms or suboptimal in calculating an\nempirical privacy. In this work, we present a tight and efficient auditing\nprocedure and analysis that can effectively assess the privacy of mechanisms.\nOur approach is efficient; similar to the recent work of Steinke, Nasr, and\nJagielski (2023), our auditing procedure leverages the randomness of examples\nin the input dataset and requires only a single run of the target mechanism.\nAnd it is more accurate; we provide a novel analysis that enables us to achieve\ntight empirical privacy estimates by using the hypothesized $f$-DP curve of the\nmechanism, which provides a more accurate measure of privacy than the\ntraditional $\\epsilon,\\delta$ differential privacy parameters. We use our\nauditing procure and analysis to obtain empirical privacy, demonstrating that\nour auditing procedure delivers tighter privacy estimates.",
    "updated" : "2024-10-29T17:02:22Z",
    "published" : "2024-10-29T17:02:22Z",
    "authors" : [
      {
        "name" : "Saeed Mahloujifar"
      },
      {
        "name" : "Luca Melis"
      },
      {
        "name" : "Kamalika Chaudhuri"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.22108v1",
    "title" : "Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench",
    "summary" : "Generative models such as Large Language Models (LLM) and Multimodal Large\nLanguage models (MLLMs) trained on massive web corpora can memorize and\ndisclose individuals' confidential and private data, raising legal and ethical\nconcerns. While many previous works have addressed this issue in LLM via\nmachine unlearning, it remains largely unexplored for MLLMs. To tackle this\nchallenge, we introduce Multimodal Large Language Model Unlearning Benchmark\n(MLLMU-Bench), a novel benchmark aimed at advancing the understanding of\nmultimodal machine unlearning. MLLMU-Bench consists of 500 fictitious profiles\nand 153 profiles for public celebrities, each profile feature over 14\ncustomized question-answer pairs, evaluated from both multimodal (image+text)\nand unimodal (text) perspectives. The benchmark is divided into four sets to\nassess unlearning algorithms in terms of efficacy, generalizability, and model\nutility. Finally, we provide baseline results using existing generative model\nunlearning algorithms. Surprisingly, our experiments show that unimodal\nunlearning algorithms excel in generation and cloze tasks, while multimodal\nunlearning approaches perform better in classification tasks with multimodal\ninputs.",
    "updated" : "2024-10-29T15:07:23Z",
    "published" : "2024-10-29T15:07:23Z",
    "authors" : [
      {
        "name" : "Zheyuan Liu"
      },
      {
        "name" : "Guangyao Dou"
      },
      {
        "name" : "Mengzhao Jia"
      },
      {
        "name" : "Zhaoxuan Tan"
      },
      {
        "name" : "Qingkai Zeng"
      },
      {
        "name" : "Yongle Yuan"
      },
      {
        "name" : "Meng Jiang"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.21675v1",
    "title" : "BF-Meta: Secure Blockchain-enhanced Privacy-preserving Federated\n  Learning for Metaverse",
    "summary" : "The metaverse, emerging as a revolutionary platform for social and economic\nactivities, provides various virtual services while posing security and privacy\nchallenges. Wearable devices serve as bridges between the real world and the\nmetaverse. To provide intelligent services without revealing users' privacy in\nthe metaverse, leveraging federated learning (FL) to train models on local\nwearable devices is a promising solution. However, centralized model\naggregation in traditional FL may suffer from external attacks, resulting in a\nsingle point of failure. Furthermore, the absence of incentive mechanisms may\nweaken users' participation during FL training, leading to degraded performance\nof the trained model and reduced quality of intelligent services. In this\npaper, we propose BF-Meta, a secure blockchain-empowered FL framework with\ndecentralized model aggregation, to mitigate the negative influence of\nmalicious users and provide secure virtual services in the metaverse. In\naddition, we design an incentive mechanism to give feedback to users based on\ntheir behaviors. Experiments conducted on five datasets demonstrate the\neffectiveness and applicability of BF-Meta.",
    "updated" : "2024-10-29T02:52:49Z",
    "published" : "2024-10-29T02:52:49Z",
    "authors" : [
      {
        "name" : "Wenbo Liu"
      },
      {
        "name" : "Handi Chen"
      },
      {
        "name" : "Edith C. H. Ngai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.21605v1",
    "title" : "Accelerating Privacy-Preserving Medical Record Linkage: A Three-Party\n  MPC Approach",
    "summary" : "Motivation: Record linkage is a crucial concept for integrating data from\nmultiple sources, particularly when datasets lack exact identifiers, and it has\ndiverse applications in real-world data analysis. Privacy-Preserving Record\nLinkage (PPRL) ensures this integration occurs securely, protecting sensitive\ninformation from unauthorized access. This is especially important in sectors\nsuch as healthcare, where datasets include private identity information (IDAT)\ngoverned by strict privacy laws. However, maintaining both privacy and\nefficiency in large-scale record linkage poses significant challenges.\nConsequently, researchers must develop advanced methods to protect data privacy\nwhile optimizing processing performance. This paper presents a novel and\nefficient PPRL method based on a secure 3-party computation (MPC) framework.\nOur approach allows multiple parties to compute linkage results without\nexposing their private inputs and significantly improves the speed of linkage\nprocess compared to existing privacy-preserving solutions. Results: We\ndemonstrated that our method preserves the linkage quality of the\nstate-of-the-art PPRL method while achieving up to 14 times faster performance.\nFor example, linking a record against a database of 10,000 records takes just\n8.74 seconds in a realistic network with 700 Mbps bandwidth and 60 ms latency.\nEven on a slower internet connection with 100 Mbps bandwidth and 60 ms latency,\nthe linkage completes in 28 seconds, highlighting the scalability and\nefficiency of our solution.",
    "updated" : "2024-10-28T23:13:01Z",
    "published" : "2024-10-28T23:13:01Z",
    "authors" : [
      {
        "name" : "Şeyma Selcan Mağara"
      },
      {
        "name" : "Noah Dietrich"
      },
      {
        "name" : "Ali Burak Ünal"
      },
      {
        "name" : "Mete Akgün"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.18666v2",
    "title" : "DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe\n  Dataset Curation",
    "summary" : "Image restoration (IR) in real-world scenarios presents significant\nchallenges due to the lack of high-capacity models and comprehensive datasets.\nTo tackle these issues, we present a dual strategy: GenIR, an innovative data\ncuration pipeline, and DreamClear, a cutting-edge Diffusion Transformer\n(DiT)-based image restoration model. GenIR, our pioneering contribution, is a\ndual-prompt learning pipeline that overcomes the limitations of existing\ndatasets, which typically comprise only a few thousand images and thus offer\nlimited generalizability for larger models. GenIR streamlines the process into\nthree stages: image-text pair construction, dual-prompt based fine-tuning, and\ndata generation & filtering. This approach circumvents the laborious data\ncrawling process, ensuring copyright compliance and providing a cost-effective,\nprivacy-safe solution for IR dataset construction. The result is a large-scale\ndataset of one million high-quality images. Our second contribution,\nDreamClear, is a DiT-based image restoration model. It utilizes the generative\npriors of text-to-image (T2I) diffusion models and the robust perceptual\ncapabilities of multi-modal large language models (MLLMs) to achieve\nphotorealistic restoration. To boost the model's adaptability to diverse\nreal-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM).\nIt employs token-wise degradation priors to dynamically integrate various\nrestoration experts, thereby expanding the range of degradations the model can\naddress. Our exhaustive experiments confirm DreamClear's superior performance,\nunderlining the efficacy of our dual strategy for real-world image restoration.\nCode and pre-trained models are available at:\nhttps://github.com/shallowdream204/DreamClear.",
    "updated" : "2024-10-29T05:50:12Z",
    "published" : "2024-10-24T11:57:20Z",
    "authors" : [
      {
        "name" : "Yuang Ai"
      },
      {
        "name" : "Xiaoqiang Zhou"
      },
      {
        "name" : "Huaibo Huang"
      },
      {
        "name" : "Xiaotian Han"
      },
      {
        "name" : "Zhengyu Chen"
      },
      {
        "name" : "Quanzeng You"
      },
      {
        "name" : "Hongxia Yang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.21986v1",
    "title" : "From 5G to 6G: A Survey on Security, Privacy, and Standardization\n  Pathways",
    "summary" : "The vision for 6G aims to enhance network capabilities with faster data\nrates, near-zero latency, and higher capacity, supporting more connected\ndevices and seamless experiences within an intelligent digital ecosystem where\nartificial intelligence (AI) plays a crucial role in network management and\ndata analysis. This advancement seeks to enable immersive mixed-reality\nexperiences, holographic communications, and smart city infrastructures.\nHowever, the expansion of 6G raises critical security and privacy concerns,\nsuch as unauthorized access and data breaches. This is due to the increased\nintegration of IoT devices, edge computing, and AI-driven analytics. This paper\nprovides a comprehensive overview of 6G protocols, focusing on security and\nprivacy, identifying risks, and presenting mitigation strategies. The survey\nexamines current risk assessment frameworks and advocates for tailored 6G\nsolutions. We further discuss industry visions, government projects, and\nstandardization efforts to balance technological innovation with robust\nsecurity and privacy measures.",
    "updated" : "2024-10-04T03:03:44Z",
    "published" : "2024-10-04T03:03:44Z",
    "authors" : [
      {
        "name" : "Mengmeng Yang"
      },
      {
        "name" : "Youyang Qu"
      },
      {
        "name" : "Thilina Ranbaduge"
      },
      {
        "name" : "Chandra Thapa"
      },
      {
        "name" : "Nazatul Sultan"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Hajime Suzuki"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "Sharif Abuadbba"
      },
      {
        "name" : "David Smith"
      },
      {
        "name" : "Paul Tyler"
      },
      {
        "name" : "Josef Pieprzyk"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      },
      {
        "name" : "Xinlong Guan"
      },
      {
        "name" : "Sirine M'rabet"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.22784v1",
    "title" : "Contrastive Learning and Adversarial Disentanglement for\n  Privacy-Preserving Task-Oriented Semantic Communications",
    "summary" : "Task-oriented semantic communication systems have emerged as a promising\napproach to achieving efficient and intelligent data transmission, where only\ninformation relevant to a specific task is communicated. However, existing\nmethods struggle to fully disentangle task-relevant and task-irrelevant\ninformation, leading to privacy concerns and subpar performance. To address\nthis, we propose an information-bottleneck method, named CLAD (contrastive\nlearning and adversarial disentanglement). CLAD leverages contrastive learning\nto effectively capture task-relevant features while employing adversarial\ndisentanglement to discard task-irrelevant information. Additionally, due to\nthe lack of reliable and reproducible methods to gain insight into the\ninformativeness and minimality of the encoded feature vectors, we introduce a\nnew technique to compute the information retention index (IRI), a comparative\nmetric used as a proxy for the mutual information between the encoded features\nand the input, reflecting the minimality of the encoded features. The IRI\nquantifies the minimality and informativeness of the encoded feature vectors\nacross different task-oriented communication techniques. Our extensive\nexperiments demonstrate that CLAD outperforms state-of-the-art baselines in\nterms of task performance, privacy preservation, and IRI. CLAD achieves a\npredictive performance improvement of around 2.5-3%, along with a 77-90%\nreduction in IRI and a 57-76% decrease in adversarial accuracy.",
    "updated" : "2024-10-30T07:59:52Z",
    "published" : "2024-10-30T07:59:52Z",
    "authors" : [
      {
        "name" : "Omar Erak"
      },
      {
        "name" : "Omar Alhussein"
      },
      {
        "name" : "Wen Tong"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.IT",
      "eess.IV",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.22673v1",
    "title" : "Calibrating Practical Privacy Risks for Differentially Private Machine\n  Learning",
    "summary" : "Differential privacy quantifies privacy through the privacy budget\n$\\epsilon$, yet its practical interpretation is complicated by variations\nacross models and datasets. Recent research on differentially private machine\nlearning and membership inference has highlighted that with the same\ntheoretical $\\epsilon$ setting, the likelihood-ratio-based membership inference\n(LiRA) attacking success rate (ASR) may vary according to specific datasets and\nmodels, which might be a better indicator for evaluating real-world privacy\nrisks. Inspired by this practical privacy measure, we study the approaches that\ncan lower the attacking success rate to allow for more flexible privacy budget\nsettings in model training. We find that by selectively suppressing\nprivacy-sensitive features, we can achieve lower ASR values without\ncompromising application-specific data utility. We use the SHAP and LIME model\nexplainer to evaluate feature sensitivities and develop feature-masking\nstrategies. Our findings demonstrate that the LiRA $ASR^M$ on model $M$ can\nproperly indicate the inherent privacy risk of a dataset for modeling, and it's\npossible to modify datasets to enable the use of larger theoretical $\\epsilon$\nsettings to achieve equivalent practical privacy protection. We have conducted\nextensive experiments to show the inherent link between ASR and the dataset's\nprivacy risk. By carefully selecting features to mask, we can preserve more\ndata utility with equivalent practical privacy protection and relaxed\n$\\epsilon$ settings. The implementation details are shared online at the\nprovided GitHub URL\n\\url{https://anonymous.4open.science/r/On-sensitive-features-and-empirical-epsilon-lower-bounds-BF67/}.",
    "updated" : "2024-10-30T03:52:01Z",
    "published" : "2024-10-30T03:52:01Z",
    "authors" : [
      {
        "name" : "Yuechun Gu"
      },
      {
        "name" : "Keke Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.22651v1",
    "title" : "FT-PrivacyScore: Personalized Privacy Scoring Service for Machine\n  Learning Participation",
    "summary" : "Training data privacy has been a top concern in AI modeling. While methods\nlike differentiated private learning allow data contributors to quantify\nacceptable privacy loss, model utility is often significantly damaged. In\npractice, controlled data access remains a mainstream method for protecting\ndata privacy in many industrial and research environments. In controlled data\naccess, authorized model builders work in a restricted environment to access\nsensitive data, which can fully preserve data utility with reduced risk of data\nleak. However, unlike differential privacy, there is no quantitative measure\nfor individual data contributors to tell their privacy risk before\nparticipating in a machine learning task. We developed the demo prototype\nFT-PrivacyScore to show that it's possible to efficiently and quantitatively\nestimate the privacy risk of participating in a model fine-tuning task. The\ndemo source code will be available at\n\\url{https://github.com/RhincodonE/demo_privacy_scoring}.",
    "updated" : "2024-10-30T02:41:26Z",
    "published" : "2024-10-30T02:41:26Z",
    "authors" : [
      {
        "name" : "Yuechun Gu"
      },
      {
        "name" : "Jiajie He"
      },
      {
        "name" : "Keke Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.22623v1",
    "title" : "PV-VTT: A Privacy-Centric Dataset for Mission-Specific Anomaly Detection\n  and Natural Language Interpretation",
    "summary" : "Video crime detection is a significant application of computer vision and\nartificial intelligence. However, existing datasets primarily focus on\ndetecting severe crimes by analyzing entire video clips, often neglecting the\nprecursor activities (i.e., privacy violations) that could potentially prevent\nthese crimes. To address this limitation, we present PV-VTT (Privacy Violation\nVideo To Text), a unique multimodal dataset aimed at identifying privacy\nviolations. PV-VTT provides detailed annotations for both video and text in\nscenarios. To ensure the privacy of individuals in the videos, we only provide\nvideo feature vectors, avoiding the release of any raw video data. This\nprivacy-focused approach allows researchers to use the dataset while protecting\nparticipant confidentiality. Recognizing that privacy violations are often\nambiguous and context-dependent, we propose a Graph Neural Network (GNN)-based\nvideo description model. Our model generates a GNN-based prompt with image for\nLarge Language Model (LLM), which deliver cost-effective and high-quality video\ndescriptions. By leveraging a single video frame along with relevant text, our\nmethod reduces the number of input tokens required, maintaining descriptive\nquality while optimizing LLM API-usage. Extensive experiments validate the\neffectiveness and interpretability of our approach in video description tasks\nand flexibility of our PV-VTT dataset.",
    "updated" : "2024-10-30T01:02:20Z",
    "published" : "2024-10-30T01:02:20Z",
    "authors" : [
      {
        "name" : "Ryozo Masukawa"
      },
      {
        "name" : "Sanggeon Yun"
      },
      {
        "name" : "Yoshiki Yamaguchi"
      },
      {
        "name" : "Mohsen Imani"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.22488v1",
    "title" : "Privacy-Preserving Dynamic Assortment Selection",
    "summary" : "With the growing demand for personalized assortment recommendations, concerns\nover data privacy have intensified, highlighting the urgent need for effective\nprivacy-preserving strategies. This paper presents a novel framework for\nprivacy-preserving dynamic assortment selection using the multinomial logit\n(MNL) bandits model. Our approach employs a perturbed upper confidence bound\nmethod, integrating calibrated noise into user utility estimates to balance\nbetween exploration and exploitation while ensuring robust privacy protection.\nWe rigorously prove that our policy satisfies Joint Differential Privacy (JDP),\nwhich better suits dynamic environments than traditional differential privacy,\neffectively mitigating inference attack risks. This analysis is built upon a\nnovel objective perturbation technique tailored for MNL bandits, which is also\nof independent interest. Theoretically, we derive a near-optimal regret bound\nof $\\tilde{O}(\\sqrt{T})$ for our policy and explicitly quantify how privacy\nprotection impacts regret. Through extensive simulations and an application to\nthe Expedia hotel dataset, we demonstrate substantial performance enhancements\nover the benchmark method.",
    "updated" : "2024-10-29T19:28:01Z",
    "published" : "2024-10-29T19:28:01Z",
    "authors" : [
      {
        "name" : "Young Hyun Cho"
      },
      {
        "name" : "Will Wei Sun"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.24066v1",
    "title" : "Cough-E: A multimodal, privacy-preserving cough detection algorithm for\n  the edge",
    "summary" : "Continuous cough monitors can greatly aid doctors in home monitoring and\ntreatment of respiratory diseases. Although many algorithms have been proposed,\nthey still face limitations in data privacy and short-term monitoring. Edge-AI\noffers a promising solution by processing privacy-sensitive data near the\nsource, but challenges arise in deploying resource-intensive algorithms on\nconstrained devices. From a suitable selection of audio and kinematic signals,\nour methodology aims at the optimal selection of features via Recursive Feature\nElimination with Cross-Validation (RFECV), which exploits the explainability of\nthe selected XGB model. Additionally, it analyzes the use of Mel spectrogram\nfeatures, instead of the more common MFCC. Moreover, a set of hyperparameters\nfor a multimodal implementation of the classifier is explored. Finally, it\nevaluates the performance based on clinically relevant event-based metrics. We\napply our methodology to develop Cough-E, an energy-efficient, multimodal and\nedge AI cough detection algorithm. It exploits audio and kinematic data in two\ndistinct classifiers, jointly cooperating for a balanced energy and performance\ntrade-off. We demonstrate that our algorithm can be executed in real-time on an\nARM Cortex M33 microcontroller. Cough-E achieves a 70.56\\% energy saving when\ncompared to the audio-only approach, at the cost of a 1.26\\% relative\nperformance drop, resulting in a 0.78 F1-score. Both Cough-E and the edge-aware\nmodel optimization methodology are publicly available as open-source code. This\napproach demonstrates the benefits of the proposed hardware-aware methodology\nto enable privacy-preserving cough monitors on the edge, paving the way to\nefficient cough monitoring.",
    "updated" : "2024-10-31T16:00:19Z",
    "published" : "2024-10-31T16:00:19Z",
    "authors" : [
      {
        "name" : "Stefano Albini"
      },
      {
        "name" : "Lara Orlandic"
      },
      {
        "name" : "Jonathan Dan"
      },
      {
        "name" : "Jérôme Thevenot"
      },
      {
        "name" : "Tomas Teijeiro"
      },
      {
        "name" : "Denisa Andreea Constantinescu"
      },
      {
        "name" : "David Atienza"
      }
    ],
    "categories" : [
      "eess.AS",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.23759v1",
    "title" : "Converting BPMN Diagrams to Privacy Calculus",
    "summary" : "The ecosystem of Privacy Calculus is a formal framework for privacy\ncomprising (a) the Privacy Calculus, a Turing-complete language of\nmessage-exchanging processes based on the pi-calculus, (b) a privacy policy\nlanguage, and (c) a type checker that checks adherence of Privacy Calculus\nterms to privacy policies. BPMN is a standard for the graphical description of\nbusiness processes which aims to be understandable by all business users, from\nthose with no technical background to those implementing software. This paper\npresents how (a subset of) BPMN diagrams can be converted to Privacy Calculus\nterms, in the hope that it will serve as a small piece of larger workflows for\nbuilding privacy-preserving software. The conversion is described\nmathematically in the paper, but has also been implemented as a software tool.",
    "updated" : "2024-10-31T09:25:23Z",
    "published" : "2024-10-31T09:25:23Z",
    "authors" : [
      {
        "name" : "Georgios V. Pitsiladis"
      },
      {
        "name" : "Petros S. Stefaneas"
      }
    ],
    "categories" : [
      "cs.LO",
      "cs.CY",
      "cs.SE",
      "K.4.1;D.2.4;F.3.1;I.2.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.14607v1",
    "title" : "Evaluating Privacy Measures in Healthcare Apps Predominantly Used by\n  Older Adults",
    "summary" : "The widespread adoption of telehealth systems has led to a significant\nincrease in the use of healthcare apps among older adults, but this rapid\ngrowth has also heightened concerns about the privacy of their health\ninformation. While HIPAA in the US and GDPR in the EU establish essential\nprivacy protections for health information, limited research exists on the\neffectiveness of healthcare app privacy policies, particularly those used\npredominantly by older adults. To address this, we evaluated 28 healthcare apps\nacross multiple dimensions, including regulatory compliance, data handling\npractices, and privacy-focused usability. To do this, we created a Privacy Risk\nAssessment Framework (PRAF) and used it to evaluate the privacy risks\nassociated with these healthcare apps designed for older adults. Our analysis\nrevealed significant gaps in compliance with privacy standards to such, only\n25% of apps explicitly state compliance with HIPAA, and only 18% mention GDPR.\nSurprisingly, 79% of these applications lack breach protocols, putting older\nadults at risk in the event of a data breach.",
    "updated" : "2024-10-18T17:01:14Z",
    "published" : "2024-10-18T17:01:14Z",
    "authors" : [
      {
        "name" : "Suleiman Saka"
      },
      {
        "name" : "Sanchari Das"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.19548v3",
    "title" : "Privacy-Preserving Federated Learning via Dataset Distillation",
    "summary" : "Federated Learning (FL) allows users to share knowledge instead of raw data\nto train a model with high accuracy. Unfortunately, during the training, users\nlose control over the knowledge shared, which causes serious data privacy\nissues. We hold that users are only willing and need to share the essential\nknowledge to the training task to obtain the FL model with high accuracy.\nHowever, existing efforts cannot help users minimize the shared knowledge\naccording to the user intention in the FL training procedure. This work\nproposes FLiP, which aims to bring the principle of least privilege (PoLP) to\nFL training. The key design of FLiP is applying elaborate information reduction\non the training data through a local-global dataset distillation design. We\nmeasure the privacy performance through attribute inference and membership\ninference attacks. Extensive experiments show that FLiP strikes a good balance\nbetween model accuracy and privacy protection.",
    "updated" : "2024-11-04T06:42:53Z",
    "published" : "2024-10-25T13:20:40Z",
    "authors" : [
      {
        "name" : "ShiMao Xu"
      },
      {
        "name" : "Xiaopeng Ke"
      },
      {
        "name" : "Xing Su"
      },
      {
        "name" : "Shucheng Li"
      },
      {
        "name" : "Hao Wu"
      },
      {
        "name" : "Sheng Zhong"
      },
      {
        "name" : "Fengyuan Xu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  }
]