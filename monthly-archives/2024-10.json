[
  {
    "id" : "http://arxiv.org/abs/2410.01068v1",
    "title" : "Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness",
    "summary" : "We study the Differential Privacy (DP) guarantee of hidden-state Noisy-SGD\nalgorithms over a bounded domain. Standard privacy analysis for Noisy-SGD\nassumes all internal states are revealed, which leads to a divergent R'enyi DP\nbound with respect to the number of iterations. Ye & Shokri (2022) and\nAltschuler & Talwar (2022) proved convergent bounds for smooth (strongly)\nconvex losses, and raise open questions about whether these assumptions can be\nrelaxed. We provide positive answers by proving convergent R'enyi DP bound for\nnon-convex non-smooth losses, where we show that requiring losses to have\nH\\\"older continuous gradient is sufficient. We also provide a strictly better\nprivacy bound compared to state-of-the-art results for smooth strongly convex\nlosses. Our analysis relies on the improvement of shifted divergence analysis\nin multiple aspects, including forward Wasserstein distance tracking,\nidentifying the optimal shifts allocation, and the H\"older reduction lemma. Our\nresults further elucidate the benefit of hidden-state analysis for DP and its\napplicability.",
    "updated" : "2024-10-01T20:52:08Z",
    "published" : "2024-10-01T20:52:08Z",
    "authors" : [
      {
        "name" : "Eli Chien"
      },
      {
        "name" : "Pan Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00897v1",
    "title" : "The Gradient of Health Data Privacy",
    "summary" : "In the era of digital health and artificial intelligence, the management of\npatient data privacy has become increasingly complex, with significant\nimplications for global health equity and patient trust. This paper introduces\na novel \"privacy gradient\" approach to health data governance, offering a more\nnuanced and adaptive framework than traditional binary privacy models. Our\nmultidimensional concept considers factors such as data sensitivity,\nstakeholder relationships, purpose of use, and temporal aspects, allowing for\ncontext-sensitive privacy protections. Through policy analyses, ethical\nconsiderations, and case studies spanning adolescent health, integrated care,\nand genomic research, we demonstrate how this approach can address critical\nprivacy challenges in diverse healthcare settings worldwide. The privacy\ngradient model has the potential to enhance patient engagement, improve care\ncoordination, and accelerate medical research while safeguarding individual\nprivacy rights. We provide policy recommendations for implementing this\napproach, considering its impact on healthcare systems, research\ninfrastructures, and global health initiatives. This work aims to inform\npolicymakers, healthcare leaders, and digital health innovators, contributing\nto a more equitable, trustworthy, and effective global health data ecosystem in\nthe digital age.",
    "updated" : "2024-10-01T17:35:18Z",
    "published" : "2024-10-01T17:35:18Z",
    "authors" : [
      {
        "name" : "Baihan Lin"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "q-bio.OT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00836v1",
    "title" : "Towards Fairness and Privacy: A Novel Data Pre-processing Optimization\n  Framework for Non-binary Protected Attributes",
    "summary" : "The reason behind the unfair outcomes of AI is often rooted in biased\ndatasets. Therefore, this work presents a framework for addressing fairness by\ndebiasing datasets containing a (non-)binary protected attribute. The framework\nproposes a combinatorial optimization problem where heuristics such as genetic\nalgorithms can be used to solve for the stated fairness objectives. The\nframework addresses this by finding a data subset that minimizes a certain\ndiscrimination measure. Depending on a user-defined setting, the framework\nenables different use cases, such as data removal, the addition of synthetic\ndata, or exclusive use of synthetic data. The exclusive use of synthetic data\nin particular enhances the framework's ability to preserve privacy while\noptimizing for fairness. In a comprehensive evaluation, we demonstrate that\nunder our framework, genetic algorithms can effectively yield fairer datasets\ncompared to the original data. In contrast to prior work, the framework\nexhibits a high degree of flexibility as it is metric- and task-agnostic, can\nbe applied to both binary or non-binary protected attributes, and demonstrates\nefficient runtime.",
    "updated" : "2024-10-01T16:17:43Z",
    "published" : "2024-10-01T16:17:43Z",
    "authors" : [
      {
        "name" : "Manh Khoi Duong"
      },
      {
        "name" : "Stefan Conrad"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00751v1",
    "title" : "Thinking Outside of the Differential Privacy Box: A Case Study in Text\n  Privatization with Language Model Prompting",
    "summary" : "The field of privacy-preserving Natural Language Processing has risen in\npopularity, particularly at a time when concerns about privacy grow with the\nproliferation of Large Language Models. One solution consistently appearing in\nrecent literature has been the integration of Differential Privacy (DP) into\nNLP techniques. In this paper, we take these approaches into critical view,\ndiscussing the restrictions that DP integration imposes, as well as bring to\nlight the challenges that such restrictions entail. To accomplish this, we\nfocus on $\\textbf{DP-Prompt}$, a recent method for text privatization\nleveraging language models to rewrite texts. In particular, we explore this\nrewriting task in multiple scenarios, both with DP and without DP. To drive the\ndiscussion on the merits of DP in NLP, we conduct empirical utility and privacy\nexperiments. Our results demonstrate the need for more discussion on the\nusability of DP in NLP and its benefits over non-DP approaches.",
    "updated" : "2024-10-01T14:46:15Z",
    "published" : "2024-10-01T14:46:15Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00542v1",
    "title" : "Differentially Private Active Learning: Balancing Effective Data\n  Selection and Privacy",
    "summary" : "Active learning (AL) is a widely used technique for optimizing data labeling\nin machine learning by iteratively selecting, labeling, and training on the\nmost informative data. However, its integration with formal privacy-preserving\nmethods, particularly differential privacy (DP), remains largely underexplored.\nWhile some works have explored differentially private AL for specialized\nscenarios like online learning, the fundamental challenge of combining AL with\nDP in standard learning settings has remained unaddressed, severely limiting\nAL's applicability in privacy-sensitive domains. This work addresses this gap\nby introducing differentially private active learning (DP-AL) for standard\nlearning settings. We demonstrate that naively integrating DP-SGD training into\nAL presents substantial challenges in privacy budget allocation and data\nutilization. To overcome these challenges, we propose step amplification, which\nleverages individual sampling probabilities in batch creation to maximize data\npoint participation in training steps, thus optimizing data utilization.\nAdditionally, we investigate the effectiveness of various acquisition functions\nfor data selection under privacy constraints, revealing that many commonly used\nfunctions become impractical. Our experiments on vision and natural language\nprocessing tasks show that DP-AL can improve performance for specific datasets\nand model architectures. However, our findings also highlight the limitations\nof AL in privacy-constrained environments, emphasizing the trade-offs between\nprivacy, model accuracy, and data selection accuracy.",
    "updated" : "2024-10-01T09:34:06Z",
    "published" : "2024-10-01T09:34:06Z",
    "authors" : [
      {
        "name" : "Kristian Schwethelm"
      },
      {
        "name" : "Johannes Kaiser"
      },
      {
        "name" : "Jonas Kuntzer"
      },
      {
        "name" : "Mehmet Yigitsoy"
      },
      {
        "name" : "Daniel Rueckert"
      },
      {
        "name" : "Georgios Kaissis"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00433v1",
    "title" : "PrivTuner with Homomorphic Encryption and LoRA: A P3EFT Scheme for\n  Privacy-Preserving Parameter-Efficient Fine-Tuning of AI Foundation Models",
    "summary" : "AI foundation models have recently demonstrated impressive capabilities\nacross a wide range of tasks. Fine-tuning (FT) is a method of customizing a\npre-trained AI foundation model by further training it on a smaller, targeted\ndataset. In this paper, we initiate the study of the Privacy-Preserving\nParameter-Efficient FT (P3EFT) framework, which can be viewed as the\nintersection of Parameter-Efficient FT (PEFT) and Privacy-Preserving FT (PPFT).\nPEFT modifies only a small subset of the model's parameters to achieve FT\n(i.e., adapting a pre-trained model to a specific dataset), while PPFT uses\nprivacy-preserving technologies to protect the confidentiality of the model\nduring the FT process. There have been many studies on PEFT or PPFT but very\nfew on their fusion, which motivates our work on P3EFT to achieve both\nparameter efficiency and model privacy. To exemplify our P3EFT, we present the\nPrivTuner scheme, which incorporates Fully Homomorphic Encryption (FHE) enabled\nprivacy protection into LoRA (short for ``Low-Rank Adapter''). Intuitively\nspeaking, PrivTuner allows the model owner and the external data owners to\ncollaboratively implement PEFT with encrypted data. After describing PrivTuner\nin detail, we further investigate its energy consumption and privacy\nprotection. Then, we consider a PrivTuner system over wireless communications\nand formulate a joint optimization problem to adaptively minimize energy while\nmaximizing privacy protection, with the optimization variables including FDMA\nbandwidth allocation, wireless transmission power, computational resource\nallocation, and privacy protection. A resource allocation algorithm is devised\nto solve the problem. Experiments demonstrate that our algorithm can\nsignificantly reduce energy consumption while adapting to different privacy\nrequirements.",
    "updated" : "2024-10-01T06:30:06Z",
    "published" : "2024-10-01T06:30:06Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Wenhan Yu"
      },
      {
        "name" : "Jun Zhao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02547v1",
    "title" : "Personalized Quantum Federated Learning for Privacy Image Classification",
    "summary" : "Quantum federated learning has brought about the improvement of privacy image\nclassification, while the lack of personality of the client model may\ncontribute to the suboptimal of quantum federated learning. A personalized\nquantum federated learning algorithm for privacy image classification is\nproposed to enhance the personality of the client model in the case of an\nimbalanced distribution of images. First, a personalized quantum federated\nlearning model is constructed, in which a personalized layer is set for the\nclient model to maintain the personalized parameters. Second, a personalized\nquantum federated learning algorithm is introduced to secure the information\nexchanged between the client and server.Third, the personalized federated\nlearning is applied to image classification on the FashionMNIST dataset, and\nthe experimental results indicate that the personalized quantum federated\nlearning algorithm can obtain global and local models with excellent\nperformance, even in situations where local training samples are imbalanced.\nThe server's accuracy is 100% with 8 clients and a distribution parameter of\n100, outperforming the non-personalized model by 7%. The average client\naccuracy is 2.9% higher than that of the non-personalized model with 2 clients\nand a distribution parameter of 1. Compared to previous quantum federated\nlearning algorithms, the proposed personalized quantum federated learning\nalgorithm eliminates the need for additional local training while safeguarding\nboth model and data privacy.It may facilitate broader adoption and application\nof quantum technologies, and pave the way for more secure, scalable, and\nefficient quantum distribute machine learning solutions.",
    "updated" : "2024-10-03T14:53:04Z",
    "published" : "2024-10-03T14:53:04Z",
    "authors" : [
      {
        "name" : "Jinjing Shi"
      },
      {
        "name" : "Tian Chen"
      },
      {
        "name" : "Shichao Zhang"
      },
      {
        "name" : "Xuelong Li"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02371v1",
    "title" : "NTU-NPU System for Voice Privacy 2024 Challenge",
    "summary" : "In this work, we describe our submissions for the Voice Privacy Challenge\n2024. Rather than proposing a novel speech anonymization system, we enhance the\nprovided baselines to meet all required conditions and improve evaluated\nmetrics. Specifically, we implement emotion embedding and experiment with WavLM\nand ECAPA2 speaker embedders for the B3 baseline. Additionally, we compare\ndifferent speaker and prosody anonymization techniques. Furthermore, we\nintroduce Mean Reversion F0 for B5, which helps to enhance privacy without a\nloss in utility. Finally, we explore disentanglement models, namely $\\beta$-VAE\nand NaturalSpeech3 FACodec.",
    "updated" : "2024-10-03T10:45:10Z",
    "published" : "2024-10-03T10:45:10Z",
    "authors" : [
      {
        "name" : "Nikita Kuzmin"
      },
      {
        "name" : "Hieu-Thi Luong"
      },
      {
        "name" : "Jixun Yao"
      },
      {
        "name" : "Lei Xie"
      },
      {
        "name" : "Kong Aik Lee"
      },
      {
        "name" : "Eng Siong Chng"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02246v1",
    "title" : "PFGuard: A Generative Framework with Privacy and Fairness Safeguards",
    "summary" : "Generative models must ensure both privacy and fairness for Trustworthy AI.\nWhile these goals have been pursued separately, recent studies propose to\ncombine existing privacy and fairness techniques to achieve both goals.\nHowever, naively combining these techniques can be insufficient due to\nprivacy-fairness conflicts, where a sample in a minority group may be amplified\nfor fairness, only to be suppressed for privacy. We demonstrate how these\nconflicts lead to adverse effects, such as privacy violations and unexpected\nfairness-utility tradeoffs. To mitigate these risks, we propose PFGuard, a\ngenerative framework with privacy and fairness safeguards, which simultaneously\naddresses privacy, fairness, and utility. By using an ensemble of multiple\nteacher models, PFGuard balances privacy-fairness conflicts between fair and\nprivate training stages and achieves high utility based on ensemble learning.\nExtensive experiments show that PFGuard successfully generates synthetic data\non high-dimensional data while providing both fairness convergence and strict\nDP guarantees - the first of its kind to our knowledge.",
    "updated" : "2024-10-03T06:37:16Z",
    "published" : "2024-10-03T06:37:16Z",
    "authors" : [
      {
        "name" : "Soyeon Kim"
      },
      {
        "name" : "Yuji Roh"
      },
      {
        "name" : "Geon Heo"
      },
      {
        "name" : "Steven Euijong Whang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03621v1",
    "title" : "A Global Medical Data Security and Privacy Preserving Standards\n  Identification Framework for Electronic Healthcare Consumers",
    "summary" : "Electronic Health Records (EHR) are crucial for the success of digital\nhealthcare, with a focus on putting consumers at the center of this\ntransformation. However, the digitalization of healthcare records brings along\nsecurity and privacy risks for personal data. The major concern is that\ndifferent countries have varying standards for the security and privacy of\nmedical data. This paper proposed a novel and comprehensive framework to\nstandardize these rules globally, bringing them together on a common platform.\nTo support this proposal, the study reviews existing literature to understand\nthe research interest in this issue. It also examines six key laws and\nstandards related to security and privacy, identifying twenty concepts. The\nproposed framework utilized K-means clustering to categorize these concepts and\nidentify five key factors. Finally, an Ordinal Priority Approach is applied to\ndetermine the preferred implementation of these factors in the context of EHRs.\nThe proposed study provides a descriptive then prescriptive framework for the\nimplementation of privacy and security in the context of electronic health\nrecords. Therefore, the findings of the proposed framework are useful for\nprofessionals and policymakers in improving the security and privacy associated\nwith EHRs.",
    "updated" : "2024-10-04T17:22:55Z",
    "published" : "2024-10-04T17:22:55Z",
    "authors" : [
      {
        "name" : "Vinaytosh Mishra"
      },
      {
        "name" : "Kishu Gupta"
      },
      {
        "name" : "Deepika Saxena"
      },
      {
        "name" : "Ashutosh Kumar Singh"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03407v1",
    "title" : "Camel: Communication-Efficient and Maliciously Secure Federated Learning\n  in the Shuffle Model of Differential Privacy",
    "summary" : "Federated learning (FL) has rapidly become a compelling paradigm that enables\nmultiple clients to jointly train a model by sharing only gradient updates for\naggregation, without revealing their local private data. In order to protect\nthe gradient updates which could also be privacy-sensitive, there has been a\nline of work studying local differential privacy (LDP) mechanisms to provide a\nformal privacy guarantee. With LDP mechanisms, clients locally perturb their\ngradient updates before sharing them out for aggregation. However, such\napproaches are known for greatly degrading the model utility, due to heavy\nnoise addition. To enable a better privacy-utility tradeoff, a recently\nemerging trend is to apply the shuffle model of DP in FL, which relies on an\nintermediate shuffling operation on the perturbed gradient updates to achieve\nprivacy amplification. Following this trend, in this paper, we present Camel, a\nnew communication-efficient and maliciously secure FL framework in the shuffle\nmodel of DP. Camel first departs from existing works by ambitiously supporting\nintegrity check for the shuffle computation, achieving security against\nmalicious adversary. Specifically, Camel builds on the trending cryptographic\nprimitive of secret-shared shuffle, with custom techniques we develop for\noptimizing system-wide communication efficiency, and for lightweight integrity\nchecks to harden the security of server-side computation. In addition, we also\nderive a significantly tighter bound on the privacy loss through analyzing the\nRenyi differential privacy (RDP) of the overall FL process. Extensive\nexperiments demonstrate that Camel achieves better privacy-utility trade-offs\nthan the state-of-the-art work, with promising performance.",
    "updated" : "2024-10-04T13:13:44Z",
    "published" : "2024-10-04T13:13:44Z",
    "authors" : [
      {
        "name" : "Shuangqing Xu"
      },
      {
        "name" : "Yifeng Zheng"
      },
      {
        "name" : "Zhongyun Hua"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03069v1",
    "title" : "Interactive GDPR-Compliant Privacy Policy Generation for Software\n  Applications",
    "summary" : "Software applications are designed to assist users in conducting a wide range\nof tasks or interactions. They have become prevalent and play an integral part\nin people's lives in this digital era. To use those software applications,\nusers are sometimes requested to provide their personal information. As privacy\nhas become a significant concern and many data protection regulations exist\nworldwide, software applications must provide users with a privacy policy\ndetailing how their personal information is collected and processed. We propose\nan approach that generates a comprehensive and compliant privacy policy with\nrespect to the General Data Protection Regulation (GDPR) for diverse software\napplications. To support this, we first built a library of privacy clauses\nbased on existing privacy policy analysis. We then developed an interactive\nrule-based system that prompts software developers with a series of questions\nand uses their answers to generate a customised privacy policy for a given\nsoftware application. We evaluated privacy policies generated by our approach\nin terms of readability, completeness and coverage and compared them to privacy\npolicies generated by three existing privacy policy generators and a Generative\nAI-based tool. Our evaluation results show that the privacy policy generated by\nour approach is the most complete and comprehensive.",
    "updated" : "2024-10-04T01:22:16Z",
    "published" : "2024-10-04T01:22:16Z",
    "authors" : [
      {
        "name" : "Pattaraporn Sangaroonsilp"
      },
      {
        "name" : "Hoa Khanh Dam"
      },
      {
        "name" : "Omar Haggag"
      },
      {
        "name" : "John Grundy"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02912v1",
    "title" : "Fine-Tuning Language Models with Differential Privacy through Adaptive\n  Noise Allocation",
    "summary" : "Language models are capable of memorizing detailed patterns and information,\nleading to a double-edged effect: they achieve impressive modeling performance\non downstream tasks with the stored knowledge but also raise significant\nprivacy concerns. Traditional differential privacy based training approaches\noffer robust safeguards by employing a uniform noise distribution across all\nparameters. However, this overlooks the distinct sensitivities and\ncontributions of individual parameters in privacy protection and often results\nin suboptimal models. To address these limitations, we propose ANADP, a novel\nalgorithm that adaptively allocates additive noise based on the importance of\nmodel parameters. We demonstrate that ANADP narrows the performance gap between\nregular fine-tuning and traditional DP fine-tuning on a series of datasets\nwhile maintaining the required privacy constraints.",
    "updated" : "2024-10-03T19:02:50Z",
    "published" : "2024-10-03T19:02:50Z",
    "authors" : [
      {
        "name" : "Xianzhi Li"
      },
      {
        "name" : "Ran Zmigrod"
      },
      {
        "name" : "Zhiqiang Ma"
      },
      {
        "name" : "Xiaomo Liu"
      },
      {
        "name" : "Xiaodan Zhu"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05033v1",
    "title" : "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
    "summary" : "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
    "updated" : "2024-10-07T13:33:23Z",
    "published" : "2024-10-07T13:33:23Z",
    "authors" : [
      {
        "name" : "Amirreza Zamani"
      },
      {
        "name" : "Mikael Skoglund"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05020v1",
    "title" : "FRIDA: Free-Rider Detection using Privacy Attacks",
    "summary" : "Federated learning is increasingly popular as it enables multiple parties\nwith limited datasets and resources to train a high-performing machine learning\nmodel collaboratively. However, similarly to other collaborative systems,\nfederated learning is vulnerable to free-riders -- participants who do not\ncontribute to the training but still benefit from the shared model. Free-riders\nnot only compromise the integrity of the learning process but also slow down\nthe convergence of the global model, resulting in increased costs for the\nhonest participants.\n  To address this challenge, we propose FRIDA: free-rider detection using\nprivacy attacks, a framework that leverages inference attacks to detect\nfree-riders. Unlike traditional methods that only capture the implicit effects\nof free-riding, FRIDA directly infers details of the underlying training\ndatasets, revealing characteristics that indicate free-rider behaviour. Through\nextensive experiments, we demonstrate that membership and property inference\nattacks are effective for this purpose. Our evaluation shows that FRIDA\noutperforms state-of-the-art methods, especially in non-IID settings.",
    "updated" : "2024-10-07T13:20:26Z",
    "published" : "2024-10-07T13:20:26Z",
    "authors" : [
      {
        "name" : "Pol G. Recasens"
      },
      {
        "name" : "Ádám Horváth"
      },
      {
        "name" : "Alberto Gutierrez-Torre"
      },
      {
        "name" : "Jordi Torres"
      },
      {
        "name" : "Josep Ll. Berral"
      },
      {
        "name" : "Balázs Pejó"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.04754v1",
    "title" : "A Comprehensive Study on GDPR-Oriented Analysis of Privacy Policies:\n  Taxonomy, Corpus and GDPR Concept Classifiers",
    "summary" : "Machine learning based classifiers that take a privacy policy as the input\nand predict relevant concepts are useful in different applications such as\n(semi-)automated compliance analysis against requirements of the EU GDPR. In\nall past studies, such classifiers produce a concept label per segment (e.g.,\nsentence or paragraph) and their performances were evaluated by using a dataset\nof labeled segments without considering the privacy policy they belong to.\nHowever, such an approach could overestimate the performance in real-world\nsettings, where all segments in a new privacy policy are supposed to be unseen.\nAdditionally, we also observed other research gaps, including the lack of a\nmore complete GDPR taxonomy and the less consideration of hierarchical\ninformation in privacy policies. To fill such research gaps, we developed a\nmore complete GDPR taxonomy, created the first corpus of labeled privacy\npolicies with hierarchical information, and conducted the most comprehensive\nperformance evaluation of GDPR concept classifiers for privacy policies. Our\nwork leads to multiple novel findings, including the confirmed\ninappropriateness of splitting training and test sets at the segment level, the\nbenefits of considering hierarchical information, and the limitations of the\n\"one size fits all\" approach, and the significance of testing cross-corpus\ngeneralizability.",
    "updated" : "2024-10-07T05:19:12Z",
    "published" : "2024-10-07T05:19:12Z",
    "authors" : [
      {
        "name" : "Peng Tang"
      },
      {
        "name" : "Xin Li"
      },
      {
        "name" : "Yuxin Chen"
      },
      {
        "name" : "Weidong Qiu"
      },
      {
        "name" : "Haochen Mei"
      },
      {
        "name" : "Allison Holmes"
      },
      {
        "name" : "Fenghua Li"
      },
      {
        "name" : "Shujun Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.04606v1",
    "title" : "Privacy's Peril: Unmasking the Unregulated Underground Market of Data\n  Brokers and the Suggested Framework",
    "summary" : "The internet is a common place for businesses to collect and store as much\nclient data as possible and computer storage capacity has increased\nexponentially due to this trend. Businesses utilize this data to enhance\ncustomer satisfaction, generate revenue, boost sales, and increase profile.\nHowever, the emerging sector of data brokers is plagued with legal challenges.\nIn part I, we will look at what a data broker is, how it collects information,\nthe data industry, and some of the difficulties it encounters. In Part II, we\nwill look at potential options for regulating data brokers. All options are\nprovided in light of the EU General Data Protection Regulation (GDPR). In Part\nIII, we shall present our analysis and findings.",
    "updated" : "2024-10-06T19:51:31Z",
    "published" : "2024-10-06T19:51:31Z",
    "authors" : [
      {
        "name" : "Rabia Bajwa"
      },
      {
        "name" : "Farah Tasnur Meem"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.04302v1",
    "title" : "PANav: Toward Privacy-Aware Robot Navigation via Vision-Language Models",
    "summary" : "Navigating robots discreetly in human work environments while considering the\npossible privacy implications of robotic tasks presents significant challenges.\nSuch scenarios are increasingly common, for instance, when robots transport\nsensitive objects that demand high levels of privacy in spaces crowded with\nhuman activities. While extensive research has been conducted on robotic path\nplanning and social awareness, current robotic systems still lack the\nfunctionality of privacy-aware navigation in public environments. To address\nthis, we propose a new framework for mobile robot navigation that leverages\nvision-language models to incorporate privacy awareness into adaptive path\nplanning. Specifically, all potential paths from the starting point to the\ndestination are generated using the A* algorithm. Concurrently, the\nvision-language model is used to infer the optimal path for privacy-awareness,\ngiven the environmental layout and the navigational instruction. This approach\naims to minimize the robot's exposure to human activities and preserve the\nprivacy of the robot and its surroundings. Experimental results on the S3DIS\ndataset demonstrate that our framework significantly enhances mobile robots'\nprivacy awareness of navigation in human-shared public environments.\nFurthermore, we demonstrate the practical applicability of our framework by\nsuccessfully navigating a robotic platform through real-world office\nenvironments. The supplementary video and code can be accessed via the\nfollowing link: https://sites.google.com/view/privacy-aware-nav.",
    "updated" : "2024-10-05T22:54:31Z",
    "published" : "2024-10-05T22:54:31Z",
    "authors" : [
      {
        "name" : "Bangguo Yu"
      },
      {
        "name" : "Hamidreza Kasaei"
      },
      {
        "name" : "Ming Cao"
      }
    ],
    "categories" : [
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03925v1",
    "title" : "C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy\n  Policies to Enable Scalable Regulatory Compliance Audits",
    "summary" : "The development of tools and techniques to analyze and extract organizations\ndata habits from privacy policies are critical for scalable regulatory\ncompliance audits. Unfortunately, these tools are becoming increasingly limited\nin their ability to identify compliance issues and fixes. After all, most were\ndeveloped using regulation-agnostic datasets of annotated privacy policies\nobtained from a time before the introduction of landmark privacy regulations\nsuch as EUs GDPR and Californias CCPA. In this paper, we describe the first\nopen regulation-aware dataset of expert-annotated privacy policies, C3PA (CCPA\nPrivacy Policy Provision Annotations), aimed to address this challenge. C3PA\ncontains over 48K expert-labeled privacy policy text segments associated with\nresponses to CCPA-specific disclosure mandates from 411 unique organizations.\nWe demonstrate that the C3PA dataset is uniquely suited for aiding automated\naudits of compliance with CCPA-related disclosure mandates.",
    "updated" : "2024-10-04T21:04:39Z",
    "published" : "2024-10-04T21:04:39Z",
    "authors" : [
      {
        "name" : "Maaz Bin Musa"
      },
      {
        "name" : "Steven M. Winston"
      },
      {
        "name" : "Garrison Allen"
      },
      {
        "name" : "Jacob Schiller"
      },
      {
        "name" : "Kevin Moore"
      },
      {
        "name" : "Sean Quick"
      },
      {
        "name" : "Johnathan Melvin"
      },
      {
        "name" : "Padmini Srinivasan"
      },
      {
        "name" : "Mihailis E. Diamantis"
      },
      {
        "name" : "Rishab Nithyanand"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.06814v1",
    "title" : "Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning",
    "summary" : "Over-parameterized models are typically vulnerable to membership inference\nattacks, which aim to determine whether a specific sample is included in the\ntraining of a given model. Previous Weight regularizations (e.g., L1\nregularization) typically impose uniform penalties on all parameters, leading\nto a suboptimal tradeoff between model utility and privacy. In this work, we\nfirst show that only a small fraction of parameters substantially impact the\nprivacy risk. In light of this, we propose Privacy-aware Sparsity Tuning\n(PAST), a simple fix to the L1 Regularization, by employing adaptive penalties\nto different parameters. Our key idea behind PAST is to promote sparsity in\nparameters that significantly contribute to privacy leakage. In particular, we\nconstruct the adaptive weight for each parameter based on its privacy\nsensitivity, i.e., the gradient of the loss gap with respect to the parameter.\nUsing PAST, the network shrinks the loss gap between members and non-members,\nleading to strong resistance to privacy attacks. Extensive experiments\ndemonstrate the superiority of PAST, achieving a state-of-the-art balance in\nthe privacy-utility trade-off.",
    "updated" : "2024-10-09T12:13:49Z",
    "published" : "2024-10-09T12:13:49Z",
    "authors" : [
      {
        "name" : "Qiang Hu"
      },
      {
        "name" : "Hengxiang Zhang"
      },
      {
        "name" : "Hongxin Wei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.06587v1",
    "title" : "Bots can Snoop: Uncovering and Mitigating Privacy Risks of Bots in Group\n  Chats",
    "summary" : "New privacy concerns arise with chatbots on group messaging platforms.\nChatbots may access information beyond their intended functionalities, such as\nmessages unintended for chatbots or sender's identities. Chatbot operators may\nexploit such information to infer personal information and link users across\ngroups, potentially leading to personal data breaches, pervasive tracking, and\ntargeted advertising. Our analysis of conversation datasets shows that (1)\nchatbots often access far more messages than needed, and (2) when a user joins\na new group with chatbots, there is a 3.4% chance that at least one of the\nchatbots can recognize and associate the user with their previous interactions\nin other groups. Although state-of-the-art group messaging protocols provide\nrobust end-to-end security and some platforms have implemented policies to\nlimit chatbot access, no platforms successfully combine these features. This\npaper introduces SnoopGuard, a secure group messaging protocol that ensures\nuser privacy against chatbots while maintaining strong end-to-end security. Our\nmethod offers selective message access, preventing chatbots from accessing\nunrelated messages, and ensures sender anonymity within the group. SnoopGuard\nachieves $O(\\log n + m)$ message-sending complexity for a group of $n$ users\nand $m$ chatbots, compared to $O(\\log(n + m))$ in state-of-the-art protocols,\nwith acceptable overhead for enhanced privacy. Our prototype implementation\nshows that sending a message in a group of 50 users and 10 chatbots takes about\n30 milliseconds when integrated with Message Layer Security (MLS).",
    "updated" : "2024-10-09T06:37:41Z",
    "published" : "2024-10-09T06:37:41Z",
    "authors" : [
      {
        "name" : "Kai-Hsiang Chou"
      },
      {
        "name" : "Yi-Min Lin"
      },
      {
        "name" : "Yi-An Wang"
      },
      {
        "name" : "Jonathan Weiping Li"
      },
      {
        "name" : "Tiffany Hyun-Jin Kim"
      },
      {
        "name" : "Hsu-Chun Hsiao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.06266v1",
    "title" : "Near Exact Privacy Amplification for Matrix Mechanisms",
    "summary" : "We study the problem of computing the privacy parameters for DP machine\nlearning when using privacy amplification via random batching and noise\ncorrelated across rounds via a correlation matrix $\\textbf{C}$ (i.e., the\nmatrix mechanism). Past work on this problem either only applied to banded\n$\\textbf{C}$, or gave loose privacy parameters. In this work, we give a\nframework for computing near-exact privacy parameters for any lower-triangular,\nnon-negative $\\textbf{C}$. Our framework allows us to optimize the correlation\nmatrix $\\textbf{C}$ while accounting for amplification, whereas past work could\nnot. Empirically, we show this lets us achieve smaller RMSE on prefix sums than\nthe previous state-of-the-art (SOTA). We also show that we can improve on the\nSOTA performance on deep learning tasks. Our two main technical tools are (i)\nusing Monte Carlo accounting to bypass composition, which was the main\ntechnical challenge for past work, and (ii) a \"balls-in-bins\" batching scheme\nthat enables easy privacy analysis and is closer to practical random batching\nthan Poisson sampling.",
    "updated" : "2024-10-08T18:05:56Z",
    "published" : "2024-10-08T18:05:56Z",
    "authors" : [
      {
        "name" : "Christopher A. Choquette-Choo"
      },
      {
        "name" : "Arun Ganesh"
      },
      {
        "name" : "Saminul Haque"
      },
      {
        "name" : "Thomas Steinke"
      },
      {
        "name" : "Abhradeep Thakurta"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05930v1",
    "title" : "Fortify Your Foundations: Practical Privacy and Security for Foundation\n  Model Deployments In The Cloud",
    "summary" : "Foundation Models (FMs) display exceptional performance in tasks such as\nnatural language processing and are being applied across a growing range of\ndisciplines. Although typically trained on large public datasets, FMs are often\nfine-tuned or integrated into Retrieval-Augmented Generation (RAG) systems,\nwhich rely on private data. This access, along with their size and costly\ntraining, heightens the risk of intellectual property theft. Moreover,\nmultimodal FMs may expose sensitive information. In this work, we examine the\nFM threat model and discuss the practicality and comprehensiveness of various\napproaches for securing against them, such as ML-based methods and trusted\nexecution environments (TEEs). We demonstrate that TEEs offer an effective\nbalance between strong security properties, usability, and performance.\nSpecifically, we present a solution achieving less than 10\\% overhead versus\nbare metal for the full Llama2 7B and 13B inference pipelines running inside\n\\intel\\ SGX and \\intel\\ TDX. We also share our configuration files and insights\nfrom our implementation. To our knowledge, our work is the first to show the\npracticality of TEEs for securing FMs.",
    "updated" : "2024-10-08T11:33:09Z",
    "published" : "2024-10-08T11:33:09Z",
    "authors" : [
      {
        "name" : "Marcin Chrapek"
      },
      {
        "name" : "Anjo Vahldiek-Oberwagner"
      },
      {
        "name" : "Marcin Spoczynski"
      },
      {
        "name" : "Scott Constable"
      },
      {
        "name" : "Mona Vij"
      },
      {
        "name" : "Torsten Hoefler"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05907v1",
    "title" : "Privacy-Enhanced Over-the-Air Federated Learning via Client-Driven Power\n  Balancing",
    "summary" : "This paper introduces a novel privacy-enhanced over-the-air Federated\nLearning (OTA-FL) framework using client-driven power balancing (CDPB) to\naddress privacy concerns in OTA-FL systems. In recent studies, a server\ndetermines the power balancing based on the continuous transmission of channel\nstate information (CSI) from each client. Furthermore, they concentrate on\nfulfilling privacy requirements in every global iteration, which can heighten\nthe risk of privacy exposure as the learning process extends. To mitigate these\nrisks, we propose two CDPB strategies -- CDPB-n (noisy) and CDPB-i (idle) --\nallowing clients to adjust transmission power independently, without sharing\nCSI. CDPB-n transmits noise during poor conditions, while CDPB-i pauses\ntransmission until conditions improve. To further enhance privacy and learning\nefficiency, we show a mixed strategy, CDPB-mixed, which combines CDPB-n and\nCDPB-i. Our experimental results show that CDPB outperforms traditional\napproaches in terms of model accuracy and privacy guarantees, providing a\npractical solution for enhancing OTA-FL in resource-constrained environments.",
    "updated" : "2024-10-08T11:05:37Z",
    "published" : "2024-10-08T11:05:37Z",
    "authors" : [
      {
        "name" : "Bumjun Kim"
      },
      {
        "name" : "Hyowoon Seo"
      },
      {
        "name" : "Wan Choi"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05842v1",
    "title" : "Privacy-aware Fully Model-Free Event-triggered Cloud-based HVAC Control",
    "summary" : "Privacy is a major concern when computing-as-a-service (CaaS) platforms,\ne.g., cloud-computing platforms, are utilized for building automation, as CaaS\nplatforms can infer sensitive information, such as occupancy, using the sensor\nmeasurements of a building. Although the existing encrypted model-based control\nalgorithms can ensure the security and privacy of sensor measurements, they are\nhighly complex to implement and require high computational resources, which\nresult in a high cost of using CaaS platforms. To address these issues, in this\npaper, we propose an encrypted fully model-free event-triggered cloud-based\nHVAC control framework that ensures the privacy of occupancy information and\nminimizes the communication and computation overhead associated with encrypted\nHVAC control. To this end, we first develop a model-free controller for\nregulating indoor temperature and CO2 levels. We then design a model-free\nevent-triggering unit which reduces the communication and computation costs of\nencrypted HVAC control using an optimal triggering policy. Finally, we evaluate\nthe performance of the proposed encrypted fully model-free event-triggered\ncloud-based HVAC control framework using the TRNSYS simulator, comparing it to\nan encrypted model-based event-triggered control framework, which uses model\npredictive control to regulate the indoor climate. Our numerical results\ndemonstrate that, compared to the encrypted model-based method, the proposed\nfully model-free framework improves the control performance while reducing the\ncommunication and computation costs. More specifically, it reduces the\ncommunication between the system and the CaaS platform by 64% amount, and its\ncomputation time is 75% less than that of the model-based control.",
    "updated" : "2024-10-08T09:15:21Z",
    "published" : "2024-10-08T09:15:21Z",
    "authors" : [
      {
        "name" : "Zhenan Feng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05725v1",
    "title" : "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge\n  Distillation from Server",
    "summary" : "The success of large language models (LLMs) facilitate many parties to\nfine-tune LLMs on their own private data. However, this practice raises privacy\nconcerns due to the memorization of LLMs. Existing solutions, such as utilizing\nsynthetic data for substitution, struggle to simultaneously improve performance\nand preserve privacy. They either rely on a local model for generation,\nresulting in a performance decline, or take advantage of APIs, directly\nexposing the data to API servers. To address this issue, we propose\n\\textit{KnowledgeSG}, a novel client-server framework which enhances synthetic\ndata quality and improves model performance while ensuring privacy. We achieve\nthis by learning local knowledge from the private data with differential\nprivacy (DP) and distilling professional knowledge from the server.\nAdditionally, inspired by federated learning, we transmit models rather than\ndata between the client and server to prevent privacy leakage. Extensive\nexperiments in medical and financial domains demonstrate the effectiveness of\nKnowledgeSG. Our code is now publicly available at\nhttps://github.com/wwh0411/KnowledgeSG.",
    "updated" : "2024-10-08T06:42:28Z",
    "published" : "2024-10-08T06:42:28Z",
    "authors" : [
      {
        "name" : "Wenhao Wang"
      },
      {
        "name" : "Xiaoyu Liang"
      },
      {
        "name" : "Rui Ye"
      },
      {
        "name" : "Jingyi Chai"
      },
      {
        "name" : "Siheng Chen"
      },
      {
        "name" : "Yanfeng Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05653v1",
    "title" : "A Blockchain-Enhanced Framework for Privacy and Data Integrity in\n  Crowdsourced Drone Services",
    "summary" : "We present an innovative framework that integrates consumer-grade drones into\nbushfire management, addressing both service improvement and data privacy\nconcerns under Australia's Privacy Act 1988. This system establishes a\nmarketplace where bushfire management authorities, as data consumers, access\ncritical information from drone operators, who serve as data providers. The\nframework employs local differential privacy to safeguard the privacy of data\nproviders from all system entities, ensuring compliance with privacy standards.\nAdditionally, a blockchain-based solution facilitates fair data and fee\nexchanges while maintaining immutable records for enhanced accountability.\nValidated through a proof-of-concept implementation, the framework's\nscalability and adaptability make it well-suited for large-scale, real-world\napplications in bushfire management.",
    "updated" : "2024-10-08T03:08:47Z",
    "published" : "2024-10-08T03:08:47Z",
    "authors" : [
      {
        "name" : "Junaid Akram"
      },
      {
        "name" : "Ali Anaissi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05506v1",
    "title" : "Privacy Vulnerabilities in Marginals-based Synthetic Data",
    "summary" : "When acting as a privacy-enhancing technology, synthetic data generation\n(SDG) aims to maintain a resemblance to the real data while excluding\npersonally-identifiable information. Many SDG algorithms provide robust\ndifferential privacy (DP) guarantees to this end. However, we show that the\nstrongest class of SDG algorithms--those that preserve \\textit{marginal\nprobabilities}, or similar statistics, from the underlying data--leak\ninformation about individuals that can be recovered more efficiently than\npreviously understood. We demonstrate this by presenting a novel membership\ninference attack, MAMA-MIA, and evaluate it against three seminal DP SDG\nalgorithms: MST, PrivBayes, and Private-GSD. MAMA-MIA leverages knowledge of\nwhich SDG algorithm was used, allowing it to learn information about the hidden\ndata more accurately, and orders-of-magnitude faster, than other leading\nattacks. We use MAMA-MIA to lend insight into existing SDG vulnerabilities. Our\napproach went on to win the first SNAKE (SaNitization Algorithm under attacK\n... $\\varepsilon$) competition.",
    "updated" : "2024-10-07T21:24:22Z",
    "published" : "2024-10-07T21:24:22Z",
    "authors" : [
      {
        "name" : "Steven Golob"
      },
      {
        "name" : "Sikha Pentyala"
      },
      {
        "name" : "Anuar Maratkhan"
      },
      {
        "name" : "Martine De Cock"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.08122v1",
    "title" : "PP-GWAS: Privacy Preserving Multi-Site Genome-wide Association Studies",
    "summary" : "Genome-wide association studies are pivotal in understanding the genetic\nunderpinnings of complex traits and diseases. Collaborative, multi-site GWAS\naim to enhance statistical power but face obstacles due to the sensitive nature\nof genomic data sharing. Current state-of-the-art methods provide a\nprivacy-focused approach utilizing computationally expensive methods such as\nSecure Multi-Party Computation and Homomorphic Encryption. In this context, we\npresent a novel algorithm PP-GWAS designed to improve upon existing standards\nin terms of computational efficiency and scalability without sacrificing data\nprivacy. This algorithm employs randomized encoding within a distributed\narchitecture to perform stacked ridge regression on a Linear Mixed Model to\nensure rigorous analysis. Experimental evaluation with real world and synthetic\ndata indicates that PP-GWAS can achieve computational speeds twice as fast as\nsimilar state-of-the-art algorithms while using lesser computational resources,\nall while adhering to a robust security model that caters to an all-but-one\nsemi-honest adversary setting. We have assessed its performance using various\ndatasets, emphasizing its potential in facilitating more efficient and private\ngenomic analyses.",
    "updated" : "2024-10-10T17:07:57Z",
    "published" : "2024-10-10T17:07:57Z",
    "authors" : [
      {
        "name" : "Arjhun Swaminathan"
      },
      {
        "name" : "Anika Hannemann"
      },
      {
        "name" : "Ali Burak Ünal"
      },
      {
        "name" : "Nico Pfeifer"
      },
      {
        "name" : "Mete Akgün"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.07900v1",
    "title" : "CL3: A Collaborative Learning Framework for the Medical Data Ensuring\n  Data Privacy in the Hyperconnected Environment",
    "summary" : "In a hyperconnected environment, medical institutions are particularly\nconcerned with data privacy when sharing and transmitting sensitive patient\ninformation due to the risk of data breaches, where malicious actors could\nintercept sensitive information. A collaborative learning framework, including\ntransfer, federated, and incremental learning, can generate efficient, secure,\nand scalable models while requiring less computation, maintaining patient data\nprivacy, and ensuring an up-to-date model. This study aims to address the\ndetection of COVID-19 using chest X-ray images through a proposed collaborative\nlearning framework called CL3. Initially, transfer learning is employed,\nleveraging knowledge from a pre-trained model as the starting global model.\nLocal models from different medical institutes are then integrated, and a new\nglobal model is constructed to adapt to any data drift observed in the local\nmodels. Additionally, incremental learning is considered, allowing continuous\nadaptation to new medical data without forgetting previously learned\ninformation. Experimental results demonstrate that the CL3 framework achieved a\nglobal accuracy of 89.99\\% when using Xception with a batch size of 16 after\nbeing trained for six federated communication rounds.",
    "updated" : "2024-10-10T13:29:12Z",
    "published" : "2024-10-10T13:29:12Z",
    "authors" : [
      {
        "name" : "Mohamamd Zavid Parvez"
      },
      {
        "name" : "Rafiqul Islam"
      },
      {
        "name" : "Md Zahidul Islam"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.07772v1",
    "title" : "Towards Quantifying The Privacy Of Redacted Text",
    "summary" : "In this paper we propose use of a k-anonymity-like approach for evaluating\nthe privacy of redacted text. Given a piece of redacted text we use a state of\nthe art transformer-based deep learning network to reconstruct the original\ntext. This generates multiple full texts that are consistent with the redacted\ntext, i.e. which are grammatical, have the same non-redacted words etc, and\nrepresents each of these using an embedding vector that captures sentence\nsimilarity. In this way we can estimate the number, diversity and quality of\nfull text consistent with the redacted text and so evaluate privacy.",
    "updated" : "2024-10-10T10:00:27Z",
    "published" : "2024-10-10T10:00:27Z",
    "authors" : [
      {
        "name" : "Vaibhav Gusain"
      },
      {
        "name" : "Douglas Leith"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.07632v1",
    "title" : "Provable Privacy Attacks on Trained Shallow Neural Networks",
    "summary" : "We study what provable privacy attacks can be shown on trained, 2-layer ReLU\nneural networks. We explore two types of attacks; data reconstruction attacks,\nand membership inference attacks. We prove that theoretical results on the\nimplicit bias of 2-layer neural networks can be used to provably reconstruct a\nset of which at least a constant fraction are training points in a univariate\nsetting, and can also be used to identify with high probability whether a given\npoint was used in the training set in a high dimensional setting. To the best\nof our knowledge, our work is the first to show provable vulnerabilities in\nthis setting.",
    "updated" : "2024-10-10T05:54:01Z",
    "published" : "2024-10-10T05:54:01Z",
    "authors" : [
      {
        "name" : "Guy Smorodinsky"
      },
      {
        "name" : "Gal Vardi"
      },
      {
        "name" : "Itay Safran"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.07414v1",
    "title" : "Bayes-Nash Generative Privacy Protection Against Membership Inference\n  Attacks",
    "summary" : "An ability to share data, even in aggregated form, is critical to advancing\nboth conventional and data science. However, insofar as such datasets are\ncomprised of individuals, their membership in these datasets is often viewed as\nsensitive, with membership inference attacks (MIAs) threatening to violate\ntheir privacy. We propose a Bayesian game model for privacy-preserving\npublishing of data-sharing mechanism outputs (for example, summary statistics\nfor sharing genomic data). In this game, the defender minimizes a combination\nof expected utility and privacy loss, with the latter being maximized by a\nBayes-rational attacker. We propose a GAN-style algorithm to approximate a\nBayes-Nash equilibrium of this game, and introduce the notions of Bayes-Nash\ngenerative privacy (BNGP) and Bayes generative privacy (BGP) risk that aims to\noptimally balance the defender's privacy and utility in a way that is robust to\nthe attacker's heterogeneous preferences with respect to true and false\npositives. We demonstrate the properties of composition and post-processing for\nBGP risk and establish conditions under which BNGP and pure differential\nprivacy (PDP) are equivalent. We apply our method to sharing summary\nstatistics, where MIAs can re-identify individuals even from aggregated data.\nTheoretical analysis and empirical results demonstrate that our Bayesian\ngame-theoretic method outperforms state-of-the-art approaches for\nprivacy-preserving sharing of summary statistics.",
    "updated" : "2024-10-09T20:29:04Z",
    "published" : "2024-10-09T20:29:04Z",
    "authors" : [
      {
        "name" : "Tao Zhang"
      },
      {
        "name" : "Rajagopal Venkatesaraman"
      },
      {
        "name" : "Rajat K. De"
      },
      {
        "name" : "Bradley A. Malin"
      },
      {
        "name" : "Yevgeniy Vorobeychik"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05725v2",
    "title" : "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge\n  Distillation from Server",
    "summary" : "The success of large language models (LLMs) facilitate many parties to\nfine-tune LLMs on their own private data. However, this practice raises privacy\nconcerns due to the memorization of LLMs. Existing solutions, such as utilizing\nsynthetic data for substitution, struggle to simultaneously improve performance\nand preserve privacy. They either rely on a local model for generation,\nresulting in a performance decline, or take advantage of APIs, directly\nexposing the data to API servers. To address this issue, we propose\nKnowledgeSG, a novel client-server framework which enhances synthetic data\nquality and improves model performance while ensuring privacy. We achieve this\nby learning local knowledge from the private data with differential privacy\n(DP) and distilling professional knowledge from the server. Additionally,\ninspired by federated learning, we transmit models rather than data between the\nclient and server to prevent privacy leakage. Extensive experiments in medical\nand financial domains demonstrate the effectiveness of KnowledgeSG. Our code is\nnow publicly available at https://github.com/wwh0411/KnowledgeSG.",
    "updated" : "2024-10-10T03:58:35Z",
    "published" : "2024-10-08T06:42:28Z",
    "authors" : [
      {
        "name" : "Wenhao Wang"
      },
      {
        "name" : "Xiaoyu Liang"
      },
      {
        "name" : "Rui Ye"
      },
      {
        "name" : "Jingyi Chai"
      },
      {
        "name" : "Siheng Chen"
      },
      {
        "name" : "Yanfeng Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  }
]