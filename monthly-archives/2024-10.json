[
  {
    "id" : "http://arxiv.org/abs/2410.01068v1",
    "title" : "Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness",
    "summary" : "We study the Differential Privacy (DP) guarantee of hidden-state Noisy-SGD\nalgorithms over a bounded domain. Standard privacy analysis for Noisy-SGD\nassumes all internal states are revealed, which leads to a divergent R'enyi DP\nbound with respect to the number of iterations. Ye & Shokri (2022) and\nAltschuler & Talwar (2022) proved convergent bounds for smooth (strongly)\nconvex losses, and raise open questions about whether these assumptions can be\nrelaxed. We provide positive answers by proving convergent R'enyi DP bound for\nnon-convex non-smooth losses, where we show that requiring losses to have\nH\\\"older continuous gradient is sufficient. We also provide a strictly better\nprivacy bound compared to state-of-the-art results for smooth strongly convex\nlosses. Our analysis relies on the improvement of shifted divergence analysis\nin multiple aspects, including forward Wasserstein distance tracking,\nidentifying the optimal shifts allocation, and the H\"older reduction lemma. Our\nresults further elucidate the benefit of hidden-state analysis for DP and its\napplicability.",
    "updated" : "2024-10-01T20:52:08Z",
    "published" : "2024-10-01T20:52:08Z",
    "authors" : [
      {
        "name" : "Eli Chien"
      },
      {
        "name" : "Pan Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00897v1",
    "title" : "The Gradient of Health Data Privacy",
    "summary" : "In the era of digital health and artificial intelligence, the management of\npatient data privacy has become increasingly complex, with significant\nimplications for global health equity and patient trust. This paper introduces\na novel \"privacy gradient\" approach to health data governance, offering a more\nnuanced and adaptive framework than traditional binary privacy models. Our\nmultidimensional concept considers factors such as data sensitivity,\nstakeholder relationships, purpose of use, and temporal aspects, allowing for\ncontext-sensitive privacy protections. Through policy analyses, ethical\nconsiderations, and case studies spanning adolescent health, integrated care,\nand genomic research, we demonstrate how this approach can address critical\nprivacy challenges in diverse healthcare settings worldwide. The privacy\ngradient model has the potential to enhance patient engagement, improve care\ncoordination, and accelerate medical research while safeguarding individual\nprivacy rights. We provide policy recommendations for implementing this\napproach, considering its impact on healthcare systems, research\ninfrastructures, and global health initiatives. This work aims to inform\npolicymakers, healthcare leaders, and digital health innovators, contributing\nto a more equitable, trustworthy, and effective global health data ecosystem in\nthe digital age.",
    "updated" : "2024-10-01T17:35:18Z",
    "published" : "2024-10-01T17:35:18Z",
    "authors" : [
      {
        "name" : "Baihan Lin"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "q-bio.OT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00836v1",
    "title" : "Towards Fairness and Privacy: A Novel Data Pre-processing Optimization\n  Framework for Non-binary Protected Attributes",
    "summary" : "The reason behind the unfair outcomes of AI is often rooted in biased\ndatasets. Therefore, this work presents a framework for addressing fairness by\ndebiasing datasets containing a (non-)binary protected attribute. The framework\nproposes a combinatorial optimization problem where heuristics such as genetic\nalgorithms can be used to solve for the stated fairness objectives. The\nframework addresses this by finding a data subset that minimizes a certain\ndiscrimination measure. Depending on a user-defined setting, the framework\nenables different use cases, such as data removal, the addition of synthetic\ndata, or exclusive use of synthetic data. The exclusive use of synthetic data\nin particular enhances the framework's ability to preserve privacy while\noptimizing for fairness. In a comprehensive evaluation, we demonstrate that\nunder our framework, genetic algorithms can effectively yield fairer datasets\ncompared to the original data. In contrast to prior work, the framework\nexhibits a high degree of flexibility as it is metric- and task-agnostic, can\nbe applied to both binary or non-binary protected attributes, and demonstrates\nefficient runtime.",
    "updated" : "2024-10-01T16:17:43Z",
    "published" : "2024-10-01T16:17:43Z",
    "authors" : [
      {
        "name" : "Manh Khoi Duong"
      },
      {
        "name" : "Stefan Conrad"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00751v1",
    "title" : "Thinking Outside of the Differential Privacy Box: A Case Study in Text\n  Privatization with Language Model Prompting",
    "summary" : "The field of privacy-preserving Natural Language Processing has risen in\npopularity, particularly at a time when concerns about privacy grow with the\nproliferation of Large Language Models. One solution consistently appearing in\nrecent literature has been the integration of Differential Privacy (DP) into\nNLP techniques. In this paper, we take these approaches into critical view,\ndiscussing the restrictions that DP integration imposes, as well as bring to\nlight the challenges that such restrictions entail. To accomplish this, we\nfocus on $\\textbf{DP-Prompt}$, a recent method for text privatization\nleveraging language models to rewrite texts. In particular, we explore this\nrewriting task in multiple scenarios, both with DP and without DP. To drive the\ndiscussion on the merits of DP in NLP, we conduct empirical utility and privacy\nexperiments. Our results demonstrate the need for more discussion on the\nusability of DP in NLP and its benefits over non-DP approaches.",
    "updated" : "2024-10-01T14:46:15Z",
    "published" : "2024-10-01T14:46:15Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00542v1",
    "title" : "Differentially Private Active Learning: Balancing Effective Data\n  Selection and Privacy",
    "summary" : "Active learning (AL) is a widely used technique for optimizing data labeling\nin machine learning by iteratively selecting, labeling, and training on the\nmost informative data. However, its integration with formal privacy-preserving\nmethods, particularly differential privacy (DP), remains largely underexplored.\nWhile some works have explored differentially private AL for specialized\nscenarios like online learning, the fundamental challenge of combining AL with\nDP in standard learning settings has remained unaddressed, severely limiting\nAL's applicability in privacy-sensitive domains. This work addresses this gap\nby introducing differentially private active learning (DP-AL) for standard\nlearning settings. We demonstrate that naively integrating DP-SGD training into\nAL presents substantial challenges in privacy budget allocation and data\nutilization. To overcome these challenges, we propose step amplification, which\nleverages individual sampling probabilities in batch creation to maximize data\npoint participation in training steps, thus optimizing data utilization.\nAdditionally, we investigate the effectiveness of various acquisition functions\nfor data selection under privacy constraints, revealing that many commonly used\nfunctions become impractical. Our experiments on vision and natural language\nprocessing tasks show that DP-AL can improve performance for specific datasets\nand model architectures. However, our findings also highlight the limitations\nof AL in privacy-constrained environments, emphasizing the trade-offs between\nprivacy, model accuracy, and data selection accuracy.",
    "updated" : "2024-10-01T09:34:06Z",
    "published" : "2024-10-01T09:34:06Z",
    "authors" : [
      {
        "name" : "Kristian Schwethelm"
      },
      {
        "name" : "Johannes Kaiser"
      },
      {
        "name" : "Jonas Kuntzer"
      },
      {
        "name" : "Mehmet Yigitsoy"
      },
      {
        "name" : "Daniel Rueckert"
      },
      {
        "name" : "Georgios Kaissis"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00433v1",
    "title" : "PrivTuner with Homomorphic Encryption and LoRA: A P3EFT Scheme for\n  Privacy-Preserving Parameter-Efficient Fine-Tuning of AI Foundation Models",
    "summary" : "AI foundation models have recently demonstrated impressive capabilities\nacross a wide range of tasks. Fine-tuning (FT) is a method of customizing a\npre-trained AI foundation model by further training it on a smaller, targeted\ndataset. In this paper, we initiate the study of the Privacy-Preserving\nParameter-Efficient FT (P3EFT) framework, which can be viewed as the\nintersection of Parameter-Efficient FT (PEFT) and Privacy-Preserving FT (PPFT).\nPEFT modifies only a small subset of the model's parameters to achieve FT\n(i.e., adapting a pre-trained model to a specific dataset), while PPFT uses\nprivacy-preserving technologies to protect the confidentiality of the model\nduring the FT process. There have been many studies on PEFT or PPFT but very\nfew on their fusion, which motivates our work on P3EFT to achieve both\nparameter efficiency and model privacy. To exemplify our P3EFT, we present the\nPrivTuner scheme, which incorporates Fully Homomorphic Encryption (FHE) enabled\nprivacy protection into LoRA (short for ``Low-Rank Adapter''). Intuitively\nspeaking, PrivTuner allows the model owner and the external data owners to\ncollaboratively implement PEFT with encrypted data. After describing PrivTuner\nin detail, we further investigate its energy consumption and privacy\nprotection. Then, we consider a PrivTuner system over wireless communications\nand formulate a joint optimization problem to adaptively minimize energy while\nmaximizing privacy protection, with the optimization variables including FDMA\nbandwidth allocation, wireless transmission power, computational resource\nallocation, and privacy protection. A resource allocation algorithm is devised\nto solve the problem. Experiments demonstrate that our algorithm can\nsignificantly reduce energy consumption while adapting to different privacy\nrequirements.",
    "updated" : "2024-10-01T06:30:06Z",
    "published" : "2024-10-01T06:30:06Z",
    "authors" : [
      {
        "name" : "Yang Li"
      },
      {
        "name" : "Wenhan Yu"
      },
      {
        "name" : "Jun Zhao"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02547v1",
    "title" : "Personalized Quantum Federated Learning for Privacy Image Classification",
    "summary" : "Quantum federated learning has brought about the improvement of privacy image\nclassification, while the lack of personality of the client model may\ncontribute to the suboptimal of quantum federated learning. A personalized\nquantum federated learning algorithm for privacy image classification is\nproposed to enhance the personality of the client model in the case of an\nimbalanced distribution of images. First, a personalized quantum federated\nlearning model is constructed, in which a personalized layer is set for the\nclient model to maintain the personalized parameters. Second, a personalized\nquantum federated learning algorithm is introduced to secure the information\nexchanged between the client and server.Third, the personalized federated\nlearning is applied to image classification on the FashionMNIST dataset, and\nthe experimental results indicate that the personalized quantum federated\nlearning algorithm can obtain global and local models with excellent\nperformance, even in situations where local training samples are imbalanced.\nThe server's accuracy is 100% with 8 clients and a distribution parameter of\n100, outperforming the non-personalized model by 7%. The average client\naccuracy is 2.9% higher than that of the non-personalized model with 2 clients\nand a distribution parameter of 1. Compared to previous quantum federated\nlearning algorithms, the proposed personalized quantum federated learning\nalgorithm eliminates the need for additional local training while safeguarding\nboth model and data privacy.It may facilitate broader adoption and application\nof quantum technologies, and pave the way for more secure, scalable, and\nefficient quantum distribute machine learning solutions.",
    "updated" : "2024-10-03T14:53:04Z",
    "published" : "2024-10-03T14:53:04Z",
    "authors" : [
      {
        "name" : "Jinjing Shi"
      },
      {
        "name" : "Tian Chen"
      },
      {
        "name" : "Shichao Zhang"
      },
      {
        "name" : "Xuelong Li"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02371v1",
    "title" : "NTU-NPU System for Voice Privacy 2024 Challenge",
    "summary" : "In this work, we describe our submissions for the Voice Privacy Challenge\n2024. Rather than proposing a novel speech anonymization system, we enhance the\nprovided baselines to meet all required conditions and improve evaluated\nmetrics. Specifically, we implement emotion embedding and experiment with WavLM\nand ECAPA2 speaker embedders for the B3 baseline. Additionally, we compare\ndifferent speaker and prosody anonymization techniques. Furthermore, we\nintroduce Mean Reversion F0 for B5, which helps to enhance privacy without a\nloss in utility. Finally, we explore disentanglement models, namely $\\beta$-VAE\nand NaturalSpeech3 FACodec.",
    "updated" : "2024-10-03T10:45:10Z",
    "published" : "2024-10-03T10:45:10Z",
    "authors" : [
      {
        "name" : "Nikita Kuzmin"
      },
      {
        "name" : "Hieu-Thi Luong"
      },
      {
        "name" : "Jixun Yao"
      },
      {
        "name" : "Lei Xie"
      },
      {
        "name" : "Kong Aik Lee"
      },
      {
        "name" : "Eng Siong Chng"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02246v1",
    "title" : "PFGuard: A Generative Framework with Privacy and Fairness Safeguards",
    "summary" : "Generative models must ensure both privacy and fairness for Trustworthy AI.\nWhile these goals have been pursued separately, recent studies propose to\ncombine existing privacy and fairness techniques to achieve both goals.\nHowever, naively combining these techniques can be insufficient due to\nprivacy-fairness conflicts, where a sample in a minority group may be amplified\nfor fairness, only to be suppressed for privacy. We demonstrate how these\nconflicts lead to adverse effects, such as privacy violations and unexpected\nfairness-utility tradeoffs. To mitigate these risks, we propose PFGuard, a\ngenerative framework with privacy and fairness safeguards, which simultaneously\naddresses privacy, fairness, and utility. By using an ensemble of multiple\nteacher models, PFGuard balances privacy-fairness conflicts between fair and\nprivate training stages and achieves high utility based on ensemble learning.\nExtensive experiments show that PFGuard successfully generates synthetic data\non high-dimensional data while providing both fairness convergence and strict\nDP guarantees - the first of its kind to our knowledge.",
    "updated" : "2024-10-03T06:37:16Z",
    "published" : "2024-10-03T06:37:16Z",
    "authors" : [
      {
        "name" : "Soyeon Kim"
      },
      {
        "name" : "Yuji Roh"
      },
      {
        "name" : "Geon Heo"
      },
      {
        "name" : "Steven Euijong Whang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03621v1",
    "title" : "A Global Medical Data Security and Privacy Preserving Standards\n  Identification Framework for Electronic Healthcare Consumers",
    "summary" : "Electronic Health Records (EHR) are crucial for the success of digital\nhealthcare, with a focus on putting consumers at the center of this\ntransformation. However, the digitalization of healthcare records brings along\nsecurity and privacy risks for personal data. The major concern is that\ndifferent countries have varying standards for the security and privacy of\nmedical data. This paper proposed a novel and comprehensive framework to\nstandardize these rules globally, bringing them together on a common platform.\nTo support this proposal, the study reviews existing literature to understand\nthe research interest in this issue. It also examines six key laws and\nstandards related to security and privacy, identifying twenty concepts. The\nproposed framework utilized K-means clustering to categorize these concepts and\nidentify five key factors. Finally, an Ordinal Priority Approach is applied to\ndetermine the preferred implementation of these factors in the context of EHRs.\nThe proposed study provides a descriptive then prescriptive framework for the\nimplementation of privacy and security in the context of electronic health\nrecords. Therefore, the findings of the proposed framework are useful for\nprofessionals and policymakers in improving the security and privacy associated\nwith EHRs.",
    "updated" : "2024-10-04T17:22:55Z",
    "published" : "2024-10-04T17:22:55Z",
    "authors" : [
      {
        "name" : "Vinaytosh Mishra"
      },
      {
        "name" : "Kishu Gupta"
      },
      {
        "name" : "Deepika Saxena"
      },
      {
        "name" : "Ashutosh Kumar Singh"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03407v1",
    "title" : "Camel: Communication-Efficient and Maliciously Secure Federated Learning\n  in the Shuffle Model of Differential Privacy",
    "summary" : "Federated learning (FL) has rapidly become a compelling paradigm that enables\nmultiple clients to jointly train a model by sharing only gradient updates for\naggregation, without revealing their local private data. In order to protect\nthe gradient updates which could also be privacy-sensitive, there has been a\nline of work studying local differential privacy (LDP) mechanisms to provide a\nformal privacy guarantee. With LDP mechanisms, clients locally perturb their\ngradient updates before sharing them out for aggregation. However, such\napproaches are known for greatly degrading the model utility, due to heavy\nnoise addition. To enable a better privacy-utility tradeoff, a recently\nemerging trend is to apply the shuffle model of DP in FL, which relies on an\nintermediate shuffling operation on the perturbed gradient updates to achieve\nprivacy amplification. Following this trend, in this paper, we present Camel, a\nnew communication-efficient and maliciously secure FL framework in the shuffle\nmodel of DP. Camel first departs from existing works by ambitiously supporting\nintegrity check for the shuffle computation, achieving security against\nmalicious adversary. Specifically, Camel builds on the trending cryptographic\nprimitive of secret-shared shuffle, with custom techniques we develop for\noptimizing system-wide communication efficiency, and for lightweight integrity\nchecks to harden the security of server-side computation. In addition, we also\nderive a significantly tighter bound on the privacy loss through analyzing the\nRenyi differential privacy (RDP) of the overall FL process. Extensive\nexperiments demonstrate that Camel achieves better privacy-utility trade-offs\nthan the state-of-the-art work, with promising performance.",
    "updated" : "2024-10-04T13:13:44Z",
    "published" : "2024-10-04T13:13:44Z",
    "authors" : [
      {
        "name" : "Shuangqing Xu"
      },
      {
        "name" : "Yifeng Zheng"
      },
      {
        "name" : "Zhongyun Hua"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03069v1",
    "title" : "Interactive GDPR-Compliant Privacy Policy Generation for Software\n  Applications",
    "summary" : "Software applications are designed to assist users in conducting a wide range\nof tasks or interactions. They have become prevalent and play an integral part\nin people's lives in this digital era. To use those software applications,\nusers are sometimes requested to provide their personal information. As privacy\nhas become a significant concern and many data protection regulations exist\nworldwide, software applications must provide users with a privacy policy\ndetailing how their personal information is collected and processed. We propose\nan approach that generates a comprehensive and compliant privacy policy with\nrespect to the General Data Protection Regulation (GDPR) for diverse software\napplications. To support this, we first built a library of privacy clauses\nbased on existing privacy policy analysis. We then developed an interactive\nrule-based system that prompts software developers with a series of questions\nand uses their answers to generate a customised privacy policy for a given\nsoftware application. We evaluated privacy policies generated by our approach\nin terms of readability, completeness and coverage and compared them to privacy\npolicies generated by three existing privacy policy generators and a Generative\nAI-based tool. Our evaluation results show that the privacy policy generated by\nour approach is the most complete and comprehensive.",
    "updated" : "2024-10-04T01:22:16Z",
    "published" : "2024-10-04T01:22:16Z",
    "authors" : [
      {
        "name" : "Pattaraporn Sangaroonsilp"
      },
      {
        "name" : "Hoa Khanh Dam"
      },
      {
        "name" : "Omar Haggag"
      },
      {
        "name" : "John Grundy"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02912v1",
    "title" : "Fine-Tuning Language Models with Differential Privacy through Adaptive\n  Noise Allocation",
    "summary" : "Language models are capable of memorizing detailed patterns and information,\nleading to a double-edged effect: they achieve impressive modeling performance\non downstream tasks with the stored knowledge but also raise significant\nprivacy concerns. Traditional differential privacy based training approaches\noffer robust safeguards by employing a uniform noise distribution across all\nparameters. However, this overlooks the distinct sensitivities and\ncontributions of individual parameters in privacy protection and often results\nin suboptimal models. To address these limitations, we propose ANADP, a novel\nalgorithm that adaptively allocates additive noise based on the importance of\nmodel parameters. We demonstrate that ANADP narrows the performance gap between\nregular fine-tuning and traditional DP fine-tuning on a series of datasets\nwhile maintaining the required privacy constraints.",
    "updated" : "2024-10-03T19:02:50Z",
    "published" : "2024-10-03T19:02:50Z",
    "authors" : [
      {
        "name" : "Xianzhi Li"
      },
      {
        "name" : "Ran Zmigrod"
      },
      {
        "name" : "Zhiqiang Ma"
      },
      {
        "name" : "Xiaomo Liu"
      },
      {
        "name" : "Xiaodan Zhu"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05033v1",
    "title" : "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
    "summary" : "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
    "updated" : "2024-10-07T13:33:23Z",
    "published" : "2024-10-07T13:33:23Z",
    "authors" : [
      {
        "name" : "Amirreza Zamani"
      },
      {
        "name" : "Mikael Skoglund"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.05020v1",
    "title" : "FRIDA: Free-Rider Detection using Privacy Attacks",
    "summary" : "Federated learning is increasingly popular as it enables multiple parties\nwith limited datasets and resources to train a high-performing machine learning\nmodel collaboratively. However, similarly to other collaborative systems,\nfederated learning is vulnerable to free-riders -- participants who do not\ncontribute to the training but still benefit from the shared model. Free-riders\nnot only compromise the integrity of the learning process but also slow down\nthe convergence of the global model, resulting in increased costs for the\nhonest participants.\n  To address this challenge, we propose FRIDA: free-rider detection using\nprivacy attacks, a framework that leverages inference attacks to detect\nfree-riders. Unlike traditional methods that only capture the implicit effects\nof free-riding, FRIDA directly infers details of the underlying training\ndatasets, revealing characteristics that indicate free-rider behaviour. Through\nextensive experiments, we demonstrate that membership and property inference\nattacks are effective for this purpose. Our evaluation shows that FRIDA\noutperforms state-of-the-art methods, especially in non-IID settings.",
    "updated" : "2024-10-07T13:20:26Z",
    "published" : "2024-10-07T13:20:26Z",
    "authors" : [
      {
        "name" : "Pol G. Recasens"
      },
      {
        "name" : "Ádám Horváth"
      },
      {
        "name" : "Alberto Gutierrez-Torre"
      },
      {
        "name" : "Jordi Torres"
      },
      {
        "name" : "Josep Ll. Berral"
      },
      {
        "name" : "Balázs Pejó"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.04754v1",
    "title" : "A Comprehensive Study on GDPR-Oriented Analysis of Privacy Policies:\n  Taxonomy, Corpus and GDPR Concept Classifiers",
    "summary" : "Machine learning based classifiers that take a privacy policy as the input\nand predict relevant concepts are useful in different applications such as\n(semi-)automated compliance analysis against requirements of the EU GDPR. In\nall past studies, such classifiers produce a concept label per segment (e.g.,\nsentence or paragraph) and their performances were evaluated by using a dataset\nof labeled segments without considering the privacy policy they belong to.\nHowever, such an approach could overestimate the performance in real-world\nsettings, where all segments in a new privacy policy are supposed to be unseen.\nAdditionally, we also observed other research gaps, including the lack of a\nmore complete GDPR taxonomy and the less consideration of hierarchical\ninformation in privacy policies. To fill such research gaps, we developed a\nmore complete GDPR taxonomy, created the first corpus of labeled privacy\npolicies with hierarchical information, and conducted the most comprehensive\nperformance evaluation of GDPR concept classifiers for privacy policies. Our\nwork leads to multiple novel findings, including the confirmed\ninappropriateness of splitting training and test sets at the segment level, the\nbenefits of considering hierarchical information, and the limitations of the\n\"one size fits all\" approach, and the significance of testing cross-corpus\ngeneralizability.",
    "updated" : "2024-10-07T05:19:12Z",
    "published" : "2024-10-07T05:19:12Z",
    "authors" : [
      {
        "name" : "Peng Tang"
      },
      {
        "name" : "Xin Li"
      },
      {
        "name" : "Yuxin Chen"
      },
      {
        "name" : "Weidong Qiu"
      },
      {
        "name" : "Haochen Mei"
      },
      {
        "name" : "Allison Holmes"
      },
      {
        "name" : "Fenghua Li"
      },
      {
        "name" : "Shujun Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.04606v1",
    "title" : "Privacy's Peril: Unmasking the Unregulated Underground Market of Data\n  Brokers and the Suggested Framework",
    "summary" : "The internet is a common place for businesses to collect and store as much\nclient data as possible and computer storage capacity has increased\nexponentially due to this trend. Businesses utilize this data to enhance\ncustomer satisfaction, generate revenue, boost sales, and increase profile.\nHowever, the emerging sector of data brokers is plagued with legal challenges.\nIn part I, we will look at what a data broker is, how it collects information,\nthe data industry, and some of the difficulties it encounters. In Part II, we\nwill look at potential options for regulating data brokers. All options are\nprovided in light of the EU General Data Protection Regulation (GDPR). In Part\nIII, we shall present our analysis and findings.",
    "updated" : "2024-10-06T19:51:31Z",
    "published" : "2024-10-06T19:51:31Z",
    "authors" : [
      {
        "name" : "Rabia Bajwa"
      },
      {
        "name" : "Farah Tasnur Meem"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.04302v1",
    "title" : "PANav: Toward Privacy-Aware Robot Navigation via Vision-Language Models",
    "summary" : "Navigating robots discreetly in human work environments while considering the\npossible privacy implications of robotic tasks presents significant challenges.\nSuch scenarios are increasingly common, for instance, when robots transport\nsensitive objects that demand high levels of privacy in spaces crowded with\nhuman activities. While extensive research has been conducted on robotic path\nplanning and social awareness, current robotic systems still lack the\nfunctionality of privacy-aware navigation in public environments. To address\nthis, we propose a new framework for mobile robot navigation that leverages\nvision-language models to incorporate privacy awareness into adaptive path\nplanning. Specifically, all potential paths from the starting point to the\ndestination are generated using the A* algorithm. Concurrently, the\nvision-language model is used to infer the optimal path for privacy-awareness,\ngiven the environmental layout and the navigational instruction. This approach\naims to minimize the robot's exposure to human activities and preserve the\nprivacy of the robot and its surroundings. Experimental results on the S3DIS\ndataset demonstrate that our framework significantly enhances mobile robots'\nprivacy awareness of navigation in human-shared public environments.\nFurthermore, we demonstrate the practical applicability of our framework by\nsuccessfully navigating a robotic platform through real-world office\nenvironments. The supplementary video and code can be accessed via the\nfollowing link: https://sites.google.com/view/privacy-aware-nav.",
    "updated" : "2024-10-05T22:54:31Z",
    "published" : "2024-10-05T22:54:31Z",
    "authors" : [
      {
        "name" : "Bangguo Yu"
      },
      {
        "name" : "Hamidreza Kasaei"
      },
      {
        "name" : "Ming Cao"
      }
    ],
    "categories" : [
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.03925v1",
    "title" : "C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy\n  Policies to Enable Scalable Regulatory Compliance Audits",
    "summary" : "The development of tools and techniques to analyze and extract organizations\ndata habits from privacy policies are critical for scalable regulatory\ncompliance audits. Unfortunately, these tools are becoming increasingly limited\nin their ability to identify compliance issues and fixes. After all, most were\ndeveloped using regulation-agnostic datasets of annotated privacy policies\nobtained from a time before the introduction of landmark privacy regulations\nsuch as EUs GDPR and Californias CCPA. In this paper, we describe the first\nopen regulation-aware dataset of expert-annotated privacy policies, C3PA (CCPA\nPrivacy Policy Provision Annotations), aimed to address this challenge. C3PA\ncontains over 48K expert-labeled privacy policy text segments associated with\nresponses to CCPA-specific disclosure mandates from 411 unique organizations.\nWe demonstrate that the C3PA dataset is uniquely suited for aiding automated\naudits of compliance with CCPA-related disclosure mandates.",
    "updated" : "2024-10-04T21:04:39Z",
    "published" : "2024-10-04T21:04:39Z",
    "authors" : [
      {
        "name" : "Maaz Bin Musa"
      },
      {
        "name" : "Steven M. Winston"
      },
      {
        "name" : "Garrison Allen"
      },
      {
        "name" : "Jacob Schiller"
      },
      {
        "name" : "Kevin Moore"
      },
      {
        "name" : "Sean Quick"
      },
      {
        "name" : "Johnathan Melvin"
      },
      {
        "name" : "Padmini Srinivasan"
      },
      {
        "name" : "Mihailis E. Diamantis"
      },
      {
        "name" : "Rishab Nithyanand"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.IR"
    ]
  }
]