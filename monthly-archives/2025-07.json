[
  {
    "id" : "http://arxiv.org/abs/2507.01808v1",
    "title" : "Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study\n  in Privacy-Preserving Machine Learning to Solve Real-World Problems",
    "summary" : "Small- and medium-sized manufacturers need innovative data tools but, because\nof competition and privacy concerns, often do not want to share their\nproprietary data with researchers who might be interested in helping. This\npaper introduces a privacy-preserving platform by which manufacturers may\nsafely share their data with researchers through secure methods, so that those\nresearchers then create innovative tools to solve the manufacturers' real-world\nproblems, and then provide tools that execute solutions back onto the platform\nfor others to use with privacy and confidentiality guarantees. We illustrate\nthis problem through a particular use case which addresses an important problem\nin the large-scale manufacturing of food crystals, which is that quality\ncontrol relies on image analysis tools. Previous to our research, food crystals\nin the images were manually counted, which required substantial and\ntime-consuming human efforts, but we have developed and deployed a crystal\nanalysis tool which makes this process both more rapid and accurate. The tool\nenables automatic characterization of the crystal size distribution and numbers\nfrom microscope images while the natural imperfections from the sample\npreparation are automatically removed; a machine learning model to count high\nresolution translucent crystals and agglomeration of crystals was also\ndeveloped to aid in these efforts. The resulting algorithm was then packaged\nfor real-world use on the factory floor via a web-based app secured through the\noriginating privacy-preserving platform, allowing manufacturers to use it while\nkeeping their proprietary data secure. After demonstrating this full process,\nfuture directions are also explored.",
    "updated" : "2025-07-02T15:25:43Z",
    "published" : "2025-07-02T15:25:43Z",
    "authors" : [
      {
        "name" : "Xiaoyu Ji"
      },
      {
        "name" : "Jessica Shorland"
      },
      {
        "name" : "Joshua Shank"
      },
      {
        "name" : "Pascal Delpe-Brice"
      },
      {
        "name" : "Latanya Sweeney"
      },
      {
        "name" : "Jan Allebach"
      },
      {
        "name" : "Ali Shakouri"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "68T01, 68T05, 68T45, 94A60"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01752v1",
    "title" : "Tuning without Peeking: Provable Privacy and Generalization Bounds for\n  LLM Post-Training",
    "summary" : "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.",
    "updated" : "2025-07-02T14:29:30Z",
    "published" : "2025-07-02T14:29:30Z",
    "authors" : [
      {
        "name" : "Ismail Labiad"
      },
      {
        "name" : "Mathurin Videau"
      },
      {
        "name" : "Matthieu Kowalski"
      },
      {
        "name" : "Marc Schoenauer"
      },
      {
        "name" : "Alessandro Leite"
      },
      {
        "name" : "Julia Kempe"
      },
      {
        "name" : "Olivier Teytaud"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01581v1",
    "title" : "A Privacy-Preserving Indoor Localization System based on Hierarchical\n  Federated Learning",
    "summary" : "Location information serves as the fundamental element for numerous Internet\nof Things (IoT) applications. Traditional indoor localization techniques often\nproduce significant errors and raise privacy concerns due to centralized data\ncollection. In response, Machine Learning (ML) techniques offer promising\nsolutions by capturing indoor environment variations. However, they typically\nrequire central data aggregation, leading to privacy, bandwidth, and server\nreliability issues. To overcome these challenges, in this paper, we propose a\nFederated Learning (FL)-based approach for dynamic indoor localization using a\nDeep Neural Network (DNN) model. Experimental results show that FL has the\nnearby performance to Centralized Model (CL) while keeping the data privacy,\nbandwidth efficiency and server reliability. This research demonstrates that\nour proposed FL approach provides a viable solution for privacy-enhanced indoor\nlocalization, paving the way for advancements in secure and efficient indoor\nlocalization systems.",
    "updated" : "2025-07-02T10:53:31Z",
    "published" : "2025-07-02T10:53:31Z",
    "authors" : [
      {
        "name" : "Masood Jan"
      },
      {
        "name" : "Wafa Njima"
      },
      {
        "name" : "Xun Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01487v1",
    "title" : "How to Securely Shuffle? A survey about Secure Shufflers for\n  privacy-preserving computations",
    "summary" : "Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building\nblock for private data aggregation. Recently, the field of differential privacy\nhas revived interest in secure shufflers by highlighting the privacy\namplification they can provide in various computations. Although several works\nargue for the utility of secure shufflers, they often treat them as black\nboxes; overlooking the practical vulnerabilities and performance trade-offs of\nexisting implementations. This leaves a central question open: what makes a\ngood secure shuffler?\n  This survey addresses that question by identifying, categorizing, and\ncomparing 26 secure protocols that realize the necessary shuffling\nfunctionality. To enable a meaningful comparison, we adapt and unify existing\nsecurity definitions into a consistent set of properties. We also present an\noverview of privacy-preserving technologies that rely on secure shufflers,\noffer practical guidelines for selecting appropriate protocols, and outline\npromising directions for future work.",
    "updated" : "2025-07-02T08:48:53Z",
    "published" : "2025-07-02T08:48:53Z",
    "authors" : [
      {
        "name" : "Marc Damie"
      },
      {
        "name" : "Florian Hahn"
      },
      {
        "name" : "Andreas Peter"
      },
      {
        "name" : "Jan Ramon"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01216v1",
    "title" : "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
    "summary" : "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
    "updated" : "2025-07-01T22:27:21Z",
    "published" : "2025-07-01T22:27:21Z",
    "authors" : [
      {
        "name" : "Xingke Yang"
      },
      {
        "name" : "Liang Li"
      },
      {
        "name" : "Zhiyi Wan"
      },
      {
        "name" : "Sicong Li"
      },
      {
        "name" : "Hao Wang"
      },
      {
        "name" : "Xiaoqi Qi"
      },
      {
        "name" : "Jiang Liu"
      },
      {
        "name" : "Tomoaki Ohtsuki"
      },
      {
        "name" : "Xin Fu"
      },
      {
        "name" : "Miao Pan"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00920v1",
    "title" : "Privacy-Preserving Quantized Federated Learning with Diverse Precision",
    "summary" : "Federated learning (FL) has emerged as a promising paradigm for distributed\nmachine learning, enabling collaborative training of a global model across\nmultiple local devices without requiring them to share raw data. Despite its\nadvancements, FL is limited by factors such as: (i) privacy risks arising from\nthe unprotected transmission of local model updates to the fusion center (FC)\nand (ii) decreased learning utility caused by heterogeneity in model\nquantization resolution across participating devices. Prior work typically\naddresses only one of these challenges because maintaining learning utility\nunder both privacy risks and quantization heterogeneity is a non-trivial task.\nIn this paper, our aim is therefore to improve the learning utility of a\nprivacy-preserving FL that allows clusters of devices with different\nquantization resolutions to participate in each FL round. Specifically, we\nintroduce a novel stochastic quantizer (SQ) that is designed to simultaneously\nachieve differential privacy (DP) and minimum quantization error. Notably, the\nproposed SQ guarantees bounded distortion, unlike other DP approaches. To\naddress quantization heterogeneity, we introduce a cluster size optimization\ntechnique combined with a linear fusion approach to enhance model aggregation\naccuracy. Numerical simulations validate the benefits of our approach in terms\nof privacy protection and learning utility compared to the conventional\nLaplaceSQ-FL algorithm.",
    "updated" : "2025-07-01T16:26:20Z",
    "published" : "2025-07-01T16:26:20Z",
    "authors" : [
      {
        "name" : "Dang Qua Nguyen"
      },
      {
        "name" : "Morteza Hashemi"
      },
      {
        "name" : "Erik Perrins"
      },
      {
        "name" : "Sergiy A. Vorobyov"
      },
      {
        "name" : "David J. Love"
      },
      {
        "name" : "Taejoon Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00596v1",
    "title" : "Gaze3P: Gaze-Based Prediction of User-Perceived Privacy",
    "summary" : "Privacy is a highly subjective concept and perceived variably by different\nindividuals. Previous research on quantifying user-perceived privacy has\nprimarily relied on questionnaires. Furthermore, applying user-perceived\nprivacy to optimise the parameters of privacy-preserving techniques (PPT)\nremains insufficiently explored. To address these limitations, we introduce\nGaze3P -- the first dataset specifically designed to facilitate systematic\ninvestigations into user-perceived privacy. Our dataset comprises gaze data\nfrom 100 participants and 1,000 stimuli, encompassing a range of private and\nsafe attributes. With Gaze3P, we train a machine learning model to implicitly\nand dynamically predict perceived privacy from human eye gaze. Through\ncomprehensive experiments, we show that the resulting models achieve high\naccuracy. Finally, we illustrate how predicted privacy can be used to optimise\nthe parameters of differentially private mechanisms, thereby enhancing their\nalignment with user expectations.",
    "updated" : "2025-07-01T09:26:38Z",
    "published" : "2025-07-01T09:26:38Z",
    "authors" : [
      {
        "name" : "Mayar Elfares"
      },
      {
        "name" : "Pascal Reisert"
      },
      {
        "name" : "Ralf Küsters"
      },
      {
        "name" : "Andreas Bulling"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00402v1",
    "title" : "GRAND: Graph Release with Assured Node Differential Privacy",
    "summary" : "Differential privacy is a well-established framework for safeguarding\nsensitive information in data. While extensively applied across various\ndomains, its application to network data -- particularly at the node level --\nremains underexplored. Existing methods for node-level privacy either focus\nexclusively on query-based approaches, which restrict output to pre-specified\nnetwork statistics, or fail to preserve key structural properties of the\nnetwork. In this work, we propose GRAND (Graph Release with Assured Node\nDifferential privacy), which is, to the best of our knowledge, the first\nnetwork release mechanism that releases entire networks while ensuring\nnode-level differential privacy and preserving structural properties. Under a\nbroad class of latent space models, we show that the released network\nasymptotically follows the same distribution as the original network. The\neffectiveness of the approach is evaluated through extensive experiments on\nboth synthetic and real-world datasets.",
    "updated" : "2025-07-01T03:39:08Z",
    "published" : "2025-07-01T03:39:08Z",
    "authors" : [
      {
        "name" : "Suqing Liu"
      },
      {
        "name" : "Xuan Bi"
      },
      {
        "name" : "Tianxi Li"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.02727v1",
    "title" : "Quantifying Classifier Utility under Local Differential Privacy",
    "summary" : "Local differential privacy (LDP) provides a rigorous and quantifiable privacy\nguarantee for personal data by introducing perturbation at the data source.\nHowever, quantifying the impact of these perturbations on classifier utility\nremains a theoretical challenge, particularly for complex or black-box\nclassifiers.\n  This paper presents a framework for theoretically quantifying classifier\nutility under LDP mechanisms. The key insight is that LDP perturbation is\nconcentrated around the original data with a specific probability, transforming\nutility analysis of the classifier into its robustness analysis in this\nconcentrated region. Our framework connects the concentration analysis of LDP\nmechanisms with the robustness analysis of classifiers. It treats LDP\nmechanisms as general distributional functions and classifiers as black-box\nfunctions, thus applicable to any LDP mechanism and classifier. A direct\napplication of our utility quantification is guiding the selection of LDP\nmechanisms and privacy parameters for a given classifier. Notably, our analysis\nshows that a piecewise-based mechanism leads to better utility compared to\nalternatives in common scenarios.\n  Using this framework alongside two novel refinement techniques, we conduct\ncase studies on utility quantification for typical mechanism-classifier\ncombinations. The results demonstrate that our theoretical utility\nquantification aligns closely with empirical observations, particularly when\nclassifiers operate in lower-dimensional input spaces.",
    "updated" : "2025-07-03T15:42:10Z",
    "published" : "2025-07-03T15:42:10Z",
    "authors" : [
      {
        "name" : "Ye Zheng"
      },
      {
        "name" : "Yidan Hu"
      }
    ],
    "categories" : [
      "cs.CR",
      "E.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.02414v1",
    "title" : "Privacy-preserving Preselection for Face Identification Based on Packing",
    "summary" : "Face identification systems operating in the ciphertext domain have garnered\nsignificant attention due to increasing privacy concerns and the potential\nrecovery of original facial data. However, as the size of ciphertext template\nlibraries grows, the face retrieval process becomes progressively more\ntime-intensive. To address this challenge, we propose a novel and efficient\nscheme for face retrieval in the ciphertext domain, termed Privacy-Preserving\nPreselection for Face Identification Based on Packing (PFIP). PFIP incorporates\nan innovative preselection mechanism to reduce computational overhead and a\npacking module to enhance the flexibility of biometric systems during the\nenrollment stage. Extensive experiments conducted on the LFW and CASIA datasets\ndemonstrate that PFIP preserves the accuracy of the original face recognition\nmodel, achieving a 100% hit rate while retrieving 1,000 ciphertext face\ntemplates within 300 milliseconds. Compared to existing approaches, PFIP\nachieves a nearly 50x improvement in retrieval efficiency.",
    "updated" : "2025-07-03T08:15:07Z",
    "published" : "2025-07-03T08:15:07Z",
    "authors" : [
      {
        "name" : "Rundong Xin"
      },
      {
        "name" : "Taotao Wang"
      },
      {
        "name" : "Jin Wang"
      },
      {
        "name" : "Chonghe Zhao"
      },
      {
        "name" : "Jing Wang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00920v2",
    "title" : "Privacy-Preserving Quantized Federated Learning with Diverse Precision",
    "summary" : "Federated learning (FL) has emerged as a promising paradigm for distributed\nmachine learning, enabling collaborative training of a global model across\nmultiple local devices without requiring them to share raw data. Despite its\nadvancements, FL is limited by factors such as: (i) privacy risks arising from\nthe unprotected transmission of local model updates to the fusion center (FC)\nand (ii) decreased learning utility caused by heterogeneity in model\nquantization resolution across participating devices. Prior work typically\naddresses only one of these challenges because maintaining learning utility\nunder both privacy risks and quantization heterogeneity is a non-trivial task.\nIn this paper, our aim is therefore to improve the learning utility of a\nprivacy-preserving FL that allows clusters of devices with different\nquantization resolutions to participate in each FL round. Specifically, we\nintroduce a novel stochastic quantizer (SQ) that is designed to simultaneously\nachieve differential privacy (DP) and minimum quantization error. Notably, the\nproposed SQ guarantees bounded distortion, unlike other DP approaches. To\naddress quantization heterogeneity, we introduce a cluster size optimization\ntechnique combined with a linear fusion approach to enhance model aggregation\naccuracy. Numerical simulations validate the benefits of our approach in terms\nof privacy protection and learning utility compared to the conventional\nLaplaceSQ-FL algorithm.",
    "updated" : "2025-07-03T01:49:31Z",
    "published" : "2025-07-01T16:26:20Z",
    "authors" : [
      {
        "name" : "Dang Qua Nguyen"
      },
      {
        "name" : "Morteza Hashemi"
      },
      {
        "name" : "Erik Perrins"
      },
      {
        "name" : "Sergiy A. Vorobyov"
      },
      {
        "name" : "David J. Love"
      },
      {
        "name" : "Taejoon Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05175v1",
    "title" : "Blind Targeting: Personalization under Third-Party Privacy Constraints",
    "summary" : "Major advertising platforms recently increased privacy protections by\nlimiting advertisers' access to individual-level data. Instead of providing\naccess to granular raw data, the platforms only allow a limited number of\naggregate queries to a dataset, which is further protected by adding\ndifferentially private noise. This paper studies whether and how advertisers\ncan design effective targeting policies within these restrictive privacy\npreserving data environments. To achieve this, I develop a probabilistic\nmachine learning method based on Bayesian optimization, which facilitates\ndynamic data exploration. Since Bayesian optimization was designed to sample\npoints from a function to find its maximum, it is not applicable to aggregate\nqueries and to targeting. Therefore, I introduce two innovations: (i) integral\nupdating of posteriors which allows to select the best regions of the data to\nquery rather than individual points and (ii) a targeting-aware acquisition\nfunction that dynamically selects the most informative regions for the\ntargeting task. I identify the conditions of the dataset and privacy\nenvironment that necessitate the use of such a \"smart\" querying strategy. I\napply the strategic querying method to the Criteo AI Labs dataset for uplift\nmodeling (Diemert et al., 2018) that contains visit and conversion data from\n14M users. I show that an intuitive benchmark strategy only achieves 33% of the\nnon-privacy-preserving targeting potential in some cases, while my strategic\nquerying method achieves 97-101% of that potential, and is statistically\nindistinguishable from Causal Forest (Athey et al., 2019): a state-of-the-art\nnon-privacy-preserving machine learning targeting method.",
    "updated" : "2025-07-07T16:30:40Z",
    "published" : "2025-07-07T16:30:40Z",
    "authors" : [
      {
        "name" : "Anya Shchetkina"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.LG",
      "econ.EM",
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04771v1",
    "title" : "Efficient Unlearning with Privacy Guarantees",
    "summary" : "Privacy protection laws, such as the GDPR, grant individuals the right to\nrequest the forgetting of their personal data not only from databases but also\nfrom machine learning (ML) models trained on them. Machine unlearning has\nemerged as a practical means to facilitate model forgetting of data instances\nseen during training. Although some existing machine unlearning methods\nguarantee exact forgetting, they are typically costly in computational terms.\nOn the other hand, more affordable methods do not offer forgetting guarantees\nand are applicable only to specific ML models. In this paper, we present\n\\emph{efficient unlearning with privacy guarantees} (EUPG), a novel machine\nunlearning framework that offers formal privacy guarantees to individuals whose\ndata are being unlearned. EUPG involves pre-training ML models on data\nprotected using privacy models, and it enables {\\em efficient unlearning with\nthe privacy guarantees offered by the privacy models in use}. Through empirical\nevaluation on four heterogeneous data sets protected with $k$-anonymity and\n$\\epsilon$-differential privacy as privacy models, our approach demonstrates\nutility and forgetting effectiveness comparable to those of exact unlearning\nmethods, while significantly reducing computational and storage costs. Our code\nis available at https://github.com/najeebjebreel/EUPG.",
    "updated" : "2025-07-07T08:46:02Z",
    "published" : "2025-07-07T08:46:02Z",
    "authors" : [
      {
        "name" : "Josep Domingo-Ferrer"
      },
      {
        "name" : "Najeeb Jebreel"
      },
      {
        "name" : "David Sánchez"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04528v1",
    "title" : "Towards integration of Privacy Enhancing Technologies in Explainable\n  Artificial Intelligence",
    "summary" : "Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating\nthe risk of non-transparency in the decision-making process of black-box\nArtificial Intelligence (AI) systems. However, despite the benefits, XAI\nmethods are found to leak the privacy of individuals whose data is used in\ntraining or querying the models. Researchers have demonstrated privacy attacks\nthat exploit explanations to infer sensitive personal information of\nindividuals. Currently there is a lack of defenses against known privacy\nattacks targeting explanations when vulnerable XAI are used in production and\nmachine learning as a service system. To address this gap, in this article, we\nexplore Privacy Enhancing Technologies (PETs) as a defense mechanism against\nattribute inference on explanations provided by feature-based XAI methods. We\nempirically evaluate 3 types of PETs, namely synthetic training data,\ndifferentially private training and noise addition, on two categories of\nfeature-based XAI. Our evaluation determines different responses from the\nmitigation methods and side-effects of PETs on other system properties such as\nutility and performance. In the best case, PETs integration in explanations\nreduced the risk of the attack by 49.47%, while maintaining model utility and\nexplanation quality. Through our evaluation, we identify strategies for using\nPETs in XAI for maximizing benefits and minimizing the success of this privacy\nattack on sensitive personal information.",
    "updated" : "2025-07-06T20:45:34Z",
    "published" : "2025-07-06T20:45:34Z",
    "authors" : [
      {
        "name" : "Sonal Allana"
      },
      {
        "name" : "Rozita Dara"
      },
      {
        "name" : "Xiaodong Lin"
      },
      {
        "name" : "Pulei Xiong"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04185v1",
    "title" : "From Legal Text to Tech Specs: Generative AI's Interpretation of Consent\n  in Privacy Law",
    "summary" : "Privacy law and regulation have turned to \"consent\" as the legitimate basis\nfor collecting and processing individuals' data. As governments have rushed to\nenshrine consent requirements in their privacy laws, such as the California\nConsumer Privacy Act (CCPA), significant challenges remain in understanding how\nthese legal mandates are operationalized in software. The opaque nature of\nsoftware development processes further complicates this translation. To address\nthis, we explore the use of Large Language Models (LLMs) in requirements\nengineering to bridge the gap between legal requirements and technical\nimplementation. This study employs a three-step pipeline that involves using an\nLLM to classify software use cases for compliance, generating LLM modifications\nfor non-compliant cases, and manually validating these changes against legal\nstandards. Our preliminary findings highlight the potential of LLMs in\nautomating compliance tasks, while also revealing limitations in their\nreasoning capabilities. By benchmarking LLMs against real-world use cases, this\nresearch provides insights into leveraging AI-driven solutions to enhance legal\ncompliance of software.",
    "updated" : "2025-07-05T23:36:05Z",
    "published" : "2025-07-05T23:36:05Z",
    "authors" : [
      {
        "name" : "Aniket Kesari"
      },
      {
        "name" : "Travis Breaux"
      },
      {
        "name" : "Tom Norton"
      },
      {
        "name" : "Sarah Santos"
      },
      {
        "name" : "Anmol Singhal"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04104v1",
    "title" : "Human-Centered Interactive Anonymization for Privacy-Preserving Machine\n  Learning: A Case for Human-Guided k-Anonymity",
    "summary" : "Privacy-preserving machine learning (ML) seeks to balance data utility and\nprivacy, especially as regulations like the GDPR mandate the anonymization of\npersonal data for ML applications. Conventional anonymization approaches often\nreduce data utility due to indiscriminate generalization or suppression of data\nattributes. In this study, we propose an interactive approach that incorporates\nhuman input into the k-anonymization process, enabling domain experts to guide\nattribute preservation based on contextual importance. Using the UCI Adult\ndataset, we compare classification outcomes of interactive human-influenced\nanonymization with traditional, fully automated methods. Our results show that\nhuman input can enhance data utility in some cases, although results vary\nacross tasks and settings. We discuss limitations of our approach and suggest\npotential areas for improved interactive frameworks in privacy-aware ML.",
    "updated" : "2025-07-05T17:20:18Z",
    "published" : "2025-07-05T17:20:18Z",
    "authors" : [
      {
        "name" : "Sri Harsha Gajavalli"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.03694v1",
    "title" : "Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital\n  Wills",
    "summary" : "This work presents a novel decentralized protocol for digital estate planning\nthat integrates advances distributed computing, and cryptography. The original\nproof-of-concept was constructed using purely solidity contracts. Since then,\nwe have enhanced the implementation into a layer-1 protocol that uses modern\ninterchain communication to connect several heterogeneous chain types. A key\ncontribution of this research is the implementation of several modern\ncryptographic primitives to support various forms of claims for information\nvalidation. These primitives introduce an unmatched level of privacy to the\nprocess of digital inheritance. We also demonstrate on a set of heterogeneous\nsmart contracts, following the same spec, on each chain to serve as entry\npoints, gateways, or bridge contracts that are invoked via a path from the will\nmodule on our protocol, to the contract. This ensures a fair and secure\ndistribution of digital assets in accordance with the wishes of the decedent\nwithout the requirement of moving their funds. This research further extends\nits innovations with a user interaction model, featuring a check-in system and\naccount abstraction process, which enhances flexibility and user-friendliness\nwithout compromising on security. By developing a dedicated permissionless\nblockchain that is secured by a network of validators, and interchain relayers,\nthe proposed protocol signifies a transformation in the digital estate planning\nindustry and illustrates the potential of blockchain technology in\nrevolutionizing traditional legal and personal spheres. Implementing a\ncryptoeconomic network at the core of inheritance planning allows for unique\nincentive compatible economic mechanisms to be constructed.",
    "updated" : "2025-07-04T16:23:32Z",
    "published" : "2025-07-04T16:23:32Z",
    "authors" : [
      {
        "name" : "Jovonni L. PHarr"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CE",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.03033v1",
    "title" : "Preserving Privacy, Increasing Accessibility, and Reducing Cost: An\n  On-Device Artificial Intelligence Model for Medical Transcription and Note\n  Generation",
    "summary" : "Background: Clinical documentation represents a significant burden for\nhealthcare providers, with physicians spending up to 2 hours daily on\nadministrative tasks. Recent advances in large language models (LLMs) offer\npromising solutions, but privacy concerns and computational requirements limit\ntheir adoption in healthcare settings. Objective: To develop and evaluate a\nprivacy-preserving, on-device medical transcription system using a fine-tuned\nLlama 3.2 1B model capable of generating structured medical notes from medical\ntranscriptions while maintaining complete data sovereignty entirely in the\nbrowser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient\nFine-Tuning (PEFT) with LoRA on 1,500 synthetic medical\ntranscription-to-structured note pairs. The model was evaluated against the\nbase Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140\nmodified ACI benchmark cases. Evaluation employed both statistical metrics\n(ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple\nclinical quality dimensions. Results: The fine-tuned OnDevice model\ndemonstrated substantial improvements over the base model. On the ACI\nbenchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1\nimproved from 0.832 to 0.866. Clinical quality assessments showed marked\nreduction in major hallucinations (from 85 to 35 cases) and enhanced factual\ncorrectness (2.81 to 3.54 on 5-point scale). Similar improvements were observed\non the internal evaluation dataset, with composite scores increasing from 3.13\nto 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical\ntranscription yields clinically meaningful improvements while enabling complete\non-device browser deployment. This approach addresses key barriers to AI\nadoption in healthcare: privacy preservation, cost reduction, and accessibility\nfor resource-constrained environments.",
    "updated" : "2025-07-03T01:51:49Z",
    "published" : "2025-07-03T01:51:49Z",
    "authors" : [
      {
        "name" : "Johnson Thomas"
      },
      {
        "name" : "Ayush Mudgal"
      },
      {
        "name" : "Wendao Liu"
      },
      {
        "name" : "Nisten Tahiraj"
      },
      {
        "name" : "Zeeshaan Mohammed"
      },
      {
        "name" : "Dhruv Diddi"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06008v1",
    "title" : "The Impact of Event Data Partitioning on Privacy-aware Process Discovery",
    "summary" : "Information systems support the execution of business processes. The event\nlogs of these executions generally contain sensitive information about\ncustomers, patients, and employees. The corresponding privacy challenges can be\naddressed by anonymizing the event logs while still retaining utility for\nprocess discovery. However, trading off utility and privacy is difficult: the\nhigher the complexity of event log, the higher the loss of utility by\nanonymization. In this work, we propose a pipeline that combines anonymization\nand event data partitioning, where event abstraction is utilized for\npartitioning. By leveraging event abstraction, event logs can be segmented into\nmultiple parts, allowing each sub-log to be anonymized separately. This\npipeline preserves privacy while mitigating the loss of utility. To validate\nour approach, we study the impact of event partitioning on two anonymization\ntechniques using three real-world event logs and two process discovery\ntechniques. Our results demonstrate that event partitioning can bring\nimprovements in process discovery utility for directly-follows-based\nanonymization techniques.",
    "updated" : "2025-07-08T14:13:44Z",
    "published" : "2025-07-08T14:13:44Z",
    "authors" : [
      {
        "name" : "Jungeun Lim"
      },
      {
        "name" : "Stephan A. Fahrenkrog-Petersen"
      },
      {
        "name" : "Xixi Lu"
      },
      {
        "name" : "Jan Mendling"
      },
      {
        "name" : "Minseok Song"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05875v1",
    "title" : "Post-Processing in Local Differential Privacy: An Extensive Evaluation\n  and Benchmark Platform",
    "summary" : "Local differential privacy (LDP) has recently gained prominence as a powerful\nparadigm for collecting and analyzing sensitive data from users' devices.\nHowever, the inherent perturbation added by LDP protocols reduces the utility\nof the collected data. To mitigate this issue, several post-processing (PP)\nmethods have been developed. Yet, the comparative performance of PP methods\nunder diverse settings remains underexplored. In this paper, we present an\nextensive benchmark comprising 6 popular LDP protocols, 7 PP methods, 4 utility\nmetrics, and 6 datasets to evaluate the behaviors and optimality of PP methods\nunder diverse conditions. Through extensive experiments, we show that while PP\ncan substantially improve utility when the privacy budget is small (i.e.,\nstrict privacy), its benefit diminishes as the privacy budget grows. Moreover,\nour findings reveal that the optimal PP method depends on multiple factors,\nincluding the choice of LDP protocol, privacy budget, data characteristics\n(such as distribution and domain size), and the specific utility metric. To\nadvance research in this area and assist practitioners in identifying the most\nsuitable PP method for their setting, we introduce LDP$^3$, an open-source\nbenchmark platform. LDP$^3$ contains all methods used in our experimental\nanalysis, and it is designed in a modular, extensible, and multi-threaded way\nfor future use and development.",
    "updated" : "2025-07-08T10:59:49Z",
    "published" : "2025-07-08T10:59:49Z",
    "authors" : [
      {
        "name" : "Alireza Khodaie"
      },
      {
        "name" : "Berkay Kemal Balioglu"
      },
      {
        "name" : "Mehmet Emre Gursoy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05872v1",
    "title" : "LDP$^3$: An Extensible and Multi-Threaded Toolkit for Local Differential\n  Privacy Protocols and Post-Processing Methods",
    "summary" : "Local differential privacy (LDP) has become a prominent notion for\nprivacy-preserving data collection. While numerous LDP protocols and\npost-processing (PP) methods have been developed, selecting an optimal\ncombination under different privacy budgets and datasets remains a challenge.\nMoreover, the lack of a comprehensive and extensible LDP benchmarking toolkit\nraises difficulties in evaluating new protocols and PP methods. To address\nthese concerns, this paper presents LDP$^3$ (pronounced LDP-Cube), an\nopen-source, extensible, and multi-threaded toolkit for LDP researchers and\npractitioners. LDP$^3$ contains implementations of several LDP protocols, PP\nmethods, and utility metrics in a modular and extensible design. Its modular\ndesign enables developers to conveniently integrate new protocols and PP\nmethods. Furthermore, its multi-threaded nature enables significant reductions\nin execution times via parallelization. Experimental evaluations demonstrate\nthat: (i) using LDP$^3$ to select a good protocol and post-processing method\nsubstantially improves utility compared to a bad or random choice, and (ii) the\nmulti-threaded design of LDP$^3$ brings substantial benefits in terms of\nefficiency.",
    "updated" : "2025-07-08T10:51:42Z",
    "published" : "2025-07-08T10:51:42Z",
    "authors" : [
      {
        "name" : "Berkay Kemal Balioglu"
      },
      {
        "name" : "Alireza Khodaie"
      },
      {
        "name" : "Mehmet Emre Gursoy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05610v1",
    "title" : "On the Inherent Privacy of Zeroth Order Projected Gradient Descent",
    "summary" : "Differentially private zeroth-order optimization methods have recently gained\npopularity in private fine tuning of machine learning models due to their\nreduced memory requirements. Current approaches for privatizing zeroth-order\nmethods rely on adding Gaussian noise to the estimated zeroth-order gradients.\nHowever, since the search direction in the zeroth-order methods is inherently\nrandom, researchers including Tang et al. (2024) and Zhang et al. (2024a) have\nraised an important question: is the inherent noise in zeroth-order estimators\nsufficient to ensure the overall differential privacy of the algorithm? This\nwork settles this question for a class of oracle-based optimization algorithms\nwhere the oracle returns zeroth-order gradient estimates. In particular, we\nshow that for a fixed initialization, there exist strongly convex objective\nfunctions such that running (Projected) Zeroth-Order Gradient Descent (ZO-GD)\nis not differentially private. Furthermore, we show that even with random\ninitialization and without revealing (initial and) intermediate iterates, the\nprivacy loss in ZO-GD can grow superlinearly with the number of iterations when\nminimizing convex objective functions.",
    "updated" : "2025-07-08T02:38:14Z",
    "published" : "2025-07-08T02:38:14Z",
    "authors" : [
      {
        "name" : "Devansh Gupta"
      },
      {
        "name" : "Meisam Razaviyayn"
      },
      {
        "name" : "Vatsal Sharan"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05415v1",
    "title" : "Layered, Overlapping, and Inconsistent: A Large-Scale Analysis of the\n  Multiple Privacy Policies and Controls of U.S. Banks",
    "summary" : "Privacy policies are often complex. An exception is the two-page standardized\nnotice that U.S. financial institutions must provide under the\nGramm-Leach-Bliley Act (GLBA). However, banks now operate websites, mobile\napps, and other services that involve complex data sharing practices that\nrequire additional privacy notices and do-not-sell opt-outs. We conducted a\nlarge-scale analysis of how U.S. banks implement privacy policies and controls\nin response to GLBA; other federal privacy policy requirements; and the\nCalifornia Consumer Privacy Act (CCPA), a key example for U.S. state privacy\nlaws. We focused on the disclosure and control of a set of especially\nprivacy-invasive practices: third-party data sharing for marketing-related\npurposes. We collected privacy policies for the 2,067 largest U.S. banks,\n45.3\\% of which provided multiple policies. Across disclosures and controls\nwithin the \\textit{same} bank, we identified frequent, concerning\ninconsistencies -- such as banks indicating in GLBA notices that they do not\nshare with third parties but disclosing sharing elsewhere, or using third-party\nmarketing/advertising cookies without disclosure. This multiplicity of\npolicies, with the inconsistencies it causes, may create consumer confusion and\nundermine the transparency goals of the very laws that require them. Our\nfindings call into question whether current policy requirements, such as the\nGLBA notice, are achieving their intended goals in today's online banking\nlandscape. We discuss potential avenues for reforming and harmonizing privacy\npolicies and control requirements across federal and state laws.",
    "updated" : "2025-07-07T18:55:48Z",
    "published" : "2025-07-07T18:55:48Z",
    "authors" : [
      {
        "name" : "Lu Xian"
      },
      {
        "name" : "Van Tran"
      },
      {
        "name" : "Lauren Lee"
      },
      {
        "name" : "Meera Kumar"
      },
      {
        "name" : "Yichen Zhang"
      },
      {
        "name" : "Florian Schaub"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05391v1",
    "title" : "Controlling What You Share: Assessing Language Model Adherence to\n  Privacy Preferences",
    "summary" : "Large language models (LLMs) are primarily accessed via commercial APIs, but\nthis often requires users to expose their data to service providers. In this\npaper, we explore how users can stay in control of their data by using privacy\nprofiles: simple natural language instructions that say what should and should\nnot be revealed. We build a framework where a local model uses these\ninstructions to rewrite queries, only hiding details deemed sensitive by the\nuser, before sending them to an external model, thus balancing privacy with\nperformance. To support this research, we introduce PEEP, a multilingual\ndataset of real user queries annotated to mark private content and paired with\nsynthetic privacy profiles. Our experiments with lightweight LLMs show they can\nfollow these instructions to some extent, but also face consistent challenges,\nhighlighting the need for models that better understand and comply with\nuser-defined privacy preferences.",
    "updated" : "2025-07-07T18:22:55Z",
    "published" : "2025-07-07T18:22:55Z",
    "authors" : [
      {
        "name" : "Guillem Ramírez"
      },
      {
        "name" : "Alexandra Birch"
      },
      {
        "name" : "Ivan Titov"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06969v1",
    "title" : "Unifying Re-Identification, Attribute Inference, and Data Reconstruction\n  Risks in Differential Privacy",
    "summary" : "Differentially private (DP) mechanisms are difficult to interpret and\ncalibrate because existing methods for mapping standard privacy parameters to\nconcrete privacy risks -- re-identification, attribute inference, and data\nreconstruction -- are both overly pessimistic and inconsistent. In this work,\nwe use the hypothesis-testing interpretation of DP ($f$-DP), and determine that\nbounds on attack success can take the same unified form across\nre-identification, attribute inference, and data reconstruction risks. Our\nunified bounds are (1) consistent across a multitude of attack settings, and\n(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary\n(including worst-case) levels of baseline risk. Empirically, our results are\ntighter than prior methods using $\\varepsilon$-DP, R\\'enyi DP, and concentrated\nDP. As a result, calibrating noise using our bounds can reduce the required\nnoise by 20% at the same risk level, which yields, e.g., more than 15pp\naccuracy increase in a text classification task. Overall, this unifying\nperspective provides a principled framework for interpreting and calibrating\nthe degree of protection in DP against specific levels of re-identification,\nattribute inference, or data reconstruction risk.",
    "updated" : "2025-07-09T15:59:30Z",
    "published" : "2025-07-09T15:59:30Z",
    "authors" : [
      {
        "name" : "Bogdan Kulynych"
      },
      {
        "name" : "Juan Felipe Gomez"
      },
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Jamie Hayes"
      },
      {
        "name" : "Borja Balle"
      },
      {
        "name" : "Flavio du Pin Calmon"
      },
      {
        "name" : "Jean Louis Raisaro"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06652v1",
    "title" : "Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating\n  for Privacy and Scalable Decision Making",
    "summary" : "Fuzzy systems are a way to allow machines, systems and frameworks to deal\nwith uncertainty, which is not possible in binary systems that most computers\nuse. These systems have already been deployed for certain use cases, and fuzzy\nsystems could be further improved as proposed in this paper. Such technologies\nto draw inspiration from include machine learning and federated learning.\nMachine learning is one of the recent breakthroughs of technology and could be\napplied to fuzzy systems to further improve the results it produces. Federated\nlearning is also one of the recent technologies that have huge potential, which\nallows machine learning training to improve by reducing privacy risk, reducing\nburden on networking infrastructure, and reducing latency of the latest model.\nAspects from federated learning could be used to improve federated learning,\nsuch as applying the idea of updating the fuzzy rules that make up a key part\nof fuzzy systems, to further improve it over time. This paper discusses how\nthese improvements would be implemented in fuzzy systems, and how it would\nimprove fuzzy systems. It also discusses certain limitations on the potential\nimprovements. It concludes that these proposed ideas and improvements require\nfurther investigation to see how far the improvements are, but the potential is\nthere to improve fuzzy systems.",
    "updated" : "2025-07-09T08:34:24Z",
    "published" : "2025-07-09T08:34:24Z",
    "authors" : [
      {
        "name" : "Arthur Alexander Lim"
      },
      {
        "name" : "Zhen Bin It"
      },
      {
        "name" : "Jovan Bowen Heng"
      },
      {
        "name" : "Tee Hui Teo"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06619v1",
    "title" : "Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets\n  with Differential Privacy with HAM10000",
    "summary" : "When applying machine learning to medical image classification, data leakage\nis a critical issue. Previous methods, such as adding noise to gradients for\ndifferential privacy, work well on large datasets like MNIST and CIFAR-100, but\nfail on small, imbalanced medical datasets like HAM10000. This is because the\nimbalanced distribution causes gradients from minority classes to be clipped\nand lose crucial information, while majority classes dominate. This leads the\nmodel to fall into suboptimal solutions early. To address this, we propose\nSAD-DPSGD, which uses a linear decaying mechanism for noise and clipping\nthresholds. By allocating more privacy budget and using higher clipping\nthresholds in the initial training phases, the model avoids suboptimal\nsolutions and enhances performance. Experiments show that SAD-DPSGD outperforms\nAuto-DPSGD on HAM10000, improving accuracy by 2.15% under $\\epsilon = 3.0$ ,\n$\\delta = 10^{-3}$.",
    "updated" : "2025-07-09T07:46:29Z",
    "published" : "2025-07-09T07:46:29Z",
    "authors" : [
      {
        "name" : "Xiaobo Huang"
      },
      {
        "name" : "Fang Xie"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06508v1",
    "title" : "Subgraph Counting under Edge Local Differential Privacy Based on Noisy\n  Adjacency Matrix",
    "summary" : "When analyzing connection patterns within graphs, subgraph counting serves as\nan effective and fundamental approach. Edge-local differential privacy\n(edge-LDP) and shuffle model have been employed to achieve subgraph counting\nunder a privacy-preserving situation. Existing algorithms are plagued by high\ntime complexity, excessive download costs, low accuracy, or dependence on\ntrusted third parties. To address the aforementioned challenges, we propose the\nNoisy Adjacency Matrix (NAM), which combines differential privacy with the\nadjacency matrix of the graph. NAM offers strong versatility and scalability,\nmaking it applicable to a wider range of DP variants, DP mechanisms, and graph\ntypes. Based on NAM, we designed five algorithms (TriOR, TriTR, TriMTR, QuaTR,\nand 2STAR) to count three types of subgraphs: triangles, quadrangles, and\n2-stars. Theoretical and experimental results demonstrate that in triangle\ncounting, TriOR maximizes accuracy with reduced time complexity among one-round\nalgorithms, TriTR achieves optimal accuracy, TriMTR achieves the highest\naccuracy under low download costs, and QuaTR stands as the first quadrangle\ncounting algorithm under pure edge-LDP. We implement edge-LDP for noisy data\nvia a confidence interval-inspired method, providing DP guarantees on\nrandomized data. Our 2STAR algorithm achieves the highest accuracy in 2-star\ncounting and can be derived as a byproduct of two-round triangle or quadrangle\ncounting algorithms, enabling efficient joint estimation of triangle,\nquadrangle, and 2-star counts within two query rounds.",
    "updated" : "2025-07-09T03:13:15Z",
    "published" : "2025-07-09T03:13:15Z",
    "authors" : [
      {
        "name" : "Jintao Guo"
      },
      {
        "name" : "Ying Zhou"
      },
      {
        "name" : "Chao Li"
      },
      {
        "name" : "Guixun Luo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06350v1",
    "title" : "An Architecture for Privacy-Preserving Telemetry Scheme",
    "summary" : "We present a privacy-preserving telemetry aggregation scheme. Our underlying\nfrequency estimation routine works within the framework of differential\nprivacy. The design philosophy follows a client-server architecture.\nFurthermore, the system uses a local differential privacy scheme where data\ngets randomized on the client before submitting the request to the resource\nserver. This scheme allows for data analysis on de-identified data by carefully\nadding noise to prevent re-identification attacks, thereby facilitating public\ndata release without compromising the identifiability of the individual record.\nThis work further enhances privacy guarantees by leveraging Oblivious HTTP\n(OHTTP) to achieve increased privacy protection for data in transit that\naddresses pre-existing privacy vulnerabilities in raw HTTP. We provide an\nimplementation that focuses on frequency estimation with a histogram of a known\ndictionary. Our resulting formulation based on OHTTP has provided stricter\nprivacy safeguards when compared to trusting an organization to manually delete\nidentifying information from the client's request in the ingestor as deployed\nin reference work~\\cite{apple2017}. Code available at\nhttps://github.com/kenluck2001/miscellaneous/tree/master/src/Privacy-Preserving-Telemetry.",
    "updated" : "2025-07-08T19:20:56Z",
    "published" : "2025-07-08T19:20:56Z",
    "authors" : [
      {
        "name" : "Kenneth Odoh"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05610v2",
    "title" : "On the Inherent Privacy of Zeroth Order Projected Gradient Descent",
    "summary" : "Differentially private zeroth-order optimization methods have recently gained\npopularity in private fine tuning of machine learning models due to their\nreduced memory requirements. Current approaches for privatizing zeroth-order\nmethods rely on adding Gaussian noise to the estimated zeroth-order gradients.\nHowever, since the search direction in the zeroth-order methods is inherently\nrandom, researchers including Tang et al. (2024) and Zhang et al. (2024a) have\nraised an important question: is the inherent noise in zeroth-order estimators\nsufficient to ensure the overall differential privacy of the algorithm? This\nwork settles this question for a class of oracle-based optimization algorithms\nwhere the oracle returns zeroth-order gradient estimates. In particular, we\nshow that for a fixed initialization, there exist strongly convex objective\nfunctions such that running (Projected) Zeroth-Order Gradient Descent (ZO-GD)\nis not differentially private. Furthermore, we show that even with random\ninitialization and without revealing (initial and) intermediate iterates, the\nprivacy loss in ZO-GD can grow superlinearly with the number of iterations when\nminimizing convex objective functions.",
    "updated" : "2025-07-09T02:44:06Z",
    "published" : "2025-07-08T02:38:14Z",
    "authors" : [
      {
        "name" : "Devansh Gupta"
      },
      {
        "name" : "Meisam Razaviyayn"
      },
      {
        "name" : "Vatsal Sharan"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.07700v1",
    "title" : "Rethinking the Privacy of Text Embeddings: A Reproducibility Study of\n  \"Text Embeddings Reveal (Almost) As Much As Text\"",
    "summary" : "Text embeddings are fundamental to many natural language processing (NLP)\ntasks, extensively applied in domains such as recommendation systems and\ninformation retrieval (IR). Traditionally, transmitting embeddings instead of\nraw text has been seen as privacy-preserving. However, recent methods such as\nVec2Text challenge this assumption by demonstrating that controlled decoding\ncan successfully reconstruct original texts from black-box embeddings. The\nunexpectedly strong results reported by Vec2Text motivated us to conduct\nfurther verification, particularly considering the typically non-intuitive and\nopaque structure of high-dimensional embedding spaces. In this work, we\nreproduce the Vec2Text framework and evaluate it from two perspectives: (1)\nvalidating the original claims, and (2) extending the study through targeted\nexperiments. First, we successfully replicate the original key results in both\nin-domain and out-of-domain settings, with only minor discrepancies arising due\nto missing artifacts, such as model checkpoints and dataset splits.\nFurthermore, we extend the study by conducting a parameter sensitivity\nanalysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,\npasswords), and exploring embedding quantization as a lightweight privacy\ndefense. Our results show that Vec2Text is effective under ideal conditions,\ncapable of reconstructing even password-like sequences that lack clear\nsemantics. However, we identify key limitations, including its sensitivity to\ninput sequence length. We also find that Gaussian noise and quantization\ntechniques can mitigate the privacy risks posed by Vec2Text, with quantization\noffering a simpler and more widely applicable solution. Our findings emphasize\nthe need for caution in using text embeddings and highlight the importance of\nfurther research into robust defense mechanisms for NLP systems.",
    "updated" : "2025-07-10T12:27:03Z",
    "published" : "2025-07-10T12:27:03Z",
    "authors" : [
      {
        "name" : "Dominykas Seputis"
      },
      {
        "name" : "Yongkang Li"
      },
      {
        "name" : "Karsten Langerak"
      },
      {
        "name" : "Serghei Mihailov"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.07565v1",
    "title" : "Secure Cooperative Gradient Coding: Optimality, Reliability, and Global\n  Privacy",
    "summary" : "This paper studies privacy-sensitive federated learning (FL) with unreliable\ncommunication, focusing on secure aggregation and straggler mitigation. While\nsecure aggregation cryptographically reconstructs the global model without\nexposing client updates, random link failures disrupt its key coordination,\ndegrading model accuracy. Moreover, unreliable communication can lead to\nobjective inconsistency, causing the global model to converge to arbitrary,\nsub-optimal points far from the intended optimum. This paper proposes Secure\nCooperative Gradient Coding (SecCoGC), a practical solution that achieves\nsecure aggregation with arbitrarily strong privacy guarantees and robust\nstraggler mitigation under unreliable communication. SecCoGC operates natively\nin the real field, making it directly applicable to practical deployments. To\nensure equitable privacy protection across clients, we further introduce\nFair-SecCoGC, an extension that enforces fairness in the level of privacy\noffered to all users. To conclude, this paper formally formulates the problem\nof secure aggregation in the real field and presents both general and\ncomputationally efficient key construction methods. Moreover, it provides a\ncomprehensive privacy analysis under Local Mutual Information Privacy (LMIP)\nand Local Differential Privacy (LDP) across all protocol layers. Robustness and\nconvergence properties are also rigorously analyzed. Finally, extensive\nsimulations are performed across diverse network conditions and benchmark\ndatasets to validate the effectiveness of the proposed methods. The results\nshow that SecCoGC achieves strong robustness to unreliable communication under\narbitrarily strong privacy guarantees. It outperforms existing\nprivacy-preserving methods with performance gains of up to 20\\%-70\\%.",
    "updated" : "2025-07-10T09:10:03Z",
    "published" : "2025-07-10T09:10:03Z",
    "authors" : [
      {
        "name" : "Shudi Weng"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.07320v1",
    "title" : "Optimizing Communication and Device Clustering for Clustered Federated\n  Learning with Differential Privacy",
    "summary" : "In this paper, a secure and communication-efficient clustered federated\nlearning (CFL) design is proposed. In our model, several base stations (BSs)\nwith heterogeneous task-handling capabilities and multiple users with\nnon-independent and identically distributed (non-IID) data jointly perform CFL\ntraining incorporating differential privacy (DP) techniques. Since each BS can\nprocess only a subset of the learning tasks and has limited wireless resource\nblocks (RBs) to allocate to users for federated learning (FL) model parameter\ntransmission, it is necessary to jointly optimize RB allocation and user\nscheduling for CFL performance optimization. Meanwhile, our considered CFL\nmethod requires devices to use their limited data and FL model information to\ndetermine their task identities, which may introduce additional communication\noverhead. We formulate an optimization problem whose goal is to minimize the\ntraining loss of all learning tasks while considering device clustering, RB\nallocation, DP noise, and FL model transmission delay. To solve the problem, we\npropose a novel dynamic penalty function assisted value decomposed multi-agent\nreinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to\nindependently determine their connected users, RBs, and DP noise of the\nconnected users but jointly minimize the training loss of all learning tasks\nacross all BSs. Different from the existing MARL methods that assign a large\npenalty for invalid actions, we propose a novel penalty assignment scheme that\nassigns penalty depending on the number of devices that cannot meet\ncommunication constraints (e.g., delay), which can guide the MARL scheme to\nquickly find valid actions, thus improving the convergence speed. Simulation\nresults show that the DPVD-MARL can improve the convergence rate by up to 20%\nand the ultimate accumulated rewards by 15% compared to independent Q-learning.",
    "updated" : "2025-07-09T22:44:26Z",
    "published" : "2025-07-09T22:44:26Z",
    "authors" : [
      {
        "name" : "Dongyu Wei"
      },
      {
        "name" : "Xiaoren Xu"
      },
      {
        "name" : "Shiwen Mao"
      },
      {
        "name" : "Mingzhe Chen"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.07258v1",
    "title" : "FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware\n  Detection in Cross-Silo Federated Learning",
    "summary" : "As IoT ecosystems continue to expand across critical sectors, they have\nbecome prominent targets for increasingly sophisticated and large-scale malware\nattacks. The evolving threat landscape, combined with the sensitive nature of\nIoT-generated data, demands detection frameworks that are both\nprivacy-preserving and resilient to data heterogeneity. Federated Learning (FL)\noffers a promising solution by enabling decentralized model training without\nexposing raw data. However, standard FL algorithms such as FedAvg and FedProx\noften fall short in real-world deployments characterized by class imbalance and\nnon-IID data distributions -- particularly in the presence of rare or disjoint\nmalware classes. To address these challenges, we propose FedP3E\n(Privacy-Preserving Prototype Exchange), a novel FL framework that supports\nindirect cross-client representation sharing while maintaining data privacy.\nEach client constructs class-wise prototypes using Gaussian Mixture Models\n(GMMs), perturbs them with Gaussian noise, and transmits only these compact\nsummaries to the server. The aggregated prototypes are then distributed back to\nclients and integrated into local training, supported by SMOTE-based\naugmentation to enhance representation of minority malware classes. Rather than\nrelying solely on parameter averaging, our prototype-driven mechanism enables\nclients to enrich their local models with complementary structural patterns\nobserved across the federation -- without exchanging raw data or gradients.\nThis targeted strategy reduces the adverse impact of statistical heterogeneity\nwith minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset\nunder realistic cross-silo scenarios with varying degrees of data imbalance.",
    "updated" : "2025-07-09T20:07:35Z",
    "published" : "2025-07-09T20:07:35Z",
    "authors" : [
      {
        "name" : "Rami Darwish"
      },
      {
        "name" : "Mahmoud Abdelsalam"
      },
      {
        "name" : "Sajad Khorsandroo"
      },
      {
        "name" : "Kaushik Roy"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.07210v1",
    "title" : "WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch",
    "summary" : "Smartwatches such as the Apple Watch collect vast amounts of intimate health\nand fitness data as we wear them. Users have little choice regarding how this\ndata is processed: The Apple Watch can only be used with Apple's iPhones, using\ntheir software and their cloud services. We are the first to publicly\nreverse-engineer the watch's wireless protocols, which led to discovering\nmultiple security issues in Apple's proprietary implementation. With\nWatchWitch, our custom Android reimplementation, we break out of Apple's walled\ngarden -- demonstrating practical interoperability with enhanced privacy\ncontrols and data autonomy. We thus pave the way for more consumer choice in\nthe smartwatch ecosystem, offering users more control over their devices.",
    "updated" : "2025-07-09T18:33:58Z",
    "published" : "2025-07-09T18:33:58Z",
    "authors" : [
      {
        "name" : "Nils Rollshausen"
      },
      {
        "name" : "Alexander Heinrich"
      },
      {
        "name" : "Matthias Hollick"
      },
      {
        "name" : "Jiska Classen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08412v1",
    "title" : "Enforcing Speech Content Privacy in Environmental Sound Recordings using\n  Segment-wise Waveform Reversal",
    "summary" : "Environmental sound recordings often contain intelligible speech, raising\nprivacy concerns that limit analysis, sharing and reuse of data. In this paper,\nwe introduce a method that renders speech unintelligible while preserving both\nthe integrity of the acoustic scene, and the overall audio quality. Our\napproach involves reversing waveform segments to distort speech content. This\nprocess is enhanced through a voice activity detection and speech separation\npipeline, which allows for more precise targeting of speech.\n  In order to demonstrate the effectivness of the proposed approach, we\nconsider a three-part evaluation protocol that assesses: 1) speech\nintelligibility using Word Error Rate (WER), 2) sound sources detectability\nusing Sound source Classification Accuracy-Drop (SCAD) from a widely used\npre-trained model, and 3) audio quality using the Fr\\'echet Audio Distance\n(FAD), computed with our reference dataset that contains unaltered speech.\nExperiments on this simulated evaluation dataset, which consists of linear\nmixtures of speech and environmental sound scenes, show that our method\nachieves satisfactory speech intelligibility reduction (97.9% WER), minimal\ndegradation of the sound sources detectability (2.7% SCAD), and high perceptual\nquality (FAD of 1.40). An ablation study further highlights the contribution of\neach component of the pipeline. We also show that incorporating random splicing\nto our speech content privacy enforcement method can enhance the algorithm's\nrobustness to attempt to recover the clean speech, at a slight cost of audio\nquality.",
    "updated" : "2025-07-11T08:48:59Z",
    "published" : "2025-07-11T08:48:59Z",
    "authors" : [
      {
        "name" : "Modan Tailleur"
      },
      {
        "name" : "Mathieu Lagrange"
      },
      {
        "name" : "Pierre Aumond"
      },
      {
        "name" : "Vincent Tourre"
      }
    ],
    "categories" : [
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08158v1",
    "title" : "Beyond the Worst Case: Extending Differential Privacy Guarantees to\n  Realistic Adversaries",
    "summary" : "Differential Privacy (DP) is a family of definitions that bound the\nworst-case privacy leakage of a mechanism. One important feature of the\nworst-case DP guarantee is it naturally implies protections against adversaries\nwith less prior information, more sophisticated attack goals, and complex\nmeasures of a successful attack. However, the analytical tradeoffs between the\nadversarial model and the privacy protections conferred by DP are not well\nunderstood thus far. To that end, this work sheds light on what the worst-case\nguarantee of DP implies about the success of attackers that are more\nrepresentative of real-world privacy risks.\n  In this paper, we present a single flexible framework that generalizes and\nextends the patchwork of bounds on DP mechanisms found in prior work. Our\nframework allows us to compute high-probability guarantees for DP mechanisms on\na large family of natural attack settings that previous bounds do not capture.\nOne class of such settings is the approximate reconstruction of multiple\nindividuals' data, such as inferring nearly entire columns of a tabular data\nset from noisy marginals and extracting sensitive information from DP-trained\nlanguage models.\n  We conduct two empirical case studies to illustrate the versatility of our\nbounds and compare them to the success of state-of-the-art attacks.\nSpecifically, we study attacks that extract non-uniform PII from a DP-trained\nlanguage model, as well as multi-column reconstruction attacks where the\nadversary has access to some columns in the clear and attempts to reconstruct\nthe remaining columns for each person's record. We find that the absolute\nprivacy risk of attacking non-uniform data is highly dependent on the\nadversary's prior probability of success. Our high probability bounds give us a\nnuanced understanding of the privacy leakage of DP mechanisms in a variety of\npreviously understudied attack settings.",
    "updated" : "2025-07-10T20:36:31Z",
    "published" : "2025-07-10T20:36:31Z",
    "authors" : [
      {
        "name" : "Marika Swanberg"
      },
      {
        "name" : "Meenatchi Sundaram Muthu Selva Annamalai"
      },
      {
        "name" : "Jamie Hayes"
      },
      {
        "name" : "Borja Balle"
      },
      {
        "name" : "Adam Smith"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08050v1",
    "title" : "An Enhanced Privacy-preserving Federated Few-shot Learning Framework for\n  Respiratory Disease Diagnosis",
    "summary" : "The labor-intensive nature of medical data annotation presents a significant\nchallenge for respiratory disease diagnosis, resulting in a scarcity of\nhigh-quality labeled datasets in resource-constrained settings. Moreover,\npatient privacy concerns complicate the direct sharing of local medical data\nacross institutions, and existing centralized data-driven approaches, which\nrely on amounts of available data, often compromise data privacy. This study\nproposes a federated few-shot learning framework with privacy-preserving\nmechanisms to address the issues of limited labeled data and privacy protection\nin diagnosing respiratory diseases. In particular, a meta-stochastic gradient\ndescent algorithm is proposed to mitigate the overfitting problem that arises\nfrom insufficient data when employing traditional gradient descent methods for\nneural network training. Furthermore, to ensure data privacy against gradient\nleakage, differential privacy noise from a standard Gaussian distribution is\nintegrated into the gradients during the training of private models with local\ndata, thereby preventing the reconstruction of medical images. Given the\nimpracticality of centralizing respiratory disease data dispersed across\nvarious medical institutions, a weighted average algorithm is employed to\naggregate local diagnostic models from different clients, enhancing the\nadaptability of a model across diverse scenarios. Experimental results show\nthat the proposed method yields compelling results with the implementation of\ndifferential privacy, while effectively diagnosing respiratory diseases using\ndata from different structures, categories, and distributions.",
    "updated" : "2025-07-10T07:47:58Z",
    "published" : "2025-07-10T07:47:58Z",
    "authors" : [
      {
        "name" : "Ming Wang"
      },
      {
        "name" : "Zhaoyang Duan"
      },
      {
        "name" : "Dong Xue"
      },
      {
        "name" : "Fangzhou Liu"
      },
      {
        "name" : "Zhongheng Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  }
]