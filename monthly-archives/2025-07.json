[
  {
    "id" : "http://arxiv.org/abs/2507.01808v1",
    "title" : "Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study\n  in Privacy-Preserving Machine Learning to Solve Real-World Problems",
    "summary" : "Small- and medium-sized manufacturers need innovative data tools but, because\nof competition and privacy concerns, often do not want to share their\nproprietary data with researchers who might be interested in helping. This\npaper introduces a privacy-preserving platform by which manufacturers may\nsafely share their data with researchers through secure methods, so that those\nresearchers then create innovative tools to solve the manufacturers' real-world\nproblems, and then provide tools that execute solutions back onto the platform\nfor others to use with privacy and confidentiality guarantees. We illustrate\nthis problem through a particular use case which addresses an important problem\nin the large-scale manufacturing of food crystals, which is that quality\ncontrol relies on image analysis tools. Previous to our research, food crystals\nin the images were manually counted, which required substantial and\ntime-consuming human efforts, but we have developed and deployed a crystal\nanalysis tool which makes this process both more rapid and accurate. The tool\nenables automatic characterization of the crystal size distribution and numbers\nfrom microscope images while the natural imperfections from the sample\npreparation are automatically removed; a machine learning model to count high\nresolution translucent crystals and agglomeration of crystals was also\ndeveloped to aid in these efforts. The resulting algorithm was then packaged\nfor real-world use on the factory floor via a web-based app secured through the\noriginating privacy-preserving platform, allowing manufacturers to use it while\nkeeping their proprietary data secure. After demonstrating this full process,\nfuture directions are also explored.",
    "updated" : "2025-07-02T15:25:43Z",
    "published" : "2025-07-02T15:25:43Z",
    "authors" : [
      {
        "name" : "Xiaoyu Ji"
      },
      {
        "name" : "Jessica Shorland"
      },
      {
        "name" : "Joshua Shank"
      },
      {
        "name" : "Pascal Delpe-Brice"
      },
      {
        "name" : "Latanya Sweeney"
      },
      {
        "name" : "Jan Allebach"
      },
      {
        "name" : "Ali Shakouri"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "68T01, 68T05, 68T45, 94A60"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01752v1",
    "title" : "Tuning without Peeking: Provable Privacy and Generalization Bounds for\n  LLM Post-Training",
    "summary" : "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.",
    "updated" : "2025-07-02T14:29:30Z",
    "published" : "2025-07-02T14:29:30Z",
    "authors" : [
      {
        "name" : "Ismail Labiad"
      },
      {
        "name" : "Mathurin Videau"
      },
      {
        "name" : "Matthieu Kowalski"
      },
      {
        "name" : "Marc Schoenauer"
      },
      {
        "name" : "Alessandro Leite"
      },
      {
        "name" : "Julia Kempe"
      },
      {
        "name" : "Olivier Teytaud"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01581v1",
    "title" : "A Privacy-Preserving Indoor Localization System based on Hierarchical\n  Federated Learning",
    "summary" : "Location information serves as the fundamental element for numerous Internet\nof Things (IoT) applications. Traditional indoor localization techniques often\nproduce significant errors and raise privacy concerns due to centralized data\ncollection. In response, Machine Learning (ML) techniques offer promising\nsolutions by capturing indoor environment variations. However, they typically\nrequire central data aggregation, leading to privacy, bandwidth, and server\nreliability issues. To overcome these challenges, in this paper, we propose a\nFederated Learning (FL)-based approach for dynamic indoor localization using a\nDeep Neural Network (DNN) model. Experimental results show that FL has the\nnearby performance to Centralized Model (CL) while keeping the data privacy,\nbandwidth efficiency and server reliability. This research demonstrates that\nour proposed FL approach provides a viable solution for privacy-enhanced indoor\nlocalization, paving the way for advancements in secure and efficient indoor\nlocalization systems.",
    "updated" : "2025-07-02T10:53:31Z",
    "published" : "2025-07-02T10:53:31Z",
    "authors" : [
      {
        "name" : "Masood Jan"
      },
      {
        "name" : "Wafa Njima"
      },
      {
        "name" : "Xun Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01487v1",
    "title" : "How to Securely Shuffle? A survey about Secure Shufflers for\n  privacy-preserving computations",
    "summary" : "Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building\nblock for private data aggregation. Recently, the field of differential privacy\nhas revived interest in secure shufflers by highlighting the privacy\namplification they can provide in various computations. Although several works\nargue for the utility of secure shufflers, they often treat them as black\nboxes; overlooking the practical vulnerabilities and performance trade-offs of\nexisting implementations. This leaves a central question open: what makes a\ngood secure shuffler?\n  This survey addresses that question by identifying, categorizing, and\ncomparing 26 secure protocols that realize the necessary shuffling\nfunctionality. To enable a meaningful comparison, we adapt and unify existing\nsecurity definitions into a consistent set of properties. We also present an\noverview of privacy-preserving technologies that rely on secure shufflers,\noffer practical guidelines for selecting appropriate protocols, and outline\npromising directions for future work.",
    "updated" : "2025-07-02T08:48:53Z",
    "published" : "2025-07-02T08:48:53Z",
    "authors" : [
      {
        "name" : "Marc Damie"
      },
      {
        "name" : "Florian Hahn"
      },
      {
        "name" : "Andreas Peter"
      },
      {
        "name" : "Jan Ramon"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01216v1",
    "title" : "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
    "summary" : "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
    "updated" : "2025-07-01T22:27:21Z",
    "published" : "2025-07-01T22:27:21Z",
    "authors" : [
      {
        "name" : "Xingke Yang"
      },
      {
        "name" : "Liang Li"
      },
      {
        "name" : "Zhiyi Wan"
      },
      {
        "name" : "Sicong Li"
      },
      {
        "name" : "Hao Wang"
      },
      {
        "name" : "Xiaoqi Qi"
      },
      {
        "name" : "Jiang Liu"
      },
      {
        "name" : "Tomoaki Ohtsuki"
      },
      {
        "name" : "Xin Fu"
      },
      {
        "name" : "Miao Pan"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00920v1",
    "title" : "Privacy-Preserving Quantized Federated Learning with Diverse Precision",
    "summary" : "Federated learning (FL) has emerged as a promising paradigm for distributed\nmachine learning, enabling collaborative training of a global model across\nmultiple local devices without requiring them to share raw data. Despite its\nadvancements, FL is limited by factors such as: (i) privacy risks arising from\nthe unprotected transmission of local model updates to the fusion center (FC)\nand (ii) decreased learning utility caused by heterogeneity in model\nquantization resolution across participating devices. Prior work typically\naddresses only one of these challenges because maintaining learning utility\nunder both privacy risks and quantization heterogeneity is a non-trivial task.\nIn this paper, our aim is therefore to improve the learning utility of a\nprivacy-preserving FL that allows clusters of devices with different\nquantization resolutions to participate in each FL round. Specifically, we\nintroduce a novel stochastic quantizer (SQ) that is designed to simultaneously\nachieve differential privacy (DP) and minimum quantization error. Notably, the\nproposed SQ guarantees bounded distortion, unlike other DP approaches. To\naddress quantization heterogeneity, we introduce a cluster size optimization\ntechnique combined with a linear fusion approach to enhance model aggregation\naccuracy. Numerical simulations validate the benefits of our approach in terms\nof privacy protection and learning utility compared to the conventional\nLaplaceSQ-FL algorithm.",
    "updated" : "2025-07-01T16:26:20Z",
    "published" : "2025-07-01T16:26:20Z",
    "authors" : [
      {
        "name" : "Dang Qua Nguyen"
      },
      {
        "name" : "Morteza Hashemi"
      },
      {
        "name" : "Erik Perrins"
      },
      {
        "name" : "Sergiy A. Vorobyov"
      },
      {
        "name" : "David J. Love"
      },
      {
        "name" : "Taejoon Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00596v1",
    "title" : "Gaze3P: Gaze-Based Prediction of User-Perceived Privacy",
    "summary" : "Privacy is a highly subjective concept and perceived variably by different\nindividuals. Previous research on quantifying user-perceived privacy has\nprimarily relied on questionnaires. Furthermore, applying user-perceived\nprivacy to optimise the parameters of privacy-preserving techniques (PPT)\nremains insufficiently explored. To address these limitations, we introduce\nGaze3P -- the first dataset specifically designed to facilitate systematic\ninvestigations into user-perceived privacy. Our dataset comprises gaze data\nfrom 100 participants and 1,000 stimuli, encompassing a range of private and\nsafe attributes. With Gaze3P, we train a machine learning model to implicitly\nand dynamically predict perceived privacy from human eye gaze. Through\ncomprehensive experiments, we show that the resulting models achieve high\naccuracy. Finally, we illustrate how predicted privacy can be used to optimise\nthe parameters of differentially private mechanisms, thereby enhancing their\nalignment with user expectations.",
    "updated" : "2025-07-01T09:26:38Z",
    "published" : "2025-07-01T09:26:38Z",
    "authors" : [
      {
        "name" : "Mayar Elfares"
      },
      {
        "name" : "Pascal Reisert"
      },
      {
        "name" : "Ralf Küsters"
      },
      {
        "name" : "Andreas Bulling"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00402v1",
    "title" : "GRAND: Graph Release with Assured Node Differential Privacy",
    "summary" : "Differential privacy is a well-established framework for safeguarding\nsensitive information in data. While extensively applied across various\ndomains, its application to network data -- particularly at the node level --\nremains underexplored. Existing methods for node-level privacy either focus\nexclusively on query-based approaches, which restrict output to pre-specified\nnetwork statistics, or fail to preserve key structural properties of the\nnetwork. In this work, we propose GRAND (Graph Release with Assured Node\nDifferential privacy), which is, to the best of our knowledge, the first\nnetwork release mechanism that releases entire networks while ensuring\nnode-level differential privacy and preserving structural properties. Under a\nbroad class of latent space models, we show that the released network\nasymptotically follows the same distribution as the original network. The\neffectiveness of the approach is evaluated through extensive experiments on\nboth synthetic and real-world datasets.",
    "updated" : "2025-07-01T03:39:08Z",
    "published" : "2025-07-01T03:39:08Z",
    "authors" : [
      {
        "name" : "Suqing Liu"
      },
      {
        "name" : "Xuan Bi"
      },
      {
        "name" : "Tianxi Li"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.02727v1",
    "title" : "Quantifying Classifier Utility under Local Differential Privacy",
    "summary" : "Local differential privacy (LDP) provides a rigorous and quantifiable privacy\nguarantee for personal data by introducing perturbation at the data source.\nHowever, quantifying the impact of these perturbations on classifier utility\nremains a theoretical challenge, particularly for complex or black-box\nclassifiers.\n  This paper presents a framework for theoretically quantifying classifier\nutility under LDP mechanisms. The key insight is that LDP perturbation is\nconcentrated around the original data with a specific probability, transforming\nutility analysis of the classifier into its robustness analysis in this\nconcentrated region. Our framework connects the concentration analysis of LDP\nmechanisms with the robustness analysis of classifiers. It treats LDP\nmechanisms as general distributional functions and classifiers as black-box\nfunctions, thus applicable to any LDP mechanism and classifier. A direct\napplication of our utility quantification is guiding the selection of LDP\nmechanisms and privacy parameters for a given classifier. Notably, our analysis\nshows that a piecewise-based mechanism leads to better utility compared to\nalternatives in common scenarios.\n  Using this framework alongside two novel refinement techniques, we conduct\ncase studies on utility quantification for typical mechanism-classifier\ncombinations. The results demonstrate that our theoretical utility\nquantification aligns closely with empirical observations, particularly when\nclassifiers operate in lower-dimensional input spaces.",
    "updated" : "2025-07-03T15:42:10Z",
    "published" : "2025-07-03T15:42:10Z",
    "authors" : [
      {
        "name" : "Ye Zheng"
      },
      {
        "name" : "Yidan Hu"
      }
    ],
    "categories" : [
      "cs.CR",
      "E.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.02414v1",
    "title" : "Privacy-preserving Preselection for Face Identification Based on Packing",
    "summary" : "Face identification systems operating in the ciphertext domain have garnered\nsignificant attention due to increasing privacy concerns and the potential\nrecovery of original facial data. However, as the size of ciphertext template\nlibraries grows, the face retrieval process becomes progressively more\ntime-intensive. To address this challenge, we propose a novel and efficient\nscheme for face retrieval in the ciphertext domain, termed Privacy-Preserving\nPreselection for Face Identification Based on Packing (PFIP). PFIP incorporates\nan innovative preselection mechanism to reduce computational overhead and a\npacking module to enhance the flexibility of biometric systems during the\nenrollment stage. Extensive experiments conducted on the LFW and CASIA datasets\ndemonstrate that PFIP preserves the accuracy of the original face recognition\nmodel, achieving a 100% hit rate while retrieving 1,000 ciphertext face\ntemplates within 300 milliseconds. Compared to existing approaches, PFIP\nachieves a nearly 50x improvement in retrieval efficiency.",
    "updated" : "2025-07-03T08:15:07Z",
    "published" : "2025-07-03T08:15:07Z",
    "authors" : [
      {
        "name" : "Rundong Xin"
      },
      {
        "name" : "Taotao Wang"
      },
      {
        "name" : "Jin Wang"
      },
      {
        "name" : "Chonghe Zhao"
      },
      {
        "name" : "Jing Wang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00920v2",
    "title" : "Privacy-Preserving Quantized Federated Learning with Diverse Precision",
    "summary" : "Federated learning (FL) has emerged as a promising paradigm for distributed\nmachine learning, enabling collaborative training of a global model across\nmultiple local devices without requiring them to share raw data. Despite its\nadvancements, FL is limited by factors such as: (i) privacy risks arising from\nthe unprotected transmission of local model updates to the fusion center (FC)\nand (ii) decreased learning utility caused by heterogeneity in model\nquantization resolution across participating devices. Prior work typically\naddresses only one of these challenges because maintaining learning utility\nunder both privacy risks and quantization heterogeneity is a non-trivial task.\nIn this paper, our aim is therefore to improve the learning utility of a\nprivacy-preserving FL that allows clusters of devices with different\nquantization resolutions to participate in each FL round. Specifically, we\nintroduce a novel stochastic quantizer (SQ) that is designed to simultaneously\nachieve differential privacy (DP) and minimum quantization error. Notably, the\nproposed SQ guarantees bounded distortion, unlike other DP approaches. To\naddress quantization heterogeneity, we introduce a cluster size optimization\ntechnique combined with a linear fusion approach to enhance model aggregation\naccuracy. Numerical simulations validate the benefits of our approach in terms\nof privacy protection and learning utility compared to the conventional\nLaplaceSQ-FL algorithm.",
    "updated" : "2025-07-03T01:49:31Z",
    "published" : "2025-07-01T16:26:20Z",
    "authors" : [
      {
        "name" : "Dang Qua Nguyen"
      },
      {
        "name" : "Morteza Hashemi"
      },
      {
        "name" : "Erik Perrins"
      },
      {
        "name" : "Sergiy A. Vorobyov"
      },
      {
        "name" : "David J. Love"
      },
      {
        "name" : "Taejoon Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05175v1",
    "title" : "Blind Targeting: Personalization under Third-Party Privacy Constraints",
    "summary" : "Major advertising platforms recently increased privacy protections by\nlimiting advertisers' access to individual-level data. Instead of providing\naccess to granular raw data, the platforms only allow a limited number of\naggregate queries to a dataset, which is further protected by adding\ndifferentially private noise. This paper studies whether and how advertisers\ncan design effective targeting policies within these restrictive privacy\npreserving data environments. To achieve this, I develop a probabilistic\nmachine learning method based on Bayesian optimization, which facilitates\ndynamic data exploration. Since Bayesian optimization was designed to sample\npoints from a function to find its maximum, it is not applicable to aggregate\nqueries and to targeting. Therefore, I introduce two innovations: (i) integral\nupdating of posteriors which allows to select the best regions of the data to\nquery rather than individual points and (ii) a targeting-aware acquisition\nfunction that dynamically selects the most informative regions for the\ntargeting task. I identify the conditions of the dataset and privacy\nenvironment that necessitate the use of such a \"smart\" querying strategy. I\napply the strategic querying method to the Criteo AI Labs dataset for uplift\nmodeling (Diemert et al., 2018) that contains visit and conversion data from\n14M users. I show that an intuitive benchmark strategy only achieves 33% of the\nnon-privacy-preserving targeting potential in some cases, while my strategic\nquerying method achieves 97-101% of that potential, and is statistically\nindistinguishable from Causal Forest (Athey et al., 2019): a state-of-the-art\nnon-privacy-preserving machine learning targeting method.",
    "updated" : "2025-07-07T16:30:40Z",
    "published" : "2025-07-07T16:30:40Z",
    "authors" : [
      {
        "name" : "Anya Shchetkina"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.LG",
      "econ.EM",
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04771v1",
    "title" : "Efficient Unlearning with Privacy Guarantees",
    "summary" : "Privacy protection laws, such as the GDPR, grant individuals the right to\nrequest the forgetting of their personal data not only from databases but also\nfrom machine learning (ML) models trained on them. Machine unlearning has\nemerged as a practical means to facilitate model forgetting of data instances\nseen during training. Although some existing machine unlearning methods\nguarantee exact forgetting, they are typically costly in computational terms.\nOn the other hand, more affordable methods do not offer forgetting guarantees\nand are applicable only to specific ML models. In this paper, we present\n\\emph{efficient unlearning with privacy guarantees} (EUPG), a novel machine\nunlearning framework that offers formal privacy guarantees to individuals whose\ndata are being unlearned. EUPG involves pre-training ML models on data\nprotected using privacy models, and it enables {\\em efficient unlearning with\nthe privacy guarantees offered by the privacy models in use}. Through empirical\nevaluation on four heterogeneous data sets protected with $k$-anonymity and\n$\\epsilon$-differential privacy as privacy models, our approach demonstrates\nutility and forgetting effectiveness comparable to those of exact unlearning\nmethods, while significantly reducing computational and storage costs. Our code\nis available at https://github.com/najeebjebreel/EUPG.",
    "updated" : "2025-07-07T08:46:02Z",
    "published" : "2025-07-07T08:46:02Z",
    "authors" : [
      {
        "name" : "Josep Domingo-Ferrer"
      },
      {
        "name" : "Najeeb Jebreel"
      },
      {
        "name" : "David Sánchez"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04528v1",
    "title" : "Towards integration of Privacy Enhancing Technologies in Explainable\n  Artificial Intelligence",
    "summary" : "Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating\nthe risk of non-transparency in the decision-making process of black-box\nArtificial Intelligence (AI) systems. However, despite the benefits, XAI\nmethods are found to leak the privacy of individuals whose data is used in\ntraining or querying the models. Researchers have demonstrated privacy attacks\nthat exploit explanations to infer sensitive personal information of\nindividuals. Currently there is a lack of defenses against known privacy\nattacks targeting explanations when vulnerable XAI are used in production and\nmachine learning as a service system. To address this gap, in this article, we\nexplore Privacy Enhancing Technologies (PETs) as a defense mechanism against\nattribute inference on explanations provided by feature-based XAI methods. We\nempirically evaluate 3 types of PETs, namely synthetic training data,\ndifferentially private training and noise addition, on two categories of\nfeature-based XAI. Our evaluation determines different responses from the\nmitigation methods and side-effects of PETs on other system properties such as\nutility and performance. In the best case, PETs integration in explanations\nreduced the risk of the attack by 49.47%, while maintaining model utility and\nexplanation quality. Through our evaluation, we identify strategies for using\nPETs in XAI for maximizing benefits and minimizing the success of this privacy\nattack on sensitive personal information.",
    "updated" : "2025-07-06T20:45:34Z",
    "published" : "2025-07-06T20:45:34Z",
    "authors" : [
      {
        "name" : "Sonal Allana"
      },
      {
        "name" : "Rozita Dara"
      },
      {
        "name" : "Xiaodong Lin"
      },
      {
        "name" : "Pulei Xiong"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04185v1",
    "title" : "From Legal Text to Tech Specs: Generative AI's Interpretation of Consent\n  in Privacy Law",
    "summary" : "Privacy law and regulation have turned to \"consent\" as the legitimate basis\nfor collecting and processing individuals' data. As governments have rushed to\nenshrine consent requirements in their privacy laws, such as the California\nConsumer Privacy Act (CCPA), significant challenges remain in understanding how\nthese legal mandates are operationalized in software. The opaque nature of\nsoftware development processes further complicates this translation. To address\nthis, we explore the use of Large Language Models (LLMs) in requirements\nengineering to bridge the gap between legal requirements and technical\nimplementation. This study employs a three-step pipeline that involves using an\nLLM to classify software use cases for compliance, generating LLM modifications\nfor non-compliant cases, and manually validating these changes against legal\nstandards. Our preliminary findings highlight the potential of LLMs in\nautomating compliance tasks, while also revealing limitations in their\nreasoning capabilities. By benchmarking LLMs against real-world use cases, this\nresearch provides insights into leveraging AI-driven solutions to enhance legal\ncompliance of software.",
    "updated" : "2025-07-05T23:36:05Z",
    "published" : "2025-07-05T23:36:05Z",
    "authors" : [
      {
        "name" : "Aniket Kesari"
      },
      {
        "name" : "Travis Breaux"
      },
      {
        "name" : "Tom Norton"
      },
      {
        "name" : "Sarah Santos"
      },
      {
        "name" : "Anmol Singhal"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04104v1",
    "title" : "Human-Centered Interactive Anonymization for Privacy-Preserving Machine\n  Learning: A Case for Human-Guided k-Anonymity",
    "summary" : "Privacy-preserving machine learning (ML) seeks to balance data utility and\nprivacy, especially as regulations like the GDPR mandate the anonymization of\npersonal data for ML applications. Conventional anonymization approaches often\nreduce data utility due to indiscriminate generalization or suppression of data\nattributes. In this study, we propose an interactive approach that incorporates\nhuman input into the k-anonymization process, enabling domain experts to guide\nattribute preservation based on contextual importance. Using the UCI Adult\ndataset, we compare classification outcomes of interactive human-influenced\nanonymization with traditional, fully automated methods. Our results show that\nhuman input can enhance data utility in some cases, although results vary\nacross tasks and settings. We discuss limitations of our approach and suggest\npotential areas for improved interactive frameworks in privacy-aware ML.",
    "updated" : "2025-07-05T17:20:18Z",
    "published" : "2025-07-05T17:20:18Z",
    "authors" : [
      {
        "name" : "Sri Harsha Gajavalli"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.03694v1",
    "title" : "Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital\n  Wills",
    "summary" : "This work presents a novel decentralized protocol for digital estate planning\nthat integrates advances distributed computing, and cryptography. The original\nproof-of-concept was constructed using purely solidity contracts. Since then,\nwe have enhanced the implementation into a layer-1 protocol that uses modern\ninterchain communication to connect several heterogeneous chain types. A key\ncontribution of this research is the implementation of several modern\ncryptographic primitives to support various forms of claims for information\nvalidation. These primitives introduce an unmatched level of privacy to the\nprocess of digital inheritance. We also demonstrate on a set of heterogeneous\nsmart contracts, following the same spec, on each chain to serve as entry\npoints, gateways, or bridge contracts that are invoked via a path from the will\nmodule on our protocol, to the contract. This ensures a fair and secure\ndistribution of digital assets in accordance with the wishes of the decedent\nwithout the requirement of moving their funds. This research further extends\nits innovations with a user interaction model, featuring a check-in system and\naccount abstraction process, which enhances flexibility and user-friendliness\nwithout compromising on security. By developing a dedicated permissionless\nblockchain that is secured by a network of validators, and interchain relayers,\nthe proposed protocol signifies a transformation in the digital estate planning\nindustry and illustrates the potential of blockchain technology in\nrevolutionizing traditional legal and personal spheres. Implementing a\ncryptoeconomic network at the core of inheritance planning allows for unique\nincentive compatible economic mechanisms to be constructed.",
    "updated" : "2025-07-04T16:23:32Z",
    "published" : "2025-07-04T16:23:32Z",
    "authors" : [
      {
        "name" : "Jovonni L. PHarr"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CE",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.03033v1",
    "title" : "Preserving Privacy, Increasing Accessibility, and Reducing Cost: An\n  On-Device Artificial Intelligence Model for Medical Transcription and Note\n  Generation",
    "summary" : "Background: Clinical documentation represents a significant burden for\nhealthcare providers, with physicians spending up to 2 hours daily on\nadministrative tasks. Recent advances in large language models (LLMs) offer\npromising solutions, but privacy concerns and computational requirements limit\ntheir adoption in healthcare settings. Objective: To develop and evaluate a\nprivacy-preserving, on-device medical transcription system using a fine-tuned\nLlama 3.2 1B model capable of generating structured medical notes from medical\ntranscriptions while maintaining complete data sovereignty entirely in the\nbrowser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient\nFine-Tuning (PEFT) with LoRA on 1,500 synthetic medical\ntranscription-to-structured note pairs. The model was evaluated against the\nbase Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140\nmodified ACI benchmark cases. Evaluation employed both statistical metrics\n(ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple\nclinical quality dimensions. Results: The fine-tuned OnDevice model\ndemonstrated substantial improvements over the base model. On the ACI\nbenchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1\nimproved from 0.832 to 0.866. Clinical quality assessments showed marked\nreduction in major hallucinations (from 85 to 35 cases) and enhanced factual\ncorrectness (2.81 to 3.54 on 5-point scale). Similar improvements were observed\non the internal evaluation dataset, with composite scores increasing from 3.13\nto 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical\ntranscription yields clinically meaningful improvements while enabling complete\non-device browser deployment. This approach addresses key barriers to AI\nadoption in healthcare: privacy preservation, cost reduction, and accessibility\nfor resource-constrained environments.",
    "updated" : "2025-07-03T01:51:49Z",
    "published" : "2025-07-03T01:51:49Z",
    "authors" : [
      {
        "name" : "Johnson Thomas"
      },
      {
        "name" : "Ayush Mudgal"
      },
      {
        "name" : "Wendao Liu"
      },
      {
        "name" : "Nisten Tahiraj"
      },
      {
        "name" : "Zeeshaan Mohammed"
      },
      {
        "name" : "Dhruv Diddi"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06008v1",
    "title" : "The Impact of Event Data Partitioning on Privacy-aware Process Discovery",
    "summary" : "Information systems support the execution of business processes. The event\nlogs of these executions generally contain sensitive information about\ncustomers, patients, and employees. The corresponding privacy challenges can be\naddressed by anonymizing the event logs while still retaining utility for\nprocess discovery. However, trading off utility and privacy is difficult: the\nhigher the complexity of event log, the higher the loss of utility by\nanonymization. In this work, we propose a pipeline that combines anonymization\nand event data partitioning, where event abstraction is utilized for\npartitioning. By leveraging event abstraction, event logs can be segmented into\nmultiple parts, allowing each sub-log to be anonymized separately. This\npipeline preserves privacy while mitigating the loss of utility. To validate\nour approach, we study the impact of event partitioning on two anonymization\ntechniques using three real-world event logs and two process discovery\ntechniques. Our results demonstrate that event partitioning can bring\nimprovements in process discovery utility for directly-follows-based\nanonymization techniques.",
    "updated" : "2025-07-08T14:13:44Z",
    "published" : "2025-07-08T14:13:44Z",
    "authors" : [
      {
        "name" : "Jungeun Lim"
      },
      {
        "name" : "Stephan A. Fahrenkrog-Petersen"
      },
      {
        "name" : "Xixi Lu"
      },
      {
        "name" : "Jan Mendling"
      },
      {
        "name" : "Minseok Song"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05875v1",
    "title" : "Post-Processing in Local Differential Privacy: An Extensive Evaluation\n  and Benchmark Platform",
    "summary" : "Local differential privacy (LDP) has recently gained prominence as a powerful\nparadigm for collecting and analyzing sensitive data from users' devices.\nHowever, the inherent perturbation added by LDP protocols reduces the utility\nof the collected data. To mitigate this issue, several post-processing (PP)\nmethods have been developed. Yet, the comparative performance of PP methods\nunder diverse settings remains underexplored. In this paper, we present an\nextensive benchmark comprising 6 popular LDP protocols, 7 PP methods, 4 utility\nmetrics, and 6 datasets to evaluate the behaviors and optimality of PP methods\nunder diverse conditions. Through extensive experiments, we show that while PP\ncan substantially improve utility when the privacy budget is small (i.e.,\nstrict privacy), its benefit diminishes as the privacy budget grows. Moreover,\nour findings reveal that the optimal PP method depends on multiple factors,\nincluding the choice of LDP protocol, privacy budget, data characteristics\n(such as distribution and domain size), and the specific utility metric. To\nadvance research in this area and assist practitioners in identifying the most\nsuitable PP method for their setting, we introduce LDP$^3$, an open-source\nbenchmark platform. LDP$^3$ contains all methods used in our experimental\nanalysis, and it is designed in a modular, extensible, and multi-threaded way\nfor future use and development.",
    "updated" : "2025-07-08T10:59:49Z",
    "published" : "2025-07-08T10:59:49Z",
    "authors" : [
      {
        "name" : "Alireza Khodaie"
      },
      {
        "name" : "Berkay Kemal Balioglu"
      },
      {
        "name" : "Mehmet Emre Gursoy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05872v1",
    "title" : "LDP$^3$: An Extensible and Multi-Threaded Toolkit for Local Differential\n  Privacy Protocols and Post-Processing Methods",
    "summary" : "Local differential privacy (LDP) has become a prominent notion for\nprivacy-preserving data collection. While numerous LDP protocols and\npost-processing (PP) methods have been developed, selecting an optimal\ncombination under different privacy budgets and datasets remains a challenge.\nMoreover, the lack of a comprehensive and extensible LDP benchmarking toolkit\nraises difficulties in evaluating new protocols and PP methods. To address\nthese concerns, this paper presents LDP$^3$ (pronounced LDP-Cube), an\nopen-source, extensible, and multi-threaded toolkit for LDP researchers and\npractitioners. LDP$^3$ contains implementations of several LDP protocols, PP\nmethods, and utility metrics in a modular and extensible design. Its modular\ndesign enables developers to conveniently integrate new protocols and PP\nmethods. Furthermore, its multi-threaded nature enables significant reductions\nin execution times via parallelization. Experimental evaluations demonstrate\nthat: (i) using LDP$^3$ to select a good protocol and post-processing method\nsubstantially improves utility compared to a bad or random choice, and (ii) the\nmulti-threaded design of LDP$^3$ brings substantial benefits in terms of\nefficiency.",
    "updated" : "2025-07-08T10:51:42Z",
    "published" : "2025-07-08T10:51:42Z",
    "authors" : [
      {
        "name" : "Berkay Kemal Balioglu"
      },
      {
        "name" : "Alireza Khodaie"
      },
      {
        "name" : "Mehmet Emre Gursoy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05610v1",
    "title" : "On the Inherent Privacy of Zeroth Order Projected Gradient Descent",
    "summary" : "Differentially private zeroth-order optimization methods have recently gained\npopularity in private fine tuning of machine learning models due to their\nreduced memory requirements. Current approaches for privatizing zeroth-order\nmethods rely on adding Gaussian noise to the estimated zeroth-order gradients.\nHowever, since the search direction in the zeroth-order methods is inherently\nrandom, researchers including Tang et al. (2024) and Zhang et al. (2024a) have\nraised an important question: is the inherent noise in zeroth-order estimators\nsufficient to ensure the overall differential privacy of the algorithm? This\nwork settles this question for a class of oracle-based optimization algorithms\nwhere the oracle returns zeroth-order gradient estimates. In particular, we\nshow that for a fixed initialization, there exist strongly convex objective\nfunctions such that running (Projected) Zeroth-Order Gradient Descent (ZO-GD)\nis not differentially private. Furthermore, we show that even with random\ninitialization and without revealing (initial and) intermediate iterates, the\nprivacy loss in ZO-GD can grow superlinearly with the number of iterations when\nminimizing convex objective functions.",
    "updated" : "2025-07-08T02:38:14Z",
    "published" : "2025-07-08T02:38:14Z",
    "authors" : [
      {
        "name" : "Devansh Gupta"
      },
      {
        "name" : "Meisam Razaviyayn"
      },
      {
        "name" : "Vatsal Sharan"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05415v1",
    "title" : "Layered, Overlapping, and Inconsistent: A Large-Scale Analysis of the\n  Multiple Privacy Policies and Controls of U.S. Banks",
    "summary" : "Privacy policies are often complex. An exception is the two-page standardized\nnotice that U.S. financial institutions must provide under the\nGramm-Leach-Bliley Act (GLBA). However, banks now operate websites, mobile\napps, and other services that involve complex data sharing practices that\nrequire additional privacy notices and do-not-sell opt-outs. We conducted a\nlarge-scale analysis of how U.S. banks implement privacy policies and controls\nin response to GLBA; other federal privacy policy requirements; and the\nCalifornia Consumer Privacy Act (CCPA), a key example for U.S. state privacy\nlaws. We focused on the disclosure and control of a set of especially\nprivacy-invasive practices: third-party data sharing for marketing-related\npurposes. We collected privacy policies for the 2,067 largest U.S. banks,\n45.3\\% of which provided multiple policies. Across disclosures and controls\nwithin the \\textit{same} bank, we identified frequent, concerning\ninconsistencies -- such as banks indicating in GLBA notices that they do not\nshare with third parties but disclosing sharing elsewhere, or using third-party\nmarketing/advertising cookies without disclosure. This multiplicity of\npolicies, with the inconsistencies it causes, may create consumer confusion and\nundermine the transparency goals of the very laws that require them. Our\nfindings call into question whether current policy requirements, such as the\nGLBA notice, are achieving their intended goals in today's online banking\nlandscape. We discuss potential avenues for reforming and harmonizing privacy\npolicies and control requirements across federal and state laws.",
    "updated" : "2025-07-07T18:55:48Z",
    "published" : "2025-07-07T18:55:48Z",
    "authors" : [
      {
        "name" : "Lu Xian"
      },
      {
        "name" : "Van Tran"
      },
      {
        "name" : "Lauren Lee"
      },
      {
        "name" : "Meera Kumar"
      },
      {
        "name" : "Yichen Zhang"
      },
      {
        "name" : "Florian Schaub"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05391v1",
    "title" : "Controlling What You Share: Assessing Language Model Adherence to\n  Privacy Preferences",
    "summary" : "Large language models (LLMs) are primarily accessed via commercial APIs, but\nthis often requires users to expose their data to service providers. In this\npaper, we explore how users can stay in control of their data by using privacy\nprofiles: simple natural language instructions that say what should and should\nnot be revealed. We build a framework where a local model uses these\ninstructions to rewrite queries, only hiding details deemed sensitive by the\nuser, before sending them to an external model, thus balancing privacy with\nperformance. To support this research, we introduce PEEP, a multilingual\ndataset of real user queries annotated to mark private content and paired with\nsynthetic privacy profiles. Our experiments with lightweight LLMs show they can\nfollow these instructions to some extent, but also face consistent challenges,\nhighlighting the need for models that better understand and comply with\nuser-defined privacy preferences.",
    "updated" : "2025-07-07T18:22:55Z",
    "published" : "2025-07-07T18:22:55Z",
    "authors" : [
      {
        "name" : "Guillem Ramírez"
      },
      {
        "name" : "Alexandra Birch"
      },
      {
        "name" : "Ivan Titov"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06969v1",
    "title" : "Unifying Re-Identification, Attribute Inference, and Data Reconstruction\n  Risks in Differential Privacy",
    "summary" : "Differentially private (DP) mechanisms are difficult to interpret and\ncalibrate because existing methods for mapping standard privacy parameters to\nconcrete privacy risks -- re-identification, attribute inference, and data\nreconstruction -- are both overly pessimistic and inconsistent. In this work,\nwe use the hypothesis-testing interpretation of DP ($f$-DP), and determine that\nbounds on attack success can take the same unified form across\nre-identification, attribute inference, and data reconstruction risks. Our\nunified bounds are (1) consistent across a multitude of attack settings, and\n(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary\n(including worst-case) levels of baseline risk. Empirically, our results are\ntighter than prior methods using $\\varepsilon$-DP, R\\'enyi DP, and concentrated\nDP. As a result, calibrating noise using our bounds can reduce the required\nnoise by 20% at the same risk level, which yields, e.g., more than 15pp\naccuracy increase in a text classification task. Overall, this unifying\nperspective provides a principled framework for interpreting and calibrating\nthe degree of protection in DP against specific levels of re-identification,\nattribute inference, or data reconstruction risk.",
    "updated" : "2025-07-09T15:59:30Z",
    "published" : "2025-07-09T15:59:30Z",
    "authors" : [
      {
        "name" : "Bogdan Kulynych"
      },
      {
        "name" : "Juan Felipe Gomez"
      },
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Jamie Hayes"
      },
      {
        "name" : "Borja Balle"
      },
      {
        "name" : "Flavio du Pin Calmon"
      },
      {
        "name" : "Jean Louis Raisaro"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06652v1",
    "title" : "Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating\n  for Privacy and Scalable Decision Making",
    "summary" : "Fuzzy systems are a way to allow machines, systems and frameworks to deal\nwith uncertainty, which is not possible in binary systems that most computers\nuse. These systems have already been deployed for certain use cases, and fuzzy\nsystems could be further improved as proposed in this paper. Such technologies\nto draw inspiration from include machine learning and federated learning.\nMachine learning is one of the recent breakthroughs of technology and could be\napplied to fuzzy systems to further improve the results it produces. Federated\nlearning is also one of the recent technologies that have huge potential, which\nallows machine learning training to improve by reducing privacy risk, reducing\nburden on networking infrastructure, and reducing latency of the latest model.\nAspects from federated learning could be used to improve federated learning,\nsuch as applying the idea of updating the fuzzy rules that make up a key part\nof fuzzy systems, to further improve it over time. This paper discusses how\nthese improvements would be implemented in fuzzy systems, and how it would\nimprove fuzzy systems. It also discusses certain limitations on the potential\nimprovements. It concludes that these proposed ideas and improvements require\nfurther investigation to see how far the improvements are, but the potential is\nthere to improve fuzzy systems.",
    "updated" : "2025-07-09T08:34:24Z",
    "published" : "2025-07-09T08:34:24Z",
    "authors" : [
      {
        "name" : "Arthur Alexander Lim"
      },
      {
        "name" : "Zhen Bin It"
      },
      {
        "name" : "Jovan Bowen Heng"
      },
      {
        "name" : "Tee Hui Teo"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06619v1",
    "title" : "Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets\n  with Differential Privacy with HAM10000",
    "summary" : "When applying machine learning to medical image classification, data leakage\nis a critical issue. Previous methods, such as adding noise to gradients for\ndifferential privacy, work well on large datasets like MNIST and CIFAR-100, but\nfail on small, imbalanced medical datasets like HAM10000. This is because the\nimbalanced distribution causes gradients from minority classes to be clipped\nand lose crucial information, while majority classes dominate. This leads the\nmodel to fall into suboptimal solutions early. To address this, we propose\nSAD-DPSGD, which uses a linear decaying mechanism for noise and clipping\nthresholds. By allocating more privacy budget and using higher clipping\nthresholds in the initial training phases, the model avoids suboptimal\nsolutions and enhances performance. Experiments show that SAD-DPSGD outperforms\nAuto-DPSGD on HAM10000, improving accuracy by 2.15% under $\\epsilon = 3.0$ ,\n$\\delta = 10^{-3}$.",
    "updated" : "2025-07-09T07:46:29Z",
    "published" : "2025-07-09T07:46:29Z",
    "authors" : [
      {
        "name" : "Xiaobo Huang"
      },
      {
        "name" : "Fang Xie"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06508v1",
    "title" : "Subgraph Counting under Edge Local Differential Privacy Based on Noisy\n  Adjacency Matrix",
    "summary" : "When analyzing connection patterns within graphs, subgraph counting serves as\nan effective and fundamental approach. Edge-local differential privacy\n(edge-LDP) and shuffle model have been employed to achieve subgraph counting\nunder a privacy-preserving situation. Existing algorithms are plagued by high\ntime complexity, excessive download costs, low accuracy, or dependence on\ntrusted third parties. To address the aforementioned challenges, we propose the\nNoisy Adjacency Matrix (NAM), which combines differential privacy with the\nadjacency matrix of the graph. NAM offers strong versatility and scalability,\nmaking it applicable to a wider range of DP variants, DP mechanisms, and graph\ntypes. Based on NAM, we designed five algorithms (TriOR, TriTR, TriMTR, QuaTR,\nand 2STAR) to count three types of subgraphs: triangles, quadrangles, and\n2-stars. Theoretical and experimental results demonstrate that in triangle\ncounting, TriOR maximizes accuracy with reduced time complexity among one-round\nalgorithms, TriTR achieves optimal accuracy, TriMTR achieves the highest\naccuracy under low download costs, and QuaTR stands as the first quadrangle\ncounting algorithm under pure edge-LDP. We implement edge-LDP for noisy data\nvia a confidence interval-inspired method, providing DP guarantees on\nrandomized data. Our 2STAR algorithm achieves the highest accuracy in 2-star\ncounting and can be derived as a byproduct of two-round triangle or quadrangle\ncounting algorithms, enabling efficient joint estimation of triangle,\nquadrangle, and 2-star counts within two query rounds.",
    "updated" : "2025-07-09T03:13:15Z",
    "published" : "2025-07-09T03:13:15Z",
    "authors" : [
      {
        "name" : "Jintao Guo"
      },
      {
        "name" : "Ying Zhou"
      },
      {
        "name" : "Chao Li"
      },
      {
        "name" : "Guixun Luo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06350v1",
    "title" : "An Architecture for Privacy-Preserving Telemetry Scheme",
    "summary" : "We present a privacy-preserving telemetry aggregation scheme. Our underlying\nfrequency estimation routine works within the framework of differential\nprivacy. The design philosophy follows a client-server architecture.\nFurthermore, the system uses a local differential privacy scheme where data\ngets randomized on the client before submitting the request to the resource\nserver. This scheme allows for data analysis on de-identified data by carefully\nadding noise to prevent re-identification attacks, thereby facilitating public\ndata release without compromising the identifiability of the individual record.\nThis work further enhances privacy guarantees by leveraging Oblivious HTTP\n(OHTTP) to achieve increased privacy protection for data in transit that\naddresses pre-existing privacy vulnerabilities in raw HTTP. We provide an\nimplementation that focuses on frequency estimation with a histogram of a known\ndictionary. Our resulting formulation based on OHTTP has provided stricter\nprivacy safeguards when compared to trusting an organization to manually delete\nidentifying information from the client's request in the ingestor as deployed\nin reference work~\\cite{apple2017}. Code available at\nhttps://github.com/kenluck2001/miscellaneous/tree/master/src/Privacy-Preserving-Telemetry.",
    "updated" : "2025-07-08T19:20:56Z",
    "published" : "2025-07-08T19:20:56Z",
    "authors" : [
      {
        "name" : "Kenneth Odoh"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05610v2",
    "title" : "On the Inherent Privacy of Zeroth Order Projected Gradient Descent",
    "summary" : "Differentially private zeroth-order optimization methods have recently gained\npopularity in private fine tuning of machine learning models due to their\nreduced memory requirements. Current approaches for privatizing zeroth-order\nmethods rely on adding Gaussian noise to the estimated zeroth-order gradients.\nHowever, since the search direction in the zeroth-order methods is inherently\nrandom, researchers including Tang et al. (2024) and Zhang et al. (2024a) have\nraised an important question: is the inherent noise in zeroth-order estimators\nsufficient to ensure the overall differential privacy of the algorithm? This\nwork settles this question for a class of oracle-based optimization algorithms\nwhere the oracle returns zeroth-order gradient estimates. In particular, we\nshow that for a fixed initialization, there exist strongly convex objective\nfunctions such that running (Projected) Zeroth-Order Gradient Descent (ZO-GD)\nis not differentially private. Furthermore, we show that even with random\ninitialization and without revealing (initial and) intermediate iterates, the\nprivacy loss in ZO-GD can grow superlinearly with the number of iterations when\nminimizing convex objective functions.",
    "updated" : "2025-07-09T02:44:06Z",
    "published" : "2025-07-08T02:38:14Z",
    "authors" : [
      {
        "name" : "Devansh Gupta"
      },
      {
        "name" : "Meisam Razaviyayn"
      },
      {
        "name" : "Vatsal Sharan"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.07700v1",
    "title" : "Rethinking the Privacy of Text Embeddings: A Reproducibility Study of\n  \"Text Embeddings Reveal (Almost) As Much As Text\"",
    "summary" : "Text embeddings are fundamental to many natural language processing (NLP)\ntasks, extensively applied in domains such as recommendation systems and\ninformation retrieval (IR). Traditionally, transmitting embeddings instead of\nraw text has been seen as privacy-preserving. However, recent methods such as\nVec2Text challenge this assumption by demonstrating that controlled decoding\ncan successfully reconstruct original texts from black-box embeddings. The\nunexpectedly strong results reported by Vec2Text motivated us to conduct\nfurther verification, particularly considering the typically non-intuitive and\nopaque structure of high-dimensional embedding spaces. In this work, we\nreproduce the Vec2Text framework and evaluate it from two perspectives: (1)\nvalidating the original claims, and (2) extending the study through targeted\nexperiments. First, we successfully replicate the original key results in both\nin-domain and out-of-domain settings, with only minor discrepancies arising due\nto missing artifacts, such as model checkpoints and dataset splits.\nFurthermore, we extend the study by conducting a parameter sensitivity\nanalysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,\npasswords), and exploring embedding quantization as a lightweight privacy\ndefense. Our results show that Vec2Text is effective under ideal conditions,\ncapable of reconstructing even password-like sequences that lack clear\nsemantics. However, we identify key limitations, including its sensitivity to\ninput sequence length. We also find that Gaussian noise and quantization\ntechniques can mitigate the privacy risks posed by Vec2Text, with quantization\noffering a simpler and more widely applicable solution. Our findings emphasize\nthe need for caution in using text embeddings and highlight the importance of\nfurther research into robust defense mechanisms for NLP systems.",
    "updated" : "2025-07-10T12:27:03Z",
    "published" : "2025-07-10T12:27:03Z",
    "authors" : [
      {
        "name" : "Dominykas Seputis"
      },
      {
        "name" : "Yongkang Li"
      },
      {
        "name" : "Karsten Langerak"
      },
      {
        "name" : "Serghei Mihailov"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.07565v1",
    "title" : "Secure Cooperative Gradient Coding: Optimality, Reliability, and Global\n  Privacy",
    "summary" : "This paper studies privacy-sensitive federated learning (FL) with unreliable\ncommunication, focusing on secure aggregation and straggler mitigation. While\nsecure aggregation cryptographically reconstructs the global model without\nexposing client updates, random link failures disrupt its key coordination,\ndegrading model accuracy. Moreover, unreliable communication can lead to\nobjective inconsistency, causing the global model to converge to arbitrary,\nsub-optimal points far from the intended optimum. This paper proposes Secure\nCooperative Gradient Coding (SecCoGC), a practical solution that achieves\nsecure aggregation with arbitrarily strong privacy guarantees and robust\nstraggler mitigation under unreliable communication. SecCoGC operates natively\nin the real field, making it directly applicable to practical deployments. To\nensure equitable privacy protection across clients, we further introduce\nFair-SecCoGC, an extension that enforces fairness in the level of privacy\noffered to all users. To conclude, this paper formally formulates the problem\nof secure aggregation in the real field and presents both general and\ncomputationally efficient key construction methods. Moreover, it provides a\ncomprehensive privacy analysis under Local Mutual Information Privacy (LMIP)\nand Local Differential Privacy (LDP) across all protocol layers. Robustness and\nconvergence properties are also rigorously analyzed. Finally, extensive\nsimulations are performed across diverse network conditions and benchmark\ndatasets to validate the effectiveness of the proposed methods. The results\nshow that SecCoGC achieves strong robustness to unreliable communication under\narbitrarily strong privacy guarantees. It outperforms existing\nprivacy-preserving methods with performance gains of up to 20\\%-70\\%.",
    "updated" : "2025-07-10T09:10:03Z",
    "published" : "2025-07-10T09:10:03Z",
    "authors" : [
      {
        "name" : "Shudi Weng"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.07320v1",
    "title" : "Optimizing Communication and Device Clustering for Clustered Federated\n  Learning with Differential Privacy",
    "summary" : "In this paper, a secure and communication-efficient clustered federated\nlearning (CFL) design is proposed. In our model, several base stations (BSs)\nwith heterogeneous task-handling capabilities and multiple users with\nnon-independent and identically distributed (non-IID) data jointly perform CFL\ntraining incorporating differential privacy (DP) techniques. Since each BS can\nprocess only a subset of the learning tasks and has limited wireless resource\nblocks (RBs) to allocate to users for federated learning (FL) model parameter\ntransmission, it is necessary to jointly optimize RB allocation and user\nscheduling for CFL performance optimization. Meanwhile, our considered CFL\nmethod requires devices to use their limited data and FL model information to\ndetermine their task identities, which may introduce additional communication\noverhead. We formulate an optimization problem whose goal is to minimize the\ntraining loss of all learning tasks while considering device clustering, RB\nallocation, DP noise, and FL model transmission delay. To solve the problem, we\npropose a novel dynamic penalty function assisted value decomposed multi-agent\nreinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to\nindependently determine their connected users, RBs, and DP noise of the\nconnected users but jointly minimize the training loss of all learning tasks\nacross all BSs. Different from the existing MARL methods that assign a large\npenalty for invalid actions, we propose a novel penalty assignment scheme that\nassigns penalty depending on the number of devices that cannot meet\ncommunication constraints (e.g., delay), which can guide the MARL scheme to\nquickly find valid actions, thus improving the convergence speed. Simulation\nresults show that the DPVD-MARL can improve the convergence rate by up to 20%\nand the ultimate accumulated rewards by 15% compared to independent Q-learning.",
    "updated" : "2025-07-09T22:44:26Z",
    "published" : "2025-07-09T22:44:26Z",
    "authors" : [
      {
        "name" : "Dongyu Wei"
      },
      {
        "name" : "Xiaoren Xu"
      },
      {
        "name" : "Shiwen Mao"
      },
      {
        "name" : "Mingzhe Chen"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.07258v1",
    "title" : "FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware\n  Detection in Cross-Silo Federated Learning",
    "summary" : "As IoT ecosystems continue to expand across critical sectors, they have\nbecome prominent targets for increasingly sophisticated and large-scale malware\nattacks. The evolving threat landscape, combined with the sensitive nature of\nIoT-generated data, demands detection frameworks that are both\nprivacy-preserving and resilient to data heterogeneity. Federated Learning (FL)\noffers a promising solution by enabling decentralized model training without\nexposing raw data. However, standard FL algorithms such as FedAvg and FedProx\noften fall short in real-world deployments characterized by class imbalance and\nnon-IID data distributions -- particularly in the presence of rare or disjoint\nmalware classes. To address these challenges, we propose FedP3E\n(Privacy-Preserving Prototype Exchange), a novel FL framework that supports\nindirect cross-client representation sharing while maintaining data privacy.\nEach client constructs class-wise prototypes using Gaussian Mixture Models\n(GMMs), perturbs them with Gaussian noise, and transmits only these compact\nsummaries to the server. The aggregated prototypes are then distributed back to\nclients and integrated into local training, supported by SMOTE-based\naugmentation to enhance representation of minority malware classes. Rather than\nrelying solely on parameter averaging, our prototype-driven mechanism enables\nclients to enrich their local models with complementary structural patterns\nobserved across the federation -- without exchanging raw data or gradients.\nThis targeted strategy reduces the adverse impact of statistical heterogeneity\nwith minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset\nunder realistic cross-silo scenarios with varying degrees of data imbalance.",
    "updated" : "2025-07-09T20:07:35Z",
    "published" : "2025-07-09T20:07:35Z",
    "authors" : [
      {
        "name" : "Rami Darwish"
      },
      {
        "name" : "Mahmoud Abdelsalam"
      },
      {
        "name" : "Sajad Khorsandroo"
      },
      {
        "name" : "Kaushik Roy"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.07210v1",
    "title" : "WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch",
    "summary" : "Smartwatches such as the Apple Watch collect vast amounts of intimate health\nand fitness data as we wear them. Users have little choice regarding how this\ndata is processed: The Apple Watch can only be used with Apple's iPhones, using\ntheir software and their cloud services. We are the first to publicly\nreverse-engineer the watch's wireless protocols, which led to discovering\nmultiple security issues in Apple's proprietary implementation. With\nWatchWitch, our custom Android reimplementation, we break out of Apple's walled\ngarden -- demonstrating practical interoperability with enhanced privacy\ncontrols and data autonomy. We thus pave the way for more consumer choice in\nthe smartwatch ecosystem, offering users more control over their devices.",
    "updated" : "2025-07-09T18:33:58Z",
    "published" : "2025-07-09T18:33:58Z",
    "authors" : [
      {
        "name" : "Nils Rollshausen"
      },
      {
        "name" : "Alexander Heinrich"
      },
      {
        "name" : "Matthias Hollick"
      },
      {
        "name" : "Jiska Classen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08412v1",
    "title" : "Enforcing Speech Content Privacy in Environmental Sound Recordings using\n  Segment-wise Waveform Reversal",
    "summary" : "Environmental sound recordings often contain intelligible speech, raising\nprivacy concerns that limit analysis, sharing and reuse of data. In this paper,\nwe introduce a method that renders speech unintelligible while preserving both\nthe integrity of the acoustic scene, and the overall audio quality. Our\napproach involves reversing waveform segments to distort speech content. This\nprocess is enhanced through a voice activity detection and speech separation\npipeline, which allows for more precise targeting of speech.\n  In order to demonstrate the effectivness of the proposed approach, we\nconsider a three-part evaluation protocol that assesses: 1) speech\nintelligibility using Word Error Rate (WER), 2) sound sources detectability\nusing Sound source Classification Accuracy-Drop (SCAD) from a widely used\npre-trained model, and 3) audio quality using the Fr\\'echet Audio Distance\n(FAD), computed with our reference dataset that contains unaltered speech.\nExperiments on this simulated evaluation dataset, which consists of linear\nmixtures of speech and environmental sound scenes, show that our method\nachieves satisfactory speech intelligibility reduction (97.9% WER), minimal\ndegradation of the sound sources detectability (2.7% SCAD), and high perceptual\nquality (FAD of 1.40). An ablation study further highlights the contribution of\neach component of the pipeline. We also show that incorporating random splicing\nto our speech content privacy enforcement method can enhance the algorithm's\nrobustness to attempt to recover the clean speech, at a slight cost of audio\nquality.",
    "updated" : "2025-07-11T08:48:59Z",
    "published" : "2025-07-11T08:48:59Z",
    "authors" : [
      {
        "name" : "Modan Tailleur"
      },
      {
        "name" : "Mathieu Lagrange"
      },
      {
        "name" : "Pierre Aumond"
      },
      {
        "name" : "Vincent Tourre"
      }
    ],
    "categories" : [
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08158v1",
    "title" : "Beyond the Worst Case: Extending Differential Privacy Guarantees to\n  Realistic Adversaries",
    "summary" : "Differential Privacy (DP) is a family of definitions that bound the\nworst-case privacy leakage of a mechanism. One important feature of the\nworst-case DP guarantee is it naturally implies protections against adversaries\nwith less prior information, more sophisticated attack goals, and complex\nmeasures of a successful attack. However, the analytical tradeoffs between the\nadversarial model and the privacy protections conferred by DP are not well\nunderstood thus far. To that end, this work sheds light on what the worst-case\nguarantee of DP implies about the success of attackers that are more\nrepresentative of real-world privacy risks.\n  In this paper, we present a single flexible framework that generalizes and\nextends the patchwork of bounds on DP mechanisms found in prior work. Our\nframework allows us to compute high-probability guarantees for DP mechanisms on\na large family of natural attack settings that previous bounds do not capture.\nOne class of such settings is the approximate reconstruction of multiple\nindividuals' data, such as inferring nearly entire columns of a tabular data\nset from noisy marginals and extracting sensitive information from DP-trained\nlanguage models.\n  We conduct two empirical case studies to illustrate the versatility of our\nbounds and compare them to the success of state-of-the-art attacks.\nSpecifically, we study attacks that extract non-uniform PII from a DP-trained\nlanguage model, as well as multi-column reconstruction attacks where the\nadversary has access to some columns in the clear and attempts to reconstruct\nthe remaining columns for each person's record. We find that the absolute\nprivacy risk of attacking non-uniform data is highly dependent on the\nadversary's prior probability of success. Our high probability bounds give us a\nnuanced understanding of the privacy leakage of DP mechanisms in a variety of\npreviously understudied attack settings.",
    "updated" : "2025-07-10T20:36:31Z",
    "published" : "2025-07-10T20:36:31Z",
    "authors" : [
      {
        "name" : "Marika Swanberg"
      },
      {
        "name" : "Meenatchi Sundaram Muthu Selva Annamalai"
      },
      {
        "name" : "Jamie Hayes"
      },
      {
        "name" : "Borja Balle"
      },
      {
        "name" : "Adam Smith"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08050v1",
    "title" : "An Enhanced Privacy-preserving Federated Few-shot Learning Framework for\n  Respiratory Disease Diagnosis",
    "summary" : "The labor-intensive nature of medical data annotation presents a significant\nchallenge for respiratory disease diagnosis, resulting in a scarcity of\nhigh-quality labeled datasets in resource-constrained settings. Moreover,\npatient privacy concerns complicate the direct sharing of local medical data\nacross institutions, and existing centralized data-driven approaches, which\nrely on amounts of available data, often compromise data privacy. This study\nproposes a federated few-shot learning framework with privacy-preserving\nmechanisms to address the issues of limited labeled data and privacy protection\nin diagnosing respiratory diseases. In particular, a meta-stochastic gradient\ndescent algorithm is proposed to mitigate the overfitting problem that arises\nfrom insufficient data when employing traditional gradient descent methods for\nneural network training. Furthermore, to ensure data privacy against gradient\nleakage, differential privacy noise from a standard Gaussian distribution is\nintegrated into the gradients during the training of private models with local\ndata, thereby preventing the reconstruction of medical images. Given the\nimpracticality of centralizing respiratory disease data dispersed across\nvarious medical institutions, a weighted average algorithm is employed to\naggregate local diagnostic models from different clients, enhancing the\nadaptability of a model across diverse scenarios. Experimental results show\nthat the proposed method yields compelling results with the implementation of\ndifferential privacy, while effectively diagnosing respiratory diseases using\ndata from different structures, categories, and distributions.",
    "updated" : "2025-07-10T07:47:58Z",
    "published" : "2025-07-10T07:47:58Z",
    "authors" : [
      {
        "name" : "Ming Wang"
      },
      {
        "name" : "Zhaoyang Duan"
      },
      {
        "name" : "Dong Xue"
      },
      {
        "name" : "Fangzhou Liu"
      },
      {
        "name" : "Zhongheng Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.10489v1",
    "title" : "SynthGuard: Redefining Synthetic Data Generation with a Scalable and\n  Privacy-Preserving Workflow Framework",
    "summary" : "The growing reliance on data-driven applications in sectors such as\nhealthcare, finance, and law enforcement underscores the need for secure,\nprivacy-preserving, and scalable mechanisms for data generation and sharing.\nSynthetic data generation (SDG) has emerged as a promising approach but often\nrelies on centralized or external processing, raising concerns about data\nsovereignty, domain ownership, and compliance with evolving regulatory\nstandards. To overcome these issues, we introduce SynthGuard, a framework\ndesigned to ensure computational governance by enabling data owners to maintain\ncontrol over SDG workflows. SynthGuard supports modular and privacy-preserving\nworkflows, ensuring secure, auditable, and reproducible execution across\ndiverse environments. In this paper, we demonstrate how SynthGuard addresses\nthe complexities at the intersection of domain-specific needs and scalable SDG\nby aligning with requirements for data sovereignty and regulatory compliance.\nDeveloped iteratively with domain expert input, SynthGuard has been validated\nthrough real-world use cases, demonstrating its ability to balance security,\nprivacy, and scalability while ensuring compliance. The evaluation confirms its\neffectiveness in implementing and executing SDG workflows and integrating\nprivacy and utility assessments across various computational environments.",
    "updated" : "2025-07-14T17:11:20Z",
    "published" : "2025-07-14T17:11:20Z",
    "authors" : [
      {
        "name" : "Eduardo Brito"
      },
      {
        "name" : "Mahmoud Shoush"
      },
      {
        "name" : "Kristian Tamm"
      },
      {
        "name" : "Paula Etti"
      },
      {
        "name" : "Liina Kamm"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.10474v1",
    "title" : "Privacy-Preserving Multi-Stage Fall Detection Framework with\n  Semi-supervised Federated Learning and Robotic Vision Confirmation",
    "summary" : "The aging population is growing rapidly, and so is the danger of falls in\nolder adults. A major cause of injury is falling, and detection in time can\ngreatly save medical expenses and recovery time. However, to provide timely\nintervention and avoid unnecessary alarms, detection systems must be effective\nand reliable while addressing privacy concerns regarding the user. In this\nwork, we propose a framework for detecting falls using several complementary\nsystems: a semi-supervised federated learning-based fall detection system\n(SF2D), an indoor localization and navigation system, and a vision-based human\nfall recognition system. A wearable device and an edge device identify a fall\nscenario in the first system. On top of that, the second system uses an indoor\nlocalization technique first to localize the fall location and then navigate a\nrobot to inspect the scenario. A vision-based detection system running on an\nedge device with a mounted camera on a robot is used to recognize fallen\npeople. Each of the systems of this proposed framework achieves different\naccuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to\n99.19% accuracy, while the vision-based fallen people detection achieves 96.3%\naccuracy. However, when we combine the accuracy of these two systems with the\naccuracy of the navigation system (95% success rate), our proposed framework\ncreates a highly reliable performance for fall detection, with an overall\naccuracy of 99.99%. Not only is the proposed framework safe for older adults,\nbut it is also a privacy-preserving solution for detecting falls.",
    "updated" : "2025-07-14T16:55:11Z",
    "published" : "2025-07-14T16:55:11Z",
    "authors" : [
      {
        "name" : "Seyed Alireza Rahimi Azghadi"
      },
      {
        "name" : "Truong-Thanh-Hung Nguyen"
      },
      {
        "name" : "Helene Fournier"
      },
      {
        "name" : "Monica Wachowicz"
      },
      {
        "name" : "Rene Richard"
      },
      {
        "name" : "Francis Palma"
      },
      {
        "name" : "Hung Cao"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.09699v1",
    "title" : "Interpreting Differential Privacy in Terms of Disclosure Risk",
    "summary" : "As the use of differential privacy (DP) becomes widespread, the development\nof effective tools for reasoning about the privacy guarantee becomes\nincreasingly critical. In pursuit of this goal, we demonstrate novel\nrelationships between DP and measures of statistical disclosure risk. We\nsuggest how experts and non-experts can use these results to explain the DP\nguarantee, interpret DP composition theorems, select and justify privacy\nparameters, and identify worst-case adversary prior probabilities.",
    "updated" : "2025-07-13T16:20:13Z",
    "published" : "2025-07-13T16:20:13Z",
    "authors" : [
      {
        "name" : "Zeki Kazan"
      },
      {
        "name" : "Sagar Sharma"
      },
      {
        "name" : "Wanrong Zhang"
      },
      {
        "name" : "Bo Jiang"
      },
      {
        "name" : "Qiang Yan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.09678v1",
    "title" : "Conformal Prediction for Privacy-Preserving Machine Learning",
    "summary" : "We investigate the integration of Conformal Prediction (CP) with supervised\nlearning on deterministically encrypted data, aiming to bridge the gap between\nrigorous uncertainty quantification and privacy-preserving machine learning.\nUsing AES-encrypted variants of the MNIST dataset, we demonstrate that CP\nmethods remain effective even when applied directly in the encrypted domain,\nowing to the preservation of data exchangeability under fixed-key encryption.\nWe test traditional $p$-value-based against $e$-value-based conformal\npredictors. Our empirical evaluation reveals that models trained on\ndeterministically encrypted data retain the ability to extract meaningful\nstructure, achieving 36.88\\% test accuracy -- significantly above random\nguessing (9.56\\%) observed with per-instance encryption. Moreover,\n$e$-value-based CP achieves predictive set coverage of over 60\\% with 4.3\nloss-threshold calibration, correctly capturing the true label in 4888 out of\n5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive\nsets but with reduced coverage accuracy. These findings highlight both the\npromise and limitations of CP in encrypted data settings and underscore\ncritical trade-offs between prediction set compactness and reliability. %Our\nwork sets a foundation for principled uncertainty quantification in secure,\nprivacy-aware learning systems.",
    "updated" : "2025-07-13T15:29:14Z",
    "published" : "2025-07-13T15:29:14Z",
    "authors" : [
      {
        "name" : "Alexander David Balinsky"
      },
      {
        "name" : "Dominik Krzeminski"
      },
      {
        "name" : "Alexander Balinsky"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.09453v1",
    "title" : "SmartphoneDemocracy: Privacy-Preserving E-Voting on Decentralized\n  Infrastructure using Novel European Identity",
    "summary" : "The digitization of democratic processes promises greater accessibility but\npresents challenges in terms of security, privacy, and verifiability. Existing\nelectronic voting systems often rely on centralized architectures, creating\nsingle points of failure and forcing too much trust in authorities, which\ncontradicts democratic principles. This research addresses the challenge of\ncreating a secure, private e-voting system with minimized trust dependencies\ndesigned for the most versatile personal device: the smartphone. We introduce\nSmartphoneDemocracy, a novel e-voting protocol that combines three key\ntechnologies: the emerging European Digital Identity (EUDI) Wallet for\nSybil-resistant identity verification, Zero-Knowledge Proofs for\nprivacy-preserving validation, and a peer-to-peer blockchain (TrustChain) for a\nresilient, serverless public bulletin board. Our protocol enables voters to\nregister and cast ballots anonymously and verifiably directly from their\nsmartphones. We provide a detailed protocol design, a security analysis against\na defined threat model, and a performance evaluation demonstrating that the\ncomputational and network overhead is feasible for medium- to large-scale\nelections. By developing and prototyping this system, we demonstrate a viable\npath to empower citizens with a trustworthy, accessible, and user-controlled\ndigital voting experience.",
    "updated" : "2025-07-13T02:39:10Z",
    "published" : "2025-07-13T02:39:10Z",
    "authors" : [
      {
        "name" : "Michał Jóźwik"
      },
      {
        "name" : "Johan Pouwelse"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.09067v1",
    "title" : "Quantum-Resilient Privacy Ledger (QRPL): A Sovereign Digital Currency\n  for the Post-Quantum Era",
    "summary" : "The emergence of quantum computing presents profound challenges to existing\ncryptographic infrastructures, whilst the development of central bank digital\ncurrencies (CBDCs) has raised concerns regarding privacy preservation and\nexcessive centralisation in digital payment systems. This paper proposes the\nQuantum-Resilient Privacy Ledger (QRPL) as an innovative token-based digital\ncurrency architecture that incorporates National Institute of Standards and\nTechnology (NIST)-standardised post-quantum cryptography (PQC) with hash-based\nzero-knowledge proofs to ensure user sovereignty, scalability, and transaction\nconfidentiality. Key contributions include adaptations of ephemeral proof\nchains for unlinkable transactions, a privacy-weighted Proof-of-Stake (PoS)\nconsensus to promote equitable participation, and a novel zero-knowledge\nproof-based mechanism for privacy-preserving selective disclosure. QRPL aims to\naddress critical shortcomings in prevailing CBDC designs, including risks of\npervasive surveillance, with a 10-20 second block time to balance security and\nthroughput in future monetary systems. While conceptual, empirical prototypes\nare planned. Future work includes prototype development to validate these\nmodels empirically.",
    "updated" : "2025-07-11T23:02:45Z",
    "published" : "2025-07-11T23:02:45Z",
    "authors" : [
      {
        "name" : "Serhan W. Bahar"
      }
    ],
    "categories" : [
      "cs.ET",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.09051v1",
    "title" : "SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant\n  Reviews from Mental Health Apps",
    "summary" : "Mental health (MH) apps often require sensitive user data to customize\nservices for mental wellness needs. However, such data collection practices in\nsome MH apps raise significant privacy concerns for users. These concerns are\noften mentioned in app reviews, but other feedback categories, such as\nreliability and usability, tend to take precedence. This poses a significant\nchallenge in automatically identifying privacy requirements-relevant reviews\n(privacy reviews) that can be utilized to extract privacy requirements and\naddress users' privacy concerns. Thus, this study introduces SAGE, a\ncontext-aware approach to automatically mining privacy reviews from MH apps\nusing Natural Language Inference (NLI) with MH domain-specific privacy\nhypotheses (provides domain-specific context awareness) and a GPT model\n(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a\ndataset of 204K app reviews achieved an F1 score of 0.85 without any\nfine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.\nFurthermore, SAGE extracted 748 privacy reviews previously overlooked by\nkeyword-based methods, demonstrating its effectiveness through qualitative\nevaluation. These reviews can later be refined into actionable privacy\nrequirement artifacts.",
    "updated" : "2025-07-11T21:53:56Z",
    "published" : "2025-07-11T21:53:56Z",
    "authors" : [
      {
        "name" : "Aakash Sorathiya"
      },
      {
        "name" : "Gouri Ginde"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08978v1",
    "title" : "Characterizing Security and Privacy Teaching Standards for Schools in\n  the United States",
    "summary" : "Increasingly, students begin learning aspects of security and privacy during\ntheir primary and secondary education (grades K-12 in the United States).\nIndividual U.S. states and some national organizations publish teaching\nstandards -- guidance that outlines expectations for what students should learn\n-- which often form the basis for course curricula. However, research has not\nyet examined what is covered by these standards and whether the topics align\nwith what the broader security and privacy community thinks students should\nknow. To shed light on these questions, we started by collecting computer\nscience teaching standards from all U.S. states and eight national\norganizations. After manually examining a total of 11,954 standards, we labeled\n3,778 of them as being related to security and privacy, further classifying\nthese into 103 topics. Topics ranged from technical subjects like encryption,\nnetwork security, and embedded systems to social subjects such as laws, ethics,\nand appropriate online behavior. Subsequently, we interviewed 11 security and\nprivacy professionals to examine how the teaching standards align with their\nexpectations. We found that, while the specific topics they mentioned mostly\noverlapped with those of existing standards, professionals placed a greater\nemphasis on threat modeling and security mindset.",
    "updated" : "2025-07-11T19:20:08Z",
    "published" : "2025-07-11T19:20:08Z",
    "authors" : [
      {
        "name" : "Katherine Limes"
      },
      {
        "name" : "Nathan Malkin"
      },
      {
        "name" : "Kelsey R. Fulton"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08882v1",
    "title" : "Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air\n  Traffic Controllers",
    "summary" : "Air traffic control (ATC) demands multi-tasking under time pressure with high\nconsequences of an error. This can induce stress. Detecting stress is a key\npoint in maintaining the high safety standards of ATC. However, processing ATC\nvoice data entails privacy restrictions, e.g. the General Data Protection\nRegulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with\nthese restrictions. In this paper, different architectures for stress detection\nfor anonymized ATCO speech are evaluated. Our best networks reach a stress\ndetection accuracy of 93.6% on an anonymized version of the Speech Under\nSimulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our\nanonymized ATC simulation dataset. This shows that privacy does not have to be\nan impediment in building well-performing deep-learning-based models.",
    "updated" : "2025-07-10T11:48:29Z",
    "published" : "2025-07-10T11:48:29Z",
    "authors" : [
      {
        "name" : "Janaki Viswanathan"
      },
      {
        "name" : "Alexander Blatt"
      },
      {
        "name" : "Konrad Hagemann"
      },
      {
        "name" : "Dietrich Klakow"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CL",
      "eess.AS",
      "I.2.7; I.5.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08878v1",
    "title" : "Towards Privacy-Preserving and Personalized Smart Homes via Tailored\n  Small Language Models",
    "summary" : "Large Language Models (LLMs) have showcased remarkable generalizability in\nlanguage comprehension and hold significant potential to revolutionize\nhuman-computer interaction in smart homes. Existing LLM-based smart home\nassistants typically transmit user commands, along with user profiles and home\nconfigurations, to remote servers to obtain personalized services. However,\nusers are increasingly concerned about the potential privacy leaks to the\nremote servers. To address this issue, we develop HomeLLaMA, an on-device\nassistant for privacy-preserving and personalized smart home serving with a\ntailored small language model (SLM). HomeLLaMA learns from cloud LLMs to\ndeliver satisfactory responses and enable user-friendly interactions. Once\ndeployed, HomeLLaMA facilitates proactive interactions by continuously updating\nlocal SLMs and user profiles. To further enhance user experience while\nprotecting their privacy, we develop PrivShield to offer an optional\nprivacy-preserving LLM-based smart home serving for those users, who are\nunsatisfied with local responses and willing to send less-sensitive queries to\nremote servers. For evaluation, we build a comprehensive benchmark DevFinder to\nassess the service quality. Extensive experiments and user studies (M=100)\ndemonstrate that HomeLLaMA can provide personalized services while\nsignificantly enhancing user privacy.",
    "updated" : "2025-07-10T05:36:32Z",
    "published" : "2025-07-10T05:36:32Z",
    "authors" : [
      {
        "name" : "Xinyu Huang"
      },
      {
        "name" : "Leming Shen"
      },
      {
        "name" : "Zijing Ma"
      },
      {
        "name" : "Yuanqing Zheng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08864v1",
    "title" : "Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic\n  Management System",
    "summary" : "Location-based vehicular traffic management faces significant challenges in\nprotecting sensitive geographical data while maintaining utility for traffic\nmanagement and fairness across regions. Existing state-of-the-art solutions\noften fail to meet the required level of protection against linkage attacks and\ndemographic biases, leading to privacy leakage and inequity in data analysis.\nIn this paper, we propose a novel algorithm designed to address the challenges\nregarding the balance of privacy, utility, and fairness in location-based\nvehicular traffic management systems. In this context, utility means providing\nreliable and meaningful traffic information, while fairness ensures that all\nregions and individuals are treated equitably in data use and decision-making.\nEmploying differential privacy techniques, we enhance data security by\nintegrating query-based data access with iterative shuffling and calibrated\nnoise injection, ensuring that sensitive geographical data remains protected.\nWe ensure adherence to epsilon-differential privacy standards by implementing\nthe Laplace mechanism. We implemented our algorithm on vehicular location-based\ndata from Norway, demonstrating its ability to maintain data utility for\ntraffic management and urban planning while ensuring fair representation of all\ngeographical areas without being overrepresented or underrepresented.\nAdditionally, we have created a heatmap of Norway based on our model,\nillustrating the privatized and fair representation of the traffic conditions\nacross various cities. Our algorithm provides privacy in vehicular traffic",
    "updated" : "2025-07-09T13:49:13Z",
    "published" : "2025-07-09T13:49:13Z",
    "authors" : [
      {
        "name" : "Poushali Sengupta"
      },
      {
        "name" : "Sabita Maharjan"
      },
      {
        "name" : "frank Eliassen"
      },
      {
        "name" : "Yan Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08853v1",
    "title" : "Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital\n  Archives",
    "summary" : "As archives turn to artificial intelligence to manage growing volumes of\ndigital records, privacy risks inherent in current AI data practices raise\ncritical concerns about data sovereignty and ethical accountability. This paper\nexplores how privacy-enhancing technologies (PETs) and Web3 architectures can\nsupport archives to preserve control over sensitive content while still being\nable to make it available for access by researchers. We present Clio-X, a\ndecentralized, privacy-first Web3 digital solution designed to embed PETs into\narchival workflows and support AI-enabled reference and access. Drawing on a\nuser evaluation of a medium-fidelity prototype, the study reveals both interest\nin the potential of the solution and significant barriers to adoption related\nto trust, system opacity, economic concerns, and governance. Using Rogers'\nDiffusion of Innovation theory, we analyze the sociotechnical dimensions of\nthese barriers and propose a path forward centered on participatory design and\ndecentralized governance through a Clio-X Decentralized Autonomous\nOrganization. By integrating technical safeguards with community-based\noversight, Clio-X offers a novel model to ethically deploy AI in cultural\nheritage contexts.",
    "updated" : "2025-07-09T05:30:38Z",
    "published" : "2025-07-09T05:30:38Z",
    "authors" : [
      {
        "name" : "Victoria L. Lemieux"
      },
      {
        "name" : "Rosa Gil"
      },
      {
        "name" : "Faith Molosiwa"
      },
      {
        "name" : "Qihong Zhou"
      },
      {
        "name" : "Binming Li"
      },
      {
        "name" : "Roberto Garcia"
      },
      {
        "name" : "Luis De La Torre Cubillo"
      },
      {
        "name" : "Zehua Wang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.DL",
      "D.2.11, H.3.4, H.3.7, J.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.08843v1",
    "title" : "Can We Predict Your Next Move Without Breaking Your Privacy?",
    "summary" : "We propose FLLL3M--Federated Learning with Large Language Models for Mobility\nModeling--a privacy-preserving framework for Next-Location Prediction (NxLP).\nBy retaining user data locally and leveraging LLMs through an efficient outer\nproduct mechanism, FLLL3M ensures high accuracy with low resource demands. It\nachieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,\n0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while\nreducing parameters by up to 45.6% and memory usage by 52.7%.",
    "updated" : "2025-07-08T08:13:34Z",
    "published" : "2025-07-08T08:13:34Z",
    "authors" : [
      {
        "name" : "Arpita Soni"
      },
      {
        "name" : "Sahil Tripathi"
      },
      {
        "name" : "Gautam Siddharth Kashyap"
      },
      {
        "name" : "Manaswi Kulahara"
      },
      {
        "name" : "Mohammad Anas Azeez"
      },
      {
        "name" : "Zohaib Hasan Siddiqui"
      },
      {
        "name" : "Nipun Joshi"
      },
      {
        "name" : "Jiechao Gao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.11324v1",
    "title" : "A Review of Privacy Metrics for Privacy-Preserving Synthetic Data\n  Generation",
    "summary" : "Privacy Preserving Synthetic Data Generation (PP-SDG) has emerged to produce\nsynthetic datasets from personal data while maintaining privacy and utility.\nDifferential privacy (DP) is the property of a PP-SDG mechanism that\nestablishes how protected individuals are when sharing their sensitive data. It\nis however difficult to interpret the privacy loss ($\\varepsilon$) expressed by\nDP. To make the actual risk associated with the privacy loss more transparent,\nmultiple privacy metrics (PMs) have been proposed to assess the privacy risk of\nthe data. These PMs are utilized in separate studies to assess newly introduced\nPP-SDG mechanisms. Consequently, these PMs embody the same assumptions as the\nPP-SDG mechanism they were made to assess. Therefore, a thorough definition of\nhow these are calculated is necessary. In this work, we present the assumptions\nand mathematical formulations of 17 distinct privacy metrics.",
    "updated" : "2025-07-15T13:56:02Z",
    "published" : "2025-07-15T13:56:02Z",
    "authors" : [
      {
        "name" : "Frederik Marinus Trudslev"
      },
      {
        "name" : "Matteo Lissandrini"
      },
      {
        "name" : "Juan Manuel Rodriguez"
      },
      {
        "name" : "Martin Bøgsted"
      },
      {
        "name" : "Daniele Dell'Aglio"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.11187v1",
    "title" : "Striking the Perfect Balance: Preserving Privacy While Boosting Utility\n  in Collaborative Medical Prediction Platforms",
    "summary" : "Online collaborative medical prediction platforms offer convenience and\nreal-time feedback by leveraging massive electronic health records. However,\ngrowing concerns about privacy and low prediction quality can deter patient\nparticipation and doctor cooperation. In this paper, we first clarify the\nprivacy attacks, namely attribute attacks targeting patients and model\nextraction attacks targeting doctors, and specify the corresponding privacy\nprinciples. We then propose a privacy-preserving mechanism and integrate it\ninto a novel one-shot distributed learning framework, aiming to simultaneously\nmeet both privacy requirements and prediction performance objectives. Within\nthe framework of statistical learning theory, we theoretically demonstrate that\nthe proposed distributed learning framework can achieve the optimal prediction\nperformance under specific privacy requirements. We further validate the\ndeveloped privacy-preserving collaborative medical prediction platform through\nboth toy simulations and real-world data experiments.",
    "updated" : "2025-07-15T10:41:55Z",
    "published" : "2025-07-15T10:41:55Z",
    "authors" : [
      {
        "name" : "Shao-Bo Lin"
      },
      {
        "name" : "Xiaotong Liu"
      },
      {
        "name" : "Yao Wang"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.10946v1",
    "title" : "Solving Linear Programs with Differential Privacy",
    "summary" : "We study the problem of solving linear programs of the form $Ax\\le b$,\n$x\\ge0$ with differential privacy. For homogeneous LPs $Ax\\ge0$, we give an\nefficient $(\\epsilon,\\delta)$-differentially private algorithm which with\nprobability at least $1-\\beta$ finds in polynomial time a solution that\nsatisfies all but\n$O(\\frac{d^{2}}{\\epsilon}\\log^{2}\\frac{d}{\\delta\\beta}\\sqrt{\\log\\frac{1}{\\rho_{0}}})$\nconstraints, for problems with margin $\\rho_{0}>0$. This improves the bound of\n$O(\\frac{d^{5}}{\\epsilon}\\log^{1.5}\\frac{1}{\\rho_{0}}\\mathrm{poly}\\log(d,\\frac{1}{\\delta},\\frac{1}{\\beta}))$\nby [Kaplan-Mansour-Moran-Stemmer-Tur, STOC '25]. For general LPs $Ax\\le b$,\n$x\\ge0$ with potentially zero margin, we give an efficient\n$(\\epsilon,\\delta)$-differentially private algorithm that w.h.p drops\n$O(\\frac{d^{4}}{\\epsilon}\\log^{2.5}\\frac{d}{\\delta}\\sqrt{\\log dU})$\nconstraints, where $U$ is an upper bound for the entries of $A$ and $b$ in\nabsolute value. This improves the result by Kaplan et al. by at least a factor\nof $d^{5}$. Our techniques build upon privatizing a rescaling perceptron\nalgorithm by [Hoberg-Rothvoss, IPCO '17] and a more refined iterative procedure\nfor identifying equality constraints by Kaplan et al.",
    "updated" : "2025-07-15T03:22:47Z",
    "published" : "2025-07-15T03:22:47Z",
    "authors" : [
      {
        "name" : "Alina Ene"
      },
      {
        "name" : "Huy Le Nguyen"
      },
      {
        "name" : "Ta Duy Nguyen"
      },
      {
        "name" : "Adrian Vladu"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.10786v1",
    "title" : "\"Is it always watching? Is it always listening?\" Exploring Contextual\n  Privacy and Security Concerns Toward Domestic Social Robots",
    "summary" : "Equipped with artificial intelligence (AI) and advanced sensing capabilities,\nsocial robots are gaining interest among consumers in the United States. These\nrobots seem like a natural evolution of traditional smart home devices.\nHowever, their extensive data collection capabilities, anthropomorphic\nfeatures, and capacity to interact with their environment make social robots a\nmore significant security and privacy threat. Increased risks include data\nlinkage, unauthorized data sharing, and the physical safety of users and their\nhomes. It is critical to investigate U.S. users' security and privacy needs and\nconcerns to guide the design of social robots while these devices are still in\nthe early stages of commercialization in the U.S. market. Through 19\nsemi-structured interviews, we identified significant security and privacy\nconcerns, highlighting the need for transparency, usability, and robust privacy\ncontrols to support adoption. For educational applications, participants\nworried most about misinformation, and in medical use cases, they worried about\nthe reliability of these devices. Participants were also concerned with the\ndata inference that social robots could enable. We found that participants\nexpect tangible privacy controls, indicators of data collection, and\ncontext-appropriate functionality.",
    "updated" : "2025-07-14T20:27:40Z",
    "published" : "2025-07-14T20:27:40Z",
    "authors" : [
      {
        "name" : "Henry Bell"
      },
      {
        "name" : "Jabari Kwesi"
      },
      {
        "name" : "Hiba Laabadli"
      },
      {
        "name" : "Pardis Emami-Naeini"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR",
      "cs.ET",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.10695v1",
    "title" : "Exploring User Security and Privacy Attitudes and Concerns Toward the\n  Use of General-Purpose LLM Chatbots for Mental Health",
    "summary" : "Individuals are increasingly relying on large language model (LLM)-enabled\nconversational agents for emotional support. While prior research has examined\nprivacy and security issues in chatbots specifically designed for mental health\npurposes, these chatbots are overwhelmingly \"rule-based\" offerings that do not\nleverage generative AI. Little empirical research currently measures users'\nprivacy and security concerns, attitudes, and expectations when using\ngeneral-purpose LLM-enabled chatbots to manage and improve mental health.\nThrough 21 semi-structured interviews with U.S. participants, we identified\ncritical misconceptions and a general lack of risk awareness. Participants\nconflated the human-like empathy exhibited by LLMs with human-like\naccountability and mistakenly believed that their interactions with these\nchatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures\nwith a licensed therapist. We introduce the concept of \"intangible\nvulnerability,\" where emotional or psychological disclosures are undervalued\ncompared to more tangible forms of information (e.g., financial or\nlocation-based data). To address this, we propose recommendations to safeguard\nuser mental health disclosures with general-purpose LLM-enabled chatbots more\neffectively.",
    "updated" : "2025-07-14T18:10:21Z",
    "published" : "2025-07-14T18:10:21Z",
    "authors" : [
      {
        "name" : "Jabari Kwesi"
      },
      {
        "name" : "Jiaxun Cao"
      },
      {
        "name" : "Riya Manchanda"
      },
      {
        "name" : "Pardis Emami-Naeini"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR",
      "cs.ET",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.10640v1",
    "title" : "SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy\n  Concerns from User Reviews in Social-Media Applications",
    "summary" : "The widespread use of social media applications has raised significant\nprivacy concerns, often highlighted in user reviews. These reviews also provide\ndevelopers with valuable insights into improving apps by addressing issues and\nintroducing better features. However, the sheer volume and nuanced nature of\nreviews make manual identification and prioritization of privacy-related\nconcerns challenging for developers. Previous studies have developed software\nutilities to automatically classify user reviews as privacy-relevant,\nprivacy-irrelevant, bug reports, feature requests, etc., using machine\nlearning. Notably, there is a lack of focus on classifying reviews specifically\nas privacy-related feature requests, privacy-related bug reports, or\nprivacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated\nonline annotation tool designed to help developers annotate and classify user\nreviews into these categories. For automating the annotation of such reviews,\nthis paper introduces the annotation model, GRACE (GRU-based Attention with\nCBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words\n(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven\npopular social media apps on Google Play Store, including Instagram, Facebook,\nWhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were\nanalyzed. Two annotators manually labelled the reviews, achieving a Cohen's\nKappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement\nfor training machine learning models. Among the models tested, GRACE\ndemonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:\n0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates\nsignificant potential to assist developers with extracting and addressing\nprivacy-related feature requests or bug reports from user reviews, enhancing\nuser privacy and trust.",
    "updated" : "2025-07-14T14:58:04Z",
    "published" : "2025-07-14T14:58:04Z",
    "authors" : [
      {
        "name" : "Labiba Farah"
      },
      {
        "name" : "Mohammad Ridwan Kabir"
      },
      {
        "name" : "Shohel Ahmed"
      },
      {
        "name" : "MD Mohaymen Ul Anam"
      },
      {
        "name" : "Md. Sakibul Islam"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.LG",
      "cs.SI",
      "D.2.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.10627v1",
    "title" : "Crypto-Assisted Graph Degree Sequence Release under Local Differential\n  Privacy",
    "summary" : "Given a graph $G$ defined in a domain $\\mathcal{G}$, we investigate locally\ndifferentially private mechanisms to release a degree sequence on $\\mathcal{G}$\nthat accurately approximates the actual degree distribution. Existing solutions\nfor this problem mostly use graph projection techniques based on edge deletion\nprocess, using a threshold parameter $\\theta$ to bound node degrees. However,\nthis approach presents a fundamental trade-off in threshold parameter\nselection. While large $\\theta$ values introduce substantial noise in the\nreleased degree sequence, small $\\theta$ values result in more edges removed\nthan necessary. Furthermore, $\\theta$ selection leads to an excessive\ncommunication cost. To remedy existing solutions' deficiencies, we present\nCADR-LDP, an efficient framework incorporating encryption techniques and\ndifferentially private mechanisms to release the degree sequence. In CADR-LDP,\nwe first use the crypto-assisted Optimal-$\\theta$-Selection method to select\nthe optimal parameter with a low communication cost. Then, we use the LPEA-LOW\nmethod to add some edges for each node with the edge addition process in local\nprojection. LPEA-LOW prioritizes the projection with low-degree nodes, which\ncan retain more edges for such nodes and reduce the projection error.\nTheoretical analysis shows that CADR-LDP satisfies $\\epsilon$-node local\ndifferential privacy. The experimental results on eight graph datasets show\nthat our solution outperforms existing methods.",
    "updated" : "2025-07-14T07:04:08Z",
    "published" : "2025-07-14T07:04:08Z",
    "authors" : [
      {
        "name" : "Xiaojian Zhang"
      },
      {
        "name" : "Junqing Wang"
      },
      {
        "name" : "Kerui Chen"
      },
      {
        "name" : "Peiyuan Zhao"
      },
      {
        "name" : "Huiyuan Bai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.10582v1",
    "title" : "Transforming Sensitive Documents into Quantitative Data: An AI-Based\n  Preprocessing Toolchain for Structured and Privacy-Conscious Analysis",
    "summary" : "Unstructured text from legal, medical, and administrative sources offers a\nrich but underutilized resource for research in public health and the social\nsciences. However, large-scale analysis is hampered by two key challenges: the\npresence of sensitive, personally identifiable information, and significant\nheterogeneity in structure and language. We present a modular toolchain that\nprepares such text data for embedding-based analysis, relying entirely on\nopen-weight models that run on local hardware, requiring only a\nworkstation-level GPU and supporting privacy-sensitive research.\n  The toolchain employs large language model (LLM) prompting to standardize,\nsummarize, and, when needed, translate texts to English for greater\ncomparability. Anonymization is achieved via LLM-based redaction, supplemented\nwith named entity recognition and rule-based methods to minimize the risk of\ndisclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court\ndecisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.\nEach document is processed into an anonymized, standardized summary and\ntransformed into a document-level embedding. Validation, including manual\nreview, automated scanning, and predictive evaluation shows the toolchain\neffectively removes identifying information while retaining semantic content.\nAs an illustrative application, we train a predictive model using embedding\nvectors derived from a small set of manually labeled summaries, demonstrating\nthe toolchain's capacity for semi-automated content analysis at scale.\n  By enabling structured, privacy-conscious analysis of sensitive documents,\nour toolchain opens new possibilities for large-scale research in domains where\ntextual data was previously inaccessible due to privacy and heterogeneity\nconstraints.",
    "updated" : "2025-07-11T11:58:36Z",
    "published" : "2025-07-11T11:58:36Z",
    "authors" : [
      {
        "name" : "Anders Ledberg"
      },
      {
        "name" : "Anna Thalén"
      }
    ],
    "categories" : [
      "cs.CL",
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.07565v2",
    "title" : "Secure Cooperative Gradient Coding: Optimality, Reliability, and Global\n  Privacy",
    "summary" : "This paper studies privacy-sensitive federated learning (FL) under unreliable\ncommunication, with a focus on secure aggregation and straggler mitigation. To\npreserve user privacy without compromising the utility of the global model,\nsecure aggregation emerges as a promising approach by coordinating the use of\nprivacy-preserving noise (secret keys) across participating clients. However,\nthe unreliable communication will randomly disrupt the key coordination and\ndisable the exact recovery of the global model in secure aggregation.\nFurthermore, unreliable communication can distort the optimization trajectory,\ncausing the global model to deviate further from the intended global optimum.To\naddress these challenges, we propose Secure Cooperative Gradient Coding\n(SecCoGC), a practical solution that achieves accurate aggregation with\narbitrarily strong privacy guarantees and is inherently robust to communication\nuncertainties. To ensure fairness in privacy protection, we further introduce\nFair-SecCoGC, an extension of SecCoGC that enforces equitable privacy\npreservation across all clients. Notably, Fair-SecCoGC achieves optimal privacy\nunder a per-key total power constraint. We formally formulate the problem of\nsecure aggregation in the real field and present both general and\ncomputationally efficient methods for secret key construction. Our privacy\nanalysis covers both Local Mutual Information Privacy (LMIP) and Local\nDifferential Privacy (LDP) across all protocol layers, accounting for\nintermittent networks and correlation among secret keys. In addition, we\ncharacterize the system reliability and convergence properties of the proposed\nscheme. Experimental results demonstrate that SecCoGC achieves strong\nresilience to unreliable communication while maintaining arbitrarily strong\nprivacy guarantees, yielding test accuracy improvements of 20% to 70% over\nexisting privacy-preserving methods.",
    "updated" : "2025-07-15T16:32:21Z",
    "published" : "2025-07-10T09:10:03Z",
    "authors" : [
      {
        "name" : "Shudi Weng"
      },
      {
        "name" : "Chao Ren"
      },
      {
        "name" : "Yizhou Zhao"
      },
      {
        "name" : "Ming Xiao"
      },
      {
        "name" : "Mikael Skoglund"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.12098v1",
    "title" : "A Privacy-Preserving Framework for Advertising Personalization\n  Incorporating Federated Learning and Differential Privacy",
    "summary" : "To mitigate privacy leakage and performance issues in personalized\nadvertising, this paper proposes a framework that integrates federated learning\nand differential privacy. The system combines distributed feature extraction,\ndynamic privacy budget allocation, and robust model aggregation to balance\nmodel accuracy, communication overhead, and privacy protection. Multi-party\nsecure computing and anomaly detection mechanisms further enhance system\nresilience against malicious attacks. Experimental results demonstrate that the\nframework achieves dual optimization of recommendation accuracy and system\nefficiency while ensuring privacy, providing both a practical solution and a\ntheoretical foundation for applying privacy protection technologies in\nadvertisement recommendation.",
    "updated" : "2025-07-16T10:07:19Z",
    "published" : "2025-07-16T10:07:19Z",
    "authors" : [
      {
        "name" : "Xiang Li"
      },
      {
        "name" : "Yifan Lin"
      },
      {
        "name" : "Yuanzhe Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.11943v1",
    "title" : "Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation\n  for Privacy-Preserving Image Classification",
    "summary" : "We propose a low-rank adaptation method for training privacy-preserving\nvision transformer (ViT) models that efficiently freezes pre-trained ViT model\nweights. In the proposed method, trainable rank decomposition matrices are\ninjected into each layer of the ViT architecture, and moreover, the patch\nembedding layer is not frozen, unlike in the case of the conventional low-rank\nadaptation methods. The proposed method allows us not only to reduce the number\nof trainable parameters but to also maintain almost the same accuracy as that\nof full-time tuning.",
    "updated" : "2025-07-16T06:18:52Z",
    "published" : "2025-07-16T06:18:52Z",
    "authors" : [
      {
        "name" : "Haiwei Lin"
      },
      {
        "name" : "Shoko Imaizumi"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.11908v1",
    "title" : "Unveiling Usability Challenges in Web Privacy Controls",
    "summary" : "With the increasing concerns around privacy and the enforcement of data\nprivacy laws, many websites now provide users with privacy controls. However,\nlocating these controls can be challenging, as they are frequently hidden\nwithin multiple settings and layers. Moreover, the lack of standardization\nmeans these controls can vary widely across services. The technical or\nconfusing terminology used to describe these controls further complicates\nusers' ability to understand and use them effectively. This paper presents a\nlarge-scale empirical analysis investigating usability challenges of web\nprivacy controls across 18,628 websites. While aiming for a multi-scenario\nview, our automated data collection faced significant hurdles, particularly in\nsimulating sign-up and authenticated user visits, leading to more focused\ninsights on guest visit scenarios and challenges in automated capture of\ndynamic user interactions. Our heuristic evaluation of three different user\nvisit scenarios identifies significant website usability issues. Our results\nshow that privacy policies are most common across all visit scenarios, with\nnudges and notices being prevalent in sign-up situations. We recommend\ndesigning privacy controls that: enhance awareness through pop-up nudges and\nnotices; offer a table of contents as navigational aids and customized settings\nlinks in policies for more informed choice; and ensure accessibility via direct\nlinks to privacy settings from nudges.",
    "updated" : "2025-07-16T04:47:35Z",
    "published" : "2025-07-16T04:47:35Z",
    "authors" : [
      {
        "name" : "Rahat Masood"
      },
      {
        "name" : "Sunday Oyinlola Ogundoyin"
      },
      {
        "name" : "Muhammad Ikram"
      },
      {
        "name" : "Alex Ye"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.11649v1",
    "title" : "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation\n  using Zero-Knowledge Proofs",
    "summary" : "Federated Learning (FL) enables collaborative model training on decentralized\ndata without exposing raw data. However, the evaluation phase in FL may leak\nsensitive information through shared performance metrics. In this paper, we\npropose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to\nenable privacy-preserving and verifiable evaluation for FL. Instead of\nrevealing raw loss values, clients generate a succinct proof asserting that\ntheir local loss is below a predefined threshold. Our approach is implemented\nwithout reliance on external APIs, using self-contained modules for federated\nlearning simulation, ZKP circuit design, and experimental evaluation on both\nthe MNIST and Human Activity Recognition (HAR) datasets. We focus on a\nthreshold-based proof for a simple Convolutional Neural Network (CNN) model\n(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate\nthe approach in terms of computational overhead, communication cost, and\nverifiability.",
    "updated" : "2025-07-15T18:34:14Z",
    "published" : "2025-07-15T18:34:14Z",
    "authors" : [
      {
        "name" : "Daniel Commey"
      },
      {
        "name" : "Benjamin Appiah"
      },
      {
        "name" : "Griffith S. Klogo"
      },
      {
        "name" : "Garth V. Crosby"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.10786v2",
    "title" : "\"Is it always watching? Is it always listening?\" Exploring Contextual\n  Privacy and Security Concerns Toward Domestic Social Robots",
    "summary" : "Equipped with artificial intelligence (AI) and advanced sensing capabilities,\nsocial robots are gaining interest among consumers in the United States. These\nrobots seem like a natural evolution of traditional smart home devices.\nHowever, their extensive data collection capabilities, anthropomorphic\nfeatures, and capacity to interact with their environment make social robots a\nmore significant security and privacy threat. Increased risks include data\nlinkage, unauthorized data sharing, and the physical safety of users and their\nhomes. It is critical to investigate U.S. users' security and privacy needs and\nconcerns to guide the design of social robots while these devices are still in\nthe early stages of commercialization in the U.S. market. Through 19\nsemi-structured interviews, we identified significant security and privacy\nconcerns, highlighting the need for transparency, usability, and robust privacy\ncontrols to support adoption. For educational applications, participants\nworried most about misinformation, and in medical use cases, they worried about\nthe reliability of these devices. Participants were also concerned with the\ndata inference that social robots could enable. We found that participants\nexpect tangible privacy controls, indicators of data collection, and\ncontext-appropriate functionality.",
    "updated" : "2025-07-16T16:58:46Z",
    "published" : "2025-07-14T20:27:40Z",
    "authors" : [
      {
        "name" : "Henry Bell"
      },
      {
        "name" : "Jabari Kwesi"
      },
      {
        "name" : "Hiba Laabadli"
      },
      {
        "name" : "Pardis Emami-Naeini"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.CR",
      "cs.ET",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.13286v1",
    "title" : "Privacy-Preserving Fusion for Multi-Sensor Systems Under Multiple Packet\n  Dropouts",
    "summary" : "Wireless sensor networks (WSNs) are critical components in modern\ncyber-physical systems, enabling efficient data collection and fusion through\nspatially distributed sensors. However, the inherent risks of eavesdropping and\npacket dropouts in such networks pose significant challenges to secure state\nestimation. In this paper, we address the privacy-preserving fusion estimation\n(PPFE) problem for multi-sensor systems under multiple packet dropouts and\neavesdropping attacks. To mitigate these issues, we propose a distributed\nencoding-based privacy-preserving mechanism (PPM) within a control-theoretic\nframework, ensuring data privacy during transmission while maintaining the\nperformance of legitimate state estimation. A centralized fusion filter is\ndeveloped, accounting for the coupling effects of packet dropouts and the\nencoding-based PPM. Boundedness conditions for the legitimate user's estimation\nerror covariance are derived via a modified algebraic Riccati equation.\nAdditionally, by demonstrating the divergence of the eavesdropper's mean\nestimation error, the proposed PPFE algorithm's data confidentiality is\nrigorously analyzed. Simulation results for an Internet-based three-tank system\nvalidate the effectiveness of the proposed approach, highlighting its potential\nto enhance privacy without compromising estimation accuracy.",
    "updated" : "2025-07-17T16:50:15Z",
    "published" : "2025-07-17T16:50:15Z",
    "authors" : [
      {
        "name" : "Jie Huang"
      },
      {
        "name" : "Jason J. R. Liu"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.12932v1",
    "title" : "Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy\n  Protection against Voice Deepfakes",
    "summary" : "The rapid advancement of voice deepfake technologies has raised serious\nconcerns about user audio privacy, as attackers increasingly exploit publicly\navailable voice data to generate convincing fake audio for malicious purposes\nsuch as identity theft, financial fraud, and misinformation campaigns. While\nexisting defense methods offer partial protection, they face critical\nlimitations, including weak adaptability to unseen user data, poor scalability\nto long audio, rigid reliance on white-box knowledge, and high computational\nand temporal costs during the encryption process. To address these challenges\nand defend against personalized voice deepfake threats, we propose Enkidu, a\nnovel user-oriented privacy-preserving framework that leverages universal\nfrequential perturbations generated through black-box knowledge and few-shot\ntraining on a small amount of user data. These highly malleable\nfrequency-domain noise patches enable real-time, lightweight protection with\nstrong generalization across variable-length audio and robust resistance to\nvoice deepfake attacks, all while preserving perceptual quality and speech\nintelligibility. Notably, Enkidu achieves over 50 to 200 times processing\nmemory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime\nefficiency (real-time coefficient as low as 0.004) compared to six\nstate-of-the-art countermeasures. Extensive experiments across six mainstream\ntext-to-speech models and five cutting-edge automated speaker verification\nmodels demonstrate the effectiveness, transferability, and practicality of\nEnkidu in defending against both vanilla and adaptive voice deepfake attacks.",
    "updated" : "2025-07-17T09:12:36Z",
    "published" : "2025-07-17T09:12:36Z",
    "authors" : [
      {
        "name" : "Zhou Feng"
      },
      {
        "name" : "Jiahao Chen"
      },
      {
        "name" : "Chunyi Zhou"
      },
      {
        "name" : "Yuwen Pu"
      },
      {
        "name" : "Qingming Li"
      },
      {
        "name" : "Tianyu Du"
      },
      {
        "name" : "Shouling Ji"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.12730v1",
    "title" : "A Privacy-Preserving Semantic-Segmentation Method Using\n  Domain-Adaptation Technique",
    "summary" : "We propose a privacy-preserving semantic-segmentation method for applying\nperceptual encryption to images used for model training in addition to test\nimages. This method also provides almost the same accuracy as models without\nany encryption. The above performance is achieved using a domain-adaptation\ntechnique on the embedding structure of the Vision Transformer (ViT). The\neffectiveness of the proposed method was experimentally confirmed in terms of\nthe accuracy of semantic segmentation when using a powerful\nsemantic-segmentation model with ViT called Segmentation Transformer.",
    "updated" : "2025-07-17T02:14:50Z",
    "published" : "2025-07-17T02:14:50Z",
    "authors" : [
      {
        "name" : "Homare Sueyoshi"
      },
      {
        "name" : "Kiyoshi Nishikawa"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.12652v1",
    "title" : "Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and\n  Performance Perspective",
    "summary" : "Invasive and non-invasive neural interfaces hold promise as high-bandwidth\ninput devices for next-generation technologies. However, neural signals\ninherently encode sensitive information about an individual's identity and\nhealth, making data sharing for decoder training a critical privacy challenge.\nFederated learning (FL), a distributed, privacy-preserving learning framework,\npresents a promising solution, but it remains unexplored in closed-loop\nadaptive neural interfaces. Here, we introduce FL-based neural decoding and\nsystematically evaluate its performance and privacy using high-dimensional\nelectromyography signals in both open- and closed-loop scenarios. In open-loop\nsimulations, FL significantly outperformed local learning baselines,\ndemonstrating its potential for high-performance, privacy-conscious neural\ndecoding. In contrast, closed-loop user studies required adapting FL methods to\naccommodate single-user, real-time interactions, a scenario not supported by\nstandard FL. This modification resulted in local learning decoders surpassing\nthe adapted FL approach in closed-loop performance, yet local learning still\ncarried higher privacy risks. Our findings highlight a critical\nperformance-privacy tradeoff in real-time adaptive applications and indicate\nthe need for FL methods specifically designed for co-adaptive, single-user\napplications.",
    "updated" : "2025-07-16T21:59:25Z",
    "published" : "2025-07-16T21:59:25Z",
    "authors" : [
      {
        "name" : "Kai Malcolm"
      },
      {
        "name" : "César Uribe"
      },
      {
        "name" : "Momona Yamagami"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.13981v1",
    "title" : "Evaluation of Human Visual Privacy Protection: A Three-Dimensional\n  Framework and Benchmark Dataset",
    "summary" : "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.",
    "updated" : "2025-07-18T14:43:24Z",
    "published" : "2025-07-18T14:43:24Z",
    "authors" : [
      {
        "name" : "Sara Abdulaziz"
      },
      {
        "name" : "Giacomo D'Amicantonio"
      },
      {
        "name" : "Egor Bondarev"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.13926v1",
    "title" : "Developers Insight On Manifest v3 Privacy and Security Webextensions",
    "summary" : "Webextensions can improve web browser privacy, security, and user experience.\nThe APIs offered by the browser to webextensions affect possible functionality.\nCurrently, Chrome transitions to a modified set of APIs called Manifest v3.\nThis paper studies the challenges and opportunities of Manifest v3 with an\nin-depth structured qualitative research. Even though some projects observed\npositive effects, a majority expresses concerns over limited benefits to users,\nremoval of crucial APIs, or the need to find workarounds. Our findings indicate\nthat the transition affects different types of webextensions differently; some\ncan migrate without losing functionality, while other projects remove\nfunctionality or decline to update. The respondents identified several critical\nmissing APIs, including reliable APIs to inject content scripts, APIs for\nstoring confidential content, and others.",
    "updated" : "2025-07-18T14:00:16Z",
    "published" : "2025-07-18T14:00:16Z",
    "authors" : [
      {
        "name" : "Libor Polčák"
      },
      {
        "name" : "Giorgio Maone"
      },
      {
        "name" : "Michael McMahon"
      },
      {
        "name" : "Martin Bednář"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.13639v1",
    "title" : "Differential Privacy in Kernelized Contextual Bandits via Random\n  Projections",
    "summary" : "We consider the problem of contextual kernel bandits with stochastic\ncontexts, where the underlying reward function belongs to a known Reproducing\nKernel Hilbert Space. We study this problem under an additional constraint of\nDifferential Privacy, where the agent needs to ensure that the sequence of\nquery points is differentially private with respect to both the sequence of\ncontexts and rewards. We propose a novel algorithm that achieves the\nstate-of-the-art cumulative regret of\n$\\widetilde{\\mathcal{O}}(\\sqrt{\\gamma_TT}+\\frac{\\gamma_T}{\\varepsilon_{\\mathrm{DP}}})$\nand\n$\\widetilde{\\mathcal{O}}(\\sqrt{\\gamma_TT}+\\frac{\\gamma_T\\sqrt{T}}{\\varepsilon_{\\mathrm{DP}}})$\nover a time horizon of $T$ in the joint and local models of differential\nprivacy, respectively, where $\\gamma_T$ is the effective dimension of the\nkernel and $\\varepsilon_{\\mathrm{DP}} > 0$ is the privacy parameter. The key\ningredient of the proposed algorithm is a novel private kernel-ridge regression\nestimator which is based on a combination of private covariance estimation and\nprivate random projections. It offers a significantly reduced sensitivity\ncompared to its classical counterpart while maintaining a high prediction\naccuracy, allowing our algorithm to achieve the state-of-the-art performance\nguarantees.",
    "updated" : "2025-07-18T03:54:49Z",
    "published" : "2025-07-18T03:54:49Z",
    "authors" : [
      {
        "name" : "Nikola Pavlovic"
      },
      {
        "name" : "Sudeep Salgia"
      },
      {
        "name" : "Qing Zhao"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.11649v2",
    "title" : "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation\n  using Zero-Knowledge Proofs",
    "summary" : "Federated Learning (FL) enables collaborative model training on decentralized\ndata without exposing raw data. However, the evaluation phase in FL may leak\nsensitive information through shared performance metrics. In this paper, we\npropose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to\nenable privacy-preserving and verifiable evaluation for FL. Instead of\nrevealing raw loss values, clients generate a succinct proof asserting that\ntheir local loss is below a predefined threshold. Our approach is implemented\nwithout reliance on external APIs, using self-contained modules for federated\nlearning simulation, ZKP circuit design, and experimental evaluation on both\nthe MNIST and Human Activity Recognition (HAR) datasets. We focus on a\nthreshold-based proof for a simple Convolutional Neural Network (CNN) model\n(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate\nthe approach in terms of computational overhead, communication cost, and\nverifiability.",
    "updated" : "2025-07-18T03:24:50Z",
    "published" : "2025-07-15T18:34:14Z",
    "authors" : [
      {
        "name" : "Daniel Commey"
      },
      {
        "name" : "Benjamin Appiah"
      },
      {
        "name" : "Griffith S. Klogo"
      },
      {
        "name" : "Garth V. Crosby"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.09067v2",
    "title" : "Quantum-Resilient Privacy Ledger (QRPL): A Sovereign Digital Currency\n  for the Post-Quantum Era",
    "summary" : "The emergence of quantum computing presents profound challenges to existing\ncryptographic infrastructures, whilst the development of central bank digital\ncurrencies (CBDCs) has raised concerns regarding privacy preservation and\nexcessive centralisation in digital payment systems. This paper proposes the\nQuantum-Resilient Privacy Ledger (QRPL) as an innovative token-based digital\ncurrency architecture that incorporates National Institute of Standards and\nTechnology (NIST)-standardised post-quantum cryptography (PQC) with hash-based\nzero-knowledge proofs to ensure user sovereignty, scalability, and transaction\nconfidentiality. Key contributions include adaptations of ephemeral proof\nchains for unlinkable transactions, a privacy-weighted Proof-of-Stake (PoS)\nconsensus to promote equitable participation, and a novel zero-knowledge\nproof-based mechanism for privacy-preserving selective disclosure. QRPL aims to\naddress critical shortcomings in prevailing CBDC designs, including risks of\npervasive surveillance, with a 10-20 second block time to balance security and\nthroughput in future monetary systems. While conceptual, empirical prototypes\nare planned. Future work includes prototype development to validate these\nmodels empirically.",
    "updated" : "2025-07-18T16:51:19Z",
    "published" : "2025-07-11T23:02:45Z",
    "authors" : [
      {
        "name" : "Serhan W. Bahar"
      }
    ],
    "categories" : [
      "cs.ET",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.15836v1",
    "title" : "Optimizing Canaries for Privacy Auditing with Metagradient Descent",
    "summary" : "In this work we study black-box privacy auditing, where the goal is to lower\nbound the privacy parameter of a differentially private learning algorithm\nusing only the algorithm's outputs (i.e., final trained model). For DP-SGD (the\nmost successful method for training differentially private deep learning\nmodels), the canonical approach auditing uses membership inference-an auditor\ncomes with a small set of special \"canary\" examples, inserts a random subset of\nthem into the training set, and then tries to discern which of their canaries\nwere included in the training set (typically via a membership inference\nattack). The auditor's success rate then provides a lower bound on the privacy\nparameters of the learning algorithm. Our main contribution is a method for\noptimizing the auditor's canary set to improve privacy auditing, leveraging\nrecent work on metagradient optimization. Our empirical evaluation demonstrates\nthat by using such optimized canaries, we can improve empirical lower bounds\nfor differentially private image classification models by over 2x in certain\ninstances. Furthermore, we demonstrate that our method is transferable and\nefficient: canaries optimized for non-private SGD with a small model\narchitecture remain effective when auditing larger models trained with DP-SGD.",
    "updated" : "2025-07-21T17:47:33Z",
    "published" : "2025-07-21T17:47:33Z",
    "authors" : [
      {
        "name" : "Matteo Boglioni"
      },
      {
        "name" : "Terrance Liu"
      },
      {
        "name" : "Andrew Ilyas"
      },
      {
        "name" : "Zhiwei Steven Wu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.15460v1",
    "title" : "Privacy-Preserving Multimodal News Recommendation through Federated\n  Learning",
    "summary" : "Personalized News Recommendation systems (PNR) have emerged as a solution to\ninformation overload by predicting and suggesting news items tailored to\nindividual user interests. However, traditional PNR systems face several\nchallenges, including an overreliance on textual content, common neglect of\nshort-term user interests, and significant privacy concerns due to centralized\ndata storage. This paper addresses these issues by introducing a novel\nmultimodal federated learning-based approach for news recommendation. First, it\nintegrates both textual and visual features of news items using a multimodal\nmodel, enabling a more comprehensive representation of content. Second, it\nemploys a time-aware model that balances users' long-term and short-term\ninterests through multi-head self-attention networks, improving recommendation\naccuracy. Finally, to enhance privacy, a federated learning framework is\nimplemented, enabling collaborative model training without sharing user data.\nThe framework divides the recommendation model into a large server-maintained\nnews model and a lightweight user model shared between the server and clients.\nThe client requests news representations (vectors) and a user model from the\ncentral server, then computes gradients with user local data, and finally sends\ntheir locally computed gradients to the server for aggregation. The central\nserver aggregates gradients to update the global user model and news model. The\nupdated news model is further used to infer news representation by the server.\nTo further safeguard user privacy, a secure aggregation algorithm based on\nShamir's secret sharing is employed. Experiments on a real-world news dataset\ndemonstrate strong performance compared to existing systems, representing a\nsignificant advancement in privacy-preserving personalized news recommendation.",
    "updated" : "2025-07-21T10:14:00Z",
    "published" : "2025-07-21T10:14:00Z",
    "authors" : [
      {
        "name" : "Mehdi Khalaj"
      },
      {
        "name" : "Shahrzad Golestani Najafabadi"
      },
      {
        "name" : "Julita Vassileva"
      }
    ],
    "categories" : [
      "cs.SI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.15124v1",
    "title" : "Comprehensive Privacy Risk Assessment in Social Networks Using User\n  Attributes Social Graphs and Text Analysis",
    "summary" : "The rise of social networking platforms has amplified privacy threats as\nusers increasingly share sensitive information across profiles, content, and\nsocial connections. We present a Comprehensive Privacy Risk Scoring (CPRS)\nframework that quantifies privacy risk by integrating user attributes, social\ngraph structures, and user-generated content. Our framework computes risk\nscores across these dimensions using sensitivity, visibility, structural\nsimilarity, and entity-level analysis, then aggregates them into a unified risk\nscore. We validate CPRS on two real-world datasets: the SNAP Facebook Ego\nNetwork (4,039 users) and the Koo microblogging dataset (1M posts, 1M\ncomments). The average CPRS is 0.478 with equal weighting, rising to 0.501 in\ngraph-sensitive scenarios. Component-wise, graph-based risks (mean 0.52)\nsurpass content (0.48) and profile attributes (0.45). High-risk attributes\ninclude email, date of birth, and mobile number. Our user study with 100\nparticipants shows 85% rated the dashboard as clear and actionable, confirming\nCPRS's practical utility. This work enables personalized privacy risk insights\nand contributes a holistic, scalable methodology for privacy management. Future\ndirections include incorporating temporal dynamics and multimodal content for\nbroader applicability.",
    "updated" : "2025-07-20T21:18:50Z",
    "published" : "2025-07-20T21:18:50Z",
    "authors" : [
      {
        "name" : "Md Jahangir Alam"
      },
      {
        "name" : "Ismail Hossain"
      },
      {
        "name" : "Sai Puppala"
      },
      {
        "name" : "Sajedul Talukder"
      }
    ],
    "categories" : [
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.14985v1",
    "title" : "Metaverse Security and Privacy Research: A Systematic Review",
    "summary" : "The rapid growth of metaverse technologies, including virtual worlds,\naugmented reality, and lifelogging, has accelerated their adoption across\ndiverse domains. This rise exposes users to significant new security and\nprivacy challenges due to sociotechnical complexity, pervasive connectivity,\nand extensive user data collection in immersive environments. We present a\nsystematic review of the literature published between 2013 and 2024, offering a\ncomprehensive analysis of how the research community has addressed\nmetaverse-related security and privacy issues over the past decade. We organize\nthe studies by method, examined the security and privacy properties, immersive\ncomponents, and evaluation strategies. Our investigation reveals a sharp\nincrease in research activity in the last five years, a strong focus on\npractical and user-centered approaches, and a predominant use of benchmarking,\nhuman experimentation, and qualitative methods. Authentication and\nunobservability are the most frequently studied properties. However, critical\ngaps remain in areas such as policy compliance, accessibility,\ninteroperability, and back-end infrastructure security. We emphasize the\nintertwined technical complexity and human factors of the metaverse and call\nfor integrated, interdisciplinary approaches to securing inclusive and\ntrustworthy immersive environments.",
    "updated" : "2025-07-20T14:42:17Z",
    "published" : "2025-07-20T14:42:17Z",
    "authors" : [
      {
        "name" : "Argianto Rahartomo"
      },
      {
        "name" : "Leonel Merino"
      },
      {
        "name" : "Mohammad Ghafari"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.ET",
      "cs.HC",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.14853v1",
    "title" : "A Privacy-Centric Approach: Scalable and Secure Federated Learning\n  Enabled by Hybrid Homomorphic Encryption",
    "summary" : "Federated Learning (FL) enables collaborative model training without sharing\nraw data, making it a promising approach for privacy-sensitive domains. Despite\nits potential, FL faces significant challenges, particularly in terms of\ncommunication overhead and data privacy. Privacy-preserving Techniques (PPTs)\nsuch as Homomorphic Encryption (HE) have been used to mitigate these concerns.\nHowever, these techniques introduce substantial computational and communication\ncosts, limiting their practical deployment. In this work, we explore how Hybrid\nHomomorphic Encryption (HHE), a cryptographic protocol that combines symmetric\nencryption with HE, can be effectively integrated with FL to address both\ncommunication and privacy challenges, paving the way for scalable and secure\ndecentralized learning system.",
    "updated" : "2025-07-20T07:46:53Z",
    "published" : "2025-07-20T07:46:53Z",
    "authors" : [
      {
        "name" : "Khoa Nguyen"
      },
      {
        "name" : "Tanveer Khan"
      },
      {
        "name" : "Antonis Michalas"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.14713v1",
    "title" : "Privacy-Preserving Drone Navigation Through Homomorphic Encryption for\n  Collision Avoidance",
    "summary" : "As drones increasingly deliver packages in neighborhoods, concerns about\ncollisions arise. One solution is to share flight paths within a specific zip\ncode, but this compromises business privacy by revealing delivery routes. For\nexample, it could disclose which stores send packages to certain addresses. To\navoid exposing path information, we propose using homomorphic encryption-based\ncomparison to compute path intersections. This allows drones to identify\npotential collisions without revealing path and destination details, allowing\nthem to adjust altitude to avoid crashes. We implemented and tested our\napproach on resource-limited virtual machines to mimic the computational power\nof drones. Our results demonstrate that our method is significantly faster and\nrequires less network communication compared to a garbled circuit-based\napproach. We also provide a security analysis of the approach against potential\nattacks.",
    "updated" : "2025-07-19T18:16:02Z",
    "published" : "2025-07-19T18:16:02Z",
    "authors" : [
      {
        "name" : "Allan Luedeman"
      },
      {
        "name" : "Nicholas Baum"
      },
      {
        "name" : "Andrew Quijano"
      },
      {
        "name" : "Kemal Akkaya"
      }
    ],
    "categories" : [
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.14629v1",
    "title" : "VMask: Tunable Label Privacy Protection for Vertical Federated Learning\n  via Layer Masking",
    "summary" : "Though vertical federated learning (VFL) is generally considered to be\nprivacy-preserving, recent studies have shown that VFL system is vulnerable to\nlabel inference attacks originating from various attack surfaces. Among these\nattacks, the model completion (MC) attack is currently the most powerful one.\nExisting defense methods against it either sacrifice model accuracy or incur\nimpractical computational overhead. In this paper, we propose VMask, a novel\nlabel privacy protection framework designed to defend against MC attack from\nthe perspective of layer masking. Our key insight is to disrupt the strong\ncorrelation between input data and intermediate outputs by applying the secret\nsharing (SS) technique to mask layer parameters in the attacker's model. We\ndevise a strategy for selecting critical layers to mask, reducing the overhead\nthat would arise from naively applying SS to the entire model. Moreover, VMask\nis the first framework to offer a tunable privacy budget to defenders, allowing\nfor flexible control over the levels of label privacy according to actual\nrequirements. We built a VFL system, implemented VMask on it, and extensively\nevaluated it using five model architectures and 13 datasets with different\nmodalities, comparing it to 12 other defense methods. The results demonstrate\nthat VMask achieves the best privacy-utility trade-off, successfully thwarting\nthe MC attack (reducing the label inference accuracy to a random guessing\nlevel) while preserving model performance (e.g., in Transformer-based model,\nthe averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up\nto 60,846 times faster than cryptography-based methods, and it only marginally\nexceeds that of standard VFL by 1.8 times in a large Transformer-based model,\nwhich is generally acceptable.",
    "updated" : "2025-07-19T13:51:09Z",
    "published" : "2025-07-19T13:51:09Z",
    "authors" : [
      {
        "name" : "Juntao Tan"
      },
      {
        "name" : "Lan Zhang"
      },
      {
        "name" : "Zhonghao Hu"
      },
      {
        "name" : "Kai Yang"
      },
      {
        "name" : "Peng Ran"
      },
      {
        "name" : "Bo Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.14519v1",
    "title" : "Towards Efficient Privacy-Preserving Machine Learning: A Systematic\n  Review from Protocol, Model, and System Perspectives",
    "summary" : "Privacy-preserving machine learning (PPML) based on cryptographic protocols\nhas emerged as a promising paradigm to protect user data privacy in cloud-based\nmachine learning services. While it achieves formal privacy protection, PPML\noften incurs significant efficiency and scalability costs due to orders of\nmagnitude overhead compared to the plaintext counterpart. Therefore, there has\nbeen a considerable focus on mitigating the efficiency gap for PPML. In this\nsurvey, we provide a comprehensive and systematic review of recent PPML studies\nwith a focus on cross-level optimizations. Specifically, we categorize existing\npapers into protocol level, model level, and system level, and review progress\nat each level. We also provide qualitative and quantitative comparisons of\nexisting works with technical insights, based on which we discuss future\nresearch directions and highlight the necessity of integrating optimizations\nacross protocol, model, and system levels. We hope this survey can provide an\noverarching understanding of existing approaches and potentially inspire future\nbreakthroughs in the PPML field. As the field is evolving fast, we also provide\na public GitHub repository to continuously track the developments, which is\navailable at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.",
    "updated" : "2025-07-19T07:45:39Z",
    "published" : "2025-07-19T07:45:39Z",
    "authors" : [
      {
        "name" : "Wenxuan Zeng"
      },
      {
        "name" : "Tianshi Xu"
      },
      {
        "name" : "Yi Chen"
      },
      {
        "name" : "Yifan Zhou"
      },
      {
        "name" : "Mingzhe Zhang"
      },
      {
        "name" : "Jin Tan"
      },
      {
        "name" : "Cheng Hong"
      },
      {
        "name" : "Meng Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.14214v1",
    "title" : "Let's Measure the Elephant in the Room: Facilitating Personalized\n  Automated Analysis of Privacy Policies at Scale",
    "summary" : "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites despite claiming\notherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that\nassists users with personalized privacy policy analysis. PoliAnalyzer uses\nNatural Language Processing (NLP) to extract formal representations of data\nusage practices from policy texts. In favor of deterministic, logical inference\nis applied to compare user preferences with the formal privacy policy\nrepresentation and produce a compliance report. To achieve this, we extend an\nexisting formal Data Terms of Use policy language to model privacy policies as\napp policies and user preferences as data policies. In our evaluation using our\nenriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated\nhigh accuracy in identifying relevant data usage practices, achieving F1-score\nof 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can\nmodel diverse user data-sharing preferences, derived from prior research as 23\nuser profiles, and perform compliance analysis against the top 100 most-visited\nwebsites. This analysis revealed that, on average, 95.2% of a privacy policy's\nsegments do not conflict with the analyzed user preferences, enabling users to\nconcentrate on understanding the 4.8% (636 / 13205) that violates preferences,\nsignificantly reducing cognitive burden. Further, we identified common\npractices in privacy policies that violate user expectations - such as the\nsharing of location data with 3rd parties. This paper demonstrates that\nPoliAnalyzer can support automated personalized privacy policy analysis at\nscale using off-the-shelf NLP tools. This sheds light on a pathway to help\nindividuals regain control over their data and encourage societal discussions\non platform data practices to promote a fairer power dynamic.",
    "updated" : "2025-07-15T20:19:33Z",
    "published" : "2025-07-15T20:19:33Z",
    "authors" : [
      {
        "name" : "Rui Zhao"
      },
      {
        "name" : "Vladyslav Melnychuk"
      },
      {
        "name" : "Jun Zhao"
      },
      {
        "name" : "Jesse Wright"
      },
      {
        "name" : "Nigel Shadbolt"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.09051v2",
    "title" : "SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant\n  Reviews from Mental Health Apps",
    "summary" : "Mental health (MH) apps often require sensitive user data to customize\nservices for mental wellness needs. However, such data collection practices in\nsome MH apps raise significant privacy concerns for users. These concerns are\noften mentioned in app reviews, but other feedback categories, such as\nreliability and usability, tend to take precedence. This poses a significant\nchallenge in automatically identifying privacy requirements-relevant reviews\n(privacy reviews) that can be utilized to extract privacy requirements and\naddress users' privacy concerns. Thus, this study introduces SAGE, a\ncontext-aware approach to automatically mining privacy reviews from MH apps\nusing Natural Language Inference (NLI) with MH domain-specific privacy\nhypotheses (provides domain-specific context awareness) and a GPT model\n(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a\ndataset of 204K app reviews achieved an F1 score of 0.85 without any\nfine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.\nFurthermore, SAGE extracted 748 privacy reviews previously overlooked by\nkeyword-based methods, demonstrating its effectiveness through qualitative\nevaluation. These reviews can later be refined into actionable privacy\nrequirement artifacts.",
    "updated" : "2025-07-20T04:37:04Z",
    "published" : "2025-07-11T21:53:56Z",
    "authors" : [
      {
        "name" : "Aakash Sorathiya"
      },
      {
        "name" : "Gouri Ginde"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.16788v1",
    "title" : "AUTOPSY: A Framework for Tackling Privacy Challenges in the Automotive\n  Industry",
    "summary" : "With the General Data Protection Regulation (GDPR) in place, all domains have\nto ensure compliance with privacy legislation. However, compliance does not\nnecessarily result in a privacy-friendly system as for example getting users'\nconsent to process their data does not improve the privacy-friendliness of the\nsystem. Therefore, the goal of the AUTOPSY project was to support the privacy\nengineering process in the automotive domain by providing several building\nblocks which technically improve the privacy-friendliness of modern, i.e.,\nconnected and (partially) automated vehicles. This paper presents the results\nof the AUTOPSY project: a system model to identify relevant entities and\nlocations to apply privacy enhancing technologies (PETs); the privacy manager\naiming at more control of the data flow from the vehicle, a PET selection\napproach based on GDPR principles, and an architectural framework for\nautomotive privacy. Furthermore, we built a demonstrator for location-based\nservices to evaluate the architectural framework.",
    "updated" : "2025-07-22T17:32:20Z",
    "published" : "2025-07-22T17:32:20Z",
    "authors" : [
      {
        "name" : "Sebastian Pape"
      },
      {
        "name" : "Anis Bkakria"
      },
      {
        "name" : "Maurice Heymann"
      },
      {
        "name" : "Badreddine Chah"
      },
      {
        "name" : "Abdeljalil Abbas-Turki"
      },
      {
        "name" : "Sarah Syed-Winkler"
      },
      {
        "name" : "Matthias Hiller"
      },
      {
        "name" : "Reda Yaich"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.16391v1",
    "title" : "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
    "summary" : "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
    "updated" : "2025-07-22T09:35:59Z",
    "published" : "2025-07-22T09:35:59Z",
    "authors" : [
      {
        "name" : "Chenqi Lin"
      },
      {
        "name" : "Kang Yang"
      },
      {
        "name" : "Tianshi Xu"
      },
      {
        "name" : "Ling Liang"
      },
      {
        "name" : "Yufei Wang"
      },
      {
        "name" : "Zhaohui Chen"
      },
      {
        "name" : "Runsheng Wang"
      },
      {
        "name" : "Mingyu Gao"
      },
      {
        "name" : "Meng Li"
      }
    ],
    "categories" : [
      "cs.AR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.16372v1",
    "title" : "Depth Gives a False Sense of Privacy: LLM Internal States Inversion",
    "summary" : "Large Language Models (LLMs) are increasingly integrated into daily routines,\nyet they raise significant privacy and safety concerns. Recent research\nproposes collaborative inference, which outsources the early-layer inference to\nensure data locality, and introduces model safety auditing based on inner\nneuron patterns. Both techniques expose the LLM's Internal States (ISs), which\nare traditionally considered irreversible to inputs due to optimization\nchallenges and the highly abstract representations in deep layers. In this\nwork, we challenge this assumption by proposing four inversion attacks that\nsignificantly improve the semantic similarity and token matching rate of\ninverted inputs. Specifically, we first develop two white-box\noptimization-based attacks tailored for low-depth and high-depth ISs. These\nattacks avoid local minima convergence, a limitation observed in prior work,\nthrough a two-phase inversion process. Then, we extend our optimization attack\nunder more practical black-box weight access by leveraging the transferability\nbetween the source and the derived LLMs. Additionally, we introduce a\ngeneration-based attack that treats inversion as a translation task, employing\nan inversion model to reconstruct inputs. Extensive evaluation of short and\nlong prompts from medical consulting and coding assistance datasets and 6 LLMs\nvalidates the effectiveness of our inversion attacks. Notably, a 4,112-token\nlong medical consulting prompt can be nearly perfectly inverted with 86.88 F1\ntoken matching from the middle layer of Llama-3 model. Finally, we evaluate\nfour practical defenses that we found cannot perfectly prevent ISs inversion\nand draw conclusions for future mitigation design.",
    "updated" : "2025-07-22T09:15:11Z",
    "published" : "2025-07-22T09:15:11Z",
    "authors" : [
      {
        "name" : "Tian Dong"
      },
      {
        "name" : "Yan Meng"
      },
      {
        "name" : "Shaofeng Li"
      },
      {
        "name" : "Guoxing Chen"
      },
      {
        "name" : "Zhen Liu"
      },
      {
        "name" : "Haojin Zhu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.16247v1",
    "title" : "PRAC3 (Privacy, Reputation, Accountability, Consent, Credit,\n  Compensation): Long Tailed Risks of Voice Actors in AI Data-Economy",
    "summary" : "Early large-scale audio datasets, such as LibriSpeech, were built with\nhundreds of individual contributors whose voices were instrumental in the\ndevelopment of speech technologies, including audiobooks and voice assistants.\nYet, a decade later, these same contributions have exposed voice actors to a\nrange of risks. While existing ethical frameworks emphasize Consent, Credit,\nand Compensation (C3), they do not adequately address the emergent risks\ninvolving vocal identities that are increasingly decoupled from context,\nauthorship, and control. Drawing on qualitative interviews with 20 professional\nvoice actors, this paper reveals how the synthetic replication of voice without\nenforceable constraints exposes individuals to a range of threats. Beyond\nreputational harm, such as re-purposing voice data in erotic content, offensive\npolitical messaging, and meme culture, we document concerns about\naccountability breakdowns when their voice is leveraged to clone voices that\nare deployed in high-stakes scenarios such as financial fraud, misinformation\ncampaigns, or impersonation scams. In such cases, actors face social and legal\nfallout without recourse, while very few of them have a legal representative or\nunion protection. To make sense of these shifting dynamics, we introduce the\nPRAC3 framework, an expansion of C3 that foregrounds Privacy, Reputation,\nAccountability, Consent, Credit, and Compensation as interdependent pillars of\ndata used in the synthetic voice economy. This framework captures how privacy\nrisks are amplified through non-consensual training, how reputational harm\narises from decontextualized deployment, and how accountability can be\nreimagined AI Data ecosystems. We argue that voice, as both a biometric\nidentifier and creative labor, demands governance models that restore creator\nagency, ensure traceability, and establish enforceable boundaries for ethical\nreuse.",
    "updated" : "2025-07-22T05:39:39Z",
    "published" : "2025-07-22T05:39:39Z",
    "authors" : [
      {
        "name" : "Tanusree Sharma"
      },
      {
        "name" : "Yihao Zhou"
      },
      {
        "name" : "Visar Berisha"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.16134v1",
    "title" : "DP2Guard: A Lightweight and Byzantine-Robust Privacy-Preserving\n  Federated Learning Scheme for Industrial IoT",
    "summary" : "Privacy-Preserving Federated Learning (PPFL) has emerged as a secure\ndistributed Machine Learning (ML) paradigm that aggregates locally trained\ngradients without exposing raw data. To defend against model poisoning threats,\nseveral robustness-enhanced PPFL schemes have been proposed by integrating\nanomaly detection. Nevertheless, they still face two major challenges: (1) the\nreliance on heavyweight encryption techniques results in substantial\ncommunication and computation overhead; and (2) single-strategy defense\nmechanisms often fail to provide sufficient robustness against adaptive\nadversaries. To overcome these challenges, we propose DP2Guard, a lightweight\nPPFL framework that enhances both privacy and robustness. DP2Guard leverages a\nlightweight gradient masking mechanism to replace costly cryptographic\noperations while ensuring the privacy of local gradients. A hybrid defense\nstrategy is proposed, which extracts gradient features using singular value\ndecomposition and cosine similarity, and applies a clustering algorithm to\neffectively identify malicious gradients. Additionally, DP2Guard adopts a trust\nscore-based adaptive aggregation scheme that adjusts client weights according\nto historical behavior, while blockchain records aggregated results and trust\nscores to ensure tamper-proof and auditable training. Extensive experiments\nconducted on two public datasets demonstrate that DP2Guard effectively defends\nagainst four advanced poisoning attacks while ensuring privacy with reduced\ncommunication and computation costs.",
    "updated" : "2025-07-22T01:06:39Z",
    "published" : "2025-07-22T01:06:39Z",
    "authors" : [
      {
        "name" : "Baofu Han"
      },
      {
        "name" : "Bing Li"
      },
      {
        "name" : "Yining Qi"
      },
      {
        "name" : "Raja Jurdak"
      },
      {
        "name" : "Kaibin Huang"
      },
      {
        "name" : "Chau Yuen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.16124v1",
    "title" : "Benchmarking LLM Privacy Recognition for Social Robot Decision Making",
    "summary" : "Social robots are embodied agents that interact with people while following\nhuman communication norms. These robots interact using verbal and non-verbal\ncues, and share the physical environments of people. While social robots have\npreviously utilized rule-based systems or probabilistic models for user\ninteraction, the rapid evolution of large language models (LLMs) presents new\nopportunities to develop LLM-empowered social robots for enhanced human-robot\ninteraction. To fully realize these capabilities, however, robots need to\ncollect data such as audio, fine-grained images, video, and locations. As a\nresult, LLMs often process sensitive personal information, particularly within\nhome environments. Given the tension between utility and privacy risks,\nevaluating how current LLMs manage sensitive data is critical. Specifically, we\naim to explore the extent to which out-of-the-box LLMs are privacy-aware in the\ncontext of household social robots. In this study, we present a set of\nprivacy-relevant scenarios crafted through the lens of Contextual Integrity\n(CI). We first survey users' privacy preferences regarding in-home social robot\nbehaviors and then examine how their privacy orientation affects their choices\nof these behaviors (N = 450). We then provide the same set of scenarios and\nquestions to state-of-the-art LLMs (N = 10) and find that the agreement between\nhumans and LLMs is low. To further investigate the capabilities of LLMs as a\npotential privacy controller, we implement four additional prompting strategies\nand compare their results. Finally, we discuss the implications and potential\nof AI privacy awareness in human-robot interaction.",
    "updated" : "2025-07-22T00:36:59Z",
    "published" : "2025-07-22T00:36:59Z",
    "authors" : [
      {
        "name" : "Dakota Sullivan"
      },
      {
        "name" : "Shirley Zhang"
      },
      {
        "name" : "Jennica Li"
      },
      {
        "name" : "Heather Kirkorian"
      },
      {
        "name" : "Bilge Mutlu"
      },
      {
        "name" : "Kassem Fawaz"
      }
    ],
    "categories" : [
      "cs.RO",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.16034v1",
    "title" : "Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images\n  Applied to Privacy-Preserving Object-Goal Navigation",
    "summary" : "User privacy in mobile robotics has become a critical concern. Existing\nmethods typically prioritize either the performance of downstream robotic tasks\nor privacy protection, with the latter often constraining the effectiveness of\ntask execution. To jointly address both objectives, we study semantic-based\nrobot navigation in an ultra-low-resolution setting to preserve visual privacy.\nA key challenge in such scenarios is recovering semantic segmentation from\nultra-low-resolution RGB images. In this work, we introduce a novel fully\njoint-learning method that integrates an agglomerative feature extractor and a\nsegmentation-aware discriminator to solve ultra-low-resolution semantic\nsegmentation, thereby enabling privacy-preserving, semantic object-goal\nnavigation. Our method outperforms different baselines on ultra-low-resolution\nsemantic segmentation and our improved segmentation results increase the\nsuccess rate of the semantic object-goal navigation in a real-world\nprivacy-constrained scenario.",
    "updated" : "2025-07-21T19:53:40Z",
    "published" : "2025-07-21T19:53:40Z",
    "authors" : [
      {
        "name" : "Xuying Huang"
      },
      {
        "name" : "Sicong Pan"
      },
      {
        "name" : "Olga Zatsarynna"
      },
      {
        "name" : "Juergen Gall"
      },
      {
        "name" : "Maren Bennewitz"
      }
    ],
    "categories" : [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.15997v1",
    "title" : "\"We Need a Standard\": Toward an Expert-Informed Privacy Label for\n  Differential Privacy",
    "summary" : "The increasing adoption of differential privacy (DP) leads to public-facing\nDP deployments by both government agencies and companies. However, real-world\nDP deployments often do not fully disclose their privacy guarantees, which vary\ngreatly between deployments. Failure to disclose certain DP parameters can lead\nto misunderstandings about the strength of the privacy guarantee, undermining\nthe trust in DP. In this work, we seek to inform future standards for\ncommunicating the privacy guarantees of DP deployments. Based on\nsemi-structured interviews with 12 DP experts, we identify important DP\nparameters necessary to comprehensively communicate DP guarantees, and describe\nwhy and how they should be disclosed. Based on expert recommendations, we\ndesign an initial privacy label for DP to comprehensively communicate privacy\nguarantees in a standardized format.",
    "updated" : "2025-07-21T18:32:04Z",
    "published" : "2025-07-21T18:32:04Z",
    "authors" : [
      {
        "name" : "Onyinye Dibia"
      },
      {
        "name" : "Mengyi Lu"
      },
      {
        "name" : "Prianka Bhattacharjee"
      },
      {
        "name" : "Joseph P. Near"
      },
      {
        "name" : "Yuanyuan Feng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC",
      "68-XX 68-XX 68-XX"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.15460v2",
    "title" : "Privacy-Preserving Multimodal News Recommendation through Federated\n  Learning",
    "summary" : "Personalized News Recommendation systems (PNR) have emerged as a solution to\ninformation overload by predicting and suggesting news items tailored to\nindividual user interests. However, traditional PNR systems face several\nchallenges, including an overreliance on textual content, common neglect of\nshort-term user interests, and significant privacy concerns due to centralized\ndata storage. This paper addresses these issues by introducing a novel\nmultimodal federated learning-based approach for news recommendation. First, it\nintegrates both textual and visual features of news items using a multimodal\nmodel, enabling a more comprehensive representation of content. Second, it\nemploys a time-aware model that balances users' long-term and short-term\ninterests through multi-head self-attention networks, improving recommendation\naccuracy. Finally, to enhance privacy, a federated learning framework is\nimplemented, enabling collaborative model training without sharing user data.\nThe framework divides the recommendation model into a large server-maintained\nnews model and a lightweight user model shared between the server and clients.\nThe client requests news representations (vectors) and a user model from the\ncentral server, then computes gradients with user local data, and finally sends\ntheir locally computed gradients to the server for aggregation. The central\nserver aggregates gradients to update the global user model and news model. The\nupdated news model is further used to infer news representation by the server.\nTo further safeguard user privacy, a secure aggregation algorithm based on\nShamir's secret sharing is employed. Experiments on a real-world news dataset\ndemonstrate strong performance compared to existing systems, representing a\nsignificant advancement in privacy-preserving personalized news recommendation.",
    "updated" : "2025-07-22T09:04:45Z",
    "published" : "2025-07-21T10:14:00Z",
    "authors" : [
      {
        "name" : "Mehdi Khalaj"
      },
      {
        "name" : "Shahrzad Golestani Najafabadi"
      },
      {
        "name" : "Julita Vassileva"
      }
    ],
    "categories" : [
      "cs.SI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.11324v2",
    "title" : "A Review of Privacy Metrics for Privacy-Preserving Synthetic Data\n  Generation",
    "summary" : "Privacy Preserving Synthetic Data Generation (PP-SDG) has emerged to produce\nsynthetic datasets from personal data while maintaining privacy and utility.\nDifferential privacy (DP) is the property of a PP-SDG mechanism that\nestablishes how protected individuals are when sharing their sensitive data. It\nis however difficult to interpret the privacy budget ($\\varepsilon$) expressed\nby DP. To make the actual risk associated with the privacy budget more\ntransparent, multiple privacy metrics (PMs) have been proposed to assess the\nprivacy risk of the data. These PMs are utilized in separate studies to assess\nnewly introduced PP-SDG mechanisms. Consequently, these PMs embody the same\nassumptions as the PP-SDG mechanism they were made to assess. Therefore, a\nthorough definition of how these are calculated is necessary. In this work, we\npresent the assumptions and mathematical formulations of 17 distinct privacy\nmetrics.",
    "updated" : "2025-07-22T09:17:56Z",
    "published" : "2025-07-15T13:56:02Z",
    "authors" : [
      {
        "name" : "Frederik Marinus Trudslev"
      },
      {
        "name" : "Matteo Lissandrini"
      },
      {
        "name" : "Juan Manuel Rodriguez"
      },
      {
        "name" : "Martin Bøgsted"
      },
      {
        "name" : "Daniele Dell'Aglio"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.17516v1",
    "title" : "Frequency Estimation of Correlated Multi-attribute Data under Local\n  Differential Privacy",
    "summary" : "Large-scale data collection, from national censuses to IoT-enabled smart\nhomes, routinely gathers dozens of attributes per individual. These\nmulti-attribute datasets are vital for analytics but pose significant privacy\nrisks. Local Differential Privacy (LDP) is a powerful tool to protect user data\nprivacy by allowing users to locally perturb their records before releasing to\nan untrusted data aggregator. However, existing LDP mechanisms either split the\nprivacy budget across all attributes or treat each attribute independently,\nignoring natural inter-attribute correlations. This leads to excessive noise or\nfragmented budgets, resulting in significant utility loss, particularly in\nhigh-dimensional settings.\n  To overcome these limitations, we propose Correlated Randomized Response\n(Corr-RR), a novel LDP mechanism that leverages correlations among attributes\nto substantially improve utility while maintaining rigorous LDP guarantees.\nCorr-RR allocates the full privacy budget to perturb a single, randomly\nselected attribute and reconstructs the remaining attributes using estimated\ninterattribute dependencies, without incurring additional privacy cost. To\nenable this, Corr-RR operates in two phases: (1) a subset of users apply\nstandard LDP mechanisms to estimate correlations, and (2) each remaining user\nperturbs one attribute and infers the others using the learned correlations. We\ntheoretically prove that Corr-RR satisfies $\\epsilon$-LDP, and extensive\nexperiments on synthetic and real-world datasets demonstrate that Corr-RR\nconsistently outperforms state-of-the-art LDP mechanisms, particularly in\nscenarios with many attributes and strong inter-attribute correlations.",
    "updated" : "2025-07-23T13:52:45Z",
    "published" : "2025-07-23T13:52:45Z",
    "authors" : [
      {
        "name" : "Shafizur Rahman Seeam"
      },
      {
        "name" : "Ye Zheng"
      },
      {
        "name" : "Yidan Hu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.17228v1",
    "title" : "P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous\n  Edge Devices",
    "summary" : "Split Learning (SL) is an emerging privacy-preserving machine learning\ntechnique that enables resource constrained edge devices to participate in\nmodel training by partitioning a model into client-side and server-side\nsub-models. While SL reduces computational overhead on edge devices, it\nencounters significant challenges in heterogeneous environments where devices\nvary in computing resources, communication capabilities, environmental\nconditions, and privacy requirements. Although recent studies have explored\nheterogeneous SL frameworks that optimize split points for devices with varying\nresource constraints, they often neglect personalized privacy requirements and\nlocal model customization under varying environmental conditions. To address\nthese limitations, we propose P3SL, a Personalized Privacy-Preserving Split\nLearning framework designed for heterogeneous, resource-constrained edge device\nsystems. The key contributions of this work are twofold. First, we design a\npersonalized sequential split learning pipeline that allows each client to\nachieve customized privacy protection and maintain personalized local models\ntailored to their computational resources, environmental conditions, and\nprivacy needs. Second, we adopt a bi-level optimization technique that empowers\nclients to determine their own optimal personalized split points without\nsharing private sensitive information (i.e., computational resources,\nenvironmental conditions, privacy requirements) with the server. This approach\nbalances energy consumption and privacy leakage risks while maintaining high\nmodel accuracy. We implement and evaluate P3SL on a testbed consisting of 7\ndevices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,\nusing diverse model architectures and datasets under varying environmental\nconditions.",
    "updated" : "2025-07-23T05:50:33Z",
    "published" : "2025-07-23T05:50:33Z",
    "authors" : [
      {
        "name" : "Wei Fan"
      },
      {
        "name" : "JinYi Yoon"
      },
      {
        "name" : "Xiaochang Li"
      },
      {
        "name" : "Huajie Shao"
      },
      {
        "name" : "Bo Ji"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.17199v1",
    "title" : "Threshold-Protected Searchable Sharing: Privacy Preserving\n  Aggregated-ANN Search for Collaborative RAG",
    "summary" : "LLM-powered search services have driven data integration as a significant\ntrend. However, this trend's progress is fundamentally hindered, despite the\nfact that combining individual knowledge can significantly improve the\nrelevance and quality of responses in specialized queries and make AI more\nprofessional at providing services. Two key bottlenecks are private data\nrepositories' locality constraints and the need to maintain compatibility with\nmainstream search techniques, particularly Hierarchical Navigable Small World\n(HNSW) indexing for high-dimensional vector spaces. In this work, we develop a\nsecure and privacy-preserving aggregated approximate nearest neighbor search\n(SP-A$^2$NN) with HNSW compatibility under a threshold-based searchable sharing\nprimitive. A sharable bitgraph structure is constructed and extended to support\nsearches and dynamical insertions over shared data without compromising the\nunderlying graph topology. The approach reduces the complexity of a search from\n$O(n^2)$ to $O(n)$ compared to naive (undirected) graph-sharing approach when\norganizing graphs in the identical HNSW manner.\n  On the theoretical front, we explore a novel security analytical framework\nthat incorporates privacy analysis via reductions. The proposed\nleakage-guessing proof system is built upon an entirely different interactive\ngame that is independent of existing coin-toss game design. Rather than being\npurely theoretical, this system is rooted in existing proof systems but goes\nbeyond them to specifically address leakage concerns and standardize leakage\nanalysis -- one of the most critical security challenges with AI's rapid\ndevelopment.",
    "updated" : "2025-07-23T04:45:01Z",
    "published" : "2025-07-23T04:45:01Z",
    "authors" : [
      {
        "name" : "Ruoyang Rykie Guo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.17180v1",
    "title" : "A Privacy-Preserving Data Collection Method for Diversified Statistical\n  Analysis",
    "summary" : "Data perturbation-based privacy-preserving methods have been widely adopted\nin various scenarios due to their efficiency and the elimination of the need\nfor a trusted third party. However, these methods primarily focus on individual\nstatistical indicators, neglecting the overall quality of the collected data\nfrom a distributional perspective. Consequently, they often fall short of\nmeeting the diverse statistical analysis requirements encountered in practical\ndata analysis. As a promising sensitive data perturbation method, negative\nsurvey methods is able to complete the task of collecting sensitive information\ndistribution while protecting personal privacy. Yet, existing negative survey\nmethods are primarily designed for discrete sensitive information and are\ninadequate for real-valued data distributions. To bridge this gap, this paper\nproposes a novel real-value negative survey model, termed RVNS, for the first\ntime in the field of real-value sensitive information collection. The RVNS\nmodel exempts users from the necessity of discretizing their data and only\nrequires them to sample a set of data from a range that deviates from their\nactual sensitive details, thereby preserving the privacy of their genuine\ninformation. Moreover, to accurately capture the distribution of sensitive\ninformation, an optimization problem is formulated, and a novel approach is\nemployed to solve it. Rigorous theoretical analysis demonstrates that the RVNS\nmodel conforms to the differential privacy model, ensuring robust privacy\npreservation. Comprehensive experiments conducted on both synthetic and\nreal-world datasets further validate the efficacy of the proposed method.",
    "updated" : "2025-07-23T04:05:33Z",
    "published" : "2025-07-23T04:05:33Z",
    "authors" : [
      {
        "name" : "Hao Jiang"
      },
      {
        "name" : "Quan Zhou"
      },
      {
        "name" : "Dongdong Zhao"
      },
      {
        "name" : "Shangshang Yang"
      },
      {
        "name" : "Wenjian Luo"
      },
      {
        "name" : "Xingyi Zhang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.17066v1",
    "title" : "Risk In Context: Benchmarking Privacy Leakage of Foundation Models in\n  Synthetic Tabular Data Generation",
    "summary" : "Synthetic tabular data is essential for machine learning workflows,\nespecially for expanding small or imbalanced datasets and enabling\nprivacy-preserving data sharing. However, state-of-the-art generative models\n(GANs, VAEs, diffusion models) rely on large datasets with thousands of\nexamples. In low-data settings, often the primary motivation for synthetic\ndata, these models can overfit, leak sensitive records, and require frequent\nretraining. Recent work uses large pre-trained transformers to generate rows\nvia in-context learning (ICL), which needs only a few seed examples and no\nparameter updates, avoiding retraining. But ICL repeats seed rows verbatim,\nintroducing a new privacy risk that has only been studied in text. The severity\nof this risk in tabular synthesis-where a single row may identify a\nperson-remains unclear. We address this gap with the first benchmark of three\nfoundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four\nbaselines on 35 real-world tables from health, finance, and policy. We evaluate\nstatistical fidelity, downstream utility, and membership inference leakage.\nResults show foundation models consistently have the highest privacy risk.\nLLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at\n1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly\nvulnerable. We plot the privacy-utility frontier and show that CTGAN and\nGPT-4o-mini offer better tradeoffs. A factorial study finds that three\nzero-cost prompt tweaks-small batch size, low temperature, and using summary\nstatistics-can reduce worst-case AUC by 14 points and rare-class leakage by up\nto 39 points while maintaining over 90% fidelity. Our benchmark offers a\npractical guide for safer low-data synthesis with foundation models.",
    "updated" : "2025-07-22T22:59:08Z",
    "published" : "2025-07-22T22:59:08Z",
    "authors" : [
      {
        "name" : "Jessup Byun"
      },
      {
        "name" : "Xiaofeng Lin"
      },
      {
        "name" : "Joshua Ward"
      },
      {
        "name" : "Guang Cheng"
      }
    ],
    "categories" : [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.16391v2",
    "title" : "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
    "summary" : "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
    "updated" : "2025-07-23T09:31:01Z",
    "published" : "2025-07-22T09:35:59Z",
    "authors" : [
      {
        "name" : "Chenqi Lin"
      },
      {
        "name" : "Kang Yang"
      },
      {
        "name" : "Tianshi Xu"
      },
      {
        "name" : "Ling Liang"
      },
      {
        "name" : "Yufei Wang"
      },
      {
        "name" : "Zhaohui Chen"
      },
      {
        "name" : "Runsheng Wang"
      },
      {
        "name" : "Mingyu Gao"
      },
      {
        "name" : "Meng Li"
      }
    ],
    "categories" : [
      "cs.AR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.16872v1",
    "title" : "CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage",
    "summary" : "Model compression is crucial for minimizing memory storage and accelerating\ninference in deep learning (DL) models, including recent foundation models like\nlarge language models (LLMs). Users can access different compressed model\nversions according to their resources and budget. However, while existing\ncompression operations primarily focus on optimizing the trade-off between\nresource efficiency and model performance, the privacy risks introduced by\ncompression remain overlooked and insufficiently understood.\n  In this work, through the lens of membership inference attack (MIA), we\npropose CompLeak, the first privacy risk evaluation framework examining three\nwidely used compression configurations that are pruning, quantization, and\nweight clustering supported by the commercial model compression framework of\nGoogle's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has\nthree variants, given available access to the number of compressed models and\noriginal model. CompLeakNR starts by adopting existing MIA methods to attack a\nsingle compressed model, and identifies that different compressed models\ninfluence members and non-members differently. When the original model and one\ncompressed model are available, CompLeakSR leverages the compressed model as a\nreference to the original model and uncovers more privacy by combining meta\ninformation (e.g., confidence vector) from both models. When multiple\ncompressed models are available with/without accessing the original model,\nCompLeakMR innovatively exploits privacy leakage info from multiple compressed\nversions to substantially signify the overall privacy leakage. We conduct\nextensive experiments on seven diverse model architectures (from ResNet to\nfoundation models of BERT and GPT-2), and six image and textual benchmark\ndatasets.",
    "updated" : "2025-07-22T08:02:46Z",
    "published" : "2025-07-22T08:02:46Z",
    "authors" : [
      {
        "name" : "Na Li"
      },
      {
        "name" : "Yansong Gao"
      },
      {
        "name" : "Hongsheng Hu"
      },
      {
        "name" : "Boyu Kuang"
      },
      {
        "name" : "Anmin Fu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.15460v3",
    "title" : "Privacy-Preserving Multimodal News Recommendation through Federated\n  Learning",
    "summary" : "Personalized News Recommendation systems (PNR) have emerged as a solution to\ninformation overload by predicting and suggesting news items tailored to\nindividual user interests. However, traditional PNR systems face several\nchallenges, including an overreliance on textual content, common neglect of\nshort-term user interests, and significant privacy concerns due to centralized\ndata storage. This paper addresses these issues by introducing a novel\nmultimodal federated learning-based approach for news recommendation. First, it\nintegrates both textual and visual features of news items using a multimodal\nmodel, enabling a more comprehensive representation of content. Second, it\nemploys a time-aware model that balances users' long-term and short-term\ninterests through multi-head self-attention networks, improving recommendation\naccuracy. Finally, to enhance privacy, a federated learning framework is\nimplemented, enabling collaborative model training without sharing user data.\nThe framework divides the recommendation model into a large server-maintained\nnews model and a lightweight user model shared between the server and clients.\nThe client requests news representations (vectors) and a user model from the\ncentral server, then computes gradients with user local data, and finally sends\ntheir locally computed gradients to the server for aggregation. The central\nserver aggregates gradients to update the global user model and news model. The\nupdated news model is further used to infer news representation by the server.\nTo further safeguard user privacy, a secure aggregation algorithm based on\nShamir's secret sharing is employed. Experiments on a real-world news dataset\ndemonstrate strong performance compared to existing systems, representing a\nsignificant advancement in privacy-preserving personalized news recommendation.",
    "updated" : "2025-07-23T03:40:18Z",
    "published" : "2025-07-21T10:14:00Z",
    "authors" : [
      {
        "name" : "Mehdi Khalaj"
      },
      {
        "name" : "Shahrzad Golestani Najafabadi"
      },
      {
        "name" : "Julita Vassileva"
      }
    ],
    "categories" : [
      "cs.SI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.18518v1",
    "title" : "Transform Before You Query: A Privacy-Preserving Approach for Vector\n  Retrieval with Embedding Space Alignment",
    "summary" : "Vector Database (VDB) can efficiently index and search high-dimensional\nvector embeddings from unstructured data, crucially enabling fast semantic\nsimilarity search essential for modern AI applications like generative AI and\nrecommendation systems. Since current VDB service providers predominantly use\nproprietary black-box models, users are forced to expose raw query text to them\nvia API in exchange for the vector retrieval services. Consequently, if query\ntext involves confidential records from finance or healthcare domains, this\nmechanism inevitably leads to critical leakage of user's sensitive information.\nTo address this issue, we introduce STEER (\\textbf{S}ecure \\textbf{T}ransformed\n\\textbf{E}mbedding v\\textbf{E}ctor\\textbf{ R}etrieval), a private vector\nretrieval framework that leverages the alignment relationship between the\nsemantic spaces of different embedding models to derive approximate embeddings\nfor the query text. STEER performs the retrieval using the approximate\nembeddings within the original VDB and requires no modifications to the server\nside. Our theoretical and experimental analyses demonstrate that STEER\neffectively safeguards query text privacy while maintaining the retrieval\naccuracy. Even though approximate embeddings are approximations of the\nembeddings from proprietary models, they still prevent the providers from\nrecovering the query text through Embedding Inversion Attacks (EIAs). Extensive\nexperimental results show that Recall@100 of STEER can basically achieve a\ndecrease of less than 5\\%. Furthermore, even when searching within a text\ncorpus of millions of entries, STEER achieves a Recall@20 accuracy 20\\% higher\nthan current baselines.",
    "updated" : "2025-07-24T15:41:34Z",
    "published" : "2025-07-24T15:41:34Z",
    "authors" : [
      {
        "name" : "Ruiqi He"
      },
      {
        "name" : "Zekun Fei"
      },
      {
        "name" : "Jiaqi Li"
      },
      {
        "name" : "Xinyuan Zhu"
      },
      {
        "name" : "Biao Yi"
      },
      {
        "name" : "Siyi Lv"
      },
      {
        "name" : "Weijie Liu"
      },
      {
        "name" : "Zheli Liu"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.18365v1",
    "title" : "RecPS: Privacy Risk Scoring for Recommender Systems",
    "summary" : "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning. Our code is\navailable at https://anonymous.4open.science/r/RsLiRA-4BD3/readme.md.",
    "updated" : "2025-07-24T12:46:30Z",
    "published" : "2025-07-24T12:46:30Z",
    "authors" : [
      {
        "name" : "Jiajie He"
      },
      {
        "name" : "Yuechun Gu"
      },
      {
        "name" : "Keke Chen"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.18253v1",
    "title" : "Countering Privacy Nihilism",
    "summary" : "Of growing concern in privacy scholarship is artificial intelligence (AI), as\na powerful producer of inferences. Taken to its limits, AI may be presumed\ncapable of inferring \"everything from everything,\" thereby making untenable any\nnormative scheme, including privacy theory and privacy regulation, which rests\non protecting privacy based on categories of data - sensitive versus\nnon-sensitive, private versus public. Discarding data categories as a normative\nanchoring in privacy and data protection as a result of an unconditional\nacceptance of AI's inferential capacities is what we call privacy nihilism. An\nethically reasoned response to AI inferences requires a sober consideration of\nAI capabilities rather than issuing an epistemic carte blanche. We introduce\nthe notion of conceptual overfitting to expose how privacy nihilism turns a\nblind eye toward flawed epistemic practices in AI development. Conceptual\noverfitting refers to the adoption of norms of convenience that simplify the\ndevelopment of AI models by forcing complex constructs to fit data that are\nconceptually under-representative or even irrelevant. While conceptual\noverfitting serves as a helpful device to counter normative suggestions\ngrounded in hyperbolic AI capability claims, AI inferences shake any privacy\nregulation that hinges protections based on restrictions around data\ncategories. We propose moving away from privacy frameworks that focus solely on\ndata type, neglecting all other factors. Theories like contextual integrity\nevaluate the normative value of privacy across several parameters, including\nthe type of data, the actors involved in sharing it, and the purposes for which\nthe information is used.",
    "updated" : "2025-07-24T09:52:18Z",
    "published" : "2025-07-24T09:52:18Z",
    "authors" : [
      {
        "name" : "Severin Engelmann"
      },
      {
        "name" : "Helen Nissenbaum"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.18072v1",
    "title" : "C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving\n  Activity Recognition in Healthcare Sensor Streams",
    "summary" : "Wearable accelerometers and gyroscopes encode fine-grained behavioural\nsignatures that can be exploited to re-identify users, making privacy\nprotection essential for healthcare applications. We introduce C-AAE, a\ncompressive anonymizing autoencoder that marries an Anonymizing AutoEncoder\n(AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first\nprojects raw sensor windows into a latent space that retains activity-relevant\nfeatures while suppressing identity cues. ADPCM then differentially encodes\nthis latent stream, further masking residual identity information and shrinking\nthe bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE\ncuts user re-identification F1 scores by 10-15 percentage points relative to\nAAE alone, while keeping activity-recognition F1 within 5 percentage points of\nthe unprotected baseline. ADPCM also reduces data volume by roughly 75 %,\neasing transmission and storage overheads. These results demonstrate that C-AAE\noffers a practical route to balancing privacy and utility in continuous,\nsensor-based activity recognition for healthcare.",
    "updated" : "2025-07-24T03:55:04Z",
    "published" : "2025-07-24T03:55:04Z",
    "authors" : [
      {
        "name" : "Ryusei Fujimoto"
      },
      {
        "name" : "Yugo Nakamura"
      },
      {
        "name" : "Yutaka Arakawa"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.18055v1",
    "title" : "Privacy-Preserving Synthetic Review Generation with Diverse Writing\n  Styles Using LLMs",
    "summary" : "The increasing use of synthetic data generated by Large Language Models\n(LLMs) presents both opportunities and challenges in data-driven applications.\nWhile synthetic data provides a cost-effective, scalable alternative to\nreal-world data to facilitate model training, its diversity and privacy risks\nremain underexplored. Focusing on text-based synthetic data, we propose a\ncomprehensive set of metrics to quantitatively assess the diversity (i.e.,\nlinguistic expression, sentiment, and user perspective), and privacy (i.e.,\nre-identification risk and stylistic outliers) of synthetic datasets generated\nby several state-of-the-art LLMs. Experiment results reveal significant\nlimitations in LLMs' capabilities in generating diverse and privacy-preserving\nsynthetic data. Guided by the evaluation results, a prompt-based approach is\nproposed to enhance the diversity of synthetic reviews while preserving\nreviewer privacy.",
    "updated" : "2025-07-24T03:12:16Z",
    "published" : "2025-07-24T03:12:16Z",
    "authors" : [
      {
        "name" : "Tevin Atwal"
      },
      {
        "name" : "Chan Nam Tieu"
      },
      {
        "name" : "Yefeng Yuan"
      },
      {
        "name" : "Zhan Shi"
      },
      {
        "name" : "Yuhong Liu"
      },
      {
        "name" : "Liang Cheng"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.19116v1",
    "title" : "Graph Structure Learning with Privacy Guarantees for Open Graph Data",
    "summary" : "Ensuring privacy in large-scale open datasets is increasingly challenging\nunder regulations such as the General Data Protection Regulation (GDPR). While\ndifferential privacy (DP) provides strong theoretical guarantees, it primarily\nfocuses on noise injection during model training, neglecting privacy\npreservation at the data publishing stage. Existing privacy-preserving data\npublishing (PPDP) approaches struggle to balance privacy and utility,\nparticularly when data publishers and users are distinct entities. To address\nthis gap, we focus on the graph recovery problem and propose a novel\nprivacy-preserving estimation framework for open graph data, leveraging\nGaussian DP (GDP) with a structured noise-injection mechanism. Unlike\ntraditional methods that perturb gradients or model updates, our approach\nensures unbiased graph structure recovery while enforcing DP at the data\npublishing stage. Moreover, we provide theoretical guarantees on estimation\naccuracy and extend our method to discrete-variable graphs, a setting often\noverlooked in DP research. Experimental results in graph learning demonstrate\nrobust performance, offering a viable solution for privacy-conscious graph\nanalysis.",
    "updated" : "2025-07-25T09:51:12Z",
    "published" : "2025-07-25T09:51:12Z",
    "authors" : [
      {
        "name" : "Muhao Guo"
      },
      {
        "name" : "Jiaqi Wu"
      },
      {
        "name" : "Yang Weng"
      },
      {
        "name" : "Yizheng Liao"
      },
      {
        "name" : "Shengzhe Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.20688v1",
    "title" : "Guard-GBDT: Efficient Privacy-Preserving Approximated GBDT Training on\n  Vertical Dataset",
    "summary" : "In light of increasing privacy concerns and stringent legal regulations,\nusing secure multiparty computation (MPC) to enable collaborative GBDT model\ntraining among multiple data owners has garnered significant attention. Despite\nthis, existing MPC-based GBDT frameworks face efficiency challenges due to high\ncommunication costs and the computation burden of non-linear operations, such\nas division and sigmoid calculations. In this work, we introduce Guard-GBDT, an\ninnovative framework tailored for efficient and privacy-preserving GBDT\ntraining on vertical datasets. Guard-GBDT bypasses MPC-unfriendly division and\nsigmoid functions by using more streamlined approximations and reduces\ncommunication overhead by compressing the messages exchanged during gradient\naggregation. We implement a prototype of Guard-GBDT and extensively evaluate\nits performance and accuracy on various real-world datasets. The results show\nthat Guard-GBDT outperforms state-of-the-art HEP-XGB (CIKM'21) and SiGBDT (ASIA\nCCS'24) by up to $2.71\\times$ and $12.21 \\times$ on LAN network and up to\n$2.7\\times$ and $8.2\\times$ on WAN network. Guard-GBDT also achieves comparable\naccuracy with SiGBDT and plaintext XGBoost (better than HEP-XGB ), which\nexhibits a deviation of $\\pm1\\%$ to $\\pm2\\%$ only. Our implementation code is\nprovided at https://github.com/XidianNSS/Guard-GBDT.git.",
    "updated" : "2025-07-28T10:16:37Z",
    "published" : "2025-07-28T10:16:37Z",
    "authors" : [
      {
        "name" : "Anxiao Song"
      },
      {
        "name" : "Shujie Cui"
      },
      {
        "name" : "Jianli Bai"
      },
      {
        "name" : "Ke Cheng"
      },
      {
        "name" : "Yulong Shen"
      },
      {
        "name" : "Giovanni Russello"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.20573v1",
    "title" : "Reminiscence Attack on Residuals: Exploiting Approximate Machine\n  Unlearning for Privacy",
    "summary" : "Machine unlearning enables the removal of specific data from ML models to\nuphold the right to be forgotten. While approximate unlearning algorithms offer\nefficient alternatives to full retraining, this work reveals that they fail to\nadequately protect the privacy of unlearned data. In particular, these\nalgorithms introduce implicit residuals which facilitate privacy attacks\ntargeting at unlearned data. We observe that these residuals persist regardless\nof model architectures, parameters, and unlearning algorithms, exposing a new\nattack surface beyond conventional output-based leakage. Based on this insight,\nwe propose the Reminiscence Attack (ReA), which amplifies the correlation\nbetween residuals and membership privacy through targeted fine-tuning\nprocesses. ReA achieves up to 1.90x and 1.12x higher accuracy than prior\nattacks when inferring class-wise and sample-wise membership, respectively. To\nmitigate such residual-induced privacy risk, we develop a dual-phase\napproximate unlearning framework that first eliminates deep-layer unlearned\ndata traces and then enforces convergence stability to prevent models from\n\"pseudo-convergence\", where their outputs are similar to retrained models but\nstill preserve unlearned residuals. Our framework works for both classification\nand generation tasks. Experimental evaluations confirm that our approach\nmaintains high unlearning efficacy, while reducing the adaptive privacy attack\naccuracy to nearly random guess, at the computational cost of 2-12% of full\nretraining from scratch.",
    "updated" : "2025-07-28T07:12:12Z",
    "published" : "2025-07-28T07:12:12Z",
    "authors" : [
      {
        "name" : "Yaxin Xiao"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Li Hu"
      },
      {
        "name" : "Huadi Zheng"
      },
      {
        "name" : "Haibo Hu"
      },
      {
        "name" : "Zi Liang"
      },
      {
        "name" : "Haoyang Li"
      },
      {
        "name" : "Yijie Jiao"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.20557v1",
    "title" : "FED-PsyAU: Privacy-Preserving Micro-Expression Recognition via\n  Psychological AU Coordination and Dynamic Facial Motion Modeling",
    "summary" : "Micro-expressions (MEs) are brief, low-intensity, often localized facial\nexpressions. They could reveal genuine emotions individuals may attempt to\nconceal, valuable in contexts like criminal interrogation and psychological\ncounseling. However, ME recognition (MER) faces challenges, such as small\nsample sizes and subtle features, which hinder efficient modeling.\nAdditionally, real-world applications encounter ME data privacy issues, leaving\nthe task of enhancing recognition across settings under privacy constraints\nlargely unexplored. To address these issues, we propose a FED-PsyAU research\nframework. We begin with a psychological study on the coordination of upper and\nlower facial action units (AUs) to provide structured prior knowledge of facial\nmuscle dynamics. We then develop a DPK-GAT network that combines these\npsychological priors with statistical AU patterns, enabling hierarchical\nlearning of facial motion features from regional to global levels, effectively\nenhancing MER performance. Additionally, our federated learning framework\nadvances MER capabilities across multiple clients without data sharing,\npreserving privacy and alleviating the limited-sample issue for each client.\nExtensive experiments on commonly-used ME databases demonstrate the\neffectiveness of our approach.",
    "updated" : "2025-07-28T06:42:15Z",
    "published" : "2025-07-28T06:42:15Z",
    "authors" : [
      {
        "name" : "Jingting Li"
      },
      {
        "name" : "Yu Qian"
      },
      {
        "name" : "Lin Zhao"
      },
      {
        "name" : "Su-Jing Wang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.20537v1",
    "title" : "Next-Generation Quantum Neural Networks: Enhancing Efficiency, Security,\n  and Privacy",
    "summary" : "This paper provides an integrated perspective on addressing key challenges in\ndeveloping reliable and secure Quantum Neural Networks (QNNs) in the Noisy\nIntermediate-Scale Quantum (NISQ) era. In this paper, we present an integrated\nframework that leverages and combines existing approaches to enhance QNN\nefficiency, security, and privacy. Specifically, established optimization\nstrategies, including efficient parameter initialization, residual quantum\ncircuit connections, and systematic quantum architecture exploration, are\nintegrated to mitigate issues such as barren plateaus and error propagation.\nMoreover, the methodology incorporates current defensive mechanisms against\nadversarial attacks. Finally, Quantum Federated Learning (QFL) is adopted\nwithin this framework to facilitate privacy-preserving collaborative training\nacross distributed quantum systems. Collectively, this synthesized approach\nseeks to enhance the robustness and real-world applicability of QNNs, laying\nthe foundation for reliable quantum-enhanced machine learning applications in\nfinance, healthcare, and cybersecurity.",
    "updated" : "2025-07-28T05:43:02Z",
    "published" : "2025-07-28T05:43:02Z",
    "authors" : [
      {
        "name" : "Nouhaila Innan"
      },
      {
        "name" : "Muhammad Kashif"
      },
      {
        "name" : "Alberto Marchisio"
      },
      {
        "name" : "Mohamed Bennai"
      },
      {
        "name" : "Muhammad Shafique"
      }
    ],
    "categories" : [
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.20060v1",
    "title" : "ModShift: Model Privacy via Designed Shifts",
    "summary" : "In this paper, shifts are introduced to preserve model privacy against an\neavesdropper in federated learning. Model learning is treated as a parameter\nestimation problem. This perspective allows us to derive the Fisher Information\nmatrix of the model updates from the shifted updates and drive them to\nsingularity, thus posing a hard estimation problem for Eve. The shifts are\nsecurely shared with the central server to maintain model accuracy at the\nserver and participating devices. A convergence test is proposed to detect if\nmodel updates have been tampered with and we show that our scheme passes this\ntest. Numerical results show that our scheme achieves a higher model shift when\ncompared to a noise injection scheme while requiring a lesser bandwidth secret\nchannel.",
    "updated" : "2025-07-26T21:00:56Z",
    "published" : "2025-07-26T21:00:56Z",
    "authors" : [
      {
        "name" : "Nomaan A. Kherani"
      },
      {
        "name" : "Urbashi Mitra"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.21904v1",
    "title" : "Privacy-Preserving Anonymization of System and Network Event Logs Using\n  Salt-Based Hashing and Temporal Noise",
    "summary" : "System and network event logs are essential for security analytics, threat\ndetection, and operational monitoring. However, these logs often contain\nPersonally Identifiable Information (PII), raising significant privacy concerns\nwhen shared or analyzed. A key challenge in log anonymization is balancing\nprivacy protection with the retention of sufficient structure for meaningful\nanalysis. Overly aggressive anonymization can destroy contextual integrity,\nwhile weak techniques risk re-identification through linkage or inference\nattacks. This paper introduces novel field-specific anonymization methods that\naddress this trade-off. For IP addresses, we propose a salt-based hashing\ntechnique applied at the per-octet level, preserving both subnet and host\nstructure to enable correlation across various log entries while ensuring\nnon-reversibility. For port numbers, full-value hashing with range mapping\nmaintains interpretability. We also present an order-preserving timestamp\nanonymization scheme using adaptive noise injection, which obfuscates exact\ntimes without disrupting event sequences. An open-source tool implementing\nthese techniques has been released to support practical deployment and\nreproducible research. Evaluations using entropy metrics, collision rates, and\nresidual leakage analysis demonstrate that the proposed approach effectively\nprotects privacy while preserving analytical utility.",
    "updated" : "2025-07-29T15:16:42Z",
    "published" : "2025-07-29T15:16:42Z",
    "authors" : [
      {
        "name" : "Shreyas Bargale"
      },
      {
        "name" : "Akshit Vakati Venkata"
      },
      {
        "name" : "Jaimandeep Singh"
      },
      {
        "name" : "Chester Rebeiro"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.21769v1",
    "title" : "Factorization by extremal privacy mechanisms: new insights into\n  efficiency",
    "summary" : "We study the problem of efficiency under $\\alpha$ local differential privacy\n($\\alpha$ LDP) in both discrete and continuous settings. Building on a\nfactorization lemma, which shows that any privacy mechanism can be decomposed\ninto an extremal mechanism followed by additional randomization, we reduce the\nFisher information maximization problem to a search over extremal mechanisms.\nThe representation of extremal mechanisms requires working in infinite\ndimensional spaces and invokes advanced tools from convex and functional\nanalysis, such as Choquet's theorem. Our analysis establishes matching upper\nand lower bounds on the Fisher information in the high privacy regime ($\\alpha\n\\to 0$), and proves that the maximization problem always admits a solution for\nany $\\alpha$. As a concrete application, we consider the problem of estimating\nthe parameter of a uniform distribution on $[0, \\theta]$ under $\\alpha$ LDP.\nGuided by our theoretical findings, we design an extremal mechanism that yields\na consistent and asymptotically efficient estimator in high privacy regime.\nNumerical experiments confirm our theoretical results.",
    "updated" : "2025-07-29T12:52:09Z",
    "published" : "2025-07-29T12:52:09Z",
    "authors" : [
      {
        "name" : "Chiara Amorino"
      },
      {
        "name" : "Arnaud Gloter"
      }
    ],
    "categories" : [
      "math.ST",
      "math.PR",
      "stat.TH",
      "2020 subject classifications: Primary 62F12, 68P27, secondary 62B15,\n  46A55"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.18365v2",
    "title" : "RecPS: Privacy Risk Scoring for Recommender Systems",
    "summary" : "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning.",
    "updated" : "2025-07-29T12:22:41Z",
    "published" : "2025-07-24T12:46:30Z",
    "authors" : [
      {
        "name" : "Jiajie He"
      },
      {
        "name" : "Yuechun Gu"
      },
      {
        "name" : "Keke Chen"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.21142v1",
    "title" : "Privacy Artifact ConnecTor (PACT): Embedding Enterprise Artifacts for\n  Compliance AI Agents",
    "summary" : "Enterprise environments contain a heterogeneous, rapidly growing collection\nof internal artifacts related to code, data, and many different tools. Critical\ninformation for assessing privacy risk and ensuring regulatory compliance is\noften embedded across these varied resources, each with their own arcane\ndiscovery and extraction techniques. Therefore, large-scale privacy compliance\nin adherence to governmental regulations requires systems to discern the\ninterconnected nature of diverse artifacts in a common, shared universe.\n  We present Privacy Artifact ConnecT or (PACT), an embeddings-driven graph\nthat links millions of artifacts spanning multiple artifact types generated by\na variety of teams and projects. Powered by the state-of-the-art DRAGON\nembedding model, PACT uses a contrastive learning objective with light\nfine-tuning to link artifacts via their textual components such as raw\nmetadata, ownership specifics, and compliance context. Experimental results\nshow that PACT's fine-tuned model improves recall@1 from 18% to 53%, the query\nmatch rate from 9.6% to 69.7% when paired with a baseline AI agent, and the\nhitrate@1 from 25.7% to 44.9% for candidate selection in a standard recommender\nsystem.",
    "updated" : "2025-07-23T08:00:20Z",
    "published" : "2025-07-23T08:00:20Z",
    "authors" : [
      {
        "name" : "Chenhao Fang"
      },
      {
        "name" : "Yanqing Peng"
      },
      {
        "name" : "Rajeev Rao"
      },
      {
        "name" : "Matt Sarmiento"
      },
      {
        "name" : "Wendy Summer"
      },
      {
        "name" : "Arya Pudota"
      },
      {
        "name" : "Alex Goncalves"
      },
      {
        "name" : "Jordi Mola"
      },
      {
        "name" : "Hervé Robert"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.21139v1",
    "title" : "Learning-based Privacy-Preserving Graph Publishing Against Sensitive\n  Link Inference Attacks",
    "summary" : "Publishing graph data is widely desired to enable a variety of structural\nanalyses and downstream tasks. However, it also potentially poses severe\nprivacy leakage, as attackers may leverage the released graph data to launch\nattacks and precisely infer private information such as the existence of hidden\nsensitive links in the graph. Prior studies on privacy-preserving graph data\npublishing relied on heuristic graph modification strategies and it is\ndifficult to determine the graph with the optimal privacy--utility trade-off\nfor publishing. In contrast, we propose the first privacy-preserving graph\nstructure learning framework against sensitive link inference attacks, named\nPPGSL, which can automatically learn a graph with the optimal privacy--utility\ntrade-off. The PPGSL operates by first simulating a powerful surrogate attacker\nconducting sensitive link attacks on a given graph. It then trains a\nparameterized graph to defend against the simulated adversarial attacks while\nmaintaining the favorable utility of the original graph. To learn the\nparameters of both parts of the PPGSL, we introduce a secure iterative training\nprotocol. It can enhance privacy preservation and ensure stable convergence\nduring the training process, as supported by the theoretical proof.\nAdditionally, we incorporate multiple acceleration techniques to improve the\nefficiency of the PPGSL in handling large-scale graphs. The experimental\nresults confirm that the PPGSL achieves state-of-the-art privacy--utility\ntrade-off performance and effectively thwarts various sensitive link inference\nattacks.",
    "updated" : "2025-07-23T04:19:29Z",
    "published" : "2025-07-23T04:19:29Z",
    "authors" : [
      {
        "name" : "Yucheng Wu"
      },
      {
        "name" : "Yuncong Yang"
      },
      {
        "name" : "Xiao Han"
      },
      {
        "name" : "Leye Wang"
      },
      {
        "name" : "Junjie Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.22534v1",
    "title" : "The Risks and Detection of Overestimated Privacy Protection in Voice\n  Anonymisation",
    "summary" : "Voice anonymisation aims to conceal the voice identity of speakers in speech\nrecordings. Privacy protection is usually estimated from the difficulty of\nusing a speaker verification system to re-identify the speaker\npost-anonymisation. Performance assessments are therefore dependent on the\nverification model as well as the anonymisation system. There is hence\npotential for privacy protection to be overestimated when the verification\nsystem is poorly trained, perhaps with mismatched data. In this paper, we\ndemonstrate the insidious risk of overestimating anonymisation performance and\nshow examples of exaggerated performance reported in the literature. For the\nworst case we identified, performance is overestimated by 74% relative. We then\nintroduce a means to detect when performance assessment might be untrustworthy\nand show that it can identify all overestimation scenarios presented in the\npaper. Our solution is openly available as a fork of the 2024 VoicePrivacy\nChallenge evaluation toolkit.",
    "updated" : "2025-07-30T10:02:17Z",
    "published" : "2025-07-30T10:02:17Z",
    "authors" : [
      {
        "name" : "Michele Panariello"
      },
      {
        "name" : "Sarina Meyer"
      },
      {
        "name" : "Pierre Champion"
      },
      {
        "name" : "Xiaoxiao Miao"
      },
      {
        "name" : "Massimiliano Todisco"
      },
      {
        "name" : "Ngoc Thang Vu"
      },
      {
        "name" : "Nicholas Evans"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.22208v1",
    "title" : "Quantum-Inspired Audio Unlearning: Towards Privacy-Preserving Voice\n  Biometrics",
    "summary" : "The widespread adoption of voice-enabled authentication and audio biometric\nsystems have significantly increased privacy vulnerabilities associated with\nsensitive speech data. Compliance with privacy regulations such as GDPR's right\nto be forgotten and India's DPDP Act necessitates targeted and efficient\nerasure of individual-specific voice signatures from already-trained biometric\nmodels. Existing unlearning methods designed for visual data inadequately\nhandle the sequential, temporal, and high-dimensional nature of audio signals,\nleading to ineffective or incomplete speaker and accent erasure. To address\nthis, we introduce QPAudioEraser, a quantum-inspired audio unlearning\nframework. Our our-phase approach involves: (1) weight initialization using\ndestructive interference to nullify target features, (2) superposition-based\nlabel transformations that obscure class identity, (3) an\nuncertainty-maximizing quantum loss function, and (4) entanglement-inspired\nmixing of correlated weights to retain model knowledge. Comprehensive\nevaluations with ResNet18, ViT, and CNN architectures across AudioMNIST, Speech\nCommands, LibriSpeech, and Speech Accent Archive datasets validate\nQPAudioEraser's superior performance. The framework achieves complete erasure\nof target data (0% Forget Accuracy) while incurring minimal impact on model\nutility, with a performance degradation on retained data as low as 0.05%.\nQPAudioEraser consistently surpasses conventional baselines across\nsingle-class, multi-class, sequential, and accent-level erasure scenarios,\nestablishing the proposed approach as a robust privacy-preserving solution.",
    "updated" : "2025-07-29T20:12:24Z",
    "published" : "2025-07-29T20:12:24Z",
    "authors" : [
      {
        "name" : "Shreyansh Pathak"
      },
      {
        "name" : "Sonu Shreshtha"
      },
      {
        "name" : "Richa Singh"
      },
      {
        "name" : "Mayank Vatsa"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.22153v1",
    "title" : "Towards Privacy-preserving Photorealistic Self-avatars in Mixed Reality",
    "summary" : "Photorealistic 3D avatar generation has rapidly improved in recent years, and\nrealistic avatars that match a user's true appearance are more feasible in\nMixed Reality (MR) than ever before. Yet, there are known risks to sharing\none's likeness online, and photorealistic MR avatars could exacerbate these\nrisks. If user likenesses were to be shared broadly, there are risks for cyber\nabuse or targeted fraud based on user appearances. We propose an alternate\navatar rendering scheme for broader social MR -- synthesizing realistic avatars\nthat preserve a user's demographic identity while being distinct enough from\nthe individual user to protect facial biometric information. We introduce a\nmethodology for privatizing appearance by isolating identity within the feature\nspace of identity-encoding generative models. We develop two algorithms that\nthen obfuscate identity: \\epsmethod{} provides differential privacy guarantees\nand \\thetamethod{} provides fine-grained control for the level of identity\noffset. These methods are shown to successfully generate de-identified virtual\navatars across multiple generative architectures in 2D and 3D. With these\ntechniques, it is possible to protect user privacy while largely preserving\nattributes related to sense of self. Employing these techniques in public\nsettings could enable the use of photorealistic avatars broadly in MR,\nmaintaining high realism and immersion without privacy risk.",
    "updated" : "2025-07-29T18:37:24Z",
    "published" : "2025-07-29T18:37:24Z",
    "authors" : [
      {
        "name" : "Ethan Wilson"
      },
      {
        "name" : "Vincent Bindschaedler"
      },
      {
        "name" : "Sophie Jörg"
      },
      {
        "name" : "Sean Sheikholeslam"
      },
      {
        "name" : "Kevin Butler"
      },
      {
        "name" : "Eakta Jain"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.18365v2",
    "title" : "RecPS: Privacy Risk Scoring for Recommender Systems",
    "summary" : "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning.",
    "updated" : "2025-07-29T12:22:41Z",
    "published" : "2025-07-24T12:46:30Z",
    "authors" : [
      {
        "name" : "Jiajie He"
      },
      {
        "name" : "Yuechun Gu"
      },
      {
        "name" : "Keke Chen"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.23569v1",
    "title" : "Gaussian Splatting Feature Fields for Privacy-Preserving Visual\n  Localization",
    "summary" : "Visual localization is the task of estimating a camera pose in a known\nenvironment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based\nrepresentations for accurate and privacy-preserving visual localization. We\npropose Gaussian Splatting Feature Fields (GSFFs), a scene representation for\nvisual localization that combines an explicit geometry model (3DGS) with an\nimplicit feature field. We leverage the dense geometric information and\ndifferentiable rasterization algorithm from 3DGS to learn robust feature\nrepresentations grounded in 3D. In particular, we align a 3D scale-aware\nfeature field and a 2D feature encoder in a common embedding space through a\ncontrastive framework. Using a 3D structure-informed clustering procedure, we\nfurther regularize the representation learning and seamlessly convert the\nfeatures to segmentations, which can be used for privacy-preserving visual\nlocalization. Pose refinement, which involves aligning either feature maps or\nsegmentations from a query image with those rendered from the GSFFs scene\nrepresentation, is used to achieve localization. The resulting privacy- and\nnon-privacy-preserving localization pipelines, evaluated on multiple real-world\ndatasets, show state-of-the-art performances.",
    "updated" : "2025-07-31T13:58:15Z",
    "published" : "2025-07-31T13:58:15Z",
    "authors" : [
      {
        "name" : "Maxime Pietrantoni"
      },
      {
        "name" : "Gabriela Csurka"
      },
      {
        "name" : "Torsten Sattler"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.23432v1",
    "title" : "Scalable contribution bounding to achieve privacy",
    "summary" : "In modern datasets, where single records can have multiple owners, enforcing\nuser-level differential privacy requires capping each user's total\ncontribution. This \"contribution bounding\" becomes a significant combinatorial\nchallenge. Existing sequential algorithms for this task are computationally\nintensive and do not scale to the massive datasets prevalent today. To address\nthis scalability bottleneck, we propose a novel and efficient distributed\nalgorithm. Our approach models the complex ownership structure as a hypergraph,\nwhere users are vertices and records are hyperedges. The algorithm proceeds in\nrounds, allowing users to propose records in parallel. A record is added to the\nfinal dataset only if all its owners unanimously agree, thereby ensuring that\nno user's predefined contribution limit is violated. This method aims to\nmaximize the size of the resulting dataset for high utility while providing a\npractical, scalable solution for implementing user-level privacy in large,\nreal-world systems.",
    "updated" : "2025-07-31T11:14:17Z",
    "published" : "2025-07-31T11:14:17Z",
    "authors" : [
      {
        "name" : "Vincent Cohen-Addad"
      },
      {
        "name" : "Alessandro Epasto"
      },
      {
        "name" : "Jason Lee"
      },
      {
        "name" : "Morteza Zadimoghaddam"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.23291v1",
    "title" : "Evaluating the Dynamics of Membership Privacy in Deep Learning",
    "summary" : "Membership inference attacks (MIAs) pose a critical threat to the privacy of\ntraining data in deep learning. Despite significant progress in attack\nmethodologies, our understanding of when and how models encode membership\ninformation during training remains limited. This paper presents a dynamic\nanalytical framework for dissecting and quantifying privacy leakage dynamics at\nthe individual sample level. By tracking per-sample vulnerabilities on an\nFPR-TPR plane throughout training, our framework systematically measures how\nfactors such as dataset complexity, model architecture, and optimizer choice\ninfluence the rate and severity at which samples become vulnerable. Crucially,\nwe discover a robust correlation between a sample's intrinsic learning\ndifficulty, and find that the privacy risk of samples highly vulnerable in the\nfinal trained model is largely determined early during training. Our results\nthus provide a deeper understanding of how privacy risks dynamically emerge\nduring training, laying the groundwork for proactive, privacy-aware model\ntraining strategies.",
    "updated" : "2025-07-31T07:09:52Z",
    "published" : "2025-07-31T07:09:52Z",
    "authors" : [
      {
        "name" : "Yuetian Chen"
      },
      {
        "name" : "Zhiqi Wang"
      },
      {
        "name" : "Nathalie Baracaldo"
      },
      {
        "name" : "Swanand Ravindra Kadhe"
      },
      {
        "name" : "Lei Yu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.23229v1",
    "title" : "Fine-Grained Privacy Extraction from Retrieval-Augmented Generation\n  Systems via Knowledge Asymmetry Exploitation",
    "summary" : "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge bases, but this advancement introduces\nsignificant privacy risks. Existing privacy attacks on RAG systems can trigger\ndata leakage but often fail to accurately isolate knowledge-base-derived\nsentences within mixed responses. They also lack robustness when applied across\nmultiple domains. This paper addresses these challenges by presenting a novel\nblack-box attack framework that exploits knowledge asymmetry between RAG and\nstandard LLMs to achieve fine-grained privacy extraction across heterogeneous\nknowledge landscapes. We propose a chain-of-thought reasoning strategy that\ncreates adaptive prompts to steer RAG systems away from sensitive content.\nSpecifically, we first decompose adversarial queries to maximize information\ndisparity and then apply a semantic relationship scoring to resolve lexical and\nsyntactic ambiguities. We finally train a neural network on these feature\nscores to precisely identify sentences containing private information. Unlike\nprior work, our framework generalizes to unseen domains through iterative\nrefinement without pre-defined knowledge. Experimental results show that we\nachieve over 91% privacy extraction rate in single-domain and 83% in\nmulti-domain scenarios, reducing sensitive sentence exposure by over 65% in\ncase studies. This work bridges the gap between attack and defense in RAG\nsystems, enabling precise extraction of private information while providing a\nfoundation for adaptive mitigation.",
    "updated" : "2025-07-31T03:50:16Z",
    "published" : "2025-07-31T03:50:16Z",
    "authors" : [
      {
        "name" : "Yufei Chen"
      },
      {
        "name" : "Yao Wang"
      },
      {
        "name" : "Haibin Zhang"
      },
      {
        "name" : "Tao Gu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.18518v2",
    "title" : "Transform Before You Query: A Privacy-Preserving Approach for Vector\n  Retrieval with Embedding Space Alignment",
    "summary" : "Vector Database (VDB) can efficiently index and search high-dimensional\nvector embeddings from unstructured data, crucially enabling fast semantic\nsimilarity search essential for modern AI applications like generative AI and\nrecommendation systems. Since current VDB service providers predominantly use\nproprietary black-box models, users are forced to expose raw query text to them\nvia API in exchange for the vector retrieval services. Consequently, if query\ntext involves confidential records from finance or healthcare domains, this\nmechanism inevitably leads to critical leakage of user's sensitive information.\nTo address this issue, we introduce STEER (\\textbf{S}ecure \\textbf{T}ransformed\n\\textbf{E}mbedding v\\textbf{E}ctor\\textbf{ R}etrieval), a private vector\nretrieval framework that leverages the alignment relationship between the\nsemantic spaces of different embedding models to derive approximate embeddings\nfor the query text. STEER performs the retrieval using the approximate\nembeddings within the original VDB and requires no modifications to the server\nside. Our theoretical and experimental analyses demonstrate that STEER\neffectively safeguards query text privacy while maintaining the retrieval\naccuracy. Even though approximate embeddings are approximations of the\nembeddings from proprietary models, they still prevent the providers from\nrecovering the query text through Embedding Inversion Attacks (EIAs). Extensive\nexperimental results show that Recall@100 of STEER can basically achieve a\ndecrease of less than 5\\%. Furthermore, even when searching within a text\ncorpus of millions of entries, STEER achieves a Recall@20 accuracy 20\\% higher\nthan current baselines.",
    "updated" : "2025-07-31T06:47:49Z",
    "published" : "2025-07-24T15:41:34Z",
    "authors" : [
      {
        "name" : "Ruiqi He"
      },
      {
        "name" : "Zekun Fei"
      },
      {
        "name" : "Jiaqi Li"
      },
      {
        "name" : "Xinyuan Zhu"
      },
      {
        "name" : "Biao Yi"
      },
      {
        "name" : "Siyi Lv"
      },
      {
        "name" : "Weijie Liu"
      },
      {
        "name" : "Zheli Liu"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.22908v1",
    "title" : "A Privacy-Preserving Federated Framework with Hybrid Quantum-Enhanced\n  Learning for Financial Fraud Detection",
    "summary" : "Rapid growth of digital transactions has led to a surge in fraudulent\nactivities, challenging traditional detection methods in the financial sector.\nTo tackle this problem, we introduce a specialised federated learning framework\nthat uniquely combines a quantum-enhanced Long Short-Term Memory (LSTM) model\nwith advanced privacy preserving techniques. By integrating quantum layers into\nthe LSTM architecture, our approach adeptly captures complex\ncross-transactional patters, resulting in an approximate 5% performance\nimprovement across key evaluation metrics compared to conventional models.\nCentral to our framework is \"FedRansel\", a novel method designed to defend\nagainst poisoning and inference attacks, thereby reducing model degradation and\ninference accuracy by 4-8%, compared to standard differential privacy\nmechanisms. This pseudo-centralised setup with a Quantum LSTM model, enhances\nfraud detection accuracy and reinforces the security and confidentiality of\nsensitive financial data.",
    "updated" : "2025-07-15T17:29:12Z",
    "published" : "2025-07-15T17:29:12Z",
    "authors" : [
      {
        "name" : "Abhishek Sawaika"
      },
      {
        "name" : "Swetang Krishna"
      },
      {
        "name" : "Tushar Tomar"
      },
      {
        "name" : "Durga Pritam Suggisetti"
      },
      {
        "name" : "Aditi Lal"
      },
      {
        "name" : "Tanmaya Shrivastav"
      },
      {
        "name" : "Nouhaila Innan"
      },
      {
        "name" : "Muhammad Shafique"
      }
    ],
    "categories" : [
      "q-fin.CP",
      "cs.AI",
      "cs.LG",
      "I.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.00128v1",
    "title" : "How Quantization Impacts Privacy Risk on LLMs for Code?",
    "summary" : "Large language models for code (LLMs4Code) rely heavily on massive training\ndata, including sensitive data, such as cloud service credentials of the\nprojects and personal identifiable information of the developers, raising\nserious privacy concerns. Membership inference (MI) has recently emerged as an\neffective tool for assessing privacy risk by identifying whether specific data\nbelong to a model's training set. In parallel, model compression techniques,\nespecially quantization, have gained traction for reducing computational costs\nand enabling the deployment of large models. However, while quantized models\nstill retain knowledge learned from the original training data, it remains\nunclear whether quantization affects their ability to retain and expose privacy\ninformation. Answering this question is of great importance to understanding\nprivacy risks in real-world deployments. In this work, we conduct the first\nempirical study on how quantization influences task performance and privacy\nrisk simultaneously in LLMs4Code. To do this, we implement widely used\nquantization techniques (static and dynamic) to three representative model\nfamilies, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that\nquantization has a significant impact on reducing the privacy risk relative to\nthe original model. We also uncover a positive correlation between task\nperformance and privacy risk, indicating an underlying tradeoff. Moreover, we\nreveal the possibility that quantizing larger models could yield better balance\nthan using full-precision small models. Finally, we demonstrate that these\nfindings generalize across different architectures, model sizes and MI methods,\noffering practical guidance for safeguarding privacy when deploying compressed\nLLMs4Code.",
    "updated" : "2025-07-31T19:28:31Z",
    "published" : "2025-07-31T19:28:31Z",
    "authors" : [
      {
        "name" : "Md Nazmul Haque"
      },
      {
        "name" : "Hua Yang"
      },
      {
        "name" : "Zhou Yang"
      },
      {
        "name" : "Bowen Xu"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.18365v3",
    "title" : "RecPS: Privacy Risk Scoring for Recommender Systems",
    "summary" : "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning.",
    "updated" : "2025-08-01T17:19:56Z",
    "published" : "2025-07-24T12:46:30Z",
    "authors" : [
      {
        "name" : "Jiajie He"
      },
      {
        "name" : "Yuechun Gu"
      },
      {
        "name" : "Keke Chen"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.23291v2",
    "title" : "Evaluating the Dynamics of Membership Privacy in Deep Learning",
    "summary" : "Membership inference attacks (MIAs) pose a critical threat to the privacy of\ntraining data in deep learning. Despite significant progress in attack\nmethodologies, our understanding of when and how models encode membership\ninformation during training remains limited. This paper presents a dynamic\nanalytical framework for dissecting and quantifying privacy leakage dynamics at\nthe individual sample level. By tracking per-sample vulnerabilities on an\nFPR-TPR plane throughout training, our framework systematically measures how\nfactors such as dataset complexity, model architecture, and optimizer choice\ninfluence the rate and severity at which samples become vulnerable. Crucially,\nwe discover a robust correlation between a sample's intrinsic learning\ndifficulty, and find that the privacy risk of samples highly vulnerable in the\nfinal trained model is largely determined early during training. Our results\nthus provide a deeper understanding of how privacy risks dynamically emerge\nduring training, laying the groundwork for proactive, privacy-aware model\ntraining strategies.",
    "updated" : "2025-08-03T23:23:03Z",
    "published" : "2025-07-31T07:09:52Z",
    "authors" : [
      {
        "name" : "Yuetian Chen"
      },
      {
        "name" : "Zhiqi Wang"
      },
      {
        "name" : "Nathalie Baracaldo"
      },
      {
        "name" : "Swanand Ravindra Kadhe"
      },
      {
        "name" : "Lei Yu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2508.02836v1",
    "title" : "Agentic Privacy-Preserving Machine Learning",
    "summary" : "Privacy-preserving machine learning (PPML) is critical to ensure data privacy\nin AI. Over the past few years, the community has proposed a wide range of\nprovably secure PPML schemes that rely on various cryptography primitives.\nHowever, when it comes to large language models (LLMs) with billions of\nparameters, the efficiency of PPML is everything but acceptable. For instance,\nthe state-of-the-art solution for confidential LLM inference represents at\nleast 10,000-fold slower performance compared to plaintext inference. The\nperformance gap is even larger when the context length increases. In this\nposition paper, we propose a novel framework named Agentic-PPML to make PPML in\nLLMs practical. Our key insight is to employ a general-purpose LLM for intent\nunderstanding and delegate cryptographically secure inference to specialized\nmodels trained on vertical domains. By modularly separating language intent\nparsing - which typically involves little or no sensitive information - from\nprivacy-critical computation, Agentic-PPML completely eliminates the need for\nthe LLMs to process the encrypted prompts, enabling practical deployment of\nprivacy-preserving LLM-centric services.",
    "updated" : "2025-07-30T08:20:45Z",
    "published" : "2025-07-30T08:20:45Z",
    "authors" : [
      {
        "name" : "Mengyu Zhang"
      },
      {
        "name" : "Zhuotao Liu"
      },
      {
        "name" : "Jingwen Huang"
      },
      {
        "name" : "Xuanqi Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.17228v2",
    "title" : "P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous\n  Edge Devices",
    "summary" : "Split Learning (SL) is an emerging privacy-preserving machine learning\ntechnique that enables resource constrained edge devices to participate in\nmodel training by partitioning a model into client-side and server-side\nsub-models. While SL reduces computational overhead on edge devices, it\nencounters significant challenges in heterogeneous environments where devices\nvary in computing resources, communication capabilities, environmental\nconditions, and privacy requirements. Although recent studies have explored\nheterogeneous SL frameworks that optimize split points for devices with varying\nresource constraints, they often neglect personalized privacy requirements and\nlocal model customization under varying environmental conditions. To address\nthese limitations, we propose P3SL, a Personalized Privacy-Preserving Split\nLearning framework designed for heterogeneous, resource-constrained edge device\nsystems. The key contributions of this work are twofold. First, we design a\npersonalized sequential split learning pipeline that allows each client to\nachieve customized privacy protection and maintain personalized local models\ntailored to their computational resources, environmental conditions, and\nprivacy needs. Second, we adopt a bi-level optimization technique that empowers\nclients to determine their own optimal personalized split points without\nsharing private sensitive information (i.e., computational resources,\nenvironmental conditions, privacy requirements) with the server. This approach\nbalances energy consumption and privacy leakage risks while maintaining high\nmodel accuracy. We implement and evaluate P3SL on a testbed consisting of 7\ndevices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,\nusing diverse model architectures and datasets under varying environmental\nconditions.",
    "updated" : "2025-08-05T02:08:01Z",
    "published" : "2025-07-23T05:50:33Z",
    "authors" : [
      {
        "name" : "Wei Fan"
      },
      {
        "name" : "JinYi Yoon"
      },
      {
        "name" : "Xiaochang Li"
      },
      {
        "name" : "Huajie Shao"
      },
      {
        "name" : "Bo Ji"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ]
  }
]