[
  {
    "id" : "http://arxiv.org/abs/2507.01808v1",
    "title" : "Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study\n  in Privacy-Preserving Machine Learning to Solve Real-World Problems",
    "summary" : "Small- and medium-sized manufacturers need innovative data tools but, because\nof competition and privacy concerns, often do not want to share their\nproprietary data with researchers who might be interested in helping. This\npaper introduces a privacy-preserving platform by which manufacturers may\nsafely share their data with researchers through secure methods, so that those\nresearchers then create innovative tools to solve the manufacturers' real-world\nproblems, and then provide tools that execute solutions back onto the platform\nfor others to use with privacy and confidentiality guarantees. We illustrate\nthis problem through a particular use case which addresses an important problem\nin the large-scale manufacturing of food crystals, which is that quality\ncontrol relies on image analysis tools. Previous to our research, food crystals\nin the images were manually counted, which required substantial and\ntime-consuming human efforts, but we have developed and deployed a crystal\nanalysis tool which makes this process both more rapid and accurate. The tool\nenables automatic characterization of the crystal size distribution and numbers\nfrom microscope images while the natural imperfections from the sample\npreparation are automatically removed; a machine learning model to count high\nresolution translucent crystals and agglomeration of crystals was also\ndeveloped to aid in these efforts. The resulting algorithm was then packaged\nfor real-world use on the factory floor via a web-based app secured through the\noriginating privacy-preserving platform, allowing manufacturers to use it while\nkeeping their proprietary data secure. After demonstrating this full process,\nfuture directions are also explored.",
    "updated" : "2025-07-02T15:25:43Z",
    "published" : "2025-07-02T15:25:43Z",
    "authors" : [
      {
        "name" : "Xiaoyu Ji"
      },
      {
        "name" : "Jessica Shorland"
      },
      {
        "name" : "Joshua Shank"
      },
      {
        "name" : "Pascal Delpe-Brice"
      },
      {
        "name" : "Latanya Sweeney"
      },
      {
        "name" : "Jan Allebach"
      },
      {
        "name" : "Ali Shakouri"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "68T01, 68T05, 68T45, 94A60"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01752v1",
    "title" : "Tuning without Peeking: Provable Privacy and Generalization Bounds for\n  LLM Post-Training",
    "summary" : "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.",
    "updated" : "2025-07-02T14:29:30Z",
    "published" : "2025-07-02T14:29:30Z",
    "authors" : [
      {
        "name" : "Ismail Labiad"
      },
      {
        "name" : "Mathurin Videau"
      },
      {
        "name" : "Matthieu Kowalski"
      },
      {
        "name" : "Marc Schoenauer"
      },
      {
        "name" : "Alessandro Leite"
      },
      {
        "name" : "Julia Kempe"
      },
      {
        "name" : "Olivier Teytaud"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01581v1",
    "title" : "A Privacy-Preserving Indoor Localization System based on Hierarchical\n  Federated Learning",
    "summary" : "Location information serves as the fundamental element for numerous Internet\nof Things (IoT) applications. Traditional indoor localization techniques often\nproduce significant errors and raise privacy concerns due to centralized data\ncollection. In response, Machine Learning (ML) techniques offer promising\nsolutions by capturing indoor environment variations. However, they typically\nrequire central data aggregation, leading to privacy, bandwidth, and server\nreliability issues. To overcome these challenges, in this paper, we propose a\nFederated Learning (FL)-based approach for dynamic indoor localization using a\nDeep Neural Network (DNN) model. Experimental results show that FL has the\nnearby performance to Centralized Model (CL) while keeping the data privacy,\nbandwidth efficiency and server reliability. This research demonstrates that\nour proposed FL approach provides a viable solution for privacy-enhanced indoor\nlocalization, paving the way for advancements in secure and efficient indoor\nlocalization systems.",
    "updated" : "2025-07-02T10:53:31Z",
    "published" : "2025-07-02T10:53:31Z",
    "authors" : [
      {
        "name" : "Masood Jan"
      },
      {
        "name" : "Wafa Njima"
      },
      {
        "name" : "Xun Zhang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01487v1",
    "title" : "How to Securely Shuffle? A survey about Secure Shufflers for\n  privacy-preserving computations",
    "summary" : "Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building\nblock for private data aggregation. Recently, the field of differential privacy\nhas revived interest in secure shufflers by highlighting the privacy\namplification they can provide in various computations. Although several works\nargue for the utility of secure shufflers, they often treat them as black\nboxes; overlooking the practical vulnerabilities and performance trade-offs of\nexisting implementations. This leaves a central question open: what makes a\ngood secure shuffler?\n  This survey addresses that question by identifying, categorizing, and\ncomparing 26 secure protocols that realize the necessary shuffling\nfunctionality. To enable a meaningful comparison, we adapt and unify existing\nsecurity definitions into a consistent set of properties. We also present an\noverview of privacy-preserving technologies that rely on secure shufflers,\noffer practical guidelines for selecting appropriate protocols, and outline\npromising directions for future work.",
    "updated" : "2025-07-02T08:48:53Z",
    "published" : "2025-07-02T08:48:53Z",
    "authors" : [
      {
        "name" : "Marc Damie"
      },
      {
        "name" : "Florian Hahn"
      },
      {
        "name" : "Andreas Peter"
      },
      {
        "name" : "Jan Ramon"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.01216v1",
    "title" : "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
    "summary" : "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
    "updated" : "2025-07-01T22:27:21Z",
    "published" : "2025-07-01T22:27:21Z",
    "authors" : [
      {
        "name" : "Xingke Yang"
      },
      {
        "name" : "Liang Li"
      },
      {
        "name" : "Zhiyi Wan"
      },
      {
        "name" : "Sicong Li"
      },
      {
        "name" : "Hao Wang"
      },
      {
        "name" : "Xiaoqi Qi"
      },
      {
        "name" : "Jiang Liu"
      },
      {
        "name" : "Tomoaki Ohtsuki"
      },
      {
        "name" : "Xin Fu"
      },
      {
        "name" : "Miao Pan"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00920v1",
    "title" : "Privacy-Preserving Quantized Federated Learning with Diverse Precision",
    "summary" : "Federated learning (FL) has emerged as a promising paradigm for distributed\nmachine learning, enabling collaborative training of a global model across\nmultiple local devices without requiring them to share raw data. Despite its\nadvancements, FL is limited by factors such as: (i) privacy risks arising from\nthe unprotected transmission of local model updates to the fusion center (FC)\nand (ii) decreased learning utility caused by heterogeneity in model\nquantization resolution across participating devices. Prior work typically\naddresses only one of these challenges because maintaining learning utility\nunder both privacy risks and quantization heterogeneity is a non-trivial task.\nIn this paper, our aim is therefore to improve the learning utility of a\nprivacy-preserving FL that allows clusters of devices with different\nquantization resolutions to participate in each FL round. Specifically, we\nintroduce a novel stochastic quantizer (SQ) that is designed to simultaneously\nachieve differential privacy (DP) and minimum quantization error. Notably, the\nproposed SQ guarantees bounded distortion, unlike other DP approaches. To\naddress quantization heterogeneity, we introduce a cluster size optimization\ntechnique combined with a linear fusion approach to enhance model aggregation\naccuracy. Numerical simulations validate the benefits of our approach in terms\nof privacy protection and learning utility compared to the conventional\nLaplaceSQ-FL algorithm.",
    "updated" : "2025-07-01T16:26:20Z",
    "published" : "2025-07-01T16:26:20Z",
    "authors" : [
      {
        "name" : "Dang Qua Nguyen"
      },
      {
        "name" : "Morteza Hashemi"
      },
      {
        "name" : "Erik Perrins"
      },
      {
        "name" : "Sergiy A. Vorobyov"
      },
      {
        "name" : "David J. Love"
      },
      {
        "name" : "Taejoon Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00596v1",
    "title" : "Gaze3P: Gaze-Based Prediction of User-Perceived Privacy",
    "summary" : "Privacy is a highly subjective concept and perceived variably by different\nindividuals. Previous research on quantifying user-perceived privacy has\nprimarily relied on questionnaires. Furthermore, applying user-perceived\nprivacy to optimise the parameters of privacy-preserving techniques (PPT)\nremains insufficiently explored. To address these limitations, we introduce\nGaze3P -- the first dataset specifically designed to facilitate systematic\ninvestigations into user-perceived privacy. Our dataset comprises gaze data\nfrom 100 participants and 1,000 stimuli, encompassing a range of private and\nsafe attributes. With Gaze3P, we train a machine learning model to implicitly\nand dynamically predict perceived privacy from human eye gaze. Through\ncomprehensive experiments, we show that the resulting models achieve high\naccuracy. Finally, we illustrate how predicted privacy can be used to optimise\nthe parameters of differentially private mechanisms, thereby enhancing their\nalignment with user expectations.",
    "updated" : "2025-07-01T09:26:38Z",
    "published" : "2025-07-01T09:26:38Z",
    "authors" : [
      {
        "name" : "Mayar Elfares"
      },
      {
        "name" : "Pascal Reisert"
      },
      {
        "name" : "Ralf Küsters"
      },
      {
        "name" : "Andreas Bulling"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00402v1",
    "title" : "GRAND: Graph Release with Assured Node Differential Privacy",
    "summary" : "Differential privacy is a well-established framework for safeguarding\nsensitive information in data. While extensively applied across various\ndomains, its application to network data -- particularly at the node level --\nremains underexplored. Existing methods for node-level privacy either focus\nexclusively on query-based approaches, which restrict output to pre-specified\nnetwork statistics, or fail to preserve key structural properties of the\nnetwork. In this work, we propose GRAND (Graph Release with Assured Node\nDifferential privacy), which is, to the best of our knowledge, the first\nnetwork release mechanism that releases entire networks while ensuring\nnode-level differential privacy and preserving structural properties. Under a\nbroad class of latent space models, we show that the released network\nasymptotically follows the same distribution as the original network. The\neffectiveness of the approach is evaluated through extensive experiments on\nboth synthetic and real-world datasets.",
    "updated" : "2025-07-01T03:39:08Z",
    "published" : "2025-07-01T03:39:08Z",
    "authors" : [
      {
        "name" : "Suqing Liu"
      },
      {
        "name" : "Xuan Bi"
      },
      {
        "name" : "Tianxi Li"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.02727v1",
    "title" : "Quantifying Classifier Utility under Local Differential Privacy",
    "summary" : "Local differential privacy (LDP) provides a rigorous and quantifiable privacy\nguarantee for personal data by introducing perturbation at the data source.\nHowever, quantifying the impact of these perturbations on classifier utility\nremains a theoretical challenge, particularly for complex or black-box\nclassifiers.\n  This paper presents a framework for theoretically quantifying classifier\nutility under LDP mechanisms. The key insight is that LDP perturbation is\nconcentrated around the original data with a specific probability, transforming\nutility analysis of the classifier into its robustness analysis in this\nconcentrated region. Our framework connects the concentration analysis of LDP\nmechanisms with the robustness analysis of classifiers. It treats LDP\nmechanisms as general distributional functions and classifiers as black-box\nfunctions, thus applicable to any LDP mechanism and classifier. A direct\napplication of our utility quantification is guiding the selection of LDP\nmechanisms and privacy parameters for a given classifier. Notably, our analysis\nshows that a piecewise-based mechanism leads to better utility compared to\nalternatives in common scenarios.\n  Using this framework alongside two novel refinement techniques, we conduct\ncase studies on utility quantification for typical mechanism-classifier\ncombinations. The results demonstrate that our theoretical utility\nquantification aligns closely with empirical observations, particularly when\nclassifiers operate in lower-dimensional input spaces.",
    "updated" : "2025-07-03T15:42:10Z",
    "published" : "2025-07-03T15:42:10Z",
    "authors" : [
      {
        "name" : "Ye Zheng"
      },
      {
        "name" : "Yidan Hu"
      }
    ],
    "categories" : [
      "cs.CR",
      "E.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.02414v1",
    "title" : "Privacy-preserving Preselection for Face Identification Based on Packing",
    "summary" : "Face identification systems operating in the ciphertext domain have garnered\nsignificant attention due to increasing privacy concerns and the potential\nrecovery of original facial data. However, as the size of ciphertext template\nlibraries grows, the face retrieval process becomes progressively more\ntime-intensive. To address this challenge, we propose a novel and efficient\nscheme for face retrieval in the ciphertext domain, termed Privacy-Preserving\nPreselection for Face Identification Based on Packing (PFIP). PFIP incorporates\nan innovative preselection mechanism to reduce computational overhead and a\npacking module to enhance the flexibility of biometric systems during the\nenrollment stage. Extensive experiments conducted on the LFW and CASIA datasets\ndemonstrate that PFIP preserves the accuracy of the original face recognition\nmodel, achieving a 100% hit rate while retrieving 1,000 ciphertext face\ntemplates within 300 milliseconds. Compared to existing approaches, PFIP\nachieves a nearly 50x improvement in retrieval efficiency.",
    "updated" : "2025-07-03T08:15:07Z",
    "published" : "2025-07-03T08:15:07Z",
    "authors" : [
      {
        "name" : "Rundong Xin"
      },
      {
        "name" : "Taotao Wang"
      },
      {
        "name" : "Jin Wang"
      },
      {
        "name" : "Chonghe Zhao"
      },
      {
        "name" : "Jing Wang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.00920v2",
    "title" : "Privacy-Preserving Quantized Federated Learning with Diverse Precision",
    "summary" : "Federated learning (FL) has emerged as a promising paradigm for distributed\nmachine learning, enabling collaborative training of a global model across\nmultiple local devices without requiring them to share raw data. Despite its\nadvancements, FL is limited by factors such as: (i) privacy risks arising from\nthe unprotected transmission of local model updates to the fusion center (FC)\nand (ii) decreased learning utility caused by heterogeneity in model\nquantization resolution across participating devices. Prior work typically\naddresses only one of these challenges because maintaining learning utility\nunder both privacy risks and quantization heterogeneity is a non-trivial task.\nIn this paper, our aim is therefore to improve the learning utility of a\nprivacy-preserving FL that allows clusters of devices with different\nquantization resolutions to participate in each FL round. Specifically, we\nintroduce a novel stochastic quantizer (SQ) that is designed to simultaneously\nachieve differential privacy (DP) and minimum quantization error. Notably, the\nproposed SQ guarantees bounded distortion, unlike other DP approaches. To\naddress quantization heterogeneity, we introduce a cluster size optimization\ntechnique combined with a linear fusion approach to enhance model aggregation\naccuracy. Numerical simulations validate the benefits of our approach in terms\nof privacy protection and learning utility compared to the conventional\nLaplaceSQ-FL algorithm.",
    "updated" : "2025-07-03T01:49:31Z",
    "published" : "2025-07-01T16:26:20Z",
    "authors" : [
      {
        "name" : "Dang Qua Nguyen"
      },
      {
        "name" : "Morteza Hashemi"
      },
      {
        "name" : "Erik Perrins"
      },
      {
        "name" : "Sergiy A. Vorobyov"
      },
      {
        "name" : "David J. Love"
      },
      {
        "name" : "Taejoon Kim"
      }
    ],
    "categories" : [
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05175v1",
    "title" : "Blind Targeting: Personalization under Third-Party Privacy Constraints",
    "summary" : "Major advertising platforms recently increased privacy protections by\nlimiting advertisers' access to individual-level data. Instead of providing\naccess to granular raw data, the platforms only allow a limited number of\naggregate queries to a dataset, which is further protected by adding\ndifferentially private noise. This paper studies whether and how advertisers\ncan design effective targeting policies within these restrictive privacy\npreserving data environments. To achieve this, I develop a probabilistic\nmachine learning method based on Bayesian optimization, which facilitates\ndynamic data exploration. Since Bayesian optimization was designed to sample\npoints from a function to find its maximum, it is not applicable to aggregate\nqueries and to targeting. Therefore, I introduce two innovations: (i) integral\nupdating of posteriors which allows to select the best regions of the data to\nquery rather than individual points and (ii) a targeting-aware acquisition\nfunction that dynamically selects the most informative regions for the\ntargeting task. I identify the conditions of the dataset and privacy\nenvironment that necessitate the use of such a \"smart\" querying strategy. I\napply the strategic querying method to the Criteo AI Labs dataset for uplift\nmodeling (Diemert et al., 2018) that contains visit and conversion data from\n14M users. I show that an intuitive benchmark strategy only achieves 33% of the\nnon-privacy-preserving targeting potential in some cases, while my strategic\nquerying method achieves 97-101% of that potential, and is statistically\nindistinguishable from Causal Forest (Athey et al., 2019): a state-of-the-art\nnon-privacy-preserving machine learning targeting method.",
    "updated" : "2025-07-07T16:30:40Z",
    "published" : "2025-07-07T16:30:40Z",
    "authors" : [
      {
        "name" : "Anya Shchetkina"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.LG",
      "econ.EM",
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04771v1",
    "title" : "Efficient Unlearning with Privacy Guarantees",
    "summary" : "Privacy protection laws, such as the GDPR, grant individuals the right to\nrequest the forgetting of their personal data not only from databases but also\nfrom machine learning (ML) models trained on them. Machine unlearning has\nemerged as a practical means to facilitate model forgetting of data instances\nseen during training. Although some existing machine unlearning methods\nguarantee exact forgetting, they are typically costly in computational terms.\nOn the other hand, more affordable methods do not offer forgetting guarantees\nand are applicable only to specific ML models. In this paper, we present\n\\emph{efficient unlearning with privacy guarantees} (EUPG), a novel machine\nunlearning framework that offers formal privacy guarantees to individuals whose\ndata are being unlearned. EUPG involves pre-training ML models on data\nprotected using privacy models, and it enables {\\em efficient unlearning with\nthe privacy guarantees offered by the privacy models in use}. Through empirical\nevaluation on four heterogeneous data sets protected with $k$-anonymity and\n$\\epsilon$-differential privacy as privacy models, our approach demonstrates\nutility and forgetting effectiveness comparable to those of exact unlearning\nmethods, while significantly reducing computational and storage costs. Our code\nis available at https://github.com/najeebjebreel/EUPG.",
    "updated" : "2025-07-07T08:46:02Z",
    "published" : "2025-07-07T08:46:02Z",
    "authors" : [
      {
        "name" : "Josep Domingo-Ferrer"
      },
      {
        "name" : "Najeeb Jebreel"
      },
      {
        "name" : "David Sánchez"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04528v1",
    "title" : "Towards integration of Privacy Enhancing Technologies in Explainable\n  Artificial Intelligence",
    "summary" : "Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating\nthe risk of non-transparency in the decision-making process of black-box\nArtificial Intelligence (AI) systems. However, despite the benefits, XAI\nmethods are found to leak the privacy of individuals whose data is used in\ntraining or querying the models. Researchers have demonstrated privacy attacks\nthat exploit explanations to infer sensitive personal information of\nindividuals. Currently there is a lack of defenses against known privacy\nattacks targeting explanations when vulnerable XAI are used in production and\nmachine learning as a service system. To address this gap, in this article, we\nexplore Privacy Enhancing Technologies (PETs) as a defense mechanism against\nattribute inference on explanations provided by feature-based XAI methods. We\nempirically evaluate 3 types of PETs, namely synthetic training data,\ndifferentially private training and noise addition, on two categories of\nfeature-based XAI. Our evaluation determines different responses from the\nmitigation methods and side-effects of PETs on other system properties such as\nutility and performance. In the best case, PETs integration in explanations\nreduced the risk of the attack by 49.47%, while maintaining model utility and\nexplanation quality. Through our evaluation, we identify strategies for using\nPETs in XAI for maximizing benefits and minimizing the success of this privacy\nattack on sensitive personal information.",
    "updated" : "2025-07-06T20:45:34Z",
    "published" : "2025-07-06T20:45:34Z",
    "authors" : [
      {
        "name" : "Sonal Allana"
      },
      {
        "name" : "Rozita Dara"
      },
      {
        "name" : "Xiaodong Lin"
      },
      {
        "name" : "Pulei Xiong"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04185v1",
    "title" : "From Legal Text to Tech Specs: Generative AI's Interpretation of Consent\n  in Privacy Law",
    "summary" : "Privacy law and regulation have turned to \"consent\" as the legitimate basis\nfor collecting and processing individuals' data. As governments have rushed to\nenshrine consent requirements in their privacy laws, such as the California\nConsumer Privacy Act (CCPA), significant challenges remain in understanding how\nthese legal mandates are operationalized in software. The opaque nature of\nsoftware development processes further complicates this translation. To address\nthis, we explore the use of Large Language Models (LLMs) in requirements\nengineering to bridge the gap between legal requirements and technical\nimplementation. This study employs a three-step pipeline that involves using an\nLLM to classify software use cases for compliance, generating LLM modifications\nfor non-compliant cases, and manually validating these changes against legal\nstandards. Our preliminary findings highlight the potential of LLMs in\nautomating compliance tasks, while also revealing limitations in their\nreasoning capabilities. By benchmarking LLMs against real-world use cases, this\nresearch provides insights into leveraging AI-driven solutions to enhance legal\ncompliance of software.",
    "updated" : "2025-07-05T23:36:05Z",
    "published" : "2025-07-05T23:36:05Z",
    "authors" : [
      {
        "name" : "Aniket Kesari"
      },
      {
        "name" : "Travis Breaux"
      },
      {
        "name" : "Tom Norton"
      },
      {
        "name" : "Sarah Santos"
      },
      {
        "name" : "Anmol Singhal"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.04104v1",
    "title" : "Human-Centered Interactive Anonymization for Privacy-Preserving Machine\n  Learning: A Case for Human-Guided k-Anonymity",
    "summary" : "Privacy-preserving machine learning (ML) seeks to balance data utility and\nprivacy, especially as regulations like the GDPR mandate the anonymization of\npersonal data for ML applications. Conventional anonymization approaches often\nreduce data utility due to indiscriminate generalization or suppression of data\nattributes. In this study, we propose an interactive approach that incorporates\nhuman input into the k-anonymization process, enabling domain experts to guide\nattribute preservation based on contextual importance. Using the UCI Adult\ndataset, we compare classification outcomes of interactive human-influenced\nanonymization with traditional, fully automated methods. Our results show that\nhuman input can enhance data utility in some cases, although results vary\nacross tasks and settings. We discuss limitations of our approach and suggest\npotential areas for improved interactive frameworks in privacy-aware ML.",
    "updated" : "2025-07-05T17:20:18Z",
    "published" : "2025-07-05T17:20:18Z",
    "authors" : [
      {
        "name" : "Sri Harsha Gajavalli"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.03694v1",
    "title" : "Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital\n  Wills",
    "summary" : "This work presents a novel decentralized protocol for digital estate planning\nthat integrates advances distributed computing, and cryptography. The original\nproof-of-concept was constructed using purely solidity contracts. Since then,\nwe have enhanced the implementation into a layer-1 protocol that uses modern\ninterchain communication to connect several heterogeneous chain types. A key\ncontribution of this research is the implementation of several modern\ncryptographic primitives to support various forms of claims for information\nvalidation. These primitives introduce an unmatched level of privacy to the\nprocess of digital inheritance. We also demonstrate on a set of heterogeneous\nsmart contracts, following the same spec, on each chain to serve as entry\npoints, gateways, or bridge contracts that are invoked via a path from the will\nmodule on our protocol, to the contract. This ensures a fair and secure\ndistribution of digital assets in accordance with the wishes of the decedent\nwithout the requirement of moving their funds. This research further extends\nits innovations with a user interaction model, featuring a check-in system and\naccount abstraction process, which enhances flexibility and user-friendliness\nwithout compromising on security. By developing a dedicated permissionless\nblockchain that is secured by a network of validators, and interchain relayers,\nthe proposed protocol signifies a transformation in the digital estate planning\nindustry and illustrates the potential of blockchain technology in\nrevolutionizing traditional legal and personal spheres. Implementing a\ncryptoeconomic network at the core of inheritance planning allows for unique\nincentive compatible economic mechanisms to be constructed.",
    "updated" : "2025-07-04T16:23:32Z",
    "published" : "2025-07-04T16:23:32Z",
    "authors" : [
      {
        "name" : "Jovonni L. PHarr"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CE",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.03033v1",
    "title" : "Preserving Privacy, Increasing Accessibility, and Reducing Cost: An\n  On-Device Artificial Intelligence Model for Medical Transcription and Note\n  Generation",
    "summary" : "Background: Clinical documentation represents a significant burden for\nhealthcare providers, with physicians spending up to 2 hours daily on\nadministrative tasks. Recent advances in large language models (LLMs) offer\npromising solutions, but privacy concerns and computational requirements limit\ntheir adoption in healthcare settings. Objective: To develop and evaluate a\nprivacy-preserving, on-device medical transcription system using a fine-tuned\nLlama 3.2 1B model capable of generating structured medical notes from medical\ntranscriptions while maintaining complete data sovereignty entirely in the\nbrowser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient\nFine-Tuning (PEFT) with LoRA on 1,500 synthetic medical\ntranscription-to-structured note pairs. The model was evaluated against the\nbase Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140\nmodified ACI benchmark cases. Evaluation employed both statistical metrics\n(ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple\nclinical quality dimensions. Results: The fine-tuned OnDevice model\ndemonstrated substantial improvements over the base model. On the ACI\nbenchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1\nimproved from 0.832 to 0.866. Clinical quality assessments showed marked\nreduction in major hallucinations (from 85 to 35 cases) and enhanced factual\ncorrectness (2.81 to 3.54 on 5-point scale). Similar improvements were observed\non the internal evaluation dataset, with composite scores increasing from 3.13\nto 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical\ntranscription yields clinically meaningful improvements while enabling complete\non-device browser deployment. This approach addresses key barriers to AI\nadoption in healthcare: privacy preservation, cost reduction, and accessibility\nfor resource-constrained environments.",
    "updated" : "2025-07-03T01:51:49Z",
    "published" : "2025-07-03T01:51:49Z",
    "authors" : [
      {
        "name" : "Johnson Thomas"
      },
      {
        "name" : "Ayush Mudgal"
      },
      {
        "name" : "Wendao Liu"
      },
      {
        "name" : "Nisten Tahiraj"
      },
      {
        "name" : "Zeeshaan Mohammed"
      },
      {
        "name" : "Dhruv Diddi"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.06008v1",
    "title" : "The Impact of Event Data Partitioning on Privacy-aware Process Discovery",
    "summary" : "Information systems support the execution of business processes. The event\nlogs of these executions generally contain sensitive information about\ncustomers, patients, and employees. The corresponding privacy challenges can be\naddressed by anonymizing the event logs while still retaining utility for\nprocess discovery. However, trading off utility and privacy is difficult: the\nhigher the complexity of event log, the higher the loss of utility by\nanonymization. In this work, we propose a pipeline that combines anonymization\nand event data partitioning, where event abstraction is utilized for\npartitioning. By leveraging event abstraction, event logs can be segmented into\nmultiple parts, allowing each sub-log to be anonymized separately. This\npipeline preserves privacy while mitigating the loss of utility. To validate\nour approach, we study the impact of event partitioning on two anonymization\ntechniques using three real-world event logs and two process discovery\ntechniques. Our results demonstrate that event partitioning can bring\nimprovements in process discovery utility for directly-follows-based\nanonymization techniques.",
    "updated" : "2025-07-08T14:13:44Z",
    "published" : "2025-07-08T14:13:44Z",
    "authors" : [
      {
        "name" : "Jungeun Lim"
      },
      {
        "name" : "Stephan A. Fahrenkrog-Petersen"
      },
      {
        "name" : "Xixi Lu"
      },
      {
        "name" : "Jan Mendling"
      },
      {
        "name" : "Minseok Song"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05875v1",
    "title" : "Post-Processing in Local Differential Privacy: An Extensive Evaluation\n  and Benchmark Platform",
    "summary" : "Local differential privacy (LDP) has recently gained prominence as a powerful\nparadigm for collecting and analyzing sensitive data from users' devices.\nHowever, the inherent perturbation added by LDP protocols reduces the utility\nof the collected data. To mitigate this issue, several post-processing (PP)\nmethods have been developed. Yet, the comparative performance of PP methods\nunder diverse settings remains underexplored. In this paper, we present an\nextensive benchmark comprising 6 popular LDP protocols, 7 PP methods, 4 utility\nmetrics, and 6 datasets to evaluate the behaviors and optimality of PP methods\nunder diverse conditions. Through extensive experiments, we show that while PP\ncan substantially improve utility when the privacy budget is small (i.e.,\nstrict privacy), its benefit diminishes as the privacy budget grows. Moreover,\nour findings reveal that the optimal PP method depends on multiple factors,\nincluding the choice of LDP protocol, privacy budget, data characteristics\n(such as distribution and domain size), and the specific utility metric. To\nadvance research in this area and assist practitioners in identifying the most\nsuitable PP method for their setting, we introduce LDP$^3$, an open-source\nbenchmark platform. LDP$^3$ contains all methods used in our experimental\nanalysis, and it is designed in a modular, extensible, and multi-threaded way\nfor future use and development.",
    "updated" : "2025-07-08T10:59:49Z",
    "published" : "2025-07-08T10:59:49Z",
    "authors" : [
      {
        "name" : "Alireza Khodaie"
      },
      {
        "name" : "Berkay Kemal Balioglu"
      },
      {
        "name" : "Mehmet Emre Gursoy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05872v1",
    "title" : "LDP$^3$: An Extensible and Multi-Threaded Toolkit for Local Differential\n  Privacy Protocols and Post-Processing Methods",
    "summary" : "Local differential privacy (LDP) has become a prominent notion for\nprivacy-preserving data collection. While numerous LDP protocols and\npost-processing (PP) methods have been developed, selecting an optimal\ncombination under different privacy budgets and datasets remains a challenge.\nMoreover, the lack of a comprehensive and extensible LDP benchmarking toolkit\nraises difficulties in evaluating new protocols and PP methods. To address\nthese concerns, this paper presents LDP$^3$ (pronounced LDP-Cube), an\nopen-source, extensible, and multi-threaded toolkit for LDP researchers and\npractitioners. LDP$^3$ contains implementations of several LDP protocols, PP\nmethods, and utility metrics in a modular and extensible design. Its modular\ndesign enables developers to conveniently integrate new protocols and PP\nmethods. Furthermore, its multi-threaded nature enables significant reductions\nin execution times via parallelization. Experimental evaluations demonstrate\nthat: (i) using LDP$^3$ to select a good protocol and post-processing method\nsubstantially improves utility compared to a bad or random choice, and (ii) the\nmulti-threaded design of LDP$^3$ brings substantial benefits in terms of\nefficiency.",
    "updated" : "2025-07-08T10:51:42Z",
    "published" : "2025-07-08T10:51:42Z",
    "authors" : [
      {
        "name" : "Berkay Kemal Balioglu"
      },
      {
        "name" : "Alireza Khodaie"
      },
      {
        "name" : "Mehmet Emre Gursoy"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05610v1",
    "title" : "On the Inherent Privacy of Zeroth Order Projected Gradient Descent",
    "summary" : "Differentially private zeroth-order optimization methods have recently gained\npopularity in private fine tuning of machine learning models due to their\nreduced memory requirements. Current approaches for privatizing zeroth-order\nmethods rely on adding Gaussian noise to the estimated zeroth-order gradients.\nHowever, since the search direction in the zeroth-order methods is inherently\nrandom, researchers including Tang et al. (2024) and Zhang et al. (2024a) have\nraised an important question: is the inherent noise in zeroth-order estimators\nsufficient to ensure the overall differential privacy of the algorithm? This\nwork settles this question for a class of oracle-based optimization algorithms\nwhere the oracle returns zeroth-order gradient estimates. In particular, we\nshow that for a fixed initialization, there exist strongly convex objective\nfunctions such that running (Projected) Zeroth-Order Gradient Descent (ZO-GD)\nis not differentially private. Furthermore, we show that even with random\ninitialization and without revealing (initial and) intermediate iterates, the\nprivacy loss in ZO-GD can grow superlinearly with the number of iterations when\nminimizing convex objective functions.",
    "updated" : "2025-07-08T02:38:14Z",
    "published" : "2025-07-08T02:38:14Z",
    "authors" : [
      {
        "name" : "Devansh Gupta"
      },
      {
        "name" : "Meisam Razaviyayn"
      },
      {
        "name" : "Vatsal Sharan"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05415v1",
    "title" : "Layered, Overlapping, and Inconsistent: A Large-Scale Analysis of the\n  Multiple Privacy Policies and Controls of U.S. Banks",
    "summary" : "Privacy policies are often complex. An exception is the two-page standardized\nnotice that U.S. financial institutions must provide under the\nGramm-Leach-Bliley Act (GLBA). However, banks now operate websites, mobile\napps, and other services that involve complex data sharing practices that\nrequire additional privacy notices and do-not-sell opt-outs. We conducted a\nlarge-scale analysis of how U.S. banks implement privacy policies and controls\nin response to GLBA; other federal privacy policy requirements; and the\nCalifornia Consumer Privacy Act (CCPA), a key example for U.S. state privacy\nlaws. We focused on the disclosure and control of a set of especially\nprivacy-invasive practices: third-party data sharing for marketing-related\npurposes. We collected privacy policies for the 2,067 largest U.S. banks,\n45.3\\% of which provided multiple policies. Across disclosures and controls\nwithin the \\textit{same} bank, we identified frequent, concerning\ninconsistencies -- such as banks indicating in GLBA notices that they do not\nshare with third parties but disclosing sharing elsewhere, or using third-party\nmarketing/advertising cookies without disclosure. This multiplicity of\npolicies, with the inconsistencies it causes, may create consumer confusion and\nundermine the transparency goals of the very laws that require them. Our\nfindings call into question whether current policy requirements, such as the\nGLBA notice, are achieving their intended goals in today's online banking\nlandscape. We discuss potential avenues for reforming and harmonizing privacy\npolicies and control requirements across federal and state laws.",
    "updated" : "2025-07-07T18:55:48Z",
    "published" : "2025-07-07T18:55:48Z",
    "authors" : [
      {
        "name" : "Lu Xian"
      },
      {
        "name" : "Van Tran"
      },
      {
        "name" : "Lauren Lee"
      },
      {
        "name" : "Meera Kumar"
      },
      {
        "name" : "Yichen Zhang"
      },
      {
        "name" : "Florian Schaub"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2507.05391v1",
    "title" : "Controlling What You Share: Assessing Language Model Adherence to\n  Privacy Preferences",
    "summary" : "Large language models (LLMs) are primarily accessed via commercial APIs, but\nthis often requires users to expose their data to service providers. In this\npaper, we explore how users can stay in control of their data by using privacy\nprofiles: simple natural language instructions that say what should and should\nnot be revealed. We build a framework where a local model uses these\ninstructions to rewrite queries, only hiding details deemed sensitive by the\nuser, before sending them to an external model, thus balancing privacy with\nperformance. To support this research, we introduce PEEP, a multilingual\ndataset of real user queries annotated to mark private content and paired with\nsynthetic privacy profiles. Our experiments with lightweight LLMs show they can\nfollow these instructions to some extent, but also face consistent challenges,\nhighlighting the need for models that better understand and comply with\nuser-defined privacy preferences.",
    "updated" : "2025-07-07T18:22:55Z",
    "published" : "2025-07-07T18:22:55Z",
    "authors" : [
      {
        "name" : "Guillem Ramírez"
      },
      {
        "name" : "Alexandra Birch"
      },
      {
        "name" : "Ivan Titov"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  }
]