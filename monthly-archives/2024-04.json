[
  {
    "id" : "http://arxiv.org/abs/2404.01619v1",
    "title" : "Making Privacy-preserving Federated Graph Analytics with Strong\n  Guarantees Practical (for Certain Queries)",
    "summary" : "Privacy-preserving federated graph analytics is an emerging area of research.\nThe goal is to run graph analytics queries over a set of devices that are\norganized as a graph while keeping the raw data on the devices rather than\ncentralizing it. Further, no entity may learn any new information except for\nthe final query result. For instance, a device may not learn a neighbor's data.\nThe state-of-the-art prior work for this problem provides privacy guarantees\nfor a broad set of queries in a strong threat model where the devices can be\nmalicious. However, it imposes an impractical overhead: each device locally\nrequires over 8.79 hours of cpu time and 5.73 GiBs of network transfers per\nquery. This paper presents Colo, a new, low-cost system for privacy-preserving\nfederated graph analytics that requires minutes of cpu time and a few MiBs in\nnetwork transfers, for a particular subset of queries. At the heart of Colo is\na new secure computation protocol that enables a device to securely and\nefficiently evaluate a graph query in its local neighborhood while hiding\ndevice data, edge data, and topology data. An implementation and evaluation of\nColo shows that for running a variety of COVID-19 queries over a population of\n1M devices, it requires less than 8.4 minutes of a device's CPU time and 4.93\nMiBs in network transfers - improvements of up to three orders of magnitude.",
    "updated" : "2024-04-02T04:01:31Z",
    "published" : "2024-04-02T04:01:31Z",
    "authors" : [
      {
        "name" : "Kunlong Liu"
      },
      {
        "name" : "Trinabh Gupta"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01283v1",
    "title" : "Evaluating Privacy Perceptions, Experience, and Behavior of Software\n  Development Teams",
    "summary" : "With the increase in the number of privacy regulations, small development\nteams are forced to make privacy decisions on their own. In this paper, we\nconduct a mixed-method survey study, including statistical and qualitative\nanalysis, to evaluate the privacy perceptions, practices, and knowledge of\nmembers involved in various phases of software development (SDLC). Our survey\nincludes 362 participants from 23 countries, encompassing roles such as product\nmanagers, developers, and testers. Our results show diverse definitions of\nprivacy across SDLC roles, emphasizing the need for a holistic privacy approach\nthroughout SDLC. We find that software teams, regardless of their region, are\nless familiar with privacy concepts (such as anonymization), relying on\nself-teaching and forums. Most participants are more familiar with GDPR and\nHIPAA than other regulations, with multi-jurisdictional compliance being their\nprimary concern. Our results advocate the need for role-dependent solutions to\naddress the privacy challenges, and we highlight research directions and\neducational takeaways to help improve privacy-aware software development.",
    "updated" : "2024-04-01T17:55:10Z",
    "published" : "2024-04-01T17:55:10Z",
    "authors" : [
      {
        "name" : "Maxwell Prybylo"
      },
      {
        "name" : "Sara Haghighi"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Sepideh Ghanavati"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01270v1",
    "title" : "Decentralized Collaborative Learning Framework with External Privacy\n  Leakage Analysis",
    "summary" : "This paper presents two methodological advancements in decentralized\nmulti-task learning under privacy constraints, aiming to pave the way for\nfuture developments in next-generation Blockchain platforms. First, we expand\nthe existing framework for collaborative dictionary learning (CollabDict),\nwhich has previously been limited to Gaussian mixture models, by incorporating\ndeep variational autoencoders (VAEs) into the framework, with a particular\nfocus on anomaly detection. We demonstrate that the VAE-based anomaly score\nfunction shares the same mathematical structure as the non-deep model, and\nprovide comprehensive qualitative comparison. Second, considering the\nwidespread use of \"pre-trained models,\" we provide a mathematical analysis on\ndata privacy leakage when models trained with CollabDict are shared externally.\nWe show that the CollabDict approach, when applied to Gaussian mixtures,\nadheres to a Renyi differential privacy criterion. Additionally, we propose a\npractical metric for monitoring internal privacy breaches during the learning\nprocess.",
    "updated" : "2024-04-01T17:46:17Z",
    "published" : "2024-04-01T17:46:17Z",
    "authors" : [
      {
        "name" : "Tsuyoshi Idé"
      },
      {
        "name" : "Dzung T. Phan"
      },
      {
        "name" : "Rudy Raymond"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01231v1",
    "title" : "Privacy Backdoors: Enhancing Membership Inference through Poisoning\n  Pre-trained Models",
    "summary" : "It is commonplace to produce application-specific models by fine-tuning large\npre-trained models using a small bespoke dataset. The widespread availability\nof foundation model checkpoints on the web poses considerable risks, including\nthe vulnerability to backdoor attacks. In this paper, we unveil a new\nvulnerability: the privacy backdoor attack. This black-box privacy attack aims\nto amplify the privacy leakage that arises when fine-tuning a model: when a\nvictim fine-tunes a backdoored model, their training data will be leaked at a\nsignificantly higher rate than if they had fine-tuned a typical model. We\nconduct extensive experiments on various datasets and models, including both\nvision-language models (CLIP) and large language models, demonstrating the\nbroad applicability and effectiveness of such an attack. Additionally, we carry\nout multiple ablation studies with different fine-tuning methods and inference\nstrategies to thoroughly analyze this new threat. Our findings highlight a\ncritical privacy concern within the machine learning community and call for a\nreevaluation of safety protocols in the use of open-source pre-trained models.",
    "updated" : "2024-04-01T16:50:54Z",
    "published" : "2024-04-01T16:50:54Z",
    "authors" : [
      {
        "name" : "Yuxin Wen"
      },
      {
        "name" : "Leo Marchyok"
      },
      {
        "name" : "Sanghyun Hong"
      },
      {
        "name" : "Jonas Geiping"
      },
      {
        "name" : "Tom Goldstein"
      },
      {
        "name" : "Nicholas Carlini"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.00847v1",
    "title" : "Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised\n  Video Anomaly Detection: A New Baseline",
    "summary" : "Unsupervised (US) video anomaly detection (VAD) in surveillance applications\nis gaining more popularity recently due to its practical real-world\napplications. As surveillance videos are privacy sensitive and the availability\nof large-scale video data may enable better US-VAD systems, collaborative\nlearning can be highly rewarding in this setting. However, due to the extremely\nchallenging nature of the US-VAD task, where learning is carried out without\nany annotations, privacy-preserving collaborative learning of US-VAD systems\nhas not been studied yet. In this paper, we propose a new baseline for anomaly\ndetection capable of localizing anomalous events in complex surveillance videos\nin a fully unsupervised fashion without any labels on a privacy-preserving\nparticipant-based distributed training configuration. Additionally, we propose\nthree new evaluation protocols to benchmark anomaly detection approaches on\nvarious scenarios of collaborations and data availability. Based on these\nprotocols, we modify existing VAD datasets to extensively evaluate our approach\nas well as existing US SOTA methods on two large-scale datasets including\nUCF-Crime and XD-Violence. All proposed evaluation protocols, dataset splits,\nand codes are available here: https://github.com/AnasEmad11/CLAP",
    "updated" : "2024-04-01T01:25:06Z",
    "published" : "2024-04-01T01:25:06Z",
    "authors" : [
      {
        "name" : "Anas Al-lahham"
      },
      {
        "name" : "Muhammad Zaigham Zaheer"
      },
      {
        "name" : "Nurbek Tastan"
      },
      {
        "name" : "Karthik Nandakumar"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.02696v1",
    "title" : "Deep Privacy Funnel Model: From a Discriminative to a Generative\n  Approach with an Application to Face Recognition",
    "summary" : "In this study, we apply the information-theoretic Privacy Funnel (PF) model\nto the domain of face recognition, developing a novel method for\nprivacy-preserving representation learning within an end-to-end training\nframework. Our approach addresses the trade-off between obfuscation and utility\nin data protection, quantified through logarithmic loss, also known as\nself-information loss. This research provides a foundational exploration into\nthe integration of information-theoretic privacy principles with representation\nlearning, focusing specifically on the face recognition systems. We\nparticularly highlight the adaptability of our framework with recent\nadvancements in face recognition networks, such as AdaFace and ArcFace. In\naddition, we introduce the Generative Privacy Funnel ($\\mathsf{GenPF}$) model,\na paradigm that extends beyond the traditional scope of the PF model, referred\nto as the Discriminative Privacy Funnel ($\\mathsf{DisPF}$). This\n$\\mathsf{GenPF}$ model brings new perspectives on data generation methods with\nestimation-theoretic and information-theoretic privacy guarantees.\nComplementing these developments, we also present the deep variational PF\n(DVPF) model. This model proposes a tractable variational bound for measuring\ninformation leakage, enhancing the understanding of privacy preservation\nchallenges in deep representation learning. The DVPF model, associated with\nboth $\\mathsf{DisPF}$ and $\\mathsf{GenPF}$ models, sheds light on connections\nwith various generative models such as Variational Autoencoders (VAEs),\nGenerative Adversarial Networks (GANs), and Diffusion models. Complementing our\ntheoretical contributions, we release a reproducible PyTorch package,\nfacilitating further exploration and application of these privacy-preserving\nmethodologies in face recognition systems.",
    "updated" : "2024-04-03T12:50:45Z",
    "published" : "2024-04-03T12:50:45Z",
    "authors" : [
      {
        "name" : "Behrooz Razeghi"
      },
      {
        "name" : "Parsa Rahimi"
      },
      {
        "name" : "Sébastien Marcel"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.02327v1",
    "title" : "Robust Constrained Consensus and Inequality-constrained Distributed\n  Optimization with Guaranteed Differential Privacy and Accurate Convergence",
    "summary" : "We address differential privacy for fully distributed optimization subject to\na shared inequality constraint. By co-designing the distributed optimization\nmechanism and the differential-privacy noise injection mechanism, we propose\nthe first distributed constrained optimization algorithm that can ensure both\nprovable convergence to a global optimal solution and rigorous\n$\\epsilon$-differential privacy, even when the number of iterations tends to\ninfinity. Our approach does not require the Lagrangian function to be strictly\nconvex/concave, and allows the global objective function to be non-separable.\nAs a byproduct of the co-design, we also propose a new constrained consensus\nalgorithm that can achieve rigorous $\\epsilon$-differential privacy while\nmaintaining accurate convergence, which, to our knowledge, has not been\nachieved before. Numerical simulation results on a demand response control\nproblem in smart grid confirm the effectiveness of the proposed approach.",
    "updated" : "2024-04-02T21:53:43Z",
    "published" : "2024-04-02T21:53:43Z",
    "authors" : [
      {
        "name" : "Yongqiang Wang"
      },
      {
        "name" : "Angelia Nedic"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03524v1",
    "title" : "Approximate Gradient Coding for Privacy-Flexible Federated Learning with\n  Non-IID Data",
    "summary" : "This work focuses on the challenges of non-IID data and stragglers/dropouts\nin federated learning. We introduce and explore a privacy-flexible paradigm\nthat models parts of the clients' local data as non-private, offering a more\nversatile and business-oriented perspective on privacy. Within this framework,\nwe propose a data-driven strategy for mitigating the effects of label\nheterogeneity and client straggling on federated learning. Our solution\ncombines both offline data sharing and approximate gradient coding techniques.\nThrough numerical simulations using the MNIST dataset, we demonstrate that our\napproach enables achieving a deliberate trade-off between privacy and utility,\nleading to improved model convergence and accuracy while using an adaptable\nportion of non-private data.",
    "updated" : "2024-04-04T15:29:50Z",
    "published" : "2024-04-04T15:29:50Z",
    "authors" : [
      {
        "name" : "Okko Makkonen"
      },
      {
        "name" : "Sampo Niemelä"
      },
      {
        "name" : "Camilla Hollanti"
      },
      {
        "name" : "Serge Kas Hanna"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03514v1",
    "title" : "Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive\n  Model-Aware Approach",
    "summary" : "Retrieval-augmented large language models (LLMs) have been remarkably\ncompetent in various NLP tasks. Despite their great success, the knowledge\nprovided by the retrieval process is not always useful for improving the model\nprediction, since in some samples LLMs may already be quite knowledgeable and\nthus be able to answer the question correctly without retrieval. Aiming to save\nthe cost of retrieval, previous work has proposed to determine when to do/skip\nthe retrieval in a data-aware manner by analyzing the LLMs' pretraining data.\nHowever, these data-aware methods pose privacy risks and memory limitations,\nespecially when requiring access to sensitive or extensive pretraining data.\nMoreover, these methods offer limited adaptability under fine-tuning or\ncontinual learning settings. We hypothesize that token embeddings are able to\ncapture the model's intrinsic knowledge, which offers a safer and more\nstraightforward way to judge the need for retrieval without the privacy risks\nassociated with accessing pre-training data. Moreover, it alleviates the need\nto retain all the data utilized during model pre-training, necessitating only\nthe upkeep of the token embeddings. Extensive experiments and in-depth analyses\ndemonstrate the superiority of our model-aware approach.",
    "updated" : "2024-04-04T15:21:22Z",
    "published" : "2024-04-04T15:21:22Z",
    "authors" : [
      {
        "name" : "Chengkai Huang"
      },
      {
        "name" : "Rui Wang"
      },
      {
        "name" : "Kaige Xie"
      },
      {
        "name" : "Tong Yu"
      },
      {
        "name" : "Lina Yao"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03509v1",
    "title" : "Privacy-Enhancing Technologies for Artificial Intelligence-Enabled\n  Systems",
    "summary" : "Artificial intelligence (AI) models introduce privacy vulnerabilities to\nsystems. These vulnerabilities may impact model owners or system users; they\nexist during model development, deployment, and inference phases, and threats\ncan be internal or external to the system. In this paper, we investigate\npotential threats and propose the use of several privacy-enhancing technologies\n(PETs) to defend AI-enabled systems. We then provide a framework for PETs\nevaluation for a AI-enabled systems and discuss the impact PETs may have on\nsystem-level variables.",
    "updated" : "2024-04-04T15:14:40Z",
    "published" : "2024-04-04T15:14:40Z",
    "authors" : [
      {
        "name" : "Liv d'Aliberti"
      },
      {
        "name" : "Evan Gronberg"
      },
      {
        "name" : "Joseph Kovba"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03442v1",
    "title" : "Privacy Engineering From Principles to Practice: A Roadmap",
    "summary" : "Privacy engineering is gaining momentum in industry and academia alike. So\nfar, manifold low-level primitives and higher-level methods and strategies have\nsuccessfully been established. Still, fostering adoption in real-world\ninformation systems calls for additional aspects to be consciously considered\nin research and practice.",
    "updated" : "2024-04-04T13:39:49Z",
    "published" : "2024-04-04T13:39:49Z",
    "authors" : [
      {
        "name" : "Frank Pallas"
      },
      {
        "name" : "Katharina Koerner"
      },
      {
        "name" : "Isabel Barberá"
      },
      {
        "name" : "Jaap-Henk Hoepman"
      },
      {
        "name" : "Meiko Jensen"
      },
      {
        "name" : "Nandita Rao Narla"
      },
      {
        "name" : "Nikita Samarin"
      },
      {
        "name" : "Max-R. Ulbricht"
      },
      {
        "name" : "Isabel Wagner"
      },
      {
        "name" : "Kim Wuyts"
      },
      {
        "name" : "Christian Zimmermann"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.SE",
      "K.5.0; H.1.0; D.2.1; D.2.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03324v1",
    "title" : "A Comparative Analysis of Word-Level Metric Differential Privacy:\n  Benchmarking The Privacy-Utility Trade-off",
    "summary" : "The application of Differential Privacy to Natural Language Processing\ntechniques has emerged in relevance in recent years, with an increasing number\nof studies published in established NLP outlets. In particular, the adaptation\nof Differential Privacy for use in NLP tasks has first focused on the\n$\\textit{word-level}$, where calibrated noise is added to word embedding\nvectors to achieve \"noisy\" representations. To this end, several\nimplementations have appeared in the literature, each presenting an alternative\nmethod of achieving word-level Differential Privacy. Although each of these\nincludes its own evaluation, no comparative analysis has been performed to\ninvestigate the performance of such methods relative to each other. In this\nwork, we conduct such an analysis, comparing seven different algorithms on two\nNLP tasks with varying hyperparameters, including the $\\textit{epsilon\n($\\varepsilon$)}$ parameter, or privacy budget. In addition, we provide an\nin-depth analysis of the results with a focus on the privacy-utility trade-off,\nas well as open-source our implementation code for further reproduction. As a\nresult of our analysis, we give insight into the benefits and challenges of\nword-level Differential Privacy, and accordingly, we suggest concrete steps\nforward for the research field.",
    "updated" : "2024-04-04T09:48:14Z",
    "published" : "2024-04-04T09:48:14Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Nihildev Nandakumar"
      },
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03165v1",
    "title" : "Towards Collaborative Family-Centered Design for Online Safety, Privacy\n  and Security",
    "summary" : "Traditional online safety technologies often overly restrict teens and invade\ntheir privacy, while parents often lack knowledge regarding their digital\nprivacy. As such, prior researchers have called for more collaborative\napproaches on adolescent online safety and networked privacy. In this paper, we\npropose family-centered approaches to foster parent-teen collaboration in\nensuring their mobile privacy and online safety while respecting individual\nprivacy, to enhance open discussion and teens' self-regulation. However,\nchallenges such as power imbalances and conflicts with family values arise when\nimplementing such approaches, making parent-teen collaboration difficult.\nTherefore, attending the family-centered design workshop will provide an\ninvaluable opportunity for us to discuss these challenges and identify best\nresearch practices for the future of collaborative online safety and privacy\nwithin families.",
    "updated" : "2024-04-04T02:34:46Z",
    "published" : "2024-04-04T02:34:46Z",
    "authors" : [
      {
        "name" : "Mamtaj Akter"
      },
      {
        "name" : "Zainab Agha"
      },
      {
        "name" : "Ashwaq Alsoubai"
      },
      {
        "name" : "Naima Ali"
      },
      {
        "name" : "Pamela Wisniewski"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04098v1",
    "title" : "You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep\n  Neural Networks",
    "summary" : "Image data have been extensively used in Deep Neural Network (DNN) tasks in\nvarious scenarios, e.g., autonomous driving and medical image analysis, which\nincurs significant privacy concerns. Existing privacy protection techniques are\nunable to efficiently protect such data. For example, Differential Privacy (DP)\nthat is an emerging technique protects data with strong privacy guarantee\ncannot effectively protect visual features of exposed image dataset. In this\npaper, we propose a novel privacy-preserving framework VisualMixer that\nprotects the training data of visual DNN tasks by pixel shuffling, while not\ninjecting any noises. VisualMixer utilizes a new privacy metric called Visual\nFeature Entropy (VFE) to effectively quantify the visual features of an image\nfrom both biological and machine vision aspects. In VisualMixer, we devise a\ntask-agnostic image obfuscation method to protect the visual privacy of data\nfor DNN training and inference. For each image, it determines regions for pixel\nshuffling in the image and the sizes of these regions according to the desired\nVFE. It shuffles pixels both in the spatial domain and in the chromatic channel\nspace in the regions without injecting noises so that it can prevent visual\nfeatures from being discerned and recognized, while incurring negligible\naccuracy loss. Extensive experiments on real-world datasets demonstrate that\nVisualMixer can effectively preserve the visual privacy with negligible\naccuracy loss, i.e., at average 2.35 percentage points of model accuracy loss,\nand almost no performance degradation on model training.",
    "updated" : "2024-04-05T13:49:27Z",
    "published" : "2024-04-05T13:49:27Z",
    "authors" : [
      {
        "name" : "Qiushi Li"
      },
      {
        "name" : "Yan Zhang"
      },
      {
        "name" : "Ju Ren"
      },
      {
        "name" : "Qi Li"
      },
      {
        "name" : "Yaoxue Zhang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04006v1",
    "title" : "From Theory to Comprehension: A Comparative Study of Differential\n  Privacy and $k$-Anonymity",
    "summary" : "The notion of $\\varepsilon$-differential privacy is a widely used concept of\nproviding quantifiable privacy to individuals. However, it is unclear how to\nexplain the level of privacy protection provided by a differential privacy\nmechanism with a set $\\varepsilon$. In this study, we focus on users'\ncomprehension of the privacy protection provided by a differential privacy\nmechanism. To do so, we study three variants of explaining the privacy\nprotection provided by differential privacy: (1) the original mathematical\ndefinition; (2) $\\varepsilon$ translated into a specific privacy risk; and (3)\nan explanation using the randomized response technique. We compare users'\ncomprehension of privacy protection employing these explanatory models with\ntheir comprehension of privacy protection of $k$-anonymity as baseline\ncomprehensibility. Our findings suggest that participants' comprehension of\ndifferential privacy protection is enhanced by the privacy risk model and the\nrandomized response-based model. Moreover, our results confirm our intuition\nthat privacy protection provided by $k$-anonymity is more comprehensible.",
    "updated" : "2024-04-05T10:30:26Z",
    "published" : "2024-04-05T10:30:26Z",
    "authors" : [
      {
        "name" : "Saskia Nuñez von Voigt"
      },
      {
        "name" : "Luise Mehner"
      },
      {
        "name" : "Florian Tschorsch"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03873v1",
    "title" : "PrivShape: Extracting Shapes in Time Series under User-Level Local\n  Differential Privacy",
    "summary" : "Time series have numerous applications in finance, healthcare, IoT, and smart\ncity. In many of these applications, time series typically contain personal\ndata, so privacy infringement may occur if they are released directly to the\npublic. Recently, local differential privacy (LDP) has emerged as the\nstate-of-the-art approach to protecting data privacy. However, existing works\non LDP-based collections cannot preserve the shape of time series. A recent\nwork, PatternLDP, attempts to address this problem, but it can only protect a\nfinite group of elements in a time series due to {\\omega}-event level privacy\nguarantee. In this paper, we propose PrivShape, a trie-based mechanism under\nuser-level LDP to protect all elements. PrivShape first transforms a time\nseries to reduce its length, and then adopts trie-expansion and two-level\nrefinement to improve utility. By extensive experiments on real-world datasets,\nwe demonstrate that PrivShape outperforms PatternLDP when adapted for offline\nuse, and can effectively extract frequent shapes.",
    "updated" : "2024-04-05T03:22:47Z",
    "published" : "2024-04-05T03:22:47Z",
    "authors" : [
      {
        "name" : "Yulian Mao"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Haibo Hu"
      },
      {
        "name" : "Qi Wang"
      },
      {
        "name" : "Kai Huang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05598v1",
    "title" : "Hook-in Privacy Techniques for gRPC-based Microservice Communication",
    "summary" : "gRPC is at the heart of modern distributed system architectures. Based on\nHTTP/2 and Protocol Buffers, it provides highly performant, standardized, and\npolyglot communication across loosely coupled microservices and is increasingly\npreferred over REST- or GraphQL-based service APIs in practice. Despite its\nwidespread adoption, gRPC lacks any advanced privacy techniques beyond\ntransport encryption and basic token-based authentication. Such advanced\ntechniques are, however, increasingly important for fulfilling regulatory\nrequirements. For instance, anonymizing or otherwise minimizing (personal) data\nbefore responding to requests, or pre-processing data based on the purpose of\nthe access may be crucial in certain usecases. In this paper, we therefore\npropose a novel approach for integrating such advanced privacy techniques into\nthe gRPC framework in a practically viable way. Specifically, we present a\ngeneral approach along with a working prototype that implements privacy\ntechniques, such as data minimization and purpose limitation, in a\nconfigurable, extensible, and gRPC-native way utilizing a gRPC interceptor. We\nalso showcase how to integrate this contribution into a realistic example of a\nfood delivery use case. Alongside these implementations, a preliminary\nperformance evaluation shows practical applicability with reasonable overheads.\nAltogether, we present a viable solution for integrating advanced privacy\ntechniques into real-world gRPC-based microservice architectures, thereby\nfacilitating regulatory compliance ``by design''.",
    "updated" : "2024-04-08T15:18:42Z",
    "published" : "2024-04-08T15:18:42Z",
    "authors" : [
      {
        "name" : "Louis Loechel"
      },
      {
        "name" : "Siar-Remzi Akbayin"
      },
      {
        "name" : "Elias Grünewald"
      },
      {
        "name" : "Jannis Kiesel"
      },
      {
        "name" : "Inga Strelnikova"
      },
      {
        "name" : "Thomas Janke"
      },
      {
        "name" : "Frank Pallas"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.DC",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05257v1",
    "title" : "Sensing-Resistance-Oriented Beamforming for Privacy Protection from ISAC\n  Devices",
    "summary" : "With the evolution of integrated sensing and communication (ISAC) technology,\na growing number of devices go beyond conventional communication functions with\nsensing abilities. Therefore, future networks are divinable to encounter new\nprivacy concerns on sensing, such as the exposure of position information to\nunintended receivers. In contrast to traditional privacy preserving schemes\naiming to prevent eavesdropping, this contribution conceives a novel\nbeamforming design toward sensing resistance (SR). Specifically, we expect to\nguarantee the communication quality while masking the real direction of the SR\ntransmitter during the communication. To evaluate the SR performance, a metric\ntermed angular-domain peak-to-average ratio (ADPAR) is first defined and\nanalyzed. Then, we resort to the null-space technique to conceal the real\ndirection, hence to convert the optimization problem to a more tractable form.\nMoreover, semidefinite relaxation along with index optimization is further\nutilized to obtain the optimal beamformer. Finally, simulation results\ndemonstrate the feasibility of the proposed SR-oriented beamforming design\ntoward privacy protection from ISAC receivers.",
    "updated" : "2024-04-08T07:45:10Z",
    "published" : "2024-04-08T07:45:10Z",
    "authors" : [
      {
        "name" : "Teng Ma"
      },
      {
        "name" : "Yue Xiao"
      },
      {
        "name" : "Xia Lei"
      },
      {
        "name" : "Ming Xiao"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05130v1",
    "title" : "Enabling Privacy-Preserving Cyber Threat Detection with Federated\n  Learning",
    "summary" : "Despite achieving good performance and wide adoption, machine learning based\nsecurity detection models (e.g., malware classifiers) are subject to concept\ndrift and evasive evolution of attackers, which renders up-to-date threat data\nas a necessity. However, due to enforcement of various privacy protection\nregulations (e.g., GDPR), it is becoming increasingly challenging or even\nprohibitive for security vendors to collect individual-relevant and\nprivacy-sensitive threat datasets, e.g., SMS spam/non-spam messages from mobile\ndevices. To address such obstacles, this study systematically profiles the\n(in)feasibility of federated learning for privacy-preserving cyber threat\ndetection in terms of effectiveness, byzantine resilience, and efficiency. This\nis made possible by the build-up of multiple threat datasets and threat\ndetection models, and more importantly, the design of realistic and\nsecurity-specific experiments.\n  We evaluate FL on two representative threat detection tasks, namely SMS spam\ndetection and Android malware detection. It shows that FL-trained detection\nmodels can achieve a performance that is comparable to centrally trained\ncounterparts. Also, most non-IID data distributions have either minor or\nnegligible impact on the model performance, while a label-based non-IID\ndistribution of a high extent can incur non-negligible fluctuation and delay in\nFL training. Then, under a realistic threat model, FL turns out to be\nadversary-resistant to attacks of both data poisoning and model poisoning.\nParticularly, the attacking impact of a practical data poisoning attack is no\nmore than 0.14\\% loss in model accuracy. Regarding FL efficiency, a\nbootstrapping strategy turns out to be effective to mitigate the training delay\nas observed in label-based non-IID scenarios.",
    "updated" : "2024-04-08T01:16:56Z",
    "published" : "2024-04-08T01:16:56Z",
    "authors" : [
      {
        "name" : "Yu Bi"
      },
      {
        "name" : "Yekai Li"
      },
      {
        "name" : "Xuan Feng"
      },
      {
        "name" : "Xianghang Mi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05049v1",
    "title" : "PlateSegFL: A Privacy-Preserving License Plate Detection Using Federated\n  Segmentation Learning",
    "summary" : "Automatic License Plate Recognition (ALPR) is an integral component of an\nintelligent transport system with extensive applications in secure\ntransportation, vehicle-to-vehicle communication, stolen vehicles detection,\ntraffic violations, and traffic flow management. The existing license plate\ndetection system focuses on one-shot learners or pre-trained models that\noperate with a geometric bounding box, limiting the model's performance.\nFurthermore, continuous video data streams uploaded to the central server\nresult in network and complexity issues. To combat this, PlateSegFL was\nintroduced, which implements U-Net-based segmentation along with Federated\nLearning (FL). U-Net is well-suited for multi-class image segmentation tasks\nbecause it can analyze a large number of classes and generate a pixel-level\nsegmentation map for each class. Federated Learning is used to reduce the\nquantity of data required while safeguarding the user's privacy. Different\ncomputing platforms, such as mobile phones, are able to collaborate on the\ndevelopment of a standard prediction model where it makes efficient use of\none's time; incorporates more diverse data; delivers projections in real-time;\nand requires no physical effort from the user; resulting around 95% F1 score.",
    "updated" : "2024-04-07T19:10:02Z",
    "published" : "2024-04-07T19:10:02Z",
    "authors" : [
      {
        "name" : "Md. Shahriar Rahman Anuvab"
      },
      {
        "name" : "Mishkat Sultana"
      },
      {
        "name" : "Md. Atif Hossain"
      },
      {
        "name" : "Shashwata Das"
      },
      {
        "name" : "Suvarthi Chowdhury"
      },
      {
        "name" : "Rafeed Rahman"
      },
      {
        "name" : "Dibyo Fabian Dofadar"
      },
      {
        "name" : "Shahriar Rahman Rana"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05047v1",
    "title" : "Initial Exploration of Zero-Shot Privacy Utility Tradeoffs in Tabular\n  Data Using GPT-4",
    "summary" : "We investigate the application of large language models (LLMs), specifically\nGPT-4, to scenarios involving the tradeoff between privacy and utility in\ntabular data. Our approach entails prompting GPT-4 by transforming tabular data\npoints into textual format, followed by the inclusion of precise sanitization\ninstructions in a zero-shot manner. The primary objective is to sanitize the\ntabular data in such a way that it hinders existing machine learning models\nfrom accurately inferring private features while allowing models to accurately\ninfer utility-related attributes. We explore various sanitization instructions.\nNotably, we discover that this relatively simple approach yields performance\ncomparable to more complex adversarial optimization methods used for managing\nprivacy-utility tradeoffs. Furthermore, while the prompts successfully obscure\nprivate features from the detection capabilities of existing machine learning\nmodels, we observe that this obscuration alone does not necessarily meet a\nrange of fairness metrics. Nevertheless, our research indicates the potential\neffectiveness of LLMs in adhering to these fairness metrics, with some of our\nexperimental results aligning with those achieved by well-established\nadversarial optimization techniques.",
    "updated" : "2024-04-07T19:02:50Z",
    "published" : "2024-04-07T19:02:50Z",
    "authors" : [
      {
        "name" : "Bishwas Mandal"
      },
      {
        "name" : "George Amariucai"
      },
      {
        "name" : "Shuangqing Wei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05043v1",
    "title" : "Optimizing Privacy and Utility Tradeoffs for Group Interests Through\n  Harmonization",
    "summary" : "We propose a novel problem formulation to address the privacy-utility\ntradeoff, specifically when dealing with two distinct user groups characterized\nby unique sets of private and utility attributes. Unlike previous studies that\nprimarily focus on scenarios where all users share identical private and\nutility attributes and often rely on auxiliary datasets or manual annotations,\nwe introduce a collaborative data-sharing mechanism between two user groups\nthrough a trusted third party. This third party uses adversarial privacy\ntechniques with our proposed data-sharing mechanism to internally sanitize data\nfor both groups and eliminates the need for manual annotation or auxiliary\ndatasets. Our methodology ensures that private attributes cannot be accurately\ninferred while enabling highly accurate predictions of utility features.\nImportantly, even if analysts or adversaries possess auxiliary datasets\ncontaining raw data, they are unable to accurately deduce private features.\nAdditionally, our data-sharing mechanism is compatible with various existing\nadversarially trained privacy techniques. We empirically demonstrate the\neffectiveness of our approach using synthetic and real-world datasets,\nshowcasing its ability to balance the conflicting goals of privacy and utility.",
    "updated" : "2024-04-07T18:55:33Z",
    "published" : "2024-04-07T18:55:33Z",
    "authors" : [
      {
        "name" : "Bishwas Mandal"
      },
      {
        "name" : "George Amariucai"
      },
      {
        "name" : "Shuangqing Wei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04861v1",
    "title" : "Privacy-Preserving Traceable Functional Encryption for Inner Product",
    "summary" : "Functional encryption introduces a new paradigm of public key encryption that\ndecryption only reveals the function value of encrypted data. To curb key\nleakage issues and trace users in FE-IP, a new primitive called traceable\nfunctional encryption for inner product (TFE-IP) has been proposed. However,\nthe privacy protection of user's identities has not been considered in the\nexisting TFE-IP schemes. In order to balance privacy and accountability, we\npropose the concept of privacy-preserving traceable functional encryption for\ninner product (PPTFE-IP) and give a concrete construction. Our scheme provides\nthe following features: (1) To prevent key sharing, a user's key is bound with\nboth his/her identity and a vector; (2) The key generation center (KGC) and a\nuser execute a two-party secure computing protocol to generate a key without\nthe former knowing anything about the latter's identity; (3) Each user can\nverify the correctness of his/her key; (4) A user can calculate the inner\nproduct of the two vectors embedded in his/her key and in a ciphertext; (5)\nOnly the tracer can trace the identity embedded in a key. The security of our\nscheme is formally reduced to well-known complexity assumptions, and the\nimplementation is conducted to evaluate its efficiency. The novelty of our\nscheme is to protect users' privacy and provide traceability if required.",
    "updated" : "2024-04-07T08:09:46Z",
    "published" : "2024-04-07T08:09:46Z",
    "authors" : [
      {
        "name" : "Muyao Qiu"
      },
      {
        "name" : "Jinguang Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04769v1",
    "title" : "Safeguarding Voice Privacy: Harnessing Near-Ultrasonic Interference To\n  Protect Against Unauthorized Audio Recording",
    "summary" : "The widespread adoption of voice-activated systems has modified routine\nhuman-machine interaction but has also introduced new vulnerabilities. This\npaper investigates the susceptibility of automatic speech recognition (ASR)\nalgorithms in these systems to interference from near-ultrasonic noise.\nBuilding upon prior research that demonstrated the ability of near-ultrasonic\nfrequencies (16 kHz - 22 kHz) to exploit the inherent properties of\nmicroelectromechanical systems (MEMS) microphones, our study explores\nalternative privacy enforcement means using this interference phenomenon. We\nexpose a critical vulnerability in the most common microphones used in modern\nvoice-activated devices, which inadvertently demodulate near-ultrasonic\nfrequencies into the audible spectrum, disrupting the ASR process. Through a\nsystematic analysis of the impact of near-ultrasonic noise on various ASR\nsystems, we demonstrate that this vulnerability is consistent across different\ndevices and under varying conditions, such as broadcast distance and specific\nphoneme structures. Our findings highlight the need to develop robust\ncountermeasures to protect voice-activated systems from malicious exploitation\nof this vulnerability. Furthermore, we explore the potential applications of\nthis phenomenon in enhancing privacy by disrupting unauthorized audio recording\nor eavesdropping. This research underscores the importance of a comprehensive\napproach to securing voice-activated systems, combining technological\ninnovation, responsible development practices, and informed policy decisions to\nensure the privacy and security of users in an increasingly connected world.",
    "updated" : "2024-04-07T00:49:19Z",
    "published" : "2024-04-07T00:49:19Z",
    "authors" : [
      {
        "name" : "Forrest McKee"
      },
      {
        "name" : "David Noever"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04706v1",
    "title" : "Advances in Differential Privacy and Differentially Private Machine\n  Learning",
    "summary" : "There has been an explosion of research on differential privacy (DP) and its\nvarious applications in recent years, ranging from novel variants and\naccounting techniques in differential privacy to the thriving field of\ndifferentially private machine learning (DPML) to newer implementations in\npractice, like those by various companies and organisations such as census\nbureaus. Most recent surveys focus on the applications of differential privacy\nin particular contexts like data publishing, specific machine learning tasks,\nanalysis of unstructured data, location privacy, etc. This work thus seeks to\nfill the gap for a survey that primarily discusses recent developments in the\ntheory of differential privacy along with newer DP variants, viz. Renyi DP and\nConcentrated DP, novel mechanisms and techniques, and the theoretical\ndevelopments in differentially private machine learning in proper detail. In\naddition, this survey discusses its applications to privacy-preserving machine\nlearning in practice and a few practical implementations of DP.",
    "updated" : "2024-04-06T18:49:24Z",
    "published" : "2024-04-06T18:49:24Z",
    "authors" : [
      {
        "name" : "Saswat Das"
      },
      {
        "name" : "Subhankar Mishra"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06216v1",
    "title" : "Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking",
    "summary" : "As eye tracking becomes pervasive with screen-based devices and head-mounted\ndisplays, privacy concerns regarding eye-tracking data have escalated. While\nstate-of-the-art approaches for privacy-preserving eye tracking mostly involve\ndifferential privacy and empirical data manipulations, previous research has\nnot focused on methods for scanpaths. We introduce a novel privacy-preserving\nscanpath comparison protocol designed for the widely used Needleman-Wunsch\nalgorithm, a generalized version of the edit distance algorithm. Particularly,\nby incorporating the Paillier homomorphic encryption scheme, our protocol\nensures that no private information is revealed. Furthermore, we introduce a\nrandom processing strategy and a multi-layered masking method to obfuscate the\nvalues while preserving the original order of encrypted editing operation\ncosts. This minimizes communication overhead, requiring a single communication\nround for each iteration of the Needleman-Wunsch process. We demonstrate the\nefficiency and applicability of our protocol on three publicly available\ndatasets with comprehensive computational performance analyses and make our\nsource code publicly accessible.",
    "updated" : "2024-04-09T11:07:57Z",
    "published" : "2024-04-09T11:07:57Z",
    "authors" : [
      {
        "name" : "Suleyman Ozdel"
      },
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Enkelejda Kasneci"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06144v1",
    "title" : "Differential Privacy for Anomaly Detection: Analyzing the Trade-off\n  Between Privacy and Explainability",
    "summary" : "Anomaly detection (AD), also referred to as outlier detection, is a\nstatistical process aimed at identifying observations within a dataset that\nsignificantly deviate from the expected pattern of the majority of the data.\nSuch a process finds wide application in various fields, such as finance and\nhealthcare. While the primary objective of AD is to yield high detection\naccuracy, the requirements of explainability and privacy are also paramount.\nThe first ensures the transparency of the AD process, while the second\nguarantees that no sensitive information is leaked to untrusted parties. In\nthis work, we exploit the trade-off of applying Explainable AI (XAI) through\nSHapley Additive exPlanations (SHAP) and differential privacy (DP). We perform\nAD with different models and on various datasets, and we thoroughly evaluate\nthe cost of privacy in terms of decreased accuracy and explainability. Our\nresults show that the enforcement of privacy through DP has a significant\nimpact on detection accuracy and explainability, which depends on both the\ndataset and the considered AD model. We further show that the visual\ninterpretation of explanations is also influenced by the choice of the AD\nalgorithm.",
    "updated" : "2024-04-09T09:09:36Z",
    "published" : "2024-04-09T09:09:36Z",
    "authors" : [
      {
        "name" : "Fatima Ezzeddine"
      },
      {
        "name" : "Mirna Saad"
      },
      {
        "name" : "Omran Ayoub"
      },
      {
        "name" : "Davide Andreoletti"
      },
      {
        "name" : "Martin Gjoreski"
      },
      {
        "name" : "Ihab Sbeity"
      },
      {
        "name" : "Marc Langheinrich"
      },
      {
        "name" : "Silvia Giordano"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06001v1",
    "title" : "Privacy Preserving Prompt Engineering: A Survey",
    "summary" : "Pre-trained language models (PLMs) have demonstrated significant proficiency\nin solving a wide range of general natural language processing (NLP) tasks.\nResearchers have observed a direct correlation between the performance of these\nmodels and their sizes. As a result, the sizes of these models have notably\nexpanded in recent years, persuading researchers to adopt the term large\nlanguage models (LLMs) to characterize the larger-sized PLMs. The increased\nsize is accompanied by a distinct capability known as in-context learning\n(ICL), which represents a specialized form of prompting. This enables the\nutilization of LLMs for specific downstream tasks by presenting them with\ndemonstration examples while keeping the model parameters frozen. Although\ninteresting, privacy concerns have become a major obstacle in its widespread\nusage. Multiple studies have examined the privacy risks linked to ICL and\nprompting in general, and have devised techniques to alleviate these risks.\nThus, there is a necessity to organize these mitigation techniques for the\nbenefit of the community. This survey provides a systematic overview of the\nprivacy protection methods employed during ICL and prompting in general. We\nreview, analyze, and compare different methods under this paradigm.\nFurthermore, we provide a summary of the resources accessible for the\ndevelopment of these frameworks. Finally, we discuss the limitations of these\nframeworks and offer a detailed examination of the promising areas that\nnecessitate further exploration.",
    "updated" : "2024-04-09T04:11:25Z",
    "published" : "2024-04-09T04:11:25Z",
    "authors" : [
      {
        "name" : "Kennedy Edemacu"
      },
      {
        "name" : "Xintao Wu"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05876v1",
    "title" : "Privacy and Security of Women's Reproductive Health Apps in a Changing\n  Legal Landscape",
    "summary" : "FemTech, a rising trend in mobile apps, empowers women to digitally manage\ntheir health and family planning. However, privacy and security vulnerabilities\nin period-tracking and fertility-monitoring apps present significant risks,\nsuch as unintended pregnancies and legal consequences. Our approach involves\nmanual observations of privacy policies and app permissions, along with dynamic\nand static analysis using multiple evaluation frameworks. Our research reveals\nthat many of these apps gather personally identifiable information (PII) and\nsensitive healthcare data. Furthermore, our analysis identifies that 61% of the\ncode vulnerabilities found in the apps are classified under the top-ten Open\nWeb Application Security Project (OWASP) vulnerabilities. Our research\nemphasizes the significance of tackling the privacy and security\nvulnerabilities present in period-tracking and fertility-monitoring mobile\napps. By highlighting these crucial risks, we aim to initiate a vital\ndiscussion and advocate for increased accountability and transparency of\ndigital tools for women's health. We encourage the industry to prioritize user\nprivacy and security, ultimately promoting a safer and more secure environment\nfor women's health management.",
    "updated" : "2024-04-08T21:19:10Z",
    "published" : "2024-04-08T21:19:10Z",
    "authors" : [
      {
        "name" : "Shalini Saini"
      },
      {
        "name" : "Nitesh Saxena"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05828v1",
    "title" : "Privacy-Preserving Deep Learning Using Deformable Operators for Secure\n  Task Learning",
    "summary" : "In the era of cloud computing and data-driven applications, it is crucial to\nprotect sensitive information to maintain data privacy, ensuring truly reliable\nsystems. As a result, preserving privacy in deep learning systems has become a\ncritical concern. Existing methods for privacy preservation rely on image\nencryption or perceptual transformation approaches. However, they often suffer\nfrom reduced task performance and high computational costs. To address these\nchallenges, we propose a novel Privacy-Preserving framework that uses a set of\ndeformable operators for secure task learning. Our method involves shuffling\npixels during the analog-to-digital conversion process to generate visually\nprotected data. Those are then fed into a well-known network enhanced with\ndeformable operators. Using our approach, users can achieve equivalent\nperformance to original images without additional training using a secret key.\nMoreover, our method enables access control against unauthorized users.\nExperimental results demonstrate the efficacy of our approach, showcasing its\npotential in cloud-based scenarios and privacy-sensitive applications.",
    "updated" : "2024-04-08T19:46:20Z",
    "published" : "2024-04-08T19:46:20Z",
    "authors" : [
      {
        "name" : "Fabian Perez"
      },
      {
        "name" : "Jhon Lopez"
      },
      {
        "name" : "Henry Arguello"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06868v1",
    "title" : "The 'Sandwich' meta-framework for architecture agnostic deep\n  privacy-preserving transfer learning for non-invasive brainwave decoding",
    "summary" : "Machine learning has enhanced the performance of decoding signals indicating\nhuman behaviour. EEG decoding, as an exemplar indicating neural activity and\nhuman thoughts non-invasively, has been helpful in neural activity analysis and\naiding patients via brain-computer interfaces. However, training machine\nlearning algorithms on EEG encounters two primary challenges: variability\nacross data sets and privacy concerns using data from individuals and data\ncentres. Our objective is to address these challenges by integrating transfer\nlearning for data variability and federated learning for data privacy into a\nunified approach. We introduce the Sandwich as a novel deep privacy-preserving\nmeta-framework combining transfer learning and federated learning. The Sandwich\nframework comprises three components: federated networks (first layers) that\nhandle data set differences at the input level, a shared network (middle layer)\nlearning common rules and applying transfer learning, and individual\nclassifiers (final layers) for specific tasks of each data set. It enables the\ncentral network (central server) to benefit from multiple data sets, while\nlocal branches (local servers) maintain data and label privacy. We evaluated\nthe `Sandwich' meta-architecture in various configurations using the BEETL\nmotor imagery challenge, a benchmark for heterogeneous EEG data sets. Compared\nwith baseline models, our `Sandwich' implementations showed superior\nperformance. The best-performing model, the Inception Sandwich with deep set\nalignment (Inception-SD-Deepset), exceeded baseline methods by 9%. The\n`Sandwich' framework demonstrates significant advancements in federated deep\ntransfer learning for diverse tasks and data sets. It outperforms conventional\ndeep learning methods, showcasing the potential for effective use of larger,\nheterogeneous data sets with enhanced privacy as a model-agnostic\nmeta-framework.",
    "updated" : "2024-04-10T09:47:14Z",
    "published" : "2024-04-10T09:47:14Z",
    "authors" : [
      {
        "name" : "Xiaoxi Wei"
      },
      {
        "name" : "Jyotindra Narayan"
      },
      {
        "name" : "A. Aldo Faisal"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06721v1",
    "title" : "Poisoning Prevention in Federated Learning and Differential Privacy via\n  Stateful Proofs of Execution",
    "summary" : "The rise in IoT-driven distributed data analytics, coupled with increasing\nprivacy concerns, has led to a demand for effective privacy-preserving and\nfederated data collection/model training mechanisms. In response, approaches\nsuch as Federated Learning (FL) and Local Differential Privacy (LDP) have been\nproposed and attracted much attention over the past few years. However, they\nstill share the common limitation of being vulnerable to poisoning attacks\nwherein adversaries compromising edge devices feed forged (a.k.a. poisoned)\ndata to aggregation back-ends, undermining the integrity of FL/LDP results.\n  In this work, we propose a system-level approach to remedy this issue based\non a novel security notion of Proofs of Stateful Execution (PoSX) for\nIoT/embedded devices' software. To realize the PoSX concept, we design SLAPP: a\nSystem-Level Approach for Poisoning Prevention. SLAPP leverages commodity\nsecurity features of embedded devices - in particular ARM TrustZoneM security\nextensions - to verifiably bind raw sensed data to their correct usage as part\nof FL/LDP edge device routines. As a consequence, it offers robust security\nguarantees against poisoning. Our evaluation, based on real-world prototypes\nfeaturing multiple cryptographic primitives and data collection schemes,\nshowcases SLAPP's security and low overhead.",
    "updated" : "2024-04-10T04:18:26Z",
    "published" : "2024-04-10T04:18:26Z",
    "authors" : [
      {
        "name" : "Norrathep Rattanavipanon"
      },
      {
        "name" : "Ivan de Oliviera Nunes"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06686v1",
    "title" : "Atlas-X Equity Financing: Unlocking New Methods to Securely Obfuscate\n  Axe Inventory Data Based on Differential Privacy",
    "summary" : "Banks publish daily a list of available securities/assets (axe list) to\nselected clients to help them effectively locate Long (buy) or Short (sell)\ntrades at reduced financing rates. This reduces costs for the bank, as the list\naggregates the bank's internal firm inventory per asset for all clients of long\nas well as short trades. However, this is somewhat problematic: (1) the bank's\ninventory is revealed; (2) trades of clients who contribute to the aggregated\nlist, particularly those deemed large, are revealed to other clients. Clients\nconducting sizable trades with the bank and possessing a portion of the\naggregated asset exceeding $50\\%$ are considered to be concentrated clients.\nThis could potentially reveal a trading concentrated client's activity to their\ncompetitors, thus providing an unfair advantage over the market.\n  Atlas-X Axe Obfuscation, powered by new differential private methods, enables\na bank to obfuscate its published axe list on a daily basis while under\ncontinual observation, thus maintaining an acceptable inventory Profit and Loss\n(P&L) cost pertaining to the noisy obfuscated axe list while reducing the\nclients' trading activity leakage. Our main differential private innovation is\na differential private aggregator for streams (time series data) of both\npositive and negative integers under continual observation.\n  For the last two years, Atlas-X system has been live in production across\nthree major regions-USA, Europe, and Asia-at J.P. Morgan, a major financial\ninstitution, facilitating significant profitability. To our knowledge, it is\nthe first differential privacy solution to be deployed in the financial sector.\nWe also report benchmarks of our algorithm based on (anonymous) real and\nsynthetic data to showcase the quality of our obfuscation and its success in\nproduction.",
    "updated" : "2024-04-10T02:19:37Z",
    "published" : "2024-04-10T02:19:37Z",
    "authors" : [
      {
        "name" : "Antigoni Polychroniadou"
      },
      {
        "name" : "Gabriele Cipriani"
      },
      {
        "name" : "Richard Hua"
      },
      {
        "name" : "Tucker Balch"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.07437v1",
    "title" : "Privacy preserving layer partitioning for Deep Neural Network models",
    "summary" : "MLaaS (Machine Learning as a Service) has become popular in the cloud\ncomputing domain, allowing users to leverage cloud resources for running\nprivate inference of ML models on their data. However, ensuring user input\nprivacy and secure inference execution is essential. One of the approaches to\nprotect data privacy and integrity is to use Trusted Execution Environments\n(TEEs) by enabling execution of programs in secure hardware enclave. Using TEEs\ncan introduce significant performance overhead due to the additional layers of\nencryption, decryption, security and integrity checks. This can lead to slower\ninference times compared to running on unprotected hardware. In our work, we\nenhance the runtime performance of ML models by introducing layer partitioning\ntechnique and offloading computations to GPU. The technique comprises two\ndistinct partitions: one executed within the TEE, and the other carried out\nusing a GPU accelerator. Layer partitioning exposes intermediate feature maps\nin the clear which can lead to reconstruction attacks to recover the input. We\nconduct experiments to demonstrate the effectiveness of our approach in\nprotecting against input reconstruction attacks developed using trained\nconditional Generative Adversarial Network(c-GAN). The evaluation is performed\non widely used models such as VGG-16, ResNet-50, and EfficientNetB0, using two\ndatasets: ImageNet for Image classification and TON IoT dataset for\ncybersecurity attack detection.",
    "updated" : "2024-04-11T02:39:48Z",
    "published" : "2024-04-11T02:39:48Z",
    "authors" : [
      {
        "name" : "Kishore Rajasekar"
      },
      {
        "name" : "Randolph Loh"
      },
      {
        "name" : "Kar Wai Fok"
      },
      {
        "name" : "Vrizlynn L. L. Thing"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.07345v1",
    "title" : "Indoor Location Fingerprinting Privacy: A Comprehensive Survey",
    "summary" : "The pervasive integration of Indoor Positioning Systems (IPS) arises from the\nlimitations of Global Navigation Satellite Systems (GNSS) in indoor\nenvironments, leading to the widespread adoption of Location-Based Services\n(LBS). Specifically, indoor location fingerprinting employs diverse signal\nfingerprints from user devices, enabling precise location identification by\nLocation Service Providers (LSP). Despite its broad applications across various\ndomains, indoor location fingerprinting introduces a notable privacy risk, as\nboth LSP and potential adversaries inherently have access to this sensitive\ninformation, compromising users' privacy. Consequently, concerns regarding\nprivacy vulnerabilities in this context necessitate a focused exploration of\nprivacy-preserving mechanisms. In response to these concerns, this survey\npresents a comprehensive review of Privacy-Preserving Mechanisms in Indoor\nLocation Fingerprinting (ILFPPM) based on cryptographic, anonymization,\ndifferential privacy (DP), and federated learning (FL) techniques. We also\npropose a distinctive and novel grouping of privacy vulnerabilities, adversary\nand attack models, and available evaluation metrics specific to indoor location\nfingerprinting systems. Given the identified limitations and research gaps in\nthis survey, we highlight numerous prospective opportunities for future\ninvestigation, aiming to motivate researchers interested in advancing this\nfield. This survey serves as a valuable reference for researchers and provides\na clear overview for those beyond this specific research domain.",
    "updated" : "2024-04-10T21:02:58Z",
    "published" : "2024-04-10T21:02:58Z",
    "authors" : [
      {
        "name" : "Amir Fathalizadeh"
      },
      {
        "name" : "Vahideh Moghtadaiee"
      },
      {
        "name" : "Mina Alishahi"
      }
    ],
    "categories" : [
      "cs.CR",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06721v2",
    "title" : "Poisoning Prevention in Federated Learning and Differential Privacy via\n  Stateful Proofs of Execution",
    "summary" : "The rise in IoT-driven distributed data analytics, coupled with increasing\nprivacy concerns, has led to a demand for effective privacy-preserving and\nfederated data collection/model training mechanisms. In response, approaches\nsuch as Federated Learning (FL) and Local Differential Privacy (LDP) have been\nproposed and attracted much attention over the past few years. However, they\nstill share the common limitation of being vulnerable to poisoning attacks\nwherein adversaries compromising edge devices feed forged (a.k.a. poisoned)\ndata to aggregation back-ends, undermining the integrity of FL/LDP results.\n  In this work, we propose a system-level approach to remedy this issue based\non a novel security notion of Proofs of Stateful Execution (PoSX) for\nIoT/embedded devices' software. To realize the PoSX concept, we design SLAPP: a\nSystem-Level Approach for Poisoning Prevention. SLAPP leverages commodity\nsecurity features of embedded devices - in particular ARM TrustZoneM security\nextensions - to verifiably bind raw sensed data to their correct usage as part\nof FL/LDP edge device routines. As a consequence, it offers robust security\nguarantees against poisoning. Our evaluation, based on real-world prototypes\nfeaturing multiple cryptographic primitives and data collection schemes,\nshowcases SLAPP's security and low overhead.",
    "updated" : "2024-04-11T12:05:52Z",
    "published" : "2024-04-10T04:18:26Z",
    "authors" : [
      {
        "name" : "Norrathep Rattanavipanon"
      },
      {
        "name" : "Ivan De Oliveira Nunes"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06001v2",
    "title" : "Privacy Preserving Prompt Engineering: A Survey",
    "summary" : "Pre-trained language models (PLMs) have demonstrated significant proficiency\nin solving a wide range of general natural language processing (NLP) tasks.\nResearchers have observed a direct correlation between the performance of these\nmodels and their sizes. As a result, the sizes of these models have notably\nexpanded in recent years, persuading researchers to adopt the term large\nlanguage models (LLMs) to characterize the larger-sized PLMs. The size\nexpansion comes with a distinct capability called in-context learning (ICL),\nwhich represents a special form of prompting and allows the models to be\nutilized through the presentation of demonstration examples without\nmodifications to the model parameters. Although interesting, privacy concerns\nhave become a major obstacle in its widespread usage. Multiple studies have\nexamined the privacy risks linked to ICL and prompting in general, and have\ndevised techniques to alleviate these risks. Thus, there is a necessity to\norganize these mitigation techniques for the benefit of the community. This\nsurvey provides a systematic overview of the privacy protection methods\nemployed during ICL and prompting in general. We review, analyze, and compare\ndifferent methods under this paradigm. Furthermore, we provide a summary of the\nresources accessible for the development of these frameworks. Finally, we\ndiscuss the limitations of these frameworks and offer a detailed examination of\nthe promising areas that necessitate further exploration.",
    "updated" : "2024-04-11T00:17:18Z",
    "published" : "2024-04-09T04:11:25Z",
    "authors" : [
      {
        "name" : "Kennedy Edemacu"
      },
      {
        "name" : "Xintao Wu"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.08261v1",
    "title" : "QI-DPFL: Quality-Aware and Incentive-Boosted Federated Learning with\n  Differential Privacy",
    "summary" : "Federated Learning (FL) has increasingly been recognized as an innovative and\nsecure distributed model training paradigm, aiming to coordinate multiple edge\nclients to collaboratively train a shared model without uploading their private\ndatasets. The challenge of encouraging mobile edge devices to participate\nzealously in FL model training procedures, while mitigating the privacy leakage\nrisks during wireless transmission, remains comparatively unexplored so far. In\nthis paper, we propose a novel approach, named QI-DPFL (Quality-Aware and\nIncentive-Boosted Federated Learning with Differential Privacy), to address the\naforementioned intractable issue. To select clients with high-quality datasets,\nwe first propose a quality-aware client selection mechanism based on the Earth\nMover's Distance (EMD) metric. Furthermore, to attract high-quality data\ncontributors, we design an incentive-boosted mechanism that constructs the\ninteractions between the central server and the selected clients as a two-stage\nStackelberg game, where the central server designs the time-dependent reward to\nminimize its cost by considering the trade-off between accuracy loss and total\nreward allocated, and each selected client decides the privacy budget to\nmaximize its utility. The Nash Equilibrium of the Stackelberg game is derived\nto find the optimal solution in each global iteration. The extensive\nexperimental results on different real-world datasets demonstrate the\neffectiveness of our proposed FL framework, by realizing the goal of privacy\nprotection and incentive compatibility.",
    "updated" : "2024-04-12T06:18:25Z",
    "published" : "2024-04-12T06:18:25Z",
    "authors" : [
      {
        "name" : "Wenhao Yuan"
      },
      {
        "name" : "Xuehe Wang"
      }
    ],
    "categories" : [
      "cs.GT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.09724v1",
    "title" : "Privacy-Preserving Federated Unlearning with Certified Client Removal",
    "summary" : "In recent years, Federated Unlearning (FU) has gained attention for\naddressing the removal of a client's influence from the global model in\nFederated Learning (FL) systems, thereby ensuring the ``right to be forgotten\"\n(RTBF). State-of-the-art methods for unlearning use historical data from FL\nclients, such as gradients or locally trained models. However, studies have\nrevealed significant information leakage in this setting, with the possibility\nof reconstructing a user's local data from their uploaded information.\nAddressing this, we propose Starfish, a privacy-preserving federated unlearning\nscheme using Two-Party Computation (2PC) techniques and shared historical\nclient data between two non-colluding servers. Starfish builds upon existing FU\nmethods to ensure privacy in unlearning processes. To enhance the efficiency of\nprivacy-preserving FU evaluations, we suggest 2PC-friendly alternatives for\ncertain FU algorithm operations. We also implement strategies to reduce costs\nassociated with 2PC operations and lessen cumulative approximation errors.\nMoreover, we establish a theoretical bound for the difference between the\nunlearned global model via Starfish and a global model retrained from scratch\nfor certified client removal. Our theoretical and experimental analyses\ndemonstrate that Starfish achieves effective unlearning with reasonable\nefficiency, maintaining privacy and security in FL systems.",
    "updated" : "2024-04-15T12:27:07Z",
    "published" : "2024-04-15T12:27:07Z",
    "authors" : [
      {
        "name" : "Ziyao Liu"
      },
      {
        "name" : "Huanyi Ye"
      },
      {
        "name" : "Yu Jiang"
      },
      {
        "name" : "Jiyuan Shen"
      },
      {
        "name" : "Jiale Guo"
      },
      {
        "name" : "Ivan Tjuawinata"
      },
      {
        "name" : "Kwok-Yan Lam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.09625v1",
    "title" : "Privacy-Preserving Intrusion Detection using Convolutional Neural\n  Networks",
    "summary" : "Privacy-preserving analytics is designed to protect valuable assets. A common\nservice provision involves the input data from the client and the model on the\nanalyst's side. The importance of the privacy preservation is fuelled by legal\nobligations and intellectual property concerns. We explore the use case of a\nmodel owner providing an analytic service on customer's private data. No\ninformation about the data shall be revealed to the analyst and no information\nabout the model shall be leaked to the customer. Current methods involve costs:\naccuracy deterioration and computational complexity. The complexity, in turn,\nresults in a longer processing time, increased requirement on computing\nresources, and involves data communication between the client and the server.\nIn order to deploy such service architecture, we need to evaluate the optimal\nsetting that fits the constraints. And that is what this paper addresses. In\nthis work, we enhance an attack detection system based on Convolutional Neural\nNetworks with privacy-preserving technology based on PriMIA framework that is\ninitially designed for medical data.",
    "updated" : "2024-04-15T09:56:36Z",
    "published" : "2024-04-15T09:56:36Z",
    "authors" : [
      {
        "name" : "Martin Kodys"
      },
      {
        "name" : "Zhongmin Dai"
      },
      {
        "name" : "Vrizlynn L. L. Thing"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.09536v1",
    "title" : "Beyond Noise: Privacy-Preserving Decentralized Learning with Virtual\n  Nodes",
    "summary" : "Decentralized learning (DL) enables collaborative learning without a server\nand without training data leaving the users' devices. However, the models\nshared in DL can still be used to infer training data. Conventional privacy\ndefenses such as differential privacy and secure aggregation fall short in\neffectively safeguarding user privacy in DL. We introduce Shatter, a novel DL\napproach in which nodes create virtual nodes (VNs) to disseminate chunks of\ntheir full model on their behalf. This enhances privacy by (i) preventing\nattackers from collecting full models from other nodes, and (ii) hiding the\nidentity of the original node that produced a given model chunk. We\ntheoretically prove the convergence of Shatter and provide a formal analysis\ndemonstrating how Shatter reduces the efficacy of attacks compared to when\nexchanging full models between participating nodes. We evaluate the convergence\nand attack resilience of Shatter with existing DL algorithms, with\nheterogeneous datasets, and against three standard privacy attacks, including\ngradient inversion. Our evaluation shows that Shatter not only renders these\nprivacy attacks infeasible when each node operates 16 VNs but also exhibits a\npositive impact on model convergence compared to standard DL. This enhanced\nprivacy comes with a manageable increase in communication volume.",
    "updated" : "2024-04-15T07:59:11Z",
    "published" : "2024-04-15T07:59:11Z",
    "authors" : [
      {
        "name" : "Sayan Biswas"
      },
      {
        "name" : "Mathieu Even"
      },
      {
        "name" : "Anne-Marie Kermarrec"
      },
      {
        "name" : "Laurent Massoulie"
      },
      {
        "name" : "Rafael Pires"
      },
      {
        "name" : "Rishi Sharma"
      },
      {
        "name" : "Martijn de Vos"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.09481v1",
    "title" : "SpamDam: Towards Privacy-Preserving and Adversary-Resistant SMS Spam\n  Detection",
    "summary" : "In this study, we introduce SpamDam, a SMS spam detection framework designed\nto overcome key challenges in detecting and understanding SMS spam, such as the\nlack of public SMS spam datasets, increasing privacy concerns of collecting SMS\ndata, and the need for adversary-resistant detection models. SpamDam comprises\nfour innovative modules: an SMS spam radar that identifies spam messages from\nonline social networks(OSNs); an SMS spam inspector for statistical analysis;\nSMS spam detectors(SSDs) that enable both central training and federated\nlearning; and an SSD analyzer that evaluates model resistance against\nadversaries in realistic scenarios. Leveraging SpamDam, we have compiled over\n76K SMS spam messages from Twitter and Weibo between 2018 and 2023, forming the\nlargest dataset of its kind. This dataset has enabled new insights into recent\nspam campaigns and the training of high-performing binary and multi-label\nclassifiers for spam detection. Furthermore, effectiveness of federated\nlearning has been well demonstrated to enable privacy-preserving SMS spam\ndetection. Additionally, we have rigorously tested the adversarial robustness\nof SMS spam detection models, introducing the novel reverse backdoor attack,\nwhich has shown effectiveness and stealthiness in practical tests.",
    "updated" : "2024-04-15T06:07:10Z",
    "published" : "2024-04-15T06:07:10Z",
    "authors" : [
      {
        "name" : "Yekai Li"
      },
      {
        "name" : "Rufan Zhang"
      },
      {
        "name" : "Wenxin Rong"
      },
      {
        "name" : "Xianghang Mi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.09430v1",
    "title" : "On the Efficiency of Privacy Attacks in Federated Learning",
    "summary" : "Recent studies have revealed severe privacy risks in federated learning,\nrepresented by Gradient Leakage Attacks. However, existing studies mainly aim\nat increasing the privacy attack success rate and overlook the high computation\ncosts for recovering private data, making the privacy attack impractical in\nreal applications. In this study, we examine privacy attacks from the\nperspective of efficiency and propose a framework for improving the Efficiency\nof Privacy Attacks in Federated Learning (EPAFL). We make three novel\ncontributions. First, we systematically evaluate the computational costs for\nrepresentative privacy attacks in federated learning, which exhibits a high\npotential to optimize efficiency. Second, we propose three early-stopping\ntechniques to effectively reduce the computational costs of these privacy\nattacks. Third, we perform experiments on benchmark datasets and show that our\nproposed method can significantly reduce computational costs and maintain\ncomparable attack success rates for state-of-the-art privacy attacks in\nfederated learning. We provide the codes on GitHub at\nhttps://github.com/mlsysx/EPAFL.",
    "updated" : "2024-04-15T03:04:37Z",
    "published" : "2024-04-15T03:04:37Z",
    "authors" : [
      {
        "name" : "Nawrin Tabassum"
      },
      {
        "name" : "Ka-Ho Chow"
      },
      {
        "name" : "Xuyu Wang"
      },
      {
        "name" : "Wenbin Zhang"
      },
      {
        "name" : "Yanzhao Wu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.09391v1",
    "title" : "Privacy at a Price: Exploring its Dual Impact on AI Fairness",
    "summary" : "The worldwide adoption of machine learning (ML) and deep learning models,\nparticularly in critical sectors, such as healthcare and finance, presents\nsubstantial challenges in maintaining individual privacy and fairness. These\ntwo elements are vital to a trustworthy environment for learning systems. While\nnumerous studies have concentrated on protecting individual privacy through\ndifferential privacy (DP) mechanisms, emerging research indicates that\ndifferential privacy in machine learning models can unequally impact separate\ndemographic subgroups regarding prediction accuracy. This leads to a fairness\nconcern, and manifests as biased performance. Although the prevailing view is\nthat enhancing privacy intensifies fairness disparities, a smaller, yet\nsignificant, subset of research suggests the opposite view. In this article,\nwith extensive evaluation results, we demonstrate that the impact of\ndifferential privacy on fairness is not monotonous. Instead, we observe that\nthe accuracy disparity initially grows as more DP noise (enhanced privacy) is\nadded to the ML process, but subsequently diminishes at higher privacy levels\nwith even more noise. Moreover, implementing gradient clipping in the\ndifferentially private stochastic gradient descent ML method can mitigate the\nnegative impact of DP noise on fairness. This mitigation is achieved by\nmoderating the disparity growth through a lower clipping threshold.",
    "updated" : "2024-04-15T00:23:41Z",
    "published" : "2024-04-15T00:23:41Z",
    "authors" : [
      {
        "name" : "Mengmeng Yang"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Youyang Qu"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "David Smith"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.08686v1",
    "title" : "Extractive text summarisation of Privacy Policy documents using machine\n  learning approaches",
    "summary" : "This work demonstrates two Privacy Policy (PP) summarisation models based on\ntwo different clustering algorithms: K-means clustering and Pre-determined\nCentroid (PDC) clustering. K-means is decided to be used for the first model\nafter an extensive evaluation of ten commonly used clustering algorithms. The\nsummariser model based on the PDC-clustering algorithm summarises PP documents\nby segregating individual sentences by Euclidean distance from each sentence to\nthe pre-defined cluster centres. The cluster centres are defined according to\nGeneral Data Protection Regulation (GDPR)'s 14 essential topics that must be\nincluded in any privacy notices. The PDC model outperformed the K-means model\nfor two evaluation methods, Sum of Squared Distance (SSD) and ROUGE by some\nmargin (27% and 24% respectively). This result contrasts the K-means model's\nbetter performance in the general clustering of sentence vectors before running\nthe task-specific evaluation. This indicates the effectiveness of operating\ntask-specific fine-tuning measures on unsupervised machine-learning models. The\nsummarisation mechanisms implemented in this paper demonstrates an idea of how\nto efficiently extract essential sentences that should be included in any PP\ndocuments. The summariser models could be further developed to an application\nthat tests the GDPR-compliance (or any data privacy legislation) of PP\ndocuments.",
    "updated" : "2024-04-09T04:54:08Z",
    "published" : "2024-04-09T04:54:08Z",
    "authors" : [
      {
        "name" : "Chanwoo Choi"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04861v2",
    "title" : "Privacy-Preserving Traceable Functional Encryption for Inner Product",
    "summary" : "Functional encryption introduces a new paradigm of public key encryption that\ndecryption only reveals the function value of encrypted data. To curb key\nleakage issues and trace users in FE-IP, a new primitive called traceable\nfunctional encryption for inner product (TFE-IP) has been proposed. However,\nthe privacy protection of user's identities has not been considered in the\nexisting TFE-IP schemes. In order to balance privacy and accountability, we\npropose the concept of privacy-preserving traceable functional encryption for\ninner product (PPTFE-IP) and give a concrete construction. Our scheme provides\nthe following features: (1) To prevent key sharing, a user's key is bound with\nboth his/her identity and a vector; (2) The key generation center (KGC) and a\nuser execute a two-party secure computing protocol to generate a key without\nthe former knowing anything about the latter's identity; (3) Each user can\nverify the correctness of his/her key; (4) A user can calculate the inner\nproduct of the two vectors embedded in his/her key and in a ciphertext; (5)\nOnly the tracer can trace the identity embedded in a key. The security of our\nscheme is formally reduced to well-known complexity assumptions, and the\nimplementation is conducted to evaluate its efficiency. The novelty of our\nscheme is to protect users' privacy and provide traceability if required.",
    "updated" : "2024-04-15T03:22:30Z",
    "published" : "2024-04-07T08:09:46Z",
    "authors" : [
      {
        "name" : "Muyao Qiu"
      },
      {
        "name" : "Jinguang Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03165v2",
    "title" : "Towards Collaborative Family-Centered Design for Online Safety, Privacy\n  and Security",
    "summary" : "Traditional online safety technologies often overly restrict teens and invade\ntheir privacy, while parents often lack knowledge regarding their digital\nprivacy. As such, prior researchers have called for more collaborative\napproaches on adolescent online safety and networked privacy. In this paper, we\npropose family-centered approaches to foster parent-teen collaboration in\nensuring their mobile privacy and online safety while respecting individual\nprivacy, to enhance open discussion and teens' self-regulation. However,\nchallenges such as power imbalances and conflicts with family values arise when\nimplementing such approaches, making parent-teen collaboration difficult.\nTherefore, attending the family-centered design workshop will provide an\ninvaluable opportunity for us to discuss these challenges and identify best\nresearch practices for the future of collaborative online safety and privacy\nwithin families.",
    "updated" : "2024-04-15T00:06:49Z",
    "published" : "2024-04-04T02:34:46Z",
    "authors" : [
      {
        "name" : "Mamtaj Akter"
      },
      {
        "name" : "Zainab Agha"
      },
      {
        "name" : "Ashwaq Alsoubai"
      },
      {
        "name" : "Naima Ali"
      },
      {
        "name" : "Pamela Wisniewski"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.10767v1",
    "title" : "Privacy Can Arise Endogenously in an Economic System with Learning\n  Agents",
    "summary" : "We study price-discrimination games between buyers and a seller where privacy\narises endogenously--that is, utility maximization yields equilibrium\nstrategies where privacy occurs naturally. In this game, buyers with a high\nvaluation for a good have an incentive to keep their valuation private, lest\nthe seller charge them a higher price. This yields an equilibrium where some\nbuyers will send a signal that misrepresents their type with some probability;\nwe refer to this as buyer-induced privacy. When the seller is able to publicly\ncommit to providing a certain privacy level, we find that their equilibrium\nresponse is to commit to ignore buyers' signals with some positive probability;\nwe refer to this as seller-induced privacy. We then turn our attention to a\nrepeated interaction setting where the game parameters are unknown and the\nseller cannot credibly commit to a level of seller-induced privacy. In this\nsetting, players must learn strategies based on information revealed in past\nrounds. We find that, even without commitment ability, seller-induced privacy\narises as a result of reputation building. We characterize the resulting\nseller-induced privacy and seller's utility under no-regret and\nno-policy-regret learning algorithms and verify these results through\nsimulations.",
    "updated" : "2024-04-16T17:51:40Z",
    "published" : "2024-04-16T17:51:40Z",
    "authors" : [
      {
        "name" : "Nivasini Ananthakrishnan"
      },
      {
        "name" : "Tiffany Ding"
      },
      {
        "name" : "Mariel Werner"
      },
      {
        "name" : "Sai Praneeth Karimireddy"
      },
      {
        "name" : "Michael I. Jordan"
      }
    ],
    "categories" : [
      "cs.GT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.10258v1",
    "title" : "CO-oPS: A Mobile App for Community Oversight of Privacy and Security",
    "summary" : "Smartphone users install numerous mobile apps that require access to\ndifferent information from their devices. Much of this information is very\nsensitive, and users often struggle to manage these accesses due to their lack\nof tech expertise and knowledge regarding mobile privacy. Thus, they often seek\nhelp from others to make decisions regarding their mobile privacy and security.\nWe embedded these social processes in a mobile app titled \"CO-oPS'' (\"Community\nOversight for Privacy and Security\"). CO-oPS allows trusted community members\nto review one another's apps installed and permissions granted to those apps.\nCommunity members can provide feedback to one another regarding their privacy\nbehaviors. Users are also allowed to hide some of their mobile apps that they\ndo not like others to see, ensuring their personal privacy.",
    "updated" : "2024-04-16T03:25:43Z",
    "published" : "2024-04-16T03:25:43Z",
    "authors" : [
      {
        "name" : "Mamtaj Akter"
      },
      {
        "name" : "Leena Alghamdi"
      },
      {
        "name" : "Dylan Gillespie"
      },
      {
        "name" : "Nazmus Miazi"
      },
      {
        "name" : "Jess Kropczynski"
      },
      {
        "name" : "Heather Lipford"
      },
      {
        "name" : "Pamela Wisniewski"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.10255v1",
    "title" : "Privacy-Preserving Training-as-a-Service for On-Device Intelligence:\n  Concept, Architectural Scheme, and Open Problems",
    "summary" : "On-device intelligence (ODI) enables artificial intelligence (AI)\napplications to run on end devices, providing real-time and customized AI\nservices without relying on remote servers. However, training models for\non-device deployment face significant challenges due to the decentralized and\nprivacy-sensitive nature of users' data, along with end-side constraints\nrelated to network connectivity, computation efficiency, etc. Existing training\nparadigms, such as cloud-based training, federated learning, and transfer\nlearning, fail to sufficiently address these practical constraints that are\nprevalent for devices. To overcome these challenges, we propose\nPrivacy-Preserving Training-as-a-Service (PTaaS), a novel service computing\nparadigm that provides privacy-friendly, customized AI model training for end\ndevices. PTaaS outsources the core training process to remote and powerful\ncloud or edge servers, efficiently developing customized on-device models based\non uploaded anonymous queries, ensuring data privacy while reducing the\ncomputation load on individual devices. We explore the definition, goals, and\ndesign principles of PTaaS, alongside emerging technologies that support the\nPTaaS paradigm. An architectural scheme for PTaaS is also presented, followed\nby a series of open problems that set the stage for future research directions\nin the field of PTaaS.",
    "updated" : "2024-04-16T03:18:27Z",
    "published" : "2024-04-16T03:18:27Z",
    "authors" : [
      {
        "name" : "Zhiyuan Wu"
      },
      {
        "name" : "Sheng Sun"
      },
      {
        "name" : "Yuwei Wang"
      },
      {
        "name" : "Min Liu"
      },
      {
        "name" : "Bo Gao"
      },
      {
        "name" : "Tianliu He"
      },
      {
        "name" : "Wen Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.09816v1",
    "title" : "FedP3: Federated Personalized and Privacy-friendly Network Pruning under\n  Model Heterogeneity",
    "summary" : "The interest in federated learning has surged in recent research due to its\nunique ability to train a global model using privacy-secured information held\nlocally on each client. This paper pays particular attention to the issue of\nclient-side model heterogeneity, a pervasive challenge in the practical\nimplementation of FL that escalates its complexity. Assuming a scenario where\neach client possesses varied memory storage, processing capabilities and\nnetwork bandwidth - a phenomenon referred to as system heterogeneity - there is\na pressing need to customize a unique model for each client. In response to\nthis, we present an effective and adaptable federated framework FedP3,\nrepresenting Federated Personalized and Privacy-friendly network Pruning,\ntailored for model heterogeneity scenarios. Our proposed methodology can\nincorporate and adapt well-established techniques to its specific instances. We\noffer a theoretical interpretation of FedP3 and its locally\ndifferential-private variant, DP-FedP3, and theoretically validate their\nefficiencies.",
    "updated" : "2024-04-15T14:14:05Z",
    "published" : "2024-04-15T14:14:05Z",
    "authors" : [
      {
        "name" : "Kai Yi"
      },
      {
        "name" : "Nidham Gazagnadou"
      },
      {
        "name" : "Peter Richtárik"
      },
      {
        "name" : "Lingjuan Lyu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.10029v1",
    "title" : "Federated Learning on Riemannian Manifolds with Differential Privacy",
    "summary" : "In recent years, federated learning (FL) has emerged as a prominent paradigm\nin distributed machine learning. Despite the partial safeguarding of agents'\ninformation within FL systems, a malicious adversary can potentially infer\nsensitive information through various means. In this paper, we propose a\ngeneric private FL framework defined on Riemannian manifolds (PriRFed) based on\nthe differential privacy (DP) technique. We analyze the privacy guarantee while\nestablishing the convergence properties. To the best of our knowledge, this is\nthe first federated learning framework on Riemannian manifold with a privacy\nguarantee and convergence results. Numerical simulations are performed on\nsynthetic and real-world datasets to showcase the efficacy of the proposed\nPriRFed approach.",
    "updated" : "2024-04-15T12:32:20Z",
    "published" : "2024-04-15T12:32:20Z",
    "authors" : [
      {
        "name" : "Zhenwei Huang"
      },
      {
        "name" : "Wen Huang"
      },
      {
        "name" : "Pratik Jawanpuria"
      },
      {
        "name" : "Bamdev Mishra"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.CR",
      "cs.LG",
      "68W15, 68P27, 90C30, 90C48"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.10026v1",
    "title" : "Distributed Federated Learning-Based Deep Learning Model for Privacy MRI\n  Brain Tumor Detection",
    "summary" : "Distributed training can facilitate the processing of large medical image\ndatasets, and improve the accuracy and efficiency of disease diagnosis while\nprotecting patient privacy, which is crucial for achieving efficient medical\nimage analysis and accelerating medical research progress. This paper presents\nan innovative approach to medical image classification, leveraging Federated\nLearning (FL) to address the dual challenges of data privacy and efficient\ndisease diagnosis. Traditional Centralized Machine Learning models, despite\ntheir widespread use in medical imaging for tasks such as disease diagnosis,\nraise significant privacy concerns due to the sensitive nature of patient data.\nAs an alternative, FL emerges as a promising solution by allowing the training\nof a collective global model across local clients without centralizing the\ndata, thus preserving privacy. Focusing on the application of FL in Magnetic\nResonance Imaging (MRI) brain tumor detection, this study demonstrates the\neffectiveness of the Federated Learning framework coupled with EfficientNet-B0\nand the FedAvg algorithm in enhancing both privacy and diagnostic accuracy.\nThrough a meticulous selection of preprocessing methods, algorithms, and\nhyperparameters, and a comparative analysis of various Convolutional Neural\nNetwork (CNN) architectures, the research uncovers optimal strategies for image\nclassification. The experimental results reveal that EfficientNet-B0\noutperforms other models like ResNet in handling data heterogeneity and\nachieving higher accuracy and lower loss, highlighting the potential of FL in\novercoming the limitations of traditional models. The study underscores the\nsignificance of addressing data heterogeneity and proposes further research\ndirections for broadening the applicability of FL in medical image analysis.",
    "updated" : "2024-04-15T09:07:19Z",
    "published" : "2024-04-15T09:07:19Z",
    "authors" : [
      {
        "name" : "Lisang Zhou"
      },
      {
        "name" : "Meng Wang"
      },
      {
        "name" : "Ning Zhou"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.11515v1",
    "title" : "Embedding Privacy in Computational Social Science and Artificial\n  Intelligence Research",
    "summary" : "Privacy is a human right. It ensures that individuals are free to engage in\ndiscussions, participate in groups, and form relationships online or offline\nwithout fear of their data being inappropriately harvested, analyzed, or\notherwise used to harm them. Preserving privacy has emerged as a critical\nfactor in research, particularly in the computational social science (CSS),\nartificial intelligence (AI) and data science domains, given their reliance on\nindividuals' data for novel insights. The increasing use of advanced\ncomputational models stands to exacerbate privacy concerns because, if\ninappropriately used, they can quickly infringe privacy rights and lead to\nadverse effects for individuals - especially vulnerable groups - and society.\nWe have already witnessed a host of privacy issues emerge with the advent of\nlarge language models (LLMs), such as ChatGPT, which further demonstrate the\nimportance of embedding privacy from the start. This article contributes to the\nfield by discussing the role of privacy and the primary issues that researchers\nworking in CSS, AI, data science and related domains are likely to face. It\nthen presents several key considerations for researchers to ensure participant\nprivacy is best preserved in their research design, data collection and use,\nanalysis, and dissemination of research results.",
    "updated" : "2024-04-17T16:07:53Z",
    "published" : "2024-04-17T16:07:53Z",
    "authors" : [
      {
        "name" : "Keenan Jones"
      },
      {
        "name" : "Fatima Zahrah"
      },
      {
        "name" : "Jason R. C. Nurse"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CY",
      "cs.ET",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.11470v1",
    "title" : "A Federated Learning Approach to Privacy Preserving Offensive Language\n  Identification",
    "summary" : "The spread of various forms of offensive speech online is an important\nconcern in social media. While platforms have been investing heavily in ways of\ncoping with this problem, the question of privacy remains largely unaddressed.\nModels trained to detect offensive language on social media are trained and/or\nfine-tuned using large amounts of data often stored in centralized servers.\nSince most social media data originates from end users, we propose a privacy\npreserving decentralized architecture for identifying offensive language online\nby introducing Federated Learning (FL) in the context of offensive language\nidentification. FL is a decentralized architecture that allows multiple models\nto be trained locally without the need for data sharing hence preserving users'\nprivacy. We propose a model fusion approach to perform FL. We trained multiple\ndeep learning models on four publicly available English benchmark datasets\n(AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail. We\nalso present initial cross-lingual experiments in English and Spanish. We show\nthat the proposed model fusion approach outperforms baselines in all the\ndatasets while preserving privacy.",
    "updated" : "2024-04-17T15:23:12Z",
    "published" : "2024-04-17T15:23:12Z",
    "authors" : [
      {
        "name" : "Marcos Zampieri"
      },
      {
        "name" : "Damith Premasiri"
      },
      {
        "name" : "Tharindu Ranasinghe"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.11450v1",
    "title" : "Real-Time Trajectory Synthesis with Local Differential Privacy",
    "summary" : "Trajectory streams are being generated from location-aware devices, such as\nsmartphones and in-vehicle navigation systems. Due to the sensitive nature of\nthe location data, directly sharing user trajectories suffers from privacy\nleakage issues. Local differential privacy (LDP), which perturbs sensitive data\non the user side before it is shared or analyzed, emerges as a promising\nsolution for private trajectory stream collection and analysis. Unfortunately,\nexisting stream release approaches often neglect the rich spatial-temporal\ncontext information within trajectory streams, resulting in suboptimal utility\nand limited types of downstream applications. To this end, we propose RetraSyn,\na novel real-time trajectory synthesis framework, which is able to perform\non-the-fly trajectory synthesis based on the mobility patterns privately\nextracted from users' trajectory streams. Thus, the downstream trajectory\nanalysis can be performed on the high-utility synthesized data with privacy\nprotection. We also take the genuine behaviors of real-world mobile travelers\ninto consideration, ensuring authenticity and practicality. The key components\nof RetraSyn include the global mobility model, dynamic mobility update\nmechanism, real-time synthesis, and adaptive allocation strategy. We conduct\nextensive experiments on multiple real-world and synthetic trajectory datasets\nunder various location-based utility metrics, encompassing both streaming and\nhistorical scenarios. The empirical results demonstrate the superiority and\nversatility of our proposed framework.",
    "updated" : "2024-04-17T14:55:49Z",
    "published" : "2024-04-17T14:55:49Z",
    "authors" : [
      {
        "name" : "Yujia Hu"
      },
      {
        "name" : "Yuntao Du"
      },
      {
        "name" : "Zhikun Zhang"
      },
      {
        "name" : "Ziquan Fang"
      },
      {
        "name" : "Lu Chen"
      },
      {
        "name" : "Kai Zheng"
      },
      {
        "name" : "Yunjun Gao"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.11388v1",
    "title" : "Enhancing Data Privacy In Wireless Sensor Networks: Investigating\n  Techniques And Protocols To Protect Privacy Of Data Transmitted Over Wireless\n  Sensor Networks In Critical Applications Of Healthcare And National Security",
    "summary" : "The article discusses the emergence of Wireless Sensor Networks (WSNs) as a\ngroundbreaking technology in data processing and communication. It outlines how\nWSNs, composed of dispersed autonomous sensors, are utilized to monitor\nphysical and environmental factors, transmitting data wirelessly for analysis.\nThe article explores various applications of WSNs in healthcare, national\nsecurity, emergency response, and infrastructure monitoring, highlighting their\nroles in enhancing patient care, public health surveillance, border security,\ndisaster management, and military operations. Additionally, it examines the\nfoundational concepts of data privacy in WSNs, focusing on encryption\ntechniques, authentication mechanisms, anonymization techniques, and access\ncontrol mechanisms. The article also addresses vulnerabilities, threats, and\nchallenges related to data privacy in healthcare and national security\ncontexts, emphasizing regulatory compliance, ethical considerations, and\nsocio-economic factors. Furthermore, it introduces the Diffusion of Innovation\nTheory as a framework for understanding the adoption of privacy-enhancing\ntechnologies in WSNs. Finally, the article reviews empirical studies\ndemonstrating the efficacy of security solutions in preserving data privacy in\nWSNs, offering insights into advancements in safeguarding sensitive\ninformation.",
    "updated" : "2024-04-17T13:48:30Z",
    "published" : "2024-04-17T13:48:30Z",
    "authors" : [
      {
        "name" : "Akinsola Ahmed"
      },
      {
        "name" : "Ejiofor Oluomachi"
      },
      {
        "name" : "Akinde Abdullah"
      },
      {
        "name" : "Njoku Tochukwu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.10995v1",
    "title" : "Clipped SGD Algorithms for Privacy Preserving Performative Prediction:\n  Bias Amplification and Remedies",
    "summary" : "Clipped stochastic gradient descent (SGD) algorithms are among the most\npopular algorithms for privacy preserving optimization that reduces the leakage\nof users' identity in model training. This paper studies the convergence\nproperties of these algorithms in a performative prediction setting, where the\ndata distribution may shift due to the deployed prediction model. For example,\nthe latter is caused by strategical users during the training of loan policy\nfor banks. Our contributions are two-fold. First, we show that the\nstraightforward implementation of a projected clipped SGD (PCSGD) algorithm may\nconverge to a biased solution compared to the performative stable solution. We\nquantify the lower and upper bound for the magnitude of the bias and\ndemonstrate a bias amplification phenomenon where the bias grows with the\nsensitivity of the data distribution. Second, we suggest two remedies to the\nbias amplification effect. The first one utilizes an optimal step size design\nfor PCSGD that takes the privacy guarantee into account. The second one uses\nthe recently proposed DiceSGD algorithm [Zhang et al., 2024]. We show that the\nlatter can successfully remove the bias and converge to the performative stable\nsolution. Numerical experiments verify our analysis.",
    "updated" : "2024-04-17T02:17:05Z",
    "published" : "2024-04-17T02:17:05Z",
    "authors" : [
      {
        "name" : "Qiang Li"
      },
      {
        "name" : "Michal Yemini"
      },
      {
        "name" : "Hoi-To Wai"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.12186v1",
    "title" : "Privacy-Preserving UCB Decision Process Verification via zk-SNARKs",
    "summary" : "With the increasingly widespread application of machine learning, how to\nstrike a balance between protecting the privacy of data and algorithm\nparameters and ensuring the verifiability of machine learning has always been a\nchallenge. This study explores the intersection of reinforcement learning and\ndata privacy, specifically addressing the Multi-Armed Bandit (MAB) problem with\nthe Upper Confidence Bound (UCB) algorithm. We introduce zkUCB, an innovative\nalgorithm that employs the Zero-Knowledge Succinct Non-Interactive Argument of\nKnowledge (zk-SNARKs) to enhance UCB. zkUCB is carefully designed to safeguard\nthe confidentiality of training data and algorithmic parameters, ensuring\ntransparent UCB decision-making. Experiments highlight zkUCB's superior\nperformance, attributing its enhanced reward to judicious quantization bit\nusage that reduces information entropy in the decision-making process. zkUCB's\nproof size and verification time scale linearly with the execution steps of\nzkUCB. This showcases zkUCB's adept balance between data security and\noperational efficiency. This approach contributes significantly to the ongoing\ndiscourse on reinforcing data privacy in complex decision-making processes,\noffering a promising solution for privacy-sensitive applications.",
    "updated" : "2024-04-18T13:49:07Z",
    "published" : "2024-04-18T13:49:07Z",
    "authors" : [
      {
        "name" : "Xikun Jiang"
      },
      {
        "name" : "He Lyu"
      },
      {
        "name" : "Chenhao Ying"
      },
      {
        "name" : "Yibin Xu"
      },
      {
        "name" : "Boris Düdder"
      },
      {
        "name" : "Yuan Luo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.11938v1",
    "title" : "HyDiscGAN: A Hybrid Distributed cGAN for Audio-Visual Privacy\n  Preservation in Multimodal Sentiment Analysis",
    "summary" : "Multimodal Sentiment Analysis (MSA) aims to identify speakers' sentiment\ntendencies in multimodal video content, raising serious concerns about privacy\nrisks associated with multimodal data, such as voiceprints and facial images.\nRecent distributed collaborative learning has been verified as an effective\nparadigm for privacy preservation in multimodal tasks. However, they often\noverlook the privacy distinctions among different modalities, struggling to\nstrike a balance between performance and privacy preservation. Consequently, it\nposes an intriguing question of maximizing multimodal utilization to improve\nperformance while simultaneously protecting necessary modalities. This paper\nforms the first attempt at modality-specified (i.e., audio and visual) privacy\npreservation in MSA tasks. We propose a novel Hybrid Distributed cross-modality\ncGAN framework (HyDiscGAN), which learns multimodality alignment to generate\nfake audio and visual features conditioned on shareable de-identified textual\ndata. The objective is to leverage the fake features to approximate real audio\nand visual content to guarantee privacy preservation while effectively\nenhancing performance. Extensive experiments show that compared with the\nstate-of-the-art MSA model, HyDiscGAN can achieve superior or competitive\nperformance while preserving privacy.",
    "updated" : "2024-04-18T06:38:02Z",
    "published" : "2024-04-18T06:38:02Z",
    "authors" : [
      {
        "name" : "Zhuojia Wu"
      },
      {
        "name" : "Qi Zhang"
      },
      {
        "name" : "Duoqian Miao"
      },
      {
        "name" : "Kun Yi"
      },
      {
        "name" : "Wei Fan"
      },
      {
        "name" : "Liang Hu"
      }
    ],
    "categories" : [
      "cs.MM",
      "cs.DC",
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.12837v1",
    "title" : "Towards a decentralized data privacy protocol for self-sovereignty in\n  the digital world",
    "summary" : "A typical user interacts with many digital services nowadays, providing these\nservices with their data. As of now, the management of privacy preferences is\nservice-centric: Users must manage their privacy preferences according to the\nrules of each service provider, meaning that every provider offers its unique\nmechanisms for users to control their privacy settings. However, managing\nprivacy preferences holistically (i.e., across multiple digital services) is\njust impractical. In this vision paper, we propose a paradigm shift towards an\nenriched user-centric approach for cross-service privacy preferences\nmanagement: the realization of a decentralized data privacy protocol.",
    "updated" : "2024-04-19T12:19:04Z",
    "published" : "2024-04-19T12:19:04Z",
    "authors" : [
      {
        "name" : "Rodrigo Falcão"
      },
      {
        "name" : "Arghavan Hosseinzadeh"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.12730v1",
    "title" : "PATE-TripleGAN: Privacy-Preserving Image Synthesis with Gaussian\n  Differential Privacy",
    "summary" : "Conditional Generative Adversarial Networks (CGANs) exhibit significant\npotential in supervised learning model training by virtue of their ability to\ngenerate realistic labeled images. However, numerous studies have indicated the\nprivacy leakage risk in CGANs models. The solution DPCGAN, incorporating the\ndifferential privacy framework, faces challenges such as heavy reliance on\nlabeled data for model training and potential disruptions to original gradient\ninformation due to excessive gradient clipping, making it difficult to ensure\nmodel accuracy. To address these challenges, we present a privacy-preserving\ntraining framework called PATE-TripleGAN. This framework incorporates a\nclassifier to pre-classify unlabeled data, establishing a three-party min-max\ngame to reduce dependence on labeled data. Furthermore, we present a hybrid\ngradient desensitization algorithm based on the Private Aggregation of Teacher\nEnsembles (PATE) framework and Differential Private Stochastic Gradient Descent\n(DPSGD) method. This algorithm allows the model to retain gradient information\nmore effectively while ensuring privacy protection, thereby enhancing the\nmodel's utility. Privacy analysis and extensive experiments affirm that the\nPATE-TripleGAN model can generate a higher quality labeled image dataset while\nensuring the privacy of the training data.",
    "updated" : "2024-04-19T09:22:20Z",
    "published" : "2024-04-19T09:22:20Z",
    "authors" : [
      {
        "name" : "Zepeng Jiang"
      },
      {
        "name" : "Weiwei Ni"
      },
      {
        "name" : "Yifan Zhang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.13426v1",
    "title" : "Data Privacy Vocabulary (DPV) -- Version 2",
    "summary" : "The Data Privacy Vocabulary (DPV), developed by the W3C Data Privacy\nVocabularies and Controls Community Group (DPVCG), enables the creation of\nmachine-readable, interoperable, and standards-based representations for\ndescribing the processing of personal data. The group has also published\nextensions to the DPV to describe specific applications to support legislative\nrequirements such as the EU's GDPR. The DPV fills a crucial niche in the state\nof the art by providing a vocabulary that can be embedded and used alongside\nother existing standards such as W3C ODRL, and which can be customised and\nextended for adapting to specifics of use-cases or domains. This article\ndescribes the version 2 iteration of the DPV in terms of its contents,\nmethodology, current adoptions and uses, and future potential. It also\ndescribes the relevance and role of DPV in acting as a common vocabulary to\nsupport various regulatory (e.g. EU's DGA and AI Act) and community initiatives\n(e.g. Solid) emerging across the globe.",
    "updated" : "2024-04-20T17:24:33Z",
    "published" : "2024-04-20T17:24:33Z",
    "authors" : [
      {
        "name" : "Harshvardhan J. Pandit"
      },
      {
        "name" : "Beatriz Esteves"
      },
      {
        "name" : "Georg P. Krog"
      },
      {
        "name" : "Paul Ryan"
      },
      {
        "name" : "Delaram Golpayegani"
      },
      {
        "name" : "Julian Flake"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.13407v1",
    "title" : "A Framework for Managing Multifaceted Privacy Leakage While Optimizing\n  Utility in Continuous LBS Interactions",
    "summary" : "Privacy in Location-Based Services (LBS) has become a paramount concern with\nthe ubiquity of mobile devices and the increasing integration of location data\ninto various applications. In this paper, we present several novel\ncontributions aimed at advancing the understanding and management of privacy\nleakage in LBS. Our contributions provides a more comprehensive framework for\nanalyzing privacy concerns across different facets of location-based\ninteractions. Specifically, we introduce $(\\epsilon, \\delta)$-location privacy,\n$(\\epsilon, \\delta, \\theta)$-trajectory privacy, and $(\\epsilon, \\delta,\n\\theta)$-POI privacy, which offer refined mechanisms for quantifying privacy\nrisks associated with location, trajectory, and points of interest when\ncontinuously interacting with LBS. Furthermore, we establish fundamental\nconnections between these privacy notions, facilitating a holistic approach to\nprivacy preservation in LBS. Additionally, we present a lower bound analysis to\nevaluate the utility of the proposed privacy-preserving mechanisms, offering\ninsights into the trade-offs between privacy protection and data utility.\nFinally, we instantiate our framework with the Plannar Isotopic Mechanism to\ndemonstrate its practical applicability while ensuring optimal utility and\nquantifying privacy leakages across various dimensions. The conducted\nevaluations provide a comprehensive insight into the efficacy of our framework\nin capturing privacy loss on location, trajectory, and Points of Interest (POI)\nwhile facilitating quantification of the ensured accuracy.",
    "updated" : "2024-04-20T15:20:01Z",
    "published" : "2024-04-20T15:20:01Z",
    "authors" : [
      {
        "name" : "Anis Bkakria"
      },
      {
        "name" : "Reda Yaich"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.13220v1",
    "title" : "Security and Privacy Product Inclusion",
    "summary" : "In this paper, we explore the challenges of ensuring security and privacy for\nusers from diverse demographic backgrounds. We propose a threat modeling\napproach to identify potential risks and countermeasures for product inclusion\nin security and privacy. We discuss various factors that can affect a user's\nability to achieve a high level of security and privacy, including low-income\ndemographics, poor connectivity, shared device usage, ML fairness, etc. We\npresent results from a global security and privacy user experience survey and\ndiscuss the implications for product developers. Our work highlights the need\nfor a more inclusive approach to security and privacy and provides a framework\nfor researchers and practitioners to consider when designing products and\nservices for a diverse range of users.",
    "updated" : "2024-04-20T00:36:54Z",
    "published" : "2024-04-20T00:36:54Z",
    "authors" : [
      {
        "name" : "Dave Kleidermacher"
      },
      {
        "name" : "Emmanuel Arriaga"
      },
      {
        "name" : "Eric Wang"
      },
      {
        "name" : "Sebastian Porst"
      },
      {
        "name" : "Roger Piqueras Jover"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.13194v1",
    "title" : "Privacy-Preserving Debiasing using Data Augmentation and Machine\n  Unlearning",
    "summary" : "Data augmentation is widely used to mitigate data bias in the training\ndataset. However, data augmentation exposes machine learning models to privacy\nattacks, such as membership inference attacks. In this paper, we propose an\neffective combination of data augmentation and machine unlearning, which can\nreduce data bias while providing a provable defense against known attacks.\nSpecifically, we maintain the fairness of the trained model with\ndiffusion-based data augmentation, and then utilize multi-shard unlearning to\nremove identifying information of original data from the ML model for\nprotection against privacy attacks. Experimental evaluation across diverse\ndatasets demonstrates that our approach can achieve significant improvements in\nbias reduction as well as robustness against state-of-the-art privacy attacks.",
    "updated" : "2024-04-19T21:54:20Z",
    "published" : "2024-04-19T21:54:20Z",
    "authors" : [
      {
        "name" : "Zhixin Pan"
      },
      {
        "name" : "Emma Andrews"
      },
      {
        "name" : "Laura Chang"
      },
      {
        "name" : "Prabhat Mishra"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.13087v1",
    "title" : "Demystifying Legalese: An Automated Approach for Summarizing and\n  Analyzing Overlaps in Privacy Policies and Terms of Service",
    "summary" : "The complexities of legalese in terms and policy documents can bind\nindividuals to contracts they do not fully comprehend, potentially leading to\nuninformed data sharing. Our work seeks to alleviate this issue by developing\nlanguage models that provide automated, accessible summaries and scores for\nsuch documents, aiming to enhance user understanding and facilitate informed\ndecisions. We compared transformer-based and conventional models during\ntraining on our dataset, and RoBERTa performed better overall with a remarkable\n0.74 F1-score. Leveraging our best-performing model, RoBERTa, we highlighted\nredundancies and potential guideline violations by identifying overlaps in\nGDPR-required documents, underscoring the necessity for stricter GDPR\ncompliance.",
    "updated" : "2024-04-17T19:53:59Z",
    "published" : "2024-04-17T19:53:59Z",
    "authors" : [
      {
        "name" : "Shikha Soneji"
      },
      {
        "name" : "Mitchell Hoesing"
      },
      {
        "name" : "Sujay Koujalgi"
      },
      {
        "name" : "Jonathan Dodge"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.LG"
    ]
  }
]