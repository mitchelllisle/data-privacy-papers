[
  {
    "id" : "http://arxiv.org/abs/2404.01619v1",
    "title" : "Making Privacy-preserving Federated Graph Analytics with Strong\n  Guarantees Practical (for Certain Queries)",
    "summary" : "Privacy-preserving federated graph analytics is an emerging area of research.\nThe goal is to run graph analytics queries over a set of devices that are\norganized as a graph while keeping the raw data on the devices rather than\ncentralizing it. Further, no entity may learn any new information except for\nthe final query result. For instance, a device may not learn a neighbor's data.\nThe state-of-the-art prior work for this problem provides privacy guarantees\nfor a broad set of queries in a strong threat model where the devices can be\nmalicious. However, it imposes an impractical overhead: each device locally\nrequires over 8.79 hours of cpu time and 5.73 GiBs of network transfers per\nquery. This paper presents Colo, a new, low-cost system for privacy-preserving\nfederated graph analytics that requires minutes of cpu time and a few MiBs in\nnetwork transfers, for a particular subset of queries. At the heart of Colo is\na new secure computation protocol that enables a device to securely and\nefficiently evaluate a graph query in its local neighborhood while hiding\ndevice data, edge data, and topology data. An implementation and evaluation of\nColo shows that for running a variety of COVID-19 queries over a population of\n1M devices, it requires less than 8.4 minutes of a device's CPU time and 4.93\nMiBs in network transfers - improvements of up to three orders of magnitude.",
    "updated" : "2024-04-02T04:01:31Z",
    "published" : "2024-04-02T04:01:31Z",
    "authors" : [
      {
        "name" : "Kunlong Liu"
      },
      {
        "name" : "Trinabh Gupta"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01283v1",
    "title" : "Evaluating Privacy Perceptions, Experience, and Behavior of Software\n  Development Teams",
    "summary" : "With the increase in the number of privacy regulations, small development\nteams are forced to make privacy decisions on their own. In this paper, we\nconduct a mixed-method survey study, including statistical and qualitative\nanalysis, to evaluate the privacy perceptions, practices, and knowledge of\nmembers involved in various phases of software development (SDLC). Our survey\nincludes 362 participants from 23 countries, encompassing roles such as product\nmanagers, developers, and testers. Our results show diverse definitions of\nprivacy across SDLC roles, emphasizing the need for a holistic privacy approach\nthroughout SDLC. We find that software teams, regardless of their region, are\nless familiar with privacy concepts (such as anonymization), relying on\nself-teaching and forums. Most participants are more familiar with GDPR and\nHIPAA than other regulations, with multi-jurisdictional compliance being their\nprimary concern. Our results advocate the need for role-dependent solutions to\naddress the privacy challenges, and we highlight research directions and\neducational takeaways to help improve privacy-aware software development.",
    "updated" : "2024-04-01T17:55:10Z",
    "published" : "2024-04-01T17:55:10Z",
    "authors" : [
      {
        "name" : "Maxwell Prybylo"
      },
      {
        "name" : "Sara Haghighi"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Sepideh Ghanavati"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01270v1",
    "title" : "Decentralized Collaborative Learning Framework with External Privacy\n  Leakage Analysis",
    "summary" : "This paper presents two methodological advancements in decentralized\nmulti-task learning under privacy constraints, aiming to pave the way for\nfuture developments in next-generation Blockchain platforms. First, we expand\nthe existing framework for collaborative dictionary learning (CollabDict),\nwhich has previously been limited to Gaussian mixture models, by incorporating\ndeep variational autoencoders (VAEs) into the framework, with a particular\nfocus on anomaly detection. We demonstrate that the VAE-based anomaly score\nfunction shares the same mathematical structure as the non-deep model, and\nprovide comprehensive qualitative comparison. Second, considering the\nwidespread use of \"pre-trained models,\" we provide a mathematical analysis on\ndata privacy leakage when models trained with CollabDict are shared externally.\nWe show that the CollabDict approach, when applied to Gaussian mixtures,\nadheres to a Renyi differential privacy criterion. Additionally, we propose a\npractical metric for monitoring internal privacy breaches during the learning\nprocess.",
    "updated" : "2024-04-01T17:46:17Z",
    "published" : "2024-04-01T17:46:17Z",
    "authors" : [
      {
        "name" : "Tsuyoshi Idé"
      },
      {
        "name" : "Dzung T. Phan"
      },
      {
        "name" : "Rudy Raymond"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01231v1",
    "title" : "Privacy Backdoors: Enhancing Membership Inference through Poisoning\n  Pre-trained Models",
    "summary" : "It is commonplace to produce application-specific models by fine-tuning large\npre-trained models using a small bespoke dataset. The widespread availability\nof foundation model checkpoints on the web poses considerable risks, including\nthe vulnerability to backdoor attacks. In this paper, we unveil a new\nvulnerability: the privacy backdoor attack. This black-box privacy attack aims\nto amplify the privacy leakage that arises when fine-tuning a model: when a\nvictim fine-tunes a backdoored model, their training data will be leaked at a\nsignificantly higher rate than if they had fine-tuned a typical model. We\nconduct extensive experiments on various datasets and models, including both\nvision-language models (CLIP) and large language models, demonstrating the\nbroad applicability and effectiveness of such an attack. Additionally, we carry\nout multiple ablation studies with different fine-tuning methods and inference\nstrategies to thoroughly analyze this new threat. Our findings highlight a\ncritical privacy concern within the machine learning community and call for a\nreevaluation of safety protocols in the use of open-source pre-trained models.",
    "updated" : "2024-04-01T16:50:54Z",
    "published" : "2024-04-01T16:50:54Z",
    "authors" : [
      {
        "name" : "Yuxin Wen"
      },
      {
        "name" : "Leo Marchyok"
      },
      {
        "name" : "Sanghyun Hong"
      },
      {
        "name" : "Jonas Geiping"
      },
      {
        "name" : "Tom Goldstein"
      },
      {
        "name" : "Nicholas Carlini"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.00847v1",
    "title" : "Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised\n  Video Anomaly Detection: A New Baseline",
    "summary" : "Unsupervised (US) video anomaly detection (VAD) in surveillance applications\nis gaining more popularity recently due to its practical real-world\napplications. As surveillance videos are privacy sensitive and the availability\nof large-scale video data may enable better US-VAD systems, collaborative\nlearning can be highly rewarding in this setting. However, due to the extremely\nchallenging nature of the US-VAD task, where learning is carried out without\nany annotations, privacy-preserving collaborative learning of US-VAD systems\nhas not been studied yet. In this paper, we propose a new baseline for anomaly\ndetection capable of localizing anomalous events in complex surveillance videos\nin a fully unsupervised fashion without any labels on a privacy-preserving\nparticipant-based distributed training configuration. Additionally, we propose\nthree new evaluation protocols to benchmark anomaly detection approaches on\nvarious scenarios of collaborations and data availability. Based on these\nprotocols, we modify existing VAD datasets to extensively evaluate our approach\nas well as existing US SOTA methods on two large-scale datasets including\nUCF-Crime and XD-Violence. All proposed evaluation protocols, dataset splits,\nand codes are available here: https://github.com/AnasEmad11/CLAP",
    "updated" : "2024-04-01T01:25:06Z",
    "published" : "2024-04-01T01:25:06Z",
    "authors" : [
      {
        "name" : "Anas Al-lahham"
      },
      {
        "name" : "Muhammad Zaigham Zaheer"
      },
      {
        "name" : "Nurbek Tastan"
      },
      {
        "name" : "Karthik Nandakumar"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.02696v1",
    "title" : "Deep Privacy Funnel Model: From a Discriminative to a Generative\n  Approach with an Application to Face Recognition",
    "summary" : "In this study, we apply the information-theoretic Privacy Funnel (PF) model\nto the domain of face recognition, developing a novel method for\nprivacy-preserving representation learning within an end-to-end training\nframework. Our approach addresses the trade-off between obfuscation and utility\nin data protection, quantified through logarithmic loss, also known as\nself-information loss. This research provides a foundational exploration into\nthe integration of information-theoretic privacy principles with representation\nlearning, focusing specifically on the face recognition systems. We\nparticularly highlight the adaptability of our framework with recent\nadvancements in face recognition networks, such as AdaFace and ArcFace. In\naddition, we introduce the Generative Privacy Funnel ($\\mathsf{GenPF}$) model,\na paradigm that extends beyond the traditional scope of the PF model, referred\nto as the Discriminative Privacy Funnel ($\\mathsf{DisPF}$). This\n$\\mathsf{GenPF}$ model brings new perspectives on data generation methods with\nestimation-theoretic and information-theoretic privacy guarantees.\nComplementing these developments, we also present the deep variational PF\n(DVPF) model. This model proposes a tractable variational bound for measuring\ninformation leakage, enhancing the understanding of privacy preservation\nchallenges in deep representation learning. The DVPF model, associated with\nboth $\\mathsf{DisPF}$ and $\\mathsf{GenPF}$ models, sheds light on connections\nwith various generative models such as Variational Autoencoders (VAEs),\nGenerative Adversarial Networks (GANs), and Diffusion models. Complementing our\ntheoretical contributions, we release a reproducible PyTorch package,\nfacilitating further exploration and application of these privacy-preserving\nmethodologies in face recognition systems.",
    "updated" : "2024-04-03T12:50:45Z",
    "published" : "2024-04-03T12:50:45Z",
    "authors" : [
      {
        "name" : "Behrooz Razeghi"
      },
      {
        "name" : "Parsa Rahimi"
      },
      {
        "name" : "Sébastien Marcel"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.02327v1",
    "title" : "Robust Constrained Consensus and Inequality-constrained Distributed\n  Optimization with Guaranteed Differential Privacy and Accurate Convergence",
    "summary" : "We address differential privacy for fully distributed optimization subject to\na shared inequality constraint. By co-designing the distributed optimization\nmechanism and the differential-privacy noise injection mechanism, we propose\nthe first distributed constrained optimization algorithm that can ensure both\nprovable convergence to a global optimal solution and rigorous\n$\\epsilon$-differential privacy, even when the number of iterations tends to\ninfinity. Our approach does not require the Lagrangian function to be strictly\nconvex/concave, and allows the global objective function to be non-separable.\nAs a byproduct of the co-design, we also propose a new constrained consensus\nalgorithm that can achieve rigorous $\\epsilon$-differential privacy while\nmaintaining accurate convergence, which, to our knowledge, has not been\nachieved before. Numerical simulation results on a demand response control\nproblem in smart grid confirm the effectiveness of the proposed approach.",
    "updated" : "2024-04-02T21:53:43Z",
    "published" : "2024-04-02T21:53:43Z",
    "authors" : [
      {
        "name" : "Yongqiang Wang"
      },
      {
        "name" : "Angelia Nedic"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03524v1",
    "title" : "Approximate Gradient Coding for Privacy-Flexible Federated Learning with\n  Non-IID Data",
    "summary" : "This work focuses on the challenges of non-IID data and stragglers/dropouts\nin federated learning. We introduce and explore a privacy-flexible paradigm\nthat models parts of the clients' local data as non-private, offering a more\nversatile and business-oriented perspective on privacy. Within this framework,\nwe propose a data-driven strategy for mitigating the effects of label\nheterogeneity and client straggling on federated learning. Our solution\ncombines both offline data sharing and approximate gradient coding techniques.\nThrough numerical simulations using the MNIST dataset, we demonstrate that our\napproach enables achieving a deliberate trade-off between privacy and utility,\nleading to improved model convergence and accuracy while using an adaptable\nportion of non-private data.",
    "updated" : "2024-04-04T15:29:50Z",
    "published" : "2024-04-04T15:29:50Z",
    "authors" : [
      {
        "name" : "Okko Makkonen"
      },
      {
        "name" : "Sampo Niemelä"
      },
      {
        "name" : "Camilla Hollanti"
      },
      {
        "name" : "Serge Kas Hanna"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03514v1",
    "title" : "Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive\n  Model-Aware Approach",
    "summary" : "Retrieval-augmented large language models (LLMs) have been remarkably\ncompetent in various NLP tasks. Despite their great success, the knowledge\nprovided by the retrieval process is not always useful for improving the model\nprediction, since in some samples LLMs may already be quite knowledgeable and\nthus be able to answer the question correctly without retrieval. Aiming to save\nthe cost of retrieval, previous work has proposed to determine when to do/skip\nthe retrieval in a data-aware manner by analyzing the LLMs' pretraining data.\nHowever, these data-aware methods pose privacy risks and memory limitations,\nespecially when requiring access to sensitive or extensive pretraining data.\nMoreover, these methods offer limited adaptability under fine-tuning or\ncontinual learning settings. We hypothesize that token embeddings are able to\ncapture the model's intrinsic knowledge, which offers a safer and more\nstraightforward way to judge the need for retrieval without the privacy risks\nassociated with accessing pre-training data. Moreover, it alleviates the need\nto retain all the data utilized during model pre-training, necessitating only\nthe upkeep of the token embeddings. Extensive experiments and in-depth analyses\ndemonstrate the superiority of our model-aware approach.",
    "updated" : "2024-04-04T15:21:22Z",
    "published" : "2024-04-04T15:21:22Z",
    "authors" : [
      {
        "name" : "Chengkai Huang"
      },
      {
        "name" : "Rui Wang"
      },
      {
        "name" : "Kaige Xie"
      },
      {
        "name" : "Tong Yu"
      },
      {
        "name" : "Lina Yao"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03509v1",
    "title" : "Privacy-Enhancing Technologies for Artificial Intelligence-Enabled\n  Systems",
    "summary" : "Artificial intelligence (AI) models introduce privacy vulnerabilities to\nsystems. These vulnerabilities may impact model owners or system users; they\nexist during model development, deployment, and inference phases, and threats\ncan be internal or external to the system. In this paper, we investigate\npotential threats and propose the use of several privacy-enhancing technologies\n(PETs) to defend AI-enabled systems. We then provide a framework for PETs\nevaluation for a AI-enabled systems and discuss the impact PETs may have on\nsystem-level variables.",
    "updated" : "2024-04-04T15:14:40Z",
    "published" : "2024-04-04T15:14:40Z",
    "authors" : [
      {
        "name" : "Liv d'Aliberti"
      },
      {
        "name" : "Evan Gronberg"
      },
      {
        "name" : "Joseph Kovba"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03442v1",
    "title" : "Privacy Engineering From Principles to Practice: A Roadmap",
    "summary" : "Privacy engineering is gaining momentum in industry and academia alike. So\nfar, manifold low-level primitives and higher-level methods and strategies have\nsuccessfully been established. Still, fostering adoption in real-world\ninformation systems calls for additional aspects to be consciously considered\nin research and practice.",
    "updated" : "2024-04-04T13:39:49Z",
    "published" : "2024-04-04T13:39:49Z",
    "authors" : [
      {
        "name" : "Frank Pallas"
      },
      {
        "name" : "Katharina Koerner"
      },
      {
        "name" : "Isabel Barberá"
      },
      {
        "name" : "Jaap-Henk Hoepman"
      },
      {
        "name" : "Meiko Jensen"
      },
      {
        "name" : "Nandita Rao Narla"
      },
      {
        "name" : "Nikita Samarin"
      },
      {
        "name" : "Max-R. Ulbricht"
      },
      {
        "name" : "Isabel Wagner"
      },
      {
        "name" : "Kim Wuyts"
      },
      {
        "name" : "Christian Zimmermann"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.SE",
      "K.5.0; H.1.0; D.2.1; D.2.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03324v1",
    "title" : "A Comparative Analysis of Word-Level Metric Differential Privacy:\n  Benchmarking The Privacy-Utility Trade-off",
    "summary" : "The application of Differential Privacy to Natural Language Processing\ntechniques has emerged in relevance in recent years, with an increasing number\nof studies published in established NLP outlets. In particular, the adaptation\nof Differential Privacy for use in NLP tasks has first focused on the\n$\\textit{word-level}$, where calibrated noise is added to word embedding\nvectors to achieve \"noisy\" representations. To this end, several\nimplementations have appeared in the literature, each presenting an alternative\nmethod of achieving word-level Differential Privacy. Although each of these\nincludes its own evaluation, no comparative analysis has been performed to\ninvestigate the performance of such methods relative to each other. In this\nwork, we conduct such an analysis, comparing seven different algorithms on two\nNLP tasks with varying hyperparameters, including the $\\textit{epsilon\n($\\varepsilon$)}$ parameter, or privacy budget. In addition, we provide an\nin-depth analysis of the results with a focus on the privacy-utility trade-off,\nas well as open-source our implementation code for further reproduction. As a\nresult of our analysis, we give insight into the benefits and challenges of\nword-level Differential Privacy, and accordingly, we suggest concrete steps\nforward for the research field.",
    "updated" : "2024-04-04T09:48:14Z",
    "published" : "2024-04-04T09:48:14Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Nihildev Nandakumar"
      },
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03165v1",
    "title" : "Towards Collaborative Family-Centered Design for Online Safety, Privacy\n  and Security",
    "summary" : "Traditional online safety technologies often overly restrict teens and invade\ntheir privacy, while parents often lack knowledge regarding their digital\nprivacy. As such, prior researchers have called for more collaborative\napproaches on adolescent online safety and networked privacy. In this paper, we\npropose family-centered approaches to foster parent-teen collaboration in\nensuring their mobile privacy and online safety while respecting individual\nprivacy, to enhance open discussion and teens' self-regulation. However,\nchallenges such as power imbalances and conflicts with family values arise when\nimplementing such approaches, making parent-teen collaboration difficult.\nTherefore, attending the family-centered design workshop will provide an\ninvaluable opportunity for us to discuss these challenges and identify best\nresearch practices for the future of collaborative online safety and privacy\nwithin families.",
    "updated" : "2024-04-04T02:34:46Z",
    "published" : "2024-04-04T02:34:46Z",
    "authors" : [
      {
        "name" : "Mamtaj Akter"
      },
      {
        "name" : "Zainab Agha"
      },
      {
        "name" : "Ashwaq Alsoubai"
      },
      {
        "name" : "Naima Ali"
      },
      {
        "name" : "Pamela Wisniewski"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04098v1",
    "title" : "You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep\n  Neural Networks",
    "summary" : "Image data have been extensively used in Deep Neural Network (DNN) tasks in\nvarious scenarios, e.g., autonomous driving and medical image analysis, which\nincurs significant privacy concerns. Existing privacy protection techniques are\nunable to efficiently protect such data. For example, Differential Privacy (DP)\nthat is an emerging technique protects data with strong privacy guarantee\ncannot effectively protect visual features of exposed image dataset. In this\npaper, we propose a novel privacy-preserving framework VisualMixer that\nprotects the training data of visual DNN tasks by pixel shuffling, while not\ninjecting any noises. VisualMixer utilizes a new privacy metric called Visual\nFeature Entropy (VFE) to effectively quantify the visual features of an image\nfrom both biological and machine vision aspects. In VisualMixer, we devise a\ntask-agnostic image obfuscation method to protect the visual privacy of data\nfor DNN training and inference. For each image, it determines regions for pixel\nshuffling in the image and the sizes of these regions according to the desired\nVFE. It shuffles pixels both in the spatial domain and in the chromatic channel\nspace in the regions without injecting noises so that it can prevent visual\nfeatures from being discerned and recognized, while incurring negligible\naccuracy loss. Extensive experiments on real-world datasets demonstrate that\nVisualMixer can effectively preserve the visual privacy with negligible\naccuracy loss, i.e., at average 2.35 percentage points of model accuracy loss,\nand almost no performance degradation on model training.",
    "updated" : "2024-04-05T13:49:27Z",
    "published" : "2024-04-05T13:49:27Z",
    "authors" : [
      {
        "name" : "Qiushi Li"
      },
      {
        "name" : "Yan Zhang"
      },
      {
        "name" : "Ju Ren"
      },
      {
        "name" : "Qi Li"
      },
      {
        "name" : "Yaoxue Zhang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04006v1",
    "title" : "From Theory to Comprehension: A Comparative Study of Differential\n  Privacy and $k$-Anonymity",
    "summary" : "The notion of $\\varepsilon$-differential privacy is a widely used concept of\nproviding quantifiable privacy to individuals. However, it is unclear how to\nexplain the level of privacy protection provided by a differential privacy\nmechanism with a set $\\varepsilon$. In this study, we focus on users'\ncomprehension of the privacy protection provided by a differential privacy\nmechanism. To do so, we study three variants of explaining the privacy\nprotection provided by differential privacy: (1) the original mathematical\ndefinition; (2) $\\varepsilon$ translated into a specific privacy risk; and (3)\nan explanation using the randomized response technique. We compare users'\ncomprehension of privacy protection employing these explanatory models with\ntheir comprehension of privacy protection of $k$-anonymity as baseline\ncomprehensibility. Our findings suggest that participants' comprehension of\ndifferential privacy protection is enhanced by the privacy risk model and the\nrandomized response-based model. Moreover, our results confirm our intuition\nthat privacy protection provided by $k$-anonymity is more comprehensible.",
    "updated" : "2024-04-05T10:30:26Z",
    "published" : "2024-04-05T10:30:26Z",
    "authors" : [
      {
        "name" : "Saskia Nuñez von Voigt"
      },
      {
        "name" : "Luise Mehner"
      },
      {
        "name" : "Florian Tschorsch"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03873v1",
    "title" : "PrivShape: Extracting Shapes in Time Series under User-Level Local\n  Differential Privacy",
    "summary" : "Time series have numerous applications in finance, healthcare, IoT, and smart\ncity. In many of these applications, time series typically contain personal\ndata, so privacy infringement may occur if they are released directly to the\npublic. Recently, local differential privacy (LDP) has emerged as the\nstate-of-the-art approach to protecting data privacy. However, existing works\non LDP-based collections cannot preserve the shape of time series. A recent\nwork, PatternLDP, attempts to address this problem, but it can only protect a\nfinite group of elements in a time series due to {\\omega}-event level privacy\nguarantee. In this paper, we propose PrivShape, a trie-based mechanism under\nuser-level LDP to protect all elements. PrivShape first transforms a time\nseries to reduce its length, and then adopts trie-expansion and two-level\nrefinement to improve utility. By extensive experiments on real-world datasets,\nwe demonstrate that PrivShape outperforms PatternLDP when adapted for offline\nuse, and can effectively extract frequent shapes.",
    "updated" : "2024-04-05T03:22:47Z",
    "published" : "2024-04-05T03:22:47Z",
    "authors" : [
      {
        "name" : "Yulian Mao"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Haibo Hu"
      },
      {
        "name" : "Qi Wang"
      },
      {
        "name" : "Kai Huang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  }
]