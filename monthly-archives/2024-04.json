[
  {
    "id" : "http://arxiv.org/abs/2404.01619v1",
    "title" : "Making Privacy-preserving Federated Graph Analytics with Strong\n  Guarantees Practical (for Certain Queries)",
    "summary" : "Privacy-preserving federated graph analytics is an emerging area of research.\nThe goal is to run graph analytics queries over a set of devices that are\norganized as a graph while keeping the raw data on the devices rather than\ncentralizing it. Further, no entity may learn any new information except for\nthe final query result. For instance, a device may not learn a neighbor's data.\nThe state-of-the-art prior work for this problem provides privacy guarantees\nfor a broad set of queries in a strong threat model where the devices can be\nmalicious. However, it imposes an impractical overhead: each device locally\nrequires over 8.79 hours of cpu time and 5.73 GiBs of network transfers per\nquery. This paper presents Colo, a new, low-cost system for privacy-preserving\nfederated graph analytics that requires minutes of cpu time and a few MiBs in\nnetwork transfers, for a particular subset of queries. At the heart of Colo is\na new secure computation protocol that enables a device to securely and\nefficiently evaluate a graph query in its local neighborhood while hiding\ndevice data, edge data, and topology data. An implementation and evaluation of\nColo shows that for running a variety of COVID-19 queries over a population of\n1M devices, it requires less than 8.4 minutes of a device's CPU time and 4.93\nMiBs in network transfers - improvements of up to three orders of magnitude.",
    "updated" : "2024-04-02T04:01:31Z",
    "published" : "2024-04-02T04:01:31Z",
    "authors" : [
      {
        "name" : "Kunlong Liu"
      },
      {
        "name" : "Trinabh Gupta"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01283v1",
    "title" : "Evaluating Privacy Perceptions, Experience, and Behavior of Software\n  Development Teams",
    "summary" : "With the increase in the number of privacy regulations, small development\nteams are forced to make privacy decisions on their own. In this paper, we\nconduct a mixed-method survey study, including statistical and qualitative\nanalysis, to evaluate the privacy perceptions, practices, and knowledge of\nmembers involved in various phases of software development (SDLC). Our survey\nincludes 362 participants from 23 countries, encompassing roles such as product\nmanagers, developers, and testers. Our results show diverse definitions of\nprivacy across SDLC roles, emphasizing the need for a holistic privacy approach\nthroughout SDLC. We find that software teams, regardless of their region, are\nless familiar with privacy concepts (such as anonymization), relying on\nself-teaching and forums. Most participants are more familiar with GDPR and\nHIPAA than other regulations, with multi-jurisdictional compliance being their\nprimary concern. Our results advocate the need for role-dependent solutions to\naddress the privacy challenges, and we highlight research directions and\neducational takeaways to help improve privacy-aware software development.",
    "updated" : "2024-04-01T17:55:10Z",
    "published" : "2024-04-01T17:55:10Z",
    "authors" : [
      {
        "name" : "Maxwell Prybylo"
      },
      {
        "name" : "Sara Haghighi"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Sepideh Ghanavati"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01270v1",
    "title" : "Decentralized Collaborative Learning Framework with External Privacy\n  Leakage Analysis",
    "summary" : "This paper presents two methodological advancements in decentralized\nmulti-task learning under privacy constraints, aiming to pave the way for\nfuture developments in next-generation Blockchain platforms. First, we expand\nthe existing framework for collaborative dictionary learning (CollabDict),\nwhich has previously been limited to Gaussian mixture models, by incorporating\ndeep variational autoencoders (VAEs) into the framework, with a particular\nfocus on anomaly detection. We demonstrate that the VAE-based anomaly score\nfunction shares the same mathematical structure as the non-deep model, and\nprovide comprehensive qualitative comparison. Second, considering the\nwidespread use of \"pre-trained models,\" we provide a mathematical analysis on\ndata privacy leakage when models trained with CollabDict are shared externally.\nWe show that the CollabDict approach, when applied to Gaussian mixtures,\nadheres to a Renyi differential privacy criterion. Additionally, we propose a\npractical metric for monitoring internal privacy breaches during the learning\nprocess.",
    "updated" : "2024-04-01T17:46:17Z",
    "published" : "2024-04-01T17:46:17Z",
    "authors" : [
      {
        "name" : "Tsuyoshi Idé"
      },
      {
        "name" : "Dzung T. Phan"
      },
      {
        "name" : "Rudy Raymond"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01231v1",
    "title" : "Privacy Backdoors: Enhancing Membership Inference through Poisoning\n  Pre-trained Models",
    "summary" : "It is commonplace to produce application-specific models by fine-tuning large\npre-trained models using a small bespoke dataset. The widespread availability\nof foundation model checkpoints on the web poses considerable risks, including\nthe vulnerability to backdoor attacks. In this paper, we unveil a new\nvulnerability: the privacy backdoor attack. This black-box privacy attack aims\nto amplify the privacy leakage that arises when fine-tuning a model: when a\nvictim fine-tunes a backdoored model, their training data will be leaked at a\nsignificantly higher rate than if they had fine-tuned a typical model. We\nconduct extensive experiments on various datasets and models, including both\nvision-language models (CLIP) and large language models, demonstrating the\nbroad applicability and effectiveness of such an attack. Additionally, we carry\nout multiple ablation studies with different fine-tuning methods and inference\nstrategies to thoroughly analyze this new threat. Our findings highlight a\ncritical privacy concern within the machine learning community and call for a\nreevaluation of safety protocols in the use of open-source pre-trained models.",
    "updated" : "2024-04-01T16:50:54Z",
    "published" : "2024-04-01T16:50:54Z",
    "authors" : [
      {
        "name" : "Yuxin Wen"
      },
      {
        "name" : "Leo Marchyok"
      },
      {
        "name" : "Sanghyun Hong"
      },
      {
        "name" : "Jonas Geiping"
      },
      {
        "name" : "Tom Goldstein"
      },
      {
        "name" : "Nicholas Carlini"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.00847v1",
    "title" : "Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised\n  Video Anomaly Detection: A New Baseline",
    "summary" : "Unsupervised (US) video anomaly detection (VAD) in surveillance applications\nis gaining more popularity recently due to its practical real-world\napplications. As surveillance videos are privacy sensitive and the availability\nof large-scale video data may enable better US-VAD systems, collaborative\nlearning can be highly rewarding in this setting. However, due to the extremely\nchallenging nature of the US-VAD task, where learning is carried out without\nany annotations, privacy-preserving collaborative learning of US-VAD systems\nhas not been studied yet. In this paper, we propose a new baseline for anomaly\ndetection capable of localizing anomalous events in complex surveillance videos\nin a fully unsupervised fashion without any labels on a privacy-preserving\nparticipant-based distributed training configuration. Additionally, we propose\nthree new evaluation protocols to benchmark anomaly detection approaches on\nvarious scenarios of collaborations and data availability. Based on these\nprotocols, we modify existing VAD datasets to extensively evaluate our approach\nas well as existing US SOTA methods on two large-scale datasets including\nUCF-Crime and XD-Violence. All proposed evaluation protocols, dataset splits,\nand codes are available here: https://github.com/AnasEmad11/CLAP",
    "updated" : "2024-04-01T01:25:06Z",
    "published" : "2024-04-01T01:25:06Z",
    "authors" : [
      {
        "name" : "Anas Al-lahham"
      },
      {
        "name" : "Muhammad Zaigham Zaheer"
      },
      {
        "name" : "Nurbek Tastan"
      },
      {
        "name" : "Karthik Nandakumar"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.02696v1",
    "title" : "Deep Privacy Funnel Model: From a Discriminative to a Generative\n  Approach with an Application to Face Recognition",
    "summary" : "In this study, we apply the information-theoretic Privacy Funnel (PF) model\nto the domain of face recognition, developing a novel method for\nprivacy-preserving representation learning within an end-to-end training\nframework. Our approach addresses the trade-off between obfuscation and utility\nin data protection, quantified through logarithmic loss, also known as\nself-information loss. This research provides a foundational exploration into\nthe integration of information-theoretic privacy principles with representation\nlearning, focusing specifically on the face recognition systems. We\nparticularly highlight the adaptability of our framework with recent\nadvancements in face recognition networks, such as AdaFace and ArcFace. In\naddition, we introduce the Generative Privacy Funnel ($\\mathsf{GenPF}$) model,\na paradigm that extends beyond the traditional scope of the PF model, referred\nto as the Discriminative Privacy Funnel ($\\mathsf{DisPF}$). This\n$\\mathsf{GenPF}$ model brings new perspectives on data generation methods with\nestimation-theoretic and information-theoretic privacy guarantees.\nComplementing these developments, we also present the deep variational PF\n(DVPF) model. This model proposes a tractable variational bound for measuring\ninformation leakage, enhancing the understanding of privacy preservation\nchallenges in deep representation learning. The DVPF model, associated with\nboth $\\mathsf{DisPF}$ and $\\mathsf{GenPF}$ models, sheds light on connections\nwith various generative models such as Variational Autoencoders (VAEs),\nGenerative Adversarial Networks (GANs), and Diffusion models. Complementing our\ntheoretical contributions, we release a reproducible PyTorch package,\nfacilitating further exploration and application of these privacy-preserving\nmethodologies in face recognition systems.",
    "updated" : "2024-04-03T12:50:45Z",
    "published" : "2024-04-03T12:50:45Z",
    "authors" : [
      {
        "name" : "Behrooz Razeghi"
      },
      {
        "name" : "Parsa Rahimi"
      },
      {
        "name" : "Sébastien Marcel"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.02327v1",
    "title" : "Robust Constrained Consensus and Inequality-constrained Distributed\n  Optimization with Guaranteed Differential Privacy and Accurate Convergence",
    "summary" : "We address differential privacy for fully distributed optimization subject to\na shared inequality constraint. By co-designing the distributed optimization\nmechanism and the differential-privacy noise injection mechanism, we propose\nthe first distributed constrained optimization algorithm that can ensure both\nprovable convergence to a global optimal solution and rigorous\n$\\epsilon$-differential privacy, even when the number of iterations tends to\ninfinity. Our approach does not require the Lagrangian function to be strictly\nconvex/concave, and allows the global objective function to be non-separable.\nAs a byproduct of the co-design, we also propose a new constrained consensus\nalgorithm that can achieve rigorous $\\epsilon$-differential privacy while\nmaintaining accurate convergence, which, to our knowledge, has not been\nachieved before. Numerical simulation results on a demand response control\nproblem in smart grid confirm the effectiveness of the proposed approach.",
    "updated" : "2024-04-02T21:53:43Z",
    "published" : "2024-04-02T21:53:43Z",
    "authors" : [
      {
        "name" : "Yongqiang Wang"
      },
      {
        "name" : "Angelia Nedic"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03524v1",
    "title" : "Approximate Gradient Coding for Privacy-Flexible Federated Learning with\n  Non-IID Data",
    "summary" : "This work focuses on the challenges of non-IID data and stragglers/dropouts\nin federated learning. We introduce and explore a privacy-flexible paradigm\nthat models parts of the clients' local data as non-private, offering a more\nversatile and business-oriented perspective on privacy. Within this framework,\nwe propose a data-driven strategy for mitigating the effects of label\nheterogeneity and client straggling on federated learning. Our solution\ncombines both offline data sharing and approximate gradient coding techniques.\nThrough numerical simulations using the MNIST dataset, we demonstrate that our\napproach enables achieving a deliberate trade-off between privacy and utility,\nleading to improved model convergence and accuracy while using an adaptable\nportion of non-private data.",
    "updated" : "2024-04-04T15:29:50Z",
    "published" : "2024-04-04T15:29:50Z",
    "authors" : [
      {
        "name" : "Okko Makkonen"
      },
      {
        "name" : "Sampo Niemelä"
      },
      {
        "name" : "Camilla Hollanti"
      },
      {
        "name" : "Serge Kas Hanna"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03514v1",
    "title" : "Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive\n  Model-Aware Approach",
    "summary" : "Retrieval-augmented large language models (LLMs) have been remarkably\ncompetent in various NLP tasks. Despite their great success, the knowledge\nprovided by the retrieval process is not always useful for improving the model\nprediction, since in some samples LLMs may already be quite knowledgeable and\nthus be able to answer the question correctly without retrieval. Aiming to save\nthe cost of retrieval, previous work has proposed to determine when to do/skip\nthe retrieval in a data-aware manner by analyzing the LLMs' pretraining data.\nHowever, these data-aware methods pose privacy risks and memory limitations,\nespecially when requiring access to sensitive or extensive pretraining data.\nMoreover, these methods offer limited adaptability under fine-tuning or\ncontinual learning settings. We hypothesize that token embeddings are able to\ncapture the model's intrinsic knowledge, which offers a safer and more\nstraightforward way to judge the need for retrieval without the privacy risks\nassociated with accessing pre-training data. Moreover, it alleviates the need\nto retain all the data utilized during model pre-training, necessitating only\nthe upkeep of the token embeddings. Extensive experiments and in-depth analyses\ndemonstrate the superiority of our model-aware approach.",
    "updated" : "2024-04-04T15:21:22Z",
    "published" : "2024-04-04T15:21:22Z",
    "authors" : [
      {
        "name" : "Chengkai Huang"
      },
      {
        "name" : "Rui Wang"
      },
      {
        "name" : "Kaige Xie"
      },
      {
        "name" : "Tong Yu"
      },
      {
        "name" : "Lina Yao"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03509v1",
    "title" : "Privacy-Enhancing Technologies for Artificial Intelligence-Enabled\n  Systems",
    "summary" : "Artificial intelligence (AI) models introduce privacy vulnerabilities to\nsystems. These vulnerabilities may impact model owners or system users; they\nexist during model development, deployment, and inference phases, and threats\ncan be internal or external to the system. In this paper, we investigate\npotential threats and propose the use of several privacy-enhancing technologies\n(PETs) to defend AI-enabled systems. We then provide a framework for PETs\nevaluation for a AI-enabled systems and discuss the impact PETs may have on\nsystem-level variables.",
    "updated" : "2024-04-04T15:14:40Z",
    "published" : "2024-04-04T15:14:40Z",
    "authors" : [
      {
        "name" : "Liv d'Aliberti"
      },
      {
        "name" : "Evan Gronberg"
      },
      {
        "name" : "Joseph Kovba"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03442v1",
    "title" : "Privacy Engineering From Principles to Practice: A Roadmap",
    "summary" : "Privacy engineering is gaining momentum in industry and academia alike. So\nfar, manifold low-level primitives and higher-level methods and strategies have\nsuccessfully been established. Still, fostering adoption in real-world\ninformation systems calls for additional aspects to be consciously considered\nin research and practice.",
    "updated" : "2024-04-04T13:39:49Z",
    "published" : "2024-04-04T13:39:49Z",
    "authors" : [
      {
        "name" : "Frank Pallas"
      },
      {
        "name" : "Katharina Koerner"
      },
      {
        "name" : "Isabel Barberá"
      },
      {
        "name" : "Jaap-Henk Hoepman"
      },
      {
        "name" : "Meiko Jensen"
      },
      {
        "name" : "Nandita Rao Narla"
      },
      {
        "name" : "Nikita Samarin"
      },
      {
        "name" : "Max-R. Ulbricht"
      },
      {
        "name" : "Isabel Wagner"
      },
      {
        "name" : "Kim Wuyts"
      },
      {
        "name" : "Christian Zimmermann"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.SE",
      "K.5.0; H.1.0; D.2.1; D.2.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03324v1",
    "title" : "A Comparative Analysis of Word-Level Metric Differential Privacy:\n  Benchmarking The Privacy-Utility Trade-off",
    "summary" : "The application of Differential Privacy to Natural Language Processing\ntechniques has emerged in relevance in recent years, with an increasing number\nof studies published in established NLP outlets. In particular, the adaptation\nof Differential Privacy for use in NLP tasks has first focused on the\n$\\textit{word-level}$, where calibrated noise is added to word embedding\nvectors to achieve \"noisy\" representations. To this end, several\nimplementations have appeared in the literature, each presenting an alternative\nmethod of achieving word-level Differential Privacy. Although each of these\nincludes its own evaluation, no comparative analysis has been performed to\ninvestigate the performance of such methods relative to each other. In this\nwork, we conduct such an analysis, comparing seven different algorithms on two\nNLP tasks with varying hyperparameters, including the $\\textit{epsilon\n($\\varepsilon$)}$ parameter, or privacy budget. In addition, we provide an\nin-depth analysis of the results with a focus on the privacy-utility trade-off,\nas well as open-source our implementation code for further reproduction. As a\nresult of our analysis, we give insight into the benefits and challenges of\nword-level Differential Privacy, and accordingly, we suggest concrete steps\nforward for the research field.",
    "updated" : "2024-04-04T09:48:14Z",
    "published" : "2024-04-04T09:48:14Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Nihildev Nandakumar"
      },
      {
        "name" : "Alexandra Klymenko"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03165v1",
    "title" : "Towards Collaborative Family-Centered Design for Online Safety, Privacy\n  and Security",
    "summary" : "Traditional online safety technologies often overly restrict teens and invade\ntheir privacy, while parents often lack knowledge regarding their digital\nprivacy. As such, prior researchers have called for more collaborative\napproaches on adolescent online safety and networked privacy. In this paper, we\npropose family-centered approaches to foster parent-teen collaboration in\nensuring their mobile privacy and online safety while respecting individual\nprivacy, to enhance open discussion and teens' self-regulation. However,\nchallenges such as power imbalances and conflicts with family values arise when\nimplementing such approaches, making parent-teen collaboration difficult.\nTherefore, attending the family-centered design workshop will provide an\ninvaluable opportunity for us to discuss these challenges and identify best\nresearch practices for the future of collaborative online safety and privacy\nwithin families.",
    "updated" : "2024-04-04T02:34:46Z",
    "published" : "2024-04-04T02:34:46Z",
    "authors" : [
      {
        "name" : "Mamtaj Akter"
      },
      {
        "name" : "Zainab Agha"
      },
      {
        "name" : "Ashwaq Alsoubai"
      },
      {
        "name" : "Naima Ali"
      },
      {
        "name" : "Pamela Wisniewski"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04098v1",
    "title" : "You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep\n  Neural Networks",
    "summary" : "Image data have been extensively used in Deep Neural Network (DNN) tasks in\nvarious scenarios, e.g., autonomous driving and medical image analysis, which\nincurs significant privacy concerns. Existing privacy protection techniques are\nunable to efficiently protect such data. For example, Differential Privacy (DP)\nthat is an emerging technique protects data with strong privacy guarantee\ncannot effectively protect visual features of exposed image dataset. In this\npaper, we propose a novel privacy-preserving framework VisualMixer that\nprotects the training data of visual DNN tasks by pixel shuffling, while not\ninjecting any noises. VisualMixer utilizes a new privacy metric called Visual\nFeature Entropy (VFE) to effectively quantify the visual features of an image\nfrom both biological and machine vision aspects. In VisualMixer, we devise a\ntask-agnostic image obfuscation method to protect the visual privacy of data\nfor DNN training and inference. For each image, it determines regions for pixel\nshuffling in the image and the sizes of these regions according to the desired\nVFE. It shuffles pixels both in the spatial domain and in the chromatic channel\nspace in the regions without injecting noises so that it can prevent visual\nfeatures from being discerned and recognized, while incurring negligible\naccuracy loss. Extensive experiments on real-world datasets demonstrate that\nVisualMixer can effectively preserve the visual privacy with negligible\naccuracy loss, i.e., at average 2.35 percentage points of model accuracy loss,\nand almost no performance degradation on model training.",
    "updated" : "2024-04-05T13:49:27Z",
    "published" : "2024-04-05T13:49:27Z",
    "authors" : [
      {
        "name" : "Qiushi Li"
      },
      {
        "name" : "Yan Zhang"
      },
      {
        "name" : "Ju Ren"
      },
      {
        "name" : "Qi Li"
      },
      {
        "name" : "Yaoxue Zhang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04006v1",
    "title" : "From Theory to Comprehension: A Comparative Study of Differential\n  Privacy and $k$-Anonymity",
    "summary" : "The notion of $\\varepsilon$-differential privacy is a widely used concept of\nproviding quantifiable privacy to individuals. However, it is unclear how to\nexplain the level of privacy protection provided by a differential privacy\nmechanism with a set $\\varepsilon$. In this study, we focus on users'\ncomprehension of the privacy protection provided by a differential privacy\nmechanism. To do so, we study three variants of explaining the privacy\nprotection provided by differential privacy: (1) the original mathematical\ndefinition; (2) $\\varepsilon$ translated into a specific privacy risk; and (3)\nan explanation using the randomized response technique. We compare users'\ncomprehension of privacy protection employing these explanatory models with\ntheir comprehension of privacy protection of $k$-anonymity as baseline\ncomprehensibility. Our findings suggest that participants' comprehension of\ndifferential privacy protection is enhanced by the privacy risk model and the\nrandomized response-based model. Moreover, our results confirm our intuition\nthat privacy protection provided by $k$-anonymity is more comprehensible.",
    "updated" : "2024-04-05T10:30:26Z",
    "published" : "2024-04-05T10:30:26Z",
    "authors" : [
      {
        "name" : "Saskia Nuñez von Voigt"
      },
      {
        "name" : "Luise Mehner"
      },
      {
        "name" : "Florian Tschorsch"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.03873v1",
    "title" : "PrivShape: Extracting Shapes in Time Series under User-Level Local\n  Differential Privacy",
    "summary" : "Time series have numerous applications in finance, healthcare, IoT, and smart\ncity. In many of these applications, time series typically contain personal\ndata, so privacy infringement may occur if they are released directly to the\npublic. Recently, local differential privacy (LDP) has emerged as the\nstate-of-the-art approach to protecting data privacy. However, existing works\non LDP-based collections cannot preserve the shape of time series. A recent\nwork, PatternLDP, attempts to address this problem, but it can only protect a\nfinite group of elements in a time series due to {\\omega}-event level privacy\nguarantee. In this paper, we propose PrivShape, a trie-based mechanism under\nuser-level LDP to protect all elements. PrivShape first transforms a time\nseries to reduce its length, and then adopts trie-expansion and two-level\nrefinement to improve utility. By extensive experiments on real-world datasets,\nwe demonstrate that PrivShape outperforms PatternLDP when adapted for offline\nuse, and can effectively extract frequent shapes.",
    "updated" : "2024-04-05T03:22:47Z",
    "published" : "2024-04-05T03:22:47Z",
    "authors" : [
      {
        "name" : "Yulian Mao"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Haibo Hu"
      },
      {
        "name" : "Qi Wang"
      },
      {
        "name" : "Kai Huang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05598v1",
    "title" : "Hook-in Privacy Techniques for gRPC-based Microservice Communication",
    "summary" : "gRPC is at the heart of modern distributed system architectures. Based on\nHTTP/2 and Protocol Buffers, it provides highly performant, standardized, and\npolyglot communication across loosely coupled microservices and is increasingly\npreferred over REST- or GraphQL-based service APIs in practice. Despite its\nwidespread adoption, gRPC lacks any advanced privacy techniques beyond\ntransport encryption and basic token-based authentication. Such advanced\ntechniques are, however, increasingly important for fulfilling regulatory\nrequirements. For instance, anonymizing or otherwise minimizing (personal) data\nbefore responding to requests, or pre-processing data based on the purpose of\nthe access may be crucial in certain usecases. In this paper, we therefore\npropose a novel approach for integrating such advanced privacy techniques into\nthe gRPC framework in a practically viable way. Specifically, we present a\ngeneral approach along with a working prototype that implements privacy\ntechniques, such as data minimization and purpose limitation, in a\nconfigurable, extensible, and gRPC-native way utilizing a gRPC interceptor. We\nalso showcase how to integrate this contribution into a realistic example of a\nfood delivery use case. Alongside these implementations, a preliminary\nperformance evaluation shows practical applicability with reasonable overheads.\nAltogether, we present a viable solution for integrating advanced privacy\ntechniques into real-world gRPC-based microservice architectures, thereby\nfacilitating regulatory compliance ``by design''.",
    "updated" : "2024-04-08T15:18:42Z",
    "published" : "2024-04-08T15:18:42Z",
    "authors" : [
      {
        "name" : "Louis Loechel"
      },
      {
        "name" : "Siar-Remzi Akbayin"
      },
      {
        "name" : "Elias Grünewald"
      },
      {
        "name" : "Jannis Kiesel"
      },
      {
        "name" : "Inga Strelnikova"
      },
      {
        "name" : "Thomas Janke"
      },
      {
        "name" : "Frank Pallas"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.DC",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05257v1",
    "title" : "Sensing-Resistance-Oriented Beamforming for Privacy Protection from ISAC\n  Devices",
    "summary" : "With the evolution of integrated sensing and communication (ISAC) technology,\na growing number of devices go beyond conventional communication functions with\nsensing abilities. Therefore, future networks are divinable to encounter new\nprivacy concerns on sensing, such as the exposure of position information to\nunintended receivers. In contrast to traditional privacy preserving schemes\naiming to prevent eavesdropping, this contribution conceives a novel\nbeamforming design toward sensing resistance (SR). Specifically, we expect to\nguarantee the communication quality while masking the real direction of the SR\ntransmitter during the communication. To evaluate the SR performance, a metric\ntermed angular-domain peak-to-average ratio (ADPAR) is first defined and\nanalyzed. Then, we resort to the null-space technique to conceal the real\ndirection, hence to convert the optimization problem to a more tractable form.\nMoreover, semidefinite relaxation along with index optimization is further\nutilized to obtain the optimal beamformer. Finally, simulation results\ndemonstrate the feasibility of the proposed SR-oriented beamforming design\ntoward privacy protection from ISAC receivers.",
    "updated" : "2024-04-08T07:45:10Z",
    "published" : "2024-04-08T07:45:10Z",
    "authors" : [
      {
        "name" : "Teng Ma"
      },
      {
        "name" : "Yue Xiao"
      },
      {
        "name" : "Xia Lei"
      },
      {
        "name" : "Ming Xiao"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05130v1",
    "title" : "Enabling Privacy-Preserving Cyber Threat Detection with Federated\n  Learning",
    "summary" : "Despite achieving good performance and wide adoption, machine learning based\nsecurity detection models (e.g., malware classifiers) are subject to concept\ndrift and evasive evolution of attackers, which renders up-to-date threat data\nas a necessity. However, due to enforcement of various privacy protection\nregulations (e.g., GDPR), it is becoming increasingly challenging or even\nprohibitive for security vendors to collect individual-relevant and\nprivacy-sensitive threat datasets, e.g., SMS spam/non-spam messages from mobile\ndevices. To address such obstacles, this study systematically profiles the\n(in)feasibility of federated learning for privacy-preserving cyber threat\ndetection in terms of effectiveness, byzantine resilience, and efficiency. This\nis made possible by the build-up of multiple threat datasets and threat\ndetection models, and more importantly, the design of realistic and\nsecurity-specific experiments.\n  We evaluate FL on two representative threat detection tasks, namely SMS spam\ndetection and Android malware detection. It shows that FL-trained detection\nmodels can achieve a performance that is comparable to centrally trained\ncounterparts. Also, most non-IID data distributions have either minor or\nnegligible impact on the model performance, while a label-based non-IID\ndistribution of a high extent can incur non-negligible fluctuation and delay in\nFL training. Then, under a realistic threat model, FL turns out to be\nadversary-resistant to attacks of both data poisoning and model poisoning.\nParticularly, the attacking impact of a practical data poisoning attack is no\nmore than 0.14\\% loss in model accuracy. Regarding FL efficiency, a\nbootstrapping strategy turns out to be effective to mitigate the training delay\nas observed in label-based non-IID scenarios.",
    "updated" : "2024-04-08T01:16:56Z",
    "published" : "2024-04-08T01:16:56Z",
    "authors" : [
      {
        "name" : "Yu Bi"
      },
      {
        "name" : "Yekai Li"
      },
      {
        "name" : "Xuan Feng"
      },
      {
        "name" : "Xianghang Mi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05049v1",
    "title" : "PlateSegFL: A Privacy-Preserving License Plate Detection Using Federated\n  Segmentation Learning",
    "summary" : "Automatic License Plate Recognition (ALPR) is an integral component of an\nintelligent transport system with extensive applications in secure\ntransportation, vehicle-to-vehicle communication, stolen vehicles detection,\ntraffic violations, and traffic flow management. The existing license plate\ndetection system focuses on one-shot learners or pre-trained models that\noperate with a geometric bounding box, limiting the model's performance.\nFurthermore, continuous video data streams uploaded to the central server\nresult in network and complexity issues. To combat this, PlateSegFL was\nintroduced, which implements U-Net-based segmentation along with Federated\nLearning (FL). U-Net is well-suited for multi-class image segmentation tasks\nbecause it can analyze a large number of classes and generate a pixel-level\nsegmentation map for each class. Federated Learning is used to reduce the\nquantity of data required while safeguarding the user's privacy. Different\ncomputing platforms, such as mobile phones, are able to collaborate on the\ndevelopment of a standard prediction model where it makes efficient use of\none's time; incorporates more diverse data; delivers projections in real-time;\nand requires no physical effort from the user; resulting around 95% F1 score.",
    "updated" : "2024-04-07T19:10:02Z",
    "published" : "2024-04-07T19:10:02Z",
    "authors" : [
      {
        "name" : "Md. Shahriar Rahman Anuvab"
      },
      {
        "name" : "Mishkat Sultana"
      },
      {
        "name" : "Md. Atif Hossain"
      },
      {
        "name" : "Shashwata Das"
      },
      {
        "name" : "Suvarthi Chowdhury"
      },
      {
        "name" : "Rafeed Rahman"
      },
      {
        "name" : "Dibyo Fabian Dofadar"
      },
      {
        "name" : "Shahriar Rahman Rana"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05047v1",
    "title" : "Initial Exploration of Zero-Shot Privacy Utility Tradeoffs in Tabular\n  Data Using GPT-4",
    "summary" : "We investigate the application of large language models (LLMs), specifically\nGPT-4, to scenarios involving the tradeoff between privacy and utility in\ntabular data. Our approach entails prompting GPT-4 by transforming tabular data\npoints into textual format, followed by the inclusion of precise sanitization\ninstructions in a zero-shot manner. The primary objective is to sanitize the\ntabular data in such a way that it hinders existing machine learning models\nfrom accurately inferring private features while allowing models to accurately\ninfer utility-related attributes. We explore various sanitization instructions.\nNotably, we discover that this relatively simple approach yields performance\ncomparable to more complex adversarial optimization methods used for managing\nprivacy-utility tradeoffs. Furthermore, while the prompts successfully obscure\nprivate features from the detection capabilities of existing machine learning\nmodels, we observe that this obscuration alone does not necessarily meet a\nrange of fairness metrics. Nevertheless, our research indicates the potential\neffectiveness of LLMs in adhering to these fairness metrics, with some of our\nexperimental results aligning with those achieved by well-established\nadversarial optimization techniques.",
    "updated" : "2024-04-07T19:02:50Z",
    "published" : "2024-04-07T19:02:50Z",
    "authors" : [
      {
        "name" : "Bishwas Mandal"
      },
      {
        "name" : "George Amariucai"
      },
      {
        "name" : "Shuangqing Wei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05043v1",
    "title" : "Optimizing Privacy and Utility Tradeoffs for Group Interests Through\n  Harmonization",
    "summary" : "We propose a novel problem formulation to address the privacy-utility\ntradeoff, specifically when dealing with two distinct user groups characterized\nby unique sets of private and utility attributes. Unlike previous studies that\nprimarily focus on scenarios where all users share identical private and\nutility attributes and often rely on auxiliary datasets or manual annotations,\nwe introduce a collaborative data-sharing mechanism between two user groups\nthrough a trusted third party. This third party uses adversarial privacy\ntechniques with our proposed data-sharing mechanism to internally sanitize data\nfor both groups and eliminates the need for manual annotation or auxiliary\ndatasets. Our methodology ensures that private attributes cannot be accurately\ninferred while enabling highly accurate predictions of utility features.\nImportantly, even if analysts or adversaries possess auxiliary datasets\ncontaining raw data, they are unable to accurately deduce private features.\nAdditionally, our data-sharing mechanism is compatible with various existing\nadversarially trained privacy techniques. We empirically demonstrate the\neffectiveness of our approach using synthetic and real-world datasets,\nshowcasing its ability to balance the conflicting goals of privacy and utility.",
    "updated" : "2024-04-07T18:55:33Z",
    "published" : "2024-04-07T18:55:33Z",
    "authors" : [
      {
        "name" : "Bishwas Mandal"
      },
      {
        "name" : "George Amariucai"
      },
      {
        "name" : "Shuangqing Wei"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04861v1",
    "title" : "Privacy-Preserving Traceable Functional Encryption for Inner Product",
    "summary" : "Functional encryption introduces a new paradigm of public key encryption that\ndecryption only reveals the function value of encrypted data. To curb key\nleakage issues and trace users in FE-IP, a new primitive called traceable\nfunctional encryption for inner product (TFE-IP) has been proposed. However,\nthe privacy protection of user's identities has not been considered in the\nexisting TFE-IP schemes. In order to balance privacy and accountability, we\npropose the concept of privacy-preserving traceable functional encryption for\ninner product (PPTFE-IP) and give a concrete construction. Our scheme provides\nthe following features: (1) To prevent key sharing, a user's key is bound with\nboth his/her identity and a vector; (2) The key generation center (KGC) and a\nuser execute a two-party secure computing protocol to generate a key without\nthe former knowing anything about the latter's identity; (3) Each user can\nverify the correctness of his/her key; (4) A user can calculate the inner\nproduct of the two vectors embedded in his/her key and in a ciphertext; (5)\nOnly the tracer can trace the identity embedded in a key. The security of our\nscheme is formally reduced to well-known complexity assumptions, and the\nimplementation is conducted to evaluate its efficiency. The novelty of our\nscheme is to protect users' privacy and provide traceability if required.",
    "updated" : "2024-04-07T08:09:46Z",
    "published" : "2024-04-07T08:09:46Z",
    "authors" : [
      {
        "name" : "Muyao Qiu"
      },
      {
        "name" : "Jinguang Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04769v1",
    "title" : "Safeguarding Voice Privacy: Harnessing Near-Ultrasonic Interference To\n  Protect Against Unauthorized Audio Recording",
    "summary" : "The widespread adoption of voice-activated systems has modified routine\nhuman-machine interaction but has also introduced new vulnerabilities. This\npaper investigates the susceptibility of automatic speech recognition (ASR)\nalgorithms in these systems to interference from near-ultrasonic noise.\nBuilding upon prior research that demonstrated the ability of near-ultrasonic\nfrequencies (16 kHz - 22 kHz) to exploit the inherent properties of\nmicroelectromechanical systems (MEMS) microphones, our study explores\nalternative privacy enforcement means using this interference phenomenon. We\nexpose a critical vulnerability in the most common microphones used in modern\nvoice-activated devices, which inadvertently demodulate near-ultrasonic\nfrequencies into the audible spectrum, disrupting the ASR process. Through a\nsystematic analysis of the impact of near-ultrasonic noise on various ASR\nsystems, we demonstrate that this vulnerability is consistent across different\ndevices and under varying conditions, such as broadcast distance and specific\nphoneme structures. Our findings highlight the need to develop robust\ncountermeasures to protect voice-activated systems from malicious exploitation\nof this vulnerability. Furthermore, we explore the potential applications of\nthis phenomenon in enhancing privacy by disrupting unauthorized audio recording\nor eavesdropping. This research underscores the importance of a comprehensive\napproach to securing voice-activated systems, combining technological\ninnovation, responsible development practices, and informed policy decisions to\nensure the privacy and security of users in an increasingly connected world.",
    "updated" : "2024-04-07T00:49:19Z",
    "published" : "2024-04-07T00:49:19Z",
    "authors" : [
      {
        "name" : "Forrest McKee"
      },
      {
        "name" : "David Noever"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.04706v1",
    "title" : "Advances in Differential Privacy and Differentially Private Machine\n  Learning",
    "summary" : "There has been an explosion of research on differential privacy (DP) and its\nvarious applications in recent years, ranging from novel variants and\naccounting techniques in differential privacy to the thriving field of\ndifferentially private machine learning (DPML) to newer implementations in\npractice, like those by various companies and organisations such as census\nbureaus. Most recent surveys focus on the applications of differential privacy\nin particular contexts like data publishing, specific machine learning tasks,\nanalysis of unstructured data, location privacy, etc. This work thus seeks to\nfill the gap for a survey that primarily discusses recent developments in the\ntheory of differential privacy along with newer DP variants, viz. Renyi DP and\nConcentrated DP, novel mechanisms and techniques, and the theoretical\ndevelopments in differentially private machine learning in proper detail. In\naddition, this survey discusses its applications to privacy-preserving machine\nlearning in practice and a few practical implementations of DP.",
    "updated" : "2024-04-06T18:49:24Z",
    "published" : "2024-04-06T18:49:24Z",
    "authors" : [
      {
        "name" : "Saswat Das"
      },
      {
        "name" : "Subhankar Mishra"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06216v1",
    "title" : "Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking",
    "summary" : "As eye tracking becomes pervasive with screen-based devices and head-mounted\ndisplays, privacy concerns regarding eye-tracking data have escalated. While\nstate-of-the-art approaches for privacy-preserving eye tracking mostly involve\ndifferential privacy and empirical data manipulations, previous research has\nnot focused on methods for scanpaths. We introduce a novel privacy-preserving\nscanpath comparison protocol designed for the widely used Needleman-Wunsch\nalgorithm, a generalized version of the edit distance algorithm. Particularly,\nby incorporating the Paillier homomorphic encryption scheme, our protocol\nensures that no private information is revealed. Furthermore, we introduce a\nrandom processing strategy and a multi-layered masking method to obfuscate the\nvalues while preserving the original order of encrypted editing operation\ncosts. This minimizes communication overhead, requiring a single communication\nround for each iteration of the Needleman-Wunsch process. We demonstrate the\nefficiency and applicability of our protocol on three publicly available\ndatasets with comprehensive computational performance analyses and make our\nsource code publicly accessible.",
    "updated" : "2024-04-09T11:07:57Z",
    "published" : "2024-04-09T11:07:57Z",
    "authors" : [
      {
        "name" : "Suleyman Ozdel"
      },
      {
        "name" : "Efe Bozkir"
      },
      {
        "name" : "Enkelejda Kasneci"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06144v1",
    "title" : "Differential Privacy for Anomaly Detection: Analyzing the Trade-off\n  Between Privacy and Explainability",
    "summary" : "Anomaly detection (AD), also referred to as outlier detection, is a\nstatistical process aimed at identifying observations within a dataset that\nsignificantly deviate from the expected pattern of the majority of the data.\nSuch a process finds wide application in various fields, such as finance and\nhealthcare. While the primary objective of AD is to yield high detection\naccuracy, the requirements of explainability and privacy are also paramount.\nThe first ensures the transparency of the AD process, while the second\nguarantees that no sensitive information is leaked to untrusted parties. In\nthis work, we exploit the trade-off of applying Explainable AI (XAI) through\nSHapley Additive exPlanations (SHAP) and differential privacy (DP). We perform\nAD with different models and on various datasets, and we thoroughly evaluate\nthe cost of privacy in terms of decreased accuracy and explainability. Our\nresults show that the enforcement of privacy through DP has a significant\nimpact on detection accuracy and explainability, which depends on both the\ndataset and the considered AD model. We further show that the visual\ninterpretation of explanations is also influenced by the choice of the AD\nalgorithm.",
    "updated" : "2024-04-09T09:09:36Z",
    "published" : "2024-04-09T09:09:36Z",
    "authors" : [
      {
        "name" : "Fatima Ezzeddine"
      },
      {
        "name" : "Mirna Saad"
      },
      {
        "name" : "Omran Ayoub"
      },
      {
        "name" : "Davide Andreoletti"
      },
      {
        "name" : "Martin Gjoreski"
      },
      {
        "name" : "Ihab Sbeity"
      },
      {
        "name" : "Marc Langheinrich"
      },
      {
        "name" : "Silvia Giordano"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06001v1",
    "title" : "Privacy Preserving Prompt Engineering: A Survey",
    "summary" : "Pre-trained language models (PLMs) have demonstrated significant proficiency\nin solving a wide range of general natural language processing (NLP) tasks.\nResearchers have observed a direct correlation between the performance of these\nmodels and their sizes. As a result, the sizes of these models have notably\nexpanded in recent years, persuading researchers to adopt the term large\nlanguage models (LLMs) to characterize the larger-sized PLMs. The increased\nsize is accompanied by a distinct capability known as in-context learning\n(ICL), which represents a specialized form of prompting. This enables the\nutilization of LLMs for specific downstream tasks by presenting them with\ndemonstration examples while keeping the model parameters frozen. Although\ninteresting, privacy concerns have become a major obstacle in its widespread\nusage. Multiple studies have examined the privacy risks linked to ICL and\nprompting in general, and have devised techniques to alleviate these risks.\nThus, there is a necessity to organize these mitigation techniques for the\nbenefit of the community. This survey provides a systematic overview of the\nprivacy protection methods employed during ICL and prompting in general. We\nreview, analyze, and compare different methods under this paradigm.\nFurthermore, we provide a summary of the resources accessible for the\ndevelopment of these frameworks. Finally, we discuss the limitations of these\nframeworks and offer a detailed examination of the promising areas that\nnecessitate further exploration.",
    "updated" : "2024-04-09T04:11:25Z",
    "published" : "2024-04-09T04:11:25Z",
    "authors" : [
      {
        "name" : "Kennedy Edemacu"
      },
      {
        "name" : "Xintao Wu"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05876v1",
    "title" : "Privacy and Security of Women's Reproductive Health Apps in a Changing\n  Legal Landscape",
    "summary" : "FemTech, a rising trend in mobile apps, empowers women to digitally manage\ntheir health and family planning. However, privacy and security vulnerabilities\nin period-tracking and fertility-monitoring apps present significant risks,\nsuch as unintended pregnancies and legal consequences. Our approach involves\nmanual observations of privacy policies and app permissions, along with dynamic\nand static analysis using multiple evaluation frameworks. Our research reveals\nthat many of these apps gather personally identifiable information (PII) and\nsensitive healthcare data. Furthermore, our analysis identifies that 61% of the\ncode vulnerabilities found in the apps are classified under the top-ten Open\nWeb Application Security Project (OWASP) vulnerabilities. Our research\nemphasizes the significance of tackling the privacy and security\nvulnerabilities present in period-tracking and fertility-monitoring mobile\napps. By highlighting these crucial risks, we aim to initiate a vital\ndiscussion and advocate for increased accountability and transparency of\ndigital tools for women's health. We encourage the industry to prioritize user\nprivacy and security, ultimately promoting a safer and more secure environment\nfor women's health management.",
    "updated" : "2024-04-08T21:19:10Z",
    "published" : "2024-04-08T21:19:10Z",
    "authors" : [
      {
        "name" : "Shalini Saini"
      },
      {
        "name" : "Nitesh Saxena"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.05828v1",
    "title" : "Privacy-Preserving Deep Learning Using Deformable Operators for Secure\n  Task Learning",
    "summary" : "In the era of cloud computing and data-driven applications, it is crucial to\nprotect sensitive information to maintain data privacy, ensuring truly reliable\nsystems. As a result, preserving privacy in deep learning systems has become a\ncritical concern. Existing methods for privacy preservation rely on image\nencryption or perceptual transformation approaches. However, they often suffer\nfrom reduced task performance and high computational costs. To address these\nchallenges, we propose a novel Privacy-Preserving framework that uses a set of\ndeformable operators for secure task learning. Our method involves shuffling\npixels during the analog-to-digital conversion process to generate visually\nprotected data. Those are then fed into a well-known network enhanced with\ndeformable operators. Using our approach, users can achieve equivalent\nperformance to original images without additional training using a secret key.\nMoreover, our method enables access control against unauthorized users.\nExperimental results demonstrate the efficacy of our approach, showcasing its\npotential in cloud-based scenarios and privacy-sensitive applications.",
    "updated" : "2024-04-08T19:46:20Z",
    "published" : "2024-04-08T19:46:20Z",
    "authors" : [
      {
        "name" : "Fabian Perez"
      },
      {
        "name" : "Jhon Lopez"
      },
      {
        "name" : "Henry Arguello"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06868v1",
    "title" : "The 'Sandwich' meta-framework for architecture agnostic deep\n  privacy-preserving transfer learning for non-invasive brainwave decoding",
    "summary" : "Machine learning has enhanced the performance of decoding signals indicating\nhuman behaviour. EEG decoding, as an exemplar indicating neural activity and\nhuman thoughts non-invasively, has been helpful in neural activity analysis and\naiding patients via brain-computer interfaces. However, training machine\nlearning algorithms on EEG encounters two primary challenges: variability\nacross data sets and privacy concerns using data from individuals and data\ncentres. Our objective is to address these challenges by integrating transfer\nlearning for data variability and federated learning for data privacy into a\nunified approach. We introduce the Sandwich as a novel deep privacy-preserving\nmeta-framework combining transfer learning and federated learning. The Sandwich\nframework comprises three components: federated networks (first layers) that\nhandle data set differences at the input level, a shared network (middle layer)\nlearning common rules and applying transfer learning, and individual\nclassifiers (final layers) for specific tasks of each data set. It enables the\ncentral network (central server) to benefit from multiple data sets, while\nlocal branches (local servers) maintain data and label privacy. We evaluated\nthe `Sandwich' meta-architecture in various configurations using the BEETL\nmotor imagery challenge, a benchmark for heterogeneous EEG data sets. Compared\nwith baseline models, our `Sandwich' implementations showed superior\nperformance. The best-performing model, the Inception Sandwich with deep set\nalignment (Inception-SD-Deepset), exceeded baseline methods by 9%. The\n`Sandwich' framework demonstrates significant advancements in federated deep\ntransfer learning for diverse tasks and data sets. It outperforms conventional\ndeep learning methods, showcasing the potential for effective use of larger,\nheterogeneous data sets with enhanced privacy as a model-agnostic\nmeta-framework.",
    "updated" : "2024-04-10T09:47:14Z",
    "published" : "2024-04-10T09:47:14Z",
    "authors" : [
      {
        "name" : "Xiaoxi Wei"
      },
      {
        "name" : "Jyotindra Narayan"
      },
      {
        "name" : "A. Aldo Faisal"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06721v1",
    "title" : "Poisoning Prevention in Federated Learning and Differential Privacy via\n  Stateful Proofs of Execution",
    "summary" : "The rise in IoT-driven distributed data analytics, coupled with increasing\nprivacy concerns, has led to a demand for effective privacy-preserving and\nfederated data collection/model training mechanisms. In response, approaches\nsuch as Federated Learning (FL) and Local Differential Privacy (LDP) have been\nproposed and attracted much attention over the past few years. However, they\nstill share the common limitation of being vulnerable to poisoning attacks\nwherein adversaries compromising edge devices feed forged (a.k.a. poisoned)\ndata to aggregation back-ends, undermining the integrity of FL/LDP results.\n  In this work, we propose a system-level approach to remedy this issue based\non a novel security notion of Proofs of Stateful Execution (PoSX) for\nIoT/embedded devices' software. To realize the PoSX concept, we design SLAPP: a\nSystem-Level Approach for Poisoning Prevention. SLAPP leverages commodity\nsecurity features of embedded devices - in particular ARM TrustZoneM security\nextensions - to verifiably bind raw sensed data to their correct usage as part\nof FL/LDP edge device routines. As a consequence, it offers robust security\nguarantees against poisoning. Our evaluation, based on real-world prototypes\nfeaturing multiple cryptographic primitives and data collection schemes,\nshowcases SLAPP's security and low overhead.",
    "updated" : "2024-04-10T04:18:26Z",
    "published" : "2024-04-10T04:18:26Z",
    "authors" : [
      {
        "name" : "Norrathep Rattanavipanon"
      },
      {
        "name" : "Ivan de Oliviera Nunes"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.06686v1",
    "title" : "Atlas-X Equity Financing: Unlocking New Methods to Securely Obfuscate\n  Axe Inventory Data Based on Differential Privacy",
    "summary" : "Banks publish daily a list of available securities/assets (axe list) to\nselected clients to help them effectively locate Long (buy) or Short (sell)\ntrades at reduced financing rates. This reduces costs for the bank, as the list\naggregates the bank's internal firm inventory per asset for all clients of long\nas well as short trades. However, this is somewhat problematic: (1) the bank's\ninventory is revealed; (2) trades of clients who contribute to the aggregated\nlist, particularly those deemed large, are revealed to other clients. Clients\nconducting sizable trades with the bank and possessing a portion of the\naggregated asset exceeding $50\\%$ are considered to be concentrated clients.\nThis could potentially reveal a trading concentrated client's activity to their\ncompetitors, thus providing an unfair advantage over the market.\n  Atlas-X Axe Obfuscation, powered by new differential private methods, enables\na bank to obfuscate its published axe list on a daily basis while under\ncontinual observation, thus maintaining an acceptable inventory Profit and Loss\n(P&L) cost pertaining to the noisy obfuscated axe list while reducing the\nclients' trading activity leakage. Our main differential private innovation is\na differential private aggregator for streams (time series data) of both\npositive and negative integers under continual observation.\n  For the last two years, Atlas-X system has been live in production across\nthree major regions-USA, Europe, and Asia-at J.P. Morgan, a major financial\ninstitution, facilitating significant profitability. To our knowledge, it is\nthe first differential privacy solution to be deployed in the financial sector.\nWe also report benchmarks of our algorithm based on (anonymous) real and\nsynthetic data to showcase the quality of our obfuscation and its success in\nproduction.",
    "updated" : "2024-04-10T02:19:37Z",
    "published" : "2024-04-10T02:19:37Z",
    "authors" : [
      {
        "name" : "Antigoni Polychroniadou"
      },
      {
        "name" : "Gabriele Cipriani"
      },
      {
        "name" : "Richard Hua"
      },
      {
        "name" : "Tucker Balch"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  }
]