[
  {
    "id" : "http://arxiv.org/abs/2404.01619v1",
    "title" : "Making Privacy-preserving Federated Graph Analytics with Strong\n  Guarantees Practical (for Certain Queries)",
    "summary" : "Privacy-preserving federated graph analytics is an emerging area of research.\nThe goal is to run graph analytics queries over a set of devices that are\norganized as a graph while keeping the raw data on the devices rather than\ncentralizing it. Further, no entity may learn any new information except for\nthe final query result. For instance, a device may not learn a neighbor's data.\nThe state-of-the-art prior work for this problem provides privacy guarantees\nfor a broad set of queries in a strong threat model where the devices can be\nmalicious. However, it imposes an impractical overhead: each device locally\nrequires over 8.79 hours of cpu time and 5.73 GiBs of network transfers per\nquery. This paper presents Colo, a new, low-cost system for privacy-preserving\nfederated graph analytics that requires minutes of cpu time and a few MiBs in\nnetwork transfers, for a particular subset of queries. At the heart of Colo is\na new secure computation protocol that enables a device to securely and\nefficiently evaluate a graph query in its local neighborhood while hiding\ndevice data, edge data, and topology data. An implementation and evaluation of\nColo shows that for running a variety of COVID-19 queries over a population of\n1M devices, it requires less than 8.4 minutes of a device's CPU time and 4.93\nMiBs in network transfers - improvements of up to three orders of magnitude.",
    "updated" : "2024-04-02T04:01:31Z",
    "published" : "2024-04-02T04:01:31Z",
    "authors" : [
      {
        "name" : "Kunlong Liu"
      },
      {
        "name" : "Trinabh Gupta"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01283v1",
    "title" : "Evaluating Privacy Perceptions, Experience, and Behavior of Software\n  Development Teams",
    "summary" : "With the increase in the number of privacy regulations, small development\nteams are forced to make privacy decisions on their own. In this paper, we\nconduct a mixed-method survey study, including statistical and qualitative\nanalysis, to evaluate the privacy perceptions, practices, and knowledge of\nmembers involved in various phases of software development (SDLC). Our survey\nincludes 362 participants from 23 countries, encompassing roles such as product\nmanagers, developers, and testers. Our results show diverse definitions of\nprivacy across SDLC roles, emphasizing the need for a holistic privacy approach\nthroughout SDLC. We find that software teams, regardless of their region, are\nless familiar with privacy concepts (such as anonymization), relying on\nself-teaching and forums. Most participants are more familiar with GDPR and\nHIPAA than other regulations, with multi-jurisdictional compliance being their\nprimary concern. Our results advocate the need for role-dependent solutions to\naddress the privacy challenges, and we highlight research directions and\neducational takeaways to help improve privacy-aware software development.",
    "updated" : "2024-04-01T17:55:10Z",
    "published" : "2024-04-01T17:55:10Z",
    "authors" : [
      {
        "name" : "Maxwell Prybylo"
      },
      {
        "name" : "Sara Haghighi"
      },
      {
        "name" : "Sai Teja Peddinti"
      },
      {
        "name" : "Sepideh Ghanavati"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01270v1",
    "title" : "Decentralized Collaborative Learning Framework with External Privacy\n  Leakage Analysis",
    "summary" : "This paper presents two methodological advancements in decentralized\nmulti-task learning under privacy constraints, aiming to pave the way for\nfuture developments in next-generation Blockchain platforms. First, we expand\nthe existing framework for collaborative dictionary learning (CollabDict),\nwhich has previously been limited to Gaussian mixture models, by incorporating\ndeep variational autoencoders (VAEs) into the framework, with a particular\nfocus on anomaly detection. We demonstrate that the VAE-based anomaly score\nfunction shares the same mathematical structure as the non-deep model, and\nprovide comprehensive qualitative comparison. Second, considering the\nwidespread use of \"pre-trained models,\" we provide a mathematical analysis on\ndata privacy leakage when models trained with CollabDict are shared externally.\nWe show that the CollabDict approach, when applied to Gaussian mixtures,\nadheres to a Renyi differential privacy criterion. Additionally, we propose a\npractical metric for monitoring internal privacy breaches during the learning\nprocess.",
    "updated" : "2024-04-01T17:46:17Z",
    "published" : "2024-04-01T17:46:17Z",
    "authors" : [
      {
        "name" : "Tsuyoshi Idé"
      },
      {
        "name" : "Dzung T. Phan"
      },
      {
        "name" : "Rudy Raymond"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.01231v1",
    "title" : "Privacy Backdoors: Enhancing Membership Inference through Poisoning\n  Pre-trained Models",
    "summary" : "It is commonplace to produce application-specific models by fine-tuning large\npre-trained models using a small bespoke dataset. The widespread availability\nof foundation model checkpoints on the web poses considerable risks, including\nthe vulnerability to backdoor attacks. In this paper, we unveil a new\nvulnerability: the privacy backdoor attack. This black-box privacy attack aims\nto amplify the privacy leakage that arises when fine-tuning a model: when a\nvictim fine-tunes a backdoored model, their training data will be leaked at a\nsignificantly higher rate than if they had fine-tuned a typical model. We\nconduct extensive experiments on various datasets and models, including both\nvision-language models (CLIP) and large language models, demonstrating the\nbroad applicability and effectiveness of such an attack. Additionally, we carry\nout multiple ablation studies with different fine-tuning methods and inference\nstrategies to thoroughly analyze this new threat. Our findings highlight a\ncritical privacy concern within the machine learning community and call for a\nreevaluation of safety protocols in the use of open-source pre-trained models.",
    "updated" : "2024-04-01T16:50:54Z",
    "published" : "2024-04-01T16:50:54Z",
    "authors" : [
      {
        "name" : "Yuxin Wen"
      },
      {
        "name" : "Leo Marchyok"
      },
      {
        "name" : "Sanghyun Hong"
      },
      {
        "name" : "Jonas Geiping"
      },
      {
        "name" : "Tom Goldstein"
      },
      {
        "name" : "Nicholas Carlini"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.00847v1",
    "title" : "Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised\n  Video Anomaly Detection: A New Baseline",
    "summary" : "Unsupervised (US) video anomaly detection (VAD) in surveillance applications\nis gaining more popularity recently due to its practical real-world\napplications. As surveillance videos are privacy sensitive and the availability\nof large-scale video data may enable better US-VAD systems, collaborative\nlearning can be highly rewarding in this setting. However, due to the extremely\nchallenging nature of the US-VAD task, where learning is carried out without\nany annotations, privacy-preserving collaborative learning of US-VAD systems\nhas not been studied yet. In this paper, we propose a new baseline for anomaly\ndetection capable of localizing anomalous events in complex surveillance videos\nin a fully unsupervised fashion without any labels on a privacy-preserving\nparticipant-based distributed training configuration. Additionally, we propose\nthree new evaluation protocols to benchmark anomaly detection approaches on\nvarious scenarios of collaborations and data availability. Based on these\nprotocols, we modify existing VAD datasets to extensively evaluate our approach\nas well as existing US SOTA methods on two large-scale datasets including\nUCF-Crime and XD-Violence. All proposed evaluation protocols, dataset splits,\nand codes are available here: https://github.com/AnasEmad11/CLAP",
    "updated" : "2024-04-01T01:25:06Z",
    "published" : "2024-04-01T01:25:06Z",
    "authors" : [
      {
        "name" : "Anas Al-lahham"
      },
      {
        "name" : "Muhammad Zaigham Zaheer"
      },
      {
        "name" : "Nurbek Tastan"
      },
      {
        "name" : "Karthik Nandakumar"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.02696v1",
    "title" : "Deep Privacy Funnel Model: From a Discriminative to a Generative\n  Approach with an Application to Face Recognition",
    "summary" : "In this study, we apply the information-theoretic Privacy Funnel (PF) model\nto the domain of face recognition, developing a novel method for\nprivacy-preserving representation learning within an end-to-end training\nframework. Our approach addresses the trade-off between obfuscation and utility\nin data protection, quantified through logarithmic loss, also known as\nself-information loss. This research provides a foundational exploration into\nthe integration of information-theoretic privacy principles with representation\nlearning, focusing specifically on the face recognition systems. We\nparticularly highlight the adaptability of our framework with recent\nadvancements in face recognition networks, such as AdaFace and ArcFace. In\naddition, we introduce the Generative Privacy Funnel ($\\mathsf{GenPF}$) model,\na paradigm that extends beyond the traditional scope of the PF model, referred\nto as the Discriminative Privacy Funnel ($\\mathsf{DisPF}$). This\n$\\mathsf{GenPF}$ model brings new perspectives on data generation methods with\nestimation-theoretic and information-theoretic privacy guarantees.\nComplementing these developments, we also present the deep variational PF\n(DVPF) model. This model proposes a tractable variational bound for measuring\ninformation leakage, enhancing the understanding of privacy preservation\nchallenges in deep representation learning. The DVPF model, associated with\nboth $\\mathsf{DisPF}$ and $\\mathsf{GenPF}$ models, sheds light on connections\nwith various generative models such as Variational Autoencoders (VAEs),\nGenerative Adversarial Networks (GANs), and Diffusion models. Complementing our\ntheoretical contributions, we release a reproducible PyTorch package,\nfacilitating further exploration and application of these privacy-preserving\nmethodologies in face recognition systems.",
    "updated" : "2024-04-03T12:50:45Z",
    "published" : "2024-04-03T12:50:45Z",
    "authors" : [
      {
        "name" : "Behrooz Razeghi"
      },
      {
        "name" : "Parsa Rahimi"
      },
      {
        "name" : "Sébastien Marcel"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2404.02327v1",
    "title" : "Robust Constrained Consensus and Inequality-constrained Distributed\n  Optimization with Guaranteed Differential Privacy and Accurate Convergence",
    "summary" : "We address differential privacy for fully distributed optimization subject to\na shared inequality constraint. By co-designing the distributed optimization\nmechanism and the differential-privacy noise injection mechanism, we propose\nthe first distributed constrained optimization algorithm that can ensure both\nprovable convergence to a global optimal solution and rigorous\n$\\epsilon$-differential privacy, even when the number of iterations tends to\ninfinity. Our approach does not require the Lagrangian function to be strictly\nconvex/concave, and allows the global objective function to be non-separable.\nAs a byproduct of the co-design, we also propose a new constrained consensus\nalgorithm that can achieve rigorous $\\epsilon$-differential privacy while\nmaintaining accurate convergence, which, to our knowledge, has not been\nachieved before. Numerical simulation results on a demand response control\nproblem in smart grid confirm the effectiveness of the proposed approach.",
    "updated" : "2024-04-02T21:53:43Z",
    "published" : "2024-04-02T21:53:43Z",
    "authors" : [
      {
        "name" : "Yongqiang Wang"
      },
      {
        "name" : "Angelia Nedic"
      }
    ],
    "categories" : [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ]
  }
]