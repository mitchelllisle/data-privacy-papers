[
  {
    "id" : "http://arxiv.org/abs/2512.01832v1",
    "title" : "A Privacy-Preserving Information-Sharing Protocol for Federated Authentication",
    "summary" : "This paper presents a privacy-preserving protocol for identity registration and information sharing in federated authentication systems. The goal is to enable Identity Providers (IdPs) to detect duplicate or fraudulent identity enrollments without revealing users personal data or enabling cross-domain correlation. The protocol relies on Oblivious Pseudorandom Functions (OPRFs) combined with domain-specific transformations, ensuring that each IdP generates independent pseudonymous identifiers derived from a shared cryptographic service while maintaining full input confidentiality. A central authority maintains a blind registry that records successful and failed identity verifications using only pseudonymous identifiers, allowing global consistency checks without exposing sensitive information or linking users across domains. The proposed construction provides a general and abstract framework suitable for a wide range of federated authentication systems, achieving strong privacy guarantees while supporting effective fraud-prevention mechanisms during identity registration.",
    "updated" : "2025-12-01T16:13:41Z",
    "published" : "2025-12-01T16:13:41Z",
    "authors" : [
      {
        "name" : "Francesco Buccafurri"
      },
      {
        "name" : "Carmen Licciardi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.01748v1",
    "title" : "SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models",
    "summary" : "Despite advances in the use of large language models (LLMs) in downstream tasks, their ability to memorize information has raised privacy concerns. Therefore, protecting personally identifiable information (PII) during LLM training remains a fundamental challenge. Conventional methods like Differential Privacy-Stochastic Gradient Descent (DP-SGD) provide robust privacy protection via uniform noising, protecting PII regardless of its distinct sensitivity. This comes at the expense of the model's utility, leading to a trade-off. In this paper, we propose SA-ADP, a sensitivity-aware approach that allocates noise based on the sensitivity of individual PII. We evaluated our method on four datasets (ABCD, CUSTOMERSIM, Wikitext-2, and UNSW-NB15 ). Our results show that SA-ADP achieves results comparable to the baseline (No-DP) and the conventional DP-SGD. This means that our method did not degrade the model's utility while still maintaining strong privacy protection.",
    "updated" : "2025-12-01T14:50:59Z",
    "published" : "2025-12-01T14:50:59Z",
    "authors" : [
      {
        "name" : "Stella Etuk"
      },
      {
        "name" : "Ashraf Matrawy"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.02369v1",
    "title" : "SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains",
    "summary" : "Domain generalization for semantic segmentation aims to mitigate the degradation in model performance caused by domain shifts. However, in many real-world scenarios, we are unable to access the model parameters and architectural details due to privacy concerns and security constraints. Traditional fine-tuning or adaptation is hindered, leading to the demand for input-level strategies that can enhance generalization without modifying model weights. To this end, we propose a \\textbf{S}tyle-\\textbf{A}daptive \\textbf{GE}neralization framework (\\textbf{SAGE}), which improves the generalization of frozen models under privacy constraints. SAGE learns to synthesize visual prompts that implicitly align feature distributions across styles instead of directly fine-tuning the backbone. Specifically, we first utilize style transfer to construct a diverse style representation of the source domain, thereby learning a set of style characteristics that can cover a wide range of visual features. Then, the model adaptively fuses these style cues according to the visual context of each input, forming a dynamic prompt that harmonizes the image appearance without touching the interior of the model. Through this closed-loop design, SAGE effectively bridges the gap between frozen model invariance and the diversity of unseen domains. Extensive experiments on five benchmark datasets demonstrate that SAGE achieves competitive or superior performance compared to state-of-the-art methods under privacy constraints and outperforms full fine-tuning baselines in all settings.",
    "updated" : "2025-12-02T03:20:22Z",
    "published" : "2025-12-02T03:20:22Z",
    "authors" : [
      {
        "name" : "Qingmei Li"
      },
      {
        "name" : "Yang Zhang"
      },
      {
        "name" : "Peifeng Zhang"
      },
      {
        "name" : "Haohuan Fu"
      },
      {
        "name" : "Juepeng Zheng"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.02301v1",
    "title" : "Quantum Vanguard: Server Optimized Privacy Fortified Federated Intelligence for Future Vehicles",
    "summary" : "This work presents vQFL (vehicular Quantum Federated Learning), a new framework that leverages quantum machine learning techniques to tackle key privacy and security issues in autonomous vehicular networks. Furthermore, we propose a server-side adapted fine-tuning method, ft-VQFL,to achieve enhanced and more resilient performance. By integrating quantum federated learning with differential privacy and quantum key distribution (QKD), our quantum vanguard approach creates a multi-layered defense against both classical and quantum threats while preserving model utility. Extensive experimentation with industry-standard datasets (KITTI, Waymo, and nuScenes) demonstrates that vQFL maintains accuracy comparable to standard QFL while significantly improving privacy guaranties and communication security. Our implementation using various quantum models (VQC, QCNN, and SamplerQNN) reveals minimal performance overhead despite the added security measures. This work establishes a crucial foundation for quantum-resistant autonomous vehicle systems that can operate securely in the post-quantum era while efficiently processing the massive data volumes (20-40TB/day per vehicle) generated by modern autonomous fleets. The modular design of the framework allows for seamless integration with existing vehicular networks, positioning vQFL as an essential component for future intelligent transportation infrastructure.",
    "updated" : "2025-12-02T00:43:48Z",
    "published" : "2025-12-02T00:43:48Z",
    "authors" : [
      {
        "name" : "Dev Gurung"
      },
      {
        "name" : "Shiva Raj Pokhrel"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.03793v1",
    "title" : "The enshittification of online search? Privacy and quality of Google, Bing and Apple in coding advice",
    "summary" : "Even though currently being challenged by ChatGPT and other large-language models (LLMs), Google Search remains one of the primary means for many individuals to find information on the internet. Interestingly, the way that we retrieve information on the web has hardly changed ever since Google was established in 1998, raising concerns as to Google's dominance in search and lack of competition. If the market for search was sufficiently competitive, then we should probably see a steady increase in search quality over time as well as alternative approaches to the Google's approach to search. However, hardly any research has so far looked at search quality, which is a key facet of a competitive market, especially not over time.\n  In this report, we conducted a relatively large-scale quantitative comparison of search quality of 1,467 search queries relating to coding advice in October 2023. We focus on coding advice because the study of general search quality is difficult, with the aim of learning more about the assessment of search quality and motivating follow-up research into this important topic. We evaluate the search quality of Google Search, Microsoft Bing, and Apple Search, with a special emphasis on Apple Search, a widely used search engine that has never been explored in previous research. For the assessment of search quality, we use two independent metrics of search quality: 1) the number of trackers on the first search result, as a measure of privacy in web search, and 2) the average rank of the first Stack Overflow search result, under the assumption that Stack Overflow gives the best coding advice. Our results suggest that the privacy of search results is higher on Bing than on Google and Apple. Similarly, the quality of coding advice -- as measured by the average rank of Stack Overflow -- was highest on Bing.",
    "updated" : "2025-12-03T13:42:22Z",
    "published" : "2025-12-03T13:42:22Z",
    "authors" : [
      {
        "name" : "Konrad Kollnig"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.03791v1",
    "title" : "CCN: Decentralized Cross-Chain Channel Networks Supporting Secure and Privacy-Preserving Multi-Hop Interactions",
    "summary" : "Cross-chain technology enables interoperability among otherwise isolated blockchains, supporting interactions across heterogeneous networks. Similar to how multi-hop communication became fundamental in the evolution of the Internet, the demand for multi-hop cross-chain interactions is gaining increasing attention. However, this growing demand introduces new security and privacy challenges. On the security side, multi-hop interactions depend on the availability of multiple participating nodes. If any node becomes temporarily offline during execution, the protocol may fail to complete correctly, leading to settlement failure or fund loss. On the privacy side, the need for on-chain transparency to validate intermediate states may unintentionally leak linkable information, compromising the unlinkability of user interactions. In this paper, we propose the Cross-Chain Channel Network (CCN), a decentralized network designed to support secure and privacy-preserving multi-hop cross-chain transactions. Through experimental evaluation, we identify two critical types of offline failures, referred to as active and passive offline cases, which have not been adequately addressed by existing solutions. To mitigate these issues, we introduce R-HTLC, a core protocol within CCN. R-HTLC incorporates an hourglass mechanism and a multi-path refund strategy to ensure settlement correctness even when some nodes go offline during execution. Importantly, CCN addresses not only the correctness under offline conditions but also maintains unlinkability in such adversarial settings. To overcome this, CCN leverages zero-knowledge proofs and off-chain coordination, ensuring that interaction relationships remain indistinguishable even when certain nodes are temporarily offline.",
    "updated" : "2025-12-03T13:41:02Z",
    "published" : "2025-12-03T13:41:02Z",
    "authors" : [
      {
        "name" : "Minghui Xu"
      },
      {
        "name" : "Yihao Guo"
      },
      {
        "name" : "Yanqiang Zhang"
      },
      {
        "name" : "Zhiguang Shan"
      },
      {
        "name" : "Guangyong Shang"
      },
      {
        "name" : "Zhen Ma"
      },
      {
        "name" : "Bin Xiao"
      },
      {
        "name" : "Xiuzhen Cheng"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.03694v1",
    "title" : "SRPG: Semantically Reconstructed Privacy Guard for Zero-Trust Privacy in Educational Multi-Agent Systems",
    "summary" : "Multi-Agent Systems (MAS) with large language models (LLMs) enable personalized education but risk leaking minors personally identifiable information (PII) via unstructured dialogue. Existing privacy methods struggle to balance security and utility: role-based access control fails on unstructured text, while naive masking destroys pedagogical context. We propose SRPG, a privacy guard for educational MAS, using a Dual-Stream Reconstruction Mechanism: a strict sanitization stream ensures zero PII leakage, and a context reconstruction stream (LLM driven) recovers mathematical logic. This decouples instructional content from private data, preserving teaching efficacy. Tests on MathDial show SRPG works across models; with GPT-4o, it achieves 0.0000 Attack Success Rate (ASR) (zero leakage) and 0.8267 Exact Match, far outperforming the zero trust Pure LLM baseline (0.2138). SRPG effectively protects minors privacy without sacrificing mathematical instructional quality.",
    "updated" : "2025-12-03T11:36:33Z",
    "published" : "2025-12-03T11:36:33Z",
    "authors" : [
      {
        "name" : "Shuang Guo"
      },
      {
        "name" : "Zihui Li"
      }
    ],
    "categories" : [
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.03669v1",
    "title" : "Towards Privacy-Preserving Range Queries with Secure Learned Spatial Index over Encrypted Data",
    "summary" : "With the growing reliance on cloud services for large-scale data management, preserving the security and privacy of outsourced datasets has become increasingly critical. While encrypting data and queries can prevent direct content exposure, recent research reveals that adversaries can still infer sensitive information via access pattern and search path analysis. However, existing solutions that offer strong access pattern privacy often incur substantial performance overhead. In this paper, we propose a novel privacy-preserving range query scheme over encrypted datasets, offering strong security guarantees while maintaining high efficiency. To achieve this, we develop secure learned spatial index (SLS-INDEX), a secure learned index that integrates the Paillier cryptosystem with a hierarchical prediction architecture and noise-injected buckets, enabling data-aware query acceleration in the encrypted domain. To further obfuscate query execution paths, SLS-INDEXbased Range Queries (SLRQ) employs a permutation-based secure bucket prediction protocol. Additionally, we introduce a secure point extraction protocol that generates candidate results to reduce the overhead of secure computation. We provide formal security analysis under realistic leakage functions and implement a prototype to evaluate its practical performance. Extensive experiments on both real-world and synthetic datasets demonstrate that SLRQ significantly outperforms existing solutions in query efficiency while ensuring dataset, query, result, and access pattern privacy.",
    "updated" : "2025-12-03T10:59:40Z",
    "published" : "2025-12-03T10:59:40Z",
    "authors" : [
      {
        "name" : "Zuan Wang"
      },
      {
        "name" : "Juntao Lu"
      },
      {
        "name" : "Jiazhuang Wu"
      },
      {
        "name" : "Youliang Tian"
      },
      {
        "name" : "Wei Song"
      },
      {
        "name" : "Qiuxian Li"
      },
      {
        "name" : "Duo Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.03358v1",
    "title" : "Scaling Trust in Quantum Federated Learning: A Multi-Protocol Privacy Design",
    "summary" : "Quantum Federated Learning (QFL) promises to revolutionize distributed machine learning by combining the computational power of quantum devices with collaborative model training. Yet, privacy of both data and models remains a critical challenge. In this work, we propose a privacy-preserving QFL framework where a network of $n$ quantum devices trains local models and transmits them to a central server under a multi-layered privacy protocol. Our design leverages Singular Value Decomposition (SVD), Quantum Key Distribution (QKD), and Analytic Quantum Gradient Descent (AQGD) to secure data preparation, model sharing, and training stages. Through theoretical analysis and experiments on contemporary quantum platforms and datasets, we demonstrate that the framework robustly safeguards data and model confidentiality while maintaining training efficiency.",
    "updated" : "2025-12-03T01:45:48Z",
    "published" : "2025-12-03T01:45:48Z",
    "authors" : [
      {
        "name" : "Dev Gurung"
      },
      {
        "name" : "Shiva Raj Pokhrel"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.03238v1",
    "title" : "How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy",
    "summary" : "High quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, \\emph{Differentially Private Synthetic data}, refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization.\n  In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches.",
    "updated" : "2025-12-02T21:14:39Z",
    "published" : "2025-12-02T21:14:39Z",
    "authors" : [
      {
        "name" : "Natalia Ponomareva"
      },
      {
        "name" : "Zheng Xu"
      },
      {
        "name" : "H. Brendan McMahan"
      },
      {
        "name" : "Peter Kairouz"
      },
      {
        "name" : "Lucas Rosenblatt"
      },
      {
        "name" : "Vincent Cohen-Addad"
      },
      {
        "name" : "Cristóbal Guzmán"
      },
      {
        "name" : "Ryan McKenna"
      },
      {
        "name" : "Galen Andrew"
      },
      {
        "name" : "Alex Bie"
      },
      {
        "name" : "Da Yu"
      },
      {
        "name" : "Alex Kurakin"
      },
      {
        "name" : "Morteza Zadimoghaddam"
      },
      {
        "name" : "Sergei Vassilvitskii"
      },
      {
        "name" : "Andreas Terzis"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.03207v1",
    "title" : "Technical Report: The Need for a (Research) Sandstorm through the Privacy Sandbox",
    "summary" : "The Privacy Sandbox, launched in 2019, is a series of proposals from Google to reduce ``cross-site and cross-app tracking while helping to keep online content and services free for all''. Over the years, Google implemented, experimented, and deprecated some of these APIs into their own products (Chrome, Android, etc.) which raised concerns about the potential of these mechanisms to fundamentally disrupt the advertising, mobile, and web ecosystems. As a result, it is paramount for researchers to understand the consequences that these new technologies, and future ones, will have on billions of users if and when deployed. In this report, we outline our call for privacy, security, usability, and utility evaluations of these APIs, our efforts materialized through the creation and operation of Privacy Sandstorm (https://privacysandstorm.github.io); a research portal to systematically gather resources (overview, analyses, artifacts, etc.) about such proposals. We find that our inventory provides a better visibility and broader perspective on the research findings in that space than what Google lets show through official channels.",
    "updated" : "2025-12-02T20:14:44Z",
    "published" : "2025-12-02T20:14:44Z",
    "authors" : [
      {
        "name" : "Yohan Beugin"
      },
      {
        "name" : "Patrick McDaniel"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.03100v1",
    "title" : "Ensemble Privacy Defense for Knowledge-Intensive LLMs against Membership Inference Attacks",
    "summary" : "Retrieval-Augmented Generation (RAG) and Supervised Finetuning (SFT) have become the predominant paradigms for equipping Large Language Models (LLMs) with external knowledge for diverse, knowledge-intensive tasks. However, while such knowledge injection improves performance, it also exposes new attack surfaces. Membership Inference Attacks (MIAs), which aim to determine whether a given data sample was included in a model's training set, pose serious threats to privacy and trust in sensitive domains. To this end, we first systematically evaluate the vulnerability of RAG- and SFT-based LLMs to various MIAs. Then, to address the privacy risk, we further introduce a novel, model-agnostic defense framework, Ensemble Privacy Defense (EPD), which aggregates and evaluates the outputs of a knowledge-injected LLM, a base LLM, and a dedicated judge model to enhance resistance against MIAs. Comprehensive experiments show that, on average, EPD reduces MIA success by up to 27.8\\% for SFT and 526.3\\% for RAG compared to inference-time baseline, while maintaining answer quality.",
    "updated" : "2025-12-01T18:12:18Z",
    "published" : "2025-12-01T18:12:18Z",
    "authors" : [
      {
        "name" : "Haowei Fu"
      },
      {
        "name" : "Bo Ni"
      },
      {
        "name" : "Han Xu"
      },
      {
        "name" : "Kunpeng Liu"
      },
      {
        "name" : "Dan Lin"
      },
      {
        "name" : "Tyler Derr"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.05065v1",
    "title" : "Personalizing Agent Privacy Decisions via Logical Entailment",
    "summary" : "Personal language model-based agents are becoming more widespread for completing tasks on behalf of users; however, this raises serious privacy questions regarding whether these models will appropriately disclose user data. While prior work has evaluated language models on data-sharing scenarios based on general privacy norms, we focus on personalizing language models' privacy decisions, grounding their judgments directly in prior user privacy decisions. Our findings suggest that general privacy norms are insufficient for effective personalization of privacy decisions. Furthermore, we find that eliciting privacy judgments from the model through In-context Learning (ICL) is unreliable to due misalignment with the user's prior privacy judgments and opaque reasoning traces, which make it difficult for the user to interpret the reasoning behind the model's decisions. To address these limitations, we propose ARIEL (Agentic Reasoning with Individualized Entailment Logic), a framework that jointly leverages a language model and rule-based logic for structured data-sharing reasoning. ARIEL is based on formulating personalization of data sharing as an entailment, whether a prior user judgment on a data-sharing request implies the same judgment for an incoming request. Our experimental evaluations on advanced models and publicly-available datasets demonstrate that ARIEL can reduce the F1 score error by $\\textbf{39.1%}$ over language model-based reasoning (ICL), demonstrating that ARIEL is effective at correctly judging requests where the user would approve data sharing. Overall, our findings suggest that combining LLMs with strict logical entailment is a highly effective strategy for enabling personalized privacy judgments for agents.",
    "updated" : "2025-12-04T18:24:56Z",
    "published" : "2025-12-04T18:24:56Z",
    "authors" : [
      {
        "name" : "James Flemings"
      },
      {
        "name" : "Ren Yi"
      },
      {
        "name" : "Octavian Suciu"
      },
      {
        "name" : "Kassem Fawaz"
      },
      {
        "name" : "Murali Annavaram"
      },
      {
        "name" : "Marco Gruteser"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.05053v1",
    "title" : "A Randomized Scheduling Framework for Privacy-Preserving Multi-robot Rendezvous given Prior Information",
    "summary" : "Privacy has become a critical concern in modern multi-robot systems, driven by both ethical considerations and operational constraints. As a result, growing attention has been directed toward privacy-preserving coordination in dynamical multi-robot systems. This work introduces a randomized scheduling mechanism for privacy-preserving robot rendezvous. The proposed approach achieves improved privacy even at lower communication rates, where privacy is quantified via pointwise maximal leakage. We show that lower transmission rates provide stronger privacy guarantees and prove that rendezvous is still achieved under the randomized scheduling mechanism. Numerical simulations are provided to demonstrate the effectiveness of the method.",
    "updated" : "2025-12-04T18:07:17Z",
    "published" : "2025-12-04T18:07:17Z",
    "authors" : [
      {
        "name" : "Le Liu"
      },
      {
        "name" : "Yu Kawano"
      },
      {
        "name" : "Ming Cao"
      }
    ],
    "categories" : [
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.04852v1",
    "title" : "Ask Safely: Privacy-Aware LLM Query Generation for Knowledge Graphs",
    "summary" : "Large Language Models (LLMs) are increasingly used to query knowledge graphs (KGs) due to their strong semantic understanding and extrapolation capabilities compared to traditional approaches. However, these methods cannot be applied when the KG contains sensitive data and the user lacks the resources to deploy a local generative LLM. To address this issue, we propose a privacy-aware query generation approach for KGs. Our method identifies sensitive information in the graph based on its structure and omits such values before requesting the LLM to translate natural language questions into Cypher queries. Experimental results show that our approach preserves the quality of the generated queries while preventing sensitive data from being transmitted to third-party services.",
    "updated" : "2025-12-04T14:37:00Z",
    "published" : "2025-12-04T14:37:00Z",
    "authors" : [
      {
        "name" : "Mauro Dalle Lucca Tosi"
      },
      {
        "name" : "Jordi Cabot"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.04316v1",
    "title" : "ConsentDiff at Scale: Longitudinal Audits of Web Privacy Policy Changes and UI Frictions",
    "summary" : "Web privacy is experienced via two public artifacts: site utterances in policy texts, and the actions users are required to take during consent interfaces. In the extensive cross-section audits we've studied, there is a lack of longitudinal data detailing how these artifacts are changing together, and if interfaces are actually doing what they promise in policy. ConsentDiff provides that longitudinal view. We build a reproducible pipeline that snapshots sites every month, semantically aligns policy clauses to track clause-level churn, and classifies consent-UI patterns by pulling together DOM signals with cues provided by screenshots. We introduce a novel weighted claim-UI alignment score, connecting common policy claims to observable predicates, and enabling comparisons over time, regions, and verticals. Our measurements suggest continued policy churn, systematic changes to eliminate a higher-friction banner design, and significantly higher alignment where rejecting is visible and lower friction.",
    "updated" : "2025-12-03T23:05:42Z",
    "published" : "2025-12-03T23:05:42Z",
    "authors" : [
      {
        "name" : "Haoze Guo"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.04225v1",
    "title" : "GOPHER: Optimization-based Phenotype Randomization for Genome-Wide Association Studies with Differential Privacy",
    "summary" : "Genome-wide association studies (GWAS) are an essential tool in biomedical research for identifying genetic factors linked to health and disease. However, publicly releasing GWAS summary statistics poses well-recognized privacy risks, including the potential to infer an individual's participation in the study or to reveal sensitive phenotypic information (e.g., disease status). While differential privacy (DP) offers a rigorous mathematical framework for mitigating these risks, existing DP techniques for GWAS either introduce excessive noise or restrict the release to a limited set of results. In this work, we present practical DP mechanisms for releasing the complete set of genome-wide association statistics with privacy guarantees. We demonstrate the accuracy of the privacy-preserving statistics released by our mechanisms on a range of GWAS datasets from the UK Biobank, utilizing both real and simulated phenotypes. We introduce two key techniques to overcome the limitations of prior approaches: (1) an optimization-based randomization mechanism that directly minimizes the expected error in GWAS results to enhance utility, and (2) the use of personalized priors, derived from predictive models privately trained on a subset of the dataset, to enable sample-specific optimization which further reduces the amount of noise introduced by DP. Overall, our work provides practical tools for accurately releasing comprehensive GWAS results with provable protection of study participants.",
    "updated" : "2025-12-03T19:44:50Z",
    "published" : "2025-12-03T19:44:50Z",
    "authors" : [
      {
        "name" : "Anupama Nandi"
      },
      {
        "name" : "Seth Neel"
      },
      {
        "name" : "Hyunghoon Cho"
      }
    ],
    "categories" : [
      "q-bio.QM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.05730v1",
    "title" : "De mythe van geïnformeerde toestemming: online privacybescherming kan beter [Informed Consent: We Can Do Better to Defend Privacy]",
    "summary" : "We need to rethink our approach to defend privacy on the internet. Currently, policymakers focus heavily on the idea of informed consent as a means to defend privacy. For instance, in many countries the law requires firms to obtain an individual's consent before they use data about her; with such informed consent requirements, the law aims to empower people to make privacy choices in their best interests. But behavioural studies cast doubt on this approach's effectiveness, as people tend to click OK to almost any request they see on their screens. To improve privacy protection, this article argues for a combined approach of protecting and empowering the individual. This article discusses practical problems with informed consent as a means to protect privacy, and illustrates the problems with current data privacy rules regarding behavioural targeting. First, the privacy problems of behavioural targeting, and the central role of informed consent in privacy law are discussed. Following that, practical problems with informed consent are highlighted. Then, the article argues that policymakers should give more attention to rules that protect, rather than empower, people.",
    "updated" : "2025-12-05T14:08:34Z",
    "published" : "2025-12-05T14:08:34Z",
    "authors" : [
      {
        "name" : "Frederik Zuiderveen Borgesius"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.05729v1",
    "title" : "Informed Consent: We Can Do Better to Defend Privacy",
    "summary" : "We need to rethink our approach to defend privacy on the internet. Currently, policymakers focus heavily on the idea of informed consent as a means to defend privacy. For instance, in many countries the law requires firms to obtain an individual's consent before they use data about her; with such informed consent requirements, the law aims to empower people to make privacy choices in their best interests. But behavioural studies cast doubt on this approach's effectiveness, as people tend to click OK to almost any request they see on their screens. To improve privacy protection, this article argues for a combined approach of protecting and empowering the individual. This article discusses practical problems with informed consent as a means to protect privacy, and illustrates the problems with current data privacy rules regarding behavioural targeting. First, the privacy problems of behavioural targeting, and the central role of informed consent in privacy law are discussed. Following that, practical problems with informed consent are highlighted. Then, the article argues that policymakers should give more attention to rules that protect, rather than empower, people.",
    "updated" : "2025-12-05T14:08:29Z",
    "published" : "2025-12-05T14:08:29Z",
    "authors" : [
      {
        "name" : "Frederik Zuiderveen Borgesius"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.05728v1",
    "title" : "Open Data, Privacy, and Fair Information Principles: Towards a Balancing Framework",
    "summary" : "Open data are held to contribute to a wide variety of social and political goals, including strengthening transparency, public participation and democratic accountability, promoting economic growth and innovation, and enabling greater public sector efficiency and cost savings. However, releasing government data that contain personal information may threaten privacy and related rights and interests. In this Article we ask how these privacy interests can be respected, without unduly hampering benefits from disclosing public sector information. We propose a balancing framework to help public authorities address this question in different contexts. The framework takes into account different levels of privacy risks for different types of data. It also separates decisions about access and re-use, and highlights a range of different disclosure routes. A circumstance catalogue lists factors that might be considered when assessing whether, under which conditions, and how a dataset can be released. While open data remains an important route for the publication of government information, we conclude that it is not the only route, and there must be clear and robust public interest arguments in order to justify the disclosure of personal information as open data.",
    "updated" : "2025-12-05T14:08:26Z",
    "published" : "2025-12-05T14:08:26Z",
    "authors" : [
      {
        "name" : "Frederik Zuiderveen Borgesius"
      },
      {
        "name" : "Jonathan Gray"
      },
      {
        "name" : "Mireille van Eechoud"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.05473v1",
    "title" : "Privacy-Preserving Fully Distributed Gaussian Process Regression",
    "summary" : "Although distributed Gaussian process regression (GPR) enables multiple agents with separate datasets to jointly learn a model of the target function, its collaborative nature poses risks of private data leakage. To address this, we propose a privacy-preserving fully distributed GPR protocol based on secure multi-party computation (SMPC) that preserves the confidentiality of each agent's local dataset. Building upon a secure distributed average consensus algorithm, the protocol guarantees that each agent's local model practically converges to the same global model that would be obtained by the standard distributed GPR. Further, we adopt the paradigm of simulation based security to provide formal privacy guarantees, and extend the proposed protocol to enable kernel hyperparameter optimization, which is critical yet often overlooked in the literature. Experimental results demonstrate the effectiveness and practical applicability of the proposed method.",
    "updated" : "2025-12-05T07:05:54Z",
    "published" : "2025-12-05T07:05:54Z",
    "authors" : [
      {
        "name" : "Yeongjun Jang"
      },
      {
        "name" : "Kaoru Teranishi"
      },
      {
        "name" : "Jihoon Suh"
      },
      {
        "name" : "Takashi Tanaka"
      }
    ],
    "categories" : [
      "eess.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2512.05459v1",
    "title" : "PrivCode: When Code Generation Meets Differential Privacy",
    "summary" : "Large language models (LLMs) have presented outstanding performance in code generation and completion. However, fine-tuning these models on private datasets can raise privacy and proprietary concerns, such as the leakage of sensitive personal information. Differentially private (DP) code generation provides theoretical guarantees for protecting sensitive code by generating synthetic datasets that preserve statistical properties while reducing privacy leakage concerns. However, DP code generation faces significant challenges due to the strict syntactic dependencies and the privacy-utility trade-off.\n  We propose PrivCode, the first DP synthesizer specifically designed for code datasets. It incorporates a two-stage framework to improve both privacy and utility. In the first stage, termed \"privacy-sanitizing\", PrivCode generates DP-compliant synthetic code by training models using DP-SGD while introducing syntactic information to preserve code structure. The second stage, termed \"utility-boosting\", fine-tunes a larger pre-trained LLM on the synthetic privacy-free code to mitigate the utility loss caused by DP, enhancing the utility of the generated code. Extensive experiments on four LLMs show that PrivCode generates higher-utility code across various testing tasks under four benchmarks. The experiments also confirm its ability to protect sensitive data under varying privacy budgets. We provide the replication package at the anonymous link.",
    "updated" : "2025-12-05T06:27:06Z",
    "published" : "2025-12-05T06:27:06Z",
    "authors" : [
      {
        "name" : "Zheng Liu"
      },
      {
        "name" : "Chen Gong"
      },
      {
        "name" : "Terry Yue Zhuo"
      },
      {
        "name" : "Kecen Li"
      },
      {
        "name" : "Weichen Yu"
      },
      {
        "name" : "Matt Fredrikson"
      },
      {
        "name" : "Tianhao Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  }
]