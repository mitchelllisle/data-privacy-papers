[{"id":"http://arxiv.org/abs/2512.01832v1","title":"A Privacy-Preserving Information-Sharing Protocol for Federated Authentication","summary":"This paper presents a privacy-preserving protocol for identity registration and information sharing in federated authentication systems. The goal is to enable Identity Providers (IdPs) to detect duplicate or fraudulent identity enrollments without revealing users personal data or enabling cross-domain correlation. The protocol relies on Oblivious Pseudorandom Functions (OPRFs) combined with domain-specific transformations, ensuring that each IdP generates independent pseudonymous identifiers derived from a shared cryptographic service while maintaining full input confidentiality. A central authority maintains a blind registry that records successful and failed identity verifications using only pseudonymous identifiers, allowing global consistency checks without exposing sensitive information or linking users across domains. The proposed construction provides a general and abstract framework suitable for a wide range of federated authentication systems, achieving strong privacy guarantees while supporting effective fraud-prevention mechanisms during identity registration.","updated":"2025-12-01T16:13:41Z","published":"2025-12-01T16:13:41Z","authors":[{"name":"Francesco Buccafurri"},{"name":"Carmen Licciardi"}],"categories":["cs.CR"]},{"id":"http://arxiv.org/abs/2512.01748v1","title":"SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models","summary":"Despite advances in the use of large language models (LLMs) in downstream tasks, their ability to memorize information has raised privacy concerns. Therefore, protecting personally identifiable information (PII) during LLM training remains a fundamental challenge. Conventional methods like Differential Privacy-Stochastic Gradient Descent (DP-SGD) provide robust privacy protection via uniform noising, protecting PII regardless of its distinct sensitivity. This comes at the expense of the model's utility, leading to a trade-off. In this paper, we propose SA-ADP, a sensitivity-aware approach that allocates noise based on the sensitivity of individual PII. We evaluated our method on four datasets (ABCD, CUSTOMERSIM, Wikitext-2, and UNSW-NB15 ). Our results show that SA-ADP achieves results comparable to the baseline (No-DP) and the conventional DP-SGD. This means that our method did not degrade the model's utility while still maintaining strong privacy protection.","updated":"2025-12-01T14:50:59Z","published":"2025-12-01T14:50:59Z","authors":[{"name":"Stella Etuk"},{"name":"Ashraf Matrawy"}],"categories":["cs.LG"]}]