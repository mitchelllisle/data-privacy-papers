[
  {
    "id" : "http://arxiv.org/abs/2412.02578v1",
    "title" : "Private Linear Regression with Differential Privacy and PAC Privacy",
    "summary" : "Linear regression is a fundamental tool for statistical analysis, which has\nmotivated the development of linear regression methods that satisfy provable\nprivacy guarantees so that the learned model reveals little about any one data\npoint used to construct it. Most existing privacy-preserving linear regression\nmethods rely on the well-established framework of differential privacy, while\nthe newly proposed PAC Privacy has not yet been explored in this context. In\nthis paper, we systematically compare linear regression models trained with\ndifferential privacy and PAC privacy across three real-world datasets,\nobserving several key findings that impact the performance of\nprivacy-preserving linear regression.",
    "updated" : "2024-12-03T17:04:14Z",
    "published" : "2024-12-03T17:04:14Z",
    "authors" : [
      {
        "name" : "Hillary Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02538v1",
    "title" : "On the Privacy, Security, and Trustworthy for Distributed Wireless Large\n  AI Model (WLAM)",
    "summary" : "Combining wireless communication with large artificial intelligence (AI)\nmodels can open up a myriad of novel application scenarios. In sixth generation\n(6G) networks, ubiquitous communication and computing resources allow large AI\nmodels to serve democratic large AI models-related services to enable real-time\napplications like autonomous vehicles, smart cities, and Internet of Things\n(IoT) ecosystems. However, the security considerations and sustainable\ncommunication resources limit the deployment of large AI models over\ndistributed wireless networks. This paper provides a comprehensive overview of\nprivacy, security, and trustworthy for distributed wireless large AI model\n(WLAM). In particular, the detailed privacy and security are analysis for\ndistributed WLAM is fist revealed. The classifications and theoretical findings\nabout privacy and security in distributed WLAM are discussed. Then the\ntrustworthy and ethics for implementing distributed WLAM are described.\nFinally, the comprehensive applications of distributed WLAM is provided in the\naspect of electromagnetic signal processing.",
    "updated" : "2024-12-03T16:32:19Z",
    "published" : "2024-12-03T16:32:19Z",
    "authors" : [
      {
        "name" : "Zhaohui Yang"
      },
      {
        "name" : "Wei Xu"
      },
      {
        "name" : "Le Liang"
      },
      {
        "name" : "Yuanhao Cui"
      },
      {
        "name" : "Zhijin Qin"
      },
      {
        "name" : "Merouane Debbah"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02340v1",
    "title" : "Federated Analytics in Practice: Engineering for Privacy, Scalability\n  and Practicality",
    "summary" : "Cross-device Federated Analytics (FA) is a distributed computation paradigm\ndesigned to answer analytics queries about and derive insights from data held\nlocally on users' devices. On-device computations combined with other privacy\nand security measures ensure that only minimal data is transmitted off-device,\nachieving a high standard of data protection. Despite FA's broad relevance, the\napplicability of existing FA systems is limited by compromised accuracy; lack\nof flexibility for data analytics; and an inability to scale effectively. In\nthis paper, we describe our approach to combine privacy, scalability, and\npracticality to build and deploy a system that overcomes these limitations. Our\nFA system leverages trusted execution environments (TEEs) and optimizes the use\nof on-device computing resources to facilitate federated data processing across\nlarge fleets of devices, while ensuring robust, defensible, and verifiable\nprivacy safeguards. We focus on federated analytics (statistics and\nmonitoring), in contrast to systems for federated learning (ML workloads), and\nwe flag the key differences.",
    "updated" : "2024-12-03T10:03:12Z",
    "published" : "2024-12-03T10:03:12Z",
    "authors" : [
      {
        "name" : "Harish Srinivas"
      },
      {
        "name" : "Graham Cormode"
      },
      {
        "name" : "Mehrdad Honarkhah"
      },
      {
        "name" : "Samuel Lurye"
      },
      {
        "name" : "Jonathan Hehir"
      },
      {
        "name" : "Lunwen He"
      },
      {
        "name" : "George Hong"
      },
      {
        "name" : "Ahmed Magdy"
      },
      {
        "name" : "Dzmitry Huba"
      },
      {
        "name" : "Kaikai Wang"
      },
      {
        "name" : "Shen Guo"
      },
      {
        "name" : "Shoubhik Bhattacharya"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02130v1",
    "title" : "A privacy-preserving distributed credible evidence fusion algorithm for\n  collective decision-making",
    "summary" : "The theory of evidence reasoning has been applied to collective\ndecision-making in recent years. However, existing distributed evidence fusion\nmethods lead to participants' preference leakage and fusion failures as they\ndirectly exchange raw evidence and do not assess evidence credibility like\ncentralized credible evidence fusion (CCEF) does. To do so, a\nprivacy-preserving distributed credible evidence fusion method with three-level\nconsensus (PCEF) is proposed in this paper. In evidence difference measure\n(EDM) neighbor consensus, an evidence-free equivalent expression of EDM among\nneighbored agents is derived with the shared dot product protocol for pignistic\nprobability and the identical judgment of two events with maximal subjective\nprobabilities, so that evidence privacy is guaranteed due to such irreversible\nevidence transformation. In EDM network consensus, the non-neighbored EDMs are\ninferred and neighbored EDMs reach uniformity via interaction between linear\naverage consensus (LAC) and low-rank matrix completion with rank adaptation to\nguarantee EDM consensus convergence and no solution of inferring raw evidence\nin numerical iteration style. In fusion network consensus, a privacy-preserving\nLAC with a self-cancelling differential privacy term is proposed, where each\nagent adds its randomness to the sharing content and step-by-step cancels such\nrandomness in consensus iterations. Besides, the sufficient condition of the\nconvergence to the CCEF is explored, and it is proven that raw evidence is\nimpossibly inferred in such an iterative consensus. The simulations show that\nPCEF is close to CCEF both in credibility and fusion results and obtains higher\ndecision accuracy with less time-comsuming than existing methods.",
    "updated" : "2024-12-03T03:36:42Z",
    "published" : "2024-12-03T03:36:42Z",
    "authors" : [
      {
        "name" : "Chaoxiong Ma"
      },
      {
        "name" : "Yan Liang"
      },
      {
        "name" : "Xinyu Yang"
      },
      {
        "name" : "Han Wu"
      },
      {
        "name" : "Huixia Zhang"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01756v1",
    "title" : "Adversarial Sample-Based Approach for Tighter Privacy Auditing in Final\n  Model-Only Scenarios",
    "summary" : "Auditing Differentially Private Stochastic Gradient Descent (DP-SGD) in the\nfinal model setting is challenging and often results in empirical lower bounds\nthat are significantly looser than theoretical privacy guarantees. We introduce\na novel auditing method that achieves tighter empirical lower bounds without\nadditional assumptions by crafting worst-case adversarial samples through\nloss-based input-space auditing. Our approach surpasses traditional\ncanary-based heuristics and is effective in both white-box and black-box\nscenarios. Specifically, with a theoretical privacy budget of $\\varepsilon =\n10.0$, our method achieves empirical lower bounds of $6.68$ in white-box\nsettings and $4.51$ in black-box settings, compared to the baseline of $4.11$\nfor MNIST. Moreover, we demonstrate that significant privacy auditing results\ncan be achieved using in-distribution (ID) samples as canaries, obtaining an\nempirical lower bound of $4.33$ where traditional methods produce near-zero\nleakage detection. Our work offers a practical framework for reliable and\naccurate privacy auditing in differentially private machine learning.",
    "updated" : "2024-12-02T17:52:16Z",
    "published" : "2024-12-02T17:52:16Z",
    "authors" : [
      {
        "name" : "Sangyeon Yoon"
      },
      {
        "name" : "Wonje Jeung"
      },
      {
        "name" : "Albert No"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01671v2",
    "title" : "Verified Foundations for Differential Privacy",
    "summary" : "Differential privacy (DP) has become the gold standard for privacy-preserving\ndata analysis, but implementing it correctly has proven challenging. Prior work\nhas focused on verifying DP at a high level, assuming the foundations are\ncorrect and a perfect source of randomness is available. However, the\nunderlying theory of differential privacy can be very complex and subtle. Flaws\nin basic mechanisms and random number generation have been a critical source of\nvulnerabilities in real-world DP systems.\n  In this paper, we present SampCert, the first comprehensive, mechanized\nfoundation for differential privacy. SampCert is written in Lean with over\n12,000 lines of proof. It offers a generic and extensible notion of DP, a\nframework for constructing and composing DP mechanisms, and formally verified\nimplementations of Laplace and Gaussian sampling algorithms. SampCert provides\n(1) a mechanized foundation for developing the next generation of\ndifferentially private algorithms, and (2) mechanically verified primitives\nthat can be deployed in production systems. Indeed, SampCert's verified\nalgorithms power the DP offerings of Amazon Web Services (AWS), demonstrating\nits real-world impact.\n  SampCert's key innovations include: (1) A generic DP foundation that can be\ninstantiated for various DP definitions (e.g., pure, concentrated, R\\'enyi DP);\n(2) formally verified discrete Laplace and Gaussian sampling algorithms that\navoid the pitfalls of floating-point implementations; and (3) a simple\nprobability monad and novel proof techniques that streamline the formalization.\nTo enable proving complex correctness properties of DP and random number\ngeneration, SampCert makes heavy use of Lean's extensive Mathlib library,\nleveraging theorems in Fourier analysis, measure and probability theory, number\ntheory, and topology.",
    "updated" : "2024-12-03T18:53:16Z",
    "published" : "2024-12-02T16:19:47Z",
    "authors" : [
      {
        "name" : "Markus de Medeiros"
      },
      {
        "name" : "Muhammad Naveed"
      },
      {
        "name" : "Tancrede Lepoint"
      },
      {
        "name" : "Temesghen Kahsai"
      },
      {
        "name" : "Tristan Ravitch"
      },
      {
        "name" : "Stefan Zetzsche"
      },
      {
        "name" : "Anjali Joshi"
      },
      {
        "name" : "Joseph Tassarotti"
      },
      {
        "name" : "Aws Albarghouthi"
      },
      {
        "name" : "Jean-Baptiste Tristan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01650v2",
    "title" : "Privacy-Preserving Federated Learning via Homomorphic Adversarial\n  Networks",
    "summary" : "Privacy-preserving federated learning (PPFL) aims to train a global model for\nmultiple clients while maintaining their data privacy. However, current PPFL\nprotocols exhibit one or more of the following insufficiencies: considerable\ndegradation in accuracy, the requirement for sharing keys, and cooperation\nduring the key generation or decryption processes. As a mitigation, we develop\nthe first protocol that utilizes neural networks to implement PPFL, as well as\nincorporating an Aggregatable Hybrid Encryption scheme tailored to the needs of\nPPFL. We name these networks as Homomorphic Adversarial Networks (HANs) which\ndemonstrate that neural networks are capable of performing tasks similar to\nmulti-key homomorphic encryption (MK-HE) while solving the problems of key\ndistribution and collaborative decryption. Our experiments show that HANs are\nrobust against privacy attacks. Compared with non-private federated learning,\nexperiments conducted on multiple datasets demonstrate that HANs exhibit a\nnegligible accuracy loss (at most 1.35%). Compared to traditional MK-HE\nschemes, HANs increase encryption aggregation speed by 6,075 times while\nincurring a 29.2 times increase in communication overhead.",
    "updated" : "2024-12-03T05:46:35Z",
    "published" : "2024-12-02T15:59:35Z",
    "authors" : [
      {
        "name" : "Wenhan Dong"
      },
      {
        "name" : "Chao Lin"
      },
      {
        "name" : "Xinlei He"
      },
      {
        "name" : "Xinyi Huang"
      },
      {
        "name" : "Shengmin Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01541v1",
    "title" : "Effectiveness of L2 Regularization in Privacy-Preserving Machine\n  Learning",
    "summary" : "Artificial intelligence, machine learning, and deep learning as a service\nhave become the status quo for many industries, leading to the widespread\ndeployment of models that handle sensitive data. Well-performing models, the\nindustry seeks, usually rely on a large volume of training data. However, the\nuse of such data raises serious privacy concerns due to the potential risks of\nleaks of highly sensitive information. One prominent threat is the Membership\nInference Attack, where adversaries attempt to deduce whether a specific data\npoint was used in a model's training process. An adversary's ability to\ndetermine an individual's presence represents a significant privacy threat,\nespecially when related to a group of users sharing sensitive information.\nHence, well-designed privacy-preserving machine learning solutions are\ncritically needed in the industry. In this work, we compare the effectiveness\nof L2 regularization and differential privacy in mitigating Membership\nInference Attack risks. Even though regularization techniques like L2\nregularization are commonly employed to reduce overfitting, a condition that\nenhances the effectiveness of Membership Inference Attacks, their impact on\nmitigating these attacks has not been systematically explored.",
    "updated" : "2024-12-02T14:31:11Z",
    "published" : "2024-12-02T14:31:11Z",
    "authors" : [
      {
        "name" : "Nikolaos Chandrinos"
      },
      {
        "name" : "Iliana Loi"
      },
      {
        "name" : "Panagiotis Zachos"
      },
      {
        "name" : "Ioannis Symeonidis"
      },
      {
        "name" : "Aristotelis Spiliotis"
      },
      {
        "name" : "Maria Panou"
      },
      {
        "name" : "Konstantinos Moustakas"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01141v1",
    "title" : "Lossless and Privacy-Preserving Graph Convolution Network for Federated\n  Item Recommendation",
    "summary" : "Graph neural network (GNN) has emerged as a state-of-the-art solution for\nitem recommendation. However, existing GNN-based recommendation methods rely on\na centralized storage of fragmented user-item interaction sub-graphs and\ntraining on an aggregated global graph, which will lead to privacy concerns. As\na response, some recent works develop GNN-based federated recommendation\nmethods by exploiting decentralized and fragmented user-item sub-graphs in\norder to preserve user privacy. However, due to privacy constraints, the graph\nconvolution process in existing federated recommendation methods is incomplete\ncompared with the centralized counterpart, causing a degradation of the\nrecommendation performance. In this paper, we propose a novel lossless and\nprivacy-preserving graph convolution network (LP-GCN), which fully completes\nthe graph convolution process with decentralized user-item interaction\nsub-graphs while ensuring privacy. It is worth mentioning that its performance\nis equivalent to that of the non-federated (i.e., centralized) counterpart.\nMoreover, we validate its effectiveness through both theoretical analysis and\nempirical studies. Extensive experiments on three real-world datasets show that\nour LP-GCN outperforms the existing federated recommendation methods. The code\nwill be publicly available once the paper is accepted.",
    "updated" : "2024-12-02T05:31:22Z",
    "published" : "2024-12-02T05:31:22Z",
    "authors" : [
      {
        "name" : "Guowei Wu"
      },
      {
        "name" : "Weike Pan"
      },
      {
        "name" : "Qiang Yang"
      },
      {
        "name" : "Zhong Ming"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01079v1",
    "title" : "Federated Motor Imagery Classification for Privacy-Preserving\n  Brain-Computer Interfaces",
    "summary" : "Training an accurate classifier for EEG-based brain-computer interface (BCI)\nrequires EEG data from a large number of users, whereas protecting their data\nprivacy is a critical consideration. Federated learning (FL) is a promising\nsolution to this challenge. This paper proposes Federated classification with\nlocal Batch-specific batch normalization and Sharpness-aware minimization\n(FedBS) for privacy protection in EEG-based motor imagery (MI) classification.\nFedBS utilizes local batch-specific batch normalization to reduce data\ndiscrepancies among different clients, and sharpness-aware minimization\noptimizer in local training to improve model generalization. Experiments on\nthree public MI datasets using three popular deep learning models demonstrated\nthat FedBS outperformed six state-of-the-art FL approaches. Remarkably, it also\noutperformed centralized training, which does not consider privacy protection\nat all. In summary, FedBS protects user EEG data privacy, enabling multiple BCI\nusers to participate in large-scale machine learning model training, which in\nturn improves the BCI decoding accuracy.",
    "updated" : "2024-12-02T03:35:27Z",
    "published" : "2024-12-02T03:35:27Z",
    "authors" : [
      {
        "name" : "Tianwang Jia"
      },
      {
        "name" : "Lubin Meng"
      },
      {
        "name" : "Siyang Li"
      },
      {
        "name" : "Jiajing Liu"
      },
      {
        "name" : "Dongrui Wu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01072v1",
    "title" : "When Fine-Tuning LLMs Meets Data Privacy: An Empirical Study of\n  Federated Learning in LLM-Based Program Repair",
    "summary" : "Software systems have been evolving rapidly and inevitably introducing bugs\nat an increasing rate, leading to significant losses in resources consumed by\nsoftware maintenance. Recently, large language models (LLMs) have demonstrated\nremarkable potential in enhancing software development and maintenance\npractices, particularly in automated program repair (APR) with improved\naccuracy and efficiency of bug fixing. However, LLM-based APR heavily relies on\nhigh-quality code repositories. A larger portion of existing code repositories\nare for private use and proprietary assets from various industries, reflecting\nmore diversity and nuances in the data since real-world industries often have\nmore extensive software development practices, which cannot be covered by\nmerely public datasets. Therefore, utilizing private datasets shows significant\npotential in enhancing software development and maintenance. However, obtaining\nsuch data from various industries is hindered by data privacy concerns, as\ncompanies are reluctant to share their codebases. To address the gap, we\ninvestigate the use of federated learning as a privacy-preserving approach that\nenables private entities to fine-tune LLMs on proprietary and decentralized\ndata, facilitating the collaboration between clients to fully utilize their\ndata to help enhance software development and maintenance. Our evaluation\nreveals that federated fine-tuning can effectively enhance program repair\ncapabilities. Notably, the impact of heterogeneous code on LLM fine-tuning is\nnegligible, indicating that real-world industries can benefit from\ncollaborative development regardless of diverse data distributions.\nFurthermore, each type of federated algorithm exhibits unique strengths across\ndifferent LLMs, suggesting that fine-tuning for program repair can be enhanced\nby tailoring the optimization process to specific characteristics of different\nLLMs.",
    "updated" : "2024-12-02T03:18:47Z",
    "published" : "2024-12-02T03:18:47Z",
    "authors" : [
      {
        "name" : "Wenqiang Luo"
      },
      {
        "name" : "Jacky Wai Keung"
      },
      {
        "name" : "Boyang Yang"
      },
      {
        "name" : "He Ye"
      },
      {
        "name" : "Claire Le Goues"
      },
      {
        "name" : "Tegawende F. Bissyande"
      },
      {
        "name" : "Haoye Tian"
      },
      {
        "name" : "Bach Le"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01056v1",
    "title" : "Classifying Simulated Gait Impairments using Privacy-preserving\n  Explainable Artificial Intelligence and Mobile Phone Videos",
    "summary" : "Accurate diagnosis of gait impairments is often hindered by subjective or\ncostly assessment methods, with current solutions requiring either expensive\nmulti-camera equipment or relying on subjective clinical observation. There is\na critical need for accessible, objective tools that can aid in gait assessment\nwhile preserving patient privacy. In this work, we present a mobile\nphone-based, privacy-preserving artificial intelligence (AI) system for\nclassifying gait impairments and introduce a novel dataset of 743 videos\ncapturing seven distinct gait patterns. The dataset consists of frontal and\nsagittal views of trained subjects simulating normal gait and six types of\npathological gait (circumduction, Trendelenburg, antalgic, crouch,\nParkinsonian, and vaulting), recorded using standard mobile phone cameras. Our\nsystem achieved 86.5% accuracy using combined frontal and sagittal views, with\nsagittal views generally outperforming frontal views except for specific gait\npatterns like Circumduction. Model feature importance analysis revealed that\nfrequency-domain features and entropy measures were critical for classifcation\nperformance, specifically lower limb keypoints proved most important for\nclassification, aligning with clinical understanding of gait assessment. These\nfindings demonstrate that mobile phone-based systems can effectively classify\ndiverse gait patterns while preserving privacy through on-device processing.\nThe high accuracy achieved using simulated gait data suggests their potential\nfor rapid prototyping of gait analysis systems, though clinical validation with\npatient data remains necessary. This work represents a significant step toward\naccessible, objective gait assessment tools for clinical, community, and\ntele-rehabilitation settings",
    "updated" : "2024-12-02T02:35:40Z",
    "published" : "2024-12-02T02:35:40Z",
    "authors" : [
      {
        "name" : "Lauhitya Reddy"
      },
      {
        "name" : "Ketan Anand"
      },
      {
        "name" : "Shoibolina Kaushik"
      },
      {
        "name" : "Corey Rodrigo"
      },
      {
        "name" : "J. Lucas McKay"
      },
      {
        "name" : "Trisha M. Kesar"
      },
      {
        "name" : "Hyeokhyen Kwon"
      }
    ],
    "categories" : [
      "cs.CV",
      "I.2.10"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.00898v1",
    "title" : "Preserving Privacy in Software Composition Analysis: A Study of\n  Technical Solutions and Enhancements",
    "summary" : "Software composition analysis (SCA) denotes the process of identifying\nopen-source software components in an input software application. SCA has been\nextensively developed and adopted by academia and industry. However, we notice\nthat the modern SCA techniques in industry scenarios still need to be improved\ndue to privacy concerns. Overall, SCA requires the users to upload their\napplications' source code to a remote SCA server, which then inspects the\napplications and reports the component usage to users. This process is\nprivacy-sensitive since the applications may contain sensitive information,\nsuch as proprietary source code, algorithms, trade secrets, and user data.\n  Privacy concerns have prevented the SCA technology from being used in\nreal-world scenarios. Therefore, academia and the industry demand\nprivacy-preserving SCA solutions. For the first time, we analyze the privacy\nrequirements of SCA and provide a landscape depicting possible technical\nsolutions with varying privacy gains and overheads. In particular, given that\nde facto SCA frameworks are primarily driven by code similarity-based\ntechniques, we explore combining several privacy-preserving protocols to\nencapsulate the similarity-based SCA framework. Among all viable solutions, we\nfind that multi-party computation (MPC) offers the strongest privacy guarantee\nand plausible accuracy; it, however, incurs high overhead (184 times). We\noptimize the MPC-based SCA framework by reducing the amount of crypto protocol\ntransactions using program analysis techniques. The evaluation results show\nthat our proposed optimizations can reduce the MPC-based SCA overhead to only\n8.5% without sacrificing SCA's privacy guarantee or accuracy.",
    "updated" : "2024-12-01T17:17:29Z",
    "published" : "2024-12-01T17:17:29Z",
    "authors" : [
      {
        "name" : "Huaijin Wang"
      },
      {
        "name" : "Zhibo Liu"
      },
      {
        "name" : "Yanbo Dai"
      },
      {
        "name" : "Shuai Wang"
      },
      {
        "name" : "Qiyi Tang"
      },
      {
        "name" : "Sen Nie"
      },
      {
        "name" : "Shi Wu"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.00774v1",
    "title" : "Post-Vaccination COVID-19 Data Analysis: Privacy and Ethics",
    "summary" : "The COVID-19 pandemic has severely affected the world in terms of health,\neconomy and peace. Fortunately, the countries are trying to overcome the\nsituation by actively carrying out vaccinations. However, like any other\nmassive operation involving humans such as human resource management,\nelections, surveys, etc., the vaccination process raises several questions\nabout citizen privacy and misuse of personal data. In most of the countries,\nfew attempts have been made to verify the vaccination statistics as reported by\nthe health centers. These issues collectively require the solutions of\nanonymity of citizens' personal information, immutability of vaccination data\nand easy yet restricted access by adversarial bodies such as the government for\nthe verification and analysis of the data. This paper introduces a\nblockchain-based application to simulate and monitor the vaccination process.\nThe structure of data model used in the proposed system is based on the IEEE\nStandard for Data Format for Blockchain Systems 2418.2TM-2020. The proposed\nsystem enables authorized stakeholders to share and access relevant information\nfor vaccination process chain while preserving citizen privacy and\naccountability of the system. It is implemented on the Ethereum blockchain and\nuses a Python API for the simulation and validation of each step of the\nvaccination process.",
    "updated" : "2024-12-01T11:41:32Z",
    "published" : "2024-12-01T11:41:32Z",
    "authors" : [
      {
        "name" : "Sankha Das"
      },
      {
        "name" : "Amit Dua"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.00687v1",
    "title" : "Towards Privacy-Preserving Medical Imaging: Federated Learning with\n  Differential Privacy and Secure Aggregation Using a Modified ResNet\n  Architecture",
    "summary" : "With increasing concerns over privacy in healthcare, especially for sensitive\nmedical data, this research introduces a federated learning framework that\ncombines local differential privacy and secure aggregation using Secure\nMulti-Party Computation for medical image classification. Further, we propose\nDPResNet, a modified ResNet architecture optimized for differential privacy.\nLeveraging the BloodMNIST benchmark dataset, we simulate a realistic\ndata-sharing environment across different hospitals, addressing the distinct\nprivacy challenges posed by federated healthcare data. Experimental results\nindicate that our privacy-preserving federated model achieves accuracy levels\nclose to non-private models, surpassing traditional approaches while\nmaintaining strict data confidentiality. By enhancing the privacy, efficiency,\nand reliability of healthcare data management, our approach offers substantial\nbenefits to patients, healthcare providers, and the broader healthcare\necosystem.",
    "updated" : "2024-12-01T05:52:29Z",
    "published" : "2024-12-01T05:52:29Z",
    "authors" : [
      {
        "name" : "Mohamad Haj Fares"
      },
      {
        "name" : "Ahmed Mohamed Saad Emam Saad"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.00620v1",
    "title" : "TraCS: Trajectory Collection in Continuous Space under Local\n  Differential Privacy",
    "summary" : "Trajectory collection is fundamental for location-based services but often\ninvolves sensitive information, such as a user's daily routine, raising privacy\nconcerns. Local differential privacy (LDP) provides provable privacy guarantees\nfor users, even when the data collector is untrusted. Existing trajectory\ncollection methods ensure LDP only for discrete location spaces, where the\nnumber of locations affects their privacy guarantees and trajectory utility.\nMoreover, the location space is often naturally continuous, such as in flying\nand sailing trajectories, making these methods unsuitable. This paper proposes\ntwo trajectory collection methods that ensure LDP for continuous spaces:\nTraCS-D, which perturbs the direction and distance of locations, and TraCS-C,\nwhich perturbs the Cartesian coordinates of locations. Both methods are\ntheoretically and experimentally analyzed for trajectory utility. TraCS can\nalso be applied to discrete spaces by rounding perturbed locations to the\nnearest discrete points. It is independent of the number of locations and has\nonly $\\Theta(1)$ time complexity in each perturbation generation. Evaluation\nresults on discrete location spaces validate this advantage and show that TraCS\noutperforms state-of-the-art methods with improved trajectory utility,\nespecially for large privacy parameters.",
    "updated" : "2024-12-01T00:07:04Z",
    "published" : "2024-12-01T00:07:04Z",
    "authors" : [
      {
        "name" : "Ye Zheng"
      },
      {
        "name" : "Yidan Hu"
      }
    ],
    "categories" : [
      "cs.CR",
      "68P27"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02987v1",
    "title" : "Advancing Conversational Psychotherapy: Integrating Privacy,\n  Dual-Memory, and Domain Expertise with Large Language Models",
    "summary" : "Mental health has increasingly become a global issue that reveals the\nlimitations of traditional conversational psychotherapy, constrained by\nlocation, time, expense, and privacy concerns. In response to these challenges,\nwe introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed\nto democratize access to psychotherapy. SoulSpeak improves upon the\ncapabilities of standard LLM-enabled chatbots by incorporating a novel\ndual-memory component that combines short-term and long-term context via\nRetrieval Augmented Generation (RAG) to offer personalized responses while\nensuring the preservation of user privacy and intimacy through a dedicated\nprivacy module. In addition, it leverages a counseling chat dataset of\ntherapist-client interactions and various prompting techniques to align the\ngenerated responses with psychotherapeutic methods. We introduce two fine-tuned\nBERT models to evaluate the system against existing LLMs and human therapists:\nthe Conversational Psychotherapy Preference Model (CPPM) to simulate human\npreference among responses and another to assess response relevance to user\ninput. CPPM is useful for training and evaluating psychotherapy-focused\nlanguage models independent from SoulSpeak, helping with the constrained\nresources available for psychotherapy. Furthermore, the effectiveness of the\ndual-memory component and the robustness of the privacy module are also\nexamined. Our findings highlight the potential and challenge of enhancing\nmental health care by offering an alternative that combines the expertise of\ntraditional therapy with the advantages of LLMs, providing a promising way to\naddress the accessibility and personalization gap in current mental health\nservices.",
    "updated" : "2024-12-04T03:02:46Z",
    "published" : "2024-12-04T03:02:46Z",
    "authors" : [
      {
        "name" : "XiuYu Zhang"
      },
      {
        "name" : "Zening Luo"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02934v1",
    "title" : "BGTplanner: Maximizing Training Accuracy for Differentially Private\n  Federated Recommenders via Strategic Privacy Budget Allocation",
    "summary" : "To mitigate the rising concern about privacy leakage, the federated\nrecommender (FR) paradigm emerges, in which decentralized clients co-train the\nrecommendation model without exposing their raw user-item rating data. The\ndifferentially private federated recommender (DPFR) further enhances FR by\ninjecting differentially private (DP) noises into clients. Yet, current DPFRs,\nsuffering from noise distortion, cannot achieve satisfactory accuracy. Various\nefforts have been dedicated to improving DPFRs by adaptively allocating the\nprivacy budget over the learning process. However, due to the intricate\nrelation between privacy budget allocation and model accuracy, existing works\nare still far from maximizing DPFR accuracy. To address this challenge, we\ndevelop BGTplanner (Budget Planner) to strategically allocate the privacy\nbudget for each round of DPFR training, improving overall training performance.\nSpecifically, we leverage the Gaussian process regression and historical\ninformation to predict the change in recommendation accuracy with a certain\nallocated privacy budget. Additionally, Contextual Multi-Armed Bandit (CMAB) is\nharnessed to make privacy budget allocation decisions by reconciling the\ncurrent improvement and long-term privacy constraints. Our extensive\nexperimental results on real datasets demonstrate that \\emph{BGTplanner}\nachieves an average improvement of 6.76\\% in training performance compared to\nstate-of-the-art baselines.",
    "updated" : "2024-12-04T01:07:04Z",
    "published" : "2024-12-04T01:07:04Z",
    "authors" : [
      {
        "name" : "Xianzhi Zhang"
      },
      {
        "name" : "Yipeng Zhou"
      },
      {
        "name" : "Miao Hu"
      },
      {
        "name" : "Di Wu"
      },
      {
        "name" : "Pengshan Liao"
      },
      {
        "name" : "Mohsen Guizani"
      },
      {
        "name" : "Michael Sheng"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02868v1",
    "title" : "A Novel Compact LLM Framework for Local, High-Privacy EHR Data\n  Applications",
    "summary" : "Large Language Models (LLMs) have shown impressive capabilities in natural\nlanguage processing, yet their use in sensitive domains like healthcare,\nparticularly with Electronic Health Records (EHR), faces significant challenges\ndue to privacy concerns and limited computational resources. This paper\npresents a compact LLM framework designed for local deployment in settings with\nstrict privacy requirements and limited access to high-performance GPUs. We\nintroduce a novel preprocessing technique that uses information extraction\nmethods, e.g., regular expressions, to filter and emphasize critical\ninformation in clinical notes, enhancing the performance of smaller LLMs on EHR\ndata. Our framework is evaluated using zero-shot and few-shot learning\nparadigms on both private and publicly available (MIMIC-IV) datasets, and we\nalso compare its performance with fine-tuned LLMs on the MIMIC-IV dataset. The\nresults demonstrate that our preprocessing approach significantly boosts the\nprediction accuracy of smaller LLMs, making them suitable for high-privacy,\nresource-constrained applications. This study offers valuable insights into\noptimizing LLM performance for sensitive, data-intensive tasks while addressing\ncomputational and privacy limitations.",
    "updated" : "2024-12-03T22:06:55Z",
    "published" : "2024-12-03T22:06:55Z",
    "authors" : [
      {
        "name" : "Yixiang Qu"
      },
      {
        "name" : "Yifan Dai"
      },
      {
        "name" : "Shilin Yu"
      },
      {
        "name" : "Pradham Tanikella"
      },
      {
        "name" : "Travis Schrank"
      },
      {
        "name" : "Trevor Hackman"
      },
      {
        "name" : "Didong Li"
      },
      {
        "name" : "Di Wu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02538v2",
    "title" : "On Privacy, Security, and Trustworthiness in Distributed Wireless Large\n  AI Models (WLAM)",
    "summary" : "Combining wireless communication with large artificial intelligence (AI)\nmodels can open up a myriad of novel application scenarios. In sixth generation\n(6G) networks, ubiquitous communication and computing resources allow large AI\nmodels to serve democratic large AI models-related services to enable real-time\napplications like autonomous vehicles, smart cities, and Internet of Things\n(IoT) ecosystems. However, the security considerations and sustainable\ncommunication resources limit the deployment of large AI models over\ndistributed wireless networks. This paper provides a comprehensive overview of\nprivacy, security, and trustworthy for distributed wireless large AI model\n(WLAM). In particular, a detailed privacy and security are analysis for\ndistributed WLAM is fist revealed. The classifications and theoretical findings\nabout privacy and security in distributed WLAM are discussed. Then the\ntrustworthy and ethics for implementing distributed WLAM are described.\nFinally, the comprehensive applications of distributed WLAM are presented in\nthe context of electromagnetic signal processing.",
    "updated" : "2024-12-04T07:11:07Z",
    "published" : "2024-12-03T16:32:19Z",
    "authors" : [
      {
        "name" : "Zhaohui Yang"
      },
      {
        "name" : "Wei Xu"
      },
      {
        "name" : "Le Liang"
      },
      {
        "name" : "Yuanhao Cui"
      },
      {
        "name" : "Zhijin Qin"
      },
      {
        "name" : "Merouane Debbah"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.04408v1",
    "title" : "Providing Differential Privacy for Federated Learning Over Wireless: A\n  Cross-layer Framework",
    "summary" : "Federated Learning (FL) is a distributed machine learning framework that\ninherently allows edge devices to maintain their local training data, thus\nproviding some level of privacy. However, FL's model updates still pose a risk\nof privacy leakage, which must be mitigated. Over-the-air FL (OTA-FL) is an\nadapted FL design for wireless edge networks that leverages the natural\nsuperposition property of the wireless medium. We propose a wireless physical\nlayer (PHY) design for OTA-FL which improves differential privacy (DP) through\na decentralized, dynamic power control that utilizes both inherent Gaussian\nnoise in the wireless channel and a cooperative jammer (CJ) for additional\nartificial noise generation when higher privacy levels are required. Although\nprimarily implemented within the Upcycled-FL framework, where a\nresource-efficient method with first-order approximations is used at every even\niteration to decrease the required information from clients, our power control\nstrategy is applicable to any FL framework, including FedAvg and FedProx as\nshown in the paper. This adaptation showcases the flexibility and effectiveness\nof our design across different learning algorithms while maintaining a strong\nemphasis on privacy. Our design removes the need for client-side artificial\nnoise injection for DP, utilizing a cooperative jammer to enhance privacy\nwithout affecting transmission efficiency for higher privacy demands. Privacy\nanalysis is provided using the Moments Accountant method. We perform a\nconvergence analysis for non-convex objectives to tackle heterogeneous data\ndistributions, highlighting the inherent trade-offs between privacy and\naccuracy. Numerical results show that our approach with various FL algorithms\noutperforms the state-of-the-art under the same DP conditions on the non-i.i.d.\nFEMNIST dataset, and highlight the cooperative jammer's effectiveness in\nensuring strict privacy.",
    "updated" : "2024-12-05T18:27:09Z",
    "published" : "2024-12-05T18:27:09Z",
    "authors" : [
      {
        "name" : "Jiayu Mao"
      },
      {
        "name" : "Tongxin Yin"
      },
      {
        "name" : "Aylin Yener"
      },
      {
        "name" : "Mingyan Liu"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.04178v1",
    "title" : "Multi-Layer Privacy-Preserving Record Linkage with Clerical Review based\n  on gradual information disclosure",
    "summary" : "Privacy-Preserving Record linkage (PPRL) is an essential component in data\nintegration tasks of sensitive information. The linkage quality determines the\nusability of combined datasets and (machine learning) applications based on\nthem. We present a novel privacy-preserving protocol that integrates clerical\nreview in PPRL using a multi-layer active learning process. Uncertain match\ncandidates are reviewed on several layers by human and non-human oracles to\nreduce the amount of disclosed information per record and in total. Predictions\nare propagated back to update previous layers, resulting in an improved linkage\nperformance for non-reviewed candidates as well. The data owners remain in\ncontrol of the amount of information they share for each record. Therefore, our\napproach follows need-to-know and data sovereignty principles. The experimental\nevaluation on real-world datasets shows considerable linkage quality\nimprovements with limited labeling effort and privacy risks.",
    "updated" : "2024-12-05T14:18:50Z",
    "published" : "2024-12-05T14:18:50Z",
    "authors" : [
      {
        "name" : "Florens Rohde"
      },
      {
        "name" : "Victor Christen"
      },
      {
        "name" : "Martin Franke"
      },
      {
        "name" : "Erhard Rahm"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.04031v1",
    "title" : "Dimension Reduction via Random Projection for Privacy in Multi-Agent\n  Systems",
    "summary" : "The agents in a Multi-Agent System (MAS) make observations about the system\nand send that information to a fusion center. The fusion center aggregates the\ninformation and concludes about the system parameters with as much accuracy as\npossible. However for the purposes of better efficiency of the system at large,\nthe agents need to append some private parameters to the observed data. In this\nscenario, the data sent to the fusion center is faced with privacy risks. The\ndata communicated to the fusion center must be secured against data privacy\nbreaches and inference attacks in a decentralized manner. However, this in turn\nleads to a loss of utility of the data being sent to the fusion center. We\nquantify the utility and privacy of the system using Cosine similarity. We\nformulate our MAS problem in terms of deducing a concept for which\ncompression-based methods are there in literature. Next, we propose a novel\nsanitization mechanism for our MAS using one such compression-based method\nwhile addressing the utility-privacy tradeoff problem.",
    "updated" : "2024-12-05T10:09:13Z",
    "published" : "2024-12-05T10:09:13Z",
    "authors" : [
      {
        "name" : "Puspanjali Ghoshal"
      },
      {
        "name" : "Ashok Singh Sairam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.03924v1",
    "title" : "Privacy-Preserving in Medical Image Analysis: A Review of Methods and\n  Applications",
    "summary" : "With the rapid advancement of artificial intelligence and deep learning,\nmedical image analysis has become a critical tool in modern healthcare,\nsignificantly improving diagnostic accuracy and efficiency. However, AI-based\nmethods also raise serious privacy concerns, as medical images often contain\nhighly sensitive patient information. This review offers a comprehensive\noverview of privacy-preserving techniques in medical image analysis, including\nencryption, differential privacy, homomorphic encryption, federated learning,\nand generative adversarial networks. We explore the application of these\ntechniques across various medical image analysis tasks, such as diagnosis,\npathology, and telemedicine. Notably, we organizes the review based on specific\nchallenges and their corresponding solutions in different medical image\nanalysis applications, so that technical applications are directly aligned with\npractical issues, addressing gaps in the current research landscape.\nAdditionally, we discuss emerging trends, such as zero-knowledge proofs and\nsecure multi-party computation, offering insights for future research. This\nreview serves as a valuable resource for researchers and practitioners and can\nhelp advance privacy-preserving in medical image analysis.",
    "updated" : "2024-12-05T06:56:06Z",
    "published" : "2024-12-05T06:56:06Z",
    "authors" : [
      {
        "name" : "Yanming Zhu"
      },
      {
        "name" : "Xuefei Yin"
      },
      {
        "name" : "Alan Wee-Chung Liew"
      },
      {
        "name" : "Hui Tian"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.05183v1",
    "title" : "Privacy Drift: Evolving Privacy Concerns in Incremental Learning",
    "summary" : "In the evolving landscape of machine learning (ML), Federated Learning (FL)\npresents a paradigm shift towards decentralized model training while preserving\nuser data privacy. This paper introduces the concept of ``privacy drift\", an\ninnovative framework that parallels the well-known phenomenon of concept drift.\nWhile concept drift addresses the variability in model accuracy over time due\nto changes in the data, privacy drift encapsulates the variation in the leakage\nof private information as models undergo incremental training. By defining and\nexamining privacy drift, this study aims to unveil the nuanced relationship\nbetween the evolution of model performance and the integrity of data privacy.\nThrough rigorous experimentation, we investigate the dynamics of privacy drift\nin FL systems, focusing on how model updates and data distribution shifts\ninfluence the susceptibility of models to privacy attacks, such as membership\ninference attacks (MIA). Our results highlight a complex interplay between\nmodel accuracy and privacy safeguards, revealing that enhancements in model\nperformance can lead to increased privacy risks. We provide empirical evidence\nfrom experiments on customized datasets derived from CIFAR-100 (Canadian\nInstitute for Advanced Research, 100 classes), showcasing the impact of data\nand concept drift on privacy. This work lays the groundwork for future research\non privacy-aware machine learning, aiming to achieve a delicate balance between\nmodel accuracy and data privacy in decentralized environments.",
    "updated" : "2024-12-06T17:04:09Z",
    "published" : "2024-12-06T17:04:09Z",
    "authors" : [
      {
        "name" : "Sayyed Farid Ahamed"
      },
      {
        "name" : "Soumya Banerjee"
      },
      {
        "name" : "Sandip Roy"
      },
      {
        "name" : "Aayush Kapoor"
      },
      {
        "name" : "Marc Vucovich"
      },
      {
        "name" : "Kevin Choi"
      },
      {
        "name" : "Abdul Rahman"
      },
      {
        "name" : "Edward Bowen"
      },
      {
        "name" : "Sachin Shetty"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.05164v1",
    "title" : "A Differentially Private Kaplan-Meier Estimator for Privacy-Preserving\n  Survival Analysis",
    "summary" : "This paper presents a differentially private approach to Kaplan-Meier\nestimation that achieves accurate survival probability estimates while\nsafeguarding individual privacy. The Kaplan-Meier estimator is widely used in\nsurvival analysis to estimate survival functions over time, yet applying it to\nsensitive datasets, such as clinical records, risks revealing private\ninformation. To address this, we introduce a novel algorithm that applies\ntime-indexed Laplace noise, dynamic clipping, and smoothing to produce a\nprivacy-preserving survival curve while maintaining the cumulative structure of\nthe Kaplan-Meier estimator. By scaling noise over time, the algorithm accounts\nfor decreasing sensitivity as fewer individuals remain at risk, while dynamic\nclipping and smoothing prevent extreme values and reduce fluctuations,\npreserving the natural shape of the survival curve.\n  Our results, evaluated on the NCCTG lung cancer dataset, show that the\nproposed method effectively lowers root mean squared error (RMSE) and enhances\naccuracy across privacy budgets ($\\epsilon$). At $\\epsilon = 10$, the algorithm\nachieves an RMSE as low as 0.04, closely approximating non-private estimates.\nAdditionally, membership inference attacks reveal that higher $\\epsilon$ values\n(e.g., $\\epsilon \\geq 6$) significantly reduce influential points, particularly\nat higher thresholds, lowering susceptibility to inference attacks. These\nfindings confirm that our approach balances privacy and utility, advancing\nprivacy-preserving survival analysis.",
    "updated" : "2024-12-06T16:29:53Z",
    "published" : "2024-12-06T16:29:53Z",
    "authors" : [
      {
        "name" : "Narasimha Raghavan Veeraragavan"
      },
      {
        "name" : "Sai Praneeth Karimireddy"
      },
      {
        "name" : "Jan Franz Nygrd"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.04697v1",
    "title" : "Privacy-Preserving Retrieval Augmented Generation with Differential\n  Privacy",
    "summary" : "With the recent remarkable advancement of large language models (LLMs), there\nhas been a growing interest in utilizing them in the domains with highly\nsensitive data that lies outside their training data. For this purpose,\nretrieval augmented generation (RAG) is particularly effective -- it assists\nLLMs by directly providing relevant information from the external knowledge\nsources. However, without extra privacy safeguards, RAG outputs risk leaking\nsensitive information from the external data source. In this work, we explore\nRAG under differential privacy (DP), a formal guarantee of data privacy. The\nmain challenge with differentially private RAG is how to generate long accurate\nanswers within a moderate privacy budget. We address this by proposing an\nalgorithm that smartly spends privacy budget only for the tokens that require\nthe sensitive information and uses the non-private LLM for other tokens. Our\nextensive empirical evaluations reveal that our algorithm outperforms the\nnon-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$\nacross different models and datasets.",
    "updated" : "2024-12-06T01:20:16Z",
    "published" : "2024-12-06T01:20:16Z",
    "authors" : [
      {
        "name" : "Tatsuki Koga"
      },
      {
        "name" : "Ruihan Wu"
      },
      {
        "name" : "Kamalika Chaudhuri"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.04518v1",
    "title" : "Privacy-Preserving Gesture Tracking System Utilizing Frequency-Hopping\n  RFID Signals",
    "summary" : "Gesture tracking technology provides users with a hands free interactive\nexperience without the need to hold or touch devices. However, current gesture\ntracking research has primarily focused on tracking accuracy while neglecting\nissues of user privacy protection and security. This study aims to develop a\ngesture tracking system based on frequency hopping RFID signals that\neffectively protects user privacy without compromising tracking efficiency and\naccuracy. By introducing frequency hopping technology, we have designed a\nmechanism that prevents potential eavesdroppers from obtaining raw RFID\nsignals, thereby enhancing the systems privacy protection capabilities. The\nsystem architec ture includes the collection of RFID signals, data processing,\nsignal recovery, and gesture tracking. Experimental results show that our\nmethod significantly improves privacy protection levels while maintaining real\ntime and accuracy. This research not only provides a new perspective for the\nfield of gesture tracking but also offers valuable insights for the use of RFID\ntechnology in privacy-sensitive applications.",
    "updated" : "2024-12-05T08:51:02Z",
    "published" : "2024-12-05T08:51:02Z",
    "authors" : [
      {
        "name" : "Bojun Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.06728v1",
    "title" : "Byzantine-Eavesdropper Alliance: How to Achieve Symmetric Privacy in\n  Quantum $X$-Secure $B$-Byzantine $E$-Eavesdropped $U$-Unresponsive\n  $T$-Colluding PIR?",
    "summary" : "We consider the quantum \\emph{symmetric} private information retrieval\n(QSPIR) problem in a system with $N$ databases and $K$ messages, with $U$\nunresponsive servers, $T$-colluding servers, and $X$-security parameter, under\nseveral fundamental threat models. In the first model, there are\n$\\mathcal{E}_1$ eavesdropped links in the uplink direction (the direction from\nthe user to the $N$ servers), $\\mathcal{E}_2$ eavesdropped links in the\ndownlink direction (the direction from the servers to the user), where\n$|\\mathcal{E}_1|, |\\mathcal{E}_2| \\leq E$; we coin this eavesdropper setting as\n\\emph{dynamic} eavesdroppers. We show that super-dense coding gain can be\nachieved for some regimes. In the second model, we consider the case with\nByzantine servers, i.e., servers that can coordinate to devise a plan to harm\nthe privacy and security of the system together with static eavesdroppers, by\nlistening to the same links in both uplink and downlink directions. It is\nimportant to note the considerable difference between the two threat models,\nsince the eavesdroppers can take huge advantage of the presence of the\nByzantine servers. Unlike the previous works in SPIR with Byzantine servers,\nthat assume that the Byzantine servers can send only random symbols independent\nof the stored messages, we follow the definition of Byzantine servers in\n\\cite{byzantine_tpir}, where the Byzantine servers can send symbols that can be\nfunctions of the storage, queries, as well as the random symbols in a way that\ncan produce worse harm to the system. In the third and the most novel threat\nmodel, we consider the presence of Byzantine servers and dynamic eavesdroppers\ntogether. We show that having dynamic eavesdroppers along with Byzantine\nservers in the same system model creates more threats to the system than having\nstatic eavesdroppers with Byzantine servers.",
    "updated" : "2024-12-09T18:17:24Z",
    "published" : "2024-12-09T18:17:24Z",
    "authors" : [
      {
        "name" : "Mohamed Nomeir"
      },
      {
        "name" : "Alptug Aytekin"
      },
      {
        "name" : "Sennur Ulukus"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.NI",
      "eess.SP",
      "math.IT",
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.06689v1",
    "title" : "Impact of Privacy Parameters on Deep Learning Models for Image\n  Classification",
    "summary" : "The project aims to develop differentially private deep learning models for\nimage classification on CIFAR-10 datasets \\cite{cifar10} and analyze the impact\nof various privacy parameters on model accuracy. We have implemented five\ndifferent deep learning models, namely ConvNet, ResNet18, EfficientNet, ViT,\nand DenseNet121 and three supervised classifiers namely K-Nearest Neighbors,\nNaive Bayes Classifier and Support Vector Machine. We evaluated the performance\nof these models under varying settings. Our best performing model to date is\nEfficientNet with test accuracy of $59.63\\%$ with the following parameters\n(Adam optimizer, batch size 256, epoch size 100, epsilon value 5.0, learning\nrate $1e-3$, clipping threshold 1.0, and noise multiplier 0.912).",
    "updated" : "2024-12-09T17:31:55Z",
    "published" : "2024-12-09T17:31:55Z",
    "authors" : [
      {
        "name" : "Basanta Chaulagain"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.06541v1",
    "title" : "Numerical Estimation of Spatial Distributions under Differential Privacy",
    "summary" : "Estimating spatial distributions is important in data analysis, such as\ntraffic flow forecasting and epidemic prevention. To achieve accurate spatial\ndistribution estimation, the analysis needs to collect sufficient user data.\nHowever, collecting data directly from individuals could compromise their\nprivacy. Most previous works focused on private distribution estimation for\none-dimensional data, which does not consider spatial data relation and leads\nto poor accuracy for spatial distribution estimation. In this paper, we address\nthe problem of private spatial distribution estimation, where we collect\nspatial data from individuals and aim to minimize the distance between the\nactual distribution and estimated one under Local Differential Privacy (LDP).\nTo leverage the numerical nature of the domain, we project spatial data and its\nrelationships onto a one-dimensional distribution. We then use this projection\nto estimate the overall spatial distribution. Specifically, we propose a\nreporting mechanism called Disk Area Mechanism (DAM), which projects the\nspatial domain onto a line and optimizes the estimation using the sliced\nWasserstein distance. Through extensive experiments, we show the effectiveness\nof our DAM approach on both real and synthetic data sets, compared with the\nstate-of-the-art methods, such as Multi-dimensional Square Wave Mechanism\n(MDSW) and Subset Exponential Mechanism with Geo-I (SEM-Geo-I). Our results\nshow that our DAM always performs better than MDSW and is better than SEM-Geo-I\nwhen the data granularity is fine enough.",
    "updated" : "2024-12-09T14:53:57Z",
    "published" : "2024-12-09T14:53:57Z",
    "authors" : [
      {
        "name" : "Leilei Du"
      },
      {
        "name" : "Peng Cheng"
      },
      {
        "name" : "Libin Zheng"
      },
      {
        "name" : "Xiang Lian"
      },
      {
        "name" : "Lei Chen"
      },
      {
        "name" : "Wei Xi"
      },
      {
        "name" : "Wangze Ni"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.06248v1",
    "title" : "Rendering-Refined Stable Diffusion for Privacy Compliant Synthetic Data",
    "summary" : "Growing privacy concerns and regulations like GDPR and CCPA necessitate\npseudonymization techniques that protect identity in image datasets. However,\nretaining utility is also essential. Traditional methods like masking and\nblurring degrade quality and obscure critical context, especially in\nhuman-centric images. We introduce Rendering-Refined Stable Diffusion (RefSD),\na pipeline that combines 3D-rendering with Stable Diffusion, enabling\nprompt-based control over human attributes while preserving posture. Unlike\nstandard diffusion models that fail to retain posture or GANs that lack realism\nand flexible attribute control, RefSD balances posture preservation, realism,\nand customization. We also propose HumanGenAI, a framework for human perception\nand utility evaluation. Human perception assessments reveal attribute-specific\nstrengths and weaknesses of RefSD. Our utility experiments show that models\ntrained on RefSD pseudonymized data outperform those trained on real data in\ndetection tasks, with further performance gains when combining RefSD with real\ndata. For classification tasks, we consistently observe performance\nimprovements when using RefSD data with real data, confirming the utility of\nour pseudonymized data.",
    "updated" : "2024-12-09T06:47:29Z",
    "published" : "2024-12-09T06:47:29Z",
    "authors" : [
      {
        "name" : "Kartik Patwari"
      },
      {
        "name" : "David Schneider"
      },
      {
        "name" : "Xiaoxiao Sun"
      },
      {
        "name" : "Chen-Nee Chuah"
      },
      {
        "name" : "Lingjuan Lyu"
      },
      {
        "name" : "Vivek Sharma"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.06196v1",
    "title" : "BECS: A Privacy-Preserving Computing Sharing Mechanism in 6G Computing\n  Power Network",
    "summary" : "5G networks provide secure and reliable information transmission services for\nthe Internet of Everything, thus paving the way for 6G networks, which is\nanticipated to be an AI-based network, supporting unprecedented intelligence\nacross applications. Abundant computing resources will establish the 6G\nComputing Power Network (CPN) to facilitate ubiquitous intelligent services. In\nthis article, we propose BECS, a computing sharing mechanism based on\nevolutionary algorithm and blockchain, designed to balance task offloading\namong user devices, edge devices, and cloud resources within 6G CPN, thereby\nenhancing the computing resource utilization. We model computing sharing as a\nmulti-objective optimization problem, aiming to improve resource utilization\nwhile balancing other issues. To tackle this NP-hard problem, we devise a\nkernel distance-based dominance relation and incorporated it into the\nNon-dominated Sorting Genetic Algorithm III, significantly enhancing the\ndiversity of the evolutionary population. In addition, we propose a pseudonym\nscheme based on zero-knowledge proof to protect the privacy of users\nparticipating in computing sharing. Finally, the security analysis and\nsimulation results demonstrate that BECS can fully and effectively utilize all\ncomputing resources in 6G CPN, significantly improving the computing resource\nutilization while protecting user privacy.",
    "updated" : "2024-12-09T04:26:14Z",
    "published" : "2024-12-09T04:26:14Z",
    "authors" : [
      {
        "name" : "Kun Yan"
      },
      {
        "name" : "Wenping Ma"
      },
      {
        "name" : "Shaohui Sun"
      }
    ],
    "categories" : [
      "cs.NI",
      "C.2.1; C.2.4"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.06120v1",
    "title" : "Lightweight Federated Learning with Differential Privacy and Straggler\n  Resilience",
    "summary" : "Federated learning (FL) enables collaborative model training through model\nparameter exchanges instead of raw data. To avoid potential inference attacks\nfrom exchanged parameters, differential privacy (DP) offers rigorous guarantee\nagainst various attacks. However, conventional methods of ensuring DP by adding\nlocal noise alone often result in low training accuracy. Combining secure\nmulti-party computation (SMPC) with DP, while improving the accuracy, incurs\nhigh communication and computation overheads and straggler vulnerability, in\neither client-to-server or client-to-client links. In this paper, we propose\nLightDP-FL, a novel lightweight scheme that ensures provable DP against\nuntrusted peers and server, while maintaining straggler-resilience, low\noverheads and high training accuracy. Our approach incorporates both individual\nand pairwise noise into each client's parameter, which can be implemented with\nminimal overheads. Given the uncertain straggler and colluder sets, we utilize\nthe upper bound on the numbers of stragglers and colluders to prove sufficient\nnoise variance conditions to ensure DP in the worst case. Moreover, we optimize\nthe expected convergence bound to ensure accuracy performance by flexibly\ncontrolling the noise variances. Using the CIFAR-10 dataset, our experimental\nresults demonstrate that LightDP-FL achieves faster convergence and stronger\nstraggler resilience of our scheme compared to baseline methods of the same DP\nlevel.",
    "updated" : "2024-12-09T00:54:00Z",
    "published" : "2024-12-09T00:54:00Z",
    "authors" : [
      {
        "name" : "Shu Hong"
      },
      {
        "name" : "Xiaojun Lin"
      },
      {
        "name" : "Lingjie Duan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.06113v1",
    "title" : "Privacy-Preserving Large Language Models: Mechanisms, Applications, and\n  Future Directions",
    "summary" : "The rapid advancement of large language models (LLMs) has revolutionized\nnatural language processing, enabling applications in diverse domains such as\nhealthcare, finance and education. However, the growing reliance on extensive\ndata for training and inference has raised significant privacy concerns,\nranging from data leakage to adversarial attacks. This survey comprehensively\nexplores the landscape of privacy-preserving mechanisms tailored for LLMs,\nincluding differential privacy, federated learning, cryptographic protocols,\nand trusted execution environments. We examine their efficacy in addressing key\nprivacy challenges, such as membership inference and model inversion attacks,\nwhile balancing trade-offs between privacy and model utility. Furthermore, we\nanalyze privacy-preserving applications of LLMs in privacy-sensitive domains,\nhighlighting successful implementations and inherent limitations. Finally, this\nsurvey identifies emerging research directions, emphasizing the need for novel\nframeworks that integrate privacy by design into the lifecycle of LLMs. By\nsynthesizing state-of-the-art approaches and future trends, this paper provides\na foundation for developing robust, privacy-preserving large language models\nthat safeguard sensitive information without compromising performance.",
    "updated" : "2024-12-09T00:24:09Z",
    "published" : "2024-12-09T00:24:09Z",
    "authors" : [
      {
        "name" : "Guoshenghui Zhao"
      },
      {
        "name" : "Eric Song"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.05894v1",
    "title" : "FedRBE -- a decentralized privacy-preserving federated batch effect\n  correction tool for omics data based on limma",
    "summary" : "Batch effects in omics data obscure true biological signals and constitute a\nmajor challenge for privacy-preserving analyses of distributed patient data.\nExisting batch effect correction methods either require data centralization,\nwhich may easily conflict with privacy requirements, or lack support for\nmissing values and automated workflows. To bridge this gap, we developed\nfedRBE, a federated implementation of limma's removeBatchEffect method. We\nimplemented it as an app for the FeatureCloud platform. Unlike its existing\nanalogs, fedRBE effectively handles data with missing values and offers an\nautomated, user-friendly online user interface\n(https://featurecloud.ai/app/fedrbe). Leveraging secure multi-party computation\nprovides enhanced security guarantees over classical federated learning\napproaches. We evaluated our fedRBE algorithm on simulated and real omics data,\nachieving performance comparable to the centralized method with negligible\ndifferences (no greater than 3.6E-13). By enabling collaborative correction\nwithout data sharing, fedRBE facilitates large-scale omics studies where batch\neffect correction is crucial.",
    "updated" : "2024-12-08T11:23:31Z",
    "published" : "2024-12-08T11:23:31Z",
    "authors" : [
      {
        "name" : "Yuliya Burankova"
      },
      {
        "name" : "Julian Klemm"
      },
      {
        "name" : "Jens J. G. Lohmann"
      },
      {
        "name" : "Ahmad Taheri"
      },
      {
        "name" : "Niklas Probul"
      },
      {
        "name" : "Jan Baumbach"
      },
      {
        "name" : "Olga Zolotareva"
      }
    ],
    "categories" : [
      "q-bio.QM",
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.05767v1",
    "title" : "DeMem: Privacy-Enhanced Robust Adversarial Learning via De-Memorization",
    "summary" : "Adversarial robustness, the ability of a model to withstand manipulated\ninputs that cause errors, is essential for ensuring the trustworthiness of\nmachine learning models in real-world applications. However, previous studies\nhave shown that enhancing adversarial robustness through adversarial training\nincreases vulnerability to privacy attacks. While differential privacy can\nmitigate these attacks, it often compromises robustness against both natural\nand adversarial samples. Our analysis reveals that differential privacy\ndisproportionately impacts low-risk samples, causing an unintended performance\ndrop. To address this, we propose DeMem, which selectively targets high-risk\nsamples, achieving a better balance between privacy protection and model\nrobustness. DeMem is versatile and can be seamlessly integrated into various\nadversarial training techniques. Extensive evaluations across multiple training\nmethods and datasets demonstrate that DeMem significantly reduces privacy\nleakage while maintaining robustness against both natural and adversarial\nsamples. These results confirm DeMem's effectiveness and broad applicability in\nenhancing privacy without compromising robustness.",
    "updated" : "2024-12-08T00:22:58Z",
    "published" : "2024-12-08T00:22:58Z",
    "authors" : [
      {
        "name" : "Xiaoyu Luo"
      },
      {
        "name" : "Qiongxiu Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.05734v1",
    "title" : "PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage",
    "summary" : "Recent studies have discovered that LLMs have serious privacy leakage\nconcerns, where an LLM may be fooled into outputting private information under\ncarefully crafted adversarial prompts. These risks include leaking system\nprompts, personally identifiable information, training data, and model\nparameters. Most existing red-teaming approaches for privacy leakage rely on\nhumans to craft the adversarial prompts. A few automated methods are proposed\nfor system prompt extraction, but they cannot be applied to more severe risks\n(e.g., training data extraction) and have limited effectiveness even for system\nprompt extraction.\n  In this paper, we propose PrivAgent, a novel black-box red-teaming framework\nfor LLM privacy leakage. We formulate different risks as a search problem with\na unified attack goal. Our framework trains an open-source LLM through\nreinforcement learning as the attack agent to generate adversarial prompts for\ndifferent target models under different risks. We propose a novel reward\nfunction to provide effective and fine-grained rewards for the attack agent.\nFinally, we introduce customizations to better fit our general framework to\nsystem prompt extraction and training data extraction. Through extensive\nevaluations, we first show that PrivAgent outperforms existing automated\nmethods in system prompt leakage against six popular LLMs. Notably, our\napproach achieves a 100% success rate in extracting system prompts from\nreal-world applications in OpenAI's GPT Store. We also show PrivAgent's\neffectiveness in extracting training data from an open-source LLM with a\nsuccess rate of 5.9%. We further demonstrate PrivAgent's effectiveness in\nevading the existing guardrail defense and its helpfulness in enabling better\nsafety alignment. Finally, we validate our customized designs through a\ndetailed ablation study. We release our code here\nhttps://github.com/rucnyz/RedAgent.",
    "updated" : "2024-12-07T20:09:01Z",
    "published" : "2024-12-07T20:09:01Z",
    "authors" : [
      {
        "name" : "Yuzhou Nie"
      },
      {
        "name" : "Zhun Wang"
      },
      {
        "name" : "Ye Yu"
      },
      {
        "name" : "Xian Wu"
      },
      {
        "name" : "Xuandong Zhao"
      },
      {
        "name" : "Wenbo Guo"
      },
      {
        "name" : "Dawn Song"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.05636v1",
    "title" : "A Game-Theoretic Framework for Privacy-Aware Client Sampling in\n  Federated Learning",
    "summary" : "This paper aims to design a Privacy-aware Client Sampling framework in\nFederated learning, named FedPCS, to tackle the heterogeneous client sampling\nissues and improve model performance. First, we obtain a pioneering upper bound\nfor the accuracy loss of the FL model with privacy-aware client sampling\nprobabilities. Based on this, we model the interactions between the central\nserver and participating clients as a two-stage Stackelberg game. In Stage I,\nthe central server designs the optimal time-dependent reward for cost\nminimization by considering the trade-off between the accuracy loss of the FL\nmodel and the rewards allocated. In Stage II, each client determines the\ncorrection factor that dynamically adjusts its privacy budget based on the\nreward allocated to maximize its utility. To surmount the obstacle of\napproximating other clients' private information, we introduce the mean-field\nestimator to estimate the average privacy budget. We analytically demonstrate\nthe existence and convergence of the fixed point for the mean-field estimator\nand derive the Stackelberg Nash Equilibrium to obtain the optimal strategy\nprofile. By rigorously theoretical convergence analysis, we guarantee the\nrobustness of FedPCS. Moreover, considering the conventional sampling strategy\nin privacy-preserving FL, we prove that the random sampling approach's PoA can\nbe arbitrarily large. To remedy such efficiency loss, we show that the proposed\nprivacy-aware client sampling strategy successfully reduces PoA, which is upper\nbounded by a reachable constant. To address the challenge of varying privacy\nrequirements throughout different training phases in FL, we extend our model\nand analysis and derive the adaptive optimal sampling ratio for the central\nserver. Experimental results on different datasets demonstrate the superiority\nof FedPCS compared with the existing SOTA FL strategies under IID and Non-IID\ndatasets.",
    "updated" : "2024-12-07T12:42:57Z",
    "published" : "2024-12-07T12:42:57Z",
    "authors" : [
      {
        "name" : "Wenhao Yuan"
      },
      {
        "name" : "Xuehe Wang"
      }
    ],
    "categories" : [
      "cs.GT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.05533v1",
    "title" : "Can large language models be privacy preserving and fair medical coders?",
    "summary" : "Protecting patient data privacy is a critical concern when deploying machine\nlearning algorithms in healthcare. Differential privacy (DP) is a common method\nfor preserving privacy in such settings and, in this work, we examine two key\ntrade-offs in applying DP to the NLP task of medical coding (ICD\nclassification). Regarding the privacy-utility trade-off, we observe a\nsignificant performance drop in the privacy preserving models, with more than a\n40% reduction in micro F1 scores on the top 50 labels in the MIMIC-III dataset.\nFrom the perspective of the privacy-fairness trade-off, we also observe an\nincrease of over 3% in the recall gap between male and female patients in the\nDP models. Further understanding these trade-offs will help towards the\nchallenges of real-world deployment.",
    "updated" : "2024-12-07T04:27:05Z",
    "published" : "2024-12-07T04:27:05Z",
    "authors" : [
      {
        "name" : "Ali Dadsetan"
      },
      {
        "name" : "Dorsa Soleymani"
      },
      {
        "name" : "Xijie Zeng"
      },
      {
        "name" : "Frank Rudzicz"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.07687v1",
    "title" : "Privacy-Preserving Customer Support: A Framework for Secure and Scalable\n  Interactions",
    "summary" : "The growing reliance on artificial intelligence (AI) in customer support has\nsignificantly improved operational efficiency and user experience. However,\ntraditional machine learning (ML) approaches, which require extensive local\ntraining on sensitive datasets, pose substantial privacy risks and compliance\nchallenges with regulations like the General Data Protection Regulation (GDPR)\nand California Consumer Privacy Act (CCPA). Existing privacy-preserving\ntechniques, such as anonymization, differential privacy, and federated\nlearning, address some concerns but face limitations in utility, scalability,\nand complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning\n(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in\na zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates\nthe need for local training on sensitive data by utilizing pre-trained LLMs to\ngenerate responses directly. The framework incorporates real-time data\nanonymization to redact or mask sensitive information, retrieval-augmented\ngeneration (RAG) for domain-specific query resolution, and robust\npost-processing to ensure compliance with regulatory standards. This\ncombination reduces privacy risks, simplifies compliance, and enhances\nscalability and operational efficiency. Empirical analysis demonstrates that\nthe PP-ZSL framework provides accurate, privacy-compliant responses while\nsignificantly lowering the costs and complexities of deploying AI-driven\ncustomer support systems. The study highlights potential applications across\nindustries, including financial services, healthcare, e-commerce, legal\nsupport, telecommunications, and government services. By addressing the dual\nchallenges of privacy and performance, this framework establishes a foundation\nfor secure, efficient, and regulatory-compliant AI applications in customer\ninteractions.",
    "updated" : "2024-12-10T17:20:47Z",
    "published" : "2024-12-10T17:20:47Z",
    "authors" : [
      {
        "name" : "Anant Prakash Awasthi"
      },
      {
        "name" : "Chandraketu Singh"
      },
      {
        "name" : "Rakshit Varma"
      },
      {
        "name" : "Sanchit Sharma"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.06960v1",
    "title" : "Simplications: Why and how we should rethink data of/by/for the people\n  in smart homes and its privacy implications",
    "summary" : "More and more smart devices enter our homes. Often these devices come with a\nvariety of sensors, mostly simple sensors, e.g., for light, temperature,\nhumidity or motion. And they all collect data. While it is data of the home\nenvironment it is also data of domestic life in the home. Thus it is data of\nthe people and by the people in the home capturing their presence, arrival and\ndeparture, typical domestic activities, bad habits, health status etc. Based on\nprevious as well as ongoing research we know that people are actually able to\nmake sense of simple sensor data and that they will make use of it for their\nown purposes. Simple sensors, when critically reflected, are often only\n\"simple\" in a technical sense. The unreflected design and use of these sensors\ncan easily lead to unintended implications, i.e. for privacy. However, it may\nnot even need a Big Brother or data experts or AI to make the data of these\nsensors sensitive, e.g., if used for lateral surveillance within families.\nOften unintended but wicked implications emerge despite good intentions, such\nas improving efficiency or energy saving through collecting sensor data. Thus\nsensor data from the home is actually data of/by/for the people in the home.\nFirst, we explain how this might have relevance across scales of community of\npeople - not only for the domain of the home but also in broader meaning.\nSecond, we relate our previous as well as ongoing research in the domain of\nsmart homes to this topic.",
    "updated" : "2024-12-09T20:08:26Z",
    "published" : "2024-12-09T20:08:26Z",
    "authors" : [
      {
        "name" : "Albrecht Kurze"
      },
      {
        "name" : "Alexa Becker"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.05767v2",
    "title" : "DeMem: Privacy-Enhanced Robust Adversarial Learning via De-Memorization",
    "summary" : "Adversarial robustness, the ability of a model to withstand manipulated\ninputs that cause errors, is essential for ensuring the trustworthiness of\nmachine learning models in real-world applications. However, previous studies\nhave shown that enhancing adversarial robustness through adversarial training\nincreases vulnerability to privacy attacks. While differential privacy can\nmitigate these attacks, it often compromises robustness against both natural\nand adversarial samples. Our analysis reveals that differential privacy\ndisproportionately impacts low-risk samples, causing an unintended performance\ndrop. To address this, we propose DeMem, which selectively targets high-risk\nsamples, achieving a better balance between privacy protection and model\nrobustness. DeMem is versatile and can be seamlessly integrated into various\nadversarial training techniques. Extensive evaluations across multiple training\nmethods and datasets demonstrate that DeMem significantly reduces privacy\nleakage while maintaining robustness against both natural and adversarial\nsamples. These results confirm DeMem's effectiveness and broad applicability in\nenhancing privacy without compromising robustness.",
    "updated" : "2024-12-10T16:59:55Z",
    "published" : "2024-12-08T00:22:58Z",
    "authors" : [
      {
        "name" : "Xiaoyu Luo"
      },
      {
        "name" : "Qiongxiu Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.08559v1",
    "title" : "Underestimated Privacy Risks for Minority Populations in Large Language\n  Model Unlearning",
    "summary" : "Large Language Models are trained on extensive datasets that often contain\nsensitive, human-generated information, raising significant concerns about\nprivacy breaches. While certified unlearning approaches offer strong privacy\nguarantees, they rely on restrictive model assumptions that are not applicable\nto LLMs. As a result, various unlearning heuristics have been proposed, with\nthe associated privacy risks assessed only empirically. The standard evaluation\npipelines typically randomly select data for removal from the training set,\napply unlearning techniques, and use membership inference attacks to compare\nthe unlearned models against models retrained without the to-be-unlearned data.\nHowever, since every data point is subject to the right to be forgotten,\nunlearning should be considered in the worst-case scenario from the privacy\nperspective. Prior work shows that data outliers may exhibit higher\nmemorization effects. Intuitively, they are harder to be unlearn and thus the\nprivacy risk of unlearning them is underestimated in the current evaluation. In\nthis paper, we leverage minority data to identify such a critical flaw in\npreviously widely adopted evaluations. We substantiate this claim through\ncarefully designed experiments, including unlearning canaries related to\nminority groups, inspired by privacy auditing literature. Using personally\nidentifiable information as a representative minority identifier, we\ndemonstrate that minority groups experience at least 20% more privacy leakage\nin most cases across six unlearning approaches, three MIAs, three benchmark\ndatasets, and two LLMs of different scales. Given that the right to be\nforgotten should be upheld for every individual, we advocate for a more\nrigorous evaluation of LLM unlearning methods. Our minority-aware evaluation\nframework represents an initial step toward ensuring more equitable assessments\nof LLM unlearning efficacy.",
    "updated" : "2024-12-11T17:22:07Z",
    "published" : "2024-12-11T17:22:07Z",
    "authors" : [
      {
        "name" : "Rongzhe Wei"
      },
      {
        "name" : "Mufei Li"
      },
      {
        "name" : "Mohsen Ghassemi"
      },
      {
        "name" : "Eleonora Kreai"
      },
      {
        "name" : "Yifan Li"
      },
      {
        "name" : "Xiang Yue"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Vamsi K. Potluru"
      },
      {
        "name" : "Pan Li"
      },
      {
        "name" : "Eli Chien"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.08544v1",
    "title" : "Training Data Reconstruction: Privacy due to Uncertainty?",
    "summary" : "Being able to reconstruct training data from the parameters of a neural\nnetwork is a major privacy concern. Previous works have shown that\nreconstructing training data, under certain circumstances, is possible. In this\nwork, we analyse such reconstructions empirically and propose a new formulation\nof the reconstruction as a solution to a bilevel optimisation problem. We\ndemonstrate that our formulation as well as previous approaches highly depend\non the initialisation of the training images $x$ to reconstruct. In particular,\nwe show that a random initialisation of $x$ can lead to reconstructions that\nresemble valid training samples while not being part of the actual training\ndataset. Thus, our experiments on affine and one-hidden layer networks suggest\nthat when reconstructing natural images, yet an adversary cannot identify\nwhether reconstructed images have indeed been part of the set of training\nsamples.",
    "updated" : "2024-12-11T17:00:29Z",
    "published" : "2024-12-11T17:00:29Z",
    "authors" : [
      {
        "name" : "Christina Runkel"
      },
      {
        "name" : "Kanchana Vaishnavi Gandikota"
      },
      {
        "name" : "Jonas Geiping"
      },
      {
        "name" : "Carola-Bibiane Schnlieb"
      },
      {
        "name" : "Michael Moeller"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.08534v1",
    "title" : "Protecting Confidentiality, Privacy and Integrity in Collaborative\n  Learning",
    "summary" : "A collaboration between dataset owners and model owners is needed to\nfacilitate effective machine learning (ML) training. During this collaboration,\nhowever, dataset owners and model owners want to protect the confidentiality of\ntheir respective assets (i.e., datasets, models and training code), with the\ndataset owners also caring about the privacy of individual users whose data is\nin their datasets. Existing solutions either provide limited confidentiality\nfor models and training code, or suffer from privacy issues due to collusion.\n  We present Citadel++, a scalable collaborative ML training system designed to\nsimultaneously protect the confidentiality of datasets, models and training\ncode, as well as the privacy of individual users. Citadel++ enhances\ndifferential privacy techniques to safeguard the privacy of individual user\ndata while maintaining model utility. By employing Virtual Machine-level\nTrusted Execution Environments (TEEs) and improved integrity protection\ntechniques through various OS-level mechanisms, Citadel++ effectively preserves\nthe confidentiality of datasets, models and training code, and enforces our\nprivacy mechanisms even when the models and training code have been maliciously\ndesigned. Our experiments show that Citadel++ provides privacy, model utility\nand performance while adhering to confidentiality and privacy requirements of\ndataset owners and model owners, outperforming the state-of-the-art\nprivacy-preserving training systems by up to 543x on CPU and 113x on GPU TEEs.",
    "updated" : "2024-12-11T16:48:18Z",
    "published" : "2024-12-11T16:48:18Z",
    "authors" : [
      {
        "name" : "Dong Chen"
      },
      {
        "name" : "Alice Dethise"
      },
      {
        "name" : "Istemi Ekin Akkus"
      },
      {
        "name" : "Ivica Rimac"
      },
      {
        "name" : "Klaus Satzke"
      },
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "Marco Canini"
      },
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Ruichuan Chen"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.08276v1",
    "title" : "Local Features Meet Stochastic Anonymization: Revolutionizing\n  Privacy-Preserving Face Recognition for Black-Box Models",
    "summary" : "The task of privacy-preserving face recognition (PPFR) currently faces two\nmajor unsolved challenges: (1) existing methods are typically effective only on\nspecific face recognition models and struggle to generalize to black-box face\nrecognition models; (2) current methods employ data-driven reversible\nrepresentation encoding for privacy protection, making them susceptible to\nadversarial learning and reconstruction of the original image. We observe that\nface recognition models primarily rely on local features ({e.g., face contour,\nskin texture, and so on) for identification. Thus, by disrupting global\nfeatures while enhancing local features, we achieve effective recognition even\nin black-box environments. Additionally, to prevent adversarial models from\nlearning and reversing the anonymization process, we adopt an adversarial\nlearning-based approach with irreversible stochastic injection to ensure the\nstochastic nature of the anonymization. Experimental results demonstrate that\nour method achieves an average recognition accuracy of 94.21\\% on black-box\nmodels, outperforming existing methods in both privacy protection and\nanti-reconstruction capabilities.",
    "updated" : "2024-12-11T10:49:15Z",
    "published" : "2024-12-11T10:49:15Z",
    "authors" : [
      {
        "name" : "Yuanwei Liu"
      },
      {
        "name" : "Chengyu Jia"
      },
      {
        "name" : "Ruqi Xiao"
      },
      {
        "name" : "Xuemai Jia"
      },
      {
        "name" : "Hui Wei"
      },
      {
        "name" : "Kui Jiang"
      },
      {
        "name" : "Zheng Wang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.06541v2",
    "title" : "Numerical Estimation of Spatial Distributions under Differential Privacy",
    "summary" : "Estimating spatial distributions is important in data analysis, such as\ntraffic flow forecasting and epidemic prevention. To achieve accurate spatial\ndistribution estimation, the analysis needs to collect sufficient user data.\nHowever, collecting data directly from individuals could compromise their\nprivacy. Most previous works focused on private distribution estimation for\none-dimensional data, which does not consider spatial data relation and leads\nto poor accuracy for spatial distribution estimation. In this paper, we address\nthe problem of private spatial distribution estimation, where we collect\nspatial data from individuals and aim to minimize the distance between the\nactual distribution and estimated one under Local Differential Privacy (LDP).\nTo leverage the numerical nature of the domain, we project spatial data and its\nrelationships onto a one-dimensional distribution. We then use this projection\nto estimate the overall spatial distribution. Specifically, we propose a\nreporting mechanism called Disk Area Mechanism (DAM), which projects the\nspatial domain onto a line and optimizes the estimation using the sliced\nWasserstein distance. Through extensive experiments, we show the effectiveness\nof our DAM approach on both real and synthetic data sets, compared with the\nstate-of-the-art methods, such as Multi-dimensional Square Wave Mechanism\n(MDSW) and Subset Exponential Mechanism with Geo-I (SEM-Geo-I). Our results\nshow that our DAM always performs better than MDSW and is better than SEM-Geo-I\nwhen the data granularity is fine enough.",
    "updated" : "2024-12-11T09:02:54Z",
    "published" : "2024-12-09T14:53:57Z",
    "authors" : [
      {
        "name" : "Leilei Du"
      },
      {
        "name" : "Peng Cheng"
      },
      {
        "name" : "Libin Zheng"
      },
      {
        "name" : "Xiang Lian"
      },
      {
        "name" : "Lei Chen"
      },
      {
        "name" : "Wei Xi"
      },
      {
        "name" : "Wangze Ni"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.07796v1",
    "title" : "MRP-LLM: Multitask Reflective Large Language Models for\n  Privacy-Preserving Next POI Recommendation",
    "summary" : "Large language models (LLMs) have shown promising potential for next\nPoint-of-Interest (POI) recommendation. However, existing methods only perform\ndirect zero-shot prompting, leading to ineffective extraction of user\npreferences, insufficient injection of collaborative signals, and a lack of\nuser privacy protection. As such, we propose a novel Multitask Reflective Large\nLanguage Model for Privacy-preserving Next POI Recommendation (MRP-LLM), aiming\nto exploit LLMs for better next POI recommendation while preserving user\nprivacy. Specifically, the Multitask Reflective Preference Extraction Module\nfirst utilizes LLMs to distill each user's fine-grained (i.e., categorical,\ntemporal, and spatial) preferences into a knowledge base (KB). The Neighbor\nPreference Retrieval Module retrieves and summarizes the preferences of similar\nusers from the KB to obtain collaborative signals. Subsequently, aggregating\nthe user's preferences with those of similar users, the Multitask Next POI\nRecommendation Module generates the next POI recommendations via multitask\nprompting. Meanwhile, during data collection, a Privacy Transmission Module is\nspecifically devised to preserve sensitive POI data. Extensive experiments on\nthree real-world datasets demonstrate the efficacy of our proposed MRP-LLM in\nproviding more accurate next POI recommendations with user privacy preserved.",
    "updated" : "2024-12-03T09:45:02Z",
    "published" : "2024-12-03T09:45:02Z",
    "authors" : [
      {
        "name" : "Ziqing Wu"
      },
      {
        "name" : "Zhu Sun"
      },
      {
        "name" : "Dongxia Wang"
      },
      {
        "name" : "Lu Zhang"
      },
      {
        "name" : "Jie Zhang"
      },
      {
        "name" : "Yew Soon Ong"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.09256v1",
    "title" : "Differential Privacy Releasing of Hierarchical Origin/Destination Data\n  with a TopDown Approach",
    "summary" : "This paper presents a novel method to generate differentially private tabular\ndatasets for hierarchical data, with a specific focus on origin-destination\n(O/D) trips. The approach builds upon the TopDown algorithm, a constraint-based\nmechanism designed to incorporate invariant queries into tabular data,\ndeveloped by the US Census. O/D hierarchical data refers to datasets\nrepresenting trips between geographical areas organized in a hierarchical\nstructure (e.g., region $\\rightarrow$ province $\\rightarrow$ city). The\ndeveloped method is crafted to improve accuracy on queries spanning wider\ngeographical areas that can be obtained by aggregation. Maintaining high\naccuracy for aggregated geographical queries is a crucial attribute of the\ndifferentially private dataset, particularly for practitioners. Furthermore,\nthe approach is designed to minimize false positives detection and to replicate\nthe sparsity of the sensitive data. The key technical contributions of this\npaper include a novel TopDown algorithm that employs constrained optimization\nwith Chebyshev distance minimization, with theoretical guarantees based on the\nmaximum absolute error. Additionally, we propose a new integer optimization\nalgorithm that significantly reduces the incidence of false positives. The\neffectiveness of the proposed approach is validated using both real-world and\nsynthetic O/D datasets, demonstrating its ability to generate private data with\nhigh utility and a reduced number of false positives. We emphasize that the\nproposed algorithm is applicable to any tabular data with a hierarchical\nstructure.",
    "updated" : "2024-12-12T13:14:15Z",
    "published" : "2024-12-12T13:14:15Z",
    "authors" : [
      {
        "name" : "Fabrizio Boninsegna"
      },
      {
        "name" : "Francesco Silvestri"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.09222v1",
    "title" : "Building a Privacy Web with SPIDEr -- Secure Pipeline for Information\n  De-Identification with End-to-End Encryption",
    "summary" : "Data de-identification makes it possible to glean insights from data while\npreserving user privacy. The use of Trusted Execution Environments (TEEs) allow\nfor the execution of de-identification applications on the cloud without the\nneed for a user to trust the third-party application provider. In this paper,\nwe present \\textit{SPIDEr - Secure Pipeline for Information De-Identification\nwith End-to-End Encryption}, our implementation of an end-to-end encrypted data\nde-identification pipeline. SPIDEr supports classical anonymisation techniques\nsuch as suppression, pseudonymisation, generalisation, and aggregation, as well\nas techniques that offer a formal privacy guarantee such as k-anonymisation and\ndifferential privacy. To enable scalability and improve performance on\nconstrained TEE hardware, we enable batch processing of data for differential\nprivacy computations. We present our design of the control flows for end-to-end\nsecure execution of de-identification operations within a TEE. As part of the\ncontrol flow for running SPIDEr within the TEE, we perform attestation, a\nprocess that verifies that the software binaries were properly instantiated on\na known, trusted platform.",
    "updated" : "2024-12-12T12:24:12Z",
    "published" : "2024-12-12T12:24:12Z",
    "authors" : [
      {
        "name" : "Novoneel Chakraborty"
      },
      {
        "name" : "Anshoo Tandon"
      },
      {
        "name" : "Kailash Reddy"
      },
      {
        "name" : "Kaushal Kirpekar"
      },
      {
        "name" : "Bryan Paul Robert"
      },
      {
        "name" : "Hari Dilip Kumar"
      },
      {
        "name" : "Abhilash Venkatesh"
      },
      {
        "name" : "Abhay Sharma"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.09195v1",
    "title" : "On the Generation and Removal of Speaker Adversarial Perturbation for\n  Voice-Privacy Protection",
    "summary" : "Neural networks are commonly known to be vulnerable to adversarial attacks\nmounted through subtle perturbation on the input data. Recent development in\nvoice-privacy protection has shown the positive use cases of the same technique\nto conceal speaker's voice attribute with additive perturbation signal\ngenerated by an adversarial network. This paper examines the reversibility\nproperty where an entity generating the adversarial perturbations is authorized\nto remove them and restore original speech (e.g., the speaker him/herself). A\nsimilar technique could also be used by an investigator to deanonymize a\nvoice-protected speech to restore criminals' identities in security and\nforensic analysis. In this setting, the perturbation generative module is\nassumed to be known in the removal process. To this end, a joint training of\nperturbation generation and removal modules is proposed. Experimental results\non the LibriSpeech dataset demonstrated that the subtle perturbations added to\nthe original speech can be predicted from the anonymized speech while achieving\nthe goal of privacy protection. By removing these perturbations from the\nanonymized sample, the original speech can be restored. Audio samples can be\nfound in \\url{https://voiceprivacy.github.io/Perturbation-Generation-Removal/}.",
    "updated" : "2024-12-12T11:46:07Z",
    "published" : "2024-12-12T11:46:07Z",
    "authors" : [
      {
        "name" : "Chenyang Guo"
      },
      {
        "name" : "Liping Chen"
      },
      {
        "name" : "Zhuhai Li"
      },
      {
        "name" : "Kong Aik Lee"
      },
      {
        "name" : "Zhen-Hua Ling"
      },
      {
        "name" : "Wu Guo"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.09812v1",
    "title" : "ScaleOT: Privacy-utility-scalable Offsite-tuning with Dynamic\n  LayerReplace and Selective Rank Compression",
    "summary" : "Offsite-tuning is a privacy-preserving method for tuning large language\nmodels (LLMs) by sharing a lossy compressed emulator from the LLM owners with\ndata owners for downstream task tuning. This approach protects the privacy of\nboth the model and data owners. However, current offsite tuning methods often\nsuffer from adaptation degradation, high computational costs, and limited\nprotection strength due to uniformly dropping LLM layers or relying on\nexpensive knowledge distillation. To address these issues, we propose ScaleOT,\na novel privacy-utility-scalable offsite-tuning framework that effectively\nbalances privacy and utility. ScaleOT introduces a novel layerwise lossy\ncompression algorithm that uses reinforcement learning to obtain the importance\nof each layer. It employs lightweight networks, termed harmonizers, to replace\nthe raw LLM layers. By combining important original LLM layers and harmonizers\nin different ratios, ScaleOT generates emulators tailored for optimal\nperformance with various model scales for enhanced privacy protection.\nAdditionally, we present a rank reduction method to further compress the\noriginal LLM layers, significantly enhancing privacy with negligible impact on\nutility. Comprehensive experiments show that ScaleOT can achieve nearly\nlossless offsite tuning performance compared with full fine-tuning while\nobtaining better model privacy.",
    "updated" : "2024-12-13T03:00:48Z",
    "published" : "2024-12-13T03:00:48Z",
    "authors" : [
      {
        "name" : "Kai Yao"
      },
      {
        "name" : "Zhaorui Tan"
      },
      {
        "name" : "Tiandi Ye"
      },
      {
        "name" : "Lichun Li"
      },
      {
        "name" : "Yuan Zhao"
      },
      {
        "name" : "Wenyan Liu"
      },
      {
        "name" : "Wei Wang"
      },
      {
        "name" : "Jianke Zhu"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  }
]