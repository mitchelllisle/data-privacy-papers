[
  {
    "id" : "http://arxiv.org/abs/2412.02578v1",
    "title" : "Private Linear Regression with Differential Privacy and PAC Privacy",
    "summary" : "Linear regression is a fundamental tool for statistical analysis, which has\nmotivated the development of linear regression methods that satisfy provable\nprivacy guarantees so that the learned model reveals little about any one data\npoint used to construct it. Most existing privacy-preserving linear regression\nmethods rely on the well-established framework of differential privacy, while\nthe newly proposed PAC Privacy has not yet been explored in this context. In\nthis paper, we systematically compare linear regression models trained with\ndifferential privacy and PAC privacy across three real-world datasets,\nobserving several key findings that impact the performance of\nprivacy-preserving linear regression.",
    "updated" : "2024-12-03T17:04:14Z",
    "published" : "2024-12-03T17:04:14Z",
    "authors" : [
      {
        "name" : "Hillary Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02538v1",
    "title" : "On the Privacy, Security, and Trustworthy for Distributed Wireless Large\n  AI Model (WLAM)",
    "summary" : "Combining wireless communication with large artificial intelligence (AI)\nmodels can open up a myriad of novel application scenarios. In sixth generation\n(6G) networks, ubiquitous communication and computing resources allow large AI\nmodels to serve democratic large AI models-related services to enable real-time\napplications like autonomous vehicles, smart cities, and Internet of Things\n(IoT) ecosystems. However, the security considerations and sustainable\ncommunication resources limit the deployment of large AI models over\ndistributed wireless networks. This paper provides a comprehensive overview of\nprivacy, security, and trustworthy for distributed wireless large AI model\n(WLAM). In particular, the detailed privacy and security are analysis for\ndistributed WLAM is fist revealed. The classifications and theoretical findings\nabout privacy and security in distributed WLAM are discussed. Then the\ntrustworthy and ethics for implementing distributed WLAM are described.\nFinally, the comprehensive applications of distributed WLAM is provided in the\naspect of electromagnetic signal processing.",
    "updated" : "2024-12-03T16:32:19Z",
    "published" : "2024-12-03T16:32:19Z",
    "authors" : [
      {
        "name" : "Zhaohui Yang"
      },
      {
        "name" : "Wei Xu"
      },
      {
        "name" : "Le Liang"
      },
      {
        "name" : "Yuanhao Cui"
      },
      {
        "name" : "Zhijin Qin"
      },
      {
        "name" : "Merouane Debbah"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02340v1",
    "title" : "Federated Analytics in Practice: Engineering for Privacy, Scalability\n  and Practicality",
    "summary" : "Cross-device Federated Analytics (FA) is a distributed computation paradigm\ndesigned to answer analytics queries about and derive insights from data held\nlocally on users' devices. On-device computations combined with other privacy\nand security measures ensure that only minimal data is transmitted off-device,\nachieving a high standard of data protection. Despite FA's broad relevance, the\napplicability of existing FA systems is limited by compromised accuracy; lack\nof flexibility for data analytics; and an inability to scale effectively. In\nthis paper, we describe our approach to combine privacy, scalability, and\npracticality to build and deploy a system that overcomes these limitations. Our\nFA system leverages trusted execution environments (TEEs) and optimizes the use\nof on-device computing resources to facilitate federated data processing across\nlarge fleets of devices, while ensuring robust, defensible, and verifiable\nprivacy safeguards. We focus on federated analytics (statistics and\nmonitoring), in contrast to systems for federated learning (ML workloads), and\nwe flag the key differences.",
    "updated" : "2024-12-03T10:03:12Z",
    "published" : "2024-12-03T10:03:12Z",
    "authors" : [
      {
        "name" : "Harish Srinivas"
      },
      {
        "name" : "Graham Cormode"
      },
      {
        "name" : "Mehrdad Honarkhah"
      },
      {
        "name" : "Samuel Lurye"
      },
      {
        "name" : "Jonathan Hehir"
      },
      {
        "name" : "Lunwen He"
      },
      {
        "name" : "George Hong"
      },
      {
        "name" : "Ahmed Magdy"
      },
      {
        "name" : "Dzmitry Huba"
      },
      {
        "name" : "Kaikai Wang"
      },
      {
        "name" : "Shen Guo"
      },
      {
        "name" : "Shoubhik Bhattacharya"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02130v1",
    "title" : "A privacy-preserving distributed credible evidence fusion algorithm for\n  collective decision-making",
    "summary" : "The theory of evidence reasoning has been applied to collective\ndecision-making in recent years. However, existing distributed evidence fusion\nmethods lead to participants' preference leakage and fusion failures as they\ndirectly exchange raw evidence and do not assess evidence credibility like\ncentralized credible evidence fusion (CCEF) does. To do so, a\nprivacy-preserving distributed credible evidence fusion method with three-level\nconsensus (PCEF) is proposed in this paper. In evidence difference measure\n(EDM) neighbor consensus, an evidence-free equivalent expression of EDM among\nneighbored agents is derived with the shared dot product protocol for pignistic\nprobability and the identical judgment of two events with maximal subjective\nprobabilities, so that evidence privacy is guaranteed due to such irreversible\nevidence transformation. In EDM network consensus, the non-neighbored EDMs are\ninferred and neighbored EDMs reach uniformity via interaction between linear\naverage consensus (LAC) and low-rank matrix completion with rank adaptation to\nguarantee EDM consensus convergence and no solution of inferring raw evidence\nin numerical iteration style. In fusion network consensus, a privacy-preserving\nLAC with a self-cancelling differential privacy term is proposed, where each\nagent adds its randomness to the sharing content and step-by-step cancels such\nrandomness in consensus iterations. Besides, the sufficient condition of the\nconvergence to the CCEF is explored, and it is proven that raw evidence is\nimpossibly inferred in such an iterative consensus. The simulations show that\nPCEF is close to CCEF both in credibility and fusion results and obtains higher\ndecision accuracy with less time-comsuming than existing methods.",
    "updated" : "2024-12-03T03:36:42Z",
    "published" : "2024-12-03T03:36:42Z",
    "authors" : [
      {
        "name" : "Chaoxiong Ma"
      },
      {
        "name" : "Yan Liang"
      },
      {
        "name" : "Xinyu Yang"
      },
      {
        "name" : "Han Wu"
      },
      {
        "name" : "Huixia Zhang"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01756v1",
    "title" : "Adversarial Sample-Based Approach for Tighter Privacy Auditing in Final\n  Model-Only Scenarios",
    "summary" : "Auditing Differentially Private Stochastic Gradient Descent (DP-SGD) in the\nfinal model setting is challenging and often results in empirical lower bounds\nthat are significantly looser than theoretical privacy guarantees. We introduce\na novel auditing method that achieves tighter empirical lower bounds without\nadditional assumptions by crafting worst-case adversarial samples through\nloss-based input-space auditing. Our approach surpasses traditional\ncanary-based heuristics and is effective in both white-box and black-box\nscenarios. Specifically, with a theoretical privacy budget of $\\varepsilon =\n10.0$, our method achieves empirical lower bounds of $6.68$ in white-box\nsettings and $4.51$ in black-box settings, compared to the baseline of $4.11$\nfor MNIST. Moreover, we demonstrate that significant privacy auditing results\ncan be achieved using in-distribution (ID) samples as canaries, obtaining an\nempirical lower bound of $4.33$ where traditional methods produce near-zero\nleakage detection. Our work offers a practical framework for reliable and\naccurate privacy auditing in differentially private machine learning.",
    "updated" : "2024-12-02T17:52:16Z",
    "published" : "2024-12-02T17:52:16Z",
    "authors" : [
      {
        "name" : "Sangyeon Yoon"
      },
      {
        "name" : "Wonje Jeung"
      },
      {
        "name" : "Albert No"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01671v2",
    "title" : "Verified Foundations for Differential Privacy",
    "summary" : "Differential privacy (DP) has become the gold standard for privacy-preserving\ndata analysis, but implementing it correctly has proven challenging. Prior work\nhas focused on verifying DP at a high level, assuming the foundations are\ncorrect and a perfect source of randomness is available. However, the\nunderlying theory of differential privacy can be very complex and subtle. Flaws\nin basic mechanisms and random number generation have been a critical source of\nvulnerabilities in real-world DP systems.\n  In this paper, we present SampCert, the first comprehensive, mechanized\nfoundation for differential privacy. SampCert is written in Lean with over\n12,000 lines of proof. It offers a generic and extensible notion of DP, a\nframework for constructing and composing DP mechanisms, and formally verified\nimplementations of Laplace and Gaussian sampling algorithms. SampCert provides\n(1) a mechanized foundation for developing the next generation of\ndifferentially private algorithms, and (2) mechanically verified primitives\nthat can be deployed in production systems. Indeed, SampCert's verified\nalgorithms power the DP offerings of Amazon Web Services (AWS), demonstrating\nits real-world impact.\n  SampCert's key innovations include: (1) A generic DP foundation that can be\ninstantiated for various DP definitions (e.g., pure, concentrated, R\\'enyi DP);\n(2) formally verified discrete Laplace and Gaussian sampling algorithms that\navoid the pitfalls of floating-point implementations; and (3) a simple\nprobability monad and novel proof techniques that streamline the formalization.\nTo enable proving complex correctness properties of DP and random number\ngeneration, SampCert makes heavy use of Lean's extensive Mathlib library,\nleveraging theorems in Fourier analysis, measure and probability theory, number\ntheory, and topology.",
    "updated" : "2024-12-03T18:53:16Z",
    "published" : "2024-12-02T16:19:47Z",
    "authors" : [
      {
        "name" : "Markus de Medeiros"
      },
      {
        "name" : "Muhammad Naveed"
      },
      {
        "name" : "Tancrede Lepoint"
      },
      {
        "name" : "Temesghen Kahsai"
      },
      {
        "name" : "Tristan Ravitch"
      },
      {
        "name" : "Stefan Zetzsche"
      },
      {
        "name" : "Anjali Joshi"
      },
      {
        "name" : "Joseph Tassarotti"
      },
      {
        "name" : "Aws Albarghouthi"
      },
      {
        "name" : "Jean-Baptiste Tristan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01650v2",
    "title" : "Privacy-Preserving Federated Learning via Homomorphic Adversarial\n  Networks",
    "summary" : "Privacy-preserving federated learning (PPFL) aims to train a global model for\nmultiple clients while maintaining their data privacy. However, current PPFL\nprotocols exhibit one or more of the following insufficiencies: considerable\ndegradation in accuracy, the requirement for sharing keys, and cooperation\nduring the key generation or decryption processes. As a mitigation, we develop\nthe first protocol that utilizes neural networks to implement PPFL, as well as\nincorporating an Aggregatable Hybrid Encryption scheme tailored to the needs of\nPPFL. We name these networks as Homomorphic Adversarial Networks (HANs) which\ndemonstrate that neural networks are capable of performing tasks similar to\nmulti-key homomorphic encryption (MK-HE) while solving the problems of key\ndistribution and collaborative decryption. Our experiments show that HANs are\nrobust against privacy attacks. Compared with non-private federated learning,\nexperiments conducted on multiple datasets demonstrate that HANs exhibit a\nnegligible accuracy loss (at most 1.35%). Compared to traditional MK-HE\nschemes, HANs increase encryption aggregation speed by 6,075 times while\nincurring a 29.2 times increase in communication overhead.",
    "updated" : "2024-12-03T05:46:35Z",
    "published" : "2024-12-02T15:59:35Z",
    "authors" : [
      {
        "name" : "Wenhan Dong"
      },
      {
        "name" : "Chao Lin"
      },
      {
        "name" : "Xinlei He"
      },
      {
        "name" : "Xinyi Huang"
      },
      {
        "name" : "Shengmin Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01541v1",
    "title" : "Effectiveness of L2 Regularization in Privacy-Preserving Machine\n  Learning",
    "summary" : "Artificial intelligence, machine learning, and deep learning as a service\nhave become the status quo for many industries, leading to the widespread\ndeployment of models that handle sensitive data. Well-performing models, the\nindustry seeks, usually rely on a large volume of training data. However, the\nuse of such data raises serious privacy concerns due to the potential risks of\nleaks of highly sensitive information. One prominent threat is the Membership\nInference Attack, where adversaries attempt to deduce whether a specific data\npoint was used in a model's training process. An adversary's ability to\ndetermine an individual's presence represents a significant privacy threat,\nespecially when related to a group of users sharing sensitive information.\nHence, well-designed privacy-preserving machine learning solutions are\ncritically needed in the industry. In this work, we compare the effectiveness\nof L2 regularization and differential privacy in mitigating Membership\nInference Attack risks. Even though regularization techniques like L2\nregularization are commonly employed to reduce overfitting, a condition that\nenhances the effectiveness of Membership Inference Attacks, their impact on\nmitigating these attacks has not been systematically explored.",
    "updated" : "2024-12-02T14:31:11Z",
    "published" : "2024-12-02T14:31:11Z",
    "authors" : [
      {
        "name" : "Nikolaos Chandrinos"
      },
      {
        "name" : "Iliana Loi"
      },
      {
        "name" : "Panagiotis Zachos"
      },
      {
        "name" : "Ioannis Symeonidis"
      },
      {
        "name" : "Aristotelis Spiliotis"
      },
      {
        "name" : "Maria Panou"
      },
      {
        "name" : "Konstantinos Moustakas"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01141v1",
    "title" : "Lossless and Privacy-Preserving Graph Convolution Network for Federated\n  Item Recommendation",
    "summary" : "Graph neural network (GNN) has emerged as a state-of-the-art solution for\nitem recommendation. However, existing GNN-based recommendation methods rely on\na centralized storage of fragmented user-item interaction sub-graphs and\ntraining on an aggregated global graph, which will lead to privacy concerns. As\na response, some recent works develop GNN-based federated recommendation\nmethods by exploiting decentralized and fragmented user-item sub-graphs in\norder to preserve user privacy. However, due to privacy constraints, the graph\nconvolution process in existing federated recommendation methods is incomplete\ncompared with the centralized counterpart, causing a degradation of the\nrecommendation performance. In this paper, we propose a novel lossless and\nprivacy-preserving graph convolution network (LP-GCN), which fully completes\nthe graph convolution process with decentralized user-item interaction\nsub-graphs while ensuring privacy. It is worth mentioning that its performance\nis equivalent to that of the non-federated (i.e., centralized) counterpart.\nMoreover, we validate its effectiveness through both theoretical analysis and\nempirical studies. Extensive experiments on three real-world datasets show that\nour LP-GCN outperforms the existing federated recommendation methods. The code\nwill be publicly available once the paper is accepted.",
    "updated" : "2024-12-02T05:31:22Z",
    "published" : "2024-12-02T05:31:22Z",
    "authors" : [
      {
        "name" : "Guowei Wu"
      },
      {
        "name" : "Weike Pan"
      },
      {
        "name" : "Qiang Yang"
      },
      {
        "name" : "Zhong Ming"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01079v1",
    "title" : "Federated Motor Imagery Classification for Privacy-Preserving\n  Brain-Computer Interfaces",
    "summary" : "Training an accurate classifier for EEG-based brain-computer interface (BCI)\nrequires EEG data from a large number of users, whereas protecting their data\nprivacy is a critical consideration. Federated learning (FL) is a promising\nsolution to this challenge. This paper proposes Federated classification with\nlocal Batch-specific batch normalization and Sharpness-aware minimization\n(FedBS) for privacy protection in EEG-based motor imagery (MI) classification.\nFedBS utilizes local batch-specific batch normalization to reduce data\ndiscrepancies among different clients, and sharpness-aware minimization\noptimizer in local training to improve model generalization. Experiments on\nthree public MI datasets using three popular deep learning models demonstrated\nthat FedBS outperformed six state-of-the-art FL approaches. Remarkably, it also\noutperformed centralized training, which does not consider privacy protection\nat all. In summary, FedBS protects user EEG data privacy, enabling multiple BCI\nusers to participate in large-scale machine learning model training, which in\nturn improves the BCI decoding accuracy.",
    "updated" : "2024-12-02T03:35:27Z",
    "published" : "2024-12-02T03:35:27Z",
    "authors" : [
      {
        "name" : "Tianwang Jia"
      },
      {
        "name" : "Lubin Meng"
      },
      {
        "name" : "Siyang Li"
      },
      {
        "name" : "Jiajing Liu"
      },
      {
        "name" : "Dongrui Wu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01072v1",
    "title" : "When Fine-Tuning LLMs Meets Data Privacy: An Empirical Study of\n  Federated Learning in LLM-Based Program Repair",
    "summary" : "Software systems have been evolving rapidly and inevitably introducing bugs\nat an increasing rate, leading to significant losses in resources consumed by\nsoftware maintenance. Recently, large language models (LLMs) have demonstrated\nremarkable potential in enhancing software development and maintenance\npractices, particularly in automated program repair (APR) with improved\naccuracy and efficiency of bug fixing. However, LLM-based APR heavily relies on\nhigh-quality code repositories. A larger portion of existing code repositories\nare for private use and proprietary assets from various industries, reflecting\nmore diversity and nuances in the data since real-world industries often have\nmore extensive software development practices, which cannot be covered by\nmerely public datasets. Therefore, utilizing private datasets shows significant\npotential in enhancing software development and maintenance. However, obtaining\nsuch data from various industries is hindered by data privacy concerns, as\ncompanies are reluctant to share their codebases. To address the gap, we\ninvestigate the use of federated learning as a privacy-preserving approach that\nenables private entities to fine-tune LLMs on proprietary and decentralized\ndata, facilitating the collaboration between clients to fully utilize their\ndata to help enhance software development and maintenance. Our evaluation\nreveals that federated fine-tuning can effectively enhance program repair\ncapabilities. Notably, the impact of heterogeneous code on LLM fine-tuning is\nnegligible, indicating that real-world industries can benefit from\ncollaborative development regardless of diverse data distributions.\nFurthermore, each type of federated algorithm exhibits unique strengths across\ndifferent LLMs, suggesting that fine-tuning for program repair can be enhanced\nby tailoring the optimization process to specific characteristics of different\nLLMs.",
    "updated" : "2024-12-02T03:18:47Z",
    "published" : "2024-12-02T03:18:47Z",
    "authors" : [
      {
        "name" : "Wenqiang Luo"
      },
      {
        "name" : "Jacky Wai Keung"
      },
      {
        "name" : "Boyang Yang"
      },
      {
        "name" : "He Ye"
      },
      {
        "name" : "Claire Le Goues"
      },
      {
        "name" : "Tegawende F. Bissyande"
      },
      {
        "name" : "Haoye Tian"
      },
      {
        "name" : "Bach Le"
      }
    ],
    "categories" : [
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.01056v1",
    "title" : "Classifying Simulated Gait Impairments using Privacy-preserving\n  Explainable Artificial Intelligence and Mobile Phone Videos",
    "summary" : "Accurate diagnosis of gait impairments is often hindered by subjective or\ncostly assessment methods, with current solutions requiring either expensive\nmulti-camera equipment or relying on subjective clinical observation. There is\na critical need for accessible, objective tools that can aid in gait assessment\nwhile preserving patient privacy. In this work, we present a mobile\nphone-based, privacy-preserving artificial intelligence (AI) system for\nclassifying gait impairments and introduce a novel dataset of 743 videos\ncapturing seven distinct gait patterns. The dataset consists of frontal and\nsagittal views of trained subjects simulating normal gait and six types of\npathological gait (circumduction, Trendelenburg, antalgic, crouch,\nParkinsonian, and vaulting), recorded using standard mobile phone cameras. Our\nsystem achieved 86.5% accuracy using combined frontal and sagittal views, with\nsagittal views generally outperforming frontal views except for specific gait\npatterns like Circumduction. Model feature importance analysis revealed that\nfrequency-domain features and entropy measures were critical for classifcation\nperformance, specifically lower limb keypoints proved most important for\nclassification, aligning with clinical understanding of gait assessment. These\nfindings demonstrate that mobile phone-based systems can effectively classify\ndiverse gait patterns while preserving privacy through on-device processing.\nThe high accuracy achieved using simulated gait data suggests their potential\nfor rapid prototyping of gait analysis systems, though clinical validation with\npatient data remains necessary. This work represents a significant step toward\naccessible, objective gait assessment tools for clinical, community, and\ntele-rehabilitation settings",
    "updated" : "2024-12-02T02:35:40Z",
    "published" : "2024-12-02T02:35:40Z",
    "authors" : [
      {
        "name" : "Lauhitya Reddy"
      },
      {
        "name" : "Ketan Anand"
      },
      {
        "name" : "Shoibolina Kaushik"
      },
      {
        "name" : "Corey Rodrigo"
      },
      {
        "name" : "J. Lucas McKay"
      },
      {
        "name" : "Trisha M. Kesar"
      },
      {
        "name" : "Hyeokhyen Kwon"
      }
    ],
    "categories" : [
      "cs.CV",
      "I.2.10"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.00898v1",
    "title" : "Preserving Privacy in Software Composition Analysis: A Study of\n  Technical Solutions and Enhancements",
    "summary" : "Software composition analysis (SCA) denotes the process of identifying\nopen-source software components in an input software application. SCA has been\nextensively developed and adopted by academia and industry. However, we notice\nthat the modern SCA techniques in industry scenarios still need to be improved\ndue to privacy concerns. Overall, SCA requires the users to upload their\napplications' source code to a remote SCA server, which then inspects the\napplications and reports the component usage to users. This process is\nprivacy-sensitive since the applications may contain sensitive information,\nsuch as proprietary source code, algorithms, trade secrets, and user data.\n  Privacy concerns have prevented the SCA technology from being used in\nreal-world scenarios. Therefore, academia and the industry demand\nprivacy-preserving SCA solutions. For the first time, we analyze the privacy\nrequirements of SCA and provide a landscape depicting possible technical\nsolutions with varying privacy gains and overheads. In particular, given that\nde facto SCA frameworks are primarily driven by code similarity-based\ntechniques, we explore combining several privacy-preserving protocols to\nencapsulate the similarity-based SCA framework. Among all viable solutions, we\nfind that multi-party computation (MPC) offers the strongest privacy guarantee\nand plausible accuracy; it, however, incurs high overhead (184 times). We\noptimize the MPC-based SCA framework by reducing the amount of crypto protocol\ntransactions using program analysis techniques. The evaluation results show\nthat our proposed optimizations can reduce the MPC-based SCA overhead to only\n8.5% without sacrificing SCA's privacy guarantee or accuracy.",
    "updated" : "2024-12-01T17:17:29Z",
    "published" : "2024-12-01T17:17:29Z",
    "authors" : [
      {
        "name" : "Huaijin Wang"
      },
      {
        "name" : "Zhibo Liu"
      },
      {
        "name" : "Yanbo Dai"
      },
      {
        "name" : "Shuai Wang"
      },
      {
        "name" : "Qiyi Tang"
      },
      {
        "name" : "Sen Nie"
      },
      {
        "name" : "Shi Wu"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.00774v1",
    "title" : "Post-Vaccination COVID-19 Data Analysis: Privacy and Ethics",
    "summary" : "The COVID-19 pandemic has severely affected the world in terms of health,\neconomy and peace. Fortunately, the countries are trying to overcome the\nsituation by actively carrying out vaccinations. However, like any other\nmassive operation involving humans such as human resource management,\nelections, surveys, etc., the vaccination process raises several questions\nabout citizen privacy and misuse of personal data. In most of the countries,\nfew attempts have been made to verify the vaccination statistics as reported by\nthe health centers. These issues collectively require the solutions of\nanonymity of citizens' personal information, immutability of vaccination data\nand easy yet restricted access by adversarial bodies such as the government for\nthe verification and analysis of the data. This paper introduces a\nblockchain-based application to simulate and monitor the vaccination process.\nThe structure of data model used in the proposed system is based on the IEEE\nStandard for Data Format for Blockchain Systems 2418.2TM-2020. The proposed\nsystem enables authorized stakeholders to share and access relevant information\nfor vaccination process chain while preserving citizen privacy and\naccountability of the system. It is implemented on the Ethereum blockchain and\nuses a Python API for the simulation and validation of each step of the\nvaccination process.",
    "updated" : "2024-12-01T11:41:32Z",
    "published" : "2024-12-01T11:41:32Z",
    "authors" : [
      {
        "name" : "Sankha Das"
      },
      {
        "name" : "Amit Dua"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.00687v1",
    "title" : "Towards Privacy-Preserving Medical Imaging: Federated Learning with\n  Differential Privacy and Secure Aggregation Using a Modified ResNet\n  Architecture",
    "summary" : "With increasing concerns over privacy in healthcare, especially for sensitive\nmedical data, this research introduces a federated learning framework that\ncombines local differential privacy and secure aggregation using Secure\nMulti-Party Computation for medical image classification. Further, we propose\nDPResNet, a modified ResNet architecture optimized for differential privacy.\nLeveraging the BloodMNIST benchmark dataset, we simulate a realistic\ndata-sharing environment across different hospitals, addressing the distinct\nprivacy challenges posed by federated healthcare data. Experimental results\nindicate that our privacy-preserving federated model achieves accuracy levels\nclose to non-private models, surpassing traditional approaches while\nmaintaining strict data confidentiality. By enhancing the privacy, efficiency,\nand reliability of healthcare data management, our approach offers substantial\nbenefits to patients, healthcare providers, and the broader healthcare\necosystem.",
    "updated" : "2024-12-01T05:52:29Z",
    "published" : "2024-12-01T05:52:29Z",
    "authors" : [
      {
        "name" : "Mohamad Haj Fares"
      },
      {
        "name" : "Ahmed Mohamed Saad Emam Saad"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.00620v1",
    "title" : "TraCS: Trajectory Collection in Continuous Space under Local\n  Differential Privacy",
    "summary" : "Trajectory collection is fundamental for location-based services but often\ninvolves sensitive information, such as a user's daily routine, raising privacy\nconcerns. Local differential privacy (LDP) provides provable privacy guarantees\nfor users, even when the data collector is untrusted. Existing trajectory\ncollection methods ensure LDP only for discrete location spaces, where the\nnumber of locations affects their privacy guarantees and trajectory utility.\nMoreover, the location space is often naturally continuous, such as in flying\nand sailing trajectories, making these methods unsuitable. This paper proposes\ntwo trajectory collection methods that ensure LDP for continuous spaces:\nTraCS-D, which perturbs the direction and distance of locations, and TraCS-C,\nwhich perturbs the Cartesian coordinates of locations. Both methods are\ntheoretically and experimentally analyzed for trajectory utility. TraCS can\nalso be applied to discrete spaces by rounding perturbed locations to the\nnearest discrete points. It is independent of the number of locations and has\nonly $\\Theta(1)$ time complexity in each perturbation generation. Evaluation\nresults on discrete location spaces validate this advantage and show that TraCS\noutperforms state-of-the-art methods with improved trajectory utility,\nespecially for large privacy parameters.",
    "updated" : "2024-12-01T00:07:04Z",
    "published" : "2024-12-01T00:07:04Z",
    "authors" : [
      {
        "name" : "Ye Zheng"
      },
      {
        "name" : "Yidan Hu"
      }
    ],
    "categories" : [
      "cs.CR",
      "68P27"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02987v1",
    "title" : "Advancing Conversational Psychotherapy: Integrating Privacy,\n  Dual-Memory, and Domain Expertise with Large Language Models",
    "summary" : "Mental health has increasingly become a global issue that reveals the\nlimitations of traditional conversational psychotherapy, constrained by\nlocation, time, expense, and privacy concerns. In response to these challenges,\nwe introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed\nto democratize access to psychotherapy. SoulSpeak improves upon the\ncapabilities of standard LLM-enabled chatbots by incorporating a novel\ndual-memory component that combines short-term and long-term context via\nRetrieval Augmented Generation (RAG) to offer personalized responses while\nensuring the preservation of user privacy and intimacy through a dedicated\nprivacy module. In addition, it leverages a counseling chat dataset of\ntherapist-client interactions and various prompting techniques to align the\ngenerated responses with psychotherapeutic methods. We introduce two fine-tuned\nBERT models to evaluate the system against existing LLMs and human therapists:\nthe Conversational Psychotherapy Preference Model (CPPM) to simulate human\npreference among responses and another to assess response relevance to user\ninput. CPPM is useful for training and evaluating psychotherapy-focused\nlanguage models independent from SoulSpeak, helping with the constrained\nresources available for psychotherapy. Furthermore, the effectiveness of the\ndual-memory component and the robustness of the privacy module are also\nexamined. Our findings highlight the potential and challenge of enhancing\nmental health care by offering an alternative that combines the expertise of\ntraditional therapy with the advantages of LLMs, providing a promising way to\naddress the accessibility and personalization gap in current mental health\nservices.",
    "updated" : "2024-12-04T03:02:46Z",
    "published" : "2024-12-04T03:02:46Z",
    "authors" : [
      {
        "name" : "XiuYu Zhang"
      },
      {
        "name" : "Zening Luo"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02934v1",
    "title" : "BGTplanner: Maximizing Training Accuracy for Differentially Private\n  Federated Recommenders via Strategic Privacy Budget Allocation",
    "summary" : "To mitigate the rising concern about privacy leakage, the federated\nrecommender (FR) paradigm emerges, in which decentralized clients co-train the\nrecommendation model without exposing their raw user-item rating data. The\ndifferentially private federated recommender (DPFR) further enhances FR by\ninjecting differentially private (DP) noises into clients. Yet, current DPFRs,\nsuffering from noise distortion, cannot achieve satisfactory accuracy. Various\nefforts have been dedicated to improving DPFRs by adaptively allocating the\nprivacy budget over the learning process. However, due to the intricate\nrelation between privacy budget allocation and model accuracy, existing works\nare still far from maximizing DPFR accuracy. To address this challenge, we\ndevelop BGTplanner (Budget Planner) to strategically allocate the privacy\nbudget for each round of DPFR training, improving overall training performance.\nSpecifically, we leverage the Gaussian process regression and historical\ninformation to predict the change in recommendation accuracy with a certain\nallocated privacy budget. Additionally, Contextual Multi-Armed Bandit (CMAB) is\nharnessed to make privacy budget allocation decisions by reconciling the\ncurrent improvement and long-term privacy constraints. Our extensive\nexperimental results on real datasets demonstrate that \\emph{BGTplanner}\nachieves an average improvement of 6.76\\% in training performance compared to\nstate-of-the-art baselines.",
    "updated" : "2024-12-04T01:07:04Z",
    "published" : "2024-12-04T01:07:04Z",
    "authors" : [
      {
        "name" : "Xianzhi Zhang"
      },
      {
        "name" : "Yipeng Zhou"
      },
      {
        "name" : "Miao Hu"
      },
      {
        "name" : "Di Wu"
      },
      {
        "name" : "Pengshan Liao"
      },
      {
        "name" : "Mohsen Guizani"
      },
      {
        "name" : "Michael Sheng"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02868v1",
    "title" : "A Novel Compact LLM Framework for Local, High-Privacy EHR Data\n  Applications",
    "summary" : "Large Language Models (LLMs) have shown impressive capabilities in natural\nlanguage processing, yet their use in sensitive domains like healthcare,\nparticularly with Electronic Health Records (EHR), faces significant challenges\ndue to privacy concerns and limited computational resources. This paper\npresents a compact LLM framework designed for local deployment in settings with\nstrict privacy requirements and limited access to high-performance GPUs. We\nintroduce a novel preprocessing technique that uses information extraction\nmethods, e.g., regular expressions, to filter and emphasize critical\ninformation in clinical notes, enhancing the performance of smaller LLMs on EHR\ndata. Our framework is evaluated using zero-shot and few-shot learning\nparadigms on both private and publicly available (MIMIC-IV) datasets, and we\nalso compare its performance with fine-tuned LLMs on the MIMIC-IV dataset. The\nresults demonstrate that our preprocessing approach significantly boosts the\nprediction accuracy of smaller LLMs, making them suitable for high-privacy,\nresource-constrained applications. This study offers valuable insights into\noptimizing LLM performance for sensitive, data-intensive tasks while addressing\ncomputational and privacy limitations.",
    "updated" : "2024-12-03T22:06:55Z",
    "published" : "2024-12-03T22:06:55Z",
    "authors" : [
      {
        "name" : "Yixiang Qu"
      },
      {
        "name" : "Yifan Dai"
      },
      {
        "name" : "Shilin Yu"
      },
      {
        "name" : "Pradham Tanikella"
      },
      {
        "name" : "Travis Schrank"
      },
      {
        "name" : "Trevor Hackman"
      },
      {
        "name" : "Didong Li"
      },
      {
        "name" : "Di Wu"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.02538v2",
    "title" : "On Privacy, Security, and Trustworthiness in Distributed Wireless Large\n  AI Models (WLAM)",
    "summary" : "Combining wireless communication with large artificial intelligence (AI)\nmodels can open up a myriad of novel application scenarios. In sixth generation\n(6G) networks, ubiquitous communication and computing resources allow large AI\nmodels to serve democratic large AI models-related services to enable real-time\napplications like autonomous vehicles, smart cities, and Internet of Things\n(IoT) ecosystems. However, the security considerations and sustainable\ncommunication resources limit the deployment of large AI models over\ndistributed wireless networks. This paper provides a comprehensive overview of\nprivacy, security, and trustworthy for distributed wireless large AI model\n(WLAM). In particular, a detailed privacy and security are analysis for\ndistributed WLAM is fist revealed. The classifications and theoretical findings\nabout privacy and security in distributed WLAM are discussed. Then the\ntrustworthy and ethics for implementing distributed WLAM are described.\nFinally, the comprehensive applications of distributed WLAM are presented in\nthe context of electromagnetic signal processing.",
    "updated" : "2024-12-04T07:11:07Z",
    "published" : "2024-12-03T16:32:19Z",
    "authors" : [
      {
        "name" : "Zhaohui Yang"
      },
      {
        "name" : "Wei Xu"
      },
      {
        "name" : "Le Liang"
      },
      {
        "name" : "Yuanhao Cui"
      },
      {
        "name" : "Zhijin Qin"
      },
      {
        "name" : "Merouane Debbah"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.04408v1",
    "title" : "Providing Differential Privacy for Federated Learning Over Wireless: A\n  Cross-layer Framework",
    "summary" : "Federated Learning (FL) is a distributed machine learning framework that\ninherently allows edge devices to maintain their local training data, thus\nproviding some level of privacy. However, FL's model updates still pose a risk\nof privacy leakage, which must be mitigated. Over-the-air FL (OTA-FL) is an\nadapted FL design for wireless edge networks that leverages the natural\nsuperposition property of the wireless medium. We propose a wireless physical\nlayer (PHY) design for OTA-FL which improves differential privacy (DP) through\na decentralized, dynamic power control that utilizes both inherent Gaussian\nnoise in the wireless channel and a cooperative jammer (CJ) for additional\nartificial noise generation when higher privacy levels are required. Although\nprimarily implemented within the Upcycled-FL framework, where a\nresource-efficient method with first-order approximations is used at every even\niteration to decrease the required information from clients, our power control\nstrategy is applicable to any FL framework, including FedAvg and FedProx as\nshown in the paper. This adaptation showcases the flexibility and effectiveness\nof our design across different learning algorithms while maintaining a strong\nemphasis on privacy. Our design removes the need for client-side artificial\nnoise injection for DP, utilizing a cooperative jammer to enhance privacy\nwithout affecting transmission efficiency for higher privacy demands. Privacy\nanalysis is provided using the Moments Accountant method. We perform a\nconvergence analysis for non-convex objectives to tackle heterogeneous data\ndistributions, highlighting the inherent trade-offs between privacy and\naccuracy. Numerical results show that our approach with various FL algorithms\noutperforms the state-of-the-art under the same DP conditions on the non-i.i.d.\nFEMNIST dataset, and highlight the cooperative jammer's effectiveness in\nensuring strict privacy.",
    "updated" : "2024-12-05T18:27:09Z",
    "published" : "2024-12-05T18:27:09Z",
    "authors" : [
      {
        "name" : "Jiayu Mao"
      },
      {
        "name" : "Tongxin Yin"
      },
      {
        "name" : "Aylin Yener"
      },
      {
        "name" : "Mingyan Liu"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.04178v1",
    "title" : "Multi-Layer Privacy-Preserving Record Linkage with Clerical Review based\n  on gradual information disclosure",
    "summary" : "Privacy-Preserving Record linkage (PPRL) is an essential component in data\nintegration tasks of sensitive information. The linkage quality determines the\nusability of combined datasets and (machine learning) applications based on\nthem. We present a novel privacy-preserving protocol that integrates clerical\nreview in PPRL using a multi-layer active learning process. Uncertain match\ncandidates are reviewed on several layers by human and non-human oracles to\nreduce the amount of disclosed information per record and in total. Predictions\nare propagated back to update previous layers, resulting in an improved linkage\nperformance for non-reviewed candidates as well. The data owners remain in\ncontrol of the amount of information they share for each record. Therefore, our\napproach follows need-to-know and data sovereignty principles. The experimental\nevaluation on real-world datasets shows considerable linkage quality\nimprovements with limited labeling effort and privacy risks.",
    "updated" : "2024-12-05T14:18:50Z",
    "published" : "2024-12-05T14:18:50Z",
    "authors" : [
      {
        "name" : "Florens Rohde"
      },
      {
        "name" : "Victor Christen"
      },
      {
        "name" : "Martin Franke"
      },
      {
        "name" : "Erhard Rahm"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.04031v1",
    "title" : "Dimension Reduction via Random Projection for Privacy in Multi-Agent\n  Systems",
    "summary" : "The agents in a Multi-Agent System (MAS) make observations about the system\nand send that information to a fusion center. The fusion center aggregates the\ninformation and concludes about the system parameters with as much accuracy as\npossible. However for the purposes of better efficiency of the system at large,\nthe agents need to append some private parameters to the observed data. In this\nscenario, the data sent to the fusion center is faced with privacy risks. The\ndata communicated to the fusion center must be secured against data privacy\nbreaches and inference attacks in a decentralized manner. However, this in turn\nleads to a loss of utility of the data being sent to the fusion center. We\nquantify the utility and privacy of the system using Cosine similarity. We\nformulate our MAS problem in terms of deducing a concept for which\ncompression-based methods are there in literature. Next, we propose a novel\nsanitization mechanism for our MAS using one such compression-based method\nwhile addressing the utility-privacy tradeoff problem.",
    "updated" : "2024-12-05T10:09:13Z",
    "published" : "2024-12-05T10:09:13Z",
    "authors" : [
      {
        "name" : "Puspanjali Ghoshal"
      },
      {
        "name" : "Ashok Singh Sairam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.03924v1",
    "title" : "Privacy-Preserving in Medical Image Analysis: A Review of Methods and\n  Applications",
    "summary" : "With the rapid advancement of artificial intelligence and deep learning,\nmedical image analysis has become a critical tool in modern healthcare,\nsignificantly improving diagnostic accuracy and efficiency. However, AI-based\nmethods also raise serious privacy concerns, as medical images often contain\nhighly sensitive patient information. This review offers a comprehensive\noverview of privacy-preserving techniques in medical image analysis, including\nencryption, differential privacy, homomorphic encryption, federated learning,\nand generative adversarial networks. We explore the application of these\ntechniques across various medical image analysis tasks, such as diagnosis,\npathology, and telemedicine. Notably, we organizes the review based on specific\nchallenges and their corresponding solutions in different medical image\nanalysis applications, so that technical applications are directly aligned with\npractical issues, addressing gaps in the current research landscape.\nAdditionally, we discuss emerging trends, such as zero-knowledge proofs and\nsecure multi-party computation, offering insights for future research. This\nreview serves as a valuable resource for researchers and practitioners and can\nhelp advance privacy-preserving in medical image analysis.",
    "updated" : "2024-12-05T06:56:06Z",
    "published" : "2024-12-05T06:56:06Z",
    "authors" : [
      {
        "name" : "Yanming Zhu"
      },
      {
        "name" : "Xuefei Yin"
      },
      {
        "name" : "Alan Wee-Chung Liew"
      },
      {
        "name" : "Hui Tian"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.05183v1",
    "title" : "Privacy Drift: Evolving Privacy Concerns in Incremental Learning",
    "summary" : "In the evolving landscape of machine learning (ML), Federated Learning (FL)\npresents a paradigm shift towards decentralized model training while preserving\nuser data privacy. This paper introduces the concept of ``privacy drift\", an\ninnovative framework that parallels the well-known phenomenon of concept drift.\nWhile concept drift addresses the variability in model accuracy over time due\nto changes in the data, privacy drift encapsulates the variation in the leakage\nof private information as models undergo incremental training. By defining and\nexamining privacy drift, this study aims to unveil the nuanced relationship\nbetween the evolution of model performance and the integrity of data privacy.\nThrough rigorous experimentation, we investigate the dynamics of privacy drift\nin FL systems, focusing on how model updates and data distribution shifts\ninfluence the susceptibility of models to privacy attacks, such as membership\ninference attacks (MIA). Our results highlight a complex interplay between\nmodel accuracy and privacy safeguards, revealing that enhancements in model\nperformance can lead to increased privacy risks. We provide empirical evidence\nfrom experiments on customized datasets derived from CIFAR-100 (Canadian\nInstitute for Advanced Research, 100 classes), showcasing the impact of data\nand concept drift on privacy. This work lays the groundwork for future research\non privacy-aware machine learning, aiming to achieve a delicate balance between\nmodel accuracy and data privacy in decentralized environments.",
    "updated" : "2024-12-06T17:04:09Z",
    "published" : "2024-12-06T17:04:09Z",
    "authors" : [
      {
        "name" : "Sayyed Farid Ahamed"
      },
      {
        "name" : "Soumya Banerjee"
      },
      {
        "name" : "Sandip Roy"
      },
      {
        "name" : "Aayush Kapoor"
      },
      {
        "name" : "Marc Vucovich"
      },
      {
        "name" : "Kevin Choi"
      },
      {
        "name" : "Abdul Rahman"
      },
      {
        "name" : "Edward Bowen"
      },
      {
        "name" : "Sachin Shetty"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.05164v1",
    "title" : "A Differentially Private Kaplan-Meier Estimator for Privacy-Preserving\n  Survival Analysis",
    "summary" : "This paper presents a differentially private approach to Kaplan-Meier\nestimation that achieves accurate survival probability estimates while\nsafeguarding individual privacy. The Kaplan-Meier estimator is widely used in\nsurvival analysis to estimate survival functions over time, yet applying it to\nsensitive datasets, such as clinical records, risks revealing private\ninformation. To address this, we introduce a novel algorithm that applies\ntime-indexed Laplace noise, dynamic clipping, and smoothing to produce a\nprivacy-preserving survival curve while maintaining the cumulative structure of\nthe Kaplan-Meier estimator. By scaling noise over time, the algorithm accounts\nfor decreasing sensitivity as fewer individuals remain at risk, while dynamic\nclipping and smoothing prevent extreme values and reduce fluctuations,\npreserving the natural shape of the survival curve.\n  Our results, evaluated on the NCCTG lung cancer dataset, show that the\nproposed method effectively lowers root mean squared error (RMSE) and enhances\naccuracy across privacy budgets ($\\epsilon$). At $\\epsilon = 10$, the algorithm\nachieves an RMSE as low as 0.04, closely approximating non-private estimates.\nAdditionally, membership inference attacks reveal that higher $\\epsilon$ values\n(e.g., $\\epsilon \\geq 6$) significantly reduce influential points, particularly\nat higher thresholds, lowering susceptibility to inference attacks. These\nfindings confirm that our approach balances privacy and utility, advancing\nprivacy-preserving survival analysis.",
    "updated" : "2024-12-06T16:29:53Z",
    "published" : "2024-12-06T16:29:53Z",
    "authors" : [
      {
        "name" : "Narasimha Raghavan Veeraragavan"
      },
      {
        "name" : "Sai Praneeth Karimireddy"
      },
      {
        "name" : "Jan Franz Nygrd"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.04697v1",
    "title" : "Privacy-Preserving Retrieval Augmented Generation with Differential\n  Privacy",
    "summary" : "With the recent remarkable advancement of large language models (LLMs), there\nhas been a growing interest in utilizing them in the domains with highly\nsensitive data that lies outside their training data. For this purpose,\nretrieval augmented generation (RAG) is particularly effective -- it assists\nLLMs by directly providing relevant information from the external knowledge\nsources. However, without extra privacy safeguards, RAG outputs risk leaking\nsensitive information from the external data source. In this work, we explore\nRAG under differential privacy (DP), a formal guarantee of data privacy. The\nmain challenge with differentially private RAG is how to generate long accurate\nanswers within a moderate privacy budget. We address this by proposing an\nalgorithm that smartly spends privacy budget only for the tokens that require\nthe sensitive information and uses the non-private LLM for other tokens. Our\nextensive empirical evaluations reveal that our algorithm outperforms the\nnon-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$\nacross different models and datasets.",
    "updated" : "2024-12-06T01:20:16Z",
    "published" : "2024-12-06T01:20:16Z",
    "authors" : [
      {
        "name" : "Tatsuki Koga"
      },
      {
        "name" : "Ruihan Wu"
      },
      {
        "name" : "Kamalika Chaudhuri"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2412.04518v1",
    "title" : "Privacy-Preserving Gesture Tracking System Utilizing Frequency-Hopping\n  RFID Signals",
    "summary" : "Gesture tracking technology provides users with a hands free interactive\nexperience without the need to hold or touch devices. However, current gesture\ntracking research has primarily focused on tracking accuracy while neglecting\nissues of user privacy protection and security. This study aims to develop a\ngesture tracking system based on frequency hopping RFID signals that\neffectively protects user privacy without compromising tracking efficiency and\naccuracy. By introducing frequency hopping technology, we have designed a\nmechanism that prevents potential eavesdroppers from obtaining raw RFID\nsignals, thereby enhancing the systems privacy protection capabilities. The\nsystem architec ture includes the collection of RFID signals, data processing,\nsignal recovery, and gesture tracking. Experimental results show that our\nmethod significantly improves privacy protection levels while maintaining real\ntime and accuracy. This research not only provides a new perspective for the\nfield of gesture tracking but also offers valuable insights for the use of RFID\ntechnology in privacy-sensitive applications.",
    "updated" : "2024-12-05T08:51:02Z",
    "published" : "2024-12-05T08:51:02Z",
    "authors" : [
      {
        "name" : "Bojun Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  }
]