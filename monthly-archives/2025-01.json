[
  {
    "id" : "http://arxiv.org/abs/2501.01353v1",
    "title" : "Privacy Preservation in MIMO-OFDM Localization Systems: A Beamforming\n  Approach",
    "summary" : "We investigate an uplink MIMO-OFDM localization scenario where a legitimate\nbase station (BS) aims to localize a user equipment (UE) using pilot signals\ntransmitted by the UE, while an unauthorized BS attempts to localize the UE by\neavesdropping on these pilots, posing a risk to the UE's location privacy. To\nenhance legitimate localization performance while protecting the UE's privacy,\nwe formulate an optimization problem regarding the beamformers at the UE,\naiming to minimize the Cram\\'er-Rao bound (CRB) for legitimate localization\nwhile constraining the CRB for unauthorized localization above a threshold. A\npenalty dual decomposition optimization framework is employed to solve the\nproblem, leading to a novel beamforming approach for location privacy\npreservation. Numerical results confirm the effectiveness of the proposed\napproach and demonstrate its superiority over existing benchmarks.",
    "updated" : "2025-01-02T17:08:15Z",
    "published" : "2025-01-02T17:08:15Z",
    "authors" : [
      {
        "name" : "Yuchen Zhang"
      },
      {
        "name" : "Hui Chen"
      },
      {
        "name" : "Musa Furkan Keskin"
      },
      {
        "name" : "Alireza Pourafzal"
      },
      {
        "name" : "Pinjun Zheng"
      },
      {
        "name" : "Henk Wymeersch"
      },
      {
        "name" : "Tareq Y. Al-Naffouri"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01131v1",
    "title" : "Privacy Bills of Materials: A Transparent Privacy Information Inventory\n  for Collaborative Privacy Notice Generation in Mobile App Development",
    "summary" : "Privacy regulations mandate that developers must provide authentic and\ncomprehensive privacy notices, e.g., privacy policies or labels, to inform\nusers of their apps' privacy practices. However, due to a lack of knowledge of\nprivacy requirements, developers often struggle to create accurate privacy\nnotices, especially for sophisticated mobile apps with complex features and in\ncrowded development teams. To address these challenges, we introduce Privacy\nBills of Materials (PriBOM), a systematic software engineering approach that\nleverages different development team roles to better capture and coordinate\nmobile app privacy information. PriBOM facilitates transparency-centric privacy\ndocumentation and specific privacy notice creation, enabling traceability and\ntrackability of privacy practices. We present a pre-fill of PriBOM based on\nstatic analysis and privacy notice analysis techniques. We demonstrate the\nperceived usefulness of PriBOM through a human evaluation with 150 diverse\nparticipants. Our findings suggest that PriBOM could serve as a significant\nsolution for providing privacy support in DevOps for mobile apps.",
    "updated" : "2025-01-02T08:14:52Z",
    "published" : "2025-01-02T08:14:52Z",
    "authors" : [
      {
        "name" : "Zhen Tao"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Zhenchang Xing"
      },
      {
        "name" : "Xiaoyu Sun"
      },
      {
        "name" : "Omar Haggag"
      },
      {
        "name" : "John Grundy"
      },
      {
        "name" : "Ze Shi Li"
      },
      {
        "name" : "Jingjie Li"
      },
      {
        "name" : "Liming Zhu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01063v1",
    "title" : "FAPL-DM-BC: A Secure and Scalable FL Framework with Adaptive Privacy and\n  Dynamic Masking, Blockchain, and XAI for the IoVs",
    "summary" : "The FAPL-DM-BC solution is a new FL-based privacy, security, and scalability\nsolution for the Internet of Vehicles (IoV). It leverages Federated Adaptive\nPrivacy-Aware Learning (FAPL) and Dynamic Masking (DM) to learn and adaptively\nchange privacy policies in response to changing data sensitivity and state in\nreal-time, for the optimal privacy-utility tradeoff. Secure Logging and\nVerification, Blockchain-based provenance and decentralized validation, and\nCloud Microservices Secure Aggregation using FedAvg (Federated Averaging) and\nSecure Multi-Party Computation (SMPC). Two-model feedback, driven by\nModel-Agnostic Explainable AI (XAI), certifies local predictions and\nexplanations to drive it to the next level of efficiency. Combining local\nfeedback with world knowledge through a weighted mean computation, FAPL-DM-BC\nassures federated learning that is secure, scalable, and interpretable.\nSelf-driving cars, traffic management, and forecasting, vehicular network\ncybersecurity in real-time, and smart cities are a few possible applications of\nthis integrated, privacy-safe, and high-performance IoV platform.",
    "updated" : "2025-01-02T05:21:52Z",
    "published" : "2025-01-02T05:21:52Z",
    "authors" : [
      {
        "name" : "Sathwik Narkedimilli"
      },
      {
        "name" : "Amballa Venkata Sriram"
      },
      {
        "name" : "Sujith Makam"
      },
      {
        "name" : "MSVPJ Sathvik"
      },
      {
        "name" : "Sai Prashanth Mallellu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.00824v1",
    "title" : "Information Sifting Funnel: Privacy-preserving Collaborative Inference\n  Against Model Inversion Attacks",
    "summary" : "The complexity of neural networks and inference tasks, coupled with demands\nfor computational efficiency and real-time feedback, poses significant\nchallenges for resource-constrained edge devices. Collaborative inference\nmitigates this by assigning shallow feature extraction to edge devices and\noffloading features to the cloud for further inference, reducing computational\nload. However, transmitted features remain susceptible to model inversion\nattacks (MIAs), which can reconstruct original input data. Current defenses,\nsuch as perturbation and information bottleneck techniques, offer explainable\nprotection but face limitations, including the lack of standardized criteria\nfor assessing MIA difficulty, challenges in mutual information estimation, and\ntrade-offs among usability, privacy, and deployability.\n  To address these challenges, we introduce the first criterion to evaluate MIA\ndifficulty in collaborative inference, supported by theoretical analysis of\nexisting attacks and defenses, validated using experiments with the Mutual\nInformation Neural Estimator (MINE). Based on these findings, we propose\nSiftFunnel, a privacy-preserving framework for collaborative inference. The\nedge model is trained with linear and non-linear correlation constraints to\nreduce redundant information in transmitted features, enhancing privacy\nprotection. Label smoothing and a cloud-based upsampling module are added to\nbalance usability and privacy. To improve deployability, the edge model\nincorporates a funnel-shaped structure and attention mechanisms, preserving\nboth privacy and usability. Extensive experiments demonstrate that SiftFunnel\noutperforms state-of-the-art defenses against MIAs, achieving superior privacy\nprotection with less than 3% accuracy loss and striking an optimal balance\namong usability, privacy, and practicality.",
    "updated" : "2025-01-01T13:00:01Z",
    "published" : "2025-01-01T13:00:01Z",
    "authors" : [
      {
        "name" : "Rongke Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01786v1",
    "title" : "Advancing privacy in learning analytics using differential privacy",
    "summary" : "This paper addresses the challenge of balancing learner data privacy with the\nuse of data in learning analytics (LA) by proposing a novel framework by\napplying Differential Privacy (DP). The need for more robust privacy protection\nkeeps increasing, driven by evolving legal regulations and heightened privacy\nconcerns, as well as traditional anonymization methods being insufficient for\nthe complexities of educational data. To address this, we introduce the first\nDP framework specifically designed for LA and provide practical guidance for\nits implementation. We demonstrate the use of this framework through a LA usage\nscenario and validate DP in safeguarding data privacy against potential attacks\nthrough an experiment on a well-known LA dataset. Additionally, we explore the\ntrade-offs between data privacy and utility across various DP settings. Our\nwork contributes to the field of LA by offering a practical DP framework that\ncan support researchers and practitioners in adopting DP in their works.",
    "updated" : "2025-01-03T12:36:11Z",
    "published" : "2025-01-03T12:36:11Z",
    "authors" : [
      {
        "name" : "Qinyi Liu"
      },
      {
        "name" : "Ronas Shakya"
      },
      {
        "name" : "Mohammad Khalil"
      },
      {
        "name" : "Jelena Jovanovic"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01639v1",
    "title" : "Artificial Intelligent Implications on Health Data Privacy and\n  Confidentiality",
    "summary" : "The rapid integration of artificial intelligence (AI) in healthcare is\nrevolutionizing medical diagnostics, personalized medicine, and operational\nefficiency. However, alongside these advancements, significant challenges arise\nconcerning patient data privacy, ethical considerations, and regulatory\ncompliance. This paper examines the dual impact of AI on healthcare,\nhighlighting its transformative potential and the critical need for\nsafeguarding sensitive health information. It explores the role of the Health\nInsurance Portability and Accountability Act (HIPAA) as a regulatory framework\nfor ensuring data privacy and security, emphasizing the importance of robust\nsafeguards and ethical standards in AI-driven healthcare. Through case studies,\nincluding AI applications in diabetic retinopathy, oncology, and the\ncontroversies surrounding data sharing, this study underscores the ethical and\nlegal complexities of AI implementation. A balanced approach that fosters\ninnovation while maintaining patient trust and privacy is imperative. The\nfindings emphasize the importance of continuous education, transparency, and\nadherence to regulatory frameworks to harness AI's full potential responsibly\nand ethically in healthcare.",
    "updated" : "2025-01-03T05:17:23Z",
    "published" : "2025-01-03T05:17:23Z",
    "authors" : [
      {
        "name" : "Ahmad Momani"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03222v1",
    "title" : "Characterizing the Accuracy-Communication-Privacy Trade-off in\n  Distributed Stochastic Convex Optimization",
    "summary" : "We consider the problem of differentially private stochastic convex\noptimization (DP-SCO) in a distributed setting with $M$ clients, where each of\nthem has a local dataset of $N$ i.i.d. data samples from an underlying data\ndistribution. The objective is to design an algorithm to minimize a convex\npopulation loss using a collaborative effort across $M$ clients, while ensuring\nthe privacy of the local datasets. In this work, we investigate the\naccuracy-communication-privacy trade-off for this problem. We establish\nmatching converse and achievability results using a novel lower bound and a new\nalgorithm for distributed DP-SCO based on Vaidya's plane cutting method. Thus,\nour results provide a complete characterization of the\naccuracy-communication-privacy trade-off for DP-SCO in the distributed setting.",
    "updated" : "2025-01-06T18:57:05Z",
    "published" : "2025-01-06T18:57:05Z",
    "authors" : [
      {
        "name" : "Sudeep Salgia"
      },
      {
        "name" : "Nikola Pavlovic"
      },
      {
        "name" : "Yuejie Chi"
      },
      {
        "name" : "Qing Zhao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02893v1",
    "title" : "A Volumetric Approach to Privacy of Dynamical Systems",
    "summary" : "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack.",
    "updated" : "2025-01-06T10:15:21Z",
    "published" : "2025-01-06T10:15:21Z",
    "authors" : [
      {
        "name" : "Chuanghong Weng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02804v1",
    "title" : "Latency and Privacy-Aware Resource Allocation in Vehicular Edge\n  Computing",
    "summary" : "The rapid increase in the number of connected vehicles has led to the\ngeneration of vast amounts of data. As a significant portion of this data\npertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it\nis predominantly generated at the edge. Considering the enormous volume of\ndata, real-time applications, and privacy concerns, it is crucial to process\nthe data at the edge. Neglecting the management of processing resources in\nvehicular edge computing (VEC) could lead to numerous challenges as a\nsubstantial number of vehicles with diverse safety, economic, and entertainment\napplications, along with their data processing, emerge in the near future [1].\nPrevious research in VEC resource allocation has primarily focused on issues\nsuch as response time and privacy preservation techniques. However, an approach\nthat takes into account privacy-aware resource allocation based on vehicular\nnetwork architecture and application requirements has not yet been proposed. In\nthis paper, we present a privacy and latency-aware approach for allocating\nprocessing resources at the edge of the vehicular network, considering the\nspecific requirements of different applications. Our approach involves\ncategorizing vehicular network applications based on their processing accuracy,\nreal-time processing needs, and privacy preservation requirements. We further\ndivide the vehicular network edge into two parts: the user layer (OBUs) is\nconsidered for processing applications with privacy requirements, while the\nallocation of resources in the RSUs and cloud layer is based on the specific\nneeds of different applications. In this study, we evaluate the quality of\nservice based on parameters such as privacy preservation, processing cost,\nmeeting deadlines, and result quality. Comparative analyses demonstrate that\nour approach enhances service quality by 55% compared to existing\nstate-of-the-art methods.",
    "updated" : "2025-01-06T06:44:49Z",
    "published" : "2025-01-06T06:44:49Z",
    "authors" : [
      {
        "name" : "HHossein Ahmadvand"
      },
      {
        "name" : "Fouzhan Foroutan"
      }
    ],
    "categories" : [
      "cs.PF"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02354v1",
    "title" : "PrivDPR: Synthetic Graph Publishing with Deep PageRank under\n  Differential Privacy",
    "summary" : "The objective of privacy-preserving synthetic graph publishing is to\nsafeguard individuals' privacy while retaining the utility of original data.\nMost existing methods focus on graph neural networks under differential privacy\n(DP), and yet two fundamental problems in generating synthetic graphs remain\nopen. First, the current research often encounters high sensitivity due to the\nintricate relationships between nodes in a graph. Second, DP is usually\nachieved through advanced composition mechanisms that tend to converge\nprematurely when working with a small privacy budget. In this paper, inspired\nby the simplicity, effectiveness, and ease of analysis of PageRank, we design\nPrivDPR, a novel privacy-preserving deep PageRank for graph synthesis. In\nparticular, we achieve DP by adding noise to the gradient for a specific weight\nduring learning. Utilizing weight normalization as a bridge, we theoretically\nreveal that increasing the number of layers in PrivDPR can effectively mitigate\nthe high sensitivity and privacy budget splitting. Through formal privacy\nanalysis, we prove that the synthetic graph generated by PrivDPR satisfies\nnode-level DP. Experiments on real-world graph datasets show that PrivDPR\npreserves high data utility across multiple graph structural properties.",
    "updated" : "2025-01-04T18:19:21Z",
    "published" : "2025-01-04T18:19:21Z",
    "authors" : [
      {
        "name" : "Sen Zhang"
      },
      {
        "name" : "Haibo Hu"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Jianliang Xu"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02091v1",
    "title" : "PriveShield: Enhancing User Privacy Using Automatic Isolated Profiles in\n  Browsers",
    "summary" : "Online tracking is a widespread practice on the web with questionable ethics,\nsecurity, and privacy concerns. While web tracking can offer personalized and\ncurated content to Internet users, it operates as a sophisticated surveillance\nmechanism to gather extensive user information. This paper introduces\nPriveShield, a light-weight privacy mechanism that disrupts the information\ngathering cycle while offering more control to Internet users to maintain their\nprivacy. PriveShield is implemented as a browser extension that offers an\nadjustable privacy feature to surf the web with multiple identities or accounts\nsimultaneously without any changes to underlying browser code or services. When\nnecessary, multiple factors are automatically analyzed on the client side to\nisolate cookies and other information that are the basis of online tracking.\nPriveShield creates isolated profiles for clients based on their browsing\nhistory, interactions with websites, and the amount of time they spend on\nspecific websites. This allows the users to easily prevent unwanted browsing\ninformation from being shared with third parties and ad exchanges without the\nneed for manual configuration. Our evaluation results from 54 real-world\nscenarios show that our extension is effective in preventing retargeted ads in\n91% of those scenarios.",
    "updated" : "2025-01-03T20:29:33Z",
    "published" : "2025-01-03T20:29:33Z",
    "authors" : [
      {
        "name" : "Seyed Ali Akhavani"
      },
      {
        "name" : "Engin Kirda"
      },
      {
        "name" : "Amin Kharraz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01639v2",
    "title" : "Implications of Artificial Intelligence on Health Data Privacy and\n  Confidentiality",
    "summary" : "The rapid integration of artificial intelligence (AI) in healthcare is\nrevolutionizing medical diagnostics, personalized medicine, and operational\nefficiency. However, alongside these advancements, significant challenges arise\nconcerning patient data privacy, ethical considerations, and regulatory\ncompliance. This paper examines the dual impact of AI on healthcare,\nhighlighting its transformative potential and the critical need for\nsafeguarding sensitive health information. It explores the role of the Health\nInsurance Portability and Accountability Act (HIPAA) as a regulatory framework\nfor ensuring data privacy and security, emphasizing the importance of robust\nsafeguards and ethical standards in AI-driven healthcare. Through case studies,\nincluding AI applications in diabetic retinopathy, oncology, and the\ncontroversies surrounding data sharing, this study underscores the ethical and\nlegal complexities of AI implementation. A balanced approach that fosters\ninnovation while maintaining patient trust and privacy is imperative. The\nfindings emphasize the importance of continuous education, transparency, and\nadherence to regulatory frameworks to harness AI's full potential responsibly\nand ethically in healthcare.",
    "updated" : "2025-01-06T18:52:32Z",
    "published" : "2025-01-03T05:17:23Z",
    "authors" : [
      {
        "name" : "Ahmad Momani"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03941v1",
    "title" : "Synthetic Data Privacy Metrics",
    "summary" : "Recent advancements in generative AI have made it possible to create\nsynthetic datasets that can be as accurate as real-world data for training AI\nmodels, powering statistical insights, and fostering collaboration with\nsensitive datasets while offering strong privacy guarantees. Effectively\nmeasuring the empirical privacy of synthetic data is an important step in the\nprocess. However, while there is a multitude of new privacy metrics being\npublished every day, there currently is no standardization. In this paper, we\nreview the pros and cons of popular metrics that include simulations of\nadversarial attacks. We also review current best practices for amending\ngenerative models to enhance the privacy of the data they create (e.g.\ndifferential privacy).",
    "updated" : "2025-01-07T17:02:33Z",
    "published" : "2025-01-07T17:02:33Z",
    "authors" : [
      {
        "name" : "Amy Steier"
      },
      {
        "name" : "Lipika Ramaswamy"
      },
      {
        "name" : "Andre Manoel"
      },
      {
        "name" : "Alexa Haushalter"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03451v1",
    "title" : "Structure-Preference Enabled Graph Embedding Generation under\n  Differential Privacy",
    "summary" : "Graph embedding generation techniques aim to learn low-dimensional vectors\nfor each node in a graph and have recently gained increasing research\nattention. Publishing low-dimensional node vectors enables various graph\nanalysis tasks, such as structural equivalence and link prediction. Yet,\nimproper publication opens a backdoor to malicious attackers, who can infer\nsensitive information of individuals from the low-dimensional node vectors.\nExisting methods tackle this issue by developing deep graph learning models\nwith differential privacy (DP). However, they often suffer from large noise\ninjections and cannot provide structural preferences consistent with mining\nobjectives. Recently, skip-gram based graph embedding generation techniques are\nwidely used due to their ability to extract customizable structures. Based on\nskip-gram, we present SE-PrivGEmb, a structure-preference enabled graph\nembedding generation under DP. For arbitrary structure preferences, we design a\nunified noise tolerance mechanism via perturbing non-zero vectors. This\nmechanism mitigates utility degradation caused by high sensitivity. By\ncarefully designing negative sampling probabilities in skip-gram, we\ntheoretically demonstrate that skip-gram can preserve arbitrary proximities,\nwhich quantify structural features in graphs. Extensive experiments show that\nour method outperforms existing state-of-the-art methods under structural\nequivalence and link prediction tasks.",
    "updated" : "2025-01-07T00:43:18Z",
    "published" : "2025-01-07T00:43:18Z",
    "authors" : [
      {
        "name" : "Sen Zhang"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Haibo Hu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03391v1",
    "title" : "Privacy-Preserving Smart Contracts for Permissioned Blockchains: A\n  zk-SNARK-Based Recipe Part-1",
    "summary" : "The Bitcoin white paper introduced blockchain technology, enabling trustful\ntransactions without intermediaries. Smart contracts emerged with Ethereum and\nblockchains expanded beyond cryptocurrency, applying to auctions, crowdfunding\nand electronic voting. However, blockchain's transparency raised privacy\nconcerns and initial anonymity measures proved ineffective. Smart contract\nprivacy solutions employed zero-knowledge proofs, homomorphic encryption and\ntrusted execution environments. These approaches have practical drawbacks, such\nas limited functionality, high computation times and trust on third parties\nrequirements, being not fully decentralized. This work proposes a solution\nutilizing zk-SNARKs to provide privacy in smart contracts and blockchains. The\nsolution supports both fungible and nonfungible tokens. Additionally, the\nproposal includes a new type of transactions, called delegated transactions,\nwhich enable use cases like Delivery vs Payment (DvP).",
    "updated" : "2025-01-06T21:16:33Z",
    "published" : "2025-01-06T21:16:33Z",
    "authors" : [
      {
        "name" : "Aldenio Burgos"
      },
      {
        "name" : "Eduardo Alchieri"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02893v2",
    "title" : "A Volumetric Approach to Privacy of Dynamical Systems",
    "summary" : "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack.",
    "updated" : "2025-01-07T13:21:10Z",
    "published" : "2025-01-06T10:15:21Z",
    "authors" : [
      {
        "name" : "Chuanghong Weng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02804v2",
    "title" : "Latency and Privacy-Aware Resource Allocation in Vehicular Edge\n  Computing",
    "summary" : "The rapid increase in the number of connected vehicles has led to the\ngeneration of vast amounts of data. As a significant portion of this data\npertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it\nis predominantly generated at the edge. Considering the enormous volume of\ndata, real-time applications, and privacy concerns, it is crucial to process\nthe data at the edge. Neglecting the management of processing resources in\nvehicular edge computing (VEC) could lead to numerous challenges as a\nsubstantial number of vehicles with diverse safety, economic, and entertainment\napplications, along with their data processing, emerge in the near future [1].\nPrevious research in VEC resource allocation has primarily focused on issues\nsuch as response time and privacy preservation techniques. However, an approach\nthat takes into account privacy-aware resource allocation based on vehicular\nnetwork architecture and application requirements has not yet been proposed. In\nthis paper, we present a privacy and latency-aware approach for allocating\nprocessing resources at the edge of the vehicular network, considering the\nspecific requirements of different applications. Our approach involves\ncategorizing vehicular network applications based on their processing accuracy,\nreal-time processing needs, and privacy preservation requirements. We further\ndivide the vehicular network edge into two parts: the user layer (OBUs) is\nconsidered for processing applications with privacy requirements, while the\nallocation of resources in the RSUs and cloud layer is based on the specific\nneeds of different applications. In this study, we evaluate the quality of\nservice based on parameters such as privacy preservation, processing cost,\nmeeting deadlines, and result quality. Comparative analyses demonstrate that\nour approach enhances service quality by 55% compared to existing\nstate-of-the-art methods.",
    "updated" : "2025-01-07T07:17:16Z",
    "published" : "2025-01-06T06:44:49Z",
    "authors" : [
      {
        "name" : "Hossein Ahmadvand"
      },
      {
        "name" : "Fouzhan Foroutan"
      }
    ],
    "categories" : [
      "cs.PF"
    ]
  }
]