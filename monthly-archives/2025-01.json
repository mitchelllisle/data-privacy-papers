[
  {
    "id" : "http://arxiv.org/abs/2501.01353v1",
    "title" : "Privacy Preservation in MIMO-OFDM Localization Systems: A Beamforming\n  Approach",
    "summary" : "We investigate an uplink MIMO-OFDM localization scenario where a legitimate\nbase station (BS) aims to localize a user equipment (UE) using pilot signals\ntransmitted by the UE, while an unauthorized BS attempts to localize the UE by\neavesdropping on these pilots, posing a risk to the UE's location privacy. To\nenhance legitimate localization performance while protecting the UE's privacy,\nwe formulate an optimization problem regarding the beamformers at the UE,\naiming to minimize the Cram\\'er-Rao bound (CRB) for legitimate localization\nwhile constraining the CRB for unauthorized localization above a threshold. A\npenalty dual decomposition optimization framework is employed to solve the\nproblem, leading to a novel beamforming approach for location privacy\npreservation. Numerical results confirm the effectiveness of the proposed\napproach and demonstrate its superiority over existing benchmarks.",
    "updated" : "2025-01-02T17:08:15Z",
    "published" : "2025-01-02T17:08:15Z",
    "authors" : [
      {
        "name" : "Yuchen Zhang"
      },
      {
        "name" : "Hui Chen"
      },
      {
        "name" : "Musa Furkan Keskin"
      },
      {
        "name" : "Alireza Pourafzal"
      },
      {
        "name" : "Pinjun Zheng"
      },
      {
        "name" : "Henk Wymeersch"
      },
      {
        "name" : "Tareq Y. Al-Naffouri"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01131v1",
    "title" : "Privacy Bills of Materials: A Transparent Privacy Information Inventory\n  for Collaborative Privacy Notice Generation in Mobile App Development",
    "summary" : "Privacy regulations mandate that developers must provide authentic and\ncomprehensive privacy notices, e.g., privacy policies or labels, to inform\nusers of their apps' privacy practices. However, due to a lack of knowledge of\nprivacy requirements, developers often struggle to create accurate privacy\nnotices, especially for sophisticated mobile apps with complex features and in\ncrowded development teams. To address these challenges, we introduce Privacy\nBills of Materials (PriBOM), a systematic software engineering approach that\nleverages different development team roles to better capture and coordinate\nmobile app privacy information. PriBOM facilitates transparency-centric privacy\ndocumentation and specific privacy notice creation, enabling traceability and\ntrackability of privacy practices. We present a pre-fill of PriBOM based on\nstatic analysis and privacy notice analysis techniques. We demonstrate the\nperceived usefulness of PriBOM through a human evaluation with 150 diverse\nparticipants. Our findings suggest that PriBOM could serve as a significant\nsolution for providing privacy support in DevOps for mobile apps.",
    "updated" : "2025-01-02T08:14:52Z",
    "published" : "2025-01-02T08:14:52Z",
    "authors" : [
      {
        "name" : "Zhen Tao"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Zhenchang Xing"
      },
      {
        "name" : "Xiaoyu Sun"
      },
      {
        "name" : "Omar Haggag"
      },
      {
        "name" : "John Grundy"
      },
      {
        "name" : "Ze Shi Li"
      },
      {
        "name" : "Jingjie Li"
      },
      {
        "name" : "Liming Zhu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01063v1",
    "title" : "FAPL-DM-BC: A Secure and Scalable FL Framework with Adaptive Privacy and\n  Dynamic Masking, Blockchain, and XAI for the IoVs",
    "summary" : "The FAPL-DM-BC solution is a new FL-based privacy, security, and scalability\nsolution for the Internet of Vehicles (IoV). It leverages Federated Adaptive\nPrivacy-Aware Learning (FAPL) and Dynamic Masking (DM) to learn and adaptively\nchange privacy policies in response to changing data sensitivity and state in\nreal-time, for the optimal privacy-utility tradeoff. Secure Logging and\nVerification, Blockchain-based provenance and decentralized validation, and\nCloud Microservices Secure Aggregation using FedAvg (Federated Averaging) and\nSecure Multi-Party Computation (SMPC). Two-model feedback, driven by\nModel-Agnostic Explainable AI (XAI), certifies local predictions and\nexplanations to drive it to the next level of efficiency. Combining local\nfeedback with world knowledge through a weighted mean computation, FAPL-DM-BC\nassures federated learning that is secure, scalable, and interpretable.\nSelf-driving cars, traffic management, and forecasting, vehicular network\ncybersecurity in real-time, and smart cities are a few possible applications of\nthis integrated, privacy-safe, and high-performance IoV platform.",
    "updated" : "2025-01-02T05:21:52Z",
    "published" : "2025-01-02T05:21:52Z",
    "authors" : [
      {
        "name" : "Sathwik Narkedimilli"
      },
      {
        "name" : "Amballa Venkata Sriram"
      },
      {
        "name" : "Sujith Makam"
      },
      {
        "name" : "MSVPJ Sathvik"
      },
      {
        "name" : "Sai Prashanth Mallellu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.00824v1",
    "title" : "Information Sifting Funnel: Privacy-preserving Collaborative Inference\n  Against Model Inversion Attacks",
    "summary" : "The complexity of neural networks and inference tasks, coupled with demands\nfor computational efficiency and real-time feedback, poses significant\nchallenges for resource-constrained edge devices. Collaborative inference\nmitigates this by assigning shallow feature extraction to edge devices and\noffloading features to the cloud for further inference, reducing computational\nload. However, transmitted features remain susceptible to model inversion\nattacks (MIAs), which can reconstruct original input data. Current defenses,\nsuch as perturbation and information bottleneck techniques, offer explainable\nprotection but face limitations, including the lack of standardized criteria\nfor assessing MIA difficulty, challenges in mutual information estimation, and\ntrade-offs among usability, privacy, and deployability.\n  To address these challenges, we introduce the first criterion to evaluate MIA\ndifficulty in collaborative inference, supported by theoretical analysis of\nexisting attacks and defenses, validated using experiments with the Mutual\nInformation Neural Estimator (MINE). Based on these findings, we propose\nSiftFunnel, a privacy-preserving framework for collaborative inference. The\nedge model is trained with linear and non-linear correlation constraints to\nreduce redundant information in transmitted features, enhancing privacy\nprotection. Label smoothing and a cloud-based upsampling module are added to\nbalance usability and privacy. To improve deployability, the edge model\nincorporates a funnel-shaped structure and attention mechanisms, preserving\nboth privacy and usability. Extensive experiments demonstrate that SiftFunnel\noutperforms state-of-the-art defenses against MIAs, achieving superior privacy\nprotection with less than 3% accuracy loss and striking an optimal balance\namong usability, privacy, and practicality.",
    "updated" : "2025-01-01T13:00:01Z",
    "published" : "2025-01-01T13:00:01Z",
    "authors" : [
      {
        "name" : "Rongke Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01786v1",
    "title" : "Advancing privacy in learning analytics using differential privacy",
    "summary" : "This paper addresses the challenge of balancing learner data privacy with the\nuse of data in learning analytics (LA) by proposing a novel framework by\napplying Differential Privacy (DP). The need for more robust privacy protection\nkeeps increasing, driven by evolving legal regulations and heightened privacy\nconcerns, as well as traditional anonymization methods being insufficient for\nthe complexities of educational data. To address this, we introduce the first\nDP framework specifically designed for LA and provide practical guidance for\nits implementation. We demonstrate the use of this framework through a LA usage\nscenario and validate DP in safeguarding data privacy against potential attacks\nthrough an experiment on a well-known LA dataset. Additionally, we explore the\ntrade-offs between data privacy and utility across various DP settings. Our\nwork contributes to the field of LA by offering a practical DP framework that\ncan support researchers and practitioners in adopting DP in their works.",
    "updated" : "2025-01-03T12:36:11Z",
    "published" : "2025-01-03T12:36:11Z",
    "authors" : [
      {
        "name" : "Qinyi Liu"
      },
      {
        "name" : "Ronas Shakya"
      },
      {
        "name" : "Mohammad Khalil"
      },
      {
        "name" : "Jelena Jovanovic"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01639v1",
    "title" : "Artificial Intelligent Implications on Health Data Privacy and\n  Confidentiality",
    "summary" : "The rapid integration of artificial intelligence (AI) in healthcare is\nrevolutionizing medical diagnostics, personalized medicine, and operational\nefficiency. However, alongside these advancements, significant challenges arise\nconcerning patient data privacy, ethical considerations, and regulatory\ncompliance. This paper examines the dual impact of AI on healthcare,\nhighlighting its transformative potential and the critical need for\nsafeguarding sensitive health information. It explores the role of the Health\nInsurance Portability and Accountability Act (HIPAA) as a regulatory framework\nfor ensuring data privacy and security, emphasizing the importance of robust\nsafeguards and ethical standards in AI-driven healthcare. Through case studies,\nincluding AI applications in diabetic retinopathy, oncology, and the\ncontroversies surrounding data sharing, this study underscores the ethical and\nlegal complexities of AI implementation. A balanced approach that fosters\ninnovation while maintaining patient trust and privacy is imperative. The\nfindings emphasize the importance of continuous education, transparency, and\nadherence to regulatory frameworks to harness AI's full potential responsibly\nand ethically in healthcare.",
    "updated" : "2025-01-03T05:17:23Z",
    "published" : "2025-01-03T05:17:23Z",
    "authors" : [
      {
        "name" : "Ahmad Momani"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03222v1",
    "title" : "Characterizing the Accuracy-Communication-Privacy Trade-off in\n  Distributed Stochastic Convex Optimization",
    "summary" : "We consider the problem of differentially private stochastic convex\noptimization (DP-SCO) in a distributed setting with $M$ clients, where each of\nthem has a local dataset of $N$ i.i.d. data samples from an underlying data\ndistribution. The objective is to design an algorithm to minimize a convex\npopulation loss using a collaborative effort across $M$ clients, while ensuring\nthe privacy of the local datasets. In this work, we investigate the\naccuracy-communication-privacy trade-off for this problem. We establish\nmatching converse and achievability results using a novel lower bound and a new\nalgorithm for distributed DP-SCO based on Vaidya's plane cutting method. Thus,\nour results provide a complete characterization of the\naccuracy-communication-privacy trade-off for DP-SCO in the distributed setting.",
    "updated" : "2025-01-06T18:57:05Z",
    "published" : "2025-01-06T18:57:05Z",
    "authors" : [
      {
        "name" : "Sudeep Salgia"
      },
      {
        "name" : "Nikola Pavlovic"
      },
      {
        "name" : "Yuejie Chi"
      },
      {
        "name" : "Qing Zhao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02893v1",
    "title" : "A Volumetric Approach to Privacy of Dynamical Systems",
    "summary" : "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack.",
    "updated" : "2025-01-06T10:15:21Z",
    "published" : "2025-01-06T10:15:21Z",
    "authors" : [
      {
        "name" : "Chuanghong Weng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02804v1",
    "title" : "Latency and Privacy-Aware Resource Allocation in Vehicular Edge\n  Computing",
    "summary" : "The rapid increase in the number of connected vehicles has led to the\ngeneration of vast amounts of data. As a significant portion of this data\npertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it\nis predominantly generated at the edge. Considering the enormous volume of\ndata, real-time applications, and privacy concerns, it is crucial to process\nthe data at the edge. Neglecting the management of processing resources in\nvehicular edge computing (VEC) could lead to numerous challenges as a\nsubstantial number of vehicles with diverse safety, economic, and entertainment\napplications, along with their data processing, emerge in the near future [1].\nPrevious research in VEC resource allocation has primarily focused on issues\nsuch as response time and privacy preservation techniques. However, an approach\nthat takes into account privacy-aware resource allocation based on vehicular\nnetwork architecture and application requirements has not yet been proposed. In\nthis paper, we present a privacy and latency-aware approach for allocating\nprocessing resources at the edge of the vehicular network, considering the\nspecific requirements of different applications. Our approach involves\ncategorizing vehicular network applications based on their processing accuracy,\nreal-time processing needs, and privacy preservation requirements. We further\ndivide the vehicular network edge into two parts: the user layer (OBUs) is\nconsidered for processing applications with privacy requirements, while the\nallocation of resources in the RSUs and cloud layer is based on the specific\nneeds of different applications. In this study, we evaluate the quality of\nservice based on parameters such as privacy preservation, processing cost,\nmeeting deadlines, and result quality. Comparative analyses demonstrate that\nour approach enhances service quality by 55% compared to existing\nstate-of-the-art methods.",
    "updated" : "2025-01-06T06:44:49Z",
    "published" : "2025-01-06T06:44:49Z",
    "authors" : [
      {
        "name" : "HHossein Ahmadvand"
      },
      {
        "name" : "Fouzhan Foroutan"
      }
    ],
    "categories" : [
      "cs.PF"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02354v1",
    "title" : "PrivDPR: Synthetic Graph Publishing with Deep PageRank under\n  Differential Privacy",
    "summary" : "The objective of privacy-preserving synthetic graph publishing is to\nsafeguard individuals' privacy while retaining the utility of original data.\nMost existing methods focus on graph neural networks under differential privacy\n(DP), and yet two fundamental problems in generating synthetic graphs remain\nopen. First, the current research often encounters high sensitivity due to the\nintricate relationships between nodes in a graph. Second, DP is usually\nachieved through advanced composition mechanisms that tend to converge\nprematurely when working with a small privacy budget. In this paper, inspired\nby the simplicity, effectiveness, and ease of analysis of PageRank, we design\nPrivDPR, a novel privacy-preserving deep PageRank for graph synthesis. In\nparticular, we achieve DP by adding noise to the gradient for a specific weight\nduring learning. Utilizing weight normalization as a bridge, we theoretically\nreveal that increasing the number of layers in PrivDPR can effectively mitigate\nthe high sensitivity and privacy budget splitting. Through formal privacy\nanalysis, we prove that the synthetic graph generated by PrivDPR satisfies\nnode-level DP. Experiments on real-world graph datasets show that PrivDPR\npreserves high data utility across multiple graph structural properties.",
    "updated" : "2025-01-04T18:19:21Z",
    "published" : "2025-01-04T18:19:21Z",
    "authors" : [
      {
        "name" : "Sen Zhang"
      },
      {
        "name" : "Haibo Hu"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Jianliang Xu"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02091v1",
    "title" : "PriveShield: Enhancing User Privacy Using Automatic Isolated Profiles in\n  Browsers",
    "summary" : "Online tracking is a widespread practice on the web with questionable ethics,\nsecurity, and privacy concerns. While web tracking can offer personalized and\ncurated content to Internet users, it operates as a sophisticated surveillance\nmechanism to gather extensive user information. This paper introduces\nPriveShield, a light-weight privacy mechanism that disrupts the information\ngathering cycle while offering more control to Internet users to maintain their\nprivacy. PriveShield is implemented as a browser extension that offers an\nadjustable privacy feature to surf the web with multiple identities or accounts\nsimultaneously without any changes to underlying browser code or services. When\nnecessary, multiple factors are automatically analyzed on the client side to\nisolate cookies and other information that are the basis of online tracking.\nPriveShield creates isolated profiles for clients based on their browsing\nhistory, interactions with websites, and the amount of time they spend on\nspecific websites. This allows the users to easily prevent unwanted browsing\ninformation from being shared with third parties and ad exchanges without the\nneed for manual configuration. Our evaluation results from 54 real-world\nscenarios show that our extension is effective in preventing retargeted ads in\n91% of those scenarios.",
    "updated" : "2025-01-03T20:29:33Z",
    "published" : "2025-01-03T20:29:33Z",
    "authors" : [
      {
        "name" : "Seyed Ali Akhavani"
      },
      {
        "name" : "Engin Kirda"
      },
      {
        "name" : "Amin Kharraz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01639v2",
    "title" : "Implications of Artificial Intelligence on Health Data Privacy and\n  Confidentiality",
    "summary" : "The rapid integration of artificial intelligence (AI) in healthcare is\nrevolutionizing medical diagnostics, personalized medicine, and operational\nefficiency. However, alongside these advancements, significant challenges arise\nconcerning patient data privacy, ethical considerations, and regulatory\ncompliance. This paper examines the dual impact of AI on healthcare,\nhighlighting its transformative potential and the critical need for\nsafeguarding sensitive health information. It explores the role of the Health\nInsurance Portability and Accountability Act (HIPAA) as a regulatory framework\nfor ensuring data privacy and security, emphasizing the importance of robust\nsafeguards and ethical standards in AI-driven healthcare. Through case studies,\nincluding AI applications in diabetic retinopathy, oncology, and the\ncontroversies surrounding data sharing, this study underscores the ethical and\nlegal complexities of AI implementation. A balanced approach that fosters\ninnovation while maintaining patient trust and privacy is imperative. The\nfindings emphasize the importance of continuous education, transparency, and\nadherence to regulatory frameworks to harness AI's full potential responsibly\nand ethically in healthcare.",
    "updated" : "2025-01-06T18:52:32Z",
    "published" : "2025-01-03T05:17:23Z",
    "authors" : [
      {
        "name" : "Ahmad Momani"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03941v1",
    "title" : "Synthetic Data Privacy Metrics",
    "summary" : "Recent advancements in generative AI have made it possible to create\nsynthetic datasets that can be as accurate as real-world data for training AI\nmodels, powering statistical insights, and fostering collaboration with\nsensitive datasets while offering strong privacy guarantees. Effectively\nmeasuring the empirical privacy of synthetic data is an important step in the\nprocess. However, while there is a multitude of new privacy metrics being\npublished every day, there currently is no standardization. In this paper, we\nreview the pros and cons of popular metrics that include simulations of\nadversarial attacks. We also review current best practices for amending\ngenerative models to enhance the privacy of the data they create (e.g.\ndifferential privacy).",
    "updated" : "2025-01-07T17:02:33Z",
    "published" : "2025-01-07T17:02:33Z",
    "authors" : [
      {
        "name" : "Amy Steier"
      },
      {
        "name" : "Lipika Ramaswamy"
      },
      {
        "name" : "Andre Manoel"
      },
      {
        "name" : "Alexa Haushalter"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03451v1",
    "title" : "Structure-Preference Enabled Graph Embedding Generation under\n  Differential Privacy",
    "summary" : "Graph embedding generation techniques aim to learn low-dimensional vectors\nfor each node in a graph and have recently gained increasing research\nattention. Publishing low-dimensional node vectors enables various graph\nanalysis tasks, such as structural equivalence and link prediction. Yet,\nimproper publication opens a backdoor to malicious attackers, who can infer\nsensitive information of individuals from the low-dimensional node vectors.\nExisting methods tackle this issue by developing deep graph learning models\nwith differential privacy (DP). However, they often suffer from large noise\ninjections and cannot provide structural preferences consistent with mining\nobjectives. Recently, skip-gram based graph embedding generation techniques are\nwidely used due to their ability to extract customizable structures. Based on\nskip-gram, we present SE-PrivGEmb, a structure-preference enabled graph\nembedding generation under DP. For arbitrary structure preferences, we design a\nunified noise tolerance mechanism via perturbing non-zero vectors. This\nmechanism mitigates utility degradation caused by high sensitivity. By\ncarefully designing negative sampling probabilities in skip-gram, we\ntheoretically demonstrate that skip-gram can preserve arbitrary proximities,\nwhich quantify structural features in graphs. Extensive experiments show that\nour method outperforms existing state-of-the-art methods under structural\nequivalence and link prediction tasks.",
    "updated" : "2025-01-07T00:43:18Z",
    "published" : "2025-01-07T00:43:18Z",
    "authors" : [
      {
        "name" : "Sen Zhang"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Haibo Hu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03391v1",
    "title" : "Privacy-Preserving Smart Contracts for Permissioned Blockchains: A\n  zk-SNARK-Based Recipe Part-1",
    "summary" : "The Bitcoin white paper introduced blockchain technology, enabling trustful\ntransactions without intermediaries. Smart contracts emerged with Ethereum and\nblockchains expanded beyond cryptocurrency, applying to auctions, crowdfunding\nand electronic voting. However, blockchain's transparency raised privacy\nconcerns and initial anonymity measures proved ineffective. Smart contract\nprivacy solutions employed zero-knowledge proofs, homomorphic encryption and\ntrusted execution environments. These approaches have practical drawbacks, such\nas limited functionality, high computation times and trust on third parties\nrequirements, being not fully decentralized. This work proposes a solution\nutilizing zk-SNARKs to provide privacy in smart contracts and blockchains. The\nsolution supports both fungible and nonfungible tokens. Additionally, the\nproposal includes a new type of transactions, called delegated transactions,\nwhich enable use cases like Delivery vs Payment (DvP).",
    "updated" : "2025-01-06T21:16:33Z",
    "published" : "2025-01-06T21:16:33Z",
    "authors" : [
      {
        "name" : "Aldenio Burgos"
      },
      {
        "name" : "Eduardo Alchieri"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02893v2",
    "title" : "A Volumetric Approach to Privacy of Dynamical Systems",
    "summary" : "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack.",
    "updated" : "2025-01-07T13:21:10Z",
    "published" : "2025-01-06T10:15:21Z",
    "authors" : [
      {
        "name" : "Chuanghong Weng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02804v2",
    "title" : "Latency and Privacy-Aware Resource Allocation in Vehicular Edge\n  Computing",
    "summary" : "The rapid increase in the number of connected vehicles has led to the\ngeneration of vast amounts of data. As a significant portion of this data\npertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it\nis predominantly generated at the edge. Considering the enormous volume of\ndata, real-time applications, and privacy concerns, it is crucial to process\nthe data at the edge. Neglecting the management of processing resources in\nvehicular edge computing (VEC) could lead to numerous challenges as a\nsubstantial number of vehicles with diverse safety, economic, and entertainment\napplications, along with their data processing, emerge in the near future [1].\nPrevious research in VEC resource allocation has primarily focused on issues\nsuch as response time and privacy preservation techniques. However, an approach\nthat takes into account privacy-aware resource allocation based on vehicular\nnetwork architecture and application requirements has not yet been proposed. In\nthis paper, we present a privacy and latency-aware approach for allocating\nprocessing resources at the edge of the vehicular network, considering the\nspecific requirements of different applications. Our approach involves\ncategorizing vehicular network applications based on their processing accuracy,\nreal-time processing needs, and privacy preservation requirements. We further\ndivide the vehicular network edge into two parts: the user layer (OBUs) is\nconsidered for processing applications with privacy requirements, while the\nallocation of resources in the RSUs and cloud layer is based on the specific\nneeds of different applications. In this study, we evaluate the quality of\nservice based on parameters such as privacy preservation, processing cost,\nmeeting deadlines, and result quality. Comparative analyses demonstrate that\nour approach enhances service quality by 55% compared to existing\nstate-of-the-art methods.",
    "updated" : "2025-01-07T07:17:16Z",
    "published" : "2025-01-06T06:44:49Z",
    "authors" : [
      {
        "name" : "Hossein Ahmadvand"
      },
      {
        "name" : "Fouzhan Foroutan"
      }
    ],
    "categories" : [
      "cs.PF"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04420v1",
    "title" : "A Closer Look on Gender Stereotypes in Movie Recommender Systems and\n  Their Implications with Privacy",
    "summary" : "The movie recommender system typically leverages user feedback to provide\npersonalized recommendations that align with user preferences and increase\nbusiness revenue. This study investigates the impact of gender stereotypes on\nsuch systems through a specific attack scenario. In this scenario, an attacker\ndetermines users' gender, a private attribute, by exploiting gender stereotypes\nabout movie preferences and analyzing users' feedback data, which is either\npublicly available or observed within the system. The study consists of two\nphases. In the first phase, a user study involving 630 participants identified\ngender stereotypes associated with movie genres, which often influence viewing\nchoices. In the second phase, four inference algorithms were applied to detect\ngender stereotypes by combining the findings from the first phase with users'\nfeedback data. Results showed that these algorithms performed more effectively\nthan relying solely on feedback data for gender inference. Additionally, we\nquantified the extent of gender stereotypes to evaluate their broader impact on\ndigital computational science. The latter part of the study utilized two major\nmovie recommender datasets: MovieLens 1M and Yahoo!Movie. Detailed experimental\ninformation is available on our GitHub repository:\nhttps://github.com/fr-iit/GSMRS",
    "updated" : "2025-01-08T11:08:58Z",
    "published" : "2025-01-08T11:08:58Z",
    "authors" : [
      {
        "name" : "Falguni Roy"
      },
      {
        "name" : "Yiduo Shen"
      },
      {
        "name" : "Na Zhao"
      },
      {
        "name" : "Xiaofeng Ding"
      },
      {
        "name" : "Md. Omar Faruk"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04409v1",
    "title" : "Lossless Privacy-Preserving Aggregation for Decentralized Federated\n  Learning",
    "summary" : "Privacy concerns arise as sensitive data proliferate. Despite decentralized\nfederated learning (DFL) aggregating gradients from neighbors to avoid direct\ndata transmission, it still poses indirect data leaks from the transmitted\ngradients. Existing privacy-preserving methods for DFL add noise to gradients.\nThey either diminish the model predictive accuracy or suffer from ineffective\ngradient protection. In this paper, we propose a novel lossless\nprivacy-preserving aggregation rule named LPPA to enhance gradient protection\nas much as possible but without loss of DFL model predictive accuracy. LPPA\nsubtly injects the noise difference between the sent and received noise into\ntransmitted gradients for gradient protection. The noise difference\nincorporates neighbors' randomness for each client, effectively safeguarding\nagainst data leaks. LPPA employs the noise flow conservation theory to ensure\nthat the noise impact can be globally eliminated. The global sum of all noise\ndifferences remains zero, ensuring that accurate gradient aggregation is\nunaffected and the model accuracy remains intact. We theoretically prove that\nthe privacy-preserving capacity of LPPA is \\sqrt{2} times greater than that of\nnoise addition, while maintaining comparable model accuracy to the standard DFL\naggregation without noise injection. Experimental results verify the\ntheoretical findings and show that LPPA achieves a 13% mean improvement in\naccuracy over noise addition. We also demonstrate the effectiveness of LPPA in\nprotecting raw data and guaranteeing lossless model accuracy.",
    "updated" : "2025-01-08T10:49:06Z",
    "published" : "2025-01-08T10:49:06Z",
    "authors" : [
      {
        "name" : "Xiaoye Miao"
      },
      {
        "name" : "Bin Li"
      },
      {
        "name" : "Yangyang Wu"
      },
      {
        "name" : "Meng Xi"
      },
      {
        "name" : "Xinkui Zhao"
      },
      {
        "name" : "Jianwei Yin"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04323v1",
    "title" : "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models",
    "summary" : "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance.",
    "updated" : "2025-01-08T07:47:43Z",
    "published" : "2025-01-08T07:47:43Z",
    "authors" : [
      {
        "name" : "Shi Haonan"
      },
      {
        "name" : "Ouyang Tu"
      },
      {
        "name" : "Wang An"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04222v1",
    "title" : "Privacy-Preserving Distributed Online Mirror Descent for Nonconvex\n  Optimization",
    "summary" : "We investigate the distributed online nonconvex optimization problem with\ndifferential privacy over time-varying networks. Each node minimizes the sum of\nseveral nonconvex functions while preserving the node's differential privacy.\nWe propose a privacy-preserving distributed online mirror descent algorithm for\nnonconvex optimization, which uses the mirror descent to update decision\nvariables and the Laplace differential privacy mechanism to protect privacy.\nUnlike the existing works, the proposed algorithm allows the cost functions to\nbe nonconvex, which is more applicable. Based upon these, we prove that if the\ncommunication network is $B$-strongly connected and the constraint set is\ncompact, then by choosing the step size properly, the algorithm guarantees\n$\\epsilon$-differential privacy at each time. Furthermore, we prove that if the\nlocal cost functions are $\\beta$-smooth, then the regret over time horizon $T$\ngrows sublinearly while preserving differential privacy, with an upper bound\n$O(\\sqrt{T})$. Finally, the effectiveness of the algorithm is demonstrated\nthrough numerical simulations.",
    "updated" : "2025-01-08T01:39:10Z",
    "published" : "2025-01-08T01:39:10Z",
    "authors" : [
      {
        "name" : "Yingjie Zhou"
      },
      {
        "name" : "Tao Li"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04134v1",
    "title" : "Mixing Times and Privacy Analysis for the Projected Langevin Algorithm\n  under a Modulus of Continuity",
    "summary" : "We study the mixing time of the projected Langevin algorithm (LA) and the\nprivacy curve of noisy Stochastic Gradient Descent (SGD), beyond nonexpansive\niterations. Specifically, we derive new mixing time bounds for the projected LA\nwhich are, in some important cases, dimension-free and poly-logarithmic on the\naccuracy, closely matching the existing results in the smooth convex case.\nAdditionally, we establish new upper bounds for the privacy curve of the\nsubsampled noisy SGD algorithm. These bounds show a crucial dependency on the\nregularity of gradients, and are useful for a wide range of convex losses\nbeyond the smooth case. Our analysis relies on a suitable extension of the\nPrivacy Amplification by Iteration (PABI) framework (Feldman et al., 2018;\nAltschuler and Talwar, 2022, 2023) to noisy iterations whose gradient map is\nnot necessarily nonexpansive. This extension is achieved by designing an\noptimization problem which accounts for the best possible R\\'enyi divergence\nbound obtained by an application of PABI, where the tractability of the problem\nis crucially related to the modulus of continuity of the associated gradient\nmapping. We show that, in several interesting cases -- including the nonsmooth\nconvex, weakly smooth and (strongly) dissipative -- such optimization problem\ncan be solved exactly and explicitly. This yields the tightest possible\nPABI-based bounds, where our results are either new or substantially sharper\nthan those in previous works.",
    "updated" : "2025-01-07T20:46:59Z",
    "published" : "2025-01-07T20:46:59Z",
    "authors" : [
      {
        "name" : "Mario Bravo"
      },
      {
        "name" : "Juan P. Flores-Mella"
      },
      {
        "name" : "Cristóbal Guzmán"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "math.OC",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04058v1",
    "title" : "Homomorphic Encryption in Healthcare Industry Applications for\n  Protecting Data Privacy",
    "summary" : "Focussing on two different use cases-Quality Control methods in industrial\ncontexts and Neural Network algorithms for healthcare diagnostics-this research\ninvestigates the inclusion of Fully Homomorphic Encryption into real-world\napplications in the healthcare sector. We evaluate the performance, resource\nrequirements, and viability of deploying FHE in these settings through\nextensive testing and analysis, highlighting the progress made in FHE tooling\nand the obstacles still facing addressing the gap between conceptual research\nand practical applications. We start our research by describing the specific\ncase study and trust model were working with. Choosing the two FHE frameworks\nmost appropriate for industry development, we assess the resources and\nperformance requirements for implementing each of the two FHE frameworks in the\nfirst scenario, Quality Control algorithms. In conclusion, our findings\ndemonstrate the effectiveness and resource consumption of the two use\ncases-complex NN models and simple QC algorithms-when implemented in an FHE\nsetting.",
    "updated" : "2025-01-07T07:42:41Z",
    "published" : "2025-01-07T07:42:41Z",
    "authors" : [
      {
        "name" : "J. S. Rauthan"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR"
    ]
  }
]