[
  {
    "id" : "http://arxiv.org/abs/2501.01353v1",
    "title" : "Privacy Preservation in MIMO-OFDM Localization Systems: A Beamforming\n  Approach",
    "summary" : "We investigate an uplink MIMO-OFDM localization scenario where a legitimate\nbase station (BS) aims to localize a user equipment (UE) using pilot signals\ntransmitted by the UE, while an unauthorized BS attempts to localize the UE by\neavesdropping on these pilots, posing a risk to the UE's location privacy. To\nenhance legitimate localization performance while protecting the UE's privacy,\nwe formulate an optimization problem regarding the beamformers at the UE,\naiming to minimize the Cram\\'er-Rao bound (CRB) for legitimate localization\nwhile constraining the CRB for unauthorized localization above a threshold. A\npenalty dual decomposition optimization framework is employed to solve the\nproblem, leading to a novel beamforming approach for location privacy\npreservation. Numerical results confirm the effectiveness of the proposed\napproach and demonstrate its superiority over existing benchmarks.",
    "updated" : "2025-01-02T17:08:15Z",
    "published" : "2025-01-02T17:08:15Z",
    "authors" : [
      {
        "name" : "Yuchen Zhang"
      },
      {
        "name" : "Hui Chen"
      },
      {
        "name" : "Musa Furkan Keskin"
      },
      {
        "name" : "Alireza Pourafzal"
      },
      {
        "name" : "Pinjun Zheng"
      },
      {
        "name" : "Henk Wymeersch"
      },
      {
        "name" : "Tareq Y. Al-Naffouri"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01131v1",
    "title" : "Privacy Bills of Materials: A Transparent Privacy Information Inventory\n  for Collaborative Privacy Notice Generation in Mobile App Development",
    "summary" : "Privacy regulations mandate that developers must provide authentic and\ncomprehensive privacy notices, e.g., privacy policies or labels, to inform\nusers of their apps' privacy practices. However, due to a lack of knowledge of\nprivacy requirements, developers often struggle to create accurate privacy\nnotices, especially for sophisticated mobile apps with complex features and in\ncrowded development teams. To address these challenges, we introduce Privacy\nBills of Materials (PriBOM), a systematic software engineering approach that\nleverages different development team roles to better capture and coordinate\nmobile app privacy information. PriBOM facilitates transparency-centric privacy\ndocumentation and specific privacy notice creation, enabling traceability and\ntrackability of privacy practices. We present a pre-fill of PriBOM based on\nstatic analysis and privacy notice analysis techniques. We demonstrate the\nperceived usefulness of PriBOM through a human evaluation with 150 diverse\nparticipants. Our findings suggest that PriBOM could serve as a significant\nsolution for providing privacy support in DevOps for mobile apps.",
    "updated" : "2025-01-02T08:14:52Z",
    "published" : "2025-01-02T08:14:52Z",
    "authors" : [
      {
        "name" : "Zhen Tao"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Zhenchang Xing"
      },
      {
        "name" : "Xiaoyu Sun"
      },
      {
        "name" : "Omar Haggag"
      },
      {
        "name" : "John Grundy"
      },
      {
        "name" : "Ze Shi Li"
      },
      {
        "name" : "Jingjie Li"
      },
      {
        "name" : "Liming Zhu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01063v1",
    "title" : "FAPL-DM-BC: A Secure and Scalable FL Framework with Adaptive Privacy and\n  Dynamic Masking, Blockchain, and XAI for the IoVs",
    "summary" : "The FAPL-DM-BC solution is a new FL-based privacy, security, and scalability\nsolution for the Internet of Vehicles (IoV). It leverages Federated Adaptive\nPrivacy-Aware Learning (FAPL) and Dynamic Masking (DM) to learn and adaptively\nchange privacy policies in response to changing data sensitivity and state in\nreal-time, for the optimal privacy-utility tradeoff. Secure Logging and\nVerification, Blockchain-based provenance and decentralized validation, and\nCloud Microservices Secure Aggregation using FedAvg (Federated Averaging) and\nSecure Multi-Party Computation (SMPC). Two-model feedback, driven by\nModel-Agnostic Explainable AI (XAI), certifies local predictions and\nexplanations to drive it to the next level of efficiency. Combining local\nfeedback with world knowledge through a weighted mean computation, FAPL-DM-BC\nassures federated learning that is secure, scalable, and interpretable.\nSelf-driving cars, traffic management, and forecasting, vehicular network\ncybersecurity in real-time, and smart cities are a few possible applications of\nthis integrated, privacy-safe, and high-performance IoV platform.",
    "updated" : "2025-01-02T05:21:52Z",
    "published" : "2025-01-02T05:21:52Z",
    "authors" : [
      {
        "name" : "Sathwik Narkedimilli"
      },
      {
        "name" : "Amballa Venkata Sriram"
      },
      {
        "name" : "Sujith Makam"
      },
      {
        "name" : "MSVPJ Sathvik"
      },
      {
        "name" : "Sai Prashanth Mallellu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.00824v1",
    "title" : "Information Sifting Funnel: Privacy-preserving Collaborative Inference\n  Against Model Inversion Attacks",
    "summary" : "The complexity of neural networks and inference tasks, coupled with demands\nfor computational efficiency and real-time feedback, poses significant\nchallenges for resource-constrained edge devices. Collaborative inference\nmitigates this by assigning shallow feature extraction to edge devices and\noffloading features to the cloud for further inference, reducing computational\nload. However, transmitted features remain susceptible to model inversion\nattacks (MIAs), which can reconstruct original input data. Current defenses,\nsuch as perturbation and information bottleneck techniques, offer explainable\nprotection but face limitations, including the lack of standardized criteria\nfor assessing MIA difficulty, challenges in mutual information estimation, and\ntrade-offs among usability, privacy, and deployability.\n  To address these challenges, we introduce the first criterion to evaluate MIA\ndifficulty in collaborative inference, supported by theoretical analysis of\nexisting attacks and defenses, validated using experiments with the Mutual\nInformation Neural Estimator (MINE). Based on these findings, we propose\nSiftFunnel, a privacy-preserving framework for collaborative inference. The\nedge model is trained with linear and non-linear correlation constraints to\nreduce redundant information in transmitted features, enhancing privacy\nprotection. Label smoothing and a cloud-based upsampling module are added to\nbalance usability and privacy. To improve deployability, the edge model\nincorporates a funnel-shaped structure and attention mechanisms, preserving\nboth privacy and usability. Extensive experiments demonstrate that SiftFunnel\noutperforms state-of-the-art defenses against MIAs, achieving superior privacy\nprotection with less than 3% accuracy loss and striking an optimal balance\namong usability, privacy, and practicality.",
    "updated" : "2025-01-01T13:00:01Z",
    "published" : "2025-01-01T13:00:01Z",
    "authors" : [
      {
        "name" : "Rongke Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01786v1",
    "title" : "Advancing privacy in learning analytics using differential privacy",
    "summary" : "This paper addresses the challenge of balancing learner data privacy with the\nuse of data in learning analytics (LA) by proposing a novel framework by\napplying Differential Privacy (DP). The need for more robust privacy protection\nkeeps increasing, driven by evolving legal regulations and heightened privacy\nconcerns, as well as traditional anonymization methods being insufficient for\nthe complexities of educational data. To address this, we introduce the first\nDP framework specifically designed for LA and provide practical guidance for\nits implementation. We demonstrate the use of this framework through a LA usage\nscenario and validate DP in safeguarding data privacy against potential attacks\nthrough an experiment on a well-known LA dataset. Additionally, we explore the\ntrade-offs between data privacy and utility across various DP settings. Our\nwork contributes to the field of LA by offering a practical DP framework that\ncan support researchers and practitioners in adopting DP in their works.",
    "updated" : "2025-01-03T12:36:11Z",
    "published" : "2025-01-03T12:36:11Z",
    "authors" : [
      {
        "name" : "Qinyi Liu"
      },
      {
        "name" : "Ronas Shakya"
      },
      {
        "name" : "Mohammad Khalil"
      },
      {
        "name" : "Jelena Jovanovic"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01639v1",
    "title" : "Artificial Intelligent Implications on Health Data Privacy and\n  Confidentiality",
    "summary" : "The rapid integration of artificial intelligence (AI) in healthcare is\nrevolutionizing medical diagnostics, personalized medicine, and operational\nefficiency. However, alongside these advancements, significant challenges arise\nconcerning patient data privacy, ethical considerations, and regulatory\ncompliance. This paper examines the dual impact of AI on healthcare,\nhighlighting its transformative potential and the critical need for\nsafeguarding sensitive health information. It explores the role of the Health\nInsurance Portability and Accountability Act (HIPAA) as a regulatory framework\nfor ensuring data privacy and security, emphasizing the importance of robust\nsafeguards and ethical standards in AI-driven healthcare. Through case studies,\nincluding AI applications in diabetic retinopathy, oncology, and the\ncontroversies surrounding data sharing, this study underscores the ethical and\nlegal complexities of AI implementation. A balanced approach that fosters\ninnovation while maintaining patient trust and privacy is imperative. The\nfindings emphasize the importance of continuous education, transparency, and\nadherence to regulatory frameworks to harness AI's full potential responsibly\nand ethically in healthcare.",
    "updated" : "2025-01-03T05:17:23Z",
    "published" : "2025-01-03T05:17:23Z",
    "authors" : [
      {
        "name" : "Ahmad Momani"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  }
]