[
  {
    "id" : "http://arxiv.org/abs/2501.01353v1",
    "title" : "Privacy Preservation in MIMO-OFDM Localization Systems: A Beamforming\n  Approach",
    "summary" : "We investigate an uplink MIMO-OFDM localization scenario where a legitimate\nbase station (BS) aims to localize a user equipment (UE) using pilot signals\ntransmitted by the UE, while an unauthorized BS attempts to localize the UE by\neavesdropping on these pilots, posing a risk to the UE's location privacy. To\nenhance legitimate localization performance while protecting the UE's privacy,\nwe formulate an optimization problem regarding the beamformers at the UE,\naiming to minimize the Cram\\'er-Rao bound (CRB) for legitimate localization\nwhile constraining the CRB for unauthorized localization above a threshold. A\npenalty dual decomposition optimization framework is employed to solve the\nproblem, leading to a novel beamforming approach for location privacy\npreservation. Numerical results confirm the effectiveness of the proposed\napproach and demonstrate its superiority over existing benchmarks.",
    "updated" : "2025-01-02T17:08:15Z",
    "published" : "2025-01-02T17:08:15Z",
    "authors" : [
      {
        "name" : "Yuchen Zhang"
      },
      {
        "name" : "Hui Chen"
      },
      {
        "name" : "Musa Furkan Keskin"
      },
      {
        "name" : "Alireza Pourafzal"
      },
      {
        "name" : "Pinjun Zheng"
      },
      {
        "name" : "Henk Wymeersch"
      },
      {
        "name" : "Tareq Y. Al-Naffouri"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01131v1",
    "title" : "Privacy Bills of Materials: A Transparent Privacy Information Inventory\n  for Collaborative Privacy Notice Generation in Mobile App Development",
    "summary" : "Privacy regulations mandate that developers must provide authentic and\ncomprehensive privacy notices, e.g., privacy policies or labels, to inform\nusers of their apps' privacy practices. However, due to a lack of knowledge of\nprivacy requirements, developers often struggle to create accurate privacy\nnotices, especially for sophisticated mobile apps with complex features and in\ncrowded development teams. To address these challenges, we introduce Privacy\nBills of Materials (PriBOM), a systematic software engineering approach that\nleverages different development team roles to better capture and coordinate\nmobile app privacy information. PriBOM facilitates transparency-centric privacy\ndocumentation and specific privacy notice creation, enabling traceability and\ntrackability of privacy practices. We present a pre-fill of PriBOM based on\nstatic analysis and privacy notice analysis techniques. We demonstrate the\nperceived usefulness of PriBOM through a human evaluation with 150 diverse\nparticipants. Our findings suggest that PriBOM could serve as a significant\nsolution for providing privacy support in DevOps for mobile apps.",
    "updated" : "2025-01-02T08:14:52Z",
    "published" : "2025-01-02T08:14:52Z",
    "authors" : [
      {
        "name" : "Zhen Tao"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Zhenchang Xing"
      },
      {
        "name" : "Xiaoyu Sun"
      },
      {
        "name" : "Omar Haggag"
      },
      {
        "name" : "John Grundy"
      },
      {
        "name" : "Ze Shi Li"
      },
      {
        "name" : "Jingjie Li"
      },
      {
        "name" : "Liming Zhu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01063v1",
    "title" : "FAPL-DM-BC: A Secure and Scalable FL Framework with Adaptive Privacy and\n  Dynamic Masking, Blockchain, and XAI for the IoVs",
    "summary" : "The FAPL-DM-BC solution is a new FL-based privacy, security, and scalability\nsolution for the Internet of Vehicles (IoV). It leverages Federated Adaptive\nPrivacy-Aware Learning (FAPL) and Dynamic Masking (DM) to learn and adaptively\nchange privacy policies in response to changing data sensitivity and state in\nreal-time, for the optimal privacy-utility tradeoff. Secure Logging and\nVerification, Blockchain-based provenance and decentralized validation, and\nCloud Microservices Secure Aggregation using FedAvg (Federated Averaging) and\nSecure Multi-Party Computation (SMPC). Two-model feedback, driven by\nModel-Agnostic Explainable AI (XAI), certifies local predictions and\nexplanations to drive it to the next level of efficiency. Combining local\nfeedback with world knowledge through a weighted mean computation, FAPL-DM-BC\nassures federated learning that is secure, scalable, and interpretable.\nSelf-driving cars, traffic management, and forecasting, vehicular network\ncybersecurity in real-time, and smart cities are a few possible applications of\nthis integrated, privacy-safe, and high-performance IoV platform.",
    "updated" : "2025-01-02T05:21:52Z",
    "published" : "2025-01-02T05:21:52Z",
    "authors" : [
      {
        "name" : "Sathwik Narkedimilli"
      },
      {
        "name" : "Amballa Venkata Sriram"
      },
      {
        "name" : "Sujith Makam"
      },
      {
        "name" : "MSVPJ Sathvik"
      },
      {
        "name" : "Sai Prashanth Mallellu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.00824v1",
    "title" : "Information Sifting Funnel: Privacy-preserving Collaborative Inference\n  Against Model Inversion Attacks",
    "summary" : "The complexity of neural networks and inference tasks, coupled with demands\nfor computational efficiency and real-time feedback, poses significant\nchallenges for resource-constrained edge devices. Collaborative inference\nmitigates this by assigning shallow feature extraction to edge devices and\noffloading features to the cloud for further inference, reducing computational\nload. However, transmitted features remain susceptible to model inversion\nattacks (MIAs), which can reconstruct original input data. Current defenses,\nsuch as perturbation and information bottleneck techniques, offer explainable\nprotection but face limitations, including the lack of standardized criteria\nfor assessing MIA difficulty, challenges in mutual information estimation, and\ntrade-offs among usability, privacy, and deployability.\n  To address these challenges, we introduce the first criterion to evaluate MIA\ndifficulty in collaborative inference, supported by theoretical analysis of\nexisting attacks and defenses, validated using experiments with the Mutual\nInformation Neural Estimator (MINE). Based on these findings, we propose\nSiftFunnel, a privacy-preserving framework for collaborative inference. The\nedge model is trained with linear and non-linear correlation constraints to\nreduce redundant information in transmitted features, enhancing privacy\nprotection. Label smoothing and a cloud-based upsampling module are added to\nbalance usability and privacy. To improve deployability, the edge model\nincorporates a funnel-shaped structure and attention mechanisms, preserving\nboth privacy and usability. Extensive experiments demonstrate that SiftFunnel\noutperforms state-of-the-art defenses against MIAs, achieving superior privacy\nprotection with less than 3% accuracy loss and striking an optimal balance\namong usability, privacy, and practicality.",
    "updated" : "2025-01-01T13:00:01Z",
    "published" : "2025-01-01T13:00:01Z",
    "authors" : [
      {
        "name" : "Rongke Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01786v1",
    "title" : "Advancing privacy in learning analytics using differential privacy",
    "summary" : "This paper addresses the challenge of balancing learner data privacy with the\nuse of data in learning analytics (LA) by proposing a novel framework by\napplying Differential Privacy (DP). The need for more robust privacy protection\nkeeps increasing, driven by evolving legal regulations and heightened privacy\nconcerns, as well as traditional anonymization methods being insufficient for\nthe complexities of educational data. To address this, we introduce the first\nDP framework specifically designed for LA and provide practical guidance for\nits implementation. We demonstrate the use of this framework through a LA usage\nscenario and validate DP in safeguarding data privacy against potential attacks\nthrough an experiment on a well-known LA dataset. Additionally, we explore the\ntrade-offs between data privacy and utility across various DP settings. Our\nwork contributes to the field of LA by offering a practical DP framework that\ncan support researchers and practitioners in adopting DP in their works.",
    "updated" : "2025-01-03T12:36:11Z",
    "published" : "2025-01-03T12:36:11Z",
    "authors" : [
      {
        "name" : "Qinyi Liu"
      },
      {
        "name" : "Ronas Shakya"
      },
      {
        "name" : "Mohammad Khalil"
      },
      {
        "name" : "Jelena Jovanovic"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01639v1",
    "title" : "Artificial Intelligent Implications on Health Data Privacy and\n  Confidentiality",
    "summary" : "The rapid integration of artificial intelligence (AI) in healthcare is\nrevolutionizing medical diagnostics, personalized medicine, and operational\nefficiency. However, alongside these advancements, significant challenges arise\nconcerning patient data privacy, ethical considerations, and regulatory\ncompliance. This paper examines the dual impact of AI on healthcare,\nhighlighting its transformative potential and the critical need for\nsafeguarding sensitive health information. It explores the role of the Health\nInsurance Portability and Accountability Act (HIPAA) as a regulatory framework\nfor ensuring data privacy and security, emphasizing the importance of robust\nsafeguards and ethical standards in AI-driven healthcare. Through case studies,\nincluding AI applications in diabetic retinopathy, oncology, and the\ncontroversies surrounding data sharing, this study underscores the ethical and\nlegal complexities of AI implementation. A balanced approach that fosters\ninnovation while maintaining patient trust and privacy is imperative. The\nfindings emphasize the importance of continuous education, transparency, and\nadherence to regulatory frameworks to harness AI's full potential responsibly\nand ethically in healthcare.",
    "updated" : "2025-01-03T05:17:23Z",
    "published" : "2025-01-03T05:17:23Z",
    "authors" : [
      {
        "name" : "Ahmad Momani"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03222v1",
    "title" : "Characterizing the Accuracy-Communication-Privacy Trade-off in\n  Distributed Stochastic Convex Optimization",
    "summary" : "We consider the problem of differentially private stochastic convex\noptimization (DP-SCO) in a distributed setting with $M$ clients, where each of\nthem has a local dataset of $N$ i.i.d. data samples from an underlying data\ndistribution. The objective is to design an algorithm to minimize a convex\npopulation loss using a collaborative effort across $M$ clients, while ensuring\nthe privacy of the local datasets. In this work, we investigate the\naccuracy-communication-privacy trade-off for this problem. We establish\nmatching converse and achievability results using a novel lower bound and a new\nalgorithm for distributed DP-SCO based on Vaidya's plane cutting method. Thus,\nour results provide a complete characterization of the\naccuracy-communication-privacy trade-off for DP-SCO in the distributed setting.",
    "updated" : "2025-01-06T18:57:05Z",
    "published" : "2025-01-06T18:57:05Z",
    "authors" : [
      {
        "name" : "Sudeep Salgia"
      },
      {
        "name" : "Nikola Pavlovic"
      },
      {
        "name" : "Yuejie Chi"
      },
      {
        "name" : "Qing Zhao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02893v1",
    "title" : "A Volumetric Approach to Privacy of Dynamical Systems",
    "summary" : "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack.",
    "updated" : "2025-01-06T10:15:21Z",
    "published" : "2025-01-06T10:15:21Z",
    "authors" : [
      {
        "name" : "Chuanghong Weng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02804v1",
    "title" : "Latency and Privacy-Aware Resource Allocation in Vehicular Edge\n  Computing",
    "summary" : "The rapid increase in the number of connected vehicles has led to the\ngeneration of vast amounts of data. As a significant portion of this data\npertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it\nis predominantly generated at the edge. Considering the enormous volume of\ndata, real-time applications, and privacy concerns, it is crucial to process\nthe data at the edge. Neglecting the management of processing resources in\nvehicular edge computing (VEC) could lead to numerous challenges as a\nsubstantial number of vehicles with diverse safety, economic, and entertainment\napplications, along with their data processing, emerge in the near future [1].\nPrevious research in VEC resource allocation has primarily focused on issues\nsuch as response time and privacy preservation techniques. However, an approach\nthat takes into account privacy-aware resource allocation based on vehicular\nnetwork architecture and application requirements has not yet been proposed. In\nthis paper, we present a privacy and latency-aware approach for allocating\nprocessing resources at the edge of the vehicular network, considering the\nspecific requirements of different applications. Our approach involves\ncategorizing vehicular network applications based on their processing accuracy,\nreal-time processing needs, and privacy preservation requirements. We further\ndivide the vehicular network edge into two parts: the user layer (OBUs) is\nconsidered for processing applications with privacy requirements, while the\nallocation of resources in the RSUs and cloud layer is based on the specific\nneeds of different applications. In this study, we evaluate the quality of\nservice based on parameters such as privacy preservation, processing cost,\nmeeting deadlines, and result quality. Comparative analyses demonstrate that\nour approach enhances service quality by 55% compared to existing\nstate-of-the-art methods.",
    "updated" : "2025-01-06T06:44:49Z",
    "published" : "2025-01-06T06:44:49Z",
    "authors" : [
      {
        "name" : "HHossein Ahmadvand"
      },
      {
        "name" : "Fouzhan Foroutan"
      }
    ],
    "categories" : [
      "cs.PF"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02354v1",
    "title" : "PrivDPR: Synthetic Graph Publishing with Deep PageRank under\n  Differential Privacy",
    "summary" : "The objective of privacy-preserving synthetic graph publishing is to\nsafeguard individuals' privacy while retaining the utility of original data.\nMost existing methods focus on graph neural networks under differential privacy\n(DP), and yet two fundamental problems in generating synthetic graphs remain\nopen. First, the current research often encounters high sensitivity due to the\nintricate relationships between nodes in a graph. Second, DP is usually\nachieved through advanced composition mechanisms that tend to converge\nprematurely when working with a small privacy budget. In this paper, inspired\nby the simplicity, effectiveness, and ease of analysis of PageRank, we design\nPrivDPR, a novel privacy-preserving deep PageRank for graph synthesis. In\nparticular, we achieve DP by adding noise to the gradient for a specific weight\nduring learning. Utilizing weight normalization as a bridge, we theoretically\nreveal that increasing the number of layers in PrivDPR can effectively mitigate\nthe high sensitivity and privacy budget splitting. Through formal privacy\nanalysis, we prove that the synthetic graph generated by PrivDPR satisfies\nnode-level DP. Experiments on real-world graph datasets show that PrivDPR\npreserves high data utility across multiple graph structural properties.",
    "updated" : "2025-01-04T18:19:21Z",
    "published" : "2025-01-04T18:19:21Z",
    "authors" : [
      {
        "name" : "Sen Zhang"
      },
      {
        "name" : "Haibo Hu"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Jianliang Xu"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02091v1",
    "title" : "PriveShield: Enhancing User Privacy Using Automatic Isolated Profiles in\n  Browsers",
    "summary" : "Online tracking is a widespread practice on the web with questionable ethics,\nsecurity, and privacy concerns. While web tracking can offer personalized and\ncurated content to Internet users, it operates as a sophisticated surveillance\nmechanism to gather extensive user information. This paper introduces\nPriveShield, a light-weight privacy mechanism that disrupts the information\ngathering cycle while offering more control to Internet users to maintain their\nprivacy. PriveShield is implemented as a browser extension that offers an\nadjustable privacy feature to surf the web with multiple identities or accounts\nsimultaneously without any changes to underlying browser code or services. When\nnecessary, multiple factors are automatically analyzed on the client side to\nisolate cookies and other information that are the basis of online tracking.\nPriveShield creates isolated profiles for clients based on their browsing\nhistory, interactions with websites, and the amount of time they spend on\nspecific websites. This allows the users to easily prevent unwanted browsing\ninformation from being shared with third parties and ad exchanges without the\nneed for manual configuration. Our evaluation results from 54 real-world\nscenarios show that our extension is effective in preventing retargeted ads in\n91% of those scenarios.",
    "updated" : "2025-01-03T20:29:33Z",
    "published" : "2025-01-03T20:29:33Z",
    "authors" : [
      {
        "name" : "Seyed Ali Akhavani"
      },
      {
        "name" : "Engin Kirda"
      },
      {
        "name" : "Amin Kharraz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01639v2",
    "title" : "Implications of Artificial Intelligence on Health Data Privacy and\n  Confidentiality",
    "summary" : "The rapid integration of artificial intelligence (AI) in healthcare is\nrevolutionizing medical diagnostics, personalized medicine, and operational\nefficiency. However, alongside these advancements, significant challenges arise\nconcerning patient data privacy, ethical considerations, and regulatory\ncompliance. This paper examines the dual impact of AI on healthcare,\nhighlighting its transformative potential and the critical need for\nsafeguarding sensitive health information. It explores the role of the Health\nInsurance Portability and Accountability Act (HIPAA) as a regulatory framework\nfor ensuring data privacy and security, emphasizing the importance of robust\nsafeguards and ethical standards in AI-driven healthcare. Through case studies,\nincluding AI applications in diabetic retinopathy, oncology, and the\ncontroversies surrounding data sharing, this study underscores the ethical and\nlegal complexities of AI implementation. A balanced approach that fosters\ninnovation while maintaining patient trust and privacy is imperative. The\nfindings emphasize the importance of continuous education, transparency, and\nadherence to regulatory frameworks to harness AI's full potential responsibly\nand ethically in healthcare.",
    "updated" : "2025-01-06T18:52:32Z",
    "published" : "2025-01-03T05:17:23Z",
    "authors" : [
      {
        "name" : "Ahmad Momani"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03941v1",
    "title" : "Synthetic Data Privacy Metrics",
    "summary" : "Recent advancements in generative AI have made it possible to create\nsynthetic datasets that can be as accurate as real-world data for training AI\nmodels, powering statistical insights, and fostering collaboration with\nsensitive datasets while offering strong privacy guarantees. Effectively\nmeasuring the empirical privacy of synthetic data is an important step in the\nprocess. However, while there is a multitude of new privacy metrics being\npublished every day, there currently is no standardization. In this paper, we\nreview the pros and cons of popular metrics that include simulations of\nadversarial attacks. We also review current best practices for amending\ngenerative models to enhance the privacy of the data they create (e.g.\ndifferential privacy).",
    "updated" : "2025-01-07T17:02:33Z",
    "published" : "2025-01-07T17:02:33Z",
    "authors" : [
      {
        "name" : "Amy Steier"
      },
      {
        "name" : "Lipika Ramaswamy"
      },
      {
        "name" : "Andre Manoel"
      },
      {
        "name" : "Alexa Haushalter"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03451v1",
    "title" : "Structure-Preference Enabled Graph Embedding Generation under\n  Differential Privacy",
    "summary" : "Graph embedding generation techniques aim to learn low-dimensional vectors\nfor each node in a graph and have recently gained increasing research\nattention. Publishing low-dimensional node vectors enables various graph\nanalysis tasks, such as structural equivalence and link prediction. Yet,\nimproper publication opens a backdoor to malicious attackers, who can infer\nsensitive information of individuals from the low-dimensional node vectors.\nExisting methods tackle this issue by developing deep graph learning models\nwith differential privacy (DP). However, they often suffer from large noise\ninjections and cannot provide structural preferences consistent with mining\nobjectives. Recently, skip-gram based graph embedding generation techniques are\nwidely used due to their ability to extract customizable structures. Based on\nskip-gram, we present SE-PrivGEmb, a structure-preference enabled graph\nembedding generation under DP. For arbitrary structure preferences, we design a\nunified noise tolerance mechanism via perturbing non-zero vectors. This\nmechanism mitigates utility degradation caused by high sensitivity. By\ncarefully designing negative sampling probabilities in skip-gram, we\ntheoretically demonstrate that skip-gram can preserve arbitrary proximities,\nwhich quantify structural features in graphs. Extensive experiments show that\nour method outperforms existing state-of-the-art methods under structural\nequivalence and link prediction tasks.",
    "updated" : "2025-01-07T00:43:18Z",
    "published" : "2025-01-07T00:43:18Z",
    "authors" : [
      {
        "name" : "Sen Zhang"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Haibo Hu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03391v1",
    "title" : "Privacy-Preserving Smart Contracts for Permissioned Blockchains: A\n  zk-SNARK-Based Recipe Part-1",
    "summary" : "The Bitcoin white paper introduced blockchain technology, enabling trustful\ntransactions without intermediaries. Smart contracts emerged with Ethereum and\nblockchains expanded beyond cryptocurrency, applying to auctions, crowdfunding\nand electronic voting. However, blockchain's transparency raised privacy\nconcerns and initial anonymity measures proved ineffective. Smart contract\nprivacy solutions employed zero-knowledge proofs, homomorphic encryption and\ntrusted execution environments. These approaches have practical drawbacks, such\nas limited functionality, high computation times and trust on third parties\nrequirements, being not fully decentralized. This work proposes a solution\nutilizing zk-SNARKs to provide privacy in smart contracts and blockchains. The\nsolution supports both fungible and nonfungible tokens. Additionally, the\nproposal includes a new type of transactions, called delegated transactions,\nwhich enable use cases like Delivery vs Payment (DvP).",
    "updated" : "2025-01-06T21:16:33Z",
    "published" : "2025-01-06T21:16:33Z",
    "authors" : [
      {
        "name" : "Aldenio Burgos"
      },
      {
        "name" : "Eduardo Alchieri"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02893v2",
    "title" : "A Volumetric Approach to Privacy of Dynamical Systems",
    "summary" : "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack.",
    "updated" : "2025-01-07T13:21:10Z",
    "published" : "2025-01-06T10:15:21Z",
    "authors" : [
      {
        "name" : "Chuanghong Weng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02804v2",
    "title" : "Latency and Privacy-Aware Resource Allocation in Vehicular Edge\n  Computing",
    "summary" : "The rapid increase in the number of connected vehicles has led to the\ngeneration of vast amounts of data. As a significant portion of this data\npertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it\nis predominantly generated at the edge. Considering the enormous volume of\ndata, real-time applications, and privacy concerns, it is crucial to process\nthe data at the edge. Neglecting the management of processing resources in\nvehicular edge computing (VEC) could lead to numerous challenges as a\nsubstantial number of vehicles with diverse safety, economic, and entertainment\napplications, along with their data processing, emerge in the near future [1].\nPrevious research in VEC resource allocation has primarily focused on issues\nsuch as response time and privacy preservation techniques. However, an approach\nthat takes into account privacy-aware resource allocation based on vehicular\nnetwork architecture and application requirements has not yet been proposed. In\nthis paper, we present a privacy and latency-aware approach for allocating\nprocessing resources at the edge of the vehicular network, considering the\nspecific requirements of different applications. Our approach involves\ncategorizing vehicular network applications based on their processing accuracy,\nreal-time processing needs, and privacy preservation requirements. We further\ndivide the vehicular network edge into two parts: the user layer (OBUs) is\nconsidered for processing applications with privacy requirements, while the\nallocation of resources in the RSUs and cloud layer is based on the specific\nneeds of different applications. In this study, we evaluate the quality of\nservice based on parameters such as privacy preservation, processing cost,\nmeeting deadlines, and result quality. Comparative analyses demonstrate that\nour approach enhances service quality by 55% compared to existing\nstate-of-the-art methods.",
    "updated" : "2025-01-07T07:17:16Z",
    "published" : "2025-01-06T06:44:49Z",
    "authors" : [
      {
        "name" : "Hossein Ahmadvand"
      },
      {
        "name" : "Fouzhan Foroutan"
      }
    ],
    "categories" : [
      "cs.PF"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04420v1",
    "title" : "A Closer Look on Gender Stereotypes in Movie Recommender Systems and\n  Their Implications with Privacy",
    "summary" : "The movie recommender system typically leverages user feedback to provide\npersonalized recommendations that align with user preferences and increase\nbusiness revenue. This study investigates the impact of gender stereotypes on\nsuch systems through a specific attack scenario. In this scenario, an attacker\ndetermines users' gender, a private attribute, by exploiting gender stereotypes\nabout movie preferences and analyzing users' feedback data, which is either\npublicly available or observed within the system. The study consists of two\nphases. In the first phase, a user study involving 630 participants identified\ngender stereotypes associated with movie genres, which often influence viewing\nchoices. In the second phase, four inference algorithms were applied to detect\ngender stereotypes by combining the findings from the first phase with users'\nfeedback data. Results showed that these algorithms performed more effectively\nthan relying solely on feedback data for gender inference. Additionally, we\nquantified the extent of gender stereotypes to evaluate their broader impact on\ndigital computational science. The latter part of the study utilized two major\nmovie recommender datasets: MovieLens 1M and Yahoo!Movie. Detailed experimental\ninformation is available on our GitHub repository:\nhttps://github.com/fr-iit/GSMRS",
    "updated" : "2025-01-08T11:08:58Z",
    "published" : "2025-01-08T11:08:58Z",
    "authors" : [
      {
        "name" : "Falguni Roy"
      },
      {
        "name" : "Yiduo Shen"
      },
      {
        "name" : "Na Zhao"
      },
      {
        "name" : "Xiaofeng Ding"
      },
      {
        "name" : "Md. Omar Faruk"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04409v1",
    "title" : "Lossless Privacy-Preserving Aggregation for Decentralized Federated\n  Learning",
    "summary" : "Privacy concerns arise as sensitive data proliferate. Despite decentralized\nfederated learning (DFL) aggregating gradients from neighbors to avoid direct\ndata transmission, it still poses indirect data leaks from the transmitted\ngradients. Existing privacy-preserving methods for DFL add noise to gradients.\nThey either diminish the model predictive accuracy or suffer from ineffective\ngradient protection. In this paper, we propose a novel lossless\nprivacy-preserving aggregation rule named LPPA to enhance gradient protection\nas much as possible but without loss of DFL model predictive accuracy. LPPA\nsubtly injects the noise difference between the sent and received noise into\ntransmitted gradients for gradient protection. The noise difference\nincorporates neighbors' randomness for each client, effectively safeguarding\nagainst data leaks. LPPA employs the noise flow conservation theory to ensure\nthat the noise impact can be globally eliminated. The global sum of all noise\ndifferences remains zero, ensuring that accurate gradient aggregation is\nunaffected and the model accuracy remains intact. We theoretically prove that\nthe privacy-preserving capacity of LPPA is \\sqrt{2} times greater than that of\nnoise addition, while maintaining comparable model accuracy to the standard DFL\naggregation without noise injection. Experimental results verify the\ntheoretical findings and show that LPPA achieves a 13% mean improvement in\naccuracy over noise addition. We also demonstrate the effectiveness of LPPA in\nprotecting raw data and guaranteeing lossless model accuracy.",
    "updated" : "2025-01-08T10:49:06Z",
    "published" : "2025-01-08T10:49:06Z",
    "authors" : [
      {
        "name" : "Xiaoye Miao"
      },
      {
        "name" : "Bin Li"
      },
      {
        "name" : "Yangyang Wu"
      },
      {
        "name" : "Meng Xi"
      },
      {
        "name" : "Xinkui Zhao"
      },
      {
        "name" : "Jianwei Yin"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04323v1",
    "title" : "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models",
    "summary" : "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance.",
    "updated" : "2025-01-08T07:47:43Z",
    "published" : "2025-01-08T07:47:43Z",
    "authors" : [
      {
        "name" : "Shi Haonan"
      },
      {
        "name" : "Ouyang Tu"
      },
      {
        "name" : "Wang An"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04222v1",
    "title" : "Privacy-Preserving Distributed Online Mirror Descent for Nonconvex\n  Optimization",
    "summary" : "We investigate the distributed online nonconvex optimization problem with\ndifferential privacy over time-varying networks. Each node minimizes the sum of\nseveral nonconvex functions while preserving the node's differential privacy.\nWe propose a privacy-preserving distributed online mirror descent algorithm for\nnonconvex optimization, which uses the mirror descent to update decision\nvariables and the Laplace differential privacy mechanism to protect privacy.\nUnlike the existing works, the proposed algorithm allows the cost functions to\nbe nonconvex, which is more applicable. Based upon these, we prove that if the\ncommunication network is $B$-strongly connected and the constraint set is\ncompact, then by choosing the step size properly, the algorithm guarantees\n$\\epsilon$-differential privacy at each time. Furthermore, we prove that if the\nlocal cost functions are $\\beta$-smooth, then the regret over time horizon $T$\ngrows sublinearly while preserving differential privacy, with an upper bound\n$O(\\sqrt{T})$. Finally, the effectiveness of the algorithm is demonstrated\nthrough numerical simulations.",
    "updated" : "2025-01-08T01:39:10Z",
    "published" : "2025-01-08T01:39:10Z",
    "authors" : [
      {
        "name" : "Yingjie Zhou"
      },
      {
        "name" : "Tao Li"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04134v1",
    "title" : "Mixing Times and Privacy Analysis for the Projected Langevin Algorithm\n  under a Modulus of Continuity",
    "summary" : "We study the mixing time of the projected Langevin algorithm (LA) and the\nprivacy curve of noisy Stochastic Gradient Descent (SGD), beyond nonexpansive\niterations. Specifically, we derive new mixing time bounds for the projected LA\nwhich are, in some important cases, dimension-free and poly-logarithmic on the\naccuracy, closely matching the existing results in the smooth convex case.\nAdditionally, we establish new upper bounds for the privacy curve of the\nsubsampled noisy SGD algorithm. These bounds show a crucial dependency on the\nregularity of gradients, and are useful for a wide range of convex losses\nbeyond the smooth case. Our analysis relies on a suitable extension of the\nPrivacy Amplification by Iteration (PABI) framework (Feldman et al., 2018;\nAltschuler and Talwar, 2022, 2023) to noisy iterations whose gradient map is\nnot necessarily nonexpansive. This extension is achieved by designing an\noptimization problem which accounts for the best possible R\\'enyi divergence\nbound obtained by an application of PABI, where the tractability of the problem\nis crucially related to the modulus of continuity of the associated gradient\nmapping. We show that, in several interesting cases -- including the nonsmooth\nconvex, weakly smooth and (strongly) dissipative -- such optimization problem\ncan be solved exactly and explicitly. This yields the tightest possible\nPABI-based bounds, where our results are either new or substantially sharper\nthan those in previous works.",
    "updated" : "2025-01-07T20:46:59Z",
    "published" : "2025-01-07T20:46:59Z",
    "authors" : [
      {
        "name" : "Mario Bravo"
      },
      {
        "name" : "Juan P. Flores-Mella"
      },
      {
        "name" : "Cristóbal Guzmán"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "math.OC",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04058v1",
    "title" : "Homomorphic Encryption in Healthcare Industry Applications for\n  Protecting Data Privacy",
    "summary" : "Focussing on two different use cases-Quality Control methods in industrial\ncontexts and Neural Network algorithms for healthcare diagnostics-this research\ninvestigates the inclusion of Fully Homomorphic Encryption into real-world\napplications in the healthcare sector. We evaluate the performance, resource\nrequirements, and viability of deploying FHE in these settings through\nextensive testing and analysis, highlighting the progress made in FHE tooling\nand the obstacles still facing addressing the gap between conceptual research\nand practical applications. We start our research by describing the specific\ncase study and trust model were working with. Choosing the two FHE frameworks\nmost appropriate for industry development, we assess the resources and\nperformance requirements for implementing each of the two FHE frameworks in the\nfirst scenario, Quality Control algorithms. In conclusion, our findings\ndemonstrate the effectiveness and resource consumption of the two use\ncases-complex NN models and simple QC algorithms-when implemented in an FHE\nsetting.",
    "updated" : "2025-01-07T07:42:41Z",
    "published" : "2025-01-07T07:42:41Z",
    "authors" : [
      {
        "name" : "J. S. Rauthan"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.05053v1",
    "title" : "TAPFed: Threshold Secure Aggregation for Privacy-Preserving Federated\n  Learning",
    "summary" : "Federated learning is a computing paradigm that enhances privacy by enabling\nmultiple parties to collaboratively train a machine learning model without\nrevealing personal data. However, current research indicates that traditional\nfederated learning platforms are unable to ensure privacy due to privacy leaks\ncaused by the interchange of gradients. To achieve privacy-preserving federated\nlearning, integrating secure aggregation mechanisms is essential.\nUnfortunately, existing solutions are vulnerable to recently demonstrated\ninference attacks such as the disaggregation attack. This paper proposes\nTAPFed, an approach for achieving privacy-preserving federated learning in the\ncontext of multiple decentralized aggregators with malicious actors. TAPFed\nuses a proposed threshold functional encryption scheme and allows for a certain\nnumber of malicious aggregators while maintaining security and privacy. We\nprovide formal security and privacy analyses of TAPFed and compare it to\nvarious baselines through experimental evaluation. Our results show that TAPFed\noffers equivalent performance in terms of model quality compared to\nstate-of-the-art approaches while reducing transmission overhead by 29%-45%\nacross different model training scenarios. Most importantly, TAPFed can defend\nagainst recently demonstrated inference attacks caused by curious aggregators,\nwhich the majority of existing approaches are susceptible to.",
    "updated" : "2025-01-09T08:24:10Z",
    "published" : "2025-01-09T08:24:10Z",
    "authors" : [
      {
        "name" : "Runhua Xu"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Chao Li"
      },
      {
        "name" : "James B. D. Joshi"
      },
      {
        "name" : "Shuai Ma"
      },
      {
        "name" : "Jianxin Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04940v1",
    "title" : "A New Perspective on Privacy Protection in Federated Learning with\n  Granular-Ball Computing",
    "summary" : "Federated Learning (FL) facilitates collaborative model training while\nprioritizing privacy by avoiding direct data sharing. However, most existing\narticles attempt to address challenges within the model's internal parameters\nand corresponding outputs, while neglecting to solve them at the input level.\nTo address this gap, we propose a novel framework called Granular-Ball\nFederated Learning (GrBFL) for image classification. GrBFL diverges from\ntraditional methods that rely on the finest-grained input data. Instead, it\nsegments images into multiple regions with optimal coarse granularity, which\nare then reconstructed into a graph structure. We designed a two-dimensional\nbinary search segmentation algorithm based on variance constraints for GrBFL,\nwhich effectively removes redundant information while preserving key\nrepresentative features. Extensive theoretical analysis and experiments\ndemonstrate that GrBFL not only safeguards privacy and enhances efficiency but\nalso maintains robust utility, consistently outperforming other\nstate-of-the-art FL methods. The code is available at\nhttps://github.com/AIGNLAI/GrBFL.",
    "updated" : "2025-01-09T03:14:03Z",
    "published" : "2025-01-09T03:14:03Z",
    "authors" : [
      {
        "name" : "Guannan Lai"
      },
      {
        "name" : "Yihui Feng"
      },
      {
        "name" : "Xin Yang"
      },
      {
        "name" : "Xiaoyu Deng"
      },
      {
        "name" : "Hao Yu"
      },
      {
        "name" : "Shuyin Xia"
      },
      {
        "name" : "Guoyin Wang"
      },
      {
        "name" : "Tianrui Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04323v2",
    "title" : "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models",
    "summary" : "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance.",
    "updated" : "2025-01-09T02:33:04Z",
    "published" : "2025-01-08T07:47:43Z",
    "authors" : [
      {
        "name" : "Haonan Shi"
      },
      {
        "name" : "Tu Ouyang"
      },
      {
        "name" : "An Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.06161v1",
    "title" : "RIOT-based smart metering system for privacy-preserving data aggregation\n  using watermarking and encryption",
    "summary" : "The remarkable advancement of smart grid technology in the IoT sector has\nraised concerns over the privacy and security of the data collected and\ntransferred in real-time. Smart meters generate detailed information about\nconsumers' energy consumption patterns, increasing the risks of data breaches,\nidentity theft, and other forms of cyber attacks. This study proposes a\nprivacy-preserving data aggregation protocol that uses reversible watermarking\nand AES cryptography to ensure the security and privacy of the data. There are\ntwo versions of the protocol: one for low-frequency smart meters that uses\nLSB-shifting-based reversible watermarking (RLS) and another for high-frequency\nsmart meters that uses difference expansion-based reversible watermarking\n(RDE). This enables the aggregation of smart meter data, maintaining\nconfidentiality, integrity, and authenticity. The proposed protocol\nsignificantly enhances privacy-preserving measures for smart metering systems,\nconducting an experimental evaluation with real hardware implementation using\nNucleo microcontroller boards and the RIOT operating system and comparing the\nresults to existing security schemes.",
    "updated" : "2025-01-10T18:37:20Z",
    "published" : "2025-01-10T18:37:20Z",
    "authors" : [
      {
        "name" : "David Megias"
      },
      {
        "name" : "Farzana Kabir"
      },
      {
        "name" : "Krzysztof Cabaj"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.05535v1",
    "title" : "On Fair Ordering and Differential Privacy",
    "summary" : "In blockchain systems, fair transaction ordering is crucial for a trusted and\nregulation-compliant economic ecosystem. Unlike traditional State Machine\nReplication (SMR) systems, which focus solely on liveness and safety,\nblockchain systems also require a fairness property. This paper examines these\nproperties and aims to eliminate algorithmic bias in transaction ordering\nservices.\n  We build on the notion of equal opportunity. We characterize transactions in\nterms of relevant and irrelevant features, requiring that the order be\ndetermined solely by the relevant ones. Specifically, transactions with\nidentical relevant features should have an equal chance of being ordered before\none another. We extend this framework to define a property where the greater\nthe distance in relevant features between transactions, the higher the\nprobability of prioritizing one over the other.\n  We reveal a surprising link between equal opportunity in SMR and Differential\nPrivacy (DP), showing that any DP mechanism can be used to ensure fairness in\nSMR. This connection not only enhances our understanding of the interplay\nbetween privacy and fairness in distributed computing but also opens up new\nopportunities for designing fair distributed protocols using well-established\nDP techniques.",
    "updated" : "2025-01-09T19:17:43Z",
    "published" : "2025-01-09T19:17:43Z",
    "authors" : [
      {
        "name" : "Shir Cohen"
      },
      {
        "name" : "Neel Basu"
      },
      {
        "name" : "Soumya Basu"
      },
      {
        "name" : "Lorenzo Alvisi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.07262v1",
    "title" : "OblivCDN: A Practical Privacy-preserving CDN with Oblivious Content\n  Access",
    "summary" : "Content providers increasingly utilise Content Delivery Networks (CDNs) to\nenhance users' content download experience. However, this deployment scenario\nraises significant security concerns regarding content confidentiality and user\nprivacy due to the involvement of third-party providers. Prior proposals using\nprivate information retrieval (PIR) and oblivious RAM (ORAM) have proven\nimpractical due to high computation and communication costs, as well as\nintegration challenges within distributed CDN architectures. In response, we\npresent \\textsf{OblivCDN}, a practical privacy-preserving system meticulously\ndesigned for seamless integration with the existing real-world Internet-CDN\ninfrastructure. Our design strategically adapts Range ORAM primitives to\noptimise memory and disk seeks when accessing contiguous blocks of CDN content,\nboth at the origin and edge servers, while preserving both content\nconfidentiality and user access pattern hiding features. Also, we carefully\ncustomise several oblivious building blocks that integrate the distributed\ntrust model into the ORAM client, thereby eliminating the computational\nbottleneck in the origin server and reducing communication costs between the\norigin server and edge servers. Moreover, the newly-designed ORAM client also\neliminates the need for trusted hardware on edge servers, and thus\nsignificantly ameliorates the compatibility towards networks with massive\nlegacy devices.In real-world streaming evaluations, OblivCDN} demonstrates\nremarkable performance, downloading a $256$ MB video in just $5.6$ seconds.\nThis achievement represents a speedup of $90\\times$ compared to a strawman\napproach (direct ORAM adoption) and a $366\\times$ improvement over the prior\nart, OblivP2P.",
    "updated" : "2025-01-13T12:23:23Z",
    "published" : "2025-01-13T12:23:23Z",
    "authors" : [
      {
        "name" : "Viet Vo"
      },
      {
        "name" : "Shangqi Lai"
      },
      {
        "name" : "Xingliang Yuan"
      },
      {
        "name" : "Surya Nepal"
      },
      {
        "name" : "Qi Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.07209v1",
    "title" : "Privacy-Preserving Authentication: Theory vs. Practice",
    "summary" : "With the increasing use of online services, the protection of the privacy of\nusers becomes more and more important. This is particularly critical as\nauthentication and authorization as realized on the Internet nowadays,\ntypically relies on centralized identity management solutions. Although those\nare very convenient from a user's perspective, they are quite intrusive from a\nprivacy perspective and are currently far from implementing the concept of data\nminimization. Fortunately, cryptography offers exciting primitives such as\nzero-knowledge proofs and advanced signature schemes to realize various forms\nof so-called anonymous credentials. Such primitives allow to realize online\nauthentication and authorization with a high level of built-in privacy\nprotection (what we call privacy-preserving authentication). Though these\nprimitives have already been researched for various decades and are well\nunderstood in the research community, unfortunately, they lack widespread\nadoption. In this paper, we look at the problems, what cryptography can do,\nsome deployment examples, and barriers to widespread adoption. Latter using the\nexample of the EU Digital Identity Wallet (EUDIW) and the recent discussion and\nfeedback from cryptography experts around this topic. We also briefly comment\non the transition to post-quantum cryptography.",
    "updated" : "2025-01-13T11:04:05Z",
    "published" : "2025-01-13T11:04:05Z",
    "authors" : [
      {
        "name" : "Daniel Slamanig"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.07154v1",
    "title" : "Privacy-Preserving Data Quality Assessment for Time-Series IoT Sensors",
    "summary" : "Data from Internet of Things (IoT) sensors has emerged as a key contributor\nto decision-making processes in various domains. However, the quality of the\ndata is crucial to the effectiveness of applications built on it, and\nassessment of the data quality is heavily context-dependent. Further,\npreserving the privacy of the data during quality assessment is critical in\ndomains where sensitive data is prevalent. This paper proposes a novel\nframework for automated, objective, and privacy-preserving data quality\nassessment of time-series data from IoT sensors deployed in smart cities. We\nleverage custom, autonomously computable metrics that parameterise the temporal\nperformance and adherence to a declarative schema document to achieve\nobjectivity. Additionally, we utilise a trusted execution environment to create\na \"data-blind\" model that ensures individual privacy, eliminates assessee bias,\nand enhances adaptability across data types. This paper describes this data\nquality assessment methodology for IoT sensors, emphasising its relevance\nwithin the smart-city context while addressing the growing need for privacy in\nthe face of extensive data collection practices.",
    "updated" : "2025-01-13T09:28:42Z",
    "published" : "2025-01-13T09:28:42Z",
    "authors" : [
      {
        "name" : "Novoneel Chakraborty"
      },
      {
        "name" : "Abhay Sharma"
      },
      {
        "name" : "Jyotirmoy Dutta"
      },
      {
        "name" : "Hari Dilip Kumar"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.06913v1",
    "title" : "Towards Fair and Privacy-Aware Transfer Learning for Educational\n  Predictive Modeling: A Case Study on Retention Prediction in Community\n  Colleges",
    "summary" : "Predictive analytics is widely used in learning analytics, but many\nresource-constrained institutions lack the capacity to develop their own models\nor rely on proprietary ones trained in different contexts with little\ntransparency. Transfer learning holds promise for expanding equitable access to\npredictive analytics but remains underexplored due to legal and technical\nconstraints. This paper examines transfer learning strategies for retention\nprediction at U.S. two-year community colleges. We envision a scenario where\ncommunity colleges collaborate with each other and four-year universities to\ndevelop retention prediction models under privacy constraints and evaluate\nrisks and improvement strategies of cross-institutional model transfer. Using\nadministrative records from 4 research universities and 23 community colleges\ncovering over 800,000 students across 7 cohorts, we identify performance and\nfairness degradation when external models are deployed locally without\nadaptation. Publicly available contextual information can forecast these\nperformance drops and offer early guidance for model portability. For\ndevelopers under privacy regulations, sequential training selecting\ninstitutions based on demographic similarities enhances fairness without\ncompromising performance. For institutions lacking local data to fine-tune\nsource models, customizing evaluation thresholds for sensitive groups\noutperforms standard transfer techniques in improving performance and fairness.\nOur findings suggest the value of transfer learning for more accessible\neducational predictive modeling and call for judicious use of contextual\ninformation in model training, selection, and deployment to achieve reliable\nand equitable model transfer.",
    "updated" : "2025-01-12T19:49:28Z",
    "published" : "2025-01-12T19:49:28Z",
    "authors" : [
      {
        "name" : "Chengyuan Yao"
      },
      {
        "name" : "Carmen Cortez"
      },
      {
        "name" : "Renzhe Yu"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.06300v1",
    "title" : "Tensorization of neural networks for improved privacy and\n  interpretability",
    "summary" : "We present a tensorization algorithm for constructing tensor train\nrepresentations of functions, drawing on sketching and cross interpolation\nideas. The method only requires black-box access to the target function and a\nsmall set of sample points defining the domain of interest. Thus, it is\nparticularly well-suited for machine learning models, where the domain of\ninterest is naturally defined by the training dataset. We show that this\napproach can be used to enhance the privacy and interpretability of neural\nnetwork models. Specifically, we apply our decomposition to (i) obfuscate\nneural networks whose parameters encode patterns tied to the training data\ndistribution, and (ii) estimate topological phases of matter that are easily\naccessible from the tensor train representation. Additionally, we show that\nthis tensorization can serve as an efficient initialization method for\noptimizing tensor trains in general settings, and that, for model compression,\nour algorithm achieves a superior trade-off between memory and time complexity\ncompared to conventional tensorization methods of neural networks.",
    "updated" : "2025-01-10T19:00:06Z",
    "published" : "2025-01-10T19:00:06Z",
    "authors" : [
      {
        "name" : "José Ramón Pareja Monturiol"
      },
      {
        "name" : "Alejandro Pozas-Kerstjens"
      },
      {
        "name" : "David Pérez-García"
      }
    ],
    "categories" : [
      "math.NA",
      "cs.LG",
      "cs.NA",
      "physics.comp-ph",
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.06161v2",
    "title" : "RIOT-based smart metering system for privacy-preserving data aggregation\n  using watermarking and encryption",
    "summary" : "The remarkable advancement of smart grid technology in the IoT sector has\nraised concerns over the privacy and security of the data collected and\ntransferred in real-time. Smart meters generate detailed information about\nconsumers' energy consumption patterns, increasing the risks of data breaches,\nidentity theft, and other forms of cyber attacks. This study proposes a\nprivacy-preserving data aggregation protocol that uses reversible watermarking\nand AES cryptography to ensure the security and privacy of the data. There are\ntwo versions of the protocol: one for low-frequency smart meters that uses\nLSB-shifting-based reversible watermarking (RLS) and another for high-frequency\nsmart meters that uses difference expansion-based reversible watermarking\n(RDE). This enables the aggregation of smart meter data, maintaining\nconfidentiality, integrity, and authenticity. The proposed protocol\nsignificantly enhances privacy-preserving measures for smart metering systems,\nconducting an experimental evaluation with real hardware implementation using\nNucleo microcontroller boards and the RIOT operating system and comparing the\nresults to existing security schemes.",
    "updated" : "2025-01-13T16:58:59Z",
    "published" : "2025-01-10T18:37:20Z",
    "authors" : [
      {
        "name" : "Farzana Kabir"
      },
      {
        "name" : "David Megias"
      },
      {
        "name" : "Krzysztof Cabaj"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.08236v1",
    "title" : "Privacy-Preserving Model and Preprocessing Verification for Machine\n  Learning",
    "summary" : "This paper presents a framework for privacy-preserving verification of\nmachine learning models, focusing on models trained on sensitive data.\nIntegrating Local Differential Privacy (LDP) with model explanations from LIME\nand SHAP, our framework enables robust verification without compromising\nindividual privacy. It addresses two key tasks: binary classification, to\nverify if a target model was trained correctly by applying the appropriate\npreprocessing steps, and multi-class classification, to identify specific\npreprocessing errors. Evaluations on three real-world datasets-Diabetes, Adult,\nand Student Record-demonstrate that while the ML-based approach is particularly\neffective in binary tasks, the threshold-based method performs comparably in\nmulti-class tasks. Results indicate that although verification accuracy varies\nacross datasets and noise levels, the framework provides effective detection of\npreprocessing errors, strong privacy guarantees, and practical applicability\nfor safeguarding sensitive data.",
    "updated" : "2025-01-14T16:21:54Z",
    "published" : "2025-01-14T16:21:54Z",
    "authors" : [
      {
        "name" : "Wenbiao Li"
      },
      {
        "name" : "Anisa Halimi"
      },
      {
        "name" : "Xiaoqian Jiang"
      },
      {
        "name" : "Jaideep Vaidya"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.07844v1",
    "title" : "Towards A Hybrid Quantum Differential Privacy",
    "summary" : "Quantum computing offers unparalleled processing power but raises significant\ndata privacy challenges. Quantum Differential Privacy (QDP) leverages inherent\nquantum noise to safeguard privacy, surpassing traditional DP. This paper\ndevelops comprehensive noise profiles, identifies noise types beneficial for\nQDP, and highlights teh need for practical implementations beyond theoretical\nmodels. Existing QDP mechanisms, limited to single noise sources, fail to\nreflect teh multi-source noise reality of quantum systems. We propose a\nresilient hybrid QDP mechanism utilizing channel and measurement noise,\noptimizing privacy budgets to balance privacy and utility. Additionally, we\nintroduce Lifted Quantum Differential Privacy, offering enhanced randomness for\nimproved privacy audits and quantum algorithm evaluation.",
    "updated" : "2025-01-14T05:13:37Z",
    "published" : "2025-01-14T05:13:37Z",
    "authors" : [
      {
        "name" : "Baobao Song"
      },
      {
        "name" : "Shiva Raj Pokhrel"
      },
      {
        "name" : "Athanasios V. Vasilakos"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Gang Li"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.08665v1",
    "title" : "A Survey on Facial Image Privacy Preservation in Cloud-Based Services",
    "summary" : "Facial recognition models are increasingly employed by commercial\nenterprises, government agencies, and cloud service providers for identity\nverification, consumer services, and surveillance. These models are often\ntrained using vast amounts of facial data processed and stored in cloud-based\nplatforms, raising significant privacy concerns. Users' facial images may be\nexploited without their consent, leading to potential data breaches and misuse.\nThis survey presents a comprehensive review of current methods aimed at\npreserving facial image privacy in cloud-based services. We categorize these\nmethods into two primary approaches: image obfuscation-based protection and\nadversarial perturbation-based protection. We provide an in-depth analysis of\nboth categories, offering qualitative and quantitative comparisons of their\neffectiveness. Additionally, we highlight unresolved challenges and propose\nfuture research directions to improve privacy preservation in cloud computing\nenvironments.",
    "updated" : "2025-01-15T09:00:32Z",
    "published" : "2025-01-15T09:00:32Z",
    "authors" : [
      {
        "name" : "Chen Chen"
      },
      {
        "name" : "Mengyuan Sun"
      },
      {
        "name" : "Xueluan Gong"
      },
      {
        "name" : "Yanjiao Chen"
      },
      {
        "name" : "Qian Wang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.08449v1",
    "title" : "A Refreshment Stirred, Not Shaken (II): Invariant-Preserving Deployments\n  of Differential Privacy for the US Decennial Census",
    "summary" : "Through the lens of the system of differential privacy specifications\ndeveloped in Part I of a trio of articles, this second paper examines two\nstatistical disclosure control (SDC) methods for the United States Decennial\nCensus: the Permutation Swapping Algorithm (PSA), which is similar to the 2010\nCensus's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA),\nwhich was used in the 2020 DAS. To varying degrees, both methods leave\nunaltered some statistics of the confidential data $\\unicode{x2013}$ which are\ncalled the method's invariants $\\unicode{x2013}$ and hence neither can be\nreadily reconciled with differential privacy (DP), at least as it was\noriginally conceived. Nevertheless, we establish that the PSA satisfies\n$\\varepsilon$-DP subject to the invariants it necessarily induces, thereby\nshowing that this traditional SDC method can in fact still be understood within\nour more-general system of DP specifications. By a similar modification to\n$\\rho$-zero concentrated DP, we also provide a DP specification for the TDA.\nFinally, as a point of comparison, we consider the counterfactual scenario in\nwhich the PSA was adopted for the 2020 Census, resulting in a reduction in the\nnominal privacy loss, but at the cost of releasing many more invariants.\nTherefore, while our results explicate the mathematical guarantees of SDC\nprovided by the PSA, the TDA and the 2020 DAS in general, care must be taken in\ntheir translation to actual privacy protection $\\unicode{x2013}$ just as is the\ncase for any DP deployment.",
    "updated" : "2025-01-14T21:38:01Z",
    "published" : "2025-01-14T21:38:01Z",
    "authors" : [
      {
        "name" : "James Bailie"
      },
      {
        "name" : "Ruobin Gong"
      },
      {
        "name" : "Xiao-Li Meng"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.DS",
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.07844v2",
    "title" : "Towards A Hybrid Quantum Differential Privacy",
    "summary" : "Quantum computing offers unparalleled processing power but raises significant\ndata privacy challenges. Quantum Differential Privacy (QDP) leverages inherent\nquantum noise to safeguard privacy, surpassing traditional DP. This paper\ndevelops comprehensive noise profiles, identifies noise types beneficial for\nQDP, and highlights teh need for practical implementations beyond theoretical\nmodels. Existing QDP mechanisms, limited to single noise sources, fail to\nreflect teh multi-source noise reality of quantum systems. We propose a\nresilient hybrid QDP mechanism utilizing channel and measurement noise,\noptimizing privacy budgets to balance privacy and utility. Additionally, we\nintroduce Lifted Quantum Differential Privacy, offering enhanced randomness for\nimproved privacy audits and quantum algorithm evaluation.",
    "updated" : "2025-01-15T15:10:13Z",
    "published" : "2025-01-14T05:13:37Z",
    "authors" : [
      {
        "name" : "Baobao Song"
      },
      {
        "name" : "Shiva Raj Pokhrel"
      },
      {
        "name" : "Athanasios V. Vasilakos"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Gang Li"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.09191v1",
    "title" : "Detecting Vulnerabilities in Encrypted Software Code while Ensuring Code\n  Privacy",
    "summary" : "Software vulnerabilities continue to be the main cause of occurrence for\ncyber attacks. In an attempt to reduce them and improve software quality,\nsoftware code analysis has emerged as a service offered by companies\nspecialising in software testing. However, this service requires software\ncompanies to provide access to their software's code, which raises concerns\nabout code privacy and intellectual property theft. This paper presents a novel\napproach to Software Quality and Privacy, in which testing companies can\nperform code analysis tasks on encrypted software code provided by software\ncompanies while code privacy is preserved. The approach combines Static Code\nAnalysis and Searchable Symmetric Encryption in order to process the source\ncode and build an encrypted inverted index that represents its data and control\nflows. The index is then used to discover vulnerabilities by carrying out\nstatic analysis tasks in a confidential way. With this approach, this paper\nalso defines a new research field -- Confidential Code Analysis --, from which\nother types of code analysis tasks and approaches can be derived. We\nimplemented the approach in a new tool called CoCoA and evaluated it\nexperimentally with synthetic and real PHP web applications. The results show\nthat the tool has similar precision as standard (non-confidential) static\nanalysis tools and a modest average performance overhead of 42.7%.",
    "updated" : "2025-01-15T22:39:50Z",
    "published" : "2025-01-15T22:39:50Z",
    "authors" : [
      {
        "name" : "Jorge Martins"
      },
      {
        "name" : "David Dantas"
      },
      {
        "name" : "Rafael Ramires"
      },
      {
        "name" : "Bernardo Ferreira"
      },
      {
        "name" : "Ibéria Medeiros"
      }
    ],
    "categories" : [
      "cs.SE",
      "cs.CR",
      "D.2.5; E.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.09031v1",
    "title" : "Synthetic Data and Health Privacy",
    "summary" : "This Viewpoint discusses generative artificial intelligence and safeguarding\nprivacy by using synthetic data as a substitute for private health data.",
    "updated" : "2025-01-13T10:23:14Z",
    "published" : "2025-01-13T10:23:14Z",
    "authors" : [
      {
        "name" : "Gwénolé Abgrall"
      },
      {
        "name" : "Xavier Monnet"
      },
      {
        "name" : "Anmol Arora"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.00824v2",
    "title" : "Information Sifting Funnel: Privacy-preserving Collaborative Inference\n  Against Model Inversion Attacks",
    "summary" : "The complexity of neural networks and inference tasks, coupled with demands\nfor computational efficiency and real-time feedback, poses significant\nchallenges for resource-constrained edge devices. Collaborative inference\nmitigates this by assigning shallow feature extraction to edge devices and\noffloading features to the cloud for further inference, reducing computational\nload. However, transmitted features remain susceptible to model inversion\nattacks (MIAs), which can reconstruct original input data. Current defenses,\nsuch as perturbation and information bottleneck techniques, offer explainable\nprotection but face limitations, including the lack of standardized criteria\nfor assessing MIA difficulty, challenges in mutual information estimation, and\ntrade-offs among usability, privacy, and deployability.\n  To address these challenges, we introduce the first criterion to evaluate MIA\ndifficulty in collaborative inference, supported by theoretical analysis of\nexisting attacks and defenses, validated using experiments with the Mutual\nInformation Neural Estimator (MINE). Based on these findings, we propose\nSiftFunnel, a privacy-preserving framework for collaborative inference. The\nedge model is trained with linear and non-linear correlation constraints to\nreduce redundant information in transmitted features, enhancing privacy\nprotection. Label smoothing and a cloud-based upsampling module are added to\nbalance usability and privacy. To improve deployability, the edge model\nincorporates a funnel-shaped structure and attention mechanisms, preserving\nboth privacy and usability. Extensive experiments demonstrate that SiftFunnel\noutperforms state-of-the-art defenses against MIAs, achieving superior privacy\nprotection with less than 3% accuracy loss and striking an optimal balance\namong usability, privacy, and practicality.",
    "updated" : "2025-01-16T02:38:55Z",
    "published" : "2025-01-01T13:00:01Z",
    "authors" : [
      {
        "name" : "Rongke Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.10319v1",
    "title" : "Natural Language Processing of Privacy Policies: A Survey",
    "summary" : "Natural Language Processing (NLP) is an essential subset of artificial\nintelligence. It has become effective in several domains, such as healthcare,\nfinance, and media, to identify perceptions, opinions, and misuse, among\nothers. Privacy is no exception, and initiatives have been taken to address the\nchallenges of usable privacy notifications to users with the help of NLP. To\nthis aid, we conduct a literature review by analyzing 109 papers at the\nintersection of NLP and privacy policies. First, we provide a brief\nintroduction to privacy policies and discuss various facets of associated\nproblems, which necessitate the application of NLP to elevate the current state\nof privacy notices and disclosures to users. Subsequently, we a) provide an\noverview of the implementation and effectiveness of NLP approaches for better\nprivacy policy communication; b) identify the methodologies that can be further\nenhanced to provide robust privacy policies; and c) identify the gaps in the\ncurrent state-of-the-art research. Our systematic analysis reveals that several\nresearch papers focus on annotating and classifying privacy texts for analysis\nbut need to adequately dwell on other aspects of NLP applications, such as\nsummarization. More specifically, ample research opportunities exist in this\ndomain, covering aspects such as corpus generation, summarization vectors,\ncontextualized word embedding, identification of privacy-relevant statement\ncategories, fine-grained classification, and domain-specific model tuning.",
    "updated" : "2025-01-17T17:47:15Z",
    "published" : "2025-01-17T17:47:15Z",
    "authors" : [
      {
        "name" : "Andrick Adhikari"
      },
      {
        "name" : "Sanchari Das"
      },
      {
        "name" : "Rinku Dewri"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.10099v1",
    "title" : "Several Representations of $α$-Mutual Information and\n  Interpretations as Privacy Leakage Measures",
    "summary" : "In this paper, we present several novel representations of $\\alpha$-mutual\ninformation ($\\alpha$-MI) in terms of R{\\' e}nyi divergence and conditional\nR{\\' e}nyi entropy. The representations are based on the variational\ncharacterizations of $\\alpha$-MI using a reverse channel. Based on these\nrepresentations, we provide several interpretations of the $\\alpha$-MI as\nprivacy leakage measures using generalized mean and gain functions. Further, as\nbyproducts of the representations, we propose novel conditional R{\\' e}nyi\nentropies that satisfy the property that conditioning reduces entropy and\ndata-processing inequality.",
    "updated" : "2025-01-17T10:36:05Z",
    "published" : "2025-01-17T10:36:05Z",
    "authors" : [
      {
        "name" : "Akira Kamatsuka"
      },
      {
        "name" : "Takashiro Yoshida"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.12359v1",
    "title" : "Measured Hockey-Stick Divergence and its Applications to Quantum\n  Pufferfish Privacy",
    "summary" : "The hockey-stick divergence is a fundamental quantity characterizing several\nstatistical privacy frameworks that ensure privacy for classical and quantum\ndata. In such quantum privacy frameworks, the adversary is allowed to perform\nall possible measurements. However, in practice, there are typically\nlimitations to the set of measurements that can be performed. To this end,\nhere, we comprehensively analyze the measured hockey-stick divergence under\nseveral classes of practically relevant measurement classes. We prove several\nof its properties, including data processing and convexity. We show that it is\nefficiently computable by semi-definite programming for some classes of\nmeasurements and can be analytically evaluated for Werner and isotropic states.\nNotably, we show that the measured hockey-stick divergence characterizes\noptimal privacy parameters in the quantum pufferfish privacy framework. With\nthis connection and the developed technical tools, we enable methods to\nquantify and audit privacy for several practically relevant settings. Lastly,\nwe introduce the measured hockey-stick divergence of channels and explore its\napplications in ensuring privacy for channels.",
    "updated" : "2025-01-21T18:39:48Z",
    "published" : "2025-01-21T18:39:48Z",
    "authors" : [
      {
        "name" : "Theshani Nuradha"
      },
      {
        "name" : "Vishal Singh"
      },
      {
        "name" : "Mark M. Wilde"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.12193v1",
    "title" : "MyDigiTwin: A Privacy-Preserving Framework for Personalized\n  Cardiovascular Risk Prediction and Scenario Exploration",
    "summary" : "Cardiovascular disease (CVD) remains a leading cause of death, and primary\nprevention through personalized interventions is crucial. This paper introduces\nMyDigiTwin, a framework that integrates health digital twins with personal\nhealth environments to empower patients in exploring personalized health\nscenarios while ensuring data privacy. MyDigiTwin uses federated learning to\ntrain predictive models across distributed datasets without transferring raw\ndata, and a novel data harmonization framework addresses semantic and format\ninconsistencies in health data. A proof-of-concept demonstrates the feasibility\nof harmonizing and using cohort data to train privacy-preserving CVD prediction\nmodels. This framework offers a scalable solution for proactive, personalized\ncardiovascular care and sets the stage for future applications in real-world\nhealthcare settings.",
    "updated" : "2025-01-21T15:01:34Z",
    "published" : "2025-01-21T15:01:34Z",
    "authors" : [
      {
        "name" : "Héctor Cadavid"
      },
      {
        "name" : "Hyunho Mo"
      },
      {
        "name" : "Bauke Arends"
      },
      {
        "name" : "Katarzyna Dziopa"
      },
      {
        "name" : "Esther E. Bron"
      },
      {
        "name" : "Daniel Bos"
      },
      {
        "name" : "Sonja Georgievska"
      },
      {
        "name" : "Pim van der Harst"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.12046v1",
    "title" : "Communication-Efficient and Privacy-Adaptable Mechanism for Federated\n  Learning",
    "summary" : "Training machine learning models on decentralized private data via federated\nlearning (FL) poses two key challenges: communication efficiency and privacy\nprotection. In this work, we address these challenges within the trusted\naggregator model by introducing a novel approach called the\nCommunication-Efficient and Privacy-Adaptable Mechanism (CEPAM), achieving both\nobjectives simultaneously. In particular, CEPAM leverages the rejection-sampled\nuniversal quantizer (RSUQ), a construction of randomized vector quantizer whose\nresulting distortion is equivalent to a prescribed noise, such as Gaussian or\nLaplace noise, enabling joint differential privacy and compression. Moreover,\nwe analyze the trade-offs among user privacy, global utility, and transmission\nrate of CEPAM by defining appropriate metrics for FL with differential privacy\nand compression. Our CEPAM provides the additional benefit of privacy\nadaptability, allowing clients and the server to customize privacy protection\nbased on required accuracy and protection. We assess CEPAM's utility\nperformance using MNIST dataset, demonstrating that CEPAM surpasses baseline\nmodels in terms of learning accuracy.",
    "updated" : "2025-01-21T11:16:05Z",
    "published" : "2025-01-21T11:16:05Z",
    "authors" : [
      {
        "name" : "Chih Wei Ling"
      },
      {
        "name" : "Youqi Wu"
      },
      {
        "name" : "Jiande Sun"
      },
      {
        "name" : "Cheuk Ting Li"
      },
      {
        "name" : "Linqi Song"
      },
      {
        "name" : "Weitao Xu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.12006v1",
    "title" : "The Dilemma of Privacy Protection for Developers in the Metaverse",
    "summary" : "To investigate the level of support and awareness developers possess for\ndealing with sensitive data in the metaverse, we surveyed developers, consulted\nlegal frameworks, and analyzed API documentation in the metaverse. Our\npreliminary results suggest that privacy is a major concern, but developer\nawareness and existing support are limited. Developers lack strategies to\nidentify sensitive data that are exclusive to the metaverse. The API\ndocumentation contains guidelines for collecting sensitive information, but it\nomits instructions for identifying and protecting it. Legal frameworks include\ndefinitions that are subject to individual interpretation. These findings\nhighlight the urgent need to build a transparent and common ground for privacy\ndefinitions, identify sensitive data, and implement usable protection measures.",
    "updated" : "2025-01-21T09:56:44Z",
    "published" : "2025-01-21T09:56:44Z",
    "authors" : [
      {
        "name" : "Argianto Rahartomo"
      },
      {
        "name" : "Leonel Merino"
      },
      {
        "name" : "Mohammad Ghafari"
      },
      {
        "name" : "Yoshiki Ohshima"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.11757v1",
    "title" : "An Information Geometric Approach to Local Information Privacy with\n  Applications to Max-lift and Local Differential Privacy",
    "summary" : "We study an information-theoretic privacy mechanism design, where an agent\nobserves useful data $Y$ and wants to reveal the information to a user. Since\nthe useful data is correlated with the private data $X$, the agent uses a\nprivacy mechanism to produce disclosed data $U$ that can be released. We assume\nthat the agent observes $Y$ and has no direct access to $X$, i.e., the private\ndata is hidden. We study the privacy mechanism design that maximizes the\nrevealed information about $Y$ while satisfying a bounded Local Information\nPrivacy (LIP) criterion. When the leakage is sufficiently small, concepts from\ninformation geometry allow us to locally approximate the mutual information. By\nutilizing this approximation the main privacy-utility trade-off problem can be\nrewritten as a quadratic optimization problem that has closed-form solution\nunder some constraints. For the cases where the closed-form solution is not\nobtained we provide lower bounds on it. In contrast to the previous works that\nhave complexity issues, here, we provide simple privacy designs with low\ncomplexity which are based on finding the maximum singular value and singular\nvector of a matrix. To do so, we follow two approaches where in the first one\nwe find a lower bound on the main problem and then approximate it, however, in\nthe second approach we approximate the main problem directly. In this work, we\npresent geometrical interpretations of the proposed methods and in a numerical\nexample we compare our results considering both approaches with the optimal\nsolution and the previous methods. Furthermore, we discuss how our method can\nbe generalized considering larger amounts for the privacy leakage. Finally, we\ndiscuss how the proposed methods can be applied to deal with differential\nprivacy.",
    "updated" : "2025-01-20T21:34:55Z",
    "published" : "2025-01-20T21:34:55Z",
    "authors" : [
      {
        "name" : "Amirreza Zamani"
      },
      {
        "name" : "Parastoo Sadeghi"
      },
      {
        "name" : "Mikael Skoglund"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.11756v1",
    "title" : "Everyone's Privacy Matters! An Analysis of Privacy Leakage from\n  Real-World Facial Images on Twitter and Associated User Behaviors",
    "summary" : "Online users often post facial images of themselves and other people on\nonline social networks (OSNs) and other Web 2.0 platforms, which can lead to\npotential privacy leakage of people whose faces are included in such images.\nThere is limited research on understanding face privacy in social media while\nconsidering user behavior. It is crucial to consider privacy of subjects and\nbystanders separately. This calls for the development of privacy-aware face\ndetection classifiers that can distinguish between subjects and bystanders\nautomatically. This paper introduces such a classifier trained on face-based\nfeatures, which outperforms the two state-of-the-art methods with a significant\nmargin (by 13.1% and 3.1% for OSN images, and by 17.9% and 5.9% for non-OSN\nimages). We developed a semi-automated framework for conducting a large-scale\nanalysis of the face privacy problem by using our novel bystander-subject\nclassifier. We collected 27,800 images, each including at least one face,\nshared by 6,423 Twitter users. We then applied our framework to analyze this\ndataset thoroughly. Our analysis reveals eight key findings of different\naspects of Twitter users' real-world behaviors on face privacy, and we provide\nquantitative and qualitative results to better explain these findings. We share\nthe practical implications of our study to empower online platforms and users\nin addressing the face privacy problem efficiently.",
    "updated" : "2025-01-20T21:31:59Z",
    "published" : "2025-01-20T21:31:59Z",
    "authors" : [
      {
        "name" : "Yuqi Niu"
      },
      {
        "name" : "Weidong Qiu"
      },
      {
        "name" : "Peng Tang"
      },
      {
        "name" : "Lifan Wang"
      },
      {
        "name" : "Shuo Chen"
      },
      {
        "name" : "Shujun Li"
      },
      {
        "name" : "Nadin Kokciyan"
      },
      {
        "name" : "Ben Niu"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.11740v1",
    "title" : "PIR Over Wireless Channels: Achieving Privacy With Public Responses",
    "summary" : "This paper addresses the problem of private information retrieval (PIR) over\nan additive white Gaussian noise (AWGN) channel, considering the channel is\npublic. In such settings, each server can eavesdrop on the channel, potentially\ncompromising the user's privacy. Previous works suggested joint coding--PIR\nschemes, ignoring the fact that communication over a practical wireless channel\nis public. To address this gap, we present a novel joint wiretap--PIR coding\nscheme that leverages lattice codes to exploit the channel's additive\nproperties. This scheme integrates wiretap coding and private retrieval\ntechniques into a unified framework.",
    "updated" : "2025-01-20T20:56:56Z",
    "published" : "2025-01-20T20:56:56Z",
    "authors" : [
      {
        "name" : "Or Elimelech"
      },
      {
        "name" : "Asaf Cohen"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.10915v1",
    "title" : "LegalGuardian: A Privacy-Preserving Framework for Secure Integration of\n  Large Language Models in Legal Practice",
    "summary" : "Large Language Models (LLMs) hold promise for advancing legal practice by\nautomating complex tasks and improving access to justice. However, their\nadoption is limited by concerns over client confidentiality, especially when\nlawyers include sensitive Personally Identifiable Information (PII) in prompts,\nrisking unauthorized data exposure. To mitigate this, we introduce\nLegalGuardian, a lightweight, privacy-preserving framework tailored for lawyers\nusing LLM-based tools. LegalGuardian employs Named Entity Recognition (NER)\ntechniques and local LLMs to mask and unmask confidential PII within prompts,\nsafeguarding sensitive data before any external interaction. We detail its\ndevelopment and assess its effectiveness using a synthetic prompt library in\nimmigration law scenarios. Comparing traditional NER models with one-shot\nprompted local LLM, we find that LegalGuardian achieves a F1-score of 93% with\nGLiNER and 97% with Qwen2.5-14B in PII detection. Semantic similarity analysis\nconfirms that the framework maintains high fidelity in outputs, ensuring robust\nutility of LLM-based tools. Our findings indicate that legal professionals can\nharness advanced AI technologies without compromising client confidentiality or\nthe quality of legal documents.",
    "updated" : "2025-01-19T01:43:42Z",
    "published" : "2025-01-19T01:43:42Z",
    "authors" : [
      {
        "name" : "M. Mikail Demir"
      },
      {
        "name" : "Hakan T. Otal"
      },
      {
        "name" : "M. Abdullah Canbaz"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.IR",
      "68T50, 68U35",
      "I.2.7; K.5.0; I.7.0"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04323v3",
    "title" : "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models",
    "summary" : "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance.",
    "updated" : "2025-01-20T17:41:59Z",
    "published" : "2025-01-08T07:47:43Z",
    "authors" : [
      {
        "name" : "Haonan Shi"
      },
      {
        "name" : "Tu Ouyang"
      },
      {
        "name" : "An Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.12911v1",
    "title" : "A Selective Homomorphic Encryption Approach for Faster\n  Privacy-Preserving Federated Learning",
    "summary" : "Federated learning is a machine learning method that supports training models\non decentralized devices or servers, where each holds its local data, removing\nthe need for data exchange. This approach is especially useful in healthcare,\nas it enables training on sensitive data without needing to share them. The\nnature of federated learning necessitates robust security precautions due to\ndata leakage concerns during communication. To address this issue, we propose a\nnew approach that employs selective encryption, homomorphic encryption,\ndifferential privacy, and bit-wise scrambling to minimize data leakage while\nachieving good execution performance. Our technique , FAS (fast and secure\nfederated learning) is used to train deep learning models on medical imaging\ndata. We implemented our technique using the Flower framework and compared with\na state-of-the-art federated learning approach that also uses selective\nhomomorphic encryption. Our experiments were run in a cluster of eleven\nphysical machines to create a real-world federated learning scenario on\ndifferent datasets. We observed that our approach is up to 90\\% faster than\napplying fully homomorphic encryption on the model weights. In addition, we can\navoid the pretraining step that is required by our competitor and can save up\nto 20\\% in terms of total execution time. While our approach was faster, it\nobtained similar security results as the competitor.",
    "updated" : "2025-01-22T14:37:44Z",
    "published" : "2025-01-22T14:37:44Z",
    "authors" : [
      {
        "name" : "Abdulkadir Korkmaz"
      },
      {
        "name" : "Praveen Rao"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG",
      "C.2.0; C.2.4; I.2.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.12893v1",
    "title" : "Statistical Privacy",
    "summary" : "To analyze the privacy guarantee of personal data in a database that is\nsubject to queries it is necessary to model the prior knowledge of a possible\nattacker. Differential privacy considers a worst-case scenario where he knows\nalmost everything, which in many applications is unrealistic and requires a\nlarge utility loss.\n  This paper considers a situation called statistical privacy where an\nadversary knows the distribution by which the database is generated, but no\nexact data of all (or sufficient many) of its entries. We analyze in detail how\nthe entropy of the distribution guarantes privacy for a large class of queries\ncalled property queries. Exact formulas are obtained for the privacy\nparameters. We analyze how they depend on the probability that an entry\nfulfills the property under investigation. These formulas turn out to be\nlengthy, but can be used for tight numerical approximations of the privacy\nparameters. Such estimations are necessary for applying privacy enhancing\ntechniques in practice. For this statistical setting we further investigate the\neffect of adding noise or applying subsampling and the privacy utility\ntradeoff. The dependencies on the parameters are illustrated in detail by a\nseries of plots. Finally, these results are compared to the differential\nprivacy model.",
    "updated" : "2025-01-22T14:13:44Z",
    "published" : "2025-01-22T14:13:44Z",
    "authors" : [
      {
        "name" : "Dennis Breutigam"
      },
      {
        "name" : "Rüdiger Reischuk"
      }
    ],
    "categories" : [
      "cs.CR",
      "68P27",
      "E.3; H.1"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.12612v1",
    "title" : "T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in\n  Image Generation",
    "summary" : "Text-to-image (T2I) models have rapidly advanced, enabling the generation of\nhigh-quality images from text prompts across various domains. However, these\nmodels present notable safety concerns, including the risk of generating\nharmful, biased, or private content. Current research on assessing T2I safety\nremains in its early stages. While some efforts have been made to evaluate\nmodels on specific safety dimensions, many critical risks remain unexplored. To\naddress this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I\nmodels across three key domains: toxicity, fairness, and bias. We build a\ndetailed hierarchy of 12 tasks and 44 categories based on these three domains,\nand meticulously collect 70K corresponding prompts. Based on this taxonomy and\nprompt set, we build a large-scale T2I dataset with 68K manually annotated\nimages and train an evaluator capable of detecting critical risks that previous\nwork has failed to identify, including risks that even ultra-large proprietary\nmodels like GPTs cannot correctly detect. We evaluate 12 prominent diffusion\nmodels on T2ISafety and reveal several concerns including persistent issues\nwith racial fairness, a tendency to generate toxic content, and significant\nvariation in privacy protection across the models, even with defense methods\nlike concept erasing. Data and evaluator are released under\nhttps://github.com/adwardlee/t2i_safety.",
    "updated" : "2025-01-22T03:29:43Z",
    "published" : "2025-01-22T03:29:43Z",
    "authors" : [
      {
        "name" : "Lijun Li"
      },
      {
        "name" : "Zhelun Shi"
      },
      {
        "name" : "Xuhao Hu"
      },
      {
        "name" : "Bowen Dong"
      },
      {
        "name" : "Yiran Qin"
      },
      {
        "name" : "Xihui Liu"
      },
      {
        "name" : "Lu Sheng"
      },
      {
        "name" : "Jing Shao"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.12537v1",
    "title" : "Enhancing Privacy in the Early Detection of Sexual Predators Through\n  Federated Learning and Differential Privacy",
    "summary" : "The increased screen time and isolation caused by the COVID-19 pandemic have\nled to a significant surge in cases of online grooming, which is the use of\nstrategies by predators to lure children into sexual exploitation. Previous\nefforts to detect grooming in industry and academia have involved accessing and\nmonitoring private conversations through centrally-trained models or sending\nprivate conversations to a global server. In this work, we implement a\nprivacy-preserving pipeline for the early detection of sexual predators. We\nleverage federated learning and differential privacy in order to create safer\nonline spaces for children while respecting their privacy. We investigate\nvarious privacy-preserving implementations and discuss their benefits and\nshortcomings. Our extensive evaluation using real-world data proves that\nprivacy and utility can coexist with only a slight reduction in utility.",
    "updated" : "2025-01-21T23:01:21Z",
    "published" : "2025-01-21T23:01:21Z",
    "authors" : [
      {
        "name" : "Khaoula Chehbouni"
      },
      {
        "name" : "Martine De Cock"
      },
      {
        "name" : "Gilles Caporossi"
      },
      {
        "name" : "Afaf Taik"
      },
      {
        "name" : "Reihaneh Rabbany"
      },
      {
        "name" : "Golnoosh Farnadi"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.12456v1",
    "title" : "Deploying Privacy Guardrails for LLMs: A Comparative Analysis of\n  Real-World Applications",
    "summary" : "The adoption of Large Language Models (LLMs) has revolutionized AI\napplications but poses significant challenges in safeguarding user privacy.\nEnsuring compliance with privacy regulations such as GDPR and CCPA while\naddressing nuanced privacy risks requires robust and scalable frameworks. This\npaper presents a detailed study of OneShield Privacy Guard, a framework\ndesigned to mitigate privacy risks in user inputs and LLM outputs across\nenterprise and open-source settings. We analyze two real-world deployments:(1)\na multilingual privacy-preserving system integrated with Data and Model\nFactory, focusing on enterprise-scale data governance; and (2) PR Insights, an\nopen-source repository emphasizing automated triaging and community-driven\nrefinements. In Deployment 1, OneShield achieved a 0.95 F1 score in detecting\nsensitive entities like dates, names, and phone numbers across 26 languages,\noutperforming state-of-the-art tool such as StarPII and Presidio by up to 12\\%.\nDeployment 2, with an average F1 score of 0.86, reduced manual effort by over\n300 hours in three months, accurately flagging 8.25\\% of 1,256 pull requests\nfor privacy risks with enhanced context sensitivity. These results demonstrate\nOneShield's adaptability and efficacy in diverse environments, offering\nactionable insights for context-aware entity recognition, automated compliance,\nand ethical AI adoption. This work advances privacy-preserving frameworks,\nsupporting user trust and compliance across operational contexts.",
    "updated" : "2025-01-21T19:04:53Z",
    "published" : "2025-01-21T19:04:53Z",
    "authors" : [
      {
        "name" : "Shubhi Asthana"
      },
      {
        "name" : "Bing Zhang"
      },
      {
        "name" : "Ruchi Mahindru"
      },
      {
        "name" : "Chad DeLuca"
      },
      {
        "name" : "Anna Lisa Gentile"
      },
      {
        "name" : "Sandeep Gopisetty"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.13916v1",
    "title" : "PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy",
    "summary" : "We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),\na communication-efficient Vertical Federated Learning algorithm with\nDifferential Privacy guarantees. PBM-VFL combines Secure Multi-Party\nComputation with the recently introduced Poisson Binomial Mechanism to protect\nparties' private datasets during model training. We define the novel concept of\nfeature privacy and analyze end-to-end feature and sample privacy of our\nalgorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We\nalso provide the first theoretical characterization of the relationship between\nprivacy budget, convergence error, and communication cost in\ndifferentially-private VFL. Finally, we empirically show that our model\nperforms well with high levels of privacy.",
    "updated" : "2025-01-23T18:53:43Z",
    "published" : "2025-01-23T18:53:43Z",
    "authors" : [
      {
        "name" : "Linh Tran"
      },
      {
        "name" : "Timothy Castiglia"
      },
      {
        "name" : "Stacy Patterson"
      },
      {
        "name" : "Ana Milanova"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.13904v1",
    "title" : "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models",
    "summary" : "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank adaptation scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.",
    "updated" : "2025-01-23T18:34:09Z",
    "published" : "2025-01-23T18:34:09Z",
    "authors" : [
      {
        "name" : "Linh Tran"
      },
      {
        "name" : "Wei Sun"
      },
      {
        "name" : "Stacy Patterson"
      },
      {
        "name" : "Ana Milanova"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.13608v1",
    "title" : "AirTOWN: A Privacy-Preserving Mobile App for Real-time Pollution-Aware\n  POI Suggestion",
    "summary" : "This demo paper presents \\airtown, a privacy-preserving mobile application\nthat provides real-time, pollution-aware recommendations for points of interest\n(POIs) in urban environments. By combining real-time Air Quality Index (AQI)\ndata with user preferences, the proposed system aims to help users make\nhealth-conscious decisions about the locations they visit. The application\nutilizes collaborative filtering for personalized suggestions, and federated\nlearning for privacy protection, and integrates AQI data from sensor networks\nin cities such as Bari, Italy, and Cork, UK. In areas with sparse sensor\ncoverage, interpolation techniques approximate AQI values, ensuring broad\napplicability. This system offers a poromsing, health-oriented POI\nrecommendation solution that adapts dynamically to current urban air quality\nconditions while safeguarding user privacy.",
    "updated" : "2025-01-23T12:28:22Z",
    "published" : "2025-01-23T12:28:22Z",
    "authors" : [
      {
        "name" : "Giuseppe Fasano"
      },
      {
        "name" : "Yashar Deldjoo"
      },
      {
        "name" : "Tommaso Di Noia"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.13321v1",
    "title" : "Investigation of the Privacy Concerns in AI Systems for Young Digital\n  Citizens: A Comparative Stakeholder Analysis",
    "summary" : "The integration of Artificial Intelligence (AI) systems into technologies\nused by young digital citizens raises significant privacy concerns. This study\ninvestigates these concerns through a comparative analysis of stakeholder\nperspectives. A total of 252 participants were surveyed, with the analysis\nfocusing on 110 valid responses from parents/educators and 100 from AI\nprofessionals after data cleaning. Quantitative methods, including descriptive\nstatistics and Partial Least Squares Structural Equation Modeling, examined\nfive validated constructs: Data Ownership and Control, Parental Data Sharing,\nPerceived Risks and Benefits, Transparency and Trust, and Education and\nAwareness. Results showed Education and Awareness significantly influenced data\nownership and risk assessment, while Data Ownership and Control strongly\nimpacted Transparency and Trust. Transparency and Trust, along with Perceived\nRisks and Benefits, showed minimal influence on Parental Data Sharing,\nsuggesting other factors may play a larger role. The study underscores the need\nfor user-centric privacy controls, tailored transparency strategies, and\ntargeted educational initiatives. Incorporating diverse stakeholder\nperspectives offers actionable insights into ethical AI design and governance,\nbalancing innovation with robust privacy protections to foster trust in a\ndigital age.",
    "updated" : "2025-01-23T02:07:45Z",
    "published" : "2025-01-23T02:07:45Z",
    "authors" : [
      {
        "name" : "Molly Campbell"
      },
      {
        "name" : "Ankur Barthwal"
      },
      {
        "name" : "Sandhya Joshi"
      },
      {
        "name" : "Austin Shouli"
      },
      {
        "name" : "Ajay Kumar Shrestha"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.13278v1",
    "title" : "On Subset Retrieval and Group Testing Problems with Differential Privacy\n  Constraints",
    "summary" : "This paper focuses on the design and analysis of privacy-preserving\ntechniques for group testing and infection status retrieval. Our work is\nmotivated by the need to provide accurate information on the status of disease\nspread among a group of individuals while protecting the privacy of the\ninfection status of any single individual involved. The paper is motivated by\npractical scenarios, such as controlling the spread of infectious diseases,\nwhere individuals might be reluctant to participate in testing if their\noutcomes are not kept confidential.\n  The paper makes the following contributions. First, we present a differential\nprivacy framework for the subset retrieval problem, which focuses on sharing\nthe infection status of individuals with administrators and decision-makers. We\ncharacterize the trade-off between the accuracy of subset retrieval and the\ndegree of privacy guaranteed to the individuals. In particular, we establish\ntight lower and upper bounds on the achievable level of accuracy subject to the\ndifferential privacy constraints. We then formulate the differential privacy\nframework for the noisy group testing problem in which noise is added either\nbefore or after the pooling process. We establish a reduction between the\nprivate subset retrieval and noisy group testing problems and show that the\nconverse and achievability schemes for subset retrieval carry over to\ndifferentially private group testing.",
    "updated" : "2025-01-23T00:05:16Z",
    "published" : "2025-01-23T00:05:16Z",
    "authors" : [
      {
        "name" : "Mira Gonen"
      },
      {
        "name" : "Michael Langberg"
      },
      {
        "name" : "Alex Sprintson"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.14453v1",
    "title" : "Optimal Strategies for Federated Learning Maintaining Client Privacy",
    "summary" : "Federated Learning (FL) emerged as a learning method to enable the server to\ntrain models over data distributed among various clients. These clients are\nprotective about their data being leaked to the server, any other client, or an\nexternal adversary, and hence, locally train the model and share it with the\nserver rather than sharing the data. The introduction of sophisticated\ninferencing attacks enabled the leakage of information about data through\naccess to model parameters. To tackle this challenge, privacy-preserving\nfederated learning aims to achieve differential privacy through learning\nalgorithms like DP-SGD. However, such methods involve adding noise to the\nmodel, data, or gradients, reducing the model's performance.\n  This work provides a theoretical analysis of the tradeoff between model\nperformance and communication complexity of the FL system. We formally prove\nthat training for one local epoch per global round of training gives optimal\nperformance while preserving the same privacy budget. We also investigate the\nchange of utility (tied to privacy) of FL models with a change in the number of\nclients and observe that when clients are training using DP-SGD and argue that\nfor the same privacy budget, the utility improved with increased clients. We\nvalidate our findings through experiments on real-world datasets. The results\nfrom this paper aim to improve the performance of privacy-preserving federated\nlearning systems.",
    "updated" : "2025-01-24T12:34:38Z",
    "published" : "2025-01-24T12:34:38Z",
    "authors" : [
      {
        "name" : "Uday Bhaskar"
      },
      {
        "name" : "Varul Srivastava"
      },
      {
        "name" : "Avyukta Manjunatha Vummintala"
      },
      {
        "name" : "Naresh Manwani"
      },
      {
        "name" : "Sujit Gujar"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.14313v1",
    "title" : "Between Close Enough to Reveal and Far Enough to Protect: a New Privacy\n  Region for Correlated Data",
    "summary" : "When users make personal privacy choices, correlation between their data can\ncause inadvertent leakage about users who do not want to share their data by\nother users sharing their data. As a solution, we consider local redaction\nmechanisms. As prior works proposed data-independent privatization mechanisms,\nwe study the family of data-independent local redaction mechanisms and\nupper-bound their utility when data correlation is modeled by a stationary\nMarkov process. In contrast, we derive a novel data-dependent mechanism, which\nimproves the utility by leveraging a data-dependent leakage measure.",
    "updated" : "2025-01-24T08:14:54Z",
    "published" : "2025-01-24T08:14:54Z",
    "authors" : [
      {
        "name" : "Luis Maßny"
      },
      {
        "name" : "Rawad Bitar"
      },
      {
        "name" : "Fangwei Ye"
      },
      {
        "name" : "Salim El Rouayheb"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.14309v1",
    "title" : "BrainGuard: Privacy-Preserving Multisubject Image Reconstructions from\n  Brain Activities",
    "summary" : "Reconstructing perceived images from human brain activity forms a crucial\nlink between human and machine learning through Brain-Computer Interfaces.\nEarly methods primarily focused on training separate models for each individual\nto account for individual variability in brain activity, overlooking valuable\ncross-subject commonalities. Recent advancements have explored multisubject\nmethods, but these approaches face significant challenges, particularly in data\nprivacy and effectively managing individual variability. To overcome these\nchallenges, we introduce BrainGuard, a privacy-preserving collaborative\ntraining framework designed to enhance image reconstruction from multisubject\nfMRI data while safeguarding individual privacy. BrainGuard employs a\ncollaborative global-local architecture where individual models are trained on\neach subject's local data and operate in conjunction with a shared global model\nthat captures and leverages cross-subject patterns. This architecture\neliminates the need to aggregate fMRI data across subjects, thereby ensuring\nprivacy preservation. To tackle the complexity of fMRI data, BrainGuard\nintegrates a hybrid synchronization strategy, enabling individual models to\ndynamically incorporate parameters from the global model. By establishing a\nsecure and collaborative training environment, BrainGuard not only protects\nsensitive brain data but also improves the image reconstructions accuracy.\nExtensive experiments demonstrate that BrainGuard sets a new benchmark in both\nhigh-level and low-level metrics, advancing the state-of-the-art in brain\ndecoding through its innovative design.",
    "updated" : "2025-01-24T08:10:47Z",
    "published" : "2025-01-24T08:10:47Z",
    "authors" : [
      {
        "name" : "Zhibo Tian"
      },
      {
        "name" : "Ruijie Quan"
      },
      {
        "name" : "Fan Ma"
      },
      {
        "name" : "Kun Zhan"
      },
      {
        "name" : "Yi Yang"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.14184v1",
    "title" : "Tight Sample Complexity Bounds for Parameter Estimation Under Quantum\n  Differential Privacy for Qubits",
    "summary" : "This short note provides tight upper and lower bounds for minimal number of\nsamples (copies of quantum states) required to attain a prescribed accuracy\n(measured by error variance) for scalar parameters using unbiased estimators\nunder quantum local differential privacy for qubits. In the small privacy\nbudget $\\epsilon$ regime, i.e., $\\epsilon\\ll 1$, the sample complexity scales\nas $\\Theta(\\epsilon^{-2})$. This bound matches that of classical parameter\nestimation under differential privacy. The lower bound loosens (converges to\nzero) in the large privacy budget regime, i.e., $\\epsilon\\gg 1$, but that case\nis not particularly interesting as tight bounds for parameter estimation in the\nnoiseless case are widely known. That being said, extensions to systems with\nhigher dimensions and tightening the bounds for the large privacy budget regime\nare interesting avenues for future research.",
    "updated" : "2025-01-24T02:23:51Z",
    "published" : "2025-01-24T02:23:51Z",
    "authors" : [
      {
        "name" : "Farhad Farokhi"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.14098v1",
    "title" : "Exploring User Perspectives on Data Collection, Data Sharing\n  Preferences, and Privacy Concerns with Remote Healthcare Technology",
    "summary" : "Remote healthcare technology can help tackle societal issues by improving\naccess to quality healthcare services and enhancing diagnoses through in-place\nmonitoring. These services can be implemented through a combination of mobile\ndevices, applications, wearable sensors, and other smart technology. It is\nparamount to handle sensitive data that is collected in ways that meet users'\nprivacy expectations. We surveyed 384 people in Canada aged 20 to 93 years old\nto explore participants' comfort with data collection, sharing preferences, and\npotential privacy concerns related to remote healthcare technology. We explore\nthese topics within the context of various healthcare scenarios including\nhealth emergencies and managing chronic health conditions.",
    "updated" : "2025-01-23T21:09:03Z",
    "published" : "2025-01-23T21:09:03Z",
    "authors" : [
      {
        "name" : "Daniela Napoli"
      },
      {
        "name" : "Heather Molyneaux"
      },
      {
        "name" : "Helene Fournier"
      },
      {
        "name" : "Sonia Chiasson"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.11740v2",
    "title" : "PIR Over Wireless Channels: Achieving Privacy With Public Responses",
    "summary" : "In this paper, we address the problem of Private Information Retrieval (PIR)\nover a public Additive White Gaussian Noise (AWGN) channel. In such a setup,\nthe server's responses are visible to other servers. Thus, a curious server can\nlisten to the other responses, compromising the user's privacy. Indeed,\nprevious works on PIR over a shared medium assumed the servers cannot\ninstantaneously listen to other responses. To address this gap, we present a\nnovel randomized lattice -- PIR coding scheme that jointly codes for privacy,\nchannel noise, and curious servers which may listen to other responses. We\ndemonstrate that a positive PIR rate is achievable even in cases where the\nchannel to the curious server is stronger than the channel to the user.",
    "updated" : "2025-01-24T10:16:16Z",
    "published" : "2025-01-20T20:56:56Z",
    "authors" : [
      {
        "name" : "Or Elimelech"
      },
      {
        "name" : "Asaf Cohen"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.16307v1",
    "title" : "Privacy-aware Nash Equilibrium Synthesis with Partially Ordered LTL$_f$\n  Objectives",
    "summary" : "Nash equilibrium is a fundamental solution concept for modeling the behavior\nof self-interested agents. We develop an algorithm to synthesize pure Nash\nequilibria in two-player deterministic games on graphs where players have\npartial preferences over objectives expressed with linear temporal logic over\nfinite traces. Previous approaches for Nash equilibrium synthesis assume that\nplayers' preferences are common knowledge. Instead, we allow players'\npreferences to remain private but enable communication between players. The\nalgorithm we design synthesizes Nash equilibria for a complete-information\ngame, but synthesizes these equilibria in an incomplete-information setting\nwhere players' preferences are private. The algorithm is privacy-aware, as\ninstead of requiring that players share private preferences, the algorithm\nreduces the information sharing to a query interface. Through this interface,\nplayers exchange information about states in the game from which they can\nenforce a more desirable outcome. We prove the algorithm's completeness by\nshowing that it either returns an equilibrium or certifies that one does not\nexist. We then demonstrate, via numerical examples, the existence of games\nwhere the queries the players exchange are insufficient to reconstruct players'\npreferences, highlighting the privacy-aware nature of the algorithm we propose.",
    "updated" : "2025-01-27T18:46:15Z",
    "published" : "2025-01-27T18:46:15Z",
    "authors" : [
      {
        "name" : "Caleb Probine"
      },
      {
        "name" : "Abhishek Kulkarni"
      },
      {
        "name" : "Ufuk Topcu"
      }
    ],
    "categories" : [
      "cs.GT",
      "cs.LO"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.16033v1",
    "title" : "PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy\n  Assessment",
    "summary" : "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present PRISMe (Privacy Risk Information Scanner for Me), a novel Large\nLanguage Model (LLM)-driven privacy policy assessment tool, which helps users\nto understand the essence of a lengthy, complex privacy policy while browsing.\nThe tool, a browser extension, integrates a dashboard and an LLM chat. One\nmajor contribution is the first rigorous evaluation of such a tool. In a\nmixed-methods user study (N=22), we evaluate PRISMe's efficiency, usability,\nunderstandability of the provided information, and impacts on awareness. While\nour tool improves privacy awareness by providing a comprehensible quick\noverview and a quality chat for in-depth discussion, users note issues with\nconsistency and building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
    "updated" : "2025-01-27T13:27:04Z",
    "published" : "2025-01-27T13:27:04Z",
    "authors" : [
      {
        "name" : "Vincent Freiberger"
      },
      {
        "name" : "Arthur Fleig"
      },
      {
        "name" : "Erik Buchmann"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.AI",
      "H.m; I.2.7"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.15751v1",
    "title" : "A Privacy Model for Classical & Learned Bloom Filters",
    "summary" : "The Classical Bloom Filter (CBF) is a class of Probabilistic Data Structures\n(PDS) for handling Approximate Query Membership (AMQ). The Learned Bloom Filter\n(LBF) is a recently proposed class of PDS that combines the Classical Bloom\nFilter with a Learning Model while preserving the Bloom Filter's one-sided\nerror guarantees. Bloom Filters have been used in settings where inputs are\nsensitive and need to be private in the presence of an adversary with access to\nthe Bloom Filter through an API or in the presence of an adversary who has\naccess to the internal state of the Bloom Filter. Prior work has investigated\nthe privacy of the Classical Bloom Filter providing attacks and defenses under\nvarious privacy definitions. In this work, we formulate a stronger differential\nprivacy-based model for the Bloom Filter. We propose constructions of the\nClassical and Learned Bloom Filter that satisfy $(\\epsilon, 0)$-differential\nprivacy. This is also the first work that analyses and addresses the privacy of\nthe Learned Bloom Filter under any rigorous model, which is an open problem.",
    "updated" : "2025-01-27T03:35:25Z",
    "published" : "2025-01-27T03:35:25Z",
    "authors" : [
      {
        "name" : "Hayder Tirmazi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.15744v1",
    "title" : "Noise disturbance and lack of privacy: Modeling acoustic dissatisfaction\n  in open-plan offices",
    "summary" : "Open-plan offices are well-known to be adversely affected by acoustic issues.\nThis study aims to model acoustic dissatisfaction using measurements of room\nacoustics, sound environment during occupancy, and occupant surveys (n = 349)\nin 28 offices representing a diverse range of workplace parameters. As latent\nfactors, the contribution of $\\textit{lack of privacy}$ (LackPriv) was 25%\nhigher than $\\textit{noise disturbance}$ (NseDstrb) in predicting\n$\\textit{acoustic dissatisfaction}$ (AcDsat). Room acoustic metrics based on\nsound pressure level (SPL) decay of speech ($L_{\\text{p,A,s,4m}}$ and\n$r_{\\text{C}}$) were better in predicting these factors than distraction\ndistance ($r_{\\text{D}}$) based on speech transmission index. This contradicts\nprevious findings, and the trends for SPL-based metrics in predicting AcDsat\nand LackPriv go against expectations based on ISO 3382-3. For sound during\noccupation, $L_{\\text{A,90}}$ and psychoacoustic loudness ($N_{\\text{90}}$)\npredicted AcDsat, and a SPL fluctuation metric ($M_{\\text{A,eq}}$) predicted\nLackPriv. However, these metrics were weaker predictors than ISO 3382-3\nmetrics. Medium-sized offices exhibited higher dissatisfaction than larger\n($\\geq$50 occupants) offices. Dissatisfaction varied substantially across\nparameters including ceiling heights, number of workstations, and years of\nwork, but not between offices with fixed seating compared to more flexible and\nactivity-based working configurations. Overall, these findings highlight the\ncomplexities in characterizing occupants' perceptions using instrumental\nacoustic measurements.",
    "updated" : "2025-01-27T03:10:07Z",
    "published" : "2025-01-27T03:10:07Z",
    "authors" : [
      {
        "name" : "Manuj Yadav"
      },
      {
        "name" : "Jungsoo Kim"
      },
      {
        "name" : "Valtteri Hongisto"
      },
      {
        "name" : "Densil Cabrera"
      },
      {
        "name" : "Richard de Dear"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.15653v1",
    "title" : "A Privacy Enhancing Technique to Evade Detection by Street Video Cameras\n  Without Using Adversarial Accessories",
    "summary" : "In this paper, we propose a privacy-enhancing technique leveraging an\ninherent property of automatic pedestrian detection algorithms, namely, that\nthe training of deep neural network (DNN) based methods is generally performed\nusing curated datasets and laboratory settings, while the operational areas of\nthese methods are dynamic real-world environments. In particular, we leverage a\nnovel side effect of this gap between the laboratory and the real world:\nlocation-based weakness in pedestrian detection. We demonstrate that the\nposition (distance, angle, height) of a person, and ambient light level,\ndirectly impact the confidence of a pedestrian detector when detecting the\nperson. We then demonstrate that this phenomenon is present in pedestrian\ndetectors observing a stationary scene of pedestrian traffic, with blind spot\nareas of weak detection of pedestrians with low confidence. We show how\nprivacy-concerned pedestrians can leverage these blind spots to evade detection\nby constructing a minimum confidence path between two points in a scene,\nreducing the maximum confidence and average confidence of the path by up to\n0.09 and 0.13, respectively, over direct and random paths through the scene. To\ncounter this phenomenon, and force the use of more costly and sophisticated\nmethods to leverage this vulnerability, we propose a novel countermeasure to\nimprove the confidence of pedestrian detectors in blind spots, raising the\nmax/average confidence of paths generated by our technique by 0.09 and 0.05,\nrespectively. In addition, we demonstrate that our countermeasure improves a\nFaster R-CNN-based pedestrian detector's TPR and average true positive\nconfidence by 0.03 and 0.15, respectively.",
    "updated" : "2025-01-26T19:29:49Z",
    "published" : "2025-01-26T19:29:49Z",
    "authors" : [
      {
        "name" : "Jacob Shams"
      },
      {
        "name" : "Ben Nassi"
      },
      {
        "name" : "Satoru Koda"
      },
      {
        "name" : "Asaf Shabtai"
      },
      {
        "name" : "Yuval Elovici"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.15395v1",
    "title" : "Hiding in Plain Sight: An IoT Traffic Camouflage Framework for Enhanced\n  Privacy",
    "summary" : "The rapid growth of Internet of Things (IoT) devices has introduced\nsignificant challenges to privacy, particularly as network traffic analysis\ntechniques evolve. While encryption protects data content, traffic attributes\nsuch as packet size and timing can reveal sensitive information about users and\ndevices. Existing single-technique obfuscation methods, such as packet padding,\noften fall short in dynamic environments like smart homes due to their\npredictability, making them vulnerable to machine learning-based attacks. This\npaper introduces a multi-technique obfuscation framework designed to enhance\nprivacy by disrupting traffic analysis. The framework leverages six\ntechniques-Padding, Padding with XORing, Padding with Shifting, Constant Size\nPadding, Fragmentation, and Delay Randomization-to obscure traffic patterns\neffectively. Evaluations on three public datasets demonstrate significant\nreductions in classifier performance metrics, including accuracy, precision,\nrecall, and F1 score. We assess the framework's robustness against adversarial\ntactics by retraining and fine-tuning neural network classifiers on obfuscated\ntraffic. The results reveal a notable degradation in classifier performance,\nunderscoring the framework's resilience against adaptive attacks. Furthermore,\nwe evaluate communication and system performance, showing that higher\nobfuscation levels enhance privacy but may increase latency and communication\noverhead.",
    "updated" : "2025-01-26T04:33:44Z",
    "published" : "2025-01-26T04:33:44Z",
    "authors" : [
      {
        "name" : "Daniel Adu Worae"
      },
      {
        "name" : "Spyridon Mastorakis"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.15363v1",
    "title" : "AI-Driven Secure Data Sharing: A Trustworthy and Privacy-Preserving\n  Approach",
    "summary" : "In the era of data-driven decision-making, ensuring the privacy and security\nof shared data is paramount across various domains. Applying existing deep\nneural networks (DNNs) to encrypted data is critical and often compromises\nperformance, security, and computational overhead. To address these\nlimitations, this research introduces a secure framework consisting of a\nlearnable encryption method based on the block-pixel operation to encrypt the\ndata and subsequently integrate it with the Vision Transformer (ViT). The\nproposed framework ensures data privacy and security by creating unique\nscrambling patterns per key, providing robust performance against adversarial\nattacks without compromising computational efficiency and data integrity. The\nframework was tested on sensitive medical datasets to validate its efficacy,\nproving its ability to handle highly confidential information securely. The\nsuggested framework was validated with a 94\\% success rate after extensive\ntesting on real-world datasets, such as MRI brain tumors and histological scans\nof lung and colon cancers. Additionally, the framework was tested under diverse\nadversarial attempts against secure data sharing with optimum performance and\ndemonstrated its effectiveness in various threat scenarios. These comprehensive\nanalyses underscore its robustness, making it a trustworthy solution for secure\ndata sharing in critical applications.",
    "updated" : "2025-01-26T02:03:19Z",
    "published" : "2025-01-26T02:03:19Z",
    "authors" : [
      {
        "name" : "Al Amin"
      },
      {
        "name" : "Kamrul Hasan"
      },
      {
        "name" : "Sharif Ullah"
      },
      {
        "name" : "Liang Hong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.15032v1",
    "title" : "Stealthy Voice Eavesdropping with Acoustic Metamaterials: Unraveling a\n  New Privacy Threat",
    "summary" : "We present SuperEar, a novel privacy threat based on acoustic metamaterials.\nUnlike previous research, SuperEar can surreptitiously track and eavesdrop on\nthe phone calls of a moving outdoor target from a safe distance. To design this\nattack, SuperEar overcomes the challenges faced by traditional acoustic\nmetamaterials, including low low-frequency gain and audio distortion during\nreconstruction. It successfully magnifies the speech signal by approximately 20\ntimes, allowing the sound to be captured from the earpiece of the target phone.\nIn addition, SuperEar optimizes the trade-off between the number and size of\nacoustic metamaterials, improving the portability and concealability of the\ninterceptor while ensuring effective interception performance. This makes it\nhighly suitable for outdoor tracking and eavesdropping scenarios. Through\nextensive experimentation, we have evaluated SuperEar and our results show that\nit can achieve an eavesdropping accuracy of over 80% within a range of 4.5\nmeters in the aforementioned scenario, thus validating its great potential in\nreal-world applications.",
    "updated" : "2025-01-25T02:30:03Z",
    "published" : "2025-01-25T02:30:03Z",
    "authors" : [
      {
        "name" : "Zhiyuan Ning"
      },
      {
        "name" : "Zhanyong Tang"
      },
      {
        "name" : "Juan He"
      },
      {
        "name" : "Weizhi Meng"
      },
      {
        "name" : "Yuntian Chen"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CR",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.14974v1",
    "title" : "Private Minimum Hellinger Distance Estimation via Hellinger Distance\n  Differential Privacy",
    "summary" : "Hellinger distance has been widely used to derive objective functions that\nare alternatives to maximum likelihood methods. Motivated by recent regulatory\nprivacy requirements, estimators satisfying differential privacy constraints\nare being derived. In this paper, we describe different notions of privacy\nusing divergences and establish that Hellinger distance minimizes the added\nvariance within the class of power divergences for an additive Gaussian\nmechanism. We demonstrate that a new definition of privacy, namely Hellinger\ndifferential privacy, shares several features of the standard notion of\ndifferential privacy while allowing for sharper inference. Using these\nproperties, we develop private versions of gradient descent and Newton-Raphson\nalgorithms for obtaining private minimum Hellinger distance estimators, which\nare robust and first-order efficient. Using numerical experiments, we\nillustrate the finite sample performance and verify that they retain their\nrobustness properties under gross-error contamination.",
    "updated" : "2025-01-24T23:15:04Z",
    "published" : "2025-01-24T23:15:04Z",
    "authors" : [
      {
        "name" : "Fengnan Deng"
      },
      {
        "name" : "Anand N. Vidyashankar"
      }
    ],
    "categories" : [
      "math.ST",
      "cs.CR",
      "math.PR",
      "stat.ME",
      "stat.ML",
      "stat.TH",
      "62F35, 68P27, 62E20, 60E05"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.14928v1",
    "title" : "Decision Making in Changing Environments: Robustness, Query-Based\n  Learning, and Differential Privacy",
    "summary" : "We study the problem of interactive decision making in which the underlying\nenvironment changes over time subject to given constraints. We propose a\nframework, which we call \\textit{hybrid Decision Making with Structured\nObservations} (hybrid DMSO), that provides an interpolation between the\nstochastic and adversarial settings of decision making. Within this framework,\nwe can analyze local differentially private (LDP) decision making, query-based\nlearning (in particular, SQ learning), and robust and smooth decision making\nunder the same umbrella, deriving upper and lower bounds based on variants of\nthe Decision-Estimation Coefficient (DEC). We further establish strong\nconnections between the DEC's behavior, the SQ dimension, local minimax\ncomplexity, learnability, and joint differential privacy. To showcase the\nframework's power, we provide new results for contextual bandits under the LDP\nconstraint.",
    "updated" : "2025-01-24T21:31:50Z",
    "published" : "2025-01-24T21:31:50Z",
    "authors" : [
      {
        "name" : "Fan Chen"
      },
      {
        "name" : "Alexander Rakhlin"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.17089v1",
    "title" : "CRSet: Non-Interactive Verifiable Credential Revocation with Metadata\n  Privacy for Issuers and Everyone Else",
    "summary" : "Like any digital certificate, Verifiable Credentials (VCs) require a way to\nrevoke them in case of an error or key compromise. Existing solutions for VC\nrevocation, most prominently Bitstring Status List, are not viable for many use\ncases since they leak the issuer's behavior, which in turn leaks internal\nbusiness metrics. For instance, exact staff fluctuation through issuance and\nrevocation of employee IDs. We introduce CRSet, a revocation mechanism that\nallows an issuer to encode revocation information for years worth of VCs as a\nBloom filter cascade. Padding is used to provide deniability for issuer\nmetrics. Issuers periodically publish this filter cascade on a decentralized\nstorage system. Relying Parties (RPs) can download it to perform any number of\nrevocation checks locally. Compared to existing solutions, CRSet protects the\nmetadata of subject, RPs, and issuer equally. At the same time, it is\nnon-interactive, making it work with wallet devices having limited hardware\npower and drop-in compatible with existing VC exchange protocols and wallet\napplications. We present a prototype using the Ethereum blockchain as\ndecentralized storage. The recently introduced blob-carrying transactions,\nenabling cheaper data writes, allow us to write each CRSet directly to the\nchain. We built software for issuers and RPs that we successfully tested\nend-to-end with an existing publicly available wallet agents and the OpenID for\nVerifiable Credentials protocols. Storage and bandwidth costs paid by issuers\nand RP are higher than for Bitstring Status List, but still manageable at\naround 1 MB for an issuer issuing hundreds of thousands of VCs annually and\ncovering decades.",
    "updated" : "2025-01-28T17:23:45Z",
    "published" : "2025-01-28T17:23:45Z",
    "authors" : [
      {
        "name" : "Felix Hoops"
      },
      {
        "name" : "Jonas Gebele"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.16885v1",
    "title" : "\"My Whereabouts, my Location, it's Directly Linked to my Physical\n  Security\": An Exploratory Qualitative Study of Location-Dependent Security\n  and Privacy Perceptions among Activist Tech Users",
    "summary" : "Digital-safety research with at-risk users is particularly urgent. At-risk\nusers are more likely to be digitally attacked or targeted by surveillance and\ncould be disproportionately harmed by attacks that facilitate physical\nassaults. One group of such at-risk users are activists and politically active\nindividuals. For them, as for other at-risk users, the rise of smart\nenvironments harbors new risks. Since digitization and datafication are no\nlonger limited to a series of personal devices that can be switched on and off,\nbut increasingly and continuously surround users, granular geolocation poses\nnew safety challenges. Drawing on eight exploratory qualitative interviews of\nan ongoing research project, this contribution highlights what activists with\npowerful adversaries think about evermore data traces, including location data,\nand how they intend to deal with emerging risks. Responses of activists include\nattempts to control one's immediate technological surroundings and to more\ncarefully manage device-related location data. For some activists, threat\nmodeling has also shaped provider choices based on geopolitical considerations.\nSince many activists have not enough digital-safety knowledge for effective\nprotection, feelings of insecurity and paranoia are widespread. Channeling the\nconcerns and fears of our interlocutors, we call for more research on how\nactivists can protect themselves against evermore fine-grained location data\ntracking.",
    "updated" : "2025-01-28T12:13:53Z",
    "published" : "2025-01-28T12:13:53Z",
    "authors" : [
      {
        "name" : "Christian Eichenmüller"
      },
      {
        "name" : "Lisa Kuhn"
      },
      {
        "name" : "Zinaida Benenson"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.13916v2",
    "title" : "PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy",
    "summary" : "We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),\na communication-efficient Vertical Federated Learning algorithm with\nDifferential Privacy guarantees. PBM-VFL combines Secure Multi-Party\nComputation with the recently introduced Poisson Binomial Mechanism to protect\nparties' private datasets during model training. We define the novel concept of\nfeature privacy and analyze end-to-end feature and sample privacy of our\nalgorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We\nalso provide the first theoretical characterization of the relationship between\nprivacy budget, convergence error, and communication cost in\ndifferentially-private VFL. Finally, we empirically show that our model\nperforms well with high levels of privacy.",
    "updated" : "2025-01-27T19:50:52Z",
    "published" : "2025-01-23T18:53:43Z",
    "authors" : [
      {
        "name" : "Linh Tran"
      },
      {
        "name" : "Timothy Castiglia"
      },
      {
        "name" : "Stacy Patterson"
      },
      {
        "name" : "Ana Milanova"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.13904v2",
    "title" : "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models",
    "summary" : "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank adaptation scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.",
    "updated" : "2025-01-28T15:11:25Z",
    "published" : "2025-01-23T18:34:09Z",
    "authors" : [
      {
        "name" : "Linh Tran"
      },
      {
        "name" : "Wei Sun"
      },
      {
        "name" : "Stacy Patterson"
      },
      {
        "name" : "Ana Milanova"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.17762v1",
    "title" : "Improving Privacy Benefits of Redaction",
    "summary" : "We propose a novel redaction methodology that can be used to sanitize natural\ntext data. Our new technique provides better privacy benefits than other state\nof the art techniques while maintaining lower redaction levels.",
    "updated" : "2025-01-29T16:53:16Z",
    "published" : "2025-01-29T16:53:16Z",
    "authors" : [
      {
        "name" : "Vaibhav Gusain"
      },
      {
        "name" : "Douglas Leith"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.17750v1",
    "title" : "Privacy Audit as Bits Transmission: (Im)possibilities for Audit by One\n  Run",
    "summary" : "Auditing algorithms' privacy typically involves simulating a game-based\nprotocol that guesses which of two adjacent datasets was the original input.\nTraditional approaches require thousands of such simulations, leading to\nsignificant computational overhead. Recent methods propose single-run auditing\nof the target algorithm to address this, substantially reducing computational\ncost. However, these methods' general applicability and tightness in producing\nempirical privacy guarantees remain uncertain.\n  This work studies such problems in detail. Our contributions are twofold:\nFirst, we introduce a unifying framework for privacy audits based on\ninformation-theoretic principles, modeling the audit as a bit transmission\nproblem in a noisy channel. This formulation allows us to derive fundamental\nlimits and develop an audit approach that yields tight privacy lower bounds for\nvarious DP protocols. Second, leveraging this framework, we demystify the\nmethod of privacy audit by one run, identifying the conditions under which\nsingle-run audits are feasible or infeasible. Our analysis provides general\nguidelines for conducting privacy audits and offers deeper insights into the\nprivacy audit.\n  Finally, through experiments, we demonstrate that our approach produces\ntighter privacy lower bounds on common differentially private mechanisms while\nrequiring significantly fewer observations. We also provide a case study\nillustrating that our method successfully detects privacy violations in flawed\nimplementations of private algorithms.",
    "updated" : "2025-01-29T16:38:51Z",
    "published" : "2025-01-29T16:38:51Z",
    "authors" : [
      {
        "name" : "Zihang Xiang"
      },
      {
        "name" : "Tianhao Wang"
      },
      {
        "name" : "Di Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.17634v1",
    "title" : "Federated Learning With Individualized Privacy Through Client Sampling",
    "summary" : "With growing concerns about user data collection, individualized privacy has\nemerged as a promising solution to balance protection and utility by accounting\nfor diverse user privacy preferences. Instead of enforcing a uniform level of\nanonymization for all users, this approach allows individuals to choose privacy\nsettings that align with their comfort levels. Building on this idea, we\npropose an adapted method for enabling Individualized Differential Privacy\n(IDP) in Federated Learning (FL) by handling clients according to their\npersonal privacy preferences. By extending the SAMPLE algorithm from\ncentralized settings to FL, we calculate client-specific sampling rates based\non their heterogeneous privacy budgets and integrate them into a modified\nIDP-FedAvg algorithm. We test this method under realistic privacy distributions\nand multiple datasets. The experimental results demonstrate that our approach\nachieves clear improvements over uniform DP baselines, reducing the trade-off\nbetween privacy and utility. Compared to the alternative SCALE method in\nrelated work, which assigns differing noise scales to clients, our method\nperforms notably better. However, challenges remain for complex tasks with\nnon-i.i.d. data, primarily stemming from the constraints of the decentralized\nsetting.",
    "updated" : "2025-01-29T13:11:21Z",
    "published" : "2025-01-29T13:11:21Z",
    "authors" : [
      {
        "name" : "Lucas Lange"
      },
      {
        "name" : "Ole Borchardt"
      },
      {
        "name" : "Erhard Rahm"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.14184v2",
    "title" : "Tight Sample Complexity Bounds for Parameter Estimation Under Quantum\n  Differential Privacy for Qubits",
    "summary" : "This short note provides tight upper and lower bounds for minimal number of\nsamples (copies of quantum states) required to attain a prescribed accuracy\n(measured by error variance) for scalar parameters using unbiased estimators\nunder quantum local differential privacy for qubits. In the small privacy\nbudget $\\epsilon$ regime, i.e., $\\epsilon\\ll 1$, the sample complexity scales\nas $\\Theta(\\epsilon^{-2})$. This bound matches that of classical parameter\nestimation under differential privacy. The lower bound loosens (converges to\nzero) in the large privacy budget regime, i.e., $\\epsilon\\gg 1$, but that case\nis not particularly interesting as tight bounds for parameter estimation in the\nnoiseless case are widely known. That being said, extensions to systems with\nhigher dimensions and tightening the bounds for the large privacy budget regime\nare interesting avenues for future research.",
    "updated" : "2025-01-28T22:32:09Z",
    "published" : "2025-01-24T02:23:51Z",
    "authors" : [
      {
        "name" : "Farhad Farokhi"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.18174v1",
    "title" : "Advancing Personalized Federated Learning: Integrative Approaches with\n  AI for Enhanced Privacy and Customization",
    "summary" : "In the age of data-driven decision making, preserving privacy while providing\npersonalized experiences has become paramount. Personalized Federated Learning\n(PFL) offers a promising framework by decentralizing the learning process, thus\nensuring data privacy and reducing reliance on centralized data repositories.\nHowever, the integration of advanced Artificial Intelligence (AI) techniques\nwithin PFL remains underexplored. This paper proposes a novel approach that\nenhances PFL with cutting-edge AI methodologies including adaptive\noptimization, transfer learning, and differential privacy. We present a model\nthat not only boosts the performance of individual client models but also\nensures robust privacy-preserving mechanisms and efficient resource utilization\nacross heterogeneous networks. Empirical results demonstrate significant\nimprovements in model accuracy and personalization, along with stringent\nprivacy adherence, as compared to conventional federated learning models. This\nwork paves the way for a new era of truly personalized and privacy-conscious AI\nsystems, offering significant implications for industries requiring compliance\nwith stringent data protection regulations.",
    "updated" : "2025-01-30T07:03:29Z",
    "published" : "2025-01-30T07:03:29Z",
    "authors" : [
      {
        "name" : "Kevin Cooper"
      },
      {
        "name" : "Michael Geller"
      }
    ],
    "categories" : [
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.17762v2",
    "title" : "Improving Privacy Benefits of Redaction",
    "summary" : "We propose a novel redaction methodology that can be used to sanitize natural\ntext data. Our new technique provides better privacy benefits than other state\nof the art techniques while maintaining lower redaction levels.",
    "updated" : "2025-01-30T05:26:05Z",
    "published" : "2025-01-29T16:53:16Z",
    "authors" : [
      {
        "name" : "Vaibhav Gusain"
      },
      {
        "name" : "Douglas Leith"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.19223v1",
    "title" : "Through the Looking Glass: LLM-Based Analysis of AR/VR Android\n  Applications Privacy Policies",
    "summary" : "\\begin{abstract} This paper comprehensively analyzes privacy policies in\nAR/VR applications, leveraging BERT, a state-of-the-art text classification\nmodel, to evaluate the clarity and thoroughness of these policies. By comparing\nthe privacy policies of AR/VR applications with those of free and premium\nwebsites, this study provides a broad perspective on the current state of\nprivacy practices within the AR/VR industry. Our findings indicate that AR/VR\napplications generally offer a higher percentage of positive segments than free\ncontent but lower than premium websites. The analysis of highlighted segments\nand words revealed that AR/VR applications strategically emphasize critical\nprivacy practices and key terms. This enhances privacy policies' clarity and\neffectiveness.",
    "updated" : "2025-01-31T15:30:14Z",
    "published" : "2025-01-31T15:30:14Z",
    "authors" : [
      {
        "name" : "Abdulaziz Alghamdi"
      },
      {
        "name" : "David Mohaisen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.18862v1",
    "title" : "Scalable Distributed Reproduction Numbers of Network Epidemics with\n  Differential Privacy",
    "summary" : "Reproduction numbers are widely used for the estimation and prediction of\nepidemic spreading processes over networks. However, conventional reproduction\nnumbers of an overall network do not indicate where an epidemic is spreading.\nTherefore, we propose a novel notion of local distributed reproduction numbers\nto capture the spreading behaviors of each node in a network. We first show how\nto compute them and then use them to derive new conditions under which an\noutbreak can occur. These conditions are then used to derive new conditions for\nthe existence, uniqueness, and stability of equilibrium states of the\nunderlying epidemic model. Building upon these local distributed reproduction\nnumbers, we define cluster distributed reproduction numbers to model the spread\nbetween clusters composed of nodes. Furthermore, we demonstrate that the local\ndistributed reproduction numbers can be aggregated into cluster distributed\nreproduction numbers at different scales. However, both local and cluster\ndistributed reproduction numbers can reveal the frequency of interactions\nbetween nodes in a network, which raises privacy concerns. Thus, we next\ndevelop a privacy framework that implements a differential privacy mechanism to\nprovably protect the frequency of interactions between nodes when computing\ndistributed reproduction numbers. Numerical experiments show that, even under\ndifferential privacy, the distributed reproduction numbers provide accurate\nestimates of the epidemic spread while also providing more insights than\nconventional reproduction numbers.",
    "updated" : "2025-01-31T03:08:57Z",
    "published" : "2025-01-31T03:08:57Z",
    "authors" : [
      {
        "name" : "Bo Chen"
      },
      {
        "name" : "Baike She"
      },
      {
        "name" : "Calvin Hawkins"
      },
      {
        "name" : "Philip E. Paré"
      },
      {
        "name" : "Matthew T. Hale"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.18727v1",
    "title" : "Exploring Audio Editing Features as User-Centric Privacy Defenses\n  Against Emotion Inference Attacks",
    "summary" : "The rapid proliferation of speech-enabled technologies, including virtual\nassistants, video conferencing platforms, and wearable devices, has raised\nsignificant privacy concerns, particularly regarding the inference of sensitive\nemotional information from audio data. Existing privacy-preserving methods\noften compromise usability and security, limiting their adoption in practical\nscenarios. This paper introduces a novel, user-centric approach that leverages\nfamiliar audio editing techniques, specifically pitch and tempo manipulation,\nto protect emotional privacy without sacrificing usability. By analyzing\npopular audio editing applications on Android and iOS platforms, we identified\nthese features as both widely available and usable. We rigorously evaluated\ntheir effectiveness against a threat model, considering adversarial attacks\nfrom diverse sources, including Deep Neural Networks (DNNs), Large Language\nModels (LLMs), and and reversibility testing. Our experiments, conducted on\nthree distinct datasets, demonstrate that pitch and tempo manipulation\neffectively obfuscates emotional data. Additionally, we explore the design\nprinciples for lightweight, on-device implementation to ensure broad\napplicability across various devices and platforms.",
    "updated" : "2025-01-30T20:07:44Z",
    "published" : "2025-01-30T20:07:44Z",
    "authors" : [
      {
        "name" : "Mohd. Farhan Israk Soumik"
      },
      {
        "name" : "W. K. M. Mithsara"
      },
      {
        "name" : "Abdur R. Shahid"
      },
      {
        "name" : "Ahmed Imteaj"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.18625v1",
    "title" : "DUEF-GA: Data Utility and Privacy Evaluation Framework for Graph\n  Anonymization",
    "summary" : "Anonymization of graph-based data is a problem which has been widely studied\nover the last years and several anonymization methods have been developed.\nInformation loss measures have been used to evaluate data utility and\ninformation loss in the anonymized graphs. However, there is no consensus about\nhow to evaluate data utility and information loss in privacy-preserving and\nanonymization scenarios, where the anonymous datasets were perturbed to hinder\nre-identification processes. Authors use diverse metrics to evaluate data\nutility and, consequently, it is complex to compare different methods or\nalgorithms in literature. In this paper we propose a framework to evaluate and\ncompare anonymous datasets in a common way, providing an objective score to\nclearly compare methods and algorithms. Our framework includes metrics based on\ngeneric information loss measures, such as average distance or betweenness\ncentrality, and also task-specific information loss measures, such as community\ndetection or information flow. Additionally, we provide some metrics to examine\nre-identification and risk assessment. We demonstrate that our framework could\nhelp researchers and practitioners to select the best parametrization and/or\nalgorithm to reduce information loss and maximize data utility.",
    "updated" : "2025-01-27T12:22:40Z",
    "published" : "2025-01-27T12:22:40Z",
    "authors" : [
      {
        "name" : "Jordi Casas-Roma"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.00192v1",
    "title" : "PRECISE: PRivacy-loss-Efficient and Consistent Inference based on\n  poSterior quantilEs",
    "summary" : "Differential privacy (DP) is a mathematical framework for releasing\ninformation with formal privacy guarantees. Despite the existence of various DP\nprocedures for performing a wide range of statistical analysis and machine\nlearning tasks, methods of good utility are still lacking in valid statistical\ninference with DP guarantees. We address this gap by introducing the notion of\nvalid Privacy-Preserving Interval Estimation (PPIE) and proposing\nPRivacy-loss-Efficient and Consistent Inference based on poSterior quantilEs\n(PRECISE). PRECISE is a general-purpose Bayesian approach for constructing\nprivacy-preserving posterior intervals. We establish the Mean-Squared-Error\n(MSE) consistency for our proposed private posterior quantiles converging to\nthe population posterior quantile as sample size or privacy loss increases. We\nconduct extensive experiments to compare the utilities of PRECISE with common\nexisting privacy-preserving inferential approaches in various inferential\ntasks, data types and sizes,and privacy loss levels. The results demonstrated a\nsignificant advantage of PRECISE with its nominal coverage and substantially\nnarrower intervals than the existing methods, which are prone to either\nunder-coverage or impractically wide intervals.",
    "updated" : "2025-01-31T22:18:59Z",
    "published" : "2025-01-31T22:18:59Z",
    "authors" : [
      {
        "name" : "Ruyu Zhou"
      },
      {
        "name" : "Fang Liu"
      }
    ],
    "categories" : [
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.00068v1",
    "title" : "Privacy Preserving Charge Location Prediction for Electric Vehicles",
    "summary" : "By 2050, electric vehicles (EVs) are projected to account for 70% of global\nvehicle sales. While EVs provide environmental benefits, they also pose\nchallenges for energy generation, grid infrastructure, and data privacy.\nCurrent research on EV routing and charge management often overlooks privacy\nwhen predicting energy demands, leaving sensitive mobility data vulnerable. To\naddress this, we developed a Federated Learning Transformer Network (FLTN) to\npredict EVs' next charge location with enhanced privacy measures. Each EV\noperates as a client, training an onboard FLTN model that shares only model\nweights, not raw data with a community-based Distributed Energy Resource\nManagement System (DERMS), which aggregates them into a community global model.\nTo further enhance privacy, non-transitory EVs use peer-to-peer weight sharing\nand augmentation within their community, obfuscating individual contributions\nand improving model accuracy. Community DERMS global model weights are then\nredistributed to EVs for continuous training. Our FLTN approach achieved up to\n92% accuracy while preserving data privacy, compared to our baseline\ncentralised model, which achieved 98% accuracy with no data privacy.\nSimulations conducted across diverse charge levels confirm the FLTN's ability\nto forecast energy demands over extended periods. We present a privacy-focused\nsolution for forecasting EV charge location prediction, effectively mitigating\ndata leakage risks.",
    "updated" : "2025-01-31T03:14:36Z",
    "published" : "2025-01-31T03:14:36Z",
    "authors" : [
      {
        "name" : "Robert Marlin"
      },
      {
        "name" : "Raja Jurdak"
      },
      {
        "name" : "Alsharif Abuadbba"
      },
      {
        "name" : "Dimity Miller"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "I.6.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.18862v2",
    "title" : "Scalable Distributed Reproduction Numbers of Network Epidemics with\n  Differential Privacy",
    "summary" : "Reproduction numbers are widely used for the estimation and prediction of\nepidemic spreading processes over networks. However, conventional reproduction\nnumbers of an overall network do not indicate where an epidemic is spreading.\nTherefore, we propose a novel notion of local distributed reproduction numbers\nto capture the spreading behaviors of each node in a network. We first show how\nto compute them and then use them to derive new conditions under which an\noutbreak can occur. These conditions are then used to derive new conditions for\nthe existence, uniqueness, and stability of equilibrium states of the\nunderlying epidemic model. Building upon these local distributed reproduction\nnumbers, we define cluster distributed reproduction numbers to model the spread\nbetween clusters composed of nodes. Furthermore, we demonstrate that the local\ndistributed reproduction numbers can be aggregated into cluster distributed\nreproduction numbers at different scales. However, both local and cluster\ndistributed reproduction numbers can reveal the frequency of interactions\nbetween nodes in a network, which raises privacy concerns. Thus, we next\ndevelop a privacy framework that implements a differential privacy mechanism to\nprovably protect the frequency of interactions between nodes when computing\ndistributed reproduction numbers. Numerical experiments show that, even under\ndifferential privacy, the distributed reproduction numbers provide accurate\nestimates of the epidemic spread while also providing more insights than\nconventional reproduction numbers.",
    "updated" : "2025-02-04T02:16:21Z",
    "published" : "2025-01-31T03:08:57Z",
    "authors" : [
      {
        "name" : "Bo Chen"
      },
      {
        "name" : "Baike She"
      },
      {
        "name" : "Calvin Hawkins"
      },
      {
        "name" : "Philip E. Paré"
      },
      {
        "name" : "Matthew T. Hale"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.00067v1",
    "title" : "Decoding User Concerns in AI Health Chatbots: An Exploration of Security\n  and Privacy in App Reviews",
    "summary" : "AI powered health chatbot applications are increasingly utilized for\npersonalized healthcare services, yet they pose significant challenges related\nto user data security and privacy. This study evaluates the effectiveness of\nautomated methods, specifically BART and Gemini GenAI, in identifying security\nprivacy related (SPR) concerns within these applications' user reviews,\nbenchmarking their performance against manual qualitative analysis. Our results\nindicate that while Gemini's performance in SPR classification is comparable to\nmanual labeling, both automated methods have limitations, including the\nmisclassification of unrelated issues. Qualitative analysis revealed critical\nuser concerns, such as data collection practices, data misuse, and insufficient\ntransparency and consent mechanisms. This research enhances the understanding\nof the relationship between user trust, privacy, and emerging mobile AI health\nchatbot technologies, offering actionable insights for improving security and\nprivacy practices in AI driven health chatbots. Although exploratory, our\nfindings highlight the necessity for rigorous audits and transparent\ncommunication strategies, providing valuable guidance for app developers and\nvendors in addressing user security and privacy concerns.",
    "updated" : "2025-01-31T00:38:37Z",
    "published" : "2025-01-31T00:38:37Z",
    "authors" : [
      {
        "name" : "Muhammad Hassan"
      },
      {
        "name" : "Abdullah Ghani"
      },
      {
        "name" : "Muhammad Fareed Zaffar"
      },
      {
        "name" : "Masooda Bashir"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.ET",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2502.01649v1",
    "title" : "Privacy-Preserving Edge Speech Understanding with Tiny Foundation Models",
    "summary" : "Robust speech recognition systems rely on cloud service providers for\ninference. It needs to ensure that an untrustworthy provider cannot deduce the\nsensitive content in speech. Sanitization can be done on speech content keeping\nin mind that it has to avoid compromising transcription accuracy. Realizing the\nunder utilized capabilities of tiny speech foundation models (FMs), for the\nfirst time, we propose a novel use: enhancing speech privacy on\nresource-constrained devices. We introduce XYZ, an edge/cloud privacy\npreserving speech inference engine that can filter sensitive entities without\ncompromising transcript accuracy. We utilize a timestamp based on-device\nmasking approach that utilizes a token to entity prediction model to filter\nsensitive entities. Our choice of mask strategically conceals parts of the\ninput and hides sensitive data. The masked input is sent to a trusted cloud\nservice or to a local hub to generate the masked output. The effectiveness of\nXYZ hinges on how well the entity time segments are masked. Our recovery is a\nconfidence score based approach that chooses the best prediction between cloud\nand on-device model. We implement XYZ on a 64 bit Raspberry Pi 4B. Experiments\nshow that our solution leads to robust speech recognition without forsaking\nprivacy. XYZ with < 100 MB memory, achieves state-of-the-art (SOTA) speech\ntranscription performance while filtering about 83% of private entities\ndirectly on-device. XYZ is 16x smaller in memory and 17x more compute efficient\nthan prior privacy preserving speech frameworks and has a relative reduction in\nword error rate (WER) by 38.8-77.5% when compared to existing offline\ntranscription services.",
    "updated" : "2025-01-29T18:55:42Z",
    "published" : "2025-01-29T18:55:42Z",
    "authors" : [
      {
        "name" : "Afsara Benazir"
      },
      {
        "name" : "Felix Xiaozhu Lin"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04409v2",
    "title" : "Lossless Privacy-Preserving Aggregation for Decentralized Federated\n  Learning",
    "summary" : "Privacy concerns arise as sensitive data proliferate. Despite decentralized\nfederated learning (DFL) aggregating gradients from neighbors to avoid direct\ndata transmission, it still poses indirect data leaks from the transmitted\ngradients. Existing privacy-preserving methods for DFL add noise to gradients.\nThey either diminish the model predictive accuracy or suffer from ineffective\ngradient protection. In this paper, we propose a novel lossless\nprivacy-preserving aggregation rule named LPPA to enhance gradient protection\nas much as possible but without loss of DFL model predictive accuracy. LPPA\nsubtly injects the noise difference between the sent and received noise into\ntransmitted gradients for gradient protection. The noise difference\nincorporates neighbors' randomness for each client, effectively safeguarding\nagainst data leaks. LPPA employs the noise flow conservation theory to ensure\nthat the noise impact can be globally eliminated. The global sum of all noise\ndifferences remains zero, ensuring that accurate gradient aggregation is\nunaffected and the model accuracy remains intact. We theoretically prove that\nthe privacy-preserving capacity of LPPA is \\sqrt{2} times greater than that of\nnoise addition, while maintaining comparable model accuracy to the standard DFL\naggregation without noise injection. Experimental results verify the\ntheoretical findings and show that LPPA achieves a 14% mean improvement in\naccuracy over noise addition. We also demonstrate the effectiveness of LPPA in\nprotecting raw data and guaranteeing lossless model accuracy.",
    "updated" : "2025-02-04T05:00:55Z",
    "published" : "2025-01-08T10:49:06Z",
    "authors" : [
      {
        "name" : "Xiaoye Miao"
      },
      {
        "name" : "Bin Li"
      },
      {
        "name" : " Yanzhang"
      },
      {
        "name" : "Xinkui Zhao"
      },
      {
        "name" : "Yangyang Wu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  }
]