[
  {
    "id" : "http://arxiv.org/abs/2501.01353v1",
    "title" : "Privacy Preservation in MIMO-OFDM Localization Systems: A Beamforming\n  Approach",
    "summary" : "We investigate an uplink MIMO-OFDM localization scenario where a legitimate\nbase station (BS) aims to localize a user equipment (UE) using pilot signals\ntransmitted by the UE, while an unauthorized BS attempts to localize the UE by\neavesdropping on these pilots, posing a risk to the UE's location privacy. To\nenhance legitimate localization performance while protecting the UE's privacy,\nwe formulate an optimization problem regarding the beamformers at the UE,\naiming to minimize the Cram\\'er-Rao bound (CRB) for legitimate localization\nwhile constraining the CRB for unauthorized localization above a threshold. A\npenalty dual decomposition optimization framework is employed to solve the\nproblem, leading to a novel beamforming approach for location privacy\npreservation. Numerical results confirm the effectiveness of the proposed\napproach and demonstrate its superiority over existing benchmarks.",
    "updated" : "2025-01-02T17:08:15Z",
    "published" : "2025-01-02T17:08:15Z",
    "authors" : [
      {
        "name" : "Yuchen Zhang"
      },
      {
        "name" : "Hui Chen"
      },
      {
        "name" : "Musa Furkan Keskin"
      },
      {
        "name" : "Alireza Pourafzal"
      },
      {
        "name" : "Pinjun Zheng"
      },
      {
        "name" : "Henk Wymeersch"
      },
      {
        "name" : "Tareq Y. Al-Naffouri"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01131v1",
    "title" : "Privacy Bills of Materials: A Transparent Privacy Information Inventory\n  for Collaborative Privacy Notice Generation in Mobile App Development",
    "summary" : "Privacy regulations mandate that developers must provide authentic and\ncomprehensive privacy notices, e.g., privacy policies or labels, to inform\nusers of their apps' privacy practices. However, due to a lack of knowledge of\nprivacy requirements, developers often struggle to create accurate privacy\nnotices, especially for sophisticated mobile apps with complex features and in\ncrowded development teams. To address these challenges, we introduce Privacy\nBills of Materials (PriBOM), a systematic software engineering approach that\nleverages different development team roles to better capture and coordinate\nmobile app privacy information. PriBOM facilitates transparency-centric privacy\ndocumentation and specific privacy notice creation, enabling traceability and\ntrackability of privacy practices. We present a pre-fill of PriBOM based on\nstatic analysis and privacy notice analysis techniques. We demonstrate the\nperceived usefulness of PriBOM through a human evaluation with 150 diverse\nparticipants. Our findings suggest that PriBOM could serve as a significant\nsolution for providing privacy support in DevOps for mobile apps.",
    "updated" : "2025-01-02T08:14:52Z",
    "published" : "2025-01-02T08:14:52Z",
    "authors" : [
      {
        "name" : "Zhen Tao"
      },
      {
        "name" : "Shidong Pan"
      },
      {
        "name" : "Zhenchang Xing"
      },
      {
        "name" : "Xiaoyu Sun"
      },
      {
        "name" : "Omar Haggag"
      },
      {
        "name" : "John Grundy"
      },
      {
        "name" : "Ze Shi Li"
      },
      {
        "name" : "Jingjie Li"
      },
      {
        "name" : "Liming Zhu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01063v1",
    "title" : "FAPL-DM-BC: A Secure and Scalable FL Framework with Adaptive Privacy and\n  Dynamic Masking, Blockchain, and XAI for the IoVs",
    "summary" : "The FAPL-DM-BC solution is a new FL-based privacy, security, and scalability\nsolution for the Internet of Vehicles (IoV). It leverages Federated Adaptive\nPrivacy-Aware Learning (FAPL) and Dynamic Masking (DM) to learn and adaptively\nchange privacy policies in response to changing data sensitivity and state in\nreal-time, for the optimal privacy-utility tradeoff. Secure Logging and\nVerification, Blockchain-based provenance and decentralized validation, and\nCloud Microservices Secure Aggregation using FedAvg (Federated Averaging) and\nSecure Multi-Party Computation (SMPC). Two-model feedback, driven by\nModel-Agnostic Explainable AI (XAI), certifies local predictions and\nexplanations to drive it to the next level of efficiency. Combining local\nfeedback with world knowledge through a weighted mean computation, FAPL-DM-BC\nassures federated learning that is secure, scalable, and interpretable.\nSelf-driving cars, traffic management, and forecasting, vehicular network\ncybersecurity in real-time, and smart cities are a few possible applications of\nthis integrated, privacy-safe, and high-performance IoV platform.",
    "updated" : "2025-01-02T05:21:52Z",
    "published" : "2025-01-02T05:21:52Z",
    "authors" : [
      {
        "name" : "Sathwik Narkedimilli"
      },
      {
        "name" : "Amballa Venkata Sriram"
      },
      {
        "name" : "Sujith Makam"
      },
      {
        "name" : "MSVPJ Sathvik"
      },
      {
        "name" : "Sai Prashanth Mallellu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.00824v1",
    "title" : "Information Sifting Funnel: Privacy-preserving Collaborative Inference\n  Against Model Inversion Attacks",
    "summary" : "The complexity of neural networks and inference tasks, coupled with demands\nfor computational efficiency and real-time feedback, poses significant\nchallenges for resource-constrained edge devices. Collaborative inference\nmitigates this by assigning shallow feature extraction to edge devices and\noffloading features to the cloud for further inference, reducing computational\nload. However, transmitted features remain susceptible to model inversion\nattacks (MIAs), which can reconstruct original input data. Current defenses,\nsuch as perturbation and information bottleneck techniques, offer explainable\nprotection but face limitations, including the lack of standardized criteria\nfor assessing MIA difficulty, challenges in mutual information estimation, and\ntrade-offs among usability, privacy, and deployability.\n  To address these challenges, we introduce the first criterion to evaluate MIA\ndifficulty in collaborative inference, supported by theoretical analysis of\nexisting attacks and defenses, validated using experiments with the Mutual\nInformation Neural Estimator (MINE). Based on these findings, we propose\nSiftFunnel, a privacy-preserving framework for collaborative inference. The\nedge model is trained with linear and non-linear correlation constraints to\nreduce redundant information in transmitted features, enhancing privacy\nprotection. Label smoothing and a cloud-based upsampling module are added to\nbalance usability and privacy. To improve deployability, the edge model\nincorporates a funnel-shaped structure and attention mechanisms, preserving\nboth privacy and usability. Extensive experiments demonstrate that SiftFunnel\noutperforms state-of-the-art defenses against MIAs, achieving superior privacy\nprotection with less than 3% accuracy loss and striking an optimal balance\namong usability, privacy, and practicality.",
    "updated" : "2025-01-01T13:00:01Z",
    "published" : "2025-01-01T13:00:01Z",
    "authors" : [
      {
        "name" : "Rongke Liu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01786v1",
    "title" : "Advancing privacy in learning analytics using differential privacy",
    "summary" : "This paper addresses the challenge of balancing learner data privacy with the\nuse of data in learning analytics (LA) by proposing a novel framework by\napplying Differential Privacy (DP). The need for more robust privacy protection\nkeeps increasing, driven by evolving legal regulations and heightened privacy\nconcerns, as well as traditional anonymization methods being insufficient for\nthe complexities of educational data. To address this, we introduce the first\nDP framework specifically designed for LA and provide practical guidance for\nits implementation. We demonstrate the use of this framework through a LA usage\nscenario and validate DP in safeguarding data privacy against potential attacks\nthrough an experiment on a well-known LA dataset. Additionally, we explore the\ntrade-offs between data privacy and utility across various DP settings. Our\nwork contributes to the field of LA by offering a practical DP framework that\ncan support researchers and practitioners in adopting DP in their works.",
    "updated" : "2025-01-03T12:36:11Z",
    "published" : "2025-01-03T12:36:11Z",
    "authors" : [
      {
        "name" : "Qinyi Liu"
      },
      {
        "name" : "Ronas Shakya"
      },
      {
        "name" : "Mohammad Khalil"
      },
      {
        "name" : "Jelena Jovanovic"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01639v1",
    "title" : "Artificial Intelligent Implications on Health Data Privacy and\n  Confidentiality",
    "summary" : "The rapid integration of artificial intelligence (AI) in healthcare is\nrevolutionizing medical diagnostics, personalized medicine, and operational\nefficiency. However, alongside these advancements, significant challenges arise\nconcerning patient data privacy, ethical considerations, and regulatory\ncompliance. This paper examines the dual impact of AI on healthcare,\nhighlighting its transformative potential and the critical need for\nsafeguarding sensitive health information. It explores the role of the Health\nInsurance Portability and Accountability Act (HIPAA) as a regulatory framework\nfor ensuring data privacy and security, emphasizing the importance of robust\nsafeguards and ethical standards in AI-driven healthcare. Through case studies,\nincluding AI applications in diabetic retinopathy, oncology, and the\ncontroversies surrounding data sharing, this study underscores the ethical and\nlegal complexities of AI implementation. A balanced approach that fosters\ninnovation while maintaining patient trust and privacy is imperative. The\nfindings emphasize the importance of continuous education, transparency, and\nadherence to regulatory frameworks to harness AI's full potential responsibly\nand ethically in healthcare.",
    "updated" : "2025-01-03T05:17:23Z",
    "published" : "2025-01-03T05:17:23Z",
    "authors" : [
      {
        "name" : "Ahmad Momani"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03222v1",
    "title" : "Characterizing the Accuracy-Communication-Privacy Trade-off in\n  Distributed Stochastic Convex Optimization",
    "summary" : "We consider the problem of differentially private stochastic convex\noptimization (DP-SCO) in a distributed setting with $M$ clients, where each of\nthem has a local dataset of $N$ i.i.d. data samples from an underlying data\ndistribution. The objective is to design an algorithm to minimize a convex\npopulation loss using a collaborative effort across $M$ clients, while ensuring\nthe privacy of the local datasets. In this work, we investigate the\naccuracy-communication-privacy trade-off for this problem. We establish\nmatching converse and achievability results using a novel lower bound and a new\nalgorithm for distributed DP-SCO based on Vaidya's plane cutting method. Thus,\nour results provide a complete characterization of the\naccuracy-communication-privacy trade-off for DP-SCO in the distributed setting.",
    "updated" : "2025-01-06T18:57:05Z",
    "published" : "2025-01-06T18:57:05Z",
    "authors" : [
      {
        "name" : "Sudeep Salgia"
      },
      {
        "name" : "Nikola Pavlovic"
      },
      {
        "name" : "Yuejie Chi"
      },
      {
        "name" : "Qing Zhao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02893v1",
    "title" : "A Volumetric Approach to Privacy of Dynamical Systems",
    "summary" : "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack.",
    "updated" : "2025-01-06T10:15:21Z",
    "published" : "2025-01-06T10:15:21Z",
    "authors" : [
      {
        "name" : "Chuanghong Weng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02804v1",
    "title" : "Latency and Privacy-Aware Resource Allocation in Vehicular Edge\n  Computing",
    "summary" : "The rapid increase in the number of connected vehicles has led to the\ngeneration of vast amounts of data. As a significant portion of this data\npertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it\nis predominantly generated at the edge. Considering the enormous volume of\ndata, real-time applications, and privacy concerns, it is crucial to process\nthe data at the edge. Neglecting the management of processing resources in\nvehicular edge computing (VEC) could lead to numerous challenges as a\nsubstantial number of vehicles with diverse safety, economic, and entertainment\napplications, along with their data processing, emerge in the near future [1].\nPrevious research in VEC resource allocation has primarily focused on issues\nsuch as response time and privacy preservation techniques. However, an approach\nthat takes into account privacy-aware resource allocation based on vehicular\nnetwork architecture and application requirements has not yet been proposed. In\nthis paper, we present a privacy and latency-aware approach for allocating\nprocessing resources at the edge of the vehicular network, considering the\nspecific requirements of different applications. Our approach involves\ncategorizing vehicular network applications based on their processing accuracy,\nreal-time processing needs, and privacy preservation requirements. We further\ndivide the vehicular network edge into two parts: the user layer (OBUs) is\nconsidered for processing applications with privacy requirements, while the\nallocation of resources in the RSUs and cloud layer is based on the specific\nneeds of different applications. In this study, we evaluate the quality of\nservice based on parameters such as privacy preservation, processing cost,\nmeeting deadlines, and result quality. Comparative analyses demonstrate that\nour approach enhances service quality by 55% compared to existing\nstate-of-the-art methods.",
    "updated" : "2025-01-06T06:44:49Z",
    "published" : "2025-01-06T06:44:49Z",
    "authors" : [
      {
        "name" : "HHossein Ahmadvand"
      },
      {
        "name" : "Fouzhan Foroutan"
      }
    ],
    "categories" : [
      "cs.PF"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02354v1",
    "title" : "PrivDPR: Synthetic Graph Publishing with Deep PageRank under\n  Differential Privacy",
    "summary" : "The objective of privacy-preserving synthetic graph publishing is to\nsafeguard individuals' privacy while retaining the utility of original data.\nMost existing methods focus on graph neural networks under differential privacy\n(DP), and yet two fundamental problems in generating synthetic graphs remain\nopen. First, the current research often encounters high sensitivity due to the\nintricate relationships between nodes in a graph. Second, DP is usually\nachieved through advanced composition mechanisms that tend to converge\nprematurely when working with a small privacy budget. In this paper, inspired\nby the simplicity, effectiveness, and ease of analysis of PageRank, we design\nPrivDPR, a novel privacy-preserving deep PageRank for graph synthesis. In\nparticular, we achieve DP by adding noise to the gradient for a specific weight\nduring learning. Utilizing weight normalization as a bridge, we theoretically\nreveal that increasing the number of layers in PrivDPR can effectively mitigate\nthe high sensitivity and privacy budget splitting. Through formal privacy\nanalysis, we prove that the synthetic graph generated by PrivDPR satisfies\nnode-level DP. Experiments on real-world graph datasets show that PrivDPR\npreserves high data utility across multiple graph structural properties.",
    "updated" : "2025-01-04T18:19:21Z",
    "published" : "2025-01-04T18:19:21Z",
    "authors" : [
      {
        "name" : "Sen Zhang"
      },
      {
        "name" : "Haibo Hu"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Jianliang Xu"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02091v1",
    "title" : "PriveShield: Enhancing User Privacy Using Automatic Isolated Profiles in\n  Browsers",
    "summary" : "Online tracking is a widespread practice on the web with questionable ethics,\nsecurity, and privacy concerns. While web tracking can offer personalized and\ncurated content to Internet users, it operates as a sophisticated surveillance\nmechanism to gather extensive user information. This paper introduces\nPriveShield, a light-weight privacy mechanism that disrupts the information\ngathering cycle while offering more control to Internet users to maintain their\nprivacy. PriveShield is implemented as a browser extension that offers an\nadjustable privacy feature to surf the web with multiple identities or accounts\nsimultaneously without any changes to underlying browser code or services. When\nnecessary, multiple factors are automatically analyzed on the client side to\nisolate cookies and other information that are the basis of online tracking.\nPriveShield creates isolated profiles for clients based on their browsing\nhistory, interactions with websites, and the amount of time they spend on\nspecific websites. This allows the users to easily prevent unwanted browsing\ninformation from being shared with third parties and ad exchanges without the\nneed for manual configuration. Our evaluation results from 54 real-world\nscenarios show that our extension is effective in preventing retargeted ads in\n91% of those scenarios.",
    "updated" : "2025-01-03T20:29:33Z",
    "published" : "2025-01-03T20:29:33Z",
    "authors" : [
      {
        "name" : "Seyed Ali Akhavani"
      },
      {
        "name" : "Engin Kirda"
      },
      {
        "name" : "Amin Kharraz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.01639v2",
    "title" : "Implications of Artificial Intelligence on Health Data Privacy and\n  Confidentiality",
    "summary" : "The rapid integration of artificial intelligence (AI) in healthcare is\nrevolutionizing medical diagnostics, personalized medicine, and operational\nefficiency. However, alongside these advancements, significant challenges arise\nconcerning patient data privacy, ethical considerations, and regulatory\ncompliance. This paper examines the dual impact of AI on healthcare,\nhighlighting its transformative potential and the critical need for\nsafeguarding sensitive health information. It explores the role of the Health\nInsurance Portability and Accountability Act (HIPAA) as a regulatory framework\nfor ensuring data privacy and security, emphasizing the importance of robust\nsafeguards and ethical standards in AI-driven healthcare. Through case studies,\nincluding AI applications in diabetic retinopathy, oncology, and the\ncontroversies surrounding data sharing, this study underscores the ethical and\nlegal complexities of AI implementation. A balanced approach that fosters\ninnovation while maintaining patient trust and privacy is imperative. The\nfindings emphasize the importance of continuous education, transparency, and\nadherence to regulatory frameworks to harness AI's full potential responsibly\nand ethically in healthcare.",
    "updated" : "2025-01-06T18:52:32Z",
    "published" : "2025-01-03T05:17:23Z",
    "authors" : [
      {
        "name" : "Ahmad Momani"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03941v1",
    "title" : "Synthetic Data Privacy Metrics",
    "summary" : "Recent advancements in generative AI have made it possible to create\nsynthetic datasets that can be as accurate as real-world data for training AI\nmodels, powering statistical insights, and fostering collaboration with\nsensitive datasets while offering strong privacy guarantees. Effectively\nmeasuring the empirical privacy of synthetic data is an important step in the\nprocess. However, while there is a multitude of new privacy metrics being\npublished every day, there currently is no standardization. In this paper, we\nreview the pros and cons of popular metrics that include simulations of\nadversarial attacks. We also review current best practices for amending\ngenerative models to enhance the privacy of the data they create (e.g.\ndifferential privacy).",
    "updated" : "2025-01-07T17:02:33Z",
    "published" : "2025-01-07T17:02:33Z",
    "authors" : [
      {
        "name" : "Amy Steier"
      },
      {
        "name" : "Lipika Ramaswamy"
      },
      {
        "name" : "Andre Manoel"
      },
      {
        "name" : "Alexa Haushalter"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03451v1",
    "title" : "Structure-Preference Enabled Graph Embedding Generation under\n  Differential Privacy",
    "summary" : "Graph embedding generation techniques aim to learn low-dimensional vectors\nfor each node in a graph and have recently gained increasing research\nattention. Publishing low-dimensional node vectors enables various graph\nanalysis tasks, such as structural equivalence and link prediction. Yet,\nimproper publication opens a backdoor to malicious attackers, who can infer\nsensitive information of individuals from the low-dimensional node vectors.\nExisting methods tackle this issue by developing deep graph learning models\nwith differential privacy (DP). However, they often suffer from large noise\ninjections and cannot provide structural preferences consistent with mining\nobjectives. Recently, skip-gram based graph embedding generation techniques are\nwidely used due to their ability to extract customizable structures. Based on\nskip-gram, we present SE-PrivGEmb, a structure-preference enabled graph\nembedding generation under DP. For arbitrary structure preferences, we design a\nunified noise tolerance mechanism via perturbing non-zero vectors. This\nmechanism mitigates utility degradation caused by high sensitivity. By\ncarefully designing negative sampling probabilities in skip-gram, we\ntheoretically demonstrate that skip-gram can preserve arbitrary proximities,\nwhich quantify structural features in graphs. Extensive experiments show that\nour method outperforms existing state-of-the-art methods under structural\nequivalence and link prediction tasks.",
    "updated" : "2025-01-07T00:43:18Z",
    "published" : "2025-01-07T00:43:18Z",
    "authors" : [
      {
        "name" : "Sen Zhang"
      },
      {
        "name" : "Qingqing Ye"
      },
      {
        "name" : "Haibo Hu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.03391v1",
    "title" : "Privacy-Preserving Smart Contracts for Permissioned Blockchains: A\n  zk-SNARK-Based Recipe Part-1",
    "summary" : "The Bitcoin white paper introduced blockchain technology, enabling trustful\ntransactions without intermediaries. Smart contracts emerged with Ethereum and\nblockchains expanded beyond cryptocurrency, applying to auctions, crowdfunding\nand electronic voting. However, blockchain's transparency raised privacy\nconcerns and initial anonymity measures proved ineffective. Smart contract\nprivacy solutions employed zero-knowledge proofs, homomorphic encryption and\ntrusted execution environments. These approaches have practical drawbacks, such\nas limited functionality, high computation times and trust on third parties\nrequirements, being not fully decentralized. This work proposes a solution\nutilizing zk-SNARKs to provide privacy in smart contracts and blockchains. The\nsolution supports both fungible and nonfungible tokens. Additionally, the\nproposal includes a new type of transactions, called delegated transactions,\nwhich enable use cases like Delivery vs Payment (DvP).",
    "updated" : "2025-01-06T21:16:33Z",
    "published" : "2025-01-06T21:16:33Z",
    "authors" : [
      {
        "name" : "Aldenio Burgos"
      },
      {
        "name" : "Eduardo Alchieri"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02893v2",
    "title" : "A Volumetric Approach to Privacy of Dynamical Systems",
    "summary" : "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack.",
    "updated" : "2025-01-07T13:21:10Z",
    "published" : "2025-01-06T10:15:21Z",
    "authors" : [
      {
        "name" : "Chuanghong Weng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.02804v2",
    "title" : "Latency and Privacy-Aware Resource Allocation in Vehicular Edge\n  Computing",
    "summary" : "The rapid increase in the number of connected vehicles has led to the\ngeneration of vast amounts of data. As a significant portion of this data\npertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it\nis predominantly generated at the edge. Considering the enormous volume of\ndata, real-time applications, and privacy concerns, it is crucial to process\nthe data at the edge. Neglecting the management of processing resources in\nvehicular edge computing (VEC) could lead to numerous challenges as a\nsubstantial number of vehicles with diverse safety, economic, and entertainment\napplications, along with their data processing, emerge in the near future [1].\nPrevious research in VEC resource allocation has primarily focused on issues\nsuch as response time and privacy preservation techniques. However, an approach\nthat takes into account privacy-aware resource allocation based on vehicular\nnetwork architecture and application requirements has not yet been proposed. In\nthis paper, we present a privacy and latency-aware approach for allocating\nprocessing resources at the edge of the vehicular network, considering the\nspecific requirements of different applications. Our approach involves\ncategorizing vehicular network applications based on their processing accuracy,\nreal-time processing needs, and privacy preservation requirements. We further\ndivide the vehicular network edge into two parts: the user layer (OBUs) is\nconsidered for processing applications with privacy requirements, while the\nallocation of resources in the RSUs and cloud layer is based on the specific\nneeds of different applications. In this study, we evaluate the quality of\nservice based on parameters such as privacy preservation, processing cost,\nmeeting deadlines, and result quality. Comparative analyses demonstrate that\nour approach enhances service quality by 55% compared to existing\nstate-of-the-art methods.",
    "updated" : "2025-01-07T07:17:16Z",
    "published" : "2025-01-06T06:44:49Z",
    "authors" : [
      {
        "name" : "Hossein Ahmadvand"
      },
      {
        "name" : "Fouzhan Foroutan"
      }
    ],
    "categories" : [
      "cs.PF"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04420v1",
    "title" : "A Closer Look on Gender Stereotypes in Movie Recommender Systems and\n  Their Implications with Privacy",
    "summary" : "The movie recommender system typically leverages user feedback to provide\npersonalized recommendations that align with user preferences and increase\nbusiness revenue. This study investigates the impact of gender stereotypes on\nsuch systems through a specific attack scenario. In this scenario, an attacker\ndetermines users' gender, a private attribute, by exploiting gender stereotypes\nabout movie preferences and analyzing users' feedback data, which is either\npublicly available or observed within the system. The study consists of two\nphases. In the first phase, a user study involving 630 participants identified\ngender stereotypes associated with movie genres, which often influence viewing\nchoices. In the second phase, four inference algorithms were applied to detect\ngender stereotypes by combining the findings from the first phase with users'\nfeedback data. Results showed that these algorithms performed more effectively\nthan relying solely on feedback data for gender inference. Additionally, we\nquantified the extent of gender stereotypes to evaluate their broader impact on\ndigital computational science. The latter part of the study utilized two major\nmovie recommender datasets: MovieLens 1M and Yahoo!Movie. Detailed experimental\ninformation is available on our GitHub repository:\nhttps://github.com/fr-iit/GSMRS",
    "updated" : "2025-01-08T11:08:58Z",
    "published" : "2025-01-08T11:08:58Z",
    "authors" : [
      {
        "name" : "Falguni Roy"
      },
      {
        "name" : "Yiduo Shen"
      },
      {
        "name" : "Na Zhao"
      },
      {
        "name" : "Xiaofeng Ding"
      },
      {
        "name" : "Md. Omar Faruk"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04409v1",
    "title" : "Lossless Privacy-Preserving Aggregation for Decentralized Federated\n  Learning",
    "summary" : "Privacy concerns arise as sensitive data proliferate. Despite decentralized\nfederated learning (DFL) aggregating gradients from neighbors to avoid direct\ndata transmission, it still poses indirect data leaks from the transmitted\ngradients. Existing privacy-preserving methods for DFL add noise to gradients.\nThey either diminish the model predictive accuracy or suffer from ineffective\ngradient protection. In this paper, we propose a novel lossless\nprivacy-preserving aggregation rule named LPPA to enhance gradient protection\nas much as possible but without loss of DFL model predictive accuracy. LPPA\nsubtly injects the noise difference between the sent and received noise into\ntransmitted gradients for gradient protection. The noise difference\nincorporates neighbors' randomness for each client, effectively safeguarding\nagainst data leaks. LPPA employs the noise flow conservation theory to ensure\nthat the noise impact can be globally eliminated. The global sum of all noise\ndifferences remains zero, ensuring that accurate gradient aggregation is\nunaffected and the model accuracy remains intact. We theoretically prove that\nthe privacy-preserving capacity of LPPA is \\sqrt{2} times greater than that of\nnoise addition, while maintaining comparable model accuracy to the standard DFL\naggregation without noise injection. Experimental results verify the\ntheoretical findings and show that LPPA achieves a 13% mean improvement in\naccuracy over noise addition. We also demonstrate the effectiveness of LPPA in\nprotecting raw data and guaranteeing lossless model accuracy.",
    "updated" : "2025-01-08T10:49:06Z",
    "published" : "2025-01-08T10:49:06Z",
    "authors" : [
      {
        "name" : "Xiaoye Miao"
      },
      {
        "name" : "Bin Li"
      },
      {
        "name" : "Yangyang Wu"
      },
      {
        "name" : "Meng Xi"
      },
      {
        "name" : "Xinkui Zhao"
      },
      {
        "name" : "Jianwei Yin"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04323v1",
    "title" : "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models",
    "summary" : "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance.",
    "updated" : "2025-01-08T07:47:43Z",
    "published" : "2025-01-08T07:47:43Z",
    "authors" : [
      {
        "name" : "Shi Haonan"
      },
      {
        "name" : "Ouyang Tu"
      },
      {
        "name" : "Wang An"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04222v1",
    "title" : "Privacy-Preserving Distributed Online Mirror Descent for Nonconvex\n  Optimization",
    "summary" : "We investigate the distributed online nonconvex optimization problem with\ndifferential privacy over time-varying networks. Each node minimizes the sum of\nseveral nonconvex functions while preserving the node's differential privacy.\nWe propose a privacy-preserving distributed online mirror descent algorithm for\nnonconvex optimization, which uses the mirror descent to update decision\nvariables and the Laplace differential privacy mechanism to protect privacy.\nUnlike the existing works, the proposed algorithm allows the cost functions to\nbe nonconvex, which is more applicable. Based upon these, we prove that if the\ncommunication network is $B$-strongly connected and the constraint set is\ncompact, then by choosing the step size properly, the algorithm guarantees\n$\\epsilon$-differential privacy at each time. Furthermore, we prove that if the\nlocal cost functions are $\\beta$-smooth, then the regret over time horizon $T$\ngrows sublinearly while preserving differential privacy, with an upper bound\n$O(\\sqrt{T})$. Finally, the effectiveness of the algorithm is demonstrated\nthrough numerical simulations.",
    "updated" : "2025-01-08T01:39:10Z",
    "published" : "2025-01-08T01:39:10Z",
    "authors" : [
      {
        "name" : "Yingjie Zhou"
      },
      {
        "name" : "Tao Li"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04134v1",
    "title" : "Mixing Times and Privacy Analysis for the Projected Langevin Algorithm\n  under a Modulus of Continuity",
    "summary" : "We study the mixing time of the projected Langevin algorithm (LA) and the\nprivacy curve of noisy Stochastic Gradient Descent (SGD), beyond nonexpansive\niterations. Specifically, we derive new mixing time bounds for the projected LA\nwhich are, in some important cases, dimension-free and poly-logarithmic on the\naccuracy, closely matching the existing results in the smooth convex case.\nAdditionally, we establish new upper bounds for the privacy curve of the\nsubsampled noisy SGD algorithm. These bounds show a crucial dependency on the\nregularity of gradients, and are useful for a wide range of convex losses\nbeyond the smooth case. Our analysis relies on a suitable extension of the\nPrivacy Amplification by Iteration (PABI) framework (Feldman et al., 2018;\nAltschuler and Talwar, 2022, 2023) to noisy iterations whose gradient map is\nnot necessarily nonexpansive. This extension is achieved by designing an\noptimization problem which accounts for the best possible R\\'enyi divergence\nbound obtained by an application of PABI, where the tractability of the problem\nis crucially related to the modulus of continuity of the associated gradient\nmapping. We show that, in several interesting cases -- including the nonsmooth\nconvex, weakly smooth and (strongly) dissipative -- such optimization problem\ncan be solved exactly and explicitly. This yields the tightest possible\nPABI-based bounds, where our results are either new or substantially sharper\nthan those in previous works.",
    "updated" : "2025-01-07T20:46:59Z",
    "published" : "2025-01-07T20:46:59Z",
    "authors" : [
      {
        "name" : "Mario Bravo"
      },
      {
        "name" : "Juan P. Flores-Mella"
      },
      {
        "name" : "Cristóbal Guzmán"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "math.OC",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04058v1",
    "title" : "Homomorphic Encryption in Healthcare Industry Applications for\n  Protecting Data Privacy",
    "summary" : "Focussing on two different use cases-Quality Control methods in industrial\ncontexts and Neural Network algorithms for healthcare diagnostics-this research\ninvestigates the inclusion of Fully Homomorphic Encryption into real-world\napplications in the healthcare sector. We evaluate the performance, resource\nrequirements, and viability of deploying FHE in these settings through\nextensive testing and analysis, highlighting the progress made in FHE tooling\nand the obstacles still facing addressing the gap between conceptual research\nand practical applications. We start our research by describing the specific\ncase study and trust model were working with. Choosing the two FHE frameworks\nmost appropriate for industry development, we assess the resources and\nperformance requirements for implementing each of the two FHE frameworks in the\nfirst scenario, Quality Control algorithms. In conclusion, our findings\ndemonstrate the effectiveness and resource consumption of the two use\ncases-complex NN models and simple QC algorithms-when implemented in an FHE\nsetting.",
    "updated" : "2025-01-07T07:42:41Z",
    "published" : "2025-01-07T07:42:41Z",
    "authors" : [
      {
        "name" : "J. S. Rauthan"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.05053v1",
    "title" : "TAPFed: Threshold Secure Aggregation for Privacy-Preserving Federated\n  Learning",
    "summary" : "Federated learning is a computing paradigm that enhances privacy by enabling\nmultiple parties to collaboratively train a machine learning model without\nrevealing personal data. However, current research indicates that traditional\nfederated learning platforms are unable to ensure privacy due to privacy leaks\ncaused by the interchange of gradients. To achieve privacy-preserving federated\nlearning, integrating secure aggregation mechanisms is essential.\nUnfortunately, existing solutions are vulnerable to recently demonstrated\ninference attacks such as the disaggregation attack. This paper proposes\nTAPFed, an approach for achieving privacy-preserving federated learning in the\ncontext of multiple decentralized aggregators with malicious actors. TAPFed\nuses a proposed threshold functional encryption scheme and allows for a certain\nnumber of malicious aggregators while maintaining security and privacy. We\nprovide formal security and privacy analyses of TAPFed and compare it to\nvarious baselines through experimental evaluation. Our results show that TAPFed\noffers equivalent performance in terms of model quality compared to\nstate-of-the-art approaches while reducing transmission overhead by 29%-45%\nacross different model training scenarios. Most importantly, TAPFed can defend\nagainst recently demonstrated inference attacks caused by curious aggregators,\nwhich the majority of existing approaches are susceptible to.",
    "updated" : "2025-01-09T08:24:10Z",
    "published" : "2025-01-09T08:24:10Z",
    "authors" : [
      {
        "name" : "Runhua Xu"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Chao Li"
      },
      {
        "name" : "James B. D. Joshi"
      },
      {
        "name" : "Shuai Ma"
      },
      {
        "name" : "Jianxin Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04940v1",
    "title" : "A New Perspective on Privacy Protection in Federated Learning with\n  Granular-Ball Computing",
    "summary" : "Federated Learning (FL) facilitates collaborative model training while\nprioritizing privacy by avoiding direct data sharing. However, most existing\narticles attempt to address challenges within the model's internal parameters\nand corresponding outputs, while neglecting to solve them at the input level.\nTo address this gap, we propose a novel framework called Granular-Ball\nFederated Learning (GrBFL) for image classification. GrBFL diverges from\ntraditional methods that rely on the finest-grained input data. Instead, it\nsegments images into multiple regions with optimal coarse granularity, which\nare then reconstructed into a graph structure. We designed a two-dimensional\nbinary search segmentation algorithm based on variance constraints for GrBFL,\nwhich effectively removes redundant information while preserving key\nrepresentative features. Extensive theoretical analysis and experiments\ndemonstrate that GrBFL not only safeguards privacy and enhances efficiency but\nalso maintains robust utility, consistently outperforming other\nstate-of-the-art FL methods. The code is available at\nhttps://github.com/AIGNLAI/GrBFL.",
    "updated" : "2025-01-09T03:14:03Z",
    "published" : "2025-01-09T03:14:03Z",
    "authors" : [
      {
        "name" : "Guannan Lai"
      },
      {
        "name" : "Yihui Feng"
      },
      {
        "name" : "Xin Yang"
      },
      {
        "name" : "Xiaoyu Deng"
      },
      {
        "name" : "Hao Yu"
      },
      {
        "name" : "Shuyin Xia"
      },
      {
        "name" : "Guoyin Wang"
      },
      {
        "name" : "Tianrui Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.04323v2",
    "title" : "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models",
    "summary" : "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance.",
    "updated" : "2025-01-09T02:33:04Z",
    "published" : "2025-01-08T07:47:43Z",
    "authors" : [
      {
        "name" : "Haonan Shi"
      },
      {
        "name" : "Tu Ouyang"
      },
      {
        "name" : "An Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.06161v1",
    "title" : "RIOT-based smart metering system for privacy-preserving data aggregation\n  using watermarking and encryption",
    "summary" : "The remarkable advancement of smart grid technology in the IoT sector has\nraised concerns over the privacy and security of the data collected and\ntransferred in real-time. Smart meters generate detailed information about\nconsumers' energy consumption patterns, increasing the risks of data breaches,\nidentity theft, and other forms of cyber attacks. This study proposes a\nprivacy-preserving data aggregation protocol that uses reversible watermarking\nand AES cryptography to ensure the security and privacy of the data. There are\ntwo versions of the protocol: one for low-frequency smart meters that uses\nLSB-shifting-based reversible watermarking (RLS) and another for high-frequency\nsmart meters that uses difference expansion-based reversible watermarking\n(RDE). This enables the aggregation of smart meter data, maintaining\nconfidentiality, integrity, and authenticity. The proposed protocol\nsignificantly enhances privacy-preserving measures for smart metering systems,\nconducting an experimental evaluation with real hardware implementation using\nNucleo microcontroller boards and the RIOT operating system and comparing the\nresults to existing security schemes.",
    "updated" : "2025-01-10T18:37:20Z",
    "published" : "2025-01-10T18:37:20Z",
    "authors" : [
      {
        "name" : "David Megias"
      },
      {
        "name" : "Farzana Kabir"
      },
      {
        "name" : "Krzysztof Cabaj"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.05535v1",
    "title" : "On Fair Ordering and Differential Privacy",
    "summary" : "In blockchain systems, fair transaction ordering is crucial for a trusted and\nregulation-compliant economic ecosystem. Unlike traditional State Machine\nReplication (SMR) systems, which focus solely on liveness and safety,\nblockchain systems also require a fairness property. This paper examines these\nproperties and aims to eliminate algorithmic bias in transaction ordering\nservices.\n  We build on the notion of equal opportunity. We characterize transactions in\nterms of relevant and irrelevant features, requiring that the order be\ndetermined solely by the relevant ones. Specifically, transactions with\nidentical relevant features should have an equal chance of being ordered before\none another. We extend this framework to define a property where the greater\nthe distance in relevant features between transactions, the higher the\nprobability of prioritizing one over the other.\n  We reveal a surprising link between equal opportunity in SMR and Differential\nPrivacy (DP), showing that any DP mechanism can be used to ensure fairness in\nSMR. This connection not only enhances our understanding of the interplay\nbetween privacy and fairness in distributed computing but also opens up new\nopportunities for designing fair distributed protocols using well-established\nDP techniques.",
    "updated" : "2025-01-09T19:17:43Z",
    "published" : "2025-01-09T19:17:43Z",
    "authors" : [
      {
        "name" : "Shir Cohen"
      },
      {
        "name" : "Neel Basu"
      },
      {
        "name" : "Soumya Basu"
      },
      {
        "name" : "Lorenzo Alvisi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.07262v1",
    "title" : "OblivCDN: A Practical Privacy-preserving CDN with Oblivious Content\n  Access",
    "summary" : "Content providers increasingly utilise Content Delivery Networks (CDNs) to\nenhance users' content download experience. However, this deployment scenario\nraises significant security concerns regarding content confidentiality and user\nprivacy due to the involvement of third-party providers. Prior proposals using\nprivate information retrieval (PIR) and oblivious RAM (ORAM) have proven\nimpractical due to high computation and communication costs, as well as\nintegration challenges within distributed CDN architectures. In response, we\npresent \\textsf{OblivCDN}, a practical privacy-preserving system meticulously\ndesigned for seamless integration with the existing real-world Internet-CDN\ninfrastructure. Our design strategically adapts Range ORAM primitives to\noptimise memory and disk seeks when accessing contiguous blocks of CDN content,\nboth at the origin and edge servers, while preserving both content\nconfidentiality and user access pattern hiding features. Also, we carefully\ncustomise several oblivious building blocks that integrate the distributed\ntrust model into the ORAM client, thereby eliminating the computational\nbottleneck in the origin server and reducing communication costs between the\norigin server and edge servers. Moreover, the newly-designed ORAM client also\neliminates the need for trusted hardware on edge servers, and thus\nsignificantly ameliorates the compatibility towards networks with massive\nlegacy devices.In real-world streaming evaluations, OblivCDN} demonstrates\nremarkable performance, downloading a $256$ MB video in just $5.6$ seconds.\nThis achievement represents a speedup of $90\\times$ compared to a strawman\napproach (direct ORAM adoption) and a $366\\times$ improvement over the prior\nart, OblivP2P.",
    "updated" : "2025-01-13T12:23:23Z",
    "published" : "2025-01-13T12:23:23Z",
    "authors" : [
      {
        "name" : "Viet Vo"
      },
      {
        "name" : "Shangqi Lai"
      },
      {
        "name" : "Xingliang Yuan"
      },
      {
        "name" : "Surya Nepal"
      },
      {
        "name" : "Qi Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.07209v1",
    "title" : "Privacy-Preserving Authentication: Theory vs. Practice",
    "summary" : "With the increasing use of online services, the protection of the privacy of\nusers becomes more and more important. This is particularly critical as\nauthentication and authorization as realized on the Internet nowadays,\ntypically relies on centralized identity management solutions. Although those\nare very convenient from a user's perspective, they are quite intrusive from a\nprivacy perspective and are currently far from implementing the concept of data\nminimization. Fortunately, cryptography offers exciting primitives such as\nzero-knowledge proofs and advanced signature schemes to realize various forms\nof so-called anonymous credentials. Such primitives allow to realize online\nauthentication and authorization with a high level of built-in privacy\nprotection (what we call privacy-preserving authentication). Though these\nprimitives have already been researched for various decades and are well\nunderstood in the research community, unfortunately, they lack widespread\nadoption. In this paper, we look at the problems, what cryptography can do,\nsome deployment examples, and barriers to widespread adoption. Latter using the\nexample of the EU Digital Identity Wallet (EUDIW) and the recent discussion and\nfeedback from cryptography experts around this topic. We also briefly comment\non the transition to post-quantum cryptography.",
    "updated" : "2025-01-13T11:04:05Z",
    "published" : "2025-01-13T11:04:05Z",
    "authors" : [
      {
        "name" : "Daniel Slamanig"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.07154v1",
    "title" : "Privacy-Preserving Data Quality Assessment for Time-Series IoT Sensors",
    "summary" : "Data from Internet of Things (IoT) sensors has emerged as a key contributor\nto decision-making processes in various domains. However, the quality of the\ndata is crucial to the effectiveness of applications built on it, and\nassessment of the data quality is heavily context-dependent. Further,\npreserving the privacy of the data during quality assessment is critical in\ndomains where sensitive data is prevalent. This paper proposes a novel\nframework for automated, objective, and privacy-preserving data quality\nassessment of time-series data from IoT sensors deployed in smart cities. We\nleverage custom, autonomously computable metrics that parameterise the temporal\nperformance and adherence to a declarative schema document to achieve\nobjectivity. Additionally, we utilise a trusted execution environment to create\na \"data-blind\" model that ensures individual privacy, eliminates assessee bias,\nand enhances adaptability across data types. This paper describes this data\nquality assessment methodology for IoT sensors, emphasising its relevance\nwithin the smart-city context while addressing the growing need for privacy in\nthe face of extensive data collection practices.",
    "updated" : "2025-01-13T09:28:42Z",
    "published" : "2025-01-13T09:28:42Z",
    "authors" : [
      {
        "name" : "Novoneel Chakraborty"
      },
      {
        "name" : "Abhay Sharma"
      },
      {
        "name" : "Jyotirmoy Dutta"
      },
      {
        "name" : "Hari Dilip Kumar"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.06913v1",
    "title" : "Towards Fair and Privacy-Aware Transfer Learning for Educational\n  Predictive Modeling: A Case Study on Retention Prediction in Community\n  Colleges",
    "summary" : "Predictive analytics is widely used in learning analytics, but many\nresource-constrained institutions lack the capacity to develop their own models\nor rely on proprietary ones trained in different contexts with little\ntransparency. Transfer learning holds promise for expanding equitable access to\npredictive analytics but remains underexplored due to legal and technical\nconstraints. This paper examines transfer learning strategies for retention\nprediction at U.S. two-year community colleges. We envision a scenario where\ncommunity colleges collaborate with each other and four-year universities to\ndevelop retention prediction models under privacy constraints and evaluate\nrisks and improvement strategies of cross-institutional model transfer. Using\nadministrative records from 4 research universities and 23 community colleges\ncovering over 800,000 students across 7 cohorts, we identify performance and\nfairness degradation when external models are deployed locally without\nadaptation. Publicly available contextual information can forecast these\nperformance drops and offer early guidance for model portability. For\ndevelopers under privacy regulations, sequential training selecting\ninstitutions based on demographic similarities enhances fairness without\ncompromising performance. For institutions lacking local data to fine-tune\nsource models, customizing evaluation thresholds for sensitive groups\noutperforms standard transfer techniques in improving performance and fairness.\nOur findings suggest the value of transfer learning for more accessible\neducational predictive modeling and call for judicious use of contextual\ninformation in model training, selection, and deployment to achieve reliable\nand equitable model transfer.",
    "updated" : "2025-01-12T19:49:28Z",
    "published" : "2025-01-12T19:49:28Z",
    "authors" : [
      {
        "name" : "Chengyuan Yao"
      },
      {
        "name" : "Carmen Cortez"
      },
      {
        "name" : "Renzhe Yu"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.06300v1",
    "title" : "Tensorization of neural networks for improved privacy and\n  interpretability",
    "summary" : "We present a tensorization algorithm for constructing tensor train\nrepresentations of functions, drawing on sketching and cross interpolation\nideas. The method only requires black-box access to the target function and a\nsmall set of sample points defining the domain of interest. Thus, it is\nparticularly well-suited for machine learning models, where the domain of\ninterest is naturally defined by the training dataset. We show that this\napproach can be used to enhance the privacy and interpretability of neural\nnetwork models. Specifically, we apply our decomposition to (i) obfuscate\nneural networks whose parameters encode patterns tied to the training data\ndistribution, and (ii) estimate topological phases of matter that are easily\naccessible from the tensor train representation. Additionally, we show that\nthis tensorization can serve as an efficient initialization method for\noptimizing tensor trains in general settings, and that, for model compression,\nour algorithm achieves a superior trade-off between memory and time complexity\ncompared to conventional tensorization methods of neural networks.",
    "updated" : "2025-01-10T19:00:06Z",
    "published" : "2025-01-10T19:00:06Z",
    "authors" : [
      {
        "name" : "José Ramón Pareja Monturiol"
      },
      {
        "name" : "Alejandro Pozas-Kerstjens"
      },
      {
        "name" : "David Pérez-García"
      }
    ],
    "categories" : [
      "math.NA",
      "cs.LG",
      "cs.NA",
      "physics.comp-ph",
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.06161v2",
    "title" : "RIOT-based smart metering system for privacy-preserving data aggregation\n  using watermarking and encryption",
    "summary" : "The remarkable advancement of smart grid technology in the IoT sector has\nraised concerns over the privacy and security of the data collected and\ntransferred in real-time. Smart meters generate detailed information about\nconsumers' energy consumption patterns, increasing the risks of data breaches,\nidentity theft, and other forms of cyber attacks. This study proposes a\nprivacy-preserving data aggregation protocol that uses reversible watermarking\nand AES cryptography to ensure the security and privacy of the data. There are\ntwo versions of the protocol: one for low-frequency smart meters that uses\nLSB-shifting-based reversible watermarking (RLS) and another for high-frequency\nsmart meters that uses difference expansion-based reversible watermarking\n(RDE). This enables the aggregation of smart meter data, maintaining\nconfidentiality, integrity, and authenticity. The proposed protocol\nsignificantly enhances privacy-preserving measures for smart metering systems,\nconducting an experimental evaluation with real hardware implementation using\nNucleo microcontroller boards and the RIOT operating system and comparing the\nresults to existing security schemes.",
    "updated" : "2025-01-13T16:58:59Z",
    "published" : "2025-01-10T18:37:20Z",
    "authors" : [
      {
        "name" : "Farzana Kabir"
      },
      {
        "name" : "David Megias"
      },
      {
        "name" : "Krzysztof Cabaj"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.08236v1",
    "title" : "Privacy-Preserving Model and Preprocessing Verification for Machine\n  Learning",
    "summary" : "This paper presents a framework for privacy-preserving verification of\nmachine learning models, focusing on models trained on sensitive data.\nIntegrating Local Differential Privacy (LDP) with model explanations from LIME\nand SHAP, our framework enables robust verification without compromising\nindividual privacy. It addresses two key tasks: binary classification, to\nverify if a target model was trained correctly by applying the appropriate\npreprocessing steps, and multi-class classification, to identify specific\npreprocessing errors. Evaluations on three real-world datasets-Diabetes, Adult,\nand Student Record-demonstrate that while the ML-based approach is particularly\neffective in binary tasks, the threshold-based method performs comparably in\nmulti-class tasks. Results indicate that although verification accuracy varies\nacross datasets and noise levels, the framework provides effective detection of\npreprocessing errors, strong privacy guarantees, and practical applicability\nfor safeguarding sensitive data.",
    "updated" : "2025-01-14T16:21:54Z",
    "published" : "2025-01-14T16:21:54Z",
    "authors" : [
      {
        "name" : "Wenbiao Li"
      },
      {
        "name" : "Anisa Halimi"
      },
      {
        "name" : "Xiaoqian Jiang"
      },
      {
        "name" : "Jaideep Vaidya"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2501.07844v1",
    "title" : "Towards A Hybrid Quantum Differential Privacy",
    "summary" : "Quantum computing offers unparalleled processing power but raises significant\ndata privacy challenges. Quantum Differential Privacy (QDP) leverages inherent\nquantum noise to safeguard privacy, surpassing traditional DP. This paper\ndevelops comprehensive noise profiles, identifies noise types beneficial for\nQDP, and highlights teh need for practical implementations beyond theoretical\nmodels. Existing QDP mechanisms, limited to single noise sources, fail to\nreflect teh multi-source noise reality of quantum systems. We propose a\nresilient hybrid QDP mechanism utilizing channel and measurement noise,\noptimizing privacy budgets to balance privacy and utility. Additionally, we\nintroduce Lifted Quantum Differential Privacy, offering enhanced randomness for\nimproved privacy audits and quantum algorithm evaluation.",
    "updated" : "2025-01-14T05:13:37Z",
    "published" : "2025-01-14T05:13:37Z",
    "authors" : [
      {
        "name" : "Baobao Song"
      },
      {
        "name" : "Shiva Raj Pokhrel"
      },
      {
        "name" : "Athanasios V. Vasilakos"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Gang Li"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  }
]