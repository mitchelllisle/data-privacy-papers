[
  {
    "id" : "http://arxiv.org/abs/2406.02520v1",
    "title" : "Digital Privacy for Migrants: Exploring Current Research Trends and\n  Future Prospects",
    "summary" : "This paper explores digital privacy challenges for migrants, analyzing trends\nfrom 2013 to 2023. Migrants face heightened risks such as government\nsurveillance and identity theft. Understanding these threats is vital for\nraising awareness and guiding research towards effective solutions and policies\nto protect migrant digital privacy.",
    "updated" : "2024-06-04T17:41:20Z",
    "published" : "2024-06-04T17:41:20Z",
    "authors" : [
      {
        "name" : "Sarah Tabassum"
      },
      {
        "name" : "Cori Faklaris"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02463v1",
    "title" : "Click Without Compromise: Online Advertising Measurement via Per User\n  Differential Privacy",
    "summary" : "Online advertising is a cornerstone of the Internet ecosystem, with\nadvertising measurement playing a crucial role in optimizing efficiency. Ad\nmeasurement entails attributing desired behaviors, such as purchases, to ad\nexposures across various platforms, necessitating the collection of user\nactivities across these platforms. As this practice faces increasing\nrestrictions due to rising privacy concerns, safeguarding user privacy in this\ncontext is imperative. Our work is the first to formulate the real-world\nchallenge of advertising measurement systems with real-time reporting of\nstreaming data in advertising campaigns. We introduce Ads-BPC, a novel\nuser-level differential privacy protection scheme for advertising measurement\nresults. This approach optimizes global noise power and results in a\nnon-identically distributed noise distribution that preserves differential\nprivacy while enhancing measurement accuracy. Through experiments on both\nreal-world advertising campaigns and synthetic datasets, Ads-BPC achieves a 25%\nto 50% increase in accuracy over existing streaming DP mechanisms applied to\nadvertising measurement. This highlights our method's effectiveness in\nachieving superior accuracy alongside a formal privacy guarantee, thereby\nadvancing the state-of-the-art in privacy-preserving advertising measurement.",
    "updated" : "2024-06-04T16:31:19Z",
    "published" : "2024-06-04T16:31:19Z",
    "authors" : [
      {
        "name" : "Yingtai Xiao"
      },
      {
        "name" : "Jian Du"
      },
      {
        "name" : "Shikun Zhang"
      },
      {
        "name" : "Qiang Yan"
      },
      {
        "name" : "Danfeng Zhang"
      },
      {
        "name" : "Daniel Kifer"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02424v1",
    "title" : "Contextual Dynamic Pricing: Algorithms, Optimality, and Local\n  Differential Privacy Constraints",
    "summary" : "We study the contextual dynamic pricing problem where a firm sells products\nto $T$ sequentially arriving consumers that behave according to an unknown\ndemand model. The firm aims to maximize its revenue, i.e. minimize its regret\nover a clairvoyant that knows the model in advance. The demand model is a\ngeneralized linear model (GLM), allowing for a stochastic feature vector in\n$\\mathbb R^d$ that encodes product and consumer information. We first show that\nthe optimal regret upper bound is of order $\\sqrt{dT}$, up to a logarithmic\nfactor, improving upon existing upper bounds in the literature by a $\\sqrt{d}$\nfactor. This sharper rate is materialised by two algorithms: a confidence\nbound-type (supCB) algorithm and an explore-then-commit (ETC) algorithm. A key\ninsight of our theoretical result is an intrinsic connection between dynamic\npricing and the contextual multi-armed bandit problem with many arms based on a\ncareful discretization. We further study contextual dynamic pricing under the\nlocal differential privacy (LDP) constraints. In particular, we propose a\nstochastic gradient descent based ETC algorithm that achieves an optimal regret\nupper bound of order $d\\sqrt{T}/\\epsilon$, up to a logarithmic factor, where\n$\\epsilon>0$ is the privacy parameter. The regret upper bounds with and without\nLDP constraints are accompanied by newly constructed minimax lower bounds,\nwhich further characterize the cost of privacy. Extensive numerical experiments\nand a real data application on online lending are conducted to illustrate the\nefficiency and practical value of the proposed algorithms in dynamic pricing.",
    "updated" : "2024-06-04T15:44:10Z",
    "published" : "2024-06-04T15:44:10Z",
    "authors" : [
      {
        "name" : "Zifeng Zhao"
      },
      {
        "name" : "Feiyu Jiang"
      },
      {
        "name" : "Yi Yu"
      }
    ],
    "categories" : [
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01811v1",
    "title" : "A Game-Theoretic Approach to Privacy-Utility Tradeoff in Sharing Genomic\n  Summary Statistics",
    "summary" : "The advent of online genomic data-sharing services has sought to enhance the\naccessibility of large genomic datasets by allowing queries about genetic\nvariants, such as summary statistics, aiding care providers in distinguishing\nbetween spurious genomic variations and those with clinical significance.\nHowever, numerous studies have demonstrated that even sharing summary genomic\ninformation exposes individual members of such datasets to a significant\nprivacy risk due to membership inference attacks. While several approaches have\nemerged that reduce privacy risks by adding noise or reducing the amount of\ninformation shared, these typically assume non-adaptive attacks that use\nlikelihood ratio test (LRT) statistics. We propose a Bayesian game-theoretic\nframework for optimal privacy-utility tradeoff in the sharing of genomic\nsummary statistics. Our first contribution is to prove that a very general\nBayesian attacker model that anchors our game-theoretic approach is more\npowerful than the conventional LRT-based threat models in that it induces worse\nprivacy loss for the defender who is modeled as a von Neumann-Morgenstern (vNM)\ndecision-maker. We show this to be true even when the attacker uses a\nnon-informative subjective prior. Next, we present an analytically tractable\napproach to compare the Bayesian attacks with arbitrary subjective priors and\nthe Neyman-Pearson optimal LRT attacks under the Gaussian mechanism common in\ndifferential privacy frameworks. Finally, we propose an approach for\napproximating Bayes-Nash equilibria of the game using deep neural network\ngenerators to implicitly represent player mixed strategies. Our experiments\ndemonstrate that the proposed game-theoretic framework yields both stronger\nattacks and stronger defense strategies than the state of the art.",
    "updated" : "2024-06-03T22:09:47Z",
    "published" : "2024-06-03T22:09:47Z",
    "authors" : [
      {
        "name" : "Tao Zhang"
      },
      {
        "name" : "Rajagopal Venkatesaramani"
      },
      {
        "name" : "Rajat K. De"
      },
      {
        "name" : "Bradley A. Malin"
      },
      {
        "name" : "Yevgeniy Vorobeychik"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01394v1",
    "title" : "PrivacyRestore: Privacy-Preserving Inference in Large Language Models\n  via Privacy Removal and Restoration",
    "summary" : "The widespread usage of online Large Language Models (LLMs) inference\nservices has raised significant privacy concerns about the potential exposure\nof private information in user inputs to eavesdroppers or untrustworthy service\nproviders. Existing privacy protection methods for LLMs suffer from\ninsufficient privacy protection, performance degradation, or severe inference\ntime overhead. In this paper, we propose PrivacyRestore to protect the privacy\nof user inputs during LLM inference. PrivacyRestore directly removes privacy\nspans in user inputs and restores privacy information via activation steering\nduring inference. The privacy spans are encoded as restoration vectors. We\npropose Attention-aware Weighted Aggregation (AWA) which aggregates restoration\nvectors of all privacy spans in the input into a meta restoration vector. AWA\nnot only ensures proper representation of all privacy spans but also prevents\nattackers from inferring the privacy spans from the meta restoration vector\nalone. This meta restoration vector, along with the query with privacy spans\nremoved, is then sent to the server. The experimental results show that\nPrivacyRestore can protect private information while maintaining acceptable\nlevels of performance and inference efficiency.",
    "updated" : "2024-06-03T14:57:39Z",
    "published" : "2024-06-03T14:57:39Z",
    "authors" : [
      {
        "name" : "Ziqian Zeng"
      },
      {
        "name" : "Jianwei Wang"
      },
      {
        "name" : "Zhengdong Lu"
      },
      {
        "name" : "Huiping Zhuang"
      },
      {
        "name" : "Cen Chen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01363v1",
    "title" : "Privacy in LLM-based Recommendation: Recent Advances and Future\n  Directions",
    "summary" : "Nowadays, large language models (LLMs) have been integrated with conventional\nrecommendation models to improve recommendation performance. However, while\nmost of the existing works have focused on improving the model performance, the\nprivacy issue has only received comparatively less attention. In this paper, we\nreview recent advancements in privacy within LLM-based recommendation,\ncategorizing them into privacy attacks and protection mechanisms. Additionally,\nwe highlight several challenges and propose future directions for the community\nto address these critical problems.",
    "updated" : "2024-06-03T14:31:47Z",
    "published" : "2024-06-03T14:31:47Z",
    "authors" : [
      {
        "name" : "Sichun Luo"
      },
      {
        "name" : "Wei Shao"
      },
      {
        "name" : "Yuxuan Yao"
      },
      {
        "name" : "Jian Xu"
      },
      {
        "name" : "Mingyang Liu"
      },
      {
        "name" : "Qintong Li"
      },
      {
        "name" : "Bowei He"
      },
      {
        "name" : "Maolin Wang"
      },
      {
        "name" : "Guanzhi Deng"
      },
      {
        "name" : "Hanxu Hou"
      },
      {
        "name" : "Xinyi Zhang"
      },
      {
        "name" : "Linqi Song"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01085v1",
    "title" : "FedAdOb: Privacy-Preserving Federated Deep Learning with Adaptive\n  Obfuscation",
    "summary" : "Federated learning (FL) has emerged as a collaborative approach that allows\nmultiple clients to jointly learn a machine learning model without sharing\ntheir private data. The concern about privacy leakage, albeit demonstrated\nunder specific conditions, has triggered numerous follow-up research in\ndesigning powerful attacking methods and effective defending mechanisms aiming\nto thwart these attacking methods. Nevertheless, privacy-preserving mechanisms\nemployed in these defending methods invariably lead to compromised model\nperformances due to a fixed obfuscation applied to private data or gradients.\nIn this article, we, therefore, propose a novel adaptive obfuscation mechanism,\ncoined FedAdOb, to protect private data without yielding original model\nperformances. Technically, FedAdOb utilizes passport-based adaptive obfuscation\nto ensure data privacy in both horizontal and vertical federated learning\nsettings. The privacy-preserving capabilities of FedAdOb, specifically with\nregard to private features and labels, are theoretically proven through\nTheorems 1 and 2. Furthermore, extensive experimental evaluations conducted on\nvarious datasets and network architectures demonstrate the effectiveness of\nFedAdOb by manifesting its superior trade-off between privacy preservation and\nmodel performance, surpassing existing methods.",
    "updated" : "2024-06-03T08:12:09Z",
    "published" : "2024-06-03T08:12:09Z",
    "authors" : [
      {
        "name" : "Hanlin Gu"
      },
      {
        "name" : "Jiahuan Luo"
      },
      {
        "name" : "Yan Kang"
      },
      {
        "name" : "Yuan Yao"
      },
      {
        "name" : "Gongxi Zhu"
      },
      {
        "name" : "Bowen Li"
      },
      {
        "name" : "Lixin Fan"
      },
      {
        "name" : "Qiang Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01080v1",
    "title" : "No Vandalism: Privacy-Preserving and Byzantine-Robust Federated Learning",
    "summary" : "Federated learning allows several clients to train one machine learning model\njointly without sharing private data, providing privacy protection. However,\ntraditional federated learning is vulnerable to poisoning attacks, which can\nnot only decrease the model performance, but also implant malicious backdoors.\nIn addition, direct submission of local model parameters can also lead to the\nprivacy leakage of the training dataset. In this paper, we aim to build a\nprivacy-preserving and Byzantine-robust federated learning scheme to provide an\nenvironment with no vandalism (NoV) against attacks from malicious\nparticipants. Specifically, we construct a model filter for poisoned local\nmodels, protecting the global model from data and model poisoning attacks. This\nmodel filter combines zero-knowledge proofs to provide further privacy\nprotection. Then, we adopt secret sharing to provide verifiable secure\naggregation, removing malicious clients that disrupting the aggregation\nprocess. Our formal analysis proves that NoV can protect data privacy and weed\nout Byzantine attackers. Our experiments illustrate that NoV can effectively\naddress data and model poisoning attacks, including PGD, and outperforms other\nrelated schemes.",
    "updated" : "2024-06-03T07:59:10Z",
    "published" : "2024-06-03T07:59:10Z",
    "authors" : [
      {
        "name" : "Zhibo Xing"
      },
      {
        "name" : "Zijian Zhang"
      },
      {
        "name" : "Zi'ang Zhang"
      },
      {
        "name" : "Jiamou Liu"
      },
      {
        "name" : "Liehuang Zhu"
      },
      {
        "name" : "Giovanni Russello"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00966v1",
    "title" : "Guaranteeing Data Privacy in Federated Unlearning with Dynamic User\n  Participation",
    "summary" : "Federated Unlearning (FU) is gaining prominence for its capacity to eliminate\ninfluences of Federated Learning (FL) users' data from trained global FL\nmodels. A straightforward FU method involves removing the unlearned users and\nsubsequently retraining a new global FL model from scratch with all remaining\nusers, a process that leads to considerable overhead. To enhance unlearning\nefficiency, a widely adopted strategy employs clustering, dividing FL users\ninto clusters, with each cluster maintaining its own FL model. The final\ninference is then determined by aggregating the majority vote from the\ninferences of these sub-models. This method confines unlearning processes to\nindividual clusters for removing a user, thereby enhancing unlearning\nefficiency by eliminating the need for participation from all remaining users.\nHowever, current clustering-based FU schemes mainly concentrate on refining\nclustering to boost unlearning efficiency but overlook the potential\ninformation leakage from FL users' gradients, a privacy concern that has been\nextensively studied. Typically, integrating secure aggregation (SecAgg) schemes\nwithin each cluster can facilitate a privacy-preserving FU. Nevertheless,\ncrafting a clustering methodology that seamlessly incorporates SecAgg schemes\nis challenging, particularly in scenarios involving adversarial users and\ndynamic users. In this connection, we systematically explore the integration of\nSecAgg protocols within the most widely used federated unlearning scheme, which\nis based on clustering, to establish a privacy-preserving FU framework, aimed\nat ensuring privacy while effectively managing dynamic user participation.\nComprehensive theoretical assessments and experimental results show that our\nproposed scheme achieves comparable unlearning effectiveness, alongside\noffering improved privacy protection and resilience in the face of varying user\nparticipation.",
    "updated" : "2024-06-03T03:39:07Z",
    "published" : "2024-06-03T03:39:07Z",
    "authors" : [
      {
        "name" : "Ziyao Liu"
      },
      {
        "name" : "Yu Jiang"
      },
      {
        "name" : "Weifeng Jiang"
      },
      {
        "name" : "Jiale Guo"
      },
      {
        "name" : "Jun Zhao"
      },
      {
        "name" : "Kwok-Yan Lam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00808v1",
    "title" : "EchoNet-Synthetic: Privacy-preserving Video Generation for Safe Medical\n  Data Sharing",
    "summary" : "To make medical datasets accessible without sharing sensitive patient\ninformation, we introduce a novel end-to-end approach for generative\nde-identification of dynamic medical imaging data. Until now, generative\nmethods have faced constraints in terms of fidelity, spatio-temporal coherence,\nand the length of generation, failing to capture the complete details of\ndataset distributions. We present a model designed to produce high-fidelity,\nlong and complete data samples with near-real-time efficiency and explore our\napproach on a challenging task: generating echocardiogram videos. We develop\nour generation method based on diffusion models and introduce a protocol for\nmedical video dataset anonymization. As an exemplar, we present\nEchoNet-Synthetic, a fully synthetic, privacy-compliant echocardiogram dataset\nwith paired ejection fraction labels. As part of our de-identification\nprotocol, we evaluate the quality of the generated dataset and propose to use\nclinical downstream tasks as a measurement on top of widely used but\npotentially biased image quality metrics. Experimental outcomes demonstrate\nthat EchoNet-Synthetic achieves comparable dataset fidelity to the actual\ndataset, effectively supporting the ejection fraction regression task. Code,\nweights and dataset are available at\nhttps://github.com/HReynaud/EchoNet-Synthetic.",
    "updated" : "2024-06-02T17:18:06Z",
    "published" : "2024-06-02T17:18:06Z",
    "authors" : [
      {
        "name" : "Hadrien Reynaud"
      },
      {
        "name" : "Qingjie Meng"
      },
      {
        "name" : "Mischa Dombrowski"
      },
      {
        "name" : "Arijit Ghosh"
      },
      {
        "name" : "Thomas Day"
      },
      {
        "name" : "Alberto Gomez"
      },
      {
        "name" : "Paul Leeson"
      },
      {
        "name" : "Bernhard Kainz"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00359v1",
    "title" : "Location Privacy in B5G/6G: Systematization of Knowledge",
    "summary" : "As we transition into the era of B5G/6G networks, the promise of seamless,\nhigh-speed connectivity brings unprecedented opportunities and challenges.\nAmong the most critical concerns is the preservation of location privacy, given\nthe enhanced precision and pervasive connectivity of these advanced networks.\nThis paper systematically reviews the state of knowledge on location privacy in\nB5G/6G networks, highlighting the architectural advancements and\ninfrastructural complexities that contribute to increased privacy risks. The\nurgency of studying these technologies is underscored by the rapid adoption of\nB5G/6G and the growing sophistication of location tracking methods. We evaluate\ncurrent and emerging privacy-preserving mechanisms, exploring the implications\nof sophisticated tracking methods and the challenges posed by the complex\nnetwork infrastructures. Our findings reveal the effectiveness of various\nmitigation strategies and emphasize the important role of physical layer\nsecurity. Additionally, we propose innovative approaches, including\ndecentralized authentication systems and the potential of satellite\ncommunications, to enhance location privacy. By addressing these challenges,\nthis paper provides a comprehensive perspective on preserving user privacy in\nthe rapidly evolving landscape of modern communication networks.",
    "updated" : "2024-06-01T08:25:07Z",
    "published" : "2024-06-01T08:25:07Z",
    "authors" : [
      {
        "name" : "Hannah B. Pasandi"
      },
      {
        "name" : "Faith Parastar"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00256v1",
    "title" : "Over-the-Air Collaborative Inference with Feature Differential Privacy",
    "summary" : "Collaborative inference in next-generation networks can enhance Artificial\nIntelligence (AI) applications, including autonomous driving, personal\nidentification, and activity classification. This method involves a three-stage\nprocess: a) data acquisition through sensing, b) feature extraction, and c)\nfeature encoding for transmission. Transmission of the extracted features\nentails the potential risk of exposing sensitive personal data. To address this\nissue, in this work a new privacy-protecting collaborative inference mechanism\nis developed. Under this mechanism, each edge device in the network protects\nthe privacy of extracted features before transmitting them to a central server\nfor inference. This mechanism aims to achieve two main objectives while\nensuring effective inference performance: 1) reducing communication overhead,\nand 2) maintaining strict privacy guarantees during features transmission.",
    "updated" : "2024-06-01T01:39:44Z",
    "published" : "2024-06-01T01:39:44Z",
    "authors" : [
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Yuqi Nie"
      },
      {
        "name" : "Andrea Goldsmith"
      },
      {
        "name" : "Vincent Poor"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00249v1",
    "title" : "Privacy Challenges in Meta-Learning: An Investigation on Model-Agnostic\n  Meta-Learning",
    "summary" : "Meta-learning involves multiple learners, each dedicated to specific tasks,\ncollaborating in a data-constrained setting. In current meta-learning methods,\ntask learners locally learn models from sensitive data, termed support sets.\nThese task learners subsequently share model-related information, such as\ngradients or loss values, which is computed using another part of the data\ntermed query set, with a meta-learner. The meta-learner employs this\ninformation to update its meta-knowledge. Despite the absence of explicit data\nsharing, privacy concerns persist. This paper examines potential data leakage\nin a prominent metalearning algorithm, specifically Model-Agnostic\nMeta-Learning (MAML). In MAML, gradients are shared between the metalearner and\ntask-learners. The primary objective is to scrutinize the gradient and the\ninformation it encompasses about the task dataset. Subsequently, we endeavor to\npropose membership inference attacks targeting the task dataset containing\nsupport and query sets. Finally, we explore various noise injection methods\ndesigned to safeguard the privacy of task data and thwart potential attacks.\nExperimental results demonstrate the effectiveness of these attacks on MAML and\nthe efficacy of proper noise injection methods in countering them.",
    "updated" : "2024-06-01T01:10:35Z",
    "published" : "2024-06-01T01:10:35Z",
    "authors" : [
      {
        "name" : "Mina Rafiei"
      },
      {
        "name" : "Mohammadmahdi Maheri"
      },
      {
        "name" : "Hamid R. Rabiee"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03187v1",
    "title" : "Ariadne: a Privacy-Preserving Communication Protocol",
    "summary" : "In this article, we present Ariadne, a privacy-preserving communication\nnetwork layer protocol that uses a source routing approach to avoid relying on\ntrusted third parties. In Ariadne, a source node willing to send anonymized\nnetwork traffic to a destination uses a path consisting in nodes with which it\nhas pre-shared symmetric keys. Temporary keys derived from those pre-shared\nkeys to protect communication privacy using onion routing techniques, ensuring\nsession unlinkability for packets following the same path.\n  Ariadne enhances previous approaches to preserve communication privacy by\nintroducing two novelties. First, the source route is encoded in a fixed size,\nsequentially encrypted vector of routing information elements, in which the\nelements' positions in the vector are pseudo-randomly permuted. Second, the\ntemporary keys used to process the packets on the path are referenced using\nmutually known encrypted patterns. This avoids the use of an explicit key\nreference that could be used to de-anonymize the communications.",
    "updated" : "2024-06-05T12:20:12Z",
    "published" : "2024-06-05T12:20:12Z",
    "authors" : [
      {
        "name" : "Antoine Fressancourt"
      },
      {
        "name" : "Luigi Iannone"
      },
      {
        "name" : "Mael Kerichard"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02797v1",
    "title" : "Auditing Privacy Mechanisms via Label Inference Attacks",
    "summary" : "We propose reconstruction advantage measures to audit label privatization\nmechanisms. A reconstruction advantage measure quantifies the increase in an\nattacker's ability to infer the true label of an unlabeled example when\nprovided with a private version of the labels in a dataset (e.g., aggregate of\nlabels from different users or noisy labels output by randomized response),\ncompared to an attacker that only observes the feature vectors, but may have\nprior knowledge of the correlation between features and labels. We consider two\nsuch auditing measures: one additive, and one multiplicative. These incorporate\nprevious approaches taken in the literature on empirical auditing and\ndifferential privacy. The measures allow us to place a variety of proposed\nprivatization schemes -- some differentially private, some not -- on the same\nfooting. We analyze these measures theoretically under a distributional model\nwhich encapsulates reasonable adversarial settings. We also quantify their\nbehavior empirically on real and simulated prediction tasks. Across a range of\nexperimental settings, we find that differentially private schemes dominate or\nmatch the privacy-utility tradeoff of more heuristic approaches.",
    "updated" : "2024-06-04T21:48:30Z",
    "published" : "2024-06-04T21:48:30Z",
    "authors" : [
      {
        "name" : "Róbert István Busa-Fekete"
      },
      {
        "name" : "Travis Dick"
      },
      {
        "name" : "Claudio Gentile"
      },
      {
        "name" : "Andrés Muñoz Medina"
      },
      {
        "name" : "Adam Smith"
      },
      {
        "name" : "Marika Swanberg"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02794v1",
    "title" : "PriME: Privacy-aware Membership profile Estimation in networks",
    "summary" : "This paper presents a novel approach to estimating community membership\nprobabilities for network vertices generated by the Degree Corrected Mixed\nMembership Stochastic Block Model while preserving individual edge privacy.\nOperating within the $\\varepsilon$-edge local differential privacy framework,\nwe introduce an optimal private algorithm based on a symmetric edge flip\nmechanism and spectral clustering for accurate estimation of vertex community\nmemberships. We conduct a comprehensive analysis of the estimation risk and\nestablish the optimality of our procedure by providing matching lower bounds to\nthe minimax risk under privacy constraints. To validate our approach, we\ndemonstrate its performance through numerical simulations and its practical\napplication to real-world data. This work represents a significant step forward\nin balancing accurate community membership estimation with stringent privacy\npreservation in network data analysis.",
    "updated" : "2024-06-04T21:43:49Z",
    "published" : "2024-06-04T21:43:49Z",
    "authors" : [
      {
        "name" : "Abhinav Chakraborty"
      },
      {
        "name" : "Sayak Chatterjee"
      },
      {
        "name" : "Sagnik Nandy"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.SI",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03404v1",
    "title" : "ST-DPGAN: A Privacy-preserving Framework for Spatiotemporal Data\n  Generation",
    "summary" : "Spatiotemporal data is prevalent in a wide range of edge devices, such as\nthose used in personal communication and financial transactions. Recent\nadvancements have sparked a growing interest in integrating spatiotemporal\nanalysis with large-scale language models. However, spatiotemporal data often\ncontains sensitive information, making it unsuitable for open third-party\naccess. To address this challenge, we propose a Graph-GAN-based model for\ngenerating privacy-protected spatiotemporal data. Our approach incorporates\nspatial and temporal attention blocks in the discriminator and a spatiotemporal\ndeconvolution structure in the generator. These enhancements enable efficient\ntraining under Gaussian noise to achieve differential privacy. Extensive\nexperiments conducted on three real-world spatiotemporal datasets validate the\nefficacy of our model. Our method provides a privacy guarantee while\nmaintaining the data utility. The prediction model trained on our generated\ndata maintains a competitive performance compared to the model trained on the\noriginal data.",
    "updated" : "2024-06-04T04:43:54Z",
    "published" : "2024-06-04T04:43:54Z",
    "authors" : [
      {
        "name" : "Wei Shao"
      },
      {
        "name" : "Rongyi Zhu"
      },
      {
        "name" : "Cai Yang"
      },
      {
        "name" : "Chandra Thapa"
      },
      {
        "name" : "Muhammad Ejaz Ahmed"
      },
      {
        "name" : "Seyit Camtepe"
      },
      {
        "name" : "Rui Zhang"
      },
      {
        "name" : "DuYong Kim"
      },
      {
        "name" : "Hamid Menouar"
      },
      {
        "name" : "Flora D. Salim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02629v1",
    "title" : "SSNet: A Lightweight Multi-Party Computation Scheme for Practical\n  Privacy-Preserving Machine Learning Service in the Cloud",
    "summary" : "As privacy-preserving becomes a pivotal aspect of deep learning (DL)\ndevelopment, multi-party computation (MPC) has gained prominence for its\nefficiency and strong security. However, the practice of current MPC frameworks\nis limited, especially when dealing with large neural networks, exemplified by\nthe prolonged execution time of 25.8 seconds for secure inference on\nResNet-152. The primary challenge lies in the reliance of current MPC\napproaches on additive secret sharing, which incurs significant communication\noverhead with non-linear operations such as comparisons. Furthermore, additive\nsharing suffers from poor scalability on party size. In contrast, the evolving\nlandscape of MPC necessitates accommodating a larger number of compute parties\nand ensuring robust performance against malicious activities or computational\nfailures.\n  In light of these challenges, we propose SSNet, which for the first time,\nemploys Shamir's secret sharing (SSS) as the backbone of MPC-based ML\nframework. We meticulously develop all framework primitives and operations for\nsecure DL models tailored to seamlessly integrate with the SSS scheme. SSNet\ndemonstrates the ability to scale up party numbers straightforwardly and embeds\nstrategies to authenticate the computation correctness without incurring\nsignificant performance overhead. Additionally, SSNet introduces masking\nstrategies designed to reduce communication overhead associated with non-linear\noperations. We conduct comprehensive experimental evaluations on commercial\ncloud computing infrastructure from Amazon AWS, as well as across diverse\nprevalent DNN models and datasets. SSNet demonstrates a substantial performance\nboost, achieving speed-ups ranging from 3x to 14x compared to SOTA MPC\nframeworks. Moreover, SSNet also represents the first framework that is\nevaluated on a five-party computation setup, in the context of secure DL\ninference.",
    "updated" : "2024-06-04T00:55:06Z",
    "published" : "2024-06-04T00:55:06Z",
    "authors" : [
      {
        "name" : "Shijin Duan"
      },
      {
        "name" : "Chenghong Wang"
      },
      {
        "name" : "Hongwu Peng"
      },
      {
        "name" : "Yukui Luo"
      },
      {
        "name" : "Wujie Wen"
      },
      {
        "name" : "Caiwen Ding"
      },
      {
        "name" : "Xiaolin Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02599v1",
    "title" : "Privacy-Aware Randomized Quantization via Linear Programming",
    "summary" : "Differential privacy mechanisms such as the Gaussian or Laplace mechanism\nhave been widely used in data analytics for preserving individual privacy.\nHowever, they are mostly designed for continuous outputs and are unsuitable for\nscenarios where discrete values are necessary. Although various quantization\nmechanisms were proposed recently to generate discrete outputs under\ndifferential privacy, the outcomes are either biased or have an inferior\naccuracy-privacy trade-off. In this paper, we propose a family of quantization\nmechanisms that is unbiased and differentially private. It has a high degree of\nfreedom and we show that some existing mechanisms can be considered as special\ncases of ours. To find the optimal mechanism, we formulate a linear\noptimization that can be solved efficiently using linear programming tools.\nExperiments show that our proposed mechanism can attain a better\nprivacy-accuracy trade-off compared to baselines.",
    "updated" : "2024-06-01T18:40:08Z",
    "published" : "2024-06-01T18:40:08Z",
    "authors" : [
      {
        "name" : "Zhongteng Cai"
      },
      {
        "name" : "Xueru Zhang"
      },
      {
        "name" : "Mohammad Mahdi Khalili"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  }
]