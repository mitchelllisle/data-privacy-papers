[
  {
    "id" : "http://arxiv.org/abs/2406.02520v1",
    "title" : "Digital Privacy for Migrants: Exploring Current Research Trends and\n  Future Prospects",
    "summary" : "This paper explores digital privacy challenges for migrants, analyzing trends\nfrom 2013 to 2023. Migrants face heightened risks such as government\nsurveillance and identity theft. Understanding these threats is vital for\nraising awareness and guiding research towards effective solutions and policies\nto protect migrant digital privacy.",
    "updated" : "2024-06-04T17:41:20Z",
    "published" : "2024-06-04T17:41:20Z",
    "authors" : [
      {
        "name" : "Sarah Tabassum"
      },
      {
        "name" : "Cori Faklaris"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02463v1",
    "title" : "Click Without Compromise: Online Advertising Measurement via Per User\n  Differential Privacy",
    "summary" : "Online advertising is a cornerstone of the Internet ecosystem, with\nadvertising measurement playing a crucial role in optimizing efficiency. Ad\nmeasurement entails attributing desired behaviors, such as purchases, to ad\nexposures across various platforms, necessitating the collection of user\nactivities across these platforms. As this practice faces increasing\nrestrictions due to rising privacy concerns, safeguarding user privacy in this\ncontext is imperative. Our work is the first to formulate the real-world\nchallenge of advertising measurement systems with real-time reporting of\nstreaming data in advertising campaigns. We introduce Ads-BPC, a novel\nuser-level differential privacy protection scheme for advertising measurement\nresults. This approach optimizes global noise power and results in a\nnon-identically distributed noise distribution that preserves differential\nprivacy while enhancing measurement accuracy. Through experiments on both\nreal-world advertising campaigns and synthetic datasets, Ads-BPC achieves a 25%\nto 50% increase in accuracy over existing streaming DP mechanisms applied to\nadvertising measurement. This highlights our method's effectiveness in\nachieving superior accuracy alongside a formal privacy guarantee, thereby\nadvancing the state-of-the-art in privacy-preserving advertising measurement.",
    "updated" : "2024-06-04T16:31:19Z",
    "published" : "2024-06-04T16:31:19Z",
    "authors" : [
      {
        "name" : "Yingtai Xiao"
      },
      {
        "name" : "Jian Du"
      },
      {
        "name" : "Shikun Zhang"
      },
      {
        "name" : "Qiang Yan"
      },
      {
        "name" : "Danfeng Zhang"
      },
      {
        "name" : "Daniel Kifer"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02424v1",
    "title" : "Contextual Dynamic Pricing: Algorithms, Optimality, and Local\n  Differential Privacy Constraints",
    "summary" : "We study the contextual dynamic pricing problem where a firm sells products\nto $T$ sequentially arriving consumers that behave according to an unknown\ndemand model. The firm aims to maximize its revenue, i.e. minimize its regret\nover a clairvoyant that knows the model in advance. The demand model is a\ngeneralized linear model (GLM), allowing for a stochastic feature vector in\n$\\mathbb R^d$ that encodes product and consumer information. We first show that\nthe optimal regret upper bound is of order $\\sqrt{dT}$, up to a logarithmic\nfactor, improving upon existing upper bounds in the literature by a $\\sqrt{d}$\nfactor. This sharper rate is materialised by two algorithms: a confidence\nbound-type (supCB) algorithm and an explore-then-commit (ETC) algorithm. A key\ninsight of our theoretical result is an intrinsic connection between dynamic\npricing and the contextual multi-armed bandit problem with many arms based on a\ncareful discretization. We further study contextual dynamic pricing under the\nlocal differential privacy (LDP) constraints. In particular, we propose a\nstochastic gradient descent based ETC algorithm that achieves an optimal regret\nupper bound of order $d\\sqrt{T}/\\epsilon$, up to a logarithmic factor, where\n$\\epsilon>0$ is the privacy parameter. The regret upper bounds with and without\nLDP constraints are accompanied by newly constructed minimax lower bounds,\nwhich further characterize the cost of privacy. Extensive numerical experiments\nand a real data application on online lending are conducted to illustrate the\nefficiency and practical value of the proposed algorithms in dynamic pricing.",
    "updated" : "2024-06-04T15:44:10Z",
    "published" : "2024-06-04T15:44:10Z",
    "authors" : [
      {
        "name" : "Zifeng Zhao"
      },
      {
        "name" : "Feiyu Jiang"
      },
      {
        "name" : "Yi Yu"
      }
    ],
    "categories" : [
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01811v1",
    "title" : "A Game-Theoretic Approach to Privacy-Utility Tradeoff in Sharing Genomic\n  Summary Statistics",
    "summary" : "The advent of online genomic data-sharing services has sought to enhance the\naccessibility of large genomic datasets by allowing queries about genetic\nvariants, such as summary statistics, aiding care providers in distinguishing\nbetween spurious genomic variations and those with clinical significance.\nHowever, numerous studies have demonstrated that even sharing summary genomic\ninformation exposes individual members of such datasets to a significant\nprivacy risk due to membership inference attacks. While several approaches have\nemerged that reduce privacy risks by adding noise or reducing the amount of\ninformation shared, these typically assume non-adaptive attacks that use\nlikelihood ratio test (LRT) statistics. We propose a Bayesian game-theoretic\nframework for optimal privacy-utility tradeoff in the sharing of genomic\nsummary statistics. Our first contribution is to prove that a very general\nBayesian attacker model that anchors our game-theoretic approach is more\npowerful than the conventional LRT-based threat models in that it induces worse\nprivacy loss for the defender who is modeled as a von Neumann-Morgenstern (vNM)\ndecision-maker. We show this to be true even when the attacker uses a\nnon-informative subjective prior. Next, we present an analytically tractable\napproach to compare the Bayesian attacks with arbitrary subjective priors and\nthe Neyman-Pearson optimal LRT attacks under the Gaussian mechanism common in\ndifferential privacy frameworks. Finally, we propose an approach for\napproximating Bayes-Nash equilibria of the game using deep neural network\ngenerators to implicitly represent player mixed strategies. Our experiments\ndemonstrate that the proposed game-theoretic framework yields both stronger\nattacks and stronger defense strategies than the state of the art.",
    "updated" : "2024-06-03T22:09:47Z",
    "published" : "2024-06-03T22:09:47Z",
    "authors" : [
      {
        "name" : "Tao Zhang"
      },
      {
        "name" : "Rajagopal Venkatesaramani"
      },
      {
        "name" : "Rajat K. De"
      },
      {
        "name" : "Bradley A. Malin"
      },
      {
        "name" : "Yevgeniy Vorobeychik"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01394v1",
    "title" : "PrivacyRestore: Privacy-Preserving Inference in Large Language Models\n  via Privacy Removal and Restoration",
    "summary" : "The widespread usage of online Large Language Models (LLMs) inference\nservices has raised significant privacy concerns about the potential exposure\nof private information in user inputs to eavesdroppers or untrustworthy service\nproviders. Existing privacy protection methods for LLMs suffer from\ninsufficient privacy protection, performance degradation, or severe inference\ntime overhead. In this paper, we propose PrivacyRestore to protect the privacy\nof user inputs during LLM inference. PrivacyRestore directly removes privacy\nspans in user inputs and restores privacy information via activation steering\nduring inference. The privacy spans are encoded as restoration vectors. We\npropose Attention-aware Weighted Aggregation (AWA) which aggregates restoration\nvectors of all privacy spans in the input into a meta restoration vector. AWA\nnot only ensures proper representation of all privacy spans but also prevents\nattackers from inferring the privacy spans from the meta restoration vector\nalone. This meta restoration vector, along with the query with privacy spans\nremoved, is then sent to the server. The experimental results show that\nPrivacyRestore can protect private information while maintaining acceptable\nlevels of performance and inference efficiency.",
    "updated" : "2024-06-03T14:57:39Z",
    "published" : "2024-06-03T14:57:39Z",
    "authors" : [
      {
        "name" : "Ziqian Zeng"
      },
      {
        "name" : "Jianwei Wang"
      },
      {
        "name" : "Zhengdong Lu"
      },
      {
        "name" : "Huiping Zhuang"
      },
      {
        "name" : "Cen Chen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01363v1",
    "title" : "Privacy in LLM-based Recommendation: Recent Advances and Future\n  Directions",
    "summary" : "Nowadays, large language models (LLMs) have been integrated with conventional\nrecommendation models to improve recommendation performance. However, while\nmost of the existing works have focused on improving the model performance, the\nprivacy issue has only received comparatively less attention. In this paper, we\nreview recent advancements in privacy within LLM-based recommendation,\ncategorizing them into privacy attacks and protection mechanisms. Additionally,\nwe highlight several challenges and propose future directions for the community\nto address these critical problems.",
    "updated" : "2024-06-03T14:31:47Z",
    "published" : "2024-06-03T14:31:47Z",
    "authors" : [
      {
        "name" : "Sichun Luo"
      },
      {
        "name" : "Wei Shao"
      },
      {
        "name" : "Yuxuan Yao"
      },
      {
        "name" : "Jian Xu"
      },
      {
        "name" : "Mingyang Liu"
      },
      {
        "name" : "Qintong Li"
      },
      {
        "name" : "Bowei He"
      },
      {
        "name" : "Maolin Wang"
      },
      {
        "name" : "Guanzhi Deng"
      },
      {
        "name" : "Hanxu Hou"
      },
      {
        "name" : "Xinyi Zhang"
      },
      {
        "name" : "Linqi Song"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01085v1",
    "title" : "FedAdOb: Privacy-Preserving Federated Deep Learning with Adaptive\n  Obfuscation",
    "summary" : "Federated learning (FL) has emerged as a collaborative approach that allows\nmultiple clients to jointly learn a machine learning model without sharing\ntheir private data. The concern about privacy leakage, albeit demonstrated\nunder specific conditions, has triggered numerous follow-up research in\ndesigning powerful attacking methods and effective defending mechanisms aiming\nto thwart these attacking methods. Nevertheless, privacy-preserving mechanisms\nemployed in these defending methods invariably lead to compromised model\nperformances due to a fixed obfuscation applied to private data or gradients.\nIn this article, we, therefore, propose a novel adaptive obfuscation mechanism,\ncoined FedAdOb, to protect private data without yielding original model\nperformances. Technically, FedAdOb utilizes passport-based adaptive obfuscation\nto ensure data privacy in both horizontal and vertical federated learning\nsettings. The privacy-preserving capabilities of FedAdOb, specifically with\nregard to private features and labels, are theoretically proven through\nTheorems 1 and 2. Furthermore, extensive experimental evaluations conducted on\nvarious datasets and network architectures demonstrate the effectiveness of\nFedAdOb by manifesting its superior trade-off between privacy preservation and\nmodel performance, surpassing existing methods.",
    "updated" : "2024-06-03T08:12:09Z",
    "published" : "2024-06-03T08:12:09Z",
    "authors" : [
      {
        "name" : "Hanlin Gu"
      },
      {
        "name" : "Jiahuan Luo"
      },
      {
        "name" : "Yan Kang"
      },
      {
        "name" : "Yuan Yao"
      },
      {
        "name" : "Gongxi Zhu"
      },
      {
        "name" : "Bowen Li"
      },
      {
        "name" : "Lixin Fan"
      },
      {
        "name" : "Qiang Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01080v1",
    "title" : "No Vandalism: Privacy-Preserving and Byzantine-Robust Federated Learning",
    "summary" : "Federated learning allows several clients to train one machine learning model\njointly without sharing private data, providing privacy protection. However,\ntraditional federated learning is vulnerable to poisoning attacks, which can\nnot only decrease the model performance, but also implant malicious backdoors.\nIn addition, direct submission of local model parameters can also lead to the\nprivacy leakage of the training dataset. In this paper, we aim to build a\nprivacy-preserving and Byzantine-robust federated learning scheme to provide an\nenvironment with no vandalism (NoV) against attacks from malicious\nparticipants. Specifically, we construct a model filter for poisoned local\nmodels, protecting the global model from data and model poisoning attacks. This\nmodel filter combines zero-knowledge proofs to provide further privacy\nprotection. Then, we adopt secret sharing to provide verifiable secure\naggregation, removing malicious clients that disrupting the aggregation\nprocess. Our formal analysis proves that NoV can protect data privacy and weed\nout Byzantine attackers. Our experiments illustrate that NoV can effectively\naddress data and model poisoning attacks, including PGD, and outperforms other\nrelated schemes.",
    "updated" : "2024-06-03T07:59:10Z",
    "published" : "2024-06-03T07:59:10Z",
    "authors" : [
      {
        "name" : "Zhibo Xing"
      },
      {
        "name" : "Zijian Zhang"
      },
      {
        "name" : "Zi'ang Zhang"
      },
      {
        "name" : "Jiamou Liu"
      },
      {
        "name" : "Liehuang Zhu"
      },
      {
        "name" : "Giovanni Russello"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00966v1",
    "title" : "Guaranteeing Data Privacy in Federated Unlearning with Dynamic User\n  Participation",
    "summary" : "Federated Unlearning (FU) is gaining prominence for its capacity to eliminate\ninfluences of Federated Learning (FL) users' data from trained global FL\nmodels. A straightforward FU method involves removing the unlearned users and\nsubsequently retraining a new global FL model from scratch with all remaining\nusers, a process that leads to considerable overhead. To enhance unlearning\nefficiency, a widely adopted strategy employs clustering, dividing FL users\ninto clusters, with each cluster maintaining its own FL model. The final\ninference is then determined by aggregating the majority vote from the\ninferences of these sub-models. This method confines unlearning processes to\nindividual clusters for removing a user, thereby enhancing unlearning\nefficiency by eliminating the need for participation from all remaining users.\nHowever, current clustering-based FU schemes mainly concentrate on refining\nclustering to boost unlearning efficiency but overlook the potential\ninformation leakage from FL users' gradients, a privacy concern that has been\nextensively studied. Typically, integrating secure aggregation (SecAgg) schemes\nwithin each cluster can facilitate a privacy-preserving FU. Nevertheless,\ncrafting a clustering methodology that seamlessly incorporates SecAgg schemes\nis challenging, particularly in scenarios involving adversarial users and\ndynamic users. In this connection, we systematically explore the integration of\nSecAgg protocols within the most widely used federated unlearning scheme, which\nis based on clustering, to establish a privacy-preserving FU framework, aimed\nat ensuring privacy while effectively managing dynamic user participation.\nComprehensive theoretical assessments and experimental results show that our\nproposed scheme achieves comparable unlearning effectiveness, alongside\noffering improved privacy protection and resilience in the face of varying user\nparticipation.",
    "updated" : "2024-06-03T03:39:07Z",
    "published" : "2024-06-03T03:39:07Z",
    "authors" : [
      {
        "name" : "Ziyao Liu"
      },
      {
        "name" : "Yu Jiang"
      },
      {
        "name" : "Weifeng Jiang"
      },
      {
        "name" : "Jiale Guo"
      },
      {
        "name" : "Jun Zhao"
      },
      {
        "name" : "Kwok-Yan Lam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00808v1",
    "title" : "EchoNet-Synthetic: Privacy-preserving Video Generation for Safe Medical\n  Data Sharing",
    "summary" : "To make medical datasets accessible without sharing sensitive patient\ninformation, we introduce a novel end-to-end approach for generative\nde-identification of dynamic medical imaging data. Until now, generative\nmethods have faced constraints in terms of fidelity, spatio-temporal coherence,\nand the length of generation, failing to capture the complete details of\ndataset distributions. We present a model designed to produce high-fidelity,\nlong and complete data samples with near-real-time efficiency and explore our\napproach on a challenging task: generating echocardiogram videos. We develop\nour generation method based on diffusion models and introduce a protocol for\nmedical video dataset anonymization. As an exemplar, we present\nEchoNet-Synthetic, a fully synthetic, privacy-compliant echocardiogram dataset\nwith paired ejection fraction labels. As part of our de-identification\nprotocol, we evaluate the quality of the generated dataset and propose to use\nclinical downstream tasks as a measurement on top of widely used but\npotentially biased image quality metrics. Experimental outcomes demonstrate\nthat EchoNet-Synthetic achieves comparable dataset fidelity to the actual\ndataset, effectively supporting the ejection fraction regression task. Code,\nweights and dataset are available at\nhttps://github.com/HReynaud/EchoNet-Synthetic.",
    "updated" : "2024-06-02T17:18:06Z",
    "published" : "2024-06-02T17:18:06Z",
    "authors" : [
      {
        "name" : "Hadrien Reynaud"
      },
      {
        "name" : "Qingjie Meng"
      },
      {
        "name" : "Mischa Dombrowski"
      },
      {
        "name" : "Arijit Ghosh"
      },
      {
        "name" : "Thomas Day"
      },
      {
        "name" : "Alberto Gomez"
      },
      {
        "name" : "Paul Leeson"
      },
      {
        "name" : "Bernhard Kainz"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00359v1",
    "title" : "Location Privacy in B5G/6G: Systematization of Knowledge",
    "summary" : "As we transition into the era of B5G/6G networks, the promise of seamless,\nhigh-speed connectivity brings unprecedented opportunities and challenges.\nAmong the most critical concerns is the preservation of location privacy, given\nthe enhanced precision and pervasive connectivity of these advanced networks.\nThis paper systematically reviews the state of knowledge on location privacy in\nB5G/6G networks, highlighting the architectural advancements and\ninfrastructural complexities that contribute to increased privacy risks. The\nurgency of studying these technologies is underscored by the rapid adoption of\nB5G/6G and the growing sophistication of location tracking methods. We evaluate\ncurrent and emerging privacy-preserving mechanisms, exploring the implications\nof sophisticated tracking methods and the challenges posed by the complex\nnetwork infrastructures. Our findings reveal the effectiveness of various\nmitigation strategies and emphasize the important role of physical layer\nsecurity. Additionally, we propose innovative approaches, including\ndecentralized authentication systems and the potential of satellite\ncommunications, to enhance location privacy. By addressing these challenges,\nthis paper provides a comprehensive perspective on preserving user privacy in\nthe rapidly evolving landscape of modern communication networks.",
    "updated" : "2024-06-01T08:25:07Z",
    "published" : "2024-06-01T08:25:07Z",
    "authors" : [
      {
        "name" : "Hannah B. Pasandi"
      },
      {
        "name" : "Faith Parastar"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00256v1",
    "title" : "Over-the-Air Collaborative Inference with Feature Differential Privacy",
    "summary" : "Collaborative inference in next-generation networks can enhance Artificial\nIntelligence (AI) applications, including autonomous driving, personal\nidentification, and activity classification. This method involves a three-stage\nprocess: a) data acquisition through sensing, b) feature extraction, and c)\nfeature encoding for transmission. Transmission of the extracted features\nentails the potential risk of exposing sensitive personal data. To address this\nissue, in this work a new privacy-protecting collaborative inference mechanism\nis developed. Under this mechanism, each edge device in the network protects\nthe privacy of extracted features before transmitting them to a central server\nfor inference. This mechanism aims to achieve two main objectives while\nensuring effective inference performance: 1) reducing communication overhead,\nand 2) maintaining strict privacy guarantees during features transmission.",
    "updated" : "2024-06-01T01:39:44Z",
    "published" : "2024-06-01T01:39:44Z",
    "authors" : [
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Yuqi Nie"
      },
      {
        "name" : "Andrea Goldsmith"
      },
      {
        "name" : "Vincent Poor"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00249v1",
    "title" : "Privacy Challenges in Meta-Learning: An Investigation on Model-Agnostic\n  Meta-Learning",
    "summary" : "Meta-learning involves multiple learners, each dedicated to specific tasks,\ncollaborating in a data-constrained setting. In current meta-learning methods,\ntask learners locally learn models from sensitive data, termed support sets.\nThese task learners subsequently share model-related information, such as\ngradients or loss values, which is computed using another part of the data\ntermed query set, with a meta-learner. The meta-learner employs this\ninformation to update its meta-knowledge. Despite the absence of explicit data\nsharing, privacy concerns persist. This paper examines potential data leakage\nin a prominent metalearning algorithm, specifically Model-Agnostic\nMeta-Learning (MAML). In MAML, gradients are shared between the metalearner and\ntask-learners. The primary objective is to scrutinize the gradient and the\ninformation it encompasses about the task dataset. Subsequently, we endeavor to\npropose membership inference attacks targeting the task dataset containing\nsupport and query sets. Finally, we explore various noise injection methods\ndesigned to safeguard the privacy of task data and thwart potential attacks.\nExperimental results demonstrate the effectiveness of these attacks on MAML and\nthe efficacy of proper noise injection methods in countering them.",
    "updated" : "2024-06-01T01:10:35Z",
    "published" : "2024-06-01T01:10:35Z",
    "authors" : [
      {
        "name" : "Mina Rafiei"
      },
      {
        "name" : "Mohammadmahdi Maheri"
      },
      {
        "name" : "Hamid R. Rabiee"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03187v1",
    "title" : "Ariadne: a Privacy-Preserving Communication Protocol",
    "summary" : "In this article, we present Ariadne, a privacy-preserving communication\nnetwork layer protocol that uses a source routing approach to avoid relying on\ntrusted third parties. In Ariadne, a source node willing to send anonymized\nnetwork traffic to a destination uses a path consisting in nodes with which it\nhas pre-shared symmetric keys. Temporary keys derived from those pre-shared\nkeys to protect communication privacy using onion routing techniques, ensuring\nsession unlinkability for packets following the same path.\n  Ariadne enhances previous approaches to preserve communication privacy by\nintroducing two novelties. First, the source route is encoded in a fixed size,\nsequentially encrypted vector of routing information elements, in which the\nelements' positions in the vector are pseudo-randomly permuted. Second, the\ntemporary keys used to process the packets on the path are referenced using\nmutually known encrypted patterns. This avoids the use of an explicit key\nreference that could be used to de-anonymize the communications.",
    "updated" : "2024-06-05T12:20:12Z",
    "published" : "2024-06-05T12:20:12Z",
    "authors" : [
      {
        "name" : "Antoine Fressancourt"
      },
      {
        "name" : "Luigi Iannone"
      },
      {
        "name" : "Mael Kerichard"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02797v1",
    "title" : "Auditing Privacy Mechanisms via Label Inference Attacks",
    "summary" : "We propose reconstruction advantage measures to audit label privatization\nmechanisms. A reconstruction advantage measure quantifies the increase in an\nattacker's ability to infer the true label of an unlabeled example when\nprovided with a private version of the labels in a dataset (e.g., aggregate of\nlabels from different users or noisy labels output by randomized response),\ncompared to an attacker that only observes the feature vectors, but may have\nprior knowledge of the correlation between features and labels. We consider two\nsuch auditing measures: one additive, and one multiplicative. These incorporate\nprevious approaches taken in the literature on empirical auditing and\ndifferential privacy. The measures allow us to place a variety of proposed\nprivatization schemes -- some differentially private, some not -- on the same\nfooting. We analyze these measures theoretically under a distributional model\nwhich encapsulates reasonable adversarial settings. We also quantify their\nbehavior empirically on real and simulated prediction tasks. Across a range of\nexperimental settings, we find that differentially private schemes dominate or\nmatch the privacy-utility tradeoff of more heuristic approaches.",
    "updated" : "2024-06-04T21:48:30Z",
    "published" : "2024-06-04T21:48:30Z",
    "authors" : [
      {
        "name" : "Róbert István Busa-Fekete"
      },
      {
        "name" : "Travis Dick"
      },
      {
        "name" : "Claudio Gentile"
      },
      {
        "name" : "Andrés Muñoz Medina"
      },
      {
        "name" : "Adam Smith"
      },
      {
        "name" : "Marika Swanberg"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02794v1",
    "title" : "PriME: Privacy-aware Membership profile Estimation in networks",
    "summary" : "This paper presents a novel approach to estimating community membership\nprobabilities for network vertices generated by the Degree Corrected Mixed\nMembership Stochastic Block Model while preserving individual edge privacy.\nOperating within the $\\varepsilon$-edge local differential privacy framework,\nwe introduce an optimal private algorithm based on a symmetric edge flip\nmechanism and spectral clustering for accurate estimation of vertex community\nmemberships. We conduct a comprehensive analysis of the estimation risk and\nestablish the optimality of our procedure by providing matching lower bounds to\nthe minimax risk under privacy constraints. To validate our approach, we\ndemonstrate its performance through numerical simulations and its practical\napplication to real-world data. This work represents a significant step forward\nin balancing accurate community membership estimation with stringent privacy\npreservation in network data analysis.",
    "updated" : "2024-06-04T21:43:49Z",
    "published" : "2024-06-04T21:43:49Z",
    "authors" : [
      {
        "name" : "Abhinav Chakraborty"
      },
      {
        "name" : "Sayak Chatterjee"
      },
      {
        "name" : "Sagnik Nandy"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.SI",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03404v1",
    "title" : "ST-DPGAN: A Privacy-preserving Framework for Spatiotemporal Data\n  Generation",
    "summary" : "Spatiotemporal data is prevalent in a wide range of edge devices, such as\nthose used in personal communication and financial transactions. Recent\nadvancements have sparked a growing interest in integrating spatiotemporal\nanalysis with large-scale language models. However, spatiotemporal data often\ncontains sensitive information, making it unsuitable for open third-party\naccess. To address this challenge, we propose a Graph-GAN-based model for\ngenerating privacy-protected spatiotemporal data. Our approach incorporates\nspatial and temporal attention blocks in the discriminator and a spatiotemporal\ndeconvolution structure in the generator. These enhancements enable efficient\ntraining under Gaussian noise to achieve differential privacy. Extensive\nexperiments conducted on three real-world spatiotemporal datasets validate the\nefficacy of our model. Our method provides a privacy guarantee while\nmaintaining the data utility. The prediction model trained on our generated\ndata maintains a competitive performance compared to the model trained on the\noriginal data.",
    "updated" : "2024-06-04T04:43:54Z",
    "published" : "2024-06-04T04:43:54Z",
    "authors" : [
      {
        "name" : "Wei Shao"
      },
      {
        "name" : "Rongyi Zhu"
      },
      {
        "name" : "Cai Yang"
      },
      {
        "name" : "Chandra Thapa"
      },
      {
        "name" : "Muhammad Ejaz Ahmed"
      },
      {
        "name" : "Seyit Camtepe"
      },
      {
        "name" : "Rui Zhang"
      },
      {
        "name" : "DuYong Kim"
      },
      {
        "name" : "Hamid Menouar"
      },
      {
        "name" : "Flora D. Salim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02629v1",
    "title" : "SSNet: A Lightweight Multi-Party Computation Scheme for Practical\n  Privacy-Preserving Machine Learning Service in the Cloud",
    "summary" : "As privacy-preserving becomes a pivotal aspect of deep learning (DL)\ndevelopment, multi-party computation (MPC) has gained prominence for its\nefficiency and strong security. However, the practice of current MPC frameworks\nis limited, especially when dealing with large neural networks, exemplified by\nthe prolonged execution time of 25.8 seconds for secure inference on\nResNet-152. The primary challenge lies in the reliance of current MPC\napproaches on additive secret sharing, which incurs significant communication\noverhead with non-linear operations such as comparisons. Furthermore, additive\nsharing suffers from poor scalability on party size. In contrast, the evolving\nlandscape of MPC necessitates accommodating a larger number of compute parties\nand ensuring robust performance against malicious activities or computational\nfailures.\n  In light of these challenges, we propose SSNet, which for the first time,\nemploys Shamir's secret sharing (SSS) as the backbone of MPC-based ML\nframework. We meticulously develop all framework primitives and operations for\nsecure DL models tailored to seamlessly integrate with the SSS scheme. SSNet\ndemonstrates the ability to scale up party numbers straightforwardly and embeds\nstrategies to authenticate the computation correctness without incurring\nsignificant performance overhead. Additionally, SSNet introduces masking\nstrategies designed to reduce communication overhead associated with non-linear\noperations. We conduct comprehensive experimental evaluations on commercial\ncloud computing infrastructure from Amazon AWS, as well as across diverse\nprevalent DNN models and datasets. SSNet demonstrates a substantial performance\nboost, achieving speed-ups ranging from 3x to 14x compared to SOTA MPC\nframeworks. Moreover, SSNet also represents the first framework that is\nevaluated on a five-party computation setup, in the context of secure DL\ninference.",
    "updated" : "2024-06-04T00:55:06Z",
    "published" : "2024-06-04T00:55:06Z",
    "authors" : [
      {
        "name" : "Shijin Duan"
      },
      {
        "name" : "Chenghong Wang"
      },
      {
        "name" : "Hongwu Peng"
      },
      {
        "name" : "Yukui Luo"
      },
      {
        "name" : "Wujie Wen"
      },
      {
        "name" : "Caiwen Ding"
      },
      {
        "name" : "Xiaolin Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02599v1",
    "title" : "Privacy-Aware Randomized Quantization via Linear Programming",
    "summary" : "Differential privacy mechanisms such as the Gaussian or Laplace mechanism\nhave been widely used in data analytics for preserving individual privacy.\nHowever, they are mostly designed for continuous outputs and are unsuitable for\nscenarios where discrete values are necessary. Although various quantization\nmechanisms were proposed recently to generate discrete outputs under\ndifferential privacy, the outcomes are either biased or have an inferior\naccuracy-privacy trade-off. In this paper, we propose a family of quantization\nmechanisms that is unbiased and differentially private. It has a high degree of\nfreedom and we show that some existing mechanisms can be considered as special\ncases of ours. To find the optimal mechanism, we formulate a linear\noptimization that can be solved efficiently using linear programming tools.\nExperiments show that our proposed mechanism can attain a better\nprivacy-accuracy trade-off compared to baselines.",
    "updated" : "2024-06-01T18:40:08Z",
    "published" : "2024-06-01T18:40:08Z",
    "authors" : [
      {
        "name" : "Zhongteng Cai"
      },
      {
        "name" : "Xueru Zhang"
      },
      {
        "name" : "Mohammad Mahdi Khalili"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04129v1",
    "title" : "LenslessFace: An End-to-End Optimized Lensless System for\n  Privacy-Preserving Face Verification",
    "summary" : "Lensless cameras, innovatively replacing traditional lenses for ultra-thin,\nflat optics, encode light directly onto sensors, producing images that are not\nimmediately recognizable. This compact, lightweight, and cost-effective imaging\nsolution offers inherent privacy advantages, making it attractive for\nprivacy-sensitive applications like face verification. Typical lensless face\nverification adopts a two-stage process of reconstruction followed by\nverification, incurring privacy risks from reconstructed faces and high\ncomputational costs. This paper presents an end-to-end optimization approach\nfor privacy-preserving face verification directly on encoded lensless captures,\nensuring that the entire software pipeline remains encoded with no visible\nfaces as intermediate results. To achieve this, we propose several techniques\nto address unique challenges from the lensless setup which precludes\ntraditional face detection and alignment. Specifically, we propose a face\ncenter alignment scheme, an augmentation curriculum to build robustness against\nvariations, and a knowledge distillation method to smooth optimization and\nenhance performance. Evaluations under both simulation and real environment\ndemonstrate our method outperforms two-stage lensless verification while\nenhancing privacy and efficiency. Project website:\n\\url{lenslessface.github.io}.",
    "updated" : "2024-06-06T14:50:15Z",
    "published" : "2024-06-06T14:50:15Z",
    "authors" : [
      {
        "name" : "Xin Cai"
      },
      {
        "name" : "Hailong Zhang"
      },
      {
        "name" : "Chenchen Wang"
      },
      {
        "name" : "Wentao Liu"
      },
      {
        "name" : "Jinwei Gu"
      },
      {
        "name" : "Tianfan Xue"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03820v1",
    "title" : "A Survey on Intelligent Internet of Things: Applications, Security,\n  Privacy, and Future Directions",
    "summary" : "The rapid advances in the Internet of Things (IoT) have promoted a revolution\nin communication technology and offered various customer services. Artificial\nintelligence (AI) techniques have been exploited to facilitate IoT operations\nand maximize their potential in modern application scenarios. In particular,\nthe convergence of IoT and AI has led to a new networking paradigm called\nIntelligent IoT (IIoT), which has the potential to significantly transform\nbusinesses and industrial domains. This paper presents a comprehensive survey\nof IIoT by investigating its significant applications in mobile networks, as\nwell as its associated security and privacy issues. Specifically, we explore\nand discuss the roles of IIoT in a wide range of key application domains, from\nsmart healthcare and smart cities to smart transportation and smart industries.\nThrough such extensive discussions, we investigate important security issues in\nIIoT networks, where network attacks, confidentiality, integrity, and intrusion\nare analyzed, along with a discussion of potential countermeasures. Privacy\nissues in IIoT networks were also surveyed and discussed, including data,\nlocation, and model privacy leakage. Finally, we outline several key challenges\nand highlight potential research directions in this important area.",
    "updated" : "2024-06-06T07:55:30Z",
    "published" : "2024-06-06T07:55:30Z",
    "authors" : [
      {
        "name" : "Ons Aouedi"
      },
      {
        "name" : "Thai-Hoc Vu"
      },
      {
        "name" : "Alessio Sacco"
      },
      {
        "name" : "Dinh C. Nguyen"
      },
      {
        "name" : "Kandaraj Piamrat"
      },
      {
        "name" : "Guido Marchetto"
      },
      {
        "name" : "Quoc-Viet Pham"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.AI",
      "cs.CR",
      "cs.ET",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03802v1",
    "title" : "Continual Counting with Gradual Privacy Expiration",
    "summary" : "Differential privacy with gradual expiration models the setting where data\nitems arrive in a stream and at a given time $t$ the privacy loss guaranteed\nfor a data item seen at time $(t-d)$ is $\\epsilon g(d)$, where $g$ is a\nmonotonically non-decreasing function. We study the fundamental\n$\\textit{continual (binary) counting}$ problem where each data item consists of\na bit, and the algorithm needs to output at each time step the sum of all the\nbits streamed so far. For a stream of length $T$ and privacy $\\textit{without}$\nexpiration continual counting is possible with maximum (over all time steps)\nadditive error $O(\\log^2(T)/\\varepsilon)$ and the best known lower bound is\n$\\Omega(\\log(T)/\\varepsilon)$; closing this gap is a challenging open problem.\n  We show that the situation is very different for privacy with gradual\nexpiration by giving upper and lower bounds for a large set of expiration\nfunctions $g$. Specifically, our algorithm achieves an additive error of $\nO(\\log(T)/\\epsilon)$ for a large set of privacy expiration functions. We also\ngive a lower bound that shows that if $C$ is the additive error of any\n$\\epsilon$-DP algorithm for this problem, then the product of $C$ and the\nprivacy expiration function after $2C$ steps must be\n$\\Omega(\\log(T)/\\epsilon)$. Our algorithm matches this lower bound as its\nadditive error is $O(\\log(T)/\\epsilon)$, even when $g(2C) = O(1)$.\n  Our empirical evaluation shows that we achieve a slowly growing privacy loss\nwith significantly smaller empirical privacy loss for large values of $d$ than\na natural baseline algorithm.",
    "updated" : "2024-06-06T07:20:16Z",
    "published" : "2024-06-06T07:20:16Z",
    "authors" : [
      {
        "name" : "Joel Daniel Andersson"
      },
      {
        "name" : "Monika Henzinger"
      },
      {
        "name" : "Rasmus Pagh"
      },
      {
        "name" : "Teresa Anna Steiner"
      },
      {
        "name" : "Jalaj Upadhyay"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03785v1",
    "title" : "Count-mean Sketch as an Optimized Framework for Frequency Estimation\n  with Local Differential Privacy",
    "summary" : "This paper identifies that a group of state-of-the-art\nlocally-differentially-private (LDP) algorithms for frequency estimation are\nequivalent to the private Count-Mean Sketch (CMS) algorithm with different\nparameters. Therefore, we revisit the private CMS, correct errors in the\noriginal CMS paper regarding expectation and variance, modify the CMS\nimplementation to eliminate existing bias, and explore optimized parameters for\nCMS to achieve optimality in reducing the worst-case mean squared error (MSE),\n$l_1$ loss, and $l_2$ loss. Additionally, we prove that pairwise-independent\nhashing is sufficient for CMS, reducing its communication cost to the logarithm\nof the cardinality of all possible values (i.e., a dictionary). As a result,\nthe aforementioned optimized CMS is proven theoretically and empirically to be\nthe only algorithm optimized for reducing the worst-case MSE, $l_1$ loss, and\n$l_2$ loss when dealing with a very large dictionary. Furthermore, we\ndemonstrate that randomness is necessary to ensure the correctness of CMS, and\nthe communication cost of CMS, though low, is unavoidable despite the\nrandomness being public or private.",
    "updated" : "2024-06-06T06:55:08Z",
    "published" : "2024-06-06T06:55:08Z",
    "authors" : [
      {
        "name" : "Mingen Pan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03766v1",
    "title" : "Privacy Preserving Semi-Decentralized Mean Estimation over\n  Intermittently-Connected Networks",
    "summary" : "We consider the problem of privately estimating the mean of vectors\ndistributed across different nodes of an unreliable wireless network, where\ncommunications between nodes can fail intermittently. We adopt a\nsemi-decentralized setup, wherein to mitigate the impact of intermittently\nconnected links, nodes can collaborate with their neighbors to compute a local\nconsensus, which they relay to a central server. In such a setting, the\ncommunications between any pair of nodes must ensure that the privacy of the\nnodes is rigorously maintained to prevent unauthorized information leakage. We\nstudy the tradeoff between collaborative relaying and privacy leakage due to\nthe data sharing among nodes and, subsequently, propose PriCER: Private\nCollaborative Estimation via Relaying -- a differentially private collaborative\nalgorithm for mean estimation to optimize this tradeoff. The privacy guarantees\nof PriCER arise (i) implicitly, by exploiting the inherent stochasticity of the\nflaky network connections, and (ii) explicitly, by adding Gaussian\nperturbations to the estimates exchanged by the nodes. Local and central\nprivacy guarantees are provided against eavesdroppers who can observe different\nsignals, such as the communications amongst nodes during local consensus and\n(possibly multiple) transmissions from the relays to the central server. We\nsubstantiate our theoretical findings with numerical simulations. Our\nimplementation is available at\nhttps://github.com/rajarshisaha95/private-collaborative-relaying.",
    "updated" : "2024-06-06T06:12:15Z",
    "published" : "2024-06-06T06:12:15Z",
    "authors" : [
      {
        "name" : "Rajarshi Saha"
      },
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Michal Yemini"
      },
      {
        "name" : "Andrea J. Goldsmith"
      },
      {
        "name" : "H. Vincent Poor"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.DC",
      "cs.IT",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03749v1",
    "title" : "NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting\n  by Learning from Human",
    "summary" : "Increasing concerns about privacy leakage issues in academia and industry\narise when employing NLP models from third-party providers to process sensitive\ntexts. To protect privacy before sending sensitive data to those models, we\nsuggest sanitizing sensitive text using two common strategies used by humans:\ni) deleting sensitive expressions, and ii) obscuring sensitive details by\nabstracting them. To explore the issues and develop a tool for text rewriting,\nwe curate the first corpus, coined NAP^2, through both crowdsourcing and the\nuse of large language models (LLMs). Compared to the prior works based on\ndifferential privacy, which lead to a sharp drop in information utility and\nunnatural texts, the human-inspired approaches result in more natural rewrites\nand offer an improved balance between privacy protection and data utility, as\ndemonstrated by our extensive experiments.",
    "updated" : "2024-06-06T05:07:44Z",
    "published" : "2024-06-06T05:07:44Z",
    "authors" : [
      {
        "name" : "Shuo Huang"
      },
      {
        "name" : "William MacLean"
      },
      {
        "name" : "Xiaoxi Kang"
      },
      {
        "name" : "Anqi Wu"
      },
      {
        "name" : "Lizhen Qu"
      },
      {
        "name" : "Qiongkai Xu"
      },
      {
        "name" : "Zhuang Li"
      },
      {
        "name" : "Xingliang Yuan"
      },
      {
        "name" : "Gholamreza Haffari"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03695v1",
    "title" : "FACOS: Enabling Privacy Protection Through Fine-Grained Access Control\n  with On-chain and Off-chain System",
    "summary" : "Data-driven landscape across finance, government, and healthcare, the\ncontinuous generation of information demands robust solutions for secure\nstorage, efficient dissemination, and fine-grained access control. Blockchain\ntechnology emerges as a significant tool, offering decentralized storage while\nupholding the tenets of data security and accessibility. However, on-chain and\noff-chain strategies are still confronted with issues such as untrusted\noff-chain data storage, absence of data ownership, limited access control\npolicy for clients, and a deficiency in data privacy and auditability. To solve\nthese challenges, we propose a permissioned blockchain-based privacy-preserving\nfine-grained access control on-chain and off-chain system, namely FACOS. We\napplied three fine-grained access control solutions and comprehensively\nanalyzed them in different aspects, which provides an intuitive perspective for\nsystem designers and clients to choose the appropriate access control method\nfor their systems. Compared to similar work that only stores encrypted data in\ncentralized or non-fault-tolerant IPFS systems, we enhanced off-chain data\nstorage security and robustness by utilizing a highly efficient and secure\nasynchronous Byzantine fault tolerance (BFT) protocol in the off-chain\nenvironment. As each of the clients needs to be verified and authorized before\naccessing the data, we involved the Trusted Execution Environment (TEE)-based\nsolution to verify the credentials of clients. Additionally, our evaluation\nresults demonstrated that our system offers better scalability and practicality\nthan other state-of-the-art designs.",
    "updated" : "2024-06-06T02:23:12Z",
    "published" : "2024-06-06T02:23:12Z",
    "authors" : [
      {
        "name" : "Chao Liu"
      },
      {
        "name" : "Cankun Hou"
      },
      {
        "name" : "Tianyu Jiang"
      },
      {
        "name" : "Jianting Ning"
      },
      {
        "name" : "Hui Qiao"
      },
      {
        "name" : "Yusen Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04827v1",
    "title" : "Black Box Differential Privacy Auditing Using Total Variation Distance",
    "summary" : "We present a practical method to audit the differential privacy (DP)\nguarantees of a machine learning model using a small hold-out dataset that is\nnot exposed to the model during the training. Having a score function such as\nthe loss function employed during the training, our method estimates the total\nvariation (TV) distance between scores obtained with a subset of the training\ndata and the hold-out dataset. With some meta information about the underlying\nDP training algorithm, these TV distance values can be converted to\n$(\\varepsilon,\\delta)$-guarantees for any $\\delta$. We show that these score\ndistributions asymptotically give lower bounds for the DP guarantees of the\nunderlying training algorithm, however, we perform a one-shot estimation for\npracticality reasons. We specify conditions that lead to lower bounds for the\nDP guarantees with high probability. To estimate the TV distance between the\nscore distributions, we use a simple density estimation method based on\nhistograms. We show that the TV distance gives a very close to optimally robust\nestimator and has an error rate $\\mathcal{O}(k^{-1/3})$, where $k$ is the total\nnumber of samples. Numerical experiments on benchmark datasets illustrate the\neffectiveness of our approach and show improvements over baseline methods for\nblack-box auditing.",
    "updated" : "2024-06-07T10:52:15Z",
    "published" : "2024-06-07T10:52:15Z",
    "authors" : [
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "Jafar Mohammadi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04702v1",
    "title" : "Marking the Pace: A Blockchain-Enhanced Privacy-Traceable Strategy for\n  Federated Recommender Systems",
    "summary" : "Federated recommender systems have been crucially enhanced through data\nsharing and continuous model updates, attributed to the pervasive connectivity\nand distributed computing capabilities of Internet of Things (IoT) devices.\nGiven the sensitivity of IoT data, transparent data processing in data sharing\nand model updates is paramount. However, existing methods fall short in tracing\nthe flow of shared data and the evolution of model updates. Consequently, data\nsharing is vulnerable to exploitation by malicious entities, raising\nsignificant data privacy concerns, while excluding data sharing will result in\nsub-optimal recommendations. To mitigate these concerns, we present LIBERATE, a\nprivacy-traceable federated recommender system. We design a blockchain-based\ntraceability mechanism, ensuring data privacy during data sharing and model\nupdates. We further enhance privacy protection by incorporating local\ndifferential privacy in user-server communication. Extensive evaluations with\nthe real-world dataset corroborate LIBERATE's capabilities in ensuring data\nprivacy during data sharing and model update while maintaining efficiency and\nperformance. Results underscore blockchain-based traceability mechanism as a\npromising solution for privacy-preserving in federated recommender systems.",
    "updated" : "2024-06-07T07:21:21Z",
    "published" : "2024-06-07T07:21:21Z",
    "authors" : [
      {
        "name" : "Zhen Cai"
      },
      {
        "name" : "Tao Tang"
      },
      {
        "name" : "Shuo Yu"
      },
      {
        "name" : "Yunpeng Xiao"
      },
      {
        "name" : "Feng Xia"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04610v1",
    "title" : "Contrastive explainable clustering with differential privacy",
    "summary" : "This paper presents a novel approach in Explainable AI (XAI), integrating\ncontrastive explanations with differential privacy in clustering methods. For\nseveral basic clustering problems, including $k$-median and $k$-means, we give\nefficient differential private contrastive explanations that achieve\nessentially the same explanations as those that non-private clustering\nexplanations can obtain. We define contrastive explanations as the utility\ndifference between the original clustering utility and utility from clustering\nwith a specifically fixed centroid. In each contrastive scenario, we designate\na specific data point as the fixed centroid position, enabling us to measure\nthe impact of this constraint on clustering utility under differential privacy.\nExtensive experiments across various datasets show our method's effectiveness\nin providing meaningful explanations without significantly compromising data\nprivacy or clustering utility. This underscores our contribution to\nprivacy-aware machine learning, demonstrating the feasibility of achieving a\nbalance between privacy and utility in the explanation of clustering tasks.",
    "updated" : "2024-06-07T03:37:36Z",
    "published" : "2024-06-07T03:37:36Z",
    "authors" : [
      {
        "name" : "Dung Nguyen"
      },
      {
        "name" : "Ariel Vetzler"
      },
      {
        "name" : "Sarit Kraus"
      },
      {
        "name" : "Anil Vullikanti"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04535v1",
    "title" : "Tangent differential privacy",
    "summary" : "Differential privacy is a framework for protecting the identity of individual\ndata points in the decision-making process. In this note, we propose a new form\nof differential privacy called tangent differential privacy. Compared with the\nusual differential privacy that is defined uniformly across data distributions,\ntangent differential privacy is tailored towards a specific data distribution\nof interest. It also allows for general distribution distances such as total\nvariation distance and Wasserstein distance. In the case of risk minimization,\nwe show that entropic regularization guarantees tangent differential privacy\nunder rather general conditions on the risk function.",
    "updated" : "2024-06-06T22:11:31Z",
    "published" : "2024-06-06T22:11:31Z",
    "authors" : [
      {
        "name" : "Lexing Ying"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.06473v1",
    "title" : "DiffAudit: Auditing Privacy Practices of Online Services for Children\n  and Adolescents",
    "summary" : "Children's and adolescents' online data privacy are regulated by laws such as\nthe Children's Online Privacy Protection Act (COPPA) and the California\nConsumer Privacy Act (CCPA). Online services that are directed towards general\naudiences (i.e., including children, adolescents, and adults) must comply with\nthese laws. In this paper, first, we present DiffAudit, a platform-agnostic\nprivacy auditing methodology for general audience services. DiffAudit performs\ndifferential analysis of network traffic data flows to compare data processing\npractices (i) between child, adolescent, and adult users and (ii) before and\nafter consent is given and user age is disclosed. We also present a data type\nclassification method that utilizes GPT-4 and our data type ontology based on\nCOPPA and CCPA, allowing us to identify considerably more data types than prior\nwork. Second, we apply DiffAudit to a set of popular general audience mobile\nand web services and observe a rich set of behaviors extracted from over 440K\noutgoing requests, containing 3,968 unique data types we extracted and\nclassified. We reveal problematic data processing practices prior to consent\nand age disclosure, lack of differentiation between age-specific data flows,\ninconsistent privacy policy disclosures, and sharing of linkable data with\nthird parties, including advertising and tracking services.",
    "updated" : "2024-06-10T17:14:53Z",
    "published" : "2024-06-10T17:14:53Z",
    "authors" : [
      {
        "name" : "Olivia Figueira"
      },
      {
        "name" : "Rahmadi Trimananda"
      },
      {
        "name" : "Athina Markopoulou"
      },
      {
        "name" : "Scott Jordan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.06186v1",
    "title" : "A Survey on Machine Unlearning: Techniques and New Emerged Privacy Risks",
    "summary" : "The explosive growth of machine learning has made it a critical\ninfrastructure in the era of artificial intelligence. The extensive use of data\nposes a significant threat to individual privacy. Various countries have\nimplemented corresponding laws, such as GDPR, to protect individuals' data\nprivacy and the right to be forgotten. This has made machine unlearning a\nresearch hotspot in the field of privacy protection in recent years, with the\naim of efficiently removing the contribution and impact of individual data from\ntrained models. The research in academia on machine unlearning has continuously\nenriched its theoretical foundation, and many methods have been proposed,\ntargeting different data removal requests in various application scenarios.\nHowever, recently researchers have found potential privacy leakages of various\nof machine unlearning approaches, making the privacy preservation on machine\nunlearning area a critical topic. This paper provides an overview and analysis\nof the existing research on machine unlearning, aiming to present the current\nvulnerabilities of machine unlearning approaches. We analyze privacy risks in\nvarious aspects, including definitions, implementation methods, and real-world\napplications. Compared to existing reviews, we analyze the new challenges posed\nby the latest malicious attack techniques on machine unlearning from the\nperspective of privacy threats. We hope that this survey can provide an initial\nbut comprehensive discussion on this new emerging area.",
    "updated" : "2024-06-10T11:31:04Z",
    "published" : "2024-06-10T11:31:04Z",
    "authors" : [
      {
        "name" : "Hengzhu Liu"
      },
      {
        "name" : "Ping Xiong"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Philip S. Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.05858v1",
    "title" : "Comments on \"Federated Learning with Differential Privacy: Algorithms\n  and Performance Analysis\"",
    "summary" : "In the paper by Wei et al. (\"Federated Learning with Differential Privacy:\nAlgorithms and Performance Analysis\"), the convergence performance of the\nproposed differential privacy algorithm in federated learning (FL), known as\nNoising before Model Aggregation FL (NbAFL), was studied. However, the\npresented convergence upper bound of NbAFL (Theorem 2) is incorrect. This\ncomment aims to present the correct form of the convergence upper bound for\nNbAFL.",
    "updated" : "2024-06-09T17:03:56Z",
    "published" : "2024-06-09T17:03:56Z",
    "authors" : [
      {
        "name" : "Mahtab Talaei"
      },
      {
        "name" : "Iman Izadi"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR",
      "cs.PF"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.05545v1",
    "title" : "Privacy-Preserving Optimal Parameter Selection for Collaborative\n  Clustering",
    "summary" : "This study investigates the optimal selection of parameters for collaborative\nclustering while ensuring data privacy. We focus on key clustering algorithms\nwithin a collaborative framework, where multiple data owners combine their\ndata. A semi-trusted server assists in recommending the most suitable\nclustering algorithm and its parameters. Our findings indicate that the privacy\nparameter ($\\epsilon$) minimally impacts the server's recommendations, but an\nincrease in $\\epsilon$ raises the risk of membership inference attacks, where\nsensitive information might be inferred. To mitigate these risks, we implement\ndifferential privacy techniques, particularly the Randomized Response\nmechanism, to add noise and protect data privacy. Our approach demonstrates\nthat high-quality clustering can be achieved while maintaining data\nconfidentiality, as evidenced by metrics such as the Adjusted Rand Index and\nSilhouette Score. This study contributes to privacy-aware data sharing, optimal\nalgorithm and parameter selection, and effective communication between data\nowners and the server.",
    "updated" : "2024-06-08T18:21:12Z",
    "published" : "2024-06-08T18:21:12Z",
    "authors" : [
      {
        "name" : "Maryam Ghasemian"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.05459v1",
    "title" : "PriviFy: Designing Tangible Interfaces for Configuring IoT Privacy\n  Preferences",
    "summary" : "The Internet of Things (IoT) devices, such as smart speakers can collect\nsensitive user data, necessitating the need for users to manage their privacy\npreferences. However, configuring these preferences presents users with\nmultiple challenges. Existing privacy controls often lack transparency, are\nhard to understand, and do not provide meaningful choices. On top of that,\nusers struggle to locate privacy settings due to multiple menus or confusing\nlabeling, which discourages them from using these controls. We introduce\nPriviFy (Privacy Simplify-er), a novel and user-friendly tangible interface\nthat can simplify the configuration of smart devices privacy settings. PriviFy\nis designed to propose an enhancement to existing hardware by integrating\nadditional features that improve privacy management. We envision that positive\nfeedback and user experiences from our study will inspire consumer product\ndevelopers and smart device manufacturers to incorporate the useful design\nelements we have identified. Using fidelity prototyping, we iteratively\ndesigned PriviFy prototype with 20 participants to include interactive features\nsuch as knobs, buttons, lights, and notifications that allow users to configure\ntheir data privacy preferences and receive confirmation of their choices. We\nfurther evaluated PriviFy high-fidelity prototype with 20 more participants.\nOur results show that PriviFy helps simplify the complexity of privacy\npreferences configuration with a significant usability score at p < .05 (P =\n0.000000017, t = -8.8639). PriviFy successfully met users privacy needs and\nenabled them to regain control over their data. We conclude by recommending the\nimportance of designing specific privacy configuration options.",
    "updated" : "2024-06-08T12:35:46Z",
    "published" : "2024-06-08T12:35:46Z",
    "authors" : [
      {
        "name" : "Bayan Al Muhander"
      },
      {
        "name" : "Omer Rana"
      },
      {
        "name" : "Charith Perera"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.05451v1",
    "title" : "PrivacyCube: Data Physicalization for Enhancing Privacy Awareness in IoT",
    "summary" : "People are increasingly bringing Internet of Things (IoT) devices into their\nhomes without understanding how their data is gathered, processed, and used. We\ndescribe PrivacyCube, a novel data physicalization designed to increase privacy\nawareness within smart home environments. PrivacyCube visualizes IoT data\nconsumption by displaying privacy-related notices. PrivacyCube aims to assist\nsmart home occupants to (i) understand their data privacy better and (ii) have\nconversations around data management practices of IoT devices used within their\nhomes. Using PrivacyCube, households can learn and make informed privacy\ndecisions collectively. To evaluate PrivacyCube, we used multiple research\nmethods throughout the different stages of design. We first conducted a focus\ngroup study in two stages with six participants to compare PrivacyCube to text\nand state-of-the-art privacy policies. We then deployed PrivacyCube in a\n14-day-long field study with eight households. Our results show that\nPrivacyCube helps home occupants comprehend IoT privacy better with\nsignificantly increased privacy awareness at p < .05 (p=0.00041, t= -5.57).\nParticipants preferred PrivacyCube over text privacy policies because it was\ncomprehensive and easier to use. PrivacyCube and Privacy Label, a\nstate-of-the-art approach, both received positive reviews from participants,\nwith PrivacyCube being preferred for its interactivity and ability to encourage\nconversations. PrivacyCube was also considered by home occupants as a piece of\nhome furniture, encouraging them to socialize and discuss IoT privacy\nimplications using this device.",
    "updated" : "2024-06-08T12:20:42Z",
    "published" : "2024-06-08T12:20:42Z",
    "authors" : [
      {
        "name" : "Bayan Al Muhander"
      },
      {
        "name" : "Nalin Arachchilage"
      },
      {
        "name" : "Yasar Majib"
      },
      {
        "name" : "Mohammed Alosaimi"
      },
      {
        "name" : "Omer Rana"
      },
      {
        "name" : "Charith Perera"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.07314v1",
    "title" : "Rethinking the impact of noisy labels in graph classification: A utility\n  and privacy perspective",
    "summary" : "Graph neural networks based on message-passing mechanisms have achieved\nadvanced results in graph classification tasks. However, their generalization\nperformance degrades when noisy labels are present in the training data. Most\nexisting noisy labeling approaches focus on the visual domain or graph node\nclassification tasks and analyze the impact of noisy labels only from a utility\nperspective. Unlike existing work, in this paper, we measure the effects of\nnoise labels on graph classification from data privacy and model utility\nperspectives. We find that noise labels degrade the model's generalization\nperformance and enhance the ability of membership inference attacks on graph\ndata privacy. To this end, we propose the robust graph neural network approach\nwith noisy labeled graph classification. Specifically, we first accurately\nfilter the noisy samples by high-confidence samples and the first feature\nprincipal component vector of each class. Then, the robust principal component\nvectors and the model output under data augmentation are utilized to achieve\nnoise label correction guided by dual spatial information. Finally, supervised\ngraph contrastive learning is introduced to enhance the embedding quality of\nthe model and protect the privacy of the training graph data. The utility and\nprivacy of the proposed method are validated by comparing twelve different\nmethods on eight real graph classification datasets. Compared with the\nstate-of-the-art methods, the RGLC method achieves at most and at least 7.8%\nand 0.8% performance gain at 30% noisy labeling rate, respectively, and reduces\nthe accuracy of privacy attacks to below 60%.",
    "updated" : "2024-06-11T14:44:37Z",
    "published" : "2024-06-11T14:44:37Z",
    "authors" : [
      {
        "name" : "De Li"
      },
      {
        "name" : "Xianxian Li"
      },
      {
        "name" : "Zeming Gan"
      },
      {
        "name" : "Qiyu Li"
      },
      {
        "name" : "Bin Qu"
      },
      {
        "name" : "Jinyan Wang"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.06990v1",
    "title" : "Privacy-Utility Tradeoff Based on $α$-lift",
    "summary" : "Information density and its exponential form, known as lift, play a central\nrole in information privacy leakage measures. $\\alpha$-lift is the power-mean\nof lift, which is tunable between the worst-case measure max-lift\n($\\alpha=\\infty$) and more relaxed versions ($\\alpha<\\infty$). This paper\ninvestigates the optimization problem of the privacy-utility tradeoff where\n$\\alpha$-lift and mutual information are privacy and utility measures,\nrespectively. Due to the nonlinear nature of $\\alpha$-lift for $\\alpha<\\infty$,\nfinding the optimal solution is challenging. Therefore, we propose a heuristic\nalgorithm to estimate the optimal utility for each value of $\\alpha$, inspired\nby the optimal solution for $\\alpha=\\infty$. In proposing the algorithm, we\nprove and use the convexity of $\\alpha$-lift with respect to the lift.",
    "updated" : "2024-06-11T06:39:57Z",
    "published" : "2024-06-11T06:39:57Z",
    "authors" : [
      {
        "name" : "Mohammad Amin Zarrabian"
      },
      {
        "name" : "Parastoo Sadeghi"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.06755v1",
    "title" : "Optimal Federated Learning for Nonparametric Regression with\n  Heterogeneous Distributed Differential Privacy Constraints",
    "summary" : "This paper studies federated learning for nonparametric regression in the\ncontext of distributed samples across different servers, each adhering to\ndistinct differential privacy constraints. The setting we consider is\nheterogeneous, encompassing both varying sample sizes and differential privacy\nconstraints across servers. Within this framework, both global and pointwise\nestimation are considered, and optimal rates of convergence over the Besov\nspaces are established.\n  Distributed privacy-preserving estimators are proposed and their risk\nproperties are investigated. Matching minimax lower bounds, up to a logarithmic\nfactor, are established for both global and pointwise estimation. Together,\nthese findings shed light on the tradeoff between statistical accuracy and\nprivacy preservation. In particular, we characterize the compromise not only in\nterms of the privacy budget but also concerning the loss incurred by\ndistributing data within the privacy framework as a whole. This insight\ncaptures the folklore wisdom that it is easier to retain privacy in larger\nsamples, and explores the differences between pointwise and global estimation\nunder distributed privacy constraints.",
    "updated" : "2024-06-10T19:34:07Z",
    "published" : "2024-06-10T19:34:07Z",
    "authors" : [
      {
        "name" : "T. Tony Cai"
      },
      {
        "name" : "Abhinav Chakraborty"
      },
      {
        "name" : "Lasse Vuursteen"
      }
    ],
    "categories" : [
      "math.ST",
      "cs.LG",
      "stat.ML",
      "stat.TH",
      "62G08, 62C20, 68P27, 62F30,"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.06749v1",
    "title" : "Federated Nonparametric Hypothesis Testing with Differential Privacy\n  Constraints: Optimal Rates and Adaptive Tests",
    "summary" : "Federated learning has attracted significant recent attention due to its\napplicability across a wide range of settings where data is collected and\nanalyzed across disparate locations. In this paper, we study federated\nnonparametric goodness-of-fit testing in the white-noise-with-drift model under\ndistributed differential privacy (DP) constraints.\n  We first establish matching lower and upper bounds, up to a logarithmic\nfactor, on the minimax separation rate. This optimal rate serves as a benchmark\nfor the difficulty of the testing problem, factoring in model characteristics\nsuch as the number of observations, noise level, and regularity of the signal\nclass, along with the strictness of the $(\\epsilon,\\delta)$-DP requirement. The\nresults demonstrate interesting and novel phase transition phenomena.\nFurthermore, the results reveal an interesting phenomenon that distributed\none-shot protocols with access to shared randomness outperform those without\naccess to shared randomness. We also construct a data-driven testing procedure\nthat possesses the ability to adapt to an unknown regularity parameter over a\nlarge collection of function classes with minimal additional cost, all while\nmaintaining adherence to the same set of DP constraints.",
    "updated" : "2024-06-10T19:25:19Z",
    "published" : "2024-06-10T19:25:19Z",
    "authors" : [
      {
        "name" : "T. Tony Cai"
      },
      {
        "name" : "Abhinav Chakraborty"
      },
      {
        "name" : "Lasse Vuursteen"
      }
    ],
    "categories" : [
      "math.ST",
      "cs.LG",
      "stat.ML",
      "stat.TH",
      "62G10, 62C20, 68P27, 62F30"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.07973v1",
    "title" : "Unique Security and Privacy Threats of Large Language Model: A\n  Comprehensive Survey",
    "summary" : "With the rapid development of artificial intelligence, large language models\n(LLMs) have made remarkable progress in natural language processing. These\nmodels are trained on large amounts of data to demonstrate powerful language\nunderstanding and generation capabilities for various applications, from\nmachine translation and chatbots to agents. However, LLMs have exposed a\nvariety of privacy and security issues during their life cycle, which have\nbecome the focus of academic and industrial attention. Moreover, these risks\nLLMs face are pretty different from previous traditional language models. Since\ncurrent surveys lack a clear taxonomy of unique threat models based on diverse\nscenarios, we highlight unique privacy and security issues based on five\nscenarios: pre-training, fine-tuning, RAG system, deploying, and LLM-based\nagent. Concerning the characteristics of each risk, this survey provides\npotential threats and countermeasures. The research on attack and defense\nsituations LLMs face can provide feasible research directions, making more\nareas reap LLMs' benefits.",
    "updated" : "2024-06-12T07:55:32Z",
    "published" : "2024-06-12T07:55:32Z",
    "authors" : [
      {
        "name" : "Shang Wang"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Bo Liu"
      },
      {
        "name" : "Ding Ming"
      },
      {
        "name" : "Xu Guo"
      },
      {
        "name" : "Dayong Ye"
      },
      {
        "name" : "Wanlei Zhou"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.09214v1",
    "title" : "Applying Multi-Agent Negotiation to Solve the Production Routing Problem\n  With Privacy Preserving",
    "summary" : "This paper presents a novel approach to address the Production Routing\nProblem with Privacy Preserving (PRPPP) in supply chain optimization. The\nintegrated optimization of production, inventory, distribution, and routing\ndecisions in real-world industry applications poses several challenges,\nincluding increased complexity, discrepancies between planning and execution,\nand constraints on information sharing. To mitigate these challenges, this\npaper proposes the use of intelligent agent negotiation within a hybrid\nMulti-Agent System (MAS) integrated with optimization algorithms. The MAS\nfacilitates communication and coordination among entities, encapsulates private\ninformation, and enables negotiation. This, along with optimization algorithms,\nmakes it a compelling framework for establishing optimal solutions. The\napproach is supported by real-world applications and synergies between MAS and\noptimization methods, demonstrating its effectiveness in addressing complex\nsupply chain optimization problems.",
    "updated" : "2024-06-13T15:15:34Z",
    "published" : "2024-06-13T15:15:34Z",
    "authors" : [
      {
        "name" : "Luiza Pellin Biasoto"
      },
      {
        "name" : "Vinicius Renan de Carvalho"
      },
      {
        "name" : "Jaime Simão Sichman"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.09037v1",
    "title" : "Evaluating Privacy, Security, and Trust Perceptions in Conversational\n  AI: A Systematic Review",
    "summary" : "Conversational AI (CAI) systems which encompass voice- and text-based\nassistants are on the rise and have been largely integrated into people's\neveryday lives. Despite their widespread adoption, users voice concerns\nregarding privacy, security and trust in these systems. However, the\ncomposition of these perceptions, their impact on technology adoption and usage\nand the relationship between privacy, security and trust perceptions in the CAI\ncontext remain open research challenges. This study contributes to the field by\nconducting a Systematic Literature Review and offers insights into the current\nstate of research on privacy, security and trust perceptions in the context of\nCAI systems. The review covers application fields and user groups and sheds\nlight on empirical methods and tools used for assessment. Moreover, it provides\ninsights into the reliability and validity of privacy, security and trust\nscales, as well as extensively investigating the subconstructs of each item as\nwell as additional concepts which are concurrently collected. We point out that\nthe perceptions of trust, privacy and security overlap based on the\nsubconstructs we identified. While the majority of studies investigate one of\nthese concepts, only a few studies were found exploring privacy, security and\ntrust perceptions jointly. Our research aims to inform on directions to develop\nand use reliable scales for users' privacy, security and trust perceptions and\ncontribute to the development of trustworthy CAI systems.",
    "updated" : "2024-06-13T12:20:26Z",
    "published" : "2024-06-13T12:20:26Z",
    "authors" : [
      {
        "name" : "Anna Leschanowsky"
      },
      {
        "name" : "Silas Rech"
      },
      {
        "name" : "Birgit Popp"
      },
      {
        "name" : "Tom Bäckström"
      }
    ],
    "categories" : [
      "cs.HC",
      "68-06",
      "J.0"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.09005v1",
    "title" : "Privacy Aware Memory Forensics",
    "summary" : "In recent years, insider threats and attacks have been increasing in terms of\nfrequency and cost to the corporate business. The utilization of end-to-end\nencrypted instant messaging applications (WhatsApp, Telegram, VPN) by malicious\ninsiders raised data breach incidents exponentially. The Securities and\nExchange Board of India (SEBI) investigated reports on such data leak incidents\nand reported about twelve companies where earnings data and financial\ninformation were leaked using WhatsApp messages. Recent surveys indicate that\n60% of data breaches are primarily caused by malicious insider threats.\nEspecially, in the case of the defense environment, information leaks by\ninsiders will jeopardize the countrys national security. Sniffing of network\nand host-based activities will not work in an insider threat detection\nenvironment due to end-to-end encryption. Memory forensics allows access to the\nmessages sent or received over an end-to-end encrypted environment but with a\ntotal compromise of the users privacy. In this research, we present a novel\nsolution to detect data leakages by insiders in an organization. Our approach\ncaptures the RAM of the insiders device and analyses it for sensitive\ninformation leaks from a host system while maintaining the users privacy.\nSensitive data leaks are identified with context using a deep learning model.\nThe feasibility and effectiveness of the proposed idea have been demonstrated\nwith the help of a military use case. The proposed architecture can however be\nused across various use cases with minor modifications.",
    "updated" : "2024-06-13T11:18:49Z",
    "published" : "2024-06-13T11:18:49Z",
    "authors" : [
      {
        "name" : "Janardhan Kalikiri"
      },
      {
        "name" : "Gaurav Varshney"
      },
      {
        "name" : "Jaswinder Kour"
      },
      {
        "name" : "Tarandeep Singh"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.08918v1",
    "title" : "Beyond the Calibration Point: Mechanism Comparison in Differential\n  Privacy",
    "summary" : "In differentially private (DP) machine learning, the privacy guarantees of DP\nmechanisms are often reported and compared on the basis of a single\n$(\\varepsilon, \\delta)$-pair. This practice overlooks that DP guarantees can\nvary substantially \\emph{even between mechanisms sharing a given $(\\varepsilon,\n\\delta)$}, and potentially introduces privacy vulnerabilities which can remain\nundetected. This motivates the need for robust, rigorous methods for comparing\nDP guarantees in such cases. Here, we introduce the $\\Delta$-divergence between\nmechanisms which quantifies the worst-case excess privacy vulnerability of\nchoosing one mechanism over another in terms of $(\\varepsilon, \\delta)$, $f$-DP\nand in terms of a newly presented Bayesian interpretation. Moreover, as a\ngeneralisation of the Blackwell theorem, it is endowed with strong\ndecision-theoretic foundations. Through application examples, we show that our\ntechniques can facilitate informed decision-making and reveal gaps in the\ncurrent understanding of privacy risks, as current practices in DP-SGD often\nresult in choosing mechanisms with high excess privacy vulnerabilities.",
    "updated" : "2024-06-13T08:30:29Z",
    "published" : "2024-06-13T08:30:29Z",
    "authors" : [
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Stefan Kolek"
      },
      {
        "name" : "Borja Balle"
      },
      {
        "name" : "Jamie Hayes"
      },
      {
        "name" : "Daniel Rueckert"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04535v1",
    "title" : "Tangent differential privacy",
    "summary" : "Differential privacy is a framework for protecting the identity of individual\ndata points in the decision-making process. In this note, we propose a new form\nof differential privacy called tangent differential privacy. Compared with the\nusual differential privacy that is defined uniformly across data distributions,\ntangent differential privacy is tailored towards a specific data distribution\nof interest. It also allows for general distribution distances such as total\nvariation distance and Wasserstein distance. In the case of risk minimization,\nwe show that entropic regularization guarantees tangent differential privacy\nunder rather general conditions on the risk function.",
    "updated" : "2024-06-06T22:11:31Z",
    "published" : "2024-06-06T22:11:31Z",
    "authors" : [
      {
        "name" : "Lexing Ying"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.09682v1",
    "title" : "Privacy-preserving Quantification of Non-IID Degree in Federated\n  Learning",
    "summary" : "Federated learning (FL) offers a privacy-preserving approach to machine\nlearning for multiple collaborators without sharing raw data. However, the\nexistence of non-independent and non-identically distributed (non-IID) datasets\nacross different clients presents a significant challenge to FL, leading to a\nsharp drop in accuracy, reduced efficiency, and hindered implementation. To\naddress the non-IID problem, various methods have been proposed, including\nclustering and personalized FL frameworks. Nevertheless, to date, a formal\nquantitative definition of the non-IID degree between different clients'\ndatasets is still missing, hindering the clients from comparing and obtaining\nan overview of their data distributions with other clients. For the first time,\nthis paper proposes a quantitative definition of the non-IID degree in the\nfederated environment by employing the cumulative distribution function (CDF),\ncalled Fully Homomorphic Encryption-based Federated Cumulative Distribution\nFunction (FHE-FCDF). This method utilizes cryptographic primitive fully\nhomomorphic encryption to enable clients to estimate the non-IID degree while\nensuring privacy preservation. The experiments conducted on the CIFAR-100\nnon-IID dataset validate the effectiveness of our proposed method.",
    "updated" : "2024-06-14T03:08:53Z",
    "published" : "2024-06-14T03:08:53Z",
    "authors" : [
      {
        "name" : "Yuping Yan"
      },
      {
        "name" : "Yizhi Wang"
      },
      {
        "name" : "Yingchao Yu"
      },
      {
        "name" : "Yaochu Jin"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.09547v1",
    "title" : "FLea: Addressing Data Scarcity and Label Skew in Federated Learning via\n  Privacy-preserving Feature Augmentation",
    "summary" : "Federated Learning (FL) enables model development by leveraging data\ndistributed across numerous edge devices without transferring local data to a\ncentral server. However, existing FL methods still face challenges when dealing\nwith scarce and label-skewed data across devices, resulting in local model\noverfitting and drift, consequently hindering the performance of the global\nmodel. In response to these challenges, we propose a pioneering framework\ncalled FLea, incorporating the following key components: i) A global feature\nbuffer that stores activation-target pairs shared from multiple clients to\nsupport local training. This design mitigates local model drift caused by the\nabsence of certain classes; ii) A feature augmentation approach based on local\nand global activation mix-ups for local training. This strategy enlarges the\ntraining samples, thereby reducing the risk of local overfitting; iii) An\nobfuscation method to minimize the correlation between intermediate activations\nand the source data, enhancing the privacy of shared features. To verify the\nsuperiority of FLea, we conduct extensive experiments using a wide range of\ndata modalities, simulating different levels of local data scarcity and label\nskew. The results demonstrate that FLea consistently outperforms\nstate-of-the-art FL counterparts (among 13 of the experimented 18 settings, the\nimprovement is over 5% while concurrently mitigating the privacy\nvulnerabilities associated with shared features. Code is available at\nhttps://github.com/XTxiatong/FLea.git.",
    "updated" : "2024-06-13T19:28:08Z",
    "published" : "2024-06-13T19:28:08Z",
    "authors" : [
      {
        "name" : "Tong Xia"
      },
      {
        "name" : "Abhirup Ghosh"
      },
      {
        "name" : "Xinchi Qiu"
      },
      {
        "name" : "Cecilia Mascolo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.11323v1",
    "title" : "Transparency, Privacy, and Fairness in Recommender Systems",
    "summary" : "Recommender systems have become a pervasive part of our daily online\nexperience, and are one of the most widely used applications of artificial\nintelligence and machine learning. Therefore, regulations and requirements for\ntrustworthy artificial intelligence, for example, the European AI Act, which\nincludes notions such as transparency, privacy, and fairness are also highly\nrelevant for the design of recommender systems in practice. This habilitation\nelaborates on aspects related to these three notions in the light of\nrecommender systems, namely: (i) transparency and cognitive models, (ii)\nprivacy and limited preference information, and (iii) fairness and popularity\nbias in recommender systems. Specifically, with respect to aspect (i), we\nhighlight the usefulness of incorporating psychological theories for a\ntransparent design process of recommender systems. We term this type of systems\npsychology-informed recommender systems. In aspect (ii), we study and address\nthe trade-off between accuracy and privacy in differentially-private\nrecommendations. We design a novel recommendation approach for collaborative\nfiltering based on an efficient neighborhood reuse concept, which reduces the\nnumber of users that need to be protected with differential privacy.\nFurthermore, we address the related issue of limited availability of user\npreference information, e.g., click data, in the settings of session-based and\ncold-start recommendations. With respect to aspect (iii), we analyze popularity\nbias in recommender systems. We find that the recommendation frequency of an\nitem is positively correlated with this item's popularity. This also leads to\nthe unfair treatment of users with little interest in popular content. Finally,\nwe study long-term fairness dynamics in algorithmic decision support in the\nlabor market using agent-based modeling techniques.",
    "updated" : "2024-06-17T08:37:14Z",
    "published" : "2024-06-17T08:37:14Z",
    "authors" : [
      {
        "name" : "Dominik Kowald"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.11208v1",
    "title" : "Privacy-preserving Pseudonym Schemes for Personalized 3D Avatars in\n  Mobile Social Metaverses",
    "summary" : "The emergence of mobile social metaverses, a novel paradigm bridging physical\nand virtual realms, has led to the widespread adoption of avatars as digital\nrepresentations for Social Metaverse Users (SMUs) within virtual spaces.\nEquipped with immersive devices, SMUs leverage Edge Servers (ESs) to deploy\ntheir avatars and engage with other SMUs in virtual spaces. To enhance\nimmersion, SMUs incline to opt for 3D avatars for social interactions. However,\nexisting 3D avatars are typically generated through scanning the real faces of\nSMUs, which can raise concerns regarding information privacy and security, such\nas profile identity leakages. To tackle this, we introduce a new framework for\npersonalized 3D avatar construction, leveraging a two-layer network model that\nprovides SMUs with the option to customize their personal avatars for privacy\npreservation. Specifically, our approach introduces avatar pseudonyms to\njointly safeguard the profile and digital identity privacy of the generated\navatars. Then, we design a novel metric named Privacy of Personalized Avatars\n(PoPA), to evaluate effectiveness of the avatar pseudonyms. To optimize\npseudonym resource, we model the pseudonym distribution process as a\nStackelberg game and employ Deep Reinforcement Learning (DRL) to learn\nequilibrium strategies under incomplete information. Simulation results\nvalidate the efficacy and feasibility of our proposed schemes for mobile social\nmetaverses.",
    "updated" : "2024-06-17T04:58:09Z",
    "published" : "2024-06-17T04:58:09Z",
    "authors" : [
      {
        "name" : "Cheng Su"
      },
      {
        "name" : "Xiaofeng Luo"
      },
      {
        "name" : "Zhenmou Liu"
      },
      {
        "name" : "Jiawen Kang"
      },
      {
        "name" : "Min Hao"
      },
      {
        "name" : "Zehui Xiong"
      },
      {
        "name" : "Zhaohui Yang"
      },
      {
        "name" : "Chongwen Huang"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.11149v1",
    "title" : "GoldCoin: Grounding Large Language Models in Privacy Laws via Contextual\n  Integrity Theory",
    "summary" : "Privacy issues arise prominently during the inappropriate transmission of\ninformation between entities. Existing research primarily studies privacy by\nexploring various privacy attacks, defenses, and evaluations within narrowly\npredefined patterns, while neglecting that privacy is not an isolated,\ncontext-free concept limited to traditionally sensitive data (e.g., social\nsecurity numbers), but intertwined with intricate social contexts that\ncomplicate the identification and analysis of potential privacy violations. The\nadvent of Large Language Models (LLMs) offers unprecedented opportunities for\nincorporating the nuanced scenarios outlined in privacy laws to tackle these\ncomplex privacy issues. However, the scarcity of open-source relevant case\nstudies restricts the efficiency of LLMs in aligning with specific legal\nstatutes. To address this challenge, we introduce a novel framework, GoldCoin,\ndesigned to efficiently ground LLMs in privacy laws for judicial assessing\nprivacy violations. Our framework leverages the theory of contextual integrity\nas a bridge, creating numerous synthetic scenarios grounded in relevant privacy\nstatutes (e.g., HIPAA), to assist LLMs in comprehending the complex contexts\nfor identifying privacy risks in the real world. Extensive experimental results\ndemonstrate that GoldCoin markedly enhances LLMs' capabilities in recognizing\nprivacy risks across real court cases, surpassing the baselines on different\njudicial tasks.",
    "updated" : "2024-06-17T02:27:32Z",
    "published" : "2024-06-17T02:27:32Z",
    "authors" : [
      {
        "name" : "Wei Fan"
      },
      {
        "name" : "Haoran Li"
      },
      {
        "name" : "Zheye Deng"
      },
      {
        "name" : "Weiqi Wang"
      },
      {
        "name" : "Yangqiu Song"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.11087v1",
    "title" : "MemDPT: Differential Privacy for Memory Efficient Language Models",
    "summary" : "Large language models have consistently demonstrated remarkable performance\nacross a wide spectrum of applications. Nonetheless, the deployment of these\nmodels can inadvertently expose user privacy to potential risks. The\nsubstantial memory demands of these models during training represent a\nsignificant resource consumption challenge. The sheer size of these models\nimposes a considerable burden on memory resources, which is a matter of\nsignificant concern in practice. In this paper, we present an innovative\ntraining framework MemDPT that not only reduces the memory cost of large\nlanguage models but also places a strong emphasis on safeguarding user data\nprivacy. MemDPT provides edge network and reverse network designs to\naccommodate various differential privacy memory-efficient fine-tuning schemes.\nOur approach not only achieves $2 \\sim 3 \\times$ memory optimization but also\nprovides robust privacy protection, ensuring that user data remains secure and\nconfidential. Extensive experiments have demonstrated that MemDPT can\neffectively provide differential privacy efficient fine-tuning across various\ntask scenarios.",
    "updated" : "2024-06-16T22:11:41Z",
    "published" : "2024-06-16T22:11:41Z",
    "authors" : [
      {
        "name" : "Yanming Liu"
      },
      {
        "name" : "Xinyue Peng"
      },
      {
        "name" : "Jiannan Cao"
      },
      {
        "name" : "Yuwei Zhang"
      },
      {
        "name" : "Chen Ma"
      },
      {
        "name" : "Songhang Deng"
      },
      {
        "name" : "Mengchen Fu"
      },
      {
        "name" : "Xuhong Zhang"
      },
      {
        "name" : "Sheng Cheng"
      },
      {
        "name" : "Xun Wang"
      },
      {
        "name" : "Jianwei Yin"
      },
      {
        "name" : "Tianyu Du"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.10976v1",
    "title" : "Promoting Data and Model Privacy in Federated Learning through Quantized\n  LoRA",
    "summary" : "Conventional federated learning primarily aims to secure the privacy of data\ndistributed across multiple edge devices, with the global model dispatched to\nedge devices for parameter updates during the learning process. However, the\ndevelopment of large language models (LLMs) requires substantial data and\ncomputational resources, rendering them valuable intellectual properties for\ntheir developers and owners. To establish a mechanism that protects both data\nand model privacy in a federated learning context, we introduce a method that\njust needs to distribute a quantized version of the model's parameters during\ntraining. This method enables accurate gradient estimations for parameter\nupdates while preventing clients from accessing a model whose performance is\ncomparable to the centrally hosted one. Moreover, we combine this quantization\nstrategy with LoRA, a popular and parameter-efficient fine-tuning method, to\nsignificantly reduce communication costs in federated learning. The proposed\nframework, named \\textsc{FedLPP}, successfully ensures both data and model\nprivacy in the federated learning context. Additionally, the learned central\nmodel exhibits good generalization and can be trained in a resource-efficient\nmanner.",
    "updated" : "2024-06-16T15:23:07Z",
    "published" : "2024-06-16T15:23:07Z",
    "authors" : [
      {
        "name" : "JianHao Zhu"
      },
      {
        "name" : "Changze Lv"
      },
      {
        "name" : "Xiaohua Wang"
      },
      {
        "name" : "Muling Wu"
      },
      {
        "name" : "Wenhao Liu"
      },
      {
        "name" : "Tianlong Li"
      },
      {
        "name" : "Zixuan Ling"
      },
      {
        "name" : "Cenyuan Zhang"
      },
      {
        "name" : "Xiaoqing Zheng"
      },
      {
        "name" : "Xuanjing Huang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.10884v1",
    "title" : "Linkage on Security, Privacy and Fairness in Federated Learning: New\n  Balances and New Perspectives",
    "summary" : "Federated learning is fast becoming a popular paradigm for applications\ninvolving mobile devices, banking systems, healthcare, and IoT systems. Hence,\nover the past five years, researchers have undertaken extensive studies on the\nprivacy leaks, security threats, and fairness associated with these emerging\nmodels. For the most part, these three critical concepts have been studied in\nisolation; however, recent research has revealed that there may be an intricate\ninterplay between them. For instance, some researchers have discovered that\npursuing fairness may compromise privacy, or that efforts to enhance security\ncan impact fairness. These emerging insights shed light on the fundamental\nconnections between privacy, security, and fairness within federated learning,\nand, by delving deeper into these interconnections, we may be able to\nsignificantly augment research and development across the field. Consequently,\nthe aim of this survey is to offer comprehensive descriptions of the privacy,\nsecurity, and fairness issues in federated learning. Moreover, we analyze the\ncomplex relationships between these three dimensions of cyber safety and\npinpoint the fundamental elements that influence each of them. We contend that\nthere exists a trade-off between privacy and fairness and between security and\ngradient sharing. On this basis, fairness can function as a bridge between\nprivacy and security to build models that are either more secure or more\nprivate. Building upon our observations, we identify the trade-offs between\nprivacy and fairness and between security and fairness within the context of\nfederated learning. The survey then concludes with promising directions for\nfuture research in this vanguard field.",
    "updated" : "2024-06-16T10:31:45Z",
    "published" : "2024-06-16T10:31:45Z",
    "authors" : [
      {
        "name" : "Linlin Wang"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Wanlei Zhou"
      },
      {
        "name" : "Philip S. Yu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.10803v1",
    "title" : "HiddenTables & PyQTax: A Cooperative Game and Dataset For TableQA to\n  Ensure Scale and Data Privacy Across a Myriad of Taxonomies",
    "summary" : "A myriad of different Large Language Models (LLMs) face a common challenge in\ncontextually analyzing table question-answering tasks. These challenges are\nengendered from (1) finite context windows for large tables, (2) multi-faceted\ndiscrepancies amongst tokenization patterns against cell boundaries, and (3)\nvarious limitations stemming from data confidentiality in the process of using\nexternal models such as gpt-3.5-turbo. We propose a cooperative game dubbed\n\"HiddenTables\" as a potential resolution to this challenge. In essence,\n\"HiddenTables\" is played between the code-generating LLM \"Solver\" and the\n\"Oracle\" which evaluates the ability of the LLM agents to solve Table QA tasks.\nThis game is based on natural language schemas and importantly, ensures the\nsecurity of the underlying data. We provide evidential experiments on a diverse\nset of tables that demonstrate an LLM's collective inability to generalize and\nperform on complex queries, handle compositional dependencies, and align\nnatural language to programmatic commands when concrete table schemas are\nprovided. Unlike encoder-based models, we have pushed the boundaries of\n\"HiddenTables\" to not be limited by the number of rows - therefore we exhibit\nimproved efficiency in prompt and completion tokens. Our infrastructure has\nspawned a new dataset \"PyQTax\" that spans across 116,671 question-table-answer\ntriplets and provides additional fine-grained breakdowns & labels for varying\nquestion taxonomies. Therefore, in tandem with our academic contributions\nregarding LLMs' deficiency in TableQA tasks, \"HiddenTables\" is a tactile\nmanifestation of how LLMs can interact with massive datasets while ensuring\ndata security and minimizing generation costs.",
    "updated" : "2024-06-16T04:53:29Z",
    "published" : "2024-06-16T04:53:29Z",
    "authors" : [
      {
        "name" : "William Watson"
      },
      {
        "name" : "Nicole Cho"
      },
      {
        "name" : "Tucker Balch"
      },
      {
        "name" : "Manuela Veloso"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.10563v1",
    "title" : "Privacy-Preserving Heterogeneous Federated Learning for Sensitive\n  Healthcare Data",
    "summary" : "In the realm of healthcare where decentralized facilities are prevalent,\nmachine learning faces two major challenges concerning the protection of data\nand models. The data-level challenge concerns the data privacy leakage when\ncentralizing data with sensitive personal information. While the model-level\nchallenge arises from the heterogeneity of local models, which need to be\ncollaboratively trained while ensuring their confidentiality to address\nintellectual property concerns. To tackle these challenges, we propose a new\nframework termed Abstention-Aware Federated Voting (AAFV) that can\ncollaboratively and confidentially train heterogeneous local models while\nsimultaneously protecting the data privacy. This is achieved by integrating a\nnovel abstention-aware voting mechanism and a differential privacy mechanism\nonto local models' predictions. In particular, the proposed abstention-aware\nvoting mechanism exploits a threshold-based abstention method to select\nhigh-confidence votes from heterogeneous local models, which not only enhances\nthe learning utility but also protects model confidentiality. Furthermore, we\nimplement AAFV on two practical prediction tasks of diabetes and in-hospital\npatient mortality. The experiments demonstrate the effectiveness and\nconfidentiality of AAFV in testing accuracy and privacy protection.",
    "updated" : "2024-06-15T08:43:40Z",
    "published" : "2024-06-15T08:43:40Z",
    "authors" : [
      {
        "name" : "Yukai Xu"
      },
      {
        "name" : "Jingfeng Zhang"
      },
      {
        "name" : "Yujie Gu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.10280v1",
    "title" : "Transferable Embedding Inversion Attack: Uncovering Privacy Risks in\n  Text Embeddings without Model Queries",
    "summary" : "This study investigates the privacy risks associated with text embeddings,\nfocusing on the scenario where attackers cannot access the original embedding\nmodel. Contrary to previous research requiring direct model access, we explore\na more realistic threat model by developing a transfer attack method. This\napproach uses a surrogate model to mimic the victim model's behavior, allowing\nthe attacker to infer sensitive information from text embeddings without direct\naccess. Our experiments across various embedding models and a clinical dataset\ndemonstrate that our transfer attack significantly outperforms traditional\nmethods, revealing the potential privacy vulnerabilities in embedding\ntechnologies and emphasizing the need for enhanced security measures.",
    "updated" : "2024-06-12T05:09:58Z",
    "published" : "2024-06-12T05:09:58Z",
    "authors" : [
      {
        "name" : "Yu-Hsiang Huang"
      },
      {
        "name" : "Yuche Tsai"
      },
      {
        "name" : "Hsiang Hsiao"
      },
      {
        "name" : "Hong-Yi Lin"
      },
      {
        "name" : "Shou-De Lin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.12815v1",
    "title" : "Privacy Preserving Federated Learning in Medical Imaging with\n  Uncertainty Estimation",
    "summary" : "Machine learning (ML) and Artificial Intelligence (AI) have fueled remarkable\nadvancements, particularly in healthcare. Within medical imaging, ML models\nhold the promise of improving disease diagnoses, treatment planning, and\npost-treatment monitoring. Various computer vision tasks like image\nclassification, object detection, and image segmentation are poised to become\nroutine in clinical analysis. However, privacy concerns surrounding patient\ndata hinder the assembly of large training datasets needed for developing and\ntraining accurate, robust, and generalizable models. Federated Learning (FL)\nemerges as a compelling solution, enabling organizations to collaborate on ML\nmodel training by sharing model training information (gradients) rather than\ndata (e.g., medical images). FL's distributed learning framework facilitates\ninter-institutional collaboration while preserving patient privacy. However,\nFL, while robust in privacy preservation, faces several challenges. Sensitive\ninformation can still be gleaned from shared gradients that are passed on\nbetween organizations during model training. Additionally, in medical imaging,\nquantifying model confidence\\uncertainty accurately is crucial due to the noise\nand artifacts present in the data. Uncertainty estimation in FL encounters\nunique hurdles due to data heterogeneity across organizations. This paper\noffers a comprehensive review of FL, privacy preservation, and uncertainty\nestimation, with a focus on medical imaging. Alongside a survey of current\nresearch, we identify gaps in the field and suggest future directions for FL\nresearch to enhance privacy and address noisy medical imaging data challenges.",
    "updated" : "2024-06-18T17:35:52Z",
    "published" : "2024-06-18T17:35:52Z",
    "authors" : [
      {
        "name" : "Nikolas Koutsoubis"
      },
      {
        "name" : "Yasin Yilmaz"
      },
      {
        "name" : "Ravi P. Ramachandran"
      },
      {
        "name" : "Matthew Schabath"
      },
      {
        "name" : "Ghulam Rasool"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "eess.IV",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.12736v1",
    "title" : "Beyond Visual Appearances: Privacy-sensitive Objects Identification via\n  Hybrid Graph Reasoning",
    "summary" : "The Privacy-sensitive Object Identification (POI) task allocates bounding\nboxes for privacy-sensitive objects in a scene. The key to POI is settling an\nobject's privacy class (privacy-sensitive or non-privacy-sensitive). In\ncontrast to conventional object classes which are determined by the visual\nappearance of an object, one object's privacy class is derived from the scene\ncontexts and is subject to various implicit factors beyond its visual\nappearance. That is, visually similar objects may be totally opposite in their\nprivacy classes. To explicitly derive the objects' privacy class from the scene\ncontexts, in this paper, we interpret the POI task as a visual reasoning task\naimed at the privacy of each object in the scene. Following this\ninterpretation, we propose the PrivacyGuard framework for POI. PrivacyGuard\ncontains three stages. i) Structuring: an unstructured image is first converted\ninto a structured, heterogeneous scene graph that embeds rich scene contexts.\nii) Data Augmentation: a contextual perturbation oversampling strategy is\nproposed to create slightly perturbed privacy-sensitive objects in a scene\ngraph, thereby balancing the skewed distribution of privacy classes. iii)\nHybrid Graph Generation & Reasoning: the balanced, heterogeneous scene graph is\nthen transformed into a hybrid graph by endowing it with extra \"node-node\" and\n\"edge-edge\" homogeneous paths. These homogeneous paths allow direct message\npassing between nodes or edges, thereby accelerating reasoning and facilitating\nthe capturing of subtle context changes. Based on this hybrid graph... **For\nthe full abstract, see the original paper.**",
    "updated" : "2024-06-18T15:58:22Z",
    "published" : "2024-06-18T15:58:22Z",
    "authors" : [
      {
        "name" : "Zhuohang Jiang"
      },
      {
        "name" : "Bingkui Tong"
      },
      {
        "name" : "Xia Du"
      },
      {
        "name" : "Ahmed Alhammadi"
      },
      {
        "name" : "Jizhe Zhou"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.12592v1",
    "title" : "Unmasking the Veil: An Investigation into Concept Ablation for Privacy\n  and Copyright Protection in Images",
    "summary" : "In this paper, we extend the study of concept ablation within pre-trained\nmodels as introduced in 'Ablating Concepts in Text-to-Image Diffusion Models'\nby (Kumari et al.,2022). Our work focuses on reproducing the results achieved\nby the different variants of concept ablation proposed and validated through\npredefined metrics. We also introduce a novel variant of concept ablation,\nnamely 'trademark ablation'. This variant combines the principles of\nmemorization and instance ablation to tackle the nuanced influence of\nproprietary or branded elements in model outputs. Further, our research\ncontributions include an observational analysis of the model's limitations.\nMoreover, we investigate the model's behavior in response to ablation\nleakage-inducing prompts, which aim to indirectly ablate concepts, revealing\ninsights into the model's resilience and adaptability. We also observe the\nmodel's performance degradation on images generated by concepts far from its\ntarget ablation concept, documented in the appendix.",
    "updated" : "2024-06-18T13:22:29Z",
    "published" : "2024-06-18T13:22:29Z",
    "authors" : [
      {
        "name" : "Shivank Garg"
      },
      {
        "name" : "Manyana Tiwari"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.12457v1",
    "title" : "Data Trade and Consumer Privacy",
    "summary" : "This paper studies optimal mechanisms for collecting and trading data.\nConsumers benefit from revealing information about their tastes to a service\nprovider because this improves the service. However, the information is also\nvaluable to a third party as it may extract more revenue from the consumer in\nanother market called the product market. The paper characterizes the\nconstrained optimal mechanism for the service provider subject to incentive\nfeasibility. It is shown that the service provider sometimes sells no\ninformation or only partial information in order to preserve profits in the\nservice market. In a general setup, the service provision distortion and\nno-price discrimination in the product market are exclusive. Moreover, a ban on\ndata trade may reduce social welfare because it makes it harder to price\ndiscriminate in the product market.",
    "updated" : "2024-06-18T10:00:44Z",
    "published" : "2024-06-18T10:00:44Z",
    "authors" : [
      {
        "name" : "Jiadong Gu"
      }
    ],
    "categories" : [
      "econ.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.12403v1",
    "title" : "PDSS: A Privacy-Preserving Framework for Step-by-Step Distillation of\n  Large Language Models",
    "summary" : "In the context of real-world applications, leveraging large language models\n(LLMs) for domain-specific tasks often faces two major challenges:\ndomain-specific knowledge privacy and constrained resources. To address these\nissues, we propose PDSS, a privacy-preserving framework for step-by-step\ndistillation of LLMs. PDSS works on a server-client architecture, wherein\nclient transmits perturbed prompts to the server's LLM for rationale\ngeneration. The generated rationales are then decoded by the client and used to\nenrich the training of task-specific small language model(SLM) within a\nmulti-task learning paradigm. PDSS introduces two privacy protection\nstrategies: the Exponential Mechanism Strategy and the Encoder-Decoder\nStrategy, balancing prompt privacy and rationale usability. Experiments\ndemonstrate the effectiveness of PDSS in various text generation tasks,\nenabling the training of task-specific SLM with enhanced performance while\nprioritizing data privacy protection.",
    "updated" : "2024-06-18T08:48:14Z",
    "published" : "2024-06-18T08:48:14Z",
    "authors" : [
      {
        "name" : "Tao Fan"
      },
      {
        "name" : "Yan Kang"
      },
      {
        "name" : "Weijing Chen"
      },
      {
        "name" : "Hanlin Gu"
      },
      {
        "name" : "Yuanfeng Song"
      },
      {
        "name" : "Lixin Fan"
      },
      {
        "name" : "Kai Chen"
      },
      {
        "name" : "Qiang Yang"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.12330v1",
    "title" : "Security and Privacy of 6G Federated Learning-enabled Dynamic Spectrum\n  Sharing",
    "summary" : "Spectrum sharing is increasingly vital in 6G wireless communication,\nfacilitating dynamic access to unused spectrum holes. Recently, there has been\na significant shift towards employing machine learning (ML) techniques for\nsensing spectrum holes. In this context, federated learning (FL)-enabled\nspectrum sensing technology has garnered wide attention, allowing for the\nconstruction of an aggregated ML model without disclosing the private spectrum\nsensing information of wireless user devices. However, the integrity of\ncollaborative training and the privacy of spectrum information from local users\nhave remained largely unexplored. This article first examines the latest\ndevelopments in FL-enabled spectrum sharing for prospective 6G scenarios. It\nthen identifies practical attack vectors in 6G to illustrate potential\nAI-powered security and privacy threats in these contexts. Finally, the study\noutlines future directions, including practical defense challenges and\nguidelines.",
    "updated" : "2024-06-18T06:54:15Z",
    "published" : "2024-06-18T06:54:15Z",
    "authors" : [
      {
        "name" : "Viet Vo"
      },
      {
        "name" : "Thusitha Dayaratne"
      },
      {
        "name" : "Blake Haydon"
      },
      {
        "name" : "Xingliang Yuan"
      },
      {
        "name" : "Shangqi Lai"
      },
      {
        "name" : "Sharif Abuadbba"
      },
      {
        "name" : "Hajime Suzuki"
      },
      {
        "name" : "Carsten Rudolph"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.ET",
      "cs.LG",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.12238v1",
    "title" : "PFID: Privacy First Inference Delegation Framework for LLMs",
    "summary" : "This paper introduces a novel privacy-preservation framework named PFID for\nLLMs that addresses critical privacy concerns by localizing user data through\nmodel sharding and singular value decomposition. When users are interacting\nwith LLM systems, their prompts could be subject to being exposed to\neavesdroppers within or outside LLM system providers who are interested in\ncollecting users' input. In this work, we proposed a framework to camouflage\nuser input, so as to alleviate privacy issues. Our framework proposes to place\nmodel shards on the client and the public server, we sent compressed hidden\nstates instead of prompts to and from servers. Clients have held back\ninformation that can re-privatized the hidden states so that overall system\nperformance is comparable to traditional LLMs services. Our framework was\ndesigned to be communication efficient, computation can be delegated to the\nlocal client so that the server's computation burden can be lightened. We\nconduct extensive experiments on machine translation tasks to verify our\nframework's performance.",
    "updated" : "2024-06-18T03:27:09Z",
    "published" : "2024-06-18T03:27:09Z",
    "authors" : [
      {
        "name" : "Haoyan Yang"
      },
      {
        "name" : "Zhitao Li"
      },
      {
        "name" : "Yong Zhang"
      },
      {
        "name" : "Jianzong Wang"
      },
      {
        "name" : "Ning Cheng"
      },
      {
        "name" : "Ming Li"
      },
      {
        "name" : "Jing Xiao"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.12103v1",
    "title" : "Centering Policy and Practice: Research Gaps around Usable Differential\n  Privacy",
    "summary" : "As a mathematically rigorous framework that has amassed a rich theoretical\nliterature, differential privacy is considered by many experts to be the gold\nstandard for privacy-preserving data analysis. Others argue that while\ndifferential privacy is a clean formulation in theory, it poses significant\nchallenges in practice. Both perspectives are, in our view, valid and\nimportant. To bridge the gaps between differential privacy's promises and its\nreal-world usability, researchers and practitioners must work together to\nadvance policy and practice of this technology. In this paper, we outline\npressing open questions towards building usable differential privacy and offer\nrecommendations for the field, such as developing risk frameworks to align with\nuser needs, tailoring communications for different stakeholders, modeling the\nimpact of privacy-loss parameters, investing in effective user interfaces, and\nfacilitating algorithmic and procedural audits of differential privacy systems.",
    "updated" : "2024-06-17T21:32:30Z",
    "published" : "2024-06-17T21:32:30Z",
    "authors" : [
      {
        "name" : "Rachel Cummings"
      },
      {
        "name" : "Jayshree Sarathy"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.12003v1",
    "title" : "P3GNN: A Privacy-Preserving Provenance Graph-Based Model for APT\n  Detection in Software Defined Networking",
    "summary" : "Software Defined Networking (SDN) has brought significant advancements in\nnetwork management and programmability. However, this evolution has also\nheightened vulnerability to Advanced Persistent Threats (APTs), sophisticated\nand stealthy cyberattacks that traditional detection methods often fail to\ncounter, especially in the face of zero-day exploits. A prevalent issue is the\ninadequacy of existing strategies to detect novel threats while addressing data\nprivacy concerns in collaborative learning scenarios. This paper presents P3GNN\n(privacy-preserving provenance graph-based graph neural network model), a novel\nmodel that synergizes Federated Learning (FL) with Graph Convolutional Networks\n(GCN) for effective APT detection in SDN environments. P3GNN utilizes\nunsupervised learning to analyze operational patterns within provenance graphs,\nidentifying deviations indicative of security breaches. Its core feature is the\nintegration of FL with homomorphic encryption, which fortifies data\nconfidentiality and gradient integrity during collaborative learning. This\napproach addresses the critical challenge of data privacy in shared learning\ncontexts. Key innovations of P3GNN include its ability to detect anomalies at\nthe node level within provenance graphs, offering a detailed view of attack\ntrajectories and enhancing security analysis. Furthermore, the models\nunsupervised learning capability enables it to identify zero-day attacks by\nlearning standard operational patterns. Empirical evaluation using the DARPA\nTCE3 dataset demonstrates P3GNNs exceptional performance, achieving an accuracy\nof 0.93 and a low false positive rate of 0.06.",
    "updated" : "2024-06-17T18:14:03Z",
    "published" : "2024-06-17T18:14:03Z",
    "authors" : [
      {
        "name" : "Hedyeh Nazari"
      },
      {
        "name" : "Abbas Yazdinejad"
      },
      {
        "name" : "Ali Dehghantanha"
      },
      {
        "name" : "Fattane Zarrinkalam"
      },
      {
        "name" : "Gautam Srivastava"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.07973v2",
    "title" : "Unique Security and Privacy Threats of Large Language Model: A\n  Comprehensive Survey",
    "summary" : "With the rapid development of artificial intelligence, large language models\n(LLMs) have made remarkable advancements in natural language processing. These\nmodels are trained on vast datasets to exhibit powerful language understanding\nand generation capabilities across various applications, including machine\ntranslation, chatbots, and agents. However, LLMs have revealed a variety of\nprivacy and security issues throughout their life cycle, drawing significant\nacademic and industrial attention. Moreover, the risks faced by LLMs differ\nsignificantly from those encountered by traditional language models. Given that\ncurrent surveys lack a clear taxonomy of unique threat models across diverse\nscenarios, we emphasize the unique privacy and security threats associated with\nfive specific scenarios: pre-training, fine-tuning, retrieval-augmented\ngeneration systems, deployment, and LLM-based agents. Addressing the\ncharacteristics of each risk, this survey outlines potential threats and\ncountermeasures. Research on attack and defense situations can offer feasible\nresearch directions, enabling more areas to benefit from LLMs.",
    "updated" : "2024-06-18T05:37:06Z",
    "published" : "2024-06-12T07:55:32Z",
    "authors" : [
      {
        "name" : "Shang Wang"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Bo Liu"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Xu Guo"
      },
      {
        "name" : "Dayong Ye"
      },
      {
        "name" : "Wanlei Zhou"
      },
      {
        "name" : "Philip S. Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.14322v1",
    "title" : "Mind the Privacy Unit! User-Level Differential Privacy for Language\n  Model Fine-Tuning",
    "summary" : "Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are `almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff.",
    "updated" : "2024-06-20T13:54:32Z",
    "published" : "2024-06-20T13:54:32Z",
    "authors" : [
      {
        "name" : "Lynn Chua"
      },
      {
        "name" : "Badih Ghazi"
      },
      {
        "name" : "Yangsibo Huang"
      },
      {
        "name" : "Pritish Kamath"
      },
      {
        "name" : "Daogao Liu"
      },
      {
        "name" : "Pasin Manurangsi"
      },
      {
        "name" : "Amer Sinha"
      },
      {
        "name" : "Chiyuan Zhang"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.14318v1",
    "title" : "The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in\n  Prompts",
    "summary" : "The rapid adoption of online chatbots represents a significant advancement in\nartificial intelligence. However, this convenience brings considerable privacy\nconcerns, as prompts can inadvertently contain sensitive information exposed to\nlarge language models (LLMs). Limited by high computational costs, reduced task\nusability, and excessive system modifications, previous works based on local\ndeployment, embedding perturbation, and homomorphic encryption are inapplicable\nto online prompt-based LLM applications.\n  To address these issues, this paper introduces Prompt Privacy Sanitizer\n(i.e., ProSan), an end-to-end prompt privacy protection framework that can\nproduce anonymized prompts with contextual privacy removed while maintaining\ntask usability and human readability. It can also be seamlessly integrated into\nthe online LLM service pipeline. To achieve high usability and dynamic\nanonymity, ProSan flexibly adjusts its protection targets and strength based on\nthe importance of the words and the privacy leakage risk of the prompts.\nAdditionally, ProSan is capable of adapting to diverse computational resource\nconditions, ensuring privacy protection even for mobile devices with limited\ncomputing power. Our experiments demonstrate that ProSan effectively removes\nprivate information across various tasks, including question answering, text\nsummarization, and code generation, with minimal reduction in task performance.",
    "updated" : "2024-06-20T13:52:25Z",
    "published" : "2024-06-20T13:52:25Z",
    "authors" : [
      {
        "name" : "Zhili Shen"
      },
      {
        "name" : "Zihang Xi"
      },
      {
        "name" : "Ying He"
      },
      {
        "name" : "Wei Tong"
      },
      {
        "name" : "Jingyu Hua"
      },
      {
        "name" : "Sheng Zhong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.14091v1",
    "title" : "Protecting Privacy Through Approximating Optimal Parameters for Sequence\n  Unlearning in Language Models",
    "summary" : "Although language models (LMs) demonstrate exceptional capabilities on\nvarious tasks, they are potentially vulnerable to extraction attacks, which\nrepresent a significant privacy risk. To mitigate the privacy concerns of LMs,\nmachine unlearning has emerged as an important research area, which is utilized\nto induce the LM to selectively forget about some of its training data. While\ncompletely retraining the model will guarantee successful unlearning and\nprivacy assurance, it is impractical for LMs, as it would be time-consuming and\nresource-intensive. Prior works efficiently unlearn the target token sequences,\nbut upon subsequent iterations, the LM displays significant degradation in\nperformance. In this work, we propose Privacy Protection via Optimal Parameters\n(POP), a novel unlearning method that effectively forgets the target token\nsequences from the pretrained LM by applying optimal gradient updates to the\nparameters. Inspired by the gradient derivation of complete retraining, we\napproximate the optimal training objective that successfully unlearns the\ntarget sequence while retaining the knowledge from the rest of the training\ndata. Experimental results demonstrate that POP exhibits remarkable retention\nperformance post-unlearning across 9 classification and 4 dialogue benchmarks,\noutperforming the state-of-the-art by a large margin. Furthermore, we introduce\nRemnant Memorization Accuracy that quantifies privacy risks based on token\nlikelihood and validate its effectiveness through both qualitative and\nquantitative analyses.",
    "updated" : "2024-06-20T08:12:49Z",
    "published" : "2024-06-20T08:12:49Z",
    "authors" : [
      {
        "name" : "Dohyun Lee"
      },
      {
        "name" : "Daniel Rim"
      },
      {
        "name" : "Minseok Choi"
      },
      {
        "name" : "Jaegul Choo"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.13880v1",
    "title" : "Privacy-Preserving ECG Data Analysis with Differential Privacy: A\n  Literature Review and A Case Study",
    "summary" : "Differential privacy has become the preeminent technique to protect the\nprivacy of individuals in a database while allowing useful results from data\nanalysis to be shared. Notably, it guarantees the amount of privacy loss in the\nworst-case scenario. Although many theoretical research papers have been\npublished, practical real-life application of differential privacy demands\nestimating several important parameters without any clear solutions or\nguidelines. In the first part of the paper, we provide an overview of key\nconcepts in differential privacy, followed by a literature review and\ndiscussion of its application to ECG analysis. In the second part of the paper,\nwe explore how to implement differentially private query release on an\narrhythmia database using a six-step process. We provide guidelines and discuss\nthe related literature for all the steps involved, such as selection of the\n$\\epsilon$ value, distribution of the total $\\epsilon$ budget across the\nqueries, and estimation of the sensitivity for the query functions. At the end,\nwe discuss the shortcomings and challenges of applying differential privacy to\nECG datasets.",
    "updated" : "2024-06-19T23:17:16Z",
    "published" : "2024-06-19T23:17:16Z",
    "authors" : [
      {
        "name" : "Arin Ghazarian"
      },
      {
        "name" : "Jianwei Zheng"
      },
      {
        "name" : "Cyril Rakovski"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.13433v1",
    "title" : "Certificates of Differential Privacy and Unlearning for Gradient-Based\n  Training",
    "summary" : "Proper data stewardship requires that model owners protect the privacy of\nindividuals' data used during training. Whether through anonymization with\ndifferential privacy or the use of unlearning in non-anonymized settings, the\ngold-standard techniques for providing privacy guarantees can come with\nsignificant performance penalties or be too weak to provide practical\nassurances. In part, this is due to the fact that the guarantee provided by\ndifferential privacy represents the worst-case privacy leakage for any\nindividual, while the true privacy leakage of releasing the prediction for a\ngiven individual might be substantially smaller or even, as we show,\nnon-existent. This work provides a novel framework based on convex relaxations\nand bounds propagation that can compute formal guarantees (certificates) that\nreleasing specific predictions satisfies $\\epsilon=0$ privacy guarantees or do\nnot depend on data that is subject to an unlearning request. Our framework\noffers a new verification-centric approach to privacy and unlearning\nguarantees, that can be used to further engender user trust with tighter\nprivacy guarantees, provide formal proofs of robustness to certain membership\ninference attacks, identify potentially vulnerable records, and enhance current\nunlearning approaches. We validate the effectiveness of our approach on tasks\nfrom financial services, medical imaging, and natural language processing.",
    "updated" : "2024-06-19T10:47:00Z",
    "published" : "2024-06-19T10:47:00Z",
    "authors" : [
      {
        "name" : "Matthew Wicker"
      },
      {
        "name" : "Philip Sosnin"
      },
      {
        "name" : "Adrianna Janik"
      },
      {
        "name" : "Mark N. Müller"
      },
      {
        "name" : "Adrian Weller"
      },
      {
        "name" : "Calvin Tsay"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.13221v1",
    "title" : "Privacy-Preserving Logistic Regression Training on Large Datasets",
    "summary" : "Privacy-preserving machine learning is one class of cryptographic methods\nthat aim to analyze private and sensitive data while keeping privacy, such as\nhomomorphic logistic regression training over large encrypted data. In this\npaper, we propose an efficient algorithm for logistic regression training on\nlarge encrypted data using Homomorphic Encryption (HE), which is the mini-batch\nversion of recent methods using a faster gradient variant called\n$\\texttt{quadratic gradient}$. It is claimed that $\\texttt{quadratic gradient}$\ncan integrate curve information (Hessian matrix) into the gradient and\ntherefore can effectively accelerate the first-order gradient (descent)\nalgorithms. We also implement the full-batch version of their method when the\nencrypted dataset is so large that it has to be encrypted in the mini-batch\nmanner. We compare our mini-batch algorithm with our full-batch implementation\nmethod on real financial data consisting of 422,108 samples with 200 freatures.\n%Our experiments show that Nesterov's accelerated gradient (NAG) Given the\ninefficiency of HEs, our results are inspiring and demonstrate that the\nlogistic regression training on large encrypted dataset is of practical\nfeasibility, marking a significant milestone in our understanding.",
    "updated" : "2024-06-19T05:19:20Z",
    "published" : "2024-06-19T05:19:20Z",
    "authors" : [
      {
        "name" : "John Chiang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.13183v1",
    "title" : "Communication-Efficient and Privacy-Preserving Decentralized\n  Meta-Learning",
    "summary" : "Distributed learning, which does not require gathering training data in a\ncentral location, has become increasingly important in the big-data era. In\nparticular, random-walk-based decentralized algorithms are flexible in that\nthey do not need a central server trusted by all clients and do not require all\nclients to be active in all iterations. However, existing distributed learning\nalgorithms assume that all learning clients share the same task. In this paper,\nwe consider the more difficult meta-learning setting, in which different\nclients perform different (but related) tasks with limited training data. To\nreduce communication cost and allow better privacy protection, we propose\nLoDMeta (Local Decentralized Meta-learning) with the use of local auxiliary\noptimization parameters and random perturbations on the model parameter.\nTheoretical results are provided on both convergence and privacy analysis.\nEmpirical results on a number of few-shot learning data sets demonstrate that\nLoDMeta has similar meta-learning accuracy as centralized meta-learning\nalgorithms, but does not require gathering data from each client and is able to\nbetter protect data privacy for each client.",
    "updated" : "2024-06-19T03:29:51Z",
    "published" : "2024-06-19T03:29:51Z",
    "authors" : [
      {
        "name" : "Hansi Yang"
      },
      {
        "name" : "James T. Kwok"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.13012v1",
    "title" : "Data Plagiarism Index: Characterizing the Privacy Risk of Data-Copying\n  in Tabular Generative Models",
    "summary" : "The promise of tabular generative models is to produce realistic synthetic\ndata that can be shared and safely used without dangerous leakage of\ninformation from the training set. In evaluating these models, a variety of\nmethods have been proposed to measure the tendency to copy data from the\ntraining dataset when generating a sample. However, these methods suffer from\neither not considering data-copying from a privacy threat perspective, not\nbeing motivated by recent results in the data-copying literature or being\ndifficult to make compatible with the high dimensional, mixed type nature of\ntabular data. This paper proposes a new similarity metric and Membership\nInference Attack called Data Plagiarism Index (DPI) for tabular data. We show\nthat DPI evaluates a new intuitive definition of data-copying and characterizes\nthe corresponding privacy risk. We show that the data-copying identified by DPI\nposes both privacy and fairness threats to common, high performing\narchitectures; underscoring the necessity for more sophisticated generative\nmodeling techniques to mitigate this issue.",
    "updated" : "2024-06-18T19:05:24Z",
    "published" : "2024-06-18T19:05:24Z",
    "authors" : [
      {
        "name" : "Joshua Ward"
      },
      {
        "name" : "Chi-Hua Wang"
      },
      {
        "name" : "Guang Cheng"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.11087v2",
    "title" : "MemDPT: Differential Privacy for Memory Efficient Language Models",
    "summary" : "Large language models have consistently demonstrated remarkable performance\nacross a wide spectrum of applications. Nonetheless, the deployment of these\nmodels can inadvertently expose user privacy to potential risks. The\nsubstantial memory demands of these models during training represent a\nsignificant resource consumption challenge. The sheer size of these models\nimposes a considerable burden on memory resources, which is a matter of\nsignificant concern in practice. In this paper, we present an innovative\ntraining framework MemDPT that not only reduces the memory cost of large\nlanguage models but also places a strong emphasis on safeguarding user data\nprivacy. MemDPT provides edge network and reverse network designs to\naccommodate various differential privacy memory-efficient fine-tuning schemes.\nOur approach not only achieves $2 \\sim 3 \\times$ memory optimization but also\nprovides robust privacy protection, ensuring that user data remains secure and\nconfidential. Extensive experiments have demonstrated that MemDPT can\neffectively provide differential privacy efficient fine-tuning across various\ntask scenarios.",
    "updated" : "2024-06-20T05:43:50Z",
    "published" : "2024-06-16T22:11:41Z",
    "authors" : [
      {
        "name" : "Yanming Liu"
      },
      {
        "name" : "Xinyue Peng"
      },
      {
        "name" : "Jiannan Cao"
      },
      {
        "name" : "Yuwei Zhang"
      },
      {
        "name" : "Chen Ma"
      },
      {
        "name" : "Songhang Deng"
      },
      {
        "name" : "Mengchen Fu"
      },
      {
        "name" : "Xuhong Zhang"
      },
      {
        "name" : "Sheng Cheng"
      },
      {
        "name" : "Xun Wang"
      },
      {
        "name" : "Jianwei Yin"
      },
      {
        "name" : "Tianyu Du"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.15346v1",
    "title" : "Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous\n  Decentralized Federated Learning Approach",
    "summary" : "Newly diagnosed Type 1 Diabetes (T1D) patients often struggle to obtain\neffective Blood Glucose (BG) prediction models due to the lack of sufficient BG\ndata from Continuous Glucose Monitoring (CGM), presenting a significant \"cold\nstart\" problem in patient care. Utilizing population models to address this\nchallenge is a potential solution, but collecting patient data for training\npopulation models in a privacy-conscious manner is challenging, especially\ngiven that such data is often stored on personal devices. Considering the\nprivacy protection and addressing the \"cold start\" problem in diabetes care, we\npropose \"GluADFL\", blood Glucose prediction by Asynchronous Decentralized\nFederated Learning. We compared GluADFL with eight baseline methods using four\ndistinct T1D datasets, comprising 298 participants, which demonstrated its\nsuperior performance in accurately predicting BG levels for cross-patient\nanalysis. Furthermore, patients' data might be stored and shared across various\ncommunication networks in GluADFL, ranging from highly interconnected (e.g.,\nrandom, performs the best among others) to more structured topologies (e.g.,\ncluster and ring), suitable for various social networks. The asynchronous\ntraining framework supports flexible participation. By adjusting the ratios of\ninactive participants, we found it remains stable if less than 70% are\ninactive. Our results confirm that GluADFL offers a practical,\nprivacy-preserving solution for BG prediction in T1D, significantly enhancing\nthe quality of diabetes management.",
    "updated" : "2024-06-21T17:57:39Z",
    "published" : "2024-06-21T17:57:39Z",
    "authors" : [
      {
        "name" : "Chengzhe Piao"
      },
      {
        "name" : "Taiyu Zhu"
      },
      {
        "name" : "Yu Wang"
      },
      {
        "name" : "Stephanie E Baldeweg"
      },
      {
        "name" : "Paul Taylor"
      },
      {
        "name" : "Pantelis Georgiou"
      },
      {
        "name" : "Jiahao Sun"
      },
      {
        "name" : "Jun Wang"
      },
      {
        "name" : "Kezhi Li"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.15309v1",
    "title" : "The Privacy-Utility Trade-off in the Topics API",
    "summary" : "The ongoing deprecation of third-party cookies by web browser vendors has\nsparked the proposal of alternative methods to support more privacy-preserving\npersonalized advertising on web browsers and applications. The Topics API is\nbeing proposed by Google to provide third-parties with \"coarse-grained\nadvertising topics that the page visitor might currently be interested in\". In\nthis paper, we analyze the re-identification risks for individual Internet\nusers and the utility provided to advertising companies by the Topics API, i.e.\nlearning the most popular topics and distinguishing between real and random\ntopics. We provide theoretical results dependent only on the API parameters\nthat can be readily applied to evaluate the privacy and utility implications of\nfuture API updates, including novel general upper-bounds that account for\nadversaries with access to unknown, arbitrary side information, the value of\nthe differential privacy parameter $\\epsilon$, and experimental results on\nreal-world data that validate our theoretical model.",
    "updated" : "2024-06-21T17:01:23Z",
    "published" : "2024-06-21T17:01:23Z",
    "authors" : [
      {
        "name" : "Mário S. Alvim"
      },
      {
        "name" : "Natasha Fernandes"
      },
      {
        "name" : "Annabelle McIver"
      },
      {
        "name" : "Gabriel H. Nunes"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.15074v1",
    "title" : "Balancing The Perception of Cheating Detection, Privacy and Fairness: A\n  Mixed-Methods Study of Visual Data Obfuscation in Remote Proctoring",
    "summary" : "Remote proctoring technology, a cheating-preventive measure, often raises\nprivacy and fairness concerns that may affect test-takers' experiences and the\nvalidity of test results. Our study explores how selectively obfuscating\ninformation in video recordings can protect test-takers' privacy while ensuring\neffective and fair cheating detection. Interviews with experts (N=9) identified\nfour key video regions indicative of potential cheating behaviors: the\ntest-taker's face, body, background and the presence of individuals in the\nbackground. Experts recommended specific obfuscation methods for each region\nbased on privacy significance and cheating behavior frequency, ranging from\nconventional blurring to advanced methods like replacement with deepfake, 3D\navatars and silhouetting. We then conducted a vignette experiment with\npotential test-takers (N=259, non-experts) to evaluate their perceptions of\ncheating detection, visual privacy and fairness, using descriptions and\nexamples of still images for each expert-recommended combination of video\nregions and obfuscation methods. Our results indicate that the effectiveness of\nobfuscation methods varies by region. Tailoring remote proctoring with\nregion-specific advanced obfuscation methods can improve the perceptions of\nprivacy and fairness compared to the conventional methods, though it may\ndecrease perceived information sufficiency for detecting cheating. However,\nnon-experts preferred conventional blurring for videos they were more willing\nto share, highlighting a gap between the perceived effectiveness of the\nadvanced obfuscation methods and their practical acceptance. This study\ncontributes to the field of user-centered privacy by suggesting promising\ndirections to address current remote proctoring challenges and guiding future\nresearch.",
    "updated" : "2024-06-21T11:40:56Z",
    "published" : "2024-06-21T11:40:56Z",
    "authors" : [
      {
        "name" : "Suvadeep Mukherjee"
      },
      {
        "name" : "Verena Distler"
      },
      {
        "name" : "Gabriele Lenzini"
      },
      {
        "name" : "Pedro Cardoso-Leite"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.14773v1",
    "title" : "Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG)\n  via Pure Synthetic Data",
    "summary" : "Retrieval-augmented generation (RAG) enhances the outputs of language models\nby integrating relevant information retrieved from external knowledge sources.\nHowever, when the retrieval process involves private data, RAG systems may face\nsevere privacy risks, potentially leading to the leakage of sensitive\ninformation. To address this issue, we propose using synthetic data as a\nprivacy-preserving alternative for the retrieval data. We propose SAGE, a novel\ntwo-stage synthetic data generation paradigm. In the stage-1, we employ an\nattribute-based extraction and generation approach to preserve key contextual\ninformation from the original data. In the stage-2, we further enhance the\nprivacy properties of the synthetic data through an agent-based iterative\nrefinement process. Extensive experiments demonstrate that using our synthetic\ndata as the retrieval context achieves comparable performance to using the\noriginal data while substantially reducing privacy risks. Our work takes the\nfirst step towards investigating the possibility of generating high-utility and\nprivacy-preserving synthetic data for RAG, opening up new opportunities for the\nsafe application of RAG systems in various domains.",
    "updated" : "2024-06-20T22:53:09Z",
    "published" : "2024-06-20T22:53:09Z",
    "authors" : [
      {
        "name" : "Shenglai Zeng"
      },
      {
        "name" : "Jiankun Zhang"
      },
      {
        "name" : "Pengfei He"
      },
      {
        "name" : "Jie Ren"
      },
      {
        "name" : "Tianqi Zheng"
      },
      {
        "name" : "Hanqing Lu"
      },
      {
        "name" : "Han Xu"
      },
      {
        "name" : "Hui Liu"
      },
      {
        "name" : "Yue Xing"
      },
      {
        "name" : "Jiliang Tang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.14772v1",
    "title" : "Consistent community detection in multi-layer networks with\n  heterogeneous differential privacy",
    "summary" : "As network data has become increasingly prevalent, a substantial amount of\nattention has been paid to the privacy issue in publishing network data. One of\nthe critical challenges for data publishers is to preserve the topological\nstructures of the original network while protecting sensitive information. In\nthis paper, we propose a personalized edge flipping mechanism that allows data\npublishers to protect edge information based on each node's privacy preference.\nIt can achieve differential privacy while preserving the community structure\nunder the multi-layer degree-corrected stochastic block model after\nappropriately debiasing, and thus consistent community detection in the\nprivatized multi-layer networks is achievable. Theoretically, we establish the\nconsistency of community detection in the privatized multi-layer network and\nshow that better privacy protection of edges can be obtained for a proportion\nof nodes while allowing other nodes to give up their privacy. Furthermore, the\nadvantage of the proposed personalized edge-flipping mechanism is also\nsupported by its numerical performance on various synthetic networks and a\nreal-life multi-layer network.",
    "updated" : "2024-06-20T22:49:55Z",
    "published" : "2024-06-20T22:49:55Z",
    "authors" : [
      {
        "name" : "Yaoming Zhen"
      },
      {
        "name" : "Shirong Xu"
      },
      {
        "name" : "Junhui Wang"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.09547v2",
    "title" : "FLea: Addressing Data Scarcity and Label Skew in Federated Learning via\n  Privacy-preserving Feature Augmentation",
    "summary" : "Federated Learning (FL) enables model development by leveraging data\ndistributed across numerous edge devices without transferring local data to a\ncentral server. However, existing FL methods still face challenges when dealing\nwith scarce and label-skewed data across devices, resulting in local model\noverfitting and drift, consequently hindering the performance of the global\nmodel. In response to these challenges, we propose a pioneering framework\ncalled FLea, incorporating the following key components: i) A global feature\nbuffer that stores activation-target pairs shared from multiple clients to\nsupport local training. This design mitigates local model drift caused by the\nabsence of certain classes; ii) A feature augmentation approach based on local\nand global activation mix-ups for local training. This strategy enlarges the\ntraining samples, thereby reducing the risk of local overfitting; iii) An\nobfuscation method to minimize the correlation between intermediate activations\nand the source data, enhancing the privacy of shared features. To verify the\nsuperiority of FLea, we conduct extensive experiments using a wide range of\ndata modalities, simulating different levels of local data scarcity and label\nskew. The results demonstrate that FLea consistently outperforms\nstate-of-the-art FL counterparts (among 13 of the experimented 18 settings, the\nimprovement is over 5% while concurrently mitigating the privacy\nvulnerabilities associated with shared features. Code is available at\nhttps://github.com/XTxiatong/FLea.git.",
    "updated" : "2024-06-18T11:49:36Z",
    "published" : "2024-06-13T19:28:08Z",
    "authors" : [
      {
        "name" : "Tong Xia"
      },
      {
        "name" : "Abhirup Ghosh"
      },
      {
        "name" : "Xinchi Qiu"
      },
      {
        "name" : "Cecilia Mascolo"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.06990v2",
    "title" : "Privacy-Utility Tradeoff Based on $α$-lift",
    "summary" : "Information density and its exponential form, known as lift, play a central\nrole in information privacy leakage measures. $\\alpha$-lift is the power-mean\nof lift, which is tunable between the worst-case measure max-lift\n($\\alpha=\\infty$) and more relaxed versions ($\\alpha<\\infty$). This paper\ninvestigates the optimization problem of the privacy-utility tradeoff (PUT)\nwhere $\\alpha$-lift and mutual information are privacy and utility measures,\nrespectively. Due to the nonlinear nature of $\\alpha$-lift for $\\alpha<\\infty$,\nfinding the optimal solution is challenging. Therefore, we propose a heuristic\nalgorithm to estimate the optimal utility for each value of $\\alpha$, inspired\nby the optimal solution for $\\alpha=\\infty$ and the convexity of $\\alpha$-lift\nwith respect to the lift, which we prove. The numerical results show the\nefficacy of the algorithm and indicate the effective range of $\\alpha$ and\nprivacy budget $\\varepsilon$ with good PUT performance.",
    "updated" : "2024-06-21T00:31:27Z",
    "published" : "2024-06-11T06:39:57Z",
    "authors" : [
      {
        "name" : "Mohammad Amin Zarrabian"
      },
      {
        "name" : "Parastoo Sadeghi"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03820v2",
    "title" : "A Survey on Intelligent Internet of Things: Applications, Security,\n  Privacy, and Future Directions",
    "summary" : "The rapid advances in the Internet of Things (IoT) have promoted a revolution\nin communication technology and offered various customer services. Artificial\nintelligence (AI) techniques have been exploited to facilitate IoT operations\nand maximize their potential in modern application scenarios. In particular,\nthe convergence of IoT and AI has led to a new networking paradigm called\nIntelligent IoT (IIoT), which has the potential to significantly transform\nbusinesses and industrial domains. This paper presents a comprehensive survey\nof IIoT by investigating its significant applications in mobile networks, as\nwell as its associated security and privacy issues. Specifically, we explore\nand discuss the roles of IIoT in a wide range of key application domains, from\nsmart healthcare and smart cities to smart transportation and smart industries.\nThrough such extensive discussions, we investigate important security issues in\nIIoT networks, where network attacks, confidentiality, integrity, and intrusion\nare analyzed, along with a discussion of potential countermeasures. Privacy\nissues in IIoT networks were also surveyed and discussed, including data,\nlocation, and model privacy leakage. Finally, we outline several key challenges\nand highlight potential research directions in this important area.",
    "updated" : "2024-06-21T14:43:41Z",
    "published" : "2024-06-06T07:55:30Z",
    "authors" : [
      {
        "name" : "Ons Aouedi"
      },
      {
        "name" : "Thai-Hoc Vu"
      },
      {
        "name" : "Alessio Sacco"
      },
      {
        "name" : "Dinh C. Nguyen"
      },
      {
        "name" : "Kandaraj Piamrat"
      },
      {
        "name" : "Guido Marchetto"
      },
      {
        "name" : "Quoc-Viet Pham"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.AI",
      "cs.CR",
      "cs.ET",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.16826v1",
    "title" : "Practical privacy metrics for synthetic data",
    "summary" : "This paper explains how the synthpop package for R has been extended to\ninclude functions to calculate measures of identity and attribute disclosure\nrisk for synthetic data that measure risks for the records used to create the\nsynthetic data. The basic function, disclosure, calculates identity disclosure\nfor a set of quasi-identifiers (keys) and attribute disclosure for one variable\nspecified as a target from the same set of keys. The second function,\ndisclosure.summary, is a wrapper for the first and presents summary results for\na set of targets. This short paper explains the measures of disclosure risk and\ndocuments how they are calculated. We recommend two measures: $RepU$\n(replicated uniques) for identity disclosure and $DiSCO$ (Disclosive in\nSynthetic Correct Original) for attribute disclosure. Both are expressed a \\%\nof the original records and each can be compared to similar measures calculated\nfrom the original data. Experience with using the functions on real data found\nthat some apparent disclosures could be identified as coming from relationships\nin the data that would be expected to be known to anyone familiar with its\nfeatures. We flag cases when this seems to have occurred and provide means of\nexcluding them.",
    "updated" : "2024-06-24T17:35:44Z",
    "published" : "2024-06-24T17:35:44Z",
    "authors" : [
      {
        "name" : "Gillian M Raab"
      },
      {
        "name" : "Beata Nowok"
      },
      {
        "name" : "Chris Dibben"
      }
    ],
    "categories" : [
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.16456v1",
    "title" : "Automated Privacy-Preserving Techniques via Meta-Learning",
    "summary" : "Sharing private data for learning tasks is pivotal for transparent and secure\nmachine learning applications. Many privacy-preserving techniques have been\nproposed for this task aiming to transform the data while ensuring the privacy\nof individuals. Some of these techniques have been incorporated into tools,\nwhereas others are accessed through various online platforms. However, such\ntools require manual configuration, which can be complex and time-consuming.\nMoreover, they require substantial expertise, potentially restricting their use\nto those with advanced technical knowledge. In this paper, we propose AUTOPRIV,\nthe first automated privacy-preservation method, that eliminates the need for\nany manual configuration. AUTOPRIV employs meta-learning to automate the\nde-identification process, facilitating the secure release of data for machine\nlearning tasks. The main goal is to anticipate the predictive performance and\nprivacy risk of a large set of privacy configurations. We provide a ranked list\nof the most promising solutions, which are likely to achieve an optimal\napproximation within a new domain. AUTOPRIV is highly effective as it reduces\ncomputational complexity and energy consumption considerably.",
    "updated" : "2024-06-24T08:53:45Z",
    "published" : "2024-06-24T08:53:45Z",
    "authors" : [
      {
        "name" : "Tânia Carvalho"
      },
      {
        "name" : "Nuno Moniz"
      },
      {
        "name" : "Luís Antunes"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.16313v1",
    "title" : "Thinking Inside The Box: Privacy Against Stronger Adversaries",
    "summary" : "In this thesis, we study extensions of statistical cryptographic primitives.\nIn particular we study leakage-resilient secret sharing, non-malleable\nextractors, and immunized ideal one-way functions. The thesis is divided into\nthree main chapters. In the first chapter, we show that 2-out-of-2 leakage\nresilient (and also non-malleable) secret sharing requires randomness sources\nthat are also extractable. This rules out the possibility of using min-entropic\nsources. In the second, we introduce collision-resistant seeded extractors and\nshow that any seeded extractor can be made collision resistant at a small\noverhead in seed length. We then use it to give a two-source non-malleable\nextractor with entropy rate 0.81 in one source and polylogarithmic in the\nother. The non-malleable extractor lead to the first statistical privacy\namplification protocol against memory tampering adversaries. In the final\nchapter, we study the hardness of the data structure variant of the 3SUM\nproblem which is motivated by a recent construction to immunise random oracles\nagainst pre-processing adversaries. We give worst-case data structure hardness\nfor the 3SUM problem matching known barriers in data structures for adaptive\nadversaries. We also give a slightly stronger lower bound in the case of\nnon-adaptivity. Lastly, we give a novel result in the bit-probe setting.",
    "updated" : "2024-06-24T04:40:28Z",
    "published" : "2024-06-24T04:40:28Z",
    "authors" : [
      {
        "name" : "Eldon Chung"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.16305v1",
    "title" : "On Computing Pairwise Statistics with Local Differential Privacy",
    "summary" : "We study the problem of computing pairwise statistics, i.e., ones of the form\n$\\binom{n}{2}^{-1} \\sum_{i \\ne j} f(x_i, x_j)$, where $x_i$ denotes the input\nto the $i$th user, with differential privacy (DP) in the local model. This\nformulation captures important metrics such as Kendall's $\\tau$ coefficient,\nArea Under Curve, Gini's mean difference, Gini's entropy, etc. We give several\nnovel and generic algorithms for the problem, leveraging techniques from DP\nalgorithms for linear queries.",
    "updated" : "2024-06-24T04:06:09Z",
    "published" : "2024-06-24T04:06:09Z",
    "authors" : [
      {
        "name" : "Badih Ghazi"
      },
      {
        "name" : "Pritish Kamath"
      },
      {
        "name" : "Ravi Kumar"
      },
      {
        "name" : "Pasin Manurangsi"
      },
      {
        "name" : "Adam Sealfon"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.16182v1",
    "title" : "Privacy-Preserving and Trustworthy Localization in an IoT Environment",
    "summary" : "The Internet of Things (IoT) is increasingly prevalent in various\napplications, such as healthcare and logistics. One significant service of IoT\ntechnologies that is essential for these applications is localization. The goal\nof this service is to determine the precise position of a specific target. The\nlocalization data often needs to be private, accessible only to specific\nentities, and must maintain authenticity and integrity to ensure\ntrustworthiness. IoT technology has evolved significantly, with Ultra-Wide Band\n(UWB) technology enhancing localization speed and precision. However, IoT\ndevice security remains a concern, as devices can be compromised or act\nmaliciously. Furthermore, localization data is typically stored centrally,\nwhich can also be a point of vulnerability. Our approach leverages the features\nof a permissioned blockchain, specifically Hyperledger Fabric, to address these\nchallenges. Hyperledger Fabric's collection feature ensures data privacy, and\nits smart contracts (chaincode) enhance trustworthiness. We tested our solution\nusing a network of devices known as CLOVES, demonstrating robust performance\ncharacteristics with UWB technology. Additionally, we evaluated our approach\nthrough an indoor localization use case.",
    "updated" : "2024-06-23T18:13:53Z",
    "published" : "2024-06-23T18:13:53Z",
    "authors" : [
      {
        "name" : "Guglielmo Zocca"
      },
      {
        "name" : "Omar Hasan"
      }
    ],
    "categories" : [
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.15962v1",
    "title" : "Privacy Preserving Machine Learning for Electronic Health Records using\n  Federated Learning and Differential Privacy",
    "summary" : "An Electronic Health Record (EHR) is an electronic database used by\nhealthcare providers to store patients' medical records which may include\ndiagnoses, treatments, costs, and other personal information. Machine learning\n(ML) algorithms can be used to extract and analyze patient data to improve\npatient care. Patient records contain highly sensitive information, such as\nsocial security numbers (SSNs) and residential addresses, which introduces a\nneed to apply privacy-preserving techniques for these ML models using federated\nlearning and differential privacy.",
    "updated" : "2024-06-23T00:01:03Z",
    "published" : "2024-06-23T00:01:03Z",
    "authors" : [
      {
        "name" : "Naif A. Ganadily"
      },
      {
        "name" : "Han J. Xia"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.15842v1",
    "title" : "Privacy Requirements and Realities of Digital Public Goods",
    "summary" : "In the international development community, the term \"digital public goods\"\nis used to describe open-source digital products (e.g., software, datasets)\nthat aim to address the United Nations (UN) Sustainable Development Goals. DPGs\nare increasingly being used to deliver government services around the world\n(e.g., ID management, healthcare registration). Because DPGs may handle\nsensitive data, the UN has established user privacy as a first-order\nrequirement for DPGs. The privacy risks of DPGs are currently managed in part\nby the DPG standard, which includes a prerequisite questionnaire with questions\ndesigned to evaluate a DPG's privacy posture.\n  This study examines the effectiveness of the current DPG standard for\nensuring adequate privacy protections. We present a systematic assessment of\nresponses from DPGs regarding their protections of users' privacy. We also\npresent in-depth case studies from three widely-used DPGs to identify privacy\nthreats and compare this to their responses to the DPG standard. Our findings\nreveal limitations in the current DPG standard's evaluation approach. We\nconclude by presenting preliminary recommendations and suggestions for\nstrengthening the DPG standard as it relates to privacy. Additionally, we hope\nthis study encourages more usable privacy research on communicating privacy,\nnot only to end users but also third-party adopters of user-facing\ntechnologies.",
    "updated" : "2024-06-22T13:06:13Z",
    "published" : "2024-06-22T13:06:13Z",
    "authors" : [
      {
        "name" : "Geetika Gopi"
      },
      {
        "name" : "Aadyaa Maddi"
      },
      {
        "name" : "Omkhar Arasaratnam"
      },
      {
        "name" : "Giulia Fanti"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.15789v1",
    "title" : "Privacy Implications of Explainable AI in Data-Driven Systems",
    "summary" : "Machine learning (ML) models, demonstrably powerful, suffer from a lack of\ninterpretability. The absence of transparency, often referred to as the black\nbox nature of ML models, undermines trust and urges the need for efforts to\nenhance their explainability. Explainable AI (XAI) techniques address this\nchallenge by providing frameworks and methods to explain the internal\ndecision-making processes of these complex models. Techniques like\nCounterfactual Explanations (CF) and Feature Importance play a crucial role in\nachieving this goal. Furthermore, high-quality and diverse data remains the\nfoundational element for robust and trustworthy ML applications. In many\napplications, the data used to train ML and XAI explainers contain sensitive\ninformation. In this context, numerous privacy-preserving techniques can be\nemployed to safeguard sensitive information in the data, such as differential\nprivacy. Subsequently, a conflict between XAI and privacy solutions emerges due\nto their opposing goals. Since XAI techniques provide reasoning for the model\nbehavior, they reveal information relative to ML models, such as their decision\nboundaries, the values of features, or the gradients of deep learning models\nwhen explanations are exposed to a third entity. Attackers can initiate privacy\nbreaching attacks using these explanations, to perform model extraction,\ninference, and membership attacks. This dilemma underscores the challenge of\nfinding the right equilibrium between understanding ML decision-making and\nsafeguarding privacy.",
    "updated" : "2024-06-22T08:51:58Z",
    "published" : "2024-06-22T08:51:58Z",
    "authors" : [
      {
        "name" : "Fatima Ezzeddine"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.15655v1",
    "title" : "ProBE: Proportioning Privacy Budget for Complex Exploratory Decision\n  Support",
    "summary" : "This paper studies privacy in the context of complex decision support queries\ncomposed of multiple conditions on different aggregate statistics combined\nusing disjunction and conjunction operators. Utility requirements for such\nqueries necessitate the need for private mechanisms that guarantee a bound on\nthe false negative and false positive errors. This paper formally defines\ncomplex decision support queries and their accuracy requirements, and provides\nalgorithms that proportion the existing budget to optimally minimize privacy\nloss while supporting a bounded guarantee on the accuracy. Our experimental\nresults on multiple real-life datasets show that our algorithms successfully\nmaintain such utility guarantees, while also minimizing privacy loss.",
    "updated" : "2024-06-21T21:20:57Z",
    "published" : "2024-06-21T21:20:57Z",
    "authors" : [
      {
        "name" : "Nada Lahjouji"
      },
      {
        "name" : "Sameera Ghayyur"
      },
      {
        "name" : "Xi He"
      },
      {
        "name" : "Sharad Mehrotra"
      }
    ],
    "categories" : [
      "cs.DB"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.17649v1",
    "title" : "Privacy Preserving Reinforcement Learning for Population Processes",
    "summary" : "We consider the problem of privacy protection in Reinforcement Learning (RL)\nalgorithms that operate over population processes, a practical but understudied\nsetting that includes, for example, the control of epidemics in large\npopulations of dynamically interacting individuals. In this setting, the RL\nalgorithm interacts with the population over $T$ time steps by receiving\npopulation-level statistics as state and performing actions which can affect\nthe entire population at each time step. An individual's data can be collected\nacross multiple interactions and their privacy must be protected at all times.\nWe clarify the Bayesian semantics of Differential Privacy (DP) in the presence\nof correlated data in population processes through a Pufferfish Privacy\nanalysis. We then give a meta algorithm that can take any RL algorithm as input\nand make it differentially private. This is achieved by taking an approach that\nuses DP mechanisms to privatize the state and reward signal at each time step\nbefore the RL algorithm receives them as input. Our main theoretical result\nshows that the value-function approximation error when applying standard RL\nalgorithms directly to the privatized states shrinks quickly as the population\nsize and privacy budget increase. This highlights that reasonable\nprivacy-utility trade-offs are possible for differentially private RL\nalgorithms in population processes. Our theoretical findings are validated by\nexperiments performed on a simulated epidemic control problem over large\npopulation sizes.",
    "updated" : "2024-06-25T15:41:26Z",
    "published" : "2024-06-25T15:41:26Z",
    "authors" : [
      {
        "name" : "Samuel Yang-Zhao"
      },
      {
        "name" : "Kee Siong Ng"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.18491v1",
    "title" : "Enhancing Federated Learning with Adaptive Differential Privacy and\n  Priority-Based Aggregation",
    "summary" : "Federated learning (FL), a novel branch of distributed machine learning (ML),\ndevelops global models through a private procedure without direct access to\nlocal datasets. However, it is still possible to access the model updates\n(gradient updates of deep neural networks) transferred between clients and\nservers, potentially revealing sensitive local information to adversaries using\nmodel inversion attacks. Differential privacy (DP) offers a promising approach\nto addressing this issue by adding noise to the parameters. On the other hand,\nheterogeneities in data structure, storage, communication, and computational\ncapabilities of devices can cause convergence problems and delays in developing\nthe global model. A personalized weighted averaging of local parameters based\non the resources of each device can yield a better aggregated model in each\nround. In this paper, to efficiently preserve privacy, we propose a\npersonalized DP framework that injects noise based on clients' relative impact\nfactors and aggregates parameters while considering heterogeneities and\nadjusting properties. To fulfill the DP requirements, we first analyze the\nconvergence boundary of the FL algorithm when impact factors are personalized\nand fixed throughout the learning process. We then further study the\nconvergence property considering time-varying (adaptive) impact factors.",
    "updated" : "2024-06-26T16:55:07Z",
    "published" : "2024-06-26T16:55:07Z",
    "authors" : [
      {
        "name" : "Mahtab Talaei"
      },
      {
        "name" : "Iman Izadi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.18221v1",
    "title" : "Enhancing Data Privacy in Large Language Models through Private\n  Association Editing",
    "summary" : "Large Language Models (LLMs) are powerful tools with extensive applications,\nbut their tendency to memorize private information raises significant concerns\nas private data leakage can easily happen. In this paper, we introduce Private\nAssociation Editing (PAE), a novel defense approach for private data leakage.\nPAE is designed to effectively remove Personally Identifiable Information (PII)\nwithout retraining the model. Our approach consists of a four-step procedure:\ndetecting memorized PII, applying PAE cards to mitigate memorization of private\ndata, verifying resilience to targeted data extraction (TDE) attacks, and\nensuring consistency in the post-edit LLMs. The versatility and efficiency of\nPAE, which allows for batch modifications, significantly enhance data privacy\nin LLMs. Experimental results demonstrate the effectiveness of PAE in\nmitigating private data leakage. We believe PAE will serve as a critical tool\nin the ongoing effort to protect data privacy in LLMs, encouraging the\ndevelopment of safer models for real-world applications.",
    "updated" : "2024-06-26T10:08:47Z",
    "published" : "2024-06-26T10:08:47Z",
    "authors" : [
      {
        "name" : "Davide Venditti"
      },
      {
        "name" : "Elena Sofia Ruzzetti"
      },
      {
        "name" : "Giancarlo A. Xompero"
      },
      {
        "name" : "Cristina Giannone"
      },
      {
        "name" : "Andrea Favalli"
      },
      {
        "name" : "Raniero Romagnoli"
      },
      {
        "name" : "Fabio Massimo Zanzotto"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.18100v1",
    "title" : "Natural Language but Omitted? On the Ineffectiveness of Large Language\n  Models' privacy policy from End-users' Perspective",
    "summary" : "LLMs driven products were increasingly prevalent in our daily lives, With a\nnatural language based interaction style, people may potentially leak their\npersonal private information. Thus, privacy policy and user agreement played an\nimportant role in regulating and alerting people. However, there lacked the\nwork examining the reading of LLM's privacy policy. Thus, we conducted the\nfirst user study to let participants read the privacy policy and user agreement\nwith two different styles (a cursory and detailed style). We found users lack\nimportant information upon cursory reading and even detailed reading. Besides,\ntheir privacy concerns was not solved even upon detailed reading. We provided\nfour design implications based on the findings.",
    "updated" : "2024-06-26T06:31:43Z",
    "published" : "2024-06-26T06:31:43Z",
    "authors" : [
      {
        "name" : "Shuning Zhang"
      },
      {
        "name" : "Haobin Xing"
      },
      {
        "name" : "Xin Yi"
      },
      {
        "name" : "Hewu Li"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.19035v1",
    "title" : "SD-BLS: Privacy Preserving Selective Disclosure and Unlinkable\n  Revocation of Verifiable Credentials",
    "summary" : "It is of critical importance to design digital identity systems that ensure\nthe privacy of citizens as well as protecting them from issuer corruption.\nUnfortunately, what Europe's and USA's public sectors are currently developing\ndoes not offer such basic protections. We aim to solve this issue and propose a\nmethod for untraceable selective disclosure and privacy preserving revocation\nof digital credentials, using the unique homomorphic characteristics of second\norder Elliptic Curves and Boneh-Lynn-Shacham (BLS) signatures. Our approach\nensures that users can selectively reveal only the necessary credentials, while\nprotecting their privacy across multiple presentations. We also aim to protect\nusers from issuer corruption, by making it possible to apply a threshold for\nrevocation to require collective agreement among multiple revocation issuers.",
    "updated" : "2024-06-27T09:41:13Z",
    "published" : "2024-06-27T09:41:13Z",
    "authors" : [
      {
        "name" : "Denis Roio"
      },
      {
        "name" : "Rebecca Selvaggini"
      },
      {
        "name" : "Andrea D'Intino"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.18940v1",
    "title" : "Efficient Verifiable Differential Privacy with Input Authenticity in the\n  Local and Shuffle Model",
    "summary" : "Local differential privacy (LDP) is an efficient solution for providing\nprivacy to client's sensitive data while simultaneously releasing aggregate\nstatistics without relying on a trusted central server (aggregator) as in the\ncentral model of differential privacy. The shuffle model with LDP provides an\nadditional layer of privacy, by disconnecting the link between clients and the\naggregator, further improving the utility of LDP. However, LDP has been shown\nto be vulnerable to malicious clients who can perform both input and output\nmanipulation attacks, i.e., before and after applying the LDP mechanism, to\nskew the aggregator's results. In this work, we show how to prevent malicious\nclients from compromising LDP schemes. Specifically, we give efficient\nconstructions to prevent both input \\'and output manipulation attacks from\nmalicious clients for generic LDP algorithms. Our proposed schemes for\nverifiable LDP (VLDP), completely protect from output manipulation attacks, and\nprevent input attacks using signed data, requiring only one-time interaction\nbetween client and server, unlike existing alternatives [28, 33]. Most\nimportantly, we are the first to provide an efficient scheme for VLDP in the\nshuffle model. We describe and prove secure, two schemes for VLDP in the\nregular model, and one in the shuffle model. We show that all schemes are\nhighly practical, with client runtimes of < 2 seconds, and server runtimes of\n5-7 milliseconds per client.",
    "updated" : "2024-06-27T07:12:28Z",
    "published" : "2024-06-27T07:12:28Z",
    "authors" : [
      {
        "name" : "Tariq Bontekoe"
      },
      {
        "name" : "Hassan Jameel Asghar"
      },
      {
        "name" : "Fatih Turkmen"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.18812v1",
    "title" : "A Survey on Privacy Attacks Against Digital Twin Systems in AI-Robotics",
    "summary" : "Industry 4.0 has witnessed the rise of complex robots fueled by the\nintegration of Artificial Intelligence/Machine Learning (AI/ML) and Digital\nTwin (DT) technologies. While these technologies offer numerous benefits, they\nalso introduce potential privacy and security risks. This paper surveys privacy\nattacks targeting robots enabled by AI and DT models. Exfiltration and data\nleakage of ML models are discussed in addition to the potential extraction of\nmodels derived from first-principles (e.g., physics-based). We also discuss\ndesign considerations with DT-integrated robotics touching on the impact of ML\nmodel training, responsible AI and DT safeguards, data governance and ethical\nconsiderations on the effectiveness of these attacks. We advocate for a trusted\nautonomy approach, emphasizing the need to combine robotics, AI, and DT\ntechnologies with robust ethical frameworks and trustworthiness principles for\nsecure and reliable AI robotic systems.",
    "updated" : "2024-06-27T00:59:20Z",
    "published" : "2024-06-27T00:59:20Z",
    "authors" : [
      {
        "name" : "Ivan A. Fernandez"
      },
      {
        "name" : "Subash Neupane"
      },
      {
        "name" : "Trisha Chakraborty"
      },
      {
        "name" : "Shaswata Mitra"
      },
      {
        "name" : "Sudip Mittal"
      },
      {
        "name" : "Nisha Pillai"
      },
      {
        "name" : "Jingdao Chen"
      },
      {
        "name" : "Shahram Rahimi"
      }
    ],
    "categories" : [
      "cs.RO",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.18731v1",
    "title" : "WavRx: a Disease-Agnostic, Generalizable, and Privacy-Preserving Speech\n  Health Diagnostic Model",
    "summary" : "Speech is known to carry health-related attributes, which has emerged as a\nnovel venue for remote and long-term health monitoring. However, existing\nmodels are usually tailored for a specific type of disease, and have been shown\nto lack generalizability across datasets. Furthermore, concerns have been\nraised recently towards the leakage of speaker identity from health embeddings.\nTo mitigate these limitations, we propose WavRx, a speech health diagnostics\nmodel that captures the respiration and articulation related dynamics from a\nuniversal speech representation. Our in-domain and cross-domain experiments on\nsix pathological speech datasets demonstrate WavRx as a new state-of-the-art\nhealth diagnostic model. Furthermore, we show that the amount of speaker\nidentity entailed in the WavRx health embeddings is significantly reduced\nwithout extra guidance during training. An in-depth analysis of the model was\nperformed, thus providing physiological interpretation of its improved\ngeneralizability and privacy-preserving ability.",
    "updated" : "2024-06-26T19:59:21Z",
    "published" : "2024-06-26T19:59:21Z",
    "authors" : [
      {
        "name" : "Yi Zhu"
      },
      {
        "name" : "Tiago Falk"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ]
  }
]