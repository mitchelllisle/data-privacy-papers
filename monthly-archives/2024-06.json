[
  {
    "id" : "http://arxiv.org/abs/2406.02520v1",
    "title" : "Digital Privacy for Migrants: Exploring Current Research Trends and\n  Future Prospects",
    "summary" : "This paper explores digital privacy challenges for migrants, analyzing trends\nfrom 2013 to 2023. Migrants face heightened risks such as government\nsurveillance and identity theft. Understanding these threats is vital for\nraising awareness and guiding research towards effective solutions and policies\nto protect migrant digital privacy.",
    "updated" : "2024-06-04T17:41:20Z",
    "published" : "2024-06-04T17:41:20Z",
    "authors" : [
      {
        "name" : "Sarah Tabassum"
      },
      {
        "name" : "Cori Faklaris"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02463v1",
    "title" : "Click Without Compromise: Online Advertising Measurement via Per User\n  Differential Privacy",
    "summary" : "Online advertising is a cornerstone of the Internet ecosystem, with\nadvertising measurement playing a crucial role in optimizing efficiency. Ad\nmeasurement entails attributing desired behaviors, such as purchases, to ad\nexposures across various platforms, necessitating the collection of user\nactivities across these platforms. As this practice faces increasing\nrestrictions due to rising privacy concerns, safeguarding user privacy in this\ncontext is imperative. Our work is the first to formulate the real-world\nchallenge of advertising measurement systems with real-time reporting of\nstreaming data in advertising campaigns. We introduce Ads-BPC, a novel\nuser-level differential privacy protection scheme for advertising measurement\nresults. This approach optimizes global noise power and results in a\nnon-identically distributed noise distribution that preserves differential\nprivacy while enhancing measurement accuracy. Through experiments on both\nreal-world advertising campaigns and synthetic datasets, Ads-BPC achieves a 25%\nto 50% increase in accuracy over existing streaming DP mechanisms applied to\nadvertising measurement. This highlights our method's effectiveness in\nachieving superior accuracy alongside a formal privacy guarantee, thereby\nadvancing the state-of-the-art in privacy-preserving advertising measurement.",
    "updated" : "2024-06-04T16:31:19Z",
    "published" : "2024-06-04T16:31:19Z",
    "authors" : [
      {
        "name" : "Yingtai Xiao"
      },
      {
        "name" : "Jian Du"
      },
      {
        "name" : "Shikun Zhang"
      },
      {
        "name" : "Qiang Yan"
      },
      {
        "name" : "Danfeng Zhang"
      },
      {
        "name" : "Daniel Kifer"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02424v1",
    "title" : "Contextual Dynamic Pricing: Algorithms, Optimality, and Local\n  Differential Privacy Constraints",
    "summary" : "We study the contextual dynamic pricing problem where a firm sells products\nto $T$ sequentially arriving consumers that behave according to an unknown\ndemand model. The firm aims to maximize its revenue, i.e. minimize its regret\nover a clairvoyant that knows the model in advance. The demand model is a\ngeneralized linear model (GLM), allowing for a stochastic feature vector in\n$\\mathbb R^d$ that encodes product and consumer information. We first show that\nthe optimal regret upper bound is of order $\\sqrt{dT}$, up to a logarithmic\nfactor, improving upon existing upper bounds in the literature by a $\\sqrt{d}$\nfactor. This sharper rate is materialised by two algorithms: a confidence\nbound-type (supCB) algorithm and an explore-then-commit (ETC) algorithm. A key\ninsight of our theoretical result is an intrinsic connection between dynamic\npricing and the contextual multi-armed bandit problem with many arms based on a\ncareful discretization. We further study contextual dynamic pricing under the\nlocal differential privacy (LDP) constraints. In particular, we propose a\nstochastic gradient descent based ETC algorithm that achieves an optimal regret\nupper bound of order $d\\sqrt{T}/\\epsilon$, up to a logarithmic factor, where\n$\\epsilon>0$ is the privacy parameter. The regret upper bounds with and without\nLDP constraints are accompanied by newly constructed minimax lower bounds,\nwhich further characterize the cost of privacy. Extensive numerical experiments\nand a real data application on online lending are conducted to illustrate the\nefficiency and practical value of the proposed algorithms in dynamic pricing.",
    "updated" : "2024-06-04T15:44:10Z",
    "published" : "2024-06-04T15:44:10Z",
    "authors" : [
      {
        "name" : "Zifeng Zhao"
      },
      {
        "name" : "Feiyu Jiang"
      },
      {
        "name" : "Yi Yu"
      }
    ],
    "categories" : [
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01811v1",
    "title" : "A Game-Theoretic Approach to Privacy-Utility Tradeoff in Sharing Genomic\n  Summary Statistics",
    "summary" : "The advent of online genomic data-sharing services has sought to enhance the\naccessibility of large genomic datasets by allowing queries about genetic\nvariants, such as summary statistics, aiding care providers in distinguishing\nbetween spurious genomic variations and those with clinical significance.\nHowever, numerous studies have demonstrated that even sharing summary genomic\ninformation exposes individual members of such datasets to a significant\nprivacy risk due to membership inference attacks. While several approaches have\nemerged that reduce privacy risks by adding noise or reducing the amount of\ninformation shared, these typically assume non-adaptive attacks that use\nlikelihood ratio test (LRT) statistics. We propose a Bayesian game-theoretic\nframework for optimal privacy-utility tradeoff in the sharing of genomic\nsummary statistics. Our first contribution is to prove that a very general\nBayesian attacker model that anchors our game-theoretic approach is more\npowerful than the conventional LRT-based threat models in that it induces worse\nprivacy loss for the defender who is modeled as a von Neumann-Morgenstern (vNM)\ndecision-maker. We show this to be true even when the attacker uses a\nnon-informative subjective prior. Next, we present an analytically tractable\napproach to compare the Bayesian attacks with arbitrary subjective priors and\nthe Neyman-Pearson optimal LRT attacks under the Gaussian mechanism common in\ndifferential privacy frameworks. Finally, we propose an approach for\napproximating Bayes-Nash equilibria of the game using deep neural network\ngenerators to implicitly represent player mixed strategies. Our experiments\ndemonstrate that the proposed game-theoretic framework yields both stronger\nattacks and stronger defense strategies than the state of the art.",
    "updated" : "2024-06-03T22:09:47Z",
    "published" : "2024-06-03T22:09:47Z",
    "authors" : [
      {
        "name" : "Tao Zhang"
      },
      {
        "name" : "Rajagopal Venkatesaramani"
      },
      {
        "name" : "Rajat K. De"
      },
      {
        "name" : "Bradley A. Malin"
      },
      {
        "name" : "Yevgeniy Vorobeychik"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01394v1",
    "title" : "PrivacyRestore: Privacy-Preserving Inference in Large Language Models\n  via Privacy Removal and Restoration",
    "summary" : "The widespread usage of online Large Language Models (LLMs) inference\nservices has raised significant privacy concerns about the potential exposure\nof private information in user inputs to eavesdroppers or untrustworthy service\nproviders. Existing privacy protection methods for LLMs suffer from\ninsufficient privacy protection, performance degradation, or severe inference\ntime overhead. In this paper, we propose PrivacyRestore to protect the privacy\nof user inputs during LLM inference. PrivacyRestore directly removes privacy\nspans in user inputs and restores privacy information via activation steering\nduring inference. The privacy spans are encoded as restoration vectors. We\npropose Attention-aware Weighted Aggregation (AWA) which aggregates restoration\nvectors of all privacy spans in the input into a meta restoration vector. AWA\nnot only ensures proper representation of all privacy spans but also prevents\nattackers from inferring the privacy spans from the meta restoration vector\nalone. This meta restoration vector, along with the query with privacy spans\nremoved, is then sent to the server. The experimental results show that\nPrivacyRestore can protect private information while maintaining acceptable\nlevels of performance and inference efficiency.",
    "updated" : "2024-06-03T14:57:39Z",
    "published" : "2024-06-03T14:57:39Z",
    "authors" : [
      {
        "name" : "Ziqian Zeng"
      },
      {
        "name" : "Jianwei Wang"
      },
      {
        "name" : "Zhengdong Lu"
      },
      {
        "name" : "Huiping Zhuang"
      },
      {
        "name" : "Cen Chen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01363v1",
    "title" : "Privacy in LLM-based Recommendation: Recent Advances and Future\n  Directions",
    "summary" : "Nowadays, large language models (LLMs) have been integrated with conventional\nrecommendation models to improve recommendation performance. However, while\nmost of the existing works have focused on improving the model performance, the\nprivacy issue has only received comparatively less attention. In this paper, we\nreview recent advancements in privacy within LLM-based recommendation,\ncategorizing them into privacy attacks and protection mechanisms. Additionally,\nwe highlight several challenges and propose future directions for the community\nto address these critical problems.",
    "updated" : "2024-06-03T14:31:47Z",
    "published" : "2024-06-03T14:31:47Z",
    "authors" : [
      {
        "name" : "Sichun Luo"
      },
      {
        "name" : "Wei Shao"
      },
      {
        "name" : "Yuxuan Yao"
      },
      {
        "name" : "Jian Xu"
      },
      {
        "name" : "Mingyang Liu"
      },
      {
        "name" : "Qintong Li"
      },
      {
        "name" : "Bowei He"
      },
      {
        "name" : "Maolin Wang"
      },
      {
        "name" : "Guanzhi Deng"
      },
      {
        "name" : "Hanxu Hou"
      },
      {
        "name" : "Xinyi Zhang"
      },
      {
        "name" : "Linqi Song"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01085v1",
    "title" : "FedAdOb: Privacy-Preserving Federated Deep Learning with Adaptive\n  Obfuscation",
    "summary" : "Federated learning (FL) has emerged as a collaborative approach that allows\nmultiple clients to jointly learn a machine learning model without sharing\ntheir private data. The concern about privacy leakage, albeit demonstrated\nunder specific conditions, has triggered numerous follow-up research in\ndesigning powerful attacking methods and effective defending mechanisms aiming\nto thwart these attacking methods. Nevertheless, privacy-preserving mechanisms\nemployed in these defending methods invariably lead to compromised model\nperformances due to a fixed obfuscation applied to private data or gradients.\nIn this article, we, therefore, propose a novel adaptive obfuscation mechanism,\ncoined FedAdOb, to protect private data without yielding original model\nperformances. Technically, FedAdOb utilizes passport-based adaptive obfuscation\nto ensure data privacy in both horizontal and vertical federated learning\nsettings. The privacy-preserving capabilities of FedAdOb, specifically with\nregard to private features and labels, are theoretically proven through\nTheorems 1 and 2. Furthermore, extensive experimental evaluations conducted on\nvarious datasets and network architectures demonstrate the effectiveness of\nFedAdOb by manifesting its superior trade-off between privacy preservation and\nmodel performance, surpassing existing methods.",
    "updated" : "2024-06-03T08:12:09Z",
    "published" : "2024-06-03T08:12:09Z",
    "authors" : [
      {
        "name" : "Hanlin Gu"
      },
      {
        "name" : "Jiahuan Luo"
      },
      {
        "name" : "Yan Kang"
      },
      {
        "name" : "Yuan Yao"
      },
      {
        "name" : "Gongxi Zhu"
      },
      {
        "name" : "Bowen Li"
      },
      {
        "name" : "Lixin Fan"
      },
      {
        "name" : "Qiang Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01080v1",
    "title" : "No Vandalism: Privacy-Preserving and Byzantine-Robust Federated Learning",
    "summary" : "Federated learning allows several clients to train one machine learning model\njointly without sharing private data, providing privacy protection. However,\ntraditional federated learning is vulnerable to poisoning attacks, which can\nnot only decrease the model performance, but also implant malicious backdoors.\nIn addition, direct submission of local model parameters can also lead to the\nprivacy leakage of the training dataset. In this paper, we aim to build a\nprivacy-preserving and Byzantine-robust federated learning scheme to provide an\nenvironment with no vandalism (NoV) against attacks from malicious\nparticipants. Specifically, we construct a model filter for poisoned local\nmodels, protecting the global model from data and model poisoning attacks. This\nmodel filter combines zero-knowledge proofs to provide further privacy\nprotection. Then, we adopt secret sharing to provide verifiable secure\naggregation, removing malicious clients that disrupting the aggregation\nprocess. Our formal analysis proves that NoV can protect data privacy and weed\nout Byzantine attackers. Our experiments illustrate that NoV can effectively\naddress data and model poisoning attacks, including PGD, and outperforms other\nrelated schemes.",
    "updated" : "2024-06-03T07:59:10Z",
    "published" : "2024-06-03T07:59:10Z",
    "authors" : [
      {
        "name" : "Zhibo Xing"
      },
      {
        "name" : "Zijian Zhang"
      },
      {
        "name" : "Zi'ang Zhang"
      },
      {
        "name" : "Jiamou Liu"
      },
      {
        "name" : "Liehuang Zhu"
      },
      {
        "name" : "Giovanni Russello"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00966v1",
    "title" : "Guaranteeing Data Privacy in Federated Unlearning with Dynamic User\n  Participation",
    "summary" : "Federated Unlearning (FU) is gaining prominence for its capacity to eliminate\ninfluences of Federated Learning (FL) users' data from trained global FL\nmodels. A straightforward FU method involves removing the unlearned users and\nsubsequently retraining a new global FL model from scratch with all remaining\nusers, a process that leads to considerable overhead. To enhance unlearning\nefficiency, a widely adopted strategy employs clustering, dividing FL users\ninto clusters, with each cluster maintaining its own FL model. The final\ninference is then determined by aggregating the majority vote from the\ninferences of these sub-models. This method confines unlearning processes to\nindividual clusters for removing a user, thereby enhancing unlearning\nefficiency by eliminating the need for participation from all remaining users.\nHowever, current clustering-based FU schemes mainly concentrate on refining\nclustering to boost unlearning efficiency but overlook the potential\ninformation leakage from FL users' gradients, a privacy concern that has been\nextensively studied. Typically, integrating secure aggregation (SecAgg) schemes\nwithin each cluster can facilitate a privacy-preserving FU. Nevertheless,\ncrafting a clustering methodology that seamlessly incorporates SecAgg schemes\nis challenging, particularly in scenarios involving adversarial users and\ndynamic users. In this connection, we systematically explore the integration of\nSecAgg protocols within the most widely used federated unlearning scheme, which\nis based on clustering, to establish a privacy-preserving FU framework, aimed\nat ensuring privacy while effectively managing dynamic user participation.\nComprehensive theoretical assessments and experimental results show that our\nproposed scheme achieves comparable unlearning effectiveness, alongside\noffering improved privacy protection and resilience in the face of varying user\nparticipation.",
    "updated" : "2024-06-03T03:39:07Z",
    "published" : "2024-06-03T03:39:07Z",
    "authors" : [
      {
        "name" : "Ziyao Liu"
      },
      {
        "name" : "Yu Jiang"
      },
      {
        "name" : "Weifeng Jiang"
      },
      {
        "name" : "Jiale Guo"
      },
      {
        "name" : "Jun Zhao"
      },
      {
        "name" : "Kwok-Yan Lam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00808v1",
    "title" : "EchoNet-Synthetic: Privacy-preserving Video Generation for Safe Medical\n  Data Sharing",
    "summary" : "To make medical datasets accessible without sharing sensitive patient\ninformation, we introduce a novel end-to-end approach for generative\nde-identification of dynamic medical imaging data. Until now, generative\nmethods have faced constraints in terms of fidelity, spatio-temporal coherence,\nand the length of generation, failing to capture the complete details of\ndataset distributions. We present a model designed to produce high-fidelity,\nlong and complete data samples with near-real-time efficiency and explore our\napproach on a challenging task: generating echocardiogram videos. We develop\nour generation method based on diffusion models and introduce a protocol for\nmedical video dataset anonymization. As an exemplar, we present\nEchoNet-Synthetic, a fully synthetic, privacy-compliant echocardiogram dataset\nwith paired ejection fraction labels. As part of our de-identification\nprotocol, we evaluate the quality of the generated dataset and propose to use\nclinical downstream tasks as a measurement on top of widely used but\npotentially biased image quality metrics. Experimental outcomes demonstrate\nthat EchoNet-Synthetic achieves comparable dataset fidelity to the actual\ndataset, effectively supporting the ejection fraction regression task. Code,\nweights and dataset are available at\nhttps://github.com/HReynaud/EchoNet-Synthetic.",
    "updated" : "2024-06-02T17:18:06Z",
    "published" : "2024-06-02T17:18:06Z",
    "authors" : [
      {
        "name" : "Hadrien Reynaud"
      },
      {
        "name" : "Qingjie Meng"
      },
      {
        "name" : "Mischa Dombrowski"
      },
      {
        "name" : "Arijit Ghosh"
      },
      {
        "name" : "Thomas Day"
      },
      {
        "name" : "Alberto Gomez"
      },
      {
        "name" : "Paul Leeson"
      },
      {
        "name" : "Bernhard Kainz"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00359v1",
    "title" : "Location Privacy in B5G/6G: Systematization of Knowledge",
    "summary" : "As we transition into the era of B5G/6G networks, the promise of seamless,\nhigh-speed connectivity brings unprecedented opportunities and challenges.\nAmong the most critical concerns is the preservation of location privacy, given\nthe enhanced precision and pervasive connectivity of these advanced networks.\nThis paper systematically reviews the state of knowledge on location privacy in\nB5G/6G networks, highlighting the architectural advancements and\ninfrastructural complexities that contribute to increased privacy risks. The\nurgency of studying these technologies is underscored by the rapid adoption of\nB5G/6G and the growing sophistication of location tracking methods. We evaluate\ncurrent and emerging privacy-preserving mechanisms, exploring the implications\nof sophisticated tracking methods and the challenges posed by the complex\nnetwork infrastructures. Our findings reveal the effectiveness of various\nmitigation strategies and emphasize the important role of physical layer\nsecurity. Additionally, we propose innovative approaches, including\ndecentralized authentication systems and the potential of satellite\ncommunications, to enhance location privacy. By addressing these challenges,\nthis paper provides a comprehensive perspective on preserving user privacy in\nthe rapidly evolving landscape of modern communication networks.",
    "updated" : "2024-06-01T08:25:07Z",
    "published" : "2024-06-01T08:25:07Z",
    "authors" : [
      {
        "name" : "Hannah B. Pasandi"
      },
      {
        "name" : "Faith Parastar"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00256v1",
    "title" : "Over-the-Air Collaborative Inference with Feature Differential Privacy",
    "summary" : "Collaborative inference in next-generation networks can enhance Artificial\nIntelligence (AI) applications, including autonomous driving, personal\nidentification, and activity classification. This method involves a three-stage\nprocess: a) data acquisition through sensing, b) feature extraction, and c)\nfeature encoding for transmission. Transmission of the extracted features\nentails the potential risk of exposing sensitive personal data. To address this\nissue, in this work a new privacy-protecting collaborative inference mechanism\nis developed. Under this mechanism, each edge device in the network protects\nthe privacy of extracted features before transmitting them to a central server\nfor inference. This mechanism aims to achieve two main objectives while\nensuring effective inference performance: 1) reducing communication overhead,\nand 2) maintaining strict privacy guarantees during features transmission.",
    "updated" : "2024-06-01T01:39:44Z",
    "published" : "2024-06-01T01:39:44Z",
    "authors" : [
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Yuqi Nie"
      },
      {
        "name" : "Andrea Goldsmith"
      },
      {
        "name" : "Vincent Poor"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00249v1",
    "title" : "Privacy Challenges in Meta-Learning: An Investigation on Model-Agnostic\n  Meta-Learning",
    "summary" : "Meta-learning involves multiple learners, each dedicated to specific tasks,\ncollaborating in a data-constrained setting. In current meta-learning methods,\ntask learners locally learn models from sensitive data, termed support sets.\nThese task learners subsequently share model-related information, such as\ngradients or loss values, which is computed using another part of the data\ntermed query set, with a meta-learner. The meta-learner employs this\ninformation to update its meta-knowledge. Despite the absence of explicit data\nsharing, privacy concerns persist. This paper examines potential data leakage\nin a prominent metalearning algorithm, specifically Model-Agnostic\nMeta-Learning (MAML). In MAML, gradients are shared between the metalearner and\ntask-learners. The primary objective is to scrutinize the gradient and the\ninformation it encompasses about the task dataset. Subsequently, we endeavor to\npropose membership inference attacks targeting the task dataset containing\nsupport and query sets. Finally, we explore various noise injection methods\ndesigned to safeguard the privacy of task data and thwart potential attacks.\nExperimental results demonstrate the effectiveness of these attacks on MAML and\nthe efficacy of proper noise injection methods in countering them.",
    "updated" : "2024-06-01T01:10:35Z",
    "published" : "2024-06-01T01:10:35Z",
    "authors" : [
      {
        "name" : "Mina Rafiei"
      },
      {
        "name" : "Mohammadmahdi Maheri"
      },
      {
        "name" : "Hamid R. Rabiee"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03187v1",
    "title" : "Ariadne: a Privacy-Preserving Communication Protocol",
    "summary" : "In this article, we present Ariadne, a privacy-preserving communication\nnetwork layer protocol that uses a source routing approach to avoid relying on\ntrusted third parties. In Ariadne, a source node willing to send anonymized\nnetwork traffic to a destination uses a path consisting in nodes with which it\nhas pre-shared symmetric keys. Temporary keys derived from those pre-shared\nkeys to protect communication privacy using onion routing techniques, ensuring\nsession unlinkability for packets following the same path.\n  Ariadne enhances previous approaches to preserve communication privacy by\nintroducing two novelties. First, the source route is encoded in a fixed size,\nsequentially encrypted vector of routing information elements, in which the\nelements' positions in the vector are pseudo-randomly permuted. Second, the\ntemporary keys used to process the packets on the path are referenced using\nmutually known encrypted patterns. This avoids the use of an explicit key\nreference that could be used to de-anonymize the communications.",
    "updated" : "2024-06-05T12:20:12Z",
    "published" : "2024-06-05T12:20:12Z",
    "authors" : [
      {
        "name" : "Antoine Fressancourt"
      },
      {
        "name" : "Luigi Iannone"
      },
      {
        "name" : "Mael Kerichard"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02797v1",
    "title" : "Auditing Privacy Mechanisms via Label Inference Attacks",
    "summary" : "We propose reconstruction advantage measures to audit label privatization\nmechanisms. A reconstruction advantage measure quantifies the increase in an\nattacker's ability to infer the true label of an unlabeled example when\nprovided with a private version of the labels in a dataset (e.g., aggregate of\nlabels from different users or noisy labels output by randomized response),\ncompared to an attacker that only observes the feature vectors, but may have\nprior knowledge of the correlation between features and labels. We consider two\nsuch auditing measures: one additive, and one multiplicative. These incorporate\nprevious approaches taken in the literature on empirical auditing and\ndifferential privacy. The measures allow us to place a variety of proposed\nprivatization schemes -- some differentially private, some not -- on the same\nfooting. We analyze these measures theoretically under a distributional model\nwhich encapsulates reasonable adversarial settings. We also quantify their\nbehavior empirically on real and simulated prediction tasks. Across a range of\nexperimental settings, we find that differentially private schemes dominate or\nmatch the privacy-utility tradeoff of more heuristic approaches.",
    "updated" : "2024-06-04T21:48:30Z",
    "published" : "2024-06-04T21:48:30Z",
    "authors" : [
      {
        "name" : "Róbert István Busa-Fekete"
      },
      {
        "name" : "Travis Dick"
      },
      {
        "name" : "Claudio Gentile"
      },
      {
        "name" : "Andrés Muñoz Medina"
      },
      {
        "name" : "Adam Smith"
      },
      {
        "name" : "Marika Swanberg"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02794v1",
    "title" : "PriME: Privacy-aware Membership profile Estimation in networks",
    "summary" : "This paper presents a novel approach to estimating community membership\nprobabilities for network vertices generated by the Degree Corrected Mixed\nMembership Stochastic Block Model while preserving individual edge privacy.\nOperating within the $\\varepsilon$-edge local differential privacy framework,\nwe introduce an optimal private algorithm based on a symmetric edge flip\nmechanism and spectral clustering for accurate estimation of vertex community\nmemberships. We conduct a comprehensive analysis of the estimation risk and\nestablish the optimality of our procedure by providing matching lower bounds to\nthe minimax risk under privacy constraints. To validate our approach, we\ndemonstrate its performance through numerical simulations and its practical\napplication to real-world data. This work represents a significant step forward\nin balancing accurate community membership estimation with stringent privacy\npreservation in network data analysis.",
    "updated" : "2024-06-04T21:43:49Z",
    "published" : "2024-06-04T21:43:49Z",
    "authors" : [
      {
        "name" : "Abhinav Chakraborty"
      },
      {
        "name" : "Sayak Chatterjee"
      },
      {
        "name" : "Sagnik Nandy"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.SI",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03404v1",
    "title" : "ST-DPGAN: A Privacy-preserving Framework for Spatiotemporal Data\n  Generation",
    "summary" : "Spatiotemporal data is prevalent in a wide range of edge devices, such as\nthose used in personal communication and financial transactions. Recent\nadvancements have sparked a growing interest in integrating spatiotemporal\nanalysis with large-scale language models. However, spatiotemporal data often\ncontains sensitive information, making it unsuitable for open third-party\naccess. To address this challenge, we propose a Graph-GAN-based model for\ngenerating privacy-protected spatiotemporal data. Our approach incorporates\nspatial and temporal attention blocks in the discriminator and a spatiotemporal\ndeconvolution structure in the generator. These enhancements enable efficient\ntraining under Gaussian noise to achieve differential privacy. Extensive\nexperiments conducted on three real-world spatiotemporal datasets validate the\nefficacy of our model. Our method provides a privacy guarantee while\nmaintaining the data utility. The prediction model trained on our generated\ndata maintains a competitive performance compared to the model trained on the\noriginal data.",
    "updated" : "2024-06-04T04:43:54Z",
    "published" : "2024-06-04T04:43:54Z",
    "authors" : [
      {
        "name" : "Wei Shao"
      },
      {
        "name" : "Rongyi Zhu"
      },
      {
        "name" : "Cai Yang"
      },
      {
        "name" : "Chandra Thapa"
      },
      {
        "name" : "Muhammad Ejaz Ahmed"
      },
      {
        "name" : "Seyit Camtepe"
      },
      {
        "name" : "Rui Zhang"
      },
      {
        "name" : "DuYong Kim"
      },
      {
        "name" : "Hamid Menouar"
      },
      {
        "name" : "Flora D. Salim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02629v1",
    "title" : "SSNet: A Lightweight Multi-Party Computation Scheme for Practical\n  Privacy-Preserving Machine Learning Service in the Cloud",
    "summary" : "As privacy-preserving becomes a pivotal aspect of deep learning (DL)\ndevelopment, multi-party computation (MPC) has gained prominence for its\nefficiency and strong security. However, the practice of current MPC frameworks\nis limited, especially when dealing with large neural networks, exemplified by\nthe prolonged execution time of 25.8 seconds for secure inference on\nResNet-152. The primary challenge lies in the reliance of current MPC\napproaches on additive secret sharing, which incurs significant communication\noverhead with non-linear operations such as comparisons. Furthermore, additive\nsharing suffers from poor scalability on party size. In contrast, the evolving\nlandscape of MPC necessitates accommodating a larger number of compute parties\nand ensuring robust performance against malicious activities or computational\nfailures.\n  In light of these challenges, we propose SSNet, which for the first time,\nemploys Shamir's secret sharing (SSS) as the backbone of MPC-based ML\nframework. We meticulously develop all framework primitives and operations for\nsecure DL models tailored to seamlessly integrate with the SSS scheme. SSNet\ndemonstrates the ability to scale up party numbers straightforwardly and embeds\nstrategies to authenticate the computation correctness without incurring\nsignificant performance overhead. Additionally, SSNet introduces masking\nstrategies designed to reduce communication overhead associated with non-linear\noperations. We conduct comprehensive experimental evaluations on commercial\ncloud computing infrastructure from Amazon AWS, as well as across diverse\nprevalent DNN models and datasets. SSNet demonstrates a substantial performance\nboost, achieving speed-ups ranging from 3x to 14x compared to SOTA MPC\nframeworks. Moreover, SSNet also represents the first framework that is\nevaluated on a five-party computation setup, in the context of secure DL\ninference.",
    "updated" : "2024-06-04T00:55:06Z",
    "published" : "2024-06-04T00:55:06Z",
    "authors" : [
      {
        "name" : "Shijin Duan"
      },
      {
        "name" : "Chenghong Wang"
      },
      {
        "name" : "Hongwu Peng"
      },
      {
        "name" : "Yukui Luo"
      },
      {
        "name" : "Wujie Wen"
      },
      {
        "name" : "Caiwen Ding"
      },
      {
        "name" : "Xiaolin Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.02599v1",
    "title" : "Privacy-Aware Randomized Quantization via Linear Programming",
    "summary" : "Differential privacy mechanisms such as the Gaussian or Laplace mechanism\nhave been widely used in data analytics for preserving individual privacy.\nHowever, they are mostly designed for continuous outputs and are unsuitable for\nscenarios where discrete values are necessary. Although various quantization\nmechanisms were proposed recently to generate discrete outputs under\ndifferential privacy, the outcomes are either biased or have an inferior\naccuracy-privacy trade-off. In this paper, we propose a family of quantization\nmechanisms that is unbiased and differentially private. It has a high degree of\nfreedom and we show that some existing mechanisms can be considered as special\ncases of ours. To find the optimal mechanism, we formulate a linear\noptimization that can be solved efficiently using linear programming tools.\nExperiments show that our proposed mechanism can attain a better\nprivacy-accuracy trade-off compared to baselines.",
    "updated" : "2024-06-01T18:40:08Z",
    "published" : "2024-06-01T18:40:08Z",
    "authors" : [
      {
        "name" : "Zhongteng Cai"
      },
      {
        "name" : "Xueru Zhang"
      },
      {
        "name" : "Mohammad Mahdi Khalili"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04129v1",
    "title" : "LenslessFace: An End-to-End Optimized Lensless System for\n  Privacy-Preserving Face Verification",
    "summary" : "Lensless cameras, innovatively replacing traditional lenses for ultra-thin,\nflat optics, encode light directly onto sensors, producing images that are not\nimmediately recognizable. This compact, lightweight, and cost-effective imaging\nsolution offers inherent privacy advantages, making it attractive for\nprivacy-sensitive applications like face verification. Typical lensless face\nverification adopts a two-stage process of reconstruction followed by\nverification, incurring privacy risks from reconstructed faces and high\ncomputational costs. This paper presents an end-to-end optimization approach\nfor privacy-preserving face verification directly on encoded lensless captures,\nensuring that the entire software pipeline remains encoded with no visible\nfaces as intermediate results. To achieve this, we propose several techniques\nto address unique challenges from the lensless setup which precludes\ntraditional face detection and alignment. Specifically, we propose a face\ncenter alignment scheme, an augmentation curriculum to build robustness against\nvariations, and a knowledge distillation method to smooth optimization and\nenhance performance. Evaluations under both simulation and real environment\ndemonstrate our method outperforms two-stage lensless verification while\nenhancing privacy and efficiency. Project website:\n\\url{lenslessface.github.io}.",
    "updated" : "2024-06-06T14:50:15Z",
    "published" : "2024-06-06T14:50:15Z",
    "authors" : [
      {
        "name" : "Xin Cai"
      },
      {
        "name" : "Hailong Zhang"
      },
      {
        "name" : "Chenchen Wang"
      },
      {
        "name" : "Wentao Liu"
      },
      {
        "name" : "Jinwei Gu"
      },
      {
        "name" : "Tianfan Xue"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03820v1",
    "title" : "A Survey on Intelligent Internet of Things: Applications, Security,\n  Privacy, and Future Directions",
    "summary" : "The rapid advances in the Internet of Things (IoT) have promoted a revolution\nin communication technology and offered various customer services. Artificial\nintelligence (AI) techniques have been exploited to facilitate IoT operations\nand maximize their potential in modern application scenarios. In particular,\nthe convergence of IoT and AI has led to a new networking paradigm called\nIntelligent IoT (IIoT), which has the potential to significantly transform\nbusinesses and industrial domains. This paper presents a comprehensive survey\nof IIoT by investigating its significant applications in mobile networks, as\nwell as its associated security and privacy issues. Specifically, we explore\nand discuss the roles of IIoT in a wide range of key application domains, from\nsmart healthcare and smart cities to smart transportation and smart industries.\nThrough such extensive discussions, we investigate important security issues in\nIIoT networks, where network attacks, confidentiality, integrity, and intrusion\nare analyzed, along with a discussion of potential countermeasures. Privacy\nissues in IIoT networks were also surveyed and discussed, including data,\nlocation, and model privacy leakage. Finally, we outline several key challenges\nand highlight potential research directions in this important area.",
    "updated" : "2024-06-06T07:55:30Z",
    "published" : "2024-06-06T07:55:30Z",
    "authors" : [
      {
        "name" : "Ons Aouedi"
      },
      {
        "name" : "Thai-Hoc Vu"
      },
      {
        "name" : "Alessio Sacco"
      },
      {
        "name" : "Dinh C. Nguyen"
      },
      {
        "name" : "Kandaraj Piamrat"
      },
      {
        "name" : "Guido Marchetto"
      },
      {
        "name" : "Quoc-Viet Pham"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.AI",
      "cs.CR",
      "cs.ET",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03802v1",
    "title" : "Continual Counting with Gradual Privacy Expiration",
    "summary" : "Differential privacy with gradual expiration models the setting where data\nitems arrive in a stream and at a given time $t$ the privacy loss guaranteed\nfor a data item seen at time $(t-d)$ is $\\epsilon g(d)$, where $g$ is a\nmonotonically non-decreasing function. We study the fundamental\n$\\textit{continual (binary) counting}$ problem where each data item consists of\na bit, and the algorithm needs to output at each time step the sum of all the\nbits streamed so far. For a stream of length $T$ and privacy $\\textit{without}$\nexpiration continual counting is possible with maximum (over all time steps)\nadditive error $O(\\log^2(T)/\\varepsilon)$ and the best known lower bound is\n$\\Omega(\\log(T)/\\varepsilon)$; closing this gap is a challenging open problem.\n  We show that the situation is very different for privacy with gradual\nexpiration by giving upper and lower bounds for a large set of expiration\nfunctions $g$. Specifically, our algorithm achieves an additive error of $\nO(\\log(T)/\\epsilon)$ for a large set of privacy expiration functions. We also\ngive a lower bound that shows that if $C$ is the additive error of any\n$\\epsilon$-DP algorithm for this problem, then the product of $C$ and the\nprivacy expiration function after $2C$ steps must be\n$\\Omega(\\log(T)/\\epsilon)$. Our algorithm matches this lower bound as its\nadditive error is $O(\\log(T)/\\epsilon)$, even when $g(2C) = O(1)$.\n  Our empirical evaluation shows that we achieve a slowly growing privacy loss\nwith significantly smaller empirical privacy loss for large values of $d$ than\na natural baseline algorithm.",
    "updated" : "2024-06-06T07:20:16Z",
    "published" : "2024-06-06T07:20:16Z",
    "authors" : [
      {
        "name" : "Joel Daniel Andersson"
      },
      {
        "name" : "Monika Henzinger"
      },
      {
        "name" : "Rasmus Pagh"
      },
      {
        "name" : "Teresa Anna Steiner"
      },
      {
        "name" : "Jalaj Upadhyay"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03785v1",
    "title" : "Count-mean Sketch as an Optimized Framework for Frequency Estimation\n  with Local Differential Privacy",
    "summary" : "This paper identifies that a group of state-of-the-art\nlocally-differentially-private (LDP) algorithms for frequency estimation are\nequivalent to the private Count-Mean Sketch (CMS) algorithm with different\nparameters. Therefore, we revisit the private CMS, correct errors in the\noriginal CMS paper regarding expectation and variance, modify the CMS\nimplementation to eliminate existing bias, and explore optimized parameters for\nCMS to achieve optimality in reducing the worst-case mean squared error (MSE),\n$l_1$ loss, and $l_2$ loss. Additionally, we prove that pairwise-independent\nhashing is sufficient for CMS, reducing its communication cost to the logarithm\nof the cardinality of all possible values (i.e., a dictionary). As a result,\nthe aforementioned optimized CMS is proven theoretically and empirically to be\nthe only algorithm optimized for reducing the worst-case MSE, $l_1$ loss, and\n$l_2$ loss when dealing with a very large dictionary. Furthermore, we\ndemonstrate that randomness is necessary to ensure the correctness of CMS, and\nthe communication cost of CMS, though low, is unavoidable despite the\nrandomness being public or private.",
    "updated" : "2024-06-06T06:55:08Z",
    "published" : "2024-06-06T06:55:08Z",
    "authors" : [
      {
        "name" : "Mingen Pan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03766v1",
    "title" : "Privacy Preserving Semi-Decentralized Mean Estimation over\n  Intermittently-Connected Networks",
    "summary" : "We consider the problem of privately estimating the mean of vectors\ndistributed across different nodes of an unreliable wireless network, where\ncommunications between nodes can fail intermittently. We adopt a\nsemi-decentralized setup, wherein to mitigate the impact of intermittently\nconnected links, nodes can collaborate with their neighbors to compute a local\nconsensus, which they relay to a central server. In such a setting, the\ncommunications between any pair of nodes must ensure that the privacy of the\nnodes is rigorously maintained to prevent unauthorized information leakage. We\nstudy the tradeoff between collaborative relaying and privacy leakage due to\nthe data sharing among nodes and, subsequently, propose PriCER: Private\nCollaborative Estimation via Relaying -- a differentially private collaborative\nalgorithm for mean estimation to optimize this tradeoff. The privacy guarantees\nof PriCER arise (i) implicitly, by exploiting the inherent stochasticity of the\nflaky network connections, and (ii) explicitly, by adding Gaussian\nperturbations to the estimates exchanged by the nodes. Local and central\nprivacy guarantees are provided against eavesdroppers who can observe different\nsignals, such as the communications amongst nodes during local consensus and\n(possibly multiple) transmissions from the relays to the central server. We\nsubstantiate our theoretical findings with numerical simulations. Our\nimplementation is available at\nhttps://github.com/rajarshisaha95/private-collaborative-relaying.",
    "updated" : "2024-06-06T06:12:15Z",
    "published" : "2024-06-06T06:12:15Z",
    "authors" : [
      {
        "name" : "Rajarshi Saha"
      },
      {
        "name" : "Mohamed Seif"
      },
      {
        "name" : "Michal Yemini"
      },
      {
        "name" : "Andrea J. Goldsmith"
      },
      {
        "name" : "H. Vincent Poor"
      }
    ],
    "categories" : [
      "eess.SP",
      "cs.DC",
      "cs.IT",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03749v1",
    "title" : "NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting\n  by Learning from Human",
    "summary" : "Increasing concerns about privacy leakage issues in academia and industry\narise when employing NLP models from third-party providers to process sensitive\ntexts. To protect privacy before sending sensitive data to those models, we\nsuggest sanitizing sensitive text using two common strategies used by humans:\ni) deleting sensitive expressions, and ii) obscuring sensitive details by\nabstracting them. To explore the issues and develop a tool for text rewriting,\nwe curate the first corpus, coined NAP^2, through both crowdsourcing and the\nuse of large language models (LLMs). Compared to the prior works based on\ndifferential privacy, which lead to a sharp drop in information utility and\nunnatural texts, the human-inspired approaches result in more natural rewrites\nand offer an improved balance between privacy protection and data utility, as\ndemonstrated by our extensive experiments.",
    "updated" : "2024-06-06T05:07:44Z",
    "published" : "2024-06-06T05:07:44Z",
    "authors" : [
      {
        "name" : "Shuo Huang"
      },
      {
        "name" : "William MacLean"
      },
      {
        "name" : "Xiaoxi Kang"
      },
      {
        "name" : "Anqi Wu"
      },
      {
        "name" : "Lizhen Qu"
      },
      {
        "name" : "Qiongkai Xu"
      },
      {
        "name" : "Zhuang Li"
      },
      {
        "name" : "Xingliang Yuan"
      },
      {
        "name" : "Gholamreza Haffari"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.03695v1",
    "title" : "FACOS: Enabling Privacy Protection Through Fine-Grained Access Control\n  with On-chain and Off-chain System",
    "summary" : "Data-driven landscape across finance, government, and healthcare, the\ncontinuous generation of information demands robust solutions for secure\nstorage, efficient dissemination, and fine-grained access control. Blockchain\ntechnology emerges as a significant tool, offering decentralized storage while\nupholding the tenets of data security and accessibility. However, on-chain and\noff-chain strategies are still confronted with issues such as untrusted\noff-chain data storage, absence of data ownership, limited access control\npolicy for clients, and a deficiency in data privacy and auditability. To solve\nthese challenges, we propose a permissioned blockchain-based privacy-preserving\nfine-grained access control on-chain and off-chain system, namely FACOS. We\napplied three fine-grained access control solutions and comprehensively\nanalyzed them in different aspects, which provides an intuitive perspective for\nsystem designers and clients to choose the appropriate access control method\nfor their systems. Compared to similar work that only stores encrypted data in\ncentralized or non-fault-tolerant IPFS systems, we enhanced off-chain data\nstorage security and robustness by utilizing a highly efficient and secure\nasynchronous Byzantine fault tolerance (BFT) protocol in the off-chain\nenvironment. As each of the clients needs to be verified and authorized before\naccessing the data, we involved the Trusted Execution Environment (TEE)-based\nsolution to verify the credentials of clients. Additionally, our evaluation\nresults demonstrated that our system offers better scalability and practicality\nthan other state-of-the-art designs.",
    "updated" : "2024-06-06T02:23:12Z",
    "published" : "2024-06-06T02:23:12Z",
    "authors" : [
      {
        "name" : "Chao Liu"
      },
      {
        "name" : "Cankun Hou"
      },
      {
        "name" : "Tianyu Jiang"
      },
      {
        "name" : "Jianting Ning"
      },
      {
        "name" : "Hui Qiao"
      },
      {
        "name" : "Yusen Wu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04827v1",
    "title" : "Black Box Differential Privacy Auditing Using Total Variation Distance",
    "summary" : "We present a practical method to audit the differential privacy (DP)\nguarantees of a machine learning model using a small hold-out dataset that is\nnot exposed to the model during the training. Having a score function such as\nthe loss function employed during the training, our method estimates the total\nvariation (TV) distance between scores obtained with a subset of the training\ndata and the hold-out dataset. With some meta information about the underlying\nDP training algorithm, these TV distance values can be converted to\n$(\\varepsilon,\\delta)$-guarantees for any $\\delta$. We show that these score\ndistributions asymptotically give lower bounds for the DP guarantees of the\nunderlying training algorithm, however, we perform a one-shot estimation for\npracticality reasons. We specify conditions that lead to lower bounds for the\nDP guarantees with high probability. To estimate the TV distance between the\nscore distributions, we use a simple density estimation method based on\nhistograms. We show that the TV distance gives a very close to optimally robust\nestimator and has an error rate $\\mathcal{O}(k^{-1/3})$, where $k$ is the total\nnumber of samples. Numerical experiments on benchmark datasets illustrate the\neffectiveness of our approach and show improvements over baseline methods for\nblack-box auditing.",
    "updated" : "2024-06-07T10:52:15Z",
    "published" : "2024-06-07T10:52:15Z",
    "authors" : [
      {
        "name" : "Antti Koskela"
      },
      {
        "name" : "Jafar Mohammadi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04702v1",
    "title" : "Marking the Pace: A Blockchain-Enhanced Privacy-Traceable Strategy for\n  Federated Recommender Systems",
    "summary" : "Federated recommender systems have been crucially enhanced through data\nsharing and continuous model updates, attributed to the pervasive connectivity\nand distributed computing capabilities of Internet of Things (IoT) devices.\nGiven the sensitivity of IoT data, transparent data processing in data sharing\nand model updates is paramount. However, existing methods fall short in tracing\nthe flow of shared data and the evolution of model updates. Consequently, data\nsharing is vulnerable to exploitation by malicious entities, raising\nsignificant data privacy concerns, while excluding data sharing will result in\nsub-optimal recommendations. To mitigate these concerns, we present LIBERATE, a\nprivacy-traceable federated recommender system. We design a blockchain-based\ntraceability mechanism, ensuring data privacy during data sharing and model\nupdates. We further enhance privacy protection by incorporating local\ndifferential privacy in user-server communication. Extensive evaluations with\nthe real-world dataset corroborate LIBERATE's capabilities in ensuring data\nprivacy during data sharing and model update while maintaining efficiency and\nperformance. Results underscore blockchain-based traceability mechanism as a\npromising solution for privacy-preserving in federated recommender systems.",
    "updated" : "2024-06-07T07:21:21Z",
    "published" : "2024-06-07T07:21:21Z",
    "authors" : [
      {
        "name" : "Zhen Cai"
      },
      {
        "name" : "Tao Tang"
      },
      {
        "name" : "Shuo Yu"
      },
      {
        "name" : "Yunpeng Xiao"
      },
      {
        "name" : "Feng Xia"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04610v1",
    "title" : "Contrastive explainable clustering with differential privacy",
    "summary" : "This paper presents a novel approach in Explainable AI (XAI), integrating\ncontrastive explanations with differential privacy in clustering methods. For\nseveral basic clustering problems, including $k$-median and $k$-means, we give\nefficient differential private contrastive explanations that achieve\nessentially the same explanations as those that non-private clustering\nexplanations can obtain. We define contrastive explanations as the utility\ndifference between the original clustering utility and utility from clustering\nwith a specifically fixed centroid. In each contrastive scenario, we designate\na specific data point as the fixed centroid position, enabling us to measure\nthe impact of this constraint on clustering utility under differential privacy.\nExtensive experiments across various datasets show our method's effectiveness\nin providing meaningful explanations without significantly compromising data\nprivacy or clustering utility. This underscores our contribution to\nprivacy-aware machine learning, demonstrating the feasibility of achieving a\nbalance between privacy and utility in the explanation of clustering tasks.",
    "updated" : "2024-06-07T03:37:36Z",
    "published" : "2024-06-07T03:37:36Z",
    "authors" : [
      {
        "name" : "Dung Nguyen"
      },
      {
        "name" : "Ariel Vetzler"
      },
      {
        "name" : "Sarit Kraus"
      },
      {
        "name" : "Anil Vullikanti"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04535v1",
    "title" : "Tangent differential privacy",
    "summary" : "Differential privacy is a framework for protecting the identity of individual\ndata points in the decision-making process. In this note, we propose a new form\nof differential privacy called tangent differential privacy. Compared with the\nusual differential privacy that is defined uniformly across data distributions,\ntangent differential privacy is tailored towards a specific data distribution\nof interest. It also allows for general distribution distances such as total\nvariation distance and Wasserstein distance. In the case of risk minimization,\nwe show that entropic regularization guarantees tangent differential privacy\nunder rather general conditions on the risk function.",
    "updated" : "2024-06-06T22:11:31Z",
    "published" : "2024-06-06T22:11:31Z",
    "authors" : [
      {
        "name" : "Lexing Ying"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.06473v1",
    "title" : "DiffAudit: Auditing Privacy Practices of Online Services for Children\n  and Adolescents",
    "summary" : "Children's and adolescents' online data privacy are regulated by laws such as\nthe Children's Online Privacy Protection Act (COPPA) and the California\nConsumer Privacy Act (CCPA). Online services that are directed towards general\naudiences (i.e., including children, adolescents, and adults) must comply with\nthese laws. In this paper, first, we present DiffAudit, a platform-agnostic\nprivacy auditing methodology for general audience services. DiffAudit performs\ndifferential analysis of network traffic data flows to compare data processing\npractices (i) between child, adolescent, and adult users and (ii) before and\nafter consent is given and user age is disclosed. We also present a data type\nclassification method that utilizes GPT-4 and our data type ontology based on\nCOPPA and CCPA, allowing us to identify considerably more data types than prior\nwork. Second, we apply DiffAudit to a set of popular general audience mobile\nand web services and observe a rich set of behaviors extracted from over 440K\noutgoing requests, containing 3,968 unique data types we extracted and\nclassified. We reveal problematic data processing practices prior to consent\nand age disclosure, lack of differentiation between age-specific data flows,\ninconsistent privacy policy disclosures, and sharing of linkable data with\nthird parties, including advertising and tracking services.",
    "updated" : "2024-06-10T17:14:53Z",
    "published" : "2024-06-10T17:14:53Z",
    "authors" : [
      {
        "name" : "Olivia Figueira"
      },
      {
        "name" : "Rahmadi Trimananda"
      },
      {
        "name" : "Athina Markopoulou"
      },
      {
        "name" : "Scott Jordan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.06186v1",
    "title" : "A Survey on Machine Unlearning: Techniques and New Emerged Privacy Risks",
    "summary" : "The explosive growth of machine learning has made it a critical\ninfrastructure in the era of artificial intelligence. The extensive use of data\nposes a significant threat to individual privacy. Various countries have\nimplemented corresponding laws, such as GDPR, to protect individuals' data\nprivacy and the right to be forgotten. This has made machine unlearning a\nresearch hotspot in the field of privacy protection in recent years, with the\naim of efficiently removing the contribution and impact of individual data from\ntrained models. The research in academia on machine unlearning has continuously\nenriched its theoretical foundation, and many methods have been proposed,\ntargeting different data removal requests in various application scenarios.\nHowever, recently researchers have found potential privacy leakages of various\nof machine unlearning approaches, making the privacy preservation on machine\nunlearning area a critical topic. This paper provides an overview and analysis\nof the existing research on machine unlearning, aiming to present the current\nvulnerabilities of machine unlearning approaches. We analyze privacy risks in\nvarious aspects, including definitions, implementation methods, and real-world\napplications. Compared to existing reviews, we analyze the new challenges posed\nby the latest malicious attack techniques on machine unlearning from the\nperspective of privacy threats. We hope that this survey can provide an initial\nbut comprehensive discussion on this new emerging area.",
    "updated" : "2024-06-10T11:31:04Z",
    "published" : "2024-06-10T11:31:04Z",
    "authors" : [
      {
        "name" : "Hengzhu Liu"
      },
      {
        "name" : "Ping Xiong"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Philip S. Yu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.05858v1",
    "title" : "Comments on \"Federated Learning with Differential Privacy: Algorithms\n  and Performance Analysis\"",
    "summary" : "In the paper by Wei et al. (\"Federated Learning with Differential Privacy:\nAlgorithms and Performance Analysis\"), the convergence performance of the\nproposed differential privacy algorithm in federated learning (FL), known as\nNoising before Model Aggregation FL (NbAFL), was studied. However, the\npresented convergence upper bound of NbAFL (Theorem 2) is incorrect. This\ncomment aims to present the correct form of the convergence upper bound for\nNbAFL.",
    "updated" : "2024-06-09T17:03:56Z",
    "published" : "2024-06-09T17:03:56Z",
    "authors" : [
      {
        "name" : "Mahtab Talaei"
      },
      {
        "name" : "Iman Izadi"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR",
      "cs.PF"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.05545v1",
    "title" : "Privacy-Preserving Optimal Parameter Selection for Collaborative\n  Clustering",
    "summary" : "This study investigates the optimal selection of parameters for collaborative\nclustering while ensuring data privacy. We focus on key clustering algorithms\nwithin a collaborative framework, where multiple data owners combine their\ndata. A semi-trusted server assists in recommending the most suitable\nclustering algorithm and its parameters. Our findings indicate that the privacy\nparameter ($\\epsilon$) minimally impacts the server's recommendations, but an\nincrease in $\\epsilon$ raises the risk of membership inference attacks, where\nsensitive information might be inferred. To mitigate these risks, we implement\ndifferential privacy techniques, particularly the Randomized Response\nmechanism, to add noise and protect data privacy. Our approach demonstrates\nthat high-quality clustering can be achieved while maintaining data\nconfidentiality, as evidenced by metrics such as the Adjusted Rand Index and\nSilhouette Score. This study contributes to privacy-aware data sharing, optimal\nalgorithm and parameter selection, and effective communication between data\nowners and the server.",
    "updated" : "2024-06-08T18:21:12Z",
    "published" : "2024-06-08T18:21:12Z",
    "authors" : [
      {
        "name" : "Maryam Ghasemian"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.05459v1",
    "title" : "PriviFy: Designing Tangible Interfaces for Configuring IoT Privacy\n  Preferences",
    "summary" : "The Internet of Things (IoT) devices, such as smart speakers can collect\nsensitive user data, necessitating the need for users to manage their privacy\npreferences. However, configuring these preferences presents users with\nmultiple challenges. Existing privacy controls often lack transparency, are\nhard to understand, and do not provide meaningful choices. On top of that,\nusers struggle to locate privacy settings due to multiple menus or confusing\nlabeling, which discourages them from using these controls. We introduce\nPriviFy (Privacy Simplify-er), a novel and user-friendly tangible interface\nthat can simplify the configuration of smart devices privacy settings. PriviFy\nis designed to propose an enhancement to existing hardware by integrating\nadditional features that improve privacy management. We envision that positive\nfeedback and user experiences from our study will inspire consumer product\ndevelopers and smart device manufacturers to incorporate the useful design\nelements we have identified. Using fidelity prototyping, we iteratively\ndesigned PriviFy prototype with 20 participants to include interactive features\nsuch as knobs, buttons, lights, and notifications that allow users to configure\ntheir data privacy preferences and receive confirmation of their choices. We\nfurther evaluated PriviFy high-fidelity prototype with 20 more participants.\nOur results show that PriviFy helps simplify the complexity of privacy\npreferences configuration with a significant usability score at p < .05 (P =\n0.000000017, t = -8.8639). PriviFy successfully met users privacy needs and\nenabled them to regain control over their data. We conclude by recommending the\nimportance of designing specific privacy configuration options.",
    "updated" : "2024-06-08T12:35:46Z",
    "published" : "2024-06-08T12:35:46Z",
    "authors" : [
      {
        "name" : "Bayan Al Muhander"
      },
      {
        "name" : "Omer Rana"
      },
      {
        "name" : "Charith Perera"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.05451v1",
    "title" : "PrivacyCube: Data Physicalization for Enhancing Privacy Awareness in IoT",
    "summary" : "People are increasingly bringing Internet of Things (IoT) devices into their\nhomes without understanding how their data is gathered, processed, and used. We\ndescribe PrivacyCube, a novel data physicalization designed to increase privacy\nawareness within smart home environments. PrivacyCube visualizes IoT data\nconsumption by displaying privacy-related notices. PrivacyCube aims to assist\nsmart home occupants to (i) understand their data privacy better and (ii) have\nconversations around data management practices of IoT devices used within their\nhomes. Using PrivacyCube, households can learn and make informed privacy\ndecisions collectively. To evaluate PrivacyCube, we used multiple research\nmethods throughout the different stages of design. We first conducted a focus\ngroup study in two stages with six participants to compare PrivacyCube to text\nand state-of-the-art privacy policies. We then deployed PrivacyCube in a\n14-day-long field study with eight households. Our results show that\nPrivacyCube helps home occupants comprehend IoT privacy better with\nsignificantly increased privacy awareness at p < .05 (p=0.00041, t= -5.57).\nParticipants preferred PrivacyCube over text privacy policies because it was\ncomprehensive and easier to use. PrivacyCube and Privacy Label, a\nstate-of-the-art approach, both received positive reviews from participants,\nwith PrivacyCube being preferred for its interactivity and ability to encourage\nconversations. PrivacyCube was also considered by home occupants as a piece of\nhome furniture, encouraging them to socialize and discuss IoT privacy\nimplications using this device.",
    "updated" : "2024-06-08T12:20:42Z",
    "published" : "2024-06-08T12:20:42Z",
    "authors" : [
      {
        "name" : "Bayan Al Muhander"
      },
      {
        "name" : "Nalin Arachchilage"
      },
      {
        "name" : "Yasar Majib"
      },
      {
        "name" : "Mohammed Alosaimi"
      },
      {
        "name" : "Omer Rana"
      },
      {
        "name" : "Charith Perera"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.07314v1",
    "title" : "Rethinking the impact of noisy labels in graph classification: A utility\n  and privacy perspective",
    "summary" : "Graph neural networks based on message-passing mechanisms have achieved\nadvanced results in graph classification tasks. However, their generalization\nperformance degrades when noisy labels are present in the training data. Most\nexisting noisy labeling approaches focus on the visual domain or graph node\nclassification tasks and analyze the impact of noisy labels only from a utility\nperspective. Unlike existing work, in this paper, we measure the effects of\nnoise labels on graph classification from data privacy and model utility\nperspectives. We find that noise labels degrade the model's generalization\nperformance and enhance the ability of membership inference attacks on graph\ndata privacy. To this end, we propose the robust graph neural network approach\nwith noisy labeled graph classification. Specifically, we first accurately\nfilter the noisy samples by high-confidence samples and the first feature\nprincipal component vector of each class. Then, the robust principal component\nvectors and the model output under data augmentation are utilized to achieve\nnoise label correction guided by dual spatial information. Finally, supervised\ngraph contrastive learning is introduced to enhance the embedding quality of\nthe model and protect the privacy of the training graph data. The utility and\nprivacy of the proposed method are validated by comparing twelve different\nmethods on eight real graph classification datasets. Compared with the\nstate-of-the-art methods, the RGLC method achieves at most and at least 7.8%\nand 0.8% performance gain at 30% noisy labeling rate, respectively, and reduces\nthe accuracy of privacy attacks to below 60%.",
    "updated" : "2024-06-11T14:44:37Z",
    "published" : "2024-06-11T14:44:37Z",
    "authors" : [
      {
        "name" : "De Li"
      },
      {
        "name" : "Xianxian Li"
      },
      {
        "name" : "Zeming Gan"
      },
      {
        "name" : "Qiyu Li"
      },
      {
        "name" : "Bin Qu"
      },
      {
        "name" : "Jinyan Wang"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.06990v1",
    "title" : "Privacy-Utility Tradeoff Based on $α$-lift",
    "summary" : "Information density and its exponential form, known as lift, play a central\nrole in information privacy leakage measures. $\\alpha$-lift is the power-mean\nof lift, which is tunable between the worst-case measure max-lift\n($\\alpha=\\infty$) and more relaxed versions ($\\alpha<\\infty$). This paper\ninvestigates the optimization problem of the privacy-utility tradeoff where\n$\\alpha$-lift and mutual information are privacy and utility measures,\nrespectively. Due to the nonlinear nature of $\\alpha$-lift for $\\alpha<\\infty$,\nfinding the optimal solution is challenging. Therefore, we propose a heuristic\nalgorithm to estimate the optimal utility for each value of $\\alpha$, inspired\nby the optimal solution for $\\alpha=\\infty$. In proposing the algorithm, we\nprove and use the convexity of $\\alpha$-lift with respect to the lift.",
    "updated" : "2024-06-11T06:39:57Z",
    "published" : "2024-06-11T06:39:57Z",
    "authors" : [
      {
        "name" : "Mohammad Amin Zarrabian"
      },
      {
        "name" : "Parastoo Sadeghi"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.06755v1",
    "title" : "Optimal Federated Learning for Nonparametric Regression with\n  Heterogeneous Distributed Differential Privacy Constraints",
    "summary" : "This paper studies federated learning for nonparametric regression in the\ncontext of distributed samples across different servers, each adhering to\ndistinct differential privacy constraints. The setting we consider is\nheterogeneous, encompassing both varying sample sizes and differential privacy\nconstraints across servers. Within this framework, both global and pointwise\nestimation are considered, and optimal rates of convergence over the Besov\nspaces are established.\n  Distributed privacy-preserving estimators are proposed and their risk\nproperties are investigated. Matching minimax lower bounds, up to a logarithmic\nfactor, are established for both global and pointwise estimation. Together,\nthese findings shed light on the tradeoff between statistical accuracy and\nprivacy preservation. In particular, we characterize the compromise not only in\nterms of the privacy budget but also concerning the loss incurred by\ndistributing data within the privacy framework as a whole. This insight\ncaptures the folklore wisdom that it is easier to retain privacy in larger\nsamples, and explores the differences between pointwise and global estimation\nunder distributed privacy constraints.",
    "updated" : "2024-06-10T19:34:07Z",
    "published" : "2024-06-10T19:34:07Z",
    "authors" : [
      {
        "name" : "T. Tony Cai"
      },
      {
        "name" : "Abhinav Chakraborty"
      },
      {
        "name" : "Lasse Vuursteen"
      }
    ],
    "categories" : [
      "math.ST",
      "cs.LG",
      "stat.ML",
      "stat.TH",
      "62G08, 62C20, 68P27, 62F30,"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.06749v1",
    "title" : "Federated Nonparametric Hypothesis Testing with Differential Privacy\n  Constraints: Optimal Rates and Adaptive Tests",
    "summary" : "Federated learning has attracted significant recent attention due to its\napplicability across a wide range of settings where data is collected and\nanalyzed across disparate locations. In this paper, we study federated\nnonparametric goodness-of-fit testing in the white-noise-with-drift model under\ndistributed differential privacy (DP) constraints.\n  We first establish matching lower and upper bounds, up to a logarithmic\nfactor, on the minimax separation rate. This optimal rate serves as a benchmark\nfor the difficulty of the testing problem, factoring in model characteristics\nsuch as the number of observations, noise level, and regularity of the signal\nclass, along with the strictness of the $(\\epsilon,\\delta)$-DP requirement. The\nresults demonstrate interesting and novel phase transition phenomena.\nFurthermore, the results reveal an interesting phenomenon that distributed\none-shot protocols with access to shared randomness outperform those without\naccess to shared randomness. We also construct a data-driven testing procedure\nthat possesses the ability to adapt to an unknown regularity parameter over a\nlarge collection of function classes with minimal additional cost, all while\nmaintaining adherence to the same set of DP constraints.",
    "updated" : "2024-06-10T19:25:19Z",
    "published" : "2024-06-10T19:25:19Z",
    "authors" : [
      {
        "name" : "T. Tony Cai"
      },
      {
        "name" : "Abhinav Chakraborty"
      },
      {
        "name" : "Lasse Vuursteen"
      }
    ],
    "categories" : [
      "math.ST",
      "cs.LG",
      "stat.ML",
      "stat.TH",
      "62G10, 62C20, 68P27, 62F30"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.07973v1",
    "title" : "Unique Security and Privacy Threats of Large Language Model: A\n  Comprehensive Survey",
    "summary" : "With the rapid development of artificial intelligence, large language models\n(LLMs) have made remarkable progress in natural language processing. These\nmodels are trained on large amounts of data to demonstrate powerful language\nunderstanding and generation capabilities for various applications, from\nmachine translation and chatbots to agents. However, LLMs have exposed a\nvariety of privacy and security issues during their life cycle, which have\nbecome the focus of academic and industrial attention. Moreover, these risks\nLLMs face are pretty different from previous traditional language models. Since\ncurrent surveys lack a clear taxonomy of unique threat models based on diverse\nscenarios, we highlight unique privacy and security issues based on five\nscenarios: pre-training, fine-tuning, RAG system, deploying, and LLM-based\nagent. Concerning the characteristics of each risk, this survey provides\npotential threats and countermeasures. The research on attack and defense\nsituations LLMs face can provide feasible research directions, making more\nareas reap LLMs' benefits.",
    "updated" : "2024-06-12T07:55:32Z",
    "published" : "2024-06-12T07:55:32Z",
    "authors" : [
      {
        "name" : "Shang Wang"
      },
      {
        "name" : "Tianqing Zhu"
      },
      {
        "name" : "Bo Liu"
      },
      {
        "name" : "Ding Ming"
      },
      {
        "name" : "Xu Guo"
      },
      {
        "name" : "Dayong Ye"
      },
      {
        "name" : "Wanlei Zhou"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.09214v1",
    "title" : "Applying Multi-Agent Negotiation to Solve the Production Routing Problem\n  With Privacy Preserving",
    "summary" : "This paper presents a novel approach to address the Production Routing\nProblem with Privacy Preserving (PRPPP) in supply chain optimization. The\nintegrated optimization of production, inventory, distribution, and routing\ndecisions in real-world industry applications poses several challenges,\nincluding increased complexity, discrepancies between planning and execution,\nand constraints on information sharing. To mitigate these challenges, this\npaper proposes the use of intelligent agent negotiation within a hybrid\nMulti-Agent System (MAS) integrated with optimization algorithms. The MAS\nfacilitates communication and coordination among entities, encapsulates private\ninformation, and enables negotiation. This, along with optimization algorithms,\nmakes it a compelling framework for establishing optimal solutions. The\napproach is supported by real-world applications and synergies between MAS and\noptimization methods, demonstrating its effectiveness in addressing complex\nsupply chain optimization problems.",
    "updated" : "2024-06-13T15:15:34Z",
    "published" : "2024-06-13T15:15:34Z",
    "authors" : [
      {
        "name" : "Luiza Pellin Biasoto"
      },
      {
        "name" : "Vinicius Renan de Carvalho"
      },
      {
        "name" : "Jaime Simão Sichman"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.09037v1",
    "title" : "Evaluating Privacy, Security, and Trust Perceptions in Conversational\n  AI: A Systematic Review",
    "summary" : "Conversational AI (CAI) systems which encompass voice- and text-based\nassistants are on the rise and have been largely integrated into people's\neveryday lives. Despite their widespread adoption, users voice concerns\nregarding privacy, security and trust in these systems. However, the\ncomposition of these perceptions, their impact on technology adoption and usage\nand the relationship between privacy, security and trust perceptions in the CAI\ncontext remain open research challenges. This study contributes to the field by\nconducting a Systematic Literature Review and offers insights into the current\nstate of research on privacy, security and trust perceptions in the context of\nCAI systems. The review covers application fields and user groups and sheds\nlight on empirical methods and tools used for assessment. Moreover, it provides\ninsights into the reliability and validity of privacy, security and trust\nscales, as well as extensively investigating the subconstructs of each item as\nwell as additional concepts which are concurrently collected. We point out that\nthe perceptions of trust, privacy and security overlap based on the\nsubconstructs we identified. While the majority of studies investigate one of\nthese concepts, only a few studies were found exploring privacy, security and\ntrust perceptions jointly. Our research aims to inform on directions to develop\nand use reliable scales for users' privacy, security and trust perceptions and\ncontribute to the development of trustworthy CAI systems.",
    "updated" : "2024-06-13T12:20:26Z",
    "published" : "2024-06-13T12:20:26Z",
    "authors" : [
      {
        "name" : "Anna Leschanowsky"
      },
      {
        "name" : "Silas Rech"
      },
      {
        "name" : "Birgit Popp"
      },
      {
        "name" : "Tom Bäckström"
      }
    ],
    "categories" : [
      "cs.HC",
      "68-06",
      "J.0"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.09005v1",
    "title" : "Privacy Aware Memory Forensics",
    "summary" : "In recent years, insider threats and attacks have been increasing in terms of\nfrequency and cost to the corporate business. The utilization of end-to-end\nencrypted instant messaging applications (WhatsApp, Telegram, VPN) by malicious\ninsiders raised data breach incidents exponentially. The Securities and\nExchange Board of India (SEBI) investigated reports on such data leak incidents\nand reported about twelve companies where earnings data and financial\ninformation were leaked using WhatsApp messages. Recent surveys indicate that\n60% of data breaches are primarily caused by malicious insider threats.\nEspecially, in the case of the defense environment, information leaks by\ninsiders will jeopardize the countrys national security. Sniffing of network\nand host-based activities will not work in an insider threat detection\nenvironment due to end-to-end encryption. Memory forensics allows access to the\nmessages sent or received over an end-to-end encrypted environment but with a\ntotal compromise of the users privacy. In this research, we present a novel\nsolution to detect data leakages by insiders in an organization. Our approach\ncaptures the RAM of the insiders device and analyses it for sensitive\ninformation leaks from a host system while maintaining the users privacy.\nSensitive data leaks are identified with context using a deep learning model.\nThe feasibility and effectiveness of the proposed idea have been demonstrated\nwith the help of a military use case. The proposed architecture can however be\nused across various use cases with minor modifications.",
    "updated" : "2024-06-13T11:18:49Z",
    "published" : "2024-06-13T11:18:49Z",
    "authors" : [
      {
        "name" : "Janardhan Kalikiri"
      },
      {
        "name" : "Gaurav Varshney"
      },
      {
        "name" : "Jaswinder Kour"
      },
      {
        "name" : "Tarandeep Singh"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.08918v1",
    "title" : "Beyond the Calibration Point: Mechanism Comparison in Differential\n  Privacy",
    "summary" : "In differentially private (DP) machine learning, the privacy guarantees of DP\nmechanisms are often reported and compared on the basis of a single\n$(\\varepsilon, \\delta)$-pair. This practice overlooks that DP guarantees can\nvary substantially \\emph{even between mechanisms sharing a given $(\\varepsilon,\n\\delta)$}, and potentially introduces privacy vulnerabilities which can remain\nundetected. This motivates the need for robust, rigorous methods for comparing\nDP guarantees in such cases. Here, we introduce the $\\Delta$-divergence between\nmechanisms which quantifies the worst-case excess privacy vulnerability of\nchoosing one mechanism over another in terms of $(\\varepsilon, \\delta)$, $f$-DP\nand in terms of a newly presented Bayesian interpretation. Moreover, as a\ngeneralisation of the Blackwell theorem, it is endowed with strong\ndecision-theoretic foundations. Through application examples, we show that our\ntechniques can facilitate informed decision-making and reveal gaps in the\ncurrent understanding of privacy risks, as current practices in DP-SGD often\nresult in choosing mechanisms with high excess privacy vulnerabilities.",
    "updated" : "2024-06-13T08:30:29Z",
    "published" : "2024-06-13T08:30:29Z",
    "authors" : [
      {
        "name" : "Georgios Kaissis"
      },
      {
        "name" : "Stefan Kolek"
      },
      {
        "name" : "Borja Balle"
      },
      {
        "name" : "Jamie Hayes"
      },
      {
        "name" : "Daniel Rueckert"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.04535v1",
    "title" : "Tangent differential privacy",
    "summary" : "Differential privacy is a framework for protecting the identity of individual\ndata points in the decision-making process. In this note, we propose a new form\nof differential privacy called tangent differential privacy. Compared with the\nusual differential privacy that is defined uniformly across data distributions,\ntangent differential privacy is tailored towards a specific data distribution\nof interest. It also allows for general distribution distances such as total\nvariation distance and Wasserstein distance. In the case of risk minimization,\nwe show that entropic regularization guarantees tangent differential privacy\nunder rather general conditions on the risk function.",
    "updated" : "2024-06-06T22:11:31Z",
    "published" : "2024-06-06T22:11:31Z",
    "authors" : [
      {
        "name" : "Lexing Ying"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  }
]