[
  {
    "id" : "http://arxiv.org/abs/2409.02614v1",
    "title" : "Evaluating the Effects of Digital Privacy Regulations on User Trust",
    "summary" : "In today's digital society, issues related to digital privacy have become\nincreasingly important. Issues such as data breaches result in misuse of data,\nfinancial loss, and cyberbullying, which leads to less user trust in digital\nservices. This research investigates the impact of digital privacy laws on user\ntrust by comparing the regulations in the Netherlands, Ghana, and Malaysia. The\nstudy employs a comparative case study method, involving interviews with\ndigital privacy law experts, IT educators, and consumers from each country. The\nmain findings reveal that while the General Data Protection Regulation (GDPR)\nin the Netherlands is strict, its practical impact is limited by enforcement\nchallenges. In Ghana, the Data Protection Act is underutilized due to low\npublic awareness and insufficient enforcement, leading to reliance on personal\nprotective measures. In Malaysia, trust in digital services is largely\ndependent on the security practices of individual platforms rather than the\nPersonal Data Protection Act. The study highlights the importance of public\nawareness, effective enforcement, and cultural considerations in shaping the\neffectiveness of digital privacy laws. Based on these insights, a\nrecommendation framework is proposed to enhance digital privacy practices, also\naiming to provide valuable guidance for policymakers, businesses, and citizens\nin navigating the challenges of digitalization.",
    "updated" : "2024-09-04T11:11:41Z",
    "published" : "2024-09-04T11:11:41Z",
    "authors" : [
      {
        "name" : "Mehmet Berk Cetin"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02404v1",
    "title" : "Learning Privacy-Preserving Student Networks via\n  Discriminative-Generative Distillation",
    "summary" : "While deep models have proved successful in learning rich knowledge from\nmassive well-annotated data, they may pose a privacy leakage risk in practical\ndeployment. It is necessary to find an effective trade-off between high utility\nand strong privacy. In this work, we propose a discriminative-generative\ndistillation approach to learn privacy-preserving deep models. Our key idea is\ntaking models as bridge to distill knowledge from private data and then\ntransfer it to learn a student network via two streams. First, discriminative\nstream trains a baseline classifier on private data and an ensemble of teachers\non multiple disjoint private subsets, respectively. Then, generative stream\ntakes the classifier as a fixed discriminator and trains a generator in a\ndata-free manner. After that, the generator is used to generate massive\nsynthetic data which are further applied to train a variational autoencoder\n(VAE). Among these synthetic data, a few of them are fed into the teacher\nensemble to query labels via differentially private aggregation, while most of\nthem are embedded to the trained VAE for reconstructing synthetic data.\nFinally, a semi-supervised student learning is performed to simultaneously\nhandle two tasks: knowledge transfer from the teachers with distillation on few\nprivately labeled synthetic data, and knowledge enhancement with tangent-normal\nadversarial regularization on many triples of reconstructed synthetic data. In\nthis way, our approach can control query cost over private data and mitigate\naccuracy degradation in a unified manner, leading to a privacy-preserving\nstudent model. Extensive experiments and analysis clearly show the\neffectiveness of the proposed approach.",
    "updated" : "2024-09-04T03:06:13Z",
    "published" : "2024-09-04T03:06:13Z",
    "authors" : [
      {
        "name" : "Shiming Ge"
      },
      {
        "name" : "Bochao Liu"
      },
      {
        "name" : "Pengju Wang"
      },
      {
        "name" : "Yong Li"
      },
      {
        "name" : "Dan Zeng"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02375v1",
    "title" : "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review",
    "summary" : "The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights.",
    "updated" : "2024-09-04T01:51:37Z",
    "published" : "2024-09-04T01:51:37Z",
    "authors" : [
      {
        "name" : "Xichou Zhu"
      },
      {
        "name" : "Yang Liu"
      },
      {
        "name" : "Zhou Shen"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Min Li"
      },
      {
        "name" : "Yujun Chen"
      },
      {
        "name" : "Benzi John"
      },
      {
        "name" : "Zhenzhen Ma"
      },
      {
        "name" : "Tao Hu"
      },
      {
        "name" : "Bolong Yang"
      },
      {
        "name" : "Manman Wang"
      },
      {
        "name" : "Zongxing Xie"
      },
      {
        "name" : "Peng Liu"
      },
      {
        "name" : "Dan Cai"
      },
      {
        "name" : "Junhui Wang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02364v1",
    "title" : "Examining Caregiving Roles to Differentiate the Effects of Using a\n  Mobile App for Community Oversight for Privacy and Security",
    "summary" : "We conducted a 4-week field study with 101 smartphone users who\nself-organized into 22 small groups of family, friends, and neighbors to use\n``CO-oPS,'' a mobile app for co-managing mobile privacy and security. We\ndifferentiated between those who provided oversight (i.e., caregivers) and\nthose who did not (i.e., caregivees) to examine differential effects on their\nexperiences and behaviors while using CO-oPS. Caregivers reported higher power\nuse, community trust, belonging, collective efficacy, and self-efficacy than\ncaregivees. Both groups' self-efficacy and collective efficacy for mobile\nprivacy and security increased after using CO-oPS. However, this increase was\nsignificantly stronger for caregivees. Our research demonstrates how\ncommunity-based approaches can benefit people who need additional help managing\ntheir digital privacy and security. We provide recommendations to support\ncommunity-based oversight for managing privacy and security within communities\nof different roles and skills.",
    "updated" : "2024-09-04T01:21:56Z",
    "published" : "2024-09-04T01:21:56Z",
    "authors" : [
      {
        "name" : "Mamtaj Akter"
      },
      {
        "name" : "Jess Kropczynski"
      },
      {
        "name" : "Heather Lipford"
      },
      {
        "name" : "Pamela Wisniewski"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02044v1",
    "title" : "FedMinds: Privacy-Preserving Personalized Brain Visual Decoding",
    "summary" : "Exploring the mysteries of the human brain is a long-term research topic in\nneuroscience. With the help of deep learning, decoding visual information from\nhuman brain activity fMRI has achieved promising performance. However, these\ndecoding models require centralized storage of fMRI data to conduct training,\nleading to potential privacy security issues. In this paper, we focus on\nprivacy preservation in multi-individual brain visual decoding. To this end, we\nintroduce a novel framework called FedMinds, which utilizes federated learning\nto protect individuals' privacy during model training. In addition, we deploy\nindividual adapters for each subject, thus allowing personalized visual\ndecoding. We conduct experiments on the authoritative NSD datasets to evaluate\nthe performance of the proposed framework. The results demonstrate that our\nframework achieves high-precision visual decoding along with privacy\nprotection.",
    "updated" : "2024-09-03T16:46:29Z",
    "published" : "2024-09-03T16:46:29Z",
    "authors" : [
      {
        "name" : "Guangyin Bao"
      },
      {
        "name" : "Duoqian Miao"
      }
    ],
    "categories" : [
      "q-bio.NC",
      "cs.CV",
      "cs.DC",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01924v1",
    "title" : "Privacy-Preserving and Post-Quantum Counter Denial of Service Framework\n  for Wireless Networks",
    "summary" : "As network services progress and mobile and IoT environments expand, numerous\nsecurity concerns have surfaced for spectrum access systems. The omnipresent\nrisk of Denial-of-Service (DoS) attacks and raising concerns about user privacy\n(e.g., location privacy, anonymity) are among such cyber threats. These\nsecurity and privacy risks increase due to the threat of quantum computers that\ncan compromise long-term security by circumventing conventional cryptosystems\nand increasing the cost of countermeasures. While some defense mechanisms exist\nagainst these threats in isolation, there is a significant gap in the state of\nthe art on a holistic solution against DoS attacks with privacy and anonymity\nfor spectrum management systems, especially when post-quantum (PQ) security is\nin mind. In this paper, we propose a new cybersecurity framework PACDoSQ, which\nis (to the best of our knowledge) the first to offer location privacy and\nanonymity for spectrum management with counter DoS and PQ security\nsimultaneously. Our solution introduces the private spectrum bastion (database)\nconcept to exploit existing architectural features of spectrum management\nsystems and then synergizes them with multi-server private information\nretrieval and PQ-secure Tor to guarantee a location-private and anonymous\nacquisition of spectrum information together with hash-based client-server\npuzzles for counter DoS. We prove that PACDoSQ achieves its security\nobjectives, and show its feasibility via a comprehensive performance\nevaluation.",
    "updated" : "2024-09-03T14:14:41Z",
    "published" : "2024-09-03T14:14:41Z",
    "authors" : [
      {
        "name" : "Saleh Darzi"
      },
      {
        "name" : "Attila Altay Yavuz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01710v1",
    "title" : "Privacy-Preserving Multimedia Mobile Cloud Computing Using Protective\n  Perturbation",
    "summary" : "Mobile cloud computing has been adopted in many multimedia applications,\nwhere the resource-constrained mobile device sends multimedia data (e.g.,\nimages) to remote cloud servers to request computation-intensive multimedia\nservices (e.g., image recognition). While significantly improving the\nperformance of the mobile applications, the cloud-based mechanism often causes\nprivacy concerns as the multimedia data and services are offloaded from the\ntrusted user device to untrusted cloud servers. Several recent studies have\nproposed perturbation-based privacy preserving mechanisms, which obfuscate the\noffloaded multimedia data to eliminate privacy exposures without affecting the\nfunctionality of the remote multimedia services. However, the existing privacy\nprotection approaches require the deployment of computation-intensive\nperturbation generation on the resource-constrained mobile devices. Also, the\nobfuscated images are typically not compliant with the standard image\ncompression algorithms and suffer from significant bandwidth consumption. In\nthis paper, we develop a novel privacy-preserving multimedia mobile cloud\ncomputing framework, namely $PMC^2$, to address the resource and bandwidth\nchallenges. $PMC^2$ employs secure confidential computing in the cloud to\ndeploy the perturbation generator, which addresses the resource challenge while\nmaintaining the privacy. Furthermore, we develop a neural compressor\nspecifically trained to compress the perturbed images in order to address the\nbandwidth challenge. We implement $PMC^2$ in an end-to-end mobile cloud\ncomputing system, based on which our evaluations demonstrate superior latency,\npower efficiency, and bandwidth consumption achieved by $PMC^2$ while\nmaintaining high accuracy in the target multimedia service.",
    "updated" : "2024-09-03T08:47:17Z",
    "published" : "2024-09-03T08:47:17Z",
    "authors" : [
      {
        "name" : "Zhongze Tang"
      },
      {
        "name" : "Mengmei Ye"
      },
      {
        "name" : "Yao Liu"
      },
      {
        "name" : "Sheng Wei"
      }
    ],
    "categories" : [
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01661v1",
    "title" : "$S^2$NeRF: Privacy-preserving Training Framework for NeRF",
    "summary" : "Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and\ngraphics, facilitating novel view synthesis and influencing sectors like\nextended reality and e-commerce. However, NeRF's dependence on extensive data\ncollection, including sensitive scene image data, introduces significant\nprivacy risks when users upload this data for model training. To address this\nconcern, we first propose SplitNeRF, a training framework that incorporates\nsplit learning (SL) techniques to enable privacy-preserving collaborative model\ntraining between clients and servers without sharing local data. Despite its\nbenefits, we identify vulnerabilities in SplitNeRF by developing two attack\nmethods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which\nexploit the shared gradient data and a few leaked scene images to reconstruct\nprivate scene information. To counter these threats, we introduce $S^2$NeRF,\nsecure SplitNeRF that integrates effective defense mechanisms. By introducing\ndecaying noise related to the gradient norm into the shared gradient\ninformation, $S^2$NeRF preserves privacy while maintaining a high utility of\nthe NeRF model. Our extensive evaluations across multiple datasets demonstrate\nthe effectiveness of $S^2$NeRF against privacy breaches, confirming its\nviability for secure NeRF training in sensitive applications.",
    "updated" : "2024-09-03T07:08:30Z",
    "published" : "2024-09-03T07:08:30Z",
    "authors" : [
      {
        "name" : "Bokang Zhang"
      },
      {
        "name" : "Yanglin Zhang"
      },
      {
        "name" : "Zhikun Zhang"
      },
      {
        "name" : "Jinglan Yang"
      },
      {
        "name" : "Lingying Huang"
      },
      {
        "name" : "Junfeng Wu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01329v1",
    "title" : "Assessing the Impact of Image Dataset Features on Privacy-Preserving\n  Machine Learning",
    "summary" : "Machine Learning (ML) is crucial in many sectors, including computer vision.\nHowever, ML models trained on sensitive data face security challenges, as they\ncan be attacked and leak information. Privacy-Preserving Machine Learning\n(PPML) addresses this by using Differential Privacy (DP) to balance utility and\nprivacy. This study identifies image dataset characteristics that affect the\nutility and vulnerability of private and non-private Convolutional Neural\nNetwork (CNN) models. Through analyzing multiple datasets and privacy budgets,\nwe find that imbalanced datasets increase vulnerability in minority classes,\nbut DP mitigates this issue. Datasets with fewer classes improve both model\nutility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR)\ndatasets deteriorate the utility-privacy trade-off. These insights offer\nvaluable guidance for practitioners and researchers in estimating and\noptimizing the utility-privacy trade-off in image datasets, helping to inform\ndata and privacy modifications for better outcomes based on dataset\ncharacteristics.",
    "updated" : "2024-09-02T15:30:27Z",
    "published" : "2024-09-02T15:30:27Z",
    "authors" : [
      {
        "name" : "Lucas Lange"
      },
      {
        "name" : "Maurice-Maximilian Heykeroth"
      },
      {
        "name" : "Erhard Rahm"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01088v1",
    "title" : "Towards Split Learning-based Privacy-Preserving Record Linkage",
    "summary" : "Split Learning has been recently introduced to facilitate applications where\nuser data privacy is a requirement. However, it has not been thoroughly studied\nin the context of Privacy-Preserving Record Linkage, a problem in which the\nsame real-world entity should be identified among databases from different\ndataholders, but without disclosing any additional information. In this paper,\nwe investigate the potentials of Split Learning for Privacy-Preserving Record\nMatching, by introducing a novel training method through the utilization of\nReference Sets, which are publicly available data corpora, showcasing minimal\nmatching impact against a traditional centralized SVM-based technique.",
    "updated" : "2024-09-02T09:17:05Z",
    "published" : "2024-09-02T09:17:05Z",
    "authors" : [
      {
        "name" : "Michail Zervas"
      },
      {
        "name" : "Alexandros Karakasidis"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.00974v1",
    "title" : "Enhancing Privacy in Federated Learning: Secure Aggregation for\n  Real-World Healthcare Applications",
    "summary" : "Deploying federated learning (FL) in real-world scenarios, particularly in\nhealthcare, poses challenges in communication and security. In particular, with\nrespect to the federated aggregation procedure, researchers have been focusing\non the study of secure aggregation (SA) schemes to provide privacy guarantees\nover the model's parameters transmitted by the clients. Nevertheless, the\npractical availability of SA in currently available FL frameworks is currently\nlimited, due to computational and communication bottlenecks. To fill this gap,\nthis study explores the implementation of SA within the open-source Fed-BioMed\nframework. We implement and compare two SA protocols, Joye-Libert (JL) and Low\nOverhead Masking (LOM), by providing extensive benchmarks in a panel of\nhealthcare data analysis problems. Our theoretical and experimental evaluations\non four datasets demonstrate that SA protocols effectively protect privacy\nwhile maintaining task accuracy. Computational overhead during training is less\nthan 1% on a CPU and less than 50% on a GPU for large models, with protection\nphases taking less than 10 seconds. Incorporating SA into Fed-BioMed impacts\ntask accuracy by no more than 2% compared to non-SA scenarios. Overall this\nstudy demonstrates the feasibility of SA in real-world healthcare applications\nand contributes in reducing the gap towards the adoption of privacy-preserving\ntechnologies in sensitive applications.",
    "updated" : "2024-09-02T06:43:22Z",
    "published" : "2024-09-02T06:43:22Z",
    "authors" : [
      {
        "name" : "Riccardo Taiello"
      },
      {
        "name" : "Sergen Cansiz"
      },
      {
        "name" : "Marc Vesin"
      },
      {
        "name" : "Francesco Cremonesi"
      },
      {
        "name" : "Lucia Innocenti"
      },
      {
        "name" : "Melek Önen"
      },
      {
        "name" : "Marco Lorenzi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.00740v1",
    "title" : "VPVet: Vetting Privacy Policies of Virtual Reality Apps",
    "summary" : "Virtual reality (VR) apps can harvest a wider range of user data than\nweb/mobile apps running on personal computers or smartphones. Existing law and\nprivacy regulations emphasize that VR developers should inform users of what\ndata are collected/used/shared (CUS) through privacy policies. However, privacy\npolicies in the VR ecosystem are still in their early stages, and many\ndevelopers fail to write appropriate privacy policies that comply with\nregulations and meet user expectations. In this paper, we propose VPVet to\nautomatically vet privacy policy compliance issues for VR apps. VPVet first\nanalyzes the availability and completeness of a VR privacy policy and then\nrefines its analysis based on three key criteria: granularity, minimization,\nand consistency of CUS statements. Our study establishes the first and\ncurrently largest VR privacy policy dataset named VRPP, consisting of privacy\npolicies of 11,923 different VR apps from 10 mainstream platforms. Our vetting\nresults reveal severe privacy issues within the VR ecosystem, including the\nlimited availability and poor quality of privacy policies, along with their\ncoarse granularity, lack of adaptation to VR traits and the inconsistency\nbetween CUS statements in privacy policies and their actual behaviors. We\nopen-source VPVet system along with our findings at repository\nhttps://github.com/kalamoo/PPAudit, aiming to raise awareness within the VR\ncommunity and pave the way for further research in this field.",
    "updated" : "2024-09-01T15:07:11Z",
    "published" : "2024-09-01T15:07:11Z",
    "authors" : [
      {
        "name" : "Yuxia Zhan"
      },
      {
        "name" : "Yan Meng"
      },
      {
        "name" : "Lu Zhou"
      },
      {
        "name" : "Yichang Xiong"
      },
      {
        "name" : "Xiaokuan Zhang"
      },
      {
        "name" : "Lichuan Ma"
      },
      {
        "name" : "Guoxing Chen"
      },
      {
        "name" : "Qingqi Pei"
      },
      {
        "name" : "Haojin Zhu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.00739v1",
    "title" : "Designing and Evaluating Scalable Privacy Awareness and Control User\n  Interfaces for Mixed Reality",
    "summary" : "As Mixed Reality (MR) devices become increasingly popular across industries,\nthey raise significant privacy and ethical concerns due to their capacity to\ncollect extensive data on users and their environments. This paper highlights\nthe urgent need for privacy-aware user interfaces that educate and empower both\nusers and bystanders, enabling them to understand, control, and manage data\ncollection and sharing. Key research questions include improving user awareness\nof privacy implications, developing usable privacy controls, and evaluating the\neffectiveness of these measures in real-world settings. The proposed research\nroadmap aims to embed privacy considerations into the design and development of\nMR technologies, promoting responsible innovation that safeguards user privacy\nwhile preserving the functionality and appeal of these emerging technologies.",
    "updated" : "2024-09-01T15:06:40Z",
    "published" : "2024-09-01T15:06:40Z",
    "authors" : [
      {
        "name" : "Marvin Strauss"
      },
      {
        "name" : "Viktorija Paneva"
      },
      {
        "name" : "Florian Alt"
      },
      {
        "name" : "Stefan Schneegass"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02715v1",
    "title" : "Recoverable Anonymization for Pose Estimation: A Privacy-Enhancing\n  Approach",
    "summary" : "Human pose estimation (HPE) is crucial for various applications. However,\ndeploying HPE algorithms in surveillance contexts raises significant privacy\nconcerns due to the potential leakage of sensitive personal information (SPI)\nsuch as facial features, and ethnicity. Existing privacy-enhancing methods\noften compromise either privacy or performance, or they require costly\nadditional modalities. We propose a novel privacy-enhancing system that\ngenerates privacy-enhanced portraits while maintaining high HPE performance.\nOur key innovations include the reversible recovery of SPI for authorized\npersonnel and the preservation of contextual information. By jointly optimizing\na privacy-enhancing module, a privacy recovery module, and a pose estimator,\nour system ensures robust privacy protection, efficient SPI recovery, and\nhigh-performance HPE. Experimental results demonstrate the system's robust\nperformance in privacy enhancement, SPI recovery, and HPE.",
    "updated" : "2024-09-01T05:58:00Z",
    "published" : "2024-09-01T05:58:00Z",
    "authors" : [
      {
        "name" : "Wenjun Huang"
      },
      {
        "name" : "Yang Ni"
      },
      {
        "name" : "Arghavan Rezvani"
      },
      {
        "name" : "SungHeon Jeong"
      },
      {
        "name" : "Hanning Chen"
      },
      {
        "name" : "Yezi Liu"
      },
      {
        "name" : "Fei Wen"
      },
      {
        "name" : "Mohsen Imani"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03707v1",
    "title" : "A Different Level Text Protection Mechanism With Differential Privacy",
    "summary" : "The article introduces a method for extracting words of different degrees of\nimportance based on the BERT pre-training model and proves the effectiveness of\nthis method. The article also discusses the impact of maintaining the same\nperturbation results for words of different importance on the overall text\nutility. This method can be applied to long text protection.",
    "updated" : "2024-09-05T17:13:38Z",
    "published" : "2024-09-05T17:13:38Z",
    "authors" : [
      {
        "name" : "Qingwen Fu"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03655v1",
    "title" : "Privacy versus Emotion Preservation Trade-offs in Emotion-Preserving\n  Speaker Anonymization",
    "summary" : "Advances in speech technology now allow unprecedented access to personally\nidentifiable information through speech. To protect such information, the\ndifferential privacy field has explored ways to anonymize speech while\npreserving its utility, including linguistic and paralinguistic aspects.\nHowever, anonymizing speech while maintaining emotional state remains\nchallenging. We explore this problem in the context of the VoicePrivacy 2024\nchallenge. Specifically, we developed various speaker anonymization pipelines\nand find that approaches either excel at anonymization or preserving emotion\nstate, but not both simultaneously. Achieving both would require an in-domain\nemotion recognizer. Additionally, we found that it is feasible to train a\nsemi-effective speaker verification system using only emotion representations,\ndemonstrating the challenge of separating these two modalities.",
    "updated" : "2024-09-05T16:10:31Z",
    "published" : "2024-09-05T16:10:31Z",
    "authors" : [
      {
        "name" : "Zexin Cai"
      },
      {
        "name" : "Henry Li Xinyuan"
      },
      {
        "name" : "Ashi Garg"
      },
      {
        "name" : "Leibny Paola García-Perera"
      },
      {
        "name" : "Kevin Duh"
      },
      {
        "name" : "Sanjeev Khudanpur"
      },
      {
        "name" : "Nicholas Andrews"
      },
      {
        "name" : "Matthew Wiesner"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03568v1",
    "title" : "Enabling Practical and Privacy-Preserving Image Processing",
    "summary" : "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
    "updated" : "2024-09-05T14:22:02Z",
    "published" : "2024-09-05T14:22:02Z",
    "authors" : [
      {
        "name" : "Chao Wang"
      },
      {
        "name" : "Shubing Yang"
      },
      {
        "name" : "Xiaoyan Sun"
      },
      {
        "name" : "Jun Dai"
      },
      {
        "name" : "Dongfang Zhao"
      }
    ],
    "categories" : [
      "cs.CR",
      "C.2.0; K.6.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03344v1",
    "title" : "Rethinking Improved Privacy-Utility Trade-off with Pre-existing\n  Knowledge for DP Training",
    "summary" : "Differential privacy (DP) provides a provable framework for protecting\nindividuals by customizing a random mechanism over a privacy-sensitive dataset.\nDeep learning models have demonstrated privacy risks in model exposure as an\nestablished learning model unintentionally records membership-level privacy\nleakage. Differentially private stochastic gradient descent (DP- SGD) has been\nproposed to safeguard training individuals by adding random Gaussian noise to\ngradient updates in the backpropagation. Researchers identify that DP-SGD\ntypically causes utility loss since the injected homogeneous noise alters the\ngradient updates calculated at each iteration. Namely, all elements in the\ngradient are contaminated regardless of their importance in updating model\nparameters. In this work, we argue that the utility loss mainly results from\nthe homogeneity of injected noise. Consequently, we propose a generic\ndifferential privacy framework with heterogeneous noise (DP-Hero) by defining a\nheterogeneous random mechanism to abstract its property. The insight of DP-Hero\nis to leverage the knowledge encoded in the previously trained model to guide\nthe subsequent allocation of noise heterogeneity, thereby leveraging the\nstatistical perturbation and achieving enhanced utility. Atop DP-Hero, we\ninstantiate a heterogeneous version of DP-SGD, where the noise injected into\ngradients is heterogeneous and guided by prior-established model parameters. We\nconduct comprehensive experiments to verify and explain the effectiveness of\nthe proposed DP-Hero, showing improved training accuracy compared with\nstate-of-the-art works. Broadly, we shed light on improving the privacy-utility\nspace by learning the noise guidance from the pre-existing leaked knowledge\nencoded in the previously trained model, showing a different perspective of\nunderstanding the utility-improved DP training.",
    "updated" : "2024-09-05T08:40:54Z",
    "published" : "2024-09-05T08:40:54Z",
    "authors" : [
      {
        "name" : "Yu Zheng"
      },
      {
        "name" : "Wenchao Zhang"
      },
      {
        "name" : "Yonggang Zhang"
      },
      {
        "name" : "Wei Song"
      },
      {
        "name" : "Kai Zhou"
      },
      {
        "name" : "Bo Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03326v1",
    "title" : "Enhancing User-Centric Privacy Protection: An Interactive Framework\n  through Diffusion Models and Machine Unlearning",
    "summary" : "In the realm of multimedia data analysis, the extensive use of image datasets\nhas escalated concerns over privacy protection within such data. Current\nresearch predominantly focuses on privacy protection either in data sharing or\nupon the release of trained machine learning models. Our study pioneers a\ncomprehensive privacy protection framework that safeguards image data privacy\nconcurrently during data sharing and model publication. We propose an\ninteractive image privacy protection framework that utilizes generative machine\nlearning models to modify image information at the attribute level and employs\nmachine unlearning algorithms for the privacy preservation of model parameters.\nThis user-interactive framework allows for adjustments in privacy protection\nintensity based on user feedback on generated images, striking a balance\nbetween maximal privacy safeguarding and maintaining model performance. Within\nthis framework, we instantiate two modules: a differential privacy diffusion\nmodel for protecting attribute information in images and a feature unlearning\nalgorithm for efficient updates of the trained model on the revised image\ndataset. Our approach demonstrated superiority over existing methods on facial\ndatasets across various attribute classifications.",
    "updated" : "2024-09-05T07:55:55Z",
    "published" : "2024-09-05T07:55:55Z",
    "authors" : [
      {
        "name" : "Huaxi Huang"
      },
      {
        "name" : "Xin Yuan"
      },
      {
        "name" : "Qiyu Liao"
      },
      {
        "name" : "Dadong Wang"
      },
      {
        "name" : "Tongliang Liu"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03294v1",
    "title" : "Federated Prototype-based Contrastive Learning for Privacy-Preserving\n  Cross-domain Recommendation",
    "summary" : "Cross-domain recommendation (CDR) aims to improve recommendation accuracy in\nsparse domains by transferring knowledge from data-rich domains. However,\nexisting CDR methods often assume the availability of user-item interaction\ndata across domains, overlooking user privacy concerns. Furthermore, these\nmethods suffer from performance degradation in scenarios with sparse\noverlapping users, as they typically depend on a large number of fully shared\nusers for effective knowledge transfer. To address these challenges, we propose\na Federated Prototype-based Contrastive Learning (CL) method for\nPrivacy-Preserving CDR, named FedPCL-CDR. This approach utilizes\nnon-overlapping user information and prototypes to improve multi-domain\nperformance while protecting user privacy. FedPCL-CDR comprises two modules:\nlocal domain (client) learning and global server aggregation. In the local\ndomain, FedPCL-CDR clusters all user data to learn representative prototypes,\neffectively utilizing non-overlapping user information and addressing the\nsparse overlapping user issue. It then facilitates knowledge transfer by\nemploying both local and global prototypes returned from the server in a CL\nmanner. Simultaneously, the global server aggregates representative prototypes\nfrom local domains to learn both local and global prototypes. The combination\nof prototypes and federated learning (FL) ensures that sensitive user data\nremains decentralized, with only prototypes being shared across domains,\nthereby protecting user privacy. Extensive experiments on four CDR tasks using\ntwo real-world datasets demonstrate that FedPCL-CDR outperforms the\nstate-of-the-art baselines.",
    "updated" : "2024-09-05T06:59:56Z",
    "published" : "2024-09-05T06:59:56Z",
    "authors" : [
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Quangui Zhang"
      },
      {
        "name" : "Lei Sang"
      },
      {
        "name" : "Qiang Wu"
      },
      {
        "name" : "Min Xu"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04366v1",
    "title" : "Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue",
    "summary" : "Many blockchain networks aim to preserve the anonymity of validators in the\npeer-to-peer (P2P) network, ensuring that no adversary can link a validator's\nidentifier to the IP address of a peer due to associated privacy and security\nconcerns. This work demonstrates that the Ethereum P2P network does not offer\nthis anonymity. We present a methodology that enables any node in the network\nto identify validators hosted on connected peers and empirically verify the\nfeasibility of our proposed method. Using data collected from four nodes over\nthree days, we locate more than 15% of Ethereum validators in the P2P network.\nThe insights gained from our deanonymization technique provide valuable\ninformation on the distribution of validators across peers, their geographic\nlocations, and hosting organizations. We further discuss the implications and\nrisks associated with the lack of anonymity in the P2P network and propose\nmethods to help validators protect their privacy. The Ethereum Foundation has\nawarded us a bug bounty, acknowledging the impact of our results.",
    "updated" : "2024-09-06T15:57:43Z",
    "published" : "2024-09-06T15:57:43Z",
    "authors" : [
      {
        "name" : "Lioba Heimbach"
      },
      {
        "name" : "Yann Vonlanthen"
      },
      {
        "name" : "Juan Villacis"
      },
      {
        "name" : "Lucianna Kiffer"
      },
      {
        "name" : "Roger Wattenhofer"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04257v1",
    "title" : "Privacy risk from synthetic data: practical proposals",
    "summary" : "This paper proposes and compares measures of identity and attribute\ndisclosure risk for synthetic data. Data custodians can use the methods\nproposed here to inform the decision as to whether to release synthetic\nversions of confidential data. Different measures are evaluated on two data\nsets. Insight into the measures is obtained by examining the details of the\nrecords identified as posing a disclosure risk. This leads to methods to\nidentify, and possibly exclude, apparently risky records where the\nidentification or attribution would be expected by someone with background\nknowledge of the data. The methods described are available as part of the\n\\textbf{synthpop} package for \\textbf{R}.",
    "updated" : "2024-09-06T13:10:40Z",
    "published" : "2024-09-06T13:10:40Z",
    "authors" : [
      {
        "name" : "Gillian M Raab"
      }
    ],
    "categories" : [
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04194v1",
    "title" : "Towards Privacy-Preserving Relational Data Synthesis via Probabilistic\n  Relational Models",
    "summary" : "Probabilistic relational models provide a well-established formalism to\ncombine first-order logic and probabilistic models, thereby allowing to\nrepresent relationships between objects in a relational domain. At the same\ntime, the field of artificial intelligence requires increasingly large amounts\nof relational training data for various machine learning tasks. Collecting\nreal-world data, however, is often challenging due to privacy concerns, data\nprotection regulations, high costs, and so on. To mitigate these challenges,\nthe generation of synthetic data is a promising approach. In this paper, we\nsolve the problem of generating synthetic relational data via probabilistic\nrelational models. In particular, we propose a fully-fledged pipeline to go\nfrom relational database to probabilistic relational model, which can then be\nused to sample new synthetic relational data points from its underlying\nprobability distribution. As part of our proposed pipeline, we introduce a\nlearning algorithm to construct a probabilistic relational model from a given\nrelational database.",
    "updated" : "2024-09-06T11:24:25Z",
    "published" : "2024-09-06T11:24:25Z",
    "authors" : [
      {
        "name" : "Malte Luttermann"
      },
      {
        "name" : "Ralf Möller"
      },
      {
        "name" : "Mattis Hartwig"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04173v1",
    "title" : "NPU-NTU System for Voice Privacy 2024 Challenge",
    "summary" : "Speaker anonymization is an effective privacy protection solution that\nconceals the speaker's identity while preserving the linguistic content and\nparalinguistic information of the original speech. To establish a fair\nbenchmark and facilitate comparison of speaker anonymization systems, the\nVoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition\nplanned for 2024. In this paper, we describe our proposed speaker anonymization\nsystem for VPC 2024. Our system employs a disentangled neural codec\narchitecture and a serial disentanglement strategy to gradually disentangle the\nglobal speaker identity and time-variant linguistic content and paralinguistic\ninformation. We introduce multiple distillation methods to disentangle\nlinguistic content, speaker identity, and emotion. These methods include\nsemantic distillation, supervised speaker distillation, and frame-level emotion\ndistillation. Based on these distillations, we anonymize the original speaker\nidentity using a weighted sum of a set of candidate speaker identities and a\nrandomly generated speaker identity. Our system achieves the best trade-off of\nprivacy protection and emotion preservation in VPC 2024.",
    "updated" : "2024-09-06T10:32:42Z",
    "published" : "2024-09-06T10:32:42Z",
    "authors" : [
      {
        "name" : "Jixun Yao"
      },
      {
        "name" : "Nikita Kuzmin"
      },
      {
        "name" : "Qing Wang"
      },
      {
        "name" : "Pengcheng Guo"
      },
      {
        "name" : "Ziqian Ning"
      },
      {
        "name" : "Dake Guo"
      },
      {
        "name" : "Kong Aik Lee"
      },
      {
        "name" : "Eng-Siong Chng"
      },
      {
        "name" : "Lei Xie"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04167v1",
    "title" : "Do Android App Developers Accurately Report Collection of\n  Privacy-Related Data?",
    "summary" : "Many Android applications collect data from users. The European Union's\nGeneral Data Protection Regulation (GDPR) requires vendors to faithfully\ndisclose which data their apps collect. This task is complicated because many\napps use third-party code for which the same information is not readily\navailable. Hence we ask: how accurately do current Android apps fulfill these\nrequirements?\n  In this work, we first expose a multi-layered definition of privacy-related\ndata to correctly report data collection in Android apps. We further create a\ndataset of privacy-sensitive data classes that may be used as input by an\nAndroid app. This dataset takes into account data collected both through the\nuser interface and system APIs.\n  We manually examine the data safety sections of 70 Android apps to observe\nhow data collection is reported, identifying instances of over- and\nunder-reporting. Additionally, we develop a prototype to statically extract and\nlabel privacy-related data collected via app source code, user interfaces, and\npermissions. Comparing the prototype's results with the data safety sections of\n20 apps reveals reporting discrepancies. Using the results from two Messaging\nand Social Media apps (Signal and Instagram), we discuss how app developers\nunder-report and over-report data collection, respectively, and identify\ninaccurately reported data categories.\n  Our results show that app developers struggle to accurately report data\ncollection, either due to Google's abstract definition of collected data or\ninsufficient existing tool support.",
    "updated" : "2024-09-06T10:05:45Z",
    "published" : "2024-09-06T10:05:45Z",
    "authors" : [
      {
        "name" : "Mugdha Khedkar"
      },
      {
        "name" : "Ambuj Kumar Mondal"
      },
      {
        "name" : "Eric Bodden"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04048v1",
    "title" : "Exploring User Privacy Awareness on GitHub: An Empirical Study",
    "summary" : "GitHub provides developers with a practical way to distribute source code and\ncollaboratively work on common projects. To enhance account security and\nprivacy, GitHub allows its users to manage access permissions, review audit\nlogs, and enable two-factor authentication. However, despite the endless\neffort, the platform still faces various issues related to the privacy of its\nusers. This paper presents an empirical study delving into the GitHub\necosystem. Our focus is on investigating the utilization of privacy settings on\nthe platform and identifying various types of sensitive information disclosed\nby users. Leveraging a dataset comprising 6,132 developers, we report and\nanalyze their activities by means of comments on pull requests. Our findings\nindicate an active engagement by users with the available privacy settings on\nGitHub. Notably, we observe the disclosure of different forms of private\ninformation within pull request comments. This observation has prompted our\nexploration into sensitivity detection using a large language model and BERT,\nto pave the way for a personalized privacy assistant. Our work provides\ninsights into the utilization of existing privacy protection tools, such as\nprivacy settings, along with their inherent limitations. Essentially, we aim to\nadvance research in this field by providing both the motivation for creating\nsuch privacy protection tools and a proposed methodology for personalizing\nthem.",
    "updated" : "2024-09-06T06:41:46Z",
    "published" : "2024-09-06T06:41:46Z",
    "authors" : [
      {
        "name" : "Costanza Alfieri"
      },
      {
        "name" : "Juri Di Rocco"
      },
      {
        "name" : "Phuong T. Nguyen"
      },
      {
        "name" : "Paola Inverardi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04026v1",
    "title" : "Efficient Fault-Tolerant Quantum Protocol for Differential Privacy in\n  the Shuffle Model",
    "summary" : "We present a quantum protocol which securely and implicitly implements a\nrandom shuffle to realize differential privacy in the shuffle model. The\nshuffle model of differential privacy amplifies privacy achievable via local\ndifferential privacy by randomly permuting the tuple of outcomes from data\ncontributors. In practice, one needs to address how this shuffle is\nimplemented. Examples include implementing the shuffle via mix-networks, or\nshuffling via a trusted third-party. These implementation specific issues raise\nnon-trivial computational and trust requirements in a classical system. We\npropose a quantum version of the protocol using entanglement of quantum states\nand show that the shuffle can be implemented without these extra requirements.\nOur protocol implements k-ary randomized response, for any value of k > 2, and\nfurthermore, can be efficiently implemented using fault-tolerant computation.",
    "updated" : "2024-09-06T04:53:19Z",
    "published" : "2024-09-06T04:53:19Z",
    "authors" : [
      {
        "name" : "Hassan Jameel Asghar"
      },
      {
        "name" : "Arghya Mukherjee"
      },
      {
        "name" : "Gavin K. Brennen"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03796v1",
    "title" : "Protecting Activity Sensing Data Privacy Using Hierarchical Information\n  Dissociation",
    "summary" : "Smartphones and wearable devices have been integrated into our daily lives,\noffering personalized services. However, many apps become overprivileged as\ntheir collected sensing data contains unnecessary sensitive information. For\nexample, mobile sensing data could reveal private attributes (e.g., gender and\nage) and unintended sensitive features (e.g., hand gestures when entering\npasswords). To prevent sensitive information leakage, existing methods must\nobtain private labels and users need to specify privacy policies. However, they\nonly achieve limited control over information disclosure. In this work, we\npresent Hippo to dissociate hierarchical information including private metadata\nand multi-grained activity information from the sensing data. Hippo achieves\nfine-grained control over the disclosure of sensitive information without\nrequiring private labels. Specifically, we design a latent guidance-based\ndiffusion model, which generates multi-grained versions of raw sensor data\nconditioned on hierarchical latent activity features. Hippo enables users to\ncontrol the disclosure of sensitive information in sensing data, ensuring their\nprivacy while preserving the necessary features to meet the utility\nrequirements of applications. Hippo is the first unified model that achieves\ntwo goals: perturbing the sensitive attributes and controlling the disclosure\nof sensitive information in mobile sensing data. Extensive experiments show\nthat Hippo can anonymize personal attributes and transform activity information\nat various resolutions across different types of sensing data.",
    "updated" : "2024-09-04T15:38:00Z",
    "published" : "2024-09-04T15:38:00Z",
    "authors" : [
      {
        "name" : "Guangjing Wang"
      },
      {
        "name" : "Hanqing Guo"
      },
      {
        "name" : "Yuanda Wang"
      },
      {
        "name" : "Bocheng Chen"
      },
      {
        "name" : "Ce Zhou"
      },
      {
        "name" : "Qiben Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.05623v1",
    "title" : "A Framework for Differential Privacy Against Timing Attacks",
    "summary" : "The standard definition of differential privacy (DP) ensures that a\nmechanism's output distribution on adjacent datasets is indistinguishable.\nHowever, real-world implementations of DP can, and often do, reveal information\nthrough their runtime distributions, making them susceptible to timing attacks.\nIn this work, we establish a general framework for ensuring differential\nprivacy in the presence of timing side channels. We define a new notion of\ntiming privacy, which captures programs that remain differentially private to\nan adversary that observes the program's runtime in addition to the output. Our\nframework enables chaining together component programs that are timing-stable\nfollowed by a random delay to obtain DP programs that achieve timing privacy.\nImportantly, our definitions allow for measuring timing privacy and output\nprivacy using different privacy measures. We illustrate how to instantiate our\nframework by giving programs for standard DP computations in the RAM and Word\nRAM models of computation. Furthermore, we show how our framework can be\nrealized in code through a natural extension of the OpenDP Programming\nFramework.",
    "updated" : "2024-09-09T13:56:04Z",
    "published" : "2024-09-09T13:56:04Z",
    "authors" : [
      {
        "name" : "Zachary Ratliff"
      },
      {
        "name" : "Salil Vadhan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.05249v1",
    "title" : "NetDPSyn: Synthesizing Network Traces under Differential Privacy",
    "summary" : "As the utilization of network traces for the network measurement research\nbecomes increasingly prevalent, concerns regarding privacy leakage from network\ntraces have garnered the public's attention. To safeguard network traces,\nresearchers have proposed the trace synthesis that retains the essential\nproperties of the raw data. However, previous works also show that synthesis\ntraces with generative models are vulnerable under linkage attacks.\n  This paper introduces NetDPSyn, the first system to synthesize high-fidelity\nnetwork traces under privacy guarantees. NetDPSyn is built with the\nDifferential Privacy (DP) framework as its core, which is significantly\ndifferent from prior works that apply DP when training the generative model.\nThe experiments conducted on three flow and two packet datasets indicate that\nNetDPSyn achieves much better data utility in downstream tasks like anomaly\ndetection. NetDPSyn is also 2.5 times faster than the other methods on average\nin data synthesis.",
    "updated" : "2024-09-08T23:54:00Z",
    "published" : "2024-09-08T23:54:00Z",
    "authors" : [
      {
        "name" : "Danyu Sun"
      },
      {
        "name" : "Joann Qiongna Chen"
      },
      {
        "name" : "Chen Gong"
      },
      {
        "name" : "Tianhao Wang"
      },
      {
        "name" : "Zhou Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04877v1",
    "title" : "Strong Privacy-Preserving Universally Composable AKA Protocol with\n  Seamless Handover Support for Mobile Virtual Network Operator",
    "summary" : "Consumers seeking a new mobile plan have many choices in the present mobile\nlandscape. The Mobile Virtual Network Operator (MVNO) has recently gained\nconsiderable attention among these options. MVNOs offer various benefits,\nmaking them an appealing choice for a majority of consumers. These advantages\nencompass flexibility, access to cutting-edge technologies, enhanced coverage,\nsuperior customer service, and substantial cost savings. Even though MVNO\noffers several advantages, it also creates some security and privacy concerns\nfor the customer simultaneously. For instance, in the existing solution, MVNO\nneeds to hand over all the sensitive details, including the users' identities\nand master secret keys of their customers, to a mobile operator (MNO) to\nvalidate the customers while offering any services. This allows MNOs to have\nunrestricted access to the MVNO subscribers' location and mobile data,\nincluding voice calls, SMS, and Internet, which the MNOs frequently sell to\nthird parties (e.g., advertisement companies and surveillance agencies) for\nmore profit. Although critical for mass users, such privacy loss has been\nhistorically ignored due to the lack of practical and privacy-preserving\nsolutions for registration and handover procedures in cellular networks. In\nthis paper, we propose a universally composable authentication and handover\nscheme with strong user privacy support, where each MVNO user can validate a\nmobile operator (MNO) and vice-versa without compromising user anonymity and\nunlinkability support. Here, we anticipate that our proposed solution will most\nlikely be deployed by the MVNO(s) to ensure enhanced privacy support to their\ncustomer(s).",
    "updated" : "2024-09-07T18:04:54Z",
    "published" : "2024-09-07T18:04:54Z",
    "authors" : [
      {
        "name" : "Rabiah Alnashwan"
      },
      {
        "name" : "Yang Yang"
      },
      {
        "name" : "Yilu Dong"
      },
      {
        "name" : "Prosanta Gope"
      },
      {
        "name" : "Behzad Abdolmaleki"
      },
      {
        "name" : "Syed Rafiul Hussain"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04716v1",
    "title" : "Privacy enhanced collaborative inference in the Cox proportional hazards\n  model for distributed data",
    "summary" : "Data sharing barriers are paramount challenges arising from multicenter\nclinical studies where multiple data sources are stored in a distributed\nfashion at different local study sites. Particularly in the case of\ntime-to-event analysis when global risk sets are needed for the Cox\nproportional hazards model, access to a centralized database is typically\nnecessary. Merging such data sources into a common data storage for a\ncentralized statistical analysis requires a data use agreement, which is often\ntime-consuming. Furthermore, the construction and distribution of risk sets to\nparticipating clinical centers for subsequent calculations may pose a risk of\nrevealing individual-level information. We propose a new collaborative Cox\nmodel that eliminates the need for accessing the centralized database and\nconstructing global risk sets but needs only the sharing of summary statistics\nwith significantly smaller dimensions than risk sets. Thus, the proposed\ncollaborative inference enjoys maximal protection of data privacy. We show\ntheoretically and numerically that the new distributed proportional hazards\nmodel approach has little loss of statistical power when compared to the\ncentralized method that requires merging the entire data. We present a\nrenewable sieve method to establish large-sample properties for the proposed\nmethod. We illustrate its performance through simulation experiments and a\nreal-world data example from patients with kidney transplantation in the Organ\nProcurement and Transplantation Network (OPTN) to understand the factors\nassociated with the 5-year death-censored graft failure (DCGF) for patients who\nunderwent kidney transplants in the US.",
    "updated" : "2024-09-07T05:32:34Z",
    "published" : "2024-09-07T05:32:34Z",
    "authors" : [
      {
        "name" : "Mengtong Hu"
      },
      {
        "name" : "Xu Shi"
      },
      {
        "name" : "Peter X. -K. Song"
      }
    ],
    "categories" : [
      "stat.AP",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04652v1",
    "title" : "Privacy-Preserving Race/Ethnicity Estimation for Algorithmic Bias\n  Measurement in the U.S",
    "summary" : "AI fairness measurements, including tests for equal treatment, often take the\nform of disaggregated evaluations of AI systems. Such measurements are an\nimportant part of Responsible AI operations. These measurements compare system\nperformance across demographic groups or sub-populations and typically require\nmember-level demographic signals such as gender, race, ethnicity, and location.\nHowever, sensitive member-level demographic attributes like race and ethnicity\ncan be challenging to obtain and use due to platform choices, legal\nconstraints, and cultural norms. In this paper, we focus on the task of\nenabling AI fairness measurements on race/ethnicity for \\emph{U.S. LinkedIn\nmembers} in a privacy-preserving manner. We present the Privacy-Preserving\nProbabilistic Race/Ethnicity Estimation (PPRE) method for performing this task.\nPPRE combines the Bayesian Improved Surname Geocoding (BISG) model, a sparse\nLinkedIn survey sample of self-reported demographics, and privacy-enhancing\ntechnologies like secure two-party computation and differential privacy to\nenable meaningful fairness measurements while preserving member privacy. We\nprovide details of the PPRE method and its privacy guarantees. We then\nillustrate sample measurement operations. We conclude with a review of open\nresearch and engineering challenges for expanding our privacy-preserving\nfairness measurement capabilities.",
    "updated" : "2024-09-06T23:29:18Z",
    "published" : "2024-09-06T23:29:18Z",
    "authors" : [
      {
        "name" : "Saikrishna Badrinarayanan"
      },
      {
        "name" : "Osonde Osoba"
      },
      {
        "name" : "Miao Cheng"
      },
      {
        "name" : "Ryan Rogers"
      },
      {
        "name" : "Sakshi Jain"
      },
      {
        "name" : "Rahul Tandra"
      },
      {
        "name" : "Natesh S. Pillai"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06564v1",
    "title" : "Advancing Android Privacy Assessments with Automation",
    "summary" : "Android apps collecting data from users must comply with legal frameworks to\nensure data protection. This requirement has become even more important since\nthe implementation of the General Data Protection Regulation (GDPR) by the\nEuropean Union in 2018. Moreover, with the proposed Cyber Resilience Act on the\nhorizon, stakeholders will soon need to assess software against even more\nstringent security and privacy standards. Effective privacy assessments require\ncollaboration among groups with diverse expertise to function effectively as a\ncohesive unit.\n  This paper motivates the need for an automated approach that enhances\nunderstanding of data protection in Android apps and improves communication\nbetween the various parties involved in privacy assessments. We propose the\nAssessor View, a tool designed to bridge the knowledge gap between these\nparties, facilitating more effective privacy assessments of Android\napplications.",
    "updated" : "2024-09-10T14:56:51Z",
    "published" : "2024-09-10T14:56:51Z",
    "authors" : [
      {
        "name" : "Mugdha Khedkar"
      },
      {
        "name" : "Michael Schlichtig"
      },
      {
        "name" : "Eric Bodden"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06455v1",
    "title" : "Continual Domain Incremental Learning for Privacy-aware Digital\n  Pathology",
    "summary" : "In recent years, there has been remarkable progress in the field of digital\npathology, driven by the ability to model complex tissue patterns using\nadvanced deep-learning algorithms. However, the robustness of these models is\noften severely compromised in the presence of data shifts (e.g., different\nstains, organs, centers, etc.). Alternatively, continual learning (CL)\ntechniques aim to reduce the forgetting of past data when learning new data\nwith distributional shift conditions. Specifically, rehearsal-based CL\ntechniques, which store some past data in a buffer and then replay it with new\ndata, have proven effective in medical image analysis tasks. However, privacy\nconcerns arise as these approaches store past data, prompting the development\nof our novel Generative Latent Replay-based CL (GLRCL) approach. GLRCL captures\nthe previous distribution through Gaussian Mixture Models instead of storing\npast samples, which are then utilized to generate features and perform latent\nreplay with new data. We systematically evaluate our proposed framework under\ndifferent shift conditions in histopathology data, including stain and organ\nshift. Our approach significantly outperforms popular buffer-free CL approaches\nand performs similarly to rehearsal-based CL approaches that require large\nbuffers causing serious privacy violations.",
    "updated" : "2024-09-10T12:21:54Z",
    "published" : "2024-09-10T12:21:54Z",
    "authors" : [
      {
        "name" : "Pratibha Kumari"
      },
      {
        "name" : "Daniel Reisenbüchler"
      },
      {
        "name" : "Lucas Luttner"
      },
      {
        "name" : "Nadine S. Schaadt"
      },
      {
        "name" : "Friedrich Feuerhake"
      },
      {
        "name" : "Dorit Merhof"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06422v1",
    "title" : "A Pervasive, Efficient and Private Future: Realizing Privacy-Preserving\n  Machine Learning Through Hybrid Homomorphic Encryption",
    "summary" : "Machine Learning (ML) has become one of the most impactful fields of data\nscience in recent years. However, a significant concern with ML is its privacy\nrisks due to rising attacks against ML models. Privacy-Preserving Machine\nLearning (PPML) methods have been proposed to mitigate the privacy and security\nrisks of ML models. A popular approach to achieving PPML uses Homomorphic\nEncryption (HE). However, the highly publicized inefficiencies of HE make it\nunsuitable for highly scalable scenarios with resource-constrained devices.\nHence, Hybrid Homomorphic Encryption (HHE) -- a modern encryption scheme that\ncombines symmetric cryptography with HE -- has recently been introduced to\novercome these challenges. HHE potentially provides a foundation to build new\nefficient and privacy-preserving services that transfer expensive HE operations\nto the cloud. This work introduces HHE to the ML field by proposing\nresource-friendly PPML protocols for edge devices. More precisely, we utilize\nHHE as the primary building block of our PPML protocols. We assess the\nperformance of our protocols by first extensively evaluating each party's\ncommunication and computational cost on a dummy dataset and show the efficiency\nof our protocols by comparing them with similar protocols implemented using\nplain BFV. Subsequently, we demonstrate the real-world applicability of our\nconstruction by building an actual PPML application that uses HHE as its\nfoundation to classify heart disease based on sensitive ECG data.",
    "updated" : "2024-09-10T11:04:14Z",
    "published" : "2024-09-10T11:04:14Z",
    "authors" : [
      {
        "name" : "Khoa Nguyen"
      },
      {
        "name" : "Mindaugas Budzys"
      },
      {
        "name" : "Eugene Frimpong"
      },
      {
        "name" : "Tanveer Khan"
      },
      {
        "name" : "Antonis Michalas"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06360v1",
    "title" : "SoK: Evaluating 5G Protocols Against Legacy and Emerging Privacy and\n  Security Attacks",
    "summary" : "Ensuring user privacy remains a critical concern within mobile cellular\nnetworks, particularly given the proliferation of interconnected devices and\nservices. In fact, a lot of user privacy issues have been raised in 2G, 3G,\n4G/LTE networks. Recognizing this general concern, 3GPP has prioritized\naddressing these issues in the development of 5G, implementing numerous\nmodifications to enhance user privacy since 5G Release 15. In this\nsystematization of knowledge paper, we first provide a framework for studying\nprivacy and security related attacks in cellular networks, setting as privacy\nobjective the User Identity Confidentiality defined in 3GPP standards. Using\nthis framework, we discuss existing privacy and security attacks in pre-5G\nnetworks, analyzing the weaknesses that lead to these attacks. Furthermore, we\nthoroughly study the security characteristics of 5G up to the new Release 19,\nand examine mitigation mechanisms of 5G to the identified pre-5G attacks.\nAfterwards, we analyze how recent 5G attacks try to overcome these mitigation\nmechanisms. Finally, we identify current limitations and open problems in\nsecurity of 5G, and propose directions for future work.",
    "updated" : "2024-09-10T09:30:37Z",
    "published" : "2024-09-10T09:30:37Z",
    "authors" : [
      {
        "name" : "Stavros Eleftherakis"
      },
      {
        "name" : "Domenico Giustiniano"
      },
      {
        "name" : "Nicolas Kourtellis"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06233v1",
    "title" : "VBIT: Towards Enhancing Privacy Control Over IoT Devices",
    "summary" : "Internet-of-Things (IoT) devices are increasingly deployed at home, at work,\nand in other shared and public spaces. IoT devices collect and share data with\nservice providers and third parties, which poses privacy concerns. Although\nprivacy enhancing tools are quite advanced in other applications domains (\\eg~\nadvertising and tracker blockers for browsers), users have currently no\nconvenient way to know or manage what and how data is collected and shared by\nIoT devices. In this paper, we present VBIT, an interactive system combining\nMixed Reality (MR) and web-based applications that allows users to: (1) uncover\nand visualize tracking services by IoT devices in an instrumented space and (2)\ntake action to stop or limit that tracking. We design and implement VBIT to\noperate at the network traffic level, and we show that it has negligible\nperformance overhead, and offers flexibility and good usability. We perform a\nmixed-method user study consisting of an online survey and an in-person\ninterview study. We show that VBIT users appreciate VBIT's transparency,\ncontrol, and customization features, and they become significantly more willing\nto install an IoT advertising and tracking blocker, after using VBIT. In the\nprocess, we obtain design insights that can be used to further iterate and\nimprove the design of VBIT and other systems for IoT transparency and control.",
    "updated" : "2024-09-10T06:00:50Z",
    "published" : "2024-09-10T06:00:50Z",
    "authors" : [
      {
        "name" : "Jad Al Aaraj"
      },
      {
        "name" : "Olivia Figueira"
      },
      {
        "name" : "Tu Le"
      },
      {
        "name" : "Isabela Figueira"
      },
      {
        "name" : "Rahmadi Trimananda"
      },
      {
        "name" : "Athina Markopoulou"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06069v1",
    "title" : "Privacy-Preserving Data Linkage Across Private and Public Datasets for\n  Collaborative Agriculture Research",
    "summary" : "Digital agriculture leverages technology to enhance crop yield, disease\nresilience, and soil health, playing a critical role in agricultural research.\nHowever, it raises privacy concerns such as adverse pricing, price\ndiscrimination, higher insurance costs, and manipulation of resources,\ndeterring farm operators from sharing data due to potential misuse. This study\nintroduces a privacy-preserving framework that addresses these risks while\nallowing secure data sharing for digital agriculture. Our framework enables\ncomprehensive data analysis while protecting privacy. It allows stakeholders to\nharness research-driven policies that link public and private datasets. The\nproposed algorithm achieves this by: (1) identifying similar farmers based on\nprivate datasets, (2) providing aggregate information like time and location,\n(3) determining trends in price and product availability, and (4) correlating\ntrends with public policy data, such as food insecurity statistics. We validate\nthe framework with real-world Farmer's Market datasets, demonstrating its\nefficacy through machine learning models trained on linked privacy-preserved\ndata. The results support policymakers and researchers in addressing food\ninsecurity and pricing issues. This work significantly contributes to digital\nagriculture by providing a secure method for integrating and analyzing data,\ndriving advancements in agricultural technology and development.",
    "updated" : "2024-09-09T21:07:13Z",
    "published" : "2024-09-09T21:07:13Z",
    "authors" : [
      {
        "name" : "Osama Zafar"
      },
      {
        "name" : "Rosemarie Santa Gonzalez"
      },
      {
        "name" : "Gabriel Wilkins"
      },
      {
        "name" : "Alfonso Morales"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04048v2",
    "title" : "Exploring User Privacy Awareness on GitHub: An Empirical Study",
    "summary" : "GitHub provides developers with a practical way to distribute source code and\ncollaboratively work on common projects. To enhance account security and\nprivacy, GitHub allows its users to manage access permissions, review audit\nlogs, and enable two-factor authentication. However, despite the endless\neffort, the platform still faces various issues related to the privacy of its\nusers. This paper presents an empirical study delving into the GitHub\necosystem. Our focus is on investigating the utilization of privacy settings on\nthe platform and identifying various types of sensitive information disclosed\nby users. Leveraging a dataset comprising 6,132 developers, we report and\nanalyze their activities by means of comments on pull requests. Our findings\nindicate an active engagement by users with the available privacy settings on\nGitHub. Notably, we observe the disclosure of different forms of private\ninformation within pull request comments. This observation has prompted our\nexploration into sensitivity detection using a large language model and BERT,\nto pave the way for a personalized privacy assistant. Our work provides\ninsights into the utilization of existing privacy protection tools, such as\nprivacy settings, along with their inherent limitations. Essentially, we aim to\nadvance research in this field by providing both the motivation for creating\nsuch privacy protection tools and a proposed methodology for personalizing\nthem.",
    "updated" : "2024-09-10T09:35:53Z",
    "published" : "2024-09-06T06:41:46Z",
    "authors" : [
      {
        "name" : "Costanza Alfieri"
      },
      {
        "name" : "Juri Di Rocco"
      },
      {
        "name" : "Paola Inverardi"
      },
      {
        "name" : "Phuong T. Nguyen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07444v1",
    "title" : "Echoes of Privacy: Uncovering the Profiling Practices of Voice\n  Assistants",
    "summary" : "Many companies, including Google, Amazon, and Apple, offer voice assistants\nas a convenient solution for answering general voice queries and accessing\ntheir services. These voice assistants have gained popularity and can be easily\naccessed through various smart devices such as smartphones, smart speakers,\nsmartwatches, and an increasing array of other devices. However, this\nconvenience comes with potential privacy risks. For instance, while companies\nvaguely mention in their privacy policies that they may use voice interactions\nfor user profiling, it remains unclear to what extent this profiling occurs and\nwhether voice interactions pose greater privacy risks compared to other\ninteraction modalities.\n  In this paper, we conduct 1171 experiments involving a total of 24530 queries\nwith different personas and interaction modalities over the course of 20 months\nto characterize how the three most popular voice assistants profile their\nusers. We analyze factors such as the labels assigned to users, their accuracy,\nthe time taken to assign these labels, differences between voice and web\ninteractions, and the effectiveness of profiling remediation tools offered by\neach voice assistant. Our findings reveal that profiling can happen without\ninteraction, can be incorrect and inconsistent at times, may take several days\nto weeks for changes to occur, and can be influenced by the interaction\nmodality.",
    "updated" : "2024-09-11T17:44:41Z",
    "published" : "2024-09-11T17:44:41Z",
    "authors" : [
      {
        "name" : "Tina Khezresmaeilzadeh"
      },
      {
        "name" : "Elaine Zhu"
      },
      {
        "name" : "Kiersten Grieco"
      },
      {
        "name" : "Daniel J. Dubois"
      },
      {
        "name" : "Konstantinos Psounis"
      },
      {
        "name" : "David Choffnes"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07415v1",
    "title" : "SoK: Security and Privacy Risks of Medical AI",
    "summary" : "The integration of technology and healthcare has ushered in a new era where\nsoftware systems, powered by artificial intelligence and machine learning, have\nbecome essential components of medical products and services. While these\nadvancements hold great promise for enhancing patient care and healthcare\ndelivery efficiency, they also expose sensitive medical data and system\nintegrity to potential cyberattacks. This paper explores the security and\nprivacy threats posed by AI/ML applications in healthcare. Through a thorough\nexamination of existing research across a range of medical domains, we have\nidentified significant gaps in understanding the adversarial attacks targeting\nmedical AI systems. By outlining specific adversarial threat models for medical\nsettings and identifying vulnerable application domains, we lay the groundwork\nfor future research that investigates the security and resilience of AI-driven\nmedical systems. Through our analysis of different threat models and\nfeasibility studies on adversarial attacks in different medical domains, we\nprovide compelling insights into the pressing need for cybersecurity research\nin the rapidly evolving field of AI healthcare technology.",
    "updated" : "2024-09-11T16:59:58Z",
    "published" : "2024-09-11T16:59:58Z",
    "authors" : [
      {
        "name" : "Yuanhaur Chang"
      },
      {
        "name" : "Han Liu"
      },
      {
        "name" : "Evin Jaff"
      },
      {
        "name" : "Chenyang Lu"
      },
      {
        "name" : "Ning Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07224v1",
    "title" : "Analytic Class Incremental Learning for Sound Source Localization with\n  Privacy Protection",
    "summary" : "Sound Source Localization (SSL) enabling technology for applications such as\nsurveillance and robotics. While traditional Signal Processing (SP)-based SSL\nmethods provide analytic solutions under specific signal and noise assumptions,\nrecent Deep Learning (DL)-based methods have significantly outperformed them.\nHowever, their success depends on extensive training data and substantial\ncomputational resources. Moreover, they often rely on large-scale annotated\nspatial data and may struggle when adapting to evolving sound classes. To\nmitigate these challenges, we propose a novel Class Incremental Learning (CIL)\napproach, termed SSL-CIL, which avoids serious accuracy degradation due to\ncatastrophic forgetting by incrementally updating the DL-based SSL model\nthrough a closed-form analytic solution. In particular, data privacy is ensured\nsince the learning process does not revisit any historical data\n(exemplar-free), which is more suitable for smart home scenarios. Empirical\nresults in the public SSLR dataset demonstrate the superior performance of our\nproposal, achieving a localization accuracy of 90.9%, surpassing other\ncompetitive methods.",
    "updated" : "2024-09-11T12:31:07Z",
    "published" : "2024-09-11T12:31:07Z",
    "authors" : [
      {
        "name" : "Xinyuan Qian"
      },
      {
        "name" : "Xianghu Yue"
      },
      {
        "name" : "Jiadong Wang"
      },
      {
        "name" : "Huiping Zhuang"
      },
      {
        "name" : "Haizhou Li"
      }
    ],
    "categories" : [
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07187v1",
    "title" : "A Simple Linear Space Data Structure for ANN with Application in\n  Differential Privacy",
    "summary" : "Locality Sensitive Filters are known for offering a quasi-linear space data\nstructure with rigorous guarantees for the Approximate Near Neighbor search\nproblem. Building on Locality Sensitive Filters, we derive a simple data\nstructure for the Approximate Near Neighbor Counting problem under differential\nprivacy. Moreover, we provide a simple analysis leveraging a connection with\nconcomitant statistics and extreme value theory. Our approach achieves the same\nperformance as the recent findings of Andoni et al. (NeurIPS 2023) but with a\nmore straightforward method. As a side result, the paper provides a more\ncompact description and analysis of Locality Sensitive Filters for Approximate\nNear Neighbor Search under inner product similarity, improving a previous\nresult in Aum\\\"{u}ller et al. (TODS 2022).",
    "updated" : "2024-09-11T11:14:33Z",
    "published" : "2024-09-11T11:14:33Z",
    "authors" : [
      {
        "name" : "Martin Aumüller"
      },
      {
        "name" : "Fabrizio Boninsegna"
      },
      {
        "name" : "Francesco Silvestri"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06955v1",
    "title" : "Privacy-Preserving Federated Learning with Consistency via Knowledge\n  Distillation Using Conditional Generator",
    "summary" : "Federated Learning (FL) is gaining popularity as a distributed learning\nframework that only shares model parameters or gradient updates and keeps\nprivate data locally. However, FL is at risk of privacy leakage caused by\nprivacy inference attacks. And most existing privacy-preserving mechanisms in\nFL conflict with achieving high performance and efficiency. Therefore, we\npropose FedMD-CG, a novel FL method with highly competitive performance and\nhigh-level privacy preservation, which decouples each client's local model into\na feature extractor and a classifier, and utilizes a conditional generator\ninstead of the feature extractor to perform server-side model aggregation. To\nensure the consistency of local generators and classifiers, FedMD-CG leverages\nknowledge distillation to train local models and generators at both the latent\nfeature level and the logit level. Also, we construct additional classification\nlosses and design new diversity losses to enhance client-side training.\nFedMD-CG is robust to data heterogeneity and does not require training extra\ndiscriminators (like cGAN). We conduct extensive experiments on various image\nclassification tasks to validate the superiority of FedMD-CG.",
    "updated" : "2024-09-11T02:36:36Z",
    "published" : "2024-09-11T02:36:36Z",
    "authors" : [
      {
        "name" : "Kangyang Luo"
      },
      {
        "name" : "Shuai Wang"
      },
      {
        "name" : "Xiang Li"
      },
      {
        "name" : "Yunshi Lan"
      },
      {
        "name" : "Ming Gao"
      },
      {
        "name" : "Jinlong Shu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06233v1",
    "title" : "VBIT: Towards Enhancing Privacy Control Over IoT Devices",
    "summary" : "Internet-of-Things (IoT) devices are increasingly deployed at home, at work,\nand in other shared and public spaces. IoT devices collect and share data with\nservice providers and third parties, which poses privacy concerns. Although\nprivacy enhancing tools are quite advanced in other applications domains (\\eg~\nadvertising and tracker blockers for browsers), users have currently no\nconvenient way to know or manage what and how data is collected and shared by\nIoT devices. In this paper, we present VBIT, an interactive system combining\nMixed Reality (MR) and web-based applications that allows users to: (1) uncover\nand visualize tracking services by IoT devices in an instrumented space and (2)\ntake action to stop or limit that tracking. We design and implement VBIT to\noperate at the network traffic level, and we show that it has negligible\nperformance overhead, and offers flexibility and good usability. We perform a\nmixed-method user study consisting of an online survey and an in-person\ninterview study. We show that VBIT users appreciate VBIT's transparency,\ncontrol, and customization features, and they become significantly more willing\nto install an IoT advertising and tracking blocker, after using VBIT. In the\nprocess, we obtain design insights that can be used to further iterate and\nimprove the design of VBIT and other systems for IoT transparency and control.",
    "updated" : "2024-09-10T06:00:50Z",
    "published" : "2024-09-10T06:00:50Z",
    "authors" : [
      {
        "name" : "Jad Al Aaraj"
      },
      {
        "name" : "Olivia Figueira"
      },
      {
        "name" : "Tu Le"
      },
      {
        "name" : "Isabela Figueira"
      },
      {
        "name" : "Rahmadi Trimananda"
      },
      {
        "name" : "Athina Markopoulou"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06069v1",
    "title" : "Privacy-Preserving Data Linkage Across Private and Public Datasets for\n  Collaborative Agriculture Research",
    "summary" : "Digital agriculture leverages technology to enhance crop yield, disease\nresilience, and soil health, playing a critical role in agricultural research.\nHowever, it raises privacy concerns such as adverse pricing, price\ndiscrimination, higher insurance costs, and manipulation of resources,\ndeterring farm operators from sharing data due to potential misuse. This study\nintroduces a privacy-preserving framework that addresses these risks while\nallowing secure data sharing for digital agriculture. Our framework enables\ncomprehensive data analysis while protecting privacy. It allows stakeholders to\nharness research-driven policies that link public and private datasets. The\nproposed algorithm achieves this by: (1) identifying similar farmers based on\nprivate datasets, (2) providing aggregate information like time and location,\n(3) determining trends in price and product availability, and (4) correlating\ntrends with public policy data, such as food insecurity statistics. We validate\nthe framework with real-world Farmer's Market datasets, demonstrating its\nefficacy through machine learning models trained on linked privacy-preserved\ndata. The results support policymakers and researchers in addressing food\ninsecurity and pricing issues. This work significantly contributes to digital\nagriculture by providing a secure method for integrating and analyzing data,\ndriving advancements in agricultural technology and development.",
    "updated" : "2024-09-09T21:07:13Z",
    "published" : "2024-09-09T21:07:13Z",
    "authors" : [
      {
        "name" : "Osama Zafar"
      },
      {
        "name" : "Rosemarie Santa Gonzalez"
      },
      {
        "name" : "Gabriel Wilkins"
      },
      {
        "name" : "Alfonso Morales"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07997v1",
    "title" : "Privacy-preserving federated prediction of pain intensity change based\n  on multi-center survey data",
    "summary" : "Background: Patient-reported survey data are used to train prognostic models\naimed at improving healthcare. However, such data are typically available\nmulti-centric and, for privacy reasons, cannot easily be centralized in one\ndata repository. Models trained locally are less accurate, robust, and\ngeneralizable. We present and apply privacy-preserving federated machine\nlearning techniques for prognostic model building, where local survey data\nnever leaves the legally safe harbors of the medical centers. Methods: We used\ncentralized, local, and federated learning techniques on two healthcare\ndatasets (GLA:D data from the five health regions of Denmark and international\nSHARE data of 27 countries) to predict two different health outcomes. We\ncompared linear regression, random forest regression, and random forest\nclassification models trained on local data with those trained on the entire\ndata in a centralized and in a federated fashion. Results: In GLA:D data,\nfederated linear regression (R2 0.34, RMSE 18.2) and federated random forest\nregression (R2 0.34, RMSE 18.3) models outperform their local counterparts\n(i.e., R2 0.32, RMSE 18.6, R2 0.30, RMSE 18.8) with statistical significance.\nWe also found that centralized models (R2 0.34, RMSE 18.2, R2 0.32, RMSE 18.5,\nrespectively) did not perform significantly better than the federated models.\nIn SHARE, the federated model (AC 0.78, AUROC: 0.71) and centralized model (AC\n0.84, AUROC: 0.66) perform significantly better than the local models (AC:\n0.74, AUROC: 0.69). Conclusion: Federated learning enables the training of\nprognostic models from multi-center surveys without compromising privacy and\nwith only minimal or no compromise regarding model performance.",
    "updated" : "2024-09-12T12:41:58Z",
    "published" : "2024-09-12T12:41:58Z",
    "authors" : [
      {
        "name" : "Supratim Das"
      },
      {
        "name" : "Mahdie Rafie"
      },
      {
        "name" : "Paula Kammer"
      },
      {
        "name" : "Søren T. Skou"
      },
      {
        "name" : "Dorte T. Grønne"
      },
      {
        "name" : "Ewa M. Roos"
      },
      {
        "name" : "André Hajek"
      },
      {
        "name" : "Hans-Helmut König"
      },
      {
        "name" : "Md Shihab Ullaha"
      },
      {
        "name" : "Niklas Probul"
      },
      {
        "name" : "Jan Baumbacha"
      },
      {
        "name" : "Linda Baumbach"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07809v1",
    "title" : "Controllable Synthetic Clinical Note Generation with Privacy Guarantees",
    "summary" : "In the field of machine learning, domain-specific annotated data is an\ninvaluable resource for training effective models. However, in the medical\ndomain, this data often includes Personal Health Information (PHI), raising\nsignificant privacy concerns. The stringent regulations surrounding PHI limit\nthe availability and sharing of medical datasets, which poses a substantial\nchallenge for researchers and practitioners aiming to develop advanced machine\nlearning models. In this paper, we introduce a novel method to \"clone\" datasets\ncontaining PHI. Our approach ensures that the cloned datasets retain the\nessential characteristics and utility of the original data without compromising\npatient privacy. By leveraging differential-privacy techniques and a novel\nfine-tuning task, our method produces datasets that are free from identifiable\ninformation while preserving the statistical properties necessary for model\ntraining. We conduct utility testing to evaluate the performance of machine\nlearning models trained on the cloned datasets. The results demonstrate that\nour cloned datasets not only uphold privacy standards but also enhance model\nperformance compared to those trained on traditional anonymized datasets. This\nwork offers a viable solution for the ethical and effective utilization of\nsensitive medical data in machine learning, facilitating progress in medical\nresearch and the development of robust predictive models.",
    "updated" : "2024-09-12T07:38:34Z",
    "published" : "2024-09-12T07:38:34Z",
    "authors" : [
      {
        "name" : "Tal Baumel"
      },
      {
        "name" : "Andre Manoel"
      },
      {
        "name" : "Daniel Jones"
      },
      {
        "name" : "Shize Su"
      },
      {
        "name" : "Huseyin Inan"
      },
      {
        "name" : " Aaron"
      },
      {
        "name" : " Bornstein"
      },
      {
        "name" : "Robert Sim"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07773v1",
    "title" : "PDC-FRS: Privacy-preserving Data Contribution for Federated Recommender\n  System",
    "summary" : "Federated recommender systems (FedRecs) have emerged as a popular research\ndirection for protecting users' privacy in on-device recommendations. In\nFedRecs, users keep their data locally and only contribute their local\ncollaborative information by uploading model parameters to a central server.\nWhile this rigid framework protects users' raw data during training, it\nseverely compromises the recommendation model's performance due to the\nfollowing reasons: (1) Due to the power law distribution nature of user\nbehavior data, individual users have few data points to train a recommendation\nmodel, resulting in uploaded model updates that may be far from optimal; (2) As\neach user's uploaded parameters are learned from local data, which lacks global\ncollaborative information, relying solely on parameter aggregation methods such\nas FedAvg to fuse global collaborative information may be suboptimal. To bridge\nthis performance gap, we propose a novel federated recommendation framework,\nPDC-FRS. Specifically, we design a privacy-preserving data contribution\nmechanism that allows users to share their data with a differential privacy\nguarantee. Based on the shared but perturbed data, an auxiliary model is\ntrained in parallel with the original federated recommendation process. This\nauxiliary model enhances FedRec by augmenting each user's local dataset and\nintegrating global collaborative information. To demonstrate the effectiveness\nof PDC-FRS, we conduct extensive experiments on two widely used recommendation\ndatasets. The empirical results showcase the superiority of PDC-FRS compared to\nbaseline methods.",
    "updated" : "2024-09-12T06:13:07Z",
    "published" : "2024-09-12T06:13:07Z",
    "authors" : [
      {
        "name" : "Chaoqun Yang"
      },
      {
        "name" : "Wei Yuan"
      },
      {
        "name" : "Liang Qu"
      },
      {
        "name" : "Thanh Tam Nguyen"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07751v1",
    "title" : "Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption",
    "summary" : "The recently proposed Kolmogorov-Arnold Networks (KANs) offer enhanced\ninterpretability and greater model expressiveness. However, KANs also present\nchallenges related to privacy leakage during inference. Homomorphic encryption\n(HE) facilitates privacy-preserving inference for deep learning models,\nenabling resource-limited users to benefit from deep learning services while\nensuring data security. Yet, the complex structure of KANs, incorporating\nnonlinear elements like the SiLU activation function and B-spline functions,\nrenders existing privacy-preserving inference techniques inadequate. To address\nthis issue, we propose an accurate and efficient privacy-preserving inference\nscheme tailored for KANs. Our approach introduces a task-specific polynomial\napproximation for the SiLU activation function, dynamically adjusting the\napproximation range to ensure high accuracy on real-world datasets.\nAdditionally, we develop an efficient method for computing B-spline functions\nwithin the HE domain, leveraging techniques such as repeat packing, lazy\ncombination, and comparison functions. We evaluate the effectiveness of our\nprivacy-preserving KAN inference scheme on both symbolic formula evaluation and\nimage classification. The experimental results show that our model achieves\naccuracy comparable to plaintext KANs across various datasets and outperforms\nplaintext MLPs. Additionally, on the CIFAR-10 dataset, our inference latency\nachieves over 7 times speedup compared to the naive method.",
    "updated" : "2024-09-12T04:51:27Z",
    "published" : "2024-09-12T04:51:27Z",
    "authors" : [
      {
        "name" : "Zhizheng Lai"
      },
      {
        "name" : "Yufei Zhou"
      },
      {
        "name" : "Peijia Zheng"
      },
      {
        "name" : "Lin Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.08913v1",
    "title" : "HLTCOE JHU Submission to the Voice Privacy Challenge 2024",
    "summary" : "We present a number of systems for the Voice Privacy Challenge, including\nvoice conversion based systems such as the kNN-VC method and the WavLM voice\nConversion method, and text-to-speech (TTS) based systems including\nWhisper-VITS. We found that while voice conversion systems better preserve\nemotional content, they struggle to conceal speaker identity in semi-white-box\nattack scenarios; conversely, TTS methods perform better at anonymization and\nworse at emotion preservation. Finally, we propose a random admixture system\nwhich seeks to balance out the strengths and weaknesses of the two category of\nsystems, achieving a strong EER of over 40% while maintaining UAR at a\nrespectable 47%.",
    "updated" : "2024-09-13T15:29:37Z",
    "published" : "2024-09-13T15:29:37Z",
    "authors" : [
      {
        "name" : "Henry Li Xinyuan"
      },
      {
        "name" : "Zexin Cai"
      },
      {
        "name" : "Ashi Garg"
      },
      {
        "name" : "Kevin Duh"
      },
      {
        "name" : "Leibny Paola García-Perera"
      },
      {
        "name" : "Sanjeev Khudanpur"
      },
      {
        "name" : "Nicholas Andrews"
      },
      {
        "name" : "Matthew Wiesner"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.08636v1",
    "title" : "Utilizing Data Fingerprints for Privacy-Preserving Algorithm Selection\n  in Time Series Classification: Performance and Uncertainty Estimation on\n  Unseen Datasets",
    "summary" : "The selection of algorithms is a crucial step in designing AI services for\nreal-world time series classification use cases. Traditional methods such as\nneural architecture search, automated machine learning, combined algorithm\nselection, and hyperparameter optimizations are effective but require\nconsiderable computational resources and necessitate access to all data points\nto run their optimizations. In this work, we introduce a novel data fingerprint\nthat describes any time series classification dataset in a privacy-preserving\nmanner and provides insight into the algorithm selection problem without\nrequiring training on the (unseen) dataset. By decomposing the multi-target\nregression problem, only our data fingerprints are used to estimate algorithm\nperformance and uncertainty in a scalable and adaptable manner. Our approach is\nevaluated on the 112 University of California riverside benchmark datasets,\ndemonstrating its effectiveness in predicting the performance of 35\nstate-of-the-art algorithms and providing valuable insights for effective\nalgorithm selection in time series classification service systems, improving a\nnaive baseline by 7.32% on average in estimating the mean performance and\n15.81% in estimating the uncertainty.",
    "updated" : "2024-09-13T08:43:42Z",
    "published" : "2024-09-13T08:43:42Z",
    "authors" : [
      {
        "name" : "Lars Böcking"
      },
      {
        "name" : "Leopold Müller"
      },
      {
        "name" : "Niklas Kühl"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.08538v1",
    "title" : "An Efficient Privacy-aware Split Learning Framework for Satellite\n  Communications",
    "summary" : "In the rapidly evolving domain of satellite communications, integrating\nadvanced machine learning techniques, particularly split learning, is crucial\nfor enhancing data processing and model training efficiency across satellites,\nspace stations, and ground stations. Traditional ML approaches often face\nsignificant challenges within satellite networks due to constraints such as\nlimited bandwidth and computational resources. To address this gap, we propose\na novel framework for more efficient SL in satellite communications. Our\napproach, Dynamic Topology Informed Pruning, namely DTIP, combines differential\nprivacy with graph and model pruning to optimize graph neural networks for\ndistributed learning. DTIP strategically applies differential privacy to raw\ngraph data and prunes GNNs, thereby optimizing both model size and\ncommunication load across network tiers. Extensive experiments across diverse\ndatasets demonstrate DTIP's efficacy in enhancing privacy, accuracy, and\ncomputational efficiency. Specifically, on Amazon2M dataset, DTIP maintains an\naccuracy of 0.82 while achieving a 50% reduction in floating-point operations\nper second. Similarly, on ArXiv dataset, DTIP achieves an accuracy of 0.85\nunder comparable conditions. Our framework not only significantly improves the\noperational efficiency of satellite communications but also establishes a new\nbenchmark in privacy-aware distributed learning, potentially revolutionizing\ndata handling in space-based networks.",
    "updated" : "2024-09-13T04:59:35Z",
    "published" : "2024-09-13T04:59:35Z",
    "authors" : [
      {
        "name" : "Jianfei Sun"
      },
      {
        "name" : "Cong Wu"
      },
      {
        "name" : "Shahid Mumtaz"
      },
      {
        "name" : "Junyi Tao"
      },
      {
        "name" : "Mingsheng Cao"
      },
      {
        "name" : "Mei Wang"
      },
      {
        "name" : "Valerio Frascolla"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.08503v1",
    "title" : "Enhancing Privacy in ControlNet and Stable Diffusion via Split Learning",
    "summary" : "With the emerging trend of large generative models, ControlNet is introduced\nto enable users to fine-tune pre-trained models with their own data for various\nuse cases. A natural question arises: how can we train ControlNet models while\nensuring users' data privacy across distributed devices? Exploring different\ndistributed training schemes, we find conventional federated learning and split\nlearning unsuitable. Instead, we propose a new distributed learning structure\nthat eliminates the need for the server to send gradients back. Through a\ncomprehensive evaluation of existing threats, we discover that in the context\nof training ControlNet with split learning, most existing attacks are\nineffective, except for two mentioned in previous literature. To counter these\nthreats, we leverage the properties of diffusion models and design a new\ntimestep sampling policy during forward processes. We further propose a\nprivacy-preserving activation function and a method to prevent private text\nprompts from leaving clients, tailored for image generation with diffusion\nmodels. Our experimental results demonstrate that our algorithms and systems\ngreatly enhance the efficiency of distributed training for ControlNet while\nensuring users' data privacy without compromising image generation quality.",
    "updated" : "2024-09-13T02:55:22Z",
    "published" : "2024-09-13T02:55:22Z",
    "authors" : [
      {
        "name" : "Dixi Yao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07444v2",
    "title" : "Echoes of Privacy: Uncovering the Profiling Practices of Voice\n  Assistants",
    "summary" : "Many companies, including Google, Amazon, and Apple, offer voice assistants\nas a convenient solution for answering general voice queries and accessing\ntheir services. These voice assistants have gained popularity and can be easily\naccessed through various smart devices such as smartphones, smart speakers,\nsmartwatches, and an increasing array of other devices. However, this\nconvenience comes with potential privacy risks. For instance, while companies\nvaguely mention in their privacy policies that they may use voice interactions\nfor user profiling, it remains unclear to what extent this profiling occurs and\nwhether voice interactions pose greater privacy risks compared to other\ninteraction modalities.\n  In this paper, we conduct 1171 experiments involving a total of 24530 queries\nwith different personas and interaction modalities over the course of 20 months\nto characterize how the three most popular voice assistants profile their\nusers. We analyze factors such as the labels assigned to users, their accuracy,\nthe time taken to assign these labels, differences between voice and web\ninteractions, and the effectiveness of profiling remediation tools offered by\neach voice assistant. Our findings reveal that profiling can happen without\ninteraction, can be incorrect and inconsistent at times, may take several days\nto weeks for changes to occur, and can be influenced by the interaction\nmodality.",
    "updated" : "2024-09-13T08:55:21Z",
    "published" : "2024-09-11T17:44:41Z",
    "authors" : [
      {
        "name" : "Tina Khezresmaeilzadeh"
      },
      {
        "name" : "Elaine Zhu"
      },
      {
        "name" : "Kiersten Grieco"
      },
      {
        "name" : "Daniel J. Dubois"
      },
      {
        "name" : "Konstantinos Psounis"
      },
      {
        "name" : "David Choffnes"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.10411v1",
    "title" : "A Large-Scale Privacy Assessment of Android Third-Party SDKs",
    "summary" : "Third-party Software Development Kits (SDKs) are widely adopted in Android\napp development, to effortlessly accelerate development pipelines and enhance\napp functionality. However, this convenience raises substantial concerns about\nunauthorized access to users' privacy-sensitive information, which could be\nfurther abused for illegitimate purposes like user tracking or monetization.\nOur study offers a targeted analysis of user privacy protection among Android\nthird-party SDKs, filling a critical gap in the Android software supply chain.\nIt focuses on two aspects of their privacy practices, including data\nexfiltration and behavior-policy compliance (or privacy compliance), utilizing\ntechniques of taint analysis and large language models. It covers 158\nwidely-used SDKs from two key SDK release platforms, the official one and a\nlarge alternative one. From them, we identified 338 instances of privacy data\nexfiltration. On the privacy compliance, our study reveals that more than 30%\nof the examined SDKs fail to provide a privacy policy to disclose their data\nhandling practices. Among those that provide privacy policies, 37% of them\nover-collect user data, and 88% falsely claim access to sensitive data. We\nrevisit the latest versions of the SDKs after 12 months. Our analysis\ndemonstrates a persistent lack of improvement in these concerning trends. Based\non our findings, we propose three actionable recommendations to mitigate the\nprivacy leakage risks and enhance privacy protection for Android users. Our\nresearch not only serves as an urgent call for industry attention but also\nprovides crucial insights for future regulatory interventions.",
    "updated" : "2024-09-16T15:44:43Z",
    "published" : "2024-09-16T15:44:43Z",
    "authors" : [
      {
        "name" : "Mark Huasong Meng"
      },
      {
        "name" : "Chuan Yan"
      },
      {
        "name" : "Yun Hao"
      },
      {
        "name" : "Qing Zhang"
      },
      {
        "name" : "Zeyu Wang"
      },
      {
        "name" : "Kailong Wang"
      },
      {
        "name" : "Sin Gee Teo"
      },
      {
        "name" : "Guangdong Bai"
      },
      {
        "name" : "Jin Song Dong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.10337v1",
    "title" : "Security, Trust and Privacy challenges in AI-driven 6G Networks",
    "summary" : "The advent of 6G networks promises unprecedented advancements in wireless\ncommunication, offering wider bandwidth and lower latency compared to its\npredecessors. This article explores the evolving infrastructure of 6G networks,\nemphasizing the transition towards a more disaggregated structure and the\nintegration of artificial intelligence (AI) technologies. Furthermore, it\nexplores the security, trust and privacy challenges and attacks in 6G networks,\nparticularly those related to the use of AI. It presents a classification of\nnetwork attacks stemming from its AI-centric architecture and explores\ntechnologies designed to detect or mitigate these emerging threats. The paper\nconcludes by examining the implications and risks linked to the utilization of\nAI in ensuring a robust network.",
    "updated" : "2024-09-16T14:48:20Z",
    "published" : "2024-09-16T14:48:20Z",
    "authors" : [
      {
        "name" : "Helena Rifa-Pous"
      },
      {
        "name" : "Victor Garcia-Font"
      },
      {
        "name" : "Carlos Nunez-Gomez"
      },
      {
        "name" : "Julian Salas"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.10226v1",
    "title" : "Privacy-Preserving Distributed Maximum Consensus Without Accuracy Loss",
    "summary" : "In distributed networks, calculating the maximum element is a fundamental\ntask in data analysis, known as the distributed maximum consensus problem.\nHowever, the sensitive nature of the data involved makes privacy protection\nessential. Despite its importance, privacy in distributed maximum consensus has\nreceived limited attention in the literature. Traditional privacy-preserving\nmethods typically add noise to updates, degrading the accuracy of the final\nresult. To overcome these limitations, we propose a novel distributed\noptimization-based approach that preserves privacy without sacrificing\naccuracy. Our method introduces virtual nodes to form an augmented graph and\nleverages a carefully designed initialization process to ensure the privacy of\nhonest participants, even when all their neighboring nodes are dishonest.\nThrough a comprehensive information-theoretical analysis, we derive a\nsufficient condition to protect private data against both passive and\neavesdropping adversaries. Extensive experiments validate the effectiveness of\nour approach, demonstrating that it not only preserves perfect privacy but also\nmaintains accuracy, outperforming existing noise-based methods that typically\nsuffer from accuracy loss.",
    "updated" : "2024-09-16T12:21:04Z",
    "published" : "2024-09-16T12:21:04Z",
    "authors" : [
      {
        "name" : "Wenrui Yu"
      },
      {
        "name" : "Richard Heusdens"
      },
      {
        "name" : "Jun Pang"
      },
      {
        "name" : "Qiongxiu Li"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR",
      "cs.IT",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.10192v1",
    "title" : "PrePaMS: Privacy-Preserving Participant Management System for Studies\n  with Rewards and Prerequisites",
    "summary" : "Taking part in surveys, experiments, and studies is often compensated by\nrewards to increase the number of participants and encourage attendance. While\nprivacy requirements are usually considered for participation, privacy aspects\nof the reward procedure are mostly ignored. To this end, we introduce PrePaMS,\nan efficient participation management system that supports prerequisite checks\nand participation rewards in a privacy-preserving way. Our system organizes\nparticipations with potential (dis-)qualifying dependencies and enables secure\nreward payoffs. By leveraging a set of proven cryptographic primitives and\nmechanisms such as anonymous credentials and zero-knowledge proofs,\nparticipations are protected so that service providers and organizers cannot\nderive the identity of participants even within the reward process. In this\npaper, we have designed and implemented a prototype of PrePaMS to show its\neffectiveness and evaluated its performance under realistic workloads. PrePaMS\ncovers the information whether subjects have participated in surveys,\nexperiments, or studies. When combined with other secure solutions for the\nactual data collection within these events, PrePaMS can represent a cornerstone\nfor more privacy-preserving empirical research.",
    "updated" : "2024-09-16T11:35:17Z",
    "published" : "2024-09-16T11:35:17Z",
    "authors" : [
      {
        "name" : "Echo Meißner"
      },
      {
        "name" : "Frank Kargl"
      },
      {
        "name" : "Benjamin Erb"
      },
      {
        "name" : "Felix Engelmann"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.10057v1",
    "title" : "A Response to: A Note on \"Privacy Preserving n-Party Scalar Product\n  Protocol\"",
    "summary" : "We reply to the comments on our proposed privacy preserving n-party scalar\nproduct protocol made by Liu. In their comment Liu raised concerns regarding\nthe security and scalability of the $n$-party scalar product protocol. In this\nreply, we show that their concerns are unfounded and that the $n$-party scalar\nproduct protocol is safe for its intended purposes. Their concerns regarding\nthe security are based on a misunderstanding of the protocol. Additionally,\nwhile the scalability of the protocol puts limitations on its use, the protocol\nstill has numerous practical applications when applied in the correct\nscenarios. Specifically within vertically partitioned scenarios, which often\ninvolve few parties, the protocol remains practical. In this reply we clarify\nLiu's misunderstanding. Additionally, we explain why the protocols scaling is\nnot a practical problem in its intended application.",
    "updated" : "2024-09-16T07:36:37Z",
    "published" : "2024-09-16T07:36:37Z",
    "authors" : [
      {
        "name" : "Florian van Daalen"
      },
      {
        "name" : "Lianne Ippel"
      },
      {
        "name" : "Andre Dekker"
      },
      {
        "name" : "Inigo Bermejo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09972v1",
    "title" : "Securing the Future: Exploring Privacy Risks and Security Questions in\n  Robotic Systems",
    "summary" : "The integration of artificial intelligence, especially large language models\nin robotics, has led to rapid advancements in the field. We are now observing\nan unprecedented surge in the use of robots in our daily lives. The development\nand continual improvements of robots are moving at an astonishing pace.\nAlthough these remarkable improvements facilitate and enhance our lives,\nseveral security and privacy concerns have not been resolved yet. Therefore, it\nhas become crucial to address the privacy and security threats of robotic\nsystems while improving our experiences. In this paper, we aim to present\nexisting applications and threats of robotics, anticipated future evolution,\nand the security and privacy issues they may imply. We present a series of open\nquestions for researchers and practitioners to explore further.",
    "updated" : "2024-09-16T04:10:24Z",
    "published" : "2024-09-16T04:10:24Z",
    "authors" : [
      {
        "name" : "Diba Afroze"
      },
      {
        "name" : "Yazhou Tu"
      },
      {
        "name" : "Xiali Hei"
      }
    ],
    "categories" : [
      "cs.RO",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09558v1",
    "title" : "A Statistical Viewpoint on Differential Privacy: Hypothesis Testing,\n  Representation and Blackwell's Theorem",
    "summary" : "Differential privacy is widely considered the formal privacy for\nprivacy-preserving data analysis due to its robust and rigorous guarantees,\nwith increasingly broad adoption in public services, academia, and industry.\nDespite originating in the cryptographic context, in this review paper we argue\nthat, fundamentally, differential privacy can be considered a \\textit{pure}\nstatistical concept. By leveraging a theorem due to David Blackwell, our focus\nis to demonstrate that the definition of differential privacy can be formally\nmotivated from a hypothesis testing perspective, thereby showing that\nhypothesis testing is not merely convenient but also the right language for\nreasoning about differential privacy. This insight leads to the definition of\n$f$-differential privacy, which extends other differential privacy definitions\nthrough a representation theorem. We review techniques that render\n$f$-differential privacy a unified framework for analyzing privacy bounds in\ndata analysis and machine learning. Applications of this differential privacy\ndefinition to private deep learning, private convex optimization, shuffled\nmechanisms, and U.S.~Census data are discussed to highlight the benefits of\nanalyzing privacy bounds under this framework compared to existing\nalternatives.",
    "updated" : "2024-09-14T23:47:22Z",
    "published" : "2024-09-14T23:47:22Z",
    "authors" : [
      {
        "name" : "Weijie J. Su"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09532v1",
    "title" : "Using Synthetic Data to Mitigate Unfairness and Preserve Privacy through\n  Single-Shot Federated Learning",
    "summary" : "To address unfairness issues in federated learning (FL), contemporary\napproaches typically use frequent model parameter updates and transmissions\nbetween the clients and server. In such a process, client-specific information\n(e.g., local dataset size or data-related fairness metrics) must be sent to the\nserver to compute, e.g., aggregation weights. All of this results in high\ntransmission costs and the potential leakage of client information. As an\nalternative, we propose a strategy that promotes fair predictions across\nclients without the need to pass information between the clients and server\niteratively and prevents client data leakage. For each client, we first use\ntheir local dataset to obtain a synthetic dataset by solving a bilevel\noptimization problem that addresses unfairness concerns during the learning\nprocess. We then pass each client's synthetic dataset to the server, the\ncollection of which is used to train the server model using conventional\nmachine learning techniques (that do not take fairness metrics into account).\nThus, we eliminate the need to handle fairness-specific aggregation weights\nwhile preserving client privacy. Our approach requires only a single\ncommunication between the clients and the server, thus making it\ncomputationally cost-effective, able to maintain privacy, and able to ensuring\nfairness. We present empirical evidence to demonstrate the advantages of our\napproach. The results illustrate that our method effectively uses synthetic\ndata as a means to mitigate unfairness and preserve client privacy.",
    "updated" : "2024-09-14T21:04:11Z",
    "published" : "2024-09-14T21:04:11Z",
    "authors" : [
      {
        "name" : "Chia-Yuan Wu"
      },
      {
        "name" : "Frank E. Curtis"
      },
      {
        "name" : "Daniel P. Robinson"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY",
      "math.OC",
      "G.1.6; I.2.6; C.2.4; K.4.1; D.4.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09510v1",
    "title" : "Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for\n  Privacy-Preserving Personalization of Large Language Models",
    "summary" : "Privacy-preserving methods for personalizing large language models (LLMs) are\nrelatively under-explored. There are two schools of thought on this topic: (1)\ngenerating personalized outputs by personalizing the input prompt through\nretrieval augmentation from the user's personal information (RAG-based\nmethods), and (2) parameter-efficient fine-tuning of LLMs per user that\nconsiders efficiency and space limitations (PEFT-based methods). This paper\npresents the first systematic comparison between two approaches on a wide range\nof personalization tasks using seven diverse datasets. Our results indicate\nthat RAG-based and PEFT-based personalization methods on average yield 14.92%\nand 1.07% improvements over the non-personalized LLM, respectively. We find\nthat combining RAG with PEFT elevates these improvements to 15.98%.\nAdditionally, we identify a positive correlation between the amount of user\ndata and PEFT's effectiveness, indicating that RAG is a better choice for\ncold-start users (i.e., user's with limited personal data).",
    "updated" : "2024-09-14T19:18:26Z",
    "published" : "2024-09-14T19:18:26Z",
    "authors" : [
      {
        "name" : "Alireza Salemi"
      },
      {
        "name" : "Hamed Zamani"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09495v1",
    "title" : "Protecting Vehicle Location Privacy with Contextually-Driven Synthetic\n  Location Generation",
    "summary" : "Geo-obfuscation is a Location Privacy Protection Mechanism used in\nlocation-based services that allows users to report obfuscated locations\ninstead of exact ones. A formal privacy criterion, geoindistinguishability\n(Geo-Ind), requires real locations to be hard to distinguish from nearby\nlocations (by attackers) based on their obfuscated representations. However,\nGeo-Ind often fails to consider context, such as road networks and vehicle\ntraffic conditions, making it less effective in protecting the location privacy\nof vehicles, of which the mobility are heavily influenced by these factors.\n  In this paper, we introduce VehiTrack, a new threat model to demonstrate the\nvulnerability of Geo-Ind in protecting vehicle location privacy from\ncontext-aware inference attacks. Our experiments demonstrate that VehiTrack can\naccurately determine exact vehicle locations from obfuscated data, reducing\naverage inference errors by 61.20% with Laplacian noise and 47.35% with linear\nprogramming (LP) compared to traditional Bayesian attacks. By using contextual\ndata like road networks and traffic flow, VehiTrack effectively eliminates a\nsignificant number of seemingly \"impossible\" locations during its search for\nthe actual location of the vehicles. Based on these insights, we propose\nTransProtect, a new geo-obfuscation approach that limits obfuscation to\nrealistic vehicle movement patterns, complicating attackers' ability to\ndifferentiate obfuscated from actual locations. Our results show that\nTransProtect increases VehiTrack's inference error by 57.75% with Laplacian\nnoise and 27.21% with LP, significantly enhancing protection against these\nattacks.",
    "updated" : "2024-09-14T17:47:23Z",
    "published" : "2024-09-14T17:47:23Z",
    "authors" : [
      {
        "name" : "Sourabh Yadav"
      },
      {
        "name" : "Chenyang Yu"
      },
      {
        "name" : "Xinpeng Xie"
      },
      {
        "name" : "Yan Huang"
      },
      {
        "name" : "Chenxi Qiu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09363v1",
    "title" : "Security and Privacy Perspectives of People Living in Shared Home\n  Environments",
    "summary" : "Security and privacy perspectives of people in a multi-user home are a\ngrowing area of research, with many researchers reflecting on the complicated\npower imbalance and challenging access control issues of the devices involved.\nHowever, these studies primarily focused on the multi-user scenarios in\ntraditional family home settings, leaving other types of multi-user home\nenvironments, such as homes shared by co-habitants without a familial\nrelationship, under-studied. This paper closes this research gap via\nquantitative and qualitative analysis of results from an online survey and\ncontent analysis of sampled online posts on Reddit. It explores the complex\nroles of shared home users, which depend on various factors unique to the\nshared home environment, e.g., who owns what home devices, how home devices are\nused by multiple users, and more complicated relationships between the landlord\nand people in the shared home and among co-habitants. Half (50.7%) of our\nsurvey participants thought that devices in a shared home are less secure than\nin a traditional family home. This perception was found statistically\nsignificantly associated with factors such as the fear of devices being\ntampered with in their absence and (lack of) trust in other co-habitants and\ntheir visitors. Our study revealed new user types and relationships in a\nmulti-user environment such as ExternalPrimary-InternalPrimary while analysing\nthe landlord and shared home resident relationship with regard to shared home\ndevice use. We propose a threat actor model for shared home environments, which\nhas a focus on possible malicious behaviours of current and past co-habitants\nof a shared home, as a special type of insider threat in a home environment. We\nalso recommend further research to understand the complex roles co-habitants\ncan play in navigating and adapting to a shared home environment's security and\nprivacy landscape.",
    "updated" : "2024-09-14T08:34:57Z",
    "published" : "2024-09-14T08:34:57Z",
    "authors" : [
      {
        "name" : "Nandita Pattnaik"
      },
      {
        "name" : "Shujun Li"
      },
      {
        "name" : "Jason R. C. Nurse"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09272v1",
    "title" : "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
    "summary" : "Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited\nremarkable performance in generating realistic and natural audio. However,\ntheir dark side, audio deepfake poses a significant threat to both society and\nindividuals. Existing countermeasures largely focus on determining the\ngenuineness of speech based on complete original audio recordings, which\nhowever often contain private content. This oversight may refrain deepfake\ndetection from many applications, particularly in scenarios involving sensitive\ninformation like business secrets. In this paper, we propose SafeEar, a novel\nframework that aims to detect deepfake audios without relying on accessing the\nspeech content within. Our key idea is to devise a neural audio codec into a\nnovel decoupling model that well separates the semantic and acoustic\ninformation from audio samples, and only use the acoustic information (e.g.,\nprosody and timbre) for deepfake detection. In this way, no semantic content\nwill be exposed to the detector. To overcome the challenge of identifying\ndiverse deepfake audio without semantic clues, we enhance our deepfake detector\nwith real-world codec augmentation. Extensive experiments conducted on four\nbenchmark datasets demonstrate SafeEar's effectiveness in detecting various\ndeepfake techniques with an equal error rate (EER) down to 2.02%.\nSimultaneously, it shields five-language speech content from being deciphered\nby both machine and human auditory analysis, demonstrated by word error rates\n(WERs) all above 93.93% and our user study. Furthermore, our benchmark\nconstructed for anti-deepfake and anti-content recovery evaluation helps\nprovide a basis for future research in the realms of audio privacy preservation\nand deepfake detection.",
    "updated" : "2024-09-14T02:45:09Z",
    "published" : "2024-09-14T02:45:09Z",
    "authors" : [
      {
        "name" : "Xinfeng Li"
      },
      {
        "name" : "Kai Li"
      },
      {
        "name" : "Yifan Zheng"
      },
      {
        "name" : "Chen Yan"
      },
      {
        "name" : "Xiaoyu Ji"
      },
      {
        "name" : "Wenyuan Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09222v1",
    "title" : "Dark Patterns in the Opt-Out Process and Compliance with the California\n  Consumer Privacy Act (CCPA)",
    "summary" : "To protect consumer privacy, the California Consumer Privacy Act (CCPA)\nmandates that businesses provide consumers with a straightforward way to opt\nout of the sale and sharing of their personal information. However, the control\nthat businesses enjoy over the opt-out process allows them to impose hurdles on\nconsumers aiming to opt out, including by employing dark patterns. Motivated by\nthe enactment of the California Privacy Rights Act (CPRA), which strengthens\nthe CCPA and explicitly forbids certain dark patterns in the opt-out process,\nwe investigate how dark patterns are used in opt-out processes and assess their\ncompliance with CCPA regulations. Our research reveals that websites employ a\nvariety of dark patterns. Some of these patterns are explicitly prohibited\nunder the CCPA; others evidently take advantage of legal loopholes. Despite the\ninitial efforts to restrict dark patterns by policymakers, there is more work\nto be done.",
    "updated" : "2024-09-13T22:20:43Z",
    "published" : "2024-09-13T22:20:43Z",
    "authors" : [
      {
        "name" : "Van Hong Tran"
      },
      {
        "name" : "Aarushi Mehrotra"
      },
      {
        "name" : "Ranya Sharma"
      },
      {
        "name" : "Marshini Chetty"
      },
      {
        "name" : "Nick Feamster"
      },
      {
        "name" : "Jens Frankenreiter"
      },
      {
        "name" : "Lior Strahilevitz"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06955v2",
    "title" : "Privacy-Preserving Federated Learning with Consistency via Knowledge\n  Distillation Using Conditional Generator",
    "summary" : "Federated Learning (FL) is gaining popularity as a distributed learning\nframework that only shares model parameters or gradient updates and keeps\nprivate data locally. However, FL is at risk of privacy leakage caused by\nprivacy inference attacks. And most existing privacy-preserving mechanisms in\nFL conflict with achieving high performance and efficiency. Therefore, we\npropose FedMD-CG, a novel FL method with highly competitive performance and\nhigh-level privacy preservation, which decouples each client's local model into\na feature extractor and a classifier, and utilizes a conditional generator\ninstead of the feature extractor to perform server-side model aggregation. To\nensure the consistency of local generators and classifiers, FedMD-CG leverages\nknowledge distillation to train local models and generators at both the latent\nfeature level and the logit level. Also, we construct additional classification\nlosses and design new diversity losses to enhance client-side training.\nFedMD-CG is robust to data heterogeneity and does not require training extra\ndiscriminators (like cGAN). We conduct extensive experiments on various image\nclassification tasks to validate the superiority of FedMD-CG.",
    "updated" : "2024-09-16T08:23:09Z",
    "published" : "2024-09-11T02:36:36Z",
    "authors" : [
      {
        "name" : "Kangyang Luo"
      },
      {
        "name" : "Shuai Wang"
      },
      {
        "name" : "Xiang Li"
      },
      {
        "name" : "Yunshi Lan"
      },
      {
        "name" : "Ming Gao"
      },
      {
        "name" : "Jinlong Shu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  }
]