[
  {
    "id" : "http://arxiv.org/abs/2409.02614v1",
    "title" : "Evaluating the Effects of Digital Privacy Regulations on User Trust",
    "summary" : "In today's digital society, issues related to digital privacy have become\nincreasingly important. Issues such as data breaches result in misuse of data,\nfinancial loss, and cyberbullying, which leads to less user trust in digital\nservices. This research investigates the impact of digital privacy laws on user\ntrust by comparing the regulations in the Netherlands, Ghana, and Malaysia. The\nstudy employs a comparative case study method, involving interviews with\ndigital privacy law experts, IT educators, and consumers from each country. The\nmain findings reveal that while the General Data Protection Regulation (GDPR)\nin the Netherlands is strict, its practical impact is limited by enforcement\nchallenges. In Ghana, the Data Protection Act is underutilized due to low\npublic awareness and insufficient enforcement, leading to reliance on personal\nprotective measures. In Malaysia, trust in digital services is largely\ndependent on the security practices of individual platforms rather than the\nPersonal Data Protection Act. The study highlights the importance of public\nawareness, effective enforcement, and cultural considerations in shaping the\neffectiveness of digital privacy laws. Based on these insights, a\nrecommendation framework is proposed to enhance digital privacy practices, also\naiming to provide valuable guidance for policymakers, businesses, and citizens\nin navigating the challenges of digitalization.",
    "updated" : "2024-09-04T11:11:41Z",
    "published" : "2024-09-04T11:11:41Z",
    "authors" : [
      {
        "name" : "Mehmet Berk Cetin"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02404v1",
    "title" : "Learning Privacy-Preserving Student Networks via\n  Discriminative-Generative Distillation",
    "summary" : "While deep models have proved successful in learning rich knowledge from\nmassive well-annotated data, they may pose a privacy leakage risk in practical\ndeployment. It is necessary to find an effective trade-off between high utility\nand strong privacy. In this work, we propose a discriminative-generative\ndistillation approach to learn privacy-preserving deep models. Our key idea is\ntaking models as bridge to distill knowledge from private data and then\ntransfer it to learn a student network via two streams. First, discriminative\nstream trains a baseline classifier on private data and an ensemble of teachers\non multiple disjoint private subsets, respectively. Then, generative stream\ntakes the classifier as a fixed discriminator and trains a generator in a\ndata-free manner. After that, the generator is used to generate massive\nsynthetic data which are further applied to train a variational autoencoder\n(VAE). Among these synthetic data, a few of them are fed into the teacher\nensemble to query labels via differentially private aggregation, while most of\nthem are embedded to the trained VAE for reconstructing synthetic data.\nFinally, a semi-supervised student learning is performed to simultaneously\nhandle two tasks: knowledge transfer from the teachers with distillation on few\nprivately labeled synthetic data, and knowledge enhancement with tangent-normal\nadversarial regularization on many triples of reconstructed synthetic data. In\nthis way, our approach can control query cost over private data and mitigate\naccuracy degradation in a unified manner, leading to a privacy-preserving\nstudent model. Extensive experiments and analysis clearly show the\neffectiveness of the proposed approach.",
    "updated" : "2024-09-04T03:06:13Z",
    "published" : "2024-09-04T03:06:13Z",
    "authors" : [
      {
        "name" : "Shiming Ge"
      },
      {
        "name" : "Bochao Liu"
      },
      {
        "name" : "Pengju Wang"
      },
      {
        "name" : "Yong Li"
      },
      {
        "name" : "Dan Zeng"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02375v1",
    "title" : "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review",
    "summary" : "The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights.",
    "updated" : "2024-09-04T01:51:37Z",
    "published" : "2024-09-04T01:51:37Z",
    "authors" : [
      {
        "name" : "Xichou Zhu"
      },
      {
        "name" : "Yang Liu"
      },
      {
        "name" : "Zhou Shen"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Min Li"
      },
      {
        "name" : "Yujun Chen"
      },
      {
        "name" : "Benzi John"
      },
      {
        "name" : "Zhenzhen Ma"
      },
      {
        "name" : "Tao Hu"
      },
      {
        "name" : "Bolong Yang"
      },
      {
        "name" : "Manman Wang"
      },
      {
        "name" : "Zongxing Xie"
      },
      {
        "name" : "Peng Liu"
      },
      {
        "name" : "Dan Cai"
      },
      {
        "name" : "Junhui Wang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02364v1",
    "title" : "Examining Caregiving Roles to Differentiate the Effects of Using a\n  Mobile App for Community Oversight for Privacy and Security",
    "summary" : "We conducted a 4-week field study with 101 smartphone users who\nself-organized into 22 small groups of family, friends, and neighbors to use\n``CO-oPS,'' a mobile app for co-managing mobile privacy and security. We\ndifferentiated between those who provided oversight (i.e., caregivers) and\nthose who did not (i.e., caregivees) to examine differential effects on their\nexperiences and behaviors while using CO-oPS. Caregivers reported higher power\nuse, community trust, belonging, collective efficacy, and self-efficacy than\ncaregivees. Both groups' self-efficacy and collective efficacy for mobile\nprivacy and security increased after using CO-oPS. However, this increase was\nsignificantly stronger for caregivees. Our research demonstrates how\ncommunity-based approaches can benefit people who need additional help managing\ntheir digital privacy and security. We provide recommendations to support\ncommunity-based oversight for managing privacy and security within communities\nof different roles and skills.",
    "updated" : "2024-09-04T01:21:56Z",
    "published" : "2024-09-04T01:21:56Z",
    "authors" : [
      {
        "name" : "Mamtaj Akter"
      },
      {
        "name" : "Jess Kropczynski"
      },
      {
        "name" : "Heather Lipford"
      },
      {
        "name" : "Pamela Wisniewski"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02044v1",
    "title" : "FedMinds: Privacy-Preserving Personalized Brain Visual Decoding",
    "summary" : "Exploring the mysteries of the human brain is a long-term research topic in\nneuroscience. With the help of deep learning, decoding visual information from\nhuman brain activity fMRI has achieved promising performance. However, these\ndecoding models require centralized storage of fMRI data to conduct training,\nleading to potential privacy security issues. In this paper, we focus on\nprivacy preservation in multi-individual brain visual decoding. To this end, we\nintroduce a novel framework called FedMinds, which utilizes federated learning\nto protect individuals' privacy during model training. In addition, we deploy\nindividual adapters for each subject, thus allowing personalized visual\ndecoding. We conduct experiments on the authoritative NSD datasets to evaluate\nthe performance of the proposed framework. The results demonstrate that our\nframework achieves high-precision visual decoding along with privacy\nprotection.",
    "updated" : "2024-09-03T16:46:29Z",
    "published" : "2024-09-03T16:46:29Z",
    "authors" : [
      {
        "name" : "Guangyin Bao"
      },
      {
        "name" : "Duoqian Miao"
      }
    ],
    "categories" : [
      "q-bio.NC",
      "cs.CV",
      "cs.DC",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01924v1",
    "title" : "Privacy-Preserving and Post-Quantum Counter Denial of Service Framework\n  for Wireless Networks",
    "summary" : "As network services progress and mobile and IoT environments expand, numerous\nsecurity concerns have surfaced for spectrum access systems. The omnipresent\nrisk of Denial-of-Service (DoS) attacks and raising concerns about user privacy\n(e.g., location privacy, anonymity) are among such cyber threats. These\nsecurity and privacy risks increase due to the threat of quantum computers that\ncan compromise long-term security by circumventing conventional cryptosystems\nand increasing the cost of countermeasures. While some defense mechanisms exist\nagainst these threats in isolation, there is a significant gap in the state of\nthe art on a holistic solution against DoS attacks with privacy and anonymity\nfor spectrum management systems, especially when post-quantum (PQ) security is\nin mind. In this paper, we propose a new cybersecurity framework PACDoSQ, which\nis (to the best of our knowledge) the first to offer location privacy and\nanonymity for spectrum management with counter DoS and PQ security\nsimultaneously. Our solution introduces the private spectrum bastion (database)\nconcept to exploit existing architectural features of spectrum management\nsystems and then synergizes them with multi-server private information\nretrieval and PQ-secure Tor to guarantee a location-private and anonymous\nacquisition of spectrum information together with hash-based client-server\npuzzles for counter DoS. We prove that PACDoSQ achieves its security\nobjectives, and show its feasibility via a comprehensive performance\nevaluation.",
    "updated" : "2024-09-03T14:14:41Z",
    "published" : "2024-09-03T14:14:41Z",
    "authors" : [
      {
        "name" : "Saleh Darzi"
      },
      {
        "name" : "Attila Altay Yavuz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01710v1",
    "title" : "Privacy-Preserving Multimedia Mobile Cloud Computing Using Protective\n  Perturbation",
    "summary" : "Mobile cloud computing has been adopted in many multimedia applications,\nwhere the resource-constrained mobile device sends multimedia data (e.g.,\nimages) to remote cloud servers to request computation-intensive multimedia\nservices (e.g., image recognition). While significantly improving the\nperformance of the mobile applications, the cloud-based mechanism often causes\nprivacy concerns as the multimedia data and services are offloaded from the\ntrusted user device to untrusted cloud servers. Several recent studies have\nproposed perturbation-based privacy preserving mechanisms, which obfuscate the\noffloaded multimedia data to eliminate privacy exposures without affecting the\nfunctionality of the remote multimedia services. However, the existing privacy\nprotection approaches require the deployment of computation-intensive\nperturbation generation on the resource-constrained mobile devices. Also, the\nobfuscated images are typically not compliant with the standard image\ncompression algorithms and suffer from significant bandwidth consumption. In\nthis paper, we develop a novel privacy-preserving multimedia mobile cloud\ncomputing framework, namely $PMC^2$, to address the resource and bandwidth\nchallenges. $PMC^2$ employs secure confidential computing in the cloud to\ndeploy the perturbation generator, which addresses the resource challenge while\nmaintaining the privacy. Furthermore, we develop a neural compressor\nspecifically trained to compress the perturbed images in order to address the\nbandwidth challenge. We implement $PMC^2$ in an end-to-end mobile cloud\ncomputing system, based on which our evaluations demonstrate superior latency,\npower efficiency, and bandwidth consumption achieved by $PMC^2$ while\nmaintaining high accuracy in the target multimedia service.",
    "updated" : "2024-09-03T08:47:17Z",
    "published" : "2024-09-03T08:47:17Z",
    "authors" : [
      {
        "name" : "Zhongze Tang"
      },
      {
        "name" : "Mengmei Ye"
      },
      {
        "name" : "Yao Liu"
      },
      {
        "name" : "Sheng Wei"
      }
    ],
    "categories" : [
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01661v1",
    "title" : "$S^2$NeRF: Privacy-preserving Training Framework for NeRF",
    "summary" : "Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and\ngraphics, facilitating novel view synthesis and influencing sectors like\nextended reality and e-commerce. However, NeRF's dependence on extensive data\ncollection, including sensitive scene image data, introduces significant\nprivacy risks when users upload this data for model training. To address this\nconcern, we first propose SplitNeRF, a training framework that incorporates\nsplit learning (SL) techniques to enable privacy-preserving collaborative model\ntraining between clients and servers without sharing local data. Despite its\nbenefits, we identify vulnerabilities in SplitNeRF by developing two attack\nmethods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which\nexploit the shared gradient data and a few leaked scene images to reconstruct\nprivate scene information. To counter these threats, we introduce $S^2$NeRF,\nsecure SplitNeRF that integrates effective defense mechanisms. By introducing\ndecaying noise related to the gradient norm into the shared gradient\ninformation, $S^2$NeRF preserves privacy while maintaining a high utility of\nthe NeRF model. Our extensive evaluations across multiple datasets demonstrate\nthe effectiveness of $S^2$NeRF against privacy breaches, confirming its\nviability for secure NeRF training in sensitive applications.",
    "updated" : "2024-09-03T07:08:30Z",
    "published" : "2024-09-03T07:08:30Z",
    "authors" : [
      {
        "name" : "Bokang Zhang"
      },
      {
        "name" : "Yanglin Zhang"
      },
      {
        "name" : "Zhikun Zhang"
      },
      {
        "name" : "Jinglan Yang"
      },
      {
        "name" : "Lingying Huang"
      },
      {
        "name" : "Junfeng Wu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01329v1",
    "title" : "Assessing the Impact of Image Dataset Features on Privacy-Preserving\n  Machine Learning",
    "summary" : "Machine Learning (ML) is crucial in many sectors, including computer vision.\nHowever, ML models trained on sensitive data face security challenges, as they\ncan be attacked and leak information. Privacy-Preserving Machine Learning\n(PPML) addresses this by using Differential Privacy (DP) to balance utility and\nprivacy. This study identifies image dataset characteristics that affect the\nutility and vulnerability of private and non-private Convolutional Neural\nNetwork (CNN) models. Through analyzing multiple datasets and privacy budgets,\nwe find that imbalanced datasets increase vulnerability in minority classes,\nbut DP mitigates this issue. Datasets with fewer classes improve both model\nutility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR)\ndatasets deteriorate the utility-privacy trade-off. These insights offer\nvaluable guidance for practitioners and researchers in estimating and\noptimizing the utility-privacy trade-off in image datasets, helping to inform\ndata and privacy modifications for better outcomes based on dataset\ncharacteristics.",
    "updated" : "2024-09-02T15:30:27Z",
    "published" : "2024-09-02T15:30:27Z",
    "authors" : [
      {
        "name" : "Lucas Lange"
      },
      {
        "name" : "Maurice-Maximilian Heykeroth"
      },
      {
        "name" : "Erhard Rahm"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01088v1",
    "title" : "Towards Split Learning-based Privacy-Preserving Record Linkage",
    "summary" : "Split Learning has been recently introduced to facilitate applications where\nuser data privacy is a requirement. However, it has not been thoroughly studied\nin the context of Privacy-Preserving Record Linkage, a problem in which the\nsame real-world entity should be identified among databases from different\ndataholders, but without disclosing any additional information. In this paper,\nwe investigate the potentials of Split Learning for Privacy-Preserving Record\nMatching, by introducing a novel training method through the utilization of\nReference Sets, which are publicly available data corpora, showcasing minimal\nmatching impact against a traditional centralized SVM-based technique.",
    "updated" : "2024-09-02T09:17:05Z",
    "published" : "2024-09-02T09:17:05Z",
    "authors" : [
      {
        "name" : "Michail Zervas"
      },
      {
        "name" : "Alexandros Karakasidis"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.00974v1",
    "title" : "Enhancing Privacy in Federated Learning: Secure Aggregation for\n  Real-World Healthcare Applications",
    "summary" : "Deploying federated learning (FL) in real-world scenarios, particularly in\nhealthcare, poses challenges in communication and security. In particular, with\nrespect to the federated aggregation procedure, researchers have been focusing\non the study of secure aggregation (SA) schemes to provide privacy guarantees\nover the model's parameters transmitted by the clients. Nevertheless, the\npractical availability of SA in currently available FL frameworks is currently\nlimited, due to computational and communication bottlenecks. To fill this gap,\nthis study explores the implementation of SA within the open-source Fed-BioMed\nframework. We implement and compare two SA protocols, Joye-Libert (JL) and Low\nOverhead Masking (LOM), by providing extensive benchmarks in a panel of\nhealthcare data analysis problems. Our theoretical and experimental evaluations\non four datasets demonstrate that SA protocols effectively protect privacy\nwhile maintaining task accuracy. Computational overhead during training is less\nthan 1% on a CPU and less than 50% on a GPU for large models, with protection\nphases taking less than 10 seconds. Incorporating SA into Fed-BioMed impacts\ntask accuracy by no more than 2% compared to non-SA scenarios. Overall this\nstudy demonstrates the feasibility of SA in real-world healthcare applications\nand contributes in reducing the gap towards the adoption of privacy-preserving\ntechnologies in sensitive applications.",
    "updated" : "2024-09-02T06:43:22Z",
    "published" : "2024-09-02T06:43:22Z",
    "authors" : [
      {
        "name" : "Riccardo Taiello"
      },
      {
        "name" : "Sergen Cansiz"
      },
      {
        "name" : "Marc Vesin"
      },
      {
        "name" : "Francesco Cremonesi"
      },
      {
        "name" : "Lucia Innocenti"
      },
      {
        "name" : "Melek Önen"
      },
      {
        "name" : "Marco Lorenzi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.00740v1",
    "title" : "VPVet: Vetting Privacy Policies of Virtual Reality Apps",
    "summary" : "Virtual reality (VR) apps can harvest a wider range of user data than\nweb/mobile apps running on personal computers or smartphones. Existing law and\nprivacy regulations emphasize that VR developers should inform users of what\ndata are collected/used/shared (CUS) through privacy policies. However, privacy\npolicies in the VR ecosystem are still in their early stages, and many\ndevelopers fail to write appropriate privacy policies that comply with\nregulations and meet user expectations. In this paper, we propose VPVet to\nautomatically vet privacy policy compliance issues for VR apps. VPVet first\nanalyzes the availability and completeness of a VR privacy policy and then\nrefines its analysis based on three key criteria: granularity, minimization,\nand consistency of CUS statements. Our study establishes the first and\ncurrently largest VR privacy policy dataset named VRPP, consisting of privacy\npolicies of 11,923 different VR apps from 10 mainstream platforms. Our vetting\nresults reveal severe privacy issues within the VR ecosystem, including the\nlimited availability and poor quality of privacy policies, along with their\ncoarse granularity, lack of adaptation to VR traits and the inconsistency\nbetween CUS statements in privacy policies and their actual behaviors. We\nopen-source VPVet system along with our findings at repository\nhttps://github.com/kalamoo/PPAudit, aiming to raise awareness within the VR\ncommunity and pave the way for further research in this field.",
    "updated" : "2024-09-01T15:07:11Z",
    "published" : "2024-09-01T15:07:11Z",
    "authors" : [
      {
        "name" : "Yuxia Zhan"
      },
      {
        "name" : "Yan Meng"
      },
      {
        "name" : "Lu Zhou"
      },
      {
        "name" : "Yichang Xiong"
      },
      {
        "name" : "Xiaokuan Zhang"
      },
      {
        "name" : "Lichuan Ma"
      },
      {
        "name" : "Guoxing Chen"
      },
      {
        "name" : "Qingqi Pei"
      },
      {
        "name" : "Haojin Zhu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.00739v1",
    "title" : "Designing and Evaluating Scalable Privacy Awareness and Control User\n  Interfaces for Mixed Reality",
    "summary" : "As Mixed Reality (MR) devices become increasingly popular across industries,\nthey raise significant privacy and ethical concerns due to their capacity to\ncollect extensive data on users and their environments. This paper highlights\nthe urgent need for privacy-aware user interfaces that educate and empower both\nusers and bystanders, enabling them to understand, control, and manage data\ncollection and sharing. Key research questions include improving user awareness\nof privacy implications, developing usable privacy controls, and evaluating the\neffectiveness of these measures in real-world settings. The proposed research\nroadmap aims to embed privacy considerations into the design and development of\nMR technologies, promoting responsible innovation that safeguards user privacy\nwhile preserving the functionality and appeal of these emerging technologies.",
    "updated" : "2024-09-01T15:06:40Z",
    "published" : "2024-09-01T15:06:40Z",
    "authors" : [
      {
        "name" : "Marvin Strauss"
      },
      {
        "name" : "Viktorija Paneva"
      },
      {
        "name" : "Florian Alt"
      },
      {
        "name" : "Stefan Schneegass"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02715v1",
    "title" : "Recoverable Anonymization for Pose Estimation: A Privacy-Enhancing\n  Approach",
    "summary" : "Human pose estimation (HPE) is crucial for various applications. However,\ndeploying HPE algorithms in surveillance contexts raises significant privacy\nconcerns due to the potential leakage of sensitive personal information (SPI)\nsuch as facial features, and ethnicity. Existing privacy-enhancing methods\noften compromise either privacy or performance, or they require costly\nadditional modalities. We propose a novel privacy-enhancing system that\ngenerates privacy-enhanced portraits while maintaining high HPE performance.\nOur key innovations include the reversible recovery of SPI for authorized\npersonnel and the preservation of contextual information. By jointly optimizing\na privacy-enhancing module, a privacy recovery module, and a pose estimator,\nour system ensures robust privacy protection, efficient SPI recovery, and\nhigh-performance HPE. Experimental results demonstrate the system's robust\nperformance in privacy enhancement, SPI recovery, and HPE.",
    "updated" : "2024-09-01T05:58:00Z",
    "published" : "2024-09-01T05:58:00Z",
    "authors" : [
      {
        "name" : "Wenjun Huang"
      },
      {
        "name" : "Yang Ni"
      },
      {
        "name" : "Arghavan Rezvani"
      },
      {
        "name" : "SungHeon Jeong"
      },
      {
        "name" : "Hanning Chen"
      },
      {
        "name" : "Yezi Liu"
      },
      {
        "name" : "Fei Wen"
      },
      {
        "name" : "Mohsen Imani"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03707v1",
    "title" : "A Different Level Text Protection Mechanism With Differential Privacy",
    "summary" : "The article introduces a method for extracting words of different degrees of\nimportance based on the BERT pre-training model and proves the effectiveness of\nthis method. The article also discusses the impact of maintaining the same\nperturbation results for words of different importance on the overall text\nutility. This method can be applied to long text protection.",
    "updated" : "2024-09-05T17:13:38Z",
    "published" : "2024-09-05T17:13:38Z",
    "authors" : [
      {
        "name" : "Qingwen Fu"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03655v1",
    "title" : "Privacy versus Emotion Preservation Trade-offs in Emotion-Preserving\n  Speaker Anonymization",
    "summary" : "Advances in speech technology now allow unprecedented access to personally\nidentifiable information through speech. To protect such information, the\ndifferential privacy field has explored ways to anonymize speech while\npreserving its utility, including linguistic and paralinguistic aspects.\nHowever, anonymizing speech while maintaining emotional state remains\nchallenging. We explore this problem in the context of the VoicePrivacy 2024\nchallenge. Specifically, we developed various speaker anonymization pipelines\nand find that approaches either excel at anonymization or preserving emotion\nstate, but not both simultaneously. Achieving both would require an in-domain\nemotion recognizer. Additionally, we found that it is feasible to train a\nsemi-effective speaker verification system using only emotion representations,\ndemonstrating the challenge of separating these two modalities.",
    "updated" : "2024-09-05T16:10:31Z",
    "published" : "2024-09-05T16:10:31Z",
    "authors" : [
      {
        "name" : "Zexin Cai"
      },
      {
        "name" : "Henry Li Xinyuan"
      },
      {
        "name" : "Ashi Garg"
      },
      {
        "name" : "Leibny Paola García-Perera"
      },
      {
        "name" : "Kevin Duh"
      },
      {
        "name" : "Sanjeev Khudanpur"
      },
      {
        "name" : "Nicholas Andrews"
      },
      {
        "name" : "Matthew Wiesner"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03568v1",
    "title" : "Enabling Practical and Privacy-Preserving Image Processing",
    "summary" : "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
    "updated" : "2024-09-05T14:22:02Z",
    "published" : "2024-09-05T14:22:02Z",
    "authors" : [
      {
        "name" : "Chao Wang"
      },
      {
        "name" : "Shubing Yang"
      },
      {
        "name" : "Xiaoyan Sun"
      },
      {
        "name" : "Jun Dai"
      },
      {
        "name" : "Dongfang Zhao"
      }
    ],
    "categories" : [
      "cs.CR",
      "C.2.0; K.6.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03344v1",
    "title" : "Rethinking Improved Privacy-Utility Trade-off with Pre-existing\n  Knowledge for DP Training",
    "summary" : "Differential privacy (DP) provides a provable framework for protecting\nindividuals by customizing a random mechanism over a privacy-sensitive dataset.\nDeep learning models have demonstrated privacy risks in model exposure as an\nestablished learning model unintentionally records membership-level privacy\nleakage. Differentially private stochastic gradient descent (DP- SGD) has been\nproposed to safeguard training individuals by adding random Gaussian noise to\ngradient updates in the backpropagation. Researchers identify that DP-SGD\ntypically causes utility loss since the injected homogeneous noise alters the\ngradient updates calculated at each iteration. Namely, all elements in the\ngradient are contaminated regardless of their importance in updating model\nparameters. In this work, we argue that the utility loss mainly results from\nthe homogeneity of injected noise. Consequently, we propose a generic\ndifferential privacy framework with heterogeneous noise (DP-Hero) by defining a\nheterogeneous random mechanism to abstract its property. The insight of DP-Hero\nis to leverage the knowledge encoded in the previously trained model to guide\nthe subsequent allocation of noise heterogeneity, thereby leveraging the\nstatistical perturbation and achieving enhanced utility. Atop DP-Hero, we\ninstantiate a heterogeneous version of DP-SGD, where the noise injected into\ngradients is heterogeneous and guided by prior-established model parameters. We\nconduct comprehensive experiments to verify and explain the effectiveness of\nthe proposed DP-Hero, showing improved training accuracy compared with\nstate-of-the-art works. Broadly, we shed light on improving the privacy-utility\nspace by learning the noise guidance from the pre-existing leaked knowledge\nencoded in the previously trained model, showing a different perspective of\nunderstanding the utility-improved DP training.",
    "updated" : "2024-09-05T08:40:54Z",
    "published" : "2024-09-05T08:40:54Z",
    "authors" : [
      {
        "name" : "Yu Zheng"
      },
      {
        "name" : "Wenchao Zhang"
      },
      {
        "name" : "Yonggang Zhang"
      },
      {
        "name" : "Wei Song"
      },
      {
        "name" : "Kai Zhou"
      },
      {
        "name" : "Bo Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03326v1",
    "title" : "Enhancing User-Centric Privacy Protection: An Interactive Framework\n  through Diffusion Models and Machine Unlearning",
    "summary" : "In the realm of multimedia data analysis, the extensive use of image datasets\nhas escalated concerns over privacy protection within such data. Current\nresearch predominantly focuses on privacy protection either in data sharing or\nupon the release of trained machine learning models. Our study pioneers a\ncomprehensive privacy protection framework that safeguards image data privacy\nconcurrently during data sharing and model publication. We propose an\ninteractive image privacy protection framework that utilizes generative machine\nlearning models to modify image information at the attribute level and employs\nmachine unlearning algorithms for the privacy preservation of model parameters.\nThis user-interactive framework allows for adjustments in privacy protection\nintensity based on user feedback on generated images, striking a balance\nbetween maximal privacy safeguarding and maintaining model performance. Within\nthis framework, we instantiate two modules: a differential privacy diffusion\nmodel for protecting attribute information in images and a feature unlearning\nalgorithm for efficient updates of the trained model on the revised image\ndataset. Our approach demonstrated superiority over existing methods on facial\ndatasets across various attribute classifications.",
    "updated" : "2024-09-05T07:55:55Z",
    "published" : "2024-09-05T07:55:55Z",
    "authors" : [
      {
        "name" : "Huaxi Huang"
      },
      {
        "name" : "Xin Yuan"
      },
      {
        "name" : "Qiyu Liao"
      },
      {
        "name" : "Dadong Wang"
      },
      {
        "name" : "Tongliang Liu"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03294v1",
    "title" : "Federated Prototype-based Contrastive Learning for Privacy-Preserving\n  Cross-domain Recommendation",
    "summary" : "Cross-domain recommendation (CDR) aims to improve recommendation accuracy in\nsparse domains by transferring knowledge from data-rich domains. However,\nexisting CDR methods often assume the availability of user-item interaction\ndata across domains, overlooking user privacy concerns. Furthermore, these\nmethods suffer from performance degradation in scenarios with sparse\noverlapping users, as they typically depend on a large number of fully shared\nusers for effective knowledge transfer. To address these challenges, we propose\na Federated Prototype-based Contrastive Learning (CL) method for\nPrivacy-Preserving CDR, named FedPCL-CDR. This approach utilizes\nnon-overlapping user information and prototypes to improve multi-domain\nperformance while protecting user privacy. FedPCL-CDR comprises two modules:\nlocal domain (client) learning and global server aggregation. In the local\ndomain, FedPCL-CDR clusters all user data to learn representative prototypes,\neffectively utilizing non-overlapping user information and addressing the\nsparse overlapping user issue. It then facilitates knowledge transfer by\nemploying both local and global prototypes returned from the server in a CL\nmanner. Simultaneously, the global server aggregates representative prototypes\nfrom local domains to learn both local and global prototypes. The combination\nof prototypes and federated learning (FL) ensures that sensitive user data\nremains decentralized, with only prototypes being shared across domains,\nthereby protecting user privacy. Extensive experiments on four CDR tasks using\ntwo real-world datasets demonstrate that FedPCL-CDR outperforms the\nstate-of-the-art baselines.",
    "updated" : "2024-09-05T06:59:56Z",
    "published" : "2024-09-05T06:59:56Z",
    "authors" : [
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Quangui Zhang"
      },
      {
        "name" : "Lei Sang"
      },
      {
        "name" : "Qiang Wu"
      },
      {
        "name" : "Min Xu"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04366v1",
    "title" : "Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue",
    "summary" : "Many blockchain networks aim to preserve the anonymity of validators in the\npeer-to-peer (P2P) network, ensuring that no adversary can link a validator's\nidentifier to the IP address of a peer due to associated privacy and security\nconcerns. This work demonstrates that the Ethereum P2P network does not offer\nthis anonymity. We present a methodology that enables any node in the network\nto identify validators hosted on connected peers and empirically verify the\nfeasibility of our proposed method. Using data collected from four nodes over\nthree days, we locate more than 15% of Ethereum validators in the P2P network.\nThe insights gained from our deanonymization technique provide valuable\ninformation on the distribution of validators across peers, their geographic\nlocations, and hosting organizations. We further discuss the implications and\nrisks associated with the lack of anonymity in the P2P network and propose\nmethods to help validators protect their privacy. The Ethereum Foundation has\nawarded us a bug bounty, acknowledging the impact of our results.",
    "updated" : "2024-09-06T15:57:43Z",
    "published" : "2024-09-06T15:57:43Z",
    "authors" : [
      {
        "name" : "Lioba Heimbach"
      },
      {
        "name" : "Yann Vonlanthen"
      },
      {
        "name" : "Juan Villacis"
      },
      {
        "name" : "Lucianna Kiffer"
      },
      {
        "name" : "Roger Wattenhofer"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04257v1",
    "title" : "Privacy risk from synthetic data: practical proposals",
    "summary" : "This paper proposes and compares measures of identity and attribute\ndisclosure risk for synthetic data. Data custodians can use the methods\nproposed here to inform the decision as to whether to release synthetic\nversions of confidential data. Different measures are evaluated on two data\nsets. Insight into the measures is obtained by examining the details of the\nrecords identified as posing a disclosure risk. This leads to methods to\nidentify, and possibly exclude, apparently risky records where the\nidentification or attribution would be expected by someone with background\nknowledge of the data. The methods described are available as part of the\n\\textbf{synthpop} package for \\textbf{R}.",
    "updated" : "2024-09-06T13:10:40Z",
    "published" : "2024-09-06T13:10:40Z",
    "authors" : [
      {
        "name" : "Gillian M Raab"
      }
    ],
    "categories" : [
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04194v1",
    "title" : "Towards Privacy-Preserving Relational Data Synthesis via Probabilistic\n  Relational Models",
    "summary" : "Probabilistic relational models provide a well-established formalism to\ncombine first-order logic and probabilistic models, thereby allowing to\nrepresent relationships between objects in a relational domain. At the same\ntime, the field of artificial intelligence requires increasingly large amounts\nof relational training data for various machine learning tasks. Collecting\nreal-world data, however, is often challenging due to privacy concerns, data\nprotection regulations, high costs, and so on. To mitigate these challenges,\nthe generation of synthetic data is a promising approach. In this paper, we\nsolve the problem of generating synthetic relational data via probabilistic\nrelational models. In particular, we propose a fully-fledged pipeline to go\nfrom relational database to probabilistic relational model, which can then be\nused to sample new synthetic relational data points from its underlying\nprobability distribution. As part of our proposed pipeline, we introduce a\nlearning algorithm to construct a probabilistic relational model from a given\nrelational database.",
    "updated" : "2024-09-06T11:24:25Z",
    "published" : "2024-09-06T11:24:25Z",
    "authors" : [
      {
        "name" : "Malte Luttermann"
      },
      {
        "name" : "Ralf Möller"
      },
      {
        "name" : "Mattis Hartwig"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04173v1",
    "title" : "NPU-NTU System for Voice Privacy 2024 Challenge",
    "summary" : "Speaker anonymization is an effective privacy protection solution that\nconceals the speaker's identity while preserving the linguistic content and\nparalinguistic information of the original speech. To establish a fair\nbenchmark and facilitate comparison of speaker anonymization systems, the\nVoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition\nplanned for 2024. In this paper, we describe our proposed speaker anonymization\nsystem for VPC 2024. Our system employs a disentangled neural codec\narchitecture and a serial disentanglement strategy to gradually disentangle the\nglobal speaker identity and time-variant linguistic content and paralinguistic\ninformation. We introduce multiple distillation methods to disentangle\nlinguistic content, speaker identity, and emotion. These methods include\nsemantic distillation, supervised speaker distillation, and frame-level emotion\ndistillation. Based on these distillations, we anonymize the original speaker\nidentity using a weighted sum of a set of candidate speaker identities and a\nrandomly generated speaker identity. Our system achieves the best trade-off of\nprivacy protection and emotion preservation in VPC 2024.",
    "updated" : "2024-09-06T10:32:42Z",
    "published" : "2024-09-06T10:32:42Z",
    "authors" : [
      {
        "name" : "Jixun Yao"
      },
      {
        "name" : "Nikita Kuzmin"
      },
      {
        "name" : "Qing Wang"
      },
      {
        "name" : "Pengcheng Guo"
      },
      {
        "name" : "Ziqian Ning"
      },
      {
        "name" : "Dake Guo"
      },
      {
        "name" : "Kong Aik Lee"
      },
      {
        "name" : "Eng-Siong Chng"
      },
      {
        "name" : "Lei Xie"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04167v1",
    "title" : "Do Android App Developers Accurately Report Collection of\n  Privacy-Related Data?",
    "summary" : "Many Android applications collect data from users. The European Union's\nGeneral Data Protection Regulation (GDPR) requires vendors to faithfully\ndisclose which data their apps collect. This task is complicated because many\napps use third-party code for which the same information is not readily\navailable. Hence we ask: how accurately do current Android apps fulfill these\nrequirements?\n  In this work, we first expose a multi-layered definition of privacy-related\ndata to correctly report data collection in Android apps. We further create a\ndataset of privacy-sensitive data classes that may be used as input by an\nAndroid app. This dataset takes into account data collected both through the\nuser interface and system APIs.\n  We manually examine the data safety sections of 70 Android apps to observe\nhow data collection is reported, identifying instances of over- and\nunder-reporting. Additionally, we develop a prototype to statically extract and\nlabel privacy-related data collected via app source code, user interfaces, and\npermissions. Comparing the prototype's results with the data safety sections of\n20 apps reveals reporting discrepancies. Using the results from two Messaging\nand Social Media apps (Signal and Instagram), we discuss how app developers\nunder-report and over-report data collection, respectively, and identify\ninaccurately reported data categories.\n  Our results show that app developers struggle to accurately report data\ncollection, either due to Google's abstract definition of collected data or\ninsufficient existing tool support.",
    "updated" : "2024-09-06T10:05:45Z",
    "published" : "2024-09-06T10:05:45Z",
    "authors" : [
      {
        "name" : "Mugdha Khedkar"
      },
      {
        "name" : "Ambuj Kumar Mondal"
      },
      {
        "name" : "Eric Bodden"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04048v1",
    "title" : "Exploring User Privacy Awareness on GitHub: An Empirical Study",
    "summary" : "GitHub provides developers with a practical way to distribute source code and\ncollaboratively work on common projects. To enhance account security and\nprivacy, GitHub allows its users to manage access permissions, review audit\nlogs, and enable two-factor authentication. However, despite the endless\neffort, the platform still faces various issues related to the privacy of its\nusers. This paper presents an empirical study delving into the GitHub\necosystem. Our focus is on investigating the utilization of privacy settings on\nthe platform and identifying various types of sensitive information disclosed\nby users. Leveraging a dataset comprising 6,132 developers, we report and\nanalyze their activities by means of comments on pull requests. Our findings\nindicate an active engagement by users with the available privacy settings on\nGitHub. Notably, we observe the disclosure of different forms of private\ninformation within pull request comments. This observation has prompted our\nexploration into sensitivity detection using a large language model and BERT,\nto pave the way for a personalized privacy assistant. Our work provides\ninsights into the utilization of existing privacy protection tools, such as\nprivacy settings, along with their inherent limitations. Essentially, we aim to\nadvance research in this field by providing both the motivation for creating\nsuch privacy protection tools and a proposed methodology for personalizing\nthem.",
    "updated" : "2024-09-06T06:41:46Z",
    "published" : "2024-09-06T06:41:46Z",
    "authors" : [
      {
        "name" : "Costanza Alfieri"
      },
      {
        "name" : "Juri Di Rocco"
      },
      {
        "name" : "Phuong T. Nguyen"
      },
      {
        "name" : "Paola Inverardi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04026v1",
    "title" : "Efficient Fault-Tolerant Quantum Protocol for Differential Privacy in\n  the Shuffle Model",
    "summary" : "We present a quantum protocol which securely and implicitly implements a\nrandom shuffle to realize differential privacy in the shuffle model. The\nshuffle model of differential privacy amplifies privacy achievable via local\ndifferential privacy by randomly permuting the tuple of outcomes from data\ncontributors. In practice, one needs to address how this shuffle is\nimplemented. Examples include implementing the shuffle via mix-networks, or\nshuffling via a trusted third-party. These implementation specific issues raise\nnon-trivial computational and trust requirements in a classical system. We\npropose a quantum version of the protocol using entanglement of quantum states\nand show that the shuffle can be implemented without these extra requirements.\nOur protocol implements k-ary randomized response, for any value of k > 2, and\nfurthermore, can be efficiently implemented using fault-tolerant computation.",
    "updated" : "2024-09-06T04:53:19Z",
    "published" : "2024-09-06T04:53:19Z",
    "authors" : [
      {
        "name" : "Hassan Jameel Asghar"
      },
      {
        "name" : "Arghya Mukherjee"
      },
      {
        "name" : "Gavin K. Brennen"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03796v1",
    "title" : "Protecting Activity Sensing Data Privacy Using Hierarchical Information\n  Dissociation",
    "summary" : "Smartphones and wearable devices have been integrated into our daily lives,\noffering personalized services. However, many apps become overprivileged as\ntheir collected sensing data contains unnecessary sensitive information. For\nexample, mobile sensing data could reveal private attributes (e.g., gender and\nage) and unintended sensitive features (e.g., hand gestures when entering\npasswords). To prevent sensitive information leakage, existing methods must\nobtain private labels and users need to specify privacy policies. However, they\nonly achieve limited control over information disclosure. In this work, we\npresent Hippo to dissociate hierarchical information including private metadata\nand multi-grained activity information from the sensing data. Hippo achieves\nfine-grained control over the disclosure of sensitive information without\nrequiring private labels. Specifically, we design a latent guidance-based\ndiffusion model, which generates multi-grained versions of raw sensor data\nconditioned on hierarchical latent activity features. Hippo enables users to\ncontrol the disclosure of sensitive information in sensing data, ensuring their\nprivacy while preserving the necessary features to meet the utility\nrequirements of applications. Hippo is the first unified model that achieves\ntwo goals: perturbing the sensitive attributes and controlling the disclosure\nof sensitive information in mobile sensing data. Extensive experiments show\nthat Hippo can anonymize personal attributes and transform activity information\nat various resolutions across different types of sensing data.",
    "updated" : "2024-09-04T15:38:00Z",
    "published" : "2024-09-04T15:38:00Z",
    "authors" : [
      {
        "name" : "Guangjing Wang"
      },
      {
        "name" : "Hanqing Guo"
      },
      {
        "name" : "Yuanda Wang"
      },
      {
        "name" : "Bocheng Chen"
      },
      {
        "name" : "Ce Zhou"
      },
      {
        "name" : "Qiben Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.05623v1",
    "title" : "A Framework for Differential Privacy Against Timing Attacks",
    "summary" : "The standard definition of differential privacy (DP) ensures that a\nmechanism's output distribution on adjacent datasets is indistinguishable.\nHowever, real-world implementations of DP can, and often do, reveal information\nthrough their runtime distributions, making them susceptible to timing attacks.\nIn this work, we establish a general framework for ensuring differential\nprivacy in the presence of timing side channels. We define a new notion of\ntiming privacy, which captures programs that remain differentially private to\nan adversary that observes the program's runtime in addition to the output. Our\nframework enables chaining together component programs that are timing-stable\nfollowed by a random delay to obtain DP programs that achieve timing privacy.\nImportantly, our definitions allow for measuring timing privacy and output\nprivacy using different privacy measures. We illustrate how to instantiate our\nframework by giving programs for standard DP computations in the RAM and Word\nRAM models of computation. Furthermore, we show how our framework can be\nrealized in code through a natural extension of the OpenDP Programming\nFramework.",
    "updated" : "2024-09-09T13:56:04Z",
    "published" : "2024-09-09T13:56:04Z",
    "authors" : [
      {
        "name" : "Zachary Ratliff"
      },
      {
        "name" : "Salil Vadhan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.05249v1",
    "title" : "NetDPSyn: Synthesizing Network Traces under Differential Privacy",
    "summary" : "As the utilization of network traces for the network measurement research\nbecomes increasingly prevalent, concerns regarding privacy leakage from network\ntraces have garnered the public's attention. To safeguard network traces,\nresearchers have proposed the trace synthesis that retains the essential\nproperties of the raw data. However, previous works also show that synthesis\ntraces with generative models are vulnerable under linkage attacks.\n  This paper introduces NetDPSyn, the first system to synthesize high-fidelity\nnetwork traces under privacy guarantees. NetDPSyn is built with the\nDifferential Privacy (DP) framework as its core, which is significantly\ndifferent from prior works that apply DP when training the generative model.\nThe experiments conducted on three flow and two packet datasets indicate that\nNetDPSyn achieves much better data utility in downstream tasks like anomaly\ndetection. NetDPSyn is also 2.5 times faster than the other methods on average\nin data synthesis.",
    "updated" : "2024-09-08T23:54:00Z",
    "published" : "2024-09-08T23:54:00Z",
    "authors" : [
      {
        "name" : "Danyu Sun"
      },
      {
        "name" : "Joann Qiongna Chen"
      },
      {
        "name" : "Chen Gong"
      },
      {
        "name" : "Tianhao Wang"
      },
      {
        "name" : "Zhou Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04877v1",
    "title" : "Strong Privacy-Preserving Universally Composable AKA Protocol with\n  Seamless Handover Support for Mobile Virtual Network Operator",
    "summary" : "Consumers seeking a new mobile plan have many choices in the present mobile\nlandscape. The Mobile Virtual Network Operator (MVNO) has recently gained\nconsiderable attention among these options. MVNOs offer various benefits,\nmaking them an appealing choice for a majority of consumers. These advantages\nencompass flexibility, access to cutting-edge technologies, enhanced coverage,\nsuperior customer service, and substantial cost savings. Even though MVNO\noffers several advantages, it also creates some security and privacy concerns\nfor the customer simultaneously. For instance, in the existing solution, MVNO\nneeds to hand over all the sensitive details, including the users' identities\nand master secret keys of their customers, to a mobile operator (MNO) to\nvalidate the customers while offering any services. This allows MNOs to have\nunrestricted access to the MVNO subscribers' location and mobile data,\nincluding voice calls, SMS, and Internet, which the MNOs frequently sell to\nthird parties (e.g., advertisement companies and surveillance agencies) for\nmore profit. Although critical for mass users, such privacy loss has been\nhistorically ignored due to the lack of practical and privacy-preserving\nsolutions for registration and handover procedures in cellular networks. In\nthis paper, we propose a universally composable authentication and handover\nscheme with strong user privacy support, where each MVNO user can validate a\nmobile operator (MNO) and vice-versa without compromising user anonymity and\nunlinkability support. Here, we anticipate that our proposed solution will most\nlikely be deployed by the MVNO(s) to ensure enhanced privacy support to their\ncustomer(s).",
    "updated" : "2024-09-07T18:04:54Z",
    "published" : "2024-09-07T18:04:54Z",
    "authors" : [
      {
        "name" : "Rabiah Alnashwan"
      },
      {
        "name" : "Yang Yang"
      },
      {
        "name" : "Yilu Dong"
      },
      {
        "name" : "Prosanta Gope"
      },
      {
        "name" : "Behzad Abdolmaleki"
      },
      {
        "name" : "Syed Rafiul Hussain"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04716v1",
    "title" : "Privacy enhanced collaborative inference in the Cox proportional hazards\n  model for distributed data",
    "summary" : "Data sharing barriers are paramount challenges arising from multicenter\nclinical studies where multiple data sources are stored in a distributed\nfashion at different local study sites. Particularly in the case of\ntime-to-event analysis when global risk sets are needed for the Cox\nproportional hazards model, access to a centralized database is typically\nnecessary. Merging such data sources into a common data storage for a\ncentralized statistical analysis requires a data use agreement, which is often\ntime-consuming. Furthermore, the construction and distribution of risk sets to\nparticipating clinical centers for subsequent calculations may pose a risk of\nrevealing individual-level information. We propose a new collaborative Cox\nmodel that eliminates the need for accessing the centralized database and\nconstructing global risk sets but needs only the sharing of summary statistics\nwith significantly smaller dimensions than risk sets. Thus, the proposed\ncollaborative inference enjoys maximal protection of data privacy. We show\ntheoretically and numerically that the new distributed proportional hazards\nmodel approach has little loss of statistical power when compared to the\ncentralized method that requires merging the entire data. We present a\nrenewable sieve method to establish large-sample properties for the proposed\nmethod. We illustrate its performance through simulation experiments and a\nreal-world data example from patients with kidney transplantation in the Organ\nProcurement and Transplantation Network (OPTN) to understand the factors\nassociated with the 5-year death-censored graft failure (DCGF) for patients who\nunderwent kidney transplants in the US.",
    "updated" : "2024-09-07T05:32:34Z",
    "published" : "2024-09-07T05:32:34Z",
    "authors" : [
      {
        "name" : "Mengtong Hu"
      },
      {
        "name" : "Xu Shi"
      },
      {
        "name" : "Peter X. -K. Song"
      }
    ],
    "categories" : [
      "stat.AP",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04652v1",
    "title" : "Privacy-Preserving Race/Ethnicity Estimation for Algorithmic Bias\n  Measurement in the U.S",
    "summary" : "AI fairness measurements, including tests for equal treatment, often take the\nform of disaggregated evaluations of AI systems. Such measurements are an\nimportant part of Responsible AI operations. These measurements compare system\nperformance across demographic groups or sub-populations and typically require\nmember-level demographic signals such as gender, race, ethnicity, and location.\nHowever, sensitive member-level demographic attributes like race and ethnicity\ncan be challenging to obtain and use due to platform choices, legal\nconstraints, and cultural norms. In this paper, we focus on the task of\nenabling AI fairness measurements on race/ethnicity for \\emph{U.S. LinkedIn\nmembers} in a privacy-preserving manner. We present the Privacy-Preserving\nProbabilistic Race/Ethnicity Estimation (PPRE) method for performing this task.\nPPRE combines the Bayesian Improved Surname Geocoding (BISG) model, a sparse\nLinkedIn survey sample of self-reported demographics, and privacy-enhancing\ntechnologies like secure two-party computation and differential privacy to\nenable meaningful fairness measurements while preserving member privacy. We\nprovide details of the PPRE method and its privacy guarantees. We then\nillustrate sample measurement operations. We conclude with a review of open\nresearch and engineering challenges for expanding our privacy-preserving\nfairness measurement capabilities.",
    "updated" : "2024-09-06T23:29:18Z",
    "published" : "2024-09-06T23:29:18Z",
    "authors" : [
      {
        "name" : "Saikrishna Badrinarayanan"
      },
      {
        "name" : "Osonde Osoba"
      },
      {
        "name" : "Miao Cheng"
      },
      {
        "name" : "Ryan Rogers"
      },
      {
        "name" : "Sakshi Jain"
      },
      {
        "name" : "Rahul Tandra"
      },
      {
        "name" : "Natesh S. Pillai"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06564v1",
    "title" : "Advancing Android Privacy Assessments with Automation",
    "summary" : "Android apps collecting data from users must comply with legal frameworks to\nensure data protection. This requirement has become even more important since\nthe implementation of the General Data Protection Regulation (GDPR) by the\nEuropean Union in 2018. Moreover, with the proposed Cyber Resilience Act on the\nhorizon, stakeholders will soon need to assess software against even more\nstringent security and privacy standards. Effective privacy assessments require\ncollaboration among groups with diverse expertise to function effectively as a\ncohesive unit.\n  This paper motivates the need for an automated approach that enhances\nunderstanding of data protection in Android apps and improves communication\nbetween the various parties involved in privacy assessments. We propose the\nAssessor View, a tool designed to bridge the knowledge gap between these\nparties, facilitating more effective privacy assessments of Android\napplications.",
    "updated" : "2024-09-10T14:56:51Z",
    "published" : "2024-09-10T14:56:51Z",
    "authors" : [
      {
        "name" : "Mugdha Khedkar"
      },
      {
        "name" : "Michael Schlichtig"
      },
      {
        "name" : "Eric Bodden"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06455v1",
    "title" : "Continual Domain Incremental Learning for Privacy-aware Digital\n  Pathology",
    "summary" : "In recent years, there has been remarkable progress in the field of digital\npathology, driven by the ability to model complex tissue patterns using\nadvanced deep-learning algorithms. However, the robustness of these models is\noften severely compromised in the presence of data shifts (e.g., different\nstains, organs, centers, etc.). Alternatively, continual learning (CL)\ntechniques aim to reduce the forgetting of past data when learning new data\nwith distributional shift conditions. Specifically, rehearsal-based CL\ntechniques, which store some past data in a buffer and then replay it with new\ndata, have proven effective in medical image analysis tasks. However, privacy\nconcerns arise as these approaches store past data, prompting the development\nof our novel Generative Latent Replay-based CL (GLRCL) approach. GLRCL captures\nthe previous distribution through Gaussian Mixture Models instead of storing\npast samples, which are then utilized to generate features and perform latent\nreplay with new data. We systematically evaluate our proposed framework under\ndifferent shift conditions in histopathology data, including stain and organ\nshift. Our approach significantly outperforms popular buffer-free CL approaches\nand performs similarly to rehearsal-based CL approaches that require large\nbuffers causing serious privacy violations.",
    "updated" : "2024-09-10T12:21:54Z",
    "published" : "2024-09-10T12:21:54Z",
    "authors" : [
      {
        "name" : "Pratibha Kumari"
      },
      {
        "name" : "Daniel Reisenbüchler"
      },
      {
        "name" : "Lucas Luttner"
      },
      {
        "name" : "Nadine S. Schaadt"
      },
      {
        "name" : "Friedrich Feuerhake"
      },
      {
        "name" : "Dorit Merhof"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06422v1",
    "title" : "A Pervasive, Efficient and Private Future: Realizing Privacy-Preserving\n  Machine Learning Through Hybrid Homomorphic Encryption",
    "summary" : "Machine Learning (ML) has become one of the most impactful fields of data\nscience in recent years. However, a significant concern with ML is its privacy\nrisks due to rising attacks against ML models. Privacy-Preserving Machine\nLearning (PPML) methods have been proposed to mitigate the privacy and security\nrisks of ML models. A popular approach to achieving PPML uses Homomorphic\nEncryption (HE). However, the highly publicized inefficiencies of HE make it\nunsuitable for highly scalable scenarios with resource-constrained devices.\nHence, Hybrid Homomorphic Encryption (HHE) -- a modern encryption scheme that\ncombines symmetric cryptography with HE -- has recently been introduced to\novercome these challenges. HHE potentially provides a foundation to build new\nefficient and privacy-preserving services that transfer expensive HE operations\nto the cloud. This work introduces HHE to the ML field by proposing\nresource-friendly PPML protocols for edge devices. More precisely, we utilize\nHHE as the primary building block of our PPML protocols. We assess the\nperformance of our protocols by first extensively evaluating each party's\ncommunication and computational cost on a dummy dataset and show the efficiency\nof our protocols by comparing them with similar protocols implemented using\nplain BFV. Subsequently, we demonstrate the real-world applicability of our\nconstruction by building an actual PPML application that uses HHE as its\nfoundation to classify heart disease based on sensitive ECG data.",
    "updated" : "2024-09-10T11:04:14Z",
    "published" : "2024-09-10T11:04:14Z",
    "authors" : [
      {
        "name" : "Khoa Nguyen"
      },
      {
        "name" : "Mindaugas Budzys"
      },
      {
        "name" : "Eugene Frimpong"
      },
      {
        "name" : "Tanveer Khan"
      },
      {
        "name" : "Antonis Michalas"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06360v1",
    "title" : "SoK: Evaluating 5G Protocols Against Legacy and Emerging Privacy and\n  Security Attacks",
    "summary" : "Ensuring user privacy remains a critical concern within mobile cellular\nnetworks, particularly given the proliferation of interconnected devices and\nservices. In fact, a lot of user privacy issues have been raised in 2G, 3G,\n4G/LTE networks. Recognizing this general concern, 3GPP has prioritized\naddressing these issues in the development of 5G, implementing numerous\nmodifications to enhance user privacy since 5G Release 15. In this\nsystematization of knowledge paper, we first provide a framework for studying\nprivacy and security related attacks in cellular networks, setting as privacy\nobjective the User Identity Confidentiality defined in 3GPP standards. Using\nthis framework, we discuss existing privacy and security attacks in pre-5G\nnetworks, analyzing the weaknesses that lead to these attacks. Furthermore, we\nthoroughly study the security characteristics of 5G up to the new Release 19,\nand examine mitigation mechanisms of 5G to the identified pre-5G attacks.\nAfterwards, we analyze how recent 5G attacks try to overcome these mitigation\nmechanisms. Finally, we identify current limitations and open problems in\nsecurity of 5G, and propose directions for future work.",
    "updated" : "2024-09-10T09:30:37Z",
    "published" : "2024-09-10T09:30:37Z",
    "authors" : [
      {
        "name" : "Stavros Eleftherakis"
      },
      {
        "name" : "Domenico Giustiniano"
      },
      {
        "name" : "Nicolas Kourtellis"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06233v1",
    "title" : "VBIT: Towards Enhancing Privacy Control Over IoT Devices",
    "summary" : "Internet-of-Things (IoT) devices are increasingly deployed at home, at work,\nand in other shared and public spaces. IoT devices collect and share data with\nservice providers and third parties, which poses privacy concerns. Although\nprivacy enhancing tools are quite advanced in other applications domains (\\eg~\nadvertising and tracker blockers for browsers), users have currently no\nconvenient way to know or manage what and how data is collected and shared by\nIoT devices. In this paper, we present VBIT, an interactive system combining\nMixed Reality (MR) and web-based applications that allows users to: (1) uncover\nand visualize tracking services by IoT devices in an instrumented space and (2)\ntake action to stop or limit that tracking. We design and implement VBIT to\noperate at the network traffic level, and we show that it has negligible\nperformance overhead, and offers flexibility and good usability. We perform a\nmixed-method user study consisting of an online survey and an in-person\ninterview study. We show that VBIT users appreciate VBIT's transparency,\ncontrol, and customization features, and they become significantly more willing\nto install an IoT advertising and tracking blocker, after using VBIT. In the\nprocess, we obtain design insights that can be used to further iterate and\nimprove the design of VBIT and other systems for IoT transparency and control.",
    "updated" : "2024-09-10T06:00:50Z",
    "published" : "2024-09-10T06:00:50Z",
    "authors" : [
      {
        "name" : "Jad Al Aaraj"
      },
      {
        "name" : "Olivia Figueira"
      },
      {
        "name" : "Tu Le"
      },
      {
        "name" : "Isabela Figueira"
      },
      {
        "name" : "Rahmadi Trimananda"
      },
      {
        "name" : "Athina Markopoulou"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06069v1",
    "title" : "Privacy-Preserving Data Linkage Across Private and Public Datasets for\n  Collaborative Agriculture Research",
    "summary" : "Digital agriculture leverages technology to enhance crop yield, disease\nresilience, and soil health, playing a critical role in agricultural research.\nHowever, it raises privacy concerns such as adverse pricing, price\ndiscrimination, higher insurance costs, and manipulation of resources,\ndeterring farm operators from sharing data due to potential misuse. This study\nintroduces a privacy-preserving framework that addresses these risks while\nallowing secure data sharing for digital agriculture. Our framework enables\ncomprehensive data analysis while protecting privacy. It allows stakeholders to\nharness research-driven policies that link public and private datasets. The\nproposed algorithm achieves this by: (1) identifying similar farmers based on\nprivate datasets, (2) providing aggregate information like time and location,\n(3) determining trends in price and product availability, and (4) correlating\ntrends with public policy data, such as food insecurity statistics. We validate\nthe framework with real-world Farmer's Market datasets, demonstrating its\nefficacy through machine learning models trained on linked privacy-preserved\ndata. The results support policymakers and researchers in addressing food\ninsecurity and pricing issues. This work significantly contributes to digital\nagriculture by providing a secure method for integrating and analyzing data,\ndriving advancements in agricultural technology and development.",
    "updated" : "2024-09-09T21:07:13Z",
    "published" : "2024-09-09T21:07:13Z",
    "authors" : [
      {
        "name" : "Osama Zafar"
      },
      {
        "name" : "Rosemarie Santa Gonzalez"
      },
      {
        "name" : "Gabriel Wilkins"
      },
      {
        "name" : "Alfonso Morales"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04048v2",
    "title" : "Exploring User Privacy Awareness on GitHub: An Empirical Study",
    "summary" : "GitHub provides developers with a practical way to distribute source code and\ncollaboratively work on common projects. To enhance account security and\nprivacy, GitHub allows its users to manage access permissions, review audit\nlogs, and enable two-factor authentication. However, despite the endless\neffort, the platform still faces various issues related to the privacy of its\nusers. This paper presents an empirical study delving into the GitHub\necosystem. Our focus is on investigating the utilization of privacy settings on\nthe platform and identifying various types of sensitive information disclosed\nby users. Leveraging a dataset comprising 6,132 developers, we report and\nanalyze their activities by means of comments on pull requests. Our findings\nindicate an active engagement by users with the available privacy settings on\nGitHub. Notably, we observe the disclosure of different forms of private\ninformation within pull request comments. This observation has prompted our\nexploration into sensitivity detection using a large language model and BERT,\nto pave the way for a personalized privacy assistant. Our work provides\ninsights into the utilization of existing privacy protection tools, such as\nprivacy settings, along with their inherent limitations. Essentially, we aim to\nadvance research in this field by providing both the motivation for creating\nsuch privacy protection tools and a proposed methodology for personalizing\nthem.",
    "updated" : "2024-09-10T09:35:53Z",
    "published" : "2024-09-06T06:41:46Z",
    "authors" : [
      {
        "name" : "Costanza Alfieri"
      },
      {
        "name" : "Juri Di Rocco"
      },
      {
        "name" : "Paola Inverardi"
      },
      {
        "name" : "Phuong T. Nguyen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07444v1",
    "title" : "Echoes of Privacy: Uncovering the Profiling Practices of Voice\n  Assistants",
    "summary" : "Many companies, including Google, Amazon, and Apple, offer voice assistants\nas a convenient solution for answering general voice queries and accessing\ntheir services. These voice assistants have gained popularity and can be easily\naccessed through various smart devices such as smartphones, smart speakers,\nsmartwatches, and an increasing array of other devices. However, this\nconvenience comes with potential privacy risks. For instance, while companies\nvaguely mention in their privacy policies that they may use voice interactions\nfor user profiling, it remains unclear to what extent this profiling occurs and\nwhether voice interactions pose greater privacy risks compared to other\ninteraction modalities.\n  In this paper, we conduct 1171 experiments involving a total of 24530 queries\nwith different personas and interaction modalities over the course of 20 months\nto characterize how the three most popular voice assistants profile their\nusers. We analyze factors such as the labels assigned to users, their accuracy,\nthe time taken to assign these labels, differences between voice and web\ninteractions, and the effectiveness of profiling remediation tools offered by\neach voice assistant. Our findings reveal that profiling can happen without\ninteraction, can be incorrect and inconsistent at times, may take several days\nto weeks for changes to occur, and can be influenced by the interaction\nmodality.",
    "updated" : "2024-09-11T17:44:41Z",
    "published" : "2024-09-11T17:44:41Z",
    "authors" : [
      {
        "name" : "Tina Khezresmaeilzadeh"
      },
      {
        "name" : "Elaine Zhu"
      },
      {
        "name" : "Kiersten Grieco"
      },
      {
        "name" : "Daniel J. Dubois"
      },
      {
        "name" : "Konstantinos Psounis"
      },
      {
        "name" : "David Choffnes"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07415v1",
    "title" : "SoK: Security and Privacy Risks of Medical AI",
    "summary" : "The integration of technology and healthcare has ushered in a new era where\nsoftware systems, powered by artificial intelligence and machine learning, have\nbecome essential components of medical products and services. While these\nadvancements hold great promise for enhancing patient care and healthcare\ndelivery efficiency, they also expose sensitive medical data and system\nintegrity to potential cyberattacks. This paper explores the security and\nprivacy threats posed by AI/ML applications in healthcare. Through a thorough\nexamination of existing research across a range of medical domains, we have\nidentified significant gaps in understanding the adversarial attacks targeting\nmedical AI systems. By outlining specific adversarial threat models for medical\nsettings and identifying vulnerable application domains, we lay the groundwork\nfor future research that investigates the security and resilience of AI-driven\nmedical systems. Through our analysis of different threat models and\nfeasibility studies on adversarial attacks in different medical domains, we\nprovide compelling insights into the pressing need for cybersecurity research\nin the rapidly evolving field of AI healthcare technology.",
    "updated" : "2024-09-11T16:59:58Z",
    "published" : "2024-09-11T16:59:58Z",
    "authors" : [
      {
        "name" : "Yuanhaur Chang"
      },
      {
        "name" : "Han Liu"
      },
      {
        "name" : "Evin Jaff"
      },
      {
        "name" : "Chenyang Lu"
      },
      {
        "name" : "Ning Zhang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07224v1",
    "title" : "Analytic Class Incremental Learning for Sound Source Localization with\n  Privacy Protection",
    "summary" : "Sound Source Localization (SSL) enabling technology for applications such as\nsurveillance and robotics. While traditional Signal Processing (SP)-based SSL\nmethods provide analytic solutions under specific signal and noise assumptions,\nrecent Deep Learning (DL)-based methods have significantly outperformed them.\nHowever, their success depends on extensive training data and substantial\ncomputational resources. Moreover, they often rely on large-scale annotated\nspatial data and may struggle when adapting to evolving sound classes. To\nmitigate these challenges, we propose a novel Class Incremental Learning (CIL)\napproach, termed SSL-CIL, which avoids serious accuracy degradation due to\ncatastrophic forgetting by incrementally updating the DL-based SSL model\nthrough a closed-form analytic solution. In particular, data privacy is ensured\nsince the learning process does not revisit any historical data\n(exemplar-free), which is more suitable for smart home scenarios. Empirical\nresults in the public SSLR dataset demonstrate the superior performance of our\nproposal, achieving a localization accuracy of 90.9%, surpassing other\ncompetitive methods.",
    "updated" : "2024-09-11T12:31:07Z",
    "published" : "2024-09-11T12:31:07Z",
    "authors" : [
      {
        "name" : "Xinyuan Qian"
      },
      {
        "name" : "Xianghu Yue"
      },
      {
        "name" : "Jiadong Wang"
      },
      {
        "name" : "Huiping Zhuang"
      },
      {
        "name" : "Haizhou Li"
      }
    ],
    "categories" : [
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07187v1",
    "title" : "A Simple Linear Space Data Structure for ANN with Application in\n  Differential Privacy",
    "summary" : "Locality Sensitive Filters are known for offering a quasi-linear space data\nstructure with rigorous guarantees for the Approximate Near Neighbor search\nproblem. Building on Locality Sensitive Filters, we derive a simple data\nstructure for the Approximate Near Neighbor Counting problem under differential\nprivacy. Moreover, we provide a simple analysis leveraging a connection with\nconcomitant statistics and extreme value theory. Our approach achieves the same\nperformance as the recent findings of Andoni et al. (NeurIPS 2023) but with a\nmore straightforward method. As a side result, the paper provides a more\ncompact description and analysis of Locality Sensitive Filters for Approximate\nNear Neighbor Search under inner product similarity, improving a previous\nresult in Aum\\\"{u}ller et al. (TODS 2022).",
    "updated" : "2024-09-11T11:14:33Z",
    "published" : "2024-09-11T11:14:33Z",
    "authors" : [
      {
        "name" : "Martin Aumüller"
      },
      {
        "name" : "Fabrizio Boninsegna"
      },
      {
        "name" : "Francesco Silvestri"
      }
    ],
    "categories" : [
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06955v1",
    "title" : "Privacy-Preserving Federated Learning with Consistency via Knowledge\n  Distillation Using Conditional Generator",
    "summary" : "Federated Learning (FL) is gaining popularity as a distributed learning\nframework that only shares model parameters or gradient updates and keeps\nprivate data locally. However, FL is at risk of privacy leakage caused by\nprivacy inference attacks. And most existing privacy-preserving mechanisms in\nFL conflict with achieving high performance and efficiency. Therefore, we\npropose FedMD-CG, a novel FL method with highly competitive performance and\nhigh-level privacy preservation, which decouples each client's local model into\na feature extractor and a classifier, and utilizes a conditional generator\ninstead of the feature extractor to perform server-side model aggregation. To\nensure the consistency of local generators and classifiers, FedMD-CG leverages\nknowledge distillation to train local models and generators at both the latent\nfeature level and the logit level. Also, we construct additional classification\nlosses and design new diversity losses to enhance client-side training.\nFedMD-CG is robust to data heterogeneity and does not require training extra\ndiscriminators (like cGAN). We conduct extensive experiments on various image\nclassification tasks to validate the superiority of FedMD-CG.",
    "updated" : "2024-09-11T02:36:36Z",
    "published" : "2024-09-11T02:36:36Z",
    "authors" : [
      {
        "name" : "Kangyang Luo"
      },
      {
        "name" : "Shuai Wang"
      },
      {
        "name" : "Xiang Li"
      },
      {
        "name" : "Yunshi Lan"
      },
      {
        "name" : "Ming Gao"
      },
      {
        "name" : "Jinlong Shu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06233v1",
    "title" : "VBIT: Towards Enhancing Privacy Control Over IoT Devices",
    "summary" : "Internet-of-Things (IoT) devices are increasingly deployed at home, at work,\nand in other shared and public spaces. IoT devices collect and share data with\nservice providers and third parties, which poses privacy concerns. Although\nprivacy enhancing tools are quite advanced in other applications domains (\\eg~\nadvertising and tracker blockers for browsers), users have currently no\nconvenient way to know or manage what and how data is collected and shared by\nIoT devices. In this paper, we present VBIT, an interactive system combining\nMixed Reality (MR) and web-based applications that allows users to: (1) uncover\nand visualize tracking services by IoT devices in an instrumented space and (2)\ntake action to stop or limit that tracking. We design and implement VBIT to\noperate at the network traffic level, and we show that it has negligible\nperformance overhead, and offers flexibility and good usability. We perform a\nmixed-method user study consisting of an online survey and an in-person\ninterview study. We show that VBIT users appreciate VBIT's transparency,\ncontrol, and customization features, and they become significantly more willing\nto install an IoT advertising and tracking blocker, after using VBIT. In the\nprocess, we obtain design insights that can be used to further iterate and\nimprove the design of VBIT and other systems for IoT transparency and control.",
    "updated" : "2024-09-10T06:00:50Z",
    "published" : "2024-09-10T06:00:50Z",
    "authors" : [
      {
        "name" : "Jad Al Aaraj"
      },
      {
        "name" : "Olivia Figueira"
      },
      {
        "name" : "Tu Le"
      },
      {
        "name" : "Isabela Figueira"
      },
      {
        "name" : "Rahmadi Trimananda"
      },
      {
        "name" : "Athina Markopoulou"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06069v1",
    "title" : "Privacy-Preserving Data Linkage Across Private and Public Datasets for\n  Collaborative Agriculture Research",
    "summary" : "Digital agriculture leverages technology to enhance crop yield, disease\nresilience, and soil health, playing a critical role in agricultural research.\nHowever, it raises privacy concerns such as adverse pricing, price\ndiscrimination, higher insurance costs, and manipulation of resources,\ndeterring farm operators from sharing data due to potential misuse. This study\nintroduces a privacy-preserving framework that addresses these risks while\nallowing secure data sharing for digital agriculture. Our framework enables\ncomprehensive data analysis while protecting privacy. It allows stakeholders to\nharness research-driven policies that link public and private datasets. The\nproposed algorithm achieves this by: (1) identifying similar farmers based on\nprivate datasets, (2) providing aggregate information like time and location,\n(3) determining trends in price and product availability, and (4) correlating\ntrends with public policy data, such as food insecurity statistics. We validate\nthe framework with real-world Farmer's Market datasets, demonstrating its\nefficacy through machine learning models trained on linked privacy-preserved\ndata. The results support policymakers and researchers in addressing food\ninsecurity and pricing issues. This work significantly contributes to digital\nagriculture by providing a secure method for integrating and analyzing data,\ndriving advancements in agricultural technology and development.",
    "updated" : "2024-09-09T21:07:13Z",
    "published" : "2024-09-09T21:07:13Z",
    "authors" : [
      {
        "name" : "Osama Zafar"
      },
      {
        "name" : "Rosemarie Santa Gonzalez"
      },
      {
        "name" : "Gabriel Wilkins"
      },
      {
        "name" : "Alfonso Morales"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07997v1",
    "title" : "Privacy-preserving federated prediction of pain intensity change based\n  on multi-center survey data",
    "summary" : "Background: Patient-reported survey data are used to train prognostic models\naimed at improving healthcare. However, such data are typically available\nmulti-centric and, for privacy reasons, cannot easily be centralized in one\ndata repository. Models trained locally are less accurate, robust, and\ngeneralizable. We present and apply privacy-preserving federated machine\nlearning techniques for prognostic model building, where local survey data\nnever leaves the legally safe harbors of the medical centers. Methods: We used\ncentralized, local, and federated learning techniques on two healthcare\ndatasets (GLA:D data from the five health regions of Denmark and international\nSHARE data of 27 countries) to predict two different health outcomes. We\ncompared linear regression, random forest regression, and random forest\nclassification models trained on local data with those trained on the entire\ndata in a centralized and in a federated fashion. Results: In GLA:D data,\nfederated linear regression (R2 0.34, RMSE 18.2) and federated random forest\nregression (R2 0.34, RMSE 18.3) models outperform their local counterparts\n(i.e., R2 0.32, RMSE 18.6, R2 0.30, RMSE 18.8) with statistical significance.\nWe also found that centralized models (R2 0.34, RMSE 18.2, R2 0.32, RMSE 18.5,\nrespectively) did not perform significantly better than the federated models.\nIn SHARE, the federated model (AC 0.78, AUROC: 0.71) and centralized model (AC\n0.84, AUROC: 0.66) perform significantly better than the local models (AC:\n0.74, AUROC: 0.69). Conclusion: Federated learning enables the training of\nprognostic models from multi-center surveys without compromising privacy and\nwith only minimal or no compromise regarding model performance.",
    "updated" : "2024-09-12T12:41:58Z",
    "published" : "2024-09-12T12:41:58Z",
    "authors" : [
      {
        "name" : "Supratim Das"
      },
      {
        "name" : "Mahdie Rafie"
      },
      {
        "name" : "Paula Kammer"
      },
      {
        "name" : "Søren T. Skou"
      },
      {
        "name" : "Dorte T. Grønne"
      },
      {
        "name" : "Ewa M. Roos"
      },
      {
        "name" : "André Hajek"
      },
      {
        "name" : "Hans-Helmut König"
      },
      {
        "name" : "Md Shihab Ullaha"
      },
      {
        "name" : "Niklas Probul"
      },
      {
        "name" : "Jan Baumbacha"
      },
      {
        "name" : "Linda Baumbach"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07809v1",
    "title" : "Controllable Synthetic Clinical Note Generation with Privacy Guarantees",
    "summary" : "In the field of machine learning, domain-specific annotated data is an\ninvaluable resource for training effective models. However, in the medical\ndomain, this data often includes Personal Health Information (PHI), raising\nsignificant privacy concerns. The stringent regulations surrounding PHI limit\nthe availability and sharing of medical datasets, which poses a substantial\nchallenge for researchers and practitioners aiming to develop advanced machine\nlearning models. In this paper, we introduce a novel method to \"clone\" datasets\ncontaining PHI. Our approach ensures that the cloned datasets retain the\nessential characteristics and utility of the original data without compromising\npatient privacy. By leveraging differential-privacy techniques and a novel\nfine-tuning task, our method produces datasets that are free from identifiable\ninformation while preserving the statistical properties necessary for model\ntraining. We conduct utility testing to evaluate the performance of machine\nlearning models trained on the cloned datasets. The results demonstrate that\nour cloned datasets not only uphold privacy standards but also enhance model\nperformance compared to those trained on traditional anonymized datasets. This\nwork offers a viable solution for the ethical and effective utilization of\nsensitive medical data in machine learning, facilitating progress in medical\nresearch and the development of robust predictive models.",
    "updated" : "2024-09-12T07:38:34Z",
    "published" : "2024-09-12T07:38:34Z",
    "authors" : [
      {
        "name" : "Tal Baumel"
      },
      {
        "name" : "Andre Manoel"
      },
      {
        "name" : "Daniel Jones"
      },
      {
        "name" : "Shize Su"
      },
      {
        "name" : "Huseyin Inan"
      },
      {
        "name" : " Aaron"
      },
      {
        "name" : " Bornstein"
      },
      {
        "name" : "Robert Sim"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07773v1",
    "title" : "PDC-FRS: Privacy-preserving Data Contribution for Federated Recommender\n  System",
    "summary" : "Federated recommender systems (FedRecs) have emerged as a popular research\ndirection for protecting users' privacy in on-device recommendations. In\nFedRecs, users keep their data locally and only contribute their local\ncollaborative information by uploading model parameters to a central server.\nWhile this rigid framework protects users' raw data during training, it\nseverely compromises the recommendation model's performance due to the\nfollowing reasons: (1) Due to the power law distribution nature of user\nbehavior data, individual users have few data points to train a recommendation\nmodel, resulting in uploaded model updates that may be far from optimal; (2) As\neach user's uploaded parameters are learned from local data, which lacks global\ncollaborative information, relying solely on parameter aggregation methods such\nas FedAvg to fuse global collaborative information may be suboptimal. To bridge\nthis performance gap, we propose a novel federated recommendation framework,\nPDC-FRS. Specifically, we design a privacy-preserving data contribution\nmechanism that allows users to share their data with a differential privacy\nguarantee. Based on the shared but perturbed data, an auxiliary model is\ntrained in parallel with the original federated recommendation process. This\nauxiliary model enhances FedRec by augmenting each user's local dataset and\nintegrating global collaborative information. To demonstrate the effectiveness\nof PDC-FRS, we conduct extensive experiments on two widely used recommendation\ndatasets. The empirical results showcase the superiority of PDC-FRS compared to\nbaseline methods.",
    "updated" : "2024-09-12T06:13:07Z",
    "published" : "2024-09-12T06:13:07Z",
    "authors" : [
      {
        "name" : "Chaoqun Yang"
      },
      {
        "name" : "Wei Yuan"
      },
      {
        "name" : "Liang Qu"
      },
      {
        "name" : "Thanh Tam Nguyen"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07751v1",
    "title" : "Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption",
    "summary" : "The recently proposed Kolmogorov-Arnold Networks (KANs) offer enhanced\ninterpretability and greater model expressiveness. However, KANs also present\nchallenges related to privacy leakage during inference. Homomorphic encryption\n(HE) facilitates privacy-preserving inference for deep learning models,\nenabling resource-limited users to benefit from deep learning services while\nensuring data security. Yet, the complex structure of KANs, incorporating\nnonlinear elements like the SiLU activation function and B-spline functions,\nrenders existing privacy-preserving inference techniques inadequate. To address\nthis issue, we propose an accurate and efficient privacy-preserving inference\nscheme tailored for KANs. Our approach introduces a task-specific polynomial\napproximation for the SiLU activation function, dynamically adjusting the\napproximation range to ensure high accuracy on real-world datasets.\nAdditionally, we develop an efficient method for computing B-spline functions\nwithin the HE domain, leveraging techniques such as repeat packing, lazy\ncombination, and comparison functions. We evaluate the effectiveness of our\nprivacy-preserving KAN inference scheme on both symbolic formula evaluation and\nimage classification. The experimental results show that our model achieves\naccuracy comparable to plaintext KANs across various datasets and outperforms\nplaintext MLPs. Additionally, on the CIFAR-10 dataset, our inference latency\nachieves over 7 times speedup compared to the naive method.",
    "updated" : "2024-09-12T04:51:27Z",
    "published" : "2024-09-12T04:51:27Z",
    "authors" : [
      {
        "name" : "Zhizheng Lai"
      },
      {
        "name" : "Yufei Zhou"
      },
      {
        "name" : "Peijia Zheng"
      },
      {
        "name" : "Lin Chen"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.08913v1",
    "title" : "HLTCOE JHU Submission to the Voice Privacy Challenge 2024",
    "summary" : "We present a number of systems for the Voice Privacy Challenge, including\nvoice conversion based systems such as the kNN-VC method and the WavLM voice\nConversion method, and text-to-speech (TTS) based systems including\nWhisper-VITS. We found that while voice conversion systems better preserve\nemotional content, they struggle to conceal speaker identity in semi-white-box\nattack scenarios; conversely, TTS methods perform better at anonymization and\nworse at emotion preservation. Finally, we propose a random admixture system\nwhich seeks to balance out the strengths and weaknesses of the two category of\nsystems, achieving a strong EER of over 40% while maintaining UAR at a\nrespectable 47%.",
    "updated" : "2024-09-13T15:29:37Z",
    "published" : "2024-09-13T15:29:37Z",
    "authors" : [
      {
        "name" : "Henry Li Xinyuan"
      },
      {
        "name" : "Zexin Cai"
      },
      {
        "name" : "Ashi Garg"
      },
      {
        "name" : "Kevin Duh"
      },
      {
        "name" : "Leibny Paola García-Perera"
      },
      {
        "name" : "Sanjeev Khudanpur"
      },
      {
        "name" : "Nicholas Andrews"
      },
      {
        "name" : "Matthew Wiesner"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.08636v1",
    "title" : "Utilizing Data Fingerprints for Privacy-Preserving Algorithm Selection\n  in Time Series Classification: Performance and Uncertainty Estimation on\n  Unseen Datasets",
    "summary" : "The selection of algorithms is a crucial step in designing AI services for\nreal-world time series classification use cases. Traditional methods such as\nneural architecture search, automated machine learning, combined algorithm\nselection, and hyperparameter optimizations are effective but require\nconsiderable computational resources and necessitate access to all data points\nto run their optimizations. In this work, we introduce a novel data fingerprint\nthat describes any time series classification dataset in a privacy-preserving\nmanner and provides insight into the algorithm selection problem without\nrequiring training on the (unseen) dataset. By decomposing the multi-target\nregression problem, only our data fingerprints are used to estimate algorithm\nperformance and uncertainty in a scalable and adaptable manner. Our approach is\nevaluated on the 112 University of California riverside benchmark datasets,\ndemonstrating its effectiveness in predicting the performance of 35\nstate-of-the-art algorithms and providing valuable insights for effective\nalgorithm selection in time series classification service systems, improving a\nnaive baseline by 7.32% on average in estimating the mean performance and\n15.81% in estimating the uncertainty.",
    "updated" : "2024-09-13T08:43:42Z",
    "published" : "2024-09-13T08:43:42Z",
    "authors" : [
      {
        "name" : "Lars Böcking"
      },
      {
        "name" : "Leopold Müller"
      },
      {
        "name" : "Niklas Kühl"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.08538v1",
    "title" : "An Efficient Privacy-aware Split Learning Framework for Satellite\n  Communications",
    "summary" : "In the rapidly evolving domain of satellite communications, integrating\nadvanced machine learning techniques, particularly split learning, is crucial\nfor enhancing data processing and model training efficiency across satellites,\nspace stations, and ground stations. Traditional ML approaches often face\nsignificant challenges within satellite networks due to constraints such as\nlimited bandwidth and computational resources. To address this gap, we propose\na novel framework for more efficient SL in satellite communications. Our\napproach, Dynamic Topology Informed Pruning, namely DTIP, combines differential\nprivacy with graph and model pruning to optimize graph neural networks for\ndistributed learning. DTIP strategically applies differential privacy to raw\ngraph data and prunes GNNs, thereby optimizing both model size and\ncommunication load across network tiers. Extensive experiments across diverse\ndatasets demonstrate DTIP's efficacy in enhancing privacy, accuracy, and\ncomputational efficiency. Specifically, on Amazon2M dataset, DTIP maintains an\naccuracy of 0.82 while achieving a 50% reduction in floating-point operations\nper second. Similarly, on ArXiv dataset, DTIP achieves an accuracy of 0.85\nunder comparable conditions. Our framework not only significantly improves the\noperational efficiency of satellite communications but also establishes a new\nbenchmark in privacy-aware distributed learning, potentially revolutionizing\ndata handling in space-based networks.",
    "updated" : "2024-09-13T04:59:35Z",
    "published" : "2024-09-13T04:59:35Z",
    "authors" : [
      {
        "name" : "Jianfei Sun"
      },
      {
        "name" : "Cong Wu"
      },
      {
        "name" : "Shahid Mumtaz"
      },
      {
        "name" : "Junyi Tao"
      },
      {
        "name" : "Mingsheng Cao"
      },
      {
        "name" : "Mei Wang"
      },
      {
        "name" : "Valerio Frascolla"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.08503v1",
    "title" : "Enhancing Privacy in ControlNet and Stable Diffusion via Split Learning",
    "summary" : "With the emerging trend of large generative models, ControlNet is introduced\nto enable users to fine-tune pre-trained models with their own data for various\nuse cases. A natural question arises: how can we train ControlNet models while\nensuring users' data privacy across distributed devices? Exploring different\ndistributed training schemes, we find conventional federated learning and split\nlearning unsuitable. Instead, we propose a new distributed learning structure\nthat eliminates the need for the server to send gradients back. Through a\ncomprehensive evaluation of existing threats, we discover that in the context\nof training ControlNet with split learning, most existing attacks are\nineffective, except for two mentioned in previous literature. To counter these\nthreats, we leverage the properties of diffusion models and design a new\ntimestep sampling policy during forward processes. We further propose a\nprivacy-preserving activation function and a method to prevent private text\nprompts from leaving clients, tailored for image generation with diffusion\nmodels. Our experimental results demonstrate that our algorithms and systems\ngreatly enhance the efficiency of distributed training for ControlNet while\nensuring users' data privacy without compromising image generation quality.",
    "updated" : "2024-09-13T02:55:22Z",
    "published" : "2024-09-13T02:55:22Z",
    "authors" : [
      {
        "name" : "Dixi Yao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.07444v2",
    "title" : "Echoes of Privacy: Uncovering the Profiling Practices of Voice\n  Assistants",
    "summary" : "Many companies, including Google, Amazon, and Apple, offer voice assistants\nas a convenient solution for answering general voice queries and accessing\ntheir services. These voice assistants have gained popularity and can be easily\naccessed through various smart devices such as smartphones, smart speakers,\nsmartwatches, and an increasing array of other devices. However, this\nconvenience comes with potential privacy risks. For instance, while companies\nvaguely mention in their privacy policies that they may use voice interactions\nfor user profiling, it remains unclear to what extent this profiling occurs and\nwhether voice interactions pose greater privacy risks compared to other\ninteraction modalities.\n  In this paper, we conduct 1171 experiments involving a total of 24530 queries\nwith different personas and interaction modalities over the course of 20 months\nto characterize how the three most popular voice assistants profile their\nusers. We analyze factors such as the labels assigned to users, their accuracy,\nthe time taken to assign these labels, differences between voice and web\ninteractions, and the effectiveness of profiling remediation tools offered by\neach voice assistant. Our findings reveal that profiling can happen without\ninteraction, can be incorrect and inconsistent at times, may take several days\nto weeks for changes to occur, and can be influenced by the interaction\nmodality.",
    "updated" : "2024-09-13T08:55:21Z",
    "published" : "2024-09-11T17:44:41Z",
    "authors" : [
      {
        "name" : "Tina Khezresmaeilzadeh"
      },
      {
        "name" : "Elaine Zhu"
      },
      {
        "name" : "Kiersten Grieco"
      },
      {
        "name" : "Daniel J. Dubois"
      },
      {
        "name" : "Konstantinos Psounis"
      },
      {
        "name" : "David Choffnes"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.10411v1",
    "title" : "A Large-Scale Privacy Assessment of Android Third-Party SDKs",
    "summary" : "Third-party Software Development Kits (SDKs) are widely adopted in Android\napp development, to effortlessly accelerate development pipelines and enhance\napp functionality. However, this convenience raises substantial concerns about\nunauthorized access to users' privacy-sensitive information, which could be\nfurther abused for illegitimate purposes like user tracking or monetization.\nOur study offers a targeted analysis of user privacy protection among Android\nthird-party SDKs, filling a critical gap in the Android software supply chain.\nIt focuses on two aspects of their privacy practices, including data\nexfiltration and behavior-policy compliance (or privacy compliance), utilizing\ntechniques of taint analysis and large language models. It covers 158\nwidely-used SDKs from two key SDK release platforms, the official one and a\nlarge alternative one. From them, we identified 338 instances of privacy data\nexfiltration. On the privacy compliance, our study reveals that more than 30%\nof the examined SDKs fail to provide a privacy policy to disclose their data\nhandling practices. Among those that provide privacy policies, 37% of them\nover-collect user data, and 88% falsely claim access to sensitive data. We\nrevisit the latest versions of the SDKs after 12 months. Our analysis\ndemonstrates a persistent lack of improvement in these concerning trends. Based\non our findings, we propose three actionable recommendations to mitigate the\nprivacy leakage risks and enhance privacy protection for Android users. Our\nresearch not only serves as an urgent call for industry attention but also\nprovides crucial insights for future regulatory interventions.",
    "updated" : "2024-09-16T15:44:43Z",
    "published" : "2024-09-16T15:44:43Z",
    "authors" : [
      {
        "name" : "Mark Huasong Meng"
      },
      {
        "name" : "Chuan Yan"
      },
      {
        "name" : "Yun Hao"
      },
      {
        "name" : "Qing Zhang"
      },
      {
        "name" : "Zeyu Wang"
      },
      {
        "name" : "Kailong Wang"
      },
      {
        "name" : "Sin Gee Teo"
      },
      {
        "name" : "Guangdong Bai"
      },
      {
        "name" : "Jin Song Dong"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.10337v1",
    "title" : "Security, Trust and Privacy challenges in AI-driven 6G Networks",
    "summary" : "The advent of 6G networks promises unprecedented advancements in wireless\ncommunication, offering wider bandwidth and lower latency compared to its\npredecessors. This article explores the evolving infrastructure of 6G networks,\nemphasizing the transition towards a more disaggregated structure and the\nintegration of artificial intelligence (AI) technologies. Furthermore, it\nexplores the security, trust and privacy challenges and attacks in 6G networks,\nparticularly those related to the use of AI. It presents a classification of\nnetwork attacks stemming from its AI-centric architecture and explores\ntechnologies designed to detect or mitigate these emerging threats. The paper\nconcludes by examining the implications and risks linked to the utilization of\nAI in ensuring a robust network.",
    "updated" : "2024-09-16T14:48:20Z",
    "published" : "2024-09-16T14:48:20Z",
    "authors" : [
      {
        "name" : "Helena Rifa-Pous"
      },
      {
        "name" : "Victor Garcia-Font"
      },
      {
        "name" : "Carlos Nunez-Gomez"
      },
      {
        "name" : "Julian Salas"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.10226v1",
    "title" : "Privacy-Preserving Distributed Maximum Consensus Without Accuracy Loss",
    "summary" : "In distributed networks, calculating the maximum element is a fundamental\ntask in data analysis, known as the distributed maximum consensus problem.\nHowever, the sensitive nature of the data involved makes privacy protection\nessential. Despite its importance, privacy in distributed maximum consensus has\nreceived limited attention in the literature. Traditional privacy-preserving\nmethods typically add noise to updates, degrading the accuracy of the final\nresult. To overcome these limitations, we propose a novel distributed\noptimization-based approach that preserves privacy without sacrificing\naccuracy. Our method introduces virtual nodes to form an augmented graph and\nleverages a carefully designed initialization process to ensure the privacy of\nhonest participants, even when all their neighboring nodes are dishonest.\nThrough a comprehensive information-theoretical analysis, we derive a\nsufficient condition to protect private data against both passive and\neavesdropping adversaries. Extensive experiments validate the effectiveness of\nour approach, demonstrating that it not only preserves perfect privacy but also\nmaintains accuracy, outperforming existing noise-based methods that typically\nsuffer from accuracy loss.",
    "updated" : "2024-09-16T12:21:04Z",
    "published" : "2024-09-16T12:21:04Z",
    "authors" : [
      {
        "name" : "Wenrui Yu"
      },
      {
        "name" : "Richard Heusdens"
      },
      {
        "name" : "Jun Pang"
      },
      {
        "name" : "Qiongxiu Li"
      }
    ],
    "categories" : [
      "cs.DC",
      "cs.CR",
      "cs.IT",
      "eess.SP",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.10192v1",
    "title" : "PrePaMS: Privacy-Preserving Participant Management System for Studies\n  with Rewards and Prerequisites",
    "summary" : "Taking part in surveys, experiments, and studies is often compensated by\nrewards to increase the number of participants and encourage attendance. While\nprivacy requirements are usually considered for participation, privacy aspects\nof the reward procedure are mostly ignored. To this end, we introduce PrePaMS,\nan efficient participation management system that supports prerequisite checks\nand participation rewards in a privacy-preserving way. Our system organizes\nparticipations with potential (dis-)qualifying dependencies and enables secure\nreward payoffs. By leveraging a set of proven cryptographic primitives and\nmechanisms such as anonymous credentials and zero-knowledge proofs,\nparticipations are protected so that service providers and organizers cannot\nderive the identity of participants even within the reward process. In this\npaper, we have designed and implemented a prototype of PrePaMS to show its\neffectiveness and evaluated its performance under realistic workloads. PrePaMS\ncovers the information whether subjects have participated in surveys,\nexperiments, or studies. When combined with other secure solutions for the\nactual data collection within these events, PrePaMS can represent a cornerstone\nfor more privacy-preserving empirical research.",
    "updated" : "2024-09-16T11:35:17Z",
    "published" : "2024-09-16T11:35:17Z",
    "authors" : [
      {
        "name" : "Echo Meißner"
      },
      {
        "name" : "Frank Kargl"
      },
      {
        "name" : "Benjamin Erb"
      },
      {
        "name" : "Felix Engelmann"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.10057v1",
    "title" : "A Response to: A Note on \"Privacy Preserving n-Party Scalar Product\n  Protocol\"",
    "summary" : "We reply to the comments on our proposed privacy preserving n-party scalar\nproduct protocol made by Liu. In their comment Liu raised concerns regarding\nthe security and scalability of the $n$-party scalar product protocol. In this\nreply, we show that their concerns are unfounded and that the $n$-party scalar\nproduct protocol is safe for its intended purposes. Their concerns regarding\nthe security are based on a misunderstanding of the protocol. Additionally,\nwhile the scalability of the protocol puts limitations on its use, the protocol\nstill has numerous practical applications when applied in the correct\nscenarios. Specifically within vertically partitioned scenarios, which often\ninvolve few parties, the protocol remains practical. In this reply we clarify\nLiu's misunderstanding. Additionally, we explain why the protocols scaling is\nnot a practical problem in its intended application.",
    "updated" : "2024-09-16T07:36:37Z",
    "published" : "2024-09-16T07:36:37Z",
    "authors" : [
      {
        "name" : "Florian van Daalen"
      },
      {
        "name" : "Lianne Ippel"
      },
      {
        "name" : "Andre Dekker"
      },
      {
        "name" : "Inigo Bermejo"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09972v1",
    "title" : "Securing the Future: Exploring Privacy Risks and Security Questions in\n  Robotic Systems",
    "summary" : "The integration of artificial intelligence, especially large language models\nin robotics, has led to rapid advancements in the field. We are now observing\nan unprecedented surge in the use of robots in our daily lives. The development\nand continual improvements of robots are moving at an astonishing pace.\nAlthough these remarkable improvements facilitate and enhance our lives,\nseveral security and privacy concerns have not been resolved yet. Therefore, it\nhas become crucial to address the privacy and security threats of robotic\nsystems while improving our experiences. In this paper, we aim to present\nexisting applications and threats of robotics, anticipated future evolution,\nand the security and privacy issues they may imply. We present a series of open\nquestions for researchers and practitioners to explore further.",
    "updated" : "2024-09-16T04:10:24Z",
    "published" : "2024-09-16T04:10:24Z",
    "authors" : [
      {
        "name" : "Diba Afroze"
      },
      {
        "name" : "Yazhou Tu"
      },
      {
        "name" : "Xiali Hei"
      }
    ],
    "categories" : [
      "cs.RO",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09558v1",
    "title" : "A Statistical Viewpoint on Differential Privacy: Hypothesis Testing,\n  Representation and Blackwell's Theorem",
    "summary" : "Differential privacy is widely considered the formal privacy for\nprivacy-preserving data analysis due to its robust and rigorous guarantees,\nwith increasingly broad adoption in public services, academia, and industry.\nDespite originating in the cryptographic context, in this review paper we argue\nthat, fundamentally, differential privacy can be considered a \\textit{pure}\nstatistical concept. By leveraging a theorem due to David Blackwell, our focus\nis to demonstrate that the definition of differential privacy can be formally\nmotivated from a hypothesis testing perspective, thereby showing that\nhypothesis testing is not merely convenient but also the right language for\nreasoning about differential privacy. This insight leads to the definition of\n$f$-differential privacy, which extends other differential privacy definitions\nthrough a representation theorem. We review techniques that render\n$f$-differential privacy a unified framework for analyzing privacy bounds in\ndata analysis and machine learning. Applications of this differential privacy\ndefinition to private deep learning, private convex optimization, shuffled\nmechanisms, and U.S.~Census data are discussed to highlight the benefits of\nanalyzing privacy bounds under this framework compared to existing\nalternatives.",
    "updated" : "2024-09-14T23:47:22Z",
    "published" : "2024-09-14T23:47:22Z",
    "authors" : [
      {
        "name" : "Weijie J. Su"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09532v1",
    "title" : "Using Synthetic Data to Mitigate Unfairness and Preserve Privacy through\n  Single-Shot Federated Learning",
    "summary" : "To address unfairness issues in federated learning (FL), contemporary\napproaches typically use frequent model parameter updates and transmissions\nbetween the clients and server. In such a process, client-specific information\n(e.g., local dataset size or data-related fairness metrics) must be sent to the\nserver to compute, e.g., aggregation weights. All of this results in high\ntransmission costs and the potential leakage of client information. As an\nalternative, we propose a strategy that promotes fair predictions across\nclients without the need to pass information between the clients and server\niteratively and prevents client data leakage. For each client, we first use\ntheir local dataset to obtain a synthetic dataset by solving a bilevel\noptimization problem that addresses unfairness concerns during the learning\nprocess. We then pass each client's synthetic dataset to the server, the\ncollection of which is used to train the server model using conventional\nmachine learning techniques (that do not take fairness metrics into account).\nThus, we eliminate the need to handle fairness-specific aggregation weights\nwhile preserving client privacy. Our approach requires only a single\ncommunication between the clients and the server, thus making it\ncomputationally cost-effective, able to maintain privacy, and able to ensuring\nfairness. We present empirical evidence to demonstrate the advantages of our\napproach. The results illustrate that our method effectively uses synthetic\ndata as a means to mitigate unfairness and preserve client privacy.",
    "updated" : "2024-09-14T21:04:11Z",
    "published" : "2024-09-14T21:04:11Z",
    "authors" : [
      {
        "name" : "Chia-Yuan Wu"
      },
      {
        "name" : "Frank E. Curtis"
      },
      {
        "name" : "Daniel P. Robinson"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CY",
      "math.OC",
      "G.1.6; I.2.6; C.2.4; K.4.1; D.4.6"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09510v1",
    "title" : "Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for\n  Privacy-Preserving Personalization of Large Language Models",
    "summary" : "Privacy-preserving methods for personalizing large language models (LLMs) are\nrelatively under-explored. There are two schools of thought on this topic: (1)\ngenerating personalized outputs by personalizing the input prompt through\nretrieval augmentation from the user's personal information (RAG-based\nmethods), and (2) parameter-efficient fine-tuning of LLMs per user that\nconsiders efficiency and space limitations (PEFT-based methods). This paper\npresents the first systematic comparison between two approaches on a wide range\nof personalization tasks using seven diverse datasets. Our results indicate\nthat RAG-based and PEFT-based personalization methods on average yield 14.92%\nand 1.07% improvements over the non-personalized LLM, respectively. We find\nthat combining RAG with PEFT elevates these improvements to 15.98%.\nAdditionally, we identify a positive correlation between the amount of user\ndata and PEFT's effectiveness, indicating that RAG is a better choice for\ncold-start users (i.e., user's with limited personal data).",
    "updated" : "2024-09-14T19:18:26Z",
    "published" : "2024-09-14T19:18:26Z",
    "authors" : [
      {
        "name" : "Alireza Salemi"
      },
      {
        "name" : "Hamed Zamani"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09495v1",
    "title" : "Protecting Vehicle Location Privacy with Contextually-Driven Synthetic\n  Location Generation",
    "summary" : "Geo-obfuscation is a Location Privacy Protection Mechanism used in\nlocation-based services that allows users to report obfuscated locations\ninstead of exact ones. A formal privacy criterion, geoindistinguishability\n(Geo-Ind), requires real locations to be hard to distinguish from nearby\nlocations (by attackers) based on their obfuscated representations. However,\nGeo-Ind often fails to consider context, such as road networks and vehicle\ntraffic conditions, making it less effective in protecting the location privacy\nof vehicles, of which the mobility are heavily influenced by these factors.\n  In this paper, we introduce VehiTrack, a new threat model to demonstrate the\nvulnerability of Geo-Ind in protecting vehicle location privacy from\ncontext-aware inference attacks. Our experiments demonstrate that VehiTrack can\naccurately determine exact vehicle locations from obfuscated data, reducing\naverage inference errors by 61.20% with Laplacian noise and 47.35% with linear\nprogramming (LP) compared to traditional Bayesian attacks. By using contextual\ndata like road networks and traffic flow, VehiTrack effectively eliminates a\nsignificant number of seemingly \"impossible\" locations during its search for\nthe actual location of the vehicles. Based on these insights, we propose\nTransProtect, a new geo-obfuscation approach that limits obfuscation to\nrealistic vehicle movement patterns, complicating attackers' ability to\ndifferentiate obfuscated from actual locations. Our results show that\nTransProtect increases VehiTrack's inference error by 57.75% with Laplacian\nnoise and 27.21% with LP, significantly enhancing protection against these\nattacks.",
    "updated" : "2024-09-14T17:47:23Z",
    "published" : "2024-09-14T17:47:23Z",
    "authors" : [
      {
        "name" : "Sourabh Yadav"
      },
      {
        "name" : "Chenyang Yu"
      },
      {
        "name" : "Xinpeng Xie"
      },
      {
        "name" : "Yan Huang"
      },
      {
        "name" : "Chenxi Qiu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09363v1",
    "title" : "Security and Privacy Perspectives of People Living in Shared Home\n  Environments",
    "summary" : "Security and privacy perspectives of people in a multi-user home are a\ngrowing area of research, with many researchers reflecting on the complicated\npower imbalance and challenging access control issues of the devices involved.\nHowever, these studies primarily focused on the multi-user scenarios in\ntraditional family home settings, leaving other types of multi-user home\nenvironments, such as homes shared by co-habitants without a familial\nrelationship, under-studied. This paper closes this research gap via\nquantitative and qualitative analysis of results from an online survey and\ncontent analysis of sampled online posts on Reddit. It explores the complex\nroles of shared home users, which depend on various factors unique to the\nshared home environment, e.g., who owns what home devices, how home devices are\nused by multiple users, and more complicated relationships between the landlord\nand people in the shared home and among co-habitants. Half (50.7%) of our\nsurvey participants thought that devices in a shared home are less secure than\nin a traditional family home. This perception was found statistically\nsignificantly associated with factors such as the fear of devices being\ntampered with in their absence and (lack of) trust in other co-habitants and\ntheir visitors. Our study revealed new user types and relationships in a\nmulti-user environment such as ExternalPrimary-InternalPrimary while analysing\nthe landlord and shared home resident relationship with regard to shared home\ndevice use. We propose a threat actor model for shared home environments, which\nhas a focus on possible malicious behaviours of current and past co-habitants\nof a shared home, as a special type of insider threat in a home environment. We\nalso recommend further research to understand the complex roles co-habitants\ncan play in navigating and adapting to a shared home environment's security and\nprivacy landscape.",
    "updated" : "2024-09-14T08:34:57Z",
    "published" : "2024-09-14T08:34:57Z",
    "authors" : [
      {
        "name" : "Nandita Pattnaik"
      },
      {
        "name" : "Shujun Li"
      },
      {
        "name" : "Jason R. C. Nurse"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09272v1",
    "title" : "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
    "summary" : "Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited\nremarkable performance in generating realistic and natural audio. However,\ntheir dark side, audio deepfake poses a significant threat to both society and\nindividuals. Existing countermeasures largely focus on determining the\ngenuineness of speech based on complete original audio recordings, which\nhowever often contain private content. This oversight may refrain deepfake\ndetection from many applications, particularly in scenarios involving sensitive\ninformation like business secrets. In this paper, we propose SafeEar, a novel\nframework that aims to detect deepfake audios without relying on accessing the\nspeech content within. Our key idea is to devise a neural audio codec into a\nnovel decoupling model that well separates the semantic and acoustic\ninformation from audio samples, and only use the acoustic information (e.g.,\nprosody and timbre) for deepfake detection. In this way, no semantic content\nwill be exposed to the detector. To overcome the challenge of identifying\ndiverse deepfake audio without semantic clues, we enhance our deepfake detector\nwith real-world codec augmentation. Extensive experiments conducted on four\nbenchmark datasets demonstrate SafeEar's effectiveness in detecting various\ndeepfake techniques with an equal error rate (EER) down to 2.02%.\nSimultaneously, it shields five-language speech content from being deciphered\nby both machine and human auditory analysis, demonstrated by word error rates\n(WERs) all above 93.93% and our user study. Furthermore, our benchmark\nconstructed for anti-deepfake and anti-content recovery evaluation helps\nprovide a basis for future research in the realms of audio privacy preservation\nand deepfake detection.",
    "updated" : "2024-09-14T02:45:09Z",
    "published" : "2024-09-14T02:45:09Z",
    "authors" : [
      {
        "name" : "Xinfeng Li"
      },
      {
        "name" : "Kai Li"
      },
      {
        "name" : "Yifan Zheng"
      },
      {
        "name" : "Chen Yan"
      },
      {
        "name" : "Xiaoyu Ji"
      },
      {
        "name" : "Wenyuan Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09222v1",
    "title" : "Dark Patterns in the Opt-Out Process and Compliance with the California\n  Consumer Privacy Act (CCPA)",
    "summary" : "To protect consumer privacy, the California Consumer Privacy Act (CCPA)\nmandates that businesses provide consumers with a straightforward way to opt\nout of the sale and sharing of their personal information. However, the control\nthat businesses enjoy over the opt-out process allows them to impose hurdles on\nconsumers aiming to opt out, including by employing dark patterns. Motivated by\nthe enactment of the California Privacy Rights Act (CPRA), which strengthens\nthe CCPA and explicitly forbids certain dark patterns in the opt-out process,\nwe investigate how dark patterns are used in opt-out processes and assess their\ncompliance with CCPA regulations. Our research reveals that websites employ a\nvariety of dark patterns. Some of these patterns are explicitly prohibited\nunder the CCPA; others evidently take advantage of legal loopholes. Despite the\ninitial efforts to restrict dark patterns by policymakers, there is more work\nto be done.",
    "updated" : "2024-09-13T22:20:43Z",
    "published" : "2024-09-13T22:20:43Z",
    "authors" : [
      {
        "name" : "Van Hong Tran"
      },
      {
        "name" : "Aarushi Mehrotra"
      },
      {
        "name" : "Ranya Sharma"
      },
      {
        "name" : "Marshini Chetty"
      },
      {
        "name" : "Nick Feamster"
      },
      {
        "name" : "Jens Frankenreiter"
      },
      {
        "name" : "Lior Strahilevitz"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06955v2",
    "title" : "Privacy-Preserving Federated Learning with Consistency via Knowledge\n  Distillation Using Conditional Generator",
    "summary" : "Federated Learning (FL) is gaining popularity as a distributed learning\nframework that only shares model parameters or gradient updates and keeps\nprivate data locally. However, FL is at risk of privacy leakage caused by\nprivacy inference attacks. And most existing privacy-preserving mechanisms in\nFL conflict with achieving high performance and efficiency. Therefore, we\npropose FedMD-CG, a novel FL method with highly competitive performance and\nhigh-level privacy preservation, which decouples each client's local model into\na feature extractor and a classifier, and utilizes a conditional generator\ninstead of the feature extractor to perform server-side model aggregation. To\nensure the consistency of local generators and classifiers, FedMD-CG leverages\nknowledge distillation to train local models and generators at both the latent\nfeature level and the logit level. Also, we construct additional classification\nlosses and design new diversity losses to enhance client-side training.\nFedMD-CG is robust to data heterogeneity and does not require training extra\ndiscriminators (like cGAN). We conduct extensive experiments on various image\nclassification tasks to validate the superiority of FedMD-CG.",
    "updated" : "2024-09-16T08:23:09Z",
    "published" : "2024-09-11T02:36:36Z",
    "authors" : [
      {
        "name" : "Kangyang Luo"
      },
      {
        "name" : "Shuai Wang"
      },
      {
        "name" : "Xiang Li"
      },
      {
        "name" : "Yunshi Lan"
      },
      {
        "name" : "Ming Gao"
      },
      {
        "name" : "Jinlong Shu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11295v1",
    "title" : "EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage",
    "summary" : "Generalist web agents have evolved rapidly and demonstrated remarkable\npotential. However, there are unprecedented safety risks associated with these\nthem, which are nearly unexplored so far. In this work, we aim to narrow this\ngap by conducting the first study on the privacy risks of generalist web agents\nin adversarial environments. First, we present a threat model that discusses\nthe adversarial targets, constraints, and attack scenarios. Particularly, we\nconsider two types of adversarial targets: stealing users' specific personally\nidentifiable information (PII) or stealing the entire user request. To achieve\nthese objectives, we propose a novel attack method, termed Environmental\nInjection Attack (EIA). This attack injects malicious content designed to adapt\nwell to different environments where the agents operate, causing them to\nperform unintended actions. This work instantiates EIA specifically for the\nprivacy scenario. It inserts malicious web elements alongside persuasive\ninstructions that mislead web agents into leaking private information, and can\nfurther leverage CSS and JavaScript features to remain stealthy. We collect 177\nactions steps that involve diverse PII categories on realistic websites from\nthe Mind2Web dataset, and conduct extensive experiments using one of the most\ncapable generalist web agent frameworks to date, SeeAct. The results\ndemonstrate that EIA achieves up to 70% ASR in stealing users' specific PII.\nStealing full user requests is more challenging, but a relaxed version of EIA\ncan still achieve 16% ASR. Despite these concerning results, it is important to\nnote that the attack can still be detectable through careful human inspection,\nhighlighting a trade-off between high autonomy and security. This leads to our\ndetailed discussion on the efficacy of EIA under different levels of human\nsupervision as well as implications on defenses for generalist web agents.",
    "updated" : "2024-09-17T15:49:44Z",
    "published" : "2024-09-17T15:49:44Z",
    "authors" : [
      {
        "name" : "Zeyi Liao"
      },
      {
        "name" : "Lingbo Mo"
      },
      {
        "name" : "Chejian Xu"
      },
      {
        "name" : "Mintong Kang"
      },
      {
        "name" : "Jiawei Zhang"
      },
      {
        "name" : "Chaowei Xiao"
      },
      {
        "name" : "Yuan Tian"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Huan Sun"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.10667v1",
    "title" : "Benchmarking Secure Sampling Protocols for Differential Privacy",
    "summary" : "Differential privacy (DP) is widely employed to provide privacy protection\nfor individuals by limiting information leakage from the aggregated data. Two\nwell-known models of DP are the central model and the local model. The former\nrequires a trustworthy server for data aggregation, while the latter requires\nindividuals to add noise, significantly decreasing the utility of aggregated\nresults. Recently, many studies have proposed to achieve DP with Secure\nMulti-party Computation (MPC) in distributed settings, namely, the distributed\nmodel, which has utility comparable to central model while, under specific\nsecurity assumptions, preventing parties from obtaining others' information.\nOne challenge of realizing DP in distributed model is efficiently sampling\nnoise with MPC. Although many secure sampling methods have been proposed, they\nhave different security assumptions and isolated theoretical analyses. There is\na lack of experimental evaluations to measure and compare their performances.\nWe fill this gap by benchmarking existing sampling protocols in MPC and\nperforming comprehensive measurements of their efficiency. First, we present a\ntaxonomy of the underlying techniques of these sampling protocols. Second, we\nextend widely used distributed noise generation protocols to be resilient\nagainst Byzantine attackers. Third, we implement discrete sampling protocols\nand align their security settings for a fair comparison. We then conduct an\nextensive evaluation to study their efficiency and utility.",
    "updated" : "2024-09-16T19:04:47Z",
    "published" : "2024-09-16T19:04:47Z",
    "authors" : [
      {
        "name" : "Yucheng Fu"
      },
      {
        "name" : "Tianhao Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.08913v2",
    "title" : "HLTCOE JHU Submission to the Voice Privacy Challenge 2024",
    "summary" : "We present a number of systems for the Voice Privacy Challenge, including\nvoice conversion based systems such as the kNN-VC method and the WavLM voice\nConversion method, and text-to-speech (TTS) based systems including\nWhisper-VITS. We found that while voice conversion systems better preserve\nemotional content, they struggle to conceal speaker identity in semi-white-box\nattack scenarios; conversely, TTS methods perform better at anonymization and\nworse at emotion preservation. Finally, we propose a random admixture system\nwhich seeks to balance out the strengths and weaknesses of the two category of\nsystems, achieving a strong EER of over 40% while maintaining UAR at a\nrespectable 47%.",
    "updated" : "2024-09-17T14:39:44Z",
    "published" : "2024-09-13T15:29:37Z",
    "authors" : [
      {
        "name" : "Henry Li Xinyuan"
      },
      {
        "name" : "Zexin Cai"
      },
      {
        "name" : "Ashi Garg"
      },
      {
        "name" : "Kevin Duh"
      },
      {
        "name" : "Leibny Paola García-Perera"
      },
      {
        "name" : "Sanjeev Khudanpur"
      },
      {
        "name" : "Nicholas Andrews"
      },
      {
        "name" : "Matthew Wiesner"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04652v2",
    "title" : "Privacy-Preserving Race/Ethnicity Estimation for Algorithmic Bias\n  Measurement in the U.S",
    "summary" : "AI fairness measurements, including tests for equal treatment, often take the\nform of disaggregated evaluations of AI systems. Such measurements are an\nimportant part of Responsible AI operations. These measurements compare system\nperformance across demographic groups or sub-populations and typically require\nmember-level demographic signals such as gender, race, ethnicity, and location.\nHowever, sensitive member-level demographic attributes like race and ethnicity\ncan be challenging to obtain and use due to platform choices, legal\nconstraints, and cultural norms. In this paper, we focus on the task of\nenabling AI fairness measurements on race/ethnicity for \\emph{U.S. LinkedIn\nmembers} in a privacy-preserving manner. We present the Privacy-Preserving\nProbabilistic Race/Ethnicity Estimation (PPRE) method for performing this task.\nPPRE combines the Bayesian Improved Surname Geocoding (BISG) model, a sparse\nLinkedIn survey sample of self-reported demographics, and privacy-enhancing\ntechnologies like secure two-party computation and differential privacy to\nenable meaningful fairness measurements while preserving member privacy. We\nprovide details of the PPRE method and its privacy guarantees. We then\nillustrate sample measurement operations. We conclude with a review of open\nresearch and engineering challenges for expanding our privacy-preserving\nfairness measurement capabilities.",
    "updated" : "2024-09-16T18:15:18Z",
    "published" : "2024-09-06T23:29:18Z",
    "authors" : [
      {
        "name" : "Saikrishna Badrinarayanan"
      },
      {
        "name" : "Osonde Osoba"
      },
      {
        "name" : "Miao Cheng"
      },
      {
        "name" : "Ryan Rogers"
      },
      {
        "name" : "Sakshi Jain"
      },
      {
        "name" : "Rahul Tandra"
      },
      {
        "name" : "Natesh S. Pillai"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11845v1",
    "title" : "Law-based and standards-oriented approach for privacy impact assessment\n  in medical devices: a topic for lawyers, engineers and healthcare\n  practitioners in MedTech",
    "summary" : "Background: The integration of the General Data Protection Regulation (GDPR)\nand the Medical Device Regulation (MDR) creates complexities in conducting Data\nProtection Impact Assessments (DPIAs) for medical devices. The adoption of\nnon-binding standards like ISO and IEC can harmonize these processes by\nenhancing accountability and privacy by design. Methods: This study employs a\nmultidisciplinary literature review, focusing on GDPR and MDR intersection in\nmedical devices that process personal health data. It evaluates key standards,\nincluding ISO/IEC 29134 and IEC 62304, to propose a unified approach for DPIAs\nthat aligns with legal and technical frameworks. Results: The analysis reveals\nthe benefits of integrating ISO/IEC standards into DPIAs, which provide\ndetailed guidance on implementing privacy by design, risk assessment, and\nmitigation strategies specific to medical devices. The proposed framework\nensures that DPIAs are living documents, continuously updated to adapt to\nevolving data protection challenges. Conclusions: A unified approach combining\nEuropean Union (EU) regulations and international standards offers a robust\nframework for conducting DPIAs in medical devices. This integration balances\nsecurity, innovation, and privacy, enhancing compliance and fostering trust in\nmedical technologies. The study advocates for leveraging both hard law and\nstandards to systematically address privacy and safety in the design and\noperation of medical devices, thereby raising the maturity of the MedTech\necosystem.",
    "updated" : "2024-09-18T09:56:19Z",
    "published" : "2024-09-18T09:56:19Z",
    "authors" : [
      {
        "name" : "Yuri R. Ladeia"
      },
      {
        "name" : "David M. Pereira"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11805v1",
    "title" : "Inside Out or Not: Privacy Implications of Emotional Disclosure",
    "summary" : "Privacy is dynamic, sensitive, and contextual, much like our emotions.\nPrevious studies have explored the interplay between privacy and context,\nprivacy and emotion, and emotion and context. However, there remains a\nsignificant gap in understanding the interplay of these aspects simultaneously.\nIn this paper, we present a preliminary study investigating the role of\nemotions in driving individuals' information sharing behaviour, particularly in\nrelation to urban locations and social ties. We adopt a novel methodology that\nintegrates context (location and time), emotion, and personal information\nsharing behaviour, providing a comprehensive analysis of how contextual\nemotions affect privacy. The emotions are assessed with both self-reporting and\nelectrodermal activity (EDA). Our findings reveal that self-reported emotions\ninfluence personal information-sharing behaviour with distant social groups,\nwhile neutral emotions lead individuals to share less precise information with\nclose social circles, a pattern is potentially detectable with wrist-worn EDA.\nOur study helps lay the foundation for personalised emotion-aware strategies to\nmitigate oversharing risks and enhance user privacy in the digital age.",
    "updated" : "2024-09-18T08:42:45Z",
    "published" : "2024-09-18T08:42:45Z",
    "authors" : [
      {
        "name" : "Elham Naghizade"
      },
      {
        "name" : "Kaixin Ji"
      },
      {
        "name" : "Benjamin Tag"
      },
      {
        "name" : "Flora Salim"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11680v1",
    "title" : "What to Consider When Considering Differential Privacy for Policy",
    "summary" : "Differential privacy (DP) is a mathematical definition of privacy that can be\nwidely applied when publishing data. DP has been recognized as a potential\nmeans of adhering to various privacy-related legal requirements. However, it\ncan be difficult to reason about whether DP may be appropriate for a given\ncontext due to tensions that arise when it is brought from theory into\npractice. To aid policymaking around privacy concerns, we identify three\ncategories of challenges to understanding DP along with associated questions\nthat policymakers can ask about the potential deployment context to anticipate\nits impacts.",
    "updated" : "2024-09-18T03:41:15Z",
    "published" : "2024-09-18T03:41:15Z",
    "authors" : [
      {
        "name" : "Priyanka Nanayakkara"
      },
      {
        "name" : "Jessica Hullman"
      }
    ],
    "categories" : [
      "cs.CY",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11663v1",
    "title" : "GReDP: A More Robust Approach for Differential Privacy Training with\n  Gradient-Preserving Noise Reduction",
    "summary" : "Deep learning models have been extensively adopted in various regions due to\ntheir ability to represent hierarchical features, which highly rely on the\ntraining set and procedures. Thus, protecting the training process and deep\nlearning algorithms is paramount in privacy preservation. Although Differential\nPrivacy (DP) as a powerful cryptographic primitive has achieved satisfying\nresults in deep learning training, the existing schemes still fall short in\npreserving model utility, i.e., they either invoke a high noise scale or\ninevitably harm the original gradients. To address the above issues, in this\npaper, we present a more robust approach for DP training called GReDP.\nSpecifically, we compute the model gradients in the frequency domain and adopt\na new approach to reduce the noise level. Unlike the previous work, our GReDP\nonly requires half of the noise scale compared to DPSGD [1] while keeping all\nthe gradient information intact. We present a detailed analysis of our method\nboth theoretically and empirically. The experimental results show that our\nGReDP works consistently better than the baselines on all models and training\nsettings.",
    "updated" : "2024-09-18T03:01:27Z",
    "published" : "2024-09-18T03:01:27Z",
    "authors" : [
      {
        "name" : "Haodi Wang"
      },
      {
        "name" : "Tangyu Jiang"
      },
      {
        "name" : "Yu Guo"
      },
      {
        "name" : "Xiaohua Jia"
      },
      {
        "name" : "Chengjun Cai"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11536v1",
    "title" : "Obfuscation Based Privacy Preserving Representations are Recoverable\n  Using Neighborhood Information",
    "summary" : "Rapid growth in the popularity of AR/VR/MR applications and cloud-based\nvisual localization systems has given rise to an increased focus on the privacy\nof user content in the localization process.\n  This privacy concern has been further escalated by the ability of deep neural\nnetworks to recover detailed images of a scene from a sparse set of 3D or 2D\npoints and their descriptors - the so-called inversion attacks.\n  Research on privacy-preserving localization has therefore focused on\npreventing these inversion attacks on both the query image keypoints and the 3D\npoints of the scene map.\n  To this end, several geometry obfuscation techniques that lift points to\nhigher-dimensional spaces, i.e., lines or planes, or that swap coordinates\nbetween points % have been proposed.\n  In this paper, we point to a common weakness of these obfuscations that\nallows to recover approximations of the original point positions under the\nassumption of known neighborhoods.\n  We further show that these neighborhoods can be computed by learning to\nidentify descriptors that co-occur in neighborhoods.\n  Extensive experiments show that our approach for point recovery is\npractically applicable to all existing geometric obfuscation schemes.\n  Our results show that these schemes should not be considered\nprivacy-preserving, even though they are claimed to be privacy-preserving.\n  Code will be available at\n\\url{https://github.com/kunalchelani/RecoverPointsNeighborhood}.",
    "updated" : "2024-09-17T20:13:54Z",
    "published" : "2024-09-17T20:13:54Z",
    "authors" : [
      {
        "name" : "Kunal Chelani"
      },
      {
        "name" : "Assia Benbihi"
      },
      {
        "name" : "Fredrik Kahl"
      },
      {
        "name" : "Torsten Sattler"
      },
      {
        "name" : "Zuzana Kukelova"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11430v1",
    "title" : "Federated Learning with Quantum Computing and Fully Homomorphic\n  Encryption: A Novel Computing Paradigm Shift in Privacy-Preserving ML",
    "summary" : "The widespread deployment of products powered by machine learning models is\nraising concerns around data privacy and information security worldwide. To\naddress this issue, Federated Learning was first proposed as a\nprivacy-preserving alternative to conventional methods that allow multiple\nlearning clients to share model knowledge without disclosing private data. A\ncomplementary approach known as Fully Homomorphic Encryption (FHE) is a\nquantum-safe cryptographic system that enables operations to be performed on\nencrypted weights. However, implementing mechanisms such as these in practice\noften comes with significant computational overhead and can expose potential\nsecurity threats. Novel computing paradigms, such as analog, quantum, and\nspecialized digital hardware, present opportunities for implementing\nprivacy-preserving machine learning systems while enhancing security and\nmitigating performance loss. This work instantiates these ideas by applying the\nFHE scheme to a Federated Learning Neural Network architecture that integrates\nboth classical and quantum layers.",
    "updated" : "2024-09-14T01:23:26Z",
    "published" : "2024-09-14T01:23:26Z",
    "authors" : [
      {
        "name" : "Siddhant Dutta"
      },
      {
        "name" : "Pavana P Karanth"
      },
      {
        "name" : "Pedro Maciel Xavier"
      },
      {
        "name" : "Iago Leal de Freitas"
      },
      {
        "name" : "Nouhaila Innan"
      },
      {
        "name" : "Sadok Ben Yahia"
      },
      {
        "name" : "Muhammad Shafique"
      },
      {
        "name" : "David E. Bernal Neira"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11423v1",
    "title" : "Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large\n  Language Models on Generated Data",
    "summary" : "Large language models (LLMs) have shown considerable success in a range of\ndomain-specific tasks, especially after fine-tuning. However, fine-tuning with\nreal-world data usually leads to privacy risks, particularly when the\nfine-tuning samples exist in the pre-training data. To avoid the shortcomings\nof real data, developers often employ methods to automatically generate\nsynthetic data for fine-tuning, as data generated by traditional models are\noften far away from the real-world pertaining data. However, given the advanced\ncapabilities of LLMs, the distinction between real data and LLM-generated data\nhas become negligible, which may also lead to privacy risks like real data. In\nthis paper, we present an empirical analysis of this underexplored issue by\ninvestigating a key question: \"Does fine-tuning with LLM-generated data enhance\nprivacy, or does it pose additional privacy risks?\" Based on the structure of\nLLM's generated data, our research focuses on two primary approaches to\nfine-tuning with generated data: supervised fine-tuning with unstructured\ngenerated data and self-instruct tuning. The number of successful Personal\nInformation Identifier (PII) extractions for Pythia after fine-tuning our\ngenerated data raised over $20\\%$. Furthermore, the ROC-AUC score of membership\ninference attacks for Pythia-6.9b after self-instruct methods also achieves\nmore than $40\\%$ improvements on ROC-AUC score than base models. The results\nindicate the potential privacy risks in LLMs when fine-tuning with the\ngenerated data.",
    "updated" : "2024-09-12T10:14:12Z",
    "published" : "2024-09-12T10:14:12Z",
    "authors" : [
      {
        "name" : "Atilla Akkus"
      },
      {
        "name" : "Mingjie Li"
      },
      {
        "name" : "Junjie Chu"
      },
      {
        "name" : "Michael Backes"
      },
      {
        "name" : "Yang Zhang"
      },
      {
        "name" : "Sinem Sav"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.09363v2",
    "title" : "Security and Privacy Perspectives of People Living in Shared Home\n  Environments",
    "summary" : "Security and privacy perspectives of people in a multi-user home are a\ngrowing area of research, with many researchers reflecting on the complicated\npower imbalance and challenging access control issues of the devices involved.\nHowever, these studies primarily focused on the multi-user scenarios in\ntraditional family home settings, leaving other types of multi-user home\nenvironments, such as homes shared by co-habitants without a familial\nrelationship, under-studied. This paper closes this research gap via\nquantitative and qualitative analysis of results from an online survey and\ncontent analysis of sampled online posts on Reddit. It explores the complex\nroles of shared home users, which depend on various factors unique to the\nshared home environment, e.g., who owns what home devices, how home devices are\nused by multiple users, and more complicated relationships between the landlord\nand people in the shared home and among co-habitants. Half (50.7%) of our\nsurvey participants thought that devices in a shared home are less secure than\nin a traditional family home. This perception was found statistically\nsignificantly associated with factors such as the fear of devices being\ntampered with in their absence and (lack of) trust in other co-habitants and\ntheir visitors. Our study revealed new user types and relationships in a\nmulti-user environment such as ExternalPrimary-InternalPrimary while analysing\nthe landlord and shared home resident relationship with regard to shared home\ndevice use. We propose a threat actor model for shared home environments, which\nhas a focus on possible malicious behaviours of current and past co-habitants\nof a shared home, as a special type of insider threat in a home environment. We\nalso recommend further research to understand the complex roles co-habitants\ncan play in navigating and adapting to a shared home environment's security and\nprivacy landscape.",
    "updated" : "2024-09-19T08:05:16Z",
    "published" : "2024-09-14T08:34:57Z",
    "authors" : [
      {
        "name" : "Nandita Pattnaik"
      },
      {
        "name" : "Shujun Li"
      },
      {
        "name" : "Jason R. C. Nurse"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11430v2",
    "title" : "Federated Learning with Quantum Computing and Fully Homomorphic\n  Encryption: A Novel Computing Paradigm Shift in Privacy-Preserving ML",
    "summary" : "The widespread deployment of products powered by machine learning models is\nraising concerns around data privacy and information security worldwide. To\naddress this issue, Federated Learning was first proposed as a\nprivacy-preserving alternative to conventional methods that allow multiple\nlearning clients to share model knowledge without disclosing private data. A\ncomplementary approach known as Fully Homomorphic Encryption (FHE) is a\nquantum-safe cryptographic system that enables operations to be performed on\nencrypted weights. However, implementing mechanisms such as these in practice\noften comes with significant computational overhead and can expose potential\nsecurity threats. Novel computing paradigms, such as analog, quantum, and\nspecialized digital hardware, present opportunities for implementing\nprivacy-preserving machine learning systems while enhancing security and\nmitigating performance loss. This work instantiates these ideas by applying the\nFHE scheme to a Federated Learning Neural Network architecture that integrates\nboth classical and quantum layers.",
    "updated" : "2024-09-19T03:05:48Z",
    "published" : "2024-09-14T01:23:26Z",
    "authors" : [
      {
        "name" : "Siddhant Dutta"
      },
      {
        "name" : "Pavana P Karanth"
      },
      {
        "name" : "Pedro Maciel Xavier"
      },
      {
        "name" : "Iago Leal de Freitas"
      },
      {
        "name" : "Nouhaila Innan"
      },
      {
        "name" : "Sadok Ben Yahia"
      },
      {
        "name" : "Muhammad Shafique"
      },
      {
        "name" : "David E. Bernal Neira"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02375v2",
    "title" : "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review",
    "summary" : "The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights.",
    "updated" : "2024-09-20T06:32:13Z",
    "published" : "2024-09-04T01:51:37Z",
    "authors" : [
      {
        "name" : "Xichou Zhu"
      },
      {
        "name" : "Yang Liu"
      },
      {
        "name" : "Zhou Shen"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Min Li"
      },
      {
        "name" : "Yujun Chen"
      },
      {
        "name" : "Benzi John"
      },
      {
        "name" : "Zhenzhen Ma"
      },
      {
        "name" : "Zhi Li"
      },
      {
        "name" : "Tao Hu"
      },
      {
        "name" : "Bolong Yang"
      },
      {
        "name" : "Manman Wang"
      },
      {
        "name" : "Zongxing Xie"
      },
      {
        "name" : "Peng Liu"
      },
      {
        "name" : "Dan Cai"
      },
      {
        "name" : "Junhui Wang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11663v2",
    "title" : "GReDP: A More Robust Approach for Differential Private Training with\n  Gradient-Preserving Noise Reduction",
    "summary" : "Deep learning models have been extensively adopted in various regions due to\ntheir ability to represent hierarchical features, which highly rely on the\ntraining set and procedures. Thus, protecting the training process and deep\nlearning algorithms is paramount in privacy preservation. Although Differential\nPrivacy (DP) as a powerful cryptographic primitive has achieved satisfying\nresults in deep learning training, the existing schemes still fall short in\npreserving model utility, i.e., they either invoke a high noise scale or\ninevitably harm the original gradients. To address the above issues, in this\npaper, we present a more robust approach for DP training called GReDP.\nSpecifically, we compute the model gradients in the frequency domain and adopt\na new approach to reduce the noise level. Unlike the previous work, our GReDP\nonly requires half of the noise scale compared to DPSGD [1] while keeping all\nthe gradient information intact. We present a detailed analysis of our method\nboth theoretically and empirically. The experimental results show that our\nGReDP works consistently better than the baselines on all models and training\nsettings.",
    "updated" : "2024-09-21T04:06:00Z",
    "published" : "2024-09-18T03:01:27Z",
    "authors" : [
      {
        "name" : "Haodi Wang"
      },
      {
        "name" : "Tangyu Jiang"
      },
      {
        "name" : "Yu Guo"
      },
      {
        "name" : "Chengjun Cai"
      },
      {
        "name" : "Cong Wang"
      },
      {
        "name" : "Xiaohua Jia"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02375v3",
    "title" : "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review",
    "summary" : "The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights.",
    "updated" : "2024-09-23T02:30:21Z",
    "published" : "2024-09-04T01:51:37Z",
    "authors" : [
      {
        "name" : "Xichou Zhu"
      },
      {
        "name" : "Yang Liu"
      },
      {
        "name" : "Zhou Shen"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Min Li"
      },
      {
        "name" : "Yujun Chen"
      },
      {
        "name" : "Benzi John"
      },
      {
        "name" : "Zhenzhen Ma"
      },
      {
        "name" : "Tao Hu"
      },
      {
        "name" : "Zhi Li"
      },
      {
        "name" : "Bolong Yang"
      },
      {
        "name" : "Manman Wang"
      },
      {
        "name" : "Zongxing Xie"
      },
      {
        "name" : "Peng Liu"
      },
      {
        "name" : "Dan Cai"
      },
      {
        "name" : "Junhui Wang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17144v1",
    "title" : "Differential Privacy Regularization: Protecting Training Data Through\n  Loss Function Regularization",
    "summary" : "Training machine learning models based on neural networks requires large\ndatasets, which may contain sensitive information. The models, however, should\nnot expose private information from these datasets. Differentially private SGD\n[DP-SGD] requires the modification of the standard stochastic gradient descent\n[SGD] algorithm for training new models. In this short paper, a novel\nregularization strategy is proposed to achieve the same goal in a more\nefficient manner.",
    "updated" : "2024-09-25T17:59:32Z",
    "published" : "2024-09-25T17:59:32Z",
    "authors" : [
      {
        "name" : "Francisco Aguilera-Martínez"
      },
      {
        "name" : "Fernando Berzal"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.16777v1",
    "title" : "PhD Forum: Efficient Privacy-Preserving Processing via Memory-Centric\n  Computing",
    "summary" : "Privacy-preserving computation techniques like homomorphic encryption (HE)\nand secure multi-party computation (SMPC) enhance data security by enabling\nprocessing on encrypted data. However, the significant computational and\nCPU-DRAM data movement overhead resulting from the underlying cryptographic\nalgorithms impedes the adoption of these techniques in practice. Existing\napproaches focus on improving computational overhead using specialized hardware\nlike GPUs and FPGAs, but these methods still suffer from the same\nprocessor-DRAM overhead. Novel hardware technologies that support in-memory\nprocessing have the potential to address this problem. Memory-centric\ncomputing, or processing-in-memory (PIM), brings computation closer to data by\nintroducing low-power processors called data processing units (DPUs) into\nmemory. Besides its in-memory computation capability, PIM provides extensive\nparallelism, resulting in significant performance improvement over\nstate-of-the-art approaches. We propose a framework that uses recently\navailable PIM hardware to achieve efficient privacy-preserving computation. Our\ndesign consists of a four-layer architecture: (1) an application layer that\ndecouples privacy-preserving applications from the underlying protocols and\nhardware; (2) a protocol layer that implements existing secure computation\nprotocols (HE and MPC); (3) a data orchestration layer that leverages data\ncompression techniques to mitigate the data transfer overhead between DPUs and\nhost memory; (4) a computation layer which implements DPU kernels on which\nsecure computation algorithms are built.",
    "updated" : "2024-09-25T09:37:50Z",
    "published" : "2024-09-25T09:37:50Z",
    "authors" : [
      {
        "name" : "Mpoki Mwaisela"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.16688v1",
    "title" : "Cycle Counting under Local Differential Privacy for Degeneracy-bounded\n  Graphs",
    "summary" : "We propose an algorithm for counting the number of cycles under local\ndifferential privacy for degeneracy-bounded input graphs. Numerous studies have\nfocused on counting the number of triangles under the privacy notion,\ndemonstrating that the expected \\(\\ell_2\\)-error of these algorithms is\n\\(\\Omega(n^{1.5})\\), where \\(n\\) is the number of nodes in the graph. When\nparameterized by the number of cycles of length four (\\(C_4\\)), the best\nexisting triangle counting algorithm has an error of \\(O(n^{1.5} + \\sqrt{C_4})\n= O(n^2)\\). In this paper, we introduce an algorithm with an expected\n\\(\\ell_2\\)-error of \\(O(\\delta^{1.5} n^{0.5} + \\delta^{0.5} d_{\\max}^{0.5}\nn^{0.5})\\), where \\(\\delta\\) is the degeneracy and \\(d_{\\max}\\) is the maximum\ndegree of the graph. For degeneracy-bounded graphs (\\(\\delta \\in \\Theta(1)\\))\ncommonly found in practical social networks, our algorithm achieves an expected\n\\(\\ell_2\\)-error of \\(O(d_{\\max}^{0.5} n^{0.5}) = O(n)\\). Our algorithm's core\nidea is a precise count of triangles following a preprocessing step that\napproximately sorts the degree of all nodes. This approach can be extended to\napproximate the number of cycles of length \\(k\\), maintaining a similar\n\\(\\ell_2\\)-error, namely $O(\\delta^{(k-2)/2} d_{\\max}^{0.5} n^{(k-2)/2} +\n\\delta^{k/2} n^{(k-2)/2})$ or $O(d_{\\max}^{0.5} n^{(k-2)/2}) = O(n^{(k-1)/2})$\nfor degeneracy-bounded graphs.",
    "updated" : "2024-09-25T07:23:58Z",
    "published" : "2024-09-25T07:23:58Z",
    "authors" : [
      {
        "name" : "Quentin Hillebrand"
      },
      {
        "name" : "Vorapong Suppakitpaisarn"
      },
      {
        "name" : "Tetsuo Shibuya"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.16621v1",
    "title" : "Entailment-Driven Privacy Policy Classification with LLMs",
    "summary" : "While many online services provide privacy policies for end users to read and\nunderstand what personal data are being collected, these documents are often\nlengthy and complicated. As a result, the vast majority of users do not read\nthem at all, leading to data collection under uninformed consent. Several\nattempts have been made to make privacy policies more user friendly by\nsummarising them, providing automatic annotations or labels for key sections,\nor by offering chat interfaces to ask specific questions. With recent advances\nin Large Language Models (LLMs), there is an opportunity to develop more\neffective tools to parse privacy policies and help users make informed\ndecisions. In this paper, we propose an entailment-driven LLM based framework\nto classify paragraphs of privacy policies into meaningful labels that are\neasily understood by users. The results demonstrate that our framework\noutperforms traditional LLM methods, improving the F1 score in average by\n11.2%. Additionally, our framework provides inherently explainable and\nmeaningful predictions.",
    "updated" : "2024-09-25T05:07:05Z",
    "published" : "2024-09-25T05:07:05Z",
    "authors" : [
      {
        "name" : "Bhanuka Silva"
      },
      {
        "name" : "Dishanika Denipitiyage"
      },
      {
        "name" : "Suranga Seneviratne"
      },
      {
        "name" : "Anirban Mahanti"
      },
      {
        "name" : "Aruna Seneviratne"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.16340v1",
    "title" : "Future-Proofing Medical Imaging with Privacy-Preserving Federated\n  Learning and Uncertainty Quantification: A Review",
    "summary" : "Artificial Intelligence (AI) has demonstrated significant potential in\nautomating various medical imaging tasks, which could soon become routine in\nclinical practice for disease diagnosis, prognosis, treatment planning, and\npost-treatment surveillance. However, the privacy concerns surrounding patient\ndata present a major barrier to the widespread adoption of AI in medical\nimaging, as large, diverse training datasets are essential for developing\naccurate, generalizable, and robust Artificial intelligence models. Federated\nLearning (FL) offers a solution that enables organizations to train AI models\ncollaboratively without sharing sensitive data. federated learning exchanges\nmodel training information, such as gradients, between the participating sites.\nDespite its promise, federated learning is still in its developmental stages\nand faces several challenges. Notably, sensitive information can still be\ninferred from the gradients shared during model training. Quantifying AI\nmodels' uncertainty is vital due to potential data distribution shifts\npost-deployment, which can affect model performance. Uncertainty quantification\n(UQ) in FL is particularly challenging due to data heterogeneity across\nparticipating sites. This review provides a comprehensive examination of FL,\nprivacy-preserving FL (PPFL), and UQ in FL. We identify key gaps in current FL\nmethodologies and propose future research directions to enhance data privacy\nand trustworthiness in medical imaging applications.",
    "updated" : "2024-09-24T16:55:32Z",
    "published" : "2024-09-24T16:55:32Z",
    "authors" : [
      {
        "name" : "Nikolas Koutsoubis"
      },
      {
        "name" : "Asim Waqas"
      },
      {
        "name" : "Yasin Yilmaz"
      },
      {
        "name" : "Ravi P. Ramachandran"
      },
      {
        "name" : "Matthew Schabath"
      },
      {
        "name" : "Ghulam Rasool"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.16106v1",
    "title" : "Scenario of Use Scheme: Threat Model Specification for Speaker Privacy\n  Protection in the Medical Domain",
    "summary" : "Speech recordings are being more frequently used to detect and monitor\ndisease, leading to privacy concerns. Beyond cryptography, protection of speech\ncan be addressed by approaches, such as perturbation, disentanglement, and\nre-synthesis, that eliminate sensitive information of the speaker, leaving the\ninformation necessary for medical analysis purposes. In order for such privacy\nprotective approaches to be developed, clear and systematic specifications of\nassumptions concerning medical settings and the needs of medical professionals\nare necessary. In this paper, we propose a Scenario of Use Scheme that\nincorporates an Attacker Model, which characterizes the adversary against whom\nthe speaker's privacy must be defended, and a Protector Model, which specifies\nthe defense. We discuss the connection of the scheme with previous work on\nspeech privacy. Finally, we present a concrete example of a specified Scenario\nof Use and a set of experiments about protecting speaker data against gender\ninference attacks while maintaining utility for Parkinson's detection.",
    "updated" : "2024-09-24T14:07:47Z",
    "published" : "2024-09-24T14:07:47Z",
    "authors" : [
      {
        "name" : "Mehtab Ur Rahman"
      },
      {
        "name" : "Martha Larson"
      },
      {
        "name" : "Louis ten Bosch"
      },
      {
        "name" : "Cristian Tejedor-García"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.AI",
      "cs.CR",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.15868v2",
    "title" : "Privacy Evaluation Benchmarks for NLP Models",
    "summary" : "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.",
    "updated" : "2024-09-25T07:03:05Z",
    "published" : "2024-09-24T08:41:26Z",
    "authors" : [
      {
        "name" : "Wei Huang"
      },
      {
        "name" : "Yinggui Wang"
      },
      {
        "name" : "Cen Chen"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.15656v1",
    "title" : "Identified-and-Targeted: The First Early Evidence of the\n  Privacy-Invasive Use of Browser Fingerprinting for Online Tracking",
    "summary" : "While advertising has become commonplace in today's online interactions,\nthere is a notable dearth of research investigating the extent to which browser\nfingerprinting is harnessed for user tracking and targeted advertising. Prior\nstudies only measured whether fingerprinting-related scripts are being run on\nthe websites but that in itself does not necessarily mean that fingerprinting\nis being used for the privacy-invasive purpose of online tracking because\nfingerprinting might be deployed for the defensive purposes of bot/fraud\ndetection and user authentication. It is imperative to address the mounting\nconcerns regarding the utilization of browser fingerprinting in the realm of\nonline advertising.\n  To understand the privacy-invasive use of fingerprinting for user tracking,\nthis paper introduces a new framework ``FPTrace'' (fingerprinting-based\ntracking assessment and comprehensive evaluation framework) designed to\nidentify alterations in advertisements resulting from adjustments in browser\nfingerprinting settings. Our approach involves emulating genuine user\ninteractions, capturing advertiser bid data, and closely monitoring HTTP\ninformation. Using FPTrace we conduct a large-scale measurement study to\nidentify whether browser fingerprinting is being used for the purpose of user\ntracking and ad targeting. The results we have obtained provide robust evidence\nsupporting the utilization of browser fingerprinting for the purposes of\nadvertisement tracking and targeting. This is substantiated by significant\ndisparities in bid values and a reduction in HTTP records subsequent to changes\nin fingerprinting. In conclusion, our research unveils the widespread\nemployment of browser fingerprinting in online advertising, prompting critical\nconsiderations regarding user privacy and data security within the digital\nadvertising landscape.",
    "updated" : "2024-09-24T01:39:16Z",
    "published" : "2024-09-24T01:39:16Z",
    "authors" : [
      {
        "name" : "Zengrui Liu"
      },
      {
        "name" : "Jimmy Dani"
      },
      {
        "name" : "Shujiang Wu"
      },
      {
        "name" : "Yinzhi Cao"
      },
      {
        "name" : "Nitesh Saxena"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.15561v1",
    "title" : "Analyzing Privacy Implications of Data Collection in Android Automotive\n  OS",
    "summary" : "Modern vehicles have become sophisticated computation and sensor systems, as\nevidenced by advanced driver assistance systems, in-car infotainment, and\nautonomous driving capabilities. They collect and process vast amounts of data\nthrough various embedded subsystems. One significant player in this landscape\nis Android Automotive OS (AAOS), which has been integrated into over 100M\nvehicles and has become a dominant force in the in-vehicle infotainment market.\nWith this extensive data collection, privacy has become increasingly crucial.\nThe volume of data gathered by these systems raises questions about how this\ninformation is stored, used, and protected, making privacy a critical issue for\nmanufacturers and consumers. However, very little has been done on vehicle data\nprivacy. This paper focuses on the privacy implications of AAOS, examining the\nexact nature and scope of data collection and the corresponding privacy\npolicies from the original equipment manufacturers (OEMs). We develop a novel\nautomotive privacy analysis tool called PriDrive which employs three\nmethodological approaches: network traffic inspection, and both static and\ndynamic analyses of Android images using rooted emulators from various OEMs.\nThese methodologies are followed by an assessment of whether the collected data\ntypes were properly disclosed in OEMs and 3rd party apps' privacy policies (to\nidentify any discrepancies or violations). Our evaluation on three different\nOEM platforms reveals that vehicle speed is collected at a sampling rate of\nroughly 25 Hz. Other properties such as model info, climate & AC, and seat data\nare collected in a batch 30 seconds into vehicle startup. In addition, several\nvehicle property types were collected without disclosure in their respective\nprivacy policies. For example, OEM A's policies only covers 110 vehicle\nproperties or 13.02% of the properties found in our static analysis.",
    "updated" : "2024-09-23T21:35:40Z",
    "published" : "2024-09-23T21:35:40Z",
    "authors" : [
      {
        "name" : "Bulut Gözübüyük"
      },
      {
        "name" : "Brian Tang"
      },
      {
        "name" : "Kang G. Shin"
      },
      {
        "name" : "Mert D. Pesé"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.14919v1",
    "title" : "Voice Conversion-based Privacy through Adversarial Information Hiding",
    "summary" : "Privacy-preserving voice conversion aims to remove only the attributes of\nspeech audio that convey identity information, keeping other speech\ncharacteristics intact. This paper presents a mechanism for privacy-preserving\nvoice conversion that allows controlling the leakage of identity-bearing\ninformation using adversarial information hiding. This enables a deliberate\ntrade-off between maintaining source-speech characteristics and modification of\nspeaker identity. As such, the approach improves on voice-conversion techniques\nlike CycleGAN and StarGAN, which were not designed for privacy, meaning that\nconverted speech may leak personal information in unpredictable ways. Our\napproach is also more flexible than ASR-TTS voice conversion pipelines, which\nby design discard all prosodic information linked to textual content.\nEvaluations show that the proposed system successfully modifies perceived\nspeaker identity whilst well maintaining source lexical content.",
    "updated" : "2024-09-23T11:16:49Z",
    "published" : "2024-09-23T11:16:49Z",
    "authors" : [
      {
        "name" : "Jacob J Webber"
      },
      {
        "name" : "Oliver Watts"
      },
      {
        "name" : "Gustav Eje Henter"
      },
      {
        "name" : "Jennifer Williams"
      },
      {
        "name" : "Simon King"
      }
    ],
    "categories" : [
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.14879v1",
    "title" : "Privacy Policy Analysis through Prompt Engineering for LLMs",
    "summary" : "Privacy policies are often obfuscated by their complexity, which impedes\ntransparency and informed consent. Conventional machine learning approaches for\nautomatically analyzing these policies demand significant resources and\nsubstantial domain-specific training, causing adaptability issues. Moreover,\nthey depend on extensive datasets that may require regular maintenance due to\nchanging privacy concerns.\n  In this paper, we propose, apply, and assess PAPEL (Privacy Policy Analysis\nthrough Prompt Engineering for LLMs), a framework harnessing the power of Large\nLanguage Models (LLMs) through prompt engineering to automate the analysis of\nprivacy policies. PAPEL aims to streamline the extraction, annotation, and\nsummarization of information from these policies, enhancing their accessibility\nand comprehensibility without requiring additional model training. By\nintegrating zero-shot, one-shot, and few-shot learning approaches and the\nchain-of-thought prompting in creating predefined prompts and prompt templates,\nPAPEL guides LLMs to efficiently dissect, interpret, and synthesize the\ncritical aspects of privacy policies into user-friendly summaries. We\ndemonstrate the effectiveness of PAPEL with two applications: (i) annotation\nand (ii) contradiction analysis. We assess the ability of several LLaMa and GPT\nmodels to identify and articulate data handling practices, offering insights\ncomparable to existing automated analysis approaches while reducing training\nefforts and increasing the adaptability to new analytical needs. The\nexperiments demonstrate that the LLMs PAPEL utilizes (LLaMA and Chat GPT\nmodels) achieve robust performance in privacy policy annotation, with F1 scores\nreaching 0.8 and above (using the OPP-115 gold standard), underscoring the\neffectiveness of simpler prompts across various advanced language models.",
    "updated" : "2024-09-23T10:23:31Z",
    "published" : "2024-09-23T10:23:31Z",
    "authors" : [
      {
        "name" : "Arda Goknil"
      },
      {
        "name" : "Femke B. Gelderblom"
      },
      {
        "name" : "Simeon Tverdal"
      },
      {
        "name" : "Shukun Tokas"
      },
      {
        "name" : "Hui Song"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CY",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.14798v1",
    "title" : "PrivaMatch: A Privacy-Preserving DNA Matching Scheme for Forensic\n  Investigation",
    "summary" : "DNA fingerprinting and matching for identifying suspects has been a common\npractice in criminal investigation. Such proceedings involve multiple parties\nsuch as investigating agencies, suspects and forensic labs. A major challenge\nin such settings is to carry out the matching process between the suspects' DNA\nsamples and the samples obtained from the crime scene without compromising the\nprivacy of the suspects' DNA profiles. Additionally, it is necessary that\nsensitive details pertaining to the investigation such as the identities of the\nsuspects and evidence obtained from the crime scene must be kept private to the\ninvestigating agency. We present a novel DNA matching scheme, termed as\nPrivaMatch, which addresses multiple concerns about privacy of the suspects'\nDNA profiles and the crime scene evidence. In the proposed scheme, the\ninvestigating agencies oblivious transfer and zero-knowledge proofs to\nprivately obtain the DNA profiles of the suspects from the forensic lab's\ndatabase.In addition, we present a clever data obfuscation technique using\nhomomorphic encryption and modular arithmetic for the investigating agency to\nprivately obtain the DNA profile of the crime scene's sample, keeping the\nprofile oblivious from the forensic lab. The DNA profile of the crime scene\nsample is operated on using a homomorphic cryptosystem such that neither of the\nparties (e.g., the investigation agency, forensic labs, DNA database owners)\nlearns about the private data of the other parties. The proposed scheme is\nanalysed formally and the practicality of its security strengths is verified\nusing simulations under standard assumptions.",
    "updated" : "2024-09-23T08:22:31Z",
    "published" : "2024-09-23T08:22:31Z",
    "authors" : [
      {
        "name" : "Sankha Das"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.14499v1",
    "title" : "A Review of Scalable and Privacy-Preserving Multi-Agent Frameworks for\n  Distributed Energy Resource Control",
    "summary" : "Distributed energy resources (DERs) are gaining prominence due to their\nadvantages in improving energy efficiency, reducing carbon emissions, and\nenhancing grid resilience. Despite the increasing deployment, the potential of\nDERs has yet to be fully explored and exploited. A fundamental question\nrestrains the management of numerous DERs in large-scale power systems, \"How\nshould DER data be securely processed and DER operations be efficiently\noptimized?\" To address this question, this paper considers two critical issues,\nnamely privacy for processing DER data and scalability in optimizing DER\noperations, then surveys existing and emerging solutions from a multi-agent\nframework perspective. In the context of scalability, this paper reviews\nstate-of-the-art research that relies on parallel control, optimization, and\nlearning within distributed and/or decentralized information exchange\nstructures, while in the context of privacy, it identifies privacy preservation\nmeasures that can be synthesized into the aforementioned scalable structures.\nDespite research advances in these areas, challenges remain because these\nhighly interdisciplinary studies blend a wide variety of scalable computing\narchitectures and privacy preservation techniques from different fields, making\nthem difficult to adapt in practice. To mitigate this issue, this paper\nprovides a holistic review of trending strategies that orchestrate privacy and\nscalability for large-scale power system operations from a multi-agent\nperspective, particularly for DER control problems. Furthermore, this review\nextrapolates new approaches for future scalable, privacy-aware, and cybersecure\npathways to unlock the full potential of DERs through controlling, optimizing,\nand learning generic multi-agent-based cyber-physical systems.",
    "updated" : "2024-09-22T15:52:47Z",
    "published" : "2024-09-22T15:52:47Z",
    "authors" : [
      {
        "name" : "Xiang Huo"
      },
      {
        "name" : "Hao Huang"
      },
      {
        "name" : "Katherine R. Davis"
      },
      {
        "name" : "H. Vincent Poor"
      },
      {
        "name" : "Mingxi Liu"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.14472v1",
    "title" : "Blockchain Based Information Security and Privacy Protection: Challenges\n  and Future Directions using Computational Literature Review",
    "summary" : "Blockchain technology is an emerging digital innovation that has gained\nimmense popularity in enhancing individual security and privacy within\nInformation Systems (IS). This surge in interest is reflected in the\nexponential increase in research articles published on blockchain technology,\nhighlighting its growing significance in the digital landscape. However, the\nrapid proliferation of published research presents significant challenges for\nmanual analysis and synthesis due to the vast volume of information. The\ncomplexity and breadth of topics, combined with the inherent limitations of\nhuman data processing capabilities, make it difficult to comprehensively\nanalyze and draw meaningful insights from the literature. To this end, we\nadopted the Computational Literature Review (CLR) to analyze pertinent\nliterature impact and topic modelling using the Latent Dirichlet Allocation\n(LDA) technique. We identified 10 topics related to security and privacy and\nprovided a detailed description of each topic. From the critical analysis, we\nhave observed several limitations, and several future directions are provided\nas an outcome of this review.",
    "updated" : "2024-09-22T14:41:43Z",
    "published" : "2024-09-22T14:41:43Z",
    "authors" : [
      {
        "name" : "Gauri Shankar"
      },
      {
        "name" : "Md Raihan Uddin"
      },
      {
        "name" : "Saddam Mukta"
      },
      {
        "name" : "Prabhat Kumar"
      },
      {
        "name" : "Shareeful Islam"
      },
      {
        "name" : "A. K. M. Najmul Islam"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.14457v1",
    "title" : "Large Model Agents: State-of-the-Art, Cooperation Paradigms, Security\n  and Privacy, and Future Trends",
    "summary" : "Large Model (LM) agents, powered by large foundation models such as GPT-4 and\nDALL-E 2, represent a significant step towards achieving Artificial General\nIntelligence (AGI). LM agents exhibit key characteristics of autonomy,\nembodiment, and connectivity, allowing them to operate across physical,\nvirtual, and mixed-reality environments while interacting seamlessly with\nhumans, other agents, and their surroundings. This paper provides a\ncomprehensive survey of the state-of-the-art in LM agents, focusing on the\narchitecture, cooperation paradigms, security, privacy, and future prospects.\nSpecifically, we first explore the foundational principles of LM agents,\nincluding general architecture, key components, enabling technologies, and\nmodern applications. Then, we discuss practical collaboration paradigms from\ndata, computation, and knowledge perspectives towards connected intelligence of\nLM agents. Furthermore, we systematically analyze the security vulnerabilities\nand privacy breaches associated with LM agents, particularly in multi-agent\nsettings. We also explore their underlying mechanisms and review existing and\npotential countermeasures. Finally, we outline future research directions for\nbuilding robust and secure LM agent ecosystems.",
    "updated" : "2024-09-22T14:09:49Z",
    "published" : "2024-09-22T14:09:49Z",
    "authors" : [
      {
        "name" : "Yuntao Wang"
      },
      {
        "name" : "Yanghe Pan"
      },
      {
        "name" : "Quan Zhao"
      },
      {
        "name" : "Yi Deng"
      },
      {
        "name" : "Zhou Su"
      },
      {
        "name" : "Linkang Du"
      },
      {
        "name" : "Tom H. Luan"
      }
    ],
    "categories" : [
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.14261v1",
    "title" : "Re-Evaluating Privacy in Centralized and Decentralized Learning: An\n  Information-Theoretical and Empirical Study",
    "summary" : "Decentralized Federated Learning (DFL) has garnered attention for its\nrobustness and scalability compared to Centralized Federated Learning (CFL).\nWhile DFL is commonly believed to offer privacy advantages due to the\ndecentralized control of sensitive data, recent work by Pasquini et, al.\nchallenges this view, demonstrating that DFL does not inherently improve\nprivacy against empirical attacks under certain assumptions. For investigating\nfully this issue, a formal theoretical framework is required. Our study offers\na novel perspective by conducting a rigorous information-theoretical analysis\nof privacy leakage in FL using mutual information. We further investigate the\neffectiveness of privacy-enhancing techniques like Secure Aggregation (SA) in\nboth CFL and DFL. Our simulations and real-world experiments show that DFL\ngenerally offers stronger privacy preservation than CFL in practical scenarios\nwhere a fully trusted server is not available. We address discrepancies in\nprevious research by highlighting limitations in their assumptions about graph\ntopology and privacy attacks, which inadequately capture information leakage in\nFL.",
    "updated" : "2024-09-21T23:05:50Z",
    "published" : "2024-09-21T23:05:50Z",
    "authors" : [
      {
        "name" : "Changlong Ji"
      },
      {
        "name" : "Stephane Maag"
      },
      {
        "name" : "Richard Heusdens"
      },
      {
        "name" : "Qiongxiu Li"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.14039v1",
    "title" : "Towards Lightweight and Privacy-preserving Data Provision in Digital\n  Forensics for Driverless Taxi",
    "summary" : "Data provision, referring to the data upload and data access, is one key\nphase in vehicular digital forensics. The unique features of Driverless Taxi\n(DT) bring new issues to this phase: 1) efficient verification of data\nintegrity when diverse Data Providers (DPs) upload data; 2) DP privacy\npreservation during data upload; and 3) privacy preservation of both data and\nINvestigator (IN) under complex data ownership when accessing data. To this\nend, we propose a novel Lightweight and Privacy-preserving Data Provision\n(LPDP) approach consisting of three mechanisms: 1) the Privacy-friendly Batch\nVerification Mechanism (PBVm) based on elliptic curve cryptography, 2) Data\nAccess Control Mechanism (DACm) based on ciphertext-policy attribute-based\nencryption, and 3) Decentralized IN Warrant Issuance Mechanism (DIWIm) based on\nsecret sharing. Privacy preservation of data provision is achieved through: 1)\nensuring the DP privacy preservation in terms of the location privacy and\nunlinkability of data upload requests by PBVm, 2) ensuring data privacy\npreservation by DACm and DIWIm, and 3) ensuring the identity privacy of IN in\nterms of the anonymity and unlinkability of data access requests without\nsacrificing the traceability. Lightweight of data provision is achieved\nthrough: 1) ensuring scalable verification of data integrity by PBVm, and 2)\nensuring low-overhead warrant update with respect to DIWIm. Security analysis\nand performance evaluation are conducted to validate the security and\nperformance features of LPDP.",
    "updated" : "2024-09-21T06:51:26Z",
    "published" : "2024-09-21T06:51:26Z",
    "authors" : [
      {
        "name" : "Yanwei Gong"
      },
      {
        "name" : "Xiaolin Chang"
      },
      {
        "name" : "Jelena Mišić"
      },
      {
        "name" : "Vojislav B. Mišić"
      },
      {
        "name" : "Junchao Fan"
      },
      {
        "name" : "Kaiwen Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.13953v1",
    "title" : "Training Large ASR Encoders with Differential Privacy",
    "summary" : "Self-supervised learning (SSL) methods for large speech models have proven to\nbe highly effective at ASR. With the interest in public deployment of large\npre-trained models, there is a rising concern for unintended memorization and\nleakage of sensitive data points from the training data. In this paper, we\napply differentially private (DP) pre-training to a SOTA Conformer-based\nencoder, and study its performance on a downstream ASR task assuming the\nfine-tuning data is public. This paper is the first to apply DP to SSL for ASR,\ninvestigating the DP noise tolerance of the BEST-RQ pre-training method.\nNotably, we introduce a novel variant of model pruning called gradient-based\nlayer freezing that provides strong improvements in privacy-utility-compute\ntrade-offs. Our approach yields a LibriSpeech test-clean/other WER (%) of 3.78/\n8.41 with ($10$, 1e^-9)-DP for extrapolation towards low dataset scales, and\n2.81/ 5.89 with (10, 7.9e^-11)-DP for extrapolation towards high scales.",
    "updated" : "2024-09-21T00:01:49Z",
    "published" : "2024-09-21T00:01:49Z",
    "authors" : [
      {
        "name" : "Geeticka Chauhan"
      },
      {
        "name" : "Steve Chien"
      },
      {
        "name" : "Om Thakkar"
      },
      {
        "name" : "Abhradeep Thakurta"
      },
      {
        "name" : "Arun Narayanan"
      }
    ],
    "categories" : [
      "cs.SD",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.13875v1",
    "title" : "Data Distribution Shifts in (Industrial) Federated Learning as a Privacy\n  Issue",
    "summary" : "We consider industrial federated learning, a collaboration between a small\nnumber of powerful, potentially competing industrial players, mediated by a\nthird party aspiring to improve the service it provides to its customers. We\nargue that this configuration harbours covert privacy risks that do not arise\nin e.g. cross-device settings. Companies are very protective of their\nintellectual property and production processes. Information about changes to\ntheir production and the timing of which is to be kept private. We study a\nscenario in which one of the collaborators infers changes to their competitors'\nproduction by detecting potentially subtle temporal data distribution shifts.\nIn this framing, a data distribution shift is always problematic, even if it\nhas no negative effect on training convergence. Thus, our goal is to find means\nthat allow the detection of distributional shifts better than customary\nevaluation metrics. Based on the assumption that even minor shifts translate\ninto the collaboratively learned machine learning model, the attacker tracks\nthe shared models' internal state with a selection of metrics from literature\nin order to pick up on relevant changes. In an empirical study on benchmark\ndatasets, we show an honest-but-curious attacker to be capable of detecting\nsubtle distributional shifts on other clients, in some cases long before they\nbecome obvious in evaluation.",
    "updated" : "2024-09-20T20:09:19Z",
    "published" : "2024-09-20T20:09:19Z",
    "authors" : [
      {
        "name" : "David Brunner"
      },
      {
        "name" : "Alessio Montuoro"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.12874v1",
    "title" : "Privacy-Preserving Framework for Cell-Free MIMO ISAC Systems",
    "summary" : "Integrated Sensing and Communication (ISAC) systems are prone to privacy\nviolations, once they aim at handling sensitive identifiable information in\nseveral applications. This paper raises the necessity of implementing\nprivacy-preservation measures on the design of cell-free massive multiple-input\nmultiple-output ISAC systems. To that purpose, given an adversary model, we\npropose an iterative framework of two blocks, precoder design and access point\nselection. The precoder design aims at maximizing the\nsignal-to-interference-plus-noise ratio at the sensing receivers given\ncommunication constraints. The access point selection aims at minimizing the\nmutual information between the received signal at users and the sensing signal,\nby rearranging the access points that transmit ISAC-signals and the sensing\nreceivers. Results show that a reduction in the probability of detection by the\nadversary is obtained with this method.",
    "updated" : "2024-09-19T16:13:54Z",
    "published" : "2024-09-19T16:13:54Z",
    "authors" : [
      {
        "name" : "Henrik Åkesson"
      },
      {
        "name" : "Diana Pamela Moya Osorio"
      }
    ],
    "categories" : [
      "eess.SP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.12651v1",
    "title" : "A Deep Dive into Fairness, Bias, Threats, and Privacy in Recommender\n  Systems: Insights and Future Research",
    "summary" : "Recommender systems are essential for personalizing digital experiences on\ne-commerce sites, streaming services, and social media platforms. While these\nsystems are necessary for modern digital interactions, they face fairness,\nbias, threats, and privacy challenges. Bias in recommender systems can result\nin unfair treatment of specific users and item groups, and fairness concerns\ndemand that recommendations be equitable for all users and items. These systems\nare also vulnerable to various threats that compromise reliability and\nsecurity. Furthermore, privacy issues arise from the extensive use of personal\ndata, making it crucial to have robust protection mechanisms to safeguard user\ninformation. This study explores fairness, bias, threats, and privacy in\nrecommender systems. It examines how algorithmic decisions can unintentionally\nreinforce biases or marginalize specific user and item groups, emphasizing the\nneed for fair recommendation strategies. The study also looks at the range of\nthreats in the form of attacks that can undermine system integrity and\ndiscusses advanced privacy-preserving techniques. By addressing these critical\nareas, the study highlights current limitations and suggests future research\ndirections to improve recommender systems' robustness, fairness, and privacy.\nUltimately, this research aims to help develop more trustworthy and ethical\nrecommender systems that better serve diverse user populations.",
    "updated" : "2024-09-19T11:00:35Z",
    "published" : "2024-09-19T11:00:35Z",
    "authors" : [
      {
        "name" : "Falguni Roy"
      },
      {
        "name" : "Xiaofeng Ding"
      },
      {
        "name" : "K. -K. R. Choo"
      },
      {
        "name" : "Pan Zhou"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.CR",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.12384v1",
    "title" : "Privacy-Preserving Student Learning with Differentially Private\n  Data-Free Distillation",
    "summary" : "Deep learning models can achieve high inference accuracy by extracting rich\nknowledge from massive well-annotated data, but may pose the risk of data\nprivacy leakage in practical deployment. In this paper, we present an effective\nteacher-student learning approach to train privacy-preserving deep learning\nmodels via differentially private data-free distillation. The main idea is\ngenerating synthetic data to learn a student that can mimic the ability of a\nteacher well-trained on private data. In the approach, a generator is first\npretrained in a data-free manner by incorporating the teacher as a fixed\ndiscriminator. With the generator, massive synthetic data can be generated for\nmodel training without exposing data privacy. Then, the synthetic data is fed\ninto the teacher to generate private labels. Towards this end, we propose a\nlabel differential privacy algorithm termed selective randomized response to\nprotect the label information. Finally, a student is trained on the synthetic\ndata with the supervision of private labels. In this way, both data privacy and\nlabel privacy are well protected in a unified framework, leading to\nprivacy-preserving models. Extensive experiments and analysis clearly\ndemonstrate the effectiveness of our approach.",
    "updated" : "2024-09-19T01:00:18Z",
    "published" : "2024-09-19T01:00:18Z",
    "authors" : [
      {
        "name" : "Bochao Liu"
      },
      {
        "name" : "Jianghu Lu"
      },
      {
        "name" : "Pengju Wang"
      },
      {
        "name" : "Junjie Zhang"
      },
      {
        "name" : "Dan Zeng"
      },
      {
        "name" : "Zhenxing Qian"
      },
      {
        "name" : "Shiming Ge"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.12341v1",
    "title" : "Provable Privacy Guarantee for Individual Identities and Locations in\n  Large-Scale Contact Tracing",
    "summary" : "The task of infectious disease contact tracing is crucial yet challenging,\nespecially when meeting strict privacy requirements. Previous attempts in this\narea have had limitations in terms of applicable scenarios and efficiency. Our\npaper proposes a highly scalable, practical contact tracing system called\nPREVENT that can work with a variety of location collection methods to gain a\ncomprehensive overview of a person's trajectory while ensuring the privacy of\nindividuals being tracked, without revealing their plain text locations to any\nparty, including servers. Our system is very efficient and can provide\nreal-time query services for large-scale datasets with millions of locations.\nThis is made possible by a newly designed secret-sharing based architecture\nthat is tightly integrated into unique private space partitioning trees.\nNotably, our experimental results on both real and synthetic datasets\ndemonstrate that our system introduces negligible performance overhead compared\nto traditional contact tracing methods. PREVENT could be a game-changer in the\nfight against infectious diseases and set a new standard for privacy-preserving\nlocation tracking.",
    "updated" : "2024-09-18T22:19:48Z",
    "published" : "2024-09-18T22:19:48Z",
    "authors" : [
      {
        "name" : "Tyler Nicewarner"
      },
      {
        "name" : "Wei Jiang"
      },
      {
        "name" : "Aniruddha Gokhale"
      },
      {
        "name" : "Dan Lin"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.18118v1",
    "title" : "Slowly Scaling Per-Record Differential Privacy",
    "summary" : "We develop formal privacy mechanisms for releasing statistics from data with\nmany outlying values, such as income data. These mechanisms ensure that a\nper-record differential privacy guarantee degrades slowly in the protected\nrecords' influence on the statistics being released.\n  Formal privacy mechanisms generally add randomness, or \"noise,\" to published\nstatistics. If a noisy statistic's distribution changes little with the\naddition or deletion of a single record in the underlying dataset, an attacker\nlooking at this statistic will find it plausible that any particular record was\npresent or absent, preserving the records' privacy. More influential records --\nthose whose addition or deletion would change the statistics' distribution more\n-- typically suffer greater privacy loss. The per-record differential privacy\nframework quantifies these record-specific privacy guarantees, but existing\nmechanisms let these guarantees degrade rapidly (linearly or quadratically)\nwith influence. While this may be acceptable in cases with some moderately\ninfluential records, it results in unacceptably high privacy losses when\nrecords' influence varies widely, as is common in economic data.\n  We develop mechanisms with privacy guarantees that instead degrade as slowly\nas logarithmically with influence. These mechanisms allow for the accurate,\nunbiased release of statistics, while providing meaningful protection for\nhighly influential records. As an example, we consider the private release of\nsums of unbounded establishment data such as payroll, where our mechanisms\nextend meaningful privacy protection even to very large establishments. We\nevaluate these mechanisms empirically and demonstrate their utility.",
    "updated" : "2024-09-26T17:56:11Z",
    "published" : "2024-09-26T17:56:11Z",
    "authors" : [
      {
        "name" : "Brian Finley"
      },
      {
        "name" : "Anthony M Caruso"
      },
      {
        "name" : "Justin C Doty"
      },
      {
        "name" : "Ashwin Machanavajjhala"
      },
      {
        "name" : "Mikaela R Meyer"
      },
      {
        "name" : "David Pujol"
      },
      {
        "name" : "William Sexton"
      },
      {
        "name" : "Zachary Terner"
      }
    ],
    "categories" : [
      "cs.CR",
      "stat.ME"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17886v1",
    "title" : "Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze\n  Target Detection",
    "summary" : "Gaze Target Detection (GTD), i.e., determining where a person is looking\nwithin a scene from an external viewpoint, is a challenging task, particularly\nin 3D space. Existing approaches heavily rely on analyzing the person's\nappearance, primarily focusing on their face to predict the gaze target. This\npaper presents a novel approach to tackle this problem by utilizing the\nperson's upper-body pose and available depth maps to extract a 3D gaze\ndirection and employing a multi-stage or an end-to-end pipeline to predict the\ngazed target. When predicted accurately, the human body pose can provide\nvaluable information about the head pose, which is a good approximation of the\ngaze direction, as well as the position of the arms and hands, which are linked\nto the activity the person is performing and the objects they are likely\nfocusing on. Consequently, in addition to performing gaze estimation in 3D, we\nare also able to perform GTD simultaneously. We demonstrate state-of-the-art\nresults on the most comprehensive publicly accessible 3D gaze target detection\ndataset without requiring images of the person's face, thus promoting privacy\npreservation in various application contexts. The code is available at\nhttps://github.com/intelligolabs/privacy-gtd-3D.",
    "updated" : "2024-09-26T14:35:06Z",
    "published" : "2024-09-26T14:35:06Z",
    "authors" : [
      {
        "name" : "Andrea Toaiari"
      },
      {
        "name" : "Vittorio Murino"
      },
      {
        "name" : "Marco Cristani"
      },
      {
        "name" : "Cigdem Beyan"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17744v1",
    "title" : "Privacy for Quantum Annealing. Attack on Spin Reversal Transformations\n  in the case of cryptanalysis",
    "summary" : "This paper demonstrates that applying spin reversal transformations (SRT),\ncommonly known as a sufficient method for privacy enhancing in problems solved\nusing quantum annealing, does not guarantee privacy for all possible problems.\nWe show how to recover the original problem from the Ising problem obtained\nusing SRT when the resulting problem in Ising form represents the algebraic\nattack on the $E_0$ stream cipher. A small example is used to illustrate how to\nretrieve the original problem from the one transformed by SRT. Moreover, it is\nshown that our method is efficient even for full-scale problems.",
    "updated" : "2024-09-26T11:17:47Z",
    "published" : "2024-09-26T11:17:47Z",
    "authors" : [
      {
        "name" : "Mateusz Leśniak"
      },
      {
        "name" : "Michał Wroński"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17742v1",
    "title" : "TADAR: Thermal Array-based Detection and Ranging for Privacy-Preserving\n  Human Sensing",
    "summary" : "Human sensing has gained increasing attention in various applications. Among\nthe available technologies, visual images offer high accuracy, while sensing on\nthe RF spectrum preserves privacy, creating a conflict between imaging\nresolution and privacy preservation. In this paper, we explore thermal array\nsensors as an emerging modality that strikes an excellent resolution-privacy\nbalance for ubiquitous sensing. To this end, we present TADAR, the first\nmulti-user Thermal Array-based Detection and Ranging system that estimates the\ninherently missing range information, extending thermal array outputs from 2D\nthermal pixels to 3D depths and empowering them as a promising modality for\nubiquitous privacy-preserving human sensing. We prototype TADAR using a single\ncommodity thermal array sensor and conduct extensive experiments in different\nindoor environments. Our results show that TADAR achieves a mean F1 score of\n88.8% for multi-user detection and a mean accuracy of 32.0 cm for multi-user\nranging, which further improves to 20.1 cm for targets located within 3 m. We\nconduct two case studies on fall detection and occupancy estimation to showcase\nthe potential applications of TADAR. We hope TADAR will inspire the vast\ncommunity to explore new directions of thermal array sensing, beyond wireless\nand acoustic sensing. TADAR is open-sourced on GitHub:\nhttps://github.com/aiot-lab/TADAR.",
    "updated" : "2024-09-26T11:17:41Z",
    "published" : "2024-09-26T11:17:41Z",
    "authors" : [
      {
        "name" : "Xie Zhang"
      },
      {
        "name" : "Chenshu Wu"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17705v1",
    "title" : "On the Output Redundancy of LTI Systems: A Geometric Approach with\n  Application to Privacy",
    "summary" : "This paper examines the properties of output-redundant systems, that is,\nsystems possessing a larger number of outputs than inputs, through the lenses\nof the geometric approach of Wonham et al. We begin by formulating a simple\noutput allocation synthesis problem, which involves ``concealing\" input\ninformation from a malicious eavesdropper having access to the system output,\nwhile still allowing for a legitimate user to reconstruct it. It is shown that\nthe solvability of this problem requires the availability of a redundant set of\noutputs. This very problem is instrumental to unveiling the fundamental\ngeometric properties of output-redundant systems, which form the basis for our\nsubsequent constructions and results. As a direct application, we demonstrate\nhow output allocation can be employed to effectively protect the information of\ninput information from certain output eavesdroppers with guaranteed results.",
    "updated" : "2024-09-26T10:21:39Z",
    "published" : "2024-09-26T10:21:39Z",
    "authors" : [
      {
        "name" : "Guitao Yang"
      },
      {
        "name" : "Alexander J. Gallo"
      },
      {
        "name" : "Angelo Barboni"
      },
      {
        "name" : "Riccardo M. G. Ferrari"
      },
      {
        "name" : "Andrea Serrani"
      },
      {
        "name" : "Thomas Parisini"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17700v1",
    "title" : "Demystifying Privacy in 5G Stand Alone Networks",
    "summary" : "Ensuring user privacy remains critical in mobile networks, particularly with\nthe rise of connected devices and denser 5G infrastructure. Privacy concerns\nhave persisted across 2G, 3G, and 4G/LTE networks. Recognizing these concerns,\nthe 3rd Generation Partnership Project (3GPP) has made privacy enhancements in\n5G Release 15. However, the extent of operator adoption remains unclear,\nespecially as most networks operate in 5G Non Stand Alone (NSA) mode, relying\non 4G Core Networks. This study provides the first qualitative and experimental\ncomparison between 5G NSA and Stand Alone (SA) in real operator networks,\nfocusing on privacy enhancements addressing top eight pre-5G attacks based on\nrecent academic literature. Additionally, it evaluates the privacy levels of\nOpenAirInterface (OAI), a leading open-source software for 5G, against real\nnetwork deployments for the same attacks. The analysis reveals two new 5G\nprivacy vulnerabilities, underscoring the need for further research and\nstricter standards.",
    "updated" : "2024-09-26T10:14:12Z",
    "published" : "2024-09-26T10:14:12Z",
    "authors" : [
      {
        "name" : "Stavros Eleftherakis"
      },
      {
        "name" : "Timothy Otim"
      },
      {
        "name" : "Giuseppe Santaromita"
      },
      {
        "name" : "Almudena Diaz Zayas"
      },
      {
        "name" : "Domenico Giustiniano"
      },
      {
        "name" : "Nicolas Kourtellis"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17642v1",
    "title" : "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic\n  Self-Disclosure",
    "summary" : "Large language model (LLM)-based AI delegates are increasingly utilized to\nact on behalf of users, assisting them with a wide range of tasks through\nconversational interfaces. Despite their advantages, concerns arise regarding\nthe potential risk of privacy leaks, particularly in scenarios involving social\ninteractions. While existing research has focused on protecting privacy by\nlimiting the access of AI delegates to sensitive user information, many social\nscenarios require disclosing private details to achieve desired outcomes,\nnecessitating a balance between privacy protection and disclosure. To address\nthis challenge, we conduct a pilot study to investigate user preferences for AI\ndelegates across various social relations and task scenarios, and then propose\na novel AI delegate system that enables privacy-conscious self-disclosure. Our\nuser study demonstrates that the proposed AI delegate strategically protects\nprivacy, pioneering its use in diverse and dynamic social interactions.",
    "updated" : "2024-09-26T08:45:15Z",
    "published" : "2024-09-26T08:45:15Z",
    "authors" : [
      {
        "name" : "Xi Chen"
      },
      {
        "name" : "Zhiyang Zhang"
      },
      {
        "name" : "Fangkai Yang"
      },
      {
        "name" : "Xiaoting Qin"
      },
      {
        "name" : "Chao Du"
      },
      {
        "name" : "Xi Cheng"
      },
      {
        "name" : "Hangxin Liu"
      },
      {
        "name" : "Qingwei Lin"
      },
      {
        "name" : "Saravan Rajmohan"
      },
      {
        "name" : "Dongmei Zhang"
      },
      {
        "name" : "Qi Zhang"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17623v1",
    "title" : "Fully Dynamic Graph Algorithms with Edge Differential Privacy",
    "summary" : "We study differentially private algorithms for analyzing graphs in the\nchallenging setting of continual release with fully dynamic updates, where\nedges are inserted and deleted over time, and the algorithm is required to\nupdate the solution at every time step. Previous work has presented\ndifferentially private algorithms for many graph problems that can handle\ninsertions only or deletions only (called partially dynamic algorithms) and\nobtained some hardness results for the fully dynamic setting. The only\nalgorithms in the latter setting were for the edge count, given by\nFichtenberger, Henzinger, and Ost (ESA 21), and for releasing the values of all\ngraph cuts, given by Fichtenberger, Henzinger, and Upadhyay (ICML 23). We\nprovide the first differentially private and fully dynamic graph algorithms for\nseveral other fundamental graph statistics (including the triangle count, the\nnumber of connected components, the size of the maximum matching, and the\ndegree histogram), analyze their error and show strong lower bounds on the\nerror for all algorithms in this setting. We study two variants of edge\ndifferential privacy for fully dynamic graph algorithms: event-level and\nitem-level. We give upper and lower bounds on the error of both event-level and\nitem-level fully dynamic algorithms for several fundamental graph problems. No\nfully dynamic algorithms that are private at the item-level (the more stringent\nof the two notions) were known before. In the case of item-level privacy, for\nseveral problems, our algorithms match our lower bounds.",
    "updated" : "2024-09-26T08:17:49Z",
    "published" : "2024-09-26T08:17:49Z",
    "authors" : [
      {
        "name" : "Sofya Raskhodnikova"
      },
      {
        "name" : "Teresa Anna Steiner"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17578v1",
    "title" : "Expanding Perspectives on Data Privacy: Insights from Rural Togo",
    "summary" : "Passively collected \"big\" data sources are increasingly used to inform\ncritical development policy decisions in low- and middle-income countries.\nWhile prior work highlights how such approaches may reveal sensitive\ninformation, enable surveillance, and centralize power, less is known about the\ncorresponding privacy concerns, hopes, and fears of the people directly\nimpacted by these policies -- people sometimes referred to as experiential\nexperts. To understand the perspectives of experiential experts, we conducted\nsemi-structured interviews with people living in rural villages in Togo shortly\nafter an entirely digital cash transfer program was launched that used machine\nlearning and mobile phone metadata to determine program eligibility. This paper\ndocuments participants' privacy concerns surrounding the introduction of big\ndata approaches in development policy. We find that the privacy concerns of our\nexperiential experts differ from those raised by privacy and development domain\nexperts. To facilitate a more robust and constructive account of privacy, we\ndiscuss implications for policies and designs that take seriously the privacy\nconcerns raised by both experiential experts and domain experts.",
    "updated" : "2024-09-26T06:47:16Z",
    "published" : "2024-09-26T06:47:16Z",
    "authors" : [
      {
        "name" : "Zoe Kahn"
      },
      {
        "name" : "Meyebinesso Farida Carelle Pere"
      },
      {
        "name" : "Emily Aiken"
      },
      {
        "name" : "Nitin Kohli"
      },
      {
        "name" : "Joshua E. Blumenstock"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17538v1",
    "title" : "On the Implicit Relation Between Low-Rank Adaptation and Differential\n  Privacy",
    "summary" : "A significant approach in natural language processing involves large-scale\npre-training on general domain data followed by adaptation to specific tasks or\ndomains. As models grow in size, full fine-tuning all parameters becomes\nincreasingly impractical. To address this, some methods for low-rank task\nadaptation of language models have been proposed, e.g. LoRA and FLoRA. These\nmethods keep the pre-trained model weights fixed and incorporate trainable\nlow-rank decomposition matrices into some layers of the transformer\narchitecture, called adapters. This approach significantly reduces the number\nof trainable parameters required for downstream tasks compared to full\nfine-tuning all parameters. In this work, we look at low-rank adaptation from\nthe lens of data privacy. We show theoretically that the low-rank adaptation\nused in LoRA and FLoRA is equivalent to injecting some random noise into the\nbatch gradients w.r.t the adapter parameters coming from their full\nfine-tuning, and we quantify the variance of the injected noise. By\nestablishing a Berry-Esseen type bound on the total variation distance between\nthe noise distribution and a Gaussian distribution with the same variance, we\nshow that the dynamics of LoRA and FLoRA are very close to differentially\nprivate full fine-tuning the adapters, which suggests that low-rank adaptation\nimplicitly provides privacy w.r.t the fine-tuning data. Finally, using\nJohnson-Lindenstrauss lemma, we show that when augmented with gradient\nclipping, low-rank adaptation is almost equivalent to differentially private\nfull fine-tuning adapters with a fixed noise scale.",
    "updated" : "2024-09-26T04:56:49Z",
    "published" : "2024-09-26T04:56:49Z",
    "authors" : [
      {
        "name" : "Saber Malekmohammadi"
      },
      {
        "name" : "Golnoosh Farnadi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17535v1",
    "title" : "Privacy-Preserving Redaction of Diagnosis Data through Source Code\n  Analysis",
    "summary" : "Protecting sensitive information in diagnostic data such as logs, is a\ncritical concern in the industrial software diagnosis and debugging process.\nWhile there are many tools developed to automatically redact the logs for\nidentifying and removing sensitive information, they have severe limitations\nwhich can cause either over redaction and loss of critical diagnostic\ninformation (false positives), or disclosure of sensitive information (false\nnegatives), or both. To address the problem, in this paper, we argue for a\nsource code analysis approach for log redaction. To identify a log message\ncontaining sensitive information, our method locates the corresponding log\nstatement in the source code with logger code augmentation, and checks if the\nlog statement outputs data from sensitive sources by using the data flow graph\nbuilt from the source code. Appropriate redaction rules are further applied\ndepending on the sensitiveness of the data sources to preserve the privacy\ninformation in the logs. We conducted experimental evaluation and comparison\nwith other popular baselines. The results demonstrate that our approach can\nsignificantly improve the detection precision of the sensitive information and\nreduce both false positives and negatives.",
    "updated" : "2024-09-26T04:41:55Z",
    "published" : "2024-09-26T04:41:55Z",
    "authors" : [
      {
        "name" : "Lixi Zhou"
      },
      {
        "name" : "Lei Yu"
      },
      {
        "name" : "Jia Zou"
      },
      {
        "name" : "Hong Min"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17509v1",
    "title" : "BioZero: An Efficient and Privacy-Preserving Decentralized Biometric\n  Authentication Protocol on Open Blockchain",
    "summary" : "Digital identity plays a vital role in enabling secure access to resources\nand services in the digital world. Traditional identity authentication methods,\nsuch as password-based and biometric authentications, have limitations in terms\nof security, privacy, and scalability. Decentralized authentication approaches\nleveraging blockchain technology have emerged as a promising solution. However,\nexisting decentralized authentication methods often rely on indirect identity\nverification (e.g. using passwords or digital signatures as authentication\ncredentials) and face challenges such as Sybil attacks. In this paper, we\npropose BioZero, an efficient and privacy-preserving decentralized biometric\nauthentication protocol that can be implemented on open blockchain. BioZero\nleverages Pedersen commitment and homomorphic computation to protect user\nbiometric privacy while enabling efficient verification. We enhance the\nprotocol with non-interactive homomorphic computation and employ zero-knowledge\nproofs for secure on-chain verification. The unique aspect of BioZero is that\nit is fully decentralized and can be executed by blockchain smart contracts in\na very efficient way. We analyze the security of BioZero and validate its\nperformance through a prototype implementation. The results demonstrate the\neffectiveness, efficiency, and security of BioZero in decentralized\nauthentication scenarios. Our work contributes to the advancement of\ndecentralized identity authentication using biometrics.",
    "updated" : "2024-09-26T03:37:35Z",
    "published" : "2024-09-26T03:37:35Z",
    "authors" : [
      {
        "name" : "Junhao Lai"
      },
      {
        "name" : "Taotao Wang"
      },
      {
        "name" : "Shengli Zhang"
      },
      {
        "name" : "Qing Yang"
      },
      {
        "name" : "Soung Chang Liew"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17315v1",
    "title" : "KIPPS: Knowledge infusion in Privacy Preserving Synthetic Data\n  Generation",
    "summary" : "The integration of privacy measures, including differential privacy\ntechniques, ensures a provable privacy guarantee for the synthetic data.\nHowever, challenges arise for Generative Deep Learning models when tasked with\ngenerating realistic data, especially in critical domains such as Cybersecurity\nand Healthcare. Generative Models optimized for continuous data struggle to\nmodel discrete and non-Gaussian features that have domain constraints.\nChallenges increase when the training datasets are limited and not diverse. In\nsuch cases, generative models create synthetic data that repeats sensitive\nfeatures, which is a privacy risk. Moreover, generative models face\ndifficulties comprehending attribute constraints in specialized domains. This\nleads to the generation of unrealistic data that impacts downstream accuracy.\nTo address these issues, this paper proposes a novel model, KIPPS, that infuses\nDomain and Regulatory Knowledge from Knowledge Graphs into Generative Deep\nLearning models for enhanced Privacy Preserving Synthetic data generation. The\nnovel framework augments the training of generative models with supplementary\ncontext about attribute values and enforces domain constraints during training.\nThis added guidance enhances the model's capacity to generate realistic and\ndomain-compliant synthetic data. The proposed model is evaluated on real-world\ndatasets, specifically in the domains of Cybersecurity and Healthcare, where\ndomain constraints and rules add to the complexity of the data. Our experiments\nevaluate the privacy resilience and downstream accuracy of the model against\nbenchmark methods, demonstrating its effectiveness in addressing the balance\nbetween privacy preservation and data accuracy in complex domains.",
    "updated" : "2024-09-25T19:50:03Z",
    "published" : "2024-09-25T19:50:03Z",
    "authors" : [
      {
        "name" : "Anantaa Kotal"
      },
      {
        "name" : "Anupam Joshi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17283v1",
    "title" : "Investigating Privacy Attacks in the Gray-Box Setting to Enhance\n  Collaborative Learning Schemes",
    "summary" : "The notion that collaborative machine learning can ensure privacy by just\nwithholding the raw data is widely acknowledged to be flawed. Over the past\nseven years, the literature has revealed several privacy attacks that enable\nadversaries to extract information about a model's training dataset by\nexploiting access to model parameters during or after training. In this work,\nwe study privacy attacks in the gray-box setting, where the attacker has only\nlimited access - in terms of view and actions - to the model. The findings of\nour investigation provide new insights for the development of\nprivacy-preserving collaborative learning solutions. We deploy SmartCryptNN, a\nframework that tailors homomorphic encryption to protect the portions of the\nmodel posing higher privacy risks. Our solution offers a trade-off between\nprivacy and efficiency, which varies based on the extent and selection of the\nmodel components we choose to protect. We explore it on dense neural networks,\nwhere through extensive evaluation of diverse datasets and architectures, we\nuncover instances where a favorable sweet spot in the trade-off can be achieved\nby safeguarding only a single layer of the network. In one of such instances,\nour approach trains ~4 times faster compared to fully encrypted solutions,\nwhile reducing membership leakage by 17.8 times compared to plaintext\nsolutions.",
    "updated" : "2024-09-25T18:49:21Z",
    "published" : "2024-09-25T18:49:21Z",
    "authors" : [
      {
        "name" : "Federico Mazzone"
      },
      {
        "name" : "Ahmad Al Badawi"
      },
      {
        "name" : "Yuriy Polyakov"
      },
      {
        "name" : "Maarten Everts"
      },
      {
        "name" : "Florian Hahn"
      },
      {
        "name" : "Andreas Peter"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.17201v1",
    "title" : "Immersion and Invariance-based Coding for Privacy-Preserving Federated\n  Learning",
    "summary" : "Federated learning (FL) has emerged as a method to preserve privacy in\ncollaborative distributed learning. In FL, clients train AI models directly on\ntheir devices rather than sharing data with a centralized server, which can\npose privacy risks. However, it has been shown that despite FL's partial\nprotection of local data privacy, information about clients' data can still be\ninferred from shared model updates during training. In recent years, several\nprivacy-preserving approaches have been developed to mitigate this privacy\nleakage in FL, though they often provide privacy at the cost of model\nperformance or system efficiency. Balancing these trade-offs presents a\nsignificant challenge in implementing FL schemes. In this manuscript, we\nintroduce a privacy-preserving FL framework that combines differential privacy\nand system immersion tools from control theory. The core idea is to treat the\noptimization algorithms used in standard FL schemes (e.g., gradient-based\nalgorithms) as a dynamical system that we seek to immerse into a\nhigher-dimensional system (referred to as the target optimization algorithm).\nThe target algorithm's dynamics are designed such that, first, the model\nparameters of the original algorithm are immersed in its parameters; second, it\noperates on distorted parameters; and third, it converges to an encoded version\nof the true model parameters from the original algorithm. These encoded\nparameters can then be decoded at the server to retrieve the original model\nparameters. We demonstrate that the proposed privacy-preserving scheme can be\ntailored to offer any desired level of differential privacy for both local and\nglobal model parameters, while maintaining the same accuracy and convergence\nrate as standard FL algorithms.",
    "updated" : "2024-09-25T15:04:42Z",
    "published" : "2024-09-25T15:04:42Z",
    "authors" : [
      {
        "name" : "Haleh Hayati"
      },
      {
        "name" : "Carlos Murguia"
      },
      {
        "name" : "Nathan van de Wouw"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.16106v2",
    "title" : "Scenario of Use Scheme: Threat Model Specification for Speaker Privacy\n  Protection in the Medical Domain",
    "summary" : "Speech recordings are being more frequently used to detect and monitor\ndisease, leading to privacy concerns. Beyond cryptography, protection of speech\ncan be addressed by approaches, such as perturbation, disentanglement, and\nre-synthesis, that eliminate sensitive information of the speaker, leaving the\ninformation necessary for medical analysis purposes. In order for such privacy\nprotective approaches to be developed, clear and systematic specifications of\nassumptions concerning medical settings and the needs of medical professionals\nare necessary. In this paper, we propose a Scenario of Use Scheme that\nincorporates an Attacker Model, which characterizes the adversary against whom\nthe speaker's privacy must be defended, and a Protector Model, which specifies\nthe defense. We discuss the connection of the scheme with previous work on\nspeech privacy. Finally, we present a concrete example of a specified Scenario\nof Use and a set of experiments about protecting speaker data against gender\ninference attacks while maintaining utility for Parkinson's detection.",
    "updated" : "2024-09-26T13:05:36Z",
    "published" : "2024-09-24T14:07:47Z",
    "authors" : [
      {
        "name" : "Mehtab Ur Rahman"
      },
      {
        "name" : "Martha Larson"
      },
      {
        "name" : "Louis ten Bosch"
      },
      {
        "name" : "Cristian Tejedor-García"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.AI",
      "cs.CR",
      "cs.SD"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.18907v1",
    "title" : "In-depth Analysis of Privacy Threats in Federated Learning for Medical\n  Data",
    "summary" : "Federated learning is emerging as a promising machine learning technique in\nthe medical field for analyzing medical images, as it is considered an\neffective method to safeguard sensitive patient data and comply with privacy\nregulations. However, recent studies have revealed that the default settings of\nfederated learning may inadvertently expose private training data to privacy\nattacks. Thus, the intensity of such privacy risks and potential mitigation\nstrategies in the medical domain remain unclear. In this paper, we make three\noriginal contributions to privacy risk analysis and mitigation in federated\nlearning for medical data. First, we propose a holistic framework, MedPFL, for\nanalyzing privacy risks in processing medical data in the federated learning\nenvironment and developing effective mitigation strategies for protecting\nprivacy. Second, through our empirical analysis, we demonstrate the severe\nprivacy risks in federated learning to process medical images, where\nadversaries can accurately reconstruct private medical images by performing\nprivacy attacks. Third, we illustrate that the prevalent defense mechanism of\nadding random noises may not always be effective in protecting medical images\nagainst privacy attacks in federated learning, which poses unique and pressing\nchallenges related to protecting the privacy of medical data. Furthermore, the\npaper discusses several unique research questions related to the privacy\nprotection of medical data in the federated learning environment. We conduct\nextensive experiments on several benchmark medical image datasets to analyze\nand mitigate the privacy risks associated with federated learning for medical\ndata.",
    "updated" : "2024-09-27T16:45:35Z",
    "published" : "2024-09-27T16:45:35Z",
    "authors" : [
      {
        "name" : "Badhan Chandra Das"
      },
      {
        "name" : "M. Hadi Amini"
      },
      {
        "name" : "Yanzhao Wu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.18674v1",
    "title" : "Image-guided topic modeling for interpretable privacy classification",
    "summary" : "Predicting and explaining the private information contained in an image in\nhuman-understandable terms is a complex and contextual task. This task is\nchallenging even for large language models. To facilitate the understanding of\nprivacy decisions, we propose to predict image privacy based on a set of\nnatural language content descriptors. These content descriptors are associated\nwith privacy scores that reflect how people perceive image content. We generate\ndescriptors with our novel Image-guided Topic Modeling (ITM) approach. ITM\nleverages, via multimodality alignment, both vision information and image\ntextual descriptions from a vision language model. We use the ITM-generated\ndescriptors to learn a privacy predictor, Priv$\\times$ITM, whose decisions are\ninterpretable by design. Our Priv$\\times$ITM classifier outperforms the\nreference interpretable method by 5 percentage points in accuracy and performs\ncomparably to the current non-interpretable state-of-the-art model.",
    "updated" : "2024-09-27T12:02:28Z",
    "published" : "2024-09-27T12:02:28Z",
    "authors" : [
      {
        "name" : "Alina Elena Baia"
      },
      {
        "name" : "Andrea Cavallaro"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.18611v1",
    "title" : "Differentially Private Non Parametric Copulas: Generating synthetic data\n  with non parametric copulas under privacy guarantees",
    "summary" : "Creation of synthetic data models has represented a significant advancement\nacross diverse scientific fields, but this technology also brings important\nprivacy considerations for users. This work focuses on enhancing a\nnon-parametric copula-based synthetic data generation model, DPNPC, by\nincorporating Differential Privacy through an Enhanced Fourier Perturbation\nmethod. The model generates synthetic data for mixed tabular databases while\npreserving privacy. We compare DPNPC with three other models (PrivBayes,\nDP-Copula, and DP-Histogram) across three public datasets, evaluating privacy,\nutility, and execution time. DPNPC outperforms others in modeling multivariate\ndependencies, maintaining privacy for small $\\epsilon$ values, and reducing\ntraining times. However, limitations include the need to assess the model's\nperformance with different encoding methods and consider additional privacy\nattacks. Future research should address these areas to enhance\nprivacy-preserving synthetic data generation.",
    "updated" : "2024-09-27T10:18:14Z",
    "published" : "2024-09-27T10:18:14Z",
    "authors" : [
      {
        "name" : "Pablo A. Osorio-Marulanda"
      },
      {
        "name" : "John Esteban Castro Ramirez"
      },
      {
        "name" : "Mikel Hernández Jiménez"
      },
      {
        "name" : "Nicolas Moreno Reyes"
      },
      {
        "name" : "Gorka Epelde Unanue"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.DB",
      "62H05, 62G32",
      "I.2.6; H.2.8; G.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.18601v1",
    "title" : "Privacy-Preserving Quantum Annealing for Quadratic Unconstrained Binary\n  Optimization (QUBO) Problems",
    "summary" : "Quantum annealers offer a promising approach to solve Quadratic Unconstrained\nBinary Optimization (QUBO) problems, which have a wide range of applications.\nHowever, when a user submits its QUBO problem to a third-party quantum\nannealer, the problem itself may disclose the user's private information to the\nquantum annealing service provider. To mitigate this risk, we introduce a\nprivacy-preserving QUBO framework and propose a novel solution method. Our\napproach employs a combination of digit-wise splitting and matrix permutation\nto obfuscate the QUBO problem's model matrix $Q$, effectively concealing the\nmatrix elements. In addition, based on the solution to the obfuscated version\nof the QUBO problem, we can reconstruct the solution to the original problem\nwith high accuracy. Theoretical analysis and empirical tests confirm the\nefficacy and efficiency of our proposed technique, demonstrating its potential\nfor preserving user privacy in quantum annealing services.",
    "updated" : "2024-09-27T10:05:08Z",
    "published" : "2024-09-27T10:05:08Z",
    "authors" : [
      {
        "name" : "Moyang Xie"
      },
      {
        "name" : "Yuan Zhang"
      },
      {
        "name" : "Sheng Zhong"
      },
      {
        "name" : "Qun Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "quant-ph"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.18360v1",
    "title" : "Architecture for Protecting Data Privacy in Decentralized Social\n  Networks",
    "summary" : "Centralized social networks have experienced a transformative impact on our\ndigital era communication, connection, and information-sharing information.\nHowever, it has also raised significant concerns regarding users' privacy and\nindividual rights. In response to these concerns, this paper proposes a novel\nDecentralized Social Network employing Blockchain technology and Decentralized\nStorage Networks completed by Access Control Smart Contracts. The initial phase\ncomprises a comprehensive literature review, delving into decentralized social\nnetworks, explaining the review methodology, and presenting the resulting\nfindings. Building upon these findings and an analysis of previous research\ngaps, we propose a novel architecture for decentralized social networks. In\nconclusion, the principal results highlight the benefit of our decentralized\nsocial network to protect user privacy. Moreover, the users have all rights to\ntheir posted information following the General Data Protection Regulation\n(GDPR).",
    "updated" : "2024-09-27T00:35:02Z",
    "published" : "2024-09-27T00:35:02Z",
    "authors" : [
      {
        "name" : "Quang Cao"
      },
      {
        "name" : "Katerina Vgena"
      },
      {
        "name" : "Aikaterini-Georgia Mavroeidi"
      },
      {
        "name" : "Christos Kalloniatis"
      },
      {
        "name" : "Xun Yi"
      },
      {
        "name" : "Son Hoang Dau"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.18356v1",
    "title" : "FedDCL: a federated data collaboration learning as a hybrid-type\n  privacy-preserving framework based on federated learning and data\n  collaboration",
    "summary" : "Recently, federated learning has attracted much attention as a\nprivacy-preserving integrated analysis that enables integrated analysis of data\nheld by multiple institutions without sharing raw data. On the other hand,\nfederated learning requires iterative communication across institutions and has\na big challenge for implementation in situations where continuous communication\nwith the outside world is extremely difficult. In this study, we propose a\nfederated data collaboration learning (FedDCL), which solves such communication\nissues by combining federated learning with recently proposed non-model\nshare-type federated learning named as data collaboration analysis. In the\nproposed FedDCL framework, each user institution independently constructs\ndimensionality-reduced intermediate representations and shares them with\nneighboring institutions on intra-group DC servers. On each intra-group DC\nserver, intermediate representations are transformed to incorporable forms\ncalled collaboration representations. Federated learning is then conducted\nbetween intra-group DC servers. The proposed FedDCL framework does not require\niterative communication by user institutions and can be implemented in\nsituations where continuous communication with the outside world is extremely\ndifficult. The experimental results show that the performance of the proposed\nFedDCL is comparable to that of existing federated learning.",
    "updated" : "2024-09-27T00:22:38Z",
    "published" : "2024-09-27T00:22:38Z",
    "authors" : [
      {
        "name" : "Akira Imakura"
      },
      {
        "name" : "Tetsuya Sakurai"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.18245v1",
    "title" : "PDFed: Privacy-Preserving and Decentralized Asynchronous Federated\n  Learning for Diffusion Models",
    "summary" : "We present PDFed, a decentralized, aggregator-free, and asynchronous\nfederated learning protocol for training image diffusion models using a public\nblockchain. In general, diffusion models are prone to memorization of training\ndata, raising privacy and ethical concerns (e.g., regurgitation of private\ntraining data in generated images). Federated learning (FL) offers a partial\nsolution via collaborative model training across distributed nodes that\nsafeguard local data privacy. PDFed proposes a novel sample-based score that\nmeasures the novelty and quality of generated samples, incorporating these into\na blockchain-based federated learning protocol that we show reduces private\ndata memorization in the collaboratively trained model. In addition, PDFed\nenables asynchronous collaboration among participants with varying hardware\ncapabilities, facilitating broader participation. The protocol records the\nprovenance of AI models, improving transparency and auditability, while also\nconsidering automated incentive and reward mechanisms for participants. PDFed\naims to empower artists and creators by protecting the privacy of creative\nworks and enabling decentralized, peer-to-peer collaboration. The protocol\npositively impacts the creative economy by opening up novel revenue streams and\nfostering innovative ways for artists to benefit from their contributions to\nthe AI space.",
    "updated" : "2024-09-26T19:38:44Z",
    "published" : "2024-09-26T19:38:44Z",
    "authors" : [
      {
        "name" : "Kar Balan"
      },
      {
        "name" : "Andrew Gilbert"
      },
      {
        "name" : "John Collomosse"
      }
    ],
    "categories" : [
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.16688v2",
    "title" : "Cycle Counting under Local Differential Privacy for Degeneracy-bounded\n  Graphs",
    "summary" : "We propose an algorithm for counting the number of cycles under local\ndifferential privacy for degeneracy-bounded input graphs. Numerous studies have\nfocused on counting the number of triangles under the privacy notion,\ndemonstrating that the expected $\\ell_2$-error of these algorithms is\n$\\Omega(n^{1.5})$, where $n$ is the number of nodes in the graph. When\nparameterized by the number of cycles of length four ($C_4$), the best existing\ntriangle counting algorithm has an error of $O(n^{1.5} + \\sqrt{C_4}) = O(n^2)$.\nIn this paper, we introduce an algorithm with an expected $\\ell_2$-error of\n$O(\\delta^{1.5} n^{0.5} + \\delta^{0.5} d_{\\max}^{0.5} n^{0.5})$, where $\\delta$\nis the degeneracy and $d_{\\max}$ is the maximum degree of the graph. For\ndegeneracy-bounded graphs ($\\delta \\in \\Theta(1)$) commonly found in practical\nsocial networks, our algorithm achieves an expected $\\ell_2$-error of\n$O(d_{\\max}^{0.5} n^{0.5}) = O(n)$. Our algorithm's core idea is a precise\ncount of triangles following a preprocessing step that approximately sorts the\ndegree of all nodes. This approach can be extended to approximate the number of\ncycles of length $k$, maintaining a similar $\\ell_2$-error, namely\n$O(\\delta^{(k-2)/2} d_{\\max}^{0.5} n^{(k-2)/2} + \\delta^{k/2} n^{(k-2)/2})$ or\n$O(d_{\\max}^{0.5} n^{(k-2)/2}) = O(n^{(k-1)/2})$ for degeneracy-bounded graphs.",
    "updated" : "2024-09-27T03:48:50Z",
    "published" : "2024-09-25T07:23:58Z",
    "authors" : [
      {
        "name" : "Quentin Hillebrand"
      },
      {
        "name" : "Vorapong Suppakitpaisarn"
      },
      {
        "name" : "Tetsuo Shibuya"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.19988v1",
    "title" : "Enhancing Security Using Random Binary Weights in Privacy-Preserving\n  Federated Learning",
    "summary" : "In this paper, we propose a novel method for enhancing security in\nprivacy-preserving federated learning using the Vision Transformer. In\nfederated learning, learning is performed by collecting updated information\nwithout collecting raw data from each client. However, the problem is that this\nraw data may be inferred from the updated information. Conventional\ndata-guessing countermeasures (security enhancement methods) for addressing\nthis issue have a trade-off relationship between privacy protection strength\nand learning efficiency, and they generally degrade model performance. In this\npaper, we propose a novel method of federated learning that does not degrade\nmodel performance and that is robust against data-guessing attacks on updated\ninformation. In the proposed method, each client independently prepares a\nsequence of binary (0 or 1) random numbers, multiplies it by the updated\ninformation, and sends it to a server for model learning. In experiments, the\neffectiveness of the proposed method is confirmed in terms of model performance\nand resistance to the APRIL (Attention PRIvacy Leakage) restoration attack.",
    "updated" : "2024-09-30T06:28:49Z",
    "published" : "2024-09-30T06:28:49Z",
    "authors" : [
      {
        "name" : "Hiroto Sawada"
      },
      {
        "name" : "Shoko Imaizumi"
      },
      {
        "name" : "Hitoshi Kiya"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.19964v1",
    "title" : "Comments on \"Privacy-Enhanced Federated Learning Against Poisoning\n  Adversaries\"",
    "summary" : "In August 2021, Liu et al. (IEEE TIFS'21) proposed a privacy-enhanced\nframework named PEFL to efficiently detect poisoning behaviours in Federated\nLearning (FL) using homomorphic encryption. In this article, we show that PEFL\ndoes not preserve privacy. In particular, we illustrate that PEFL reveals the\nentire gradient vector of all users in clear to one of the participating\nentities, thereby violating privacy. Furthermore, we clearly show that an\nimmediate fix for this issue is still insufficient to achieve privacy by\npointing out multiple flaws in the proposed system.\n  Note: Although our privacy issues mentioned in Section II have been published\nin January 2023 (Schneider et. al., IEEE TIFS'23), several subsequent papers\ncontinued to reference Liu et al. (IEEE TIFS'21) as a potential solution for\nprivate federated learning. While a few works have acknowledged the privacy\nconcerns we raised, several of subsequent works either propagate these errors\nor adopt the constructions from Liu et al. (IEEE TIFS'21), thereby\nunintentionally inheriting the same privacy vulnerabilities. We believe this\noversight is partly due to the limited visibility of our comments paper at\nTIFS'23 (Schneider et. al., IEEE TIFS'23). Consequently, to prevent the\ncontinued propagation of the flawed algorithms in Liu et al. (IEEE TIFS'21)\ninto future research, we also put this article to an ePrint.",
    "updated" : "2024-09-30T05:34:20Z",
    "published" : "2024-09-30T05:34:20Z",
    "authors" : [
      {
        "name" : "Thomas Schneider"
      },
      {
        "name" : "Ajith Suresh"
      },
      {
        "name" : "Hossein Yalame"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.19756v1",
    "title" : "Advances in Privacy Preserving Federated Learning to Realize a Truly\n  Learning Healthcare System",
    "summary" : "The concept of a learning healthcare system (LHS) envisions a self-improving\nnetwork where multimodal data from patient care are continuously analyzed to\nenhance future healthcare outcomes. However, realizing this vision faces\nsignificant challenges in data sharing and privacy protection.\nPrivacy-Preserving Federated Learning (PPFL) is a transformative and promising\napproach that has the potential to address these challenges by enabling\ncollaborative learning from decentralized data while safeguarding patient\nprivacy. This paper proposes a vision for integrating PPFL into the healthcare\necosystem to achieve a truly LHS as defined by the Institute of Medicine (IOM)\nRoundtable.",
    "updated" : "2024-09-29T20:02:40Z",
    "published" : "2024-09-29T20:02:40Z",
    "authors" : [
      {
        "name" : "Ravi Madduri"
      },
      {
        "name" : "Zilinghan Li"
      },
      {
        "name" : "Tarak Nandi"
      },
      {
        "name" : "Kibaek Kim"
      },
      {
        "name" : "Minseok Ryu"
      },
      {
        "name" : "Alex Rodriguez"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.19413v1",
    "title" : "Membership Privacy Evaluation in Deep Spiking Neural Networks",
    "summary" : "Artificial Neural Networks (ANNs), commonly mimicking neurons with non-linear\nfunctions to output floating-point numbers, consistently receive the same\nsignals of a data point during its forward time. Unlike ANNs, Spiking Neural\nNetworks (SNNs) get various input signals in the forward time of a data point\nand simulate neurons in a biologically plausible way, i.e., producing a spike\n(a binary value) if the accumulated membrane potential of a neuron is larger\nthan a threshold. Even though ANNs have achieved remarkable success in multiple\ntasks, e.g., face recognition and object detection, SNNs have recently obtained\nattention due to their low power consumption, fast inference, and event-driven\nproperties. While privacy threats against ANNs are widely explored, much less\nwork has been done on SNNs. For instance, it is well-known that ANNs are\nvulnerable to the Membership Inference Attack (MIA), but whether the same\napplies to SNNs is not explored.\n  In this paper, we evaluate the membership privacy of SNNs by considering\neight MIAs, seven of which are inspired by MIAs against ANNs. Our evaluation\nresults show that SNNs are more vulnerable (maximum 10% higher in terms of\nbalanced attack accuracy) than ANNs when both are trained with neuromorphic\ndatasets (with time dimension). On the other hand, when training ANNs or SNNs\nwith static datasets (without time dimension), the vulnerability depends on the\ndataset used. If we convert ANNs trained with static datasets to SNNs, the\naccuracy of MIAs drops (maximum 11.5% with a reduction of 7.6% on the test\naccuracy of the target model). Next, we explore the impact factors of MIAs on\nSNNs by conducting a hyperparameter study. Finally, we show that the basic data\naugmentation method for static data and two recent data augmentation methods\nfor neuromorphic data can considerably (maximum reduction of 25.7%) decrease\nMIAs' performance on SNNs.",
    "updated" : "2024-09-28T17:13:04Z",
    "published" : "2024-09-28T17:13:04Z",
    "authors" : [
      {
        "name" : "Jiaxin Li"
      },
      {
        "name" : "Gorka Abad"
      },
      {
        "name" : "Stjepan Picek"
      },
      {
        "name" : "Mauro Conti"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.19334v1",
    "title" : "OnePath: Efficient and Privacy-Preserving Decision Tree Inference in the\n  Cloud",
    "summary" : "The expansive storage capacity and robust computational power of cloud\nservers have led to the widespread outsourcing of machine learning inference\nservices to the cloud. While this practice offers significant operational\nbenefits, it also poses substantial privacy risks, including the exposure of\nproprietary models and sensitive user data. In this paper, we introduce\nOnePath, a framework designed for secure and efficient decision tree inference\nin cloud environments. Unlike existing schemes that require traversing all\ninternal nodes of a decision tree, our protocol securely identifies and\nprocesses only the nodes on the prediction path, maintaining data privacy under\nciphertext throughout the inference process. This selective traversal enhances\nboth security and efficiency. To further optimize privacy and performance,\nOnePath employs lightweight cryptographic techniques, such as functional\nencryption, during the online phase of secure inference. Notably, our protocol\nallows both providers and clients to perform secure inference without the need\nto remain online continuously, a critical advantage for real-world\napplications. We substantiate the security of our framework with formal proofs,\ndemonstrating that OnePath robustly protects the privacy of decision tree\nclassifiers and user data. Experimental results highlight the efficiency of our\napproach, with our scheme processing query data in mere microseconds on the\ntested dataset. Through OnePath, we provide a practical solution that balances\nthe needs for security and efficiency in cloud-based decision tree inference,\nmaking it a promising option for a variety of applications.",
    "updated" : "2024-09-28T12:35:32Z",
    "published" : "2024-09-28T12:35:32Z",
    "authors" : [
      {
        "name" : "Shuai Yuan"
      },
      {
        "name" : "Hongwei Li"
      },
      {
        "name" : "Xinyuan Qian"
      },
      {
        "name" : "Wenbo Jiang"
      },
      {
        "name" : "Guowen Xu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.19306v1",
    "title" : "CausalVE: Face Video Privacy Encryption via Causal Video Prediction",
    "summary" : "Advanced facial recognition technologies and recommender systems with\ninadequate privacy technologies and policies for facial interactions increase\nconcerns about bioprivacy violations. With the proliferation of video and\nlive-streaming websites, public-face video distribution and interactions pose\ngreater privacy risks. Existing techniques typically address the risk of\nsensitive biometric information leakage through various privacy enhancement\nmethods but pose a higher security risk by corrupting the information to be\nconveyed by the interaction data, or by leaving certain biometric features\nintact that allow an attacker to infer sensitive biometric information from\nthem. To address these shortcomings, in this paper, we propose a neural network\nframework, CausalVE. We obtain cover images by adopting a diffusion model to\nachieve face swapping with face guidance and use the speech sequence features\nand spatiotemporal sequence features of the secret video for dynamic video\ninference and prediction to obtain a cover video with the same number of frames\nas the secret video. In addition, we hide the secret video by using reversible\nneural networks for video hiding so that the video can also disseminate secret\ndata. Numerous experiments prove that our CausalVE has good security in public\nvideo dissemination and outperforms state-of-the-art methods from a\nqualitative, quantitative, and visual point of view.",
    "updated" : "2024-09-28T10:34:22Z",
    "published" : "2024-09-28T10:34:22Z",
    "authors" : [
      {
        "name" : "Yubo Huang"
      },
      {
        "name" : "Wenhao Feng"
      },
      {
        "name" : "Xin Lai"
      },
      {
        "name" : "Zixi Wang"
      },
      {
        "name" : "Jingzehua Xu"
      },
      {
        "name" : "Shuai Zhang"
      },
      {
        "name" : "Hongjie He"
      },
      {
        "name" : "Fan Chen"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.19301v1",
    "title" : "Privacy Attack in Federated Learning is Not Easy: An Experimental Study",
    "summary" : "Federated learning (FL) is an emerging distributed machine learning paradigm\nproposed for privacy preservation. Unlike traditional centralized learning\napproaches, FL enables multiple users to collaboratively train a shared global\nmodel without disclosing their own data, thereby significantly reducing the\npotential risk of privacy leakage. However, recent studies have indicated that\nFL cannot entirely guarantee privacy protection, and attackers may still be\nable to extract users' private data through the communicated model gradients.\nAlthough numerous privacy attack FL algorithms have been developed, most are\ndesigned to reconstruct private data from a single step of calculated\ngradients. It remains uncertain whether these methods are effective in\nrealistic federated environments or if they have other limitations. In this\npaper, we aim to help researchers better understand and evaluate the\neffectiveness of privacy attacks on FL. We analyze and discuss recent research\npapers on this topic and conduct experiments in a real FL environment to\ncompare the performance of various attack methods. Our experimental results\nreveal that none of the existing state-of-the-art privacy attack algorithms can\neffectively breach private client data in realistic FL settings, even in the\nabsence of defense strategies. This suggests that privacy attacks in FL are\nmore challenging than initially anticipated.",
    "updated" : "2024-09-28T10:06:34Z",
    "published" : "2024-09-28T10:06:34Z",
    "authors" : [
      {
        "name" : "Hangyu Zhu"
      },
      {
        "name" : "Liyuan Huang"
      },
      {
        "name" : "Zhenping Xie"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.19092v1",
    "title" : "Federated Online Prediction from Experts with Differential Privacy:\n  Separations and Regret Speed-ups",
    "summary" : "We study the problems of differentially private federated online prediction\nfrom experts against both stochastic adversaries and oblivious adversaries. We\naim to minimize the average regret on $m$ clients working in parallel over time\nhorizon $T$ with explicit differential privacy (DP) guarantees. With stochastic\nadversaries, we propose a Fed-DP-OPE-Stoch algorithm that achieves\n$\\sqrt{m}$-fold speed-up of the per-client regret compared to the single-player\ncounterparts under both pure DP and approximate DP constraints, while\nmaintaining logarithmic communication costs. With oblivious adversaries, we\nestablish non-trivial lower bounds indicating that collaboration among clients\ndoes not lead to regret speed-up with general oblivious adversaries. We then\nconsider a special case of the oblivious adversaries setting, where there\nexists a low-loss expert. We design a new algorithm Fed-SVT and show that it\nachieves an $m$-fold regret speed-up under both pure DP and approximate DP\nconstraints over the single-player counterparts. Our lower bound indicates that\nFed-SVT is nearly optimal up to logarithmic factors. Experiments demonstrate\nthe effectiveness of our proposed algorithms. To the best of our knowledge,\nthis is the first work examining the differentially private online prediction\nfrom experts in the federated setting.",
    "updated" : "2024-09-27T18:43:24Z",
    "published" : "2024-09-27T18:43:24Z",
    "authors" : [
      {
        "name" : "Fengyu Gao"
      },
      {
        "name" : "Ruiquan Huang"
      },
      {
        "name" : "Jing Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.19078v1",
    "title" : "Differential privacy for protecting patient data in speech disorder\n  detection using deep learning",
    "summary" : "Speech pathology has impacts on communication abilities and quality of life.\nWhile deep learning-based models have shown potential in diagnosing these\ndisorders, the use of sensitive data raises critical privacy concerns. Although\ndifferential privacy (DP) has been explored in the medical imaging domain, its\napplication in pathological speech analysis remains largely unexplored despite\nthe equally critical privacy concerns. This study is the first to investigate\nDP's impact on pathological speech data, focusing on the trade-offs between\nprivacy, diagnostic accuracy, and fairness. Using a large, real-world dataset\nof 200 hours of recordings from 2,839 German-speaking participants, we observed\na maximum accuracy reduction of 3.85% when training with DP with a privacy\nbudget, denoted by {\\epsilon}, of 7.51. To generalize our findings, we\nvalidated our approach on a smaller dataset of Spanish-speaking Parkinson's\ndisease patients, demonstrating that careful pretraining on large-scale\ntask-specific datasets can maintain or even improve model accuracy under DP\nconstraints. We also conducted a comprehensive fairness analysis, revealing\nthat reasonable privacy levels (2<{\\epsilon}<10) do not introduce significant\ngender bias, though age-related disparities may require further attention. Our\nresults suggest that DP can effectively balance privacy and utility in speech\ndisorder detection, but also highlight the unique challenges in the speech\ndomain, particularly regarding the privacy-fairness trade-off. This provides a\nfoundation for future work to refine DP methodologies and address fairness\nacross diverse patient groups in real-world deployments.",
    "updated" : "2024-09-27T18:25:54Z",
    "published" : "2024-09-27T18:25:54Z",
    "authors" : [
      {
        "name" : "Soroosh Tayebi Arasteh"
      },
      {
        "name" : "Mahshad Lotfinia"
      },
      {
        "name" : "Paula Andrea Perez-Toro"
      },
      {
        "name" : "Tomas Arias-Vergara"
      },
      {
        "name" : "Juan Rafael Orozco-Arroyave"
      },
      {
        "name" : "Maria Schuster"
      },
      {
        "name" : "Andreas Maier"
      },
      {
        "name" : "Seung Hee Yang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.15868v3",
    "title" : "Privacy Evaluation Benchmarks for NLP Models",
    "summary" : "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.",
    "updated" : "2024-10-01T03:12:35Z",
    "published" : "2024-09-24T08:41:26Z",
    "authors" : [
      {
        "name" : "Wei Huang"
      },
      {
        "name" : "Yinggui Wang"
      },
      {
        "name" : "Cen Chen"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.08636v2",
    "title" : "Utilizing Data Fingerprints for Privacy-Preserving Algorithm Selection\n  in Time Series Classification: Performance and Uncertainty Estimation on\n  Unseen Datasets",
    "summary" : "The selection of algorithms is a crucial step in designing AI services for\nreal-world time series classification use cases. Traditional methods such as\nneural architecture search, automated machine learning, combined algorithm\nselection, and hyperparameter optimizations are effective but require\nconsiderable computational resources and necessitate access to all data points\nto run their optimizations. In this work, we introduce a novel data fingerprint\nthat describes any time series classification dataset in a privacy-preserving\nmanner and provides insight into the algorithm selection problem without\nrequiring training on the (unseen) dataset. By decomposing the multi-target\nregression problem, only our data fingerprints are used to estimate algorithm\nperformance and uncertainty in a scalable and adaptable manner. Our approach is\nevaluated on the 112 University of California riverside benchmark datasets,\ndemonstrating its effectiveness in predicting the performance of 35\nstate-of-the-art algorithms and providing valuable insights for effective\nalgorithm selection in time series classification service systems, improving a\nnaive baseline by 7.32% on average in estimating the mean performance and\n15.81% in estimating the uncertainty.",
    "updated" : "2024-09-30T21:14:27Z",
    "published" : "2024-09-13T08:43:42Z",
    "authors" : [
      {
        "name" : "Lars Böcking"
      },
      {
        "name" : "Leopold Müller"
      },
      {
        "name" : "Niklas Kühl"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.00069v1",
    "title" : "An interdisciplinary exploration of trade-offs between energy, privacy\n  and accuracy aspects of data",
    "summary" : "The digital era has raised many societal challenges, including ICT's rising\nenergy consumption and protecting privacy of personal data processing. This\npaper considers both aspects in relation to machine learning accuracy in an\ninterdisciplinary exploration. We first present a method to measure the effects\nof privacy-enhancing techniques on data utility and energy consumption. The\nenvironmental-privacy-accuracy trade-offs are discovered through an\nexperimental set-up. We subsequently take a storytelling approach to translate\nthese technical findings to experts in non-ICT fields. We draft two examples\nfor a governmental and auditing setting to contextualise our results.\nUltimately, users face the task of optimising their data processing operations\nin a trade-off between energy, privacy, and accuracy considerations where the\nimpact of their decisions is context-sensitive.",
    "updated" : "2024-09-30T10:01:14Z",
    "published" : "2024-09-30T10:01:14Z",
    "authors" : [
      {
        "name" : "Pepijn de Reus"
      },
      {
        "name" : "Kyra Dresen"
      },
      {
        "name" : "Ana Oprescu"
      },
      {
        "name" : "Kristina Irion"
      },
      {
        "name" : "Ans Kolk"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11295v2",
    "title" : "EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage",
    "summary" : "Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies.",
    "updated" : "2024-10-03T16:30:43Z",
    "published" : "2024-09-17T15:49:44Z",
    "authors" : [
      {
        "name" : "Zeyi Liao"
      },
      {
        "name" : "Lingbo Mo"
      },
      {
        "name" : "Chejian Xu"
      },
      {
        "name" : "Mintong Kang"
      },
      {
        "name" : "Jiawei Zhang"
      },
      {
        "name" : "Chaowei Xiao"
      },
      {
        "name" : "Yuan Tian"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Huan Sun"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.02462v1",
    "title" : "Scalable Differential Privacy Mechanisms for Real-Time Machine Learning\n  Applications",
    "summary" : "Large language models (LLMs) are increasingly integrated into real-time\nmachine learning applications, where safeguarding user privacy is paramount.\nTraditional differential privacy mechanisms often struggle to balance privacy\nand accuracy, particularly in fast-changing environments with continuously\nflowing data. To address these issues, we introduce Scalable Differential\nPrivacy (SDP), a framework tailored for real-time machine learning that\nemphasizes both robust privacy guarantees and enhanced model performance. SDP\nemploys a hierarchical architecture to facilitate efficient noise aggregation\nacross various learning agents. By integrating adaptive noise scheduling and\ngradient compression methods, our approach minimizes performance degradation\nwhile ensuring significant privacy protection. Extensive experiments on diverse\ndatasets reveal that SDP maintains high accuracy levels while applying\ndifferential privacy effectively, showcasing its suitability for deployment in\nsensitive domains. This advancement points towards the potential for widespread\nadoption of privacy-preserving techniques in machine learning workflows.",
    "updated" : "2024-09-16T20:52:04Z",
    "published" : "2024-09-16T20:52:04Z",
    "authors" : [
      {
        "name" : "Jessica Smith"
      },
      {
        "name" : "David Williams"
      },
      {
        "name" : "Emily Brown"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2410.01813v1",
    "title" : "Privacy-Preserving SAM Quantization for Efficient Edge Intelligence in\n  Healthcare",
    "summary" : "The disparity in healthcare personnel expertise and medical resources across\ndifferent regions of the world is a pressing social issue. Artificial\nintelligence technology offers new opportunities to alleviate this issue.\nSegment Anything Model (SAM), which excels in intelligent image segmentation,\nhas demonstrated exceptional performance in medical monitoring and assisted\ndiagnosis. Unfortunately, the huge computational and storage overhead of SAM\nposes significant challenges for deployment on resource-limited edge devices.\nQuantization is an effective solution for model compression; however,\ntraditional methods rely heavily on original data for calibration, which raises\nwidespread concerns about medical data privacy and security. In this paper, we\npropose a data-free quantization framework for SAM, called DFQ-SAM, which\nlearns and calibrates quantization parameters without any original data, thus\neffectively preserving data privacy during model compression. Specifically, we\npropose pseudo-positive label evolution for segmentation, combined with patch\nsimilarity, to fully leverage the semantic and distribution priors in\npre-trained models, which facilitates high-quality data synthesis as a\nsubstitute for real data. Furthermore, we introduce scale reparameterization to\nensure the accuracy of low-bit quantization. We perform extensive segmentation\nexperiments on various datasets, and DFQ-SAM consistently provides significant\nperformance on low-bit quantization. DFQ-SAM eliminates the need for data\ntransfer in cloud-edge collaboration, thereby protecting sensitive data from\npotential attacks. It enables secure, fast, and personalized healthcare\nservices at the edge, which enhances system efficiency and optimizes resource\nallocation, and thus facilitating the pervasive application of artificial\nintelligence in worldwide healthcare.",
    "updated" : "2024-09-14T10:43:35Z",
    "published" : "2024-09-14T10:43:35Z",
    "authors" : [
      {
        "name" : "Zhikai Li"
      },
      {
        "name" : "Jing Zhang"
      },
      {
        "name" : "Qingyi Gu"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.11295v3",
    "title" : "EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage",
    "summary" : "Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies.",
    "updated" : "2024-10-04T02:08:57Z",
    "published" : "2024-09-17T15:49:44Z",
    "authors" : [
      {
        "name" : "Zeyi Liao"
      },
      {
        "name" : "Lingbo Mo"
      },
      {
        "name" : "Chejian Xu"
      },
      {
        "name" : "Mintong Kang"
      },
      {
        "name" : "Jiawei Zhang"
      },
      {
        "name" : "Chaowei Xiao"
      },
      {
        "name" : "Yuan Tian"
      },
      {
        "name" : "Bo Li"
      },
      {
        "name" : "Huan Sun"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  }
]