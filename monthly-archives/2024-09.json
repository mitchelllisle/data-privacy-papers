[
  {
    "id" : "http://arxiv.org/abs/2409.02614v1",
    "title" : "Evaluating the Effects of Digital Privacy Regulations on User Trust",
    "summary" : "In today's digital society, issues related to digital privacy have become\nincreasingly important. Issues such as data breaches result in misuse of data,\nfinancial loss, and cyberbullying, which leads to less user trust in digital\nservices. This research investigates the impact of digital privacy laws on user\ntrust by comparing the regulations in the Netherlands, Ghana, and Malaysia. The\nstudy employs a comparative case study method, involving interviews with\ndigital privacy law experts, IT educators, and consumers from each country. The\nmain findings reveal that while the General Data Protection Regulation (GDPR)\nin the Netherlands is strict, its practical impact is limited by enforcement\nchallenges. In Ghana, the Data Protection Act is underutilized due to low\npublic awareness and insufficient enforcement, leading to reliance on personal\nprotective measures. In Malaysia, trust in digital services is largely\ndependent on the security practices of individual platforms rather than the\nPersonal Data Protection Act. The study highlights the importance of public\nawareness, effective enforcement, and cultural considerations in shaping the\neffectiveness of digital privacy laws. Based on these insights, a\nrecommendation framework is proposed to enhance digital privacy practices, also\naiming to provide valuable guidance for policymakers, businesses, and citizens\nin navigating the challenges of digitalization.",
    "updated" : "2024-09-04T11:11:41Z",
    "published" : "2024-09-04T11:11:41Z",
    "authors" : [
      {
        "name" : "Mehmet Berk Cetin"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02404v1",
    "title" : "Learning Privacy-Preserving Student Networks via\n  Discriminative-Generative Distillation",
    "summary" : "While deep models have proved successful in learning rich knowledge from\nmassive well-annotated data, they may pose a privacy leakage risk in practical\ndeployment. It is necessary to find an effective trade-off between high utility\nand strong privacy. In this work, we propose a discriminative-generative\ndistillation approach to learn privacy-preserving deep models. Our key idea is\ntaking models as bridge to distill knowledge from private data and then\ntransfer it to learn a student network via two streams. First, discriminative\nstream trains a baseline classifier on private data and an ensemble of teachers\non multiple disjoint private subsets, respectively. Then, generative stream\ntakes the classifier as a fixed discriminator and trains a generator in a\ndata-free manner. After that, the generator is used to generate massive\nsynthetic data which are further applied to train a variational autoencoder\n(VAE). Among these synthetic data, a few of them are fed into the teacher\nensemble to query labels via differentially private aggregation, while most of\nthem are embedded to the trained VAE for reconstructing synthetic data.\nFinally, a semi-supervised student learning is performed to simultaneously\nhandle two tasks: knowledge transfer from the teachers with distillation on few\nprivately labeled synthetic data, and knowledge enhancement with tangent-normal\nadversarial regularization on many triples of reconstructed synthetic data. In\nthis way, our approach can control query cost over private data and mitigate\naccuracy degradation in a unified manner, leading to a privacy-preserving\nstudent model. Extensive experiments and analysis clearly show the\neffectiveness of the proposed approach.",
    "updated" : "2024-09-04T03:06:13Z",
    "published" : "2024-09-04T03:06:13Z",
    "authors" : [
      {
        "name" : "Shiming Ge"
      },
      {
        "name" : "Bochao Liu"
      },
      {
        "name" : "Pengju Wang"
      },
      {
        "name" : "Yong Li"
      },
      {
        "name" : "Dan Zeng"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02375v1",
    "title" : "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review",
    "summary" : "The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights.",
    "updated" : "2024-09-04T01:51:37Z",
    "published" : "2024-09-04T01:51:37Z",
    "authors" : [
      {
        "name" : "Xichou Zhu"
      },
      {
        "name" : "Yang Liu"
      },
      {
        "name" : "Zhou Shen"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Min Li"
      },
      {
        "name" : "Yujun Chen"
      },
      {
        "name" : "Benzi John"
      },
      {
        "name" : "Zhenzhen Ma"
      },
      {
        "name" : "Tao Hu"
      },
      {
        "name" : "Bolong Yang"
      },
      {
        "name" : "Manman Wang"
      },
      {
        "name" : "Zongxing Xie"
      },
      {
        "name" : "Peng Liu"
      },
      {
        "name" : "Dan Cai"
      },
      {
        "name" : "Junhui Wang"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02364v1",
    "title" : "Examining Caregiving Roles to Differentiate the Effects of Using a\n  Mobile App for Community Oversight for Privacy and Security",
    "summary" : "We conducted a 4-week field study with 101 smartphone users who\nself-organized into 22 small groups of family, friends, and neighbors to use\n``CO-oPS,'' a mobile app for co-managing mobile privacy and security. We\ndifferentiated between those who provided oversight (i.e., caregivers) and\nthose who did not (i.e., caregivees) to examine differential effects on their\nexperiences and behaviors while using CO-oPS. Caregivers reported higher power\nuse, community trust, belonging, collective efficacy, and self-efficacy than\ncaregivees. Both groups' self-efficacy and collective efficacy for mobile\nprivacy and security increased after using CO-oPS. However, this increase was\nsignificantly stronger for caregivees. Our research demonstrates how\ncommunity-based approaches can benefit people who need additional help managing\ntheir digital privacy and security. We provide recommendations to support\ncommunity-based oversight for managing privacy and security within communities\nof different roles and skills.",
    "updated" : "2024-09-04T01:21:56Z",
    "published" : "2024-09-04T01:21:56Z",
    "authors" : [
      {
        "name" : "Mamtaj Akter"
      },
      {
        "name" : "Jess Kropczynski"
      },
      {
        "name" : "Heather Lipford"
      },
      {
        "name" : "Pamela Wisniewski"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02044v1",
    "title" : "FedMinds: Privacy-Preserving Personalized Brain Visual Decoding",
    "summary" : "Exploring the mysteries of the human brain is a long-term research topic in\nneuroscience. With the help of deep learning, decoding visual information from\nhuman brain activity fMRI has achieved promising performance. However, these\ndecoding models require centralized storage of fMRI data to conduct training,\nleading to potential privacy security issues. In this paper, we focus on\nprivacy preservation in multi-individual brain visual decoding. To this end, we\nintroduce a novel framework called FedMinds, which utilizes federated learning\nto protect individuals' privacy during model training. In addition, we deploy\nindividual adapters for each subject, thus allowing personalized visual\ndecoding. We conduct experiments on the authoritative NSD datasets to evaluate\nthe performance of the proposed framework. The results demonstrate that our\nframework achieves high-precision visual decoding along with privacy\nprotection.",
    "updated" : "2024-09-03T16:46:29Z",
    "published" : "2024-09-03T16:46:29Z",
    "authors" : [
      {
        "name" : "Guangyin Bao"
      },
      {
        "name" : "Duoqian Miao"
      }
    ],
    "categories" : [
      "q-bio.NC",
      "cs.CV",
      "cs.DC",
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01924v1",
    "title" : "Privacy-Preserving and Post-Quantum Counter Denial of Service Framework\n  for Wireless Networks",
    "summary" : "As network services progress and mobile and IoT environments expand, numerous\nsecurity concerns have surfaced for spectrum access systems. The omnipresent\nrisk of Denial-of-Service (DoS) attacks and raising concerns about user privacy\n(e.g., location privacy, anonymity) are among such cyber threats. These\nsecurity and privacy risks increase due to the threat of quantum computers that\ncan compromise long-term security by circumventing conventional cryptosystems\nand increasing the cost of countermeasures. While some defense mechanisms exist\nagainst these threats in isolation, there is a significant gap in the state of\nthe art on a holistic solution against DoS attacks with privacy and anonymity\nfor spectrum management systems, especially when post-quantum (PQ) security is\nin mind. In this paper, we propose a new cybersecurity framework PACDoSQ, which\nis (to the best of our knowledge) the first to offer location privacy and\nanonymity for spectrum management with counter DoS and PQ security\nsimultaneously. Our solution introduces the private spectrum bastion (database)\nconcept to exploit existing architectural features of spectrum management\nsystems and then synergizes them with multi-server private information\nretrieval and PQ-secure Tor to guarantee a location-private and anonymous\nacquisition of spectrum information together with hash-based client-server\npuzzles for counter DoS. We prove that PACDoSQ achieves its security\nobjectives, and show its feasibility via a comprehensive performance\nevaluation.",
    "updated" : "2024-09-03T14:14:41Z",
    "published" : "2024-09-03T14:14:41Z",
    "authors" : [
      {
        "name" : "Saleh Darzi"
      },
      {
        "name" : "Attila Altay Yavuz"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01710v1",
    "title" : "Privacy-Preserving Multimedia Mobile Cloud Computing Using Protective\n  Perturbation",
    "summary" : "Mobile cloud computing has been adopted in many multimedia applications,\nwhere the resource-constrained mobile device sends multimedia data (e.g.,\nimages) to remote cloud servers to request computation-intensive multimedia\nservices (e.g., image recognition). While significantly improving the\nperformance of the mobile applications, the cloud-based mechanism often causes\nprivacy concerns as the multimedia data and services are offloaded from the\ntrusted user device to untrusted cloud servers. Several recent studies have\nproposed perturbation-based privacy preserving mechanisms, which obfuscate the\noffloaded multimedia data to eliminate privacy exposures without affecting the\nfunctionality of the remote multimedia services. However, the existing privacy\nprotection approaches require the deployment of computation-intensive\nperturbation generation on the resource-constrained mobile devices. Also, the\nobfuscated images are typically not compliant with the standard image\ncompression algorithms and suffer from significant bandwidth consumption. In\nthis paper, we develop a novel privacy-preserving multimedia mobile cloud\ncomputing framework, namely $PMC^2$, to address the resource and bandwidth\nchallenges. $PMC^2$ employs secure confidential computing in the cloud to\ndeploy the perturbation generator, which addresses the resource challenge while\nmaintaining the privacy. Furthermore, we develop a neural compressor\nspecifically trained to compress the perturbed images in order to address the\nbandwidth challenge. We implement $PMC^2$ in an end-to-end mobile cloud\ncomputing system, based on which our evaluations demonstrate superior latency,\npower efficiency, and bandwidth consumption achieved by $PMC^2$ while\nmaintaining high accuracy in the target multimedia service.",
    "updated" : "2024-09-03T08:47:17Z",
    "published" : "2024-09-03T08:47:17Z",
    "authors" : [
      {
        "name" : "Zhongze Tang"
      },
      {
        "name" : "Mengmei Ye"
      },
      {
        "name" : "Yao Liu"
      },
      {
        "name" : "Sheng Wei"
      }
    ],
    "categories" : [
      "cs.MM"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01661v1",
    "title" : "$S^2$NeRF: Privacy-preserving Training Framework for NeRF",
    "summary" : "Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and\ngraphics, facilitating novel view synthesis and influencing sectors like\nextended reality and e-commerce. However, NeRF's dependence on extensive data\ncollection, including sensitive scene image data, introduces significant\nprivacy risks when users upload this data for model training. To address this\nconcern, we first propose SplitNeRF, a training framework that incorporates\nsplit learning (SL) techniques to enable privacy-preserving collaborative model\ntraining between clients and servers without sharing local data. Despite its\nbenefits, we identify vulnerabilities in SplitNeRF by developing two attack\nmethods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which\nexploit the shared gradient data and a few leaked scene images to reconstruct\nprivate scene information. To counter these threats, we introduce $S^2$NeRF,\nsecure SplitNeRF that integrates effective defense mechanisms. By introducing\ndecaying noise related to the gradient norm into the shared gradient\ninformation, $S^2$NeRF preserves privacy while maintaining a high utility of\nthe NeRF model. Our extensive evaluations across multiple datasets demonstrate\nthe effectiveness of $S^2$NeRF against privacy breaches, confirming its\nviability for secure NeRF training in sensitive applications.",
    "updated" : "2024-09-03T07:08:30Z",
    "published" : "2024-09-03T07:08:30Z",
    "authors" : [
      {
        "name" : "Bokang Zhang"
      },
      {
        "name" : "Yanglin Zhang"
      },
      {
        "name" : "Zhikun Zhang"
      },
      {
        "name" : "Jinglan Yang"
      },
      {
        "name" : "Lingying Huang"
      },
      {
        "name" : "Junfeng Wu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01329v1",
    "title" : "Assessing the Impact of Image Dataset Features on Privacy-Preserving\n  Machine Learning",
    "summary" : "Machine Learning (ML) is crucial in many sectors, including computer vision.\nHowever, ML models trained on sensitive data face security challenges, as they\ncan be attacked and leak information. Privacy-Preserving Machine Learning\n(PPML) addresses this by using Differential Privacy (DP) to balance utility and\nprivacy. This study identifies image dataset characteristics that affect the\nutility and vulnerability of private and non-private Convolutional Neural\nNetwork (CNN) models. Through analyzing multiple datasets and privacy budgets,\nwe find that imbalanced datasets increase vulnerability in minority classes,\nbut DP mitigates this issue. Datasets with fewer classes improve both model\nutility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR)\ndatasets deteriorate the utility-privacy trade-off. These insights offer\nvaluable guidance for practitioners and researchers in estimating and\noptimizing the utility-privacy trade-off in image datasets, helping to inform\ndata and privacy modifications for better outcomes based on dataset\ncharacteristics.",
    "updated" : "2024-09-02T15:30:27Z",
    "published" : "2024-09-02T15:30:27Z",
    "authors" : [
      {
        "name" : "Lucas Lange"
      },
      {
        "name" : "Maurice-Maximilian Heykeroth"
      },
      {
        "name" : "Erhard Rahm"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.01088v1",
    "title" : "Towards Split Learning-based Privacy-Preserving Record Linkage",
    "summary" : "Split Learning has been recently introduced to facilitate applications where\nuser data privacy is a requirement. However, it has not been thoroughly studied\nin the context of Privacy-Preserving Record Linkage, a problem in which the\nsame real-world entity should be identified among databases from different\ndataholders, but without disclosing any additional information. In this paper,\nwe investigate the potentials of Split Learning for Privacy-Preserving Record\nMatching, by introducing a novel training method through the utilization of\nReference Sets, which are publicly available data corpora, showcasing minimal\nmatching impact against a traditional centralized SVM-based technique.",
    "updated" : "2024-09-02T09:17:05Z",
    "published" : "2024-09-02T09:17:05Z",
    "authors" : [
      {
        "name" : "Michail Zervas"
      },
      {
        "name" : "Alexandros Karakasidis"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.00974v1",
    "title" : "Enhancing Privacy in Federated Learning: Secure Aggregation for\n  Real-World Healthcare Applications",
    "summary" : "Deploying federated learning (FL) in real-world scenarios, particularly in\nhealthcare, poses challenges in communication and security. In particular, with\nrespect to the federated aggregation procedure, researchers have been focusing\non the study of secure aggregation (SA) schemes to provide privacy guarantees\nover the model's parameters transmitted by the clients. Nevertheless, the\npractical availability of SA in currently available FL frameworks is currently\nlimited, due to computational and communication bottlenecks. To fill this gap,\nthis study explores the implementation of SA within the open-source Fed-BioMed\nframework. We implement and compare two SA protocols, Joye-Libert (JL) and Low\nOverhead Masking (LOM), by providing extensive benchmarks in a panel of\nhealthcare data analysis problems. Our theoretical and experimental evaluations\non four datasets demonstrate that SA protocols effectively protect privacy\nwhile maintaining task accuracy. Computational overhead during training is less\nthan 1% on a CPU and less than 50% on a GPU for large models, with protection\nphases taking less than 10 seconds. Incorporating SA into Fed-BioMed impacts\ntask accuracy by no more than 2% compared to non-SA scenarios. Overall this\nstudy demonstrates the feasibility of SA in real-world healthcare applications\nand contributes in reducing the gap towards the adoption of privacy-preserving\ntechnologies in sensitive applications.",
    "updated" : "2024-09-02T06:43:22Z",
    "published" : "2024-09-02T06:43:22Z",
    "authors" : [
      {
        "name" : "Riccardo Taiello"
      },
      {
        "name" : "Sergen Cansiz"
      },
      {
        "name" : "Marc Vesin"
      },
      {
        "name" : "Francesco Cremonesi"
      },
      {
        "name" : "Lucia Innocenti"
      },
      {
        "name" : "Melek Önen"
      },
      {
        "name" : "Marco Lorenzi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.00740v1",
    "title" : "VPVet: Vetting Privacy Policies of Virtual Reality Apps",
    "summary" : "Virtual reality (VR) apps can harvest a wider range of user data than\nweb/mobile apps running on personal computers or smartphones. Existing law and\nprivacy regulations emphasize that VR developers should inform users of what\ndata are collected/used/shared (CUS) through privacy policies. However, privacy\npolicies in the VR ecosystem are still in their early stages, and many\ndevelopers fail to write appropriate privacy policies that comply with\nregulations and meet user expectations. In this paper, we propose VPVet to\nautomatically vet privacy policy compliance issues for VR apps. VPVet first\nanalyzes the availability and completeness of a VR privacy policy and then\nrefines its analysis based on three key criteria: granularity, minimization,\nand consistency of CUS statements. Our study establishes the first and\ncurrently largest VR privacy policy dataset named VRPP, consisting of privacy\npolicies of 11,923 different VR apps from 10 mainstream platforms. Our vetting\nresults reveal severe privacy issues within the VR ecosystem, including the\nlimited availability and poor quality of privacy policies, along with their\ncoarse granularity, lack of adaptation to VR traits and the inconsistency\nbetween CUS statements in privacy policies and their actual behaviors. We\nopen-source VPVet system along with our findings at repository\nhttps://github.com/kalamoo/PPAudit, aiming to raise awareness within the VR\ncommunity and pave the way for further research in this field.",
    "updated" : "2024-09-01T15:07:11Z",
    "published" : "2024-09-01T15:07:11Z",
    "authors" : [
      {
        "name" : "Yuxia Zhan"
      },
      {
        "name" : "Yan Meng"
      },
      {
        "name" : "Lu Zhou"
      },
      {
        "name" : "Yichang Xiong"
      },
      {
        "name" : "Xiaokuan Zhang"
      },
      {
        "name" : "Lichuan Ma"
      },
      {
        "name" : "Guoxing Chen"
      },
      {
        "name" : "Qingqi Pei"
      },
      {
        "name" : "Haojin Zhu"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.00739v1",
    "title" : "Designing and Evaluating Scalable Privacy Awareness and Control User\n  Interfaces for Mixed Reality",
    "summary" : "As Mixed Reality (MR) devices become increasingly popular across industries,\nthey raise significant privacy and ethical concerns due to their capacity to\ncollect extensive data on users and their environments. This paper highlights\nthe urgent need for privacy-aware user interfaces that educate and empower both\nusers and bystanders, enabling them to understand, control, and manage data\ncollection and sharing. Key research questions include improving user awareness\nof privacy implications, developing usable privacy controls, and evaluating the\neffectiveness of these measures in real-world settings. The proposed research\nroadmap aims to embed privacy considerations into the design and development of\nMR technologies, promoting responsible innovation that safeguards user privacy\nwhile preserving the functionality and appeal of these emerging technologies.",
    "updated" : "2024-09-01T15:06:40Z",
    "published" : "2024-09-01T15:06:40Z",
    "authors" : [
      {
        "name" : "Marvin Strauss"
      },
      {
        "name" : "Viktorija Paneva"
      },
      {
        "name" : "Florian Alt"
      },
      {
        "name" : "Stefan Schneegass"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.02715v1",
    "title" : "Recoverable Anonymization for Pose Estimation: A Privacy-Enhancing\n  Approach",
    "summary" : "Human pose estimation (HPE) is crucial for various applications. However,\ndeploying HPE algorithms in surveillance contexts raises significant privacy\nconcerns due to the potential leakage of sensitive personal information (SPI)\nsuch as facial features, and ethnicity. Existing privacy-enhancing methods\noften compromise either privacy or performance, or they require costly\nadditional modalities. We propose a novel privacy-enhancing system that\ngenerates privacy-enhanced portraits while maintaining high HPE performance.\nOur key innovations include the reversible recovery of SPI for authorized\npersonnel and the preservation of contextual information. By jointly optimizing\na privacy-enhancing module, a privacy recovery module, and a pose estimator,\nour system ensures robust privacy protection, efficient SPI recovery, and\nhigh-performance HPE. Experimental results demonstrate the system's robust\nperformance in privacy enhancement, SPI recovery, and HPE.",
    "updated" : "2024-09-01T05:58:00Z",
    "published" : "2024-09-01T05:58:00Z",
    "authors" : [
      {
        "name" : "Wenjun Huang"
      },
      {
        "name" : "Yang Ni"
      },
      {
        "name" : "Arghavan Rezvani"
      },
      {
        "name" : "SungHeon Jeong"
      },
      {
        "name" : "Hanning Chen"
      },
      {
        "name" : "Yezi Liu"
      },
      {
        "name" : "Fei Wen"
      },
      {
        "name" : "Mohsen Imani"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03707v1",
    "title" : "A Different Level Text Protection Mechanism With Differential Privacy",
    "summary" : "The article introduces a method for extracting words of different degrees of\nimportance based on the BERT pre-training model and proves the effectiveness of\nthis method. The article also discusses the impact of maintaining the same\nperturbation results for words of different importance on the overall text\nutility. This method can be applied to long text protection.",
    "updated" : "2024-09-05T17:13:38Z",
    "published" : "2024-09-05T17:13:38Z",
    "authors" : [
      {
        "name" : "Qingwen Fu"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03655v1",
    "title" : "Privacy versus Emotion Preservation Trade-offs in Emotion-Preserving\n  Speaker Anonymization",
    "summary" : "Advances in speech technology now allow unprecedented access to personally\nidentifiable information through speech. To protect such information, the\ndifferential privacy field has explored ways to anonymize speech while\npreserving its utility, including linguistic and paralinguistic aspects.\nHowever, anonymizing speech while maintaining emotional state remains\nchallenging. We explore this problem in the context of the VoicePrivacy 2024\nchallenge. Specifically, we developed various speaker anonymization pipelines\nand find that approaches either excel at anonymization or preserving emotion\nstate, but not both simultaneously. Achieving both would require an in-domain\nemotion recognizer. Additionally, we found that it is feasible to train a\nsemi-effective speaker verification system using only emotion representations,\ndemonstrating the challenge of separating these two modalities.",
    "updated" : "2024-09-05T16:10:31Z",
    "published" : "2024-09-05T16:10:31Z",
    "authors" : [
      {
        "name" : "Zexin Cai"
      },
      {
        "name" : "Henry Li Xinyuan"
      },
      {
        "name" : "Ashi Garg"
      },
      {
        "name" : "Leibny Paola García-Perera"
      },
      {
        "name" : "Kevin Duh"
      },
      {
        "name" : "Sanjeev Khudanpur"
      },
      {
        "name" : "Nicholas Andrews"
      },
      {
        "name" : "Matthew Wiesner"
      }
    ],
    "categories" : [
      "eess.AS",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03568v1",
    "title" : "Enabling Practical and Privacy-Preserving Image Processing",
    "summary" : "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
    "updated" : "2024-09-05T14:22:02Z",
    "published" : "2024-09-05T14:22:02Z",
    "authors" : [
      {
        "name" : "Chao Wang"
      },
      {
        "name" : "Shubing Yang"
      },
      {
        "name" : "Xiaoyan Sun"
      },
      {
        "name" : "Jun Dai"
      },
      {
        "name" : "Dongfang Zhao"
      }
    ],
    "categories" : [
      "cs.CR",
      "C.2.0; K.6.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03344v1",
    "title" : "Rethinking Improved Privacy-Utility Trade-off with Pre-existing\n  Knowledge for DP Training",
    "summary" : "Differential privacy (DP) provides a provable framework for protecting\nindividuals by customizing a random mechanism over a privacy-sensitive dataset.\nDeep learning models have demonstrated privacy risks in model exposure as an\nestablished learning model unintentionally records membership-level privacy\nleakage. Differentially private stochastic gradient descent (DP- SGD) has been\nproposed to safeguard training individuals by adding random Gaussian noise to\ngradient updates in the backpropagation. Researchers identify that DP-SGD\ntypically causes utility loss since the injected homogeneous noise alters the\ngradient updates calculated at each iteration. Namely, all elements in the\ngradient are contaminated regardless of their importance in updating model\nparameters. In this work, we argue that the utility loss mainly results from\nthe homogeneity of injected noise. Consequently, we propose a generic\ndifferential privacy framework with heterogeneous noise (DP-Hero) by defining a\nheterogeneous random mechanism to abstract its property. The insight of DP-Hero\nis to leverage the knowledge encoded in the previously trained model to guide\nthe subsequent allocation of noise heterogeneity, thereby leveraging the\nstatistical perturbation and achieving enhanced utility. Atop DP-Hero, we\ninstantiate a heterogeneous version of DP-SGD, where the noise injected into\ngradients is heterogeneous and guided by prior-established model parameters. We\nconduct comprehensive experiments to verify and explain the effectiveness of\nthe proposed DP-Hero, showing improved training accuracy compared with\nstate-of-the-art works. Broadly, we shed light on improving the privacy-utility\nspace by learning the noise guidance from the pre-existing leaked knowledge\nencoded in the previously trained model, showing a different perspective of\nunderstanding the utility-improved DP training.",
    "updated" : "2024-09-05T08:40:54Z",
    "published" : "2024-09-05T08:40:54Z",
    "authors" : [
      {
        "name" : "Yu Zheng"
      },
      {
        "name" : "Wenchao Zhang"
      },
      {
        "name" : "Yonggang Zhang"
      },
      {
        "name" : "Wei Song"
      },
      {
        "name" : "Kai Zhou"
      },
      {
        "name" : "Bo Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03326v1",
    "title" : "Enhancing User-Centric Privacy Protection: An Interactive Framework\n  through Diffusion Models and Machine Unlearning",
    "summary" : "In the realm of multimedia data analysis, the extensive use of image datasets\nhas escalated concerns over privacy protection within such data. Current\nresearch predominantly focuses on privacy protection either in data sharing or\nupon the release of trained machine learning models. Our study pioneers a\ncomprehensive privacy protection framework that safeguards image data privacy\nconcurrently during data sharing and model publication. We propose an\ninteractive image privacy protection framework that utilizes generative machine\nlearning models to modify image information at the attribute level and employs\nmachine unlearning algorithms for the privacy preservation of model parameters.\nThis user-interactive framework allows for adjustments in privacy protection\nintensity based on user feedback on generated images, striking a balance\nbetween maximal privacy safeguarding and maintaining model performance. Within\nthis framework, we instantiate two modules: a differential privacy diffusion\nmodel for protecting attribute information in images and a feature unlearning\nalgorithm for efficient updates of the trained model on the revised image\ndataset. Our approach demonstrated superiority over existing methods on facial\ndatasets across various attribute classifications.",
    "updated" : "2024-09-05T07:55:55Z",
    "published" : "2024-09-05T07:55:55Z",
    "authors" : [
      {
        "name" : "Huaxi Huang"
      },
      {
        "name" : "Xin Yuan"
      },
      {
        "name" : "Qiyu Liao"
      },
      {
        "name" : "Dadong Wang"
      },
      {
        "name" : "Tongliang Liu"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03294v1",
    "title" : "Federated Prototype-based Contrastive Learning for Privacy-Preserving\n  Cross-domain Recommendation",
    "summary" : "Cross-domain recommendation (CDR) aims to improve recommendation accuracy in\nsparse domains by transferring knowledge from data-rich domains. However,\nexisting CDR methods often assume the availability of user-item interaction\ndata across domains, overlooking user privacy concerns. Furthermore, these\nmethods suffer from performance degradation in scenarios with sparse\noverlapping users, as they typically depend on a large number of fully shared\nusers for effective knowledge transfer. To address these challenges, we propose\na Federated Prototype-based Contrastive Learning (CL) method for\nPrivacy-Preserving CDR, named FedPCL-CDR. This approach utilizes\nnon-overlapping user information and prototypes to improve multi-domain\nperformance while protecting user privacy. FedPCL-CDR comprises two modules:\nlocal domain (client) learning and global server aggregation. In the local\ndomain, FedPCL-CDR clusters all user data to learn representative prototypes,\neffectively utilizing non-overlapping user information and addressing the\nsparse overlapping user issue. It then facilitates knowledge transfer by\nemploying both local and global prototypes returned from the server in a CL\nmanner. Simultaneously, the global server aggregates representative prototypes\nfrom local domains to learn both local and global prototypes. The combination\nof prototypes and federated learning (FL) ensures that sensitive user data\nremains decentralized, with only prototypes being shared across domains,\nthereby protecting user privacy. Extensive experiments on four CDR tasks using\ntwo real-world datasets demonstrate that FedPCL-CDR outperforms the\nstate-of-the-art baselines.",
    "updated" : "2024-09-05T06:59:56Z",
    "published" : "2024-09-05T06:59:56Z",
    "authors" : [
      {
        "name" : "Li Wang"
      },
      {
        "name" : "Quangui Zhang"
      },
      {
        "name" : "Lei Sang"
      },
      {
        "name" : "Qiang Wu"
      },
      {
        "name" : "Min Xu"
      }
    ],
    "categories" : [
      "cs.IR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04366v1",
    "title" : "Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue",
    "summary" : "Many blockchain networks aim to preserve the anonymity of validators in the\npeer-to-peer (P2P) network, ensuring that no adversary can link a validator's\nidentifier to the IP address of a peer due to associated privacy and security\nconcerns. This work demonstrates that the Ethereum P2P network does not offer\nthis anonymity. We present a methodology that enables any node in the network\nto identify validators hosted on connected peers and empirically verify the\nfeasibility of our proposed method. Using data collected from four nodes over\nthree days, we locate more than 15% of Ethereum validators in the P2P network.\nThe insights gained from our deanonymization technique provide valuable\ninformation on the distribution of validators across peers, their geographic\nlocations, and hosting organizations. We further discuss the implications and\nrisks associated with the lack of anonymity in the P2P network and propose\nmethods to help validators protect their privacy. The Ethereum Foundation has\nawarded us a bug bounty, acknowledging the impact of our results.",
    "updated" : "2024-09-06T15:57:43Z",
    "published" : "2024-09-06T15:57:43Z",
    "authors" : [
      {
        "name" : "Lioba Heimbach"
      },
      {
        "name" : "Yann Vonlanthen"
      },
      {
        "name" : "Juan Villacis"
      },
      {
        "name" : "Lucianna Kiffer"
      },
      {
        "name" : "Roger Wattenhofer"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04257v1",
    "title" : "Privacy risk from synthetic data: practical proposals",
    "summary" : "This paper proposes and compares measures of identity and attribute\ndisclosure risk for synthetic data. Data custodians can use the methods\nproposed here to inform the decision as to whether to release synthetic\nversions of confidential data. Different measures are evaluated on two data\nsets. Insight into the measures is obtained by examining the details of the\nrecords identified as posing a disclosure risk. This leads to methods to\nidentify, and possibly exclude, apparently risky records where the\nidentification or attribution would be expected by someone with background\nknowledge of the data. The methods described are available as part of the\n\\textbf{synthpop} package for \\textbf{R}.",
    "updated" : "2024-09-06T13:10:40Z",
    "published" : "2024-09-06T13:10:40Z",
    "authors" : [
      {
        "name" : "Gillian M Raab"
      }
    ],
    "categories" : [
      "stat.AP"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04194v1",
    "title" : "Towards Privacy-Preserving Relational Data Synthesis via Probabilistic\n  Relational Models",
    "summary" : "Probabilistic relational models provide a well-established formalism to\ncombine first-order logic and probabilistic models, thereby allowing to\nrepresent relationships between objects in a relational domain. At the same\ntime, the field of artificial intelligence requires increasingly large amounts\nof relational training data for various machine learning tasks. Collecting\nreal-world data, however, is often challenging due to privacy concerns, data\nprotection regulations, high costs, and so on. To mitigate these challenges,\nthe generation of synthetic data is a promising approach. In this paper, we\nsolve the problem of generating synthetic relational data via probabilistic\nrelational models. In particular, we propose a fully-fledged pipeline to go\nfrom relational database to probabilistic relational model, which can then be\nused to sample new synthetic relational data points from its underlying\nprobability distribution. As part of our proposed pipeline, we introduce a\nlearning algorithm to construct a probabilistic relational model from a given\nrelational database.",
    "updated" : "2024-09-06T11:24:25Z",
    "published" : "2024-09-06T11:24:25Z",
    "authors" : [
      {
        "name" : "Malte Luttermann"
      },
      {
        "name" : "Ralf Möller"
      },
      {
        "name" : "Mattis Hartwig"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04173v1",
    "title" : "NPU-NTU System for Voice Privacy 2024 Challenge",
    "summary" : "Speaker anonymization is an effective privacy protection solution that\nconceals the speaker's identity while preserving the linguistic content and\nparalinguistic information of the original speech. To establish a fair\nbenchmark and facilitate comparison of speaker anonymization systems, the\nVoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition\nplanned for 2024. In this paper, we describe our proposed speaker anonymization\nsystem for VPC 2024. Our system employs a disentangled neural codec\narchitecture and a serial disentanglement strategy to gradually disentangle the\nglobal speaker identity and time-variant linguistic content and paralinguistic\ninformation. We introduce multiple distillation methods to disentangle\nlinguistic content, speaker identity, and emotion. These methods include\nsemantic distillation, supervised speaker distillation, and frame-level emotion\ndistillation. Based on these distillations, we anonymize the original speaker\nidentity using a weighted sum of a set of candidate speaker identities and a\nrandomly generated speaker identity. Our system achieves the best trade-off of\nprivacy protection and emotion preservation in VPC 2024.",
    "updated" : "2024-09-06T10:32:42Z",
    "published" : "2024-09-06T10:32:42Z",
    "authors" : [
      {
        "name" : "Jixun Yao"
      },
      {
        "name" : "Nikita Kuzmin"
      },
      {
        "name" : "Qing Wang"
      },
      {
        "name" : "Pengcheng Guo"
      },
      {
        "name" : "Ziqian Ning"
      },
      {
        "name" : "Dake Guo"
      },
      {
        "name" : "Kong Aik Lee"
      },
      {
        "name" : "Eng-Siong Chng"
      },
      {
        "name" : "Lei Xie"
      }
    ],
    "categories" : [
      "eess.AS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04167v1",
    "title" : "Do Android App Developers Accurately Report Collection of\n  Privacy-Related Data?",
    "summary" : "Many Android applications collect data from users. The European Union's\nGeneral Data Protection Regulation (GDPR) requires vendors to faithfully\ndisclose which data their apps collect. This task is complicated because many\napps use third-party code for which the same information is not readily\navailable. Hence we ask: how accurately do current Android apps fulfill these\nrequirements?\n  In this work, we first expose a multi-layered definition of privacy-related\ndata to correctly report data collection in Android apps. We further create a\ndataset of privacy-sensitive data classes that may be used as input by an\nAndroid app. This dataset takes into account data collected both through the\nuser interface and system APIs.\n  We manually examine the data safety sections of 70 Android apps to observe\nhow data collection is reported, identifying instances of over- and\nunder-reporting. Additionally, we develop a prototype to statically extract and\nlabel privacy-related data collected via app source code, user interfaces, and\npermissions. Comparing the prototype's results with the data safety sections of\n20 apps reveals reporting discrepancies. Using the results from two Messaging\nand Social Media apps (Signal and Instagram), we discuss how app developers\nunder-report and over-report data collection, respectively, and identify\ninaccurately reported data categories.\n  Our results show that app developers struggle to accurately report data\ncollection, either due to Google's abstract definition of collected data or\ninsufficient existing tool support.",
    "updated" : "2024-09-06T10:05:45Z",
    "published" : "2024-09-06T10:05:45Z",
    "authors" : [
      {
        "name" : "Mugdha Khedkar"
      },
      {
        "name" : "Ambuj Kumar Mondal"
      },
      {
        "name" : "Eric Bodden"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04048v1",
    "title" : "Exploring User Privacy Awareness on GitHub: An Empirical Study",
    "summary" : "GitHub provides developers with a practical way to distribute source code and\ncollaboratively work on common projects. To enhance account security and\nprivacy, GitHub allows its users to manage access permissions, review audit\nlogs, and enable two-factor authentication. However, despite the endless\neffort, the platform still faces various issues related to the privacy of its\nusers. This paper presents an empirical study delving into the GitHub\necosystem. Our focus is on investigating the utilization of privacy settings on\nthe platform and identifying various types of sensitive information disclosed\nby users. Leveraging a dataset comprising 6,132 developers, we report and\nanalyze their activities by means of comments on pull requests. Our findings\nindicate an active engagement by users with the available privacy settings on\nGitHub. Notably, we observe the disclosure of different forms of private\ninformation within pull request comments. This observation has prompted our\nexploration into sensitivity detection using a large language model and BERT,\nto pave the way for a personalized privacy assistant. Our work provides\ninsights into the utilization of existing privacy protection tools, such as\nprivacy settings, along with their inherent limitations. Essentially, we aim to\nadvance research in this field by providing both the motivation for creating\nsuch privacy protection tools and a proposed methodology for personalizing\nthem.",
    "updated" : "2024-09-06T06:41:46Z",
    "published" : "2024-09-06T06:41:46Z",
    "authors" : [
      {
        "name" : "Costanza Alfieri"
      },
      {
        "name" : "Juri Di Rocco"
      },
      {
        "name" : "Phuong T. Nguyen"
      },
      {
        "name" : "Paola Inverardi"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04026v1",
    "title" : "Efficient Fault-Tolerant Quantum Protocol for Differential Privacy in\n  the Shuffle Model",
    "summary" : "We present a quantum protocol which securely and implicitly implements a\nrandom shuffle to realize differential privacy in the shuffle model. The\nshuffle model of differential privacy amplifies privacy achievable via local\ndifferential privacy by randomly permuting the tuple of outcomes from data\ncontributors. In practice, one needs to address how this shuffle is\nimplemented. Examples include implementing the shuffle via mix-networks, or\nshuffling via a trusted third-party. These implementation specific issues raise\nnon-trivial computational and trust requirements in a classical system. We\npropose a quantum version of the protocol using entanglement of quantum states\nand show that the shuffle can be implemented without these extra requirements.\nOur protocol implements k-ary randomized response, for any value of k > 2, and\nfurthermore, can be efficiently implemented using fault-tolerant computation.",
    "updated" : "2024-09-06T04:53:19Z",
    "published" : "2024-09-06T04:53:19Z",
    "authors" : [
      {
        "name" : "Hassan Jameel Asghar"
      },
      {
        "name" : "Arghya Mukherjee"
      },
      {
        "name" : "Gavin K. Brennen"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.03796v1",
    "title" : "Protecting Activity Sensing Data Privacy Using Hierarchical Information\n  Dissociation",
    "summary" : "Smartphones and wearable devices have been integrated into our daily lives,\noffering personalized services. However, many apps become overprivileged as\ntheir collected sensing data contains unnecessary sensitive information. For\nexample, mobile sensing data could reveal private attributes (e.g., gender and\nage) and unintended sensitive features (e.g., hand gestures when entering\npasswords). To prevent sensitive information leakage, existing methods must\nobtain private labels and users need to specify privacy policies. However, they\nonly achieve limited control over information disclosure. In this work, we\npresent Hippo to dissociate hierarchical information including private metadata\nand multi-grained activity information from the sensing data. Hippo achieves\nfine-grained control over the disclosure of sensitive information without\nrequiring private labels. Specifically, we design a latent guidance-based\ndiffusion model, which generates multi-grained versions of raw sensor data\nconditioned on hierarchical latent activity features. Hippo enables users to\ncontrol the disclosure of sensitive information in sensing data, ensuring their\nprivacy while preserving the necessary features to meet the utility\nrequirements of applications. Hippo is the first unified model that achieves\ntwo goals: perturbing the sensitive attributes and controlling the disclosure\nof sensitive information in mobile sensing data. Extensive experiments show\nthat Hippo can anonymize personal attributes and transform activity information\nat various resolutions across different types of sensing data.",
    "updated" : "2024-09-04T15:38:00Z",
    "published" : "2024-09-04T15:38:00Z",
    "authors" : [
      {
        "name" : "Guangjing Wang"
      },
      {
        "name" : "Hanqing Guo"
      },
      {
        "name" : "Yuanda Wang"
      },
      {
        "name" : "Bocheng Chen"
      },
      {
        "name" : "Ce Zhou"
      },
      {
        "name" : "Qiben Yan"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.05623v1",
    "title" : "A Framework for Differential Privacy Against Timing Attacks",
    "summary" : "The standard definition of differential privacy (DP) ensures that a\nmechanism's output distribution on adjacent datasets is indistinguishable.\nHowever, real-world implementations of DP can, and often do, reveal information\nthrough their runtime distributions, making them susceptible to timing attacks.\nIn this work, we establish a general framework for ensuring differential\nprivacy in the presence of timing side channels. We define a new notion of\ntiming privacy, which captures programs that remain differentially private to\nan adversary that observes the program's runtime in addition to the output. Our\nframework enables chaining together component programs that are timing-stable\nfollowed by a random delay to obtain DP programs that achieve timing privacy.\nImportantly, our definitions allow for measuring timing privacy and output\nprivacy using different privacy measures. We illustrate how to instantiate our\nframework by giving programs for standard DP computations in the RAM and Word\nRAM models of computation. Furthermore, we show how our framework can be\nrealized in code through a natural extension of the OpenDP Programming\nFramework.",
    "updated" : "2024-09-09T13:56:04Z",
    "published" : "2024-09-09T13:56:04Z",
    "authors" : [
      {
        "name" : "Zachary Ratliff"
      },
      {
        "name" : "Salil Vadhan"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.05249v1",
    "title" : "NetDPSyn: Synthesizing Network Traces under Differential Privacy",
    "summary" : "As the utilization of network traces for the network measurement research\nbecomes increasingly prevalent, concerns regarding privacy leakage from network\ntraces have garnered the public's attention. To safeguard network traces,\nresearchers have proposed the trace synthesis that retains the essential\nproperties of the raw data. However, previous works also show that synthesis\ntraces with generative models are vulnerable under linkage attacks.\n  This paper introduces NetDPSyn, the first system to synthesize high-fidelity\nnetwork traces under privacy guarantees. NetDPSyn is built with the\nDifferential Privacy (DP) framework as its core, which is significantly\ndifferent from prior works that apply DP when training the generative model.\nThe experiments conducted on three flow and two packet datasets indicate that\nNetDPSyn achieves much better data utility in downstream tasks like anomaly\ndetection. NetDPSyn is also 2.5 times faster than the other methods on average\nin data synthesis.",
    "updated" : "2024-09-08T23:54:00Z",
    "published" : "2024-09-08T23:54:00Z",
    "authors" : [
      {
        "name" : "Danyu Sun"
      },
      {
        "name" : "Joann Qiongna Chen"
      },
      {
        "name" : "Chen Gong"
      },
      {
        "name" : "Tianhao Wang"
      },
      {
        "name" : "Zhou Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DB",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04877v1",
    "title" : "Strong Privacy-Preserving Universally Composable AKA Protocol with\n  Seamless Handover Support for Mobile Virtual Network Operator",
    "summary" : "Consumers seeking a new mobile plan have many choices in the present mobile\nlandscape. The Mobile Virtual Network Operator (MVNO) has recently gained\nconsiderable attention among these options. MVNOs offer various benefits,\nmaking them an appealing choice for a majority of consumers. These advantages\nencompass flexibility, access to cutting-edge technologies, enhanced coverage,\nsuperior customer service, and substantial cost savings. Even though MVNO\noffers several advantages, it also creates some security and privacy concerns\nfor the customer simultaneously. For instance, in the existing solution, MVNO\nneeds to hand over all the sensitive details, including the users' identities\nand master secret keys of their customers, to a mobile operator (MNO) to\nvalidate the customers while offering any services. This allows MNOs to have\nunrestricted access to the MVNO subscribers' location and mobile data,\nincluding voice calls, SMS, and Internet, which the MNOs frequently sell to\nthird parties (e.g., advertisement companies and surveillance agencies) for\nmore profit. Although critical for mass users, such privacy loss has been\nhistorically ignored due to the lack of practical and privacy-preserving\nsolutions for registration and handover procedures in cellular networks. In\nthis paper, we propose a universally composable authentication and handover\nscheme with strong user privacy support, where each MVNO user can validate a\nmobile operator (MNO) and vice-versa without compromising user anonymity and\nunlinkability support. Here, we anticipate that our proposed solution will most\nlikely be deployed by the MVNO(s) to ensure enhanced privacy support to their\ncustomer(s).",
    "updated" : "2024-09-07T18:04:54Z",
    "published" : "2024-09-07T18:04:54Z",
    "authors" : [
      {
        "name" : "Rabiah Alnashwan"
      },
      {
        "name" : "Yang Yang"
      },
      {
        "name" : "Yilu Dong"
      },
      {
        "name" : "Prosanta Gope"
      },
      {
        "name" : "Behzad Abdolmaleki"
      },
      {
        "name" : "Syed Rafiul Hussain"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04716v1",
    "title" : "Privacy enhanced collaborative inference in the Cox proportional hazards\n  model for distributed data",
    "summary" : "Data sharing barriers are paramount challenges arising from multicenter\nclinical studies where multiple data sources are stored in a distributed\nfashion at different local study sites. Particularly in the case of\ntime-to-event analysis when global risk sets are needed for the Cox\nproportional hazards model, access to a centralized database is typically\nnecessary. Merging such data sources into a common data storage for a\ncentralized statistical analysis requires a data use agreement, which is often\ntime-consuming. Furthermore, the construction and distribution of risk sets to\nparticipating clinical centers for subsequent calculations may pose a risk of\nrevealing individual-level information. We propose a new collaborative Cox\nmodel that eliminates the need for accessing the centralized database and\nconstructing global risk sets but needs only the sharing of summary statistics\nwith significantly smaller dimensions than risk sets. Thus, the proposed\ncollaborative inference enjoys maximal protection of data privacy. We show\ntheoretically and numerically that the new distributed proportional hazards\nmodel approach has little loss of statistical power when compared to the\ncentralized method that requires merging the entire data. We present a\nrenewable sieve method to establish large-sample properties for the proposed\nmethod. We illustrate its performance through simulation experiments and a\nreal-world data example from patients with kidney transplantation in the Organ\nProcurement and Transplantation Network (OPTN) to understand the factors\nassociated with the 5-year death-censored graft failure (DCGF) for patients who\nunderwent kidney transplants in the US.",
    "updated" : "2024-09-07T05:32:34Z",
    "published" : "2024-09-07T05:32:34Z",
    "authors" : [
      {
        "name" : "Mengtong Hu"
      },
      {
        "name" : "Xu Shi"
      },
      {
        "name" : "Peter X. -K. Song"
      }
    ],
    "categories" : [
      "stat.AP",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04652v1",
    "title" : "Privacy-Preserving Race/Ethnicity Estimation for Algorithmic Bias\n  Measurement in the U.S",
    "summary" : "AI fairness measurements, including tests for equal treatment, often take the\nform of disaggregated evaluations of AI systems. Such measurements are an\nimportant part of Responsible AI operations. These measurements compare system\nperformance across demographic groups or sub-populations and typically require\nmember-level demographic signals such as gender, race, ethnicity, and location.\nHowever, sensitive member-level demographic attributes like race and ethnicity\ncan be challenging to obtain and use due to platform choices, legal\nconstraints, and cultural norms. In this paper, we focus on the task of\nenabling AI fairness measurements on race/ethnicity for \\emph{U.S. LinkedIn\nmembers} in a privacy-preserving manner. We present the Privacy-Preserving\nProbabilistic Race/Ethnicity Estimation (PPRE) method for performing this task.\nPPRE combines the Bayesian Improved Surname Geocoding (BISG) model, a sparse\nLinkedIn survey sample of self-reported demographics, and privacy-enhancing\ntechnologies like secure two-party computation and differential privacy to\nenable meaningful fairness measurements while preserving member privacy. We\nprovide details of the PPRE method and its privacy guarantees. We then\nillustrate sample measurement operations. We conclude with a review of open\nresearch and engineering challenges for expanding our privacy-preserving\nfairness measurement capabilities.",
    "updated" : "2024-09-06T23:29:18Z",
    "published" : "2024-09-06T23:29:18Z",
    "authors" : [
      {
        "name" : "Saikrishna Badrinarayanan"
      },
      {
        "name" : "Osonde Osoba"
      },
      {
        "name" : "Miao Cheng"
      },
      {
        "name" : "Ryan Rogers"
      },
      {
        "name" : "Sakshi Jain"
      },
      {
        "name" : "Rahul Tandra"
      },
      {
        "name" : "Natesh S. Pillai"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06564v1",
    "title" : "Advancing Android Privacy Assessments with Automation",
    "summary" : "Android apps collecting data from users must comply with legal frameworks to\nensure data protection. This requirement has become even more important since\nthe implementation of the General Data Protection Regulation (GDPR) by the\nEuropean Union in 2018. Moreover, with the proposed Cyber Resilience Act on the\nhorizon, stakeholders will soon need to assess software against even more\nstringent security and privacy standards. Effective privacy assessments require\ncollaboration among groups with diverse expertise to function effectively as a\ncohesive unit.\n  This paper motivates the need for an automated approach that enhances\nunderstanding of data protection in Android apps and improves communication\nbetween the various parties involved in privacy assessments. We propose the\nAssessor View, a tool designed to bridge the knowledge gap between these\nparties, facilitating more effective privacy assessments of Android\napplications.",
    "updated" : "2024-09-10T14:56:51Z",
    "published" : "2024-09-10T14:56:51Z",
    "authors" : [
      {
        "name" : "Mugdha Khedkar"
      },
      {
        "name" : "Michael Schlichtig"
      },
      {
        "name" : "Eric Bodden"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06455v1",
    "title" : "Continual Domain Incremental Learning for Privacy-aware Digital\n  Pathology",
    "summary" : "In recent years, there has been remarkable progress in the field of digital\npathology, driven by the ability to model complex tissue patterns using\nadvanced deep-learning algorithms. However, the robustness of these models is\noften severely compromised in the presence of data shifts (e.g., different\nstains, organs, centers, etc.). Alternatively, continual learning (CL)\ntechniques aim to reduce the forgetting of past data when learning new data\nwith distributional shift conditions. Specifically, rehearsal-based CL\ntechniques, which store some past data in a buffer and then replay it with new\ndata, have proven effective in medical image analysis tasks. However, privacy\nconcerns arise as these approaches store past data, prompting the development\nof our novel Generative Latent Replay-based CL (GLRCL) approach. GLRCL captures\nthe previous distribution through Gaussian Mixture Models instead of storing\npast samples, which are then utilized to generate features and perform latent\nreplay with new data. We systematically evaluate our proposed framework under\ndifferent shift conditions in histopathology data, including stain and organ\nshift. Our approach significantly outperforms popular buffer-free CL approaches\nand performs similarly to rehearsal-based CL approaches that require large\nbuffers causing serious privacy violations.",
    "updated" : "2024-09-10T12:21:54Z",
    "published" : "2024-09-10T12:21:54Z",
    "authors" : [
      {
        "name" : "Pratibha Kumari"
      },
      {
        "name" : "Daniel Reisenbüchler"
      },
      {
        "name" : "Lucas Luttner"
      },
      {
        "name" : "Nadine S. Schaadt"
      },
      {
        "name" : "Friedrich Feuerhake"
      },
      {
        "name" : "Dorit Merhof"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06422v1",
    "title" : "A Pervasive, Efficient and Private Future: Realizing Privacy-Preserving\n  Machine Learning Through Hybrid Homomorphic Encryption",
    "summary" : "Machine Learning (ML) has become one of the most impactful fields of data\nscience in recent years. However, a significant concern with ML is its privacy\nrisks due to rising attacks against ML models. Privacy-Preserving Machine\nLearning (PPML) methods have been proposed to mitigate the privacy and security\nrisks of ML models. A popular approach to achieving PPML uses Homomorphic\nEncryption (HE). However, the highly publicized inefficiencies of HE make it\nunsuitable for highly scalable scenarios with resource-constrained devices.\nHence, Hybrid Homomorphic Encryption (HHE) -- a modern encryption scheme that\ncombines symmetric cryptography with HE -- has recently been introduced to\novercome these challenges. HHE potentially provides a foundation to build new\nefficient and privacy-preserving services that transfer expensive HE operations\nto the cloud. This work introduces HHE to the ML field by proposing\nresource-friendly PPML protocols for edge devices. More precisely, we utilize\nHHE as the primary building block of our PPML protocols. We assess the\nperformance of our protocols by first extensively evaluating each party's\ncommunication and computational cost on a dummy dataset and show the efficiency\nof our protocols by comparing them with similar protocols implemented using\nplain BFV. Subsequently, we demonstrate the real-world applicability of our\nconstruction by building an actual PPML application that uses HHE as its\nfoundation to classify heart disease based on sensitive ECG data.",
    "updated" : "2024-09-10T11:04:14Z",
    "published" : "2024-09-10T11:04:14Z",
    "authors" : [
      {
        "name" : "Khoa Nguyen"
      },
      {
        "name" : "Mindaugas Budzys"
      },
      {
        "name" : "Eugene Frimpong"
      },
      {
        "name" : "Tanveer Khan"
      },
      {
        "name" : "Antonis Michalas"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06360v1",
    "title" : "SoK: Evaluating 5G Protocols Against Legacy and Emerging Privacy and\n  Security Attacks",
    "summary" : "Ensuring user privacy remains a critical concern within mobile cellular\nnetworks, particularly given the proliferation of interconnected devices and\nservices. In fact, a lot of user privacy issues have been raised in 2G, 3G,\n4G/LTE networks. Recognizing this general concern, 3GPP has prioritized\naddressing these issues in the development of 5G, implementing numerous\nmodifications to enhance user privacy since 5G Release 15. In this\nsystematization of knowledge paper, we first provide a framework for studying\nprivacy and security related attacks in cellular networks, setting as privacy\nobjective the User Identity Confidentiality defined in 3GPP standards. Using\nthis framework, we discuss existing privacy and security attacks in pre-5G\nnetworks, analyzing the weaknesses that lead to these attacks. Furthermore, we\nthoroughly study the security characteristics of 5G up to the new Release 19,\nand examine mitigation mechanisms of 5G to the identified pre-5G attacks.\nAfterwards, we analyze how recent 5G attacks try to overcome these mitigation\nmechanisms. Finally, we identify current limitations and open problems in\nsecurity of 5G, and propose directions for future work.",
    "updated" : "2024-09-10T09:30:37Z",
    "published" : "2024-09-10T09:30:37Z",
    "authors" : [
      {
        "name" : "Stavros Eleftherakis"
      },
      {
        "name" : "Domenico Giustiniano"
      },
      {
        "name" : "Nicolas Kourtellis"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06233v1",
    "title" : "VBIT: Towards Enhancing Privacy Control Over IoT Devices",
    "summary" : "Internet-of-Things (IoT) devices are increasingly deployed at home, at work,\nand in other shared and public spaces. IoT devices collect and share data with\nservice providers and third parties, which poses privacy concerns. Although\nprivacy enhancing tools are quite advanced in other applications domains (\\eg~\nadvertising and tracker blockers for browsers), users have currently no\nconvenient way to know or manage what and how data is collected and shared by\nIoT devices. In this paper, we present VBIT, an interactive system combining\nMixed Reality (MR) and web-based applications that allows users to: (1) uncover\nand visualize tracking services by IoT devices in an instrumented space and (2)\ntake action to stop or limit that tracking. We design and implement VBIT to\noperate at the network traffic level, and we show that it has negligible\nperformance overhead, and offers flexibility and good usability. We perform a\nmixed-method user study consisting of an online survey and an in-person\ninterview study. We show that VBIT users appreciate VBIT's transparency,\ncontrol, and customization features, and they become significantly more willing\nto install an IoT advertising and tracking blocker, after using VBIT. In the\nprocess, we obtain design insights that can be used to further iterate and\nimprove the design of VBIT and other systems for IoT transparency and control.",
    "updated" : "2024-09-10T06:00:50Z",
    "published" : "2024-09-10T06:00:50Z",
    "authors" : [
      {
        "name" : "Jad Al Aaraj"
      },
      {
        "name" : "Olivia Figueira"
      },
      {
        "name" : "Tu Le"
      },
      {
        "name" : "Isabela Figueira"
      },
      {
        "name" : "Rahmadi Trimananda"
      },
      {
        "name" : "Athina Markopoulou"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.06069v1",
    "title" : "Privacy-Preserving Data Linkage Across Private and Public Datasets for\n  Collaborative Agriculture Research",
    "summary" : "Digital agriculture leverages technology to enhance crop yield, disease\nresilience, and soil health, playing a critical role in agricultural research.\nHowever, it raises privacy concerns such as adverse pricing, price\ndiscrimination, higher insurance costs, and manipulation of resources,\ndeterring farm operators from sharing data due to potential misuse. This study\nintroduces a privacy-preserving framework that addresses these risks while\nallowing secure data sharing for digital agriculture. Our framework enables\ncomprehensive data analysis while protecting privacy. It allows stakeholders to\nharness research-driven policies that link public and private datasets. The\nproposed algorithm achieves this by: (1) identifying similar farmers based on\nprivate datasets, (2) providing aggregate information like time and location,\n(3) determining trends in price and product availability, and (4) correlating\ntrends with public policy data, such as food insecurity statistics. We validate\nthe framework with real-world Farmer's Market datasets, demonstrating its\nefficacy through machine learning models trained on linked privacy-preserved\ndata. The results support policymakers and researchers in addressing food\ninsecurity and pricing issues. This work significantly contributes to digital\nagriculture by providing a secure method for integrating and analyzing data,\ndriving advancements in agricultural technology and development.",
    "updated" : "2024-09-09T21:07:13Z",
    "published" : "2024-09-09T21:07:13Z",
    "authors" : [
      {
        "name" : "Osama Zafar"
      },
      {
        "name" : "Rosemarie Santa Gonzalez"
      },
      {
        "name" : "Gabriel Wilkins"
      },
      {
        "name" : "Alfonso Morales"
      },
      {
        "name" : "Erman Ayday"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2409.04048v2",
    "title" : "Exploring User Privacy Awareness on GitHub: An Empirical Study",
    "summary" : "GitHub provides developers with a practical way to distribute source code and\ncollaboratively work on common projects. To enhance account security and\nprivacy, GitHub allows its users to manage access permissions, review audit\nlogs, and enable two-factor authentication. However, despite the endless\neffort, the platform still faces various issues related to the privacy of its\nusers. This paper presents an empirical study delving into the GitHub\necosystem. Our focus is on investigating the utilization of privacy settings on\nthe platform and identifying various types of sensitive information disclosed\nby users. Leveraging a dataset comprising 6,132 developers, we report and\nanalyze their activities by means of comments on pull requests. Our findings\nindicate an active engagement by users with the available privacy settings on\nGitHub. Notably, we observe the disclosure of different forms of private\ninformation within pull request comments. This observation has prompted our\nexploration into sensitivity detection using a large language model and BERT,\nto pave the way for a personalized privacy assistant. Our work provides\ninsights into the utilization of existing privacy protection tools, such as\nprivacy settings, along with their inherent limitations. Essentially, we aim to\nadvance research in this field by providing both the motivation for creating\nsuch privacy protection tools and a proposed methodology for personalizing\nthem.",
    "updated" : "2024-09-10T09:35:53Z",
    "published" : "2024-09-06T06:41:46Z",
    "authors" : [
      {
        "name" : "Costanza Alfieri"
      },
      {
        "name" : "Juri Di Rocco"
      },
      {
        "name" : "Paola Inverardi"
      },
      {
        "name" : "Phuong T. Nguyen"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  }
]