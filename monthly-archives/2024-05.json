[
  {
    "id" : "http://arxiv.org/abs/2405.00616v1",
    "title" : "An Expectation-Maximization Relaxed Method for Privacy Funnel",
    "summary" : "The privacy funnel (PF) gives a framework of privacy-preserving data release,\nwhere the goal is to release useful data while also limiting the exposure of\nassociated sensitive information. This framework has garnered significant\ninterest due to its broad applications in characterization of the\nprivacy-utility tradeoff. Hence, there is a strong motivation to develop\nnumerical methods with high precision and theoretical convergence guarantees.\nIn this paper, we propose a novel relaxation variant based on Jensen's\ninequality of the objective function for the computation of the PF problem.\nThis model is proved to be equivalent to the original in terms of optimal\nsolutions and optimal values. Based on our proposed model, we develop an\naccurate algorithm which only involves closed-form iterations. The convergence\nof our algorithm is theoretically guaranteed through descent estimation and\nPinsker's inequality. Numerical results demonstrate the effectiveness of our\nproposed algorithm.",
    "updated" : "2024-05-01T16:35:44Z",
    "published" : "2024-05-01T16:35:44Z",
    "authors" : [
      {
        "name" : "Lingyi Chen"
      },
      {
        "name" : "Jiachuan Ye"
      },
      {
        "name" : "Shitong Wu"
      },
      {
        "name" : "Huihui Wu"
      },
      {
        "name" : "Hao Wu"
      },
      {
        "name" : "Wenyi Zhang"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.00596v1",
    "title" : "Unbundle-Rewrite-Rebundle: Runtime Detection and Rewriting of\n  Privacy-Harming Code in JavaScript Bundles",
    "summary" : "This work presents Unbundle-Rewrite-Rebundle (URR), a system for detecting\nprivacy-harming portions of bundled JavaScript code, and rewriting that code at\nruntime to remove the privacy harming behavior without breaking the surrounding\ncode or overall application. URR is a novel solution to the problem of\nJavaScript bundles, where websites pre-compile multiple code units into a\nsingle file, making it impossible for content filters and ad-blockers to\ndifferentiate between desired and unwanted resources. Where traditional content\nfiltering tools rely on URLs, URR analyzes the code at the AST level, and\nreplaces harmful AST sub-trees with privacy-and-functionality maintaining\nalternatives.\n  We present an open-sourced implementation of URR as a Firefox extension, and\nevaluate it against JavaScript bundles generated by the most popular bundling\nsystem (Webpack) deployed on the Tranco 10k. We measure the performance,\nmeasured by precision (1.00), recall (0.95), and speed (0.43s per-script) when\ndetecting and rewriting three representative privacy harming libraries often\nincluded in JavaScript bundles, and find URR to be an effective approach to a\nlarge-and-growing blind spot unaddressed by current privacy tools.",
    "updated" : "2024-05-01T16:04:42Z",
    "published" : "2024-05-01T16:04:42Z",
    "authors" : [
      {
        "name" : "Mir Masood Ali"
      },
      {
        "name" : "Peter Snyder"
      },
      {
        "name" : "Chris Kanich"
      },
      {
        "name" : "Hamed Haddadi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.00329v1",
    "title" : "Metric geometry of the privacy-utility tradeoff",
    "summary" : "Synthetic data are an attractive concept to enable privacy in data sharing. A\nfundamental question is how similar the privacy-preserving synthetic data are\ncompared to the true data. Using metric privacy, an effective generalization of\ndifferential privacy beyond the discrete setting, we raise the problem of\ncharacterizing the optimal privacy-accuracy tradeoff by the metric geometry of\nthe underlying space. We provide a partial solution to this problem in terms of\nthe \"entropic scale\", a quantity that captures the multiscale geometry of a\nmetric space via the behavior of its packing numbers. We illustrate the\napplicability of our privacy-accuracy tradeoff framework via a diverse set of\nexamples of metric spaces.",
    "updated" : "2024-05-01T05:31:53Z",
    "published" : "2024-05-01T05:31:53Z",
    "authors" : [
      {
        "name" : "March Boedihardjo"
      },
      {
        "name" : "Thomas Strohmer"
      },
      {
        "name" : "Roman Vershynin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS",
      "math.PR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01494v1",
    "title" : "Navigating Heterogeneity and Privacy in One-Shot Federated Learning with\n  Diffusion Models",
    "summary" : "Federated learning (FL) enables multiple clients to train models collectively\nwhile preserving data privacy. However, FL faces challenges in terms of\ncommunication cost and data heterogeneity. One-shot federated learning has\nemerged as a solution by reducing communication rounds, improving efficiency,\nand providing better security against eavesdropping attacks. Nevertheless, data\nheterogeneity remains a significant challenge, impacting performance. This work\nexplores the effectiveness of diffusion models in one-shot FL, demonstrating\ntheir applicability in addressing data heterogeneity and improving FL\nperformance. Additionally, we investigate the utility of our diffusion model\napproach, FedDiff, compared to other one-shot FL methods under differential\nprivacy (DP). Furthermore, to improve generated sample quality under DP\nsettings, we propose a pragmatic Fourier Magnitude Filtering (FMF) method,\nenhancing the effectiveness of generated data for global model training.",
    "updated" : "2024-05-02T17:26:52Z",
    "published" : "2024-05-02T17:26:52Z",
    "authors" : [
      {
        "name" : "Matias Mendieta"
      },
      {
        "name" : "Guangyu Sun"
      },
      {
        "name" : "Chen Chen"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01492v1",
    "title" : "Exploring Privacy Issues in Mission Critical Communication: Navigating\n  5G and Beyond Networks",
    "summary" : "Mission critical communication (MCC) involves the exchange of information and\ndata among emergency services, including the police, fire brigade, and other\nfirst responders, particularly during emergencies, disasters, or critical\nincidents. The widely-adopted TETRA (Terrestrial Trunked Radio)-based\ncommunication for mission critical services faces challenges including limited\ndata capacity, coverage limitations, spectrum congestion, and security\nconcerns. Therefore, as an alternative, mission critical communication over\ncellular networks (4G and 5G) has emerged. While cellular-based MCC enables\nfeatures like real-time video streaming and high-speed data transmission, the\ninvolvement of network operators and application service providers in the MCC\narchitecture raises privacy concerns for mission critical users and services.\nFor instance, the disclosure of a policeman's location details to the network\noperator raises privacy concerns. To the best of our knowledge, no existing\nwork considers the privacy issues in mission critical system with respect to 5G\nand upcoming technologies. Therefore, in this paper, we analyse the 3GPP\nstandardised MCC architecture within the context of 5G core network concepts\nand assess the privacy implications for MC users, network entities, and MC\nservers. The privacy analysis adheres to the deployment strategies in the\nstandard for MCC. Additionally, we explore emerging 6G technologies, such as\noff-network communications, joint communication and sensing, and non-3GPP\ncommunications, to identify privacy challenges in MCC architecture. Finally, we\npropose privacy controls to establish a next-generation privacy-preserving MCC\narchitecture.",
    "updated" : "2024-05-02T17:25:33Z",
    "published" : "2024-05-02T17:25:33Z",
    "authors" : [
      {
        "name" : "Prajnamaya Dass"
      },
      {
        "name" : "Marcel Gräfenstein"
      },
      {
        "name" : "Stefan Köpsell"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01411v1",
    "title" : "IDPFilter: Mitigating Interdependent Privacy Issues in Third-Party Apps",
    "summary" : "Third-party applications have become an essential part of today's online\necosystem, enhancing the functionality of popular platforms. However, the\nintensive data exchange underlying their proliferation has increased concerns\nabout interdependent privacy (IDP). This paper provides a comprehensive\ninvestigation into the previously underinvestigated IDP issues of third-party\napps. Specifically, first, we analyze the permission structure of multiple app\nplatforms, identifying permissions that have the potential to cause\ninterdependent privacy issues by enabling a user to share someone else's\npersonal data with an app. Second, we collect datasets and characterize the\nextent to which existing apps request these permissions, revealing the\nrelationship between characteristics such as the respective app platform, the\napp's type, and the number of interdependent privacy-related permissions it\nrequests. Third, we analyze the various reasons IDP is neglected by both data\nprotection regulations and app platforms and then devise principles that should\nbe followed when designing a mitigation solution. Finally, based on these\nprinciples and satisfying clearly defined objectives, we propose IDPFilter, a\nplatform-agnostic API that enables application providers to minimize collateral\ninformation collection by filtering out data collected from their users but\nimplicating others as data subjects. We implement a proof-of-concept prototype,\nIDPTextFilter, that implements the filtering logic on textual data, and provide\nits initial performance evaluation with regard to privacy, accuracy, and\nefficiency.",
    "updated" : "2024-05-02T16:02:13Z",
    "published" : "2024-05-02T16:02:13Z",
    "authors" : [
      {
        "name" : "Shuaishuai Liu"
      },
      {
        "name" : "Gergely Biczók"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01312v1",
    "title" : "Privacy-Enhanced Database Synthesis for Benchmark Publishing",
    "summary" : "Benchmarking is crucial for evaluating a DBMS, yet existing benchmarks often\nfail to reflect the varied nature of user workloads. As a result, there is\nincreasing momentum toward creating databases that incorporate real-world user\ndata to more accurately mirror business environments. However, privacy concerns\ndeter users from directly sharing their data, underscoring the importance of\ncreating synthesized databases for benchmarking that also prioritize privacy\nprotection. Differential privacy has become a key method for safeguarding\nprivacy when sharing data, but the focus has largely been on minimizing errors\nin aggregate queries or classification tasks, with less attention given to\nbenchmarking factors like runtime performance. This paper delves into the\ncreation of privacy-preserving databases specifically for benchmarking, aiming\nto produce a differentially private database whose query performance closely\nresembles that of the original data. Introducing PrivBench, an innovative\nsynthesis framework, we support the generation of high-quality data that\nmaintains privacy. PrivBench uses sum-product networks (SPNs) to partition and\nsample data, enhancing data representation while securing privacy. The\nframework allows users to adjust the detail of SPN partitions and privacy\nsettings, crucial for customizing privacy levels. We validate our approach,\nwhich uses the Laplace and exponential mechanisms, in maintaining privacy. Our\ntests show that PrivBench effectively generates data that maintains privacy and\nexcels in query performance, consistently reducing errors in query execution\ntime, query cardinality, and KL divergence.",
    "updated" : "2024-05-02T14:20:24Z",
    "published" : "2024-05-02T14:20:24Z",
    "authors" : [
      {
        "name" : "Yongrui Zhong"
      },
      {
        "name" : "Yunqing Ge"
      },
      {
        "name" : "Jianbin Qin"
      },
      {
        "name" : "Shuyuan Zheng"
      },
      {
        "name" : "Bo Tang"
      },
      {
        "name" : "Yu-Xuan Qiu"
      },
      {
        "name" : "Rui Mao"
      },
      {
        "name" : "Ye Yuan"
      },
      {
        "name" : "Makoto Onizuka"
      },
      {
        "name" : "Chuan Xiao"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01221v1",
    "title" : "A Survey on Semantic Communication Networks: Architecture, Security, and\n  Privacy",
    "summary" : "Semantic communication, emerging as a breakthrough beyond the classical\nShannon paradigm, aims to convey the essential meaning of source data rather\nthan merely focusing on precise yet content-agnostic bit transmission. By\ninterconnecting diverse intelligent agents (e.g., autonomous vehicles and VR\ndevices) via semantic communications, the semantic communication networks\n(SemComNet) supports semantic-oriented transmission, efficient spectrum\nutilization, and flexible networking among collaborative agents. Consequently,\nSemComNet stands out for enabling ever-increasing intelligent applications,\nsuch as autonomous driving and Metaverse. However, being built on a variety of\ncutting-edge technologies including AI and knowledge graphs, SemComNet\nintroduces diverse brand-new and unexpected threats, which pose obstacles to\nits widespread development. Besides, due to the intrinsic characteristics of\nSemComNet in terms of heterogeneous components, autonomous intelligence, and\nlarge-scale structure, a series of critical challenges emerge in securing\nSemComNet. In this paper, we provide a comprehensive and up-to-date survey of\nSemComNet from its fundamentals, security, and privacy aspects. Specifically,\nwe first introduce a novel three-layer architecture of SemComNet for\nmulti-agent interaction, which comprises the control layer, semantic\ntransmission layer, and cognitive sensing layer. Then, we discuss its working\nmodes and enabling technologies. Afterward, based on the layered architecture\nof SemComNet, we outline a taxonomy of security and privacy threats, while\ndiscussing state-of-the-art defense approaches. Finally, we present future\nresearch directions, clarifying the path toward building intelligent, robust,\nand green SemComNet. To our knowledge, this survey is the first to\ncomprehensively cover the fundamentals of SemComNet, alongside a detailed\nanalysis of its security and privacy issues.",
    "updated" : "2024-05-02T12:04:35Z",
    "published" : "2024-05-02T12:04:35Z",
    "authors" : [
      {
        "name" : "Shaolong Guo"
      },
      {
        "name" : "Yuntao Wang"
      },
      {
        "name" : "Ning Zhang"
      },
      {
        "name" : "Zhou Su"
      },
      {
        "name" : "Tom H. Luan"
      },
      {
        "name" : "Zhiyi Tian"
      },
      {
        "name" : "Xuemin Shen"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01031v1",
    "title" : "The Privacy Power of Correlated Noise in Decentralized Learning",
    "summary" : "Decentralized learning is appealing as it enables the scalable usage of large\namounts of distributed data and resources (without resorting to any central\nentity), while promoting privacy since every user minimizes the direct exposure\nof their data. Yet, without additional precautions, curious users can still\nleverage models obtained from their peers to violate privacy. In this paper, we\npropose Decor, a variant of decentralized SGD with differential privacy (DP)\nguarantees. Essentially, in Decor, users securely exchange randomness seeds in\none communication round to generate pairwise-canceling correlated Gaussian\nnoises, which are injected to protect local models at every communication\nround. We theoretically and empirically show that, for arbitrary connected\ngraphs, Decor matches the central DP optimal privacy-utility trade-off. We do\nso under SecLDP, our new relaxation of local DP, which protects all user\ncommunications against an external eavesdropper and curious users, assuming\nthat every pair of connected users shares a secret, i.e., an information hidden\nto all others. The main theoretical challenge is to control the accumulation of\nnon-canceling correlated noise due to network sparsity. We also propose a\ncompanion SecLDP privacy accountant for public use.",
    "updated" : "2024-05-02T06:14:56Z",
    "published" : "2024-05-02T06:14:56Z",
    "authors" : [
      {
        "name" : "Youssef Allouah"
      },
      {
        "name" : "Anastasia Koloskova"
      },
      {
        "name" : "Aymane El Firdoussi"
      },
      {
        "name" : "Martin Jaggi"
      },
      {
        "name" : "Rachid Guerraoui"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "math.OC",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01844v1",
    "title" : "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
    "summary" : "Caching content at the network edge is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the network edge. On the one\nhand, the multi-access open edge network provides an ideal surface for external\nattackers to obtain private data from the edge cache by extracting sensitive\ninformation. On the other hand, privacy can be infringed by curious edge\ncaching providers through caching trace analysis targeting to achieve better\ncaching performance or higher profits. Therefore, an in-depth understanding of\nprivacy issues in edge caching networks is vital and indispensable for creating\na privacy-preserving caching service at the network edge. In this article, we\nare among the first to fill in this gap by examining privacy-preserving\ntechniques for caching content at the network edge. Firstly, we provide an\nintroduction to the background of Privacy-Preserving Edge Caching (PPEC). Next,\nwe summarize the key privacy issues and present a taxonomy for caching at the\nnetwork edge from the perspective of private data. Additionally, we conduct a\nretrospective review of the state-of-the-art countermeasures against privacy\nleakage from content caching at the network edge. Finally, we conclude the\nsurvey and envision challenges for future research.",
    "updated" : "2024-05-03T04:27:32Z",
    "published" : "2024-05-03T04:27:32Z",
    "authors" : [
      {
        "name" : "Xianzhi Zhang"
      },
      {
        "name" : "Yipeng Zhou"
      },
      {
        "name" : "Di Wu"
      },
      {
        "name" : "Shazia Riaz"
      },
      {
        "name" : "Quan Z. Sheng"
      },
      {
        "name" : "Miao Hu"
      },
      {
        "name" : "Linchang Xiao"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01742v1",
    "title" : "Addressing Privacy Concerns in Joint Communication and Sensing for 6G\n  Networks: Challenges and Prospects",
    "summary" : "The vision for 6G extends beyond mere communication, incorporating sensing\ncapabilities to facilitate a diverse array of novel applications and services.\nHowever, the advent of joint communication and sensing (JCAS) technology\nintroduces concerns regarding the handling of sensitive personally identifiable\ninformation (PII) pertaining to individuals and objects, along with external\nthird-party data and disclosure. Consequently, JCAS-based applications are\nsusceptible to privacy breaches, including location tracking, identity\ndisclosure, profiling, and misuse of sensor data, raising significant\nimplications under the European Union's General Data Protection Regulation\n(GDPR) as well as other applicable standards. This paper critically examines\nemergent JCAS architectures and underscores the necessity for network functions\nto enable privacy-specific features in the 6G systems. We propose an enhanced\nJCAS architecture with additional network functions and interfaces,\nfacilitating the management of sensing policies, consent information, and\ntransparency guidelines, alongside the integration of sensing-specific\nfunctions and storage for sensing processing sessions. Furthermore, we conduct\na comprehensive threat analysis for all interfaces, employing security threat\nmodel STRIDE and privacy threat model LINDDUN. We also summarise the identified\nthreats using standard Common Weakness Enumerations (CWEs). Finally, we suggest\nthe security and privacy controls as the mitigating strategies to counter the\nidentified threats stemming from the JCAS architecture.",
    "updated" : "2024-05-02T21:25:24Z",
    "published" : "2024-05-02T21:25:24Z",
    "authors" : [
      {
        "name" : "Prajnamaya Dass"
      },
      {
        "name" : "Sonika Ujjwal"
      },
      {
        "name" : "Jiri Novotny"
      },
      {
        "name" : "Yevhen Zolotavkin"
      },
      {
        "name" : "Zakaria Laaroussi"
      },
      {
        "name" : "Stefan Köpsell"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01716v1",
    "title" : "ATTAXONOMY: Unpacking Differential Privacy Guarantees Against Practical\n  Adversaries",
    "summary" : "Differential Privacy (DP) is a mathematical framework that is increasingly\ndeployed to mitigate privacy risks associated with machine learning and\nstatistical analyses. Despite the growing adoption of DP, its technical privacy\nparameters do not lend themselves to an intelligible description of the\nreal-world privacy risks associated with that deployment: the guarantee that\nmost naturally follows from the DP definition is protection against membership\ninference by an adversary who knows all but one data record and has unlimited\nauxiliary knowledge. In many settings, this adversary is far too strong to\ninform how to set real-world privacy parameters.\n  One approach for contextualizing privacy parameters is via defining and\nmeasuring the success of technical attacks, but doing so requires a systematic\ncategorization of the relevant attack space. In this work, we offer a detailed\ntaxonomy of attacks, showing the various dimensions of attacks and highlighting\nthat many real-world settings have been understudied. Our taxonomy provides a\nroadmap for analyzing real-world deployments and developing theoretical bounds\nfor more informative privacy attacks. We operationalize our taxonomy by using\nit to analyze a real-world case study, the Israeli Ministry of Health's recent\nrelease of a birth dataset using DP, showing how the taxonomy enables\nfine-grained threat modeling and provides insight towards making informed\nprivacy parameter choices. Finally, we leverage the taxonomy towards defining a\nmore realistic attack than previously considered in the literature, namely a\ndistributional reconstruction attack: we generalize Balle et al.'s notion of\nreconstruction robustness to a less-informed adversary with distributional\nuncertainty, and extend the worst-case guarantees of DP to this average-case\nsetting.",
    "updated" : "2024-05-02T20:23:23Z",
    "published" : "2024-05-02T20:23:23Z",
    "authors" : [
      {
        "name" : "Rachel Cummings"
      },
      {
        "name" : "Shlomi Hod"
      },
      {
        "name" : "Jayshree Sarathy"
      },
      {
        "name" : "Marika Swanberg"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01704v1",
    "title" : "Privacy-aware Berrut Approximated Coded Computing for Federated Learning",
    "summary" : "Federated Learning (FL) is an interesting strategy that enables the\ncollaborative training of an AI model among different data owners without\nrevealing their private datasets. Even so, FL has some privacy vulnerabilities\nthat have been tried to be overcome by applying some techniques like\nDifferential Privacy (DP), Homomorphic Encryption (HE), or Secure Multi-Party\nComputation (SMPC). However, these techniques have some important drawbacks\nthat might narrow their range of application: problems to work with non-linear\nfunctions and to operate large matrix multiplications and high communication\nand computational costs to manage semi-honest nodes. In this context, we\npropose a solution to guarantee privacy in FL schemes that simultaneously\nsolves the previously mentioned problems. Our proposal is based on the Berrut\nApproximated Coded Computing, a technique from the Coded Distributed Computing\nparadigm, adapted to a Secret Sharing configuration, to provide input privacy\nto FL in a scalable way. It can be applied for computing non-linear functions\nand treats the special case of distributed matrix multiplication, a key\nprimitive at the core of many automated learning tasks. Because of these\ncharacteristics, it could be applied in a wide range of FL scenarios, since it\nis independent of the machine learning models or aggregation algorithms used in\nthe FL scheme. We provide analysis of the achieve privacy and complexity of our\nsolution and, due to the extensive numerical results performed, it can be\nobserved a good trade-off between privacy and precision.",
    "updated" : "2024-05-02T20:03:13Z",
    "published" : "2024-05-02T20:03:13Z",
    "authors" : [
      {
        "name" : "Xavier Martínez Luaña"
      },
      {
        "name" : "Rebeca P. Díaz Redondo"
      },
      {
        "name" : "Manuel Fernández Veiga"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CC",
      "cs.DC",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01678v1",
    "title" : "1-Diffractor: Efficient and Utility-Preserving Text Obfuscation\n  Leveraging Word-Level Metric Differential Privacy",
    "summary" : "The study of privacy-preserving Natural Language Processing (NLP) has gained\nrising attention in recent years. One promising avenue studies the integration\nof Differential Privacy in NLP, which has brought about innovative methods in a\nvariety of application settings. Of particular note are $\\textit{word-level\nMetric Local Differential Privacy (MLDP)}$ mechanisms, which work to obfuscate\npotentially sensitive input text by performing word-by-word\n$\\textit{perturbations}$. Although these methods have shown promising results\nin empirical tests, there are two major drawbacks: (1) the inevitable loss of\nutility due to addition of noise, and (2) the computational expensiveness of\nrunning these mechanisms on high-dimensional word embeddings. In this work, we\naim to address these challenges by proposing $\\texttt{1-Diffractor}$, a new\nmechanism that boasts high speedups in comparison to previous mechanisms, while\nstill demonstrating strong utility- and privacy-preserving capabilities. We\nevaluate $\\texttt{1-Diffractor}$ for utility on several NLP tasks, for\ntheoretical and task-based privacy, and for efficiency in terms of speed and\nmemory. $\\texttt{1-Diffractor}$ shows significant improvements in efficiency,\nwhile still maintaining competitive utility and privacy scores across all\nconducted comparative tests against previous MLDP mechanisms. Our code is made\navailable at: https://github.com/sjmeis/Diffractor.",
    "updated" : "2024-05-02T19:07:32Z",
    "published" : "2024-05-02T19:07:32Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Maulik Chevli"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01646v1",
    "title" : "Explaining models relating objects and privacy",
    "summary" : "Accurately predicting whether an image is private before sharing it online is\ndifficult due to the vast variety of content and the subjective nature of\nprivacy itself. In this paper, we evaluate privacy models that use objects\nextracted from an image to determine why the image is predicted as private. To\nexplain the decision of these models, we use feature-attribution to identify\nand quantify which objects (and which of their features) are more relevant to\nprivacy classification with respect to a reference input (i.e., no objects\nlocalised in an image) predicted as public. We show that the presence of the\nperson category and its cardinality is the main factor for the privacy\ndecision. Therefore, these models mostly fail to identify private images\ndepicting documents with sensitive data, vehicle ownership, and internet\nactivity, or public images with people (e.g., an outdoor concert or people\nwalking in a public space next to a famous landmark). As baselines for future\nbenchmarks, we also devise two strategies that are based on the person presence\nand cardinality and achieve comparable classification performance of the\nprivacy models.",
    "updated" : "2024-05-02T18:06:48Z",
    "published" : "2024-05-02T18:06:48Z",
    "authors" : [
      {
        "name" : "Alessio Xompero"
      },
      {
        "name" : "Myriam Bontonou"
      },
      {
        "name" : "Jean-Michel Arbona"
      },
      {
        "name" : "Emmanouil Benetos"
      },
      {
        "name" : "Andrea Cavallaro"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01031v2",
    "title" : "The Privacy Power of Correlated Noise in Decentralized Learning",
    "summary" : "Decentralized learning is appealing as it enables the scalable usage of large\namounts of distributed data and resources (without resorting to any central\nentity), while promoting privacy since every user minimizes the direct exposure\nof their data. Yet, without additional precautions, curious users can still\nleverage models obtained from their peers to violate privacy. In this paper, we\npropose Decor, a variant of decentralized SGD with differential privacy (DP)\nguarantees. Essentially, in Decor, users securely exchange randomness seeds in\none communication round to generate pairwise-canceling correlated Gaussian\nnoises, which are injected to protect local models at every communication\nround. We theoretically and empirically show that, for arbitrary connected\ngraphs, Decor matches the central DP optimal privacy-utility trade-off. We do\nso under SecLDP, our new relaxation of local DP, which protects all user\ncommunications against an external eavesdropper and curious users, assuming\nthat every pair of connected users shares a secret, i.e., an information hidden\nto all others. The main theoretical challenge is to control the accumulation of\nnon-canceling correlated noise due to network sparsity. We also propose a\ncompanion SecLDP privacy accountant for public use.",
    "updated" : "2024-05-03T08:14:22Z",
    "published" : "2024-05-02T06:14:56Z",
    "authors" : [
      {
        "name" : "Youssef Allouah"
      },
      {
        "name" : "Anastasia Koloskova"
      },
      {
        "name" : "Aymane El Firdoussi"
      },
      {
        "name" : "Martin Jaggi"
      },
      {
        "name" : "Rachid Guerraoui"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "math.OC",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.03636v1",
    "title" : "Federated Learning Privacy: Attacks, Defenses, Applications, and Policy\n  Landscape - A Survey",
    "summary" : "Deep learning has shown incredible potential across a vast array of tasks and\naccompanying this growth has been an insatiable appetite for data. However, a\nlarge amount of data needed for enabling deep learning is stored on personal\ndevices and recent concerns on privacy have further highlighted challenges for\naccessing such data. As a result, federated learning (FL) has emerged as an\nimportant privacy-preserving technology enabling collaborative training of\nmachine learning models without the need to send the raw, potentially\nsensitive, data to a central server. However, the fundamental premise that\nsending model updates to a server is privacy-preserving only holds if the\nupdates cannot be \"reverse engineered\" to infer information about the private\ntraining data. It has been shown under a wide variety of settings that this\npremise for privacy does {\\em not} hold.\n  In this survey paper, we provide a comprehensive literature review of the\ndifferent privacy attacks and defense methods in FL. We identify the current\nlimitations of these attacks and highlight the settings in which FL client\nprivacy can be broken. We dissect some of the successful industry applications\nof FL and draw lessons for future successful adoption. We survey the emerging\nlandscape of privacy regulation for FL. We conclude with future directions for\ntaking FL toward the cherished goal of generating accurate models while\npreserving the privacy of the data from its participants.",
    "updated" : "2024-05-06T16:55:20Z",
    "published" : "2024-05-06T16:55:20Z",
    "authors" : [
      {
        "name" : "Joshua C. Zhao"
      },
      {
        "name" : "Saurabh Bagchi"
      },
      {
        "name" : "Salman Avestimehr"
      },
      {
        "name" : "Kevin S. Chan"
      },
      {
        "name" : "Somali Chaterji"
      },
      {
        "name" : "Dimitris Dimitriadis"
      },
      {
        "name" : "Jiacheng Li"
      },
      {
        "name" : "Ninghui Li"
      },
      {
        "name" : "Arash Nourian"
      },
      {
        "name" : "Holger R. Roth"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "I.2; H.4; I.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.03106v1",
    "title" : "Compression-based Privacy Preservation for Distributed Nash Equilibrium\n  Seeking in Aggregative Games",
    "summary" : "This paper explores distributed aggregative games in multi-agent systems.\nCurrent methods for finding distributed Nash equilibrium require players to\nsend original messages to their neighbors, leading to communication burden and\nprivacy issues. To jointly address these issues, we propose an algorithm that\nuses stochastic compression to save communication resources and conceal\ninformation through random errors induced by compression. Our theoretical\nanalysis shows that the algorithm guarantees convergence accuracy, even with\naggressive compression errors used to protect privacy. We prove that the\nalgorithm achieves differential privacy through a stochastic quantization\nscheme. Simulation results for energy consumption games support the\neffectiveness of our approach.",
    "updated" : "2024-05-06T01:42:40Z",
    "published" : "2024-05-06T01:42:40Z",
    "authors" : [
      {
        "name" : "Wei Huo"
      },
      {
        "name" : "Xiaomeng Chen"
      },
      {
        "name" : "Kemi Ding"
      },
      {
        "name" : "Subhrakanti Dey"
      },
      {
        "name" : "Ling Shi"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.GT",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.03065v1",
    "title" : "Powering the Future of IoT: Federated Learning for Optimized Power\n  Consumption and Enhanced Privacy",
    "summary" : "The widespread use of the Internet of Things has led to the development of\nlarge amounts of perception data, making it necessary to develop effective and\nscalable data analysis tools. Federated Learning emerges as a promising\nparadigm to address the inherent challenges of power consumption and data\nprivacy in IoT environments. This paper explores the transformative potential\nof FL in enhancing the longevity of IoT devices by mitigating power consumption\nand enhancing privacy and security measures. We delve into the intricacies of\nFL, elucidating its components and applications within IoT ecosystems.\nAdditionally, we discuss the critical characteristics and challenges of IoT,\nhighlighting the need for such machine learning solutions in processing\nperception data. While FL introduces many benefits for IoT sustainability, it\nalso has limitations. Through a comprehensive discussion and analysis, this\npaper elucidates the opportunities and constraints of FL in shaping the future\nof sustainable and secure IoT systems. Our findings highlight the importance of\ndeveloping new approaches and conducting additional research to maximise the\nbenefits of FL in creating a secure and privacy-focused IoT environment.",
    "updated" : "2024-05-05T22:18:22Z",
    "published" : "2024-05-05T22:18:22Z",
    "authors" : [
      {
        "name" : "Ghazaleh Shirvani"
      },
      {
        "name" : "Saeid Ghasemshirazi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.02665v1",
    "title" : "Metric Differential Privacy at the User-Level",
    "summary" : "Metric differential privacy (DP) provides heterogeneous privacy guarantees\nbased on a distance between the pair of inputs. It is a widely popular notion\nof privacy since it captures the natural privacy semantics for many\napplications (such as, for location data) and results in better utility than\nstandard DP. However, prior work in metric DP has primarily focused on the\n\\textit{item-level} setting where every user only reports a single data item. A\nmore realistic setting is that of user-level DP where each user contributes\nmultiple items and privacy is then desired at the granularity of the user's\n\\textit{entire} contribution. In this paper, we initiate the study of metric DP\nat the user-level. Specifically, we use the earth-mover's distance\n($d_\\textsf{EM}$) as our metric to obtain a notion of privacy as it captures\nboth the magnitude and spatial aspects of changes in a user's data.\n  We make three main technical contributions. First, we design two novel\nmechanisms under $d_\\textsf{EM}$-DP to answer linear queries and item-wise\nqueries. Specifically, our analysis for the latter involves a generalization of\nthe privacy amplification by shuffling result which may be of independent\ninterest. Second, we provide a black-box reduction from the general unbounded\nto bounded $d_\\textsf{EM}$-DP (size of the dataset is fixed and public) with a\nnovel sampling based mechanism. Third, we show that our proposed mechanisms can\nprovably provide improved utility over user-level DP, for certain types of\nlinear queries and frequency estimation.",
    "updated" : "2024-05-04T13:29:11Z",
    "published" : "2024-05-04T13:29:11Z",
    "authors" : [
      {
        "name" : "Jacob Imola"
      },
      {
        "name" : "Amrita Roy Chowdhury"
      },
      {
        "name" : "Kamalika Chaudhuri"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.02437v1",
    "title" : "FastLloyd: Federated, Accurate, Secure, and Tunable $k$-Means Clustering\n  with Differential Privacy",
    "summary" : "We study the problem of privacy-preserving $k$-means clustering in the\nhorizontally federated setting. Existing federated approaches using secure\ncomputation, suffer from substantial overheads and do not offer output privacy.\nAt the same time, differentially private (DP) $k$-means algorithms assume a\ntrusted central curator and do not extend to federated settings. Naively\ncombining the secure and DP solutions results in a protocol with impractical\noverhead. Instead, our work provides enhancements to both the DP and secure\ncomputation components, resulting in a design that is faster, more private, and\nmore accurate than previous work. By utilizing the computational DP model, we\ndesign a lightweight, secure aggregation-based approach that achieves four\norders of magnitude speed-up over state-of-the-art related work. Furthermore,\nwe not only maintain the utility of the state-of-the-art in the central model\nof DP, but we improve the utility further by taking advantage of constrained\nclustering techniques.",
    "updated" : "2024-05-03T19:04:37Z",
    "published" : "2024-05-03T19:04:37Z",
    "authors" : [
      {
        "name" : "Abdulrahman Diaa"
      },
      {
        "name" : "Thomas Humphries"
      },
      {
        "name" : "Florian Kerschbaum"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.02341v1",
    "title" : "Improved Communication-Privacy Trade-offs in $L_2$ Mean Estimation under\n  Streaming Differential Privacy",
    "summary" : "We study $L_2$ mean estimation under central differential privacy and\ncommunication constraints, and address two key challenges: firstly, existing\nmean estimation schemes that simultaneously handle both constraints are usually\noptimized for $L_\\infty$ geometry and rely on random rotation or Kashin's\nrepresentation to adapt to $L_2$ geometry, resulting in suboptimal leading\nconstants in mean square errors (MSEs); secondly, schemes achieving\norder-optimal communication-privacy trade-offs do not extend seamlessly to\nstreaming differential privacy (DP) settings (e.g., tree aggregation or matrix\nfactorization), rendering them incompatible with DP-FTRL type optimizers.\n  In this work, we tackle these issues by introducing a novel privacy\naccounting method for the sparsified Gaussian mechanism that incorporates the\nrandomness inherent in sparsification into the DP noise. Unlike previous\napproaches, our accounting algorithm directly operates in $L_2$ geometry,\nyielding MSEs that fast converge to those of the uncompressed Gaussian\nmechanism. Additionally, we extend the sparsification scheme to the matrix\nfactorization framework under streaming DP and provide a precise accountant\ntailored for DP-FTRL type optimizers. Empirically, our method demonstrates at\nleast a 100x improvement of compression for DP-SGD across various FL tasks.",
    "updated" : "2024-05-02T03:48:47Z",
    "published" : "2024-05-02T03:48:47Z",
    "authors" : [
      {
        "name" : "Wei-Ning Chen"
      },
      {
        "name" : "Berivan Isik"
      },
      {
        "name" : "Peter Kairouz"
      },
      {
        "name" : "Albert No"
      },
      {
        "name" : "Sewoong Oh"
      },
      {
        "name" : "Zheng Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  }
]