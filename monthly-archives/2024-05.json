[
  {
    "id" : "http://arxiv.org/abs/2405.00616v1",
    "title" : "An Expectation-Maximization Relaxed Method for Privacy Funnel",
    "summary" : "The privacy funnel (PF) gives a framework of privacy-preserving data release,\nwhere the goal is to release useful data while also limiting the exposure of\nassociated sensitive information. This framework has garnered significant\ninterest due to its broad applications in characterization of the\nprivacy-utility tradeoff. Hence, there is a strong motivation to develop\nnumerical methods with high precision and theoretical convergence guarantees.\nIn this paper, we propose a novel relaxation variant based on Jensen's\ninequality of the objective function for the computation of the PF problem.\nThis model is proved to be equivalent to the original in terms of optimal\nsolutions and optimal values. Based on our proposed model, we develop an\naccurate algorithm which only involves closed-form iterations. The convergence\nof our algorithm is theoretically guaranteed through descent estimation and\nPinsker's inequality. Numerical results demonstrate the effectiveness of our\nproposed algorithm.",
    "updated" : "2024-05-01T16:35:44Z",
    "published" : "2024-05-01T16:35:44Z",
    "authors" : [
      {
        "name" : "Lingyi Chen"
      },
      {
        "name" : "Jiachuan Ye"
      },
      {
        "name" : "Shitong Wu"
      },
      {
        "name" : "Huihui Wu"
      },
      {
        "name" : "Hao Wu"
      },
      {
        "name" : "Wenyi Zhang"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.00596v1",
    "title" : "Unbundle-Rewrite-Rebundle: Runtime Detection and Rewriting of\n  Privacy-Harming Code in JavaScript Bundles",
    "summary" : "This work presents Unbundle-Rewrite-Rebundle (URR), a system for detecting\nprivacy-harming portions of bundled JavaScript code, and rewriting that code at\nruntime to remove the privacy harming behavior without breaking the surrounding\ncode or overall application. URR is a novel solution to the problem of\nJavaScript bundles, where websites pre-compile multiple code units into a\nsingle file, making it impossible for content filters and ad-blockers to\ndifferentiate between desired and unwanted resources. Where traditional content\nfiltering tools rely on URLs, URR analyzes the code at the AST level, and\nreplaces harmful AST sub-trees with privacy-and-functionality maintaining\nalternatives.\n  We present an open-sourced implementation of URR as a Firefox extension, and\nevaluate it against JavaScript bundles generated by the most popular bundling\nsystem (Webpack) deployed on the Tranco 10k. We measure the performance,\nmeasured by precision (1.00), recall (0.95), and speed (0.43s per-script) when\ndetecting and rewriting three representative privacy harming libraries often\nincluded in JavaScript bundles, and find URR to be an effective approach to a\nlarge-and-growing blind spot unaddressed by current privacy tools.",
    "updated" : "2024-05-01T16:04:42Z",
    "published" : "2024-05-01T16:04:42Z",
    "authors" : [
      {
        "name" : "Mir Masood Ali"
      },
      {
        "name" : "Peter Snyder"
      },
      {
        "name" : "Chris Kanich"
      },
      {
        "name" : "Hamed Haddadi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.00329v1",
    "title" : "Metric geometry of the privacy-utility tradeoff",
    "summary" : "Synthetic data are an attractive concept to enable privacy in data sharing. A\nfundamental question is how similar the privacy-preserving synthetic data are\ncompared to the true data. Using metric privacy, an effective generalization of\ndifferential privacy beyond the discrete setting, we raise the problem of\ncharacterizing the optimal privacy-accuracy tradeoff by the metric geometry of\nthe underlying space. We provide a partial solution to this problem in terms of\nthe \"entropic scale\", a quantity that captures the multiscale geometry of a\nmetric space via the behavior of its packing numbers. We illustrate the\napplicability of our privacy-accuracy tradeoff framework via a diverse set of\nexamples of metric spaces.",
    "updated" : "2024-05-01T05:31:53Z",
    "published" : "2024-05-01T05:31:53Z",
    "authors" : [
      {
        "name" : "March Boedihardjo"
      },
      {
        "name" : "Thomas Strohmer"
      },
      {
        "name" : "Roman Vershynin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS",
      "math.PR"
    ]
  }
]