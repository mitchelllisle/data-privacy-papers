[
  {
    "id" : "http://arxiv.org/abs/2405.00616v1",
    "title" : "An Expectation-Maximization Relaxed Method for Privacy Funnel",
    "summary" : "The privacy funnel (PF) gives a framework of privacy-preserving data release,\nwhere the goal is to release useful data while also limiting the exposure of\nassociated sensitive information. This framework has garnered significant\ninterest due to its broad applications in characterization of the\nprivacy-utility tradeoff. Hence, there is a strong motivation to develop\nnumerical methods with high precision and theoretical convergence guarantees.\nIn this paper, we propose a novel relaxation variant based on Jensen's\ninequality of the objective function for the computation of the PF problem.\nThis model is proved to be equivalent to the original in terms of optimal\nsolutions and optimal values. Based on our proposed model, we develop an\naccurate algorithm which only involves closed-form iterations. The convergence\nof our algorithm is theoretically guaranteed through descent estimation and\nPinsker's inequality. Numerical results demonstrate the effectiveness of our\nproposed algorithm.",
    "updated" : "2024-05-01T16:35:44Z",
    "published" : "2024-05-01T16:35:44Z",
    "authors" : [
      {
        "name" : "Lingyi Chen"
      },
      {
        "name" : "Jiachuan Ye"
      },
      {
        "name" : "Shitong Wu"
      },
      {
        "name" : "Huihui Wu"
      },
      {
        "name" : "Hao Wu"
      },
      {
        "name" : "Wenyi Zhang"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.00596v1",
    "title" : "Unbundle-Rewrite-Rebundle: Runtime Detection and Rewriting of\n  Privacy-Harming Code in JavaScript Bundles",
    "summary" : "This work presents Unbundle-Rewrite-Rebundle (URR), a system for detecting\nprivacy-harming portions of bundled JavaScript code, and rewriting that code at\nruntime to remove the privacy harming behavior without breaking the surrounding\ncode or overall application. URR is a novel solution to the problem of\nJavaScript bundles, where websites pre-compile multiple code units into a\nsingle file, making it impossible for content filters and ad-blockers to\ndifferentiate between desired and unwanted resources. Where traditional content\nfiltering tools rely on URLs, URR analyzes the code at the AST level, and\nreplaces harmful AST sub-trees with privacy-and-functionality maintaining\nalternatives.\n  We present an open-sourced implementation of URR as a Firefox extension, and\nevaluate it against JavaScript bundles generated by the most popular bundling\nsystem (Webpack) deployed on the Tranco 10k. We measure the performance,\nmeasured by precision (1.00), recall (0.95), and speed (0.43s per-script) when\ndetecting and rewriting three representative privacy harming libraries often\nincluded in JavaScript bundles, and find URR to be an effective approach to a\nlarge-and-growing blind spot unaddressed by current privacy tools.",
    "updated" : "2024-05-01T16:04:42Z",
    "published" : "2024-05-01T16:04:42Z",
    "authors" : [
      {
        "name" : "Mir Masood Ali"
      },
      {
        "name" : "Peter Snyder"
      },
      {
        "name" : "Chris Kanich"
      },
      {
        "name" : "Hamed Haddadi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.00329v1",
    "title" : "Metric geometry of the privacy-utility tradeoff",
    "summary" : "Synthetic data are an attractive concept to enable privacy in data sharing. A\nfundamental question is how similar the privacy-preserving synthetic data are\ncompared to the true data. Using metric privacy, an effective generalization of\ndifferential privacy beyond the discrete setting, we raise the problem of\ncharacterizing the optimal privacy-accuracy tradeoff by the metric geometry of\nthe underlying space. We provide a partial solution to this problem in terms of\nthe \"entropic scale\", a quantity that captures the multiscale geometry of a\nmetric space via the behavior of its packing numbers. We illustrate the\napplicability of our privacy-accuracy tradeoff framework via a diverse set of\nexamples of metric spaces.",
    "updated" : "2024-05-01T05:31:53Z",
    "published" : "2024-05-01T05:31:53Z",
    "authors" : [
      {
        "name" : "March Boedihardjo"
      },
      {
        "name" : "Thomas Strohmer"
      },
      {
        "name" : "Roman Vershynin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS",
      "math.PR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01494v1",
    "title" : "Navigating Heterogeneity and Privacy in One-Shot Federated Learning with\n  Diffusion Models",
    "summary" : "Federated learning (FL) enables multiple clients to train models collectively\nwhile preserving data privacy. However, FL faces challenges in terms of\ncommunication cost and data heterogeneity. One-shot federated learning has\nemerged as a solution by reducing communication rounds, improving efficiency,\nand providing better security against eavesdropping attacks. Nevertheless, data\nheterogeneity remains a significant challenge, impacting performance. This work\nexplores the effectiveness of diffusion models in one-shot FL, demonstrating\ntheir applicability in addressing data heterogeneity and improving FL\nperformance. Additionally, we investigate the utility of our diffusion model\napproach, FedDiff, compared to other one-shot FL methods under differential\nprivacy (DP). Furthermore, to improve generated sample quality under DP\nsettings, we propose a pragmatic Fourier Magnitude Filtering (FMF) method,\nenhancing the effectiveness of generated data for global model training.",
    "updated" : "2024-05-02T17:26:52Z",
    "published" : "2024-05-02T17:26:52Z",
    "authors" : [
      {
        "name" : "Matias Mendieta"
      },
      {
        "name" : "Guangyu Sun"
      },
      {
        "name" : "Chen Chen"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01492v1",
    "title" : "Exploring Privacy Issues in Mission Critical Communication: Navigating\n  5G and Beyond Networks",
    "summary" : "Mission critical communication (MCC) involves the exchange of information and\ndata among emergency services, including the police, fire brigade, and other\nfirst responders, particularly during emergencies, disasters, or critical\nincidents. The widely-adopted TETRA (Terrestrial Trunked Radio)-based\ncommunication for mission critical services faces challenges including limited\ndata capacity, coverage limitations, spectrum congestion, and security\nconcerns. Therefore, as an alternative, mission critical communication over\ncellular networks (4G and 5G) has emerged. While cellular-based MCC enables\nfeatures like real-time video streaming and high-speed data transmission, the\ninvolvement of network operators and application service providers in the MCC\narchitecture raises privacy concerns for mission critical users and services.\nFor instance, the disclosure of a policeman's location details to the network\noperator raises privacy concerns. To the best of our knowledge, no existing\nwork considers the privacy issues in mission critical system with respect to 5G\nand upcoming technologies. Therefore, in this paper, we analyse the 3GPP\nstandardised MCC architecture within the context of 5G core network concepts\nand assess the privacy implications for MC users, network entities, and MC\nservers. The privacy analysis adheres to the deployment strategies in the\nstandard for MCC. Additionally, we explore emerging 6G technologies, such as\noff-network communications, joint communication and sensing, and non-3GPP\ncommunications, to identify privacy challenges in MCC architecture. Finally, we\npropose privacy controls to establish a next-generation privacy-preserving MCC\narchitecture.",
    "updated" : "2024-05-02T17:25:33Z",
    "published" : "2024-05-02T17:25:33Z",
    "authors" : [
      {
        "name" : "Prajnamaya Dass"
      },
      {
        "name" : "Marcel Gräfenstein"
      },
      {
        "name" : "Stefan Köpsell"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01411v1",
    "title" : "IDPFilter: Mitigating Interdependent Privacy Issues in Third-Party Apps",
    "summary" : "Third-party applications have become an essential part of today's online\necosystem, enhancing the functionality of popular platforms. However, the\nintensive data exchange underlying their proliferation has increased concerns\nabout interdependent privacy (IDP). This paper provides a comprehensive\ninvestigation into the previously underinvestigated IDP issues of third-party\napps. Specifically, first, we analyze the permission structure of multiple app\nplatforms, identifying permissions that have the potential to cause\ninterdependent privacy issues by enabling a user to share someone else's\npersonal data with an app. Second, we collect datasets and characterize the\nextent to which existing apps request these permissions, revealing the\nrelationship between characteristics such as the respective app platform, the\napp's type, and the number of interdependent privacy-related permissions it\nrequests. Third, we analyze the various reasons IDP is neglected by both data\nprotection regulations and app platforms and then devise principles that should\nbe followed when designing a mitigation solution. Finally, based on these\nprinciples and satisfying clearly defined objectives, we propose IDPFilter, a\nplatform-agnostic API that enables application providers to minimize collateral\ninformation collection by filtering out data collected from their users but\nimplicating others as data subjects. We implement a proof-of-concept prototype,\nIDPTextFilter, that implements the filtering logic on textual data, and provide\nits initial performance evaluation with regard to privacy, accuracy, and\nefficiency.",
    "updated" : "2024-05-02T16:02:13Z",
    "published" : "2024-05-02T16:02:13Z",
    "authors" : [
      {
        "name" : "Shuaishuai Liu"
      },
      {
        "name" : "Gergely Biczók"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01312v1",
    "title" : "Privacy-Enhanced Database Synthesis for Benchmark Publishing",
    "summary" : "Benchmarking is crucial for evaluating a DBMS, yet existing benchmarks often\nfail to reflect the varied nature of user workloads. As a result, there is\nincreasing momentum toward creating databases that incorporate real-world user\ndata to more accurately mirror business environments. However, privacy concerns\ndeter users from directly sharing their data, underscoring the importance of\ncreating synthesized databases for benchmarking that also prioritize privacy\nprotection. Differential privacy has become a key method for safeguarding\nprivacy when sharing data, but the focus has largely been on minimizing errors\nin aggregate queries or classification tasks, with less attention given to\nbenchmarking factors like runtime performance. This paper delves into the\ncreation of privacy-preserving databases specifically for benchmarking, aiming\nto produce a differentially private database whose query performance closely\nresembles that of the original data. Introducing PrivBench, an innovative\nsynthesis framework, we support the generation of high-quality data that\nmaintains privacy. PrivBench uses sum-product networks (SPNs) to partition and\nsample data, enhancing data representation while securing privacy. The\nframework allows users to adjust the detail of SPN partitions and privacy\nsettings, crucial for customizing privacy levels. We validate our approach,\nwhich uses the Laplace and exponential mechanisms, in maintaining privacy. Our\ntests show that PrivBench effectively generates data that maintains privacy and\nexcels in query performance, consistently reducing errors in query execution\ntime, query cardinality, and KL divergence.",
    "updated" : "2024-05-02T14:20:24Z",
    "published" : "2024-05-02T14:20:24Z",
    "authors" : [
      {
        "name" : "Yongrui Zhong"
      },
      {
        "name" : "Yunqing Ge"
      },
      {
        "name" : "Jianbin Qin"
      },
      {
        "name" : "Shuyuan Zheng"
      },
      {
        "name" : "Bo Tang"
      },
      {
        "name" : "Yu-Xuan Qiu"
      },
      {
        "name" : "Rui Mao"
      },
      {
        "name" : "Ye Yuan"
      },
      {
        "name" : "Makoto Onizuka"
      },
      {
        "name" : "Chuan Xiao"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01221v1",
    "title" : "A Survey on Semantic Communication Networks: Architecture, Security, and\n  Privacy",
    "summary" : "Semantic communication, emerging as a breakthrough beyond the classical\nShannon paradigm, aims to convey the essential meaning of source data rather\nthan merely focusing on precise yet content-agnostic bit transmission. By\ninterconnecting diverse intelligent agents (e.g., autonomous vehicles and VR\ndevices) via semantic communications, the semantic communication networks\n(SemComNet) supports semantic-oriented transmission, efficient spectrum\nutilization, and flexible networking among collaborative agents. Consequently,\nSemComNet stands out for enabling ever-increasing intelligent applications,\nsuch as autonomous driving and Metaverse. However, being built on a variety of\ncutting-edge technologies including AI and knowledge graphs, SemComNet\nintroduces diverse brand-new and unexpected threats, which pose obstacles to\nits widespread development. Besides, due to the intrinsic characteristics of\nSemComNet in terms of heterogeneous components, autonomous intelligence, and\nlarge-scale structure, a series of critical challenges emerge in securing\nSemComNet. In this paper, we provide a comprehensive and up-to-date survey of\nSemComNet from its fundamentals, security, and privacy aspects. Specifically,\nwe first introduce a novel three-layer architecture of SemComNet for\nmulti-agent interaction, which comprises the control layer, semantic\ntransmission layer, and cognitive sensing layer. Then, we discuss its working\nmodes and enabling technologies. Afterward, based on the layered architecture\nof SemComNet, we outline a taxonomy of security and privacy threats, while\ndiscussing state-of-the-art defense approaches. Finally, we present future\nresearch directions, clarifying the path toward building intelligent, robust,\nand green SemComNet. To our knowledge, this survey is the first to\ncomprehensively cover the fundamentals of SemComNet, alongside a detailed\nanalysis of its security and privacy issues.",
    "updated" : "2024-05-02T12:04:35Z",
    "published" : "2024-05-02T12:04:35Z",
    "authors" : [
      {
        "name" : "Shaolong Guo"
      },
      {
        "name" : "Yuntao Wang"
      },
      {
        "name" : "Ning Zhang"
      },
      {
        "name" : "Zhou Su"
      },
      {
        "name" : "Tom H. Luan"
      },
      {
        "name" : "Zhiyi Tian"
      },
      {
        "name" : "Xuemin Shen"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01031v1",
    "title" : "The Privacy Power of Correlated Noise in Decentralized Learning",
    "summary" : "Decentralized learning is appealing as it enables the scalable usage of large\namounts of distributed data and resources (without resorting to any central\nentity), while promoting privacy since every user minimizes the direct exposure\nof their data. Yet, without additional precautions, curious users can still\nleverage models obtained from their peers to violate privacy. In this paper, we\npropose Decor, a variant of decentralized SGD with differential privacy (DP)\nguarantees. Essentially, in Decor, users securely exchange randomness seeds in\none communication round to generate pairwise-canceling correlated Gaussian\nnoises, which are injected to protect local models at every communication\nround. We theoretically and empirically show that, for arbitrary connected\ngraphs, Decor matches the central DP optimal privacy-utility trade-off. We do\nso under SecLDP, our new relaxation of local DP, which protects all user\ncommunications against an external eavesdropper and curious users, assuming\nthat every pair of connected users shares a secret, i.e., an information hidden\nto all others. The main theoretical challenge is to control the accumulation of\nnon-canceling correlated noise due to network sparsity. We also propose a\ncompanion SecLDP privacy accountant for public use.",
    "updated" : "2024-05-02T06:14:56Z",
    "published" : "2024-05-02T06:14:56Z",
    "authors" : [
      {
        "name" : "Youssef Allouah"
      },
      {
        "name" : "Anastasia Koloskova"
      },
      {
        "name" : "Aymane El Firdoussi"
      },
      {
        "name" : "Martin Jaggi"
      },
      {
        "name" : "Rachid Guerraoui"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "math.OC",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01844v1",
    "title" : "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
    "summary" : "Caching content at the network edge is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the network edge. On the one\nhand, the multi-access open edge network provides an ideal surface for external\nattackers to obtain private data from the edge cache by extracting sensitive\ninformation. On the other hand, privacy can be infringed by curious edge\ncaching providers through caching trace analysis targeting to achieve better\ncaching performance or higher profits. Therefore, an in-depth understanding of\nprivacy issues in edge caching networks is vital and indispensable for creating\na privacy-preserving caching service at the network edge. In this article, we\nare among the first to fill in this gap by examining privacy-preserving\ntechniques for caching content at the network edge. Firstly, we provide an\nintroduction to the background of Privacy-Preserving Edge Caching (PPEC). Next,\nwe summarize the key privacy issues and present a taxonomy for caching at the\nnetwork edge from the perspective of private data. Additionally, we conduct a\nretrospective review of the state-of-the-art countermeasures against privacy\nleakage from content caching at the network edge. Finally, we conclude the\nsurvey and envision challenges for future research.",
    "updated" : "2024-05-03T04:27:32Z",
    "published" : "2024-05-03T04:27:32Z",
    "authors" : [
      {
        "name" : "Xianzhi Zhang"
      },
      {
        "name" : "Yipeng Zhou"
      },
      {
        "name" : "Di Wu"
      },
      {
        "name" : "Shazia Riaz"
      },
      {
        "name" : "Quan Z. Sheng"
      },
      {
        "name" : "Miao Hu"
      },
      {
        "name" : "Linchang Xiao"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01742v1",
    "title" : "Addressing Privacy Concerns in Joint Communication and Sensing for 6G\n  Networks: Challenges and Prospects",
    "summary" : "The vision for 6G extends beyond mere communication, incorporating sensing\ncapabilities to facilitate a diverse array of novel applications and services.\nHowever, the advent of joint communication and sensing (JCAS) technology\nintroduces concerns regarding the handling of sensitive personally identifiable\ninformation (PII) pertaining to individuals and objects, along with external\nthird-party data and disclosure. Consequently, JCAS-based applications are\nsusceptible to privacy breaches, including location tracking, identity\ndisclosure, profiling, and misuse of sensor data, raising significant\nimplications under the European Union's General Data Protection Regulation\n(GDPR) as well as other applicable standards. This paper critically examines\nemergent JCAS architectures and underscores the necessity for network functions\nto enable privacy-specific features in the 6G systems. We propose an enhanced\nJCAS architecture with additional network functions and interfaces,\nfacilitating the management of sensing policies, consent information, and\ntransparency guidelines, alongside the integration of sensing-specific\nfunctions and storage for sensing processing sessions. Furthermore, we conduct\na comprehensive threat analysis for all interfaces, employing security threat\nmodel STRIDE and privacy threat model LINDDUN. We also summarise the identified\nthreats using standard Common Weakness Enumerations (CWEs). Finally, we suggest\nthe security and privacy controls as the mitigating strategies to counter the\nidentified threats stemming from the JCAS architecture.",
    "updated" : "2024-05-02T21:25:24Z",
    "published" : "2024-05-02T21:25:24Z",
    "authors" : [
      {
        "name" : "Prajnamaya Dass"
      },
      {
        "name" : "Sonika Ujjwal"
      },
      {
        "name" : "Jiri Novotny"
      },
      {
        "name" : "Yevhen Zolotavkin"
      },
      {
        "name" : "Zakaria Laaroussi"
      },
      {
        "name" : "Stefan Köpsell"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01716v1",
    "title" : "ATTAXONOMY: Unpacking Differential Privacy Guarantees Against Practical\n  Adversaries",
    "summary" : "Differential Privacy (DP) is a mathematical framework that is increasingly\ndeployed to mitigate privacy risks associated with machine learning and\nstatistical analyses. Despite the growing adoption of DP, its technical privacy\nparameters do not lend themselves to an intelligible description of the\nreal-world privacy risks associated with that deployment: the guarantee that\nmost naturally follows from the DP definition is protection against membership\ninference by an adversary who knows all but one data record and has unlimited\nauxiliary knowledge. In many settings, this adversary is far too strong to\ninform how to set real-world privacy parameters.\n  One approach for contextualizing privacy parameters is via defining and\nmeasuring the success of technical attacks, but doing so requires a systematic\ncategorization of the relevant attack space. In this work, we offer a detailed\ntaxonomy of attacks, showing the various dimensions of attacks and highlighting\nthat many real-world settings have been understudied. Our taxonomy provides a\nroadmap for analyzing real-world deployments and developing theoretical bounds\nfor more informative privacy attacks. We operationalize our taxonomy by using\nit to analyze a real-world case study, the Israeli Ministry of Health's recent\nrelease of a birth dataset using DP, showing how the taxonomy enables\nfine-grained threat modeling and provides insight towards making informed\nprivacy parameter choices. Finally, we leverage the taxonomy towards defining a\nmore realistic attack than previously considered in the literature, namely a\ndistributional reconstruction attack: we generalize Balle et al.'s notion of\nreconstruction robustness to a less-informed adversary with distributional\nuncertainty, and extend the worst-case guarantees of DP to this average-case\nsetting.",
    "updated" : "2024-05-02T20:23:23Z",
    "published" : "2024-05-02T20:23:23Z",
    "authors" : [
      {
        "name" : "Rachel Cummings"
      },
      {
        "name" : "Shlomi Hod"
      },
      {
        "name" : "Jayshree Sarathy"
      },
      {
        "name" : "Marika Swanberg"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01704v1",
    "title" : "Privacy-aware Berrut Approximated Coded Computing for Federated Learning",
    "summary" : "Federated Learning (FL) is an interesting strategy that enables the\ncollaborative training of an AI model among different data owners without\nrevealing their private datasets. Even so, FL has some privacy vulnerabilities\nthat have been tried to be overcome by applying some techniques like\nDifferential Privacy (DP), Homomorphic Encryption (HE), or Secure Multi-Party\nComputation (SMPC). However, these techniques have some important drawbacks\nthat might narrow their range of application: problems to work with non-linear\nfunctions and to operate large matrix multiplications and high communication\nand computational costs to manage semi-honest nodes. In this context, we\npropose a solution to guarantee privacy in FL schemes that simultaneously\nsolves the previously mentioned problems. Our proposal is based on the Berrut\nApproximated Coded Computing, a technique from the Coded Distributed Computing\nparadigm, adapted to a Secret Sharing configuration, to provide input privacy\nto FL in a scalable way. It can be applied for computing non-linear functions\nand treats the special case of distributed matrix multiplication, a key\nprimitive at the core of many automated learning tasks. Because of these\ncharacteristics, it could be applied in a wide range of FL scenarios, since it\nis independent of the machine learning models or aggregation algorithms used in\nthe FL scheme. We provide analysis of the achieve privacy and complexity of our\nsolution and, due to the extensive numerical results performed, it can be\nobserved a good trade-off between privacy and precision.",
    "updated" : "2024-05-02T20:03:13Z",
    "published" : "2024-05-02T20:03:13Z",
    "authors" : [
      {
        "name" : "Xavier Martínez Luaña"
      },
      {
        "name" : "Rebeca P. Díaz Redondo"
      },
      {
        "name" : "Manuel Fernández Veiga"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CC",
      "cs.DC",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01678v1",
    "title" : "1-Diffractor: Efficient and Utility-Preserving Text Obfuscation\n  Leveraging Word-Level Metric Differential Privacy",
    "summary" : "The study of privacy-preserving Natural Language Processing (NLP) has gained\nrising attention in recent years. One promising avenue studies the integration\nof Differential Privacy in NLP, which has brought about innovative methods in a\nvariety of application settings. Of particular note are $\\textit{word-level\nMetric Local Differential Privacy (MLDP)}$ mechanisms, which work to obfuscate\npotentially sensitive input text by performing word-by-word\n$\\textit{perturbations}$. Although these methods have shown promising results\nin empirical tests, there are two major drawbacks: (1) the inevitable loss of\nutility due to addition of noise, and (2) the computational expensiveness of\nrunning these mechanisms on high-dimensional word embeddings. In this work, we\naim to address these challenges by proposing $\\texttt{1-Diffractor}$, a new\nmechanism that boasts high speedups in comparison to previous mechanisms, while\nstill demonstrating strong utility- and privacy-preserving capabilities. We\nevaluate $\\texttt{1-Diffractor}$ for utility on several NLP tasks, for\ntheoretical and task-based privacy, and for efficiency in terms of speed and\nmemory. $\\texttt{1-Diffractor}$ shows significant improvements in efficiency,\nwhile still maintaining competitive utility and privacy scores across all\nconducted comparative tests against previous MLDP mechanisms. Our code is made\navailable at: https://github.com/sjmeis/Diffractor.",
    "updated" : "2024-05-02T19:07:32Z",
    "published" : "2024-05-02T19:07:32Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Maulik Chevli"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01646v1",
    "title" : "Explaining models relating objects and privacy",
    "summary" : "Accurately predicting whether an image is private before sharing it online is\ndifficult due to the vast variety of content and the subjective nature of\nprivacy itself. In this paper, we evaluate privacy models that use objects\nextracted from an image to determine why the image is predicted as private. To\nexplain the decision of these models, we use feature-attribution to identify\nand quantify which objects (and which of their features) are more relevant to\nprivacy classification with respect to a reference input (i.e., no objects\nlocalised in an image) predicted as public. We show that the presence of the\nperson category and its cardinality is the main factor for the privacy\ndecision. Therefore, these models mostly fail to identify private images\ndepicting documents with sensitive data, vehicle ownership, and internet\nactivity, or public images with people (e.g., an outdoor concert or people\nwalking in a public space next to a famous landmark). As baselines for future\nbenchmarks, we also devise two strategies that are based on the person presence\nand cardinality and achieve comparable classification performance of the\nprivacy models.",
    "updated" : "2024-05-02T18:06:48Z",
    "published" : "2024-05-02T18:06:48Z",
    "authors" : [
      {
        "name" : "Alessio Xompero"
      },
      {
        "name" : "Myriam Bontonou"
      },
      {
        "name" : "Jean-Michel Arbona"
      },
      {
        "name" : "Emmanouil Benetos"
      },
      {
        "name" : "Andrea Cavallaro"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01031v2",
    "title" : "The Privacy Power of Correlated Noise in Decentralized Learning",
    "summary" : "Decentralized learning is appealing as it enables the scalable usage of large\namounts of distributed data and resources (without resorting to any central\nentity), while promoting privacy since every user minimizes the direct exposure\nof their data. Yet, without additional precautions, curious users can still\nleverage models obtained from their peers to violate privacy. In this paper, we\npropose Decor, a variant of decentralized SGD with differential privacy (DP)\nguarantees. Essentially, in Decor, users securely exchange randomness seeds in\none communication round to generate pairwise-canceling correlated Gaussian\nnoises, which are injected to protect local models at every communication\nround. We theoretically and empirically show that, for arbitrary connected\ngraphs, Decor matches the central DP optimal privacy-utility trade-off. We do\nso under SecLDP, our new relaxation of local DP, which protects all user\ncommunications against an external eavesdropper and curious users, assuming\nthat every pair of connected users shares a secret, i.e., an information hidden\nto all others. The main theoretical challenge is to control the accumulation of\nnon-canceling correlated noise due to network sparsity. We also propose a\ncompanion SecLDP privacy accountant for public use.",
    "updated" : "2024-05-03T08:14:22Z",
    "published" : "2024-05-02T06:14:56Z",
    "authors" : [
      {
        "name" : "Youssef Allouah"
      },
      {
        "name" : "Anastasia Koloskova"
      },
      {
        "name" : "Aymane El Firdoussi"
      },
      {
        "name" : "Martin Jaggi"
      },
      {
        "name" : "Rachid Guerraoui"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "math.OC",
      "stat.ML"
    ]
  }
]