[
  {
    "id" : "http://arxiv.org/abs/2405.00616v1",
    "title" : "An Expectation-Maximization Relaxed Method for Privacy Funnel",
    "summary" : "The privacy funnel (PF) gives a framework of privacy-preserving data release,\nwhere the goal is to release useful data while also limiting the exposure of\nassociated sensitive information. This framework has garnered significant\ninterest due to its broad applications in characterization of the\nprivacy-utility tradeoff. Hence, there is a strong motivation to develop\nnumerical methods with high precision and theoretical convergence guarantees.\nIn this paper, we propose a novel relaxation variant based on Jensen's\ninequality of the objective function for the computation of the PF problem.\nThis model is proved to be equivalent to the original in terms of optimal\nsolutions and optimal values. Based on our proposed model, we develop an\naccurate algorithm which only involves closed-form iterations. The convergence\nof our algorithm is theoretically guaranteed through descent estimation and\nPinsker's inequality. Numerical results demonstrate the effectiveness of our\nproposed algorithm.",
    "updated" : "2024-05-01T16:35:44Z",
    "published" : "2024-05-01T16:35:44Z",
    "authors" : [
      {
        "name" : "Lingyi Chen"
      },
      {
        "name" : "Jiachuan Ye"
      },
      {
        "name" : "Shitong Wu"
      },
      {
        "name" : "Huihui Wu"
      },
      {
        "name" : "Hao Wu"
      },
      {
        "name" : "Wenyi Zhang"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.00596v1",
    "title" : "Unbundle-Rewrite-Rebundle: Runtime Detection and Rewriting of\n  Privacy-Harming Code in JavaScript Bundles",
    "summary" : "This work presents Unbundle-Rewrite-Rebundle (URR), a system for detecting\nprivacy-harming portions of bundled JavaScript code, and rewriting that code at\nruntime to remove the privacy harming behavior without breaking the surrounding\ncode or overall application. URR is a novel solution to the problem of\nJavaScript bundles, where websites pre-compile multiple code units into a\nsingle file, making it impossible for content filters and ad-blockers to\ndifferentiate between desired and unwanted resources. Where traditional content\nfiltering tools rely on URLs, URR analyzes the code at the AST level, and\nreplaces harmful AST sub-trees with privacy-and-functionality maintaining\nalternatives.\n  We present an open-sourced implementation of URR as a Firefox extension, and\nevaluate it against JavaScript bundles generated by the most popular bundling\nsystem (Webpack) deployed on the Tranco 10k. We measure the performance,\nmeasured by precision (1.00), recall (0.95), and speed (0.43s per-script) when\ndetecting and rewriting three representative privacy harming libraries often\nincluded in JavaScript bundles, and find URR to be an effective approach to a\nlarge-and-growing blind spot unaddressed by current privacy tools.",
    "updated" : "2024-05-01T16:04:42Z",
    "published" : "2024-05-01T16:04:42Z",
    "authors" : [
      {
        "name" : "Mir Masood Ali"
      },
      {
        "name" : "Peter Snyder"
      },
      {
        "name" : "Chris Kanich"
      },
      {
        "name" : "Hamed Haddadi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.00329v1",
    "title" : "Metric geometry of the privacy-utility tradeoff",
    "summary" : "Synthetic data are an attractive concept to enable privacy in data sharing. A\nfundamental question is how similar the privacy-preserving synthetic data are\ncompared to the true data. Using metric privacy, an effective generalization of\ndifferential privacy beyond the discrete setting, we raise the problem of\ncharacterizing the optimal privacy-accuracy tradeoff by the metric geometry of\nthe underlying space. We provide a partial solution to this problem in terms of\nthe \"entropic scale\", a quantity that captures the multiscale geometry of a\nmetric space via the behavior of its packing numbers. We illustrate the\napplicability of our privacy-accuracy tradeoff framework via a diverse set of\nexamples of metric spaces.",
    "updated" : "2024-05-01T05:31:53Z",
    "published" : "2024-05-01T05:31:53Z",
    "authors" : [
      {
        "name" : "March Boedihardjo"
      },
      {
        "name" : "Thomas Strohmer"
      },
      {
        "name" : "Roman Vershynin"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS",
      "math.PR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01494v1",
    "title" : "Navigating Heterogeneity and Privacy in One-Shot Federated Learning with\n  Diffusion Models",
    "summary" : "Federated learning (FL) enables multiple clients to train models collectively\nwhile preserving data privacy. However, FL faces challenges in terms of\ncommunication cost and data heterogeneity. One-shot federated learning has\nemerged as a solution by reducing communication rounds, improving efficiency,\nand providing better security against eavesdropping attacks. Nevertheless, data\nheterogeneity remains a significant challenge, impacting performance. This work\nexplores the effectiveness of diffusion models in one-shot FL, demonstrating\ntheir applicability in addressing data heterogeneity and improving FL\nperformance. Additionally, we investigate the utility of our diffusion model\napproach, FedDiff, compared to other one-shot FL methods under differential\nprivacy (DP). Furthermore, to improve generated sample quality under DP\nsettings, we propose a pragmatic Fourier Magnitude Filtering (FMF) method,\nenhancing the effectiveness of generated data for global model training.",
    "updated" : "2024-05-02T17:26:52Z",
    "published" : "2024-05-02T17:26:52Z",
    "authors" : [
      {
        "name" : "Matias Mendieta"
      },
      {
        "name" : "Guangyu Sun"
      },
      {
        "name" : "Chen Chen"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01492v1",
    "title" : "Exploring Privacy Issues in Mission Critical Communication: Navigating\n  5G and Beyond Networks",
    "summary" : "Mission critical communication (MCC) involves the exchange of information and\ndata among emergency services, including the police, fire brigade, and other\nfirst responders, particularly during emergencies, disasters, or critical\nincidents. The widely-adopted TETRA (Terrestrial Trunked Radio)-based\ncommunication for mission critical services faces challenges including limited\ndata capacity, coverage limitations, spectrum congestion, and security\nconcerns. Therefore, as an alternative, mission critical communication over\ncellular networks (4G and 5G) has emerged. While cellular-based MCC enables\nfeatures like real-time video streaming and high-speed data transmission, the\ninvolvement of network operators and application service providers in the MCC\narchitecture raises privacy concerns for mission critical users and services.\nFor instance, the disclosure of a policeman's location details to the network\noperator raises privacy concerns. To the best of our knowledge, no existing\nwork considers the privacy issues in mission critical system with respect to 5G\nand upcoming technologies. Therefore, in this paper, we analyse the 3GPP\nstandardised MCC architecture within the context of 5G core network concepts\nand assess the privacy implications for MC users, network entities, and MC\nservers. The privacy analysis adheres to the deployment strategies in the\nstandard for MCC. Additionally, we explore emerging 6G technologies, such as\noff-network communications, joint communication and sensing, and non-3GPP\ncommunications, to identify privacy challenges in MCC architecture. Finally, we\npropose privacy controls to establish a next-generation privacy-preserving MCC\narchitecture.",
    "updated" : "2024-05-02T17:25:33Z",
    "published" : "2024-05-02T17:25:33Z",
    "authors" : [
      {
        "name" : "Prajnamaya Dass"
      },
      {
        "name" : "Marcel Gräfenstein"
      },
      {
        "name" : "Stefan Köpsell"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01411v1",
    "title" : "IDPFilter: Mitigating Interdependent Privacy Issues in Third-Party Apps",
    "summary" : "Third-party applications have become an essential part of today's online\necosystem, enhancing the functionality of popular platforms. However, the\nintensive data exchange underlying their proliferation has increased concerns\nabout interdependent privacy (IDP). This paper provides a comprehensive\ninvestigation into the previously underinvestigated IDP issues of third-party\napps. Specifically, first, we analyze the permission structure of multiple app\nplatforms, identifying permissions that have the potential to cause\ninterdependent privacy issues by enabling a user to share someone else's\npersonal data with an app. Second, we collect datasets and characterize the\nextent to which existing apps request these permissions, revealing the\nrelationship between characteristics such as the respective app platform, the\napp's type, and the number of interdependent privacy-related permissions it\nrequests. Third, we analyze the various reasons IDP is neglected by both data\nprotection regulations and app platforms and then devise principles that should\nbe followed when designing a mitigation solution. Finally, based on these\nprinciples and satisfying clearly defined objectives, we propose IDPFilter, a\nplatform-agnostic API that enables application providers to minimize collateral\ninformation collection by filtering out data collected from their users but\nimplicating others as data subjects. We implement a proof-of-concept prototype,\nIDPTextFilter, that implements the filtering logic on textual data, and provide\nits initial performance evaluation with regard to privacy, accuracy, and\nefficiency.",
    "updated" : "2024-05-02T16:02:13Z",
    "published" : "2024-05-02T16:02:13Z",
    "authors" : [
      {
        "name" : "Shuaishuai Liu"
      },
      {
        "name" : "Gergely Biczók"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01312v1",
    "title" : "Privacy-Enhanced Database Synthesis for Benchmark Publishing",
    "summary" : "Benchmarking is crucial for evaluating a DBMS, yet existing benchmarks often\nfail to reflect the varied nature of user workloads. As a result, there is\nincreasing momentum toward creating databases that incorporate real-world user\ndata to more accurately mirror business environments. However, privacy concerns\ndeter users from directly sharing their data, underscoring the importance of\ncreating synthesized databases for benchmarking that also prioritize privacy\nprotection. Differential privacy has become a key method for safeguarding\nprivacy when sharing data, but the focus has largely been on minimizing errors\nin aggregate queries or classification tasks, with less attention given to\nbenchmarking factors like runtime performance. This paper delves into the\ncreation of privacy-preserving databases specifically for benchmarking, aiming\nto produce a differentially private database whose query performance closely\nresembles that of the original data. Introducing PrivBench, an innovative\nsynthesis framework, we support the generation of high-quality data that\nmaintains privacy. PrivBench uses sum-product networks (SPNs) to partition and\nsample data, enhancing data representation while securing privacy. The\nframework allows users to adjust the detail of SPN partitions and privacy\nsettings, crucial for customizing privacy levels. We validate our approach,\nwhich uses the Laplace and exponential mechanisms, in maintaining privacy. Our\ntests show that PrivBench effectively generates data that maintains privacy and\nexcels in query performance, consistently reducing errors in query execution\ntime, query cardinality, and KL divergence.",
    "updated" : "2024-05-02T14:20:24Z",
    "published" : "2024-05-02T14:20:24Z",
    "authors" : [
      {
        "name" : "Yongrui Zhong"
      },
      {
        "name" : "Yunqing Ge"
      },
      {
        "name" : "Jianbin Qin"
      },
      {
        "name" : "Shuyuan Zheng"
      },
      {
        "name" : "Bo Tang"
      },
      {
        "name" : "Yu-Xuan Qiu"
      },
      {
        "name" : "Rui Mao"
      },
      {
        "name" : "Ye Yuan"
      },
      {
        "name" : "Makoto Onizuka"
      },
      {
        "name" : "Chuan Xiao"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01221v1",
    "title" : "A Survey on Semantic Communication Networks: Architecture, Security, and\n  Privacy",
    "summary" : "Semantic communication, emerging as a breakthrough beyond the classical\nShannon paradigm, aims to convey the essential meaning of source data rather\nthan merely focusing on precise yet content-agnostic bit transmission. By\ninterconnecting diverse intelligent agents (e.g., autonomous vehicles and VR\ndevices) via semantic communications, the semantic communication networks\n(SemComNet) supports semantic-oriented transmission, efficient spectrum\nutilization, and flexible networking among collaborative agents. Consequently,\nSemComNet stands out for enabling ever-increasing intelligent applications,\nsuch as autonomous driving and Metaverse. However, being built on a variety of\ncutting-edge technologies including AI and knowledge graphs, SemComNet\nintroduces diverse brand-new and unexpected threats, which pose obstacles to\nits widespread development. Besides, due to the intrinsic characteristics of\nSemComNet in terms of heterogeneous components, autonomous intelligence, and\nlarge-scale structure, a series of critical challenges emerge in securing\nSemComNet. In this paper, we provide a comprehensive and up-to-date survey of\nSemComNet from its fundamentals, security, and privacy aspects. Specifically,\nwe first introduce a novel three-layer architecture of SemComNet for\nmulti-agent interaction, which comprises the control layer, semantic\ntransmission layer, and cognitive sensing layer. Then, we discuss its working\nmodes and enabling technologies. Afterward, based on the layered architecture\nof SemComNet, we outline a taxonomy of security and privacy threats, while\ndiscussing state-of-the-art defense approaches. Finally, we present future\nresearch directions, clarifying the path toward building intelligent, robust,\nand green SemComNet. To our knowledge, this survey is the first to\ncomprehensively cover the fundamentals of SemComNet, alongside a detailed\nanalysis of its security and privacy issues.",
    "updated" : "2024-05-02T12:04:35Z",
    "published" : "2024-05-02T12:04:35Z",
    "authors" : [
      {
        "name" : "Shaolong Guo"
      },
      {
        "name" : "Yuntao Wang"
      },
      {
        "name" : "Ning Zhang"
      },
      {
        "name" : "Zhou Su"
      },
      {
        "name" : "Tom H. Luan"
      },
      {
        "name" : "Zhiyi Tian"
      },
      {
        "name" : "Xuemin Shen"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01031v1",
    "title" : "The Privacy Power of Correlated Noise in Decentralized Learning",
    "summary" : "Decentralized learning is appealing as it enables the scalable usage of large\namounts of distributed data and resources (without resorting to any central\nentity), while promoting privacy since every user minimizes the direct exposure\nof their data. Yet, without additional precautions, curious users can still\nleverage models obtained from their peers to violate privacy. In this paper, we\npropose Decor, a variant of decentralized SGD with differential privacy (DP)\nguarantees. Essentially, in Decor, users securely exchange randomness seeds in\none communication round to generate pairwise-canceling correlated Gaussian\nnoises, which are injected to protect local models at every communication\nround. We theoretically and empirically show that, for arbitrary connected\ngraphs, Decor matches the central DP optimal privacy-utility trade-off. We do\nso under SecLDP, our new relaxation of local DP, which protects all user\ncommunications against an external eavesdropper and curious users, assuming\nthat every pair of connected users shares a secret, i.e., an information hidden\nto all others. The main theoretical challenge is to control the accumulation of\nnon-canceling correlated noise due to network sparsity. We also propose a\ncompanion SecLDP privacy accountant for public use.",
    "updated" : "2024-05-02T06:14:56Z",
    "published" : "2024-05-02T06:14:56Z",
    "authors" : [
      {
        "name" : "Youssef Allouah"
      },
      {
        "name" : "Anastasia Koloskova"
      },
      {
        "name" : "Aymane El Firdoussi"
      },
      {
        "name" : "Martin Jaggi"
      },
      {
        "name" : "Rachid Guerraoui"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "math.OC",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01844v1",
    "title" : "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
    "summary" : "Caching content at the network edge is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the network edge. On the one\nhand, the multi-access open edge network provides an ideal surface for external\nattackers to obtain private data from the edge cache by extracting sensitive\ninformation. On the other hand, privacy can be infringed by curious edge\ncaching providers through caching trace analysis targeting to achieve better\ncaching performance or higher profits. Therefore, an in-depth understanding of\nprivacy issues in edge caching networks is vital and indispensable for creating\na privacy-preserving caching service at the network edge. In this article, we\nare among the first to fill in this gap by examining privacy-preserving\ntechniques for caching content at the network edge. Firstly, we provide an\nintroduction to the background of Privacy-Preserving Edge Caching (PPEC). Next,\nwe summarize the key privacy issues and present a taxonomy for caching at the\nnetwork edge from the perspective of private data. Additionally, we conduct a\nretrospective review of the state-of-the-art countermeasures against privacy\nleakage from content caching at the network edge. Finally, we conclude the\nsurvey and envision challenges for future research.",
    "updated" : "2024-05-03T04:27:32Z",
    "published" : "2024-05-03T04:27:32Z",
    "authors" : [
      {
        "name" : "Xianzhi Zhang"
      },
      {
        "name" : "Yipeng Zhou"
      },
      {
        "name" : "Di Wu"
      },
      {
        "name" : "Shazia Riaz"
      },
      {
        "name" : "Quan Z. Sheng"
      },
      {
        "name" : "Miao Hu"
      },
      {
        "name" : "Linchang Xiao"
      }
    ],
    "categories" : [
      "cs.NI",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01742v1",
    "title" : "Addressing Privacy Concerns in Joint Communication and Sensing for 6G\n  Networks: Challenges and Prospects",
    "summary" : "The vision for 6G extends beyond mere communication, incorporating sensing\ncapabilities to facilitate a diverse array of novel applications and services.\nHowever, the advent of joint communication and sensing (JCAS) technology\nintroduces concerns regarding the handling of sensitive personally identifiable\ninformation (PII) pertaining to individuals and objects, along with external\nthird-party data and disclosure. Consequently, JCAS-based applications are\nsusceptible to privacy breaches, including location tracking, identity\ndisclosure, profiling, and misuse of sensor data, raising significant\nimplications under the European Union's General Data Protection Regulation\n(GDPR) as well as other applicable standards. This paper critically examines\nemergent JCAS architectures and underscores the necessity for network functions\nto enable privacy-specific features in the 6G systems. We propose an enhanced\nJCAS architecture with additional network functions and interfaces,\nfacilitating the management of sensing policies, consent information, and\ntransparency guidelines, alongside the integration of sensing-specific\nfunctions and storage for sensing processing sessions. Furthermore, we conduct\na comprehensive threat analysis for all interfaces, employing security threat\nmodel STRIDE and privacy threat model LINDDUN. We also summarise the identified\nthreats using standard Common Weakness Enumerations (CWEs). Finally, we suggest\nthe security and privacy controls as the mitigating strategies to counter the\nidentified threats stemming from the JCAS architecture.",
    "updated" : "2024-05-02T21:25:24Z",
    "published" : "2024-05-02T21:25:24Z",
    "authors" : [
      {
        "name" : "Prajnamaya Dass"
      },
      {
        "name" : "Sonika Ujjwal"
      },
      {
        "name" : "Jiri Novotny"
      },
      {
        "name" : "Yevhen Zolotavkin"
      },
      {
        "name" : "Zakaria Laaroussi"
      },
      {
        "name" : "Stefan Köpsell"
      }
    ],
    "categories" : [
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01716v1",
    "title" : "ATTAXONOMY: Unpacking Differential Privacy Guarantees Against Practical\n  Adversaries",
    "summary" : "Differential Privacy (DP) is a mathematical framework that is increasingly\ndeployed to mitigate privacy risks associated with machine learning and\nstatistical analyses. Despite the growing adoption of DP, its technical privacy\nparameters do not lend themselves to an intelligible description of the\nreal-world privacy risks associated with that deployment: the guarantee that\nmost naturally follows from the DP definition is protection against membership\ninference by an adversary who knows all but one data record and has unlimited\nauxiliary knowledge. In many settings, this adversary is far too strong to\ninform how to set real-world privacy parameters.\n  One approach for contextualizing privacy parameters is via defining and\nmeasuring the success of technical attacks, but doing so requires a systematic\ncategorization of the relevant attack space. In this work, we offer a detailed\ntaxonomy of attacks, showing the various dimensions of attacks and highlighting\nthat many real-world settings have been understudied. Our taxonomy provides a\nroadmap for analyzing real-world deployments and developing theoretical bounds\nfor more informative privacy attacks. We operationalize our taxonomy by using\nit to analyze a real-world case study, the Israeli Ministry of Health's recent\nrelease of a birth dataset using DP, showing how the taxonomy enables\nfine-grained threat modeling and provides insight towards making informed\nprivacy parameter choices. Finally, we leverage the taxonomy towards defining a\nmore realistic attack than previously considered in the literature, namely a\ndistributional reconstruction attack: we generalize Balle et al.'s notion of\nreconstruction robustness to a less-informed adversary with distributional\nuncertainty, and extend the worst-case guarantees of DP to this average-case\nsetting.",
    "updated" : "2024-05-02T20:23:23Z",
    "published" : "2024-05-02T20:23:23Z",
    "authors" : [
      {
        "name" : "Rachel Cummings"
      },
      {
        "name" : "Shlomi Hod"
      },
      {
        "name" : "Jayshree Sarathy"
      },
      {
        "name" : "Marika Swanberg"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01704v1",
    "title" : "Privacy-aware Berrut Approximated Coded Computing for Federated Learning",
    "summary" : "Federated Learning (FL) is an interesting strategy that enables the\ncollaborative training of an AI model among different data owners without\nrevealing their private datasets. Even so, FL has some privacy vulnerabilities\nthat have been tried to be overcome by applying some techniques like\nDifferential Privacy (DP), Homomorphic Encryption (HE), or Secure Multi-Party\nComputation (SMPC). However, these techniques have some important drawbacks\nthat might narrow their range of application: problems to work with non-linear\nfunctions and to operate large matrix multiplications and high communication\nand computational costs to manage semi-honest nodes. In this context, we\npropose a solution to guarantee privacy in FL schemes that simultaneously\nsolves the previously mentioned problems. Our proposal is based on the Berrut\nApproximated Coded Computing, a technique from the Coded Distributed Computing\nparadigm, adapted to a Secret Sharing configuration, to provide input privacy\nto FL in a scalable way. It can be applied for computing non-linear functions\nand treats the special case of distributed matrix multiplication, a key\nprimitive at the core of many automated learning tasks. Because of these\ncharacteristics, it could be applied in a wide range of FL scenarios, since it\nis independent of the machine learning models or aggregation algorithms used in\nthe FL scheme. We provide analysis of the achieve privacy and complexity of our\nsolution and, due to the extensive numerical results performed, it can be\nobserved a good trade-off between privacy and precision.",
    "updated" : "2024-05-02T20:03:13Z",
    "published" : "2024-05-02T20:03:13Z",
    "authors" : [
      {
        "name" : "Xavier Martínez Luaña"
      },
      {
        "name" : "Rebeca P. Díaz Redondo"
      },
      {
        "name" : "Manuel Fernández Veiga"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CC",
      "cs.DC",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01678v1",
    "title" : "1-Diffractor: Efficient and Utility-Preserving Text Obfuscation\n  Leveraging Word-Level Metric Differential Privacy",
    "summary" : "The study of privacy-preserving Natural Language Processing (NLP) has gained\nrising attention in recent years. One promising avenue studies the integration\nof Differential Privacy in NLP, which has brought about innovative methods in a\nvariety of application settings. Of particular note are $\\textit{word-level\nMetric Local Differential Privacy (MLDP)}$ mechanisms, which work to obfuscate\npotentially sensitive input text by performing word-by-word\n$\\textit{perturbations}$. Although these methods have shown promising results\nin empirical tests, there are two major drawbacks: (1) the inevitable loss of\nutility due to addition of noise, and (2) the computational expensiveness of\nrunning these mechanisms on high-dimensional word embeddings. In this work, we\naim to address these challenges by proposing $\\texttt{1-Diffractor}$, a new\nmechanism that boasts high speedups in comparison to previous mechanisms, while\nstill demonstrating strong utility- and privacy-preserving capabilities. We\nevaluate $\\texttt{1-Diffractor}$ for utility on several NLP tasks, for\ntheoretical and task-based privacy, and for efficiency in terms of speed and\nmemory. $\\texttt{1-Diffractor}$ shows significant improvements in efficiency,\nwhile still maintaining competitive utility and privacy scores across all\nconducted comparative tests against previous MLDP mechanisms. Our code is made\navailable at: https://github.com/sjmeis/Diffractor.",
    "updated" : "2024-05-02T19:07:32Z",
    "published" : "2024-05-02T19:07:32Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Maulik Chevli"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01646v1",
    "title" : "Explaining models relating objects and privacy",
    "summary" : "Accurately predicting whether an image is private before sharing it online is\ndifficult due to the vast variety of content and the subjective nature of\nprivacy itself. In this paper, we evaluate privacy models that use objects\nextracted from an image to determine why the image is predicted as private. To\nexplain the decision of these models, we use feature-attribution to identify\nand quantify which objects (and which of their features) are more relevant to\nprivacy classification with respect to a reference input (i.e., no objects\nlocalised in an image) predicted as public. We show that the presence of the\nperson category and its cardinality is the main factor for the privacy\ndecision. Therefore, these models mostly fail to identify private images\ndepicting documents with sensitive data, vehicle ownership, and internet\nactivity, or public images with people (e.g., an outdoor concert or people\nwalking in a public space next to a famous landmark). As baselines for future\nbenchmarks, we also devise two strategies that are based on the person presence\nand cardinality and achieve comparable classification performance of the\nprivacy models.",
    "updated" : "2024-05-02T18:06:48Z",
    "published" : "2024-05-02T18:06:48Z",
    "authors" : [
      {
        "name" : "Alessio Xompero"
      },
      {
        "name" : "Myriam Bontonou"
      },
      {
        "name" : "Jean-Michel Arbona"
      },
      {
        "name" : "Emmanouil Benetos"
      },
      {
        "name" : "Andrea Cavallaro"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.01031v2",
    "title" : "The Privacy Power of Correlated Noise in Decentralized Learning",
    "summary" : "Decentralized learning is appealing as it enables the scalable usage of large\namounts of distributed data and resources (without resorting to any central\nentity), while promoting privacy since every user minimizes the direct exposure\nof their data. Yet, without additional precautions, curious users can still\nleverage models obtained from their peers to violate privacy. In this paper, we\npropose Decor, a variant of decentralized SGD with differential privacy (DP)\nguarantees. Essentially, in Decor, users securely exchange randomness seeds in\none communication round to generate pairwise-canceling correlated Gaussian\nnoises, which are injected to protect local models at every communication\nround. We theoretically and empirically show that, for arbitrary connected\ngraphs, Decor matches the central DP optimal privacy-utility trade-off. We do\nso under SecLDP, our new relaxation of local DP, which protects all user\ncommunications against an external eavesdropper and curious users, assuming\nthat every pair of connected users shares a secret, i.e., an information hidden\nto all others. The main theoretical challenge is to control the accumulation of\nnon-canceling correlated noise due to network sparsity. We also propose a\ncompanion SecLDP privacy accountant for public use.",
    "updated" : "2024-05-03T08:14:22Z",
    "published" : "2024-05-02T06:14:56Z",
    "authors" : [
      {
        "name" : "Youssef Allouah"
      },
      {
        "name" : "Anastasia Koloskova"
      },
      {
        "name" : "Aymane El Firdoussi"
      },
      {
        "name" : "Martin Jaggi"
      },
      {
        "name" : "Rachid Guerraoui"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC",
      "math.OC",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.03636v1",
    "title" : "Federated Learning Privacy: Attacks, Defenses, Applications, and Policy\n  Landscape - A Survey",
    "summary" : "Deep learning has shown incredible potential across a vast array of tasks and\naccompanying this growth has been an insatiable appetite for data. However, a\nlarge amount of data needed for enabling deep learning is stored on personal\ndevices and recent concerns on privacy have further highlighted challenges for\naccessing such data. As a result, federated learning (FL) has emerged as an\nimportant privacy-preserving technology enabling collaborative training of\nmachine learning models without the need to send the raw, potentially\nsensitive, data to a central server. However, the fundamental premise that\nsending model updates to a server is privacy-preserving only holds if the\nupdates cannot be \"reverse engineered\" to infer information about the private\ntraining data. It has been shown under a wide variety of settings that this\npremise for privacy does {\\em not} hold.\n  In this survey paper, we provide a comprehensive literature review of the\ndifferent privacy attacks and defense methods in FL. We identify the current\nlimitations of these attacks and highlight the settings in which FL client\nprivacy can be broken. We dissect some of the successful industry applications\nof FL and draw lessons for future successful adoption. We survey the emerging\nlandscape of privacy regulation for FL. We conclude with future directions for\ntaking FL toward the cherished goal of generating accurate models while\npreserving the privacy of the data from its participants.",
    "updated" : "2024-05-06T16:55:20Z",
    "published" : "2024-05-06T16:55:20Z",
    "authors" : [
      {
        "name" : "Joshua C. Zhao"
      },
      {
        "name" : "Saurabh Bagchi"
      },
      {
        "name" : "Salman Avestimehr"
      },
      {
        "name" : "Kevin S. Chan"
      },
      {
        "name" : "Somali Chaterji"
      },
      {
        "name" : "Dimitris Dimitriadis"
      },
      {
        "name" : "Jiacheng Li"
      },
      {
        "name" : "Ninghui Li"
      },
      {
        "name" : "Arash Nourian"
      },
      {
        "name" : "Holger R. Roth"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG",
      "I.2; H.4; I.5"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.03106v1",
    "title" : "Compression-based Privacy Preservation for Distributed Nash Equilibrium\n  Seeking in Aggregative Games",
    "summary" : "This paper explores distributed aggregative games in multi-agent systems.\nCurrent methods for finding distributed Nash equilibrium require players to\nsend original messages to their neighbors, leading to communication burden and\nprivacy issues. To jointly address these issues, we propose an algorithm that\nuses stochastic compression to save communication resources and conceal\ninformation through random errors induced by compression. Our theoretical\nanalysis shows that the algorithm guarantees convergence accuracy, even with\naggressive compression errors used to protect privacy. We prove that the\nalgorithm achieves differential privacy through a stochastic quantization\nscheme. Simulation results for energy consumption games support the\neffectiveness of our approach.",
    "updated" : "2024-05-06T01:42:40Z",
    "published" : "2024-05-06T01:42:40Z",
    "authors" : [
      {
        "name" : "Wei Huo"
      },
      {
        "name" : "Xiaomeng Chen"
      },
      {
        "name" : "Kemi Ding"
      },
      {
        "name" : "Subhrakanti Dey"
      },
      {
        "name" : "Ling Shi"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.GT",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.03065v1",
    "title" : "Powering the Future of IoT: Federated Learning for Optimized Power\n  Consumption and Enhanced Privacy",
    "summary" : "The widespread use of the Internet of Things has led to the development of\nlarge amounts of perception data, making it necessary to develop effective and\nscalable data analysis tools. Federated Learning emerges as a promising\nparadigm to address the inherent challenges of power consumption and data\nprivacy in IoT environments. This paper explores the transformative potential\nof FL in enhancing the longevity of IoT devices by mitigating power consumption\nand enhancing privacy and security measures. We delve into the intricacies of\nFL, elucidating its components and applications within IoT ecosystems.\nAdditionally, we discuss the critical characteristics and challenges of IoT,\nhighlighting the need for such machine learning solutions in processing\nperception data. While FL introduces many benefits for IoT sustainability, it\nalso has limitations. Through a comprehensive discussion and analysis, this\npaper elucidates the opportunities and constraints of FL in shaping the future\nof sustainable and secure IoT systems. Our findings highlight the importance of\ndeveloping new approaches and conducting additional research to maximise the\nbenefits of FL in creating a secure and privacy-focused IoT environment.",
    "updated" : "2024-05-05T22:18:22Z",
    "published" : "2024-05-05T22:18:22Z",
    "authors" : [
      {
        "name" : "Ghazaleh Shirvani"
      },
      {
        "name" : "Saeid Ghasemshirazi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.02665v1",
    "title" : "Metric Differential Privacy at the User-Level",
    "summary" : "Metric differential privacy (DP) provides heterogeneous privacy guarantees\nbased on a distance between the pair of inputs. It is a widely popular notion\nof privacy since it captures the natural privacy semantics for many\napplications (such as, for location data) and results in better utility than\nstandard DP. However, prior work in metric DP has primarily focused on the\n\\textit{item-level} setting where every user only reports a single data item. A\nmore realistic setting is that of user-level DP where each user contributes\nmultiple items and privacy is then desired at the granularity of the user's\n\\textit{entire} contribution. In this paper, we initiate the study of metric DP\nat the user-level. Specifically, we use the earth-mover's distance\n($d_\\textsf{EM}$) as our metric to obtain a notion of privacy as it captures\nboth the magnitude and spatial aspects of changes in a user's data.\n  We make three main technical contributions. First, we design two novel\nmechanisms under $d_\\textsf{EM}$-DP to answer linear queries and item-wise\nqueries. Specifically, our analysis for the latter involves a generalization of\nthe privacy amplification by shuffling result which may be of independent\ninterest. Second, we provide a black-box reduction from the general unbounded\nto bounded $d_\\textsf{EM}$-DP (size of the dataset is fixed and public) with a\nnovel sampling based mechanism. Third, we show that our proposed mechanisms can\nprovably provide improved utility over user-level DP, for certain types of\nlinear queries and frequency estimation.",
    "updated" : "2024-05-04T13:29:11Z",
    "published" : "2024-05-04T13:29:11Z",
    "authors" : [
      {
        "name" : "Jacob Imola"
      },
      {
        "name" : "Amrita Roy Chowdhury"
      },
      {
        "name" : "Kamalika Chaudhuri"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.02437v1",
    "title" : "FastLloyd: Federated, Accurate, Secure, and Tunable $k$-Means Clustering\n  with Differential Privacy",
    "summary" : "We study the problem of privacy-preserving $k$-means clustering in the\nhorizontally federated setting. Existing federated approaches using secure\ncomputation, suffer from substantial overheads and do not offer output privacy.\nAt the same time, differentially private (DP) $k$-means algorithms assume a\ntrusted central curator and do not extend to federated settings. Naively\ncombining the secure and DP solutions results in a protocol with impractical\noverhead. Instead, our work provides enhancements to both the DP and secure\ncomputation components, resulting in a design that is faster, more private, and\nmore accurate than previous work. By utilizing the computational DP model, we\ndesign a lightweight, secure aggregation-based approach that achieves four\norders of magnitude speed-up over state-of-the-art related work. Furthermore,\nwe not only maintain the utility of the state-of-the-art in the central model\nof DP, but we improve the utility further by taking advantage of constrained\nclustering techniques.",
    "updated" : "2024-05-03T19:04:37Z",
    "published" : "2024-05-03T19:04:37Z",
    "authors" : [
      {
        "name" : "Abdulrahman Diaa"
      },
      {
        "name" : "Thomas Humphries"
      },
      {
        "name" : "Florian Kerschbaum"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.02341v1",
    "title" : "Improved Communication-Privacy Trade-offs in $L_2$ Mean Estimation under\n  Streaming Differential Privacy",
    "summary" : "We study $L_2$ mean estimation under central differential privacy and\ncommunication constraints, and address two key challenges: firstly, existing\nmean estimation schemes that simultaneously handle both constraints are usually\noptimized for $L_\\infty$ geometry and rely on random rotation or Kashin's\nrepresentation to adapt to $L_2$ geometry, resulting in suboptimal leading\nconstants in mean square errors (MSEs); secondly, schemes achieving\norder-optimal communication-privacy trade-offs do not extend seamlessly to\nstreaming differential privacy (DP) settings (e.g., tree aggregation or matrix\nfactorization), rendering them incompatible with DP-FTRL type optimizers.\n  In this work, we tackle these issues by introducing a novel privacy\naccounting method for the sparsified Gaussian mechanism that incorporates the\nrandomness inherent in sparsification into the DP noise. Unlike previous\napproaches, our accounting algorithm directly operates in $L_2$ geometry,\nyielding MSEs that fast converge to those of the uncompressed Gaussian\nmechanism. Additionally, we extend the sparsification scheme to the matrix\nfactorization framework under streaming DP and provide a precise accountant\ntailored for DP-FTRL type optimizers. Empirically, our method demonstrates at\nleast a 100x improvement of compression for DP-SGD across various FL tasks.",
    "updated" : "2024-05-02T03:48:47Z",
    "published" : "2024-05-02T03:48:47Z",
    "authors" : [
      {
        "name" : "Wei-Ning Chen"
      },
      {
        "name" : "Berivan Isik"
      },
      {
        "name" : "Peter Kairouz"
      },
      {
        "name" : "Albert No"
      },
      {
        "name" : "Sewoong Oh"
      },
      {
        "name" : "Zheng Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.04344v1",
    "title" : "Enhancing Scalability of Metric Differential Privacy via Secret Dataset\n  Partitioning and Benders Decomposition",
    "summary" : "Metric Differential Privacy (mDP) extends the concept of Differential Privacy\n(DP) to serve as a new paradigm of data perturbation. It is designed to protect\nsecret data represented in general metric space, such as text data encoded as\nword embeddings or geo-location data on the road network or grid maps. To\nderive an optimal data perturbation mechanism under mDP, a widely used method\nis linear programming (LP), which, however, might suffer from a polynomial\nexplosion of decision variables, rendering it impractical in large-scale mDP.\n  In this paper, our objective is to develop a new computation framework to\nenhance the scalability of the LP-based mDP. Considering the connections\nestablished by the mDP constraints among the secret records, we partition the\noriginal secret dataset into various subsets. Building upon the partition, we\nreformulate the LP problem for mDP and solve it via Benders Decomposition,\nwhich is composed of two stages: (1) a master program to manage the\nperturbation calculation across subsets and (2) a set of subproblems, each\nmanaging the perturbation derivation within a subset. Our experimental results\non multiple datasets, including geo-location data in the road network/grid\nmaps, text data, and synthetic data, underscore our proposed mechanism's\nsuperior scalability and efficiency.",
    "updated" : "2024-05-07T14:19:09Z",
    "published" : "2024-05-07T14:19:09Z",
    "authors" : [
      {
        "name" : "Chenxi Qiu"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.04108v1",
    "title" : "A2-DIDM: Privacy-preserving Accumulator-enabled Auditing for Distributed\n  Identity of DNN Model",
    "summary" : "Recent booming development of Generative Artificial Intelligence (GenAI) has\nfacilitated an emerging model commercialization for the purpose of\nreinforcement on model performance, such as licensing or trading Deep Neural\nNetwork (DNN) models. However, DNN model trading may trigger concerns of the\nunauthorized replications or misuses over the model, so that the benefit of the\nmodel ownership will be violated. Model identity auditing is a challenging\nissue in protecting intellectual property of DNN models and verifying the\nintegrity and ownership of models for guaranteeing trusts in transactions is\none of the critical obstacles. In this paper, we focus on the above issue and\npropose a novel Accumulator-enabled Auditing for Distributed Identity of DNN\nModel (A2-DIDM) that utilizes blockchain and zero-knowledge techniques to\nprotect data and function privacy while ensuring the lightweight on-chain\nownership verification. The proposed model presents a scheme of identity\nrecords via configuring model weight checkpoints with corresponding\nzero-knowledge proofs, which incorporates predicates to capture incremental\nstate changes in model weight checkpoints. Our scheme ensures both\ncomputational integrity of DNN training process and programmability, so that\nthe uniqueness of the weight checkpoint sequence in a DNN model is preserved,\nensuring the correctness of the model identity auditing. In addition, A2-DIDM\nalso addresses privacy protections in distributed identity via a proposed\nmethod of accumulators. We systematically analyze the security and robustness\nof our proposed model and further evaluate the effectiveness and usability of\nauditing DNN model identities.",
    "updated" : "2024-05-07T08:24:50Z",
    "published" : "2024-05-07T08:24:50Z",
    "authors" : [
      {
        "name" : "Tianxiu Xie"
      },
      {
        "name" : "Keke Gai"
      },
      {
        "name" : "Jing Yu"
      },
      {
        "name" : "Liehuang Zhu"
      },
      {
        "name" : "Kim-Kwang Raymond Choo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.04029v1",
    "title" : "Enabling Privacy-Preserving and Publicly Auditable Federated Learning",
    "summary" : "Federated learning (FL) has attracted widespread attention because it\nsupports the joint training of models by multiple participants without moving\nprivate dataset. However, there are still many security issues in FL that\ndeserve discussion. In this paper, we consider three major issues: 1) how to\nensure that the training process can be publicly audited by any third party; 2)\nhow to avoid the influence of malicious participants on training; 3) how to\nensure that private gradients and models are not leaked to third parties. Many\nsolutions have been proposed to address these issues, while solving the above\nthree problems simultaneously is seldom considered. In this paper, we propose a\npublicly auditable and privacy-preserving federated learning scheme that is\nresistant to malicious participants uploading gradients with wrong directions\nand enables anyone to audit and verify the correctness of the training process.\nIn particular, we design a robust aggregation algorithm capable of detecting\ngradients with wrong directions from malicious participants. Then, we design a\nrandom vector generation algorithm and combine it with zero sharing and\nblockchain technologies to make the joint training process publicly auditable,\nmeaning anyone can verify the correctness of the training. Finally, we conduct\na series of experiments, and the experimental results show that the model\ngenerated by the protocol is comparable in accuracy to the original FL approach\nwhile keeping security advantages.",
    "updated" : "2024-05-07T06:03:10Z",
    "published" : "2024-05-07T06:03:10Z",
    "authors" : [
      {
        "name" : "Huang Zeng"
      },
      {
        "name" : "Anjia Yang"
      },
      {
        "name" : "Jian Weng"
      },
      {
        "name" : "Min-Rong Chen"
      },
      {
        "name" : "Fengjun Xiao"
      },
      {
        "name" : "Yi Liu"
      },
      {
        "name" : "Ye Yao"
      }
    ],
    "categories" : [
      "cs.CR",
      "C.2.2; C.2.4; E.3"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.03915v1",
    "title" : "Motivating Users to Attend to Privacy: A Theory-Driven Design Study",
    "summary" : "In modern technology environments, raising users' privacy awareness is\ncrucial. Existing efforts largely focused on privacy policy presentation and\nfailed to systematically address a radical challenge of user motivation for\ninitiating privacy awareness. Leveraging the Protection Motivation Theory\n(PMT), we proposed design ideas and categories dedicated to motivating users to\nengage with privacy-related information. Using these design ideas, we created a\nconceptual prototype, enhancing the current App Store product page. Results\nfrom an online experiment and follow-up interviews showed that our design\neffectively motivated participants to attend to privacy issues, raising both\nthe threat appraisal and coping appraisal, two main factors in PMT. Our work\nindicated that effective design should consider combining PMT components,\ncalibrating information content, and integrating other design elements, such as\nvisual cues and user familiarity. Overall, our study contributes valuable\ndesign considerations driven by the PMT to amplify the motivational aspect of\nprivacy communication.",
    "updated" : "2024-05-07T00:23:42Z",
    "published" : "2024-05-07T00:23:42Z",
    "authors" : [
      {
        "name" : "Varun Shiri"
      },
      {
        "name" : "Maggie Xiong"
      },
      {
        "name" : "Jinghui Cheng"
      },
      {
        "name" : "Jin L. C. Guo"
      }
    ],
    "categories" : [
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.03903v1",
    "title" : "Unified Locational Differential Privacy Framework",
    "summary" : "Aggregating statistics over geographical regions is important for many\napplications, such as analyzing income, election results, and disease spread.\nHowever, the sensitive nature of this data necessitates strong privacy\nprotections to safeguard individuals. In this work, we present a unified\nlocational differential privacy (DP) framework to enable private aggregation of\nvarious data types, including one-hot encoded, boolean, float, and integer\narrays, over geographical regions. Our framework employs local DP mechanisms\nsuch as randomized response, the exponential mechanism, and the Gaussian\nmechanism. We evaluate our approach on four datasets representing significant\nlocation data aggregation scenarios. Results demonstrate the utility of our\nframework in providing formal DP guarantees while enabling geographical data\nanalysis.",
    "updated" : "2024-05-06T23:33:52Z",
    "published" : "2024-05-06T23:33:52Z",
    "authors" : [
      {
        "name" : "Aman Priyanshu"
      },
      {
        "name" : "Yash Maurya"
      },
      {
        "name" : "Suriya Ganesh"
      },
      {
        "name" : "Vy Tran"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.00596v2",
    "title" : "Unbundle-Rewrite-Rebundle: Runtime Detection and Rewriting of\n  Privacy-Harming Code in JavaScript Bundles",
    "summary" : "This work presents Unbundle-Rewrite-Rebundle (URR), a system for detecting\nprivacy-harming portions of bundled JavaScript code, and rewriting that code at\nruntime to remove the privacy harming behavior without breaking the surrounding\ncode or overall application. URR is a novel solution to the problem of\nJavaScript bundles, where websites pre-compile multiple code units into a\nsingle file, making it impossible for content filters and ad-blockers to\ndifferentiate between desired and unwanted resources. Where traditional content\nfiltering tools rely on URLs, URR analyzes the code at the AST level, and\nreplaces harmful AST sub-trees with privacy-and-functionality maintaining\nalternatives.\n  We present an open-sourced implementation of URR as a Firefox extension, and\nevaluate it against JavaScript bundles generated by the most popular bundling\nsystem (Webpack) deployed on the Tranco 10k. We measure the performance,\nmeasured by precision (1.00), recall (0.95), and speed (0.43s per-script) when\ndetecting and rewriting three representative privacy harming libraries often\nincluded in JavaScript bundles, and find URR to be an effective approach to a\nlarge-and-growing blind spot unaddressed by current privacy tools.",
    "updated" : "2024-05-07T15:38:20Z",
    "published" : "2024-05-01T16:04:42Z",
    "authors" : [
      {
        "name" : "Mir Masood Ali"
      },
      {
        "name" : "Peter Snyder"
      },
      {
        "name" : "Chris Kanich"
      },
      {
        "name" : "Hamed Haddadi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.05175v1",
    "title" : "Air Gap: Protecting Privacy-Conscious Conversational Agents",
    "summary" : "The growing use of large language model (LLM)-based conversational agents to\nmanage sensitive user data raises significant privacy concerns. While these\nagents excel at understanding and acting on context, this capability can be\nexploited by malicious actors. We introduce a novel threat model where\nadversarial third-party apps manipulate the context of interaction to trick\nLLM-based agents into revealing private information not relevant to the task at\nhand.\n  Grounded in the framework of contextual integrity, we introduce AirGapAgent,\na privacy-conscious agent designed to prevent unintended data leakage by\nrestricting the agent's access to only the data necessary for a specific task.\nExtensive experiments using Gemini, GPT, and Mistral models as agents validate\nour approach's effectiveness in mitigating this form of context hijacking while\nmaintaining core agent functionality. For example, we show that a single-query\ncontext hijacking attack on a Gemini Ultra agent reduces its ability to protect\nuser data from 94% to 45%, while an AirGapAgent achieves 97% protection,\nrendering the same attack ineffective.",
    "updated" : "2024-05-08T16:12:45Z",
    "published" : "2024-05-08T16:12:45Z",
    "authors" : [
      {
        "name" : "Eugene Bagdasaryan"
      },
      {
        "name" : "Ren Yi"
      },
      {
        "name" : "Sahra Ghalebikesabi"
      },
      {
        "name" : "Peter Kairouz"
      },
      {
        "name" : "Marco Gruteser"
      },
      {
        "name" : "Sewoong Oh"
      },
      {
        "name" : "Borja Balle"
      },
      {
        "name" : "Daniel Ramage"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.05930v1",
    "title" : "Trustworthy AI-Generative Content in Intelligent 6G Network:\n  Adversarial, Privacy, and Fairness",
    "summary" : "AI-generated content (AIGC) models, represented by large language models\n(LLM), have brought revolutionary changes to the content generation fields. The\nhigh-speed and extensive 6G technology is an ideal platform for providing\npowerful AIGC mobile service applications, while future 6G mobile networks also\nneed to support intelligent and personalized mobile generation services.\nHowever, the significant ethical and security issues of current AIGC models,\nsuch as adversarial attacks, privacy, and fairness, greatly affect the\ncredibility of 6G intelligent networks, especially in ensuring secure, private,\nand fair AIGC applications. In this paper, we propose TrustGAIN, a novel\nparadigm for trustworthy AIGC in 6G networks, to ensure trustworthy large-scale\nAIGC services in future 6G networks. We first discuss the adversarial attacks\nand privacy threats faced by AIGC systems in 6G networks, as well as the\ncorresponding protection issues. Subsequently, we emphasize the importance of\nensuring the unbiasedness and fairness of the mobile generative service in\nfuture intelligent networks. In particular, we conduct a use case to\ndemonstrate that TrustGAIN can effectively guide the resistance against\nmalicious or generated false information. We believe that TrustGAIN is a\nnecessary paradigm for intelligent and trustworthy 6G networks to support AIGC\nservices, ensuring the security, privacy, and fairness of AIGC network\nservices.",
    "updated" : "2024-05-09T17:16:20Z",
    "published" : "2024-05-09T17:16:20Z",
    "authors" : [
      {
        "name" : "Siyuan Li"
      },
      {
        "name" : "Xi Lin"
      },
      {
        "name" : "Yaju Liu"
      },
      {
        "name" : "Jianhua Li"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.05789v1",
    "title" : "High-Performance Privacy-Preserving Matrix Completion for Trajectory\n  Recovery",
    "summary" : "Matrix completion has important applications in trajectory recovery and\nmobile social networks. However, sending raw data containing personal,\nsensitive information to cloud computing nodes may lead to privacy exposure\nissue.The privacy-preserving matrix completion is a useful approach to perform\nmatrix completion while preserving privacy. In this paper, we propose a\nhigh-performance method for privacy-preserving matrix completion. First,we use\na lightweight encryption scheme to encrypt the raw data and then perform matrix\ncompletion using alternating direction method of multipliers (ADMM). Then,the\ncomplemented matrix is decrypted and compared with the original matrix to\ncalculate the error. This method has faster speed with higher accuracy. The\nresults of numerical experiments reveal that the proposed method is faster than\nother algorithms.",
    "updated" : "2024-05-09T14:12:41Z",
    "published" : "2024-05-09T14:12:41Z",
    "authors" : [
      {
        "name" : "Jiahao Guo"
      },
      {
        "name" : "An-Bao Xu"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NA",
      "math.NA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.05611v1",
    "title" : "Privacy-Preserving Edge Federated Learning for Intelligent Mobile-Health\n  Systems",
    "summary" : "Machine Learning (ML) algorithms are generally designed for scenarios in\nwhich all data is stored in one data center, where the training is performed.\nHowever, in many applications, e.g., in the healthcare domain, the training\ndata is distributed among several entities, e.g., different hospitals or\npatients' mobile devices/sensors. At the same time, transferring the data to a\ncentral location for learning is certainly not an option, due to privacy\nconcerns and legal issues, and in certain cases, because of the communication\nand computation overheads. Federated Learning (FL) is the state-of-the-art\ncollaborative ML approach for training an ML model across multiple parties\nholding local data samples, without sharing them. However, enabling learning\nfrom distributed data over such edge Internet of Things (IoT) systems (e.g.,\nmobile-health and wearable technologies, involving sensitive personal/medical\ndata) in a privacy-preserving fashion presents a major challenge mainly due to\ntheir stringent resource constraints, i.e., limited computing capacity,\ncommunication bandwidth, memory storage, and battery lifetime. In this paper,\nwe propose a privacy-preserving edge FL framework for resource-constrained\nmobile-health and wearable technologies over the IoT infrastructure. We\nevaluate our proposed framework extensively and provide the implementation of\nour technique on Amazon's AWS cloud platform based on the seizure detection\napplication in epilepsy monitoring using wearable technologies.",
    "updated" : "2024-05-09T08:15:31Z",
    "published" : "2024-05-09T08:15:31Z",
    "authors" : [
      {
        "name" : "Amin Aminifar"
      },
      {
        "name" : "Matin Shokri"
      },
      {
        "name" : "Amir Aminifar"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.05567v1",
    "title" : "Perfect Subset Privacy in Polynomial Computation",
    "summary" : "Delegating large-scale computations to service providers is a common practice\nwhich raises privacy concerns. This paper studies information-theoretic\nprivacy-preserving delegation of data to a service provider, who may further\ndelegate the computation to auxiliary worker nodes, in order to compute a\npolynomial over that data at a later point in time. We study techniques which\nare compatible with robust management of distributed computation systems, an\narea known as coded computing. Privacy in coded computing, however, has\ntraditionally addressed the problem of colluding workers, and assumed that the\nserver that administrates the computation is trusted. This viewpoint of privacy\ndoes not accurately reflect real-world privacy concerns, since normally, the\nservice provider as a whole (i.e., the administrator and the worker nodes) form\none cohesive entity which itself poses a privacy risk. This paper aims to shift\nthe focus of privacy in coded computing to safeguarding the privacy of the user\nagainst the service provider as a whole, instead of merely against colluding\nworkers inside the service provider. To this end, we leverage the recently\ndefined notion of perfect subset privacy, which guarantees zero information\nleakage from all subsets of the data up to a certain size. Using known\ntechniques from Reed-Muller decoding, we provide a scheme which enables\npolynomial computation with perfect subset privacy in straggler-free systems.\nFurthermore, by studying information super-sets in Reed-Muller codes, which may\nbe of independent interest, we extend the previous scheme to tolerate\nstraggling worker nodes inside the service provider.",
    "updated" : "2024-05-09T06:11:58Z",
    "published" : "2024-05-09T06:11:58Z",
    "authors" : [
      {
        "name" : " Zirui"
      },
      {
        "name" : " Deng"
      },
      {
        "name" : "Vinayak Ramkumar"
      },
      {
        "name" : "Netanel Raviv"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.04344v2",
    "title" : "Enhancing Scalability of Metric Differential Privacy via Secret Dataset\n  Partitioning and Benders Decomposition",
    "summary" : "Metric Differential Privacy (mDP) extends the concept of Differential Privacy\n(DP) to serve as a new paradigm of data perturbation. It is designed to protect\nsecret data represented in general metric space, such as text data encoded as\nword embeddings or geo-location data on the road network or grid maps. To\nderive an optimal data perturbation mechanism under mDP, a widely used method\nis linear programming (LP), which, however, might suffer from a polynomial\nexplosion of decision variables, rendering it impractical in large-scale mDP.\n  In this paper, our objective is to develop a new computation framework to\nenhance the scalability of the LP-based mDP. Considering the connections\nestablished by the mDP constraints among the secret records, we partition the\noriginal secret dataset into various subsets. Building upon the partition, we\nreformulate the LP problem for mDP and solve it via Benders Decomposition,\nwhich is composed of two stages: (1) a master program to manage the\nperturbation calculation across subsets and (2) a set of subproblems, each\nmanaging the perturbation derivation within a subset. Our experimental results\non multiple datasets, including geo-location data in the road network/grid\nmaps, text data, and synthetic data, underscore our proposed mechanism's\nsuperior scalability and efficiency.",
    "updated" : "2024-05-09T04:36:12Z",
    "published" : "2024-05-07T14:19:09Z",
    "authors" : [
      {
        "name" : "Chenxi Qiu"
      }
    ],
    "categories" : [
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.06307v1",
    "title" : "Smooth Sensitivity for Geo-Privacy",
    "summary" : "Suppose each user $i$ holds a private value $x_i$ in some metric space $(U,\n\\mathrm{dist})$, and an untrusted data analyst wishes to compute $\\sum_i\nf(x_i)$ for some function $f : U \\rightarrow \\mathbb{R}$ by asking each user to\nsend in a privatized $f(x_i)$. This is a fundamental problem in\nprivacy-preserving population analytics, and the local model of differential\nprivacy (LDP) is the predominant model under which the problem has been\nstudied. However, LDP requires any two different $x_i, x'_i$ to be\n$\\varepsilon$-distinguishable, which can be overly strong for\ngeometric/numerical data. On the other hand, Geo-Privacy (GP) stipulates that\nthe level of distinguishability be proportional to $\\mathrm{dist}(x_i, x_i')$,\nproviding an attractive alternative notion of personal data privacy in a metric\nspace. However, existing GP mechanisms for this problem, which add a uniform\nnoise to either $x_i$ or $f(x_i)$, are not satisfactory. In this paper, we\ngeneralize the smooth sensitivity framework from Differential Privacy to\nGeo-Privacy, which allows us to add noise tailored to the hardness of the given\ninstance. We provide definitions, mechanisms, and a generic procedure for\ncomputing the smooth sensitivity under GP equipped with a general metric. Then\nwe present three applications: one-way and two-way threshold functions, and\nGaussian kernel density estimation, to demonstrate the applicability and\nutility of our smooth sensitivity framework.",
    "updated" : "2024-05-10T08:32:07Z",
    "published" : "2024-05-10T08:32:07Z",
    "authors" : [
      {
        "name" : "Yuting Liang"
      },
      {
        "name" : "Ke Yi"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.06261v1",
    "title" : "Improving the Privacy Loss Under User-Level DP Composition for Fixed\n  Estimation Error",
    "summary" : "This paper considers the private release of statistics of several disjoint\nsubsets of a datasets, under user-level $\\epsilon$-differential privacy (DP).\nIn particular, we consider the user-level differentially private release of\nsample means and variances of speed values in several grids in a city, in a\npotentially sequential manner. Traditional analysis of the privacy loss due to\nthe sequential composition of queries necessitates a privacy loss degradation\nby a factor that equals the total number of grids. Our main contribution is an\niterative, instance-dependent algorithm, based on clipping the number of user\ncontributions, which seeks to reduce the overall privacy loss degradation under\na canonical Laplace mechanism, while not increasing the {worst} estimation\nerror among the different grids. We test the performance of our algorithm on\nsynthetic datasets and demonstrate improvements in the privacy loss degradation\nfactor via our algorithm. We also demonstrate improvements in the worst-case\nerror using a simple extension of a pseudo-user creation-based mechanism. An\nimportant component of this analysis is our exact characterization of the\nsensitivities and the worst-case estimation errors of sample means and\nvariances incurred by clipping user contributions in an arbitrary fashion,\nwhich we believe is of independent interest.",
    "updated" : "2024-05-10T06:24:35Z",
    "published" : "2024-05-10T06:24:35Z",
    "authors" : [
      {
        "name" : "V. Arvind Rameshwar"
      },
      {
        "name" : "Anshoo Tandon"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.07596v1",
    "title" : "Local Mutual-Information Differential Privacy",
    "summary" : "Local mutual-information differential privacy (LMIDP) is a privacy notion\nthat aims to quantify the reduction of uncertainty about the input data when\nthe output of a privacy-preserving mechanism is revealed. We study the relation\nof LMIDP with local differential privacy (LDP), the de facto standard notion of\nprivacy in context-independent (CI) scenarios, and with local information\nprivacy (LIP), the state-of-the-art notion for context-dependent settings. We\nestablish explicit conversion rules, i.e., bounds on the privacy parameters for\na LMIDP mechanism to also satisfy LDP/LIP, and vice versa. We use our bounds to\nformally verify that LMIDP is a weak privacy notion. We also show that\nuncorrelated Gaussian noise is the best-case noise in terms of CI-LMIDP if both\nthe input data and the noise are subject to an average power constraint.",
    "updated" : "2024-05-13T09:58:45Z",
    "published" : "2024-05-13T09:58:45Z",
    "authors" : [
      {
        "name" : "Khac-Hoang Ngo"
      },
      {
        "name" : "Johan Östman"
      },
      {
        "name" : "Alexandre Graell i Amat"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.07440v1",
    "title" : "Maximizing Information Gain in Privacy-Aware Active Learning of Email\n  Anomalies",
    "summary" : "Redacted emails satisfy most privacy requirements but they make it more\ndifficult to detect anomalous emails that may be indicative of data\nexfiltration. In this paper we develop an enhanced method of Active Learning\nusing an information gain maximizing heuristic, and we evaluate its\neffectiveness in a real world setting where only redacted versions of email\ncould be labeled by human analysts due to privacy concerns. In the first case\nstudy we examined how Active Learning should be carried out. We found that\nmodel performance was best when a single highly skilled (in terms of the\nlabelling task) analyst provided the labels. In the second case study we used\nconfidence ratings to estimate the labeling uncertainty of analysts and then\nprioritized instances for labeling based on the expected information gain (the\ndifference between model uncertainty and analyst uncertainty) that would be\nprovided by labelling each instance. We found that the information maximization\ngain heuristic improved model performance over existing sampling methods for\nActive Learning. Based on the results obtained, we recommend that analysts\nshould be screened, and possibly trained, prior to implementation of Active\nLearning in cybersecurity applications. We also recommend that the information\ngain maximizing sample method (based on expert confidence) should be used in\nearly stages of Active Learning, providing that well-calibrated confidence can\nbe obtained. We also note that the expertise of analysts should be assessed\nprior to Active Learning, as we found that analysts with lower labelling skill\nhad poorly calibrated (over-) confidence in their labels.",
    "updated" : "2024-05-13T02:58:59Z",
    "published" : "2024-05-13T02:58:59Z",
    "authors" : [
      {
        "name" : "Mu-Huan Miles Chung"
      },
      {
        "name" : "Sharon Li"
      },
      {
        "name" : "Jaturong Kongmanee"
      },
      {
        "name" : "Lu Wang"
      },
      {
        "name" : "Yuhong Yang"
      },
      {
        "name" : "Calvin Giang"
      },
      {
        "name" : "Khilan Jerath"
      },
      {
        "name" : "Abhay Raman"
      },
      {
        "name" : "David Lie"
      },
      {
        "name" : "Mark Chignell"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.07020v1",
    "title" : "Adaptive Online Bayesian Estimation of Frequency Distributions with\n  Local Differential Privacy",
    "summary" : "We propose a novel Bayesian approach for the adaptive and online estimation\nof the frequency distribution of a finite number of categories under the local\ndifferential privacy (LDP) framework. The proposed algorithm performs Bayesian\nparameter estimation via posterior sampling and adapts the randomization\nmechanism for LDP based on the obtained posterior samples. We propose a\nrandomized mechanism for LDP which uses a subset of categories as an input and\nwhose performance depends on the selected subset and the true frequency\ndistribution. By using the posterior sample as an estimate of the frequency\ndistribution, the algorithm performs a computationally tractable subset\nselection step to maximize the utility of the privatized response of the next\nuser. We propose several utility functions related to well-known information\nmetrics, such as (but not limited to) Fisher information matrix, total\nvariation distance, and information entropy. We compare each of these utility\nmetrics in terms of their computational complexity. We employ stochastic\ngradient Langevin dynamics for posterior sampling, a computationally efficient\napproximate Markov chain Monte Carlo method. We provide a theoretical analysis\nshowing that (i) the posterior distribution targeted by the algorithm converges\nto the true parameter even for approximate posterior sampling, and (ii) the\nalgorithm selects the optimal subset with high probability if posterior\nsampling is performed exactly. We also provide numerical results that\nempirically demonstrate the estimation accuracy of our algorithm where we\ncompare it with nonadaptive and semi-adaptive approaches under experimental\nsettings with various combinations of privacy parameters and population\ndistribution parameters.",
    "updated" : "2024-05-11T13:59:52Z",
    "published" : "2024-05-11T13:59:52Z",
    "authors" : [
      {
        "name" : "Soner Aydin"
      },
      {
        "name" : "Sinan Yildirim"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.08801v1",
    "title" : "Prospects of Privacy Advantage in Quantum Machine Learning",
    "summary" : "Ensuring data privacy in machine learning models is critical, particularly in\ndistributed settings where model gradients are typically shared among multiple\nparties to allow collaborative learning. Motivated by the increasing success of\nrecovering input data from the gradients of classical models, this study\naddresses a central question: How hard is it to recover the input data from the\ngradients of quantum machine learning models? Focusing on variational quantum\ncircuits (VQC) as learning models, we uncover the crucial role played by the\ndynamical Lie algebra (DLA) of the VQC ansatz in determining privacy\nvulnerabilities. While the DLA has previously been linked to the classical\nsimulatability and trainability of VQC models, this work, for the first time,\nestablishes its connection to the privacy of VQC models. In particular, we show\nthat properties conducive to the trainability of VQCs, such as a\npolynomial-sized DLA, also facilitate the extraction of detailed snapshots of\nthe input. We term this a weak privacy breach, as the snapshots enable training\nVQC models for distinct learning tasks without direct access to the original\ninput. Further, we investigate the conditions for a strong privacy breach where\nthe original input data can be recovered from these snapshots by classical or\nquantum-assisted polynomial time methods. We establish conditions on the\nencoding map such as classical simulatability, overlap with DLA basis, and its\nFourier frequency characteristics that enable such a privacy breach of VQC\nmodels. Our findings thus play a crucial role in detailing the prospects of\nquantum privacy advantage by guiding the requirements for designing quantum\nmachine learning models that balance trainability with robust privacy\nprotection.",
    "updated" : "2024-05-14T17:49:18Z",
    "published" : "2024-05-14T17:49:18Z",
    "authors" : [
      {
        "name" : "Jamie Heredge"
      },
      {
        "name" : "Niraj Kumar"
      },
      {
        "name" : "Dylan Herman"
      },
      {
        "name" : "Shouvanik Chakrabarti"
      },
      {
        "name" : "Romina Yalovetzky"
      },
      {
        "name" : "Shree Hari Sureshbabu"
      },
      {
        "name" : "Marco Pistoia"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.08698v1",
    "title" : "Byzantine-Resilient Secure Aggregation for Federated Learning Without\n  Privacy Compromises",
    "summary" : "Federated learning (FL) shows great promise in large scale machine learning,\nbut brings new risks in terms of privacy and security. We propose ByITFL, a\nnovel scheme for FL that provides resilience against Byzantine users while\nkeeping the users' data private from the federator and private from other\nusers. The scheme builds on the preexisting non-private FLTrust scheme, which\ntolerates malicious users through trust scores (TS) that attenuate or amplify\nthe users' gradients. The trust scores are based on the ReLU function, which we\napproximate by a polynomial. The distributed and privacy-preserving computation\nin ByITFL is designed using a combination of Lagrange coded computing,\nverifiable secret sharing and re-randomization steps. ByITFL is the first\nByzantine resilient scheme for FL with full information-theoretic privacy.",
    "updated" : "2024-05-14T15:37:56Z",
    "published" : "2024-05-14T15:37:56Z",
    "authors" : [
      {
        "name" : "Yue Xia"
      },
      {
        "name" : "Christoph Hofmeister"
      },
      {
        "name" : "Maximilian Egger"
      },
      {
        "name" : "Rawad Bitar"
      }
    ],
    "categories" : [
      "cs.IT",
      "cs.CR",
      "cs.DC",
      "cs.LG",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.08518v1",
    "title" : "Cryptography-Based Privacy-Preserving Method for Distributed\n  Optimization over Time-Varying Directed Graphs with Enhanced Efficiency",
    "summary" : "In this paper, we study the privacy-preserving distributed optimization\nproblem, aiming to prevent attackers from stealing the private information of\nagents. For this purpose, we propose a novel privacy-preserving algorithm based\non the Advanced Encryption Standard (AES), which is both secure and\ncomputationally efficient. By appropriately constructing the underlying weight\nmatrices, our algorithm can be applied to time-varying directed networks. We\nshow that the proposed algorithm can protect an agent's privacy if the agent\nhas at least one legitimate neighbor at the initial iteration. Under the\nassumption that the objective function is strongly convex and Lipschitz smooth,\nwe rigorously prove that the proposed algorithm has a linear convergence rate.\nFinally, the effectiveness of the proposed algorithm is demonstrated by\nnumerical simulations of the canonical sensor fusion problem.",
    "updated" : "2024-05-14T11:48:59Z",
    "published" : "2024-05-14T11:48:59Z",
    "authors" : [
      {
        "name" : "Bing Liu"
      },
      {
        "name" : "Furan Xie"
      },
      {
        "name" : "Li Chai"
      }
    ],
    "categories" : [
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.08356v1",
    "title" : "A Model-oriented Reasoning Framework for Privacy Analysis of Complex\n  Systems",
    "summary" : "This paper proposes a reasoning framework for privacy properties of systems\nand their environments that can capture any knowledge leaks on different\nlogical levels of the system to answer the question: which entity can learn\nwhat? With the term knowledge we refer to any kind of data, meta-data or\ninterpretation of those that might be relevant. To achieve this, we present a\nmodeling framework that forces the developers to explicitly describe which\nknowledge is available at which entity, which knowledge flows between entities\nand which knowledge can be inferred from other knowledge. In addition, privacy\nrequirements are specified as rules describing forbidden knowledge for\nentities. Our modeling approach is incremental, starting from an abstract view\nof the system and adding details through well-defined transformations. This\nwork is intended to complement existing approaches and introduces steps towards\nmore formal foundations for privacy oriented analyses while keeping them as\naccessible as possible. It is designed to be extensible through schemata and\nvocabulary to enable compatibility with external requirements and standards.",
    "updated" : "2024-05-14T06:52:56Z",
    "published" : "2024-05-14T06:52:56Z",
    "authors" : [
      {
        "name" : "Sebastian Rehms"
      },
      {
        "name" : "Stefan Köpsell"
      },
      {
        "name" : "Verena Klös"
      },
      {
        "name" : "Florian Tschorsch"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.08084v1",
    "title" : "PrivFED -- A Framework for Privacy-Preserving Federated Learning in\n  Enhanced Breast Cancer Diagnosis",
    "summary" : "In the day-to-day operations of healthcare institutions, a multitude of\nPersonally Identifiable Information (PII) data exchanges occur, exposing the\ndata to a spectrum of cybersecurity threats. This study introduces a federated\nlearning framework, trained on the Wisconsin dataset, to mitigate challenges\nsuch as data scarcity and imbalance. Techniques like the Synthetic Minority\nOver-sampling Technique (SMOTE) are incorporated to bolster robustness, while\nisolation forests are employed to fortify the model against outliers. Catboost\nserves as the classification tool across all devices. The identification of\noptimal features for heightened accuracy is pursued through Principal Component\nAnalysis (PCA),accentuating the significance of hyperparameter tuning, as\nunderscored in a comparative analysis. The model exhibits an average accuracy\nof 99.95% on edge devices and 98% on the central server.",
    "updated" : "2024-05-13T18:01:57Z",
    "published" : "2024-05-13T18:01:57Z",
    "authors" : [
      {
        "name" : "Maithili Jha"
      },
      {
        "name" : "S. Maitri"
      },
      {
        "name" : "M. Lohithdakshan"
      },
      {
        "name" : "Shiny Duela J"
      },
      {
        "name" : "K. Raja"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.09306v1",
    "title" : "Words Blending Boxes. Obfuscating Queries in Information Retrieval using\n  Differential Privacy",
    "summary" : "Ensuring the effectiveness of search queries while protecting user privacy\nremains an open issue. When an Information Retrieval System (IRS) does not\nprotect the privacy of its users, sensitive information may be disclosed\nthrough the queries sent to the system. Recent improvements, especially in NLP,\nhave shown the potential of using Differential Privacy to obfuscate texts while\nmaintaining satisfactory effectiveness. However, such approaches may protect\nthe user's privacy only from a theoretical perspective while, in practice, the\nreal user's information need can still be inferred if perturbed terms are too\nsemantically similar to the original ones. We overcome such limitations by\nproposing Word Blending Boxes, a novel differentially private mechanism for\nquery obfuscation, which protects the words in the user queries by employing\nsafe boxes. To measure the overall effectiveness of the proposed WBB mechanism,\nwe measure the privacy obtained by the obfuscation process, i.e., the lexical\nand semantic similarity between original and obfuscated queries. Moreover, we\nassess the effectiveness of the privatized queries in retrieving relevant\ndocuments from the IRS. Our findings indicate that WBB can be integrated\neffectively into existing IRSs, offering a key to the challenge of protecting\nuser privacy from both a theoretical and a practical point of view.",
    "updated" : "2024-05-15T12:51:36Z",
    "published" : "2024-05-15T12:51:36Z",
    "authors" : [
      {
        "name" : "Francesco Luigi De Faveri"
      },
      {
        "name" : "Guglielmo Faggioli"
      },
      {
        "name" : "Nicola Ferro"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.09234v1",
    "title" : "Enhancing Image Privacy in Semantic Communication over Wiretap Channels\n  leveraging Differential Privacy",
    "summary" : "Semantic communication (SemCom) enhances transmission efficiency by sending\nonly task-relevant information compared to traditional methods. However,\ntransmitting semantic-rich data over insecure or public channels poses security\nand privacy risks. This paper addresses the privacy problem of transmitting\nimages over wiretap channels and proposes a novel SemCom approach ensuring\nprivacy through a differential privacy (DP)-based image protection and\ndeprotection mechanism. The method utilizes the GAN inversion technique to\nextract disentangled semantic features and applies a DP mechanism to protect\nsensitive features within the extracted semantic information. To address the\nnon-invertibility of DP, we introduce two neural networks to approximate the DP\napplication and removal processes, offering a privacy protection level close to\nthat by the original DP process. Simulation results validate the effectiveness\nof our method in preventing eavesdroppers from obtaining sensitive information\nwhile maintaining high-fidelity image reconstruction at the legitimate\nreceiver.",
    "updated" : "2024-05-15T10:30:18Z",
    "published" : "2024-05-15T10:30:18Z",
    "authors" : [
      {
        "name" : "Weixuan Chen"
      },
      {
        "name" : "Shunpu Tang"
      },
      {
        "name" : "Qianqian Yang"
      }
    ],
    "categories" : [
      "eess.IV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.09230v1",
    "title" : "Reduce to the MACs -- Privacy Friendly Generic Probe Requests",
    "summary" : "Abstract. Since the introduction of active discovery in Wi-Fi networks, users\ncan be tracked via their probe requests. Although manufacturers typically try\nto conceal Media Access Control (MAC) addresses using MAC address\nrandomisation, probe requests still contain Information Elements (IEs) that\nfacilitate device identification. This paper introduces generic probe requests:\nBy removing all unnecessary information from IEs, the requests become\nindistinguishable from one another, letting single devices disappear in the\nlargest possible anonymity set. Conducting a comprehensive evaluation, we\ndemonstrate that a large IE set contained within undirected probe requests does\nnot necessarily imply fast connection establishment. Furthermore, we show that\nminimising IEs to nothing but Supported Rates would enable 82.55% of the\ndevices to share the same anonymity set. Our contributions provide a\nsignificant advancement in the pursuit of robust privacy solutions for wireless\nnetworks, paving the way for more user anonymity and less surveillance in\nwireless communication ecosystems.",
    "updated" : "2024-05-15T10:18:30Z",
    "published" : "2024-05-15T10:18:30Z",
    "authors" : [
      {
        "name" : "Johanna Ansohn McDougall"
      },
      {
        "name" : "Alessandro Brighente"
      },
      {
        "name" : "Anne Kunstmann"
      },
      {
        "name" : "Niklas Zapatka"
      },
      {
        "name" : "Hannes Federrath"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.09014v1",
    "title" : "Feature-based Federated Transfer Learning: Communication Efficiency,\n  Robustness and Privacy",
    "summary" : "In this paper, we propose feature-based federated transfer learning as a\nnovel approach to improve communication efficiency by reducing the uplink\npayload by multiple orders of magnitude compared to that of existing approaches\nin federated learning and federated transfer learning. Specifically, in the\nproposed feature-based federated learning, we design the extracted features and\noutputs to be uploaded instead of parameter updates. For this distributed\nlearning model, we determine the required payload and provide comparisons with\nthe existing schemes. Subsequently, we analyze the robustness of feature-based\nfederated transfer learning against packet loss, data insufficiency, and\nquantization. Finally, we address privacy considerations by defining and\nanalyzing label privacy leakage and feature privacy leakage, and investigating\nmitigating approaches. For all aforementioned analyses, we evaluate the\nperformance of the proposed learning scheme via experiments on an image\nclassification task and a natural language processing task to demonstrate its\neffectiveness.",
    "updated" : "2024-05-15T00:43:19Z",
    "published" : "2024-05-15T00:43:19Z",
    "authors" : [
      {
        "name" : "Feng Wang"
      },
      {
        "name" : "M. Cenk Gursoy"
      },
      {
        "name" : "Senem Velipasalar"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.MA"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.08920v1",
    "title" : "Neural Collapse Meets Differential Privacy: Curious Behaviors of NoisyGD\n  with Near-perfect Representation Learning",
    "summary" : "A recent study by De et al. (2022) has reported that large-scale\nrepresentation learning through pre-training on a public dataset significantly\nenhances differentially private (DP) learning in downstream tasks, despite the\nhigh dimensionality of the feature space. To theoretically explain this\nphenomenon, we consider the setting of a layer-peeled model in representation\nlearning, which results in interesting phenomena related to learned features in\ndeep learning and transfer learning, known as Neural Collapse (NC).\n  Within the framework of NC, we establish an error bound indicating that the\nmisclassification error is independent of dimension when the distance between\nactual features and the ideal ones is smaller than a threshold. Additionally,\nthe quality of the features in the last layer is empirically evaluated under\ndifferent pre-trained models within the framework of NC, showing that a more\npowerful transformer leads to a better feature representation. Furthermore, we\nreveal that DP fine-tuning is less robust compared to fine-tuning without DP,\nparticularly in the presence of perturbations. These observations are supported\nby both theoretical analyses and experimental evaluation. Moreover, to enhance\nthe robustness of DP fine-tuning, we suggest several strategies, such as\nfeature normalization or employing dimension reduction methods like Principal\nComponent Analysis (PCA). Empirically, we demonstrate a significant improvement\nin testing accuracy by conducting PCA on the last-layer features.",
    "updated" : "2024-05-14T19:18:19Z",
    "published" : "2024-05-14T19:18:19Z",
    "authors" : [
      {
        "name" : "Chendi Wang"
      },
      {
        "name" : "Yuqing Zhu"
      },
      {
        "name" : "Weijie J. Su"
      },
      {
        "name" : "Yu-Xiang Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.08801v2",
    "title" : "Prospects of Privacy Advantage in Quantum Machine Learning",
    "summary" : "Ensuring data privacy in machine learning models is critical, particularly in\ndistributed settings where model gradients are typically shared among multiple\nparties to allow collaborative learning. Motivated by the increasing success of\nrecovering input data from the gradients of classical models, this study\naddresses a central question: How hard is it to recover the input data from the\ngradients of quantum machine learning models? Focusing on variational quantum\ncircuits (VQC) as learning models, we uncover the crucial role played by the\ndynamical Lie algebra (DLA) of the VQC ansatz in determining privacy\nvulnerabilities. While the DLA has previously been linked to the classical\nsimulatability and trainability of VQC models, this work, for the first time,\nestablishes its connection to the privacy of VQC models. In particular, we show\nthat properties conducive to the trainability of VQCs, such as a\npolynomial-sized DLA, also facilitate the extraction of detailed snapshots of\nthe input. We term this a weak privacy breach, as the snapshots enable training\nVQC models for distinct learning tasks without direct access to the original\ninput. Further, we investigate the conditions for a strong privacy breach where\nthe original input data can be recovered from these snapshots by classical or\nquantum-assisted polynomial time methods. We establish conditions on the\nencoding map such as classical simulatability, overlap with DLA basis, and its\nFourier frequency characteristics that enable such a privacy breach of VQC\nmodels. Our findings thus play a crucial role in detailing the prospects of\nquantum privacy advantage by guiding the requirements for designing quantum\nmachine learning models that balance trainability with robust privacy\nprotection.",
    "updated" : "2024-05-15T17:46:34Z",
    "published" : "2024-05-14T17:49:18Z",
    "authors" : [
      {
        "name" : "Jamie Heredge"
      },
      {
        "name" : "Niraj Kumar"
      },
      {
        "name" : "Dylan Herman"
      },
      {
        "name" : "Shouvanik Chakrabarti"
      },
      {
        "name" : "Romina Yalovetzky"
      },
      {
        "name" : "Shree Hari Sureshbabu"
      },
      {
        "name" : "Changhao Li"
      },
      {
        "name" : "Marco Pistoia"
      }
    ],
    "categories" : [
      "quant-ph",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.05567v1",
    "title" : "Perfect Subset Privacy in Polynomial Computation",
    "summary" : "Delegating large-scale computations to service providers is a common practice\nwhich raises privacy concerns. This paper studies information-theoretic\nprivacy-preserving delegation of data to a service provider, who may further\ndelegate the computation to auxiliary worker nodes, in order to compute a\npolynomial over that data at a later point in time. We study techniques which\nare compatible with robust management of distributed computation systems, an\narea known as coded computing. Privacy in coded computing, however, has\ntraditionally addressed the problem of colluding workers, and assumed that the\nserver that administrates the computation is trusted. This viewpoint of privacy\ndoes not accurately reflect real-world privacy concerns, since normally, the\nservice provider as a whole (i.e., the administrator and the worker nodes) form\none cohesive entity which itself poses a privacy risk. This paper aims to shift\nthe focus of privacy in coded computing to safeguarding the privacy of the user\nagainst the service provider as a whole, instead of merely against colluding\nworkers inside the service provider. To this end, we leverage the recently\ndefined notion of perfect subset privacy, which guarantees zero information\nleakage from all subsets of the data up to a certain size. Using known\ntechniques from Reed-Muller decoding, we provide a scheme which enables\npolynomial computation with perfect subset privacy in straggler-free systems.\nFurthermore, by studying information super-sets in Reed-Muller codes, which may\nbe of independent interest, we extend the previous scheme to tolerate\nstraggling worker nodes inside the service provider.",
    "updated" : "2024-05-09T06:11:58Z",
    "published" : "2024-05-09T06:11:58Z",
    "authors" : [
      {
        "name" : "Zirui Deng"
      },
      {
        "name" : "Vinayak Ramkumar"
      },
      {
        "name" : "Netanel Raviv"
      }
    ],
    "categories" : [
      "cs.IT",
      "math.IT"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.10096v1",
    "title" : "The Effect of Quantization in Federated Learning: A Rényi Differential\n  Privacy Perspective",
    "summary" : "Federated Learning (FL) is an emerging paradigm that holds great promise for\nprivacy-preserving machine learning using distributed data. To enhance privacy,\nFL can be combined with Differential Privacy (DP), which involves adding\nGaussian noise to the model weights. However, FL faces a significant challenge\nin terms of large communication overhead when transmitting these model weights.\nTo address this issue, quantization is commonly employed. Nevertheless, the\npresence of quantized Gaussian noise introduces complexities in understanding\nprivacy protection. This research paper investigates the impact of quantization\non privacy in FL systems. We examine the privacy guarantees of quantized\nGaussian mechanisms using R\\'enyi Differential Privacy (RDP). By deriving the\nprivacy budget of quantized Gaussian mechanisms, we demonstrate that lower\nquantization bit levels provide improved privacy protection. To validate our\ntheoretical findings, we employ Membership Inference Attacks (MIA), which gauge\nthe accuracy of privacy leakage. The numerical results align with our\ntheoretical analysis, confirming that quantization can indeed enhance privacy\nprotection. This study not only enhances our understanding of the correlation\nbetween privacy and communication in FL but also underscores the advantages of\nquantization in preserving privacy.",
    "updated" : "2024-05-16T13:50:46Z",
    "published" : "2024-05-16T13:50:46Z",
    "authors" : [
      {
        "name" : "Tianqu Kang"
      },
      {
        "name" : "Lumin Liu"
      },
      {
        "name" : "Hengtao He"
      },
      {
        "name" : "Jun Zhang"
      },
      {
        "name" : "S. H. Song"
      },
      {
        "name" : "Khaled B. Letaief"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.09882v1",
    "title" : "DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy\n  Protection",
    "summary" : "With the rapid development of face recognition (FR) systems, the privacy of\nface images on social media is facing severe challenges due to the abuse of\nunauthorized FR systems. Some studies utilize adversarial attack techniques to\ndefend against malicious FR systems by generating adversarial examples.\nHowever, the generated adversarial examples, i.e., the protected face images,\ntend to suffer from subpar visual quality and low transferability. In this\npaper, we propose a novel face protection approach, dubbed DiffAM, which\nleverages the powerful generative ability of diffusion models to generate\nhigh-quality protected face images with adversarial makeup transferred from\nreference images. To be specific, we first introduce a makeup removal module to\ngenerate non-makeup images utilizing a fine-tuned diffusion model with guidance\nof textual prompts in CLIP space. As the inverse process of makeup transfer,\nmakeup removal can make it easier to establish the deterministic relationship\nbetween makeup domain and non-makeup domain regardless of elaborate text\nprompts. Then, with this relationship, a CLIP-based makeup loss along with an\nensemble attack strategy is introduced to jointly guide the direction of\nadversarial makeup domain, achieving the generation of protected face images\nwith natural-looking makeup and high black-box transferability. Extensive\nexperiments demonstrate that DiffAM achieves higher visual quality and attack\nsuccess rates with a gain of 12.98% under black-box setting compared with the\nstate of the arts. The code will be available at\nhttps://github.com/HansSunY/DiffAM.",
    "updated" : "2024-05-16T08:05:36Z",
    "published" : "2024-05-16T08:05:36Z",
    "authors" : [
      {
        "name" : "Yuhao Sun"
      },
      {
        "name" : "Lingyun Yu"
      },
      {
        "name" : "Hongtao Xie"
      },
      {
        "name" : "Jiaming Li"
      },
      {
        "name" : "Yongdong Zhang"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.08920v2",
    "title" : "Neural Collapse Meets Differential Privacy: Curious Behaviors of NoisyGD\n  with Near-perfect Representation Learning",
    "summary" : "A recent study by De et al. (2022) has reported that large-scale\nrepresentation learning through pre-training on a public dataset significantly\nenhances differentially private (DP) learning in downstream tasks, despite the\nhigh dimensionality of the feature space. To theoretically explain this\nphenomenon, we consider the setting of a layer-peeled model in representation\nlearning, which results in interesting phenomena related to learned features in\ndeep learning and transfer learning, known as Neural Collapse (NC).\n  Within the framework of NC, we establish an error bound indicating that the\nmisclassification error is independent of dimension when the distance between\nactual features and the ideal ones is smaller than a threshold. Additionally,\nthe quality of the features in the last layer is empirically evaluated under\ndifferent pre-trained models within the framework of NC, showing that a more\npowerful transformer leads to a better feature representation. Furthermore, we\nreveal that DP fine-tuning is less robust compared to fine-tuning without DP,\nparticularly in the presence of perturbations. These observations are supported\nby both theoretical analyses and experimental evaluation. Moreover, to enhance\nthe robustness of DP fine-tuning, we suggest several strategies, such as\nfeature normalization or employing dimension reduction methods like Principal\nComponent Analysis (PCA). Empirically, we demonstrate a significant improvement\nin testing accuracy by conducting PCA on the last-layer features.",
    "updated" : "2024-05-16T12:06:03Z",
    "published" : "2024-05-14T19:18:19Z",
    "authors" : [
      {
        "name" : "Chendi Wang"
      },
      {
        "name" : "Yuqing Zhu"
      },
      {
        "name" : "Weijie J. Su"
      },
      {
        "name" : "Yu-Xiang Wang"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.CV",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.10904v1",
    "title" : "Broadening Privacy and Surveillance: Eliciting Interconnected Values\n  with a Scenarios Workbook on Smart Home Cameras",
    "summary" : "We use a design workbook of speculative scenarios as a values elicitation\nactivity with 14 participants. The workbook depicts use case scenarios with\nsmart home camera technologies that involve surveillance and uneven power\nrelations. The scenarios were initially designed by the researchers to explore\nscenarios of privacy and surveillance within three social relationships\ninvolving \"primary\" and \"non-primary\" users: Parents-Children,\nLandlords-Tenants, and Residents-Domestic Workers. When the scenarios were\nutilized as part of a values elicitation activity with participants, we found\nthat they reflected on a broader set of interconnected social values beyond\nprivacy and surveillance, including autonomy and agency, physical safety,\nproperty rights, trust and accountability, and fairness. The paper suggests\nthat future research about ethical issues in smart homes should conceptualize\nprivacy as interconnected with a broader set of social values (which can align\nor be in tension with privacy), and reflects on considerations for doing\nresearch with non-primary users.",
    "updated" : "2024-05-17T16:50:49Z",
    "published" : "2024-05-17T16:50:49Z",
    "authors" : [
      {
        "name" : "Richmond Y. Wong"
      },
      {
        "name" : "Jason Caleb Valdez"
      },
      {
        "name" : "Ashten Alexander"
      },
      {
        "name" : "Ariel Chiang"
      },
      {
        "name" : "Olivia Quesada"
      },
      {
        "name" : "James Pierce"
      }
    ],
    "categories" : [
      "cs.HC",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.10870v1",
    "title" : "Multicenter Privacy-Preserving Model Training for Deep Learning Brain\n  Metastases Autosegmentation",
    "summary" : "Objectives: This work aims to explore the impact of multicenter data\nheterogeneity on deep learning brain metastases (BM) autosegmentation\nperformance, and assess the efficacy of an incremental transfer learning\ntechnique, namely learning without forgetting (LWF), to improve model\ngeneralizability without sharing raw data.\n  Materials and methods: A total of six BM datasets from University Hospital\nErlangen (UKER), University Hospital Zurich (USZ), Stanford, UCSF, NYU and\nBraTS Challenge 2023 on BM segmentation were used for this evaluation. First,\nthe multicenter performance of a convolutional neural network (DeepMedic) for\nBM autosegmentation was established for exclusive single-center training and\nfor training on pooled data, respectively. Subsequently bilateral collaboration\nwas evaluated, where a UKER pretrained model is shared to another center for\nfurther training using transfer learning (TL) either with or without LWF.\n  Results: For single-center training, average F1 scores of BM detection range\nfrom 0.625 (NYU) to 0.876 (UKER) on respective single-center test data. Mixed\nmulticenter training notably improves F1 scores at Stanford and NYU, with\nnegligible improvement at other centers. When the UKER pretrained model is\napplied to USZ, LWF achieves a higher average F1 score (0.839) than naive TL\n(0.570) and single-center training (0.688) on combined UKER and USZ test data.\nNaive TL improves sensitivity and contouring accuracy, but compromises\nprecision. Conversely, LWF demonstrates commendable sensitivity, precision and\ncontouring accuracy. When applied to Stanford, similar performance was\nobserved.\n  Conclusion: Data heterogeneity results in varying performance in BM\nautosegmentation, posing challenges to model generalizability. LWF is a\npromising approach to peer-to-peer privacy-preserving model training.",
    "updated" : "2024-05-17T16:01:11Z",
    "published" : "2024-05-17T16:01:11Z",
    "authors" : [
      {
        "name" : "Yixing Huang"
      },
      {
        "name" : "Zahra Khodabakhshi"
      },
      {
        "name" : "Ahmed Gomaa"
      },
      {
        "name" : "Manuel Schmidt"
      },
      {
        "name" : "Rainer Fietkau"
      },
      {
        "name" : "Matthias Guckenberger"
      },
      {
        "name" : "Nicolaus Andratschke"
      },
      {
        "name" : "Christoph Bert"
      },
      {
        "name" : "Stephanie Tanadini-Lang"
      },
      {
        "name" : "Florian Putz"
      }
    ],
    "categories" : [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.10868v1",
    "title" : "Air Signing and Privacy-Preserving Signature Verification for Digital\n  Documents",
    "summary" : "This paper presents a novel approach to the digital signing of electronic\ndocuments through the use of a camera-based interaction system, single-finger\ntracking for sign recognition, and multi commands executing hand gestures. The\nproposed solution, referred to as \"Air Signature,\" involves writing the\nsignature in front of the camera, rather than relying on traditional methods\nsuch as mouse drawing or physically signing on paper and showing it to a web\ncamera. The goal is to develop a state-of-the-art method for detecting and\ntracking gestures and objects in real-time. The proposed methods include\napplying existing gesture recognition and object tracking systems, improving\naccuracy through smoothing and line drawing, and maintaining continuity during\nfast finger movements. An evaluation of the fingertip detection, sketching, and\noverall signing process is performed to assess the effectiveness of the\nproposed solution. The secondary objective of this research is to develop a\nmodel that can effectively recognize the unique signature of a user. This type\nof signature can be verified by neural cores that analyze the movement, speed,\nand stroke pixels of the signing in real time. The neural cores use machine\nlearning algorithms to match air signatures to the individual's stored\nsignatures, providing a secure and efficient method of verification. Our\nproposed System does not require sensors or any hardware other than the camera.",
    "updated" : "2024-05-17T16:00:10Z",
    "published" : "2024-05-17T16:00:10Z",
    "authors" : [
      {
        "name" : "P. Sarveswarasarma"
      },
      {
        "name" : "T. Sathulakjan"
      },
      {
        "name" : "V. J. V. Godfrey"
      },
      {
        "name" : "Thanuja D. Ambegoda"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.10521v1",
    "title" : "Generative AI for Secure and Privacy-Preserving Mobile Crowdsensing",
    "summary" : "Recently, generative AI has attracted much attention from both academic and\nindustrial fields, which has shown its potential, especially in the data\ngeneration and synthesis aspects. Simultaneously, secure and privacy-preserving\nmobile crowdsensing (SPPMCS) has been widely applied in data collection/\nacquirement due to an advantage on low deployment cost, flexible\nimplementation, and high adaptability. Since generative AI can generate new\nsynthetic data to replace the original data to be analyzed and processed, it\ncan lower data attacks and privacy leakage risks for the original data.\nTherefore, integrating generative AI into SPPMCS is feasible and significant.\nMoreover, this paper investigates an integration of generative AI in SPPMCS,\nwhere we present potential research focuses, solutions, and case studies.\nSpecifically, we firstly review the preliminaries for generative AI and SPPMCS,\nwhere their integration potential is presented. Then, we discuss research\nissues and solutions for generative AI-enabled SPPMCS, including security\ndefense of malicious data injection, illegal authorization, malicious spectrum\nmanipulation at the physical layer, and privacy protection on sensing data\ncontent, sensing terminals' identification and location. Next, we propose a\nframework for sensing data content protection with generative AI, and\nsimulations results have clearly demonstrated the effectiveness of the proposed\nframework. Finally, we present major research directions for generative\nAI-enabled SPPMCS.",
    "updated" : "2024-05-17T04:00:58Z",
    "published" : "2024-05-17T04:00:58Z",
    "authors" : [
      {
        "name" : "Yaoqi Yang"
      },
      {
        "name" : "Bangning Zhang"
      },
      {
        "name" : "Daoxing Guo"
      },
      {
        "name" : "Hongyang Du"
      },
      {
        "name" : "Zehui Xiong"
      },
      {
        "name" : "Dusit Niyato"
      },
      {
        "name" : "Zhu Han"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.11975v1",
    "title" : "A Stochastic Sampling Approach to Privacy",
    "summary" : "This paper proposes an optimal stochastic sampling approach to privacy, in\nwhich a sensor observes a process which is correlated to private information.\nIn out set-up, a sampler decides to keep or discard the sensor's observations.\nThe kept samples are shared with an adversary who might attempt to infer the\nprivate process based on the sampler's output. The privacy leakages are\ncaptured with the mutual information between the private process and sampler's\noutput. We cast the optimal sampling design as an optimization problem with two\nobjectives: (i) minimizing the reconstruction error of the observed process\nusing the sampler's output, (ii) reducing the privacy leakages. We first show\nthe optimal reconstruction policy is deterministic and can be obtained by\nsolving a one-step optimization problem at each time step. We also derive the\noptimality equations of the privacy-sampler for a general class of processes\nvia the dynamic decomposition method, and show the sampler controls the\nadversary's belief about the private input. Also, we propose a simplified\ndesign for linear Gaussian processes by restricting the sampling policy to a\nspecial collection. We show that the optimal reconstruction of the system state\nand the private process is similar to Kalman filter in the linear Gaussian\ncase, and the objective of the sampler design problem can be analytically\nexpressed based on a conditional mean and covariance matrix. Furthermore, we\ndevelop an numerical algorithm to optimize the sampling and reconstruction\npolicies, wherein the policy gradient theorem for the optimal sampling design\nis derived based on the implicit function theorem. Finally, we verify our\ndesign and show it capabilities in state reconstruction, privacy protection and\ndata size reduction via simulations.",
    "updated" : "2024-05-20T12:10:26Z",
    "published" : "2024-05-20T12:10:26Z",
    "authors" : [
      {
        "name" : "Chuanghong Weng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.11923v1",
    "title" : "Rate Optimality and Phase Transition for User-Level Local Differential\n  Privacy",
    "summary" : "Most of the literature on differential privacy considers the item-level case\nwhere each user has a single observation, but a growing field of interest is\nthat of user-level privacy where each of the $n$ users holds $T$ observations\nand wishes to maintain the privacy of their entire collection.\n  In this paper, we derive a general minimax lower bound, which shows that, for\nlocally private user-level estimation problems, the risk cannot, in general, be\nmade to vanish for a fixed number of users even when each user holds an\narbitrarily large number of observations. We then derive matching, up to\nlogarithmic factors, lower and upper bounds for univariate and multidimensional\nmean estimation, sparse mean estimation and non-parametric density estimation.\nIn particular, with other model parameters held fixed, we observe phase\ntransition phenomena in the minimax rates as $T$ the number of observations\neach user holds varies.\n  In the case of (non-sparse) mean estimation and density estimation, we see\nthat, for $T$ below a phase transition boundary, the rate is the same as having\n$nT$ users in the item-level setting. Different behaviour is however observed\nin the case of $s$-sparse $d$-dimensional mean estimation, wherein consistent\nestimation is impossible when $d$ exceeds the number of observations in the\nitem-level setting, but is possible in the user-level setting when $T \\gtrsim s\n\\log (d)$, up to logarithmic factors. This may be of independent interest for\napplications as an example of a high-dimensional problem that is feasible under\nlocal privacy constraints.",
    "updated" : "2024-05-20T09:59:03Z",
    "published" : "2024-05-20T09:59:03Z",
    "authors" : [
      {
        "name" : "Alexander Kent"
      },
      {
        "name" : "Thomas B. Berrett"
      },
      {
        "name" : "Yi Yu"
      }
    ],
    "categories" : [
      "math.ST",
      "stat.ME",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.11713v1",
    "title" : "Decentralized Privacy Preservation for Critical Connections in Graphs",
    "summary" : "Many real-world interconnections among entities can be characterized as\ngraphs. Collecting local graph information with balanced privacy and data\nutility has garnered notable interest recently. This paper delves into the\nproblem of identifying and protecting critical information of entity\nconnections for individual participants in a graph based on cohesive subgraph\nsearches. This problem has not been addressed in the literature. To address the\nproblem, we propose to extract the critical connections of a queried vertex\nusing a fortress-like cohesive subgraph model known as $p$-cohesion. A user's\nconnections within a fortress are obfuscated when being released, to protect\ncritical information about the user. Novel merit and penalty score functions\nare designed to measure each participant's critical connections in the minimal\n$p$-cohesion, facilitating effective identification of the connections. We\nfurther propose to preserve the privacy of a vertex enquired by only protecting\nits critical connections when responding to queries raised by data collectors.\nWe prove that, under the decentralized differential privacy (DDP) mechanism,\none's response satisfies $(\\varepsilon, \\delta)$-DDP when its critical\nconnections are protected while the rest remains unperturbed. The effectiveness\nof our proposed method is demonstrated through extensive experiments on\nreal-life graph datasets.",
    "updated" : "2024-05-20T01:22:21Z",
    "published" : "2024-05-20T01:22:21Z",
    "authors" : [
      {
        "name" : "Conggai Li"
      },
      {
        "name" : "Wei Ni"
      },
      {
        "name" : "Ming Ding"
      },
      {
        "name" : "Youyang Qu"
      },
      {
        "name" : "Jianjun Chen"
      },
      {
        "name" : "David Smith"
      },
      {
        "name" : "Wenjie Zhang"
      },
      {
        "name" : "Thierry Rakotoarivelo"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.11712v1",
    "title" : "Trust, Because You Can't Verify:Privacy and Security Hurdles in\n  Education Technology Acquisition Practices",
    "summary" : "The education technology (EdTech) landscape is expanding rapidly in higher\neducation institutes (HEIs). This growth brings enormous complexity. Protecting\nthe extensive data collected by these tools is crucial for HEIs. Privacy\nincidents of data breaches and misuses can have dire security and privacy\nconsequences on the data subjects, particularly students, who are often\ncompelled to use these tools. This urges an in-depth understanding of HEI and\nEdTech vendor dynamics, which is largely understudied.\n  To address this gap, we conduct a semi-structured interview study with 13\nparticipants who are in the EdTech leadership roles at seven HEIs. Our study\nuncovers the EdTech acquisition process in the HEI context, the consideration\nof security and privacy issues throughout that process, the pain points of HEI\npersonnel in establishing adequate security and privacy protection mechanisms\nin service contracts, and their struggle in holding vendors accountable due to\na lack of visibility into their system and power-asymmetry, among other\nreasons. We discuss certain observations about the status quo and conclude with\nrecommendations to improve the situation.",
    "updated" : "2024-05-20T01:15:57Z",
    "published" : "2024-05-20T01:15:57Z",
    "authors" : [
      {
        "name" : "Easton Kelso"
      },
      {
        "name" : "Ananta Soneji"
      },
      {
        "name" : "Sazzadur Rahaman"
      },
      {
        "name" : "Yan Soshitaishvili"
      },
      {
        "name" : "Rakibul Hasan"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.11580v1",
    "title" : "Securing Health Data on the Blockchain: A Differential Privacy and\n  Federated Learning Framework",
    "summary" : "This study proposes a framework to enhance privacy in Blockchain-based\nInternet of Things (BIoT) systems used in the healthcare sector. The framework\naddresses the challenge of leveraging health data for analytics while\nprotecting patient privacy. To achieve this, the study integrates Differential\nPrivacy (DP) with Federated Learning (FL) to protect sensitive health data\ncollected by IoT nodes. The proposed framework utilizes dynamic personalization\nand adaptive noise distribution strategies to balance privacy and data utility.\nAdditionally, blockchain technology ensures secure and transparent aggregation\nand storage of model updates. Experimental results on the SVHN dataset\ndemonstrate that the proposed framework achieves strong privacy guarantees\nagainst various attack scenarios while maintaining high accuracy in health\nanalytics tasks. For 15 rounds of federated learning with an epsilon value of\n8.0, the model obtains an accuracy of 64.50%. The blockchain integration,\nutilizing Ethereum, Ganache, Web3.py, and IPFS, exhibits an average transaction\nlatency of around 6 seconds and consistent gas consumption across rounds,\nvalidating the practicality and feasibility of the proposed approach.",
    "updated" : "2024-05-19T15:15:18Z",
    "published" : "2024-05-19T15:15:18Z",
    "authors" : [
      {
        "name" : "Daniel Commey"
      },
      {
        "name" : "Sena Hounsinou"
      },
      {
        "name" : "Garth V. Crosby"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.DC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.11419v1",
    "title" : "Sketches-based join size estimation under local differential privacy",
    "summary" : "Join size estimation on sensitive data poses a risk of privacy leakage. Local\ndifferential privacy (LDP) is a solution to preserve privacy while collecting\nsensitive data, but it introduces significant noise when dealing with sensitive\njoin attributes that have large domains. Employing probabilistic structures\nsuch as sketches is a way to handle large domains, but it leads to\nhash-collision errors. To achieve accurate estimations, it is necessary to\nreduce both the noise error and hash-collision error. To tackle the noise error\ncaused by protecting sensitive join values with large domains, we introduce a\nnovel algorithm called LDPJoinSketch for sketch-based join size estimation\nunder LDP. Additionally, to address the inherent hash-collision errors in\nsketches under LDP, we propose an enhanced method called LDPJoinSketch+. It\nutilizes a frequency-aware perturbation mechanism that effectively separates\nhigh-frequency and low-frequency items without compromising privacy. The\nproposed methods satisfy LDP, and the estimation error is bounded. Experimental\nresults show that our method outperforms existing methods, effectively\nenhancing the accuracy of join size estimation under LDP.",
    "updated" : "2024-05-19T01:21:54Z",
    "published" : "2024-05-19T01:21:54Z",
    "authors" : [
      {
        "name" : "Meifan Zhang"
      },
      {
        "name" : "Xin Liu"
      },
      {
        "name" : "Lihua Yin"
      }
    ],
    "categories" : [
      "cs.DB",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.11341v1",
    "title" : "A Secure and Privacy-Friendly Logging Scheme",
    "summary" : "Finding a robust security mechanism for audit trail logging has long been a\npoorly satisfied goal. There are many reasons for this. The most significant of\nthese is that the audit trail is a highly sought after goal of attackers to\nensure that they do not get caught. Thus they have an incredibly strong\nincentive to prevent companies from succeeding in this worthy aim. Regulation,\nsuch as the European Union General Data Protection Regulation, has brought a\nstrong incentive for companies to achieve success in this area due to the\npunitive level of fines that can now be levied in the event of a successful\nbreach by an attacker. We seek to resolve this issue through the use of an\nencrypted audit trail process that saves encrypted records to a true immutable\ndatabase, which can ensure audit trail records are permanently retained in\nencrypted form, with no possibility of the records being compromised. This\nensures compliance with the General Data Protection Regulation can be achieved.",
    "updated" : "2024-05-18T17:10:48Z",
    "published" : "2024-05-18T17:10:48Z",
    "authors" : [
      {
        "name" : "Andreas Aßmuth"
      },
      {
        "name" : "Robert Duncan"
      },
      {
        "name" : "Simon Liebl"
      },
      {
        "name" : "Matthias Söllner"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.10989v1",
    "title" : "Learnable Privacy Neurons Localization in Language Models",
    "summary" : "Concerns regarding Large Language Models (LLMs) to memorize and disclose\nprivate information, particularly Personally Identifiable Information (PII),\nbecome prominent within the community. Many efforts have been made to mitigate\nthe privacy risks. However, the mechanism through which LLMs memorize PII\nremains poorly understood. To bridge this gap, we introduce a pioneering method\nfor pinpointing PII-sensitive neurons (privacy neurons) within LLMs. Our method\nemploys learnable binary weight masks to localize specific neurons that account\nfor the memorization of PII in LLMs through adversarial training. Our\ninvestigations discover that PII is memorized by a small subset of neurons\nacross all layers, which shows the property of PII specificity. Furthermore, we\npropose to validate the potential in PII risk mitigation by deactivating the\nlocalized privacy neurons. Both quantitative and qualitative experiments\ndemonstrate the effectiveness of our neuron localization algorithm.",
    "updated" : "2024-05-16T08:11:08Z",
    "published" : "2024-05-16T08:11:08Z",
    "authors" : [
      {
        "name" : "Ruizhe Chen"
      },
      {
        "name" : "Tianxiang Hu"
      },
      {
        "name" : "Yang Feng"
      },
      {
        "name" : "Zuozhu Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.14725v1",
    "title" : "A Systematic and Formal Study of the Impact of Local Differential\n  Privacy on Fairness: Preliminary Results",
    "summary" : "Machine learning (ML) algorithms rely primarily on the availability of\ntraining data, and, depending on the domain, these data may include sensitive\ninformation about the data providers, thus leading to significant privacy\nissues. Differential privacy (DP) is the predominant solution for\nprivacy-preserving ML, and the local model of DP is the preferred choice when\nthe server or the data collector are not trusted. Recent experimental studies\nhave shown that local DP can impact ML prediction for different subgroups of\nindividuals, thus affecting fair decision-making. However, the results are\nconflicting in the sense that some studies show a positive impact of privacy on\nfairness while others show a negative one. In this work, we conduct a\nsystematic and formal study of the effect of local DP on fairness.\nSpecifically, we perform a quantitative study of how the fairness of the\ndecisions made by the ML model changes under local DP for different levels of\nprivacy and data distributions. In particular, we provide bounds in terms of\nthe joint distributions and the privacy level, delimiting the extent to which\nlocal DP can impact the fairness of the model. We characterize the cases in\nwhich privacy reduces discrimination and those with the opposite effect. We\nvalidate our theoretical findings on synthetic and real-world datasets. Our\nresults are preliminary in the sense that, for now, we study only the case of\none sensitive attribute, and only statistical disparity, conditional\nstatistical disparity, and equal opportunity difference.",
    "updated" : "2024-05-23T15:54:03Z",
    "published" : "2024-05-23T15:54:03Z",
    "authors" : [
      {
        "name" : "Karima Makhlouf"
      },
      {
        "name" : "Tamara Stefanovic"
      },
      {
        "name" : "Heber H. Arcolezi"
      },
      {
        "name" : "Catuscia Palamidessi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.14528v1",
    "title" : "Towards Privacy-Aware and Personalised Assistive Robots: A User-Centred\n  Approach",
    "summary" : "The global increase in the elderly population necessitates innovative\nlong-term care solutions to improve the quality of life for vulnerable\nindividuals while reducing caregiver burdens. Assistive robots, leveraging\nadvancements in Machine Learning, offer promising personalised support.\nHowever, their integration into daily life raises significant privacy concerns.\nWidely used frameworks like the Robot Operating System (ROS) historically lack\ninherent privacy mechanisms, complicating data-driven approaches in robotics.\nThis research pioneers user-centric, privacy-aware technologies such as\nFederated Learning (FL) to advance assistive robotics. FL enables collaborative\nlearning without sharing sensitive data, addressing privacy and scalability\nissues. This work includes developing solutions for smart wheelchair\nassistance, enhancing user independence and well-being. By tackling challenges\nrelated to non-stationary data and heterogeneous environments, the research\naims to improve personalisation and user experience. Ultimately, it seeks to\nlead the responsible integration of assistive robots into society, enhancing\nthe quality of life for elderly and care-dependent individuals.",
    "updated" : "2024-05-23T13:14:08Z",
    "published" : "2024-05-23T13:14:08Z",
    "authors" : [
      {
        "name" : "Fernando E. Casado"
      }
    ],
    "categories" : [
      "cs.RO",
      "cs.HC",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.14457v1",
    "title" : "Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model",
    "summary" : "Machine learning models can be trained with formal privacy guarantees via\ndifferentially private optimizers such as DP-SGD. In this work, we study such\nprivacy guarantees when the adversary only accesses the final model, i.e.,\nintermediate model updates are not released. In the existing literature, this\nhidden state threat model exhibits a significant gap between the lower bound\nprovided by empirical privacy auditing and the theoretical upper bound provided\nby privacy accounting. To challenge this gap, we propose to audit this threat\nmodel with adversaries that craft a gradient sequence to maximize the privacy\nloss of the final model without accessing intermediate models. We demonstrate\nexperimentally how this approach consistently outperforms prior attempts at\nauditing the hidden state model. When the crafted gradient is inserted at every\noptimization step, our results imply that releasing only the final model does\nnot amplify privacy, providing a novel negative result. On the other hand, when\nthe crafted gradient is not inserted at every step, we show strong evidence\nthat a privacy amplification phenomenon emerges in the general non-convex\nsetting (albeit weaker than in convex regimes), suggesting that existing\nprivacy upper bounds can be improved.",
    "updated" : "2024-05-23T11:38:38Z",
    "published" : "2024-05-23T11:38:38Z",
    "authors" : [
      {
        "name" : "Tudor Cebere"
      },
      {
        "name" : "Aurélien Bellet"
      },
      {
        "name" : "Nicolas Papernot"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.14038v1",
    "title" : "FLIPHAT: Joint Differential Privacy for High Dimensional Sparse Linear\n  Bandits",
    "summary" : "High dimensional sparse linear bandits serve as an efficient model for\nsequential decision-making problems (e.g. personalized medicine), where high\ndimensional features (e.g. genomic data) on the users are available, but only a\nsmall subset of them are relevant. Motivated by data privacy concerns in these\napplications, we study the joint differentially private high dimensional sparse\nlinear bandits, where both rewards and contexts are considered as private data.\nFirst, to quantify the cost of privacy, we derive a lower bound on the regret\nachievable in this setting. To further address the problem, we design a\ncomputationally efficient bandit algorithm, \\textbf{F}orgetfu\\textbf{L}\n\\textbf{I}terative \\textbf{P}rivate \\textbf{HA}rd \\textbf{T}hresholding\n(FLIPHAT). Along with doubling of episodes and episodic forgetting, FLIPHAT\ndeploys a variant of Noisy Iterative Hard Thresholding (N-IHT) algorithm as a\nsparse linear regression oracle to ensure both privacy and regret-optimality.\nWe show that FLIPHAT achieves optimal regret up to logarithmic factors. We\nanalyze the regret by providing a novel refined analysis of the estimation\nerror of N-IHT, which is of parallel interest.",
    "updated" : "2024-05-22T22:19:12Z",
    "published" : "2024-05-22T22:19:12Z",
    "authors" : [
      {
        "name" : "Sunrit Chakraborty"
      },
      {
        "name" : "Saptarshi Roy"
      },
      {
        "name" : "Debabrota Basu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.13857v1",
    "title" : "What Do Privacy Advertisements Communicate to Consumers?",
    "summary" : "When companies release marketing materials aimed at promoting their privacy\npractices or highlighting specific privacy features, what do they actually\ncommunicate to consumers? In this paper, we explore the impact of privacy\nmarketing materials on: (1) consumers' attitude towards the organizations\nproviding the campaigns, (2) overall privacy awareness, and (3) the\nactionability of suggested privacy advice. To this end, we investigated the\nimpact of four privacy advertising videos and one privacy game published by\nfive different technology companies. We conducted 24 semi-structured interviews\nwith participants randomly assigned to view one or two of the videos or play\nthe game. Our findings suggest that awareness of privacy features can\ncontribute to positive perceptions of a company or its products. The ads we\ntested were more successful in communicating the advertised privacy features\nthan the game we tested. We observed that advertising a single privacy feature\nusing a single metaphor in a short ad increased awareness of the advertised\nfeature. The game failed to communicate privacy features or motivate study\nparticipants to use the features. Our results also suggest that privacy\ncampaigns can be useful for raising awareness about privacy features and\nimproving brand image, but may not be the most effective way to teach viewers\nhow to use privacy features.",
    "updated" : "2024-05-22T17:32:04Z",
    "published" : "2024-05-22T17:32:04Z",
    "authors" : [
      {
        "name" : "Xiaoxin Shen"
      },
      {
        "name" : "Eman Alashwali"
      },
      {
        "name" : "Lorrie Faith Cranor"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY",
      "cs.HC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.13847v1",
    "title" : "AI-Protected Blockchain-based IoT environments: Harnessing the Future of\n  Network Security and Privacy",
    "summary" : "Integrating blockchain technology with the Internet of Things offers\ntransformative possibilities for enhancing network security and privacy in the\ncontemporary digital landscape, where interconnected devices and expansive\nnetworks are ubiquitous. This paper explores the pivotal role of artificial\nintelligence in bolstering blockchain-enabled IoT systems, potentially marking\na significant leap forward in safeguarding data integrity and confidentiality\nacross networks. Blockchain technology provides a decentralized and immutable\nledger, ideal for the secure management of device identities and transactions\nin IoT networks. When coupled with AI, these systems gain the ability to not\nonly automate and optimize security protocols but also adaptively respond to\nnew and evolving cyber threats. This dual capability enhances the resilience of\nnetworks against cyber-attacks, a critical consideration as IoT devices\nincreasingly permeate critical infrastructures. The synergy between AI and\nblockchain in IoT is profound. AI algorithms can analyze vast amounts of data\nfrom IoT devices to detect patterns and anomalies that may signify security\nbreaches. Concurrently, blockchain can ensure that data records are\ntamper-proof, enhancing the reliability of AI-driven security measures.\nMoreover, this research evaluates the implications of AI-enhanced blockchain\nsystems on privacy protection within IoT networks. IoT devices often collect\nsensitive personal data, making privacy a paramount concern. AI can facilitate\nthe development of new protocols that ensure data privacy and user anonymity\nwithout compromising the functionality of IoT systems. Through comprehensive\nanalysis and case studies, this paper aims to provide an in-depth understanding\nof how AI-enhanced blockchain technology can revolutionize network security and\nprivacy in IoT environments.",
    "updated" : "2024-05-22T17:14:19Z",
    "published" : "2024-05-22T17:14:19Z",
    "authors" : [
      {
        "name" : "Ali Mohammadi Ruzbahani"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.13804v1",
    "title" : "Guarding Multiple Secrets: Enhanced Summary Statistic Privacy for Data\n  Sharing",
    "summary" : "Data sharing enables critical advances in many research areas and business\napplications, but it may lead to inadvertent disclosure of sensitive summary\nstatistics (e.g., means or quantiles). Existing literature only focuses on\nprotecting a single confidential quantity, while in practice, data sharing\ninvolves multiple sensitive statistics. We propose a novel framework to define,\nanalyze, and protect multi-secret summary statistics privacy in data sharing.\nSpecifically, we measure the privacy risk of any data release mechanism by the\nworst-case probability of an attacker successfully inferring summary statistic\nsecrets. Given an attacker's objective spanning from inferring a subset to the\nentirety of summary statistic secrets, we systematically design and analyze\ntailored privacy metrics. Defining the distortion as the worst-case distance\nbetween the original and released data distribution, we analyze the tradeoff\nbetween privacy and distortion. Our contribution also includes designing and\nanalyzing data release mechanisms tailored for different data distributions and\nsecret types. Evaluations on real-world data demonstrate the effectiveness of\nour mechanisms in practical applications.",
    "updated" : "2024-05-22T16:30:34Z",
    "published" : "2024-05-22T16:30:34Z",
    "authors" : [
      {
        "name" : "Shuaiqi Wang"
      },
      {
        "name" : "Rongzhe Wei"
      },
      {
        "name" : "Mohsen Ghassemi"
      },
      {
        "name" : "Eleonora Kreacic"
      },
      {
        "name" : "Vamsi K. Potluru"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.13801v1",
    "title" : "Bayesian Inference Under Differential Privacy: Prior Selection\n  Considerations with Application to Univariate Gaussian Data and Regression",
    "summary" : "We describe Bayesian inference for the mean and variance of bounded data\nprotected by differential privacy and modeled as Gaussian. Using this setting,\nwe demonstrate that analysts can and should take the constraints imposed by the\nbounds into account when specifying prior distributions. Additionally, we\nprovide theoretical and empirical results regarding what classes of default\npriors produce valid inference for a differentially private release in settings\nwhere substantial prior information is not available. We discuss how these\nresults can be applied to Bayesian inference for regression with differentially\nprivate data.",
    "updated" : "2024-05-22T16:27:20Z",
    "published" : "2024-05-22T16:27:20Z",
    "authors" : [
      {
        "name" : "Zeki Kazan"
      },
      {
        "name" : "Jerome P. Reiter"
      }
    ],
    "categories" : [
      "stat.ME",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.13744v1",
    "title" : "A Privacy Measure Turned Upside Down? Investigating the Use of HTTP\n  Client Hints on the Web",
    "summary" : "HTTP client hints are a set of standardized HTTP request headers designed to\nmodernize and potentially replace the traditional user agent string. While the\nuser agent string exposes a wide range of information about the client's\nbrowser and device, client hints provide a controlled and structured approach\nfor clients to selectively disclose their capabilities and preferences to\nservers. Essentially, client hints aim at more effective and privacy-friendly\ndisclosure of browser or client properties than the user agent string.\n  We present a first long-term study of the use of HTTP client hints in the\nwild. We found that despite being implemented in almost all web browsers,\nserver-side usage of client hints remains generally low. However, in the\ncontext of third-party websites, which are often linked to trackers, the\nadoption rate is significantly higher. This is concerning because client hints\nallow the retrieval of more data from the client than the user agent string\nprovides, and there are currently no mechanisms for users to detect or control\nthis potential data leakage. Our work provides valuable insights for web users,\nbrowser vendors, and researchers by exposing potential privacy violations via\nclient hints and providing help in developing remediation strategies as well as\nfurther research.",
    "updated" : "2024-05-22T15:32:12Z",
    "published" : "2024-05-22T15:32:12Z",
    "authors" : [
      {
        "name" : "Stephan Wiefling"
      },
      {
        "name" : "Marian Hönscheid"
      },
      {
        "name" : "Luigi Lo Iacono"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.NI",
      "cs.SI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.13453v1",
    "title" : "A Huber Loss Minimization Approach to Mean Estimation under User-level\n  Differential Privacy",
    "summary" : "Privacy protection of users' entire contribution of samples is important in\ndistributed systems. The most effective approach is the two-stage scheme, which\nfinds a small interval first and then gets a refined estimate by clipping\nsamples into the interval. However, the clipping operation induces bias, which\nis serious if the sample distribution is heavy-tailed. Besides, users with\nlarge local sample sizes can make the sensitivity much larger, thus the method\nis not suitable for imbalanced users. Motivated by these challenges, we propose\na Huber loss minimization approach to mean estimation under user-level\ndifferential privacy. The connecting points of Huber loss can be adaptively\nadjusted to deal with imbalanced users. Moreover, it avoids the clipping\noperation, thus significantly reducing the bias compared with the two-stage\napproach. We provide a theoretical analysis of our approach, which gives the\nnoise strength needed for privacy protection, as well as the bound of mean\nsquared error. The result shows that the new method is much less sensitive to\nthe imbalance of user-wise sample sizes and the tail of sample distributions.\nFinally, we perform numerical experiments to validate our theoretical analysis.",
    "updated" : "2024-05-22T08:46:45Z",
    "published" : "2024-05-22T08:46:45Z",
    "authors" : [
      {
        "name" : "Puning Zhao"
      },
      {
        "name" : "Lifeng Lai"
      },
      {
        "name" : "Li Shen"
      },
      {
        "name" : "Qingming Li"
      },
      {
        "name" : "Jiafei Wu"
      },
      {
        "name" : "Zhe Liu"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.13380v1",
    "title" : "The Illusion of Anonymity: Uncovering the Impact of User Actions on\n  Privacy in Web3 Social Ecosystems",
    "summary" : "The rise of Web3 social ecosystems signifies the dawn of a new chapter in\ndigital interaction, offering significant prospects for user engagement and\nfinancial advancement. Nonetheless, this progress is shadowed by potential\nprivacy concessions, especially as these platforms frequently merge with\nexisting Web2.0 social media accounts, amplifying data privacy risks for users.\n  In this study, we investigate the nuanced dynamics between user engagement on\nWeb3 social platforms and the consequent privacy concerns. We scrutinize the\nwidespread phenomenon of fabricated activities, which encompasses the\nestablishment of bogus accounts aimed at mimicking popularity and the\ndeliberate distortion of social interactions by some individuals to gain\nfinancial rewards. Such deceptive maneuvers not only distort the true measure\nof the active user base but also amplify privacy threats for all members of the\nuser community. We also find that, notwithstanding their attempts to limit\nsocial exposure, users remain entangled in privacy vulnerabilities. The actions\nof those highly engaged users, albeit often a minority group, can inadvertently\nbreach the privacy of the larger collective.\n  By casting light on the delicate interplay between user engagement, financial\nmotives, and privacy issues, we offer a comprehensive examination of the\nintrinsic challenges and hazards present in the Web3 social milieu. We\nhighlight the urgent need for more stringent privacy measures and ethical\nprotocols to navigate the complex web of social exchanges and financial\nambitions in the rapidly evolving Web3.",
    "updated" : "2024-05-22T06:26:15Z",
    "published" : "2024-05-22T06:26:15Z",
    "authors" : [
      {
        "name" : "Bin Wang"
      },
      {
        "name" : "Tianjian Liu"
      },
      {
        "name" : "Wenqi Wang"
      },
      {
        "name" : "Yuan Weng"
      },
      {
        "name" : "Chao Li"
      },
      {
        "name" : "Guangquan Xu"
      },
      {
        "name" : "Meng Shen"
      },
      {
        "name" : "Sencun Zhu"
      },
      {
        "name" : "Wei Wang"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.13156v1",
    "title" : "A Privacy-Preserving DAO Model Using NFT Authentication for the\n  Punishment not Reward Blockchain Architecture",
    "summary" : "\\This paper presents a novel decentralized autonomous organization (DAO)\nmodel leveraging non-fungible tokens (NFTs) for advanced access control and\nprivacy-preserving interactions within a Punishment not Reward (PnR) blockchain\nframework. The proposed model introduces a dual NFT architecture: Membership\nNFTs (\\(NFT_{auth}\\)) for authentication and access control, and Interaction\nNFTs (\\(NFT_{priv}\\)) for enabling private, encrypted interactions among\nparticipants. Governance is enforced through smart contracts that manage\nreputation and administer punitive measures, such as conditional identity\ndisclosure. By prioritizing privacy, security, and deterrence over financial\nrewards, this model addresses key challenges in existing blockchain incentive\nstructures, paving the way for more sustainable and decentralized governance\nframeworks.",
    "updated" : "2024-05-21T18:53:15Z",
    "published" : "2024-05-21T18:53:15Z",
    "authors" : [
      {
        "name" : "Talgar Bayan"
      },
      {
        "name" : "Richard Banach"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.SE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.11975v2",
    "title" : "A Stochastic Sampling Approach to Privacy",
    "summary" : "This paper proposes an optimal stochastic sampling approach to privacy, in\nwhich a sensor observes a process which is correlated to private information,\nand a sampler decides to keep or discard the sensor's observations. The kept\nsamples are shared with an adversary who might attempt to infer the private\nprocess. The privacy leakages are captured with the mutual information between\nthe private process and sampler's output. We cast the optimal sampling design\nas an optimization problem that (i) minimizes the reconstruction error of the\nobserved process using the sampler's output, (ii) and reduces privacy leakages.\nWe first show the optimal reconstruction is obtained by solving a one-step\noptimization problem at each time step. We derive the optimality equations of\nthe sampler for a general processes via the dynamic decomposition method, and\nshow the sampler controls adversary's belief about the private input. Also, we\npropose a simplified design for linear Gaussian processes by restricting the\nsampling policy to a special collection. We show that the optimal\nreconstruction of states, the belief state and the optimization objective can\nbe analytically expressed based on a conditional mean and covariance matrix. We\ndevelop an numerical algorithm to optimize the sampling and reconstruction\npolicies based on the implicit function theorem. Finally, we verify our design\nand show its capabilities in state reconstruction, privacy protection and data\nsize reduction via simulations.",
    "updated" : "2024-05-22T10:30:57Z",
    "published" : "2024-05-20T12:10:26Z",
    "authors" : [
      {
        "name" : "Chuanghong Weng"
      },
      {
        "name" : "Ehsan Nekouei"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.15398v1",
    "title" : "PriCE: Privacy-Preserving and Cost-Effective Scheduling for\n  Parallelizing the Large Medical Image Processing Workflow over Hybrid Clouds",
    "summary" : "Running deep neural networks for large medical images is a resource-hungry\nand time-consuming task with centralized computing. Outsourcing such medical\nimage processing tasks to hybrid clouds has benefits, such as a significant\nreduction of execution time and monetary cost. However, due to privacy\nconcerns, it is still challenging to process sensitive medical images over\nclouds, which would hinder their deployment in many real-world applications. To\novercome this, we first formulate the overall optimization objectives of the\nprivacy-preserving distributed system model, i.e., minimizing the amount of\ninformation about the private data learned by the adversaries throughout the\nprocess, reducing the maximum execution time and cost under the user budget\nconstraint. We propose a novel privacy-preserving and cost-effective method\ncalled PriCE to solve this multi-objective optimization problem. We performed\nextensive simulation experiments for artifact detection tasks on medical images\nusing an ensemble of five deep convolutional neural network inferences as the\nworkflow task. Experimental results show that PriCE successfully splits a wide\nrange of input gigapixel medical images with graph-coloring-based strategies,\nyielding desired output utility and lowering the privacy risk, makespan, and\nmonetary cost under user's budget.",
    "updated" : "2024-05-24T09:52:00Z",
    "published" : "2024-05-24T09:52:00Z",
    "authors" : [
      {
        "name" : "Yuandou Wang"
      },
      {
        "name" : "Neel Kanwal"
      },
      {
        "name" : "Kjersti Engan"
      },
      {
        "name" : "Chunming Rong"
      },
      {
        "name" : "Paola Grosso"
      },
      {
        "name" : "Zhiming Zhao"
      }
    ],
    "categories" : [
      "cs.CE",
      "cs.AI",
      "cs.CV",
      "cs.DC",
      "cs.ET"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.15272v1",
    "title" : "Physiological Data: Challenges for Privacy and Ethics",
    "summary" : "Wearable devices that measure and record physiological signals are now\nbecoming widely available to the general public with ever-increasing\naffordability and signal quality. The data from these devices introduce serious\nethical challenges that remain largely unaddressed. Users do not always\nunderstand how these data can be leveraged to reveal private information about\nthem and developers of these devices may not fully grasp how physiological data\ncollected today could be used in the future for completely different purposes.\nWe discuss the potential for wearable devices, initially designed to help users\nimprove their well-being or enhance the experience of some digital application,\nto be appropriated in ways that extend far beyond their original intended\npurpose. We identify how the currently available technology can be misused,\ndiscuss how pairing physiological data with non-physiological data can\nradically expand the predictive capacity of physiological wearables, and\nexplore the implications of these expanded capacities for a variety of\nstakeholders.",
    "updated" : "2024-05-24T06:59:42Z",
    "published" : "2024-05-24T06:59:42Z",
    "authors" : [
      {
        "name" : "Keith Davis"
      },
      {
        "name" : "Tuukka Ruotsalo"
      }
    ],
    "categories" : [
      "cs.CY",
      "K.4.0; K.4.1; K.4.2"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.15150v1",
    "title" : "Enhancing Learning with Label Differential Privacy by Vector\n  Approximation",
    "summary" : "Label differential privacy (DP) is a framework that protects the privacy of\nlabels in training datasets, while the feature vectors are public. Existing\napproaches protect the privacy of labels by flipping them randomly, and then\ntrain a model to make the output approximate the privatized label. However, as\nthe number of classes $K$ increases, stronger randomization is needed, thus the\nperformances of these methods become significantly worse. In this paper, we\npropose a vector approximation approach, which is easy to implement and\nintroduces little additional computational overhead. Instead of flipping each\nlabel into a single scalar, our method converts each label into a random vector\nwith $K$ components, whose expectations reflect class conditional\nprobabilities. Intuitively, vector approximation retains more information than\nscalar labels. A brief theoretical analysis shows that the performance of our\nmethod only decays slightly with $K$. Finally, we conduct experiments on both\nsynthesized and real datasets, which validate our theoretical analysis as well\nas the practical performance of our method.",
    "updated" : "2024-05-24T02:08:45Z",
    "published" : "2024-05-24T02:08:45Z",
    "authors" : [
      {
        "name" : "Puning Zhao"
      },
      {
        "name" : "Rongfei Fan"
      },
      {
        "name" : "Huiwen Wu"
      },
      {
        "name" : "Qingming Li"
      },
      {
        "name" : "Jiafei Wu"
      },
      {
        "name" : "Zhe Liu"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.15140v1",
    "title" : "Better Membership Inference Privacy Measurement through Discrepancy",
    "summary" : "Membership Inference Attacks have emerged as a dominant method for\nempirically measuring privacy leakage from machine learning models. Here,\nprivacy is measured by the {\\em{advantage}} or gap between a score or a\nfunction computed on the training and the test data. A major barrier to the\npractical deployment of these attacks is that they do not scale to large\nwell-generalized models -- either the advantage is relatively low, or the\nattack involves training multiple models which is highly compute-intensive. In\nthis work, inspired by discrepancy theory, we propose a new empirical privacy\nmetric that is an upper bound on the advantage of a family of membership\ninference attacks. We show that this metric does not involve training multiple\nmodels, can be applied to large Imagenet classification models in-the-wild, and\nhas higher advantage than existing metrics on models trained with more recent\nand sophisticated training recipes. Motivated by our empirical results, we also\npropose new membership inference attacks tailored to these training losses.",
    "updated" : "2024-05-24T01:33:22Z",
    "published" : "2024-05-24T01:33:22Z",
    "authors" : [
      {
        "name" : "Ruihan Wu"
      },
      {
        "name" : "Pengrun Huang"
      },
      {
        "name" : "Kamalika Chaudhuri"
      }
    ],
    "categories" : [
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.17423v1",
    "title" : "Privacy-Aware Visual Language Models",
    "summary" : "This paper aims to advance our understanding of how Visual Language Models\n(VLMs) handle privacy-sensitive information, a crucial concern as these\ntechnologies become integral to everyday life. To this end, we introduce a new\nbenchmark PrivBench, which contains images from 8 sensitive categories such as\npassports, or fingerprints. We evaluate 10 state-of-the-art VLMs on this\nbenchmark and observe a generally limited understanding of privacy,\nhighlighting a significant area for model improvement. Based on this we\nintroduce PrivTune, a new instruction-tuning dataset aimed at equipping VLMs\nwith knowledge about visual privacy. By tuning two pretrained VLMs, TinyLLaVa\nand MiniGPT-v2, on this small dataset, we achieve strong gains in their ability\nto recognize sensitive content, outperforming even GPT4-V. At the same time, we\nshow that privacy-tuning only minimally affects the VLMs performance on\nstandard benchmarks such as VQA. Overall, this paper lays out a crucial\nchallenge for making VLMs effective in handling real-world data safely and\nprovides a simple recipe that takes the first step towards building\nprivacy-aware VLMs.",
    "updated" : "2024-05-27T17:59:25Z",
    "published" : "2024-05-27T17:59:25Z",
    "authors" : [
      {
        "name" : "Laurens Samson"
      },
      {
        "name" : "Nimrod Barazani"
      },
      {
        "name" : "Sennay Ghebreab"
      },
      {
        "name" : "Yuki M. Asano"
      }
    ],
    "categories" : [
      "cs.CV",
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.17079v1",
    "title" : "Learning with User-Level Local Differential Privacy",
    "summary" : "User-level privacy is important in distributed systems. Previous research\nprimarily focuses on the central model, while the local models have received\nmuch less attention. Under the central model, user-level DP is strictly\nstronger than the item-level one. However, under the local model, the\nrelationship between user-level and item-level LDP becomes more complex, thus\nthe analysis is crucially different. In this paper, we first analyze the mean\nestimation problem and then apply it to stochastic optimization,\nclassification, and regression. In particular, we propose adaptive strategies\nto achieve optimal performance at all privacy levels. Moreover, we also obtain\ninformation-theoretic lower bounds, which show that the proposed methods are\nminimax optimal up to logarithmic factors. Unlike the central DP model, where\nuser-level DP always leads to slower convergence, our result shows that under\nthe local model, the convergence rates are nearly the same between user-level\nand item-level cases for distributions with bounded support. For heavy-tailed\ndistributions, the user-level rate is even faster than the item-level one.",
    "updated" : "2024-05-27T11:52:24Z",
    "published" : "2024-05-27T11:52:24Z",
    "authors" : [
      {
        "name" : "Puning Zhao"
      },
      {
        "name" : "Li Shen"
      },
      {
        "name" : "Rongfei Fan"
      },
      {
        "name" : "Qingming Li"
      },
      {
        "name" : "Huiwen Wu"
      },
      {
        "name" : "Jiafei Wu"
      },
      {
        "name" : "Zhe Liu"
      }
    ],
    "categories" : [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.16905v1",
    "title" : "Privacy and Security Trade-off in Interconnected Systems with Known or\n  Unknown Privacy Noise Covariance",
    "summary" : "This paper is concerned with the security problem for interconnected systems,\nwhere each subsystem is required to detect local attacks using locally\navailable information and the information received from its neighboring\nsubsystems. Moreover, we consider that there exists an additional eavesdropper\nbeing able to infer the private information by eavesdropping transmitted data\nbetween subsystems. Then, a privacy-preserving method is employed by adding\nprivacy noise to transmitted data, and the privacy level is measured by mutual\ninformation. Nevertheless, adding privacy noise to transmitted data may affect\nthe detection performance metrics such as detection probability and false alarm\nprobability. Thus, we theoretically analyze the trade-off between the privacy\nand the detection performance. An optimization problem with maximizing both the\ndegree of privacy preservation and the detection probability is established to\nobtain the covariance of the privacy noise. In addition, the attack detector of\neach subsystem may not obtain all information about the privacy noise. We\nfurther theoretically analyze the trade-off between the privacy and the false\nalarm probability when the attack detector has no knowledge of the privacy\nnoise covariance. An optimization problem with maximizing the degree of privacy\npreservation with guaranteeing a bound of false alarm distortion level is\nestablished to obtain {\\color{black}{the covariance of the privacy noise}}.\nMoreover, to analyze the effect of the privacy noise on the detection\nprobability, we consider that each subsystem can estimate the unknown privacy\nnoise covariance by the secondary data. Based on the estimated covariance, we\nconstruct another attack detector and analyze how the privacy noise affects its\ndetection performance. Finally, a numerical example is provided to verify the\neffectiveness of theoretical results.",
    "updated" : "2024-05-27T07:53:51Z",
    "published" : "2024-05-27T07:53:51Z",
    "authors" : [
      {
        "name" : "Haojun Wang"
      },
      {
        "name" : "Kun Liu"
      },
      {
        "name" : "Baojia Li"
      },
      {
        "name" : "Emilia Fridman"
      },
      {
        "name" : "Yuanqing Xia"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.16895v1",
    "title" : "Anonymization Prompt Learning for Facial Privacy-Preserving\n  Text-to-Image Generation",
    "summary" : "Text-to-image diffusion models, such as Stable Diffusion, generate highly\nrealistic images from text descriptions. However, the generation of certain\ncontent at such high quality raises concerns. A prominent issue is the accurate\ndepiction of identifiable facial images, which could lead to malicious deepfake\ngeneration and privacy violations. In this paper, we propose Anonymization\nPrompt Learning (APL) to address this problem. Specifically, we train a\nlearnable prompt prefix for text-to-image diffusion models, which forces the\nmodel to generate anonymized facial identities, even when prompted to produce\nimages of specific individuals. Extensive quantitative and qualitative\nexperiments demonstrate the successful anonymization performance of APL, which\nanonymizes any specific individuals without compromising the quality of\nnon-identity-specific image generation. Furthermore, we reveal the\nplug-and-play property of the learned prompt prefix, enabling its effective\napplication across different pretrained text-to-image models for transferrable\nprivacy and security protection against the risks of deepfakes.",
    "updated" : "2024-05-27T07:38:26Z",
    "published" : "2024-05-27T07:38:26Z",
    "authors" : [
      {
        "name" : "Liang Shi"
      },
      {
        "name" : "Jie Zhang"
      },
      {
        "name" : "Shiguang Shan"
      }
    ],
    "categories" : [
      "cs.CV"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.16058v1",
    "title" : "A Novel Privacy Enhancement Scheme with Dynamic Quantization for\n  Federated Learning",
    "summary" : "Federated learning (FL) has been widely regarded as a promising paradigm for\nprivacy preservation of raw data in machine learning. Although, the data\nprivacy in FL is locally protected to some extent, it is still a desideratum to\nenhance privacy and alleviate communication overhead caused by repetitively\ntransmitting model parameters. Typically, these challenges are addressed\nseparately, or jointly via a unified scheme that consists of noise-injected\nprivacy mechanism and communication compression, which may lead to model\ncorruption due to the introduced composite noise. In this work, we propose a\nnovel model-splitting privacy-preserving FL (MSP-FL) scheme to achieve private\nFL with precise accuracy guarantee. Based upon MSP-FL, we further propose a\nmodel-splitting privacy-preserving FL with dynamic quantization (MSPDQ-FL) to\nmitigate the communication overhead, which incorporates a shrinking\nquantization interval to reduce the quantization error. We provide privacy and\nconvergence analysis for both MSP-FL and MSPDQ-FL under non-i.i.d. dataset,\npartial clients participation and finite quantization level. Numerical results\nare presented to validate the superiority of the proposed schemes.",
    "updated" : "2024-05-25T04:56:54Z",
    "published" : "2024-05-25T04:56:54Z",
    "authors" : [
      {
        "name" : "Yifan Wang"
      },
      {
        "name" : "Xianghui Cao"
      },
      {
        "name" : "Shi Jin"
      },
      {
        "name" : "Mo-Yuen Chow"
      }
    ],
    "categories" : [
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.18430v1",
    "title" : "Feasibility of Privacy-Preserving Entity Resolution on Confidential\n  Healthcare Datasets Using Homomorphic Encryption",
    "summary" : "Patient datasets contain confidential information which is protected by laws\nand regulations such as HIPAA and GDPR. Ensuring comprehensive patient\ninformation necessitates privacy-preserving entity resolution (PPER), which\nidentifies identical patient entities across multiple databases from different\nhealthcare organizations while maintaining data privacy. Existing methods often\nlack cryptographic security or are computationally impractical for real-world\ndatasets. We introduce a PPER pipeline based on AMPPERE, a secure abstract\ncomputation model utilizing cryptographic tools like homomorphic encryption.\nOur tailored approach incorporates extensive parallelization techniques and\noptimal parameters specifically for patient datasets. Experimental results\ndemonstrate the proposed method's effectiveness in terms of accuracy and\nefficiency compared to various baselines.",
    "updated" : "2024-05-28T17:59:42Z",
    "published" : "2024-05-28T17:59:42Z",
    "authors" : [
      {
        "name" : "Yixiang Yao"
      },
      {
        "name" : "Joseph Cecil"
      },
      {
        "name" : "Praveen Angyan"
      },
      {
        "name" : "Neil Bahroos"
      },
      {
        "name" : "Srivatsan Ravi"
      }
    ],
    "categories" : [
      "cs.CE"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.17971v1",
    "title" : "A Qualitative Analysis Framework for mHealth Privacy Practices",
    "summary" : "Mobile Health (mHealth) applications have become a crucial part of health\nmonitoring and management. However, the proliferation of these applications has\nalso raised concerns over the privacy and security of Personally Identifiable\nInformation and Protected Health Information. Addressing these concerns, this\npaper introduces a novel framework for the qualitative evaluation of privacy\npractices in mHealth apps, particularly focusing on the handling and\ntransmission of sensitive user data. Our investigation encompasses an analysis\nof 152 leading mHealth apps on the Android platform, leveraging the proposed\nframework to provide a multifaceted view of their data processing activities.\nDespite stringent regulations like the General Data Protection Regulation in\nthe European Union and the Health Insurance Portability and Accountability Act\nin the United States, our findings indicate persistent issues with negligence\nand misuse of sensitive user information. We uncover significant instances of\nhealth information leakage to third-party trackers and a widespread neglect of\nprivacy-by-design and transparency principles. Our research underscores the\ncritical need for stricter enforcement of data protection laws and sets a\nfoundation for future efforts aimed at enhancing user privacy within the\nmHealth ecosystem.",
    "updated" : "2024-05-28T08:57:52Z",
    "published" : "2024-05-28T08:57:52Z",
    "authors" : [
      {
        "name" : "Thomas Cory"
      },
      {
        "name" : "Wolf Rieder"
      },
      {
        "name" : "Thu-My Huynh"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.16058v2",
    "title" : "A Novel Privacy Enhancement Scheme with Dynamic Quantization for\n  Federated Learning",
    "summary" : "Federated learning (FL) has been widely regarded as a promising paradigm for\nprivacy preservation of raw data in machine learning. Although, the data\nprivacy in FL is locally protected to some extent, it is still a desideratum to\nenhance privacy and alleviate communication overhead caused by repetitively\ntransmitting model parameters. Typically, these challenges are addressed\nseparately, or jointly via a unified scheme that consists of noise-injected\nprivacy mechanism and communication compression, which may lead to model\ncorruption due to the introduced composite noise. In this work, we propose a\nnovel model-splitting privacy-preserving FL (MSP-FL) scheme to achieve private\nFL with precise accuracy guarantee. Based upon MSP-FL, we further propose a\nmodel-splitting privacy-preserving FL with dynamic quantization (MSPDQ-FL) to\nmitigate the communication overhead, which incorporates a shrinking\nquantization interval to reduce the quantization error. We provide privacy and\nconvergence analysis for both MSP-FL and MSPDQ-FL under non-i.i.d. dataset,\npartial clients participation and finite quantization level. Numerical results\nare presented to validate the superiority of the proposed schemes.",
    "updated" : "2024-05-28T03:15:22Z",
    "published" : "2024-05-25T04:56:54Z",
    "authors" : [
      {
        "name" : "Yifan Wang"
      },
      {
        "name" : "Xianghui Cao"
      },
      {
        "name" : "Shi Jin"
      },
      {
        "name" : "Mo-Yuen Chow"
      }
    ],
    "categories" : [
      "math.OC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.19272v1",
    "title" : "Mitigating Disparate Impact of Differential Privacy in Federated\n  Learning through Robust Clustering",
    "summary" : "Federated Learning (FL) is a decentralized machine learning (ML) approach\nthat keeps data localized and often incorporates Differential Privacy (DP) to\nenhance privacy guarantees. Similar to previous work on DP in ML, we observed\nthat differentially private federated learning (DPFL) introduces performance\ndisparities, particularly affecting minority groups. Recent work has attempted\nto address performance fairness in vanilla FL through clustering, but this\nmethod remains sensitive and prone to errors, which are further exacerbated by\nthe DP noise in DPFL. To fill this gap, in this paper, we propose a novel\nclustered DPFL algorithm designed to effectively identify clients' clusters in\nhighly heterogeneous settings while maintaining high accuracy with DP\nguarantees. To this end, we propose to cluster clients based on both their\nmodel updates and training loss values. Our proposed approach also addresses\nthe server's uncertainties in clustering clients' model updates by employing\nlarger batch sizes along with Gaussian Mixture Model (GMM) to alleviate the\nimpact of noise and potential clustering errors, especially in\nprivacy-sensitive scenarios. We provide theoretical analysis of the\neffectiveness of our proposed approach. We also extensively evaluate our\napproach across diverse data distributions and privacy budgets and show its\neffectiveness in mitigating the disparate impact of DP in FL settings with a\nsmall computational cost.",
    "updated" : "2024-05-29T17:03:31Z",
    "published" : "2024-05-29T17:03:31Z",
    "authors" : [
      {
        "name" : "Saber Malekmohammadi"
      },
      {
        "name" : "Afaf Taik"
      },
      {
        "name" : "Golnoosh Farnadi"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.19259v1",
    "title" : "A Privacy-Preserving Graph Encryption Scheme Based on Oblivious RAM",
    "summary" : "Graph encryption schemes play a crucial role in facilitating secure queries\non encrypted graphs hosted on untrusted servers. With applications spanning\nnavigation systems, network topology, and social networks, the need to\nsafeguard sensitive data becomes paramount. Existing graph encryption methods,\nhowever, exhibit vulnerabilities by inadvertently revealing aspects of the\ngraph structure and query patterns, posing threats to security and privacy. In\nresponse, we propose a novel graph encryption scheme designed to mitigate\naccess pattern and query pattern leakage through the integration of oblivious\nRAM and trusted execution environment techniques, exemplified by a Trusted\nExecution Environment (TEE). Our solution establishes two key security\nobjectives: (1) ensuring that adversaries, when presented with an encrypted\ngraph, remain oblivious to any information regarding the underlying graph, and\n(2) achieving query indistinguishability by concealing access patterns.\nAdditionally, we conducted experimentation to evaluate the efficiency of the\nproposed schemes when dealing with real-world location navigation services.",
    "updated" : "2024-05-29T16:47:38Z",
    "published" : "2024-05-29T16:47:38Z",
    "authors" : [
      {
        "name" : "Seyni Kane"
      },
      {
        "name" : "Anis Bkakria"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.19187v1",
    "title" : "Algorithmic Transparency and Participation through the Handoff Lens:\n  Lessons Learned from the U.S. Census Bureau's Adoption of Differential\n  Privacy",
    "summary" : "Emerging discussions on the responsible government use of algorithmic\ntechnologies propose transparency and public participation as key mechanisms\nfor preserving accountability and trust. But in practice, the adoption and use\nof any technology shifts the social, organizational, and political context in\nwhich it is embedded. Therefore translating transparency and participation\nefforts into meaningful, effective accountability must take into account these\nshifts. We adopt two theoretical frames, Mulligan and Nissenbaum's handoff\nmodel and Star and Griesemer's boundary objects, to reveal such shifts during\nthe U.S. Census Bureau's adoption of differential privacy (DP) in its updated\ndisclosure avoidance system (DAS) for the 2020 census. This update preserved\n(and arguably strengthened) the confidentiality protections that the Bureau is\nmandated to uphold, and the Bureau engaged in a range of activities to\nfacilitate public understanding of and participation in the system design\nprocess. Using publicly available documents concerning the Census'\nimplementation of DP, this case study seeks to expand our understanding of how\ntechnical shifts implicate values, how such shifts can afford (or fail to\nafford) greater transparency and participation in system design, and the\nimportance of localized expertise throughout. We present three lessons from\nthis case study toward grounding understandings of algorithmic transparency and\nparticipation: (1) efforts towards transparency and participation in\nalgorithmic governance must center values and policy decisions, not just\ntechnical design decisions; (2) the handoff model is a useful tool for\nrevealing how such values may be cloaked beneath technical decisions; and (3)\nboundary objects alone cannot bridge distant communities without trusted\nexperts traveling alongside to broker their adoption.",
    "updated" : "2024-05-29T15:29:16Z",
    "published" : "2024-05-29T15:29:16Z",
    "authors" : [
      {
        "name" : "Amina A. Abdu"
      },
      {
        "name" : "Lauren M. Chambers"
      },
      {
        "name" : "Deirdre K. Mulligan"
      },
      {
        "name" : "Abigail Z. Jacobs"
      }
    ],
    "categories" : [
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.18888v1",
    "title" : "Proactive Load-Shaping Strategies with Privacy-Cost Trade-offs in\n  Residential Households based on Deep Reinforcement Learning",
    "summary" : "Smart meters play a crucial role in enhancing energy management and\nefficiency, but they raise significant privacy concerns by potentially\nrevealing detailed user behaviors through energy consumption patterns. Recent\nscholarly efforts have focused on developing battery-aided load-shaping\ntechniques to protect user privacy while balancing costs. This paper proposes a\nnovel deep reinforcement learning-based load-shaping algorithm (PLS-DQN)\ndesigned to protect user privacy by proactively creating artificial load\nsignatures that mislead potential attackers. We evaluate our proposed algorithm\nagainst a non-intrusive load monitoring (NILM) adversary. The results\ndemonstrate that our approach not only effectively conceals real energy usage\npatterns but also outperforms state-of-the-art methods in enhancing user\nprivacy while maintaining cost efficiency.",
    "updated" : "2024-05-29T08:45:04Z",
    "published" : "2024-05-29T08:45:04Z",
    "authors" : [
      {
        "name" : "Ruichang Zhang"
      },
      {
        "name" : "Youcheng Sun"
      },
      {
        "name" : "Mustafa A. Mustafa"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.18878v1",
    "title" : "Privacy Preserving Data Imputation via Multi-party Computation for\n  Medical Applications",
    "summary" : "Handling missing data is crucial in machine learning, but many datasets\ncontain gaps due to errors or non-response. Unlike traditional methods such as\nlistwise deletion, which are simple but inadequate, the literature offers more\nsophisticated and effective methods, thereby improving sample size and\naccuracy. However, these methods require accessing the whole dataset, which\ncontradicts the privacy regulations when the data is distributed among multiple\nsources. Especially in the medical and healthcare domain, such access reveals\nsensitive information about patients. This study addresses privacy-preserving\nimputation methods for sensitive data using secure multi-party computation,\nenabling secure computations without revealing any party's sensitive\ninformation. In this study, we realized the mean, median, regression, and kNN\nimputation methods in a privacy-preserving way. We specifically target the\nmedical and healthcare domains considering the significance of protection of\nthe patient data, showcasing our methods on a diabetes dataset. Experiments on\nthe diabetes dataset validated the correctness of our privacy-preserving\nimputation methods, yielding the largest error around $3 \\times 10^{-3}$,\nclosely matching plaintext methods. We also analyzed the scalability of our\nmethods to varying numbers of samples, showing their applicability to\nreal-world healthcare problems. Our analysis demonstrated that all our methods\nscale linearly with the number of samples. Except for kNN, the runtime of all\nour methods indicates that they can be utilized for large datasets.",
    "updated" : "2024-05-29T08:36:42Z",
    "published" : "2024-05-29T08:36:42Z",
    "authors" : [
      {
        "name" : "Julia Jentsch"
      },
      {
        "name" : "Ali Burak Ünal"
      },
      {
        "name" : "Şeyma Selcan Mağara"
      },
      {
        "name" : "Mete Akgün"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.18802v1",
    "title" : "Enhancing Security and Privacy in Federated Learning using Update\n  Digests and Voting-Based Defense",
    "summary" : "Federated Learning (FL) is a promising privacy-preserving machine learning\nparadigm that allows data owners to collaboratively train models while keeping\ntheir data localized. Despite its potential, FL faces challenges related to the\ntrustworthiness of both clients and servers, especially in the presence of\ncurious or malicious adversaries. In this paper, we introduce a novel framework\nnamed \\underline{\\textbf{F}}ederated \\underline{\\textbf{L}}earning with\n\\underline{\\textbf{U}}pdate \\underline{\\textbf{D}}igest (FLUD), which addresses\nthe critical issues of privacy preservation and resistance to Byzantine attacks\nwithin distributed learning environments. FLUD utilizes an innovative approach,\nthe $\\mathsf{LinfSample}$ method, allowing clients to compute the $l_{\\infty}$\nnorm across sliding windows of updates as an update digest. This digest enables\nthe server to calculate a shared distance matrix, significantly reducing the\noverhead associated with Secure Multi-Party Computation (SMPC) by three orders\nof magnitude while effectively distinguishing between benign and malicious\nupdates. Additionally, FLUD integrates a privacy-preserving, voting-based\ndefense mechanism that employs optimized SMPC protocols to minimize\ncommunication rounds. Our comprehensive experiments demonstrate FLUD's\neffectiveness in countering Byzantine adversaries while incurring low\ncommunication and runtime overhead. FLUD offers a scalable framework for secure\nand reliable FL in distributed environments, facilitating its application in\nscenarios requiring robust data management and security.",
    "updated" : "2024-05-29T06:46:10Z",
    "published" : "2024-05-29T06:46:10Z",
    "authors" : [
      {
        "name" : "Wenjie Li"
      },
      {
        "name" : "Kai Fan"
      },
      {
        "name" : "Jingyuan Zhang"
      },
      {
        "name" : "Hui Li"
      },
      {
        "name" : "Wei Yang Bryan Lim"
      },
      {
        "name" : "Qiang Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.18534v1",
    "title" : "Individualized Privacy Accounting via Subsampling with Applications in\n  Combinatorial Optimization",
    "summary" : "In this work, we give a new technique for analyzing individualized privacy\naccounting via the following simple observation: if an algorithm is one-sided\nadd-DP, then its subsampled variant satisfies two-sided DP. From this, we\nobtain several improved algorithms for private combinatorial optimization\nproblems, including decomposable submodular maximization and set cover. Our\nerror guarantees are asymptotically tight and our algorithm satisfies pure-DP\nwhile previously known algorithms (Gupta et al., 2010; Chaturvedi et al., 2021)\nare approximate-DP. We also show an application of our technique beyond\ncombinatorial optimization by giving a pure-DP algorithm for the shifting heavy\nhitter problem in a stream; previously, only an approximateDP algorithm was\nknown (Kaplan et al., 2021; Cohen & Lyu, 2023).",
    "updated" : "2024-05-28T19:02:30Z",
    "published" : "2024-05-28T19:02:30Z",
    "authors" : [
      {
        "name" : "Badih Ghazi"
      },
      {
        "name" : "Pritish Kamath"
      },
      {
        "name" : "Ravi Kumar"
      },
      {
        "name" : "Pasin Manurangsi"
      },
      {
        "name" : "Adam Sealfon"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.19831v1",
    "title" : "Just Rewrite It Again: A Post-Processing Method for Enhanced Semantic\n  Similarity and Privacy Preservation of Differentially Private Rewritten Text",
    "summary" : "The study of Differential Privacy (DP) in Natural Language Processing often\nviews the task of text privatization as a $\\textit{rewriting}$ task, in which\nsensitive input texts are rewritten to hide explicit or implicit private\ninformation. In order to evaluate the privacy-preserving capabilities of a DP\ntext rewriting mechanism, $\\textit{empirical privacy}$ tests are frequently\nemployed. In these tests, an adversary is modeled, who aims to infer sensitive\ninformation (e.g., gender) about the author behind a (privatized) text. Looking\nto improve the empirical protections provided by DP rewriting methods, we\npropose a simple post-processing method based on the goal of aligning rewritten\ntexts with their original counterparts, where DP rewritten texts are rewritten\n$\\textit{again}$. Our results shown that such an approach not only produces\noutputs that are more semantically reminiscent of the original inputs, but also\ntexts which score on average better in empirical privacy evaluations.\nTherefore, our approach raises the bar for DP rewriting methods in their\nempirical privacy evaluations, providing an extra layer of protection against\nmalicious adversaries.",
    "updated" : "2024-05-30T08:41:33Z",
    "published" : "2024-05-30T08:41:33Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.20914v1",
    "title" : "RASE: Efficient Privacy-preserving Data Aggregation against Disclosure\n  Attacks for IoTs",
    "summary" : "The growing popular awareness of personal privacy raises the following\nquandary: what is the new paradigm for collecting and protecting the data\nproduced by ever-increasing sensor devices. Most previous studies on co-design\nof data aggregation and privacy preservation assume that a trusted fusion\ncenter adheres to privacy regimes. Very recent work has taken steps towards\nrelaxing the assumption by allowing data contributors to locally perturb their\nown data. Although these solutions withhold some data content to mitigate\nprivacy risks, they have been shown to offer insufficient protection against\ndisclosure attacks. Aiming at providing a more rigorous data safeguard for the\nInternet of Things (IoTs), this paper initiates the study of privacy-preserving\ndata aggregation. We propose a novel paradigm (called RASE), which can be\ngeneralized into a 3-step sequential procedure, noise addition, followed by\nrandom permutation, and then parameter estimation. Specially, we design a\ndifferentially private randomizer, which carefully guides data contributors to\nobfuscate the truth. Then, a shuffler is employed to receive the noisy data\nfrom all data contributors. After that, it breaks the correct linkage between\nsenders and receivers by applying a random permutation. The estimation phase\ninvolves using inaccurate data to calculate an approximate aggregate value.\nExtensive simulations are provided to explore the privacy-utility landscape of\nour RASE.",
    "updated" : "2024-05-31T15:21:38Z",
    "published" : "2024-05-31T15:21:38Z",
    "authors" : [
      {
        "name" : "Zuyan Wang"
      },
      {
        "name" : "Jun Tao"
      },
      {
        "name" : "Dika Zou"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.20900v1",
    "title" : "Large Language Models: A New Approach for Privacy Policy Analysis at\n  Scale",
    "summary" : "The number and dynamic nature of web and mobile applications presents\nsignificant challenges for assessing their compliance with data protection\nlaws. In this context, symbolic and statistical Natural Language Processing\n(NLP) techniques have been employed for the automated analysis of these\nsystems' privacy policies. However, these techniques typically require\nlabor-intensive and potentially error-prone manually annotated datasets for\ntraining and validation. This research proposes the application of Large\nLanguage Models (LLMs) as an alternative for effectively and efficiently\nextracting privacy practices from privacy policies at scale. Particularly, we\nleverage well-known LLMs such as ChatGPT and Llama 2, and offer guidance on the\noptimal design of prompts, parameters, and models, incorporating advanced\nstrategies such as few-shot learning. We further illustrate its capability to\ndetect detailed and varied privacy practices accurately. Using several renowned\ndatasets in the domain as a benchmark, our evaluation validates its exceptional\nperformance, achieving an F1 score exceeding 93%. Besides, it does so with\nreduced costs, faster processing times, and fewer technical knowledge\nrequirements. Consequently, we advocate for LLM-based solutions as a sound\nalternative to traditional NLP techniques for the automated analysis of privacy\npolicies at scale.",
    "updated" : "2024-05-31T15:12:33Z",
    "published" : "2024-05-31T15:12:33Z",
    "authors" : [
      {
        "name" : "David Rodriguez"
      },
      {
        "name" : "Ian Yang"
      },
      {
        "name" : "Jose M. Del Alamo"
      },
      {
        "name" : "Norman Sadeh"
      }
    ],
    "categories" : [
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.20761v1",
    "title" : "Share Your Secrets for Privacy! Confidential Forecasting with Vertical\n  Federated Learning",
    "summary" : "Vertical federated learning (VFL) is a promising area for time series\nforecasting in industrial applications, such as predictive maintenance and\nmachine control. Critical challenges to address in manufacturing include data\nprivacy and over-fitting on small and noisy datasets during both training and\ninference. Additionally, to increase industry adaptability, such forecasting\nmodels must scale well with the number of parties while ensuring strong\nconvergence and low-tuning complexity. We address those challenges and propose\n'Secret-shared Time Series Forecasting with VFL' (STV), a novel framework that\nexhibits the following key features: i) a privacy-preserving algorithm for\nforecasting with SARIMAX and autoregressive trees on vertically partitioned\ndata; ii) serverless forecasting using secret sharing and multi-party\ncomputation; iii) novel N-party algorithms for matrix multiplication and\ninverse operations for direct parameter optimization, giving strong convergence\nwith minimal hyperparameter tuning complexity. We conduct evaluations on six\nrepresentative datasets from public and industry-specific contexts. Our results\ndemonstrate that STV's forecasting accuracy is comparable to those of\ncentralized approaches. They also show that our direct optimization can\noutperform centralized methods, which include state-of-the-art diffusion models\nand long-short-term memory, by 23.81% on forecasting accuracy. We also conduct\na scalability analysis by examining the communication costs of direct and\niterative optimization to navigate the choice between the two. Code and\nappendix are available: https://github.com/adis98/STV",
    "updated" : "2024-05-31T12:27:38Z",
    "published" : "2024-05-31T12:27:38Z",
    "authors" : [
      {
        "name" : "Aditya Shankar"
      },
      {
        "name" : "Lydia Y. Chen"
      },
      {
        "name" : "Jérémie Decouchant"
      },
      {
        "name" : "Dimitra Gkorou"
      },
      {
        "name" : "Rihan Hai"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.20681v1",
    "title" : "No Free Lunch Theorem for Privacy-Preserving LLM Inference",
    "summary" : "Individuals and businesses have been significantly benefited by Large\nLanguage Models (LLMs) including PaLM, Gemini and ChatGPT in various ways. For\nexample, LLMs enhance productivity, reduce costs, and enable us to focus on\nmore valuable tasks. Furthermore, LLMs possess the capacity to sift through\nextensive datasets, uncover underlying patterns, and furnish critical insights\nthat propel the frontiers of technology and science. However, LLMs also pose\nprivacy concerns. Users' interactions with LLMs may expose their sensitive\npersonal or company information. A lack of robust privacy safeguards and legal\nframeworks could permit the unwarranted intrusion or improper handling of\nindividual data, thereby risking infringements of privacy and the theft of\npersonal identities. To ensure privacy, it is essential to minimize the\ndependency between shared prompts and private information. Various\nrandomization approaches have been proposed to protect prompts' privacy, but\nthey may incur utility loss compared to unprotected LLMs prompting. Therefore,\nit is essential to evaluate the balance between the risk of privacy leakage and\nloss of utility when conducting effective protection mechanisms. The current\nstudy develops a framework for inferring privacy-protected Large Language\nModels (LLMs) and lays down a solid theoretical basis for examining the\ninterplay between privacy preservation and utility. The core insight is\nencapsulated within a theorem that is called as the NFL (abbreviation of the\nword No-Free-Lunch) Theorem.",
    "updated" : "2024-05-31T08:22:53Z",
    "published" : "2024-05-31T08:22:53Z",
    "authors" : [
      {
        "name" : "Xiaojin Zhang"
      },
      {
        "name" : "Yulin Fei"
      },
      {
        "name" : "Yan Kang"
      },
      {
        "name" : "Wei Chen"
      },
      {
        "name" : "Lixin Fan"
      },
      {
        "name" : "Hai Jin"
      },
      {
        "name" : "Qiang Yang"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.20576v1",
    "title" : "Federated Graph Analytics with Differential Privacy",
    "summary" : "Collaborative graph analysis across multiple institutions is becoming\nincreasingly popular. Realistic examples include social network analysis across\nvarious social platforms, financial transaction analysis across multiple banks,\nand analyzing the transmission of infectious diseases across multiple\nhospitals. We define the federated graph analytics, a new problem for\ncollaborative graph analytics under differential privacy. Although\ndifferentially private graph analysis has been widely studied, it fails to\nachieve a good tradeoff between utility and privacy in federated scenarios, due\nto the limited view of local clients and overlapping information across\nmultiple subgraphs. Motivated by this, we first propose a federated graph\nanalytic framework, named FEAT, which enables arbitrary downstream common graph\nstatistics while preserving individual privacy. Furthermore, we introduce an\noptimized framework based on our proposed degree-based partition algorithm,\ncalled FEAT+, which improves the overall utility by leveraging the true local\nsubgraphs. Finally, extensive experiments demonstrate that our FEAT and FEAT+\nsignificantly outperform the baseline approach by approximately one and four\norders of magnitude, respectively.",
    "updated" : "2024-05-31T02:09:43Z",
    "published" : "2024-05-31T02:09:43Z",
    "authors" : [
      {
        "name" : "Shang Liu"
      },
      {
        "name" : "Yang Cao"
      },
      {
        "name" : "Takao Murakami"
      },
      {
        "name" : "Weiran Liu"
      },
      {
        "name" : "Seng Pei Liew"
      },
      {
        "name" : "Tsubasa Takahashi"
      },
      {
        "name" : "Jinfei Liu"
      },
      {
        "name" : "Masatoshi Yoshikawa"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.20483v1",
    "title" : "Hiding Your Awful Online Choices Made More Efficient and Secure: A New\n  Privacy-Aware Recommender System",
    "summary" : "Recommender systems are an integral part of online platforms that recommend\nnew content to users with similar interests. However, they demand a\nconsiderable amount of user activity data where, if the data is not adequately\nprotected, constitute a critical threat to the user privacy. Privacy-aware\nrecommender systems enable protection of such sensitive user data while still\nmaintaining a similar recommendation accuracy compared to the traditional\nnon-private recommender systems. However, at present, the current privacy-aware\nrecommender systems suffer from a significant trade-off between privacy and\ncomputational efficiency. For instance, it is well known that architectures\nthat rely purely on cryptographic primitives offer the most robust privacy\nguarantees, however, they suffer from substantial computational and network\noverhead. Thus, it is crucial to improve this trade-off for better performance.\nThis paper presents a novel privacy-aware recommender system that combines\nprivacy-aware machine learning algorithms for practical scalability and\nefficiency with cryptographic primitives like Homomorphic Encryption and\nMulti-Party Computation - without assumptions like trusted-party or secure\nhardware - for solid privacy guarantees. Experiments on standard benchmark\ndatasets show that our approach results in time and memory gains by three\norders of magnitude compared to using cryptographic primitives in a standalone\nfor constructing a privacy-aware recommender system. Furthermore, for the first\ntime our method makes it feasible to compute private recommendations for\ndatasets containing 100 million entries, even on memory-constrained low-power\nSOC (System on Chip) devices.",
    "updated" : "2024-05-30T21:08:42Z",
    "published" : "2024-05-30T21:08:42Z",
    "authors" : [
      {
        "name" : "Shibam Mukherjee"
      },
      {
        "name" : "Roman Walch"
      },
      {
        "name" : "Fredrik Meisingseth"
      },
      {
        "name" : "Elisabeth Lex"
      },
      {
        "name" : "Christian Rechberger"
      }
    ],
    "categories" : [
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.20405v1",
    "title" : "Private Mean Estimation with Person-Level Differential Privacy",
    "summary" : "We study differentially private (DP) mean estimation in the case where each\nperson holds multiple samples. Commonly referred to as the \"user-level\"\nsetting, DP here requires the usual notion of distributional stability when all\nof a person's datapoints can be modified. Informally, if $n$ people each have\n$m$ samples from an unknown $d$-dimensional distribution with bounded $k$-th\nmoments, we show that\n  \\[n = \\tilde \\Theta\\left(\\frac{d}{\\alpha^2 m} + \\frac{d }{ \\alpha m^{1/2}\n\\varepsilon} + \\frac{d}{\\alpha^{k/(k-1)} m \\varepsilon} +\n\\frac{d}{\\varepsilon}\\right)\\]\n  people are necessary and sufficient to estimate the mean up to distance\n$\\alpha$ in $\\ell_2$-norm under $\\varepsilon$-differential privacy (and its\ncommon relaxations). In the multivariate setting, we give computationally\nefficient algorithms under approximate DP (with slightly degraded sample\ncomplexity) and computationally inefficient algorithms under pure DP, and our\nnearly matching lower bounds hold for the most permissive case of approximate\nDP. Our computationally efficient estimators are based on the well known\nnoisy-clipped-mean approach, but the analysis for our setting requires new\nbounds on the tails of sums of independent, vector-valued, bounded-moments\nrandom variables, and a new argument for bounding the bias introduced by\nclipping.",
    "updated" : "2024-05-30T18:20:35Z",
    "published" : "2024-05-30T18:20:35Z",
    "authors" : [
      {
        "name" : "Sushant Agarwal"
      },
      {
        "name" : "Gautam Kamath"
      },
      {
        "name" : "Mahbod Majid"
      },
      {
        "name" : "Argyris Mouzakis"
      },
      {
        "name" : "Rose Silver"
      },
      {
        "name" : "Jonathan Ullman"
      }
    ],
    "categories" : [
      "cs.DS",
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.19831v2",
    "title" : "Just Rewrite It Again: A Post-Processing Method for Enhanced Semantic\n  Similarity and Privacy Preservation of Differentially Private Rewritten Text",
    "summary" : "The study of Differential Privacy (DP) in Natural Language Processing often\nviews the task of text privatization as a $\\textit{rewriting}$ task, in which\nsensitive input texts are rewritten to hide explicit or implicit private\ninformation. In order to evaluate the privacy-preserving capabilities of a DP\ntext rewriting mechanism, $\\textit{empirical privacy}$ tests are frequently\nemployed. In these tests, an adversary is modeled, who aims to infer sensitive\ninformation (e.g., gender) about the author behind a (privatized) text. Looking\nto improve the empirical protections provided by DP rewriting methods, we\npropose a simple post-processing method based on the goal of aligning rewritten\ntexts with their original counterparts, where DP rewritten texts are rewritten\n$\\textit{again}$. Our results show that such an approach not only produces\noutputs that are more semantically reminiscent of the original inputs, but also\ntexts which score on average better in empirical privacy evaluations.\nTherefore, our approach raises the bar for DP rewriting methods in their\nempirical privacy evaluations, providing an extra layer of protection against\nmalicious adversaries.",
    "updated" : "2024-05-31T07:24:55Z",
    "published" : "2024-05-30T08:41:33Z",
    "authors" : [
      {
        "name" : "Stephen Meisenbacher"
      },
      {
        "name" : "Florian Matthes"
      }
    ],
    "categories" : [
      "cs.CL"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.20785v1",
    "title" : "How the Future Works at SOUPS: Analyzing Future Work Statements and\n  Their Impact on Usable Security and Privacy Research",
    "summary" : "Extending knowledge by identifying and investigating valuable research\nquestions and problems is a core function of research. Research publications\noften suggest avenues for future work to extend and build upon their results.\nConsidering these suggestions can contribute to developing research ideas that\nbuild upon previous work and produce results that tie into existing knowledge.\nUsable security and privacy researchers commonly add future work statements to\ntheir publications. However, our community lacks an in-depth understanding of\ntheir prevalence, quality, and impact on future research.\n  Our work aims to address this gap in the research literature. We reviewed all\n27 papers from the 2019 SOUPS proceedings and analyzed their future work\nstatements. Additionally, we analyzed 978 publications that cite any paper from\nSOUPS 2019 proceedings to assess their future work statements' impact. We find\nthat most papers from the SOUPS 2019 proceedings include future work\nstatements. However, they are often unspecific or ambiguous, and not always\neasy to find. Therefore, the citing publications often matched the future work\nstatements' content thematically, but rarely explicitly acknowledged them,\nindicating a limited impact. We conclude with recommendations for the usable\nsecurity and privacy community to improve the utility of future work statements\nby making them more tangible and actionable, and avenues for future work.",
    "updated" : "2024-05-30T07:07:18Z",
    "published" : "2024-05-30T07:07:18Z",
    "authors" : [
      {
        "name" : "Jacques Suray"
      },
      {
        "name" : "Jan H. Klemmer"
      },
      {
        "name" : "Juliane Schmüser"
      },
      {
        "name" : "Sascha Fahl"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.CY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.20769v1",
    "title" : "Avoiding Pitfalls for Privacy Accounting of Subsampled Mechanisms under\n  Composition",
    "summary" : "We consider the problem of computing tight privacy guarantees for the\ncomposition of subsampled differentially private mechanisms. Recent algorithms\ncan numerically compute the privacy parameters to arbitrary precision but must\nbe carefully applied.\n  Our main contribution is to address two common points of confusion. First,\nsome privacy accountants assume that the privacy guarantees for the composition\nof a subsampled mechanism are determined by self-composing the worst-case\ndatasets for the uncomposed mechanism. We show that this is not true in\ngeneral. Second, Poisson subsampling is sometimes assumed to have similar\nprivacy guarantees compared to sampling without replacement. We show that the\nprivacy guarantees may in fact differ significantly between the two sampling\nschemes. In particular, we give an example of hyperparameters that result in\n$\\varepsilon \\approx 1$ for Poisson subsampling and $\\varepsilon > 10$ for\nsampling without replacement. This occurs for some parameters that could\nrealistically be chosen for DP-SGD.",
    "updated" : "2024-05-27T20:30:12Z",
    "published" : "2024-05-27T20:30:12Z",
    "authors" : [
      {
        "name" : "Christian Janos Lebeda"
      },
      {
        "name" : "Matthew Regehr"
      },
      {
        "name" : "Gautam Kamath"
      },
      {
        "name" : "Thomas Steinke"
      }
    ],
    "categories" : [
      "cs.CR",
      "cs.DS",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2405.16905v2",
    "title" : "Privacy and Security Trade-off in Interconnected Systems with Known or\n  Unknown Privacy Noise Covariance",
    "summary" : "This paper is concerned with the security problem for interconnected systems,\nwhere each subsystem is required to detect local attacks using locally\navailable information and the information received from its neighboring\nsubsystems. Moreover, we consider that there exists an additional eavesdropper\nbeing able to infer the private information by eavesdropping transmitted data\nbetween subsystems. Then, a privacy-preserving method is employed by adding\nprivacy noise to transmitted data, and the privacy level is measured by mutual\ninformation. Nevertheless, adding privacy noise to transmitted data may affect\nthe detection performance metrics such as detection probability and false alarm\nprobability. Thus, we theoretically analyze the trade-off between the privacy\nand the detection performance. An optimization problem with maximizing both the\ndegree of privacy preservation and the detection probability is established to\nobtain the covariance of the privacy noise. In addition, the attack detector of\neach subsystem may not obtain all information about the privacy noise. We\nfurther theoretically analyze the trade-off between the privacy and the false\nalarm probability when the attack detector has no knowledge of the privacy\nnoise covariance. An optimization problem with maximizing the degree of privacy\npreservation with guaranteeing a bound of false alarm distortion level is\nestablished to obtain {\\color{black}{the covariance of the privacy noise}}.\nMoreover, to analyze the effect of the privacy noise on the detection\nprobability, we consider that each subsystem can estimate the unknown privacy\nnoise covariance by the secondary data. Based on the estimated covariance, we\nconstruct another attack detector and analyze how the privacy noise affects its\ndetection performance. Finally, a numerical example is provided to verify the\neffectiveness of theoretical results.",
    "updated" : "2024-06-01T14:49:47Z",
    "published" : "2024-05-27T07:53:51Z",
    "authors" : [
      {
        "name" : "Haojun Wang"
      },
      {
        "name" : "Kun Liu"
      },
      {
        "name" : "Baojia Li"
      },
      {
        "name" : "Emilia Fridman"
      },
      {
        "name" : "Yuanqing Xia"
      }
    ],
    "categories" : [
      "eess.SY",
      "cs.SY"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.00073v1",
    "title" : "A Novel Review of Stability Techniques for Improved Privacy-Preserving\n  Machine Learning",
    "summary" : "Machine learning models have recently enjoyed a significant increase in size\nand popularity. However, this growth has created concerns about dataset\nprivacy. To counteract data leakage, various privacy frameworks guarantee that\nthe output of machine learning models does not compromise their training data.\nHowever, this privatization comes at a cost by adding random noise to the\ntraining process, which reduces model performance. By making models more\nresistant to small changes in input and thus more stable, the necessary amount\nof noise can be decreased while still protecting privacy. This paper\ninvestigates various techniques to enhance stability, thereby minimizing the\nnegative effects of privatization in machine learning.",
    "updated" : "2024-05-31T00:30:29Z",
    "published" : "2024-05-31T00:30:29Z",
    "authors" : [
      {
        "name" : "Coleman DuPlessie"
      },
      {
        "name" : "Aidan Gao"
      }
    ],
    "categories" : [
      "cs.LG",
      "cs.CR"
    ]
  },
  {
    "id" : "http://arxiv.org/abs/2406.01603v1",
    "title" : "Privacy-preserving recommender system using the data collaboration\n  analysis for distributed datasets",
    "summary" : "In order to provide high-quality recommendations for users, it is desirable\nto share and integrate multiple datasets held by different parties. However,\nwhen sharing such distributed datasets, we need to protect personal and\nconfidential information contained in the datasets. To this end, we establish a\nframework for privacy-preserving recommender systems using the data\ncollaboration analysis of distributed datasets. Numerical experiments with two\npublic rating datasets demonstrate that our privacy-preserving method for\nrating prediction can improve the prediction accuracy for distributed datasets.\nThis study opens up new possibilities for privacy-preserving techniques in\nrecommender systems.",
    "updated" : "2024-05-24T07:43:00Z",
    "published" : "2024-05-24T07:43:00Z",
    "authors" : [
      {
        "name" : "Tomoya Yanagi"
      },
      {
        "name" : "Shunnosuke Ikeda"
      },
      {
        "name" : "Noriyoshi Sukegawa"
      },
      {
        "name" : "Yuichi Takano"
      }
    ],
    "categories" : [
      "cs.IR",
      "cs.CR",
      "cs.LG"
    ]
  }
]