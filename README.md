
<h2>2024-04</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.19744v1">PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for
  Privacy Policy Compliance Verification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-04-30T17:44:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leon Garza, Lavanya Elluri, Anantaa Kotal, Aritran Piplai, Deepti Gupta, Anupam Joshi</p>
    <p><b>Summary:</b> Data protection and privacy is becoming increasingly crucial in the digital
era. Numerous companies depend on third-party vendors and service providers to
carry out critical functions within their operations, encompassing tasks such
as data handling and storage. However, this reliance introduces potential
vulnerabilities, as these vendors' security measures and practices may not
always align with the standards expected by regulatory bodies. Businesses are
required, often under the penalty of law, to ensure compliance with the
evolving regulatory rules. Interpreting and implementing these regulations pose
challenges due to their complexity. Regulatory documents are extensive,
demanding significant effort for interpretation, while vendor-drafted privacy
policies often lack the detail required for full legal compliance, leading to
ambiguity. To ensure a concise interpretation of the regulatory requirements
and compliance of organizational privacy policy with said regulations, we
propose a Large Language Model (LLM) and Semantic Web based approach for
privacy compliance. In this paper, we develop the novel Privacy Policy
Compliance Verification Knowledge Graph, PrivComp-KG. It is designed to
efficiently store and retrieve comprehensive information concerning privacy
policies, regulatory frameworks, and domain-specific knowledge pertaining to
the legal landscape of privacy. Using Retrieval Augmented Generation, we
identify the relevant sections in a privacy policy with corresponding
regulatory rules. This information about individual privacy policies is
populated into the PrivComp-KG. Combining this with the domain context and
rules, the PrivComp-KG can be queried to check for compliance with privacy
policies by each vendor against relevant policy regulations. We demonstrate the
relevance of the PrivComp-KG, by verifying compliance of privacy policy
documents for various organizations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.19677v1">A Comprehensive Analysis of Pegasus Spyware and Its Implications for
  Digital Privacy and Security</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-30T16:10:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karwan Kareem</p>
    <p><b>Summary:</b> This paper comprehensively analyzes the Pegasus spyware and its implications
for digital privacy and security. The Israeli cyber intelligence company NSO
Group's Pegasus has gained recognition as a potent surveillance tool capable of
hacking into smartphones and extracting data without the user's knowledge [49],
[50]. The research emphasizes the technical aspects of this spyware, its
deployment methods, and the controversies surrounding its use. The research
also emphasizes the growing worries surrounding digital privacy and security as
a result of the prevalent use of advanced spyware. By delving into legal,
ethical, and policy issues, the objective of this study is to deliver a
holistic understanding of the challenges posed by Pegasus and similar spyware
tools. Through a comprehensive examination of the subject, the paper presents
potential solutions to mitigate the threats and protect users from invasive
surveillance techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.18814v1">Belt and Brace: When Federated Learning Meets Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-29T15:51:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xuebin Ren, Shusen Yang, Cong Zhao, Julie McCann, Zongben Xu</p>
    <p><b>Summary:</b> Federated learning (FL) has great potential for large-scale machine learning
(ML) without exposing raw data.Differential privacy (DP) is the de facto
standard of privacy protection with provable guarantees.Advances in ML suggest
that DP would be a perfect fit for FL with comprehensive privacy preservation.
Hence, extensive efforts have been devoted to achieving practically usable FL
with DP, which however is still challenging.Practitioners often not only are
not fully aware of its development and categorization, but also face a hard
choice between privacy and utility. Therefore, it calls for a holistic review
of current advances and an investigation on the challenges and opportunities
for highly usable FL systems with a DP guarantee. In this article, we first
introduce the primary concepts of FL and DP, and highlight the benefits of
integration. We then review the current developments by categorizing different
paradigms and notions. Aiming at usable FL with DP, we present the optimization
principles to seek a better tradeoff between model utility and privacy loss.
Finally, we discuss future challenges in the emergent areas and relevant
research topics.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.18002v1">Towards Privacy-Preserving Audio Classification Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> 
  <p><b>Published on:</b> 2024-04-27T20:36:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bhawana Chhaglani, Jeremy Gummeson, Prashant Shenoy</p>
    <p><b>Summary:</b> Audio signals can reveal intimate details about a person's life, including
their conversations, health status, emotions, location, and personal
preferences. Unauthorized access or misuse of this information can have
profound personal and social implications. In an era increasingly populated by
devices capable of audio recording, safeguarding user privacy is a critical
obligation. This work studies the ethical and privacy concerns in current audio
classification systems. We discuss the challenges and research directions in
designing privacy-preserving audio sensing systems. We propose
privacy-preserving audio features that can be used to classify wide range of
audio classes, while being privacy preserving.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.17984v1">Privacy-Preserving, Dropout-Resilient Aggregation in Decentralized
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-04-27T19:17:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ali Reza Ghavamipour, Benjamin Zi Hao Zhao, Fatih Turkmen</p>
    <p><b>Summary:</b> Decentralized learning (DL) offers a novel paradigm in machine learning by
distributing training across clients without central aggregation, enhancing
scalability and efficiency. However, DL's peer-to-peer model raises challenges
in protecting against inference attacks and privacy leaks. By forgoing central
bottlenecks, DL demands privacy-preserving aggregation methods to protect data
from 'honest but curious' clients and adversaries, maintaining network-wide
privacy. Privacy-preserving DL faces the additional hurdle of client dropout,
clients not submitting updates due to connectivity problems or unavailability,
further complicating aggregation.
  This work proposes three secret sharing-based dropout resilience approaches
for privacy-preserving DL. Our study evaluates the efficiency, performance, and
accuracy of these protocols through experiments on datasets such as MNIST,
Fashion-MNIST, SVHN, and CIFAR-10. We compare our protocols with traditional
secret-sharing solutions across scenarios, including those with up to 1000
clients. Evaluations show that our protocols significantly outperform
conventional methods, especially in scenarios with up to 30% of clients dropout
and model sizes of up to $10^6$ parameters. Our approaches demonstrate markedly
high efficiency with larger models, higher dropout rates, and extensive client
networks, highlighting their effectiveness in enhancing decentralized learning
systems' privacy and dropout robustness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.17970v1">Privacy-Preserving Aggregation for Decentralized Learning with
  Byzantine-Robustness</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-04-27T18:17:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ali Reza Ghavamipour, Benjamin Zi Hao Zhao, Oguzhan Ersoy, Fatih Turkmen</p>
    <p><b>Summary:</b> Decentralized machine learning (DL) has been receiving an increasing interest
recently due to the elimination of a single point of failure, present in
Federated learning setting. Yet, it is threatened by the looming threat of
Byzantine clients who intentionally disrupt the learning process by
broadcasting arbitrary model updates to other clients, seeking to degrade the
performance of the global model. In response, robust aggregation schemes have
emerged as promising solutions to defend against such Byzantine clients,
thereby enhancing the robustness of Decentralized Learning. Defenses against
Byzantine adversaries, however, typically require access to the updates of
other clients, a counterproductive privacy trade-off that in turn increases the
risk of inference attacks on those same model updates.
  In this paper, we introduce SecureDL, a novel DL protocol designed to enhance
the security and privacy of DL against Byzantine threats. SecureDL~facilitates
a collaborative defense, while protecting the privacy of clients' model updates
through secure multiparty computation. The protocol employs efficient
computation of cosine similarity and normalization of updates to robustly
detect and exclude model updates detrimental to model convergence. By using
MNIST, Fashion-MNIST, SVHN and CIFAR-10 datasets, we evaluated SecureDL against
various Byzantine attacks and compared its effectiveness with four existing
defense mechanisms. Our experiments show that SecureDL is effective even in the
case of attacks by the malicious majority (e.g., 80% Byzantine clients) while
preserving high training accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.00725v1">Federated Learning and Differential Privacy Techniques on Multi-hospital
  Population-scale Electrocardiogram Data</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-26T19:29:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vikhyat Agrawal, Sunil Vasu Kalmady, Venkataseetharam Manoj Malipeddi, Manisimha Varma Manthena, Weijie Sun, Saiful Islam, Abram Hindle, Padma Kaul, Russell Greiner</p>
    <p><b>Summary:</b> This research paper explores ways to apply Federated Learning (FL) and
Differential Privacy (DP) techniques to population-scale Electrocardiogram
(ECG) data. The study learns a multi-label ECG classification model using FL
and DP based on 1,565,849 ECG tracings from 7 hospitals in Alberta, Canada. The
FL approach allowed collaborative model training without sharing raw data
between hospitals while building robust ECG classification models for
diagnosing various cardiac conditions. These accurate ECG classification models
can facilitate the diagnoses while preserving patient confidentiality using FL
and DP techniques. Our results show that the performance achieved using our
implementation of the FL approach is comparable to that of the pooled approach,
where the model is trained over the aggregating data from all hospitals.
Furthermore, our findings suggest that hospitals with limited ECGs for training
can benefit from adopting the FL model compared to single-site training. In
addition, this study showcases the trade-off between model performance and data
privacy by employing DP during model training. Our code is available at
https://github.com/vikhyatt/Hospital-FL-DP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.17399v1">Evaluations of Machine Learning Privacy Defenses are Misleading</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-26T13:21:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michael Aerni, Jie Zhang, Florian Tramèr</p>
    <p><b>Summary:</b> Empirical defenses for machine learning privacy forgo the provable guarantees
of differential privacy in the hope of achieving higher utility while resisting
realistic adversaries. We identify severe pitfalls in existing empirical
privacy evaluations (based on membership inference attacks) that result in
misleading conclusions. In particular, we show that prior evaluations fail to
characterize the privacy leakage of the most vulnerable samples, use weak
attacks, and avoid comparisons with practical differential privacy baselines.
In 5 case studies of empirical privacy defenses, we find that prior evaluations
underestimate privacy leakage by an order of magnitude. Under our stronger
evaluation, none of the empirical defenses we study are competitive with a
properly tuned, high-utility DP-SGD baseline (with vacuous provable
guarantees).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.17225v1">Enhancing Privacy and Security of Autonomous UAV Navigation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E">
  <p><b>Published on:</b> 2024-04-26T07:54:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vatsal Aggarwal, Arjun Ramesh Kaushik, Charanjit Jutla, Nalini Ratha</p>
    <p><b>Summary:</b> Autonomous Unmanned Aerial Vehicles (UAVs) have become essential tools in
defense, law enforcement, disaster response, and product delivery. These
autonomous navigation systems require a wireless communication network, and of
late are deep learning based. In critical scenarios such as border protection
or disaster response, ensuring the secure navigation of autonomous UAVs is
paramount. But, these autonomous UAVs are susceptible to adversarial attacks
through the communication network or the deep learning models - eavesdropping /
man-in-the-middle / membership inference / reconstruction. To address this
susceptibility, we propose an innovative approach that combines Reinforcement
Learning (RL) and Fully Homomorphic Encryption (FHE) for secure autonomous UAV
navigation. This end-to-end secure framework is designed for real-time video
feeds captured by UAV cameras and utilizes FHE to perform inference on
encrypted input images. While FHE allows computations on encrypted data,
certain computational operators are yet to be implemented. Convolutional neural
networks, fully connected neural networks, activation functions and OpenAI Gym
Library are meticulously adapted to the FHE domain to enable encrypted data
processing. We demonstrate the efficacy of our proposed approach through
extensive experimentation. Our proposed approach ensures security and privacy
in autonomous UAV navigation with negligible loss in performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.16706v3">Efficient and Near-Optimal Noise Generation for Streaming Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computational Complexity-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-25T16:11:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Krishnamurthy Dvijotham, H. Brendan McMahan, Krishna Pillutla, Thomas Steinke, Abhradeep Thakurta</p>
    <p><b>Summary:</b> In the task of differentially private (DP) continual counting, we receive a
stream of increments and our goal is to output an approximate running total of
these increments, without revealing too much about any specific increment.
Despite its simplicity, differentially private continual counting has attracted
significant attention both in theory and in practice. Existing algorithms for
differentially private continual counting are either inefficient in terms of
their space usage or add an excessive amount of noise, inducing suboptimal
utility.
  The most practical DP continual counting algorithms add carefully correlated
Gaussian noise to the values. The task of choosing the covariance for this
noise can be expressed in terms of factoring the lower-triangular matrix of
ones (which computes prefix sums). We present two approaches from this class
(for different parameter regimes) that achieve near-optimal utility for DP
continual counting and only require logarithmic or polylogarithmic space (and
time).
  Our first approach is based on a space-efficient streaming matrix
multiplication algorithm for a class of Toeplitz matrices. We show that to
instantiate this algorithm for DP continual counting, it is sufficient to find
a low-degree rational function that approximates the square root on a circle in
the complex plane. We then apply and extend tools from approximation theory to
achieve this. We also derive efficient closed-forms for the objective function
for arbitrarily many steps, and show direct numerical optimization yields a
highly practical solution to the problem. Our second approach combines our
first approach with a recursive construction similar to the binary tree
mechanism.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.16638v1">Privacy-Preserving Statistical Data Generation: Application to Sepsis
  Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-25T14:26:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Eric Macias-Fassio, Aythami Morales, Cristina Pruenza, Julian Fierrez</p>
    <p><b>Summary:</b> The biomedical field is among the sectors most impacted by the increasing
regulation of Artificial Intelligence (AI) and data protection legislation,
given the sensitivity of patient information. However, the rise of synthetic
data generation methods offers a promising opportunity for data-driven
technologies. In this study, we propose a statistical approach for synthetic
data generation applicable in classification problems. We assess the utility
and privacy implications of synthetic data generated by Kernel Density
Estimator and K-Nearest Neighbors sampling (KDE-KNN) within a real-world
context, specifically focusing on its application in sepsis detection. The
detection of sepsis is a critical challenge in clinical practice due to its
rapid progression and potentially life-threatening consequences. Moreover, we
emphasize the benefits of KDE-KNN compared to current synthetic data generation
methodologies. Additionally, our study examines the effects of incorporating
synthetic data into model training procedures. This investigation provides
valuable insights into the effectiveness of synthetic data generation
techniques in mitigating regulatory constraints within the biomedical field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.16587v1">Understanding Privacy Risks of Embeddings Induced by Large Language
  Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-04-25T13:10:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhihao Zhu, Ninglu Shao, Defu Lian, Chenwang Wu, Zheng Liu, Yi Yang, Enhong Chen</p>
    <p><b>Summary:</b> Large language models (LLMs) show early signs of artificial general
intelligence but struggle with hallucinations. One promising solution to
mitigate these hallucinations is to store external knowledge as embeddings,
aiding LLMs in retrieval-augmented generation. However, such a solution risks
compromising privacy, as recent studies experimentally showed that the original
text can be partially reconstructed from text embeddings by pre-trained
language models. The significant advantage of LLMs over traditional pre-trained
models may exacerbate these concerns. To this end, we investigate the
effectiveness of reconstructing original knowledge and predicting entity
attributes from these embeddings when LLMs are employed. Empirical findings
indicate that LLMs significantly improve the accuracy of two evaluated tasks
over those from pre-trained models, regardless of whether the texts are
in-distribution or out-of-distribution. This underscores a heightened potential
for LLMs to jeopardize user privacy, highlighting the negative consequences of
their widespread use. We further discuss preliminary strategies to mitigate
this risk.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.16255v1">Enhancing Privacy in Face Analytics Using Fully Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-04-24T23:56:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bharat Yalavarthi, Arjun Ramesh Kaushik, Arun Ross, Vishnu Boddeti, Nalini Ratha</p>
    <p><b>Summary:</b> Modern face recognition systems utilize deep neural networks to extract
salient features from a face. These features denote embeddings in latent space
and are often stored as templates in a face recognition system. These
embeddings are susceptible to data leakage and, in some cases, can even be used
to reconstruct the original face image. To prevent compromising identities,
template protection schemes are commonly employed. However, these schemes may
still not prevent the leakage of soft biometric information such as age, gender
and race. To alleviate this issue, we propose a novel technique that combines
Fully Homomorphic Encryption (FHE) with an existing template protection scheme
known as PolyProtect. We show that the embeddings can be compressed and
encrypted using FHE and transformed into a secure PolyProtect template using
polynomial transformation, for additional protection. We demonstrate the
efficacy of the proposed approach through extensive experiments on multiple
datasets. Our proposed approach ensures irreversibility and unlinkability,
effectively preventing the leakage of soft biometric attributes from face
embeddings without compromising recognition accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.16241v1">Synergizing Privacy and Utility in Data Analytics Through Advanced
  Information Theorization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-24T22:58:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zahir Alsulaimawi</p>
    <p><b>Summary:</b> This study develops a novel framework for privacy-preserving data analytics,
addressing the critical challenge of balancing data utility with privacy
concerns. We introduce three sophisticated algorithms: a Noise-Infusion
Technique tailored for high-dimensional image data, a Variational Autoencoder
(VAE) for robust feature extraction while masking sensitive attributes and an
Expectation Maximization (EM) approach optimized for structured data privacy.
Applied to datasets such as Modified MNIST and CelebrityA, our methods
significantly reduce mutual information between sensitive attributes and
transformed data, thereby enhancing privacy. Our experimental results confirm
that these approaches achieve superior privacy protection and retain high
utility, making them viable for practical applications where both aspects are
crucial. The research contributes to the field by providing a flexible and
effective strategy for deploying privacy-preserving algorithms across various
data types and establishing new benchmarks for utility and confidentiality in
data analytics.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.15886v1">Privacy-Preserving Billing for Local Energy Markets (Long Version)</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-24T14:12:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Eman Alqahtani, Mustafa A. Mustafa</p>
    <p><b>Summary:</b> We propose a privacy-preserving billing protocol for local energy markets
(PBP-LEMs) that takes into account market participants' energy volume
deviations from their bids. PBP-LEMs enables a group of market entities to
jointly compute participants' bills in a decentralized and privacy-preserving
manner without sacrificing correctness. It also mitigates risks on individuals'
privacy arising from any potential internal collusion. We first propose a
novel, efficient, and privacy-preserving individual billing scheme, achieving
information-theoretic security, which serves as a building block. PBP-LEMs
utilizes this scheme, along with other techniques such as multiparty
computation, Pedersen commitments and inner product functional encryption, to
ensure data confidentiality and accuracy. Additionally, we present three
approaches, resulting in different levels of privacy and performance. We prove
that the protocol meets its security and privacy requirements and is feasible
for deployment in real LEMs. Our analysis also shows variations in overall
performance and identifies areas where overhead is concentrated based on the
applied approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.15859v1">Secure and Privacy-Preserving Authentication for Data Subject Rights
  Enforcement</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-24T13:17:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Malte Hansen, Andre Büttner</p>
    <p><b>Summary:</b> In light of the GDPR, data controllers (DC) need to allow data subjects (DS)
to exercise certain data subject rights. A key requirement here is that DCs can
reliably authenticate a DS. Due to a lack of clear technical specifications,
this has been realized in different ways, such as by requesting copies of ID
documents or by email address verification. However, previous research has
shown that this is associated with various security and privacy risks and that
identifying DSs can be a non-trivial task. In this paper, we review different
authentication schemes and propose an architecture that enables DCs to
authenticate DSs with the help of independent Identity Providers in a secure
and privacy-preserving manner by utilizing attribute-based credentials and
eIDs. Our work contributes to a more standardized and privacy-preserving way of
authenticating DSs, which will benefit both DCs and DSs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.15821v1">SynthEval: A Framework for Detailed Utility and Privacy Evaluation of
  Tabular Synthetic Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Performance-F9C80E">
  <p><b>Published on:</b> 2024-04-24T11:49:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anton Danholt Lautrup, Tobias Hyrup, Arthur Zimek, Peter Schneider-Kamp</p>
    <p><b>Summary:</b> With the growing demand for synthetic data to address contemporary issues in
machine learning, such as data scarcity, data fairness, and data privacy,
having robust tools for assessing the utility and potential privacy risks of
such data becomes crucial. SynthEval, a novel open-source evaluation framework
distinguishes itself from existing tools by treating categorical and numerical
attributes with equal care, without assuming any special kind of preprocessing
steps. This~makes it applicable to virtually any synthetic dataset of tabular
records. Our tool leverages statistical and machine learning techniques to
comprehensively evaluate synthetic data fidelity and privacy-preserving
integrity. SynthEval integrates a wide selection of metrics that can be used
independently or in highly customisable benchmark configurations, and can
easily be extended with additional metrics. In this paper, we describe
SynthEval and illustrate its versatility with examples. The framework
facilitates better benchmarking and more consistent comparisons of model
capabilities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.15686v2">Noise Variance Optimization in Differential Privacy: A Game-Theoretic
  Approach Through Per-Instance Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-24T06:51:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sehyun Ryu, Jonggyu Jang, Hyun Jong Yang</p>
    <p><b>Summary:</b> The concept of differential privacy (DP) can quantitatively measure privacy
loss by observing the changes in the distribution caused by the inclusion of
individuals in the target dataset. The DP, which is generally used as a
constraint, has been prominent in safeguarding datasets in machine learning in
industry giants like Apple and Google. A common methodology for guaranteeing DP
is incorporating appropriate noise into query outputs, thereby establishing
statistical defense systems against privacy attacks such as membership
inference and linkage attacks. However, especially for small datasets, existing
DP mechanisms occasionally add excessive amount of noise to query output,
thereby discarding data utility. This is because the traditional DP computes
privacy loss based on the worst-case scenario, i.e., statistical outliers. In
this work, to tackle this challenge, we utilize per-instance DP (pDP) as a
constraint, measuring privacy loss for each data instance and optimizing noise
tailored to individual instances. In a nutshell, we propose a per-instance
noise variance optimization (NVO) game, framed as a common interest sequential
game, and show that the Nash equilibrium (NE) points of it inherently guarantee
pDP for all data instances. Through extensive experiments, our proposed pDP
algorithm demonstrated an average performance improvement of up to 99.53%
compared to the conventional DP algorithm in terms of KL divergence.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.05924v1">Privacy Protection and Video Manipulation in Immersive Media</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-04-23T17:37:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leslie Wöhler, Satoshi Ikehata, Kiyoharu Aizawa</p>
    <p><b>Summary:</b> In comparison to traditional footage, 360{\deg} videos can convey engaging,
immersive experiences and even be utilized to create interactive virtual
environments. Like regular recordings, these videos need to consider the
privacy of recorded people and could be targets for video manipulations.
However, due to their properties like enhanced presence, the effects on users
might differ from traditional, non-immersive content. Therefore, we are
interested in how changes of real-world footage like adding privacy protection
or applying video manipulations could mitigate or introduce harm in the
resulting immersive media.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.14983v1">Zero-Knowledge Location Privacy via Accurate Floating Point SNARKs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-23T12:38:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jens Ernstberger, Chengru Zhang, Luca Ciprian, Philipp Jovanovic, Sebastian Steinhorst</p>
    <p><b>Summary:</b> This paper introduces Zero-Knowledge Location Privacy (ZKLP), enabling users
to prove to third parties that they are within a specified geographical region
while not disclosing their exact location. ZKLP supports varying levels of
granularity, allowing for customization depending on the use case. To realize
ZKLP, we introduce the first set of Zero-Knowledge Proof (ZKP) circuits that
are fully compliant to the IEEE 754 standard for floating-point arithmetic.
  Our results demonstrate that our floating point implementation scales
efficiently, requiring only $69$ constraints per multiplication for $2^{15}$
single-precision floating-point multiplications. We utilize our floating point
implementation to realize the ZKLP paradigm. In comparison to the
state-of-the-art, we find that our optimized implementation has $14.1 \times$
less constraints utilizing single precision floating-point values, and $11.2
\times$ less constraints when utilizing double precision floating-point values.
We demonstrate the practicability of ZKLP by building a protocol for privacy
preserving peer-to-peer proximity testing - Alice can test if she is close to
Bob by receiving a single message, without either party revealing any other
information about their location. In such a configuration, Bob can create a
proof of (non-)proximity in $0.27 s$, whereas Alice can verify her distance to
about $250$ peers per second</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.14895v1">Bayesian Approaches to Collaborative Data Analysis with Strict Privacy
  Restrictions</a></h3>
   
  <p><b>Published on:</b> 2024-04-23T10:22:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Simon Busch-Moreno, Moritz U. G. Kraemer</p>
    <p><b>Summary:</b> Collaborative data analysis between countries is crucial for enabling fast
responses to increasingly multi-country disease outbreaks. Often, data early in
outbreaks are of sensitive nature and subject to strict privacy restrictions.
Thus, federated analysis, which implies decentralised collaborative analysis
where no raw data sharing is required, emerged as a novel approach solving
issues around data privacy and confidentiality. In the present study, we
propose two approaches to federated analysis, based on simple Bayesian
statistics and exploit this simplicity to make them feasible for rapid
collaboration without the risks of data leaks and data reidentification, as
they require neither data sharing nor direct communication between devices. The
first approach uses summaries from parameters' posteriors previously obtained
at a different location to update truncated normal distributions approximating
priors of a new model. The second approach uses the entire previously sampled
posterior, approximating via a multivariate normal distribution. We test these
models on simulated and on real outbreak data to estimate the incubation period
of infectious diseases. Results indicate that both approaches can recover
incubation period parameters accurately, but they differ in terms of
inferential capacity. The posterior summary approach shows higher stability and
precision, but it cannot capture posterior correlations, meaning it is
inferentially limited. The whole posterior approach can capture correlations,
but it shows less stability, and its applicability is limited to fewer prior
distributions. We discuss results in terms of the advantages of their
simplicity and privacy-preserving properties, and in terms of their limited
generalisability to more complex analytical models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.14693v1">Double Privacy Guard: Robust Traceable Adversarial Watermarking against
  Face Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> 
  <p><b>Published on:</b> 2024-04-23T02:50:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunming Zhang, Dengpan Ye, Sipeng Shen, Caiyun Xie, Ziyi Liu, Jiacheng Deng, Long Tang</p>
    <p><b>Summary:</b> The wide deployment of Face Recognition (FR) systems poses risks of privacy
leakage. One countermeasure to address this issue is adversarial attacks, which
deceive malicious FR searches but simultaneously interfere the normal identity
verification of trusted authorizers. In this paper, we propose the first Double
Privacy Guard (DPG) scheme based on traceable adversarial watermarking. DPG
employs a one-time watermark embedding to deceive unauthorized FR models and
allows authorizers to perform identity verification by extracting the
watermark. Specifically, we propose an information-guided adversarial attack
against FR models. The encoder embeds an identity-specific watermark into the
deep feature space of the carrier, guiding recognizable features of the image
to deviate from the source identity. We further adopt a collaborative
meta-optimization strategy compatible with sub-tasks, which regularizes the
joint optimization direction of the encoder and decoder. This strategy enhances
the representation of universal carrier features, mitigating multi-objective
optimization conflicts in watermarking. Experiments confirm that DPG achieves
significant attack success rates and traceability accuracy on state-of-the-art
FR models, exhibiting remarkable robustness that outperforms the existing
privacy protection methods using adversarial attacks and deep watermarking, or
simple combinations of the two. Our work potentially opens up new insights into
proactive protection for FR privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.13426v1">Data Privacy Vocabulary (DPV) -- Version 2</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-04-20T17:24:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Harshvardhan J. Pandit, Beatriz Esteves, Georg P. Krog, Paul Ryan, Delaram Golpayegani, Julian Flake</p>
    <p><b>Summary:</b> The Data Privacy Vocabulary (DPV), developed by the W3C Data Privacy
Vocabularies and Controls Community Group (DPVCG), enables the creation of
machine-readable, interoperable, and standards-based representations for
describing the processing of personal data. The group has also published
extensions to the DPV to describe specific applications to support legislative
requirements such as the EU's GDPR. The DPV fills a crucial niche in the state
of the art by providing a vocabulary that can be embedded and used alongside
other existing standards such as W3C ODRL, and which can be customised and
extended for adapting to specifics of use-cases or domains. This article
describes the version 2 iteration of the DPV in terms of its contents,
methodology, current adoptions and uses, and future potential. It also
describes the relevance and role of DPV in acting as a common vocabulary to
support various regulatory (e.g. EU's DGA and AI Act) and community initiatives
(e.g. Solid) emerging across the globe.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.13407v1">A Framework for Managing Multifaceted Privacy Leakage While Optimizing
  Utility in Continuous LBS Interactions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-20T15:20:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anis Bkakria, Reda Yaich</p>
    <p><b>Summary:</b> Privacy in Location-Based Services (LBS) has become a paramount concern with
the ubiquity of mobile devices and the increasing integration of location data
into various applications. In this paper, we present several novel
contributions aimed at advancing the understanding and management of privacy
leakage in LBS. Our contributions provides a more comprehensive framework for
analyzing privacy concerns across different facets of location-based
interactions. Specifically, we introduce $(\epsilon, \delta)$-location privacy,
$(\epsilon, \delta, \theta)$-trajectory privacy, and $(\epsilon, \delta,
\theta)$-POI privacy, which offer refined mechanisms for quantifying privacy
risks associated with location, trajectory, and points of interest when
continuously interacting with LBS. Furthermore, we establish fundamental
connections between these privacy notions, facilitating a holistic approach to
privacy preservation in LBS. Additionally, we present a lower bound analysis to
evaluate the utility of the proposed privacy-preserving mechanisms, offering
insights into the trade-offs between privacy protection and data utility.
Finally, we instantiate our framework with the Plannar Isotopic Mechanism to
demonstrate its practical applicability while ensuring optimal utility and
quantifying privacy leakages across various dimensions. The conducted
evaluations provide a comprehensive insight into the efficacy of our framework
in capturing privacy loss on location, trajectory, and Points of Interest (POI)
while facilitating quantification of the ensured accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.13220v2">Security and Privacy Product Inclusion</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-20T00:36:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dave Kleidermacher, Emmanuel Arriaga, Eric Wang, Sebastian Porst, Roger Piqueras Jover</p>
    <p><b>Summary:</b> In this paper, we explore the challenges of ensuring security and privacy for
users from diverse demographic backgrounds. We propose a threat modeling
approach to identify potential risks and countermeasures for product inclusion
in security and privacy. We discuss various factors that can affect a user's
ability to achieve a high level of security and privacy, including low-income
demographics, poor connectivity, shared device usage, ML fairness, etc. We
present results from a global security and privacy user experience survey and
discuss the implications for product developers. Our work highlights the need
for a more inclusive approach to security and privacy and provides a framework
for researchers and practitioners to consider when designing products and
services for a diverse range of users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.13194v1">Privacy-Preserving Debiasing using Data Augmentation and Machine
  Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-04-19T21:54:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhixin Pan, Emma Andrews, Laura Chang, Prabhat Mishra</p>
    <p><b>Summary:</b> Data augmentation is widely used to mitigate data bias in the training
dataset. However, data augmentation exposes machine learning models to privacy
attacks, such as membership inference attacks. In this paper, we propose an
effective combination of data augmentation and machine unlearning, which can
reduce data bias while providing a provable defense against known attacks.
Specifically, we maintain the fairness of the trained model with
diffusion-based data augmentation, and then utilize multi-shard unlearning to
remove identifying information of original data from the ML model for
protection against privacy attacks. Experimental evaluation across diverse
datasets demonstrates that our approach can achieve significant improvements in
bias reduction as well as robustness against state-of-the-art privacy attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.12837v1">Towards a decentralized data privacy protocol for self-sovereignty in
  the digital world</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-04-19T12:19:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rodrigo Falcão, Arghavan Hosseinzadeh</p>
    <p><b>Summary:</b> A typical user interacts with many digital services nowadays, providing these
services with their data. As of now, the management of privacy preferences is
service-centric: Users must manage their privacy preferences according to the
rules of each service provider, meaning that every provider offers its unique
mechanisms for users to control their privacy settings. However, managing
privacy preferences holistically (i.e., across multiple digital services) is
just impractical. In this vision paper, we propose a paradigm shift towards an
enriched user-centric approach for cross-service privacy preferences
management: the realization of a decentralized data privacy protocol.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.12730v1">PATE-TripleGAN: Privacy-Preserving Image Synthesis with Gaussian
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-19T09:22:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zepeng Jiang, Weiwei Ni, Yifan Zhang</p>
    <p><b>Summary:</b> Conditional Generative Adversarial Networks (CGANs) exhibit significant
potential in supervised learning model training by virtue of their ability to
generate realistic labeled images. However, numerous studies have indicated the
privacy leakage risk in CGANs models. The solution DPCGAN, incorporating the
differential privacy framework, faces challenges such as heavy reliance on
labeled data for model training and potential disruptions to original gradient
information due to excessive gradient clipping, making it difficult to ensure
model accuracy. To address these challenges, we present a privacy-preserving
training framework called PATE-TripleGAN. This framework incorporates a
classifier to pre-classify unlabeled data, establishing a three-party min-max
game to reduce dependence on labeled data. Furthermore, we present a hybrid
gradient desensitization algorithm based on the Private Aggregation of Teacher
Ensembles (PATE) framework and Differential Private Stochastic Gradient Descent
(DPSGD) method. This algorithm allows the model to retain gradient information
more effectively while ensuring privacy protection, thereby enhancing the
model's utility. Privacy analysis and extensive experiments affirm that the
PATE-TripleGAN model can generate a higher quality labeled image dataset while
ensuring the privacy of the training data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.12186v1">Privacy-Preserving UCB Decision Process Verification via zk-SNARKs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-18T13:49:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xikun Jiang, He Lyu, Chenhao Ying, Yibin Xu, Boris Düdder, Yuan Luo</p>
    <p><b>Summary:</b> With the increasingly widespread application of machine learning, how to
strike a balance between protecting the privacy of data and algorithm
parameters and ensuring the verifiability of machine learning has always been a
challenge. This study explores the intersection of reinforcement learning and
data privacy, specifically addressing the Multi-Armed Bandit (MAB) problem with
the Upper Confidence Bound (UCB) algorithm. We introduce zkUCB, an innovative
algorithm that employs the Zero-Knowledge Succinct Non-Interactive Argument of
Knowledge (zk-SNARKs) to enhance UCB. zkUCB is carefully designed to safeguard
the confidentiality of training data and algorithmic parameters, ensuring
transparent UCB decision-making. Experiments highlight zkUCB's superior
performance, attributing its enhanced reward to judicious quantization bit
usage that reduces information entropy in the decision-making process. zkUCB's
proof size and verification time scale linearly with the execution steps of
zkUCB. This showcases zkUCB's adept balance between data security and
operational efficiency. This approach contributes significantly to the ongoing
discourse on reinforcing data privacy in complex decision-making processes,
offering a promising solution for privacy-sensitive applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.11938v1">HyDiscGAN: A Hybrid Distributed cGAN for Audio-Visual Privacy
  Preservation in Multimodal Sentiment Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> 
  <p><b>Published on:</b> 2024-04-18T06:38:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhuojia Wu, Qi Zhang, Duoqian Miao, Kun Yi, Wei Fan, Liang Hu</p>
    <p><b>Summary:</b> Multimodal Sentiment Analysis (MSA) aims to identify speakers' sentiment
tendencies in multimodal video content, raising serious concerns about privacy
risks associated with multimodal data, such as voiceprints and facial images.
Recent distributed collaborative learning has been verified as an effective
paradigm for privacy preservation in multimodal tasks. However, they often
overlook the privacy distinctions among different modalities, struggling to
strike a balance between performance and privacy preservation. Consequently, it
poses an intriguing question of maximizing multimodal utilization to improve
performance while simultaneously protecting necessary modalities. This paper
forms the first attempt at modality-specified (i.e., audio and visual) privacy
preservation in MSA tasks. We propose a novel Hybrid Distributed cross-modality
cGAN framework (HyDiscGAN), which learns multimodality alignment to generate
fake audio and visual features conditioned on shareable de-identified textual
data. The objective is to leverage the fake features to approximate real audio
and visual content to guarantee privacy preservation while effectively
enhancing performance. Extensive experiments show that compared with the
state-of-the-art MSA model, HyDiscGAN can achieve superior or competitive
performance while preserving privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.13087v1">Demystifying Legalese: An Automated Approach for Summarizing and
  Analyzing Overlaps in Privacy Policies and Terms of Service</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-17T19:53:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shikha Soneji, Mitchell Hoesing, Sujay Koujalgi, Jonathan Dodge</p>
    <p><b>Summary:</b> The complexities of legalese in terms and policy documents can bind
individuals to contracts they do not fully comprehend, potentially leading to
uninformed data sharing. Our work seeks to alleviate this issue by developing
language models that provide automated, accessible summaries and scores for
such documents, aiming to enhance user understanding and facilitate informed
decisions. We compared transformer-based and conventional models during
training on our dataset, and RoBERTa performed better overall with a remarkable
0.74 F1-score. Leveraging our best-performing model, RoBERTa, we highlighted
redundancies and potential guideline violations by identifying overlaps in
GDPR-required documents, underscoring the necessity for stricter GDPR
compliance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.16865v1">Improving Privacy-Preserving Techniques for Smart Grid using
  Lattice-based Cryptography</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-17T19:51:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Saleh Darzi, Bahareh Akhbari, Hassan Khodaiemehr</p>
    <p><b>Summary:</b> Advancements in communication and information tech birthed the Smart Grid,
optimizing energy and data transmission. Yet, user privacy is at risk due to
frequent data collection. Existing privacy schemes face vulnerability with
quantum machines. To tackle this, the LPM2DA scheme is introduced, utilizing
lattice-based encryption and signatures for secure data aggregation. It ensures
privacy, integrity, and authentication, enabling statistical analysis while
preserving user privacy. Traditional aggregation schemes suffer from weak
network models and centralization issues. Enter SPDBlock, a blockchain-based
solution ensuring privacy, integrity, and resistance to attacks. It detects and
prosecutes malicious entities while efficiently handling multi-dimensional data
transmission. Through distributed decryption and secret sharing, only valid
data can be decrypted with minimal involvement from smart meters. Performance
tests reveal SPDBlock's superiority in communication and computational
efficiency over traditional schemes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.11515v1">Embedding Privacy in Computational Social Science and Artificial
  Intelligence Research</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-04-17T16:07:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Keenan Jones, Fatima Zahrah, Jason R. C. Nurse</p>
    <p><b>Summary:</b> Privacy is a human right. It ensures that individuals are free to engage in
discussions, participate in groups, and form relationships online or offline
without fear of their data being inappropriately harvested, analyzed, or
otherwise used to harm them. Preserving privacy has emerged as a critical
factor in research, particularly in the computational social science (CSS),
artificial intelligence (AI) and data science domains, given their reliance on
individuals' data for novel insights. The increasing use of advanced
computational models stands to exacerbate privacy concerns because, if
inappropriately used, they can quickly infringe privacy rights and lead to
adverse effects for individuals - especially vulnerable groups - and society.
We have already witnessed a host of privacy issues emerge with the advent of
large language models (LLMs), such as ChatGPT, which further demonstrate the
importance of embedding privacy from the start. This article contributes to the
field by discussing the role of privacy and the primary issues that researchers
working in CSS, AI, data science and related domains are likely to face. It
then presents several key considerations for researchers to ensure participant
privacy is best preserved in their research design, data collection and use,
analysis, and dissemination of research results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.11470v1">A Federated Learning Approach to Privacy Preserving Offensive Language
  Identification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-17T15:23:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Marcos Zampieri, Damith Premasiri, Tharindu Ranasinghe</p>
    <p><b>Summary:</b> The spread of various forms of offensive speech online is an important
concern in social media. While platforms have been investing heavily in ways of
coping with this problem, the question of privacy remains largely unaddressed.
Models trained to detect offensive language on social media are trained and/or
fine-tuned using large amounts of data often stored in centralized servers.
Since most social media data originates from end users, we propose a privacy
preserving decentralized architecture for identifying offensive language online
by introducing Federated Learning (FL) in the context of offensive language
identification. FL is a decentralized architecture that allows multiple models
to be trained locally without the need for data sharing hence preserving users'
privacy. We propose a model fusion approach to perform FL. We trained multiple
deep learning models on four publicly available English benchmark datasets
(AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail. We
also present initial cross-lingual experiments in English and Spanish. We show
that the proposed model fusion approach outperforms baselines in all the
datasets while preserving privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.11450v1">Real-Time Trajectory Synthesis with Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-17T14:55:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yujia Hu, Yuntao Du, Zhikun Zhang, Ziquan Fang, Lu Chen, Kai Zheng, Yunjun Gao</p>
    <p><b>Summary:</b> Trajectory streams are being generated from location-aware devices, such as
smartphones and in-vehicle navigation systems. Due to the sensitive nature of
the location data, directly sharing user trajectories suffers from privacy
leakage issues. Local differential privacy (LDP), which perturbs sensitive data
on the user side before it is shared or analyzed, emerges as a promising
solution for private trajectory stream collection and analysis. Unfortunately,
existing stream release approaches often neglect the rich spatial-temporal
context information within trajectory streams, resulting in suboptimal utility
and limited types of downstream applications. To this end, we propose RetraSyn,
a novel real-time trajectory synthesis framework, which is able to perform
on-the-fly trajectory synthesis based on the mobility patterns privately
extracted from users' trajectory streams. Thus, the downstream trajectory
analysis can be performed on the high-utility synthesized data with privacy
protection. We also take the genuine behaviors of real-world mobile travelers
into consideration, ensuring authenticity and practicality. The key components
of RetraSyn include the global mobility model, dynamic mobility update
mechanism, real-time synthesis, and adaptive allocation strategy. We conduct
extensive experiments on multiple real-world and synthetic trajectory datasets
under various location-based utility metrics, encompassing both streaming and
historical scenarios. The empirical results demonstrate the superiority and
versatility of our proposed framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.11388v1">Enhancing Data Privacy In Wireless Sensor Networks: Investigating
  Techniques And Protocols To Protect Privacy Of Data Transmitted Over Wireless
  Sensor Networks In Critical Applications Of Healthcare And National Security</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-04-17T13:48:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Akinsola Ahmed, Ejiofor Oluomachi, Akinde Abdullah, Njoku Tochukwu</p>
    <p><b>Summary:</b> The article discusses the emergence of Wireless Sensor Networks (WSNs) as a
groundbreaking technology in data processing and communication. It outlines how
WSNs, composed of dispersed autonomous sensors, are utilized to monitor
physical and environmental factors, transmitting data wirelessly for analysis.
The article explores various applications of WSNs in healthcare, national
security, emergency response, and infrastructure monitoring, highlighting their
roles in enhancing patient care, public health surveillance, border security,
disaster management, and military operations. Additionally, it examines the
foundational concepts of data privacy in WSNs, focusing on encryption
techniques, authentication mechanisms, anonymization techniques, and access
control mechanisms. The article also addresses vulnerabilities, threats, and
challenges related to data privacy in healthcare and national security
contexts, emphasizing regulatory compliance, ethical considerations, and
socio-economic factors. Furthermore, it introduces the Diffusion of Innovation
Theory as a framework for understanding the adoption of privacy-enhancing
technologies in WSNs. Finally, the article reviews empirical studies
demonstrating the efficacy of security solutions in preserving data privacy in
WSNs, offering insights into advancements in safeguarding sensitive
information.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.10995v1">Clipped SGD Algorithms for Privacy Preserving Performative Prediction:
  Bias Amplification and Remedies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-17T02:17:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qiang Li, Michal Yemini, Hoi-To Wai</p>
    <p><b>Summary:</b> Clipped stochastic gradient descent (SGD) algorithms are among the most
popular algorithms for privacy preserving optimization that reduces the leakage
of users' identity in model training. This paper studies the convergence
properties of these algorithms in a performative prediction setting, where the
data distribution may shift due to the deployed prediction model. For example,
the latter is caused by strategical users during the training of loan policy
for banks. Our contributions are two-fold. First, we show that the
straightforward implementation of a projected clipped SGD (PCSGD) algorithm may
converge to a biased solution compared to the performative stable solution. We
quantify the lower and upper bound for the magnitude of the bias and
demonstrate a bias amplification phenomenon where the bias grows with the
sensitivity of the data distribution. Second, we suggest two remedies to the
bias amplification effect. The first one utilizes an optimal step size design
for PCSGD that takes the privacy guarantee into account. The second one uses
the recently proposed DiceSGD algorithm [Zhang et al., 2024]. We show that the
latter can successfully remove the bias and converge to the performative stable
solution. Numerical experiments verify our analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.10767v1">Privacy Can Arise Endogenously in an Economic System with Learning
  Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Science and Game Theory-5BC0EB">
  <p><b>Published on:</b> 2024-04-16T17:51:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nivasini Ananthakrishnan, Tiffany Ding, Mariel Werner, Sai Praneeth Karimireddy, Michael I. Jordan</p>
    <p><b>Summary:</b> We study price-discrimination games between buyers and a seller where privacy
arises endogenously--that is, utility maximization yields equilibrium
strategies where privacy occurs naturally. In this game, buyers with a high
valuation for a good have an incentive to keep their valuation private, lest
the seller charge them a higher price. This yields an equilibrium where some
buyers will send a signal that misrepresents their type with some probability;
we refer to this as buyer-induced privacy. When the seller is able to publicly
commit to providing a certain privacy level, we find that their equilibrium
response is to commit to ignore buyers' signals with some positive probability;
we refer to this as seller-induced privacy. We then turn our attention to a
repeated interaction setting where the game parameters are unknown and the
seller cannot credibly commit to a level of seller-induced privacy. In this
setting, players must learn strategies based on information revealed in past
rounds. We find that, even without commitment ability, seller-induced privacy
arises as a result of reputation building. We characterize the resulting
seller-induced privacy and seller's utility under no-regret and
no-policy-regret learning algorithms and verify these results through
simulations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.10258v1">CO-oPS: A Mobile App for Community Oversight of Privacy and Security</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-04-16T03:25:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mamtaj Akter, Leena Alghamdi, Dylan Gillespie, Nazmus Miazi, Jess Kropczynski, Heather Lipford, Pamela Wisniewski</p>
    <p><b>Summary:</b> Smartphone users install numerous mobile apps that require access to
different information from their devices. Much of this information is very
sensitive, and users often struggle to manage these accesses due to their lack
of tech expertise and knowledge regarding mobile privacy. Thus, they often seek
help from others to make decisions regarding their mobile privacy and security.
We embedded these social processes in a mobile app titled "CO-oPS'' ("Community
Oversight for Privacy and Security"). CO-oPS allows trusted community members
to review one another's apps installed and permissions granted to those apps.
Community members can provide feedback to one another regarding their privacy
behaviors. Users are also allowed to hide some of their mobile apps that they
do not like others to see, ensuring their personal privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.10255v2">Privacy-Enhanced Training-as-a-Service for On-Device Intelligence:
  Concept, Architectural Scheme, and Open Problems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-04-16T03:18:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhiyuan Wu, Sheng Sun, Yuwei Wang, Min Liu, Bo Gao, Tianliu He, Wen Wang</p>
    <p><b>Summary:</b> On-device intelligence (ODI) enables artificial intelligence (AI)
applications to run on end devices, providing real-time and customized AI
inference without relying on remote servers. However, training models for
on-device deployment face significant challenges due to the decentralized and
privacy-sensitive nature of users' data, along with end-side constraints
related to network connectivity, computation efficiency, etc. Existing training
paradigms, such as cloud-based training, federated learning, and transfer
learning, fail to sufficiently address these practical constraints that are
prevalent for devices. To overcome these challenges, we propose
Privacy-Enhanced Training-as-a-Service (PTaaS), a novel service computing
paradigm that provides privacy-friendly, customized AI model training for end
devices. PTaaS outsources the core training process to remote and powerful
cloud or edge servers, efficiently developing customized on-device models based
on uploaded anonymous queries, enhancing data privacy while reducing the
computation load on individual devices. We explore the definition, goals, and
design principles of PTaaS, alongside emerging technologies that support the
PTaaS paradigm. An architectural scheme for PTaaS is also presented, followed
by a series of open problems that set the stage for future research directions
in the field of PTaaS.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.09816v1">FedP3: Federated Personalized and Privacy-friendly Network Pruning under
  Model Heterogeneity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-15T14:14:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kai Yi, Nidham Gazagnadou, Peter Richtárik, Lingjuan Lyu</p>
    <p><b>Summary:</b> The interest in federated learning has surged in recent research due to its
unique ability to train a global model using privacy-secured information held
locally on each client. This paper pays particular attention to the issue of
client-side model heterogeneity, a pervasive challenge in the practical
implementation of FL that escalates its complexity. Assuming a scenario where
each client possesses varied memory storage, processing capabilities and
network bandwidth - a phenomenon referred to as system heterogeneity - there is
a pressing need to customize a unique model for each client. In response to
this, we present an effective and adaptable federated framework FedP3,
representing Federated Personalized and Privacy-friendly network Pruning,
tailored for model heterogeneity scenarios. Our proposed methodology can
incorporate and adapt well-established techniques to its specific instances. We
offer a theoretical interpretation of FedP3 and its locally
differential-private variant, DP-FedP3, and theoretically validate their
efficiencies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.10029v1">Federated Learning on Riemannian Manifolds with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-04-15T12:32:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhenwei Huang, Wen Huang, Pratik Jawanpuria, Bamdev Mishra</p>
    <p><b>Summary:</b> In recent years, federated learning (FL) has emerged as a prominent paradigm
in distributed machine learning. Despite the partial safeguarding of agents'
information within FL systems, a malicious adversary can potentially infer
sensitive information through various means. In this paper, we propose a
generic private FL framework defined on Riemannian manifolds (PriRFed) based on
the differential privacy (DP) technique. We analyze the privacy guarantee while
establishing the convergence properties. To the best of our knowledge, this is
the first federated learning framework on Riemannian manifold with a privacy
guarantee and convergence results. Numerical simulations are performed on
synthetic and real-world datasets to showcase the efficacy of the proposed
PriRFed approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.09724v1">Privacy-Preserving Federated Unlearning with Certified Client Removal</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-15T12:27:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ziyao Liu, Huanyi Ye, Yu Jiang, Jiyuan Shen, Jiale Guo, Ivan Tjuawinata, Kwok-Yan Lam</p>
    <p><b>Summary:</b> In recent years, Federated Unlearning (FU) has gained attention for
addressing the removal of a client's influence from the global model in
Federated Learning (FL) systems, thereby ensuring the ``right to be forgotten"
(RTBF). State-of-the-art methods for unlearning use historical data from FL
clients, such as gradients or locally trained models. However, studies have
revealed significant information leakage in this setting, with the possibility
of reconstructing a user's local data from their uploaded information.
Addressing this, we propose Starfish, a privacy-preserving federated unlearning
scheme using Two-Party Computation (2PC) techniques and shared historical
client data between two non-colluding servers. Starfish builds upon existing FU
methods to ensure privacy in unlearning processes. To enhance the efficiency of
privacy-preserving FU evaluations, we suggest 2PC-friendly alternatives for
certain FU algorithm operations. We also implement strategies to reduce costs
associated with 2PC operations and lessen cumulative approximation errors.
Moreover, we establish a theoretical bound for the difference between the
unlearned global model via Starfish and a global model retrained from scratch
for certified client removal. Our theoretical and experimental analyses
demonstrate that Starfish achieves effective unlearning with reasonable
efficiency, maintaining privacy and security in FL systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.09625v1">Privacy-Preserving Intrusion Detection using Convolutional Neural
  Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-15T09:56:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Martin Kodys, Zhongmin Dai, Vrizlynn L. L. Thing</p>
    <p><b>Summary:</b> Privacy-preserving analytics is designed to protect valuable assets. A common
service provision involves the input data from the client and the model on the
analyst's side. The importance of the privacy preservation is fuelled by legal
obligations and intellectual property concerns. We explore the use case of a
model owner providing an analytic service on customer's private data. No
information about the data shall be revealed to the analyst and no information
about the model shall be leaked to the customer. Current methods involve costs:
accuracy deterioration and computational complexity. The complexity, in turn,
results in a longer processing time, increased requirement on computing
resources, and involves data communication between the client and the server.
In order to deploy such service architecture, we need to evaluate the optimal
setting that fits the constraints. And that is what this paper addresses. In
this work, we enhance an attack detection system based on Convolutional Neural
Networks with privacy-preserving technology based on PriMIA framework that is
initially designed for medical data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.10026v1">Distributed Federated Learning-Based Deep Learning Model for Privacy MRI
  Brain Tumor Detection</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-15T09:07:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lisang Zhou, Meng Wang, Ning Zhou</p>
    <p><b>Summary:</b> Distributed training can facilitate the processing of large medical image
datasets, and improve the accuracy and efficiency of disease diagnosis while
protecting patient privacy, which is crucial for achieving efficient medical
image analysis and accelerating medical research progress. This paper presents
an innovative approach to medical image classification, leveraging Federated
Learning (FL) to address the dual challenges of data privacy and efficient
disease diagnosis. Traditional Centralized Machine Learning models, despite
their widespread use in medical imaging for tasks such as disease diagnosis,
raise significant privacy concerns due to the sensitive nature of patient data.
As an alternative, FL emerges as a promising solution by allowing the training
of a collective global model across local clients without centralizing the
data, thus preserving privacy. Focusing on the application of FL in Magnetic
Resonance Imaging (MRI) brain tumor detection, this study demonstrates the
effectiveness of the Federated Learning framework coupled with EfficientNet-B0
and the FedAvg algorithm in enhancing both privacy and diagnostic accuracy.
Through a meticulous selection of preprocessing methods, algorithms, and
hyperparameters, and a comparative analysis of various Convolutional Neural
Network (CNN) architectures, the research uncovers optimal strategies for image
classification. The experimental results reveal that EfficientNet-B0
outperforms other models like ResNet in handling data heterogeneity and
achieving higher accuracy and lower loss, highlighting the potential of FL in
overcoming the limitations of traditional models. The study underscores the
significance of addressing data heterogeneity and proposes further research
directions for broadening the applicability of FL in medical image analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.09536v1">Beyond Noise: Privacy-Preserving Decentralized Learning with Virtual
  Nodes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-15T07:59:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sayan Biswas, Mathieu Even, Anne-Marie Kermarrec, Laurent Massoulie, Rafael Pires, Rishi Sharma, Martijn de Vos</p>
    <p><b>Summary:</b> Decentralized learning (DL) enables collaborative learning without a server
and without training data leaving the users' devices. However, the models
shared in DL can still be used to infer training data. Conventional privacy
defenses such as differential privacy and secure aggregation fall short in
effectively safeguarding user privacy in DL. We introduce Shatter, a novel DL
approach in which nodes create virtual nodes (VNs) to disseminate chunks of
their full model on their behalf. This enhances privacy by (i) preventing
attackers from collecting full models from other nodes, and (ii) hiding the
identity of the original node that produced a given model chunk. We
theoretically prove the convergence of Shatter and provide a formal analysis
demonstrating how Shatter reduces the efficacy of attacks compared to when
exchanging full models between participating nodes. We evaluate the convergence
and attack resilience of Shatter with existing DL algorithms, with
heterogeneous datasets, and against three standard privacy attacks, including
gradient inversion. Our evaluation shows that Shatter not only renders these
privacy attacks infeasible when each node operates 16 VNs but also exhibits a
positive impact on model convergence compared to standard DL. This enhanced
privacy comes with a manageable increase in communication volume.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.09481v1">SpamDam: Towards Privacy-Preserving and Adversary-Resistant SMS Spam
  Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-15T06:07:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yekai Li, Rufan Zhang, Wenxin Rong, Xianghang Mi</p>
    <p><b>Summary:</b> In this study, we introduce SpamDam, a SMS spam detection framework designed
to overcome key challenges in detecting and understanding SMS spam, such as the
lack of public SMS spam datasets, increasing privacy concerns of collecting SMS
data, and the need for adversary-resistant detection models. SpamDam comprises
four innovative modules: an SMS spam radar that identifies spam messages from
online social networks(OSNs); an SMS spam inspector for statistical analysis;
SMS spam detectors(SSDs) that enable both central training and federated
learning; and an SSD analyzer that evaluates model resistance against
adversaries in realistic scenarios. Leveraging SpamDam, we have compiled over
76K SMS spam messages from Twitter and Weibo between 2018 and 2023, forming the
largest dataset of its kind. This dataset has enabled new insights into recent
spam campaigns and the training of high-performing binary and multi-label
classifiers for spam detection. Furthermore, effectiveness of federated
learning has been well demonstrated to enable privacy-preserving SMS spam
detection. Additionally, we have rigorously tested the adversarial robustness
of SMS spam detection models, introducing the novel reverse backdoor attack,
which has shown effectiveness and stealthiness in practical tests.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.09430v1">On the Efficiency of Privacy Attacks in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-15T03:04:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nawrin Tabassum, Ka-Ho Chow, Xuyu Wang, Wenbin Zhang, Yanzhao Wu</p>
    <p><b>Summary:</b> Recent studies have revealed severe privacy risks in federated learning,
represented by Gradient Leakage Attacks. However, existing studies mainly aim
at increasing the privacy attack success rate and overlook the high computation
costs for recovering private data, making the privacy attack impractical in
real applications. In this study, we examine privacy attacks from the
perspective of efficiency and propose a framework for improving the Efficiency
of Privacy Attacks in Federated Learning (EPAFL). We make three novel
contributions. First, we systematically evaluate the computational costs for
representative privacy attacks in federated learning, which exhibits a high
potential to optimize efficiency. Second, we propose three early-stopping
techniques to effectively reduce the computational costs of these privacy
attacks. Third, we perform experiments on benchmark datasets and show that our
proposed method can significantly reduce computational costs and maintain
comparable attack success rates for state-of-the-art privacy attacks in
federated learning. We provide the codes on GitHub at
https://github.com/mlsysx/EPAFL.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.09391v1">Privacy at a Price: Exploring its Dual Impact on AI Fairness</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-04-15T00:23:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mengmeng Yang, Ming Ding, Youyang Qu, Wei Ni, David Smith, Thierry Rakotoarivelo</p>
    <p><b>Summary:</b> The worldwide adoption of machine learning (ML) and deep learning models,
particularly in critical sectors, such as healthcare and finance, presents
substantial challenges in maintaining individual privacy and fairness. These
two elements are vital to a trustworthy environment for learning systems. While
numerous studies have concentrated on protecting individual privacy through
differential privacy (DP) mechanisms, emerging research indicates that
differential privacy in machine learning models can unequally impact separate
demographic subgroups regarding prediction accuracy. This leads to a fairness
concern, and manifests as biased performance. Although the prevailing view is
that enhancing privacy intensifies fairness disparities, a smaller, yet
significant, subset of research suggests the opposite view. In this article,
with extensive evaluation results, we demonstrate that the impact of
differential privacy on fairness is not monotonous. Instead, we observe that
the accuracy disparity initially grows as more DP noise (enhanced privacy) is
added to the ML process, but subsequently diminishes at higher privacy levels
with even more noise. Moreover, implementing gradient clipping in the
differentially private stochastic gradient descent ML method can mitigate the
negative impact of DP noise on fairness. This mitigation is achieved by
moderating the disparity growth through a lower clipping threshold.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.08261v1">QI-DPFL: Quality-Aware and Incentive-Boosted Federated Learning with
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Science and Game Theory-5BC0EB">
  <p><b>Published on:</b> 2024-04-12T06:18:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenhao Yuan, Xuehe Wang</p>
    <p><b>Summary:</b> Federated Learning (FL) has increasingly been recognized as an innovative and
secure distributed model training paradigm, aiming to coordinate multiple edge
clients to collaboratively train a shared model without uploading their private
datasets. The challenge of encouraging mobile edge devices to participate
zealously in FL model training procedures, while mitigating the privacy leakage
risks during wireless transmission, remains comparatively unexplored so far. In
this paper, we propose a novel approach, named QI-DPFL (Quality-Aware and
Incentive-Boosted Federated Learning with Differential Privacy), to address the
aforementioned intractable issue. To select clients with high-quality datasets,
we first propose a quality-aware client selection mechanism based on the Earth
Mover's Distance (EMD) metric. Furthermore, to attract high-quality data
contributors, we design an incentive-boosted mechanism that constructs the
interactions between the central server and the selected clients as a two-stage
Stackelberg game, where the central server designs the time-dependent reward to
minimize its cost by considering the trade-off between accuracy loss and total
reward allocated, and each selected client decides the privacy budget to
maximize its utility. The Nash Equilibrium of the Stackelberg game is derived
to find the optimal solution in each global iteration. The extensive
experimental results on different real-world datasets demonstrate the
effectiveness of our proposed FL framework, by realizing the goal of privacy
protection and incentive compatibility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.07437v1">Privacy preserving layer partitioning for Deep Neural Network models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-11T02:39:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kishore Rajasekar, Randolph Loh, Kar Wai Fok, Vrizlynn L. L. Thing</p>
    <p><b>Summary:</b> MLaaS (Machine Learning as a Service) has become popular in the cloud
computing domain, allowing users to leverage cloud resources for running
private inference of ML models on their data. However, ensuring user input
privacy and secure inference execution is essential. One of the approaches to
protect data privacy and integrity is to use Trusted Execution Environments
(TEEs) by enabling execution of programs in secure hardware enclave. Using TEEs
can introduce significant performance overhead due to the additional layers of
encryption, decryption, security and integrity checks. This can lead to slower
inference times compared to running on unprotected hardware. In our work, we
enhance the runtime performance of ML models by introducing layer partitioning
technique and offloading computations to GPU. The technique comprises two
distinct partitions: one executed within the TEE, and the other carried out
using a GPU accelerator. Layer partitioning exposes intermediate feature maps
in the clear which can lead to reconstruction attacks to recover the input. We
conduct experiments to demonstrate the effectiveness of our approach in
protecting against input reconstruction attacks developed using trained
conditional Generative Adversarial Network(c-GAN). The evaluation is performed
on widely used models such as VGG-16, ResNet-50, and EfficientNetB0, using two
datasets: ImageNet for Image classification and TON IoT dataset for
cybersecurity attack detection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.07345v1">Indoor Location Fingerprinting Privacy: A Comprehensive Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-04-10T21:02:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amir Fathalizadeh, Vahideh Moghtadaiee, Mina Alishahi</p>
    <p><b>Summary:</b> The pervasive integration of Indoor Positioning Systems (IPS) arises from the
limitations of Global Navigation Satellite Systems (GNSS) in indoor
environments, leading to the widespread adoption of Location-Based Services
(LBS). Specifically, indoor location fingerprinting employs diverse signal
fingerprints from user devices, enabling precise location identification by
Location Service Providers (LSP). Despite its broad applications across various
domains, indoor location fingerprinting introduces a notable privacy risk, as
both LSP and potential adversaries inherently have access to this sensitive
information, compromising users' privacy. Consequently, concerns regarding
privacy vulnerabilities in this context necessitate a focused exploration of
privacy-preserving mechanisms. In response to these concerns, this survey
presents a comprehensive review of Privacy-Preserving Mechanisms in Indoor
Location Fingerprinting (ILFPPM) based on cryptographic, anonymization,
differential privacy (DP), and federated learning (FL) techniques. We also
propose a distinctive and novel grouping of privacy vulnerabilities, adversary
and attack models, and available evaluation metrics specific to indoor location
fingerprinting systems. Given the identified limitations and research gaps in
this survey, we highlight numerous prospective opportunities for future
investigation, aiming to motivate researchers interested in advancing this
field. This survey serves as a valuable reference for researchers and provides
a clear overview for those beyond this specific research domain.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.06868v2">The Sandwich meta-framework for architecture agnostic deep
  privacy-preserving transfer learning for non-invasive brainwave decoding</a></h3>
  
  <p><b>Published on:</b> 2024-04-10T09:47:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaoxi Wei, Jyotindra Narayan, A. Aldo Faisal</p>
    <p><b>Summary:</b> Machine learning has enhanced the performance of decoding signals indicating
human behaviour. EEG decoding, as an exemplar indicating neural activity and
human thoughts non-invasively, has been helpful in neural activity analysis and
aiding patients via brain-computer interfaces. However, training machine
learning algorithms on EEG encounters two primary challenges: variability
across data sets and privacy concerns using data from individuals and data
centres. Our objective is to address these challenges by integrating transfer
learning for data variability and federated learning for data privacy into a
unified approach. We introduce the Sandwich as a novel deep privacy-preserving
meta-framework combining transfer learning and federated learning. The Sandwich
framework comprises three components: federated networks (first layers) that
handle data set differences at the input level, a shared network (middle layer)
learning common rules and applying transfer learning, and individual
classifiers (final layers) for specific tasks of each data set. It enables the
central network (central server) to benefit from multiple data sets, while
local branches (local servers) maintain data and label privacy. We evaluated
the `Sandwich' meta-architecture in various configurations using the BEETL
motor imagery challenge, a benchmark for heterogeneous EEG data sets. Compared
with baseline models, our `Sandwich' implementations showed superior
performance. The best-performing model, the Inception Sandwich with deep set
alignment (Inception-SD-Deepset), exceeded baseline methods by 9%. The
`Sandwich' framework demonstrates significant advancements in federated deep
transfer learning for diverse tasks and data sets. It outperforms conventional
deep learning methods, showcasing the potential for effective use of larger,
heterogeneous data sets with enhanced privacy as a model-agnostic
meta-framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.06721v2">Poisoning Prevention in Federated Learning and Differential Privacy via
  Stateful Proofs of Execution</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-10T04:18:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Norrathep Rattanavipanon, Ivan De Oliveira Nunes</p>
    <p><b>Summary:</b> The rise in IoT-driven distributed data analytics, coupled with increasing
privacy concerns, has led to a demand for effective privacy-preserving and
federated data collection/model training mechanisms. In response, approaches
such as Federated Learning (FL) and Local Differential Privacy (LDP) have been
proposed and attracted much attention over the past few years. However, they
still share the common limitation of being vulnerable to poisoning attacks
wherein adversaries compromising edge devices feed forged (a.k.a. poisoned)
data to aggregation back-ends, undermining the integrity of FL/LDP results.
  In this work, we propose a system-level approach to remedy this issue based
on a novel security notion of Proofs of Stateful Execution (PoSX) for
IoT/embedded devices' software. To realize the PoSX concept, we design SLAPP: a
System-Level Approach for Poisoning Prevention. SLAPP leverages commodity
security features of embedded devices - in particular ARM TrustZoneM security
extensions - to verifiably bind raw sensed data to their correct usage as part
of FL/LDP edge device routines. As a consequence, it offers robust security
guarantees against poisoning. Our evaluation, based on real-world prototypes
featuring multiple cryptographic primitives and data collection schemes,
showcases SLAPP's security and low overhead.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.06686v1">Atlas-X Equity Financing: Unlocking New Methods to Securely Obfuscate
  Axe Inventory Data Based on Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-10T02:19:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antigoni Polychroniadou, Gabriele Cipriani, Richard Hua, Tucker Balch</p>
    <p><b>Summary:</b> Banks publish daily a list of available securities/assets (axe list) to
selected clients to help them effectively locate Long (buy) or Short (sell)
trades at reduced financing rates. This reduces costs for the bank, as the list
aggregates the bank's internal firm inventory per asset for all clients of long
as well as short trades. However, this is somewhat problematic: (1) the bank's
inventory is revealed; (2) trades of clients who contribute to the aggregated
list, particularly those deemed large, are revealed to other clients. Clients
conducting sizable trades with the bank and possessing a portion of the
aggregated asset exceeding $50\%$ are considered to be concentrated clients.
This could potentially reveal a trading concentrated client's activity to their
competitors, thus providing an unfair advantage over the market.
  Atlas-X Axe Obfuscation, powered by new differential private methods, enables
a bank to obfuscate its published axe list on a daily basis while under
continual observation, thus maintaining an acceptable inventory Profit and Loss
(P&L) cost pertaining to the noisy obfuscated axe list while reducing the
clients' trading activity leakage. Our main differential private innovation is
a differential private aggregator for streams (time series data) of both
positive and negative integers under continual observation.
  For the last two years, Atlas-X system has been live in production across
three major regions-USA, Europe, and Asia-at J.P. Morgan, a major financial
institution, facilitating significant profitability. To our knowledge, it is
the first differential privacy solution to be deployed in the financial sector.
We also report benchmarks of our algorithm based on (anonymous) real and
synthetic data to showcase the quality of our obfuscation and its success in
production.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.06216v1">Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-04-09T11:07:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Suleyman Ozdel, Efe Bozkir, Enkelejda Kasneci</p>
    <p><b>Summary:</b> As eye tracking becomes pervasive with screen-based devices and head-mounted
displays, privacy concerns regarding eye-tracking data have escalated. While
state-of-the-art approaches for privacy-preserving eye tracking mostly involve
differential privacy and empirical data manipulations, previous research has
not focused on methods for scanpaths. We introduce a novel privacy-preserving
scanpath comparison protocol designed for the widely used Needleman-Wunsch
algorithm, a generalized version of the edit distance algorithm. Particularly,
by incorporating the Paillier homomorphic encryption scheme, our protocol
ensures that no private information is revealed. Furthermore, we introduce a
random processing strategy and a multi-layered masking method to obfuscate the
values while preserving the original order of encrypted editing operation
costs. This minimizes communication overhead, requiring a single communication
round for each iteration of the Needleman-Wunsch process. We demonstrate the
efficiency and applicability of our protocol on three publicly available
datasets with comprehensive computational performance analyses and make our
source code publicly accessible.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.06144v1">Differential Privacy for Anomaly Detection: Analyzing the Trade-off
  Between Privacy and Explainability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-04-09T09:09:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fatima Ezzeddine, Mirna Saad, Omran Ayoub, Davide Andreoletti, Martin Gjoreski, Ihab Sbeity, Marc Langheinrich, Silvia Giordano</p>
    <p><b>Summary:</b> Anomaly detection (AD), also referred to as outlier detection, is a
statistical process aimed at identifying observations within a dataset that
significantly deviate from the expected pattern of the majority of the data.
Such a process finds wide application in various fields, such as finance and
healthcare. While the primary objective of AD is to yield high detection
accuracy, the requirements of explainability and privacy are also paramount.
The first ensures the transparency of the AD process, while the second
guarantees that no sensitive information is leaked to untrusted parties. In
this work, we exploit the trade-off of applying Explainable AI (XAI) through
SHapley Additive exPlanations (SHAP) and differential privacy (DP). We perform
AD with different models and on various datasets, and we thoroughly evaluate
the cost of privacy in terms of decreased accuracy and explainability. Our
results show that the enforcement of privacy through DP has a significant
impact on detection accuracy and explainability, which depends on both the
dataset and the considered AD model. We further show that the visual
interpretation of explanations is also influenced by the choice of the AD
algorithm.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.08686v1">Extractive text summarisation of Privacy Policy documents using machine
  learning approaches</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-04-09T04:54:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chanwoo Choi</p>
    <p><b>Summary:</b> This work demonstrates two Privacy Policy (PP) summarisation models based on
two different clustering algorithms: K-means clustering and Pre-determined
Centroid (PDC) clustering. K-means is decided to be used for the first model
after an extensive evaluation of ten commonly used clustering algorithms. The
summariser model based on the PDC-clustering algorithm summarises PP documents
by segregating individual sentences by Euclidean distance from each sentence to
the pre-defined cluster centres. The cluster centres are defined according to
General Data Protection Regulation (GDPR)'s 14 essential topics that must be
included in any privacy notices. The PDC model outperformed the K-means model
for two evaluation methods, Sum of Squared Distance (SSD) and ROUGE by some
margin (27% and 24% respectively). This result contrasts the K-means model's
better performance in the general clustering of sentence vectors before running
the task-specific evaluation. This indicates the effectiveness of operating
task-specific fine-tuning measures on unsupervised machine-learning models. The
summarisation mechanisms implemented in this paper demonstrates an idea of how
to efficiently extract essential sentences that should be included in any PP
documents. The summariser models could be further developed to an application
that tests the GDPR-compliance (or any data privacy legislation) of PP
documents.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.06001v2">Privacy Preserving Prompt Engineering: A Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-04-09T04:11:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kennedy Edemacu, Xintao Wu</p>
    <p><b>Summary:</b> Pre-trained language models (PLMs) have demonstrated significant proficiency
in solving a wide range of general natural language processing (NLP) tasks.
Researchers have observed a direct correlation between the performance of these
models and their sizes. As a result, the sizes of these models have notably
expanded in recent years, persuading researchers to adopt the term large
language models (LLMs) to characterize the larger-sized PLMs. The size
expansion comes with a distinct capability called in-context learning (ICL),
which represents a special form of prompting and allows the models to be
utilized through the presentation of demonstration examples without
modifications to the model parameters. Although interesting, privacy concerns
have become a major obstacle in its widespread usage. Multiple studies have
examined the privacy risks linked to ICL and prompting in general, and have
devised techniques to alleviate these risks. Thus, there is a necessity to
organize these mitigation techniques for the benefit of the community. This
survey provides a systematic overview of the privacy protection methods
employed during ICL and prompting in general. We review, analyze, and compare
different methods under this paradigm. Furthermore, we provide a summary of the
resources accessible for the development of these frameworks. Finally, we
discuss the limitations of these frameworks and offer a detailed examination of
the promising areas that necessitate further exploration.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.05876v1">Privacy and Security of Women's Reproductive Health Apps in a Changing
  Legal Landscape</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2024-04-08T21:19:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shalini Saini, Nitesh Saxena</p>
    <p><b>Summary:</b> FemTech, a rising trend in mobile apps, empowers women to digitally manage
their health and family planning. However, privacy and security vulnerabilities
in period-tracking and fertility-monitoring apps present significant risks,
such as unintended pregnancies and legal consequences. Our approach involves
manual observations of privacy policies and app permissions, along with dynamic
and static analysis using multiple evaluation frameworks. Our research reveals
that many of these apps gather personally identifiable information (PII) and
sensitive healthcare data. Furthermore, our analysis identifies that 61% of the
code vulnerabilities found in the apps are classified under the top-ten Open
Web Application Security Project (OWASP) vulnerabilities. Our research
emphasizes the significance of tackling the privacy and security
vulnerabilities present in period-tracking and fertility-monitoring mobile
apps. By highlighting these crucial risks, we aim to initiate a vital
discussion and advocate for increased accountability and transparency of
digital tools for women's health. We encourage the industry to prioritize user
privacy and security, ultimately promoting a safer and more secure environment
for women's health management.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.05828v1">Privacy-Preserving Deep Learning Using Deformable Operators for Secure
  Task Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-04-08T19:46:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fabian Perez, Jhon Lopez, Henry Arguello</p>
    <p><b>Summary:</b> In the era of cloud computing and data-driven applications, it is crucial to
protect sensitive information to maintain data privacy, ensuring truly reliable
systems. As a result, preserving privacy in deep learning systems has become a
critical concern. Existing methods for privacy preservation rely on image
encryption or perceptual transformation approaches. However, they often suffer
from reduced task performance and high computational costs. To address these
challenges, we propose a novel Privacy-Preserving framework that uses a set of
deformable operators for secure task learning. Our method involves shuffling
pixels during the analog-to-digital conversion process to generate visually
protected data. Those are then fed into a well-known network enhanced with
deformable operators. Using our approach, users can achieve equivalent
performance to original images without additional training using a secret key.
Moreover, our method enables access control against unauthorized users.
Experimental results demonstrate the efficacy of our approach, showcasing its
potential in cloud-based scenarios and privacy-sensitive applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.05598v1">Hook-in Privacy Techniques for gRPC-based Microservice Communication</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-04-08T15:18:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Louis Loechel, Siar-Remzi Akbayin, Elias Grünewald, Jannis Kiesel, Inga Strelnikova, Thomas Janke, Frank Pallas</p>
    <p><b>Summary:</b> gRPC is at the heart of modern distributed system architectures. Based on
HTTP/2 and Protocol Buffers, it provides highly performant, standardized, and
polyglot communication across loosely coupled microservices and is increasingly
preferred over REST- or GraphQL-based service APIs in practice. Despite its
widespread adoption, gRPC lacks any advanced privacy techniques beyond
transport encryption and basic token-based authentication. Such advanced
techniques are, however, increasingly important for fulfilling regulatory
requirements. For instance, anonymizing or otherwise minimizing (personal) data
before responding to requests, or pre-processing data based on the purpose of
the access may be crucial in certain usecases. In this paper, we therefore
propose a novel approach for integrating such advanced privacy techniques into
the gRPC framework in a practically viable way. Specifically, we present a
general approach along with a working prototype that implements privacy
techniques, such as data minimization and purpose limitation, in a
configurable, extensible, and gRPC-native way utilizing a gRPC interceptor. We
also showcase how to integrate this contribution into a realistic example of a
food delivery use case. Alongside these implementations, a preliminary
performance evaluation shows practical applicability with reasonable overheads.
Altogether, we present a viable solution for integrating advanced privacy
techniques into real-world gRPC-based microservice architectures, thereby
facilitating regulatory compliance ``by design''.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.05257v1">Sensing-Resistance-Oriented Beamforming for Privacy Protection from ISAC
  Devices</a></h3>
  
  <p><b>Published on:</b> 2024-04-08T07:45:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Teng Ma, Yue Xiao, Xia Lei, Ming Xiao</p>
    <p><b>Summary:</b> With the evolution of integrated sensing and communication (ISAC) technology,
a growing number of devices go beyond conventional communication functions with
sensing abilities. Therefore, future networks are divinable to encounter new
privacy concerns on sensing, such as the exposure of position information to
unintended receivers. In contrast to traditional privacy preserving schemes
aiming to prevent eavesdropping, this contribution conceives a novel
beamforming design toward sensing resistance (SR). Specifically, we expect to
guarantee the communication quality while masking the real direction of the SR
transmitter during the communication. To evaluate the SR performance, a metric
termed angular-domain peak-to-average ratio (ADPAR) is first defined and
analyzed. Then, we resort to the null-space technique to conceal the real
direction, hence to convert the optimization problem to a more tractable form.
Moreover, semidefinite relaxation along with index optimization is further
utilized to obtain the optimal beamformer. Finally, simulation results
demonstrate the feasibility of the proposed SR-oriented beamforming design
toward privacy protection from ISAC receivers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2404.05130v1">Enabling Privacy-Preserving Cyber Threat Detection with Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-04-08T01:16:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yu Bi, Yekai Li, Xuan Feng, Xianghang Mi</p>
    <p><b>Summary:</b> Despite achieving good performance and wide adoption, machine learning based
security detection models (e.g., malware classifiers) are subject to concept
drift and evasive evolution of attackers, which renders up-to-date threat data
as a necessity. However, due to enforcement of various privacy protection
regulations (e.g., GDPR), it is becoming increasingly challenging or even
prohibitive for security vendors to collect individual-relevant and
privacy-sensitive threat datasets, e.g., SMS spam/non-spam messages from mobile
devices. To address such obstacles, this study systematically profiles the
(in)feasibility of federated learning for privacy-preserving cyber threat
detection in terms of effectiveness, byzantine resilience, and efficiency. This
is made possible by the build-up of multiple threat datasets and threat
detection models, and more importantly, the design of realistic and
security-specific experiments.
  We evaluate FL on two representative threat detection tasks, namely SMS spam
detection and Android malware detection. It shows that FL-trained detection
models can achieve a performance that is comparable to centrally trained
counterparts. Also, most non-IID data distributions have either minor or
negligible impact on the model performance, while a label-based non-IID
distribution of a high extent can incur non-negligible fluctuation and delay in
FL training. Then, under a realistic threat model, FL turns out to be
adversary-resistant to attacks of both data poisoning and model poisoning.
Particularly, the attacking impact of a practical data poisoning attack is no
more than 0.14\% loss in model accuracy. Regarding FL efficiency, a
bootstrapping strategy turns out to be effective to mitigate the training delay
as observed in label-based non-IID scenarios.</p>
  </details>
</div>



<h2>2024-05</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.07596v1">Local Mutual-Information Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-05-13T09:58:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Khac-Hoang Ngo, Johan Östman, Alexandre Graell i Amat</p>
    <p><b>Summary:</b> Local mutual-information differential privacy (LMIDP) is a privacy notion
that aims to quantify the reduction of uncertainty about the input data when
the output of a privacy-preserving mechanism is revealed. We study the relation
of LMIDP with local differential privacy (LDP), the de facto standard notion of
privacy in context-independent (CI) scenarios, and with local information
privacy (LIP), the state-of-the-art notion for context-dependent settings. We
establish explicit conversion rules, i.e., bounds on the privacy parameters for
a LMIDP mechanism to also satisfy LDP/LIP, and vice versa. We use our bounds to
formally verify that LMIDP is a weak privacy notion. We also show that
uncorrelated Gaussian noise is the best-case noise in terms of CI-LMIDP if both
the input data and the noise are subject to an average power constraint.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.07440v1">Maximizing Information Gain in Privacy-Aware Active Learning of Email
  Anomalies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-05-13T02:58:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mu-Huan Miles Chung, Sharon Li, Jaturong Kongmanee, Lu Wang, Yuhong Yang, Calvin Giang, Khilan Jerath, Abhay Raman, David Lie, Mark Chignell</p>
    <p><b>Summary:</b> Redacted emails satisfy most privacy requirements but they make it more
difficult to detect anomalous emails that may be indicative of data
exfiltration. In this paper we develop an enhanced method of Active Learning
using an information gain maximizing heuristic, and we evaluate its
effectiveness in a real world setting where only redacted versions of email
could be labeled by human analysts due to privacy concerns. In the first case
study we examined how Active Learning should be carried out. We found that
model performance was best when a single highly skilled (in terms of the
labelling task) analyst provided the labels. In the second case study we used
confidence ratings to estimate the labeling uncertainty of analysts and then
prioritized instances for labeling based on the expected information gain (the
difference between model uncertainty and analyst uncertainty) that would be
provided by labelling each instance. We found that the information maximization
gain heuristic improved model performance over existing sampling methods for
Active Learning. Based on the results obtained, we recommend that analysts
should be screened, and possibly trained, prior to implementation of Active
Learning in cybersecurity applications. We also recommend that the information
gain maximizing sample method (based on expert confidence) should be used in
early stages of Active Learning, providing that well-calibrated confidence can
be obtained. We also note that the expertise of analysts should be assessed
prior to Active Learning, as we found that analysts with lower labelling skill
had poorly calibrated (over-) confidence in their labels.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.07020v1">Adaptive Online Bayesian Estimation of Frequency Distributions with
  Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-05-11T13:59:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Soner Aydin, Sinan Yildirim</p>
    <p><b>Summary:</b> We propose a novel Bayesian approach for the adaptive and online estimation
of the frequency distribution of a finite number of categories under the local
differential privacy (LDP) framework. The proposed algorithm performs Bayesian
parameter estimation via posterior sampling and adapts the randomization
mechanism for LDP based on the obtained posterior samples. We propose a
randomized mechanism for LDP which uses a subset of categories as an input and
whose performance depends on the selected subset and the true frequency
distribution. By using the posterior sample as an estimate of the frequency
distribution, the algorithm performs a computationally tractable subset
selection step to maximize the utility of the privatized response of the next
user. We propose several utility functions related to well-known information
metrics, such as (but not limited to) Fisher information matrix, total
variation distance, and information entropy. We compare each of these utility
metrics in terms of their computational complexity. We employ stochastic
gradient Langevin dynamics for posterior sampling, a computationally efficient
approximate Markov chain Monte Carlo method. We provide a theoretical analysis
showing that (i) the posterior distribution targeted by the algorithm converges
to the true parameter even for approximate posterior sampling, and (ii) the
algorithm selects the optimal subset with high probability if posterior
sampling is performed exactly. We also provide numerical results that
empirically demonstrate the estimation accuracy of our algorithm where we
compare it with nonadaptive and semi-adaptive approaches under experimental
settings with various combinations of privacy parameters and population
distribution parameters.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.06307v1">Smooth Sensitivity for Geo-Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-05-10T08:32:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuting Liang, Ke Yi</p>
    <p><b>Summary:</b> Suppose each user $i$ holds a private value $x_i$ in some metric space $(U,
\mathrm{dist})$, and an untrusted data analyst wishes to compute $\sum_i
f(x_i)$ for some function $f : U \rightarrow \mathbb{R}$ by asking each user to
send in a privatized $f(x_i)$. This is a fundamental problem in
privacy-preserving population analytics, and the local model of differential
privacy (LDP) is the predominant model under which the problem has been
studied. However, LDP requires any two different $x_i, x'_i$ to be
$\varepsilon$-distinguishable, which can be overly strong for
geometric/numerical data. On the other hand, Geo-Privacy (GP) stipulates that
the level of distinguishability be proportional to $\mathrm{dist}(x_i, x_i')$,
providing an attractive alternative notion of personal data privacy in a metric
space. However, existing GP mechanisms for this problem, which add a uniform
noise to either $x_i$ or $f(x_i)$, are not satisfactory. In this paper, we
generalize the smooth sensitivity framework from Differential Privacy to
Geo-Privacy, which allows us to add noise tailored to the hardness of the given
instance. We provide definitions, mechanisms, and a generic procedure for
computing the smooth sensitivity under GP equipped with a general metric. Then
we present three applications: one-way and two-way threshold functions, and
Gaussian kernel density estimation, to demonstrate the applicability and
utility of our smooth sensitivity framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.06261v1">Improving the Privacy Loss Under User-Level DP Composition for Fixed
  Estimation Error</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-05-10T06:24:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> V. Arvind Rameshwar, Anshoo Tandon</p>
    <p><b>Summary:</b> This paper considers the private release of statistics of several disjoint
subsets of a datasets, under user-level $\epsilon$-differential privacy (DP).
In particular, we consider the user-level differentially private release of
sample means and variances of speed values in several grids in a city, in a
potentially sequential manner. Traditional analysis of the privacy loss due to
the sequential composition of queries necessitates a privacy loss degradation
by a factor that equals the total number of grids. Our main contribution is an
iterative, instance-dependent algorithm, based on clipping the number of user
contributions, which seeks to reduce the overall privacy loss degradation under
a canonical Laplace mechanism, while not increasing the {worst} estimation
error among the different grids. We test the performance of our algorithm on
synthetic datasets and demonstrate improvements in the privacy loss degradation
factor via our algorithm. We also demonstrate improvements in the worst-case
error using a simple extension of a pseudo-user creation-based mechanism. An
important component of this analysis is our exact characterization of the
sensitivities and the worst-case estimation errors of sample means and
variances incurred by clipping user contributions in an arbitrary fashion,
which we believe is of independent interest.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.05930v1">Trustworthy AI-Generative Content in Intelligent 6G Network:
  Adversarial, Privacy, and Fairness</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-05-09T17:16:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Siyuan Li, Xi Lin, Yaju Liu, Jianhua Li</p>
    <p><b>Summary:</b> AI-generated content (AIGC) models, represented by large language models
(LLM), have brought revolutionary changes to the content generation fields. The
high-speed and extensive 6G technology is an ideal platform for providing
powerful AIGC mobile service applications, while future 6G mobile networks also
need to support intelligent and personalized mobile generation services.
However, the significant ethical and security issues of current AIGC models,
such as adversarial attacks, privacy, and fairness, greatly affect the
credibility of 6G intelligent networks, especially in ensuring secure, private,
and fair AIGC applications. In this paper, we propose TrustGAIN, a novel
paradigm for trustworthy AIGC in 6G networks, to ensure trustworthy large-scale
AIGC services in future 6G networks. We first discuss the adversarial attacks
and privacy threats faced by AIGC systems in 6G networks, as well as the
corresponding protection issues. Subsequently, we emphasize the importance of
ensuring the unbiasedness and fairness of the mobile generative service in
future intelligent networks. In particular, we conduct a use case to
demonstrate that TrustGAIN can effectively guide the resistance against
malicious or generated false information. We believe that TrustGAIN is a
necessary paradigm for intelligent and trustworthy 6G networks to support AIGC
services, ensuring the security, privacy, and fairness of AIGC network
services.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.05789v1">High-Performance Privacy-Preserving Matrix Completion for Trajectory
  Recovery</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Numerical Analysis-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Numerical Analysis-D91E36">
  <p><b>Published on:</b> 2024-05-09T14:12:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiahao Guo, An-Bao Xu</p>
    <p><b>Summary:</b> Matrix completion has important applications in trajectory recovery and
mobile social networks. However, sending raw data containing personal,
sensitive information to cloud computing nodes may lead to privacy exposure
issue.The privacy-preserving matrix completion is a useful approach to perform
matrix completion while preserving privacy. In this paper, we propose a
high-performance method for privacy-preserving matrix completion. First,we use
a lightweight encryption scheme to encrypt the raw data and then perform matrix
completion using alternating direction method of multipliers (ADMM). Then,the
complemented matrix is decrypted and compared with the original matrix to
calculate the error. This method has faster speed with higher accuracy. The
results of numerical experiments reveal that the proposed method is faster than
other algorithms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.05611v1">Privacy-Preserving Edge Federated Learning for Intelligent Mobile-Health
  Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-05-09T08:15:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amin Aminifar, Matin Shokri, Amir Aminifar</p>
    <p><b>Summary:</b> Machine Learning (ML) algorithms are generally designed for scenarios in
which all data is stored in one data center, where the training is performed.
However, in many applications, e.g., in the healthcare domain, the training
data is distributed among several entities, e.g., different hospitals or
patients' mobile devices/sensors. At the same time, transferring the data to a
central location for learning is certainly not an option, due to privacy
concerns and legal issues, and in certain cases, because of the communication
and computation overheads. Federated Learning (FL) is the state-of-the-art
collaborative ML approach for training an ML model across multiple parties
holding local data samples, without sharing them. However, enabling learning
from distributed data over such edge Internet of Things (IoT) systems (e.g.,
mobile-health and wearable technologies, involving sensitive personal/medical
data) in a privacy-preserving fashion presents a major challenge mainly due to
their stringent resource constraints, i.e., limited computing capacity,
communication bandwidth, memory storage, and battery lifetime. In this paper,
we propose a privacy-preserving edge FL framework for resource-constrained
mobile-health and wearable technologies over the IoT infrastructure. We
evaluate our proposed framework extensively and provide the implementation of
our technique on Amazon's AWS cloud platform based on the seizure detection
application in epilepsy monitoring using wearable technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.05567v1">Perfect Subset Privacy in Polynomial Computation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-05-09T06:11:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b>  Zirui,  Deng, Vinayak Ramkumar, Netanel Raviv</p>
    <p><b>Summary:</b> Delegating large-scale computations to service providers is a common practice
which raises privacy concerns. This paper studies information-theoretic
privacy-preserving delegation of data to a service provider, who may further
delegate the computation to auxiliary worker nodes, in order to compute a
polynomial over that data at a later point in time. We study techniques which
are compatible with robust management of distributed computation systems, an
area known as coded computing. Privacy in coded computing, however, has
traditionally addressed the problem of colluding workers, and assumed that the
server that administrates the computation is trusted. This viewpoint of privacy
does not accurately reflect real-world privacy concerns, since normally, the
service provider as a whole (i.e., the administrator and the worker nodes) form
one cohesive entity which itself poses a privacy risk. This paper aims to shift
the focus of privacy in coded computing to safeguarding the privacy of the user
against the service provider as a whole, instead of merely against colluding
workers inside the service provider. To this end, we leverage the recently
defined notion of perfect subset privacy, which guarantees zero information
leakage from all subsets of the data up to a certain size. Using known
techniques from Reed-Muller decoding, we provide a scheme which enables
polynomial computation with perfect subset privacy in straggler-free systems.
Furthermore, by studying information super-sets in Reed-Muller codes, which may
be of independent interest, we extend the previous scheme to tolerate
straggling worker nodes inside the service provider.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.05175v1">Air Gap: Protecting Privacy-Conscious Conversational Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-05-08T16:12:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Eugene Bagdasaryan, Ren Yi, Sahra Ghalebikesabi, Peter Kairouz, Marco Gruteser, Sewoong Oh, Borja Balle, Daniel Ramage</p>
    <p><b>Summary:</b> The growing use of large language model (LLM)-based conversational agents to
manage sensitive user data raises significant privacy concerns. While these
agents excel at understanding and acting on context, this capability can be
exploited by malicious actors. We introduce a novel threat model where
adversarial third-party apps manipulate the context of interaction to trick
LLM-based agents into revealing private information not relevant to the task at
hand.
  Grounded in the framework of contextual integrity, we introduce AirGapAgent,
a privacy-conscious agent designed to prevent unintended data leakage by
restricting the agent's access to only the data necessary for a specific task.
Extensive experiments using Gemini, GPT, and Mistral models as agents validate
our approach's effectiveness in mitigating this form of context hijacking while
maintaining core agent functionality. For example, we show that a single-query
context hijacking attack on a Gemini Ultra agent reduces its ability to protect
user data from 94% to 45%, while an AirGapAgent achieves 97% protection,
rendering the same attack ineffective.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.04344v2">Enhancing Scalability of Metric Differential Privacy via Secret Dataset
  Partitioning and Benders Decomposition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-05-07T14:19:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenxi Qiu</p>
    <p><b>Summary:</b> Metric Differential Privacy (mDP) extends the concept of Differential Privacy
(DP) to serve as a new paradigm of data perturbation. It is designed to protect
secret data represented in general metric space, such as text data encoded as
word embeddings or geo-location data on the road network or grid maps. To
derive an optimal data perturbation mechanism under mDP, a widely used method
is linear programming (LP), which, however, might suffer from a polynomial
explosion of decision variables, rendering it impractical in large-scale mDP.
  In this paper, our objective is to develop a new computation framework to
enhance the scalability of the LP-based mDP. Considering the connections
established by the mDP constraints among the secret records, we partition the
original secret dataset into various subsets. Building upon the partition, we
reformulate the LP problem for mDP and solve it via Benders Decomposition,
which is composed of two stages: (1) a master program to manage the
perturbation calculation across subsets and (2) a set of subproblems, each
managing the perturbation derivation within a subset. Our experimental results
on multiple datasets, including geo-location data in the road network/grid
maps, text data, and synthetic data, underscore our proposed mechanism's
superior scalability and efficiency.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.04108v1">A2-DIDM: Privacy-preserving Accumulator-enabled Auditing for Distributed
  Identity of DNN Model</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-05-07T08:24:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianxiu Xie, Keke Gai, Jing Yu, Liehuang Zhu, Kim-Kwang Raymond Choo</p>
    <p><b>Summary:</b> Recent booming development of Generative Artificial Intelligence (GenAI) has
facilitated an emerging model commercialization for the purpose of
reinforcement on model performance, such as licensing or trading Deep Neural
Network (DNN) models. However, DNN model trading may trigger concerns of the
unauthorized replications or misuses over the model, so that the benefit of the
model ownership will be violated. Model identity auditing is a challenging
issue in protecting intellectual property of DNN models and verifying the
integrity and ownership of models for guaranteeing trusts in transactions is
one of the critical obstacles. In this paper, we focus on the above issue and
propose a novel Accumulator-enabled Auditing for Distributed Identity of DNN
Model (A2-DIDM) that utilizes blockchain and zero-knowledge techniques to
protect data and function privacy while ensuring the lightweight on-chain
ownership verification. The proposed model presents a scheme of identity
records via configuring model weight checkpoints with corresponding
zero-knowledge proofs, which incorporates predicates to capture incremental
state changes in model weight checkpoints. Our scheme ensures both
computational integrity of DNN training process and programmability, so that
the uniqueness of the weight checkpoint sequence in a DNN model is preserved,
ensuring the correctness of the model identity auditing. In addition, A2-DIDM
also addresses privacy protections in distributed identity via a proposed
method of accumulators. We systematically analyze the security and robustness
of our proposed model and further evaluate the effectiveness and usability of
auditing DNN model identities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.04029v1">Enabling Privacy-Preserving and Publicly Auditable Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-05-07T06:03:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Huang Zeng, Anjia Yang, Jian Weng, Min-Rong Chen, Fengjun Xiao, Yi Liu, Ye Yao</p>
    <p><b>Summary:</b> Federated learning (FL) has attracted widespread attention because it
supports the joint training of models by multiple participants without moving
private dataset. However, there are still many security issues in FL that
deserve discussion. In this paper, we consider three major issues: 1) how to
ensure that the training process can be publicly audited by any third party; 2)
how to avoid the influence of malicious participants on training; 3) how to
ensure that private gradients and models are not leaked to third parties. Many
solutions have been proposed to address these issues, while solving the above
three problems simultaneously is seldom considered. In this paper, we propose a
publicly auditable and privacy-preserving federated learning scheme that is
resistant to malicious participants uploading gradients with wrong directions
and enables anyone to audit and verify the correctness of the training process.
In particular, we design a robust aggregation algorithm capable of detecting
gradients with wrong directions from malicious participants. Then, we design a
random vector generation algorithm and combine it with zero sharing and
blockchain technologies to make the joint training process publicly auditable,
meaning anyone can verify the correctness of the training. Finally, we conduct
a series of experiments, and the experimental results show that the model
generated by the protocol is comparable in accuracy to the original FL approach
while keeping security advantages.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.03915v1">Motivating Users to Attend to Privacy: A Theory-Driven Design Study</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-05-07T00:23:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Varun Shiri, Maggie Xiong, Jinghui Cheng, Jin L. C. Guo</p>
    <p><b>Summary:</b> In modern technology environments, raising users' privacy awareness is
crucial. Existing efforts largely focused on privacy policy presentation and
failed to systematically address a radical challenge of user motivation for
initiating privacy awareness. Leveraging the Protection Motivation Theory
(PMT), we proposed design ideas and categories dedicated to motivating users to
engage with privacy-related information. Using these design ideas, we created a
conceptual prototype, enhancing the current App Store product page. Results
from an online experiment and follow-up interviews showed that our design
effectively motivated participants to attend to privacy issues, raising both
the threat appraisal and coping appraisal, two main factors in PMT. Our work
indicated that effective design should consider combining PMT components,
calibrating information content, and integrating other design elements, such as
visual cues and user familiarity. Overall, our study contributes valuable
design considerations driven by the PMT to amplify the motivational aspect of
privacy communication.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.03903v1">Unified Locational Differential Privacy Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-05-06T23:33:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Aman Priyanshu, Yash Maurya, Suriya Ganesh, Vy Tran</p>
    <p><b>Summary:</b> Aggregating statistics over geographical regions is important for many
applications, such as analyzing income, election results, and disease spread.
However, the sensitive nature of this data necessitates strong privacy
protections to safeguard individuals. In this work, we present a unified
locational differential privacy (DP) framework to enable private aggregation of
various data types, including one-hot encoded, boolean, float, and integer
arrays, over geographical regions. Our framework employs local DP mechanisms
such as randomized response, the exponential mechanism, and the Gaussian
mechanism. We evaluate our approach on four datasets representing significant
location data aggregation scenarios. Results demonstrate the utility of our
framework in providing formal DP guarantees while enabling geographical data
analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.03636v1">Federated Learning Privacy: Attacks, Defenses, Applications, and Policy
  Landscape - A Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-05-06T16:55:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Joshua C. Zhao, Saurabh Bagchi, Salman Avestimehr, Kevin S. Chan, Somali Chaterji, Dimitris Dimitriadis, Jiacheng Li, Ninghui Li, Arash Nourian, Holger R. Roth</p>
    <p><b>Summary:</b> Deep learning has shown incredible potential across a vast array of tasks and
accompanying this growth has been an insatiable appetite for data. However, a
large amount of data needed for enabling deep learning is stored on personal
devices and recent concerns on privacy have further highlighted challenges for
accessing such data. As a result, federated learning (FL) has emerged as an
important privacy-preserving technology enabling collaborative training of
machine learning models without the need to send the raw, potentially
sensitive, data to a central server. However, the fundamental premise that
sending model updates to a server is privacy-preserving only holds if the
updates cannot be "reverse engineered" to infer information about the private
training data. It has been shown under a wide variety of settings that this
premise for privacy does {\em not} hold.
  In this survey paper, we provide a comprehensive literature review of the
different privacy attacks and defense methods in FL. We identify the current
limitations of these attacks and highlight the settings in which FL client
privacy can be broken. We dissect some of the successful industry applications
of FL and draw lessons for future successful adoption. We survey the emerging
landscape of privacy regulation for FL. We conclude with future directions for
taking FL toward the cherished goal of generating accurate models while
preserving the privacy of the data from its participants.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.03106v1">Compression-based Privacy Preservation for Distributed Nash Equilibrium
  Seeking in Aggregative Games</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Science and Game Theory-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-05-06T01:42:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wei Huo, Xiaomeng Chen, Kemi Ding, Subhrakanti Dey, Ling Shi</p>
    <p><b>Summary:</b> This paper explores distributed aggregative games in multi-agent systems.
Current methods for finding distributed Nash equilibrium require players to
send original messages to their neighbors, leading to communication burden and
privacy issues. To jointly address these issues, we propose an algorithm that
uses stochastic compression to save communication resources and conceal
information through random errors induced by compression. Our theoretical
analysis shows that the algorithm guarantees convergence accuracy, even with
aggressive compression errors used to protect privacy. We prove that the
algorithm achieves differential privacy through a stochastic quantization
scheme. Simulation results for energy consumption games support the
effectiveness of our approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.03065v1">Powering the Future of IoT: Federated Learning for Optimized Power
  Consumption and Enhanced Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-05-05T22:18:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ghazaleh Shirvani, Saeid Ghasemshirazi</p>
    <p><b>Summary:</b> The widespread use of the Internet of Things has led to the development of
large amounts of perception data, making it necessary to develop effective and
scalable data analysis tools. Federated Learning emerges as a promising
paradigm to address the inherent challenges of power consumption and data
privacy in IoT environments. This paper explores the transformative potential
of FL in enhancing the longevity of IoT devices by mitigating power consumption
and enhancing privacy and security measures. We delve into the intricacies of
FL, elucidating its components and applications within IoT ecosystems.
Additionally, we discuss the critical characteristics and challenges of IoT,
highlighting the need for such machine learning solutions in processing
perception data. While FL introduces many benefits for IoT sustainability, it
also has limitations. Through a comprehensive discussion and analysis, this
paper elucidates the opportunities and constraints of FL in shaping the future
of sustainable and secure IoT systems. Our findings highlight the importance of
developing new approaches and conducting additional research to maximise the
benefits of FL in creating a secure and privacy-focused IoT environment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.02665v1">Metric Differential Privacy at the User-Level</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-05-04T13:29:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jacob Imola, Amrita Roy Chowdhury, Kamalika Chaudhuri</p>
    <p><b>Summary:</b> Metric differential privacy (DP) provides heterogeneous privacy guarantees
based on a distance between the pair of inputs. It is a widely popular notion
of privacy since it captures the natural privacy semantics for many
applications (such as, for location data) and results in better utility than
standard DP. However, prior work in metric DP has primarily focused on the
\textit{item-level} setting where every user only reports a single data item. A
more realistic setting is that of user-level DP where each user contributes
multiple items and privacy is then desired at the granularity of the user's
\textit{entire} contribution. In this paper, we initiate the study of metric DP
at the user-level. Specifically, we use the earth-mover's distance
($d_\textsf{EM}$) as our metric to obtain a notion of privacy as it captures
both the magnitude and spatial aspects of changes in a user's data.
  We make three main technical contributions. First, we design two novel
mechanisms under $d_\textsf{EM}$-DP to answer linear queries and item-wise
queries. Specifically, our analysis for the latter involves a generalization of
the privacy amplification by shuffling result which may be of independent
interest. Second, we provide a black-box reduction from the general unbounded
to bounded $d_\textsf{EM}$-DP (size of the dataset is fixed and public) with a
novel sampling based mechanism. Third, we show that our proposed mechanisms can
provably provide improved utility over user-level DP, for certain types of
linear queries and frequency estimation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.02437v1">FastLloyd: Federated, Accurate, Secure, and Tunable $k$-Means Clustering
  with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-05-03T19:04:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abdulrahman Diaa, Thomas Humphries, Florian Kerschbaum</p>
    <p><b>Summary:</b> We study the problem of privacy-preserving $k$-means clustering in the
horizontally federated setting. Existing federated approaches using secure
computation, suffer from substantial overheads and do not offer output privacy.
At the same time, differentially private (DP) $k$-means algorithms assume a
trusted central curator and do not extend to federated settings. Naively
combining the secure and DP solutions results in a protocol with impractical
overhead. Instead, our work provides enhancements to both the DP and secure
computation components, resulting in a design that is faster, more private, and
more accurate than previous work. By utilizing the computational DP model, we
design a lightweight, secure aggregation-based approach that achieves four
orders of magnitude speed-up over state-of-the-art related work. Furthermore,
we not only maintain the utility of the state-of-the-art in the central model
of DP, but we improve the utility further by taking advantage of constrained
clustering techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01844v1">A Survey on Privacy-Preserving Caching at Network Edge: Classification,
  Solutions, and Challenges</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-05-03T04:27:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xianzhi Zhang, Yipeng Zhou, Di Wu, Shazia Riaz, Quan Z. Sheng, Miao Hu, Linchang Xiao</p>
    <p><b>Summary:</b> Caching content at the network edge is a popular and effective technique
widely deployed to alleviate the burden of network backhaul, shorten service
delay and improve service quality. However, there has been some controversy
over privacy violations in caching content at the network edge. On the one
hand, the multi-access open edge network provides an ideal surface for external
attackers to obtain private data from the edge cache by extracting sensitive
information. On the other hand, privacy can be infringed by curious edge
caching providers through caching trace analysis targeting to achieve better
caching performance or higher profits. Therefore, an in-depth understanding of
privacy issues in edge caching networks is vital and indispensable for creating
a privacy-preserving caching service at the network edge. In this article, we
are among the first to fill in this gap by examining privacy-preserving
techniques for caching content at the network edge. Firstly, we provide an
introduction to the background of Privacy-Preserving Edge Caching (PPEC). Next,
we summarize the key privacy issues and present a taxonomy for caching at the
network edge from the perspective of private data. Additionally, we conduct a
retrospective review of the state-of-the-art countermeasures against privacy
leakage from content caching at the network edge. Finally, we conclude the
survey and envision challenges for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01742v1">Addressing Privacy Concerns in Joint Communication and Sensing for 6G
  Networks: Challenges and Prospects</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-05-02T21:25:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Prajnamaya Dass, Sonika Ujjwal, Jiri Novotny, Yevhen Zolotavkin, Zakaria Laaroussi, Stefan Köpsell</p>
    <p><b>Summary:</b> The vision for 6G extends beyond mere communication, incorporating sensing
capabilities to facilitate a diverse array of novel applications and services.
However, the advent of joint communication and sensing (JCAS) technology
introduces concerns regarding the handling of sensitive personally identifiable
information (PII) pertaining to individuals and objects, along with external
third-party data and disclosure. Consequently, JCAS-based applications are
susceptible to privacy breaches, including location tracking, identity
disclosure, profiling, and misuse of sensor data, raising significant
implications under the European Union's General Data Protection Regulation
(GDPR) as well as other applicable standards. This paper critically examines
emergent JCAS architectures and underscores the necessity for network functions
to enable privacy-specific features in the 6G systems. We propose an enhanced
JCAS architecture with additional network functions and interfaces,
facilitating the management of sensing policies, consent information, and
transparency guidelines, alongside the integration of sensing-specific
functions and storage for sensing processing sessions. Furthermore, we conduct
a comprehensive threat analysis for all interfaces, employing security threat
model STRIDE and privacy threat model LINDDUN. We also summarise the identified
threats using standard Common Weakness Enumerations (CWEs). Finally, we suggest
the security and privacy controls as the mitigating strategies to counter the
identified threats stemming from the JCAS architecture.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01716v1">ATTAXONOMY: Unpacking Differential Privacy Guarantees Against Practical
  Adversaries</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-05-02T20:23:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rachel Cummings, Shlomi Hod, Jayshree Sarathy, Marika Swanberg</p>
    <p><b>Summary:</b> Differential Privacy (DP) is a mathematical framework that is increasingly
deployed to mitigate privacy risks associated with machine learning and
statistical analyses. Despite the growing adoption of DP, its technical privacy
parameters do not lend themselves to an intelligible description of the
real-world privacy risks associated with that deployment: the guarantee that
most naturally follows from the DP definition is protection against membership
inference by an adversary who knows all but one data record and has unlimited
auxiliary knowledge. In many settings, this adversary is far too strong to
inform how to set real-world privacy parameters.
  One approach for contextualizing privacy parameters is via defining and
measuring the success of technical attacks, but doing so requires a systematic
categorization of the relevant attack space. In this work, we offer a detailed
taxonomy of attacks, showing the various dimensions of attacks and highlighting
that many real-world settings have been understudied. Our taxonomy provides a
roadmap for analyzing real-world deployments and developing theoretical bounds
for more informative privacy attacks. We operationalize our taxonomy by using
it to analyze a real-world case study, the Israeli Ministry of Health's recent
release of a birth dataset using DP, showing how the taxonomy enables
fine-grained threat modeling and provides insight towards making informed
privacy parameter choices. Finally, we leverage the taxonomy towards defining a
more realistic attack than previously considered in the literature, namely a
distributional reconstruction attack: we generalize Balle et al.'s notion of
reconstruction robustness to a less-informed adversary with distributional
uncertainty, and extend the worst-case guarantees of DP to this average-case
setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01704v1">Privacy-aware Berrut Approximated Coded Computing for Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computational Complexity-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-05-02T20:03:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xavier Martínez Luaña, Rebeca P. Díaz Redondo, Manuel Fernández Veiga</p>
    <p><b>Summary:</b> Federated Learning (FL) is an interesting strategy that enables the
collaborative training of an AI model among different data owners without
revealing their private datasets. Even so, FL has some privacy vulnerabilities
that have been tried to be overcome by applying some techniques like
Differential Privacy (DP), Homomorphic Encryption (HE), or Secure Multi-Party
Computation (SMPC). However, these techniques have some important drawbacks
that might narrow their range of application: problems to work with non-linear
functions and to operate large matrix multiplications and high communication
and computational costs to manage semi-honest nodes. In this context, we
propose a solution to guarantee privacy in FL schemes that simultaneously
solves the previously mentioned problems. Our proposal is based on the Berrut
Approximated Coded Computing, a technique from the Coded Distributed Computing
paradigm, adapted to a Secret Sharing configuration, to provide input privacy
to FL in a scalable way. It can be applied for computing non-linear functions
and treats the special case of distributed matrix multiplication, a key
primitive at the core of many automated learning tasks. Because of these
characteristics, it could be applied in a wide range of FL scenarios, since it
is independent of the machine learning models or aggregation algorithms used in
the FL scheme. We provide analysis of the achieve privacy and complexity of our
solution and, due to the extensive numerical results performed, it can be
observed a good trade-off between privacy and precision.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01678v1">1-Diffractor: Efficient and Utility-Preserving Text Obfuscation
  Leveraging Word-Level Metric Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-05-02T19:07:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Stephen Meisenbacher, Maulik Chevli, Florian Matthes</p>
    <p><b>Summary:</b> The study of privacy-preserving Natural Language Processing (NLP) has gained
rising attention in recent years. One promising avenue studies the integration
of Differential Privacy in NLP, which has brought about innovative methods in a
variety of application settings. Of particular note are $\textit{word-level
Metric Local Differential Privacy (MLDP)}$ mechanisms, which work to obfuscate
potentially sensitive input text by performing word-by-word
$\textit{perturbations}$. Although these methods have shown promising results
in empirical tests, there are two major drawbacks: (1) the inevitable loss of
utility due to addition of noise, and (2) the computational expensiveness of
running these mechanisms on high-dimensional word embeddings. In this work, we
aim to address these challenges by proposing $\texttt{1-Diffractor}$, a new
mechanism that boasts high speedups in comparison to previous mechanisms, while
still demonstrating strong utility- and privacy-preserving capabilities. We
evaluate $\texttt{1-Diffractor}$ for utility on several NLP tasks, for
theoretical and task-based privacy, and for efficiency in terms of speed and
memory. $\texttt{1-Diffractor}$ shows significant improvements in efficiency,
while still maintaining competitive utility and privacy scores across all
conducted comparative tests against previous MLDP mechanisms. Our code is made
available at: https://github.com/sjmeis/Diffractor.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01646v1">Explaining models relating objects and privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-05-02T18:06:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alessio Xompero, Myriam Bontonou, Jean-Michel Arbona, Emmanouil Benetos, Andrea Cavallaro</p>
    <p><b>Summary:</b> Accurately predicting whether an image is private before sharing it online is
difficult due to the vast variety of content and the subjective nature of
privacy itself. In this paper, we evaluate privacy models that use objects
extracted from an image to determine why the image is predicted as private. To
explain the decision of these models, we use feature-attribution to identify
and quantify which objects (and which of their features) are more relevant to
privacy classification with respect to a reference input (i.e., no objects
localised in an image) predicted as public. We show that the presence of the
person category and its cardinality is the main factor for the privacy
decision. Therefore, these models mostly fail to identify private images
depicting documents with sensitive data, vehicle ownership, and internet
activity, or public images with people (e.g., an outdoor concert or people
walking in a public space next to a famous landmark). As baselines for future
benchmarks, we also devise two strategies that are based on the person presence
and cardinality and achieve comparable classification performance of the
privacy models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01494v1">Navigating Heterogeneity and Privacy in One-Shot Federated Learning with
  Diffusion Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-05-02T17:26:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Matias Mendieta, Guangyu Sun, Chen Chen</p>
    <p><b>Summary:</b> Federated learning (FL) enables multiple clients to train models collectively
while preserving data privacy. However, FL faces challenges in terms of
communication cost and data heterogeneity. One-shot federated learning has
emerged as a solution by reducing communication rounds, improving efficiency,
and providing better security against eavesdropping attacks. Nevertheless, data
heterogeneity remains a significant challenge, impacting performance. This work
explores the effectiveness of diffusion models in one-shot FL, demonstrating
their applicability in addressing data heterogeneity and improving FL
performance. Additionally, we investigate the utility of our diffusion model
approach, FedDiff, compared to other one-shot FL methods under differential
privacy (DP). Furthermore, to improve generated sample quality under DP
settings, we propose a pragmatic Fourier Magnitude Filtering (FMF) method,
enhancing the effectiveness of generated data for global model training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01492v1">Exploring Privacy Issues in Mission Critical Communication: Navigating
  5G and Beyond Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-05-02T17:25:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Prajnamaya Dass, Marcel Gräfenstein, Stefan Köpsell</p>
    <p><b>Summary:</b> Mission critical communication (MCC) involves the exchange of information and
data among emergency services, including the police, fire brigade, and other
first responders, particularly during emergencies, disasters, or critical
incidents. The widely-adopted TETRA (Terrestrial Trunked Radio)-based
communication for mission critical services faces challenges including limited
data capacity, coverage limitations, spectrum congestion, and security
concerns. Therefore, as an alternative, mission critical communication over
cellular networks (4G and 5G) has emerged. While cellular-based MCC enables
features like real-time video streaming and high-speed data transmission, the
involvement of network operators and application service providers in the MCC
architecture raises privacy concerns for mission critical users and services.
For instance, the disclosure of a policeman's location details to the network
operator raises privacy concerns. To the best of our knowledge, no existing
work considers the privacy issues in mission critical system with respect to 5G
and upcoming technologies. Therefore, in this paper, we analyse the 3GPP
standardised MCC architecture within the context of 5G core network concepts
and assess the privacy implications for MC users, network entities, and MC
servers. The privacy analysis adheres to the deployment strategies in the
standard for MCC. Additionally, we explore emerging 6G technologies, such as
off-network communications, joint communication and sensing, and non-3GPP
communications, to identify privacy challenges in MCC architecture. Finally, we
propose privacy controls to establish a next-generation privacy-preserving MCC
architecture.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01411v1">IDPFilter: Mitigating Interdependent Privacy Issues in Third-Party Apps</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-05-02T16:02:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuaishuai Liu, Gergely Biczók</p>
    <p><b>Summary:</b> Third-party applications have become an essential part of today's online
ecosystem, enhancing the functionality of popular platforms. However, the
intensive data exchange underlying their proliferation has increased concerns
about interdependent privacy (IDP). This paper provides a comprehensive
investigation into the previously underinvestigated IDP issues of third-party
apps. Specifically, first, we analyze the permission structure of multiple app
platforms, identifying permissions that have the potential to cause
interdependent privacy issues by enabling a user to share someone else's
personal data with an app. Second, we collect datasets and characterize the
extent to which existing apps request these permissions, revealing the
relationship between characteristics such as the respective app platform, the
app's type, and the number of interdependent privacy-related permissions it
requests. Third, we analyze the various reasons IDP is neglected by both data
protection regulations and app platforms and then devise principles that should
be followed when designing a mitigation solution. Finally, based on these
principles and satisfying clearly defined objectives, we propose IDPFilter, a
platform-agnostic API that enables application providers to minimize collateral
information collection by filtering out data collected from their users but
implicating others as data subjects. We implement a proof-of-concept prototype,
IDPTextFilter, that implements the filtering logic on textual data, and provide
its initial performance evaluation with regard to privacy, accuracy, and
efficiency.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01312v1">Privacy-Enhanced Database Synthesis for Benchmark Publishing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-05-02T14:20:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yongrui Zhong, Yunqing Ge, Jianbin Qin, Shuyuan Zheng, Bo Tang, Yu-Xuan Qiu, Rui Mao, Ye Yuan, Makoto Onizuka, Chuan Xiao</p>
    <p><b>Summary:</b> Benchmarking is crucial for evaluating a DBMS, yet existing benchmarks often
fail to reflect the varied nature of user workloads. As a result, there is
increasing momentum toward creating databases that incorporate real-world user
data to more accurately mirror business environments. However, privacy concerns
deter users from directly sharing their data, underscoring the importance of
creating synthesized databases for benchmarking that also prioritize privacy
protection. Differential privacy has become a key method for safeguarding
privacy when sharing data, but the focus has largely been on minimizing errors
in aggregate queries or classification tasks, with less attention given to
benchmarking factors like runtime performance. This paper delves into the
creation of privacy-preserving databases specifically for benchmarking, aiming
to produce a differentially private database whose query performance closely
resembles that of the original data. Introducing PrivBench, an innovative
synthesis framework, we support the generation of high-quality data that
maintains privacy. PrivBench uses sum-product networks (SPNs) to partition and
sample data, enhancing data representation while securing privacy. The
framework allows users to adjust the detail of SPN partitions and privacy
settings, crucial for customizing privacy levels. We validate our approach,
which uses the Laplace and exponential mechanisms, in maintaining privacy. Our
tests show that PrivBench effectively generates data that maintains privacy and
excels in query performance, consistently reducing errors in query execution
time, query cardinality, and KL divergence.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01221v1">A Survey on Semantic Communication Networks: Architecture, Security, and
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-05-02T12:04:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shaolong Guo, Yuntao Wang, Ning Zhang, Zhou Su, Tom H. Luan, Zhiyi Tian, Xuemin Shen</p>
    <p><b>Summary:</b> Semantic communication, emerging as a breakthrough beyond the classical
Shannon paradigm, aims to convey the essential meaning of source data rather
than merely focusing on precise yet content-agnostic bit transmission. By
interconnecting diverse intelligent agents (e.g., autonomous vehicles and VR
devices) via semantic communications, the semantic communication networks
(SemComNet) supports semantic-oriented transmission, efficient spectrum
utilization, and flexible networking among collaborative agents. Consequently,
SemComNet stands out for enabling ever-increasing intelligent applications,
such as autonomous driving and Metaverse. However, being built on a variety of
cutting-edge technologies including AI and knowledge graphs, SemComNet
introduces diverse brand-new and unexpected threats, which pose obstacles to
its widespread development. Besides, due to the intrinsic characteristics of
SemComNet in terms of heterogeneous components, autonomous intelligence, and
large-scale structure, a series of critical challenges emerge in securing
SemComNet. In this paper, we provide a comprehensive and up-to-date survey of
SemComNet from its fundamentals, security, and privacy aspects. Specifically,
we first introduce a novel three-layer architecture of SemComNet for
multi-agent interaction, which comprises the control layer, semantic
transmission layer, and cognitive sensing layer. Then, we discuss its working
modes and enabling technologies. Afterward, based on the layered architecture
of SemComNet, we outline a taxonomy of security and privacy threats, while
discussing state-of-the-art defense approaches. Finally, we present future
research directions, clarifying the path toward building intelligent, robust,
and green SemComNet. To our knowledge, this survey is the first to
comprehensively cover the fundamentals of SemComNet, alongside a detailed
analysis of its security and privacy issues.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.01031v2">The Privacy Power of Correlated Noise in Decentralized Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> 
  <p><b>Published on:</b> 2024-05-02T06:14:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Youssef Allouah, Anastasia Koloskova, Aymane El Firdoussi, Martin Jaggi, Rachid Guerraoui</p>
    <p><b>Summary:</b> Decentralized learning is appealing as it enables the scalable usage of large
amounts of distributed data and resources (without resorting to any central
entity), while promoting privacy since every user minimizes the direct exposure
of their data. Yet, without additional precautions, curious users can still
leverage models obtained from their peers to violate privacy. In this paper, we
propose Decor, a variant of decentralized SGD with differential privacy (DP)
guarantees. Essentially, in Decor, users securely exchange randomness seeds in
one communication round to generate pairwise-canceling correlated Gaussian
noises, which are injected to protect local models at every communication
round. We theoretically and empirically show that, for arbitrary connected
graphs, Decor matches the central DP optimal privacy-utility trade-off. We do
so under SecLDP, our new relaxation of local DP, which protects all user
communications against an external eavesdropper and curious users, assuming
that every pair of connected users shares a secret, i.e., an information hidden
to all others. The main theoretical challenge is to control the accumulation of
non-canceling correlated noise due to network sparsity. We also propose a
companion SecLDP privacy accountant for public use.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.02341v1">Improved Communication-Privacy Trade-offs in $L_2$ Mean Estimation under
  Streaming Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-05-02T03:48:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wei-Ning Chen, Berivan Isik, Peter Kairouz, Albert No, Sewoong Oh, Zheng Xu</p>
    <p><b>Summary:</b> We study $L_2$ mean estimation under central differential privacy and
communication constraints, and address two key challenges: firstly, existing
mean estimation schemes that simultaneously handle both constraints are usually
optimized for $L_\infty$ geometry and rely on random rotation or Kashin's
representation to adapt to $L_2$ geometry, resulting in suboptimal leading
constants in mean square errors (MSEs); secondly, schemes achieving
order-optimal communication-privacy trade-offs do not extend seamlessly to
streaming differential privacy (DP) settings (e.g., tree aggregation or matrix
factorization), rendering them incompatible with DP-FTRL type optimizers.
  In this work, we tackle these issues by introducing a novel privacy
accounting method for the sparsified Gaussian mechanism that incorporates the
randomness inherent in sparsification into the DP noise. Unlike previous
approaches, our accounting algorithm directly operates in $L_2$ geometry,
yielding MSEs that fast converge to those of the uncompressed Gaussian
mechanism. Additionally, we extend the sparsification scheme to the matrix
factorization framework under streaming DP and provide a precise accountant
tailored for DP-FTRL type optimizers. Empirically, our method demonstrates at
least a 100x improvement of compression for DP-SGD across various FL tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.00616v1">An Expectation-Maximization Relaxed Method for Privacy Funnel</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-05-01T16:35:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lingyi Chen, Jiachuan Ye, Shitong Wu, Huihui Wu, Hao Wu, Wenyi Zhang</p>
    <p><b>Summary:</b> The privacy funnel (PF) gives a framework of privacy-preserving data release,
where the goal is to release useful data while also limiting the exposure of
associated sensitive information. This framework has garnered significant
interest due to its broad applications in characterization of the
privacy-utility tradeoff. Hence, there is a strong motivation to develop
numerical methods with high precision and theoretical convergence guarantees.
In this paper, we propose a novel relaxation variant based on Jensen's
inequality of the objective function for the computation of the PF problem.
This model is proved to be equivalent to the original in terms of optimal
solutions and optimal values. Based on our proposed model, we develop an
accurate algorithm which only involves closed-form iterations. The convergence
of our algorithm is theoretically guaranteed through descent estimation and
Pinsker's inequality. Numerical results demonstrate the effectiveness of our
proposed algorithm.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.00596v2">Unbundle-Rewrite-Rebundle: Runtime Detection and Rewriting of
  Privacy-Harming Code in JavaScript Bundles</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-05-01T16:04:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mir Masood Ali, Peter Snyder, Chris Kanich, Hamed Haddadi</p>
    <p><b>Summary:</b> This work presents Unbundle-Rewrite-Rebundle (URR), a system for detecting
privacy-harming portions of bundled JavaScript code, and rewriting that code at
runtime to remove the privacy harming behavior without breaking the surrounding
code or overall application. URR is a novel solution to the problem of
JavaScript bundles, where websites pre-compile multiple code units into a
single file, making it impossible for content filters and ad-blockers to
differentiate between desired and unwanted resources. Where traditional content
filtering tools rely on URLs, URR analyzes the code at the AST level, and
replaces harmful AST sub-trees with privacy-and-functionality maintaining
alternatives.
  We present an open-sourced implementation of URR as a Firefox extension, and
evaluate it against JavaScript bundles generated by the most popular bundling
system (Webpack) deployed on the Tranco 10k. We measure the performance,
measured by precision (1.00), recall (0.95), and speed (0.43s per-script) when
detecting and rewriting three representative privacy harming libraries often
included in JavaScript bundles, and find URR to be an effective approach to a
large-and-growing blind spot unaddressed by current privacy tools.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2405.00329v1">Metric geometry of the privacy-utility tradeoff</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Probability-5BC0EB">
  <p><b>Published on:</b> 2024-05-01T05:31:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> March Boedihardjo, Thomas Strohmer, Roman Vershynin</p>
    <p><b>Summary:</b> Synthetic data are an attractive concept to enable privacy in data sharing. A
fundamental question is how similar the privacy-preserving synthetic data are
compared to the true data. Using metric privacy, an effective generalization of
differential privacy beyond the discrete setting, we raise the problem of
characterizing the optimal privacy-accuracy tradeoff by the metric geometry of
the underlying space. We provide a partial solution to this problem in terms of
the "entropic scale", a quantity that captures the multiscale geometry of a
metric space via the behavior of its packing numbers. We illustrate the
applicability of our privacy-accuracy tradeoff framework via a diverse set of
examples of metric spaces.</p>
  </details>
</div>

