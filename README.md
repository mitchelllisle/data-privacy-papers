
<h2>2025-02</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.04758v1">Differential Privacy of Quantum and Quantum-Inspired-Classical
  Recommendation Algorithms</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-07T08:45:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenjian Li, Mingsheng Ying</p>
    <p><b>Summary:</b> We analyze the DP (differential privacy) properties of the quantum
recommendation algorithm and the quantum-inspired-classical recommendation
algorithm. We discover that the quantum recommendation algorithm is a privacy
curating mechanism on its own, requiring no external noise, which is different
from traditional differential privacy mechanisms. In our analysis, a novel
perturbation method tailored for SVD (singular value decomposition) and
low-rank matrix approximation problems is introduced. Using the perturbation
method and random matrix theory, we are able to derive that both the quantum
and quantum-inspired-classical algorithms are
$\big(\tilde{\mathcal{O}}\big(\frac 1n\big),\,\,
\tilde{\mathcal{O}}\big(\frac{1}{\min\{m,n\}}\big)\big)$-DP under some
reasonable restrictions, where $m$ and $n$ are numbers of users and products in
the input preference database respectively. Nevertheless, a comparison shows
that the quantum algorithm has better privacy preserving potential than the
classical one.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.04045v1">Comparing privacy notions for protection against reconstruction attacks
  in machine learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-02-06T13:04:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sayan Biswas, Mark Dras, Pedro Faustini, Natasha Fernandes, Annabelle McIver, Catuscia Palamidessi, Parastoo Sadeghi</p>
    <p><b>Summary:</b> Within the machine learning community, reconstruction attacks are a principal
concern and have been identified even in federated learning (FL), which was
designed with privacy preservation in mind. In response to these threats, the
privacy community recommends the use of differential privacy (DP) in the
stochastic gradient descent algorithm, termed DP-SGD. However, the
proliferation of variants of DP in recent years\textemdash such as metric
privacy\textemdash has made it challenging to conduct a fair comparison between
different mechanisms due to the different meanings of the privacy parameters
$\epsilon$ and $\delta$ across different variants. Thus, interpreting the
practical implications of $\epsilon$ and $\delta$ in the FL context and amongst
variants of DP remains ambiguous. In this paper, we lay a foundational
framework for comparing mechanisms with differing notions of privacy
guarantees, namely $(\epsilon,\delta)$-DP and metric privacy. We provide two
foundational means of comparison: firstly, via the well-established
$(\epsilon,\delta)$-DP guarantees, made possible through the R\'enyi
differential privacy framework; and secondly, via Bayes' capacity, which we
identify as an appropriate measure for reconstruction threats.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.03811v1">Privacy Risks in Health Big Data: A Systematic Literature Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-06T06:44:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhang Si Yuan, Manmeet Mahinderjit Singh</p>
    <p><b>Summary:</b> The digitization of health records has greatly improved the efficiency of the
healthcare system and promoted the formulation of related research and
policies. However, the widespread application of advanced technologies such as
electronic health records, genomic data, and wearable devices in the field of
health big data has also intensified the collection of personal sensitive data,
bringing serious privacy and security issues. Based on a systematic literature
review (SLR), this paper comprehensively outlines the key research in the field
of health big data security. By analyzing existing research, this paper
explores how cutting-edge technologies such as homomorphic encryption,
blockchain, federated learning, and artificial immune systems can enhance data
security while protecting personal privacy. This paper also points out the
current challenges and proposes a future research framework in this key area.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.03668v1">Privacy-Preserving Generative Models: A Comprehensive Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-05T23:24:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Debalina Padariya, Isabel Wagner, Aboozar Taherkhani, Eerke Boiten</p>
    <p><b>Summary:</b> Despite the generative model's groundbreaking success, the need to study its
implications for privacy and utility becomes more urgent. Although many studies
have demonstrated the privacy threats brought by GANs, no existing survey has
systematically categorized the privacy and utility perspectives of GANs and
VAEs. In this article, we comprehensively study privacy-preserving generative
models, articulating the novel taxonomies for both privacy and utility metrics
by analyzing 100 research publications. Finally, we discuss the current
challenges and future research directions that help new researchers gain
insight into the underlying concepts.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.04365v1">AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case
  Study on Detecting Time of Birth</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-05T07:01:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jorge García-Torres, Øyvind Meinich-Bache, Siren Rettedal, Kjersti Engan</p>
    <p><b>Summary:</b> Approximately 10% of newborns need some assistance to start breathing and 5\%
proper ventilation. It is crucial that interventions are initiated as soon as
possible after birth. Accurate documentation of Time of Birth (ToB) is thereby
essential for documenting and improving newborn resuscitation performance.
However, current clinical practices rely on manual recording of ToB, typically
with minute precision. In this study, we present an AI-driven, video-based
system for automated ToB detection using thermal imaging, designed to preserve
the privacy of healthcare providers and mothers by avoiding the use of
identifiable visual data. Our approach achieves 91.4% precision and 97.4%
recall in detecting ToB within thermal video clips during performance
evaluation. Additionally, our system successfully identifies ToB in 96% of test
cases with an absolute median deviation of 1 second compared to manual
annotations. This method offers a reliable solution for improving ToB
documentation and enhancing newborn resuscitation outcomes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.02913v3">Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient
  Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-05T06:20:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiayang Meng, Tao Huang, Hong Chen, Xin Shi, Qingyu Huang, Chen Hou</p>
    <p><b>Summary:</b> The widespread deployment of deep learning models in privacy-sensitive
domains has amplified concerns regarding privacy risks, particularly those
stemming from gradient leakage during training. Current privacy assessments
primarily rely on post-training attack simulations. However, these methods are
inherently reactive, unable to encompass all potential attack scenarios, and
often based on idealized adversarial assumptions. These limitations underscore
the need for proactive approaches to privacy risk assessment during the
training process. To address this gap, we propose the concept of privacy
tokens, which are derived directly from private gradients during training.
Privacy tokens encapsulate gradient features and, when combined with data
features, offer valuable insights into the extent of private information
leakage from training data, enabling real-time measurement of privacy risks
without relying on adversarial attack simulations. Additionally, we employ
Mutual Information (MI) as a robust metric to quantify the relationship between
training data and gradients, providing precise and continuous assessments of
privacy leakage throughout the training process. Extensive experiments validate
our framework, demonstrating the effectiveness of privacy tokens and MI in
identifying and quantifying privacy risks. This proactive approach marks a
significant advancement in privacy monitoring, promoting the safer deployment
of deep learning models in sensitive applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.02749v1">Unveiling Privacy and Security Gaps in Female Health Apps</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-04T22:34:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Muhammad Hassan, Mahnoor Jameel, Tian Wang, Masooda Bashir</p>
    <p><b>Summary:</b> Female Health Applications (FHA), a growing segment of FemTech, aim to
provide affordable and accessible healthcare solutions for women globally.
These applications gather and monitor health and reproductive data from
millions of users. With ongoing debates on women's reproductive rights and
privacy, it's crucial to assess how these apps protect users' privacy. In this
paper, we undertake a security and data protection assessment of 45 popular
FHAs. Our investigation uncovers harmful permissions, extensive collection of
sensitive personal and medical data, and the presence of numerous third-party
tracking libraries. Furthermore, our examination of their privacy policies
reveals deviations from fundamental data privacy principles. These findings
highlight a significant lack of privacy and security measures for FemTech apps,
especially as women's reproductive rights face growing political challenges.
The results and recommendations provide valuable insights for users, app
developers, and policymakers, paving the way for better privacy and security in
Female Health Applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.02520v1">Privacy by Design for Self-Sovereign Identity Systems: An in-depth
  Component Analysis completed by a Design Assistance Dashboard</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-02-04T17:42:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Montassar Naghmouchi, Maryline Laurent</p>
    <p><b>Summary:</b> The use of Self-Sovereign Identity (SSI) systems for digital identity
management is gaining traction and interest. Countries such as Bhutan have
already implemented an SSI infrastructure to manage the identity of their
citizens. The EU, thanks to the revised eIDAS regulation, is opening the door
for SSI vendors to develop SSI systems for the planned EU digital identity
wallet. These developments, which fall within the sovereign domain, raise
questions about individual privacy.
  The purpose of this article is to help SSI solution designers make informed
choices to ensure that the designed solution is privacy-friendly. The
observation is that the range of possible solutions is very broad, from DID and
DID resolution methods to verifiable credential types, publicly available
information (e.g. in a blockchain), type of infrastructure, etc. As a result,
the article proposes (1) to group the elementary building blocks of a SSI
system into 5 structuring layers, (2) to analyze for each layer the privacy
implications of using the chosen building block, and (3) to provide a design
assistance dashboard that gives the complete picture of the SSI, and shows the
interdependencies between architectural choices and technical building blocks,
allowing designers to make informed choices and graphically achieve a SSI
solution that meets their need for privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.02514v1">Privacy Attacks on Image AutoRegressive Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-04T17:33:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antoni Kowalczuk, Jan Dubiński, Franziska Boenisch, Adam Dziedzic</p>
    <p><b>Summary:</b> Image autoregressive (IAR) models have surpassed diffusion models (DMs) in
both image quality (FID: 1.48 vs. 1.58) and generation speed. However, their
privacy risks remain largely unexplored. To address this, we conduct a
comprehensive privacy analysis comparing IARs to DMs. We develop a novel
membership inference attack (MIA) that achieves a significantly higher success
rate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% for
DMs). Using this MIA, we perform dataset inference (DI) and find that IARs
require as few as six samples to detect dataset membership, compared to 200 for
DMs, indicating higher information leakage. Additionally, we extract hundreds
of training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight
a fundamental privacy-utility trade-off: while IARs excel in generation quality
and speed, they are significantly more vulnerable to privacy attacks. This
suggests that incorporating techniques from DMs, such as per-token probability
modeling using diffusion, could help mitigate IARs' privacy risks. Our code is
available at https://github.com/sprintml/privacy_attacks_against_iars.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.02410v1">Privacy Amplification by Structured Subsampling for Deep Differentially
  Private Time Series Forecasting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-02-04T15:29:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jan Schuchardt, Mina Dalirrooyfard, Jed Guzelkabaagac, Anderson Schneider, Yuriy Nevmyvaka, Stephan Günnemann</p>
    <p><b>Summary:</b> Many forms of sensitive data, such as web traffic, mobility data, or hospital
occupancy, are inherently sequential. The standard method for training machine
learning models while ensuring privacy for units of sensitive information, such
as individual hospital visits, is differentially private stochastic gradient
descent (DP-SGD). However, we observe in this work that the formal guarantees
of DP-SGD are incompatible with timeseries-specific tasks like forecasting,
since they rely on the privacy amplification attained by training on small,
unstructured batches sampled from an unstructured dataset. In contrast, batches
for forecasting are generated by (1) sampling sequentially structured time
series from a dataset, (2) sampling contiguous subsequences from these series,
and (3) partitioning them into context and ground-truth forecast windows. We
theoretically analyze the privacy amplification attained by this structured
subsampling to enable the training of forecasting models with sound and tight
event- and user-level privacy guarantees. Towards more private models, we
additionally prove how data augmentation amplifies privacy in self-supervised
training of sequence models. Our empirical evaluation demonstrates that
amplification by structured subsampling enables the training of forecasting
models with strong formal privacy guarantees.</p>
  </details>
</div>

