
<h2>2025-10</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.03035v1">Protecting Persona Biometric Data: The Case of Facial Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-10-03T14:16:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lambert Hogenhout, Rinzin Wangmo</p>
    <p><b>Summary:</b> The proliferation of digital technologies has led to unprecedented data
collection, with facial data emerging as a particularly sensitive commodity.
Companies are increasingly leveraging advanced facial recognition technologies,
often without the explicit consent or awareness of individuals, to build
sophisticated surveillance capabilities. This practice, fueled by weak and
fragmented laws in many jurisdictions, has created a regulatory vacuum that
allows for the commercialization of personal identity and poses significant
threats to individual privacy and autonomy. This article introduces the concept
of Facial Privacy. It analyzes the profound challenges posed by unregulated
facial recognition by conducting a comprehensive review of existing legal
frameworks. It examines and compares regulations such as the GDPR, Brazil's
LGPD, Canada's PIPEDA, and privacy acts in China, Singapore, South Korea, and
Japan, alongside sector-specific laws in the United States like the Illinois
Biometric Information Privacy Act (BIPA). The analysis highlights the societal
impacts of this technology, including the potential for discriminatory bias and
the long-lasting harm that can result from the theft of immutable biometric
data. Ultimately, the paper argues that existing legal loopholes and
ambiguities leave individuals vulnerable. It proposes a new policy framework
that shifts the paradigm from data as property to a model of inalienable
rights, ensuring that fundamental human rights are upheld against unchecked
technological expansion.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.02487v1">Interplay between Security, Privacy and Trust in 6G-enabled Intelligent
  Transportation Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-10-02T18:47:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmed Danladi Abdullahi, Erfan Bahrami, Tooska Dargahi, Mohammed Al-Khalidi, Mohammad Hammoudeh</p>
    <p><b>Summary:</b> The advancement of 6G technology has the potential to revolutionize the
transportation sector and significantly improve how we travel. 6G-enabled
Intelligent Transportation Systems (ITS) promise to offer high-speed,
low-latency communication and advanced data analytics capabilities, supporting
the development of safer, more efficient, and more sustainable transportation
solutions. However, various security and privacy challenges were identified in
the literature that must be addressed to enable the safe and secure deployment
of 6G-ITS and ensure people's trust in using these technologies. This paper
reviews the opportunities and challenges of 6G-ITS, particularly focusing on
trust, security, and privacy, with special attention to quantum technologies
that both enhance security through quantum key distribution and introduce new
vulnerabilities. It discusses the potential benefits of 6G technology in the
transportation sector, including improved communication, device
interoperability support, data analytic capabilities, and increased automation
for different components, such as transportation management and communication
systems. A taxonomy of different attack models in 6G-ITS is proposed, and a
comparison of the security threats in 5G-ITS and 6G-ITS is provided, along with
potential mitigating solutions. This research highlights the urgent need for a
comprehensive, multi-layered security framework spanning physical
infrastructure protection, network protocol security, data management
safeguards, application security measures, and trust management systems to
effectively mitigate emerging security and privacy risks and ensure the
integrity and resilience of future transportation ecosystems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.01793v1">Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of
  Privacy Filters for Synthetic Data Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-10-02T08:32:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Adil Koeken, Alexander Ziller, Moritz Knolle, Daniel Rueckert</p>
    <p><b>Summary:</b> The generation of privacy-preserving synthetic datasets is a promising avenue
for overcoming data scarcity in medical AI research. Post-hoc privacy filtering
techniques, designed to remove samples containing personally identifiable
information, have recently been proposed as a solution. However, their
effectiveness remains largely unverified. This work presents a rigorous
evaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary
to claims from the original publications, our results demonstrate that current
filters exhibit limited specificity and consistency, achieving high sensitivity
only for real images while failing to reliably detect near-duplicates generated
from training data. These results demonstrate a critical limitation of post-hoc
filtering: rather than effectively safeguarding patient privacy, these methods
may provide a false sense of security while leaving unacceptable levels of
patient information exposed. We conclude that substantial advances in filter
design are needed before these methods can be confidently deployed in sensitive
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.01645v1">Position: Privacy Is Not Just Memorization!</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-10-02T04:02:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Niloofar Mireshghallah, Tianshi Li</p>
    <p><b>Summary:</b> The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.01113v1">Privacy Preserved Federated Learning with Attention-Based Aggregation
  for Biometric Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-10-01T16:58:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kassahun Azezew, Minyechil Alehegn, Tsega Asresa, Bitew Mekuria, Tizazu Bayh, Ayenew Kassie, Amsalu Tesema, Animut Embiyale</p>
    <p><b>Summary:</b> Because biometric data is sensitive, centralized training poses a privacy
risk, even though biometric recognition is essential for contemporary
applications. Federated learning (FL), which permits decentralized training,
provides a privacy-preserving substitute. Conventional FL, however, has trouble
with interpretability and heterogeneous data (non-IID). In order to handle
non-IID biometric data, this framework adds an attention mechanism at the
central server that weights local model updates according to their
significance. Differential privacy and secure update protocols safeguard data
while preserving accuracy. The A3-FL framework is evaluated in this study using
FVC2004 fingerprint data, with each client's features extracted using a Siamese
Convolutional Neural Network (Siamese-CNN). By dynamically modifying client
contributions, the attention mechanism increases the accuracy of the global
model.The accuracy, convergence speed, and robustness of the A3-FL framework
are superior to those of standard FL (FedAvg) and static baselines, according
to experimental evaluations using fingerprint data (FVC2004). The accuracy of
the attention-based approach was 0.8413, while FedAvg, Local-only, and
Centralized approaches were 0.8164, 0.7664, and 0.7997, respectively. Accuracy
stayed high at 0.8330 even with differential privacy. A scalable and
privacy-sensitive biometric system for secure and effective recognition in
dispersed environments is presented in this work.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.00909v1">"We are not Future-ready": Understanding AI Privacy Risks and Existing
  Mitigation Strategies from the Perspective of AI Developers in Europe</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-10-01T13:51:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alexandra Klymenko, Stephen Meisenbacher, Patrick Gage Kelley, Sai Teja Peddinti, Kurt Thomas, Florian Matthes</p>
    <p><b>Summary:</b> The proliferation of AI has sparked privacy concerns related to training
data, model interfaces, downstream applications, and more. We interviewed 25 AI
developers based in Europe to understand which privacy threats they believe
pose the greatest risk to users, developers, and businesses and what protective
strategies, if any, would help to mitigate them. We find that there is little
consensus among AI developers on the relative ranking of privacy risks. These
differences stem from salient reasoning patterns that often relate to human
rather than purely technical factors. Furthermore, while AI developers are
aware of proposed mitigation strategies for addressing these risks, they
reported minimal real-world adoption. Our findings highlight both gaps and
opportunities for empowering AI developers to better address privacy risks in
AI.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.00478v1">Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving
  Domain Adaptation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-10-01T03:58:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jing Wang, Wonho Bae, Jiahong Chen, Wenxu Wang, Junhyug Noh</p>
    <p><b>Summary:</b> Recent work on latent diffusion models (LDMs) has focused almost exclusively
on generative tasks, leaving their potential for discriminative transfer
largely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a
novel LDM-based framework for a more practical variant of source-free domain
adaptation (SFDA): the source provider may share not only a pre-trained
classifier but also an auxiliary latent diffusion module, trained once on the
source data and never exposing raw source samples. DVD encodes each source
feature's label information into its latent vicinity by fitting a Gaussian
prior over its k-nearest neighbors and training the diffusion network to drift
noisy samples back to label-consistent representations. During adaptation, we
sample from each target feature's latent vicinity, apply the frozen diffusion
module to generate source-like cues, and use a simple InfoNCE loss to align the
target encoder to these cues, explicitly transferring decision boundaries
without source access. Across standard SFDA benchmarks, DVD outperforms
state-of-the-art methods. We further show that the same latent diffusion module
enhances the source classifier's accuracy on in-domain data and boosts
performance in supervised classification and domain generalization experiments.
DVD thus reinterprets LDMs as practical, privacy-preserving bridges for
explicit knowledge transfer, addressing a core challenge in source-free domain
adaptation that prior methods have yet to solve.</p>
  </details>
</div>



<h2>2025-09</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.00350v1">Security and Privacy Analysis of Tile's Location Tracking Protocol</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-30T23:25:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Akshaya Kumar, Anna Raymaker, Michael Specter</p>
    <p><b>Summary:</b> We conduct the first comprehensive security analysis of Tile, the second most
popular crowd-sourced location-tracking service behind Apple's AirTags. We
identify several exploitable vulnerabilities and design flaws, disproving many
of the platform's claimed security and privacy guarantees: Tile's servers can
persistently learn the location of all users and tags, unprivileged adversaries
can track users through Bluetooth advertisements emitted by Tile's devices, and
Tile's anti-theft mode is easily subverted.
  Despite its wide deployment -- millions of users, devices, and purpose-built
hardware tags -- Tile provides no formal description of its protocol or threat
model. Worse, Tile intentionally weakens its antistalking features to support
an antitheft use-case and relies on a novel "accountability" mechanism to
punish those abusing the system to stalk victims.
  We examine Tile's accountability mechanism, a unique feature of independent
interest; no other provider attempts to guarantee accountability. While an
ideal accountability mechanism may disincentivize abuse in crowd-sourced
location tracking protocols, we show that Tile's implementation is subvertible
and introduces new exploitable vulnerabilities. We conclude with a discussion
on the need for new, formal definitions of accountability in this setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.00165v1">Privacy-Preserving Learning-Augmented Data Structures</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-09-30T18:37:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Prabhav Goyal, Vinesh Sridhar, Wilson Zheng</p>
    <p><b>Summary:</b> Learning-augmented data structures use predicted frequency estimates to
retrieve frequently occurring database elements faster than standard data
structures. Recent work has developed data structures that optimally exploit
these frequency estimates while maintaining robustness to adversarial
prediction errors. However, the privacy and security implications of this
setting remain largely unexplored.
  In the event of a security breach, data structures should reveal minimal
information beyond their current contents. This is even more crucial for
learning-augmented data structures, whose layout adapts to the data. A data
structure is history independent if its memory representation reveals no
information about past operations except what is inferred from its current
contents. In this work, we take the first step towards privacy and security
guarantees in this setting by proposing the first learning-augmented data
structure that is strongly history independent, robust, and supports dynamic
updates.
  To achieve this, we introduce two techniques: thresholding, which
automatically makes any learning-augmented data structure robust, and pairing,
a simple technique that provides strong history independence in the dynamic
setting. Our experimental results demonstrate a tradeoff between security and
efficiency but are still competitive with the state of the art.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.00164v1">Calyx: Privacy-Preserving Multi-Token Optimistic-Rollup Protocol</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-30T18:35:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dominik Apel, Zeta Avarikioti, Matteo Maffei, Yuheng Wang</p>
    <p><b>Summary:</b> Rollup protocols have recently received significant attention as a promising
class of Layer 2 (L2) scalability solutions. By utilizing the Layer 1 (L1)
blockchain solely as a bulletin board for a summary of the executed
transactions and state changes, rollups enable secure off-chain execution while
avoiding the complexity of other L2 mechanisms. However, to ensure data
availability, current rollup protocols require the plaintext of executed
transactions to be published on-chain, resulting in inherent privacy
limitations.
  In this paper, we address this problem by introducing Calyx, the first
privacy-preserving multi-token optimistic-Rollup protocol. Calyx guarantees
full payment privacy for all L2 transactions, revealing no information about
the sender, recipient, transferred amount, or token type. The protocol further
supports atomic execution of multiple multi-token transactions and introduces a
transaction fee scheme to enable broader application scenarios while ensuring
the sustainable operation of the protocol. To enforce correctness, Calyx adopts
an efficient one-step fraud-proof mechanism. We analyze the security and
privacy guarantees of the protocol and provide an implementation and
evaluation. Our results show that executing a single transaction costs
approximately $0.06 (0.00002 ETH) and incurs only constant-size on-chain cost
in asymptotic terms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.25906v1">Federated Learning with Enhanced Privacy via Model Splitting and Random
  Client Participation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-30T07:51:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yiwei Li, Shuai Wang, Zhuojun Tian, Xiuhua Wang, Shijian Su</p>
    <p><b>Summary:</b> Federated Learning (FL) often adopts differential privacy (DP) to protect
client data, but the added noise required for privacy guarantees can
substantially degrade model accuracy. To resolve this challenge, we propose
model-splitting privacy-amplified federated learning (MS-PAFL), a novel
framework that combines structural model splitting with statistical privacy
amplification. In this framework, each client's model is partitioned into a
private submodel, retained locally, and a public submodel, shared for global
aggregation. The calibrated Gaussian noise is injected only into the public
submodel, thereby confining its adverse impact while preserving the utility of
the local model. We further present a rigorous theoretical analysis that
characterizes the joint privacy amplification achieved through random client
participation and local data subsampling under this architecture. The analysis
provides tight bounds on both single-round and total privacy loss,
demonstrating that MS-PAFL significantly reduces the noise necessary to satisfy
a target privacy protection level. Extensive experiments validate our
theoretical findings, showing that MS-PAFL consistently attains a superior
privacy-utility trade-off and enables the training of highly accurate models
under strong privacy guarantees.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.25525v1">Defeating Cerberus: Concept-Guided Privacy-Leakage Mitigation in
  Multimodal Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-29T21:27:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Boyang Zhang, Istemi Ekin Akkus, Ruichuan Chen, Alice Dethise, Klaus Satzke, Ivica Rimac, Yang Zhang</p>
    <p><b>Summary:</b> Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities in processing and reasoning over diverse modalities, but their
advanced abilities also raise significant privacy concerns, particularly
regarding Personally Identifiable Information (PII) leakage. While relevant
research has been conducted on single-modal language models to some extent, the
vulnerabilities in the multimodal setting have yet to be fully investigated. In
this work, we investigate these emerging risks with a focus on vision language
models (VLMs), a representative subclass of MLLMs that covers the two
modalities most relevant for PII leakage, vision and text. We introduce a
concept-guided mitigation approach that identifies and modifies the model's
internal states associated with PII-related content. Our method guides VLMs to
refuse PII-sensitive tasks effectively and efficiently, without requiring
re-training or fine-tuning. We also address the current lack of multimodal PII
datasets by constructing various ones that simulate real-world scenarios.
Experimental results demonstrate that the method can achieve an average refusal
rate of 93.3% for various PII-related tasks with minimal impact on unrelated
model performances. We further examine the mitigation's performance under
various conditions to show the adaptability of our proposed method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.25469v1">Balancing Compliance and Privacy in Offline CBDC Transactions Using a
  Secure Element-based System</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-29T20:19:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Panagiotis Michalopoulos, Anthony Mack, Cameron Clark, Linus Chen, Johannes Sedlmeir, Andreas Veneris</p>
    <p><b>Summary:</b> Blockchain technology has spawned a vast ecosystem of digital currencies with
Central Bank Digital Currencies (CBDCs) -- digital forms of fiat currency --
being one of them. An important feature of digital currencies is facilitating
transactions without network connectivity, which can enhance the scalability of
cryptocurrencies and the privacy of CBDC users. However, in the case of CBDCs,
this characteristic also introduces new regulatory challenges, particularly
when it comes to applying established Anti-Money Laundering and Countering the
Financing of Terrorism (AML/CFT) frameworks. This paper introduces a prototype
for offline digital currency payments, equally applicable to cryptocurrencies
and CBDCs, that leverages Secure Elements and digital credentials to address
the tension of offline payment support with regulatory compliance. Performance
evaluation results suggest that the prototype can be flexibly adapted to
different regulatory environments, with a transaction latency comparable to
real-life commercial payment systems. Furthermore, we conceptualize how the
integration of Zero-Knowledge Proofs into our design could accommodate various
tiers of enhanced privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.25072v1">Optimizing Privacy-Preserving Primitives to Support LLM-Scale
  Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-29T17:16:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yaman Jandali, Ruisi Zhang, Nojan Sheybani, Farinaz Koushanfar</p>
    <p><b>Summary:</b> Privacy-preserving technologies have introduced a paradigm shift that allows
for realizable secure computing in real-world systems. The significant barrier
to the practical adoption of these primitives is the computational and
communication overhead that is incurred when applied at scale. In this paper,
we present an overview of our efforts to bridge the gap between this overhead
and practicality for privacy-preserving learning systems using multi-party
computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic
encryption (FHE). Through meticulous hardware/software/algorithm co-design, we
show progress towards enabling LLM-scale applications in privacy-preserving
settings. We demonstrate the efficacy of our solutions in several contexts,
including DNN IP ownership, ethical LLM usage enforcement, and transformer
inference.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.24488v1">Sanitize Your Responses: Mitigating Privacy Leakage in Large Language
  Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-29T08:59:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenjie Fu, Huandong Wang, Junyao Gao, Guoan Wan, Tao Jiang</p>
    <p><b>Summary:</b> As Large Language Models (LLMs) achieve remarkable success across a wide
range of applications, such as chatbots and code copilots, concerns surrounding
the generation of harmful content have come increasingly into focus. Despite
significant advances in aligning LLMs with safety and ethical standards,
adversarial prompts can still be crafted to elicit undesirable responses.
Existing mitigation strategies are predominantly based on post-hoc filtering,
which introduces substantial latency or computational overhead, and is
incompatible with token-level streaming generation. In this work, we introduce
Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive
psychology, which emulates human self-monitor and self-repair behaviors during
conversations. Self-Sanitize comprises a lightweight Self-Monitor module that
continuously inspects high-level intentions within the LLM at the token level
via representation engineering, and a Self-Repair module that performs in-place
correction of harmful content without initiating separate review dialogues.
This design allows for real-time streaming monitoring and seamless repair, with
negligible impact on latency and resource utilization. Given that
privacy-invasive content has often been insufficiently focused in previous
studies, we perform extensive experiments on four LLMs across three privacy
leakage scenarios. The results demonstrate that Self-Sanitize achieves superior
mitigation performance with minimal overhead and without degrading the utility
of LLMs, offering a practical and robust solution for safer LLM deployments.
Our code is available at the following link:
https://github.com/wjfu99/LLM_Self_Sanitize</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.24345v1">Regulating Online Algorithmic Pricing: A Comparative Study of Privacy
  and Data Protection Laws in the EU and US</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-09-29T06:46:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zihao Li</p>
    <p><b>Summary:</b> The emergence of big data, AI and machine learning has allowed sellers and
online platforms to tailor pricing for customers in real-time. While online
algorithmic pricing can increase efficiency, market welfare, and optimize
pricing strategies for sellers and companies, it poses a threat to the
fundamental values of privacy, digital autonomy, and non-discrimination,
raising legal and ethical concerns. On both sides of the Atlantic, legislators
have endeavoured to regulate online algorithmic pricing in different ways in
the context of privacy and personal data protection. Represented by the GDPR,
the EU adopts an omnibus approach to regulate algorithmic pricing and is
supplemented by the Digital Service Act and the Digital Market Act. The US
combines federal and state laws to regulate online algorithmic pricing and
focuses on industrial regulations. Therefore, a comparative analysis of these
legal frameworks is necessary to ascertain the effectiveness of these
approaches. Taking a comparative approach, this working paper aims to explore
how EU and US respective data protection and privacy laws address the issues
posed by online algorithmic pricing. The paper evaluates whether the current
legal regime is effective in protecting individuals against the perils of
online algorithmic pricing in the EU and the US. It particularly analyses the
new EU regulatory paradigm, the Digital Service Act (DSA) and the Digital
Market Act (DMA), as supplementary mechanisms to the EU data protection law, in
order to draw lessons for US privacy law and vice versa.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.24173v1">Fundamental Limit of Discrete Distribution Estimation under
  Utility-Optimized Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-09-29T01:41:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sun-Moon Yoon, Hyun-Young Park, Seung-Hyun Nam, Si-Hyeon Lee</p>
    <p><b>Summary:</b> We study the problem of discrete distribution estimation under
utility-optimized local differential privacy (ULDP), which enforces local
differential privacy (LDP) on sensitive data while allowing more accurate
inference on non-sensitive data. In this setting, we completely characterize
the fundamental privacy-utility trade-off. The converse proof builds on several
key ideas, including a generalized uniform asymptotic Cram\'er-Rao lower bound,
a reduction showing that it suffices to consider a newly defined class of
extremal ULDP mechanisms, and a novel distribution decomposition technique
tailored to ULDP constraints. For the achievability, we propose a class of
utility-optimized block design (uBD) schemes, obtained as nontrivial
modifications of the block design mechanism known to be optimal under standard
LDP constraints, while incorporating the distribution decomposition idea used
in the converse proof and a score-based linear estimator. These results provide
a tight characterization of the estimation accuracy achievable under ULDP and
reveal new insights into the structure of optimal mechanisms for
privacy-preserving statistical inference.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.24153v1">DNS in the Time of Curiosity: A Tale of Collaborative User Privacy
  Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-29T01:09:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Philip Sjösvärd, Hongyu Jin, Panos Papadimitratos</p>
    <p><b>Summary:</b> The Domain Name System (DNS) is central to all Internet user activity,
resolving accessed domain names into Internet Protocol (IP) addresses. As a
result, curious DNS resolvers can learn everything about Internet users'
interests. Public DNS resolvers are rising in popularity, offering low-latency
resolution, high reliability, privacy-preserving policies, and support for
encrypted DNS queries. However, client-resolver traffic encryption,
increasingly deployed to protect users from eavesdroppers, does not protect
users against curious resolvers. Similarly, privacy-preserving policies are
based solely on written commitments and do not provide technical safeguards.
Although DNS query relay schemes can separate duties to limit data accessible
by each entity, they cannot prevent colluding entities from sharing user
traffic logs. Thus, a key challenge remains: organizations operating public DNS
resolvers, accounting for the majority of DNS resolutions, can potentially
collect and analyze massive volumes of Internet user activity data. With DNS
infrastructure that cannot be fully trusted, can we safeguard user privacy? We
answer positively and advocate for a user-driven approach to reduce exposure to
DNS services. We will discuss key ideas of the proposal, which aims to achieve
a high level of privacy without sacrificing performance: maintaining low
latency, network bandwidth, memory/storage overhead, and computational
overhead.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23834v1">GPM: The Gaussian Pancake Mechanism for Planting Undetectable Backdoors
  in Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-09-28T12:14:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haochen Sun, Xi He</p>
    <p><b>Summary:</b> Differential privacy (DP) has become the gold standard for preserving
individual privacy in data analysis. However, an implicit yet fundamental
assumption underlying these rigorous privacy guarantees is the correct
implementation and execution of DP mechanisms. Several incidents of unintended
privacy loss have occurred due to numerical issues and inappropriate
configurations of DP software, which have been successfully exploited in
privacy attacks. To better understand the seriousness of defective DP software,
we ask the following question: is it possible to elevate these passive defects
into active privacy attacks while maintaining covertness?
  To address this question, we present the Gaussian pancake mechanism (GPM), a
novel mechanism that is computationally indistinguishable from the widely used
Gaussian mechanism (GM), yet exhibits arbitrarily weaker statistical DP
guarantees. This unprecedented separation enables a new class of backdoor
attacks: by indistinguishably passing off as the authentic GM, GPM can covertly
degrade statistical privacy. Unlike the unintentional privacy loss caused by
GM's numerical issues, GPM is an adversarial yet undetectable backdoor attack
against data privacy. We formally prove GPM's covertness, characterize its
statistical leakage, and demonstrate a concrete distinguishing attack that can
achieve near-perfect success rates under suitable parameter choices, both
theoretically and empirically.
  Our results underscore the importance of using transparent, open-source DP
libraries and highlight the need for rigorous scrutiny and formal verification
of DP implementations to prevent subtle, undetectable privacy compromises in
real-world systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23827v1">Assessing Visual Privacy Risks in Multimodal AI: A Novel
  Taxonomy-Grounded Evaluation of Vision-Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-28T12:04:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Efthymios Tsaprazlis, Tiantian Feng, Anil Ramakrishna, Rahul Gupta, Shrikanth Narayanan</p>
    <p><b>Summary:</b> Artificial Intelligence have profoundly transformed the technological
landscape in recent years. Large Language Models (LLMs) have demonstrated
impressive abilities in reasoning, text comprehension, contextual pattern
recognition, and integrating language with visual understanding. While these
advances offer significant benefits, they also reveal critical limitations in
the models' ability to grasp the notion of privacy. There is hence substantial
interest in determining if and how these models can understand and enforce
privacy principles, particularly given the lack of supporting resources to test
such a task. In this work, we address these challenges by examining how legal
frameworks can inform the capabilities of these emerging technologies. To this
end, we introduce a comprehensive, multi-level Visual Privacy Taxonomy that
captures a wide range of privacy issues, designed to be scalable and adaptable
to existing and future research needs. Furthermore, we evaluate the
capabilities of several state-of-the-art Vision-Language Models (VLMs),
revealing significant inconsistencies in their understanding of contextual
privacy. Our work contributes both a foundational taxonomy for future research
and a critical benchmark of current model limitations, demonstrating the urgent
need for more robust, privacy-aware AI systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23680v1">A First Look at Privacy Risks of Android Task-executable Voice Assistant
  Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-09-28T06:47:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shidong Pan, Yikai Ge, Xiaoyu Sun</p>
    <p><b>Summary:</b> With the development of foundation AI technologies, task-executable voice
assistants (VAs) have become more popular, enhancing user convenience and
expanding device functionality. Android task-executable VAs are applications
that are capable of understanding complex tasks and performing corresponding
operations. Given their prevalence and great autonomy, there is no existing
work examine the privacy risks within the voice assistants from the
task-execution pattern in a holistic manner. To fill this research gap, this
paper presents a user-centric comprehensive empirical study on privacy risks in
Android task-executable VA applications. We collect ten mainstream VAs as our
research target and analyze their operational characteristics. We then
cross-check their privacy declarations across six sources, including privacy
labels, policies, and manifest files, and our findings reveal widespread
inconsistencies. Moreover, we uncover three significant privacy threat models:
(1) privacy misdisclosure in mega apps, where integrated mini apps such as
Alexa skills are inadequately represented; (2) privilege escalation via
inter-application interactions, which exploit Android's communication
mechanisms to bypass user consent; and (3) abuse of Google system applications,
enabling apps to evade the declaration of dangerous permissions. Our study
contributes actionable recommendations for practitioners and underscores
broader relevance of these privacy risks to emerging autonomous AI agents.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.02357v1">Privacy in the Age of AI: A Taxonomy of Data Risks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> 
  <p><b>Published on:</b> 2025-09-28T00:20:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Grace Billiris, Asif Gill, Madhushi Bandara</p>
    <p><b>Summary:</b> Artificial Intelligence (AI) systems introduce unprecedented privacy
challenges as they process increasingly sensitive data. Traditional privacy
frameworks prove inadequate for AI technologies due to unique characteristics
such as autonomous learning and black-box decision-making. This paper presents
a taxonomy classifying AI privacy risks, synthesised from 45 studies identified
through systematic review. We identify 19 key risks grouped under four
categories: Dataset-Level, Model-Level, Infrastructure-Level, and Insider
Threat Risks. Findings reveal a balanced distribution across these dimensions,
with human error (9.45%) emerging as the most significant factor. This taxonomy
challenges conventional security approaches that typically prioritise technical
controls over human factors, highlighting gaps in holistic understanding. By
bridging technical and behavioural dimensions of AI privacy, this paper
contributes to advancing trustworthy AI development and provides a foundation
for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.02356v1">Measuring Physical-World Privacy Awareness of Large Language Models: An
  Evaluation Benchmark</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-27T23:39:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinjie Shen, Mufei Li, Pan Li</p>
    <p><b>Summary:</b> The deployment of Large Language Models (LLMs) in embodied agents creates an
urgent need to measure their privacy awareness in the physical world. Existing
evaluation methods, however, are confined to natural language based scenarios.
To bridge this gap, we introduce EAPrivacy, a comprehensive evaluation
benchmark designed to quantify the physical-world privacy awareness of
LLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across
four tiers to test an agent's ability to handle sensitive objects, adapt to
changing environments, balance task execution with privacy constraints, and
resolve conflicts with social norms. Our measurements reveal a critical deficit
in current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\%
accuracy in scenarios involving changing physical environments. Furthermore,
when a task was accompanied by a privacy request, models prioritized completion
over the constraint in up to 86\% of cases. In high-stakes situations pitting
privacy against critical social norms, leading models like GPT-4o and
Claude-3.5-haiku disregarded the social norm over 15\% of the time. These
findings, demonstrated by our benchmark, underscore a fundamental misalignment
in LLMs regarding physically grounded privacy and establish the need for more
robust, physically-aware alignment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23525v1">Privy: Envisioning and Mitigating Privacy Risks for Consumer-facing AI
  Product Concepts</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-27T23:08:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao-Ping Lee, Yu-Ju Yang, Matthew Bilik, Isadora Krsek, Thomas Serban von Davier, Kyzyl Monteiro, Jason Lin, Shivani Agarwal, Jodi Forlizzi, Sauvik Das</p>
    <p><b>Summary:</b> AI creates and exacerbates privacy risks, yet practitioners lack effective
resources to identify and mitigate these risks. We present Privy, a tool that
guides practitioners through structured privacy impact assessments to: (i)
identify relevant risks in novel AI product concepts, and (ii) propose
appropriate mitigations. Privy was shaped by a formative study with 11
practitioners, which informed two versions -- one LLM-powered, the other
template-based. We evaluated these two versions of Privy through a
between-subjects, controlled study with 24 separate practitioners, whose
assessments were reviewed by 13 independent privacy experts. Results show that
Privy helps practitioners produce privacy assessments that experts deemed high
quality: practitioners identified relevant risks and proposed appropriate
mitigation strategies. These effects were augmented in the LLM-powered version.
Practitioners themselves rated Privy as being useful and usable, and their
feedback illustrates how it helps overcome long-standing awareness, motivation,
and ability barriers in privacy work.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23459v2">MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-09-27T19:07:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sepideh Abedini, Shubhankar Mohapatra, D. B. Emerson, Masoumeh Shafieinejad, Jesse C. Cresswell, Xi He</p>
    <p><b>Summary:</b> Large language models (LLMs) have shown promising performance on tasks that
require reasoning, such as text-to-SQL, code generation, and debugging.
However, regulatory frameworks with strict privacy requirements constrain their
integration into sensitive systems. State-of-the-art LLMs are also proprietary,
costly, and resource-intensive, making local deployment impractical.
Consequently, utilizing such LLMs often requires sharing data with third-party
providers, raising privacy concerns and risking noncompliance with regulations.
Although fine-tuned small language models (SLMs) can outperform LLMs on certain
tasks and be deployed locally to mitigate privacy concerns, they underperform
on more complex tasks such as text-to-SQL translation. In this work, we
introduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a
privacy protection mechanism to mask sensitive information in LLM prompts.
Unlike redaction, which removes content entirely, or generalization, which
broadens tokens, abstraction retains essential information while discarding
unnecessary details, striking an effective privacy-utility balance for the
text-to-SQL task. Moreover, by providing mechanisms to control the
privacy-utility tradeoff, MaskSQL facilitates adoption across a broader range
of use cases. Our experimental results show that MaskSQL outperforms leading
SLM-based text-to-SQL models and achieves performance approaching
state-of-the-art LLM-based models, while preserving privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23444v1">HoloTrace: a Location Privacy Preservation Solution for mmWave MIMO-OFDM
  Systems</a></h3>
  
  <p><b>Published on:</b> 2025-09-27T18:26:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lorenzo Italiano, Alireza Pourafzal, Hui Chen, Mattia Brambilla, Gonzalo Seco-Granados, Monica Nicoli, Henk Wymeersch</p>
    <p><b>Summary:</b> The technological innovation towards 6G cellular networks introduces
unprecedented capabilities for user equipment (UE) localization, but it also
raises serious concerns about physical layer location privacy. This paper
introduces HoloTrace, a signal-level privacy preservation framework that relies
on user-side spoofing of localization-relevant features to prevent the
extraction of precise location information from the signals received by a base
station (BS) in a mmWave MIMO-OFDM system. Spoofing is performed by the user on
location parameters such as angle of arrival (AoA), angle of departure (AoD),
and time difference of arrival (TDoA). Without requiring any protocol
modification nor network-side support, our method strategically perturbs pilot
transmissions to prevent a BS from performing non-consensual UE localization.
The methodology allows the UE to spoof its position, keeping the precoder
unchanged. We formulate spoofing as a unified rank-constrained projection
problem, and provide closed-form solutions under varying levels of channel
state information (CSI) at the UE, including scenarios with and without CSI
knowledge. Simulation results confirm that the proposed approach enables the UE
to deceive the BS, inducing significant localization errors, while the impact
on link capacity varies depending on the spoofed position. Our findings
establish HoloTrace as a practical and robust privacy-preserving solution for
future 6G networks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23246v1">Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens
  Require Equal Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-27T10:51:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Manjiang Yu, Priyanka Singh, Xue Li, Yang Cao</p>
    <p><b>Summary:</b> Large language models (LLMs) frequently memorize sensitive or personal
information, raising significant privacy concerns. Existing variants of
differential privacy stochastic gradient descent (DPSGD) inject uniform noise
into every gradient step, significantly extending training time and reducing
model accuracy. We propose that concentrating noise primarily on gradients
associated with sensitive tokens can substantially decrease DP training time,
strengthen the protection of sensitive information, and simultaneously preserve
the model's performance on non-sensitive data. We operationalize this insight
through Adaptive Token-Weighted Differential Privacy (ATDP), a modification of
vanilla DP-SGD that adaptively assigns different gradient weights to sensitive
and non-sensitive tokens. By employing a larger noise scale at the early stage
of training, ATDP rapidly disrupts memorization of sensitive content. As a
result, ATDP only requires a few additional epochs of lightweight
post-processing following standard fine-tuning, injecting targeted noise
primarily on parameters corresponding to sensitive tokens, thus minimally
affecting the model's general capabilities. ATDP can be seamlessly integrated
into any existing DP-based fine-tuning pipeline or directly applied to
non-private models as a fast privacy-enhancing measure. Additionally, combined
with an initial redacted fine-tuning phase, ATDP forms a streamlined DP
pipeline that achieves comparable canary protection to state-of-the-art DP-SGD
methods, significantly reduces the computational overhead of DP fine-tuning,
shortening training time by approximately 90 percent, while achieving
comparable or superior privacy protection and minimal accuracy degradation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23190v1">CoSIFL: Collaborative Secure and Incentivized Federated Learning with
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-27T08:45:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhanhong Xie, Meifan Zhang, Lihua Yin</p>
    <p><b>Summary:</b> Federated learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data locality. However, it still faces
challenges from malicious or compromised clients, as well as difficulties in
incentivizing participants to contribute high-quality data under strict privacy
requirements. Motivated by these considerations, we propose CoSIFL, a novel
framework that integrates proactive alarming for robust security and local
differential privacy (LDP) for inference attacks, together with a
Stackelberg-based incentive scheme to encourage client participation and data
sharing. Specifically, CoSIFL uses an active alarming mechanism and robust
aggregation to defend against Byzantine and inference attacks, while a Tullock
contest-inspired incentive module rewards honest clients for both data
contributions and reliable alarm triggers. We formulate the interplay between
the server and clients as a two-stage game: in the first stage, the server
determines total rewards, selects participants, and fixes global iteration
settings, whereas in the second stage, each client decides its mini-batch size,
privacy noise scale, and alerting strategy. We prove that the server-client
game admits a unique equilibrium, and analyze how clients' multi-dimensional
attributes - such as non-IID degrees and privacy budgets - jointly affect
system efficiency. Experimental results on standard benchmarks demonstrate that
CoSIFL outperforms state-of-the-art solutions in improving model robustness and
reducing total server costs, highlighting the effectiveness of our integrated
design.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23091v1">FedBit: Accelerating Privacy-Preserving Federated Learning via
  Bit-Interleaved Packing and Cross-Layer Co-Design</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Hardware Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-27T03:58:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiangchen Meng, Yangdi Lyu</p>
    <p><b>Summary:</b> Federated learning (FL) with fully homomorphic encryption (FHE) effectively
safeguards data privacy during model aggregation by encrypting local model
updates before transmission, mitigating threats from untrusted servers or
eavesdroppers in transmission. However, the computational burden and ciphertext
expansion associated with homomorphic encryption can significantly increase
resource and communication overhead. To address these challenges, we propose
FedBit, a hardware/software co-designed framework optimized for the
Brakerski-Fan-Vercauteren (BFV) scheme. FedBit employs bit-interleaved data
packing to embed multiple model parameters into a single ciphertext
coefficient, thereby minimizing ciphertext expansion and maximizing
computational parallelism. Additionally, we integrate a dedicated FPGA
accelerator to handle cryptographic operations and an optimized dataflow to
reduce the memory overhead. Experimental results demonstrate that FedBit
achieves a speedup of two orders of magnitude in encryption and lowers average
communication overhead by 60.7%, while maintaining high accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23030v1">DPFNAS: Differential Privacy-Enhanced Federated Neural Architecture
  Search for 6G Edge Intelligence</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-27T01:03:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yang Lv, Jin Cao, Ben Niu, Zhe Sun, Fengwei Wang, Fenghua Li, Hui Li</p>
    <p><b>Summary:</b> The Sixth-Generation (6G) network envisions pervasive artificial intelligence
(AI) as a core goal, enabled by edge intelligence through on-device data
utilization. To realize this vision, federated learning (FL) has emerged as a
key paradigm for collaborative training across edge devices. However, the
sensitivity and heterogeneity of edge data pose key challenges to FL: parameter
sharing risks data reconstruction, and a unified global model struggles to
adapt to diverse local distributions. In this paper, we propose a novel
federated learning framework that integrates personalized differential privacy
(DP) and adaptive model design. To protect training data, we leverage
sample-level representations for knowledge sharing and apply a personalized DP
strategy to resist reconstruction attacks. To ensure distribution-aware
adaptation under privacy constraints, we develop a privacy-aware neural
architecture search (NAS) algorithm that generates locally customized
architectures and hyperparameters. To the best of our knowledge, this is the
first personalized DP solution tailored for representation-based FL with
theoretical convergence guarantees. Our scheme achieves strong privacy
guarantees for training data while significantly outperforming state-of-the-art
methods in model performance. Experiments on benchmark datasets such as
CIFAR-10 and CIFAR-100 demonstrate that our scheme improves accuracy by 6.82\%
over the federated NAS method PerFedRLNAS, while reducing model size to 1/10
and communication cost to 1/20.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23022v1">Copyright Infringement Detection in Text-to-Image Diffusion Models via
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-09-27T00:38:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiafeng Man, Zhipeng Wei, Jingjing Chen</p>
    <p><b>Summary:</b> The widespread deployment of large vision models such as Stable Diffusion
raises significant legal and ethical concerns, as these models can memorize and
reproduce copyrighted content without authorization. Existing detection
approaches often lack robustness and fail to provide rigorous theoretical
underpinnings. To address these gaps, we formalize the concept of copyright
infringement and its detection from the perspective of Differential Privacy
(DP), and introduce the conditional sensitivity metric, a concept analogous to
sensitivity in DP, that quantifies the deviation in a diffusion model's output
caused by the inclusion or exclusion of a specific training data point. To
operationalize this metric, we propose D-Plus-Minus (DPM), a novel post-hoc
detection framework that identifies copyright infringement in text-to-image
diffusion models. Specifically, DPM simulates inclusion and exclusion processes
by fine-tuning models in two opposing directions: learning or unlearning.
Besides, to disentangle concept-specific influence from the global parameter
shifts induced by fine-tuning, DPM computes confidence scores over orthogonal
prompt distributions using statistical metrics. Moreover, to facilitate
standardized benchmarking, we also construct the Copyright Infringement
Detection Dataset (CIDD), a comprehensive resource for evaluating detection
across diverse categories. Our results demonstrate that DPM reliably detects
infringement content without requiring access to the original training dataset
or text prompts, offering an interpretable and practical solution for
safeguarding intellectual property in the era of generative AI.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.22900v1">Towards Context-aware Mobile Privacy Notice: Implementation of A
  Deployable Contextual Privacy Policies Generator</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-09-26T20:26:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haochen Gong, Zhen Tao, Shidong Pan, Zhenchang Xing, Xiaoyu Sun</p>
    <p><b>Summary:</b> Lengthy and legally phrased privacy policies impede users' understanding of
how mobile applications collect and process personal data. Prior work proposed
Contextual Privacy Policies (CPPs) for mobile apps to display shorter policy
snippets only in the corresponding user interface contexts, but the pipeline
could not be deployable in real-world mobile environments. In this paper, we
present PrivScan, the first deployable CPP Software Development Kit (SDK) for
Android. It captures live app screenshots to identify GUI elements associated
with types of personal data and displays CPPs in a concise, user-facing format.
We provide a lightweight floating button that offers low-friction, on-demand
control. The architecture leverages remote deployment to decouple the
multimodal backend pipeline from a mobile client comprising five modular
components, thereby reducing on-device resource demands and easing
cross-platform portability. A feasibility-oriented evaluation shows an average
execution time of 9.15\,s, demonstrating the practicality of our approach. The
source code of PrivScan is available at https://github.com/buyanghc/PrivScan
and the demo video can be found at https://www.youtube.com/watch?v=ck-25otfyHc.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.22857v1">PAPER: Privacy-Preserving ResNet Models using Low-Degree Polynomial
  Approximations and Structural Optimizations on Leveled FHE</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-26T19:10:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Eduardo Chielle, Manaar Alam, Jinting Liu, Jovan Kascelan, Michail Maniatakos</p>
    <p><b>Summary:</b> Recent work has made non-interactive privacy-preserving inference more
practical by running deep Convolution Neural Network (CNN) with Fully
Homomorphic Encryption (FHE). However, these methods remain limited by their
reliance on bootstrapping, a costly FHE operation applied across multiple
layers, severely slowing inference. They also depend on high-degree polynomial
approximations of non-linear activations, which increase multiplicative depth
and reduce accuracy by 2-5% compared to plaintext ReLU models. In this work, we
focus on ResNets, a widely adopted benchmark architecture in privacy-preserving
inference, and close the accuracy gap between their FHE-based non-interactive
models and plaintext counterparts, while also achieving faster inference than
existing methods. We use a quadratic polynomial approximation of ReLU, which
achieves the theoretical minimum multiplicative depth for non-linear
activations, along with a penalty-based training strategy. We further introduce
structural optimizations such as node fusing, weight redistribution, and tower
reuse. These optimizations reduce the required FHE levels in CNNs by nearly a
factor of five compared to prior work, allowing us to run ResNet models under
leveled FHE without bootstrapping. To further accelerate inference and recover
accuracy typically lost with polynomial approximations, we introduce parameter
clustering along with a joint strategy of data encoding layout and ensemble
techniques. Experiments with ResNet-18, ResNet-20, and ResNet-32 on CIFAR-10
and CIFAR-100 show that our approach achieves up to 4x faster private inference
than prior work with comparable accuracy to plaintext ReLU models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.22428v1">Privacy Mechanism Design based on Empirical Distributions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-09-26T14:46:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leonhard Grosse, Sara Saeidian, Mikael Skoglund, Tobias J. Oechtering</p>
    <p><b>Summary:</b> Pointwise maximal leakage (PML) is a per-outcome privacy measure based on
threat models from quantitative information flow. Privacy guarantees with PML
rely on knowledge about the distribution that generated the private data. In
this work, we propose a framework for PML privacy assessment and mechanism
design with empirical estimates of this data-generating distribution. By
extending the PML framework to consider sets of data-generating distributions,
we arrive at bounds on the worst-case leakage within a given set. We use these
bounds alongside large-deviation bounds from the literature to provide a method
for obtaining distribution-independent $(\varepsilon,\delta)$-PML guarantees
when the data-generating distribution is estimated from available data samples.
We provide an optimal binary mechanism, and show that mechanism design with
this type of uncertainty about the data-generating distribution reduces to a
linearly constrained convex program. Further, we show that optimal mechanisms
designed for a distribution estimate can be used. Finally, we apply these tools
to leakage assessment of the Laplace mechanism and the Gaussian mechanism for
binary private data, and numerically show that the presented approach to
mechanism design can yield significant utility increase compared to local
differential privacy, while retaining similar privacy guarantees.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.22213v1">Accuracy-First Rényi Differential Privacy and Post-Processing Immunity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-26T11:27:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ossi Räisä, Antti Koskela, Antti Honkela</p>
    <p><b>Summary:</b> The accuracy-first perspective of differential privacy addresses an important
shortcoming by allowing a data analyst to adaptively adjust the quantitative
privacy bound instead of sticking to a predetermined bound. Existing works on
the accuracy-first perspective have neglected an important property of
differential privacy known as post-processing immunity, which ensures that an
adversary is not able to weaken the privacy guarantee by post-processing. We
address this gap by determining which existing definitions in the
accuracy-first perspective have post-processing immunity, and which do not. The
only definition with post-processing immunity, pure ex-post privacy, lacks
useful tools for practical problems, such as an ex-post analogue of the
Gaussian mechanism, and an algorithm to check if accuracy on separate private
validation set is high enough. To address this, we propose a new definition
based on R\'enyi differential privacy that has post-processing immunity, and we
develop basic theory and tools needed for practical applications. We
demonstrate the practicality of our theory with an application to synthetic
data generation, where our algorithm successfully adjusts the privacy bound
until an accuracy threshold is met on a private validation dataset.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.22103v1">Privacy in Distributed Quantum Sensing with Gaussian Quantum Networks</a></h3>
   
  <p><b>Published on:</b> 2025-09-26T09:24:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Uesli Alushi, Roberto Di Candia</p>
    <p><b>Summary:</b> We study the privacy properties of distributed quantum sensing protocols in a
Gaussian quantum network, where each node encodes a parameter via a local phase
shift. For networks with more than two nodes, achieving perfect privacy is
possible only asymptotically, in the limit of large photon numbers. However, we
show that optimized fully symmetric Gaussian states enable rapidly approaching
perfect privacy while maintaining near-optimal sensing performance. We show
that local homodyne detection achieves a quadratic scaling of precision with
the total number of photons. We further analyze the impact of thermal noise in
the preparation stage on both privacy and estimation precision. Our results
pave the way for the development of practical, private distributed quantum
sensing protocols in continuous-variable quantum networks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.21800v1">Federated Learning of Quantile Inference under Local Differential
  Privacy</a></h3>
   
  <p><b>Published on:</b> 2025-09-26T02:56:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leheng Cai, Qirui Hu, Shuyuan Wu</p>
    <p><b>Summary:</b> In this paper, we investigate federated learning for quantile inference under
local differential privacy (LDP). We propose an estimator based on local
stochastic gradient descent (SGD), whose local gradients are perturbed via a
randomized mechanism with global parameters, making the procedure tolerant of
communication and storage constraints without compromising statistical
efficiency. Although the quantile loss and its corresponding gradient do not
satisfy standard smoothness conditions typically assumed in existing
literature, we establish asymptotic normality for our estimator as well as a
functional central limit theorem. The proposed method accommodates data
heterogeneity and allows each server to operate with an individual privacy
budget. Furthermore, we construct confidence intervals for the target value
through a self-normalization approach, thereby circumventing the need to
estimate additional nuisance parameters. Extensive numerical experiments and
real data application validate the theoretical guarantees of the proposed
methodology.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.21762v1">Privacy-Preserving Performance Profiling of In-The-Wild GPUs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Hardware Architecture-04E762">
  <p><b>Published on:</b> 2025-09-26T01:49:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ian McDougall, Michael Davies, Rahul Chatterjee, Somesh Jha, Karthikeyan Sankaralingam</p>
    <p><b>Summary:</b> GPUs are the dominant platform for many important applications today
including deep learning, accelerated computing, and scientific simulation.
However, as the complexity of both applications and hardware increases, GPU
chip manufacturers face a significant challenge: how to gather comprehensive
performance characteristics and value profiles from GPUs deployed in real-world
scenarios. Such data, encompassing the types of kernels executed and the time
spent in each, is crucial for optimizing chip design and enhancing application
performance. Unfortunately, despite the availability of low-level tools like
NSYS and NCU, current methodologies fall short, offering data collection
capabilities only on an individual user basis rather than a broader, more
informative fleet-wide scale. This paper takes on the problem of realizing a
system that allows planet-scale real-time GPU performance profiling of
low-level hardware characteristics. The three fundamental problems we solve
are: i) user experience of achieving this with no slowdown; ii) preserving user
privacy, so that no 3rd party is aware of what applications any user runs; iii)
efficacy in showing we are able to collect data and assign it applications even
when run on 1000s of GPUs. Our results simulate a 100,000 size GPU deployment,
running applications from the Torchbench suite, showing our system addresses
all 3 problems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.21712v1">Not My Agent, Not My Boundary? Elicitation of Personal Privacy
  Boundaries in AI-Delegated Information Sharing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-26T00:20:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bingcan Guo, Eryue Xu, Zhiping Zhang, Tianshi Li</p>
    <p><b>Summary:</b> Aligning AI systems with human privacy preferences requires understanding
individuals' nuanced disclosure behaviors beyond general norms. Yet eliciting
such boundaries remains challenging due to the context-dependent nature of
privacy decisions and the complex trade-offs involved. We present an AI-powered
elicitation approach that probes individuals' privacy boundaries through a
discriminative task. We conducted a between-subjects study that systematically
varied communication roles and delegation conditions, resulting in 1,681
boundary specifications from 169 participants for 61 scenarios. We examined how
these contextual factors and individual differences influence the boundary
specification. Quantitative results show that communication roles influence
individuals' acceptance of detailed and identifiable disclosure, AI delegation
and individuals' need for privacy heighten sensitivity to disclosed
identifiers, and AI delegation results in less consensus across individuals.
Our findings highlight the importance of situating privacy preference
elicitation within real-world data flows. We advocate using nuanced privacy
boundaries as an alignment goal for future AI systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.21704v1">PQFed: A Privacy-Preserving Quality-Controlled Federated Learning
  Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-25T23:56:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weiqi Yue, Wenbiao Li, Yuzhou Jiang, Anisa Halimi, Roger French, Erman Ayday</p>
    <p><b>Summary:</b> Federated learning enables collaborative model training without sharing raw
data, but data heterogeneity consistently challenges the performance of the
global model. Traditional optimization methods often rely on collaborative
global model training involving all clients, followed by local adaptation to
improve individual performance. In this work, we focus on early-stage quality
control and propose PQFed, a novel privacy-preserving personalized federated
learning framework that designs customized training strategies for each client
prior to the federated training process. PQFed extracts representative features
from each client's raw data and applies clustering techniques to estimate
inter-client dataset similarity. Based on these similarity estimates, the
framework implements a client selection strategy that enables each client to
collaborate with others who have compatible data distributions. We evaluate
PQFed on two benchmark datasets, CIFAR-10 and MNIST, integrated with three
existing federated learning algorithms. Experimental results show that PQFed
consistently improves the target client's model performance, even with a
limited number of participants. We further benchmark PQFed against a baseline
cluster-based algorithm, IFCA, and observe that PQFed also achieves better
performance in low-participation scenarios. These findings highlight PQFed's
scalability and effectiveness in personalized federated learning settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.02325v1">Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP
  Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> 
  <p><b>Published on:</b> 2025-09-25T21:25:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammed A. Shehab</p>
    <p><b>Summary:</b> This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual,
and explainable research prototype developed as a single-investigator project.
The system leverages the emerging Model Context Protocol (MCP) to orchestrate
multiple intelligent agents for patient interaction, including symptom
checking, medication suggestions, and appointment scheduling. The platform
integrates a dedicated Privacy and Compliance Layer that applies role-based
access control (RBAC), AES-GCM field-level encryption, and tamper-evident audit
logging, aligning with major healthcare data protection standards such as HIPAA
(US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate
multilingual patient-doctor interaction (English, French, Arabic) and
transparent diagnostic reasoning powered by large language models. As an
applied AI contribution, this work highlights the feasibility of combining
agentic orchestration, multilingual accessibility, and compliance-aware
architecture in healthcare applications. This platform is presented as a
research prototype and is not a certified medical device.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20867v1">Federated Markov Imputation: Privacy-Preserving Temporal Imputation in
  Multi-Centric ICU Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-25T08:00:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Christoph Düsing, Philipp Cimiano</p>
    <p><b>Summary:</b> Missing data is a persistent challenge in federated learning on electronic
health records, particularly when institutions collect time-series data at
varying temporal granularities. To address this, we propose Federated Markov
Imputation (FMI), a privacy-preserving method that enables Intensive Care Units
(ICUs) to collaboratively build global transition models for temporal
imputation. We evaluate FMI on a real-world sepsis onset prediction task using
the MIMIC-IV dataset and show that it outperforms local imputation baselines,
especially in scenarios with irregular sampling intervals across ICUs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20838v1">Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-09-25T07:23:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuo Huang, Xingliang Yuan, Gholamreza Haffari, Lizhen Qu</p>
    <p><b>Summary:</b> The increasing adoption of large language models (LLMs) in cloud-based
services has raised significant privacy concerns, as user inputs may
inadvertently expose sensitive information. Existing text anonymization and
de-identification techniques, such as rule-based redaction and scrubbing, often
struggle to balance privacy preservation with text naturalness and utility. In
this work, we propose a zero-shot, tree-search-based iterative sentence
rewriting algorithm that systematically obfuscates or deletes private
information while preserving coherence, relevance, and naturalness. Our method
incrementally rewrites privacy-sensitive segments through a structured search
guided by a reward model, enabling dynamic exploration of the rewriting space.
Experiments on privacy-sensitive datasets show that our approach significantly
outperforms existing baselines, achieving a superior balance between privacy
protection and utility preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20460v2">Differential Privacy of Network Parameters from a System Identification
  Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-09-24T18:06:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Andrew Campbell, Anna Scaglione, Hang Liu, Victor Elvira, Sean Peisert, Daniel Arnold</p>
    <p><b>Summary:</b> This paper addresses the problem of protecting network information from
privacy system identification (SI) attacks when sharing cyber-physical system
simulations. We model analyst observations of networked states as time-series
outputs of a graph filter driven by differentially private (DP) nodal
excitations, with the analyst aiming to infer the underlying graph shift
operator (GSO). Unlike traditional SI, which estimates system parameters, we
study the inverse problem: what assumptions prevent adversaries from
identifying the GSO while preserving utility for legitimate analysis. We show
that applying DP mechanisms to inputs provides formal privacy guarantees for
the GSO, linking the $(\epsilon,\delta)$-DP bound to the spectral properties of
the graph filter and noise covariance. More precisely, for DP Gaussian signals,
the spectral characteristics of both the filter and noise covariance determine
the privacy bound, with smooth filters and low-condition-number covariance
yielding greater privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20454v1">Bridging Privacy and Utility: Synthesizing anonymized EEG with
  constraining utility functions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-24T18:02:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kay Fuhrmeister, Arne Pelzer, Fabian Radke, Julia Lechinger, Mahzad Gharleghi, Thomas Köllmer, Insa Wolf</p>
    <p><b>Summary:</b> Electroencephalography (EEG) is widely used for recording brain activity and
has seen numerous applications in machine learning, such as detecting sleep
stages and neurological disorders. Several studies have successfully shown the
potential of EEG data for re-identification and leakage of other personal
information. Therefore, the increasing availability of EEG consumer devices
raises concerns about user privacy, motivating us to investigate how to
safeguard this sensitive data while retaining its utility for EEG applications.
To address this challenge, we propose a transformer-based autoencoder to create
EEG data that does not allow for subject re-identification while still
retaining its utility for specific machine learning tasks. We apply our
approach to automatic sleep staging by evaluating the re-identification and
utility potential of EEG data before and after anonymization. The results show
that the re-identifiability of the EEG signal can be substantially reduced
while preserving its utility for machine learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20324v1">RAG Security and Privacy: Formalizing the Threat Model and Attack
  Surface</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-24T17:11:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Atousa Arzanipour, Rouzbeh Behnia, Reza Ebrahimi, Kaushik Dutta</p>
    <p><b>Summary:</b> Retrieval-Augmented Generation (RAG) is an emerging approach in natural
language processing that combines large language models (LLMs) with external
document retrieval to produce more accurate and grounded responses. While RAG
has shown strong potential in reducing hallucinations and improving factual
consistency, it also introduces new privacy and security challenges that differ
from those faced by traditional LLMs. Existing research has demonstrated that
LLMs can leak sensitive information through training data memorization or
adversarial prompts, and RAG systems inherit many of these vulnerabilities. At
the same time, reliance of RAG on an external knowledge base opens new attack
surfaces, including the potential for leaking information about the presence or
content of retrieved documents, or for injecting malicious content to
manipulate model behavior. Despite these risks, there is currently no formal
framework that defines the threat landscape for RAG systems. In this paper, we
address a critical gap in the literature by proposing, to the best of our
knowledge, the first formal threat model for retrieval-RAG systems. We
introduce a structured taxonomy of adversary types based on their access to
model components and data, and we formally define key threat vectors such as
document-level membership inference and data poisoning, which pose serious
privacy and integrity risks in real-world deployments. By establishing formal
definitions and attack models, our work lays the foundation for a more rigorous
and principled understanding of privacy and security in RAG systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20283v1">Monitoring Violations of Differential Privacy over Time</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36">  
  <p><b>Published on:</b> 2025-09-24T16:15:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Önder Askin, Tim Kutta, Holger Dette</p>
    <p><b>Summary:</b> Auditing differential privacy has emerged as an important area of research
that supports the design of privacy-preserving mechanisms. Privacy audits help
to obtain empirical estimates of the privacy parameter, to expose flawed
implementations of algorithms and to compare practical with theoretical privacy
guarantees. In this work, we investigate an unexplored facet of privacy
auditing: the sustained auditing of a mechanism that can go through changes
during its development or deployment. Monitoring the privacy of algorithms over
time comes with specific challenges. Running state-of-the-art (static) auditors
repeatedly requires excessive sampling efforts, while the reliability of such
methods deteriorates over time without proper adjustments. To overcome these
obstacles, we present a new monitoring procedure that extracts information from
the entire deployment history of the algorithm. This allows us to reduce
sampling efforts, while sustaining reliable outcomes of our auditor. We derive
formal guarantees with regard to the soundness of our methods and evaluate
their performance for important mechanisms from the literature. Our theoretical
findings and experiments demonstrate the efficacy of our approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20153v2">Affective Computing and Emotional Data: Challenges and Implications in
  Privacy Regulations, The AI Act, and Ethics in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-24T14:18:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicola Fabiano</p>
    <p><b>Summary:</b> This paper examines the integration of emotional intelligence into artificial
intelligence systems, with a focus on affective computing and the growing
capabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to
recognize and respond to human emotions. Drawing on interdisciplinary research
that combines computer science, psychology, and neuroscience, the study
analyzes foundational neural architectures - CNNs for processing facial
expressions and RNNs for sequential data, such as speech and text - that enable
emotion recognition. It examines the transformation of human emotional
experiences into structured emotional data, addressing the distinction between
explicit emotional data collected with informed consent in research settings
and implicit data gathered passively through everyday digital interactions.
That raises critical concerns about lawful processing, AI transparency, and
individual autonomy over emotional expressions in digital environments. The
paper explores implications across various domains, including healthcare,
education, and customer service, while addressing challenges of cultural
variations in emotional expression and potential biases in emotion recognition
systems across different demographic groups. From a regulatory perspective, the
paper examines emotional data in the context of the GDPR and the EU AI Act
frameworks, highlighting how emotional data may be considered sensitive
personal data that requires robust safeguards, including purpose limitation,
data minimization, and meaningful consent mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20024v1">Generative Adversarial Networks Applied for Privacy Preservation in
  Biometric-Based Authentication and Identification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-24T11:39:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lubos Mjachky, Ivan Homoliak</p>
    <p><b>Summary:</b> Biometric-based authentication systems are getting broadly adopted in many
areas. However, these systems do not allow participating users to influence the
way their data is used. Furthermore, the data may leak and can be misused
without the users' knowledge. In this paper, we propose a new authentication
method that preserves the privacy of individuals and is based on a generative
adversarial network (GAN). Concretely, we suggest using the GAN for translating
images of faces to a visually private domain (e.g., flowers or shoes).
Classifiers, which are used for authentication purposes, are then trained on
the images from the visually private domain. Based on our experiments, the
method is robust against attacks and still provides meaningful utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19925v1">CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-24T09:29:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ajeet Kumar Singh, Rajsabi Surya, Anurag Tripathi, Santanu Choudhury, Sudhir Bisane</p>
    <p><b>Summary:</b> As enterprises increasingly integrate cloud-based large language models
(LLMs) such as ChatGPT and Gemini into their legal document workflows,
protecting sensitive contractual information - including Personally
Identifiable Information (PII) and commercially sensitive clauses - has emerged
as a critical challenge. In this work, we propose CON-QA, a hybrid
privacy-preserving framework designed specifically for secure question
answering over enterprise contracts, effectively combining local and
cloud-hosted LLMs. The CON-QA framework operates through three stages: (i)
semantic query decomposition and query-aware document chunk retrieval using a
locally deployed LLM analysis, (ii) anonymization of detected sensitive
entities via a structured one-to-many mapping scheme, ensuring semantic
coherence while preventing cross-session entity inference attacks, and (iii)
anonymized response generation by a cloud-based LLM, with accurate
reconstruction of the original answer locally using a session-consistent
many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce
CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world
CUAD contract documents, encompassing simple, complex, and summarization-style
queries. Empirical evaluations, complemented by detailed human assessments,
confirm that CON-QA effectively maintains both privacy and utility, preserves
answer quality, maintains fidelity to legal clause semantics, and significantly
mitigates privacy risks, demonstrating its practical suitability for secure,
enterprise-level contract documents.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19906v1">Voice Privacy Preservation with Multiple Random Orthogonal Secret Keys:
  Attack Resistance Analysis</a></h3>
  
  <p><b>Published on:</b> 2025-09-24T09:05:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kohei Tanaka, Hitoshi Kiya, Sayaka Shiota</p>
    <p><b>Summary:</b> Recently, opportunities to transmit speech data to deep learning models
executed in the cloud have increased. This has led to growing concerns about
speech privacy, including both speaker-specific information and the linguistic
content of utterances. As an approach to preserving speech privacy, a speech
privacy-preserving method based on encryption using a secret key with a random
orthogonal matrix has been proposed. This method enables cloud-based model
inference while concealing both the speech content and the speaker identity.
However, the method has limited attack resistance and is constrained in terms
of the deep learning models to which the encryption can be applied. In this
work, we propose a method that enhances the attack resistance of the
conventional speech privacy-preserving technique by employing multiple random
orthogonal matrices as secret keys. We also introduce approaches to relax the
model constraints, enabling the application of our method to a broader range of
deep learning models. Furthermore, we investigate the robustness of the
proposed method against attacks using extended attack scenarios based on the
scenarios employed in the Voice Privacy Challenge. Our experimental results
confirmed that the proposed method maintains privacy protection performance for
speaker concealment, even under more powerful attack scenarios not considered
in prior work.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19661v1">Consistent Estimation of Numerical Distributions under Local
  Differential Privacy by Wavelet Expansion</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-24T00:37:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Puning Zhao, Zhikun Zhang, Bo Sun, Li Shen, Liang Zhang, Shaowei Wang, Zhe Liu</p>
    <p><b>Summary:</b> Distribution estimation under local differential privacy (LDP) is a
fundamental and challenging task. Significant progresses have been made on
categorical data. However, due to different evaluation metrics, these methods
do not work well when transferred to numerical data. In particular, we need to
prevent the probability mass from being misplaced far away. In this paper, we
propose a new approach that express the sample distribution using wavelet
expansions. The coefficients of wavelet series are estimated under LDP. Our
method prioritizes the estimation of low-order coefficients, in order to ensure
accurate estimation at macroscopic level. Therefore, the probability mass is
prevented from being misplaced too far away from its ground truth. We establish
theoretical guarantees for our methods. Experiments show that our wavelet
expansion method significantly outperforms existing solutions under Wasserstein
and KS distances.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19599v1">Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method
  for Multi-Agent Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-23T21:46:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Danilo Trombino, Vincenzo Pecorella, Alessandro de Giulii, Davide Tresoldi</p>
    <p><b>Summary:</b> Multi-agent systems (MAS) are increasingly tasked with solving complex,
knowledge-intensive problems where effective agent orchestration is critical.
Conventional orchestration methods rely on static agent descriptions, which
often become outdated or incomplete. This limitation leads to inefficient task
routing, particularly in dynamic environments where agent capabilities
continuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a
novel approach that augments static descriptions with dynamic,
privacy-preserving relevance signals derived from each agent's internal
knowledge base (KB). In the proposed framework, when static descriptions are
insufficient for a clear routing decision, the orchestrator prompts the
subagents in parallel. Each agent then assesses the task's relevance against
its private KB, returning a lightweight ACK signal without exposing the
underlying data. These collected signals populate a shared semantic cache,
providing dynamic indicators of agent suitability for future queries. By
combining this novel mechanism with static descriptions, our method achieves
more accurate and adaptive task routing preserving agent autonomy and data
confidentiality. Benchmarks show that our KBA Orchestration significantly
outperforms static description-driven methods in routing precision and overall
system efficiency, making it suitable for large-scale systems that require
higher accuracy than standard description-driven routing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19041v1">Position: Human-Robot Interaction in Embodied Intelligence Demands a
  Shift From Static Privacy Controls to Dynamic Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-23T14:10:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuning Zhang, Hong Jia, Simin Li, Ting Dang, Yongquan `Owen' Hu, Xin Yi, Hewu Li</p>
    <p><b>Summary:</b> The reasoning capabilities of embodied agents introduce a critical,
under-explored inferential privacy challenge, where the risk of an agent
generate sensitive conclusions from ambient data. This capability creates a
fundamental tension between an agent's utility and user privacy, rendering
traditional static controls ineffective. To address this, this position paper
proposes a framework that reframes privacy as a dynamic learning problem
grounded in theory of Contextual Integrity (CI). Our approach enables agents to
proactively learn and adapt to individual privacy norms through interaction,
outlining a research agenda to develop embodied agents that are both capable
and function as trustworthy safeguards of user privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18949v1">Towards Privacy-Aware Bayesian Networks: A Credal Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-23T12:58:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Niccolò Rocchi, Fabio Stella, Cassio de Campos</p>
    <p><b>Summary:</b> Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18871v2">Uncovering Privacy Vulnerabilities through Analytical Gradient Inversion
  Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-23T10:10:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tamer Ahmed Eltaras, Qutaibah Malluhi, Alessandro Savino, Stefano Di Carlo, Adnan Qayyum</p>
    <p><b>Summary:</b> Federated learning has emerged as a prominent privacy-preserving technique
for leveraging large-scale distributed datasets by sharing gradients instead of
raw data. However, recent studies indicate that private training data can still
be exposed through gradient inversion attacks. While earlier analytical methods
have demonstrated success in reconstructing input data from fully connected
layers, their effectiveness significantly diminishes when applied to
convolutional layers, high-dimensional inputs, and scenarios involving multiple
training examples. This paper extends our previous work \cite{eltaras2024r} and
proposes three advanced algorithms to broaden the applicability of gradient
inversion attacks. The first algorithm presents a novel data leakage method
that efficiently exploits convolutional layer gradients, demonstrating that
even with non-fully invertible activation functions, such as ReLU, training
samples can be analytically reconstructed directly from gradients without the
need to reconstruct intermediate layer outputs. Building on this foundation,
the second algorithm extends this analytical approach to support
high-dimensional input data, substantially enhancing its utility across complex
real-world datasets. The third algorithm introduces an innovative analytical
method for reconstructing mini-batches, addressing a critical gap in current
research that predominantly focuses on reconstructing only a single training
example. Unlike previous studies that focused mainly on the weight constraints
of convolutional layers, our approach emphasizes the pivotal role of gradient
constraints, revealing that successful attacks can be executed with fewer than
5\% of the constraints previously deemed necessary in certain layers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18696v1">FlowCrypt: Flow-Based Lightweight Encryption with Near-Lossless Recovery
  for Cloud Photo Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-23T06:25:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaohui Yang, Ping Ping, Feng Xu</p>
    <p><b>Summary:</b> The widespread adoption of smartphone photography has led users to
increasingly rely on cloud storage for personal photo archiving and sharing,
raising critical privacy concerns. Existing deep learning-based image
encryption schemes, typically built upon CNNs or GANs, often depend on
traditional cryptographic algorithms and lack inherent architectural
reversibility, resulting in limited recovery quality and poor robustness.
Invertible neural networks (INNs) have emerged to address this issue by
enabling reversible transformations, yet the first INN-based encryption scheme
still relies on an auxiliary reference image and discards by-product
information before decryption, leading to degraded recovery and limited
practicality. To address these limitations, this paper proposes FlowCrypt, a
novel flow-based image encryption framework that simultaneously achieves
near-lossless recovery, high security, and lightweight model design. FlowCrypt
begins by applying a key-conditioned random split to the input image, enhancing
forward-process randomness and encryption strength. The resulting components
are processed through a Flow-based Encryption/Decryption (FED) module composed
of invertible blocks, which share parameters across encryption and decryption.
Thanks to its reversible architecture and reference-free design, FlowCrypt
ensures high-fidelity image recovery. Extensive experiments show that FlowCrypt
achieves recovery quality with 100dB on three datasets, produces uniformly
distributed cipher images, and maintains a compact architecture with only 1M
parameters, making it suitable for mobile and edge-device applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20388v1">Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-22T21:45:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amir AL-Maamari</p>
    <p><b>Summary:</b> The rapid integration of AI-powered coding assistants into developer
workflows has raised significant privacy and trust concerns. As developers
entrust proprietary code to services like OpenAI's GPT, Google's Gemini, and
GitHub Copilot, the unclear data handling practices of these tools create
security and compliance risks. This paper addresses this challenge by
introducing and applying a novel, expert-validated privacy scorecard. The
methodology involves a detailed analysis of four document types; from legal
policies to external audits; to score five leading assistants against 14
weighted criteria. A legal expert and a data protection officer refined these
criteria and their weighting. The results reveal a distinct hierarchy of
privacy protections, with a 20-point gap between the highest- and lowest-ranked
tools. The analysis uncovers common industry weaknesses, including the
pervasive use of opt-out consent for model training and a near-universal
failure to filter secrets from user prompts proactively. The resulting
scorecard provides actionable guidance for developers and organizations,
enabling evidence-based tool selection. This work establishes a new benchmark
for transparency and advocates for a shift towards more user-centric privacy
standards in the AI industry.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18413v1">VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership
  Inference Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-22T20:57:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Efthymios Tsaprazlis, Thanathai Lertpetchpun, Tiantian Feng, Sai Praneeth Karimireddy, Shrikanth Narayanan</p>
    <p><b>Summary:</b> Voice anonymization aims to conceal speaker identity and attributes while
preserving intelligibility, but current evaluations rely almost exclusively on
Equal Error Rate (EER) that obscures whether adversaries can mount
high-precision attacks. We argue that privacy should instead be evaluated in
the low false-positive rate (FPR) regime, where even a small number of
successful identifications constitutes a meaningful breach. To this end, we
introduce VoxGuard, a framework grounded in differential privacy and membership
inference that formalizes two complementary notions: User Privacy, preventing
speaker re-identification, and Attribute Privacy, protecting sensitive traits
such as gender and accent. Across synthetic and real datasets, we find that
informed adversaries, especially those using fine-tuned models and
max-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR
despite similar EER. For attributes, we show that simple transparent attacks
recover gender and accent with near-perfect accuracy even after anonymization.
Our results demonstrate that EER substantially underestimates leakage,
highlighting the need for low-FPR evaluation, and recommend VoxGuard as a
benchmark for evaluating privacy leakage.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18311v1">Fine-Tuning Robot Policies While Maintaining User Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E">
  <p><b>Published on:</b> 2025-09-22T18:36:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Benjamin A. Christie, Sagar Parekh, Dylan P. Losey</p>
    <p><b>Summary:</b> Recent works introduce general-purpose robot policies. These policies provide
a strong prior over how robots should behave -- e.g., how a robot arm should
manipulate food items. But in order for robots to match an individual person's
needs, users typically fine-tune these generalized policies -- e.g., showing
the robot arm how to make their own preferred dinners. Importantly, during the
process of personalizing robots, end-users leak data about their preferences,
habits, and styles (e.g., the foods they prefer to eat). Other agents can
simply roll-out the fine-tuned policy and see these personally-trained
behaviors. This leads to a fundamental challenge: how can we develop robots
that personalize actions while keeping learning private from external agents?
We here explore this emerging topic in human-robot interaction and develop
PRoP, a model-agnostic framework for personalized and private robot policies.
Our core idea is to equip each user with a unique key; this key is then used to
mathematically transform the weights of the robot's network. With the correct
key, the robot's policy switches to match that user's preferences -- but with
incorrect keys, the robot reverts to its baseline behaviors. We show the
general applicability of our method across multiple model types in imitation
learning, reinforcement learning, and classification tasks. PRoP is practically
advantageous because it retains the architecture and behaviors of the original
policy, and experimentally outperforms existing encoder-based approaches. See
videos and code here: https://prop-icra26.github.io.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18014v1">Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data
  Synthesis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-09-22T16:53:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Joshua Ward, Xiaofeng Lin, Chi-Hua Wang, Guang Cheng</p>
    <p><b>Summary:</b> Tabular Generative Models are often argued to preserve privacy by creating
synthetic datasets that resemble training data. However, auditing their
empirical privacy remains challenging, as commonly used similarity metrics fail
to effectively characterize privacy risk. Membership Inference Attacks (MIAs)
have recently emerged as a method for evaluating privacy leakage in synthetic
data, but their practical effectiveness is limited. Numerous attacks exist
across different threat models, each with distinct implementations targeting
various sources of privacy leakage, making them difficult to apply
consistently. Moreover, no single attack consistently outperforms the others,
leading to a routine underestimation of privacy risk.
  To address these issues, we propose a unified, model-agnostic threat
framework that deploys a collection of attacks to estimate the maximum
empirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an
open-source Python library that streamlines this auditing process through a
novel testbed that integrates seamlessly into existing synthetic data
evaluation pipelines through a Scikit-Learn-like API. Our software implements
13 attack methods through a Scikit-Learn-like API, designed to enable fast
systematic estimation of privacy leakage for practitioners as well as
facilitate the development of new attacks and experiments for researchers.
  We demonstrate our framework's utility in the largest tabular synthesis
privacy benchmark to date, revealing that higher synthetic data quality
corresponds to greater privacy leakage, that similarity-based privacy metrics
show weak correlation with MIA results, and that the differentially private
generator PATEGAN can fail to preserve privacy under such attacks. This
underscores the necessity of MIA-based auditing when designing and deploying
Tabular Generative Models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.17871v2">B-Privacy: Defining and Enforcing Privacy in Weighted Voting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-22T15:11:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Samuel Breckenridge, Dani Vilardell, Andrés Fábrega, Amy Zhao, Patrick McCorry, Rafael Solari, Ari Juels</p>
    <p><b>Summary:</b> In traditional, one-vote-per-person voting systems, privacy equates with
ballot secrecy: voting tallies are published, but individual voters' choices
are concealed.
  Voting systems that weight votes in proportion to token holdings, though, are
now prevalent in cryptocurrency and web3 systems. We show that these
weighted-voting systems overturn existing notions of voter privacy. Our
experiments demonstrate that even with secret ballots, publishing raw tallies
often reveals voters' choices.
  Weighted voting thus requires a new framework for privacy. We introduce a
notion called B-privacy whose basis is bribery, a key problem in voting systems
today. B-privacy captures the economic cost to an adversary of bribing voters
based on revealed voting tallies.
  We propose a mechanism to boost B-privacy by noising voting tallies. We prove
bounds on its tradeoff between B-privacy and transparency, meaning
reported-tally accuracy. Analyzing 3,582 proposals across 30 Decentralized
Autonomous Organizations (DAOs), we find that the prevalence of large voters
("whales") limits the effectiveness of any B-Privacy-enhancing technique.
However, our mechanism proves to be effective in cases without extreme voting
weight concentration: among proposals requiring coalitions of $\geq5$ voters to
flip outcomes, our mechanism raises B-privacy by a geometric mean factor of
$4.1\times$.
  Our work offers the first principled guidance on transparency-privacy
tradeoffs in weighted-voting systems, complementing existing approaches that
focus on ballot secrecy and revealing fundamental constraints that voting
weight concentration imposes on privacy mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.17488v1">Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation
  for LLM-Powered Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-22T08:19:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shouju Wang, Fenglin Yu, Xirui Liu, Xiaoting Qin, Jue Zhang, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan</p>
    <p><b>Summary:</b> The increasing autonomy of LLM agents in handling sensitive communications,
accelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A)
frameworks, creates urgent privacy challenges. While recent work reveals
significant gaps between LLMs' privacy Q&A performance and their agent
behavior, existing benchmarks remain limited to static, simplified scenarios.
We present PrivacyChecker, a model-agnostic, contextual integrity based
mitigation approach that effectively reduces privacy leakage from 36.08% to
7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving
task helpfulness. We also introduce PrivacyLens-Live, transforming static
benchmarks into dynamic MCP and A2A environments that reveal substantially
higher privacy risks in practical. Our modular mitigation approach integrates
seamlessly into agent protocols through three deployment strategies, providing
practical privacy protection for the emerging agentic ecosystem. Our data and
code will be made available at https://aka.ms/privacy_in_action.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.17266v1">Privacy-Preserving State Estimation with Crowd Sensors: An
  Information-Theoretic Respective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">  
  <p><b>Published on:</b> 2025-09-21T22:44:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Farhad Farokhi</p>
    <p><b>Summary:</b> Privacy-preserving state estimation for linear time-invariant dynamical
systems with crowd sensors is considered. At any time step, the estimator has
access to measurements from a randomly selected sensor from a pool of sensors
with pre-specified models and noise profiles. A Luenberger-like observer is
used to fuse the measurements with the underlying model of the system to
recursively generate the state estimates. An additive privacy-preserving noise
is used to constrain information leakage. Information leakage is measured via
mutual information between the identity of the sensors and the state estimate
conditioned on the actual state of the system. This captures an omnipotent
adversary that not only can access state estimates but can also gather direct
high-quality state measurements. Any prescribed level of information leakage is
shown to be achievable by appropriately selecting the variance of the
privacy-preserving noise. Therefore, privacy-utility trade-off can be
fine-tuned.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.16962v1">Temporal Drift in Privacy Recall: Users Misremember From Verbatim Loss
  to Gist-Based Overexposure Over Time</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-21T07:50:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoze Guo, Ziqi Wei</p>
    <p><b>Summary:</b> With social media content traversing the different platforms, occasionally
resurfacing after periods of time, users are increasingly prone to unintended
disclosure resulting from a misremembered acceptance of privacy. Context
collapse and interface cues are two factors considered by prior researchers,
yet we know less about how time-lapse basically alters recall of past audiences
destined for exposure. Likewise, the design space for mitigating this temporal
exposure risk remains underexplored. Our work theorizes temporal drift in
privacy recall as verbatim memory of prior settings blowing apart and
eventually settling with gist-based heuristics, which more often than not
select an audience larger than the original one. Grounded in memory research,
contextual integrity, and usable privacy, we examine why such a drift occurs,
why it tends to bias toward broader sharing, and how it compounds upon repeat
exposure. Following that, we suggest provenance-forward interface schemes and a
risk-based evaluation framework that mutates recall into recognition. The merit
of our work lies in establishing a temporal awareness of privacy design as an
essential safety rail against inadvertent overexposure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.16915v1">Differential Privacy for Euclidean Jordan Algebra with Applications to
  Private Symmetric Cone Programming</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-21T04:34:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhao Song, Jianfei Xue, Lichen Zhang</p>
    <p><b>Summary:</b> In this paper, we study differentially private mechanisms for functions whose
outputs lie in a Euclidean Jordan algebra. Euclidean Jordan algebras capture
many important mathematical structures and form the foundation of linear
programming, second-order cone programming, and semidefinite programming. Our
main contribution is a generic Gaussian mechanism for such functions, with
sensitivity measured in $\ell_2$, $\ell_1$, and $\ell_\infty$ norms. Notably,
this framework includes the important case where the function outputs are
symmetric matrices, and sensitivity is measured in the Frobenius, nuclear, or
spectral norm. We further derive private algorithms for solving symmetric cone
programs under various settings, using a combination of the multiplicative
weights update method and our generic Gaussian mechanism. As an application, we
present differentially private algorithms for semidefinite programming,
resolving a major open question posed by [Hsu, Roth, Roughgarden, and Ullman,
ICALP 2014].</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.25205v1">Polynomial Contrastive Learning for Privacy-Preserving Representation
  Learning on Graphs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Rings and Algebras-662E9B">
  <p><b>Published on:</b> 2025-09-19T20:00:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daksh Pandey</p>
    <p><b>Summary:</b> Self-supervised learning (SSL) has emerged as a powerful paradigm for
learning representations on graph data without requiring manual labels.
However, leading SSL methods like GRACE are fundamentally incompatible with
privacy-preserving technologies such as Homomorphic Encryption (HE) due to
their reliance on non-polynomial operations. This paper introduces Poly-GRACE,
a novel framework for HE-compatible self-supervised learning on graphs. Our
approach consists of a fully polynomial-friendly Graph Convolutional Network
(GCN) encoder and a novel, polynomial-based contrastive loss function. Through
experiments on three benchmark datasets -- Cora, CiteSeer, and PubMed -- we
demonstrate that Poly-GRACE not only enables private pre-training but also
achieves performance that is highly competitive with, and in the case of
CiteSeer, superior to the standard non-private baseline. Our work represents a
significant step towards practical and high-performance privacy-preserving
graph representation learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.15755v1">Utility-based Privacy Preserving Data Mining</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-09-19T08:30:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qingfeng Zhou, Wensheng Gan, Zhenlian Qi, Philip S. Yu</p>
    <p><b>Summary:</b> With the advent of big data, periodic pattern mining has demonstrated
significant value in real-world applications, including smart home systems,
healthcare systems, and the medical field. However, advances in network
technology have enabled malicious actors to extract sensitive information from
publicly available datasets, posing significant threats to data providers and,
in severe cases, hindering societal development. To mitigate such risks,
privacy-preserving utility mining (PPUM) has been proposed. However, PPUM is
unsuitable for addressing privacy concerns in periodic information mining. To
address this issue, we innovatively extend the existing PPUM framework and
propose two algorithms, Maximum sensitive Utility-MAximum maxPer item (MU-MAP)
and Maximum sensitive Utility-MInimum maxPer item (MU-MIP). These algorithms
aim to hide sensitive periodic high-utility itemsets while generating sanitized
datasets. To enhance the efficiency of the algorithms, we designed two novel
data structures: the Sensitive Itemset List (SISL) and the Sensitive Item List
(SIL), which store essential information about sensitive itemsets and their
constituent items. Moreover, several performance metrics were employed to
evaluate the performance of our algorithms compared to the state-of-the-art
PPUM algorithms. The experimental results show that our proposed algorithms
achieve an Artificial Cost (AC) value of 0 on all datasets when hiding
sensitive itemsets. In contrast, the traditional PPUM algorithm yields non-zero
AC. This indicates that our algorithms can successfully hide sensitive periodic
itemsets without introducing misleading patterns, whereas the PPUM algorithm
generates additional itemsets that may interfere with user decision-making.
Moreover, the results also reveal that our algorithms maintain Database Utility
Similarity (DUS) of over 90\% after the sensitive itemsets are hidden.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18187v1">V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor
  Fusion Framework for Road Safety & Driver Behaviour Modelling</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-18T21:55:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Muhammad Naveed, Nazia Perwaiz, Sidra Sultana, Mohaira Ahmad, Muhammad Moazam Fraz</p>
    <p><b>Summary:</b> Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.15047v1">Distributed Batch Matrix Multiplication: Trade-Offs in Download Rate,
  Randomness, and Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-09-18T15:10:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amirhosein Morteza, Remi A. Chou</p>
    <p><b>Summary:</b> We study the trade-off between communication rate and privacy for distributed
batch matrix multiplication of two independent sequences of matrices
$\mathbf{A}$ and $\mathbf{B}$ with uniformly distributed entries. In our
setting, $\mathbf{B}$ is publicly accessible by all the servers while
$\mathbf{A}$ must remain private. A user is interested in evaluating the
product $\mathbf{AB}$ with the responses from the $k$ fastest servers. For a
given parameter $\alpha \in [0, 1]$, our privacy constraint must ensure that
any set of $\ell$ colluding servers cannot learn more than a fraction $\alpha$
of $\mathbf{A}$. Additionally, we study the trade-off between the amount of
local randomness needed at the encoder and privacy. Finally, we establish the
optimal trade-offs when the matrices are square and identify a linear
relationship between information leakage and communication rate.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.15278v1">Assessing metadata privacy in neuroimaging</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> 
  <p><b>Published on:</b> 2025-09-18T12:56:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Emilie Kibsgaard, Anita Sue Jwa, Christopher J Markiewicz, David Rodriguez Gonzalez, Judith Sainz Pardo, Russell A. Poldrack, Cyril R. Pernet</p>
    <p><b>Summary:</b> The ethical and legal imperative to share research data without causing harm
requires careful attention to privacy risks. While mounting evidence
demonstrates that data sharing benefits science, legitimate concerns persist
regarding the potential leakage of personal information that could lead to
reidentification and subsequent harm. We reviewed metadata accompanying
neuroimaging datasets from six heterogeneous studies openly available on
OpenNeuro, involving participants across the lifespan, from children to older
adults, with and without clinical diagnoses, and including associated clinical
score data. Using metaprivBIDS (https://github.com/CPernet/metaprivBIDS), a
novel tool for the systematic assessment of privacy in tabular data, we found
that privacy is generally well maintained, with serious vulnerabilities being
rare. Nonetheless, minor issues were identified in nearly all datasets and
warrant mitigation. Notably, clinical score data (e.g., neuropsychological
results) posed minimal reidentification risk, whereas demographic variables
(age, sex, race, income, and geolocation) represented the principal privacy
vulnerabilities. We outline practical measures to address these risks, enabling
safer data sharing practices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14603v1">Towards Privacy-Preserving and Heterogeneity-aware Split Federated
  Learning via Probabilistic Masking</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-18T04:28:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xingchen Wang, Feijie Wu, Chenglin Miao, Tianchun Li, Haoyu Hu, Qiming Cao, Jing Gao, Lu Su</p>
    <p><b>Summary:</b> Split Federated Learning (SFL) has emerged as an efficient alternative to
traditional Federated Learning (FL) by reducing client-side computation through
model partitioning. However, exchanging of intermediate activations and model
updates introduces significant privacy risks, especially from data
reconstruction attacks that recover original inputs from intermediate
representations. Existing defenses using noise injection often degrade model
performance. To overcome these challenges, we present PM-SFL, a scalable and
privacy-preserving SFL framework that incorporates Probabilistic Mask training
to add structured randomness without relying on explicit noise. This mitigates
data reconstruction risks while maintaining model utility. To address data
heterogeneity, PM-SFL employs personalized mask learning that tailors submodel
structures to each client's local data. For system heterogeneity, we introduce
a layer-wise knowledge compensation mechanism, enabling clients with varying
resources to participate effectively under adaptive model splitting.
Theoretical analysis confirms its privacy protection, and experiments on image
and wireless sensing tasks demonstrate that PM-SFL consistently improves
accuracy, communication efficiency, and robustness to privacy attacks, with
particularly strong performance under data and system heterogeneity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14581v1">Can I Trust This Chatbot? Assessing User Privacy in AI-Healthcare
  Chatbot Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-09-18T03:29:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ramazan Yener, Guan-Hung Chen, Ece Gumusel, Masooda Bashir</p>
    <p><b>Summary:</b> As Conversational Artificial Intelligence (AI) becomes more integrated into
everyday life, AI-powered chatbot mobile applications are increasingly adopted
across industries, particularly in the healthcare domain. These chatbots offer
accessible and 24/7 support, yet their collection and processing of sensitive
health data present critical privacy concerns. While prior research has
examined chatbot security, privacy issues specific to AI healthcare chatbots
have received limited attention. Our study evaluates the privacy practices of
12 widely downloaded AI healthcare chatbot apps available on the App Store and
Google Play in the United States. We conducted a three-step assessment
analyzing: (1) privacy settings during sign-up, (2) in-app privacy controls,
and (3) the content of privacy policies. The analysis identified significant
gaps in user data protection. Our findings reveal that half of the examined
apps did not present a privacy policy during sign up, and only two provided an
option to disable data sharing at that stage. The majority of apps' privacy
policies failed to address data protection measures. Moreover, users had
minimal control over their personal data. The study provides key insights for
information science researchers, developers, and policymakers to improve
privacy protections in AI healthcare chatbot apps.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14050v1">AI For Privacy in Smart Homes: Exploring How Leveraging AI-Powered Smart
  Devices Enhances Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-09-17T14:53:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wael Albayaydh, Ivan Flechais, Rui Zhao, Jood Albayaydh</p>
    <p><b>Summary:</b> Privacy concerns and fears of unauthorized access in smart home devices often
stem from misunderstandings about how data is collected, used, and protected.
This study explores how AI-powered tools can offer innovative privacy
protections through clear, personalized, and contextual support to users.
Through 23 in-depth interviews with users, AI developers, designers, and
regulators, and using Grounded Theory analysis, we identified two key themes:
Aspirations for AI-Enhanced Privacy - how users perceive AI's potential to
empower them, address power imbalances, and improve ease of use- and AI
Ethical, Security, and Regulatory Considerations-challenges in strengthening
data security, ensuring regulatory compliance, and promoting ethical AI
practices. Our findings contribute to the field by uncovering user aspirations
for AI-driven privacy solutions, identifying key security and ethical
challenges, and providing actionable recommendations for all stakeholders,
particularly targeting smart device designers and AI developers, to guide the
co-design of AI tools that enhance privacy protection in smart home devices. By
bridging the gap between user expectations, AI capabilities, and regulatory
frameworks, this work offers practical insights for shaping the future of
privacy-conscious AI integration in smart homes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13987v1">Differential Privacy in Federated Learning: Mitigating Inference Attacks
  with Randomized Response</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-17T13:59:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ozer Ozturk, Busra Buyuktanir, Gozde Karatas Baydogmus, Kazim Yildiz</p>
    <p><b>Summary:</b> Machine learning models used for distributed architectures consisting of
servers and clients require large amounts of data to achieve high accuracy.
Data obtained from clients are collected on a central server for model
training. However, storing data on a central server raises concerns about
security and privacy. To address this issue, a federated learning architecture
has been proposed. In federated learning, each client trains a local model
using its own data. The trained models are periodically transmitted to the
central server. The server then combines the received models using federated
aggregation algorithms to obtain a global model. This global model is
distributed back to the clients, and the process continues in a cyclical
manner. Although preventing data from leaving the clients enhances security,
certain concerns still remain. Attackers can perform inference attacks on the
obtained models to approximate the training dataset, potentially causing data
leakage. In this study, differential privacy was applied to address the
aforementioned security vulnerability, and a performance analysis was
conducted. The Data-Unaware Classification Based on Association (duCBA)
algorithm was used as the federated aggregation method. Differential privacy
was implemented on the data using the Randomized Response technique, and the
trade-off between security and performance was examined under different epsilon
values. As the epsilon value decreased, the model accuracy declined, and class
prediction imbalances were observed. This indicates that higher levels of
privacy do not always lead to practical outcomes and that the balance between
security and performance must be carefully considered.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13739v1">ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-09-17T06:45:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zihou Wu, Yuecheng Li, Tianchi Liao, Jian Lou, Chuan Chen</p>
    <p><b>Summary:</b> Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13627v1">Secure, Scalable and Privacy Aware Data Strategy in Cloud</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-09-17T01:56:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vijay Kumar Butte, Sujata Butte</p>
    <p><b>Summary:</b> The enterprises today are faced with the tough challenge of processing,
storing large amounts of data in a secure, scalable manner and enabling
decision makers to make quick, informed data driven decisions. This paper
addresses this challenge and develops an effective enterprise data strategy in
the cloud. Various components of an effective data strategy are discussed and
architectures addressing security, scalability and privacy aspects are
provided.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13625v3">Privacy-Aware In-Context Learning for Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-17T01:50:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bishnu Bhusal, Manoj Acharya, Ramneet Kaur, Colin Samplawski, Anirban Roy, Adam D. Cobb, Rohit Chadha, Susmit Jha</p>
    <p><b>Summary:</b> Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models. The proposed method performs
inference on private records and aggregates the resulting per-token output
distributions. This enables the generation of longer and coherent synthetic
text while maintaining privacy guarantees. Additionally, we propose a simple
blending operation that combines private and public inference to further
enhance utility. Empirical evaluations demonstrate that our approach
outperforms previous state-of-the-art methods on in-context-learning (ICL)
tasks, making it a promising direction for privacy-preserving text generation
while maintaining high utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13509v1">Practitioners' Perspectives on a Differential Privacy Deployment
  Registry</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-16T20:15:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Priyanka Nanayakkara, Elena Ghazi, Salil Vadhan</p>
    <p><b>Summary:</b> Differential privacy (DP) -- a principled approach to producing statistical
data products with strong, mathematically provable privacy guarantees for the
individuals in the underlying dataset -- has seen substantial adoption in
practice over the past decade. Applying DP requires making several
implementation decisions, each with significant impacts on data privacy and/or
utility. Hence, to promote shared learning and accountability around DP
deployments, Dwork, Kohli, and Mulligan (2019) proposed a public-facing
repository ("registry") of DP deployments. The DP community has recently
started to work toward realizing this vision. We contribute to this effort by
(1) developing a holistic, hierarchical schema to describe any given DP
deployment and (2) designing and implementing an interactive interface to act
as a registry where practitioners can access information about past DP
deployments. We (3) populate our interface with 21 real-world DP deployments
and (4) conduct an exploratory user study with DP practitioners ($n=16$) to
understand how they would use the registry, as well as what challenges and
opportunities they foresee around its adoption. We find that participants were
enthusiastic about the registry as a valuable resource for evaluating prior
deployments and making future deployments. They also identified several
opportunities for the registry, including that it can become a "hub" for the
community and support broader communication around DP (e.g., to legal teams).
At the same time, they identified challenges around the registry gaining
adoption, including the effort and risk involved with making implementation
choices public and moderating the quality of entries. Based on our findings, we
offer recommendations for encouraging adoption and increasing the registry's
value not only to DP practitioners, but also to policymakers, data users, and
data subjects.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14284v1">The Sum Leaks More Than Its Parts: Compositional Privacy Risks and
  Mitigations in Multi-Agent Collaboration</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-09-16T16:57:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vaidehi Patil, Elias Stengel-Eskin, Mohit Bansal</p>
    <p><b>Summary:</b> As large language models (LLMs) become integral to multi-agent systems, new
privacy risks emerge that extend beyond memorization, direct inference, or
single-turn evaluations. In particular, seemingly innocuous responses, when
composed across interactions, can cumulatively enable adversaries to recover
sensitive information, a phenomenon we term compositional privacy leakage. We
present the first systematic study of such compositional privacy leaks and
possible mitigation methods in multi-agent LLM systems. First, we develop a
framework that models how auxiliary knowledge and agent interactions jointly
amplify privacy risks, even when each response is benign in isolation. Next, to
mitigate this, we propose and evaluate two defense strategies: (1)
Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent
by anticipating how their outputs may be exploited by adversaries, and (2)
Collaborative Consensus Defense (CoDef), where responder agents collaborate
with peers who vote based on a shared aggregated state to restrict sensitive
information spread. Crucially, we balance our evaluation across compositions
that expose sensitive information and compositions that yield benign
inferences. Our experiments quantify how these defense strategies differ in
balancing the privacy-utility trade-off. We find that while chain-of-thought
alone offers limited protection to leakage (~39% sensitive blocking rate), our
ToM defense substantially improves sensitive query blocking (up to 97%) but can
reduce benign task success. CoDef achieves the best balance, yielding the
highest Balanced Outcome (79.8%), highlighting the benefit of combining
explicit reasoning with defender collaboration. Together, our results expose a
new class of risks in collaborative LLM deployments and provide actionable
insights for designing safeguards against compositional, context-driven privacy
leakage.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13051v1">More than Meets the Eye: Understanding the Effect of Individual Objects
  on Perceived Visual Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-16T13:10:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mete Harun Akcay, Siddharth Prakash Rao, Alexandros Bakas, Buse Gul Atli</p>
    <p><b>Summary:</b> User-generated content, such as photos, comprises the majority of online
media content and drives engagement due to the human ability to process visual
information quickly. Consequently, many online platforms are designed for
sharing visual content, with billions of photos posted daily. However, photos
often reveal more than they intended through visible and contextual cues,
leading to privacy risks. Previous studies typically treat privacy as a
property of the entire image, overlooking individual objects that may carry
varying privacy risks and influence how users perceive it. We address this gap
with a mixed-methods study (n = 92) to understand how users evaluate the
privacy of images containing multiple sensitive objects. Our results reveal
mental models and nuanced patterns that uncover how granular details, such as
photo-capturing context and co-presence of other objects, affect privacy
perceptions. These novel insights could enable personalized, context-aware
privacy protection designs on social media and future technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12958v1">Forget What's Sensitive, Remember What Matters: Token-Level Differential
  Privacy in Memory Sculpting for Continual Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-16T11:01:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bihao Zhan, Jie Zhou, Junsong Li, Yutao Yang, Shilian Chen, Qianjun Pan, Xin Li, Wen Wu, Xingjiao Wu, Qin Chen, Hang Yan, Liang He</p>
    <p><b>Summary:</b> Continual Learning (CL) models, while adept at sequential knowledge
acquisition, face significant and often overlooked privacy challenges due to
accumulating diverse information. Traditional privacy methods, like a uniform
Differential Privacy (DP) budget, indiscriminately protect all data, leading to
substantial model utility degradation and hindering CL deployment in
privacy-sensitive areas. To overcome this, we propose a privacy-enhanced
continual learning (PeCL) framework that forgets what's sensitive and remembers
what matters. Our approach first introduces a token-level dynamic Differential
Privacy strategy that adaptively allocates privacy budgets based on the
semantic sensitivity of individual tokens. This ensures robust protection for
private entities while minimizing noise injection for non-sensitive, general
knowledge. Second, we integrate a privacy-guided memory sculpting module. This
module leverages the sensitivity analysis from our dynamic DP mechanism to
intelligently forget sensitive information from the model's memory and
parameters, while explicitly preserving the task-invariant historical knowledge
crucial for mitigating catastrophic forgetting. Extensive experiments show that
PeCL achieves a superior balance between privacy preserving and model utility,
outperforming baseline models by maintaining high accuracy on previous tasks
while ensuring robust privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12899v1">EByFTVeS: Efficient Byzantine Fault Tolerant-based Verifiable
  Secret-sharing in Distributed Privacy-preserving Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-16T09:54:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhen Li, Zijian Zhang, Wenjin Yang, Pengbo Wang, Zhaoqi Wang, Meng Li, Yan Wu, Xuyang Liu, Jing Sun, Liehuang Zhu</p>
    <p><b>Summary:</b> Verifiable Secret Sharing (VSS) has been widespread in Distributed
Privacy-preserving Machine Learning (DPML), because invalid shares from
malicious dealers or participants can be recognized by verifying the commitment
of the received shares for honest participants. However, the consistency and
the computation and communitation burden of the VSS-based DPML schemes are
still two serious challenges. Although Byzantine Fault Tolerance (BFT) system
has been brought to guarantee the consistency and improve the efficiency of the
existing VSS-based DPML schemes recently, we explore an Adaptive Share Delay
Provision (ASDP) strategy, and launch an ASDP-based Customized Model Poisoning
Attack (ACuMPA) for certain participants in this paper. We theoretically
analyzed why the ASDP strategy and the ACuMPA algorithm works to the existing
schemes. Next, we propose an [E]fficient [By]zantine [F]ault [T]olerant-based
[Ve]rifiable [S]ecret-sharing (EByFTVeS) scheme. Finally, the validity,
liveness, consistency and privacy of the EByFTVeS scheme are theoretically
analyzed, while the efficiency of the EByFTVeS scheme outperforms that of
the-state-of-art VSS scheme according to comparative experiment results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14278v1">Beyond Data Privacy: New Privacy Risks for Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-16T09:46:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuntao Du, Zitao Li, Ninghui Li, Bolin Ding</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have achieved remarkable progress in natural
language understanding, reasoning, and autonomous decision-making. However,
these advancements have also come with significant privacy concerns. While
significant research has focused on mitigating the data privacy risks of LLMs
during various stages of model training, less attention has been paid to new
threats emerging from their deployment. The integration of LLMs into widely
used applications and the weaponization of their autonomous abilities have
created new privacy vulnerabilities. These vulnerabilities provide
opportunities for both inadvertent data leakage and malicious exfiltration from
LLM-powered systems. Additionally, adversaries can exploit these systems to
launch sophisticated, large-scale privacy attacks, threatening not only
individual privacy but also financial security and societal trust. In this
paper, we systematically examine these emerging privacy risks of LLMs. We also
discuss potential mitigation strategies and call for the research community to
broaden its focus beyond data privacy risks, developing new defenses to address
the evolving threats posed by increasingly powerful LLMs and LLM-powered
systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14275v1">FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated
  LLMs in Mental Health</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-16T07:08:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nobin Sarwar, Shubhashis Roy Dipta</p>
    <p><b>Summary:</b> Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive
domains (e.g., mental health) requires balancing strict confidentiality with
model utility and safety. We propose FedMentor, a federated fine-tuning
framework that integrates Low-Rank Adaptation (LoRA) and domain-aware
Differential Privacy (DP) to meet per-domain privacy budgets while maintaining
performance. Each client (domain) applies a custom DP noise scale proportional
to its data sensitivity, and the server adaptively reduces noise when utility
falls below a threshold. In experiments on three mental health datasets, we
show that FedMentor improves safety over standard Federated Learning without
privacy, raising safe output rates by up to three points and lowering toxicity,
while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the
non-private baseline and close to the centralized upper bound. The framework
scales to backbones with up to 1.7B parameters on single-GPU clients, requiring
< 173 MB of communication per round. FedMentor demonstrates a practical
approach to privately fine-tune LLMs for safer deployments in healthcare and
other sensitive fields.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12590v1">DPCheatSheet: Using Worked and Erroneous LLM-usage Examples to Scaffold
  Differential Privacy Implementation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-16T02:33:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shao-Yu Chu, Yuhe Tian, Yu-Xiang Wang, Haojian Jin</p>
    <p><b>Summary:</b> This paper explores how programmers without specialized expertise in
differential privacy (DP) (i.e., novices) can leverage LLMs to implement DP
programs with minimal training. We first conducted a need-finding study with 6
novices and 3 experts to understand how they utilize LLMs in DP implementation.
While DP experts can implement correct DP analyses through a few prompts,
novices struggle to articulate their requirements in prompts and lack the
skills to verify the correctness of the generated code. We then developed
DPCheatSheet, an instructional tool that helps novices implement DP using LLMs.
DPCheatSheet combines two learning concepts: it annotates an expert's workflow
with LLMs as a worked example to bridge the expert mindset to novices, and it
presents five common mistakes in LLM-based DP code generation as erroneous
examples to support error-driven learning. We demonstrated the effectiveness of
DPCheatSheet with an error identification study and an open-ended DP
implementation study.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12578v1">Conflect: Designing Reflective Thinking-Based Contextual Privacy Policy
  for Mobile Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-16T02:11:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuning Zhang, Sixing Tao, Eve He, Yuting Yang, Ying Ma, Ailei Wang, Xin Yi, Hewu Li</p>
    <p><b>Summary:</b> Privacy policies are lengthy and complex, leading to user neglect. While
contextual privacy policies (CPPs) present information at the point of risk,
they may lack engagement and disrupt tasks. We propose Conflect, an interactive
CPP for mobile apps, guided by a reflective thinking framework. Through three
workshops with experienced designers and researchers, we constructed the design
space of reflective thinking-based CPP design, and identified the disconnect
between context and action as the most critical problem. Based on participants'
feedback, we designed Conflect to use sidebar alerts, allowing users to reflect
on contextualized risks and fostering their control. Our system contextually
detects privacy risks, extracts policy segments, and automatically generates
risk descriptions with 94.0% policy extraction accuracy on CPP4APP dataset and
a 4.35s latency. A user study (N=28) demonstrated that Conflect improves user
understanding, trust, and satisfaction while lowering cognitive load compared
to CPPs, privacy policies and privacy labels.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12465v1">Efficient Privacy-Preserving Training of Quantum Neural Networks by
  Using Mixed States to Represent Input Data Ensembles</a></h3>
  
  <p><b>Published on:</b> 2025-09-15T21:20:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gaoyuan Wang, Jonathan Warrell, Mark Gerstein</p>
    <p><b>Summary:</b> Quantum neural networks (QNNs) are gaining increasing interest due to their
potential to detect complex patterns in data by leveraging uniquely quantum
phenomena. This makes them particularly promising for biomedical applications.
In these applications and in other contexts, increasing statistical power often
requires aggregating data from multiple participants. However, sharing data,
especially sensitive information like personal genomic sequences, raises
significant privacy concerns. Quantum federated learning offers a way to
collaboratively train QNN models without exposing private data. However, it
faces major limitations, including high communication overhead and the need to
retrain models when the task is modified. To overcome these challenges, we
propose a privacy-preserving QNN training scheme that utilizes mixed quantum
states to encode ensembles of data. This approach allows for the secure sharing
of statistical information while safeguarding individual data points. QNNs can
be trained directly on these mixed states, eliminating the need to access raw
data. Building on this foundation, we introduce protocols supporting
multi-party collaborative QNN training applicable across diverse domains. Our
approach enables secure QNN training with only a single round of communication
per participant, provides high training speed and offers task generality, i.e.,
new analyses can be conducted without reacquiring information from
participants. We present the theoretical foundation of our scheme's utility and
privacy protections, which prevent the recovery of individual data points and
resist membership inference attacks as measured by differential privacy. We
then validate its effectiveness on three different datasets with a focus on
genomic studies with an indication of how it can used in other domains without
adaptation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12403v1">Privacy-Driven Network Data for Smart Cities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-09-15T19:48:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tânia Carvalho, José Barata, Henish Balu, Filipa Moreira, João Bastos, Luís Antunes</p>
    <p><b>Summary:</b> A smart city is essential for sustainable urban development. In addition to
citizen engagement, a smart city enables connected infrastructure, data-driven
decision making and smart mobility. For most of these features, network data
plays a critical role, particularly from public Wi-Fi infrastructures, where
cities can benefit from optimized services such as public transport management
and the safety and efficiency of large events. One of the biggest concerns in
developing a smart city is using secure and private data. This is particularly
relevant in the case of Wi-Fi network data, where sensitive information can be
collected. This paper specifically addresses the problem of sharing secure data
to enhance the quality of the Wi-Fi network in a city. Despite the high
importance of this type of data, related work focuses on improving the safety
of mobility patterns, targeting only the protection of MAC addresses. On the
opposite side, we provide a practical methodology for safeguarding all
attributes in real Wi-Fi network data. This study was developed in
collaboration with a multidisciplinary team of legal experts, data custodians
and technical privacy specialists, resulting in high-quality data. On top of
that, we show how to integrate the legal considerations for secure data
sharing. Our approach promotes data-driven innovation and privacy awareness in
the context of smart city initiatives, which have been tested in a real
scenario.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12338v1">Privacy in continuous-variable distributed quantum sensing</a></h3>
  
  <p><b>Published on:</b> 2025-09-15T18:06:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> A. de Oliveira Junior, Anton L. Andersen, Benjamin Lundgren Larsen, Sean William Moore, Damian Markham, Masahiro Takeoka, Jonatan Bohr Brask, Ulrik L. Andersen</p>
    <p><b>Summary:</b> Can a distributed network of quantum sensors estimate a global parameter
while protecting every locally encoded value? We answer this question
affirmatively by introducing and analysing a protocol for distributed quantum
sensing in the continuous-variable regime. We consider a multipartite network
in which each node encodes a local phase into a shared entangled Gaussian
state. We show that the average phase can be estimated with high precision,
exhibiting Heisenberg scaling in the total photon number, while individual
phases are inaccessible. Although complete privacy - where all other
combinations of phases remain entirely hidden - is unattainable for finite
squeezing in multi-party settings, it emerges in the large-squeezing limit. We
further investigate the impact of displacements and optical losses, revealing
trade-offs between estimation accuracy and privacy. Finally, we benchmark the
protocol against other continuous-variable resource states.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11939v1">PrivWeb: Unobtrusive and Content-aware Privacy Protection For Web Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-15T13:58:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuning Zhang, Yutong Jiang, Rongjun Ma, Yuting Yang, Mingyao Xu, Zhixin Huang, Xin Yi, Hewu Li</p>
    <p><b>Summary:</b> While web agents gained popularity by automating web interactions, their
requirement for interface access introduces significant privacy risks that are
understudied, particularly from users' perspective. Through a formative study
(N=15), we found users frequently misunderstand agents' data practices, and
desired unobtrusive, transparent data management. To achieve this, we designed
and implemented PrivWeb, a trusted add-on on web agents that utilizes a
localized LLM to anonymize private information on interfaces according to user
preferences. It features privacy categorization schema and adaptive
notifications that selectively pauses tasks for user control over information
collection for highly sensitive information, while offering non-disruptive
options for less sensitive information, minimizing human oversight. The user
study (N=14) across travel, information retrieval, shopping, and entertainment
tasks compared PrivWeb with baselines without notification and without control
for private information access, where PrivWeb reduced perceived privacy risks
with no associated increase in cognitive effort, and resulted in higher overall
satisfaction.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11917v1">Distributed Finite-Horizon Optimal Control for Consensus with
  Differential Privacy Guarantees</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-09-15T13:34:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuwen Ma, Yongqiang Wang, Sarah K. Spurgeon, Boli Chen</p>
    <p><b>Summary:</b> This paper addresses the problem of privacy-preserving consensus control for
multi-agent systems (MAS) using differential privacy. We propose a novel
distributed finite-horizon linear quadratic regulator (LQR) framework, in which
agents share individual state information while preserving the confidentiality
of their local pairwise weight matrices, which are considered sensitive data in
MAS. Protecting these matrices effectively safeguards each agent's private cost
function and control preferences. Our solution injects consensus
error-dependent Laplace noise into the communicated state information and
employs a carefully designed time-dependent scaling factor in the local cost
functions. {This approach guarantees bounded consensus and achieves rigorous
$\epsilon$-differential privacy for the weight matrices without relying on
specific noise distribution assumptions.} Additionally, we analytically
characterize the trade-off between consensus accuracy and privacy level,
offering clear guidelines on how to enhance consensus performance through
appropriate scaling of the LQR weight matrices and the privacy budget.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11870v1">Efficient Byzantine-Robust Privacy-Preserving Federated Learning via
  Dimension Compression</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-15T12:43:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xian Qin, Xue Yang, Xiaohu Tang</p>
    <p><b>Summary:</b> Federated Learning (FL) allows collaborative model training across
distributed clients without sharing raw data, thus preserving privacy. However,
the system remains vulnerable to privacy leakage from gradient updates and
Byzantine attacks from malicious clients. Existing solutions face a critical
trade-off among privacy preservation, Byzantine robustness, and computational
efficiency. We propose a novel scheme that effectively balances these competing
objectives by integrating homomorphic encryption with dimension compression
based on the Johnson-Lindenstrauss transformation. Our approach employs a
dual-server architecture that enables secure Byzantine defense in the
ciphertext domain while dramatically reducing computational overhead through
gradient compression. The dimension compression technique preserves the
geometric relationships necessary for Byzantine defence while reducing
computation complexity from $O(dn)$ to $O(kn)$ cryptographic operations, where
$k \ll d$. Extensive experiments across diverse datasets demonstrate that our
approach maintains model accuracy comparable to non-private FL while
effectively defending against Byzantine clients comprising up to $40\%$ of the
network.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11761v1">On Spatial-Provenance Recovery in Wireless Networks with Relaxed-Privacy
  Constraints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-09-15T10:28:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Manish Bansal, Pramsu Shrivastava, J. Harshan</p>
    <p><b>Summary:</b> In Vehicle-to-Everything (V2X) networks with multi-hop communication, Road
Side Units (RSUs) intend to gather location data from the vehicles to offer
various location-based services. Although vehicles use the Global Positioning
System (GPS) for navigation, they may refrain from sharing their exact GPS
coordinates to the RSUs due to privacy considerations. Thus, to address the
localization expectations of the RSUs and the privacy concerns of the vehicles,
we introduce a relaxed-privacy model wherein the vehicles share their partial
location information in order to avail the location-based services. To
implement this notion of relaxed-privacy, we propose a low-latency protocol for
spatial-provenance recovery, wherein vehicles use correlated linear Bloom
filters to embed their position information. Our proposed spatial-provenance
recovery process takes into account the resolution of localization, the
underlying ad hoc protocol, and the coverage range of the wireless technology
used by the vehicles. Through a rigorous theoretical analysis, we present
extensive analysis on the underlying trade-off between relaxed-privacy and the
communication-overhead of the protocol. Finally, using a wireless testbed, we
show that our proposed method requires a few bits in the packet header to
provide security features such as localizing a low-power jammer executing a
denial-of-service attack.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11625v2">Inducing Uncertainty on Open-Weight Models for Test-Time Privacy in
  Image Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-15T06:38:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Muhammad H. Ashiq, Peter Triantafillou, Hung Yun Tseng, Grigoris G. Chrysos</p>
    <p><b>Summary:</b> A key concern for AI safety remains understudied in the machine learning (ML)
literature: how can we ensure users of ML models do not leverage predictions on
incorrect personal data to harm others? This is particularly pertinent given
the rise of open-weight models, where simply masking model outputs does not
suffice to prevent adversaries from recovering harmful predictions. To address
this threat, which we call *test-time privacy*, we induce maximal uncertainty
on protected instances while preserving accuracy on all other instances. Our
proposed algorithm uses a Pareto optimal objective that explicitly balances
test-time privacy against utility. We also provide a certifiable approximation
algorithm which achieves $(\varepsilon, \delta)$ guarantees without convexity
assumptions. We then prove a tight bound that characterizes the privacy-utility
tradeoff that our algorithms incur. Empirically, our method obtains at least
$>3\times$ stronger uncertainty than pretraining with marginal drops in
accuracy on various image recognition benchmarks. Altogether, this framework
provides a tool to guarantee additional protection to end users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11249v1">Make Identity Unextractable yet Perceptible: Synthesis-Based Privacy
  Protection for Subject Faces in Photos</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-14T12:47:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tao Wang, Yushu Zhang, Xiangli Xiao, Kun Xu, Lin Yuan, Wenying Wen, Yuming Fang</p>
    <p><b>Summary:</b> Deep learning-based face recognition (FR) technology exacerbates privacy
concerns in photo sharing. In response, the research community developed a
suite of anti-FR methods to block identity extraction by unauthorized FR
systems. Benefiting from quasi-imperceptible alteration, perturbation-based
methods are well-suited for privacy protection of subject faces in photos, as
they allow familiar persons to recognize subjects via naked eyes. However, we
reveal that perturbation-based methods provide a false sense of privacy through
theoretical analysis and experimental validation.
  Therefore, new alternative solutions should be found to protect subject
faces. In this paper, we explore synthesis-based methods as a promising
solution, whose challenge is to enable familiar persons to recognize subjects.
To solve the challenge, we present a key insight: In most photo sharing
scenarios, familiar persons recognize subjects through identity perception
rather than meticulous face analysis. Based on the insight, we propose the
first synthesis-based method dedicated to subject faces, i.e., PerceptFace,
which can make identity unextractable yet perceptible. To enhance identity
perception, a new perceptual similarity loss is designed for faces, reducing
the alteration in regions of high sensitivity to human vision.
  As a synthesis-based method, PerceptFace can inherently provide reliable
identity protection. Meanwhile, out of the confine of meticulous face analysis,
PerceptFace focuses on identity perception from a more practical scenario,
which is also enhanced by the designed perceptual similarity loss. Sufficient
experiments show that PerceptFace achieves a superior trade-off between
identity protection and identity perception compared to existing methods. We
provide a public API of PerceptFace and believe that it has great potential to
become a practical anti-FR tool.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18134v1">A Weighted Gradient Tracking Privacy-Preserving Method for Distributed
  Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E">
  <p><b>Published on:</b> 2025-09-14T07:29:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Furan Xie, Bing Liu, Li Chai</p>
    <p><b>Summary:</b> This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11022v2">Privacy-Preserving Uncertainty Disclosure for Facilitating Enhanced
  Energy Storage Dispatch</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E">
  <p><b>Published on:</b> 2025-09-14T01:22:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ning Qi, Xiaolong Jin, Kai Hou, Zeyu Liu, Hongjie Jia, Wei Wei</p>
    <p><b>Summary:</b> This paper proposes a novel privacy-preserving uncertainty disclosure
framework, enabling system operators to release marginal value function bounds
to reduce the conservativeness of interval forecast and mitigate excessive
withholding, thereby enhancing storage dispatch and social welfare. We develop
a risk-averse storage arbitrage model based on stochastic dynamic programming,
explicitly accounting for uncertainty intervals in value function training.
Real-time marginal value function bounds are derived using a rolling-horizon
chance-constrained economic dispatch formulation. We rigorously prove that the
bounds reliably cap the true opportunity cost and dynamically converge to the
hindsight value. We verify that both the marginal value function and its bounds
monotonically decrease with the state of charge (SoC) and increase with
uncertainty, providing a theoretical basis for risk-averse strategic behaviors
and SoC-dependent designs. An adjusted storage dispatch algorithm is further
designed using these bounds. We validate the effectiveness of the proposed
framework via an agent-based simulation on the ISO-NE test system. Under 50%
renewable capacity and 35% storage capacity, the proposed bounds enhance
storage response by 38.91% and reduce the optimality gap to 3.91% through
improved interval predictions. Additionally, by mitigating excessive
withholding, the bounds yield an average system cost reduction of 0.23% and an
average storage profit increase of 13.22%. These benefits further scale with
higher prediction conservativeness, storage capacity, and system uncertainty.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.10691v1">Privacy-Preserving Decentralized Federated Learning via Explainable
  Adaptive Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-12T20:52:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fardin Jalil Piran, Zhiling Chen, Yang Zhang, Qianyu Zhou, Jiong Tang, Farhad Imani</p>
    <p><b>Summary:</b> Decentralized federated learning faces privacy risks because model updates
can leak data through inference attacks and membership inference, a concern
that grows over many client exchanges. Differential privacy offers principled
protection by injecting calibrated noise so confidential information remains
secure on resource-limited IoT devices. Yet without transparency, black-box
training cannot track noise already injected by previous clients and rounds,
which forces worst-case additions and harms accuracy. We propose PrivateDFL, an
explainable framework that joins hyperdimensional computing with differential
privacy and keeps an auditable account of cumulative noise so each client adds
only the difference between the required noise and what has already been
accumulated. We evaluate on MNIST, ISOLET, and UCI-HAR to span image, signal,
and tabular modalities, and we benchmark against transformer-based and deep
learning-based baselines trained centrally with Differentially Private
Stochastic Gradient Descent (DP-SGD) and Renyi Differential Privacy (RDP).
PrivateDFL delivers higher accuracy, lower latency, and lower energy across IID
and non-IID partitions while preserving formal (epsilon, delta) guarantees and
operating without a central server. For example, under non-IID partitions,
PrivateDFL achieves 24.42% higher accuracy than the Vision Transformer on MNIST
while using about 10x less training time, 76x lower inference latency, and 11x
less energy, and on ISOLET it exceeds Transformer accuracy by more than 80%
with roughly 10x less training time, 40x lower inference latency, and 36x less
training energy. Future work will extend the explainable accounting to
adversarial clients and adaptive topologies with heterogeneous privacy budgets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.10163v1">Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and
  Energy-Aware Resource Management in 6G Edge Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-09-12T11:41:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Francisco Javier Esono Nkulu Andong, Qi Min</p>
    <p><b>Summary:</b> As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.</p>
  </details>
</div>

