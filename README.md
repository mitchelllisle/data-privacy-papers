
<h2>2025-04</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21752v1">VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-04-30T15:46:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haochen Sun, Xi He</p>
    <p><b>Summary:</b> Despite differential privacy (DP) often being considered the de facto
standard for data privacy, its realization is vulnerable to unfaithful
execution of its mechanisms by servers, especially in distributed settings.
Specifically, servers may sample noise from incorrect distributions or generate
correlated noise while appearing to follow established protocols. This work
analyzes these malicious behaviors in a general differential privacy framework
within a distributed client-server-verifier setup. To address these adversarial
problems, we propose a novel definition called Verifiable Distributed
Differential Privacy (VDDP) by incorporating additional verification
mechanisms. We also explore the relationship between zero-knowledge proofs
(ZKP) and DP, demonstrating that while ZKPs are sufficient for achieving DP
under verifiability requirements, they are not necessary. Furthermore, we
develop two novel and efficient mechanisms that satisfy VDDP: (1) the
Verifiable Distributed Discrete Laplacian Mechanism (VDDLM), which offers up to
a $4 \times 10^5$x improvement in proof generation efficiency with only
0.1-0.2x error compared to the previous state-of-the-art verifiable
differentially private mechanism; (2) an improved solution to Verifiable
Randomized Response (VRR) under local DP, a special case of VDDP, achieving up
a reduction of up to 5000x in communication costs and the verifier's overhead.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21646v1">Diffusion-based Adversarial Identity Manipulation for Facial Privacy
  Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-30T13:49:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Liqin Wang, Qianyue Hu, Wei Lu, Xiangyang Luo</p>
    <p><b>Summary:</b> The success of face recognition (FR) systems has led to serious privacy
concerns due to potential unauthorized surveillance and user tracking on social
networks. Existing methods for enhancing privacy fail to generate natural face
images that can protect facial privacy. In this paper, we propose
diffusion-based adversarial identity manipulation (DiffAIM) to generate natural
and highly transferable adversarial faces against malicious FR systems. To be
specific, we manipulate facial identity within the low-dimensional latent space
of a diffusion model. This involves iteratively injecting gradient-based
adversarial identity guidance during the reverse diffusion process,
progressively steering the generation toward the desired adversarial faces. The
guidance is optimized for identity convergence towards a target while promoting
semantic divergence from the source, facilitating effective impersonation while
maintaining visual naturalness. We further incorporate structure-preserving
regularization to preserve facial structure consistency during manipulation.
Extensive experiments on both face verification and identification tasks
demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger
black-box attack transferability while maintaining superior visual quality. We
also demonstrate the effectiveness of the proposed approach for commercial FR
APIs, including Face++ and Aliyun.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21413v1">An Inversion Theorem for Buffered Linear Toeplitz (BLT) Matrices and
  Applications to Streaming Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-04-30T08:14:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> H. Brendan McMahan, Krishna Pillutla</p>
    <p><b>Summary:</b> Buffered Linear Toeplitz (BLT) matrices are a family of parameterized
lower-triangular matrices that play an important role in streaming differential
privacy with correlated noise. Our main result is a BLT inversion theorem: the
inverse of a BLT matrix is itself a BLT matrix with different parameters. We
also present an efficient and differentiable $O(d^3)$ algorithm to compute the
parameters of the inverse BLT matrix, where $d$ is the degree of the original
BLT (typically $d < 10$). Our characterization enables direct optimization of
BLT parameters for privacy mechanisms through automatic differentiation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21297v1">Participatory AI, Public Sector AI, Differential Privacy, Conversational
  Interfaces, Explainable AI, Citizen Engagement in AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> 
  <p><b>Published on:</b> 2025-04-30T04:10:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenjun Yang, Eyhab Al-Masri</p>
    <p><b>Summary:</b> This paper introduces a conversational interface system that enables
participatory design of differentially private AI systems in public sector
applications. Addressing the challenge of balancing mathematical privacy
guarantees with democratic accountability, we propose three key contributions:
(1) an adaptive $\epsilon$-selection protocol leveraging TOPSIS multi-criteria
decision analysis to align citizen preferences with differential privacy (DP)
parameters, (2) an explainable noise-injection framework featuring real-time
Mean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and
(3) an integrated legal-compliance mechanism that dynamically modulates privacy
budgets based on evolving regulatory constraints. Our results advance
participatory AI practices by demonstrating how conversational interfaces can
enhance public engagement in algorithmic privacy mechanisms, ensuring that
privacy-preserving AI in public sector governance remains both mathematically
robust and democratically accountable.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21182v1">Federated One-Shot Learning with Data Privacy and Objective-Hiding</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2025-04-29T21:25:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Maximilian Egger, Rüdiger Urbanke, Rawad Bitar</p>
    <p><b>Summary:</b> Privacy in federated learning is crucial, encompassing two key aspects:
safeguarding the privacy of clients' data and maintaining the privacy of the
federator's objective from the clients. While the first aspect has been
extensively studied, the second has received much less attention.
  We present a novel approach that addresses both concerns simultaneously,
drawing inspiration from techniques in knowledge distillation and private
information retrieval to provide strong information-theoretic privacy
guarantees.
  Traditional private function computation methods could be used here; however,
they are typically limited to linear or polynomial functions. To overcome these
constraints, our approach unfolds in three stages. In stage 0, clients perform
the necessary computations locally. In stage 1, these results are shared among
the clients, and in stage 2, the federator retrieves its desired objective
without compromising the privacy of the clients' data. The crux of the method
is a carefully designed protocol that combines secret-sharing-based multi-party
computation and a graph-based private information retrieval scheme. We show
that our method outperforms existing tools from the literature when properly
adapted to this setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20941v1">Conformal-DP: Differential Privacy on Riemannian Manifolds via Conformal
  Transformation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">  
  <p><b>Published on:</b> 2025-04-29T17:05:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peilin He, Liou Tang, M. Amin Rahimian, James Joshi</p>
    <p><b>Summary:</b> Differential Privacy (DP) has been established as a safeguard for private
data sharing by adding perturbations to information release. Prior research on
DP has extended beyond data in the flat Euclidean space and addressed data on
curved manifolds, e.g., diffusion tensor MRI, social networks, or organ shape
analysis, by adding perturbations along geodesic distances. However, existing
manifold-aware DP methods rely on the assumption that samples are uniformly
distributed across the manifold. In reality, data densities vary, leading to a
biased noise imbalance across manifold regions, weakening the privacy-utility
trade-offs. To address this gap, we propose a novel mechanism: Conformal-DP,
utilizing conformal transformations on the Riemannian manifold to equalize
local sample density and to redefine geodesic distances accordingly while
preserving the intrinsic geometry of the manifold. Our theoretical analysis
yields two main results. First, we prove that the conformal factor computed
from local kernel-density estimates is explicitly data-density-aware; Second,
under the conformal metric, the mechanism satisfies $ \varepsilon
$-differential privacy on any complete Riemannian manifold and admits a
closed-form upper bound on the expected geodesic error that depends only on the
maximal density ratio, not on global curvatureof the manifold. Our experimental
results validate that the mechanism achieves high utility while providing the $
\varepsilon $-DP guarantee for both homogeneous and especially heterogeneous
manifold data.</p>
  </details>
</div>



<h2>2025-05</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.20118v1">TrojanStego: Your Language Model Can Secretly Be A Steganographic
  Privacy Leaking Agent</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-26T15:20:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dominik Meier, Jan Philip Wahle, Paul Röttger, Terry Ruas, Bela Gipp</p>
    <p><b>Summary:</b> As large language models (LLMs) become integrated into sensitive workflows,
concerns grow over their potential to leak confidential information. We propose
TrojanStego, a novel threat model in which an adversary fine-tunes an LLM to
embed sensitive context information into natural-looking outputs via linguistic
steganography, without requiring explicit control over inference inputs. We
introduce a taxonomy outlining risk factors for compromised LLMs, and use it to
evaluate the risk profile of the threat. To implement TrojanStego, we propose a
practical encoding scheme based on vocabulary partitioning learnable by LLMs
via fine-tuning. Experimental results show that compromised models reliably
transmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over
97% accuracy using majority voting across three generations. Further, they
maintain high utility, can evade human detection, and preserve coherence. These
results highlight a new class of LLM data exfiltration attacks that are
passive, covert, practical, and dangerous.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.20095v1">Spurious Privacy Leakage in Neural Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-26T15:04:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenxiang Zhang, Jun Pang, Sjouke Mauw</p>
    <p><b>Summary:</b> Neural networks are vulnerable to privacy attacks aimed at stealing sensitive
data. The risks can be amplified in a real-world scenario, particularly when
models are trained on limited and biased data. In this work, we investigate the
impact of spurious correlation bias on privacy vulnerability. We introduce
\emph{spurious privacy leakage}, a phenomenon where spurious groups are
significantly more vulnerable to privacy attacks than non-spurious groups. We
further show that group privacy disparity increases in tasks with simpler
objectives (e.g. fewer classes) due to the persistence of spurious features.
Surprisingly, we find that reducing spurious correlation using spurious robust
methods does not mitigate spurious privacy leakage. This leads us to introduce
a perspective on privacy disparity based on memorization, where mitigating
spurious correlation does not mitigate the memorization of spurious data, and
therefore, neither the privacy level. Lastly, we compare the privacy of
different model architectures trained with spurious data, demonstrating that,
contrary to prior works, architectural choice can affect privacy outcomes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.19969v1">Differential Privacy Analysis of Decentralized Gossip Averaging under
  Varying Threat Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-05-26T13:31:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antti Koskela, Tejas Kulkarni</p>
    <p><b>Summary:</b> Fully decentralized training of machine learning models offers significant
advantages in scalability, robustness, and fault tolerance. However, achieving
differential privacy (DP) in such settings is challenging due to the absence of
a central aggregator and varying trust assumptions among nodes. In this work,
we present a novel privacy analysis of decentralized gossip-based averaging
algorithms with additive node-level noise, both with and without secure
summation over each node's direct neighbors. Our main contribution is a new
analytical framework based on a linear systems formulation that accurately
characterizes privacy leakage across these scenarios. This framework
significantly improves upon prior analyses, for example, reducing the R\'enyi
DP parameter growth from $O(T^2)$ to $O(T)$, where $T$ is the number of
training rounds. We validate our analysis with numerical results demonstrating
superior DP bounds compared to existing approaches. We further illustrate our
analysis with a logistic regression experiment on MNIST image classification in
a fully decentralized setting, demonstrating utility comparable to central
aggregation methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.19951v1">Novel Loss-Enhanced Universal Adversarial Patches for Sustainable
  Speaker Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-05-26T13:16:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Elvir Karimov, Alexander Varlamov, Danil Ivanov, Dmitrii Korzh, Oleg Y. Rogov</p>
    <p><b>Summary:</b> Deep learning voice models are commonly used nowadays, but the safety
processing of personal data, such as human identity and speech content, remains
suspicious. To prevent malicious user identification, speaker anonymization
methods were proposed. Current methods, particularly based on universal
adversarial patch (UAP) applications, have drawbacks such as significant
degradation of audio quality, decreased speech recognition quality, low
transferability across different voice biometrics models, and performance
dependence on the input audio length. To mitigate these drawbacks, in this
work, we introduce and leverage the novel Exponential Total Variance (TV) loss
function and provide experimental evidence that it positively affects UAP
strength and imperceptibility. Moreover, we present a novel scalable UAP
insertion procedure and demonstrate its uniformly high performance for various
audio lengths.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.19823v1">LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning
  in Heterogeneous Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-26T11:00:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pengcheng Sun, Erwu Liu, Wei Ni, Rui Wang, Yuanzhe Geng, Lijuan Lai, Abbas Jamalipour</p>
    <p><b>Summary:</b> Federated Learning (FL) is a distributed machine learning paradigm based on
protecting data privacy of devices, which however, can still be broken by
gradient leakage attack via parameter inversion techniques. Differential
privacy (DP) technology reduces the risk of private data leakage by adding
artificial noise to the gradients, but detrimental to the FL utility at the
same time, especially in the scenario where the data is Non-Independent
Identically Distributed (Non-IID). Based on the impact of heterogeneous data on
aggregation performance, this paper proposes a Lightweight Adaptive Privacy
Allocation (LAPA) strategy, which assigns personalized privacy budgets to
devices in each aggregation round without transmitting any additional
information beyond gradients, ensuring both privacy protection and aggregation
efficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG)
algorithm is employed to optimize the transmission power, in order to determine
the optimal timing at which the adaptively attenuated artificial noise aligns
with the communication noise, enabling an effective balance between DP and
system utility. Finally, a reliable aggregation strategy is designed by
integrating communication quality and data distribution characteristics, which
improves aggregation performance while preserving privacy. Experimental results
demonstrate that the personalized noise allocation and dynamic optimization
strategy based on LAPA proposed in this paper enhances convergence performance
while satisfying the privacy requirements of FL.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.18870v1">Understanding the Relationship Between Personal Data Privacy Literacy
  and Data Privacy Information Sharing by University Students</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-24T21:14:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Brady D. Lund, Bryan Anderson, Ana Roeschley, Gahangir Hossain</p>
    <p><b>Summary:</b> With constant threats to the safety of personal data in the United States,
privacy literacy has become an increasingly important competency among
university students, one that ties intimately to the information sharing
behavior of these students. This survey based study examines how university
students in the United States perceive personal data privacy and how their
privacy literacy influences their understanding and behaviors. Students
responses to a privacy literacy scale were categorized into high and low
privacy literacy groups, revealing that high literacy individuals demonstrate a
broader range of privacy practices, including multi factor authentication, VPN
usage, and phishing awareness, whereas low literacy individuals rely on more
basic security measures. Statistical analyses suggest that high literacy
respondents display greater diversity in recommendations and engagement in
privacy discussions. These findings suggest the need for enhanced educational
initiatives to improve data privacy awareness at the university level to create
a better cyber safe population.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.18786v1">Leveraging Per-Instance Privacy for Machine Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-24T16:55:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nazanin Mohammadi Sepahvand, Anvith Thudi, Berivan Isik, Ashmita Bhattacharyya, Nicolas Papernot, Eleni Triantafillou, Daniel M. Roy, Gintare Karolina Dziugaite</p>
    <p><b>Summary:</b> We present a principled, per-instance approach to quantifying the difficulty
of unlearning via fine-tuning. We begin by sharpening an analysis of noisy
gradient descent for unlearning (Chien et al., 2024), obtaining a better
utility-unlearning tradeoff by replacing worst-case privacy loss bounds with
per-instance privacy losses (Thudi et al., 2024), each of which bounds the
(Renyi) divergence to retraining without an individual data point. To
demonstrate the practical applicability of our theory, we present empirical
results showing that our theoretical predictions are born out both for
Stochastic Gradient Langevin Dynamics (SGLD) as well as for standard
fine-tuning without explicit noise. We further demonstrate that per-instance
privacy losses correlate well with several existing data difficulty metrics,
while also identifying harder groups of data points, and introduce novel
evaluation methods based on loss barriers. All together, our findings provide a
foundation for more efficient and adaptive unlearning strategies tailored to
the unique properties of individual data points.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.18386v1">Modeling interdependent privacy threats</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-23T21:22:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuaishuai Liu, Gergely Biczók</p>
    <p><b>Summary:</b> The rise of online social networks, user-gene-rated content, and third-party
apps made data sharing an inevitable trend, driven by both user behavior and
the commercial value of personal information. As service providers amass vast
amounts of data, safeguarding individual privacy has become increasingly
challenging. Privacy threat modeling has emerged as a critical tool for
identifying and mitigating risks, with methodologies such as LINDDUN, xCOMPASS,
and PANOPTIC offering systematic approaches. However, these frameworks
primarily focus on threats arising from interactions between a single user and
system components, often overlooking interdependent privacy (IDP); the
phenomenon where one user's actions affect the privacy of other users and even
non-users. IDP risks are particularly pronounced in third-party applications,
where platform permissions, APIs, and user behavior can lead to unintended and
unconsented data sharing, such as in the Cambridge Analytica case. We argue
that existing threat modeling approaches are limited in exposing IDP-related
threats, potentially underestimating privacy risks. To bridge this gap, we
propose a specialized methodology that explicitly focuses on interdependent
privacy. Our contributions are threefold: (i) we identify IDP-specific
challenges and limitations in current threat modeling frameworks, (ii) we
create IDPA, a threat modeling approach tailored to IDP threats, and (iii) we
validate our approach through a case study on WeChat. We believe that IDPA can
operate effectively on systems other than third-party apps and may motivate
further research on specialized threat modeling.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.18242v1">Privacy-Preserving Bathroom Monitoring for Elderly Emergencies Using PIR
  and LiDAR Sensors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-23T15:49:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Youssouf Sidibé, Julia Gersey</p>
    <p><b>Summary:</b> In-home elderly monitoring requires systems that can detect emergency events
- such as falls or prolonged inactivity - while preserving privacy and
requiring no user input. These systems must be embedded into the surrounding
environment, capable of capturing activity, and responding promptly. This paper
presents a low-cost, privacy-preserving solution using Passive Infrared (PIR)
and Light Detection and Ranging (LiDAR) sensors to track entries, sitting,
exits, and emergency scenarios within a home bathroom setting. We developed and
evaluated a rule-based detection system through five real-world experiments
simulating elderly behavior. Annotated time-series graphs demonstrate the
system's ability to detect dangerous states, such as motionless collapses,
while maintaining privacy through non-visual sensing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.16954v1">Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of
  Vulnerabilities in Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-05-22T17:34:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiaying Fu, Yiyang Lu, Zehua Yang, Fiona Nah, RAY LC</p>
    <p><b>Summary:</b> Traditional methods for raising awareness of privacy protection often fail to
engage users or provide hands-on insights into how privacy vulnerabilities are
exploited. To address this, we incorporate an adversarial mechanic in the
design of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to
simulate natural interactions, the game challenges players to impersonate
characters and extract sensitive information from an AI agent, Aegis. A user
study (n=22) revealed that players employed diverse deceptive linguistic
strategies, including storytelling and emotional rapport, to manipulate Aegis.
After playing, players reported connecting in-game scenarios with real-world
privacy vulnerabilities, such as phishing and impersonation, and expressed
intentions to strengthen privacy control, such as avoiding oversharing personal
information with AI systems. This work highlights the potential of LLMs to
simulate complex relational interactions in serious games, while demonstrating
how an adversarial game strategy provides unique insights for designs for
social good, particularly privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.16371v1">Privacy-Aware Cyberterrorism Network Analysis using Graph Neural
  Networks and Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-22T08:26:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anas Ali, Mubashar Husain, Peter Hans</p>
    <p><b>Summary:</b> Cyberterrorism poses a formidable threat to digital infrastructures, with
increasing reliance on encrypted, decentralized platforms that obscure threat
actor activity. To address the challenge of analyzing such adversarial networks
while preserving the privacy of distributed intelligence data, we propose a
Privacy-Aware Federated Graph Neural Network (PA-FGNN) framework. PA-FGNN
integrates graph attention networks, differential privacy, and homomorphic
encryption into a robust federated learning pipeline tailored for
cyberterrorism network analysis. Each client trains locally on sensitive graph
data and exchanges encrypted, noise-perturbed model updates with a central
aggregator, which performs secure aggregation and broadcasts global updates. We
implement anomaly detection for flagging high-risk nodes and incorporate
defenses against gradient poisoning. Experimental evaluations on simulated dark
web and cyber-intelligence graphs demonstrate that PA-FGNN achieves over 91\%
classification accuracy, maintains resilience under 20\% adversarial client
behavior, and incurs less than 18\% communication overhead. Our results
highlight that privacy-preserving GNNs can support large-scale cyber threat
detection without compromising on utility, privacy, or robustness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.17145v1">LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy
  Compliance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-22T07:30:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yu Wang, Cailing Cai, Zhihua Xiao, Peifung E. Lam</p>
    <p><b>Summary:</b> Large language models (LLMs) are increasingly applied in fields such as
finance, education, and governance due to their ability to generate human-like
text and adapt to specialized tasks. However, their widespread adoption raises
critical concerns about data privacy and security, including the risk of
sensitive data exposure.
  In this paper, we propose a security framework to enforce policy compliance
and mitigate risks in LLM interactions. Our approach introduces three key
innovations: (i) LLM-based policy enforcement: a customizable mechanism that
enhances domain-specific detection of sensitive data. (ii) Dynamic policy
customization: real-time policy adaptation and enforcement during user-LLM
interactions to ensure compliance with evolving security requirements. (iii)
Sensitive data anonymization: a format-preserving encryption technique that
protects sensitive information while maintaining contextual integrity.
Experimental results demonstrate that our framework effectively mitigates
security risks while preserving the functional accuracy of LLM-driven tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.16059v1">Monitoring in the Dark: Privacy-Preserving Runtime Verification of
  Cyber-Physical Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Logic in Computer Science-662E9B"> 
  <p><b>Published on:</b> 2025-05-21T22:20:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Charles Koll, Preston Tan Hang, Mike Rosulek, Houssam Abbas</p>
    <p><b>Summary:</b> In distributed Cyber-Physical Systems and Internet-of-Things applications,
the nodes of the system send measurements to a monitor that checks whether
these measurements satisfy given formal specifications. For instance in Urban
Air Mobility, a local traffic authority will be monitoring drone traffic to
evaluate its flow and detect emerging problematic patterns. Certain
applications require both the specification and the measurements to be private
-- i.e. known only to their owners. Examples include traffic monitoring,
testing of integrated circuit designs, and medical monitoring by wearable or
implanted devices. In this paper we propose a protocol that enables
privacy-preserving robustness monitoring. By following our protocol, both
system (e.g. drone) and monitor (e.g. traffic authority) only learn the
robustness of the measured trace w.r.t. the specification. But the system
learns nothing about the formula, and the monitor learns nothing about the
signal monitored. We do this using garbled circuits, for specifications in
Signal Temporal Logic interpreted over timed state sequences. We analyze the
runtime and memory overhead of privacy preservation, the size of the circuits,
and their practicality for three different usage scenarios: design testing,
offline monitoring, and online monitoring of Cyber-Physical Systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.15721v1">Privacy-Preserving Conformal Prediction Under Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-05-21T16:29:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Coby Penso, Bar Mahpud, Jacob Goldberger, Or Sheffet</p>
    <p><b>Summary:</b> Conformal prediction (CP) provides sets of candidate classes with a
guaranteed probability of containing the true class. However, it typically
relies on a calibration set with clean labels. We address privacy-sensitive
scenarios where the aggregator is untrusted and can only access a perturbed
version of the true labels. We propose two complementary approaches under local
differential privacy (LDP). In the first approach, users do not access the
model but instead provide their input features and a perturbed label using a
k-ary randomized response. In the second approach, which enforces stricter
privacy constraints, users add noise to their conformity score by binary search
response. This method requires access to the classification model but preserves
both data and label privacy. Both approaches compute the conformal threshold
directly from noisy data without accessing the true labels. We prove
finite-sample coverage guarantees and demonstrate robust coverage even under
severe randomization. This approach unifies strong local privacy with
predictive uncertainty control, making it well-suited for sensitive
applications such as medical imaging or large language model queries,
regardless of whether users can (or are willing to) compute their own scores.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.15483v1">Optimal Piecewise-based Mechanism for Collecting Bounded Numerical Data
  under Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-05-21T13:01:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ye Zheng, Sumita Mishra, Yidan Hu</p>
    <p><b>Summary:</b> Numerical data with bounded domains is a common data type in personal
devices, such as wearable sensors. While the collection of such data is
essential for third-party platforms, it raises significant privacy concerns.
Local differential privacy (LDP) has been shown as a framework providing
provable individual privacy, even when the third-party platform is untrusted.
For numerical data with bounded domains, existing state-of-the-art LDP
mechanisms are piecewise-based mechanisms, which are not optimal, leading to
reduced data utility.
  This paper investigates the optimal design of piecewise-based mechanisms to
maximize data utility under LDP. We demonstrate that existing piecewise-based
mechanisms are heuristic forms of the $3$-piecewise mechanism, which is far
from enough to study optimality. We generalize the $3$-piecewise mechanism to
its most general form, i.e. $m$-piecewise mechanism with no pre-defined form of
each piece. Under this form, we derive the closed-form optimal mechanism by
combining analytical proofs and off-the-shelf optimization solvers. Next, we
extend the generalized piecewise-based mechanism to the circular domain (along
with the classical domain), defined on a cyclic range where the distance
between the two endpoints is zero. By incorporating this property, we design
the optimal mechanism for the circular domain, achieving significantly improved
data utility compared with existing mechanisms.
  Our proposed mechanisms guarantee optimal data utility under LDP among all
generalized piecewise-based mechanisms. We show that they also achieve optimal
data utility in two common applications of LDP: distribution estimation and
mean estimation. Theoretical analyses and experimental evaluations prove and
validate the data utility advantages of our proposed mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.15476v1">Pura: An Efficient Privacy-Preserving Solution for Face Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-21T12:50:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guotao Xu, Bowen Zhao, Yang Xiao, Yantao Zhong, Liang Zhai, Qingqi Pei</p>
    <p><b>Summary:</b> Face recognition is an effective technology for identifying a target person
by facial images. However, sensitive facial images raises privacy concerns.
Although privacy-preserving face recognition is one of potential solutions,
this solution neither fully addresses the privacy concerns nor is efficient
enough. To this end, we propose an efficient privacy-preserving solution for
face recognition, named Pura, which sufficiently protects facial privacy and
supports face recognition over encrypted data efficiently. Specifically, we
propose a privacy-preserving and non-interactive architecture for face
recognition through the threshold Paillier cryptosystem. Additionally, we
carefully design a suite of underlying secure computing protocols to enable
efficient operations of face recognition over encrypted data directly.
Furthermore, we introduce a parallel computing mechanism to enhance the
performance of the proposed secure computing protocols. Privacy analysis
demonstrates that Pura fully safeguards personal facial privacy. Experimental
evaluations demonstrate that Pura achieves recognition speeds up to 16 times
faster than the state-of-the-art.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.15376v1">Federated Learning-Enhanced Blockchain Framework for Privacy-Preserving
  Intrusion Detection in Industrial IoT</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-21T11:11:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anas Ali, Mubashar Husain, Peter Hans</p>
    <p><b>Summary:</b> Industrial Internet of Things (IIoT) systems have become integral to smart
manufacturing, yet their growing connectivity has also exposed them to
significant cybersecurity threats. Traditional intrusion detection systems
(IDS) often rely on centralized architectures that raise concerns over data
privacy, latency, and single points of failure. In this work, we propose a
novel Federated Learning-Enhanced Blockchain Framework (FL-BCID) for
privacy-preserving intrusion detection tailored for IIoT environments. Our
architecture combines federated learning (FL) to ensure decentralized model
training with blockchain technology to guarantee data integrity, trust, and
tamper resistance across IIoT nodes. We design a lightweight intrusion
detection model collaboratively trained using FL across edge devices without
exposing sensitive data. A smart contract-enabled blockchain system records
model updates and anomaly scores to establish accountability. Experimental
evaluations using the ToN-IoT and N-BaIoT datasets demonstrate the superior
performance of our framework, achieving 97.3% accuracy while reducing
communication overhead by 41% compared to baseline centralized methods. Our
approach ensures privacy, scalability, and robustness-critical for secure
industrial operations. The proposed FL-BCID system provides a promising
solution for enhancing trust and privacy in modern IIoT security architectures.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.15156v1">Privacy-Preserving Socialized Recommendation based on Multi-View
  Clustering in a Cloud Environment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-21T06:21:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Cheng Guo, Jing Jia, Peng Wang, Jing Zhang</p>
    <p><b>Summary:</b> Recommendation as a service has improved the quality of our lives and plays a
significant role in variant aspects. However, the preference of users may
reveal some sensitive information, so that the protection of privacy is
required. In this paper, we propose a privacy-preserving, socialized,
recommendation protocol that introduces information collected from online
social networks to enhance the quality of the recommendation. The proposed
scheme can calculate the similarity between users to determine their potential
relationships and interests, and it also can protect the users' privacy from
leaking to an untrusted third party. The security analysis and experimental
results showed that our proposed scheme provides excellent performance and is
feasible for real-world applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.14959v1">Privacy Preserving Conversion Modeling in Data Clean Room</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB"> 
  <p><b>Published on:</b> 2025-05-20T22:38:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kungang Li, Xiangyi Chen, Ling Leng, Jiajing Xu, Jiankai Sun, Behnam Rezaei</p>
    <p><b>Summary:</b> In the realm of online advertising, accurately predicting the conversion rate
(CVR) is crucial for enhancing advertising efficiency and user satisfaction.
This paper addresses the challenge of CVR prediction while adhering to user
privacy preferences and advertiser requirements. Traditional methods face
obstacles such as the reluctance of advertisers to share sensitive conversion
data and the limitations of model training in secure environments like data
clean rooms. We propose a novel model training framework that enables
collaborative model training without sharing sample-level gradients with the
advertising platform. Our approach introduces several innovative components:
(1) utilizing batch-level aggregated gradients instead of sample-level
gradients to minimize privacy risks; (2) applying adapter-based
parameter-efficient fine-tuning and gradient compression to reduce
communication costs; and (3) employing de-biasing techniques to train the model
under label differential privacy, thereby maintaining accuracy despite
privacy-enhanced label perturbations. Our experimental results, conducted on
industrial datasets, demonstrate that our method achieves competitive ROCAUC
performance while significantly decreasing communication overhead and complying
with both advertiser privacy requirements and user privacy choices. This
framework establishes a new standard for privacy-preserving, high-performance
CVR prediction in the digital advertising landscape.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.14797v1">Efficient Privacy-Preserving Cross-Silo Federated Learning with
  Multi-Key Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-20T18:08:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abdullah Al Omar, Xin Yang, Euijin Choo, Omid Ardakanian</p>
    <p><b>Summary:</b> Federated Learning (FL) is susceptible to privacy attacks, such as data
reconstruction attacks, in which a semi-honest server or a malicious client
infers information about other clients' datasets from their model updates or
gradients. To enhance the privacy of FL, recent studies combined Multi-Key
Homomorphic Encryption (MKHE) and FL, making it possible to aggregate the
encrypted model updates using different keys without having to decrypt them.
Despite the privacy guarantees of MKHE, existing approaches are not well-suited
for real-world deployment due to their high computation and communication
overhead. We propose MASER, an efficient MKHE-based Privacy-Preserving FL
framework that combines consensus-based model pruning and slicing techniques to
reduce this overhead. Our experimental results show that MASER is 3.03 to 8.29
times more efficient than existing MKHE-based FL approaches in terms of
computation and communication overhead while maintaining comparable
classification accuracy to standard FL algorithms. Compared to a vanilla FL
algorithm, the overhead of MASER is only 1.48 to 5 times higher, striking a
good balance between privacy, accuracy, and efficiency in both IID and non-IID
settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.14585v1">Context Reasoner: Incentivizing Reasoning Capability for Contextualized
  Privacy and Safety Compliance via Reinforcement Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-20T16:40:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenbin Hu, Haoran Li, Huihao Jing, Qi Hu, Ziqian Zeng, Sirui Han, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song</p>
    <p><b>Summary:</b> While Large Language Models (LLMs) exhibit remarkable capabilities, they also
introduce significant safety and privacy risks. Current mitigation strategies
often fail to preserve contextual reasoning capabilities in risky scenarios.
Instead, they rely heavily on sensitive pattern matching to protect LLMs, which
limits the scope. Furthermore, they overlook established safety and privacy
standards, leading to systemic risks for legal compliance. To address these
gaps, we formulate safety and privacy issues into contextualized compliance
problems following the Contextual Integrity (CI) theory. Under the CI
framework, we align our model with three critical regulatory standards: GDPR,
EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with
a rule-based reward to incentivize contextual reasoning capabilities while
enhancing compliance with safety and privacy norms. Through extensive
experiments, we demonstrate that our method not only significantly enhances
legal compliance (achieving a +17.64% accuracy improvement in safety/privacy
benchmarks) but also further improves general reasoning capability. For
OpenThinker-7B, a strong reasoning model that significantly outperforms its
base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its
general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on
the MMLU and LegalBench benchmark, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.14507v1">Federated prediction for scalable and privacy-preserved knowledge-based
  planning in radiotherapy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-05-20T15:35:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jingyun Chen, David Horowitz, Yading Yuan</p>
    <p><b>Summary:</b> Background: Deep learning has potential to improve the efficiency and
consistency of radiation therapy planning, but clinical adoption is hindered by
the limited model generalizability due to data scarcity and heterogeneity among
institutions. Although aggregating data from different institutions could
alleviate this problem, data sharing is a practical challenge due to concerns
about patient data privacy and other technical obstacles. Purpose: This work
aims to address this dilemma by developing FedKBP+, a comprehensive federated
learning (FL) platform for predictive tasks in real-world applications in
radiotherapy treatment planning. Methods: We implemented a unified
communication stack based on Google Remote Procedure Call (gRPC) to support
communication between participants whether located on the same workstation or
distributed across multiple workstations. In addition to supporting the
centralized FL strategies commonly available in existing open-source
frameworks, FedKBP+ also provides a fully decentralized FL model where
participants directly exchange model weights to each other through Peer-to-Peer
communication. We evaluated FedKBP+ on three predictive tasks using
scale-attention network (SA-Net) as the predictive model. Conclusions: Our
results demonstrate that FedKBP+ is highly effective, efficient and robust,
showing great potential as a federated learning platform for radiation therapy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.14195v1">Unraveling Interwoven Roles of Large Language Models in Authorship
  Privacy: Obfuscation, Mimicking, and Verification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-20T10:52:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tuc Nguyen, Yifan Hu, Thai Le</p>
    <p><b>Summary:</b> Recent advancements in large language models (LLMs) have been fueled by large
scale training corpora drawn from diverse sources such as websites, news
articles, and books. These datasets often contain explicit user information,
such as person names and addresses, that LLMs may unintentionally reproduce in
their generated outputs. Beyond such explicit content, LLMs can also leak
identity revealing cues through implicit signals such as distinctive writing
styles, raising significant concerns about authorship privacy. There are three
major automated tasks in authorship privacy, namely authorship obfuscation
(AO), authorship mimicking (AM), and authorship verification (AV). Prior
research has studied AO, AM, and AV independently. However, their interplays
remain under explored, which leaves a major research gap, especially in the era
of LLMs, where they are profoundly shaping how we curate and share user
generated content, and the distinction between machine generated and human
authored text is also increasingly blurred. This work then presents the first
unified framework for analyzing the dynamic relationships among LLM enabled AO,
AM, and AV in the context of authorship privacy. We quantify how they interact
with each other to transform human authored text, examining effects at a single
point in time and iteratively over time. We also examine the role of
demographic metadata, such as gender, academic background, in modulating their
performances, inter-task dynamics, and privacy risks. All source code will be
publicly available.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.13957v1">Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal
  Retrieval-Augmented Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-20T05:37:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiankun Zhang, Shenglai Zeng, Jie Ren, Tianqi Zheng, Hui Liu, Xianfeng Tang, Hui Liu, Yi Chang</p>
    <p><b>Summary:</b> Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by
integrating external multimodal databases, but introduce unexplored privacy
vulnerabilities. While text-based RAG privacy risks have been studied,
multimodal data presents unique challenges. We provide the first systematic
analysis of MRAG privacy vulnerabilities across vision-language and
speech-language modalities. Using a novel compositional structured prompt
attack in a black-box setting, we demonstrate how attackers can extract private
information by manipulating queries. Our experiments reveal that LMMs can both
directly generate outputs resembling retrieved content and produce descriptions
that indirectly expose sensitive information, highlighting the urgent need for
robust privacy-preserving MRAG techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.13694v1">A Systematic Review and Taxonomy for Privacy Breach Classification:
  Trends, Gaps, and Future Directions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-05-19T19:52:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Clint Fuchs, John D. Hastings</p>
    <p><b>Summary:</b> In response to the rising frequency and complexity of data breaches and
evolving global privacy regulations, this study presents a comprehensive
examination of academic literature on the classification of privacy breaches
and violations between 2010-2024. Through a systematic literature review, a
corpus of screened studies was assembled and analyzed to identify primary
research themes, emerging trends, and gaps in the field. A novel taxonomy is
introduced to guide efforts by categorizing research efforts into seven
domains: breach classification, report classification, breach detection, threat
detection, breach prediction, risk analysis, and threat classification. An
analysis reveals that breach classification and detection dominate the
literature, while breach prediction and risk analysis have only recently
emerged in the literature, suggesting opportunities for potential research
impacts. Keyword and phrase frequency analysis reveal potentially underexplored
areas, including location privacy, prediction models, and healthcare data
breaches.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.13655v1">Optimal Client Sampling in Federated Learning with Client-Level
  Heterogeneous Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-19T18:55:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiahao Xu, Rui Hu, Olivera Kotevska</p>
    <p><b>Summary:</b> Federated Learning with client-level differential privacy (DP) provides a
promising framework for collaboratively training models while rigorously
protecting clients' privacy. However, classic approaches like DP-FedAvg
struggle when clients have heterogeneous privacy requirements, as they must
uniformly enforce the strictest privacy level across clients, leading to
excessive DP noise and significant model utility degradation. Existing methods
to improve the model utility in such heterogeneous privacy settings often
assume a trusted server and are largely heuristic, resulting in suboptimal
performance and lacking strong theoretical underpinnings. In this work, we
address these challenges under a practical attack model where both clients and
the server are honest-but-curious. We propose GDPFed, which partitions clients
into groups based on their privacy budgets and achieves client-level DP within
each group to reduce the privacy budget waste and hence improve the model
utility. Based on the privacy and convergence analysis of GDPFed, we find that
the magnitude of DP noise depends on both model dimensionality and the
per-group client sampling ratios. To further improve the performance of GDPFed,
we introduce GDPFed$^+$, which integrates model sparsification to eliminate
unnecessary noise and optimizes per-group client sampling ratios to minimize
convergence error. Extensive empirical evaluations on multiple benchmark
datasets demonstrate the effectiveness of GDPFed$^+$, showing substantial
performance gains compared with state-of-the-art methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.13292v1">Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms
  of AI Systems by Integrating Federated Learning and LLMs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-19T16:14:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Huaiying Luo, Cheng Ji</p>
    <p><b>Summary:</b> In the age of cloud computing, data privacy protection has become a major
challenge, especially when sharing sensitive data across cloud environments.
However, how to optimize collaboration across cloud environments remains an
unresolved problem. In this paper, we combine federated learning with
large-scale language models to optimize the collaborative mechanism of AI
systems. Based on the existing federated learning framework, we introduce a
cross-cloud architecture in which federated learning works by aggregating model
updates from decentralized nodes without exposing the original data. At the
same time, combined with large-scale language models, its powerful context and
semantic understanding capabilities are used to improve model training
efficiency and decision-making ability. We've further innovated by introducing
a secure communication layer to ensure the privacy and integrity of model
updates and training data. The model enables continuous model adaptation and
fine-tuning across different cloud environments while protecting sensitive
data. Experimental results show that the proposed method is significantly
better than the traditional federated learning model in terms of accuracy,
convergence speed and data privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.13085v2">Universal Semantic Disentangled Privacy-preserving Speech Representation
  Learning</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-19T13:19:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Biel Tura Vecino, Subhadeep Maji, Aravind Varier, Antonio Bonafonte, Ivan Valles, Michael Owen, Leif Rädel, Grant Strimel, Seyi Feyisetan, Roberto Barra Chicote, Ariya Rastrow, Constantinos Papayiannis, Volker Leutnant, Trevor Wood</p>
    <p><b>Summary:</b> The use of audio recordings of human speech to train LLMs poses privacy
concerns due to these models' potential to generate outputs that closely
resemble artifacts in the training data. In this study, we propose a speaker
privacy-preserving representation learning method through the Universal Speech
Codec (USC), a computationally efficient encoder-decoder model that
disentangles speech into: (i) privacy-preserving semantically rich
representations, capturing content and speech paralinguistics, and (ii)
residual acoustic and speaker representations that enables high-fidelity
reconstruction. Extensive evaluations presented show that USC's semantic
representation preserves content, prosody, and sentiment, while removing
potentially identifiable speaker attributes. Combining both representations,
USC achieves state-of-the-art speech reconstruction. Additionally, we introduce
an evaluation methodology for measuring privacy-preserving properties, aligning
with perceptual tests. We compare USC against other codecs in the literature
and demonstrate its effectiveness on privacy-preserving representation
learning, illustrating the trade-offs of speaker anonymization, paralinguistics
retention and content preservation in the learned semantic representations.
Audio samples are shared in https://www.amazon.science/usc-samples.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12954v1">Counting Graphlets of Size $k$ under Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computational Complexity-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-05-19T10:46:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vorapong Suppakitpaisarn, Donlapark Ponnoprat, Nicha Hirankarn, Quentin Hillebrand</p>
    <p><b>Summary:</b> The problem of counting subgraphs or graphlets under local differential
privacy is an important challenge that has attracted significant attention from
researchers. However, much of the existing work focuses on small graphlets like
triangles or $k$-stars. In this paper, we propose a non-interactive, locally
differentially private algorithm capable of counting graphlets of any size $k$.
When $n$ is the number of nodes in the input graph, we show that the expected
$\ell_2$ error of our algorithm is $O(n^{k - 1})$. Additionally, we prove that
there exists a class of input graphs and graphlets of size $k$ for which any
non-interactive counting algorithm incurs an expected $\ell_2$ error of
$\Omega(n^{k - 1})$, demonstrating the optimality of our result. Furthermore,
we establish that for certain input graphs and graphlets, any locally
differentially private algorithm must have an expected $\ell_2$ error of
$\Omega(n^{k - 1.5})$. Our experimental results show that our algorithm is more
accurate than the classical randomized response method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12869v1">Outsourced Privacy-Preserving Feature Selection Based on Fully
  Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-19T08:55:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Koki Wakiyama, Tomohiro I, Hiroshi Sakamoto</p>
    <p><b>Summary:</b> Feature selection is a technique that extracts a meaningful subset from a set
of features in training data. When the training data is large-scale,
appropriate feature selection enables the removal of redundant features, which
can improve generalization performance, accelerate the training process, and
enhance the interpretability of the model. This study proposes a
privacy-preserving computation model for feature selection. Generally, when the
data owner and analyst are the same, there is no need to conceal the private
information. However, when they are different parties or when multiple owners
exist, an appropriate privacy-preserving framework is required. Although
various private feature selection algorithms, they all require two or more
computing parties and do not guarantee security in environments where no
external party can be fully trusted. To address this issue, we propose the
first outsourcing algorithm for feature selection using fully homomorphic
encryption. Compared to a prior two-party algorithm, our result improves the
time and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n
denote the number of features and data samples, respectively. We also
implemented the proposed algorithm and conducted comparative experiments with
the naive one. The experimental result shows the efficiency of our method even
with small datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12688v1">Shielding Latent Face Representations From Privacy Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-19T04:23:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Arjun Ramesh Kaushik, Bharat Chandra Yalavarthi, Arun Ross, Vishnu Boddeti, Nalini Ratha</p>
    <p><b>Summary:</b> In today's data-driven analytics landscape, deep learning has become a
powerful tool, with latent representations, known as embeddings, playing a
central role in several applications. In the face analytics domain, such
embeddings are commonly used for biometric recognition (e.g., face
identification). However, these embeddings, or templates, can inadvertently
expose sensitive attributes such as age, gender, and ethnicity. Leaking such
information can compromise personal privacy and affect civil liberty and human
rights. To address these concerns, we introduce a multi-layer protection
framework for embeddings. It consists of a sequence of operations: (a)
encrypting embeddings using Fully Homomorphic Encryption (FHE), and (b) hashing
them using irreversible feature manifold hashing. Unlike conventional
encryption methods, FHE enables computations directly on encrypted data,
allowing downstream analytics while maintaining strong privacy guarantees. To
reduce the overhead of encrypted processing, we employ embedding compression.
Our proposed method shields latent representations of sensitive data from
leaking private attributes (such as age and gender) while retaining essential
functional capabilities (such as face identification). Extensive experiments on
two datasets using two face encoders demonstrate that our approach outperforms
several state-of-the-art privacy protection methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12610v1">hChain: Blockchain Based Large Scale EHR Data Sharing with Enhanced
  Security and Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-19T01:47:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Musharraf Alruwaill, Saraju Mohanty, Elias Kougianos</p>
    <p><b>Summary:</b> Concerns regarding privacy and data security in conventional healthcare
prompted alternative technologies. In smart healthcare, blockchain technology
addresses existing concerns with security, privacy, and electronic healthcare
transmission. Integration of Blockchain Technology with the Internet of Medical
Things (IoMT) allows real-time monitoring of protected healthcare data.
Utilizing edge devices with IoMT devices is very advantageous for addressing
security, computing, and storage challenges. Encryption using symmetric and
asymmetric keys is used to conceal sensitive information from unauthorized
parties. SHA256 is an algorithm for one-way hashing. It is used to verify that
the data has not been altered, since if it had, the hash value would have
changed. This article offers a blockchain-based smart healthcare system using
IoMT devices for continuous patient monitoring. In addition, it employs edge
resources in addition to IoMT devices to have extra computing power and storage
to hash and encrypt incoming data before sending it to the blockchain.
Symmetric key is utilized to keep the data private even in the blockchain,
allowing the patient to safely communicate the data through smart contracts
while preventing unauthorized physicians from seeing the data. Through the use
of a verification node and blockchain, an asymmetric key is used for the
signing and validation of patient data in the healthcare provider system. In
addition to other security measures, location-based authentication is
recommended to guarantee that data originates from the patient area. Through
the edge device, SHA256 is utilized to secure the data's integrity and a secret
key is used to maintain its secrecy. The hChain architecture improves the
computing power of IoMT environments, the security of EHR sharing through smart
contracts, and the privacy and authentication procedures.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12239v1">ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting
  with Privacy Preservation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-18T05:28:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jianheng Tang, Huiping Zhuang, Di Fang, Jiaxu Li, Feijiang Han, Yajiang Huang, Kejia Fan, Leye Wang, Zhanxing Zhu, Shanghang Zhang, Houbing Herbert Song, Yunhuai Liu</p>
    <p><b>Summary:</b> The development of artificial intelligence demands that models incrementally
update knowledge by Continual Learning (CL) to adapt to open-world
environments. To meet privacy and security requirements, Continual Unlearning
(CU) emerges as an important problem, aiming to sequentially forget particular
knowledge acquired during the CL phase. However, existing unlearning methods
primarily focus on single-shot joint forgetting and face significant
limitations when applied to CU. First, most existing methods require access to
the retained dataset for re-training or fine-tuning, violating the inherent
constraint in CL that historical data cannot be revisited. Second, these
methods often suffer from a poor trade-off between system efficiency and model
fidelity, making them vulnerable to being overwhelmed or degraded by
adversaries through deliberately frequent requests. In this paper, we identify
that the limitations of existing unlearning methods stem fundamentally from
their reliance on gradient-based updates. To bridge the research gap at its
root, we propose a novel gradient-free method for CU, named Analytic Continual
Unlearning (ACU), for efficient and exact forgetting with historical data
privacy preservation. In response to each unlearning request, our ACU
recursively derives an analytical (i.e., closed-form) solution in an
interpretable manner using the least squares method. Theoretical and
experimental evaluations validate the superiority of our ACU on unlearning
effectiveness, model fidelity, and system efficiency.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12153v1">Federated Deep Reinforcement Learning for Privacy-Preserving
  Robotic-Assisted Surgery</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E">
  <p><b>Published on:</b> 2025-05-17T22:02:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sana Hafeez, Sundas Rafat Mulkana, Muhammad Ali Imran, Michele Sevegnani</p>
    <p><b>Summary:</b> The integration of Reinforcement Learning (RL) into robotic-assisted surgery
(RAS) holds significant promise for advancing surgical precision, adaptability,
and autonomous decision-making. However, the development of robust RL models in
clinical settings is hindered by key challenges, including stringent patient
data privacy regulations, limited access to diverse surgical datasets, and high
procedural variability. To address these limitations, this paper presents a
Federated Deep Reinforcement Learning (FDRL) framework that enables
decentralized training of RL models across multiple healthcare institutions
without exposing sensitive patient information. A central innovation of the
proposed framework is its dynamic policy adaptation mechanism, which allows
surgical robots to select and tailor patient-specific policies in real-time,
thereby ensuring personalized and Optimised interventions. To uphold rigorous
privacy standards while facilitating collaborative learning, the FDRL framework
incorporates secure aggregation, differential privacy, and homomorphic
encryption techniques. Experimental results demonstrate a 60\% reduction in
privacy leakage compared to conventional methods, with surgical precision
maintained within a 1.5\% margin of a centralized baseline. This work
establishes a foundational approach for adaptive, secure, and patient-centric
AI-driven surgical robotics, offering a pathway toward clinical translation and
scalable deployment across diverse healthcare environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12144v1">Proof-of-Social-Capital: Privacy-Preserving Consensus Protocol Replacing
  Stake for Social Capital</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-05-17T21:28:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Juraj Mariani, Ivan Homoliak</p>
    <p><b>Summary:</b> Consensus protocols used today in blockchains often rely on computational
power or financial stakes - scarce resources. We propose a novel protocol using
social capital - trust and influence from social interactions - as a
non-transferable staking mechanism to ensure fairness and decentralization. The
methodology integrates zero-knowledge proofs, verifiable credentials, a
Whisk-like leader election, and an incentive scheme to prevent Sybil attacks
and encourage engagement. The theoretical framework would enhance privacy and
equity, though unresolved issues like off-chain bribery require further
research. This work offers a new model aligned with modern social media
behavior and lifestyle, with applications in finance, providing a practical
insight for decentralized system development.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.11094v1">Blockchain-Enabled Decentralized Privacy-Preserving Group Purchasing for
  Energy Plans</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-16T10:26:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sid Chi-Kin Chau, Yue Zhou</p>
    <p><b>Summary:</b> Retail energy markets are increasingly consumer-oriented, thanks to a growing
number of energy plans offered by a plethora of energy suppliers, retailers and
intermediaries. To maximize the benefits of competitive retail energy markets,
group purchasing is an emerging paradigm that aggregates consumers' purchasing
power by coordinating switch decisions to specific energy providers for
discounted energy plans. Traditionally, group purchasing is mediated by a
trusted third-party, which suffers from the lack of privacy and transparency.
In this paper, we introduce a novel paradigm of decentralized
privacy-preserving group purchasing, empowered by privacy-preserving blockchain
and secure multi-party computation, to enable users to form a coalition for
coordinated switch decisions in a decentralized manner, without a trusted
third-party. The coordinated switch decisions are determined by a competitive
online algorithm, based on users' private consumption data and current energy
plan tariffs. Remarkably, no private user consumption data will be revealed to
others in the online decision-making process, which is carried out in a
transparently verifiable manner to eliminate frauds from dishonest users and
supports fair mutual compensations by sharing the switching costs to
incentivize group purchasing. We implemented our decentralized group purchasing
solution as a smart contract on Solidity-supported blockchain platform (e.g.,
Ethereum), and provide extensive empirical evaluation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10965v1">Privacy and Confidentiality Requirements Engineering for Process Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-16T08:03:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fabian Haertel, Juergen Mangler, Nataliia Klievtsova, Celine Mader, Eugen Rigger, Stefanie Rinderle-Ma</p>
    <p><b>Summary:</b> The application and development of process mining techniques face significant
challenges due to the lack of publicly available real-life event logs. One
reason for companies to abstain from sharing their data are privacy and
confidentiality concerns. Privacy concerns refer to personal data as specified
in the GDPR and have been addressed in existing work by providing
privacy-preserving techniques for event logs. However, the concept of
confidentiality in event logs not pertaining to individuals remains unclear,
although they might contain a multitude of sensitive business data. This work
addresses confidentiality of process data based on the privacy and
confidentiality engineering method (PCRE). PCRE interactively explores privacy
and confidentiality requirements regarding process data with different
stakeholders and defines privacy-preserving actions to address possible
concerns. We co-construct and evaluate PCRE based on structured interviews with
process analysts in two manufacturing companies. PCRE is generic, hence
applicable in different application domains. The goal is to systematically
scrutinize process data and balance the trade-off between privacy and utility
loss.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10941v1">Privacy-Aware Lifelong Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-16T07:27:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ozan Özdenizci, Elmar Rueckert, Robert Legenstein</p>
    <p><b>Summary:</b> Lifelong learning algorithms enable models to incrementally acquire new
knowledge without forgetting previously learned information. Contrarily, the
field of machine unlearning focuses on explicitly forgetting certain previous
knowledge from pretrained models when requested, in order to comply with data
privacy regulations on the right-to-be-forgotten. Enabling efficient lifelong
learning with the capability to selectively unlearn sensitive information from
models presents a critical and largely unaddressed challenge with contradicting
objectives. We address this problem from the perspective of simultaneously
preventing catastrophic forgetting and allowing forward knowledge transfer
during task-incremental learning, while ensuring exact task unlearning and
minimizing memory requirements, based on a single neural network model to be
adapted. Our proposed solution, privacy-aware lifelong learning (PALL),
involves optimization of task-specific sparse subnetworks with parameter
sharing within a single architecture. We additionally utilize an episodic
memory rehearsal mechanism to facilitate exact unlearning without performance
degradations. We empirically demonstrate the scalability of PALL across various
architectures in image classification, and provide a state-of-the-art solution
that uniquely integrates lifelong learning and privacy-aware unlearning
mechanisms for responsible AI applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10871v1">Optimal Allocation of Privacy Budget on Hierarchical Data Release</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-05-16T05:25:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Joonhyuk Ko, Juba Ziani, Ferdinando Fioretto</p>
    <p><b>Summary:</b> Releasing useful information from datasets with hierarchical structures while
preserving individual privacy presents a significant challenge. Standard
privacy-preserving mechanisms, and in particular Differential Privacy, often
require careful allocation of a finite privacy budget across different levels
and components of the hierarchy. Sub-optimal allocation can lead to either
excessive noise, rendering the data useless, or to insufficient protections for
sensitive information. This paper addresses the critical problem of optimal
privacy budget allocation for hierarchical data release. It formulates this
challenge as a constrained optimization problem, aiming to maximize data
utility subject to a total privacy budget while considering the inherent
trade-offs between data granularity and privacy loss. The proposed approach is
supported by theoretical analysis and validated through comprehensive
experiments on real hierarchical datasets. These experiments demonstrate that
optimal privacy budget allocation significantly enhances the utility of the
released data and improves the performance of downstream tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10496v1">CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of
  Synthetic Chest Radiographs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-15T16:59:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Raman Dutt, Pedro Sanchez, Yongchen Yao, Steven McDonagh, Sotirios A. Tsaftaris, Timothy Hospedales</p>
    <p><b>Summary:</b> We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10264v1">Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack
  in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-15T13:16:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Francesco Diana, André Nusser, Chuan Xu, Giovanni Neglia</p>
    <p><b>Summary:</b> Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.09929v1">Security and Privacy Measurement on Chinese Consumer IoT Traffic based
  on Device Lifecycle</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-15T03:27:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenghua Jin, Yan Jia, Yuxin Song, Qingyin Tan, Rui Yang, Zheli Liu</p>
    <p><b>Summary:</b> In recent years, consumer Internet of Things (IoT) devices have become widely
used in daily life. With the popularity of devices, related security and
privacy risks arise at the same time as they collect user-related data and
transmit it to various service providers. Although China accounts for a larger
share of the consumer IoT industry, current analyses on consumer IoT device
traffic primarily focus on regions such as Europe, the United States, and
Australia. Research on China, however, is currently rather rare. This study
constructs the first large-scale dataset about consumer IoT device traffic in
China. Specifically, we propose a fine-grained traffic collection guidance
covering the entire lifecycle of consumer IoT devices, gathering traffic from
70 devices spanning 36 brands and 8 device categories. Based on this dataset,
we analyze traffic destinations and encryption practices across different
device types during the entire lifecycle and compare the findings with the
results of other regions. Compared to other regions, our results show that
consumer IoT devices in China rely more on domestic services and overally
perform better in terms of encryption practices. However, there are still 20/35
devices improperly conduct certificate validation, and 5/70 devices use
insecure encryption protocols. To facilitate future research, we open-source
our traffic collection guidance and make our dataset publicly available.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.09921v2">PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative
  In-Context Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-15T03:11:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yidan Wang, Yanan Cao, Yubing Ren, Fang Fang, Zheng Lin, Binxing Fang</p>
    <p><b>Summary:</b> Large Language Models (LLMs) excel in various domains but pose inherent
privacy risks. Existing methods to evaluate privacy leakage in LLMs often use
memorized prefixes or simple instructions to extract data, both of which
well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM
safety mechanisms to generate harmful content, but their role in privacy
scenarios remains underexplored. In this paper, we examine the effectiveness of
jailbreak attacks in extracting sensitive information, bridging privacy leakage
and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework
targeting Personally Identifiable Information (PII) and addressing the
limitations of current jailbreak methods. Specifically, PIG identifies PII
entities and their types in privacy queries, uses in-context learning to build
a privacy context, and iteratively updates it with three gradient-based
strategies to elicit target PII. We evaluate PIG and existing jailbreak methods
using two privacy-related datasets. Experiments on four white-box and two
black-box LLMs show that PIG outperforms baseline methods and achieves
state-of-the-art (SoTA) results. The results underscore significant privacy
risks in LLMs, emphasizing the need for stronger safeguards. Our code is
availble at https://github.com/redwyd/PrivacyJailbreak.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.09276v1">Privacy-Preserving Runtime Verification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Formal Languages and Automata Theory-D91E36">
  <p><b>Published on:</b> 2025-05-14T10:49:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Thomas A. Henzinger, Mahyar Karimi, K. S. Thejaswini</p>
    <p><b>Summary:</b> Runtime verification offers scalable solutions to improve the safety and
reliability of systems. However, systems that require verification or
monitoring by a third party to ensure compliance with a specification might
contain sensitive information, causing privacy concerns when usual runtime
verification approaches are used. Privacy is compromised if protected
information about the system, or sensitive data that is processed by the
system, is revealed. In addition, revealing the specification being monitored
may undermine the essence of third-party verification.
  In this work, we propose two novel protocols for the privacy-preserving
runtime verification of systems against formal sequential specifications. In
our first protocol, the monitor verifies whether the system satisfies the
specification without learning anything else, though both parties are aware of
the specification. Our second protocol ensures that the system remains
oblivious to the monitored specification, while the monitor learns only whether
the system satisfies the specification and nothing more. Our protocols adapt
and improve existing techniques used in cryptography, and more specifically,
multi-party computation.
  The sequential specification defines the observation step of the monitor,
whose granularity depends on the situation (e.g., banks may be monitored on a
daily basis). Our protocols exchange a single message per observation step,
after an initialisation phase. This design minimises communication overhead,
enabling relatively lightweight privacy-preserving monitoring. We implement our
approach for monitoring specifications described by register automata and
evaluate it experimentally.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.08719v1">PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-13T16:27:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yang Su, Na Yan, Yansha Deng, Robert Schober</p>
    <p><b>Summary:</b> Large language models (LLMs) hosted on cloud servers alleviate the
computational and storage burdens on local devices but raise privacy concerns
due to sensitive data transmission and require substantial communication
bandwidth, which is challenging in constrained environments. In contrast, small
language models (SLMs) running locally enhance privacy but suffer from limited
performance on complex tasks. To balance computational cost, performance, and
privacy protection under bandwidth constraints, we propose a privacy-aware
wireless collaborative mixture of experts (PWC-MoE) framework. Specifically,
PWC-MoE employs a sparse privacy-aware gating network to dynamically route
sensitive tokens to privacy experts located on local clients, while
non-sensitive tokens are routed to non-privacy experts located at the remote
base station. To achieve computational efficiency, the gating network ensures
that each token is dynamically routed to and processed by only one expert. To
enhance scalability and prevent overloading of specific experts, we introduce a
group-wise load-balancing mechanism for the gating network that evenly
distributes sensitive tokens among privacy experts and non-sensitive tokens
among non-privacy experts. To adapt to bandwidth constraints while preserving
model performance, we propose a bandwidth-adaptive and importance-aware token
offloading scheme. This scheme incorporates an importance predictor to evaluate
the importance scores of non-sensitive tokens, prioritizing the most important
tokens for transmission to the base station based on their predicted importance
and the available bandwidth. Experiments demonstrate that the PWC-MoE framework
effectively preserves privacy and maintains high performance even in
bandwidth-constrained environments, offering a practical solution for deploying
LLMs in privacy-sensitive and bandwidth-limited scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.08847v1">On the interplay of Explainability, Privacy and Predictive Performance
  with Explanation-assisted Model Extraction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-13T15:27:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fatima Ezzeddine, Rinad Akel, Ihab Sbeity, Silvia Giordano, Marc Langheinrich, Omran Ayoub</p>
    <p><b>Summary:</b> Machine Learning as a Service (MLaaS) has gained important attraction as a
means for deploying powerful predictive models, offering ease of use that
enables organizations to leverage advanced analytics without substantial
investments in specialized infrastructure or expertise. However, MLaaS
platforms must be safeguarded against security and privacy attacks, such as
model extraction (MEA) attacks. The increasing integration of explainable AI
(XAI) within MLaaS has introduced an additional privacy challenge, as attackers
can exploit model explanations particularly counterfactual explanations (CFs)
to facilitate MEA. In this paper, we investigate the trade offs among model
performance, privacy, and explainability when employing Differential Privacy
(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two
distinct DP strategies: implemented during the classification model training
and at the explainer during CF generation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.08237v1">Privacy-Preserving Analytics for Smart Meter (AMI) Data: A Hybrid
  Approach to Comply with CPUC Privacy Regulations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-05-13T05:30:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Benjamin Westrich</p>
    <p><b>Summary:</b> Advanced Metering Infrastructure (AMI) data from smart electric and gas
meters enables valuable insights for utilities and consumers, but also raises
significant privacy concerns. In California, regulatory decisions (CPUC
D.11-07-056 and D.11-08-045) mandate strict privacy protections for customer
energy usage data, guided by the Fair Information Practice Principles (FIPPs).
We comprehensively explore solutions drawn from data anonymization,
privacy-preserving machine learning (differential privacy and federated
learning), synthetic data generation, and cryptographic techniques (secure
multiparty computation, homomorphic encryption). This allows advanced
analytics, including machine learning models, statistical and econometric
analysis on energy consumption data, to be performed without compromising
individual privacy.
  We evaluate each technique's theoretical foundations, effectiveness, and
trade-offs in the context of utility data analytics, and we propose an
integrated architecture that combines these methods to meet real-world needs.
The proposed hybrid architecture is designed to ensure compliance with
California's privacy rules and FIPPs while enabling useful analytics, from
forecasting and personalized insights to academic research and econometrics,
while strictly protecting individual privacy. Mathematical definitions and
derivations are provided where appropriate to demonstrate privacy guarantees
and utility implications rigorously. We include comparative evaluations of the
techniques, an architecture diagram, and flowcharts to illustrate how they work
together in practice. The result is a blueprint for utility data scientists and
engineers to implement privacy-by-design in AMI data handling, supporting both
data-driven innovation and strict regulatory compliance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07766v1">Privacy Risks of Robot Vision: A User Study on Image Modalities and
  Resolution</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-12T17:16:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xuying Huang, Sicong Pan, Maren Bennewitz</p>
    <p><b>Summary:</b> User privacy is a crucial concern in robotic applications, especially when
mobile service robots are deployed in personal or sensitive environments.
However, many robotic downstream tasks require the use of cameras, which may
raise privacy risks. To better understand user perceptions of privacy in
relation to visual data, we conducted a user study investigating how different
image modalities and image resolutions affect users' privacy concerns. The
results show that depth images are broadly viewed as privacy-safe, and a
similarly high proportion of respondents feel the same about semantic
segmentation images. Additionally, the majority of participants consider 32*32
resolution RGB images to be almost sufficiently privacy-preserving, while most
believe that 16*16 resolution can fully guarantee privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07672v2">OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-12T15:36:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Arun S. Maiya</p>
    <p><b>Summary:</b> We present OnPrem$.$LLM, a Python-based toolkit for applying large language
models (LLMs) to sensitive, non-public data in offline or restricted
environments. The system is designed for privacy-preserving use cases and
provides prebuilt pipelines for document processing and storage,
retrieval-augmented generation (RAG), information extraction, summarization,
classification, and prompt/output processing with minimal configuration.
OnPrem$.$LLM supports multiple LLM backends -- including llama$.$cpp, Ollama,
vLLM, and Hugging Face Transformers -- with quantized model support, GPU
acceleration, and seamless backend switching. Although designed for fully local
execution, OnPrem$.$LLM also supports integration with a wide range of cloud
LLM providers when permitted, enabling hybrid deployments that balance
performance with data control. A no-code web interface extends accessibility to
non-technical users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07583v1">Privacy-Preserving Real-Time Vietnamese-English Translation on iOS using
  Edge AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-05-12T14:05:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Cong Le</p>
    <p><b>Summary:</b> This research addresses the growing need for privacy-preserving and
accessible language translation by developing a fully offline Neural Machine
Translation (NMT) system for Vietnamese-English translation on iOS devices.
Given increasing concerns about data privacy and unreliable network
connectivity, on-device translation offers critical advantages. This project
confronts challenges in deploying complex NMT models on resource-limited mobile
devices, prioritizing efficiency, accuracy, and a seamless user experience.
Leveraging advances such as MobileBERT and, specifically, the lightweight
\textbf{TinyLlama 1.1B Chat v1.0} in GGUF format, \textbf{a} quantized
Transformer-based model is implemented and optimized. The application is
realized as a real-time iOS prototype, tightly integrating modern iOS
frameworks and privacy-by-design principles. Comprehensive documentation covers
model selection, technical architecture, challenges, and final implementation,
including functional Swift code for deployment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07306v1">Enabling Privacy-Aware AI-Based Ergonomic Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-12T07:52:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sander De Coninck, Emilio Gamba, Bart Van Doninck, Abdellatif Bey-Temsamani, Sam Leroux, Pieter Simoens</p>
    <p><b>Summary:</b> Musculoskeletal disorders (MSDs) are a leading cause of injury and
productivity loss in the manufacturing industry, incurring substantial economic
costs. Ergonomic assessments can mitigate these risks by identifying workplace
adjustments that improve posture and reduce strain. Camera-based systems offer
a non-intrusive, cost-effective method for continuous ergonomic tracking, but
they also raise significant privacy concerns. To address this, we propose a
privacy-aware ergonomic assessment framework utilizing machine learning
techniques. Our approach employs adversarial training to develop a lightweight
neural network that obfuscates video data, preserving only the essential
information needed for human pose estimation. This obfuscation ensures
compatibility with standard pose estimation algorithms, maintaining high
accuracy while protecting privacy. The obfuscated video data is transmitted to
a central server, where state-of-the-art keypoint detection algorithms extract
body landmarks. Using multi-view integration, 3D keypoints are reconstructed
and evaluated with the Rapid Entire Body Assessment (REBA) method. Our system
provides a secure, effective solution for ergonomic monitoring in industrial
environments, addressing both privacy and workplace safety concerns.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07085v1">Privacy of Groups in Dense Street Imagery</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-05-11T18:16:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Matt Franchi, Hauke Sandhaus, Madiha Zahrah Choksi, Severin Engelmann, Wendy Ju, Helen Nissenbaum</p>
    <p><b>Summary:</b> Spatially and temporally dense street imagery (DSI) datasets have grown
unbounded. In 2024, individual companies possessed around 3 trillion unique
images of public streets. DSI data streams are only set to grow as companies
like Lyft and Waymo use DSI to train autonomous vehicle algorithms and analyze
collisions. Academic researchers leverage DSI to explore novel approaches to
urban analysis. Despite good-faith efforts by DSI providers to protect
individual privacy through blurring faces and license plates, these measures
fail to address broader privacy concerns. In this work, we find that increased
data density and advancements in artificial intelligence enable harmful group
membership inferences from supposedly anonymized data. We perform a penetration
test to demonstrate how easily sensitive group affiliations can be inferred
from obfuscated pedestrians in 25,232,608 dashcam images taken in New York
City. We develop a typology of identifiable groups within DSI and analyze
privacy implications through the lens of contextual integrity. Finally, we
discuss actionable recommendations for researchers working with data from DSI
providers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07041v1">Empirical Analysis of Asynchronous Federated Learning on Heterogeneous
  Devices: Efficiency, Fairness, and Privacy Trade-offs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-11T16:25:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Samaneh Mohammadi, Iraklis Symeonidis, Ali Balador, Francesco Flammini</p>
    <p><b>Summary:</b> Device heterogeneity poses major challenges in Federated Learning (FL), where
resource-constrained clients slow down synchronous schemes that wait for all
updates before aggregation. Asynchronous FL addresses this by incorporating
updates as they arrive, substantially improving efficiency. While its
efficiency gains are well recognized, its privacy costs remain largely
unexplored, particularly for high-end devices that contribute updates more
frequently, increasing their cumulative privacy exposure. This paper presents
the first comprehensive analysis of the efficiency-fairness-privacy trade-off
in synchronous vs. asynchronous FL under realistic device heterogeneity. We
empirically compare FedAvg and staleness-aware FedAsync using a physical
testbed of five edge devices spanning diverse hardware tiers, integrating Local
Differential Privacy (LDP) and the Moments Accountant to quantify per-client
privacy loss. Using Speech Emotion Recognition (SER) as a privacy-critical
benchmark, we show that FedAsync achieves up to 10x faster convergence but
exacerbates fairness and privacy disparities: high-end devices contribute 6-10x
more updates and incur up to 5x higher privacy loss, while low-end devices
suffer amplified accuracy degradation due to infrequent, stale, and
noise-perturbed updates. These findings motivate the need for adaptive FL
protocols that jointly optimize aggregation and privacy mechanisms based on
client capacity and participation dynamics, moving beyond static,
one-size-fits-all solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.06860v1">DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial
  Example for Image Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-11T06:11:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xia Du, Jiajie Zhu, Jizhe Zhou, Chi-man Pun, Zheng Lin, Cong Wu, Zhe Chen, Jun Luo</p>
    <p><b>Summary:</b> In the field of digital security, Reversible Adversarial Examples (RAE)
combine adversarial attacks with reversible data hiding techniques to
effectively protect sensitive data and prevent unauthorized analysis by
malicious Deep Neural Networks (DNNs). However, existing RAE techniques
primarily focus on white-box attacks, lacking a comprehensive evaluation of
their effectiveness in black-box scenarios. This limitation impedes their
broader deployment in complex, dynamic environments. Further more, traditional
black-box attacks are often characterized by poor transferability and high
query costs, significantly limiting their practical applicability. To address
these challenges, we propose the Dual-Phase Merging Transferable Reversible
Attack method, which generates highly transferable initial adversarial
perturbations in a white-box model and employs a memory augmented black-box
strategy to effectively mislead target mod els. Experimental results
demonstrate the superiority of our approach, achieving a 99.0% attack success
rate and 100% recovery rate in black-box scenarios, highlighting its robustness
in privacy protection. Moreover, we successfully implemented a black-box attack
on a commercial model, further substantiating the potential of this approach
for practical use.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.06759v1">Privacy-aware Berrut Approximated Coded Computing applied to general
  distributed learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-05-10T21:27:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xavier Martínez-Luaña, Manuel Fernández-Veiga, Rebeca P. Díaz-Redondo, Ana Fernández-Vilas</p>
    <p><b>Summary:</b> Coded computing is one of the techniques that can be used for privacy
protection in Federated Learning. However, most of the constructions used for
coded computing work only under the assumption that the computations involved
are exact, generally restricted to special classes of functions, and require
quantized inputs. This paper considers the use of Private Berrut Approximate
Coded Computing (PBACC) as a general solution to add strong but non-perfect
privacy to federated learning. We derive new adapted PBACC algorithms for
centralized aggregation, secure distributed training with centralized data, and
secure decentralized training with decentralized data, thus enlarging
significantly the applications of the method and the existing privacy
protection tools available for these paradigms. Particularly, PBACC can be used
robustly to attain privacy guarantees in decentralized federated learning for a
variety of models. Our numerical results show that the achievable quality of
different learning models (convolutional neural networks, variational
autoencoders, and Cox regression) is minimally altered by using these new
computing schemes, and that the privacy leakage can be bounded strictly to less
than a fraction of one bit per participant. Additionally, the computational
cost of the encoding and decoding processes depends only of the degree of
decentralization of the data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.06747v1">DPolicy: Managing Privacy Risks Across Multiple Releases with
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-10T19:49:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicolas Küchler, Alexander Viand, Hidde Lycklama, Anwar Hithnawi</p>
    <p><b>Summary:</b> Differential Privacy (DP) has emerged as a robust framework for
privacy-preserving data releases and has been successfully applied in
high-profile cases, such as the 2020 US Census. However, in organizational
settings, the use of DP remains largely confined to isolated data releases.
This approach restricts the potential of DP to serve as a framework for
comprehensive privacy risk management at an organizational level. Although one
might expect that the cumulative privacy risk of isolated releases could be
assessed using DP's compositional property, in practice, individual DP
guarantees are frequently tailored to specific releases, making it difficult to
reason about their interaction or combined impact. At the same time, less
tailored DP guarantees, which compose more easily, also offer only limited
insight because they lead to excessively large privacy budgets that convey
limited meaning. To address these limitations, we present DPolicy, a system
designed to manage cumulative privacy risks across multiple data releases using
DP. Unlike traditional approaches that treat each release in isolation or rely
on a single (global) DP guarantee, our system employs a flexible framework that
considers multiple DP guarantees simultaneously, reflecting the diverse
contexts and scopes typical of real-world DP deployments. DPolicy introduces a
high-level policy language to formalize privacy guarantees, making
traditionally implicit assumptions on scopes and contexts explicit. By deriving
the DP guarantees required to enforce complex privacy semantics from these
high-level policies, DPolicy enables fine-grained privacy risk management on an
organizational scale. We implement and evaluate DPolicy, demonstrating how it
mitigates privacy risks that can emerge without comprehensive,
organization-wide privacy risk management.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07872v2">Revenue Optimization in Video Caching Networks with Privacy-Preserving
  Demand Predictions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> 
  <p><b>Published on:</b> 2025-05-09T21:05:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yijing Zhang, Ferdous Pervej, Andreas F. Molisch</p>
    <p><b>Summary:</b> Performance of video streaming, which accounts for most of the traffic in
wireless communication, can be significantly improved by caching popular videos
at the wireless edge. Determining the cache content that optimizes performance
(defined via a revenue function) is thus an important task, and prediction of
the future demands based on past history can make this process much more
efficient. However, since practical video caching networks involve various
parties (e.g., users, isp, and csp) that do not wish to reveal information such
as past history to each other, privacy-preserving solutions are required.
Motivated by this, we propose a proactive caching method based on users'
privacy-preserving multi-slot future demand predictions -- obtained from a
trained Transformer -- to optimize revenue. Specifically, we first use a
privacy-preserving fl algorithm to train a Transformer to predict multi-slot
future demands of the users. However, prediction accuracy is not perfect and
decreases the farther into the future the prediction is done. We model the
impact of prediction errors invoking the file popularities, based on which we
formulate a long-term system revenue optimization to make the cache placement
decisions. As the formulated problem is NP-hard, we use a greedy algorithm to
efficiently obtain an approximate solution. Simulation results validate that
(i) the fl solution achieves results close to the centralized
(non-privacy-preserving) solution and (ii) optimization of revenue may provide
different solutions than the classical chr criterion.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.06122v1">Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled
  Systems via Particle Filter Reinforcement Learning</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-05-09T15:25:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haokun Yu, Jingyuan Zhou, Kaidi Yang</p>
    <p><b>Summary:</b> This paper addresses the problem of parameter privacy-preserving data sharing
in coupled systems, where a data provider shares data with a data user but
wants to protect its sensitive parameters. The shared data affects not only the
data user's decision-making but also the data provider's operations through
system interactions. To trade off control performance and privacy, we propose
an interaction-aware privacy-preserving data sharing approach. Our approach
generates distorted data by minimizing a combination of (i) mutual information,
quantifying privacy leakage of sensitive parameters, and (ii) the impact of
distorted data on the data provider's control performance, considering the
interactions between stakeholders. The optimization problem is formulated into
a Bellman equation and solved by a particle filter reinforcement learning
(RL)-based approach. Compared to existing RL-based methods, our formulation
significantly reduces history dependency and efficiently handles scenarios with
continuous state space. Validated in a mixed-autonomy platoon scenario, our
method effectively protects sensitive driving behavior parameters of
human-driven vehicles (HDVs) against inference attacks while maintaining
negligible impact on fuel efficiency.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05922v2">Cape: Context-Aware Prompt Perturbation Mechanism with Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-09T09:54:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoqi Wu, Wei Dai, Li Wang, Qiang Yan</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have gained significant popularity due to their
remarkable capabilities in text understanding and generation. However, despite
their widespread deployment in inference services such as ChatGPT, concerns
about the potential leakage of sensitive user data have arisen. Existing
solutions primarily rely on privacy-enhancing technologies to mitigate such
risks, facing the trade-off among efficiency, privacy, and utility. To narrow
this gap, we propose Cape, a context-aware prompt perturbation mechanism based
on differential privacy, to enable efficient inference with an improved
privacy-utility trade-off. Concretely, we introduce a hybrid utility function
that better captures the token similarity. Additionally, we propose a
bucketized sampling mechanism to handle large sampling space, which might lead
to long-tail phenomenons. Extensive experiments across multiple datasets, along
with ablation studies, demonstrate that Cape achieves a better privacy-utility
trade-off compared to prior state-of-the-art works.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05920v1">Privacy-Preserving Credit Card Approval Using Homomorphic SVM: Toward
  Secure Inference in FinTech Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-09T09:46:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b>  Faneela, Baraq Ghaleb, Jawad Ahmad, William J. Buchanan, Sana Ullah Jan</p>
    <p><b>Summary:</b> The growing use of machine learning in cloud environments raises critical
concerns about data security and privacy, especially in finance. Fully
Homomorphic Encryption (FHE) offers a solution by enabling computations on
encrypted data, but its high computational cost limits practicality. In this
paper, we propose PP-FinTech, a privacy-preserving scheme for financial
applications that employs a CKKS-based encrypted soft-margin SVM, enhanced with
a hybrid kernel for modeling non-linear patterns and an adaptive thresholding
mechanism for robust encrypted classification. Experiments on the Credit Card
Approval dataset demonstrate comparable performance to the plaintext models,
highlighting PP-FinTech's ability to balance privacy, and efficiency in secure
financial ML systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05859v1">Integrating Building Thermal Flexibility Into Distribution System: A
  Privacy-Preserved Dispatch Approach</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-05-09T07:53:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuai Lu, Zeyin Hou, Wei Gu, Yijun Xu</p>
    <p><b>Summary:</b> The inherent thermal storage capacity of buildings brings considerable
thermal flexibility to the heating/cooling loads, which are promising demand
response resources for power systems. It is widely believed that integrating
the thermal flexibility of buildings into the distribution system can improve
the operating economy and reliability of the system. However, the private
information of the buildings needs to be transferred to the distribution system
operator (DSO) to achieve a coordinated optimization, bringing serious privacy
concerns to users. Given this issue, we propose a novel privacy-preserved
optimal dispatch approach for the distribution system incorporating buildings.
Using it, the DSO can exploit the thermal flexibility of buildings without
accessing their private information, such as model parameters and indoor
temperature profiles. Specifically, we first develop an optimal dispatch model
for the distribution system integrating buildings, which can be extended to
other storage-like flexibility resources. Second, we reveal that the
privacy-preserved integration of buildings is a joint privacy preservation
problem for both parameters and state variables and then design a
privacy-preserved algorithm based on transformation-based encryption,
constraint relaxation, and constraint extension techniques. Besides, we
implement a detailed privacy analysis for the proposed method, considering both
semi-honest adversaries and external eavesdroppers. Case studies demonstrate
the accuracy, privacy-preserved performance, and computational efficiency of
the proposed method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05843v1">Enhancing Noisy Functional Encryption for Privacy-Preserving Machine
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-09T07:33:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Linda Scheu-Hachtel, Jasmin Zalonis</p>
    <p><b>Summary:</b> Functional encryption (FE) has recently attracted interest in
privacy-preserving machine learning (PPML) for its unique ability to compute
specific functions on encrypted data. A related line of work focuses on noisy
FE, which ensures differential privacy in the output while keeping the data
encrypted. We extend the notion of noisy multi-input functional encryption
(NMIFE) to (dynamic) noisy multi-client functional encryption ((Dy)NMCFE),
which allows for more flexibility in the number of data holders and analyses,
while protecting the privacy of the data holder with fine-grained access
through the usage of labels. Following our new definition of DyNMCFE, we
present DyNo, a concrete inner-product DyNMCFE scheme. Our scheme captures all
the functionalities previously introduced in noisy FE schemes, while being
significantly more efficient in terms of space and runtime and fulfilling a
stronger security notion by allowing the corruption of clients. To further
prove the applicability of DyNMCFE, we present a protocol for PPML based on
DyNo. According to this protocol, we train a privacy-preserving logistic
regression.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05816v1">On the Price of Differential Privacy for Spectral Clustering over
  Stochastic Block Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-05-09T06:34:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antti Koskela, Mohamed Seif, Andrea J. Goldsmith</p>
    <p><b>Summary:</b> We investigate privacy-preserving spectral clustering for community detection
within stochastic block models (SBMs). Specifically, we focus on edge
differential privacy (DP) and propose private algorithms for community
recovery. Our work explores the fundamental trade-offs between the privacy
budget and the accurate recovery of community labels. Furthermore, we establish
information-theoretic conditions that guarantee the accuracy of our methods,
providing theoretical assurances for successful community recovery under edge
DP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05707v1">Crowding Out The Noise: Algorithmic Collective Action Under Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-09T00:55:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rushabh Solanki, Meghana Bhange, Ulrich Aïvodji, Elliot Creager</p>
    <p><b>Summary:</b> The integration of AI into daily life has generated considerable attention
and excitement, while also raising concerns about automating algorithmic harms
and re-entrenching existing social inequities. While the responsible deployment
of trustworthy AI systems is a worthy goal, there are many possible ways to
realize it, from policy and regulation to improved algorithm design and
evaluation. In fact, since AI trains on social data, there is even a
possibility for everyday users, citizens, or workers to directly steer its
behavior through Algorithmic Collective Action, by deliberately modifying the
data they share with a platform to drive its learning process in their favor.
This paper considers how these grassroots efforts to influence AI interact with
methods already used by AI firms and governments to improve model
trustworthiness. In particular, we focus on the setting where the AI firm
deploys a differentially private model, motivated by the growing regulatory
focus on privacy and data protection. We investigate how the use of
Differentially Private Stochastic Gradient Descent (DPSGD) affects the
collective's ability to influence the learning process. Our findings show that
while differential privacy contributes to the protection of individual data, it
introduces challenges for effective algorithmic collective action. We
characterize lower bounds on the success of algorithmic collective action under
differential privacy as a function of the collective's size and the firm's
privacy parameters, and verify these trends experimentally by simulating
collective action during the training of deep neural network classifiers across
several datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05648v1">Privacy-Preserving Transformers: SwiftKey's Differential Privacy
  Implementation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-08T21:08:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abdelrahman Abouelenin, Mohamed Abdelrehim, Raffy Fahim, Amr Hendy, Mohamed Afify</p>
    <p><b>Summary:</b> In this paper we train a transformer using differential privacy (DP) for
language modeling in SwiftKey. We run multiple experiments to balance the
trade-off between the model size, run-time speed and accuracy. We show that we
get small and consistent gains in the next-word-prediction and accuracy with
graceful increase in memory and speed compared to the production GRU. This is
obtained by scaling down a GPT2 architecture to fit the required size and a two
stage training process that builds a seed model on general data and DP
finetunes it on typing data. The transformer is integrated using ONNX offering
both flexibility and efficiency.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05613v1">Optimal Regret of Bernoulli Bandits under Global Differential Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2025-05-08T19:48:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Achraf Azize, Yulian Wu, Junya Honda, Francesco Orabona, Shinji Ito, Debabrota Basu</p>
    <p><b>Summary:</b> As sequential learning algorithms are increasingly applied to real life,
ensuring data privacy while maintaining their utilities emerges as a timely
question. In this context, regret minimisation in stochastic bandits under
$\epsilon$-global Differential Privacy (DP) has been widely studied. Unlike
bandits without DP, there is a significant gap between the best-known regret
lower and upper bound in this setting, though they "match" in order. Thus, we
revisit the regret lower and upper bounds of $\epsilon$-global DP algorithms
for Bernoulli bandits and improve both. First, we prove a tighter regret lower
bound involving a novel information-theoretic quantity characterising the
hardness of $\epsilon$-global DP in stochastic bandits. Our lower bound
strictly improves on the existing ones across all $\epsilon$ values. Then, we
choose two asymptotically optimal bandit algorithms, i.e. DP-KLUCB and DP-IMED,
and propose their DP versions using a unified blueprint, i.e., (a) running in
arm-dependent phases, and (b) adding Laplace noise to achieve privacy. For
Bernoulli bandits, we analyse the regrets of these algorithms and show that
their regrets asymptotically match our lower bound up to a constant arbitrary
close to 1. This refutes the conjecture that forgetting past rewards is
necessary to design optimal bandit algorithms under global DP. At the core of
our algorithms lies a new concentration inequality for sums of Bernoulli
variables under Laplace mechanism, which is a new DP version of the Chernoff
bound. This result is universally useful as the DP literature commonly treats
the concentrations of Laplace noise and random variables separately, while we
couple them to yield a tighter bound.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05214v1">Overcoming the hurdle of legal expertise: A reusable model for
  smartwatch privacy policies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-05-08T13:09:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Constantin Buschhaus, Arvid Butting, Judith Michael, Verena Nitsch, Sebastian Pütz, Bernhard Rumpe, Carolin Stellmacher, Sabine Theis</p>
    <p><b>Summary:</b> Regulations for privacy protection aim to protect individuals from the
unauthorized storage, processing, and transfer of their personal data but
oftentimes fail in providing helpful support for understanding these
regulations. To better communicate privacy policies for smartwatches, we need
an in-depth understanding of their concepts and provide better ways to enable
developers to integrate them when engineering systems. Up to now, no conceptual
model exists covering privacy statements from different smartwatch
manufacturers that is reusable for developers. This paper introduces such a
conceptual model for privacy policies of smartwatches and shows its use in a
model-driven software engineering approach to create a platform for data
visualization of wearable privacy policies from different smartwatch
manufacturers. We have analyzed the privacy policies of various manufacturers
and extracted the relevant concepts. Moreover, we have checked the model with
lawyers for its correctness, instantiated it with concrete data, and used it in
a model-driven software engineering approach to create a platform for data
visualization. This reusable privacy policy model can enable developers to
easily represent privacy policies in their systems. This provides a foundation
for more structured and understandable privacy policies which, in the long run,
can increase the data sovereignty of application users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05155v1">FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data
  Preparation via Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-08T11:51:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhihao Zeng, Ziquan Fang, Wei Shao, Lu Chen, Yunjun Gao</p>
    <p><b>Summary:</b> Trajectory data, which capture the movement patterns of people and vehicles
over time and space, are crucial for applications like traffic optimization and
urban planning. However, issues such as noise and incompleteness often
compromise data quality, leading to inaccurate trajectory analyses and limiting
the potential of these applications. While Trajectory Data Preparation (TDP)
can enhance data quality, existing methods suffer from two key limitations: (i)
they do not address data privacy concerns, particularly in federated settings
where trajectory data sharing is prohibited, and (ii) they typically design
task-specific models that lack generalizability across diverse TDP scenarios.
To overcome these challenges, we propose FedTDP, a privacy-preserving and
unified framework that leverages the capabilities of Large Language Models
(LLMs) for TDP in federated environments. Specifically, we: (i) design a
trajectory privacy autoencoder to secure data transmission and protect privacy,
(ii) introduce a trajectory knowledge enhancer to improve model learning of
TDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)
propose federated parallel optimization to enhance training efficiency by
reducing data transmission and enabling parallel model training. Experiments on
6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP
consistently outperforms 13 state-of-the-art baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05130v2">CacheFL: Privacy-Preserving and Efficient Federated Cache Model
  Fine-Tuning for Vision-Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-05-08T11:07:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mengjun Yi, Hanwen Zhang, Hui Dou, Jian Zhao, Furao Shen</p>
    <p><b>Summary:</b> Large pre-trained Vision-Language Models (VLMs), such as Contrastive
Language-Image Pre-training (CLIP), have exhibited remarkable zero-shot
performance across various image classification tasks. Fine-tuning these models
on domain-specific datasets further enhances their effectiveness for downstream
applications. However, fine-tuning in cloud environments raises significant
concerns regarding data security and privacy. Federated Learning (FL) offers a
decentralized solution by enabling model training across local clients without
centralizing sensitive data, but the high communication and computation costs
of transmitting full pre-trained models during training limit its scalability.
Additionally, non-Independent and Identically Distributed (non-IID) data across
local clients can negatively impact model convergence and performance. To
address these challenges, we propose CacheFL, a novel federated learning method
that replaces traditional full model fine-tuning with lightweight cache model
fine-tuning. The cache model is initialized using a class-balanced dataset
generated by a generative pre-trained model, effectively mitigating the impact
of non-IID data. This cache model is then distributed to local clients for
fine-tuning, and the updated parameters from each client are aggregated on the
server and redistributed. With the updated cache model, the classification
performance of CLIP is improved after just a few epochs. By limiting the
training and communication to the cache model, CacheFL significantly reduces
resource demands while ensuring data privacy and security. Extensive
experiments conducted on ImageNet and 10 additional datasets demonstrate that
CacheFL outperforms traditional approaches in terms of classification accuracy,
resource efficiency, and privacy preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05031v1">LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving
  Cloud-Device Collaboration</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-05-08T08:06:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yingyi Zhang, Pengyue Jia, Xianneng Li, Derong Xu, Maolin Wang, Yichao Wang, Zhaocheng Du, Huifeng Guo, Yong Liu, Ruiming Tang, Xiangyu Zhao</p>
    <p><b>Summary:</b> Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)
for handling public user queries and on-device Small Language Models (SLMs) for
processing private user data, collectively forming a powerful and
privacy-preserving solution. However, existing approaches often fail to fully
leverage the scalable problem-solving capabilities of on-cloud LLMs while
underutilizing the advantage of on-device SLMs in accessing and processing
personalized data. This leads to two interconnected issues: 1) Limited
utilization of the problem-solving capabilities of on-cloud LLMs, which fail to
align with personalized user-task needs, and 2) Inadequate integration of user
data into on-device SLM responses, resulting in mismatches in contextual user
information.
  In this paper, we propose a Leader-Subordinate Retrieval framework for
Privacy-preserving cloud-device collaboration (LSRP), a novel solution that
bridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM
through a dynamic selection of task-specific leader strategies named as
user-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the
data advantages of on-device SLMs through small model feedback Direct
Preference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the
on-device SLM. Experiments on two datasets demonstrate that LSRP consistently
outperforms state-of-the-art baselines, significantly improving question-answer
relevance and personalization, while preserving user privacy through efficient
on-device retrieval. Our code is available at:
https://github.com/Zhang-Yingyi/LSRP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.06305v1">User Behavior Analysis in Privacy Protection with Large Language Models:
  A Study on Privacy Preferences with Limited Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-08T04:42:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haowei Yang, Qingyi Lu, Yang Wang, Sibei Liu, Jiayun Zheng, Ao Xiang</p>
    <p><b>Summary:</b> With the widespread application of large language models (LLMs), user privacy
protection has become a significant research topic. Existing privacy preference
modeling methods often rely on large-scale user data, making effective privacy
preference analysis challenging in data-limited environments. This study
explores how LLMs can analyze user behavior related to privacy protection in
scenarios with limited data and proposes a method that integrates Few-shot
Learning and Privacy Computing to model user privacy preferences. The research
utilizes anonymized user privacy settings data, survey responses, and simulated
data, comparing the performance of traditional modeling approaches with
LLM-based methods. Experimental results demonstrate that, even with limited
data, LLMs significantly improve the accuracy of privacy preference modeling.
Additionally, incorporating Differential Privacy and Federated Learning further
reduces the risk of user data exposure. The findings provide new insights into
the application of LLMs in privacy protection and offer theoretical support for
advancing privacy computing and user behavior analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05519v1">Real-Time Privacy Preservation for Robot Visual Perception</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-08T03:27:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Minkyu Choi, Yunhao Yang, Neel P. Bhatt, Kushagra Gupta, Sahil Shah, Aditya Rai, David Fridovich-Keil, Ufuk Topcu, Sandeep P. Chinchali</p>
    <p><b>Summary:</b> Many robots (e.g., iRobot's Roomba) operate based on visual observations from
live video streams, and such observations may inadvertently include
privacy-sensitive objects, such as personal identifiers. Existing approaches
for preserving privacy rely on deep learning models, differential privacy, or
cryptography. They lack guarantees for the complete concealment of all
sensitive objects. Guaranteeing concealment requires post-processing techniques
and thus is inadequate for real-time video streams. We develop a method for
privacy-constrained video streaming, PCVS, that conceals sensitive objects
within real-time video streams. PCVS takes a logical specification constraining
the existence of privacy-sensitive objects, e.g., never show faces when a
person exists. It uses a detection model to evaluate the existence of these
objects in each incoming frame. Then, it blurs out a subset of objects such
that the existence of the remaining objects satisfies the specification. We
then propose a conformal prediction approach to (i) establish a theoretical
lower bound on the probability of the existence of these objects in a sequence
of frames satisfying the specification and (ii) update the bound with the
arrival of each subsequent frame. Quantitative evaluations show that PCVS
achieves over 95 percent specification satisfaction rate in multiple datasets,
significantly outperforming other methods. The satisfaction rate is
consistently above the theoretical bounds across all datasets, indicating that
the established bounds hold. Additionally, we deploy PCVS on robots in
real-time operation and show that the robots operate normally without being
compromised when PCVS conceals objects.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04889v1">FedRE: Robust and Effective Federated Learning with Privacy Preference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-08T01:50:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianzhe Xiao, Yichen Li, Yu Zhou, Yining Qi, Yi Liu, Wei Wang, Haozhao Wang, Yi Wang, Ruixuan Li</p>
    <p><b>Summary:</b> Despite Federated Learning (FL) employing gradient aggregation at the server
for distributed training to prevent the privacy leakage of raw data, private
information can still be divulged through the analysis of uploaded gradients
from clients. Substantial efforts have been made to integrate local
differential privacy (LDP) into the system to achieve a strict privacy
guarantee. However, existing methods fail to take practical issues into account
by merely perturbing each sample with the same mechanism while each client may
have their own privacy preferences on privacy-sensitive information (PSI),
which is not uniformly distributed across the raw data. In such a case,
excessive privacy protection from private-insensitive information can
additionally introduce unnecessary noise, which may degrade the model
performance. In this work, we study the PSI within data and develop FedRE, that
can simultaneously achieve robustness and effectiveness benefits with LDP
protection. More specifically, we first define PSI with regard to the privacy
preferences of each client. Then, we optimize the LDP by allocating less
privacy budget to gradients with higher PSI in a layer-wise manner, thus
providing a stricter privacy guarantee for PSI. Furthermore, to mitigate the
performance degradation caused by LDP, we design a parameter aggregation
mechanism based on the distribution of the perturbed information. We conducted
experiments with text tamper detection on T-SROIE and DocTamper datasets, and
FedRE achieves competitive performance compared to state-of-the-art methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04799v1">Safeguard-by-Development: A Privacy-Enhanced Development Paradigm for
  Multi-Agent Collaboration Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-07T20:54:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jian Cui, Zichuan Li, Luyi Xing, Xiaojing Liao</p>
    <p><b>Summary:</b> Multi-agent collaboration systems (MACS), powered by large language models
(LLMs), solve complex problems efficiently by leveraging each agent's
specialization and communication between agents. However, the inherent exchange
of information between agents and their interaction with external environments,
such as LLM, tools, and users, inevitably introduces significant risks of
sensitive data leakage, including vulnerabilities to attacks like prompt
injection and reconnaissance. Existing MACS fail to enable privacy controls,
making it challenging to manage sensitive information securely. In this paper,
we take the first step to address the MACS's data leakage threat at the system
development level through a privacy-enhanced development paradigm, Maris. Maris
enables rigorous message flow control within MACS by embedding reference
monitors into key multi-agent conversation components. We implemented Maris as
an integral part of AutoGen, a widely adopted open-source multi-agent
development framework. Then, we evaluate Maris for its effectiveness and
performance overhead on privacy-critical MACS use cases, including healthcare,
supply chain optimization, and personalized recommendation system. The result
shows that Maris achieves satisfactory effectiveness, performance overhead and
practicability for adoption.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04570v1">Privacy-preserving neutral atom-based quantum classifier towards real
  healthcare applications</a></h3>
    
  <p><b>Published on:</b> 2025-05-07T17:03:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ettore Canonici, Filippo Caruso</p>
    <p><b>Summary:</b> Technological advances in Artificial Intelligence (AI) and Machine Learning
(ML) for the healthcare domain are rapidly arising, with a growing discussion
regarding the ethical management of their development. In general, ML
healthcare applications crucially require performance, interpretability of
data, and respect for data privacy. The latter is an increasingly debated topic
as commercial cloud computing services become more and more widespread.
Recently, dedicated methods are starting to be developed aiming to protect data
privacy. However, these generally result in a trade-off forcing one to balance
the level of data privacy and the algorithm performance. Here, a Support Vector
Machine (SVM) classifier model is proposed whose training is reformulated into
a Quadratic Unconstrained Binary Optimization (QUBO) problem, and adapted to a
neutral atom-based Quantum Processing Unit (QPU). Our final model does not
require anonymization techniques to protect data privacy since the sensitive
data are not needed to be transferred to the cloud-available QPU. Indeed, the
latter is used only during the training phase, hence allowing a future concrete
application in a real-world scenario. Finally, performance and scaling analyses
on a publicly available breast cancer dataset are discussed, both using ideal
and noisy simulations for the training process, and also successfully tested on
a currently available real neutral-atom QPU.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04361v1">RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery
  Scheme in Mobile Crowdsensing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computational Engineering, Finance, and Science-5BC0EB">
  <p><b>Published on:</b> 2025-05-07T12:20:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lijian Wu, Weikun Xie, Wei Tan, Tian Wang, Houbing Herbert Song, Anfeng Liu</p>
    <p><b>Summary:</b> Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS).
However, existing TD methods, including privacy-preserving TD approaches,
estimate the truth by weighting only the data submitted in the current round,
which often results in low data quality. Moreover, there is a lack of effective
TD methods that preserve both reputation and data privacy. To address these
issues, a Reputation and Data Privacy-Preserving based Truth Discovery
(RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD
scheme consists of two key approaches: a Reputation-based Truth Discovery (RTD)
approach, which integrates the weight of current-round data with workers'
reputation values to estimate the truth, thereby achieving more accurate
results, and a Reputation and Data Privacy-Preserving (RDPP) approach, which
ensures privacy preservation for sensing data and reputation values. First, the
RDPP approach, when seamlessly integrated with RTD, can effectively evaluate
the reliability of workers and their sensing data in a privacy-preserving
manner. Second, the RDPP scheme supports reputation-based worker recruitment
and rewards, ensuring high-quality data collection while incentivizing workers
to provide accurate information. Comprehensive theoretical analysis and
extensive experiments based on real-world datasets demonstrate that the
proposed RDPP-TD scheme provides strong privacy protection and improves data
quality by up to 33.3%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04181v1">Privacy Challenges In Image Processing Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-07T07:28:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b>  Maneesha, Bharat Gupta, Rishabh Sethi, Charvi Adita Das</p>
    <p><b>Summary:</b> As image processing systems proliferate, privacy concerns intensify given the
sensitive personal information contained in images. This paper examines privacy
challenges in image processing and surveys emerging privacy-preserving
techniques including differential privacy, secure multiparty computation,
homomorphic encryption, and anonymization. Key applications with heightened
privacy risks include healthcare, where medical images contain patient health
data, and surveillance systems that can enable unwarranted tracking.
Differential privacy offers rigorous privacy guarantees by injecting controlled
noise, while MPC facilitates collaborative analytics without exposing raw data
inputs. Homomorphic encryption enables computations on encrypted data and
anonymization directly removes identifying elements. However, balancing privacy
protections and utility remains an open challenge. Promising future directions
identified include quantum-resilient cryptography, federated learning,
dedicated hardware, and conceptual innovations like privacy by design.
Ultimately, a holistic effort combining technological innovations, ethical
considerations, and policy frameworks is necessary to uphold the fundamental
right to privacy as image processing capabilities continue advancing rapidly.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04034v1">Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency,
  and Transferability in Spiking Neural Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Neural and Evolutionary Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-07T00:27:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ayana Moshruba, Hamed Poursiami, Maryam Parsa</p>
    <p><b>Summary:</b> Biological neurons exhibit diverse temporal spike patterns, which are
believed to support efficient, robust, and adaptive neural information
processing. While models such as Izhikevich can replicate a wide range of these
firing dynamics, their complexity poses challenges for directly integrating
them into scalable spiking neural networks (SNN) training pipelines. In this
work, we propose two probabilistically driven, input-level temporal spike
transformations: Poisson-Burst and Delayed-Burst that introduce biologically
inspired temporal variability directly into standard Leaky Integrate-and-Fire
(LIF) neurons. This enables scalable training and systematic evaluation of how
spike timing dynamics affect privacy, generalization, and learning performance.
Poisson-Burst modulates burst occurrence based on input intensity, while
Delayed-Burst encodes input strength through burst onset timing. Through
extensive experiments across multiple benchmarks, we demonstrate that
Poisson-Burst maintains competitive accuracy and lower resource overhead while
exhibiting enhanced privacy robustness against membership inference attacks,
whereas Delayed-Burst provides stronger privacy protection at a modest accuracy
trade-off. These findings highlight the potential of biologically grounded
temporal spike dynamics in improving the privacy, generalization and biological
plausibility of neuromorphic learning systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.03639v1">Differential Privacy for Network Assortativity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-06T15:40:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fei Ma, Jinzhi Ouyang, Xincheng Hu</p>
    <p><b>Summary:</b> The analysis of network assortativity is of great importance for
understanding the structural characteristics of and dynamics upon networks.
Often, network assortativity is quantified using the assortativity coefficient
that is defined based on the Pearson correlation coefficient between vertex
degrees. It is well known that a network may contain sensitive information,
such as the number of friends of an individual in a social network (which is
abstracted as the degree of vertex.). So, the computation of the assortativity
coefficient leads to privacy leakage, which increases the urgent need for
privacy-preserving protocol. However, there has been no scheme addressing the
concern above.
  To bridge this gap, in this work, we are the first to propose approaches
based on differential privacy (DP for short). Specifically, we design three
DP-based algorithms: $Local_{ru}$, $Shuffle_{ru}$, and $Decentral_{ru}$. The
first two algorithms, based on Local DP (LDP) and Shuffle DP respectively, are
designed for settings where each individual only knows his/her direct friends.
In contrast, the third algorithm, based on Decentralized DP (DDP), targets
scenarios where each individual has a broader view, i.e., also knowing his/her
friends' friends. Theoretically, we prove that each algorithm enables an
unbiased estimation of the assortativity coefficient of the network. We further
evaluate the performance of the proposed algorithms using mean squared error
(MSE), showing that $Shuffle_{ru}$ achieves the best performance, followed by
$Decentral_{ru}$, with $Local_{ru}$ performing the worst. Note that these three
algorithms have different assumptions, so each has its applicability scenario.
Lastly, we conduct extensive numerical simulations, which demonstrate that the
presented approaches are adequate to achieve the estimation of network
assortativity under the demand for privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.03519v3">Revisiting Model Inversion Evaluation: From Misleading Standards to
  Reliable Privacy Assessment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-06T13:32:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sy-Tuyen Ho, Koh Jun Hao, Ngoc-Bao Nguyen, Alexander Binder, Ngai-Man Cheung</p>
    <p><b>Summary:</b> Model Inversion (MI) attacks aim to reconstruct information from private
training data by exploiting access to machine learning models T. To evaluate
such attacks, the standard evaluation framework for such attacks relies on an
evaluation model E, trained under the same task design as T. This framework has
become the de facto standard for assessing progress in MI research, used across
nearly all recent MI attacks and defenses without question. In this paper, we
present the first in-depth study of this MI evaluation framework. In
particular, we identify a critical issue of this standard MI evaluation
framework: Type-I adversarial examples. These are reconstructions that do not
capture the visual features of private training data, yet are still deemed
successful by the target model T and ultimately transferable to E. Such false
positives undermine the reliability of the standard MI evaluation framework. To
address this issue, we introduce a new MI evaluation framework that replaces
the evaluation model E with advanced Multimodal Large Language Models (MLLMs).
By leveraging their general-purpose visual understanding, our MLLM-based
framework does not depend on training of shared task design as in T, thus
reducing Type-I transferability and providing more faithful assessments of
reconstruction success. Using our MLLM-based evaluation framework, we
reevaluate 26 diverse MI attack setups and empirically reveal consistently high
false positive rates under the standard evaluation framework. Importantly, we
demonstrate that many state-of-the-art (SOTA) MI methods report inflated attack
accuracy, indicating that actual privacy leakage is significantly lower than
previously believed. By uncovering this critical issue and proposing a robust
solution, our work enables a reassessment of progress in MI research and sets a
new standard for reliable and robust evaluation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02975v1">Navigating Privacy and Trust: AI Assistants as Social Support for Older
  Adults</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-05-05T19:00:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karina LaRubbio, Malcolm Grba, Diana Freed</p>
    <p><b>Summary:</b> AI assistants are increasingly integrated into older adults' daily lives,
offering new opportunities for social support and accessibility while raising
important questions about privacy, autonomy, and trust. As these systems become
embedded in caregiving and social networks, older adults must navigate
trade-offs between usability, data privacy, and personal agency across
different interaction contexts. Although prior work has explored AI assistants'
potential benefits, further research is needed to understand how perceived
usefulness and risk shape adoption and engagement. This paper examines these
dynamics and advocates for participatory design approaches that position older
adults as active decision makers in shaping AI assistant functionality. By
advancing a framework for privacy-aware, user-centered AI design, this work
contributes to ongoing discussions on developing ethical and transparent AI
systems that enhance well-being without compromising user control.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02828v1">Privacy Risks and Preservation Methods in Explainable Artificial
  Intelligence: A Scoping Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-05-05T17:53:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sonal Allana, Mohan Kankanhalli, Rozita Dara</p>
    <p><b>Summary:</b> Explainable Artificial Intelligence (XAI) has emerged as a pillar of
Trustworthy AI and aims to bring transparency in complex models that are opaque
by nature. Despite the benefits of incorporating explanations in models, an
urgent need is found in addressing the privacy concerns of providing this
additional information to end users. In this article, we conduct a scoping
review of existing literature to elicit details on the conflict between privacy
and explainability. Using the standard methodology for scoping review, we
extracted 57 articles from 1,943 studies published from January 2019 to
December 2024. The review addresses 3 research questions to present readers
with more understanding of the topic: (1) what are the privacy risks of
releasing explanations in AI systems? (2) what current methods have researchers
employed to achieve privacy preservation in XAI systems? (3) what constitutes a
privacy preserving explanation? Based on the knowledge synthesized from the
selected studies, we categorize the privacy risks and preservation methods in
XAI and propose the characteristics of privacy preserving explanations to aid
researchers and practitioners in understanding the requirements of XAI that is
privacy compliant. Lastly, we identify the challenges in balancing privacy with
other system desiderata and provide recommendations for achieving privacy
preserving XAI. We expect that this review will shed light on the complex
relationship of privacy and explainability, both being the fundamental
principles of Trustworthy AI.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02798v1">Unifying Laplace Mechanism with Instance Optimality in Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-05-05T17:20:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> David Durfee</p>
    <p><b>Summary:</b> We adapt the canonical Laplace mechanism, widely used in differentially
private data analysis, to achieve near instance optimality with respect to the
hardness of the underlying dataset. In particular, we construct a piecewise
Laplace distribution whereby we defy traditional assumptions and show that
Laplace noise can in fact be drawn proportional to the local sensitivity when
done in a piecewise manner. While it may initially seem counterintuitive that
this satisfies (pure) differential privacy and can be sampled, we provide both
through a simple connection to the exponential mechanism and inverse
sensitivity along with the fact that the Laplace distribution is a two-sided
exponential distribution. As a result, we prove that in the continuous setting
our \textit{piecewise Laplace mechanism} strictly dominates the inverse
sensitivity mechanism, which was previously shown to both be nearly instance
optimal and uniformly outperform the smooth sensitivity framework. Furthermore,
in the worst-case where all local sensitivities equal the global sensitivity,
our method simply reduces to a Laplace mechanism. We also complement this with
an approximate local sensitivity variant to potentially ease the computational
cost, which can also extend to higher dimensions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02513v1">Trustworthy Inter-Provider Agreements in 6G Using a Privacy-Enabled
  Hybrid Blockchain Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2025-05-05T09:46:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Farhana Javed, Josep Mangues-Bafalluy</p>
    <p><b>Summary:</b> Inter-provider agreements are central to 6G networks, where administrative
domains must securely and dynamically share services. To address the dual need
for transparency and confidentiality, we propose a privacy-enabled hybrid
blockchain setup using Hyperledger Besu, integrating both public and private
transaction workflows. The system enables decentralized service registration,
selection, and SLA breach reporting through role-based smart contracts and
privacy groups. We design and deploy a proof-of-concept implementation,
evaluating performance using end-to-end latency as a key metric within privacy
groups. Results show that public interactions maintain stable latency, while
private transactions incur additional overhead due to off-chain coordination.
The block production rate governed by IBFT 2.0 had limited impact on private
transaction latency, due to encryption and peer synchronization. Lessons
learned highlight design considerations for smart contract structure, validator
management, and scalability patterns suitable for dynamic inter-domain
collaboration. Our findings offer practical insights for deploying trustworthy
agreement systems in 6G networks using privacy-enabled hybrid blockchains.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02392v3">Moneros Decentralized P2P Exchanges: Functionality, Adoption, and
  Privacy Risks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-05T06:27:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yannik Kopyciok, Friedhelm Victor, Stefan Schmid</p>
    <p><b>Summary:</b> Privacy-focused cryptocurrencies like Monero remain popular, despite
increasing regulatory scrutiny that has led to their delisting from major
centralized exchanges. The latter also explains the recent popularity of
decentralized exchanges (DEXs) with no centralized ownership structures. These
platforms typically leverage peer-to-peer (P2P) networks, promising secure and
anonymous asset trading. However, questions of liability remain, and the
academic literature lacks comprehensive insights into the functionality,
trading activity, and privacy claims of these P2P platforms. In this paper, we
provide an early systematization of the current landscape of decentralized
peer-to-peer exchanges within the Monero ecosystem. We examine several recently
developed DEX platforms, analyzing their popularity, functionality,
architectural choices, and potential weaknesses. We further identify and report
on a privacy vulnerability in the recently popularized Haveno exchange,
demonstrating that certain Haveno trades could be detected, allowing
transactions to be linked across the Monero and Bitcoin blockchains. We hope
that our findings can nourish the discussion in the research community about
more secure designs, and provide insights for regulators.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02383v1">Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs
  Between Privacy and Regret</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-05T05:48:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bingshan Hu, Zhiming Huang, Tianyue H. Zhang, Mathias Lécuyer, Nidhi Hegde</p>
    <p><b>Summary:</b> We address differentially private stochastic bandit problems from the angles
of exploring the deep connections among Thompson Sampling with Gaussian priors,
Gaussian mechanisms, and Gaussian differential privacy (GDP). We propose
DP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade
off privacy and regret. DP-TS-UCB satisfies $ \tilde{O}
\left(T^{0.25(1-\alpha)}\right)$-GDP and enjoys an $O
\left(K\ln^{\alpha+1}(T)/\Delta \right)$ regret bound, where $\alpha \in [0,1]$
controls the trade-off between privacy and regret. Theoretically, our DP-TS-UCB
relies on anti-concentration bounds of Gaussian distributions and links
exploration mechanisms in Thompson Sampling-based algorithms and Upper
Confidence Bound-based algorithms, which may be of independent interest.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01976v1">A Survey on Privacy Risks and Protection in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-04T03:04:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kang Chen, Xiuze Zhou, Yuanguo Lin, Shibo Feng, Li Shen, Pengcheng Wu</p>
    <p><b>Summary:</b> Although Large Language Models (LLMs) have become increasingly integral to
diverse applications, their capabilities raise significant privacy concerns.
This survey offers a comprehensive overview of privacy risks associated with
LLMs and examines current solutions to mitigate these challenges. First, we
analyze privacy leakage and attacks in LLMs, focusing on how these models
unintentionally expose sensitive information through techniques such as model
inversion, training data extraction, and membership inference. We investigate
the mechanisms of privacy leakage, including the unauthorized extraction of
training data and the potential exploitation of these vulnerabilities by
malicious actors. Next, we review existing privacy protection against such
risks, such as inference detection, federated learning, backdoor mitigation,
and confidential computing, and assess their effectiveness in preventing
privacy leakage. Furthermore, we highlight key practical challenges and propose
future research directions to develop secure and privacy-preserving LLMs,
emphasizing privacy risk assessment, secure knowledge transfer between models,
and interdisciplinary frameworks for privacy governance. Ultimately, this
survey aims to establish a roadmap for addressing escalating privacy challenges
in the LLMs domain.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01879v1">What to Do When Privacy Is Gone</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-05-03T17:51:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> James Brusseau</p>
    <p><b>Summary:</b> Today's ethics of privacy is largely dedicated to defending personal
information from big data technologies. This essay goes in the other direction.
It considers the struggle to be lost, and explores two strategies for living
after privacy is gone. First, total exposure embraces privacy's decline, and
then contributes to the process with transparency. All personal information is
shared without reservation. The resulting ethics is explored through a big data
version of Robert Nozick's Experience Machine thought experiment. Second,
transient existence responds to privacy's loss by ceaselessly generating new
personal identities, which translates into constantly producing temporarily
unviolated private information. The ethics is explored through Gilles Deleuze's
metaphysics of difference applied in linguistic terms to the formation of the
self. Comparing the exposure and transience alternatives leads to the
conclusion that today's big data reality splits the traditional ethical link
between authenticity and freedom. Exposure provides authenticity, but negates
human freedom. Transience provides freedom, but disdains authenticity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01788v1">Privacy Preserving Machine Learning Model Personalization through
  Federated Personalized Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-05-03T11:31:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md. Tanzib Hosain, Asif Zaman, Md. Shahriar Sajid, Shadman Sakeeb Khan, Shanjida Akter</p>
    <p><b>Summary:</b> The widespread adoption of Artificial Intelligence (AI) has been driven by
significant advances in intelligent system research. However, this progress has
raised concerns about data privacy, leading to a growing awareness of the need
for privacy-preserving AI. In response, there has been a seismic shift in
interest towards the leading paradigm for training Machine Learning (ML) models
on decentralized data silos while maintaining data privacy, Federated Learning
(FL). This research paper presents a comprehensive performance analysis of a
cutting-edge approach to personalize ML model while preserving privacy achieved
through Privacy Preserving Machine Learning with the innovative framework of
Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns
about data privacy, this study evaluates the effectiveness of PPMLFPL
addressing the critical balance between personalized model refinement and
maintaining the confidentiality of individual user data. According to our
analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential
Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the
Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption
(APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated
personalized learning settings is strongly suggested. The results offer
valuable insights creating it a promising scope for future advancements in the
field of privacy-conscious data-driven technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01524v1">The DCR Delusion: Measuring the Privacy Risk of Synthetic Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-02T18:21:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zexi Yao, Nataša Krčo, Georgi Ganev, Yves-Alexandre de Montjoye</p>
    <p><b>Summary:</b> Synthetic data has become an increasingly popular way to share data without
revealing sensitive information. Though Membership Inference Attacks (MIAs) are
widely considered the gold standard for empirically assessing the privacy of a
synthetic dataset, practitioners and researchers often rely on simpler proxy
metrics such as Distance to Closest Record (DCR). These metrics estimate
privacy by measuring the similarity between the training data and generated
synthetic data. This similarity is also compared against that between the
training data and a disjoint holdout set of real records to construct a binary
privacy test. If the synthetic data is not more similar to the training data
than the holdout set is, it passes the test and is considered private. In this
work we show that, while computationally inexpensive, DCR and other
distance-based metrics fail to identify privacy leakage. Across multiple
datasets and both classical models such as Baynet and CTGAN and more recent
diffusion models, we show that datasets deemed private by proxy metrics are
highly vulnerable to MIAs. We similarly find both the binary privacy test and
the continuous measure based on these metrics to be uninformative of actual
membership inference risk. We further show that these failures are consistent
across different metric hyperparameter settings and record selection methods.
Finally, we argue DCR and other distance-based metrics to be flawed by design
and show a example of a simple leakage they miss in practice. With this work,
we hope to motivate practitioners to move away from proxy metrics to MIAs as
the rigorous, comprehensive standard of evaluating privacy of synthetic data,
in particular to make claims of datasets being legally anonymous.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01292v1">Fine-grained Manipulation Attacks to Local Differential Privacy
  Protocols for Data Streams</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-02T14:09:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinyu Li, Xuebin Ren, Shusen Yang, Liang Shi, Chia-Mu Yu</p>
    <p><b>Summary:</b> Local Differential Privacy (LDP) enables massive data collection and analysis
while protecting end users' privacy against untrusted aggregators. It has been
applied to various data types (e.g., categorical, numerical, and graph data)
and application settings (e.g., static and streaming). Recent findings indicate
that LDP protocols can be easily disrupted by poisoning or manipulation
attacks, which leverage injected/corrupted fake users to send crafted data
conforming to the LDP reports. However, current attacks primarily target static
protocols, neglecting the security of LDP protocols in the streaming settings.
Our research fills the gap by developing novel fine-grained manipulation
attacks to LDP protocols for data streams. By reviewing the attack surfaces in
existing algorithms, We introduce a unified attack framework with composable
modules, which can manipulate the LDP estimated stream toward a target stream.
Our attack framework can adapt to state-of-the-art streaming LDP algorithms
with different analytic tasks (e.g., frequency and mean) and LDP models
(event-level, user-level, w-event level). We validate our attacks theoretically
and through extensive experiments on real-world datasets, and finally explore a
possible defense mechanism for mitigating these attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.00951v1">Preserving Privacy and Utility in LLM-Based Product Recommendations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-02T01:54:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tina Khezresmaeilzadeh, Jiang Zhang, Dimitrios Andreadis, Konstantinos Psounis</p>
    <p><b>Summary:</b> Large Language Model (LLM)-based recommendation systems leverage powerful
language models to generate personalized suggestions by processing user
interactions and preferences. Unlike traditional recommendation systems that
rely on structured data and collaborative filtering, LLM-based models process
textual and contextual information, often using cloud-based infrastructure.
This raises privacy concerns, as user data is transmitted to remote servers,
increasing the risk of exposure and reducing control over personal information.
To address this, we propose a hybrid privacy-preserving recommendation
framework which separates sensitive from nonsensitive data and only shares the
latter with the cloud to harness LLM-powered recommendations. To restore lost
recommendations related to obfuscated sensitive data, we design a
de-obfuscation module that reconstructs sensitive recommendations locally.
Experiments on real-world e-commerce datasets show that our framework achieves
almost the same recommendation utility with a system which shares all data with
an LLM, while preserving privacy to a large extend. Compared to
obfuscation-only techniques, our approach improves HR@10 scores and category
distribution alignment, offering a better balance between privacy and
recommendation quality. Furthermore, our method runs efficiently on
consumer-grade hardware, making privacy-aware LLM-based recommendation systems
practical for real-world use.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.00593v1">A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security
  and Privacy in IoT and Edge Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-01T15:26:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Muhammad Shahbaz Khan, Ahmed Al-Dubai, Jawad Ahmad, Nikolaos Pitropakis, Baraq Ghaleb</p>
    <p><b>Summary:</b> The security of image data in the Internet of Things (IoT) and edge networks
is crucial due to the increasing deployment of intelligent systems for
real-time decision-making. Traditional encryption algorithms such as AES and
RSA are computationally expensive for resource-constrained IoT devices and
ineffective for large-volume image data, leading to inefficiencies in
privacy-preserving distributed learning applications. To address these
concerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption
scheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic
Chain Permutation and Confusion mechanisms to enhance security while
maintaining efficiency. The proposed scheme consists of three stages: (1) FAPS,
which extracts and reorganizes pixels based on high and low edge intensity
features for correlation disruption; (2) Chaotic Chain Permutation, which
employs a logistic chaotic map with SHA-256-based dynamically updated keys for
block-wise permutation; and (3) Chaotic chain Confusion, which utilises
dynamically generated chaotic seed matrices for bitwise XOR operations.
Extensive security and performance evaluations demonstrate that the proposed
scheme significantly reduces pixel correlation -- almost zero, achieves high
entropy values close to 8, and resists differential cryptographic attacks. The
optimum design of the proposed scheme makes it suitable for real-time
deployment in resource-constrained environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.00257v1">Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial
  Data Circulation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-01T02:47:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhizhong Tan, Jiexin Zheng, Kevin Qi Zhang, Wenyong Wang</p>
    <p><b>Summary:</b> The sharing of external data has become a strong demand of financial
institutions, but the privacy issue has led to the difficulty of
interconnecting different platforms and the low degree of data openness. To
effectively solve the privacy problem of financial data in trans-border flow
and sharing, to ensure that the data is available but not visible, to realize
the joint portrait of all kinds of heterogeneous data of business organizations
in different industries, we propose a Heterogeneous Federated Graph Neural
Network (HFGNN) approach. In this method, the distribution of heterogeneous
business data of trans-border organizations is taken as subgraphs, and the
sharing and circulation process among subgraphs is constructed as a
statistically heterogeneous global graph through a central server. Each
subgraph learns the corresponding personalized service model through local
training to select and update the relevant subset of subgraphs with aggregated
parameters, and effectively separates and combines topological and feature
information among subgraphs. Finally, our simulation experimental results show
that the proposed method has higher accuracy performance and faster convergence
speed than existing methods.</p>
  </details>
</div>

