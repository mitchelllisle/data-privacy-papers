
<h2>2025-01</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12359v1">Measured Hockey-Stick Divergence and its Applications to Quantum
  Pufferfish Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-01-21T18:39:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Theshani Nuradha, Vishal Singh, Mark M. Wilde</p>
    <p><b>Summary:</b> The hockey-stick divergence is a fundamental quantity characterizing several
statistical privacy frameworks that ensure privacy for classical and quantum
data. In such quantum privacy frameworks, the adversary is allowed to perform
all possible measurements. However, in practice, there are typically
limitations to the set of measurements that can be performed. To this end,
here, we comprehensively analyze the measured hockey-stick divergence under
several classes of practically relevant measurement classes. We prove several
of its properties, including data processing and convexity. We show that it is
efficiently computable by semi-definite programming for some classes of
measurements and can be analytically evaluated for Werner and isotropic states.
Notably, we show that the measured hockey-stick divergence characterizes
optimal privacy parameters in the quantum pufferfish privacy framework. With
this connection and the developed technical tools, we enable methods to
quantify and audit privacy for several practically relevant settings. Lastly,
we introduce the measured hockey-stick divergence of channels and explore its
applications in ensuring privacy for channels.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12193v1">MyDigiTwin: A Privacy-Preserving Framework for Personalized
  Cardiovascular Risk Prediction and Scenario Exploration</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-01-21T15:01:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> HÃ©ctor Cadavid, Hyunho Mo, Bauke Arends, Katarzyna Dziopa, Esther E. Bron, Daniel Bos, Sonja Georgievska, Pim van der Harst</p>
    <p><b>Summary:</b> Cardiovascular disease (CVD) remains a leading cause of death, and primary
prevention through personalized interventions is crucial. This paper introduces
MyDigiTwin, a framework that integrates health digital twins with personal
health environments to empower patients in exploring personalized health
scenarios while ensuring data privacy. MyDigiTwin uses federated learning to
train predictive models across distributed datasets without transferring raw
data, and a novel data harmonization framework addresses semantic and format
inconsistencies in health data. A proof-of-concept demonstrates the feasibility
of harmonizing and using cohort data to train privacy-preserving CVD prediction
models. This framework offers a scalable solution for proactive, personalized
cardiovascular care and sets the stage for future applications in real-world
healthcare settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12046v1">Communication-Efficient and Privacy-Adaptable Mechanism for Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-21T11:16:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chih Wei Ling, Youqi Wu, Jiande Sun, Cheuk Ting Li, Linqi Song, Weitao Xu</p>
    <p><b>Summary:</b> Training machine learning models on decentralized private data via federated
learning (FL) poses two key challenges: communication efficiency and privacy
protection. In this work, we address these challenges within the trusted
aggregator model by introducing a novel approach called the
Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), achieving both
objectives simultaneously. In particular, CEPAM leverages the rejection-sampled
universal quantizer (RSUQ), a construction of randomized vector quantizer whose
resulting distortion is equivalent to a prescribed noise, such as Gaussian or
Laplace noise, enabling joint differential privacy and compression. Moreover,
we analyze the trade-offs among user privacy, global utility, and transmission
rate of CEPAM by defining appropriate metrics for FL with differential privacy
and compression. Our CEPAM provides the additional benefit of privacy
adaptability, allowing clients and the server to customize privacy protection
based on required accuracy and protection. We assess CEPAM's utility
performance using MNIST dataset, demonstrating that CEPAM surpasses baseline
models in terms of learning accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12006v1">The Dilemma of Privacy Protection for Developers in the Metaverse</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-01-21T09:56:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Argianto Rahartomo, Leonel Merino, Mohammad Ghafari, Yoshiki Ohshima</p>
    <p><b>Summary:</b> To investigate the level of support and awareness developers possess for
dealing with sensitive data in the metaverse, we surveyed developers, consulted
legal frameworks, and analyzed API documentation in the metaverse. Our
preliminary results suggest that privacy is a major concern, but developer
awareness and existing support are limited. Developers lack strategies to
identify sensitive data that are exclusive to the metaverse. The API
documentation contains guidelines for collecting sensitive information, but it
omits instructions for identifying and protecting it. Legal frameworks include
definitions that are subject to individual interpretation. These findings
highlight the urgent need to build a transparent and common ground for privacy
definitions, identify sensitive data, and implement usable protection measures.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.11757v1">An Information Geometric Approach to Local Information Privacy with
  Applications to Max-lift and Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-20T21:34:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amirreza Zamani, Parastoo Sadeghi, Mikael Skoglund</p>
    <p><b>Summary:</b> We study an information-theoretic privacy mechanism design, where an agent
observes useful data $Y$ and wants to reveal the information to a user. Since
the useful data is correlated with the private data $X$, the agent uses a
privacy mechanism to produce disclosed data $U$ that can be released. We assume
that the agent observes $Y$ and has no direct access to $X$, i.e., the private
data is hidden. We study the privacy mechanism design that maximizes the
revealed information about $Y$ while satisfying a bounded Local Information
Privacy (LIP) criterion. When the leakage is sufficiently small, concepts from
information geometry allow us to locally approximate the mutual information. By
utilizing this approximation the main privacy-utility trade-off problem can be
rewritten as a quadratic optimization problem that has closed-form solution
under some constraints. For the cases where the closed-form solution is not
obtained we provide lower bounds on it. In contrast to the previous works that
have complexity issues, here, we provide simple privacy designs with low
complexity which are based on finding the maximum singular value and singular
vector of a matrix. To do so, we follow two approaches where in the first one
we find a lower bound on the main problem and then approximate it, however, in
the second approach we approximate the main problem directly. In this work, we
present geometrical interpretations of the proposed methods and in a numerical
example we compare our results considering both approaches with the optimal
solution and the previous methods. Furthermore, we discuss how our method can
be generalized considering larger amounts for the privacy leakage. Finally, we
discuss how the proposed methods can be applied to deal with differential
privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.11756v1">Everyone's Privacy Matters! An Analysis of Privacy Leakage from
  Real-World Facial Images on Twitter and Associated User Behaviors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-01-20T21:31:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuqi Niu, Weidong Qiu, Peng Tang, Lifan Wang, Shuo Chen, Shujun Li, Nadin Kokciyan, Ben Niu</p>
    <p><b>Summary:</b> Online users often post facial images of themselves and other people on
online social networks (OSNs) and other Web 2.0 platforms, which can lead to
potential privacy leakage of people whose faces are included in such images.
There is limited research on understanding face privacy in social media while
considering user behavior. It is crucial to consider privacy of subjects and
bystanders separately. This calls for the development of privacy-aware face
detection classifiers that can distinguish between subjects and bystanders
automatically. This paper introduces such a classifier trained on face-based
features, which outperforms the two state-of-the-art methods with a significant
margin (by 13.1% and 3.1% for OSN images, and by 17.9% and 5.9% for non-OSN
images). We developed a semi-automated framework for conducting a large-scale
analysis of the face privacy problem by using our novel bystander-subject
classifier. We collected 27,800 images, each including at least one face,
shared by 6,423 Twitter users. We then applied our framework to analyze this
dataset thoroughly. Our analysis reveals eight key findings of different
aspects of Twitter users' real-world behaviors on face privacy, and we provide
quantitative and qualitative results to better explain these findings. We share
the practical implications of our study to empower online platforms and users
in addressing the face privacy problem efficiently.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.11740v1">PIR Over Wireless Channels: Achieving Privacy With Public Responses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-20T20:56:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Or Elimelech, Asaf Cohen</p>
    <p><b>Summary:</b> This paper addresses the problem of private information retrieval (PIR) over
an additive white Gaussian noise (AWGN) channel, considering the channel is
public. In such settings, each server can eavesdrop on the channel, potentially
compromising the user's privacy. Previous works suggested joint coding--PIR
schemes, ignoring the fact that communication over a practical wireless channel
is public. To address this gap, we present a novel joint wiretap--PIR coding
scheme that leverages lattice codes to exploit the channel's additive
properties. This scheme integrates wiretap coding and private retrieval
techniques into a unified framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.10915v1">LegalGuardian: A Privacy-Preserving Framework for Secure Integration of
  Large Language Models in Legal Practice</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">  
  <p><b>Published on:</b> 2025-01-19T01:43:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> M. Mikail Demir, Hakan T. Otal, M. Abdullah Canbaz</p>
    <p><b>Summary:</b> Large Language Models (LLMs) hold promise for advancing legal practice by
automating complex tasks and improving access to justice. However, their
adoption is limited by concerns over client confidentiality, especially when
lawyers include sensitive Personally Identifiable Information (PII) in prompts,
risking unauthorized data exposure. To mitigate this, we introduce
LegalGuardian, a lightweight, privacy-preserving framework tailored for lawyers
using LLM-based tools. LegalGuardian employs Named Entity Recognition (NER)
techniques and local LLMs to mask and unmask confidential PII within prompts,
safeguarding sensitive data before any external interaction. We detail its
development and assess its effectiveness using a synthetic prompt library in
immigration law scenarios. Comparing traditional NER models with one-shot
prompted local LLM, we find that LegalGuardian achieves a F1-score of 93% with
GLiNER and 97% with Qwen2.5-14B in PII detection. Semantic similarity analysis
confirms that the framework maintains high fidelity in outputs, ensuring robust
utility of LLM-based tools. Our findings indicate that legal professionals can
harness advanced AI technologies without compromising client confidentiality or
the quality of legal documents.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.10319v1">Natural Language Processing of Privacy Policies: A Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-01-17T17:47:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Andrick Adhikari, Sanchari Das, Rinku Dewri</p>
    <p><b>Summary:</b> Natural Language Processing (NLP) is an essential subset of artificial
intelligence. It has become effective in several domains, such as healthcare,
finance, and media, to identify perceptions, opinions, and misuse, among
others. Privacy is no exception, and initiatives have been taken to address the
challenges of usable privacy notifications to users with the help of NLP. To
this aid, we conduct a literature review by analyzing 109 papers at the
intersection of NLP and privacy policies. First, we provide a brief
introduction to privacy policies and discuss various facets of associated
problems, which necessitate the application of NLP to elevate the current state
of privacy notices and disclosures to users. Subsequently, we a) provide an
overview of the implementation and effectiveness of NLP approaches for better
privacy policy communication; b) identify the methodologies that can be further
enhanced to provide robust privacy policies; and c) identify the gaps in the
current state-of-the-art research. Our systematic analysis reveals that several
research papers focus on annotating and classifying privacy texts for analysis
but need to adequately dwell on other aspects of NLP applications, such as
summarization. More specifically, ample research opportunities exist in this
domain, covering aspects such as corpus generation, summarization vectors,
contextualized word embedding, identification of privacy-relevant statement
categories, fine-grained classification, and domain-specific model tuning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.10099v1">Several Representations of $Î±$-Mutual Information and
  Interpretations as Privacy Leakage Measures</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-17T10:36:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Akira Kamatsuka, Takashiro Yoshida</p>
    <p><b>Summary:</b> In this paper, we present several novel representations of $\alpha$-mutual
information ($\alpha$-MI) in terms of R{\' e}nyi divergence and conditional
R{\' e}nyi entropy. The representations are based on the variational
characterizations of $\alpha$-MI using a reverse channel. Based on these
representations, we provide several interpretations of the $\alpha$-MI as
privacy leakage measures using generalized mean and gain functions. Further, as
byproducts of the representations, we propose novel conditional R{\' e}nyi
entropies that satisfy the property that conditioning reduces entropy and
data-processing inequality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.09191v1">Detecting Vulnerabilities in Encrypted Software Code while Ensuring Code
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-01-15T22:39:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jorge Martins, David Dantas, Rafael Ramires, Bernardo Ferreira, IbÃ©ria Medeiros</p>
    <p><b>Summary:</b> Software vulnerabilities continue to be the main cause of occurrence for
cyber attacks. In an attempt to reduce them and improve software quality,
software code analysis has emerged as a service offered by companies
specialising in software testing. However, this service requires software
companies to provide access to their software's code, which raises concerns
about code privacy and intellectual property theft. This paper presents a novel
approach to Software Quality and Privacy, in which testing companies can
perform code analysis tasks on encrypted software code provided by software
companies while code privacy is preserved. The approach combines Static Code
Analysis and Searchable Symmetric Encryption in order to process the source
code and build an encrypted inverted index that represents its data and control
flows. The index is then used to discover vulnerabilities by carrying out
static analysis tasks in a confidential way. With this approach, this paper
also defines a new research field -- Confidential Code Analysis --, from which
other types of code analysis tasks and approaches can be derived. We
implemented the approach in a new tool called CoCoA and evaluated it
experimentally with synthetic and real PHP web applications. The results show
that the tool has similar precision as standard (non-confidential) static
analysis tools and a modest average performance overhead of 42.7%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.08665v1">A Survey on Facial Image Privacy Preservation in Cloud-Based Services</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-15T09:00:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chen Chen, Mengyuan Sun, Xueluan Gong, Yanjiao Chen, Qian Wang</p>
    <p><b>Summary:</b> Facial recognition models are increasingly employed by commercial
enterprises, government agencies, and cloud service providers for identity
verification, consumer services, and surveillance. These models are often
trained using vast amounts of facial data processed and stored in cloud-based
platforms, raising significant privacy concerns. Users' facial images may be
exploited without their consent, leading to potential data breaches and misuse.
This survey presents a comprehensive review of current methods aimed at
preserving facial image privacy in cloud-based services. We categorize these
methods into two primary approaches: image obfuscation-based protection and
adversarial perturbation-based protection. We provide an in-depth analysis of
both categories, offering qualitative and quantitative comparisons of their
effectiveness. Additionally, we highlight unresolved challenges and propose
future research directions to improve privacy preservation in cloud computing
environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.08449v1">A Refreshment Stirred, Not Shaken (II): Invariant-Preserving Deployments
  of Differential Privacy for the US Decennial Census</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> 
  <p><b>Published on:</b> 2025-01-14T21:38:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> James Bailie, Ruobin Gong, Xiao-Li Meng</p>
    <p><b>Summary:</b> Through the lens of the system of differential privacy specifications
developed in Part I of a trio of articles, this second paper examines two
statistical disclosure control (SDC) methods for the United States Decennial
Census: the Permutation Swapping Algorithm (PSA), which is similar to the 2010
Census's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA),
which was used in the 2020 DAS. To varying degrees, both methods leave
unaltered some statistics of the confidential data $\unicode{x2013}$ which are
called the method's invariants $\unicode{x2013}$ and hence neither can be
readily reconciled with differential privacy (DP), at least as it was
originally conceived. Nevertheless, we establish that the PSA satisfies
$\varepsilon$-DP subject to the invariants it necessarily induces, thereby
showing that this traditional SDC method can in fact still be understood within
our more-general system of DP specifications. By a similar modification to
$\rho$-zero concentrated DP, we also provide a DP specification for the TDA.
Finally, as a point of comparison, we consider the counterfactual scenario in
which the PSA was adopted for the 2020 Census, resulting in a reduction in the
nominal privacy loss, but at the cost of releasing many more invariants.
Therefore, while our results explicate the mathematical guarantees of SDC
provided by the PSA, the TDA and the 2020 DAS in general, care must be taken in
their translation to actual privacy protection $\unicode{x2013}$ just as is the
case for any DP deployment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.08236v1">Privacy-Preserving Model and Preprocessing Verification for Machine
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-14T16:21:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenbiao Li, Anisa Halimi, Xiaoqian Jiang, Jaideep Vaidya, Erman Ayday</p>
    <p><b>Summary:</b> This paper presents a framework for privacy-preserving verification of
machine learning models, focusing on models trained on sensitive data.
Integrating Local Differential Privacy (LDP) with model explanations from LIME
and SHAP, our framework enables robust verification without compromising
individual privacy. It addresses two key tasks: binary classification, to
verify if a target model was trained correctly by applying the appropriate
preprocessing steps, and multi-class classification, to identify specific
preprocessing errors. Evaluations on three real-world datasets-Diabetes, Adult,
and Student Record-demonstrate that while the ML-based approach is particularly
effective in binary tasks, the threshold-based method performs comparably in
multi-class tasks. Results indicate that although verification accuracy varies
across datasets and noise levels, the framework provides effective detection of
preprocessing errors, strong privacy guarantees, and practical applicability
for safeguarding sensitive data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.07844v2">Towards A Hybrid Quantum Differential Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-14T05:13:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Baobao Song, Shiva Raj Pokhrel, Athanasios V. Vasilakos, Tianqing Zhu, Gang Li</p>
    <p><b>Summary:</b> Quantum computing offers unparalleled processing power but raises significant
data privacy challenges. Quantum Differential Privacy (QDP) leverages inherent
quantum noise to safeguard privacy, surpassing traditional DP. This paper
develops comprehensive noise profiles, identifies noise types beneficial for
QDP, and highlights teh need for practical implementations beyond theoretical
models. Existing QDP mechanisms, limited to single noise sources, fail to
reflect teh multi-source noise reality of quantum systems. We propose a
resilient hybrid QDP mechanism utilizing channel and measurement noise,
optimizing privacy budgets to balance privacy and utility. Additionally, we
introduce Lifted Quantum Differential Privacy, offering enhanced randomness for
improved privacy audits and quantum algorithm evaluation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.07262v1">OblivCDN: A Practical Privacy-preserving CDN with Oblivious Content
  Access</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-13T12:23:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Viet Vo, Shangqi Lai, Xingliang Yuan, Surya Nepal, Qi Li</p>
    <p><b>Summary:</b> Content providers increasingly utilise Content Delivery Networks (CDNs) to
enhance users' content download experience. However, this deployment scenario
raises significant security concerns regarding content confidentiality and user
privacy due to the involvement of third-party providers. Prior proposals using
private information retrieval (PIR) and oblivious RAM (ORAM) have proven
impractical due to high computation and communication costs, as well as
integration challenges within distributed CDN architectures. In response, we
present \textsf{OblivCDN}, a practical privacy-preserving system meticulously
designed for seamless integration with the existing real-world Internet-CDN
infrastructure. Our design strategically adapts Range ORAM primitives to
optimise memory and disk seeks when accessing contiguous blocks of CDN content,
both at the origin and edge servers, while preserving both content
confidentiality and user access pattern hiding features. Also, we carefully
customise several oblivious building blocks that integrate the distributed
trust model into the ORAM client, thereby eliminating the computational
bottleneck in the origin server and reducing communication costs between the
origin server and edge servers. Moreover, the newly-designed ORAM client also
eliminates the need for trusted hardware on edge servers, and thus
significantly ameliorates the compatibility towards networks with massive
legacy devices.In real-world streaming evaluations, OblivCDN} demonstrates
remarkable performance, downloading a $256$ MB video in just $5.6$ seconds.
This achievement represents a speedup of $90\times$ compared to a strawman
approach (direct ORAM adoption) and a $366\times$ improvement over the prior
art, OblivP2P.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.07209v1">Privacy-Preserving Authentication: Theory vs. Practice</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-13T11:04:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Slamanig</p>
    <p><b>Summary:</b> With the increasing use of online services, the protection of the privacy of
users becomes more and more important. This is particularly critical as
authentication and authorization as realized on the Internet nowadays,
typically relies on centralized identity management solutions. Although those
are very convenient from a user's perspective, they are quite intrusive from a
privacy perspective and are currently far from implementing the concept of data
minimization. Fortunately, cryptography offers exciting primitives such as
zero-knowledge proofs and advanced signature schemes to realize various forms
of so-called anonymous credentials. Such primitives allow to realize online
authentication and authorization with a high level of built-in privacy
protection (what we call privacy-preserving authentication). Though these
primitives have already been researched for various decades and are well
understood in the research community, unfortunately, they lack widespread
adoption. In this paper, we look at the problems, what cryptography can do,
some deployment examples, and barriers to widespread adoption. Latter using the
example of the EU Digital Identity Wallet (EUDIW) and the recent discussion and
feedback from cryptography experts around this topic. We also briefly comment
on the transition to post-quantum cryptography.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.09031v1">Synthetic Data and Health Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-01-13T10:23:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> GwÃ©nolÃ© Abgrall, Xavier Monnet, Anmol Arora</p>
    <p><b>Summary:</b> This Viewpoint discusses generative artificial intelligence and safeguarding
privacy by using synthetic data as a substitute for private health data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.07154v1">Privacy-Preserving Data Quality Assessment for Time-Series IoT Sensors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-13T09:28:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Novoneel Chakraborty, Abhay Sharma, Jyotirmoy Dutta, Hari Dilip Kumar</p>
    <p><b>Summary:</b> Data from Internet of Things (IoT) sensors has emerged as a key contributor
to decision-making processes in various domains. However, the quality of the
data is crucial to the effectiveness of applications built on it, and
assessment of the data quality is heavily context-dependent. Further,
preserving the privacy of the data during quality assessment is critical in
domains where sensitive data is prevalent. This paper proposes a novel
framework for automated, objective, and privacy-preserving data quality
assessment of time-series data from IoT sensors deployed in smart cities. We
leverage custom, autonomously computable metrics that parameterise the temporal
performance and adherence to a declarative schema document to achieve
objectivity. Additionally, we utilise a trusted execution environment to create
a "data-blind" model that ensures individual privacy, eliminates assessee bias,
and enhances adaptability across data types. This paper describes this data
quality assessment methodology for IoT sensors, emphasising its relevance
within the smart-city context while addressing the growing need for privacy in
the face of extensive data collection practices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.06913v1">Towards Fair and Privacy-Aware Transfer Learning for Educational
  Predictive Modeling: A Case Study on Retention Prediction in Community
  Colleges</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-01-12T19:49:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chengyuan Yao, Carmen Cortez, Renzhe Yu</p>
    <p><b>Summary:</b> Predictive analytics is widely used in learning analytics, but many
resource-constrained institutions lack the capacity to develop their own models
or rely on proprietary ones trained in different contexts with little
transparency. Transfer learning holds promise for expanding equitable access to
predictive analytics but remains underexplored due to legal and technical
constraints. This paper examines transfer learning strategies for retention
prediction at U.S. two-year community colleges. We envision a scenario where
community colleges collaborate with each other and four-year universities to
develop retention prediction models under privacy constraints and evaluate
risks and improvement strategies of cross-institutional model transfer. Using
administrative records from 4 research universities and 23 community colleges
covering over 800,000 students across 7 cohorts, we identify performance and
fairness degradation when external models are deployed locally without
adaptation. Publicly available contextual information can forecast these
performance drops and offer early guidance for model portability. For
developers under privacy regulations, sequential training selecting
institutions based on demographic similarities enhances fairness without
compromising performance. For institutions lacking local data to fine-tune
source models, customizing evaluation thresholds for sensitive groups
outperforms standard transfer techniques in improving performance and fairness.
Our findings suggest the value of transfer learning for more accessible
educational predictive modeling and call for judicious use of contextual
information in model training, selection, and deployment to achieve reliable
and equitable model transfer.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.06300v1">Tensorization of neural networks for improved privacy and
  interpretability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Numerical Analysis-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Numerical Analysis-5BC0EB">  
  <p><b>Published on:</b> 2025-01-10T19:00:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> JosÃ© RamÃ³n Pareja Monturiol, Alejandro Pozas-Kerstjens, David PÃ©rez-GarcÃ­a</p>
    <p><b>Summary:</b> We present a tensorization algorithm for constructing tensor train
representations of functions, drawing on sketching and cross interpolation
ideas. The method only requires black-box access to the target function and a
small set of sample points defining the domain of interest. Thus, it is
particularly well-suited for machine learning models, where the domain of
interest is naturally defined by the training dataset. We show that this
approach can be used to enhance the privacy and interpretability of neural
network models. Specifically, we apply our decomposition to (i) obfuscate
neural networks whose parameters encode patterns tied to the training data
distribution, and (ii) estimate topological phases of matter that are easily
accessible from the tensor train representation. Additionally, we show that
this tensorization can serve as an efficient initialization method for
optimizing tensor trains in general settings, and that, for model compression,
our algorithm achieves a superior trade-off between memory and time complexity
compared to conventional tensorization methods of neural networks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.06161v2">RIOT-based smart metering system for privacy-preserving data aggregation
  using watermarking and encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-10T18:37:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Farzana Kabir, David Megias, Krzysztof Cabaj</p>
    <p><b>Summary:</b> The remarkable advancement of smart grid technology in the IoT sector has
raised concerns over the privacy and security of the data collected and
transferred in real-time. Smart meters generate detailed information about
consumers' energy consumption patterns, increasing the risks of data breaches,
identity theft, and other forms of cyber attacks. This study proposes a
privacy-preserving data aggregation protocol that uses reversible watermarking
and AES cryptography to ensure the security and privacy of the data. There are
two versions of the protocol: one for low-frequency smart meters that uses
LSB-shifting-based reversible watermarking (RLS) and another for high-frequency
smart meters that uses difference expansion-based reversible watermarking
(RDE). This enables the aggregation of smart meter data, maintaining
confidentiality, integrity, and authenticity. The proposed protocol
significantly enhances privacy-preserving measures for smart metering systems,
conducting an experimental evaluation with real hardware implementation using
Nucleo microcontroller boards and the RIOT operating system and comparing the
results to existing security schemes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.05535v1">On Fair Ordering and Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-01-09T19:17:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shir Cohen, Neel Basu, Soumya Basu, Lorenzo Alvisi</p>
    <p><b>Summary:</b> In blockchain systems, fair transaction ordering is crucial for a trusted and
regulation-compliant economic ecosystem. Unlike traditional State Machine
Replication (SMR) systems, which focus solely on liveness and safety,
blockchain systems also require a fairness property. This paper examines these
properties and aims to eliminate algorithmic bias in transaction ordering
services.
  We build on the notion of equal opportunity. We characterize transactions in
terms of relevant and irrelevant features, requiring that the order be
determined solely by the relevant ones. Specifically, transactions with
identical relevant features should have an equal chance of being ordered before
one another. We extend this framework to define a property where the greater
the distance in relevant features between transactions, the higher the
probability of prioritizing one over the other.
  We reveal a surprising link between equal opportunity in SMR and Differential
Privacy (DP), showing that any DP mechanism can be used to ensure fairness in
SMR. This connection not only enhances our understanding of the interplay
between privacy and fairness in distributed computing but also opens up new
opportunities for designing fair distributed protocols using well-established
DP techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.05053v1">TAPFed: Threshold Secure Aggregation for Privacy-Preserving Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-01-09T08:24:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Runhua Xu, Bo Li, Chao Li, James B. D. Joshi, Shuai Ma, Jianxin Li</p>
    <p><b>Summary:</b> Federated learning is a computing paradigm that enhances privacy by enabling
multiple parties to collaboratively train a machine learning model without
revealing personal data. However, current research indicates that traditional
federated learning platforms are unable to ensure privacy due to privacy leaks
caused by the interchange of gradients. To achieve privacy-preserving federated
learning, integrating secure aggregation mechanisms is essential.
Unfortunately, existing solutions are vulnerable to recently demonstrated
inference attacks such as the disaggregation attack. This paper proposes
TAPFed, an approach for achieving privacy-preserving federated learning in the
context of multiple decentralized aggregators with malicious actors. TAPFed
uses a proposed threshold functional encryption scheme and allows for a certain
number of malicious aggregators while maintaining security and privacy. We
provide formal security and privacy analyses of TAPFed and compare it to
various baselines through experimental evaluation. Our results show that TAPFed
offers equivalent performance in terms of model quality compared to
state-of-the-art approaches while reducing transmission overhead by 29%-45%
across different model training scenarios. Most importantly, TAPFed can defend
against recently demonstrated inference attacks caused by curious aggregators,
which the majority of existing approaches are susceptible to.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04940v1">A New Perspective on Privacy Protection in Federated Learning with
  Granular-Ball Computing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-09T03:14:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guannan Lai, Yihui Feng, Xin Yang, Xiaoyu Deng, Hao Yu, Shuyin Xia, Guoyin Wang, Tianrui Li</p>
    <p><b>Summary:</b> Federated Learning (FL) facilitates collaborative model training while
prioritizing privacy by avoiding direct data sharing. However, most existing
articles attempt to address challenges within the model's internal parameters
and corresponding outputs, while neglecting to solve them at the input level.
To address this gap, we propose a novel framework called Granular-Ball
Federated Learning (GrBFL) for image classification. GrBFL diverges from
traditional methods that rely on the finest-grained input data. Instead, it
segments images into multiple regions with optimal coarse granularity, which
are then reconstructed into a graph structure. We designed a two-dimensional
binary search segmentation algorithm based on variance constraints for GrBFL,
which effectively removes redundant information while preserving key
representative features. Extensive theoretical analysis and experiments
demonstrate that GrBFL not only safeguards privacy and enhances efficiency but
also maintains robust utility, consistently outperforming other
state-of-the-art FL methods. The code is available at
https://github.com/AIGNLAI/GrBFL.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04420v1">A Closer Look on Gender Stereotypes in Movie Recommender Systems and
  Their Implications with Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-01-08T11:08:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Falguni Roy, Yiduo Shen, Na Zhao, Xiaofeng Ding, Md. Omar Faruk</p>
    <p><b>Summary:</b> The movie recommender system typically leverages user feedback to provide
personalized recommendations that align with user preferences and increase
business revenue. This study investigates the impact of gender stereotypes on
such systems through a specific attack scenario. In this scenario, an attacker
determines users' gender, a private attribute, by exploiting gender stereotypes
about movie preferences and analyzing users' feedback data, which is either
publicly available or observed within the system. The study consists of two
phases. In the first phase, a user study involving 630 participants identified
gender stereotypes associated with movie genres, which often influence viewing
choices. In the second phase, four inference algorithms were applied to detect
gender stereotypes by combining the findings from the first phase with users'
feedback data. Results showed that these algorithms performed more effectively
than relying solely on feedback data for gender inference. Additionally, we
quantified the extent of gender stereotypes to evaluate their broader impact on
digital computational science. The latter part of the study utilized two major
movie recommender datasets: MovieLens 1M and Yahoo!Movie. Detailed experimental
information is available on our GitHub repository:
https://github.com/fr-iit/GSMRS</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04409v1">Lossless Privacy-Preserving Aggregation for Decentralized Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-08T10:49:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaoye Miao, Bin Li, Yangyang Wu, Meng Xi, Xinkui Zhao, Jianwei Yin</p>
    <p><b>Summary:</b> Privacy concerns arise as sensitive data proliferate. Despite decentralized
federated learning (DFL) aggregating gradients from neighbors to avoid direct
data transmission, it still poses indirect data leaks from the transmitted
gradients. Existing privacy-preserving methods for DFL add noise to gradients.
They either diminish the model predictive accuracy or suffer from ineffective
gradient protection. In this paper, we propose a novel lossless
privacy-preserving aggregation rule named LPPA to enhance gradient protection
as much as possible but without loss of DFL model predictive accuracy. LPPA
subtly injects the noise difference between the sent and received noise into
transmitted gradients for gradient protection. The noise difference
incorporates neighbors' randomness for each client, effectively safeguarding
against data leaks. LPPA employs the noise flow conservation theory to ensure
that the noise impact can be globally eliminated. The global sum of all noise
differences remains zero, ensuring that accurate gradient aggregation is
unaffected and the model accuracy remains intact. We theoretically prove that
the privacy-preserving capacity of LPPA is \sqrt{2} times greater than that of
noise addition, while maintaining comparable model accuracy to the standard DFL
aggregation without noise injection. Experimental results verify the
theoretical findings and show that LPPA achieves a 13% mean improvement in
accuracy over noise addition. We also demonstrate the effectiveness of LPPA in
protecting raw data and guaranteeing lossless model accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04323v3">Navigating the Designs of Privacy-Preserving Fine-tuning for Large
  Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-08T07:47:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haonan Shi, Tu Ouyang, An Wang</p>
    <p><b>Summary:</b> Instruction tuning has proven effective in enhancing Large Language Models'
(LLMs) performance on downstream tasks. However, real-world fine-tuning faces
inherent conflicts between model providers' intellectual property protection,
clients' data privacy requirements, and tuning costs. While recent approaches
like split learning and offsite tuning demonstrate promising architectures for
privacy-preserving fine-tuning, there is a gap in systematically addressing the
multidimensional trade-offs required for diverse real-world deployments. We
propose several indicative evaluation metrics to guide design trade-offs for
privacy-preserving fine-tuning and a series of example designs, collectively
named GuardedTuning; they result from novel combinations of system
architectures with adapted privacy-enhancement methods and emerging computation
techniques. Each design represents distinct trade-offs across model utility,
privacy guarantees, and costs. Experimental results demonstrate that these
designs protect against data reconstruction attacks while maintaining
competitive fine-tuning performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04222v1">Privacy-Preserving Distributed Online Mirror Descent for Nonconvex
  Optimization</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-01-08T01:39:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yingjie Zhou, Tao Li</p>
    <p><b>Summary:</b> We investigate the distributed online nonconvex optimization problem with
differential privacy over time-varying networks. Each node minimizes the sum of
several nonconvex functions while preserving the node's differential privacy.
We propose a privacy-preserving distributed online mirror descent algorithm for
nonconvex optimization, which uses the mirror descent to update decision
variables and the Laplace differential privacy mechanism to protect privacy.
Unlike the existing works, the proposed algorithm allows the cost functions to
be nonconvex, which is more applicable. Based upon these, we prove that if the
communication network is $B$-strongly connected and the constraint set is
compact, then by choosing the step size properly, the algorithm guarantees
$\epsilon$-differential privacy at each time. Furthermore, we prove that if the
local cost functions are $\beta$-smooth, then the regret over time horizon $T$
grows sublinearly while preserving differential privacy, with an upper bound
$O(\sqrt{T})$. Finally, the effectiveness of the algorithm is demonstrated
through numerical simulations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04134v1">Mixing Times and Privacy Analysis for the Projected Langevin Algorithm
  under a Modulus of Continuity</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-07T20:46:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mario Bravo, Juan P. Flores-Mella, CristÃ³bal GuzmÃ¡n</p>
    <p><b>Summary:</b> We study the mixing time of the projected Langevin algorithm (LA) and the
privacy curve of noisy Stochastic Gradient Descent (SGD), beyond nonexpansive
iterations. Specifically, we derive new mixing time bounds for the projected LA
which are, in some important cases, dimension-free and poly-logarithmic on the
accuracy, closely matching the existing results in the smooth convex case.
Additionally, we establish new upper bounds for the privacy curve of the
subsampled noisy SGD algorithm. These bounds show a crucial dependency on the
regularity of gradients, and are useful for a wide range of convex losses
beyond the smooth case. Our analysis relies on a suitable extension of the
Privacy Amplification by Iteration (PABI) framework (Feldman et al., 2018;
Altschuler and Talwar, 2022, 2023) to noisy iterations whose gradient map is
not necessarily nonexpansive. This extension is achieved by designing an
optimization problem which accounts for the best possible R\'enyi divergence
bound obtained by an application of PABI, where the tractability of the problem
is crucially related to the modulus of continuity of the associated gradient
mapping. We show that, in several interesting cases -- including the nonsmooth
convex, weakly smooth and (strongly) dissipative -- such optimization problem
can be solved exactly and explicitly. This yields the tightest possible
PABI-based bounds, where our results are either new or substantially sharper
than those in previous works.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.03941v1">Synthetic Data Privacy Metrics</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-01-07T17:02:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amy Steier, Lipika Ramaswamy, Andre Manoel, Alexa Haushalter</p>
    <p><b>Summary:</b> Recent advancements in generative AI have made it possible to create
synthetic datasets that can be as accurate as real-world data for training AI
models, powering statistical insights, and fostering collaboration with
sensitive datasets while offering strong privacy guarantees. Effectively
measuring the empirical privacy of synthetic data is an important step in the
process. However, while there is a multitude of new privacy metrics being
published every day, there currently is no standardization. In this paper, we
review the pros and cons of popular metrics that include simulations of
adversarial attacks. We also review current best practices for amending
generative models to enhance the privacy of the data they create (e.g.
differential privacy).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04058v1">Homomorphic Encryption in Healthcare Industry Applications for
  Protecting Data Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-07T07:42:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> J. S. Rauthan</p>
    <p><b>Summary:</b> Focussing on two different use cases-Quality Control methods in industrial
contexts and Neural Network algorithms for healthcare diagnostics-this research
investigates the inclusion of Fully Homomorphic Encryption into real-world
applications in the healthcare sector. We evaluate the performance, resource
requirements, and viability of deploying FHE in these settings through
extensive testing and analysis, highlighting the progress made in FHE tooling
and the obstacles still facing addressing the gap between conceptual research
and practical applications. We start our research by describing the specific
case study and trust model were working with. Choosing the two FHE frameworks
most appropriate for industry development, we assess the resources and
performance requirements for implementing each of the two FHE frameworks in the
first scenario, Quality Control algorithms. In conclusion, our findings
demonstrate the effectiveness and resource consumption of the two use
cases-complex NN models and simple QC algorithms-when implemented in an FHE
setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.03451v1">Structure-Preference Enabled Graph Embedding Generation under
  Differential Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-01-07T00:43:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sen Zhang, Qingqing Ye, Haibo Hu</p>
    <p><b>Summary:</b> Graph embedding generation techniques aim to learn low-dimensional vectors
for each node in a graph and have recently gained increasing research
attention. Publishing low-dimensional node vectors enables various graph
analysis tasks, such as structural equivalence and link prediction. Yet,
improper publication opens a backdoor to malicious attackers, who can infer
sensitive information of individuals from the low-dimensional node vectors.
Existing methods tackle this issue by developing deep graph learning models
with differential privacy (DP). However, they often suffer from large noise
injections and cannot provide structural preferences consistent with mining
objectives. Recently, skip-gram based graph embedding generation techniques are
widely used due to their ability to extract customizable structures. Based on
skip-gram, we present SE-PrivGEmb, a structure-preference enabled graph
embedding generation under DP. For arbitrary structure preferences, we design a
unified noise tolerance mechanism via perturbing non-zero vectors. This
mechanism mitigates utility degradation caused by high sensitivity. By
carefully designing negative sampling probabilities in skip-gram, we
theoretically demonstrate that skip-gram can preserve arbitrary proximities,
which quantify structural features in graphs. Extensive experiments show that
our method outperforms existing state-of-the-art methods under structural
equivalence and link prediction tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.03391v1">Privacy-Preserving Smart Contracts for Permissioned Blockchains: A
  zk-SNARK-Based Recipe Part-1</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-06T21:16:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Aldenio Burgos, Eduardo Alchieri</p>
    <p><b>Summary:</b> The Bitcoin white paper introduced blockchain technology, enabling trustful
transactions without intermediaries. Smart contracts emerged with Ethereum and
blockchains expanded beyond cryptocurrency, applying to auctions, crowdfunding
and electronic voting. However, blockchain's transparency raised privacy
concerns and initial anonymity measures proved ineffective. Smart contract
privacy solutions employed zero-knowledge proofs, homomorphic encryption and
trusted execution environments. These approaches have practical drawbacks, such
as limited functionality, high computation times and trust on third parties
requirements, being not fully decentralized. This work proposes a solution
utilizing zk-SNARKs to provide privacy in smart contracts and blockchains. The
solution supports both fungible and nonfungible tokens. Additionally, the
proposal includes a new type of transactions, called delegated transactions,
which enable use cases like Delivery vs Payment (DvP).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.03222v1">Characterizing the Accuracy-Communication-Privacy Trade-off in
  Distributed Stochastic Convex Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">  
  <p><b>Published on:</b> 2025-01-06T18:57:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sudeep Salgia, Nikola Pavlovic, Yuejie Chi, Qing Zhao</p>
    <p><b>Summary:</b> We consider the problem of differentially private stochastic convex
optimization (DP-SCO) in a distributed setting with $M$ clients, where each of
them has a local dataset of $N$ i.i.d. data samples from an underlying data
distribution. The objective is to design an algorithm to minimize a convex
population loss using a collaborative effort across $M$ clients, while ensuring
the privacy of the local datasets. In this work, we investigate the
accuracy-communication-privacy trade-off for this problem. We establish
matching converse and achievability results using a novel lower bound and a new
algorithm for distributed DP-SCO based on Vaidya's plane cutting method. Thus,
our results provide a complete characterization of the
accuracy-communication-privacy trade-off for DP-SCO in the distributed setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02893v2">A Volumetric Approach to Privacy of Dynamical Systems</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-01-06T10:15:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chuanghong Weng, Ehsan Nekouei</p>
    <p><b>Summary:</b> Information-theoretic metrics, such as mutual information, have been widely
used to evaluate privacy leakage in dynamic systems. However, these approaches
are typically limited to stochastic systems and face computational challenges.
In this paper, we introduce a novel volumetric framework for analyzing privacy
in systems affected by unknown but bounded noise. Our model considers a dynamic
system comprising public and private states, where an observation set of the
public state is released. An adversary utilizes the observed public state to
infer an uncertainty set of the private state, referred to as the inference
attack. We define the evolution dynamics of these inference attacks and
quantify the privacy level of the private state using the volume of its
uncertainty sets. For linear scalar systems, we derive an explicit formulation
of the uncertainty set. For multi-dimensional linear systems, we develop an
approximate computation method leveraging interval analysis. We investigate the
properties of the proposed volumetric privacy measure and demonstrate that it
is bounded by the information gain derived from the observation set.
Furthermore, we propose an optimization approach to designing privacy filter
using randomization and linear programming based on the proposed privacy
measure. The effectiveness of the optimal privacy filter design is evaluated
through a production-inventory case study, illustrating its robustness against
the inference attack.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02804v2">Latency and Privacy-Aware Resource Allocation in Vehicular Edge
  Computing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Performance-F9C80E">
  <p><b>Published on:</b> 2025-01-06T06:44:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hossein Ahmadvand, Fouzhan Foroutan</p>
    <p><b>Summary:</b> The rapid increase in the number of connected vehicles has led to the
generation of vast amounts of data. As a significant portion of this data
pertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it
is predominantly generated at the edge. Considering the enormous volume of
data, real-time applications, and privacy concerns, it is crucial to process
the data at the edge. Neglecting the management of processing resources in
vehicular edge computing (VEC) could lead to numerous challenges as a
substantial number of vehicles with diverse safety, economic, and entertainment
applications, along with their data processing, emerge in the near future [1].
Previous research in VEC resource allocation has primarily focused on issues
such as response time and privacy preservation techniques. However, an approach
that takes into account privacy-aware resource allocation based on vehicular
network architecture and application requirements has not yet been proposed. In
this paper, we present a privacy and latency-aware approach for allocating
processing resources at the edge of the vehicular network, considering the
specific requirements of different applications. Our approach involves
categorizing vehicular network applications based on their processing accuracy,
real-time processing needs, and privacy preservation requirements. We further
divide the vehicular network edge into two parts: the user layer (OBUs) is
considered for processing applications with privacy requirements, while the
allocation of resources in the RSUs and cloud layer is based on the specific
needs of different applications. In this study, we evaluate the quality of
service based on parameters such as privacy preservation, processing cost,
meeting deadlines, and result quality. Comparative analyses demonstrate that
our approach enhances service quality by 55% compared to existing
state-of-the-art methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02354v1">PrivDPR: Synthetic Graph Publishing with Deep PageRank under
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-04T18:19:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sen Zhang, Haibo Hu, Qingqing Ye, Jianliang Xu</p>
    <p><b>Summary:</b> The objective of privacy-preserving synthetic graph publishing is to
safeguard individuals' privacy while retaining the utility of original data.
Most existing methods focus on graph neural networks under differential privacy
(DP), and yet two fundamental problems in generating synthetic graphs remain
open. First, the current research often encounters high sensitivity due to the
intricate relationships between nodes in a graph. Second, DP is usually
achieved through advanced composition mechanisms that tend to converge
prematurely when working with a small privacy budget. In this paper, inspired
by the simplicity, effectiveness, and ease of analysis of PageRank, we design
PrivDPR, a novel privacy-preserving deep PageRank for graph synthesis. In
particular, we achieve DP by adding noise to the gradient for a specific weight
during learning. Utilizing weight normalization as a bridge, we theoretically
reveal that increasing the number of layers in PrivDPR can effectively mitigate
the high sensitivity and privacy budget splitting. Through formal privacy
analysis, we prove that the synthetic graph generated by PrivDPR satisfies
node-level DP. Experiments on real-world graph datasets show that PrivDPR
preserves high data utility across multiple graph structural properties.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02091v1">PriveShield: Enhancing User Privacy Using Automatic Isolated Profiles in
  Browsers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-03T20:29:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Seyed Ali Akhavani, Engin Kirda, Amin Kharraz</p>
    <p><b>Summary:</b> Online tracking is a widespread practice on the web with questionable ethics,
security, and privacy concerns. While web tracking can offer personalized and
curated content to Internet users, it operates as a sophisticated surveillance
mechanism to gather extensive user information. This paper introduces
PriveShield, a light-weight privacy mechanism that disrupts the information
gathering cycle while offering more control to Internet users to maintain their
privacy. PriveShield is implemented as a browser extension that offers an
adjustable privacy feature to surf the web with multiple identities or accounts
simultaneously without any changes to underlying browser code or services. When
necessary, multiple factors are automatically analyzed on the client side to
isolate cookies and other information that are the basis of online tracking.
PriveShield creates isolated profiles for clients based on their browsing
history, interactions with websites, and the amount of time they spend on
specific websites. This allows the users to easily prevent unwanted browsing
information from being shared with third parties and ad exchanges without the
need for manual configuration. Our evaluation results from 54 real-world
scenarios show that our extension is effective in preventing retargeted ads in
91% of those scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01786v1">Advancing privacy in learning analytics using differential privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-03T12:36:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qinyi Liu, Ronas Shakya, Mohammad Khalil, Jelena Jovanovic</p>
    <p><b>Summary:</b> This paper addresses the challenge of balancing learner data privacy with the
use of data in learning analytics (LA) by proposing a novel framework by
applying Differential Privacy (DP). The need for more robust privacy protection
keeps increasing, driven by evolving legal regulations and heightened privacy
concerns, as well as traditional anonymization methods being insufficient for
the complexities of educational data. To address this, we introduce the first
DP framework specifically designed for LA and provide practical guidance for
its implementation. We demonstrate the use of this framework through a LA usage
scenario and validate DP in safeguarding data privacy against potential attacks
through an experiment on a well-known LA dataset. Additionally, we explore the
trade-offs between data privacy and utility across various DP settings. Our
work contributes to the field of LA by offering a practical DP framework that
can support researchers and practitioners in adopting DP in their works.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01639v2">Implications of Artificial Intelligence on Health Data Privacy and
  Confidentiality</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-01-03T05:17:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmad Momani</p>
    <p><b>Summary:</b> The rapid integration of artificial intelligence (AI) in healthcare is
revolutionizing medical diagnostics, personalized medicine, and operational
efficiency. However, alongside these advancements, significant challenges arise
concerning patient data privacy, ethical considerations, and regulatory
compliance. This paper examines the dual impact of AI on healthcare,
highlighting its transformative potential and the critical need for
safeguarding sensitive health information. It explores the role of the Health
Insurance Portability and Accountability Act (HIPAA) as a regulatory framework
for ensuring data privacy and security, emphasizing the importance of robust
safeguards and ethical standards in AI-driven healthcare. Through case studies,
including AI applications in diabetic retinopathy, oncology, and the
controversies surrounding data sharing, this study underscores the ethical and
legal complexities of AI implementation. A balanced approach that fosters
innovation while maintaining patient trust and privacy is imperative. The
findings emphasize the importance of continuous education, transparency, and
adherence to regulatory frameworks to harness AI's full potential responsibly
and ethically in healthcare.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01353v1">Privacy Preservation in MIMO-OFDM Localization Systems: A Beamforming
  Approach</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-02T17:08:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuchen Zhang, Hui Chen, Musa Furkan Keskin, Alireza Pourafzal, Pinjun Zheng, Henk Wymeersch, Tareq Y. Al-Naffouri</p>
    <p><b>Summary:</b> We investigate an uplink MIMO-OFDM localization scenario where a legitimate
base station (BS) aims to localize a user equipment (UE) using pilot signals
transmitted by the UE, while an unauthorized BS attempts to localize the UE by
eavesdropping on these pilots, posing a risk to the UE's location privacy. To
enhance legitimate localization performance while protecting the UE's privacy,
we formulate an optimization problem regarding the beamformers at the UE,
aiming to minimize the Cram\'er-Rao bound (CRB) for legitimate localization
while constraining the CRB for unauthorized localization above a threshold. A
penalty dual decomposition optimization framework is employed to solve the
problem, leading to a novel beamforming approach for location privacy
preservation. Numerical results confirm the effectiveness of the proposed
approach and demonstrate its superiority over existing benchmarks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01131v1">Privacy Bills of Materials: A Transparent Privacy Information Inventory
  for Collaborative Privacy Notice Generation in Mobile App Development</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-01-02T08:14:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhen Tao, Shidong Pan, Zhenchang Xing, Xiaoyu Sun, Omar Haggag, John Grundy, Ze Shi Li, Jingjie Li, Liming Zhu</p>
    <p><b>Summary:</b> Privacy regulations mandate that developers must provide authentic and
comprehensive privacy notices, e.g., privacy policies or labels, to inform
users of their apps' privacy practices. However, due to a lack of knowledge of
privacy requirements, developers often struggle to create accurate privacy
notices, especially for sophisticated mobile apps with complex features and in
crowded development teams. To address these challenges, we introduce Privacy
Bills of Materials (PriBOM), a systematic software engineering approach that
leverages different development team roles to better capture and coordinate
mobile app privacy information. PriBOM facilitates transparency-centric privacy
documentation and specific privacy notice creation, enabling traceability and
trackability of privacy practices. We present a pre-fill of PriBOM based on
static analysis and privacy notice analysis techniques. We demonstrate the
perceived usefulness of PriBOM through a human evaluation with 150 diverse
participants. Our findings suggest that PriBOM could serve as a significant
solution for providing privacy support in DevOps for mobile apps.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01063v1">FAPL-DM-BC: A Secure and Scalable FL Framework with Adaptive Privacy and
  Dynamic Masking, Blockchain, and XAI for the IoVs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-02T05:21:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sathwik Narkedimilli, Amballa Venkata Sriram, Sujith Makam, MSVPJ Sathvik, Sai Prashanth Mallellu</p>
    <p><b>Summary:</b> The FAPL-DM-BC solution is a new FL-based privacy, security, and scalability
solution for the Internet of Vehicles (IoV). It leverages Federated Adaptive
Privacy-Aware Learning (FAPL) and Dynamic Masking (DM) to learn and adaptively
change privacy policies in response to changing data sensitivity and state in
real-time, for the optimal privacy-utility tradeoff. Secure Logging and
Verification, Blockchain-based provenance and decentralized validation, and
Cloud Microservices Secure Aggregation using FedAvg (Federated Averaging) and
Secure Multi-Party Computation (SMPC). Two-model feedback, driven by
Model-Agnostic Explainable AI (XAI), certifies local predictions and
explanations to drive it to the next level of efficiency. Combining local
feedback with world knowledge through a weighted mean computation, FAPL-DM-BC
assures federated learning that is secure, scalable, and interpretable.
Self-driving cars, traffic management, and forecasting, vehicular network
cybersecurity in real-time, and smart cities are a few possible applications of
this integrated, privacy-safe, and high-performance IoV platform.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.00824v2">Information Sifting Funnel: Privacy-preserving Collaborative Inference
  Against Model Inversion Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-01T13:00:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rongke Liu</p>
    <p><b>Summary:</b> The complexity of neural networks and inference tasks, coupled with demands
for computational efficiency and real-time feedback, poses significant
challenges for resource-constrained edge devices. Collaborative inference
mitigates this by assigning shallow feature extraction to edge devices and
offloading features to the cloud for further inference, reducing computational
load. However, transmitted features remain susceptible to model inversion
attacks (MIAs), which can reconstruct original input data. Current defenses,
such as perturbation and information bottleneck techniques, offer explainable
protection but face limitations, including the lack of standardized criteria
for assessing MIA difficulty, challenges in mutual information estimation, and
trade-offs among usability, privacy, and deployability.
  To address these challenges, we introduce the first criterion to evaluate MIA
difficulty in collaborative inference, supported by theoretical analysis of
existing attacks and defenses, validated using experiments with the Mutual
Information Neural Estimator (MINE). Based on these findings, we propose
SiftFunnel, a privacy-preserving framework for collaborative inference. The
edge model is trained with linear and non-linear correlation constraints to
reduce redundant information in transmitted features, enhancing privacy
protection. Label smoothing and a cloud-based upsampling module are added to
balance usability and privacy. To improve deployability, the edge model
incorporates a funnel-shaped structure and attention mechanisms, preserving
both privacy and usability. Extensive experiments demonstrate that SiftFunnel
outperforms state-of-the-art defenses against MIAs, achieving superior privacy
protection with less than 3% accuracy loss and striking an optimal balance
among usability, privacy, and practicality.</p>
  </details>
</div>



<h2>2024-12</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.00588v2">Privacy-Preserving Distributed Defense Framework for DC Microgrids
  Against Exponentially Unbounded False Data Injection Attacks</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-12-31T18:25:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yi Zhang, Mohamadamin Rajabinezhad, Yichao Wang, Junbo Zhao, Shan Zuo</p>
    <p><b>Summary:</b> This paper introduces a novel, fully distributed control framework for DC
microgrids, enhancing resilience against exponentially unbounded false data
injection (EU-FDI) attacks. Our framework features a consensus-based secondary
control for each converter, effectively addressing these advanced threats. To
further safeguard sensitive operational data, a privacy-preserving mechanism is
incorporated into the control design, ensuring that critical information
remains secure even under adversarial conditions. Rigorous Lyapunov stability
analysis confirms the framework's ability to maintain critical DC microgrid
operations like voltage regulation and load sharing under EU-FDI threats. The
framework's practicality is validated through hardware-in-the-loop experiments,
demonstrating its enhanced resilience and robust privacy protection against the
complex challenges posed by quick variant FDI attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.00363v1">SPDZCoder: Teaching LLMs to Synthesize Privacy Computing Code without
  Massive Training Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-12-31T09:29:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaoning Dong, Peilin Xin, Wei Xu</p>
    <p><b>Summary:</b> Privacy computing receives increasing attention but writing privacy computing
code remains challenging for developers due to limited library functions that
necessitate extensive function implementation from scratch as well as the
data-oblivious requirement which contradicts intuitive thinking and usual
practices of programmers. Large language models (LLMs) have demonstrated
surprising capabilities in coding tasks and achieved state-of-the-art
performance across many benchmarks. However, even with extensive prompting,
existing LLMs struggle with code translation task for privacy computing, such
as translating Python to MP-SPDZ, due to the scarcity of MP-SPDZ data required
for effective pre-training or fine-tuning. To address the limitation, this
paper proposes SPDZCoder, a rule-based framework to teach LLMs to synthesize
privacy computing code without asking experts to write tons of code and by
leveraging the instruction-following and in-context learning ability of LLMs.
Specifically, SPDZCoder decouples the translation task into the refactoring
stage and the generation stage, which can mitigate the semantic-expressing
differences at different levels. In addition, SPDZCoder can further improve its
performance by a feedback stage. SPDZCoder does not require fine-tuning since
it adopts an in-context learning paradigm of LLMs. To evaluate SPDZCoder, we
manually created a benchmark dataset, named SPDZEval, containing six classes of
difficult tasks to implement in MP-SPDZ. We conduct experiments on SPDZEval and
the experimental results shows that SPDZCoder achieves the state-of-the-art
performance in pass@1 and pass@2 across six data splits. Specifically,
SPDZCoder achieves an overall correctness of 85.94% and 92.01% in pass@1 and
pass@2, respectively, significantly surpassing baselines (at most 30.35% and
49.84% in pass@1 and pass@2, respectively) by a large margin.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.21069v1">Privacy-Aware Multi-Device Cooperative Edge Inference with Distributed
  Resource Bidding</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-12-30T16:37:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenhao Zhuang, Yuyi Mao</p>
    <p><b>Summary:</b> Mobile edge computing (MEC) has empowered mobile devices (MDs) in supporting
artificial intelligence (AI) applications through collaborative efforts with
proximal MEC servers. Unfortunately, despite the great promise of device-edge
cooperative AI inference, data privacy becomes an increasing concern. In this
paper, we develop a privacy-aware multi-device cooperative edge inference
system for classification tasks, which integrates a distributed bidding
mechanism for the MEC server's computational resources. Intermediate feature
compression is adopted as a principled approach to minimize data privacy
leakage. To determine the bidding values and feature compression ratios in a
distributed fashion, we formulate a decentralized partially observable Markov
decision process (DEC-POMDP) model, for which, a multi-agent deep deterministic
policy gradient (MADDPG)-based algorithm is developed. Simulation results
demonstrate the effectiveness of the proposed algorithm in privacy-preserving
cooperative edge inference. Specifically, given a sufficient level of data
privacy protection, the proposed algorithm achieves 0.31-0.95% improvements in
classification accuracy compared to the approach being agnostic to the wireless
channel conditions. The performance is further enhanced by 1.54-1.67% by
considering the difficulties of inference data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20798v2">A Tale of Two Imperatives: Privacy and Explainability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-12-30T08:43:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Supriya Manna, Niladri Sett</p>
    <p><b>Summary:</b> Deep learning's preponderance across scientific domains has reshaped
high-stakes decision-making, making it essential to follow rigorous operational
frameworks that include both Right-to-Privacy (RTP) and Right-to-Explanation
(RTE). This paper examines the complexities of combining these two
requirements. For RTP, we focus on `Differential privacy' (DP), which is
considered the current \textit{gold standard} for privacy-preserving machine
learning due to its strong quantitative guarantee of privacy. For RTE, we focus
on post-hoc explainers: they are the \textit{go-to} option for model auditing
as they operate independently of model training. We formally investigate DP
models and various commonly-used post-hoc explainers: how to evaluate these
explainers subject to RTP, and analyze the intrinsic interactions between DP
models and these explainers. Furthermore, our work throws light on how RTP and
RTE can be effectively combined in high-stakes applications. Our study
concludes by outlining an industrial software pipeline, with the example of a
wildly used use-case, that respects both RTP and RTE requirements.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20762v1">Enhancing Privacy in Federated Learning through Quantum Teleportation
  Integration</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-12-30T07:15:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Koffka Khan</p>
    <p><b>Summary:</b> Federated learning enables collaborative model training across multiple
clients without sharing raw data, thereby enhancing privacy. However, the
exchange of model updates can still expose sensitive information. Quantum
teleportation, a process that transfers quantum states between distant
locations without physical transmission of the particles themselves, has
recently been implemented in real-world networks. This position paper explores
the potential of integrating quantum teleportation into federated learning
frameworks to bolster privacy. By leveraging quantum entanglement and the
no-cloning theorem, quantum teleportation ensures that data remains secure
during transmission, as any eavesdropping attempt would be detectable. We
propose a novel architecture where quantum teleportation facilitates the secure
exchange of model parameters and gradients among clients and servers. This
integration aims to mitigate risks associated with data leakage and adversarial
attacks inherent in classical federated learning setups. We also discuss the
practical challenges of implementing such a system, including the current
limitations of quantum network infrastructure and the need for hybrid
quantum-classical protocols. Our analysis suggests that, despite these
challenges, the convergence of quantum communication technologies and federated
learning presents a promising avenue for achieving unprecedented levels of
privacy in distributed machine learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20733v1">Towards nation-wide analytical healthcare infrastructures: A
  privacy-preserving augmented knee rehabilitation case study</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2024-12-30T06:14:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Boris BaÄiÄ, Claudiu Vasile, Chengwei Feng, Marian G. CiucÄ</p>
    <p><b>Summary:</b> The purpose of this paper is to contribute towards the near-future
privacy-preserving big data analytical healthcare platforms, capable of
processing streamed or uploaded timeseries data or videos from patients. The
experimental work includes a real-life knee rehabilitation video dataset
capturing a set of exercises from simple and personalised to more general and
challenging movements aimed for returning to sport. To convert video from
mobile into privacy-preserving diagnostic timeseries data, we employed Google
MediaPipe pose estimation. The developed proof-of-concept algorithms can
augment knee exercise videos by overlaying the patient with stick figure
elements while updating generated timeseries plot with knee angle estimation
streamed as CSV file format. For patients and physiotherapists, video with
side-to-side timeseries visually indicating potential issues such as excessive
knee flexion or unstable knee movements or stick figure overlay errors is
possible by setting a-priori knee-angle parameters. To address adherence to
rehabilitation programme and quantify exercise sets and repetitions, our
adaptive algorithm can correctly identify (91.67%-100%) of all exercises from
side- and front-view videos. Transparent algorithm design for adaptive visual
analysis of various knee exercise patterns contributes towards the
interpretable AI and will inform near-future privacy-preserving, non-vendor
locking, open-source developments for both end-user computing devices and as
on-premises non-proprietary cloud platforms that can be deployed within the
national healthcare system.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20641v1">SafeSynthDP: Leveraging Large Language Models for Privacy-Preserving
  Synthetic Data Generation Using Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-30T01:10:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md Mahadi Hasan Nahid, Sadid Bin Hasan</p>
    <p><b>Summary:</b> Machine learning (ML) models frequently rely on training data that may
include sensitive or personal information, raising substantial privacy
concerns. Legislative frameworks such as the General Data Protection Regulation
(GDPR) and the California Consumer Privacy Act (CCPA) have necessitated the
development of strategies that preserve privacy while maintaining the utility
of data. In this paper, we investigate the capability of Large Language Models
(LLMs) to generate synthetic datasets integrated with Differential Privacy (DP)
mechanisms, thereby enabling data-driven research and model training without
direct exposure of sensitive information. Our approach incorporates DP-based
noise injection methods, including Laplace and Gaussian distributions, into the
data generation process. We then evaluate the utility of these DP-enhanced
synthetic datasets by comparing the performance of ML models trained on them
against models trained on the original data. To substantiate privacy
guarantees, we assess the resilience of the generated synthetic data to
membership inference attacks and related threats. The experimental results
demonstrate that integrating DP within LLM-driven synthetic data generation
offers a viable balance between privacy protection and data utility. This study
provides a foundational methodology and insight into the privacy-preserving
capabilities of LLMs, paving the way for compliant and effective ML research
and applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20603v1">Privacy-Preserving Identity and Access Management in Multiple Cloud
  Environments: Models, Issues, and Solutions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-29T22:15:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alfredo Cuzzocrea, Islam Belmerabet</p>
    <p><b>Summary:</b> This paper focuses the attention on privacy-preserving identity and access
management in multiple Cloud environments, which is an annoying problem in the
modern big data era. Within this conceptual context, the paper describes
contemporaneous models and issues, and put the basis for future solid
solutions. Finally, we provide a summary table where we embed an innovative
taxonomy of state-of-the-art research proposals in the reference scientific
field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.19496v1">Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-27T07:33:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen</p>
    <p><b>Summary:</b> Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.19291v1">RAG with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-26T17:34:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicolas Grislain</p>
    <p><b>Summary:</b> Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide *Large Language Models* (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows *differentially private token
generation* is a viable approach to private RAG.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.18992v2">Optimal Federated Learning for Functional Mean Estimation under
  Heterogeneous Privacy Constraints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-12-25T22:06:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tony Cai, Abhinav Chakraborty, Lasse Vuursteen</p>
    <p><b>Summary:</b> Federated learning (FL) is a distributed machine learning technique designed
to preserve data privacy and security, and it has gained significant importance
due to its broad range of applications. This paper addresses the problem of
optimal functional mean estimation from discretely sampled data in a federated
setting.
  We consider a heterogeneous framework where the number of individuals,
measurements per individual, and privacy parameters vary across one or more
servers, under both common and independent design settings. In the common
design setting, the same design points are measured for each individual,
whereas in the independent design, each individual has their own random
collection of design points. Within this framework, we establish minimax upper
and lower bounds for the estimation error of the underlying mean function,
highlighting the nuanced differences between common and independent designs
under distributed privacy constraints.
  We propose algorithms that achieve the optimal trade-off between privacy and
accuracy and provide optimality results that quantify the fundamental limits of
private functional mean estimation across diverse distributed settings. These
results characterize the cost of privacy and offer practical insights into the
potential for privacy-preserving statistical analysis in federated
environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.18716v1">Design and Evaluation of Privacy-Preserving Protocols for
  Agent-Facilitated Mobile Money Services in Kenya</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-25T00:27:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karen Sowon, Collins W. Munyendo, Lily Klucinec, Eunice Maingi, Gerald Suleh, Lorrie Faith Cranor, Giulia Fanti, Conrad Tucker, Assane Gueye</p>
    <p><b>Summary:</b> Mobile Money (MoMo), a technology that allows users to complete digital
financial transactions using a mobile phone without requiring a bank account,
has become a common method for processing financial transactions in Africa and
other developing regions. Operationally, users can deposit (exchange cash for
mobile money tokens) and withdraw with the help of human agents who facilitate
a near end-to-end process from customer onboarding to authentication and
recourse. During deposit and withdraw operations, know-your-customer (KYC)
processes require agents to access and verify customer information such as name
and ID number, which can introduce privacy and security risks. In this work, we
design alternative protocols for mobile money deposits and withdrawals that
protect users' privacy while enabling KYC checks. These workflows redirect the
flow of sensitive information from the agent to the MoMo provider, thus
allowing the agent to facilitate transactions without accessing a customer's
personal information. We evaluate the usability and efficiency of our proposed
protocols in a role play and semi-structured interview study with 32 users and
15 agents in Kenya. We find that users and agents both generally appear to
prefer the new protocols, due in part to convenient and efficient verification
using biometrics, better data privacy and access control, as well as better
security mechanisms for delegated transactions. Our results also highlight some
challenges and limitations that suggest the need for more work to build
deployable solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.19837v1">Data Poisoning Attacks to Local Differential Privacy Protocols for
  Graphs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-12-23T11:16:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xi He, Kai Huang, Qingqing Ye, Haibo Hu</p>
    <p><b>Summary:</b> Graph analysis has become increasingly popular with the prevalence of big
data and machine learning. Traditional graph data analysis methods often assume
the existence of a trusted third party to collect and store the graph data,
which does not align with real-world situations. To address this, some research
has proposed utilizing Local Differential Privacy (LDP) to collect graph data
or graph metrics (e.g., clustering coefficient). This line of research focuses
on collecting two atomic graph metrics (the adjacency bit vectors and node
degrees) from each node locally under LDP to synthesize an entire graph or
generate graph metrics. However, they have not considered the security issues
of LDP for graphs.
  In this paper, we bridge the gap by demonstrating that an attacker can inject
fake users into LDP protocols for graphs and design data poisoning attacks to
degrade the quality of graph metrics. In particular, we present three data
poisoning attacks to LDP protocols for graphs. As a proof of concept, we focus
on data poisoning attacks on two classical graph metrics: degree centrality and
clustering coefficient. We further design two countermeasures for these data
poisoning attacks. Experimental study on real-world datasets demonstrates that
our attacks can largely degrade the quality of collected graph metrics, and the
proposed countermeasures cannot effectively offset the effect, which calls for
the development of new defenses.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.17317v1">Better Knowledge Enhancement for Privacy-Preserving Cross-Project Defect
  Prediction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-12-23T06:21:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuying Wang, Yichen Li, Haozhao Wang, Lei Zhao, Xiaofang Zhang</p>
    <p><b>Summary:</b> Cross-Project Defect Prediction (CPDP) poses a non-trivial challenge to
construct a reliable defect predictor by leveraging data from other projects,
particularly when data owners are concerned about data privacy. In recent
years, Federated Learning (FL) has become an emerging paradigm to guarantee
privacy information by collaborative training a global model among multiple
parties without sharing raw data. While the direct application of FL to the
CPDP task offers a promising solution to address privacy concerns, the data
heterogeneity arising from proprietary projects across different companies or
organizations will bring troubles for model training. In this paper, we study
the privacy-preserving cross-project defect prediction with data heterogeneity
under the federated learning framework. To address this problem, we propose a
novel knowledge enhancement approach named FedDP with two simple but effective
solutions: 1. Local Heterogeneity Awareness and 2. Global Knowledge
Distillation. Specifically, we employ open-source project data as the
distillation dataset and optimize the global model with the heterogeneity-aware
local model ensemble via knowledge distillation. Experimental results on 19
projects from two datasets demonstrate that our method significantly
outperforms baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.17038v3">ErasableMask: A Robust and Erasable Privacy Protection Scheme against
  Black-box Face Recognition Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-22T14:30:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sipeng Shen, Yunming Zhang, Dengpan Ye, Xiuwen Shi, Long Tang, Haoran Duan, Jiacheng Deng, Ziyi Liu</p>
    <p><b>Summary:</b> While face recognition (FR) models have brought remarkable convenience in
face verification and identification, they also pose substantial privacy risks
to the public. Existing facial privacy protection schemes usually adopt
adversarial examples to disrupt face verification of FR models. However, these
schemes often suffer from weak transferability against black-box FR models and
permanently damage the identifiable information that cannot fulfill the
requirements of authorized operations such as forensics and authentication. To
address these limitations, we propose ErasableMask, a robust and erasable
privacy protection scheme against black-box FR models. Specifically, via
rethinking the inherent relationship between surrogate FR models, ErasableMask
introduces a novel meta-auxiliary attack, which boosts black-box
transferability by learning more general features in a stable and balancing
optimization strategy. It also offers a perturbation erasion mechanism that
supports the erasion of semantic perturbations in protected face without
degrading image quality. To further improve performance, ErasableMask employs a
curriculum learning strategy to mitigate optimization conflicts between
adversarial attack and perturbation erasion. Extensive experiments on the
CelebA-HQ and FFHQ datasets demonstrate that ErasableMask achieves the
state-of-the-art performance in transferability, achieving over 72% confidence
on average in commercial FR systems. Moreover, ErasableMask also exhibits
outstanding perturbation erasion performance, achieving over 90% erasion
success rate.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16916v1">On the Differential Privacy and Interactivity of Privacy Sandbox Reports</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-22T08:22:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Badih Ghazi, Charlie Harrison, Arpana Hosabettu, Pritish Kamath, Alexander Knop, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Vikas Sahu</p>
    <p><b>Summary:</b> The Privacy Sandbox initiative from Google includes APIs for enabling
privacy-preserving advertising functionalities as part of the effort around
limiting third-party cookies. In particular, the Private Aggregation API (PAA)
and the Attribution Reporting API (ARA) can be used for ad measurement while
providing different guardrails for safeguarding user privacy, including a
framework for satisfying differential privacy (DP). In this work, we provide a
formal model for analyzing the privacy of these APIs and show that they satisfy
a formal DP guarantee under certain assumptions. Our analysis handles the case
where both the queries and database can change interactively based on previous
responses from the API.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16893v1">Preventing Non-intrusive Load Monitoring Privacy Invasion: A Precise
  Adversarial Attack Scheme for Networked Smart Meters</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-22T07:06:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jialing He, Jiacheng Wang, Ning Wang, Shangwei Guo, Liehuang Zhu, Dusit Niyato, Tao Xiang</p>
    <p><b>Summary:</b> Smart grid, through networked smart meters employing the non-intrusive load
monitoring (NILM) technique, can considerably discern the usage patterns of
residential appliances. However, this technique also incurs privacy leakage. To
address this issue, we propose an innovative scheme based on adversarial attack
in this paper. The scheme effectively prevents NILM models from violating
appliance-level privacy, while also ensuring accurate billing calculation for
users. To achieve this objective, we overcome two primary challenges. First, as
NILM models fall under the category of time-series regression models, direct
application of traditional adversarial attacks designed for classification
tasks is not feasible. To tackle this issue, we formulate a novel adversarial
attack problem tailored specifically for NILM and providing a theoretical
foundation for utilizing the Jacobian of the NILM model to generate
imperceptible perturbations. Leveraging the Jacobian, our scheme can produce
perturbations, which effectively misleads the signal prediction of NILM models
to safeguard users' appliance-level privacy. The second challenge pertains to
fundamental utility requirements, where existing adversarial attack schemes
struggle to achieve accurate billing calculation for users. To handle this
problem, we introduce an additional constraint, mandating that the sum of added
perturbations within a billing period must be precisely zero. Experimental
validation on real-world power datasets REDD and UK-DALE demonstrates the
efficacy of our proposed solutions, which can significantly amplify the
discrepancy between the output of the targeted NILM model and the actual power
signal of appliances, and enable accurate billing at the same time.
Additionally, our solutions exhibit transferability, making the generated
perturbation signal from one target model applicable to other diverse NILM
models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16825v1">SoK: Usability Studies in Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-22T02:21:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Onyinye Dibia, Brad Stenger, Steven Baldasty, Mako Bates, Ivoline C. Ngong, Yuanyuan Feng, Joseph P. Near</p>
    <p><b>Summary:</b> Differential Privacy (DP) has emerged as a pivotal approach for safeguarding
individual privacy in data analysis, yet its practical adoption is often
hindered by challenges in usability in implementation and communication of the
privacy protection levels. This paper presents a comprehensive systematization
of existing research on the usability of and communication about DP,
synthesizing insights from studies on both the practical use of DP tools and
strategies for conveying DP parameters that determine the privacy protection
levels such as epsilon. By reviewing and analyzing these studies, we identify
core usability challenges, best practices, and critical gaps in current DP
tools that affect adoption across diverse user groups, including developers,
data analysts, and non-technical stakeholders. Our analysis highlights
actionable insights and pathways for future research that emphasizes
user-centered design and clear communication, fostering the development of more
accessible DP tools that meet practical needs and support broader adoption.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16669v1">Label Privacy in Split Learning for Large Models with
  Parameter-Efficient Training</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-21T15:32:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Philip Zmushko, Marat Mansurov, Ruslan Svirschevski, Denis Kuznedelev, Max Ryabinin, Aleksandr Beznosikov</p>
    <p><b>Summary:</b> As deep learning models become larger and more expensive, many practitioners
turn to fine-tuning APIs. These web services allow fine-tuning a model between
two parties: the client that provides the data, and the server that hosts the
model. While convenient, these APIs raise a new concern: the data of the client
is at risk of privacy breach during the training procedure. This challenge
presents an important practical case of vertical federated learning, where the
two parties perform parameter-efficient fine-tuning (PEFT) of a large model. In
this study, we systematically search for a way to fine-tune models over an API
while keeping the labels private. We analyze the privacy of LoRA, a popular
approach for parameter-efficient fine-tuning when training over an API. Using
this analysis, we propose P$^3$EFT, a multi-party split learning algorithm that
takes advantage of existing PEFT properties to maintain privacy at a lower
performance overhead. To validate our algorithm, we fine-tune
DeBERTa-v2-XXLarge, Flan-T5 Large and LLaMA-2 7B using LoRA adapters on a range
of NLP tasks. We find that P$^3$EFT is competitive with existing
privacy-preserving methods in multi-party and two-party setups while having
higher accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16667v1">The Good, the Bad, and the (Un)Usable: A Rapid Literature Review on
  Privacy as Code</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-21T15:30:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> NicolÃ¡s E. DÃ­az Ferreyra, Sirine Khelifi, Nalin Arachchilage, Riccardo Scandariato</p>
    <p><b>Summary:</b> Privacy and security are central to the design of information systems endowed
with sound data protection and cyber resilience capabilities. Still, developers
often struggle to incorporate these properties into software projects as they
either lack proper cybersecurity training or do not consider them a priority.
Prior work has tried to support privacy and security engineering activities
through threat modeling methods for scrutinizing flaws in system architectures.
Moreover, several techniques for the automatic identification of
vulnerabilities and the generation of secure code implementations have also
been proposed in the current literature. Conversely, such as-code approaches
seem under-investigated in the privacy domain, with little work elaborating on
(i) the automatic detection of privacy properties in source code or (ii) the
generation of privacy-friendly code. In this work, we seek to characterize the
current research landscape of Privacy as Code (PaC) methods and tools by
conducting a rapid literature review. Our results suggest that PaC research is
in its infancy, especially regarding the performance evaluation and usability
assessment of the existing approaches. Based on these findings, we outline and
discuss prospective research directions concerning empirical studies with
software practitioners, the curation of benchmark datasets, and the role of
generative AI technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16504v1">Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and
  Future Directions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-21T06:41:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Du, Shang Liu, Lele Zheng, Yang Cao, Atsuyoshi Nakamura, Lei Chen</p>
    <p><b>Summary:</b> Fine-tuning has emerged as a critical process in leveraging Large Language
Models (LLMs) for specific downstream tasks, enabling these models to achieve
state-of-the-art performance across various domains. However, the fine-tuning
process often involves sensitive datasets, introducing privacy risks that
exploit the unique characteristics of this stage. In this paper, we provide a
comprehensive survey of privacy challenges associated with fine-tuning LLMs,
highlighting vulnerabilities to various privacy attacks, including membership
inference, data extraction, and backdoor attacks. We further review defense
mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as
differential privacy, federated learning, and knowledge unlearning, discussing
their effectiveness and limitations in addressing privacy risks and maintaining
model utility. By identifying key gaps in existing research, we highlight
challenges and propose directions to advance the development of
privacy-preserving methods for fine-tuning LLMs, promoting their responsible
use in diverse applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16369v1">Navigating AI to Unpack Youth Privacy Concerns: An In-Depth Exploration
  and Systematic Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-20T22:00:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ajay Kumar Shrestha, Ankur Barthwal, Molly Campbell, Austin Shouli, Saad Syed, Sandhya Joshi, Julita Vassileva</p>
    <p><b>Summary:</b> This systematic literature review investigates perceptions, concerns, and
expectations of young digital citizens regarding privacy in artificial
intelligence (AI) systems, focusing on social media platforms, educational
technology, gaming systems, and recommendation algorithms. Using a rigorous
methodology, the review started with 2,000 papers, narrowed down to 552 after
initial screening, and finally refined to 108 for detailed analysis. Data
extraction focused on privacy concerns, data-sharing practices, the balance
between privacy and utility, trust factors in AI, transparency expectations,
and strategies to enhance user control over personal data. Findings reveal
significant privacy concerns among young users, including a perceived lack of
control over personal information, potential misuse of data by AI, and fears of
data breaches and unauthorized access. These issues are worsened by unclear
data collection practices and insufficient transparency in AI applications. The
intention to share data is closely associated with perceived benefits and data
protection assurances. The study also highlights the role of parental mediation
and the need for comprehensive education on data privacy. Balancing privacy and
utility in AI applications is crucial, as young digital citizens value
personalized services but remain wary of privacy risks. Trust in AI is
significantly influenced by transparency, reliability, predictable behavior,
and clear communication about data usage. Strategies to improve user control
over personal data include access to and correction of data, clear consent
mechanisms, and robust data protection assurances. The review identifies
research gaps and suggests future directions, such as longitudinal studies,
multicultural comparisons, and the development of ethical AI frameworks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16144v1">FedGAT: A Privacy-Preserving Federated Approximation Algorithm for Graph
  Attention Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-12-20T18:48:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Siddharth Ambekar, Yuhang Yao, Ryan Li, Carlee Joe-Wong</p>
    <p><b>Summary:</b> Federated training methods have gained popularity for graph learning with
applications including friendship graphs of social media sites and
customer-merchant interaction graphs of huge online marketplaces. However,
privacy regulations often require locally generated data to be stored on local
clients. The graph is then naturally partitioned across clients, with no client
permitted access to information stored on another. Cross-client edges arise
naturally in such cases and present an interesting challenge to federated
training methods, as training a graph model at one client requires feature
information of nodes on the other end of cross-client edges. Attempting to
retain such edges often incurs significant communication overhead, and dropping
them altogether reduces model performance. In simpler models such as Graph
Convolutional Networks, this can be fixed by communicating a limited amount of
feature information across clients before training, but GATs (Graph Attention
Networks) require additional information that cannot be pre-communicated, as it
changes from training round to round. We introduce the Federated Graph
Attention Network (FedGAT) algorithm for semi-supervised node classification,
which approximates the behavior of GATs with provable bounds on the
approximation error. FedGAT requires only one pre-training communication round,
significantly reducing the communication overhead for federated GAT training.
We then analyze the error in the approximation and examine the communication
overhead and computational complexity of the algorithm. Experiments show that
FedGAT achieves nearly the same accuracy as a GAT model in a centralised
setting, and its performance is robust to the number of clients as well as data
distribution.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.15590v1">SemDP: Semantic-level Differential Privacy Protection for Face Datasets</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-20T06:00:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaoting Zhang, Tao Wang, Junhao Ji</p>
    <p><b>Summary:</b> While large-scale face datasets have advanced deep learning-based face
analysis, they also raise privacy concerns due to the sensitive personal
information they contain. Recent schemes have implemented differential privacy
to protect face datasets. However, these schemes generally treat each image as
a separate database, which does not fully meet the core requirements of
differential privacy. In this paper, we propose a semantic-level differential
privacy protection scheme that applies to the entire face dataset. Unlike
pixel-level differential privacy approaches, our scheme guarantees that
semantic privacy in faces is not compromised. The key idea is to convert
unstructured data into structured data to enable the application of
differential privacy. Specifically, we first extract semantic information from
the face dataset to build an attribute database, then apply differential
perturbations to obscure this attribute data, and finally use an image
synthesis model to generate a protected face dataset. Extensive experimental
results show that our scheme can maintain visual naturalness and balance the
privacy-utility trade-off compared to the mainstream schemes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.15538v1">FedRLHF: A Convergence-Guaranteed Federated Framework for
  Privacy-Preserving and Personalized RLHF</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-12-20T03:56:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Flint Xiaofeng Fan, Cheston Tan, Yew-Soon Ong, Roger Wattenhofer, Wei-Tsang Ooi</p>
    <p><b>Summary:</b> In the era of increasing privacy concerns and demand for personalized
experiences, traditional Reinforcement Learning with Human Feedback (RLHF)
frameworks face significant challenges due to their reliance on centralized
data. We introduce Federated Reinforcement Learning with Human Feedback
(FedRLHF), a novel framework that decentralizes the RLHF process. FedRLHF
enables collaborative policy learning across multiple clients without
necessitating the sharing of raw data or human feedback, thereby ensuring
robust privacy preservation. Leveraging federated reinforcement learning, each
client integrates human feedback locally into their reward functions and
updates their policies through personalized RLHF processes. We establish
rigorous theoretical foundations for FedRLHF, providing convergence guarantees,
and deriving sample complexity bounds that scale efficiently with the number of
clients. Empirical evaluations on the MovieLens and IMDb datasets demonstrate
that FedRLHF not only preserves user privacy but also achieves performance on
par with centralized RLHF, while enhancing personalization across diverse
client environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16246v1">Web Privacy based on Contextual Integrity: Measuring the Collapse of
  Online Contexts</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-19T23:30:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ido Sivan-Sevilla, Parthav Poudel</p>
    <p><b>Summary:</b> The collapse of social contexts has been amplified by digital infrastructures
but surprisingly received insufficient attention from Web privacy scholars.
Users are persistently identified within and across distinct web contexts, in
varying degrees, through and by different websites and trackers, losing the
ability to maintain a fragmented identity. To systematically evaluate this
structural privacy harm we operationalize the theory of Privacy as Contextual
Integrity and measure persistent user identification within and between
distinct Web contexts. We crawl the top-700 popular websites across the
contexts of health, finance, news & media, LGBTQ, eCommerce, adult, and
education websites, for 27 days, to learn how persistent browser identification
via third-party cookies and JavaScript fingerprinting is diffused within and
between web contexts. Past work measured Web tracking in bulk, highlighting the
volume of trackers and tracking techniques. These measurements miss a crucial
privacy implication of Web tracking - the collapse of online contexts. Our
findings reveal how persistent browser identification varies between and within
contexts, diffusing user IDs to different distances, contrasting known tracking
distributions across websites, and conducted as a joint or separate effort via
cookie IDs and JS fingerprinting. Our network analysis can inform the
construction of browser storage containers to protect users against real-time
context collapse. This is a first modest step in measuring Web privacy as
contextual integrity, opening new avenues for contextual Web privacy research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.15047v1">Measuring, Modeling, and Helping People Account for Privacy Risks in
  Online Self-Disclosures with AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-19T16:53:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Isadora Krsek, Anubha Kabra, Yao Dou, Tarek Naous, Laura A. Dabbish, Alan Ritter, Wei Xu, Sauvik Das</p>
    <p><b>Summary:</b> In pseudonymous online fora like Reddit, the benefits of self-disclosure are
often apparent to users (e.g., I can vent about my in-laws to understanding
strangers), but the privacy risks are more abstract (e.g., will my partner be
able to tell that this is me?). Prior work has sought to develop natural
language processing (NLP) tools that help users identify potentially risky
self-disclosures in their text, but none have been designed for or evaluated
with the users they hope to protect. Absent this assessment, these tools will
be limited by the social-technical gap: users need assistive tools that help
them make informed decisions, not paternalistic tools that tell them to avoid
self-disclosure altogether. To bridge this gap, we conducted a study with N =
21 Reddit users; we had them use a state-of-the-art NLP disclosure detection
model on two of their authored posts and asked them questions to understand if
and how the model helped, where it fell short, and how it could be improved to
help them make more informed decisions. Despite its imperfections, users
responded positively to the model and highlighted its use as a tool that can
help them catch mistakes, inform them of risks they were unaware of, and
encourage self-reflection. However, our work also shows how, to be useful and
usable, AI for supporting privacy decision-making must account for posting
context, disclosure norms, and users' lived threat models, and provide
explanations that help contextualize detected risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.14832v2">Federated Heavy Hitter Analytics with Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-12-19T13:20:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuemin Zhang, Qingqing Ye, Haibo Hu</p>
    <p><b>Summary:</b> Federated heavy hitter analytics enables service providers to better
understand the preferences of cross-party users by analyzing the most frequent
items. As with federated learning, it faces challenges of privacy concerns,
statistical heterogeneity, and expensive communication. Local differential
privacy (LDP), as the de facto standard for privacy-preserving data collection,
solves the privacy challenge by letting each user perturb her data locally and
report the sanitized version. However, in federated settings, applying LDP
complicates the other two challenges, due to the deteriorated utility by the
injected LDP noise or increasing communication/computation costs by
perturbation mechanism. To tackle these problems, we propose a novel
target-aligning prefix tree mechanism satisfying $\epsilon$-LDP, for federated
heavy hitter analytics. In particular, we propose an adaptive extension
strategy to address the inconsistencies between covering necessary prefixes and
estimating heavy hitters within a party to enhance the utility. We also present
a consensus-based pruning strategy that utilizes noisy prior knowledge from
other parties to further align the inconsistency between finding heavy hitters
in each party and providing reasonable frequency information to identify the
global ones. To the best of our knowledge, our study is the first solution to
the federated heavy hitter analytics in a cross-party setting while satisfying
the stringent $\epsilon$-LDP. Comprehensive experiments on both real-world and
synthetic datasets confirm the effectiveness of our proposed mechanism.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13953v1">Towards privacy-preserving cooperative control via encrypted distributed
  optimization</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-12-18T15:32:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Philipp Binfet, Janis Adamek, Nils SchlÃ¼ter, Moritz Schulze Darup</p>
    <p><b>Summary:</b> Cooperative control is crucial for the effective operation of dynamical
multi-agent systems. Especially for distributed control schemes, it is
essential to exchange data between the agents. This becomes a privacy threat if
the data is sensitive. Encrypted control has shown the potential to address
this risk and ensure confidentiality. However, existing approaches mainly focus
on cloud-based control and distributed schemes are restrictive.
  In this paper, we present a novel privacy-preserving cooperative control
scheme based on encrypted distributed optimization. More precisely, we focus on
a secure distributed solution of a general consensus problem, which has
manifold applications in cooperative control, by means of the alternating
direction method of multipliers (ADMM). As a unique feature of our approach, we
explicitly take into account the common situation that local decision variables
contain copies of quantities associated with neighboring agents and ensure the
neighbor's privacy. We show the effectiveness of our method based on a
numerical case study dealing with the formation of mobile robots.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13939v1">Security and Privacy of Digital Twins for Advanced Manufacturing: A
  Survey</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-12-18T15:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alexander D. Zemskov, Yao Fu, Runchao Li, Xufei Wang, Vispi Karkaria, Ying-Kuan Tsai, Wei Chen, Jianjing Zhang, Robert Gao, Jian Cao, Kenneth A. Loparo, Pan Li</p>
    <p><b>Summary:</b> In Industry 4.0, the digital twin is one of the emerging technologies,
offering simulation abilities to predict, refine, and interpret conditions and
operations, where it is crucial to emphasize a heightened concentration on the
associated security and privacy risks. To be more specific, the adoption of
digital twins in the manufacturing industry relies on integrating technologies
like cyber-physical systems, the Industrial Internet of Things, virtualization,
and advanced manufacturing. The interactions of these technologies give rise to
numerous security and privacy vulnerabilities that remain inadequately
explored. Towards that end, this paper analyzes the cybersecurity threats of
digital twins for advanced manufacturing in the context of data collection,
data sharing, machine learning and deep learning, and system-level security and
privacy. We also provide several solutions to the threats in those four
categories that can help establish more trust in digital twins.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13818v1">Fed-AugMix: Balancing Privacy and Utility via Data Augmentation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-18T13:05:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoyang Li, Wei Chen, Xiaojin Zhang</p>
    <p><b>Summary:</b> Gradient leakage attacks pose a significant threat to the privacy guarantees
of federated learning. While distortion-based protection mechanisms are
commonly employed to mitigate this issue, they often lead to notable
performance degradation. Existing methods struggle to preserve model
performance while ensuring privacy. To address this challenge, we propose a
novel data augmentation-based framework designed to achieve a favorable
privacy-utility trade-off, with the potential to enhance model performance in
certain cases. Our framework incorporates the AugMix algorithm at the client
level, enabling data augmentation with controllable severity. By integrating
the Jensen-Shannon divergence into the loss function, we embed the distortion
introduced by AugMix into the model gradients, effectively safeguarding privacy
against deep leakage attacks. Moreover, the JS divergence promotes model
consistency across different augmentations of the same image, enhancing both
robustness and performance. Extensive experiments on benchmark datasets
demonstrate the effectiveness and stability of our method in protecting
privacy. Furthermore, our approach maintains, and in some cases improves, model
performance, showcasing its ability to achieve a robust privacy-utility
trade-off.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13678v1">Clio: Privacy-Preserving Insights into Real-World AI Use</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-18T10:05:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alex Tamkin, Miles McCain, Kunal Handa, Esin Durmus, Liane Lovitt, Ankur Rathi, Saffron Huang, Alfred Mountfield, Jerry Hong, Stuart Ritchie, Michael Stern, Brian Clarke, Landon Goldberg, Theodore R. Sumers, Jared Mueller, William McEachen, Wes Mitchell, Shan Carter, Jack Clark, Jared Kaplan, Deep Ganguli</p>
    <p><b>Summary:</b> How are AI assistants being used in the real world? While model providers in
theory have a window into this impact via their users' data, both privacy
concerns and practical challenges have made analyzing this data difficult. To
address these issues, we present Clio (Claude insights and observations), a
privacy-preserving platform that uses AI assistants themselves to analyze and
surface aggregated usage patterns across millions of conversations, without the
need for human reviewers to read raw conversations. We validate this can be
done with a high degree of accuracy and privacy by conducting extensive
evaluations. We demonstrate Clio's usefulness in two broad ways. First, we
share insights about how models are being used in the real world from one
million Claude.ai Free and Pro conversations, ranging from providing advice on
hairstyles to providing guidance on Git operations and concepts. We also
identify the most common high-level use cases on Claude.ai (coding, writing,
and research tasks) as well as patterns that differ across languages (e.g.,
conversations in Japanese discuss elder care and aging populations at
higher-than-typical rates). Second, we use Clio to make our systems safer by
identifying coordinated attempts to abuse our systems, monitoring for unknown
unknowns during critical periods like launches of new capabilities or major
world events, and improving our existing monitoring systems. We also discuss
the limitations of our approach, as well as risks and ethical concerns. By
enabling analysis of real-world AI usage, Clio provides a scalable platform for
empirically grounded AI safety and governance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13522v1">Privacy-Preserving Cyberattack Detection in Blockchain-Based IoT Systems
  Using AI and Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-18T05:46:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bui Duc Manh, Chi-Hieu Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Ming Zeng, Quoc-Viet Pham</p>
    <p><b>Summary:</b> This work proposes a novel privacy-preserving cyberattack detection framework
for blockchain-based Internet-of-Things (IoT) systems. In our approach,
artificial intelligence (AI)-driven detection modules are strategically
deployed at blockchain nodes to identify real-time attacks, ensuring high
accuracy and minimal delay. To achieve this efficiency, the model training is
conducted by a cloud service provider (CSP). Accordingly, blockchain nodes send
their data to the CSP for training, but to safeguard privacy, the data is
encrypted using homomorphic encryption (HE) before transmission. This
encryption method allows the CSP to perform computations directly on encrypted
data without the need for decryption, preserving data privacy throughout the
learning process. To handle the substantial volume of encrypted data, we
introduce an innovative packing algorithm in a Single-Instruction-Multiple-Data
(SIMD) manner, enabling efficient training on HE-encrypted data. Building on
this, we develop a novel deep neural network training algorithm optimized for
encrypted data. We further propose a privacy-preserving distributed learning
approach based on the FedAvg algorithm, which parallelizes the training across
multiple workers, significantly improving computation time. Upon completion,
the CSP distributes the trained model to the blockchain nodes, enabling them to
perform real-time, privacy-preserved detection. Our simulation results
demonstrate that our proposed method can not only mitigate the training time
but also achieve detection accuracy that is approximately identical to the
approach without encryption, with a gap of around 0.01%. Additionally, our real
implementations on various blockchain consensus algorithms and hardware
configurations show that our proposed framework can also be effectively adapted
to real-world systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13440v1">Safeguarding Virtual Healthcare: A Novel Attacker-Centric Model for Data
  Security and Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-12-18T02:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Suvineetha Herath, Haywood Gelman, John Hastings, Yong Wang</p>
    <p><b>Summary:</b> The rapid growth of remote healthcare delivery has introduced significant
security and privacy risks to protected health information (PHI). Analysis of a
comprehensive healthcare security breach dataset covering 2009-2023 reveals
their significant prevalence and impact. This study investigates the root
causes of such security incidents and introduces the Attacker-Centric Approach
(ACA), a novel threat model tailored to protect PHI. ACA addresses limitations
in existing threat models and regulatory frameworks by adopting a holistic
attacker-focused perspective, examining threats from the viewpoint of cyber
adversaries, their motivations, tactics, and potential attack vectors.
Leveraging established risk management frameworks, ACA provides a multi-layered
approach to threat identification, risk assessment, and proactive mitigation
strategies. A comprehensive threat library classifies physical, third-party,
external, and internal threats. ACA's iterative nature and feedback mechanisms
enable continuous adaptation to emerging threats, ensuring sustained
effectiveness. ACA allows healthcare providers to proactively identify and
mitigate vulnerabilities, fostering trust and supporting the secure adoption of
virtual care technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.12775v1">RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-17T10:36:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yihang Cheng, Lan Zhang, Junyang Wang, Mu Yuan, Yunhao Yao</p>
    <p><b>Summary:</b> Retrieval-augmented generation (RAG) improves the service quality of large
language models by retrieving relevant documents from credible literature and
integrating them into the context of the user query. Recently, the rise of the
cloud RAG service has made it possible for users to query relevant documents
conveniently. However, directly sending queries to the cloud brings potential
privacy leakage. In this paper, we are the first to formally define the
privacy-preserving cloud RAG service to protect the user query and propose
RemoteRAG as a solution regarding privacy, efficiency, and accuracy. For
privacy, we introduce $(n,\epsilon)$-DistanceDP to characterize privacy leakage
of the user query and the leakage inferred from relevant documents. For
efficiency, we limit the search range from the total documents to a small
number of selected documents related to a perturbed embedding generated from
$(n,\epsilon)$-DistanceDP, so that computation and communication costs required
for privacy protection significantly decrease. For accuracy, we ensure that the
small range includes target documents related to the user query with detailed
theoretical analysis. Experimental results also demonstrate that RemoteRAG can
resist existing embedding inversion attack methods while achieving no loss in
retrieval under various settings. Moreover, RemoteRAG is efficient, incurring
only $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$
GB with the non-optimized privacy-preserving scheme) when retrieving from a
total of $10^6$ documents.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.12387v2">Differential Privacy Preserving Distributed Quantum Computing</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-12-16T22:46:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hui Zhong, Keyi Ju, Jiachen Shen, Xinyue Zhang, Xiaoqi Qin, Tomoaki Ohtsuki, Miao Pan, Zhu Han</p>
    <p><b>Summary:</b> Existing quantum computers can only operate with hundreds of qubits in the
Noisy Intermediate-Scale Quantum (NISQ) state, while quantum distributed
computing (QDC) is regarded as a reliable way to address this limitation,
allowing quantum computers to achieve their full computational potential.
However, similar to classical distributed computing, QDC also faces the problem
of privacy leakage. Existing research has introduced quantum differential
privacy (QDP) for privacy protection in central quantum computing, but there is
no dedicated privacy protection mechanisms for QDC. To fill this research gap,
our paper introduces a novel concept called quantum R\'enyi differential
privacy (QRDP), which incorporates the advantages of classical R\'enyi DP and
is applicable in the QDC domain. Based on the new quantum R\'enyi divergence,
QRDP provides delicate and flexible privacy protection by introducing parameter
$\alpha$. In particular, the QRDP composition is well suited for QDC, since it
allows for more precise control of the total privacy budget in scenarios
requiring multiple quantum operations. We analyze a variety of noise mechanisms
that can implement QRDP, and derive the lowest privacy budget provided by these
mechanisms. Finally, we investigate the impact of different quantum parameters
on QRDP. Through our simulations, we also find that adding noise will make the
data less usable, but increase the level of privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.12374v1">Privacy in Metalearning and Multitask Learning: Modeling and Separations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-16T22:07:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Maryam Aliakbarpour, Konstantina Bairaktari, Adam Smith, Marika Swanberg, Jonathan Ullman</p>
    <p><b>Summary:</b> Model personalization allows a set of individuals, each facing a different
learning task, to train models that are more accurate for each person than
those they could develop individually. The goals of personalization are
captured in a variety of formal frameworks, such as multitask learning and
metalearning. Combining data for model personalization poses risks for privacy
because the output of an individual's model can depend on the data of other
individuals. In this work we undertake a systematic study of differentially
private personalized learning. Our first main contribution is to construct a
taxonomy of formal frameworks for private personalized learning. This taxonomy
captures different formal frameworks for learning as well as different threat
models for the attacker. Our second main contribution is to prove separations
between the personalized learning problems corresponding to different choices.
In particular, we prove a novel separation between private multitask learning
and private metalearning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11951v1">The Impact of Generalization Techniques on the Interplay Among Privacy,
  Utility, and Fairness in Image Classification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-16T16:35:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmad Hassanpour, Amir Zarei, Khawla Mallat, Anderson Santana de Oliveira, Bian Yang</p>
    <p><b>Summary:</b> This study investigates the trade-offs between fairness, privacy, and utility
in image classification using machine learning (ML). Recent research suggests
that generalization techniques can improve the balance between privacy and
utility. One focus of this work is sharpness-aware training (SAT) and its
integration with differential privacy (DP-SAT) to further improve this balance.
Additionally, we examine fairness in both private and non-private learning
models trained on datasets with synthetic and real-world biases. We also
measure the privacy risks involved in these scenarios by performing membership
inference attacks (MIAs) and explore the consequences of eliminating
high-privacy risk samples, termed outliers. Moreover, we introduce a new
metric, named \emph{harmonic score}, which combines accuracy, privacy, and
fairness into a single measure.
  Through empirical analysis using generalization techniques, we achieve an
accuracy of 81.11\% under $(8, 10^{-5})$-DP on CIFAR-10, surpassing the 79.5\%
reported by De et al. (2022). Moreover, our experiments show that memorization
of training samples can begin before the overfitting point, and generalization
techniques do not guarantee the prevention of this memorization. Our analysis
of synthetic biases shows that generalization techniques can amplify model bias
in both private and non-private models. Additionally, our results indicate that
increased bias in training data leads to reduced accuracy, greater
vulnerability to privacy attacks, and higher model bias. We validate these
findings with the CelebA dataset, demonstrating that similar trends persist
with real-world attribute imbalances. Finally, our experiments show that
removing outlier data decreases accuracy and further amplifies model bias.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11737v1">Efficiently Achieving Secure Model Training and Secure Aggregation to
  Ensure Bidirectional Privacy-Preservation in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-16T12:58:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xue Yang, Depan Peng, Yan Feng, Xiaohu Tang, Weijun Fang, Jun Shao</p>
    <p><b>Summary:</b> Bidirectional privacy-preservation federated learning is crucial as both
local gradients and the global model may leak privacy. However, only a few
works attempt to achieve it, and they often face challenges such as excessive
communication and computational overheads, or significant degradation of model
accuracy, which hinders their practical applications. In this paper, we design
an efficient and high-accuracy bidirectional privacy-preserving scheme for
federated learning to complete secure model training and secure aggregation. To
efficiently achieve bidirectional privacy, we design an efficient and
accuracy-lossless model perturbation method on the server side (called
$\mathbf{MP\_Server}$) that can be combined with local differential privacy
(LDP) to prevent clients from accessing the model, while ensuring that the
local gradients obtained on the server side satisfy LDP. Furthermore, to ensure
model accuracy, we customize a distributed differential privacy mechanism on
the client side (called $\mathbf{DDP\_Client}$). When combined with
$\mathbf{MP\_Server}$, it ensures LDP of the local gradients, while ensuring
that the aggregated result matches the accuracy of central differential privacy
(CDP). Extensive experiments demonstrate that our scheme significantly
outperforms state-of-the-art bidirectional privacy-preservation baselines
(SOTAs) in terms of computational cost, model accuracy, and defense ability
against privacy attacks. Particularly, given target accuracy, the training time
of SOTAs is approximately $200$ times, or even over $1000$ times, longer than
that of our scheme. When the privacy budget is set relatively small, our scheme
incurs less than $6\%$ accuracy loss compared to the privacy-ignoring method,
while SOTAs suffer up to $20\%$ accuracy loss. Experimental results also show
that the defense capability of our scheme outperforms than SOTAs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11572v1">DB-PAISA: Discovery-Based Privacy-Agile IoT Sensing+Actuation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-16T08:57:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Isita Bagayatkar, Youngil Kim, Gene Tsudik</p>
    <p><b>Summary:</b> Internet of Things (IoT) devices are becoming increasingly commonplace in
numerous public and semi-private settings. Currently, most such devices lack
mechanisms to facilitate their discovery by casual (nearby) users who are not
owners or operators. However, these users are potentially being sensed, and/or
actuated upon, by these devices, without their knowledge or consent. This
naturally triggers privacy, security, and safety issues.
  To address this problem, some recent work explored device transparency in the
IoT ecosystem. The intuitive approach is for each device to periodically and
securely broadcast (announce) its presence and capabilities to all nearby
users. While effective, when no new users are present, this push-based approach
generates a substantial amount of unnecessary network traffic and needlessly
interferes with normal device operation.
  In this work, we construct DB-PAISA which addresses these issues via a
pull-based method, whereby devices reveal their presence and capabilities only
upon explicit user request. Each device guarantees a secure timely response
(even if fully compromised by malware) based on a small active Root-of-Trust
(RoT). DB-PAISA requires no hardware modifications and is suitable for a range
of current IoT devices. To demonstrate its feasibility and practicality, we
built a fully functional and publicly available prototype. It is implemented
atop a commodity MCU (NXP LCP55S69) and operates in tandem with a
smartphone-based app. Using this prototype, we evaluate energy consumption and
other performance factors.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11394v1">Privacy-Preserving Brain-Computer Interfaces: A Systematic Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-16T02:45:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> K. Xia, W. Duch, Y. Sun, K. Xu, W. Fang, H. Luo, Y. Zhang, D. Sang, X. Xu, F-Y Wang, D. Wu</p>
    <p><b>Summary:</b> A brain-computer interface (BCI) establishes a direct communication pathway
between the human brain and a computer. It has been widely used in medical
diagnosis, rehabilitation, education, entertainment, etc. Most research so far
focuses on making BCIs more accurate and reliable, but much less attention has
been paid to their privacy. Developing a commercial BCI system usually requires
close collaborations among multiple organizations, e.g., hospitals,
universities, and/or companies. Input data in BCIs, e.g., electroencephalogram
(EEG), contain rich privacy information, and the developed machine learning
model is usually proprietary. Data and model transmission among different
parties may incur significant privacy threats, and hence privacy protection in
BCIs must be considered. Unfortunately, there does not exist any contemporary
and comprehensive review on privacy-preserving BCIs. This paper fills this gap,
by describing potential privacy threats and protection strategies in BCIs. It
also points out several challenges and future research directions in developing
privacy-preserving BCIs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11390v1">Accurate, Robust and Privacy-Preserving Brain-Computer Interface
  Decoding</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-12-16T02:37:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaoqing Chen, Tianwang Jia, Dongrui Wu</p>
    <p><b>Summary:</b> An electroencephalogram (EEG) based brain-computer interface (BCI) enables
direct communication between the brain and external devices. However, EEG-based
BCIs face at least three major challenges in real-world applications: data
scarcity and individual differences, adversarial vulnerability, and data
privacy. While previous studies have addressed one or two of these issues,
simultaneous accommodation of all three challenges remains challenging and
unexplored. This paper fills this gap, by proposing an Augmented Robustness
Ensemble (ARE) algorithm and integrating it into three privacy protection
scenarios (centralized source-free transfer, federated source-free transfer,
and source data perturbation), achieving simultaneously accurate decoding,
adversarial robustness, and privacy protection of EEG-based BCIs. Experiments
on three public EEG datasets demonstrated that our proposed approach
outperformed over 10 classic and state-of-the-art approaches in both accuracy
and robustness in all three privacy-preserving scenarios, even outperforming
state-of-the-art transfer learning approaches that do not consider privacy
protection at all. This is the first time that three major challenges in
EEG-based BCIs can be addressed simultaneously, significantly improving the
practicalness of EEG decoding in real-world BCIs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11066v1">Learning Robust and Privacy-Preserving Representations via Information
  Theory</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-15T05:51:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Binghui Zhang, Sayedeh Leila Noorbakhsh, Yun Dong, Yuan Hong, Binghui Wang</p>
    <p><b>Summary:</b> Machine learning models are vulnerable to both security attacks (e.g.,
adversarial examples) and privacy attacks (e.g., private attribute inference).
We take the first step to mitigate both the security and privacy attacks, and
maintain task utility as well. Particularly, we propose an
information-theoretic framework to achieve the goals through the lens of
representation learning, i.e., learning representations that are robust to both
adversarial examples and attribute inference adversaries. We also derive novel
theoretical results under our framework, e.g., the inherent trade-off between
adversarial robustness/utility and attribute privacy, and guaranteed attribute
privacy leakage against attribute inference adversaries.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.10652v1">Centaur: Bridging the Impossible Trinity of Privacy, Efficiency, and
  Performance in Privacy-Preserving Transformer Inference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-14T02:50:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinglong Luo, Guanzhong Chen, Yehong Zhang, Shiyu Liu, Hui Wang, Yue Yu, Xun Zhou, Yuan Qi, Zenglin Xu</p>
    <p><b>Summary:</b> As pre-trained models, like Transformers, are increasingly deployed on cloud
platforms for inference services, the privacy concerns surrounding model
parameters and inference data are becoming more acute. Current
Privacy-Preserving Transformer Inference (PPTI) frameworks struggle with the
"impossible trinity" of privacy, efficiency, and performance. For instance,
Secure Multi-Party Computation (SMPC)-based solutions offer strong privacy
guarantees but come with significant inference overhead and performance
trade-offs. On the other hand, PPTI frameworks that use random permutations
achieve inference efficiency close to that of plaintext and maintain accurate
results but require exposing some model parameters and intermediate results,
thereby risking substantial privacy breaches. Addressing this "impossible
trinity" with a single technique proves challenging. To overcome this
challenge, we propose Centaur, a novel hybrid PPTI framework. Unlike existing
methods, Centaur protects model parameters with random permutations and
inference data with SMPC, leveraging the structure of Transformer models. By
designing a series of efficient privacy-preserving algorithms, Centaur
leverages the strengths of both techniques to achieve a better balance between
privacy, efficiency, and performance in PPTI. We comprehensively evaluate the
effectiveness of Centaur on various types of Transformer models and datasets.
Experimental results demonstrate that the privacy protection capabilities
offered by Centaur can withstand various existing model inversion attack
methods. In terms of performance and efficiency, Centaur not only maintains the
same performance as plaintext inference but also improves inference speed by
$5.0-30.4$ times.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.10612v1">Meeting Utility Constraints in Differential Privacy: A Privacy-Boosting
  Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-12-13T23:34:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bo Jiang, Wanrong Zhang, Donghang Lu, Jian Du, Sagar Sharma, Qiang Yan</p>
    <p><b>Summary:</b> Data engineering often requires accuracy (utility) constraints on results,
posing significant challenges in designing differentially private (DP)
mechanisms, particularly under stringent privacy parameter $\epsilon$. In this
paper, we propose a privacy-boosting framework that is compatible with most
noise-adding DP mechanisms. Our framework enhances the likelihood of outputs
falling within a preferred subset of the support to meet utility requirements
while enlarging the overall variance to reduce privacy leakage. We characterize
the privacy loss distribution of our framework and present the privacy profile
formulation for $(\epsilon,\delta)$-DP and R\'enyi DP (RDP) guarantees. We
study special cases involving data-dependent and data-independent utility
formulations. Through extensive experiments, we demonstrate that our framework
achieves lower privacy loss than standard DP mechanisms under utility
constraints. Notably, our approach is particularly effective in reducing
privacy loss with large query sensitivity relative to the true answer, offering
a more practical and flexible approach to designing differentially private
mechanisms that meet specific utility constraints.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.09812v1">ScaleOT: Privacy-utility-scalable Offsite-tuning with Dynamic
  LayerReplace and Selective Rank Compression</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-13T03:00:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kai Yao, Zhaorui Tan, Tiandi Ye, Lichun Li, Yuan Zhao, Wenyan Liu, Wei Wang, Jianke Zhu</p>
    <p><b>Summary:</b> Offsite-tuning is a privacy-preserving method for tuning large language
models (LLMs) by sharing a lossy compressed emulator from the LLM owners with
data owners for downstream task tuning. This approach protects the privacy of
both the model and data owners. However, current offsite tuning methods often
suffer from adaptation degradation, high computational costs, and limited
protection strength due to uniformly dropping LLM layers or relying on
expensive knowledge distillation. To address these issues, we propose ScaleOT,
a novel privacy-utility-scalable offsite-tuning framework that effectively
balances privacy and utility. ScaleOT introduces a novel layerwise lossy
compression algorithm that uses reinforcement learning to obtain the importance
of each layer. It employs lightweight networks, termed harmonizers, to replace
the raw LLM layers. By combining important original LLM layers and harmonizers
in different ratios, ScaleOT generates emulators tailored for optimal
performance with various model scales for enhanced privacy protection.
Additionally, we present a rank reduction method to further compress the
original LLM layers, significantly enhancing privacy with negligible impact on
utility. Comprehensive experiments show that ScaleOT can achieve nearly
lossless offsite tuning performance compared with full fine-tuning while
obtaining better model privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.09256v1">Differential Privacy Releasing of Hierarchical Origin/Destination Data
  with a TopDown Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2024-12-12T13:14:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fabrizio Boninsegna, Francesco Silvestri</p>
    <p><b>Summary:</b> This paper presents a novel method to generate differentially private tabular
datasets for hierarchical data, with a specific focus on origin-destination
(O/D) trips. The approach builds upon the TopDown algorithm, a constraint-based
mechanism designed to incorporate invariant queries into tabular data,
developed by the US Census. O/D hierarchical data refers to datasets
representing trips between geographical areas organized in a hierarchical
structure (e.g., region $\rightarrow$ province $\rightarrow$ city). The
developed method is crafted to improve accuracy on queries spanning wider
geographical areas that can be obtained by aggregation. Maintaining high
accuracy for aggregated geographical queries is a crucial attribute of the
differentially private dataset, particularly for practitioners. Furthermore,
the approach is designed to minimize false positives detection and to replicate
the sparsity of the sensitive data. The key technical contributions of this
paper include a novel TopDown algorithm that employs constrained optimization
with Chebyshev distance minimization, with theoretical guarantees based on the
maximum absolute error. Additionally, we propose a new integer optimization
algorithm that significantly reduces the incidence of false positives. The
effectiveness of the proposed approach is validated using both real-world and
synthetic O/D datasets, demonstrating its ability to generate private data with
high utility and a reduced number of false positives. We emphasize that the
proposed algorithm is applicable to any tabular data with a hierarchical
structure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.09222v1">Building a Privacy Web with SPIDEr -- Secure Pipeline for Information
  De-Identification with End-to-End Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-12-12T12:24:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Novoneel Chakraborty, Anshoo Tandon, Kailash Reddy, Kaushal Kirpekar, Bryan Paul Robert, Hari Dilip Kumar, Abhilash Venkatesh, Abhay Sharma</p>
    <p><b>Summary:</b> Data de-identification makes it possible to glean insights from data while
preserving user privacy. The use of Trusted Execution Environments (TEEs) allow
for the execution of de-identification applications on the cloud without the
need for a user to trust the third-party application provider. In this paper,
we present \textit{SPIDEr - Secure Pipeline for Information De-Identification
with End-to-End Encryption}, our implementation of an end-to-end encrypted data
de-identification pipeline. SPIDEr supports classical anonymisation techniques
such as suppression, pseudonymisation, generalisation, and aggregation, as well
as techniques that offer a formal privacy guarantee such as k-anonymisation and
differential privacy. To enable scalability and improve performance on
constrained TEE hardware, we enable batch processing of data for differential
privacy computations. We present our design of the control flows for end-to-end
secure execution of de-identification operations within a TEE. As part of the
control flow for running SPIDEr within the TEE, we perform attestation, a
process that verifies that the software binaries were properly instantiated on
a known, trusted platform.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.09195v1">On the Generation and Removal of Speaker Adversarial Perturbation for
  Voice-Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-12-12T11:46:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenyang Guo, Liping Chen, Zhuhai Li, Kong Aik Lee, Zhen-Hua Ling, Wu Guo</p>
    <p><b>Summary:</b> Neural networks are commonly known to be vulnerable to adversarial attacks
mounted through subtle perturbation on the input data. Recent development in
voice-privacy protection has shown the positive use cases of the same technique
to conceal speaker's voice attribute with additive perturbation signal
generated by an adversarial network. This paper examines the reversibility
property where an entity generating the adversarial perturbations is authorized
to remove them and restore original speech (e.g., the speaker him/herself). A
similar technique could also be used by an investigator to deanonymize a
voice-protected speech to restore criminals' identities in security and
forensic analysis. In this setting, the perturbation generative module is
assumed to be known in the removal process. To this end, a joint training of
perturbation generation and removal modules is proposed. Experimental results
on the LibriSpeech dataset demonstrated that the subtle perturbations added to
the original speech can be predicted from the anonymized speech while achieving
the goal of privacy protection. By removing these perturbations from the
anonymized sample, the original speech can be restored. Audio samples can be
found in \url{https://voiceprivacy.github.io/Perturbation-Generation-Removal/}.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.08559v2">Underestimated Privacy Risks for Minority Populations in Large Language
  Model Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-11T17:22:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rongzhe Wei, Mufei Li, Mohsen Ghassemi, Eleonora KreaÄiÄ, Yifan Li, Xiang Yue, Bo Li, Vamsi K. Potluru, Pan Li, Eli Chien</p>
    <p><b>Summary:</b> Large Language Models are trained on extensive datasets that often contain
sensitive, human-generated information, raising significant concerns about
privacy breaches. While certified unlearning approaches offer strong privacy
guarantees, they rely on restrictive model assumptions that are not applicable
to LLMs. As a result, various unlearning heuristics have been proposed, with
the associated privacy risks assessed only empirically. The standard evaluation
pipelines typically randomly select data for removal from the training set,
apply unlearning techniques, and use membership inference attacks to compare
the unlearned models against models retrained without the to-be-unlearned data.
However, since every data point is subject to the right to be forgotten,
unlearning should be considered in the worst-case scenario from the privacy
perspective. Prior work shows that data outliers may exhibit higher
memorization effects. Intuitively, they are harder to be unlearn and thus the
privacy risk of unlearning them is underestimated in the current evaluation. In
this paper, we leverage minority data to identify such a critical flaw in
previously widely adopted evaluations. We substantiate this claim through
carefully designed experiments, including unlearning canaries related to
minority groups, inspired by privacy auditing literature. Using personally
identifiable information as a representative minority identifier, we
demonstrate that minority groups experience at least 20% more privacy leakage
in most cases across six unlearning approaches, three MIAs, three benchmark
datasets, and two LLMs of different scales. Given that the right to be
forgotten should be upheld for every individual, we advocate for a more
rigorous evaluation of LLM unlearning methods. Our minority-aware evaluation
framework represents an initial step toward ensuring more equitable assessments
of LLM unlearning efficacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.08544v1">Training Data Reconstruction: Privacy due to Uncertainty?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-11T17:00:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Christina Runkel, Kanchana Vaishnavi Gandikota, Jonas Geiping, Carola-Bibiane SchÃ¶nlieb, Michael Moeller</p>
    <p><b>Summary:</b> Being able to reconstruct training data from the parameters of a neural
network is a major privacy concern. Previous works have shown that
reconstructing training data, under certain circumstances, is possible. In this
work, we analyse such reconstructions empirically and propose a new formulation
of the reconstruction as a solution to a bilevel optimisation problem. We
demonstrate that our formulation as well as previous approaches highly depend
on the initialisation of the training images $x$ to reconstruct. In particular,
we show that a random initialisation of $x$ can lead to reconstructions that
resemble valid training samples while not being part of the actual training
dataset. Thus, our experiments on affine and one-hidden layer networks suggest
that when reconstructing natural images, yet an adversary cannot identify
whether reconstructed images have indeed been part of the set of training
samples.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.08534v1">Protecting Confidentiality, Privacy and Integrity in Collaborative
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-11T16:48:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dong Chen, Alice Dethise, Istemi Ekin Akkus, Ivica Rimac, Klaus Satzke, Antti Koskela, Marco Canini, Wei Wang, Ruichuan Chen</p>
    <p><b>Summary:</b> A collaboration between dataset owners and model owners is needed to
facilitate effective machine learning (ML) training. During this collaboration,
however, dataset owners and model owners want to protect the confidentiality of
their respective assets (i.e., datasets, models and training code), with the
dataset owners also caring about the privacy of individual users whose data is
in their datasets. Existing solutions either provide limited confidentiality
for models and training code, or suffer from privacy issues due to collusion.
  We present Citadel++, a scalable collaborative ML training system designed to
simultaneously protect the confidentiality of datasets, models and training
code, as well as the privacy of individual users. Citadel++ enhances
differential privacy techniques to safeguard the privacy of individual user
data while maintaining model utility. By employing Virtual Machine-level
Trusted Execution Environments (TEEs) and improved integrity protection
techniques through various OS-level mechanisms, Citadel++ effectively preserves
the confidentiality of datasets, models and training code, and enforces our
privacy mechanisms even when the models and training code have been maliciously
designed. Our experiments show that Citadel++ provides privacy, model utility
and performance while adhering to confidentiality and privacy requirements of
dataset owners and model owners, outperforming the state-of-the-art
privacy-preserving training systems by up to 543x on CPU and 113x on GPU TEEs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.08276v1">Local Features Meet Stochastic Anonymization: Revolutionizing
  Privacy-Preserving Face Recognition for Black-Box Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-12-11T10:49:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuanwei Liu, Chengyu Jia, Ruqi Xiao, Xuemai Jia, Hui Wei, Kui Jiang, Zheng Wang</p>
    <p><b>Summary:</b> The task of privacy-preserving face recognition (PPFR) currently faces two
major unsolved challenges: (1) existing methods are typically effective only on
specific face recognition models and struggle to generalize to black-box face
recognition models; (2) current methods employ data-driven reversible
representation encoding for privacy protection, making them susceptible to
adversarial learning and reconstruction of the original image. We observe that
face recognition models primarily rely on local features ({e.g., face contour,
skin texture, and so on) for identification. Thus, by disrupting global
features while enhancing local features, we achieve effective recognition even
in black-box environments. Additionally, to prevent adversarial models from
learning and reversing the anonymization process, we adopt an adversarial
learning-based approach with irreversible stochastic injection to ensure the
stochastic nature of the anonymization. Experimental results demonstrate that
our method achieves an average recognition accuracy of 94.21\% on black-box
models, outperforming existing methods in both privacy protection and
anti-reconstruction capabilities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.07687v2">Privacy-Preserving Customer Support: A Framework for Secure and Scalable
  Interactions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">   
  <p><b>Published on:</b> 2024-12-10T17:20:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anant Prakash Awasthi, Girdhar Gopal Agarwal, Chandraketu Singh, Rakshit Varma, Sanchit Sharma</p>
    <p><b>Summary:</b> The growing reliance on artificial intelligence (AI) in customer support has
significantly improved operational efficiency and user experience. However,
traditional machine learning (ML) approaches, which require extensive local
training on sensitive datasets, pose substantial privacy risks and compliance
challenges with regulations like the General Data Protection Regulation (GDPR)
and California Consumer Privacy Act (CCPA). Existing privacy-preserving
techniques, such as anonymization, differential privacy, and federated
learning, address some concerns but face limitations in utility, scalability,
and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning
(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in
a zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates
the need for local training on sensitive data by utilizing pre-trained LLMs to
generate responses directly. The framework incorporates real-time data
anonymization to redact or mask sensitive information, retrieval-augmented
generation (RAG) for domain-specific query resolution, and robust
post-processing to ensure compliance with regulatory standards. This
combination reduces privacy risks, simplifies compliance, and enhances
scalability and operational efficiency. Empirical analysis demonstrates that
the PP-ZSL framework provides accurate, privacy-compliant responses while
significantly lowering the costs and complexities of deploying AI-driven
customer support systems. The study highlights potential applications across
industries, including financial services, healthcare, e-commerce, legal
support, telecommunications, and government services. By addressing the dual
challenges of privacy and performance, this framework establishes a foundation
for secure, efficient, and regulatory-compliant AI applications in customer
interactions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.06960v1">Simplications: Why and how we should rethink data of/by/for the people
  in smart homes and its privacy implications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-09T20:08:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Albrecht Kurze, Alexa Becker</p>
    <p><b>Summary:</b> More and more smart devices enter our homes. Often these devices come with a
variety of sensors, mostly simple sensors, e.g., for light, temperature,
humidity or motion. And they all collect data. While it is data of the home
environment it is also data of domestic life in the home. Thus it is data of
the people and by the people in the home capturing their presence, arrival and
departure, typical domestic activities, bad habits, health status etc. Based on
previous as well as ongoing research we know that people are actually able to
make sense of simple sensor data and that they will make use of it for their
own purposes. Simple sensors, when critically reflected, are often only
"simple" in a technical sense. The unreflected design and use of these sensors
can easily lead to unintended implications, i.e. for privacy. However, it may
not even need a Big Brother or data experts or AI to make the data of these
sensors sensitive, e.g., if used for lateral surveillance within families.
Often unintended but wicked implications emerge despite good intentions, such
as improving efficiency or energy saving through collecting sensor data. Thus
sensor data from the home is actually data of/by/for the people in the home.
First, we explain how this might have relevance across scales of community of
people - not only for the domain of the home but also in broader meaning.
Second, we relate our previous as well as ongoing research in the domain of
smart homes to this topic.</p>
  </details>
</div>

