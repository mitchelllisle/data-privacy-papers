
<h2>2024-08</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00327v1">Demo: FedCampus: A Real-world Privacy-preserving Mobile Application for
  Smart Campus via Federated Learning & Analytics</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-31T01:58:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiaxiang Geng, Beilong Tang, Boyan Zhang, Jiaqi Shao, Bing Luo</p>
    <p><b>Summary:</b> In this demo, we introduce FedCampus, a privacy-preserving mobile application
for smart \underline{campus} with \underline{fed}erated learning (FL) and
federated analytics (FA). FedCampus enables cross-platform on-device FL/FA for
both iOS and Android, supporting continuously models and algorithms deployment
(MLOps). Our app integrates privacy-preserving processed data via differential
privacy (DP) from smartwatches, where the processed parameters are used for
FL/FA through the FedCampus backend platform. We distributed 100 smartwatches
to volunteers at Duke Kunshan University and have successfully completed a
series of smart campus tasks featuring capabilities such as sleep tracking,
physical activity monitoring, personalized recommendations, and heavy hitters.
Our project is opensourced at https://github.com/FedCampus/FedCampus_Flutter.
See the FedCampus video at https://youtu.be/k5iu46IjA38.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.10533v3">Ethical Challenges in Computer Vision: Ensuring Privacy and Mitigating
  Bias in Publicly Available Datasets</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-08-31T00:59:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ghalib Ahmed Tahir</p>
    <p><b>Summary:</b> This paper aims to shed light on the ethical problems of creating and
deploying computer vision tech, particularly in using publicly available
datasets. Due to the rapid growth of machine learning and artificial
intelligence, computer vision has become a vital tool in many industries,
including medical care, security systems, and trade. However, extensive use of
visual data that is often collected without consent due to an informed
discussion of its ramifications raises significant concerns about privacy and
bias. The paper also examines these issues by analyzing popular datasets such
as COCO, LFW, ImageNet, CelebA, PASCAL VOC, etc., that are usually used for
training computer vision models. We offer a comprehensive ethical framework
that addresses these challenges regarding the protection of individual rights,
minimization of bias as well as openness and responsibility. We aim to
encourage AI development that will take into account societal values as well as
ethical standards to avoid any public harm.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17378v1">Empowering Open Data Sharing for Social Good: A Privacy-Aware Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-08-30T16:14:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tânia Carvalho, Luís Antunes, Cristina Costa, Nuno Moniz</p>
    <p><b>Summary:</b> The Covid-19 pandemic has affected the world at multiple levels. Data sharing
was pivotal for advancing research to understand the underlying causes and
implement effective containment strategies. In response, many countries have
promoted the availability of daily cases to support research initiatives,
fostering collaboration between organisations and making such data available to
the public through open data platforms. Despite the several advantages of data
sharing, one of the major concerns before releasing health data is its impact
on individuals' privacy. Such a sharing process should be based on
state-of-the-art methods in Data Protection by Design and by Default. In this
paper, we use a data set related to Covid-19 cases in the second largest
hospital in Portugal to show how it is feasible to ensure data privacy while
improving the quality and maintaining the utility of the data. Our goal is to
demonstrate how knowledge exchange in multidisciplinary teams of healthcare
practitioners, data privacy, and data science experts is crucial to
co-developing strategies that ensure high utility of de-identified data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17354v1">Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language
  Models for Privacy Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T15:35:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md Rafi Ur Rashid, Jing Liu, Toshiaki Koike-Akino, Shagufta Mehnaz, Ye Wang</p>
    <p><b>Summary:</b> Fine-tuning large language models on private data for downstream applications
poses significant privacy risks in potentially exposing sensitive information.
Several popular community platforms now offer convenient distribution of a
large variety of pre-trained models, allowing anyone to publish without
rigorous verification. This scenario creates a privacy threat, as pre-trained
models can be intentionally crafted to compromise the privacy of fine-tuning
datasets. In this study, we introduce a novel poisoning technique that uses
model-unlearning as an attack tool. This approach manipulates a pre-trained
language model to increase the leakage of private data during the fine-tuning
process. Our method enhances both membership inference and data extraction
attacks while preserving model utility. Experimental results across different
models, datasets, and fine-tuning setups demonstrate that our attacks
significantly surpass baseline performance. This work serves as a cautionary
note for users who download pre-trained models from unverified sources,
highlighting the potential risks involved.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17263v1">Privacy-Preserving Set-Based Estimation Using Differential Privacy and
  Zonotopes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T13:05:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammed M. Dawoud, Changxin Liu, Karl H. Johansson, Amr Alanwar</p>
    <p><b>Summary:</b> For large-scale cyber-physical systems, the collaboration of spatially
distributed sensors is often needed to perform the state estimation process.
Privacy concerns arise from disclosing sensitive measurements to a cloud
estimator. To solve this issue, we propose a differentially private set-based
estimation protocol that guarantees true state containment in the estimated set
and differential privacy for the sensitive measurements throughout the
set-based state estimation process within the central and local differential
privacy models. Zonotopes are employed in the proposed differentially private
set-based estimator, offering computational advantages in set operations. We
consider a plant of a non-linear discrete-time dynamical system with bounded
modeling uncertainties, sensors that provide sensitive measurements with
bounded measurement uncertainties, and a cloud estimator that predicts the
system's state. The privacy-preserving noise perturbs the centers of
measurement zonotopes, thereby concealing the precise position of these
zonotopes, i.e., ensuring privacy preservation for the sets containing
sensitive measurements. Compared to existing research, our approach achieves
less privacy loss and utility loss through the central and local differential
privacy models by leveraging a numerically optimized truncated noise
distribution. The proposed estimator is perturbed by weaker noise than the
analytical approaches in the literature to guarantee the same level of privacy,
therefore improving the estimation utility. Numerical and comparison
experiments with truncated Laplace noise are presented to support our approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17151v1">Investigating Privacy Leakage in Dimensionality Reduction Methods via
  Reconstruction Attack</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-30T09:40:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chayadon Lumbut, Donlapark Ponnoprat</p>
    <p><b>Summary:</b> This study investigates privacy leakage in dimensionality reduction methods
through a novel machine learning-based reconstruction attack. Employing an
\emph{informed adversary} threat model, we develop a neural network capable of
reconstructing high-dimensional data from low-dimensional embeddings.
  We evaluate six popular dimensionality reduction techniques: PCA, sparse
random projection (SRP), multidimensional scaling (MDS), Isomap, $t$-SNE, and
UMAP. Using both MNIST and NIH Chest X-ray datasets, we perform a qualitative
analysis to identify key factors affecting reconstruction quality. Furthermore,
we assess the effectiveness of an additive noise mechanism in mitigating these
reconstruction attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17049v1">SPOQchain: Platform for Secure, Scalable, and Privacy-Preserving Supply
  Chain Tracing and Counterfeit Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T07:15:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Moritz Finke, Alexandra Dmitrienko, Jasper Stang</p>
    <p><b>Summary:</b> Product lifecycle tracing is increasingly in the focus of regulators and
producers, as shown with the initiative of the Digital Product Pass. Likewise,
new methods of counterfeit detection are developed that are, e.g., based on
Physical Unclonable Functions (PUFs). In order to ensure trust and integrity of
product lifecycle data, multiple existing supply chain tracing systems are
built on blockchain technology. However, only few solutions employ secure
identifiers such as PUFs. Furthermore, existing systems that publish the data
of individual products, in part fully transparently, have a detrimental impact
on scalability and the privacy of users. This work proposes SPOQchain, a novel
blockchain-based platform that provides comprehensive lifecycle traceability
and originality verification while ensuring high efficiency and user privacy.
The improved efficiency is achieved by a sophisticated batching mechanism that
removes lifecycle redundancies. In addition to the successful evaluation of
SPOQchain's scalability, this work provides a comprehensive analysis of privacy
and security aspects, demonstrating the need and qualification of SPOQchain for
the future of supply chain tracing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.16913v1">Analyzing Inference Privacy Risks Through Gradients in Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-29T21:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhuohang Li, Andrew Lowy, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Bradley Malin, Ye Wang</p>
    <p><b>Summary:</b> In distributed learning settings, models are iteratively updated with shared
gradients computed from potentially sensitive user data. While previous work
has studied various privacy risks of sharing gradients, our paper aims to
provide a systematic approach to analyze private information leakage from
gradients. We present a unified game-based framework that encompasses a broad
range of attacks including attribute, property, distributional, and user
disclosures. We investigate how different uncertainties of the adversary affect
their inferential power via extensive experiments on five datasets across
various data modalities. Our results demonstrate the inefficacy of solely
relying on data aggregation to achieve privacy against inference attacks in
distributed learning. We further evaluate five types of defenses, namely,
gradient pruning, signed gradient descent, adversarial perturbations,
variational information bottleneck, and differential privacy, under both static
and adaptive adversary settings. We provide an information-theoretic view for
analyzing the effectiveness of these defenses against inference from gradients.
Finally, we introduce a method for auditing attribute inference privacy,
improving the empirical estimation of worst-case privacy through crafting
adversarial canary records.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00138v1">PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in
  Action</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-29T17:58:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yijia Shao, Tianshi Li, Weiyan Shi, Yanchen Liu, Diyi Yang</p>
    <p><b>Summary:</b> As language models (LMs) are widely utilized in personalized communication
scenarios (e.g., sending emails, writing social media posts) and endowed with a
certain level of agency, ensuring they act in accordance with the contextual
privacy norms becomes increasingly critical. However, quantifying the privacy
norm awareness of LMs and the emerging privacy risk in LM-mediated
communication is challenging due to (1) the contextual and long-tailed nature
of privacy-sensitive cases, and (2) the lack of evaluation approaches that
capture realistic application scenarios. To address these challenges, we
propose PrivacyLens, a novel framework designed to extend privacy-sensitive
seeds into expressive vignettes and further into agent trajectories, enabling
multi-level evaluation of privacy leakage in LM agents' actions. We instantiate
PrivacyLens with a collection of privacy norms grounded in privacy literature
and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM
performance in answering probing questions and their actual behavior when
executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4
and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even
when prompted with privacy-enhancing instructions. We also demonstrate the
dynamic nature of PrivacyLens by extending each seed into multiple trajectories
to red-team LM privacy leakage risk. Dataset and code are available at
https://github.com/SALT-NLP/PrivacyLens.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.16304v1">Understanding Privacy Norms through Web Forms</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-29T07:11:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Cui, Rahmadi Trimananda, Athina Markopoulou</p>
    <p><b>Summary:</b> Web forms are one of the primary ways to collect personal information online,
yet they are relatively under-studied. Unlike web tracking, data collection
through web forms is explicit and contextualized. Users (i) are asked to input
specific personal information types, and (ii) know the specific context (i.e.,
on which website and for what purpose). For web forms to be trusted by users,
they must meet the common sense standards of appropriate data collection
practices within a particular context (i.e., privacy norms). In this paper, we
extract the privacy norms embedded within web forms through a measurement
study. First, we build a specialized crawler to discover web forms on websites.
We run it on 11,500 popular websites, and we create a dataset of 293K web
forms. Second, to process data of this scale, we develop a cost-efficient way
to annotate web forms with form types and personal information types, using
text classifiers trained with assistance of large language models (LLMs).
Third, by analyzing the annotated dataset, we reveal common patterns of data
collection practices. We find that (i) these patterns are explained by
functional necessities and legal obligations, thus reflecting privacy norms,
and that (ii) deviations from the observed norms often signal unnecessary data
collection. In addition, we analyze the privacy policies that accompany web
forms. We show that, despite their wide adoption and use, there is a disconnect
between privacy policy disclosures and the observed privacy norms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15694v1">Protecting Privacy in Federated Time Series Analysis: A Pragmatic
  Technology Review for Application Developers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-28T10:41:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Bachlechner, Ruben Hetfleisch, Stephan Krenn, Thomas Lorünser, Michael Rader</p>
    <p><b>Summary:</b> The federated analysis of sensitive time series has huge potential in various
domains, such as healthcare or manufacturing. Yet, to fully unlock this
potential, requirements imposed by various stakeholders must be fulfilled,
regarding, e.g., efficiency or trust assumptions. While many of these
requirements can be addressed by deploying advanced secure computation
paradigms such as fully homomorphic encryption, certain aspects require an
integration with additional privacy-preserving technologies.
  In this work, we perform a qualitative requirements elicitation based on
selected real-world use cases. We match the derived requirements categories
against the features and guarantees provided by available technologies. For
each technology, we additionally perform a maturity assessment, including the
state of standardization and availability on the market. Furthermore, we
provide a decision tree supporting application developers in identifying the
most promising technologies available matching their needs. Finally, existing
gaps are identified, highlighting research potential to advance the field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15688v1">PDSR: A Privacy-Preserving Diversified Service Recommendation Method on
  Distributed Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-08-28T10:25:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lina Wang, Huan Yang, Yiran Shen, Chao Liu, Lianyong Qi, Xiuzhen Cheng, Feng Li</p>
    <p><b>Summary:</b> The last decade has witnessed a tremendous growth of service computing, while
efficient service recommendation methods are desired to recommend high-quality
services to users. It is well known that collaborative filtering is one of the
most popular methods for service recommendation based on QoS, and many existing
proposals focus on improving recommendation accuracy, i.e., recommending
high-quality redundant services. Nevertheless, users may have different
requirements on QoS, and hence diversified recommendation has been attracting
increasing attention in recent years to fulfill users' diverse demands and to
explore potential services. Unfortunately, the recommendation performances
relies on a large volume of data (e.g., QoS data), whereas the data may be
distributed across multiple platforms. Therefore, to enable data sharing across
the different platforms for diversified service recommendation, we propose a
Privacy-preserving Diversified Service Recommendation (PDSR) method.
Specifically, we innovate in leveraging the Locality-Sensitive Hashing (LSH)
mechanism such that privacy-preserved data sharing across different platforms
is enabled to construct a service similarity graph. Based on the similarity
graph, we propose a novel accuracy-diversity metric and design a
$2$-approximation algorithm to select $K$ services to recommend by maximizing
the accuracy-diversity measure. Extensive experiments on real datasets are
conducted to verify the efficacy of our PDSR method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15621v1">Convergent Differential Privacy Analysis for General Federated Learning:
  the f-DP Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-28T08:22:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yan Sun, Li Shen, Dacheng Tao</p>
    <p><b>Summary:</b> Federated learning (FL) is an efficient collaborative training paradigm
extensively developed with a focus on local privacy protection, and
differential privacy (DP) is a classical approach to capture and ensure the
reliability of local privacy. The powerful cooperation of FL and DP provides a
promising learning framework for large-scale private clients, juggling both
privacy securing and trustworthy learning. As the predominant algorithm of DP,
the noisy perturbation has been widely studied and incorporated into various
federated algorithms, theoretically proven to offer significant privacy
protections. However, existing analyses in noisy FL-DP mostly rely on the
composition theorem and cannot tightly quantify the privacy leakage challenges,
which is nearly tight for small numbers of communication rounds but yields an
arbitrarily loose and divergent bound under the large communication rounds.
This implies a counterintuitive judgment, suggesting that FL may not provide
adequate privacy protection during long-term training. To further investigate
the convergent privacy and reliability of the FL-DP framework, in this paper,
we comprehensively evaluate the worst privacy of two classical methods under
the non-convex and smooth objectives based on the f-DP analysis, i.e.
Noisy-FedAvg and Noisy-FedProx methods. With the aid of the
shifted-interpolation technique, we successfully prove that the worst privacy
of the Noisy-FedAvg method achieves a tight convergent lower bound. Moreover,
in the Noisy-FedProx method, with the regularization of the proxy term, the
worst privacy has a stable constant lower bound. Our analysis further provides
a solid theoretical foundation for the reliability of privacy protection in
FL-DP. Meanwhile, our conclusions can also be losslessly converted to other
classical DP analytical frameworks, e.g. $(\epsilon,\delta)$-DP and
R$\acute{\text{e}}$nyi-DP (RDP).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15391v2">Examining the Interplay Between Privacy and Fairness for Speech
  Processing: A Review and Perspective</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">
  <p><b>Published on:</b> 2024-08-27T20:32:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anna Leschanowsky, Sneha Das</p>
    <p><b>Summary:</b> Speech technology has been increasingly deployed in various areas of daily
life including sensitive domains such as healthcare and law enforcement. For
these technologies to be effective, they must work reliably for all users while
preserving individual privacy. Although tradeoffs between privacy and utility,
as well as fairness and utility, have been extensively researched, the specific
interplay between privacy and fairness in speech processing remains
underexplored. This review and position paper offers an overview of emerging
privacy-fairness tradeoffs throughout the entire machine learning lifecycle for
speech processing. By drawing on well-established frameworks on fairness and
privacy, we examine existing biases and sources of privacy harm that coexist
during the development of speech processing models. We then highlight how
corresponding privacy-enhancing technologies have the potential to
inadvertently increase these biases and how bias mitigation strategies may
conversely reduce privacy. By raising open questions, we advocate for a
comprehensive evaluation of privacy-fairness tradeoffs for speech technology
and the development of privacy-enhancing and fairness-aware algorithms in this
domain.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15077v2">MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of
  Children with Autism Spectrum Disorder</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-27T14:05:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pavan Uttej Ravva, Behdokht Kiafar, Pinar Kullu, Jicheng Li, Anjana Bhat, Roghayeh Leila Barmaki</p>
    <p><b>Summary:</b> Autism spectrum disorder (ASD) is characterized by significant challenges in
social interaction and comprehending communication signals. Recently,
therapeutic interventions for ASD have increasingly utilized Deep learning
powered-computer vision techniques to monitor individual progress over time.
These models are trained on private, non-public datasets from the autism
community, creating challenges in comparing results across different models due
to privacy-preserving data-sharing issues. This work introduces MMASD+, an
enhanced version of the novel open-source dataset called Multimodal ASD
(MMASD). MMASD+ consists of diverse data modalities, including 3D-Skeleton, 3D
Body Mesh, and Optical Flow data. It integrates the capabilities of Yolov8 and
Deep SORT algorithms to distinguish between the therapist and children,
addressing a significant barrier in the original dataset. Additionally, a
Multimodal Transformer framework is proposed to predict 11 action types and the
presence of ASD. This framework achieves an accuracy of 95.03% for predicting
action types and 96.42% for predicting ASD presence, demonstrating over a 10%
improvement compared to models trained on single data modalities. These
findings highlight the advantages of integrating multiple data modalities
within the Multimodal Transformer framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14830v1">PolicyLR: A Logic Representation For Privacy Policies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-08-27T07:27:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ashish Hooda, Rishabh Khandelwal, Prasad Chalasani, Kassem Fawaz, Somesh Jha</p>
    <p><b>Summary:</b> Privacy policies are crucial in the online ecosystem, defining how services
handle user data and adhere to regulations such as GDPR and CCPA. However,
their complexity and frequent updates often make them difficult for
stakeholders to understand and analyze. Current automated analysis methods,
which utilize natural language processing, have limitations. They typically
focus on individual tasks and fail to capture the full context of the policies.
We propose PolicyLR, a new paradigm that offers a comprehensive
machine-readable representation of privacy policies, serving as an all-in-one
solution for multiple downstream tasks. PolicyLR converts privacy policies into
a machine-readable format using valuations of atomic formulae, allowing for
formal definitions of tasks like compliance and consistency. We have developed
a compiler that transforms unstructured policy text into this format using
off-the-shelf Large Language Models (LLMs). This compiler breaks down the
transformation task into a two-stage translation and entailment procedure. This
procedure considers the full context of the privacy policy to infer a complex
formula, where each formula consists of simpler atomic formulae. The advantage
of this model is that PolicyLR is interpretable by design and grounded in
segments of the privacy policy. We evaluated the compiler using ToS;DR, a
community-annotated privacy policy entailment dataset. Utilizing open-source
LLMs, our compiler achieves precision and recall values of 0.91 and 0.88,
respectively. Finally, we demonstrate the utility of PolicyLR in three privacy
tasks: Policy Compliance, Inconsistency Detection, and Privacy Comparison
Shopping.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14753v1">CoopASD: Cooperative Machine Anomalous Sound Detection with Privacy
  Concerns</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> 
  <p><b>Published on:</b> 2024-08-27T03:07:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anbai Jiang, Yuchen Shi, Pingyi Fan, Wei-Qiang Zhang, Jia Liu</p>
    <p><b>Summary:</b> Machine anomalous sound detection (ASD) has emerged as one of the most
promising applications in the Industrial Internet of Things (IIoT) due to its
unprecedented efficacy in mitigating risks of malfunctions and promoting
production efficiency. Previous works mainly investigated the machine ASD task
under centralized settings. However, developing the ASD system under
decentralized settings is crucial in practice, since the machine data are
dispersed in various factories and the data should not be explicitly shared due
to privacy concerns. To enable these factories to cooperatively develop a
scalable ASD model while preserving their privacy, we propose a novel framework
named CoopASD, where each factory trains an ASD model on its local dataset, and
a central server aggregates these local models periodically. We employ a
pre-trained model as the backbone of the ASD model to improve its robustness
and develop specialized techniques to stabilize the model under a completely
non-iid and domain shift setting. Compared with previous state-of-the-art
(SOTA) models trained in centralized settings, CoopASD showcases competitive
results with negligible degradation of 0.08%. We also conduct extensive
ablation studies to demonstrate the effectiveness of CoopASD.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14735v1">PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework
  with Correlated Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-27T02:03:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Miao Hu, Linchang Xiao</p>
    <p><b>Summary:</b> Online video streaming has evolved into an integral component of the
contemporary Internet landscape. Yet, the disclosure of user requests presents
formidable privacy challenges. As users stream their preferred online videos,
their requests are automatically seized by video content providers, potentially
leaking users' privacy.
  Unfortunately, current protection methods are not well-suited to preserving
user request privacy from content providers while maintaining high-quality
online video services. To tackle this challenge, we introduce a novel
Privacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge
devices to pre-fetch and cache videos, ensuring the privacy of users' requests
while optimizing the efficiency of edge caching. More specifically, we design
PPVF with three core components: (1) \textit{Online privacy budget scheduler},
which employs a theoretically guaranteed online algorithm to select
non-requested videos as candidates with assigned privacy budgets. Alternative
videos are chosen by an online algorithm that is theoretically guaranteed to
consider both video utilities and available privacy budgets. (2) \textit{Noisy
video request generator}, which generates redundant video requests (in addition
to original ones) utilizing correlated differential privacy to obfuscate
request privacy. (3) \textit{Online video utility predictor}, which leverages
federated learning to collaboratively evaluate video utility in an online
fashion, aiding in video selection in (1) and noise generation in (2). Finally,
we conduct extensive experiments using real-world video request traces from
Tencent Video. The results demonstrate that PPVF effectively safeguards user
request privacy while upholding high video caching performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14689v1">Federated User Preference Modeling for Privacy-Preserving Cross-Domain
  Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-08-26T23:29:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Li Wang, Shoujin Wang, Quangui Zhang, Qiang Wu, Min Xu</p>
    <p><b>Summary:</b> Cross-domain recommendation (CDR) aims to address the data-sparsity problem
by transferring knowledge across domains. Existing CDR methods generally assume
that the user-item interaction data is shareable between domains, which leads
to privacy leakage. Recently, some privacy-preserving CDR (PPCDR) models have
been proposed to solve this problem. However, they primarily transfer simple
representations learned only from user-item interaction histories, overlooking
other useful side information, leading to inaccurate user preferences.
Additionally, they transfer differentially private user-item interaction
matrices or embeddings across domains to protect privacy. However, these
methods offer limited privacy protection, as attackers may exploit external
information to infer the original data. To address these challenges, we propose
a novel Federated User Preference Modeling (FUPM) framework. In FUPM, first, a
novel comprehensive preference exploration module is proposed to learn users'
comprehensive preferences from both interaction data and additional data
including review texts and potentially positive items. Next, a private
preference transfer module is designed to first learn differentially private
local and global prototypes, and then privately transfer the global prototypes
using a federated learning strategy. These prototypes are generalized
representations of user groups, making it difficult for attackers to infer
individual information. Extensive experiments on four CDR tasks conducted on
the Amazon and Douban datasets validate the superiority of FUPM over SOTA
baselines. Code is available at https://github.com/Lili1013/FUPM.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14329v1">PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection
  Dataset</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-26T14:55:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ghazal Alinezhad Noghre, Shanle Yao, Armin Danesh Pazho, Babak Rahimi Ardabili, Vinit Katariya, Hamed Tabkhi</p>
    <p><b>Summary:</b> PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection
dataset. By removing pixel information and providing only de-identified human
annotations, PHEVA safeguards personally identifiable information. The dataset
includes seven indoor/outdoor scenes, featuring one novel, context-specific
camera, and offers over 5x the pose-annotated frames compared to the largest
previous dataset. This study benchmarks state-of-the-art methods on PHEVA using
a comprehensive set of metrics, including the 10% Error Rate (10ER), a metric
used for anomaly detection for the first time providing insights relevant to
real-world deployment. As the first of its kind, PHEVA bridges the gap between
conventional training and real-world deployment by introducing continual
learning benchmarks, with models outperforming traditional methods in 82.14% of
cases. The dataset is publicly available at
https://github.com/TeCSAR-UNCC/PHEVA.git.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13460v1">DOPPLER: Differentially Private Optimizers with Low-pass Filter for
  Privacy Noise Reduction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-24T04:27:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinwei Zhang, Zhiqi Bu, Mingyi Hong, Meisam Razaviyayn</p>
    <p><b>Summary:</b> Privacy is a growing concern in modern deep-learning systems and
applications. Differentially private (DP) training prevents the leakage of
sensitive information in the collected training data from the trained machine
learning models. DP optimizers, including DP stochastic gradient descent
(DPSGD) and its variants, privatize the training procedure by gradient clipping
and DP noise injection. However, in practice, DP models trained using DPSGD and
its variants often suffer from significant model performance degradation. Such
degradation prevents the application of DP optimization in many key tasks, such
as foundation model pretraining. In this paper, we provide a novel signal
processing perspective to the design and analysis of DP optimizers. We show
that a ``frequency domain'' operation called low-pass filtering can be used to
effectively reduce the impact of DP noise. More specifically, by defining the
``frequency domain'' for both the gradient and differential privacy (DP) noise,
we have developed a new component, called DOPPLER. This component is designed
for DP algorithms and works by effectively amplifying the gradient while
suppressing DP noise within this frequency domain. As a result, it maintains
privacy guarantees and enhances the quality of the DP-protected model. Our
experiments show that the proposed DP optimizers with a low-pass filter
outperform their counterparts without the filter by 3%-10% in test accuracy on
various models and datasets. Both theoretical and practical evidence suggest
that the DOPPLER is effective in closing the gap between DP and non-DP
training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13424v1">Enabling Humanitarian Applications with Targeted Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-24T01:34:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nitin Kohli, Joshua Blumenstock</p>
    <p><b>Summary:</b> The proliferation of mobile phones in low- and middle-income countries has
suddenly and dramatically increased the extent to which the world's poorest and
most vulnerable populations can be observed and tracked by governments and
corporations. Millions of historically "off the grid" individuals are now
passively generating digital data; these data, in turn, are being used to make
life-altering decisions about those individuals -- including whether or not
they receive government benefits, and whether they qualify for a consumer loan.
  This paper develops an approach to implementing algorithmic decisions based
on personal data, while also providing formal privacy guarantees to data
subjects. The approach adapts differential privacy to applications that require
decisions about individuals, and gives decision makers granular control over
the level of privacy guaranteed to data subjects. We show that stronger privacy
guarantees typically come at some cost, and use data from two real-world
applications -- an anti-poverty program in Togo and a consumer lending platform
in Nigeria -- to illustrate those costs. Our empirical results quantify the
tradeoff between privacy and predictive accuracy, and characterize how
different privacy guarantees impact overall program effectiveness. More
broadly, our results demonstrate a way for humanitarian programs to responsibly
use personal data, and better equip program designers to make informed
decisions about data privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13038v1">Improving the Classification Effect of Clinical Images of Diseases for
  Multi-Source Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-23T12:52:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tian Bowen, Xu Zhengyang, Yin Zhihao, Wang Jingying, Yue Yutao</p>
    <p><b>Summary:</b> Privacy data protection in the medical field poses challenges to data
sharing, limiting the ability to integrate data across hospitals for training
high-precision auxiliary diagnostic models. Traditional centralized training
methods are difficult to apply due to violations of privacy protection
principles. Federated learning, as a distributed machine learning framework,
helps address this issue, but it requires multiple hospitals to participate in
training simultaneously, which is hard to achieve in practice. To address these
challenges, we propose a medical privacy data training framework based on data
vectors. This framework allows each hospital to fine-tune pre-trained models on
private data, calculate data vectors (representing the optimization direction
of model parameters in the solution space), and sum them up to generate
synthetic weights that integrate model information from multiple hospitals.
This approach enhances model performance without exchanging private data or
requiring synchronous training. Experimental results demonstrate that this
method effectively utilizes dispersed private data resources while protecting
patient privacy. The auxiliary diagnostic model trained using this approach
significantly outperforms models trained independently by a single hospital,
providing a new perspective for resolving the conflict between medical data
privacy protection and model training and advancing the development of medical
intelligence.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12787v2">LLM-PBE: Assessing Data Privacy in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-23T01:37:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qinbin Li, Junyuan Hong, Chulin Xie, Jeffrey Tan, Rachel Xin, Junyi Hou, Xavier Yin, Zhun Wang, Dan Hendrycks, Zhangyang Wang, Bo Li, Bingsheng He, Dawn Song</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have become integral to numerous domains,
significantly advancing applications in data management, mining, and analysis.
Their profound capabilities in processing and interpreting complex language
data, however, bring to light pressing concerns regarding data privacy,
especially the risk of unintentional training data leakage. Despite the
critical nature of this issue, there has been no existing literature to offer a
comprehensive assessment of data privacy risks in LLMs. Addressing this gap,
our paper introduces LLM-PBE, a toolkit crafted specifically for the systematic
evaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze
privacy across the entire lifecycle of LLMs, incorporating diverse attack and
defense strategies, and handling various data types and metrics. Through
detailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth
exploration of data privacy concerns, shedding light on influential factors
such as model size, data characteristics, and evolving temporal dimensions.
This study not only enriches the understanding of privacy issues in LLMs but
also serves as a vital resource for future research in the field. Aimed at
enhancing the breadth of knowledge in this area, the findings, resources, and
our full technical report are made available at https://llm-pbe.github.io/,
providing an open platform for academic and practical advancements in LLM
privacy assessment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12385v1">Sharper Bounds for Chebyshev Moment Matching with Applications to
  Differential Privacy and Beyond</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-22T13:26:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Cameron Musco, Christopher Musco, Lucas Rosenblatt, Apoorv Vikram Singh</p>
    <p><b>Summary:</b> We study the problem of approximately recovering a probability distribution
given noisy measurements of its Chebyshev polynomial moments. We sharpen prior
work, proving that accurate recovery in the Wasserstein distance is possible
with more noise than previously known.
  As a main application, our result yields a simple "linear query" algorithm
for constructing a differentially private synthetic data distribution with
Wasserstein-1 error $\tilde{O}(1/n)$ based on a dataset of $n$ points in
$[-1,1]$. This bound is optimal up to log factors and matches a recent
breakthrough of Boedihardjo, Strohmer, and Vershynin [Probab. Theory. Rel.,
2024], which uses a more complex "superregular random walk" method to beat an
$O(1/\sqrt{n})$ accuracy barrier inherent to earlier approaches.
  We illustrate a second application of our new moment-based recovery bound in
numerical linear algebra: by improving an approach of Braverman, Krishnan, and
Musco [STOC 2022], our result yields a faster algorithm for estimating the
spectral density of a symmetric matrix up to small error in the Wasserstein
distance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12353v1">Distributed quasi-Newton robust estimation under differential privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-22T12:51:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chuhan Wang, Lixing Zhu, Xuehu Zhu</p>
    <p><b>Summary:</b> For distributed computing with Byzantine machines under Privacy Protection
(PP) constraints, this paper develops a robust PP distributed quasi-Newton
estimation, which only requires the node machines to transmit five vectors to
the central processor with high asymptotic relative efficiency. Compared with
the gradient descent strategy which requires more rounds of transmission and
the Newton iteration strategy which requires the entire Hessian matrix to be
transmitted, the novel quasi-Newton iteration has advantages in reducing
privacy budgeting and transmission cost. Moreover, our PP algorithm does not
depend on the boundedness of gradients and second-order derivatives. When
gradients and second-order derivatives follow sub-exponential distributions, we
offer a mechanism that can ensure PP with a sufficiently high probability.
Furthermore, this novel estimator can achieve the optimal convergence rate and
the asymptotic normality. The numerical studies on synthetic and real data sets
evaluate the performance of the proposed algorithm.</p>
  </details>
</div>



<h2>2024-09</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.11845v1">Law-based and standards-oriented approach for privacy impact assessment
  in medical devices: a topic for lawyers, engineers and healthcare
  practitioners in MedTech</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-09-18T09:56:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuri R. Ladeia, David M. Pereira</p>
    <p><b>Summary:</b> Background: The integration of the General Data Protection Regulation (GDPR)
and the Medical Device Regulation (MDR) creates complexities in conducting Data
Protection Impact Assessments (DPIAs) for medical devices. The adoption of
non-binding standards like ISO and IEC can harmonize these processes by
enhancing accountability and privacy by design. Methods: This study employs a
multidisciplinary literature review, focusing on GDPR and MDR intersection in
medical devices that process personal health data. It evaluates key standards,
including ISO/IEC 29134 and IEC 62304, to propose a unified approach for DPIAs
that aligns with legal and technical frameworks. Results: The analysis reveals
the benefits of integrating ISO/IEC standards into DPIAs, which provide
detailed guidance on implementing privacy by design, risk assessment, and
mitigation strategies specific to medical devices. The proposed framework
ensures that DPIAs are living documents, continuously updated to adapt to
evolving data protection challenges. Conclusions: A unified approach combining
European Union (EU) regulations and international standards offers a robust
framework for conducting DPIAs in medical devices. This integration balances
security, innovation, and privacy, enhancing compliance and fostering trust in
medical technologies. The study advocates for leveraging both hard law and
standards to systematically address privacy and safety in the design and
operation of medical devices, thereby raising the maturity of the MedTech
ecosystem.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.11805v1">Inside Out or Not: Privacy Implications of Emotional Disclosure</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-09-18T08:42:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Elham Naghizade, Kaixin Ji, Benjamin Tag, Flora Salim</p>
    <p><b>Summary:</b> Privacy is dynamic, sensitive, and contextual, much like our emotions.
Previous studies have explored the interplay between privacy and context,
privacy and emotion, and emotion and context. However, there remains a
significant gap in understanding the interplay of these aspects simultaneously.
In this paper, we present a preliminary study investigating the role of
emotions in driving individuals' information sharing behaviour, particularly in
relation to urban locations and social ties. We adopt a novel methodology that
integrates context (location and time), emotion, and personal information
sharing behaviour, providing a comprehensive analysis of how contextual
emotions affect privacy. The emotions are assessed with both self-reporting and
electrodermal activity (EDA). Our findings reveal that self-reported emotions
influence personal information-sharing behaviour with distant social groups,
while neutral emotions lead individuals to share less precise information with
close social circles, a pattern is potentially detectable with wrist-worn EDA.
Our study helps lay the foundation for personalised emotion-aware strategies to
mitigate oversharing risks and enhance user privacy in the digital age.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.11680v1">What to Consider When Considering Differential Privacy for Policy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-18T03:41:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Priyanka Nanayakkara, Jessica Hullman</p>
    <p><b>Summary:</b> Differential privacy (DP) is a mathematical definition of privacy that can be
widely applied when publishing data. DP has been recognized as a potential
means of adhering to various privacy-related legal requirements. However, it
can be difficult to reason about whether DP may be appropriate for a given
context due to tensions that arise when it is brought from theory into
practice. To aid policymaking around privacy concerns, we identify three
categories of challenges to understanding DP along with associated questions
that policymakers can ask about the potential deployment context to anticipate
its impacts.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.11663v2">GReDP: A More Robust Approach for Differential Private Training with
  Gradient-Preserving Noise Reduction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-09-18T03:01:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haodi Wang, Tangyu Jiang, Yu Guo, Chengjun Cai, Cong Wang, Xiaohua Jia</p>
    <p><b>Summary:</b> Deep learning models have been extensively adopted in various regions due to
their ability to represent hierarchical features, which highly rely on the
training set and procedures. Thus, protecting the training process and deep
learning algorithms is paramount in privacy preservation. Although Differential
Privacy (DP) as a powerful cryptographic primitive has achieved satisfying
results in deep learning training, the existing schemes still fall short in
preserving model utility, i.e., they either invoke a high noise scale or
inevitably harm the original gradients. To address the above issues, in this
paper, we present a more robust approach for DP training called GReDP.
Specifically, we compute the model gradients in the frequency domain and adopt
a new approach to reduce the noise level. Unlike the previous work, our GReDP
only requires half of the noise scale compared to DPSGD [1] while keeping all
the gradient information intact. We present a detailed analysis of our method
both theoretically and empirically. The experimental results show that our
GReDP works consistently better than the baselines on all models and training
settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.11536v1">Obfuscation Based Privacy Preserving Representations are Recoverable
  Using Neighborhood Information</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-09-17T20:13:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kunal Chelani, Assia Benbihi, Fredrik Kahl, Torsten Sattler, Zuzana Kukelova</p>
    <p><b>Summary:</b> Rapid growth in the popularity of AR/VR/MR applications and cloud-based
visual localization systems has given rise to an increased focus on the privacy
of user content in the localization process.
  This privacy concern has been further escalated by the ability of deep neural
networks to recover detailed images of a scene from a sparse set of 3D or 2D
points and their descriptors - the so-called inversion attacks.
  Research on privacy-preserving localization has therefore focused on
preventing these inversion attacks on both the query image keypoints and the 3D
points of the scene map.
  To this end, several geometry obfuscation techniques that lift points to
higher-dimensional spaces, i.e., lines or planes, or that swap coordinates
between points % have been proposed.
  In this paper, we point to a common weakness of these obfuscations that
allows to recover approximations of the original point positions under the
assumption of known neighborhoods.
  We further show that these neighborhoods can be computed by learning to
identify descriptors that co-occur in neighborhoods.
  Extensive experiments show that our approach for point recovery is
practically applicable to all existing geometric obfuscation schemes.
  Our results show that these schemes should not be considered
privacy-preserving, even though they are claimed to be privacy-preserving.
  Code will be available at
\url{https://github.com/kunalchelani/RecoverPointsNeighborhood}.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.11295v1">EIA: Environmental Injection Attack on Generalist Web Agents for Privacy
  Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-17T15:49:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun</p>
    <p><b>Summary:</b> Generalist web agents have evolved rapidly and demonstrated remarkable
potential. However, there are unprecedented safety risks associated with these
them, which are nearly unexplored so far. In this work, we aim to narrow this
gap by conducting the first study on the privacy risks of generalist web agents
in adversarial environments. First, we present a threat model that discusses
the adversarial targets, constraints, and attack scenarios. Particularly, we
consider two types of adversarial targets: stealing users' specific personally
identifiable information (PII) or stealing the entire user request. To achieve
these objectives, we propose a novel attack method, termed Environmental
Injection Attack (EIA). This attack injects malicious content designed to adapt
well to different environments where the agents operate, causing them to
perform unintended actions. This work instantiates EIA specifically for the
privacy scenario. It inserts malicious web elements alongside persuasive
instructions that mislead web agents into leaking private information, and can
further leverage CSS and JavaScript features to remain stealthy. We collect 177
actions steps that involve diverse PII categories on realistic websites from
the Mind2Web dataset, and conduct extensive experiments using one of the most
capable generalist web agent frameworks to date, SeeAct. The results
demonstrate that EIA achieves up to 70% ASR in stealing users' specific PII.
Stealing full user requests is more challenging, but a relaxed version of EIA
can still achieve 16% ASR. Despite these concerning results, it is important to
note that the attack can still be detectable through careful human inspection,
highlighting a trade-off between high autonomy and security. This leads to our
detailed discussion on the efficacy of EIA under different levels of human
supervision as well as implications on defenses for generalist web agents.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.10667v1">Benchmarking Secure Sampling Protocols for Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-16T19:04:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yucheng Fu, Tianhao Wang</p>
    <p><b>Summary:</b> Differential privacy (DP) is widely employed to provide privacy protection
for individuals by limiting information leakage from the aggregated data. Two
well-known models of DP are the central model and the local model. The former
requires a trustworthy server for data aggregation, while the latter requires
individuals to add noise, significantly decreasing the utility of aggregated
results. Recently, many studies have proposed to achieve DP with Secure
Multi-party Computation (MPC) in distributed settings, namely, the distributed
model, which has utility comparable to central model while, under specific
security assumptions, preventing parties from obtaining others' information.
One challenge of realizing DP in distributed model is efficiently sampling
noise with MPC. Although many secure sampling methods have been proposed, they
have different security assumptions and isolated theoretical analyses. There is
a lack of experimental evaluations to measure and compare their performances.
We fill this gap by benchmarking existing sampling protocols in MPC and
performing comprehensive measurements of their efficiency. First, we present a
taxonomy of the underlying techniques of these sampling protocols. Second, we
extend widely used distributed noise generation protocols to be resilient
against Byzantine attackers. Third, we implement discrete sampling protocols
and align their security settings for a fair comparison. We then conduct an
extensive evaluation to study their efficiency and utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.10411v1">A Large-Scale Privacy Assessment of Android Third-Party SDKs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-09-16T15:44:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mark Huasong Meng, Chuan Yan, Yun Hao, Qing Zhang, Zeyu Wang, Kailong Wang, Sin Gee Teo, Guangdong Bai, Jin Song Dong</p>
    <p><b>Summary:</b> Third-party Software Development Kits (SDKs) are widely adopted in Android
app development, to effortlessly accelerate development pipelines and enhance
app functionality. However, this convenience raises substantial concerns about
unauthorized access to users' privacy-sensitive information, which could be
further abused for illegitimate purposes like user tracking or monetization.
Our study offers a targeted analysis of user privacy protection among Android
third-party SDKs, filling a critical gap in the Android software supply chain.
It focuses on two aspects of their privacy practices, including data
exfiltration and behavior-policy compliance (or privacy compliance), utilizing
techniques of taint analysis and large language models. It covers 158
widely-used SDKs from two key SDK release platforms, the official one and a
large alternative one. From them, we identified 338 instances of privacy data
exfiltration. On the privacy compliance, our study reveals that more than 30%
of the examined SDKs fail to provide a privacy policy to disclose their data
handling practices. Among those that provide privacy policies, 37% of them
over-collect user data, and 88% falsely claim access to sensitive data. We
revisit the latest versions of the SDKs after 12 months. Our analysis
demonstrates a persistent lack of improvement in these concerning trends. Based
on our findings, we propose three actionable recommendations to mitigate the
privacy leakage risks and enhance privacy protection for Android users. Our
research not only serves as an urgent call for industry attention but also
provides crucial insights for future regulatory interventions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.10337v1">Security, Trust and Privacy challenges in AI-driven 6G Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-09-16T14:48:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Helena Rifa-Pous, Victor Garcia-Font, Carlos Nunez-Gomez, Julian Salas</p>
    <p><b>Summary:</b> The advent of 6G networks promises unprecedented advancements in wireless
communication, offering wider bandwidth and lower latency compared to its
predecessors. This article explores the evolving infrastructure of 6G networks,
emphasizing the transition towards a more disaggregated structure and the
integration of artificial intelligence (AI) technologies. Furthermore, it
explores the security, trust and privacy challenges and attacks in 6G networks,
particularly those related to the use of AI. It presents a classification of
network attacks stemming from its AI-centric architecture and explores
technologies designed to detect or mitigate these emerging threats. The paper
concludes by examining the implications and risks linked to the utilization of
AI in ensuring a robust network.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.10226v1">Privacy-Preserving Distributed Maximum Consensus Without Accuracy Loss</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">  
  <p><b>Published on:</b> 2024-09-16T12:21:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenrui Yu, Richard Heusdens, Jun Pang, Qiongxiu Li</p>
    <p><b>Summary:</b> In distributed networks, calculating the maximum element is a fundamental
task in data analysis, known as the distributed maximum consensus problem.
However, the sensitive nature of the data involved makes privacy protection
essential. Despite its importance, privacy in distributed maximum consensus has
received limited attention in the literature. Traditional privacy-preserving
methods typically add noise to updates, degrading the accuracy of the final
result. To overcome these limitations, we propose a novel distributed
optimization-based approach that preserves privacy without sacrificing
accuracy. Our method introduces virtual nodes to form an augmented graph and
leverages a carefully designed initialization process to ensure the privacy of
honest participants, even when all their neighboring nodes are dishonest.
Through a comprehensive information-theoretical analysis, we derive a
sufficient condition to protect private data against both passive and
eavesdropping adversaries. Extensive experiments validate the effectiveness of
our approach, demonstrating that it not only preserves perfect privacy but also
maintains accuracy, outperforming existing noise-based methods that typically
suffer from accuracy loss.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.10192v1">PrePaMS: Privacy-Preserving Participant Management System for Studies
  with Rewards and Prerequisites</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-16T11:35:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Echo Meißner, Frank Kargl, Benjamin Erb, Felix Engelmann</p>
    <p><b>Summary:</b> Taking part in surveys, experiments, and studies is often compensated by
rewards to increase the number of participants and encourage attendance. While
privacy requirements are usually considered for participation, privacy aspects
of the reward procedure are mostly ignored. To this end, we introduce PrePaMS,
an efficient participation management system that supports prerequisite checks
and participation rewards in a privacy-preserving way. Our system organizes
participations with potential (dis-)qualifying dependencies and enables secure
reward payoffs. By leveraging a set of proven cryptographic primitives and
mechanisms such as anonymous credentials and zero-knowledge proofs,
participations are protected so that service providers and organizers cannot
derive the identity of participants even within the reward process. In this
paper, we have designed and implemented a prototype of PrePaMS to show its
effectiveness and evaluated its performance under realistic workloads. PrePaMS
covers the information whether subjects have participated in surveys,
experiments, or studies. When combined with other secure solutions for the
actual data collection within these events, PrePaMS can represent a cornerstone
for more privacy-preserving empirical research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.10057v1">A Response to: A Note on "Privacy Preserving n-Party Scalar Product
  Protocol"</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-16T07:36:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Florian van Daalen, Lianne Ippel, Andre Dekker, Inigo Bermejo</p>
    <p><b>Summary:</b> We reply to the comments on our proposed privacy preserving n-party scalar
product protocol made by Liu. In their comment Liu raised concerns regarding
the security and scalability of the $n$-party scalar product protocol. In this
reply, we show that their concerns are unfounded and that the $n$-party scalar
product protocol is safe for its intended purposes. Their concerns regarding
the security are based on a misunderstanding of the protocol. Additionally,
while the scalability of the protocol puts limitations on its use, the protocol
still has numerous practical applications when applied in the correct
scenarios. Specifically within vertically partitioned scenarios, which often
involve few parties, the protocol remains practical. In this reply we clarify
Liu's misunderstanding. Additionally, we explain why the protocols scaling is
not a practical problem in its intended application.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.09972v1">Securing the Future: Exploring Privacy Risks and Security Questions in
  Robotic Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-09-16T04:10:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Diba Afroze, Yazhou Tu, Xiali Hei</p>
    <p><b>Summary:</b> The integration of artificial intelligence, especially large language models
in robotics, has led to rapid advancements in the field. We are now observing
an unprecedented surge in the use of robots in our daily lives. The development
and continual improvements of robots are moving at an astonishing pace.
Although these remarkable improvements facilitate and enhance our lives,
several security and privacy concerns have not been resolved yet. Therefore, it
has become crucial to address the privacy and security threats of robotic
systems while improving our experiences. In this paper, we aim to present
existing applications and threats of robotics, anticipated future evolution,
and the security and privacy issues they may imply. We present a series of open
questions for researchers and practitioners to explore further.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.09558v1">A Statistical Viewpoint on Differential Privacy: Hypothesis Testing,
  Representation and Blackwell's Theorem</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36">  
  <p><b>Published on:</b> 2024-09-14T23:47:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weijie J. Su</p>
    <p><b>Summary:</b> Differential privacy is widely considered the formal privacy for
privacy-preserving data analysis due to its robust and rigorous guarantees,
with increasingly broad adoption in public services, academia, and industry.
Despite originating in the cryptographic context, in this review paper we argue
that, fundamentally, differential privacy can be considered a \textit{pure}
statistical concept. By leveraging a theorem due to David Blackwell, our focus
is to demonstrate that the definition of differential privacy can be formally
motivated from a hypothesis testing perspective, thereby showing that
hypothesis testing is not merely convenient but also the right language for
reasoning about differential privacy. This insight leads to the definition of
$f$-differential privacy, which extends other differential privacy definitions
through a representation theorem. We review techniques that render
$f$-differential privacy a unified framework for analyzing privacy bounds in
data analysis and machine learning. Applications of this differential privacy
definition to private deep learning, private convex optimization, shuffled
mechanisms, and U.S.~Census data are discussed to highlight the benefits of
analyzing privacy bounds under this framework compared to existing
alternatives.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.09532v1">Using Synthetic Data to Mitigate Unfairness and Preserve Privacy through
  Single-Shot Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> 
  <p><b>Published on:</b> 2024-09-14T21:04:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chia-Yuan Wu, Frank E. Curtis, Daniel P. Robinson</p>
    <p><b>Summary:</b> To address unfairness issues in federated learning (FL), contemporary
approaches typically use frequent model parameter updates and transmissions
between the clients and server. In such a process, client-specific information
(e.g., local dataset size or data-related fairness metrics) must be sent to the
server to compute, e.g., aggregation weights. All of this results in high
transmission costs and the potential leakage of client information. As an
alternative, we propose a strategy that promotes fair predictions across
clients without the need to pass information between the clients and server
iteratively and prevents client data leakage. For each client, we first use
their local dataset to obtain a synthetic dataset by solving a bilevel
optimization problem that addresses unfairness concerns during the learning
process. We then pass each client's synthetic dataset to the server, the
collection of which is used to train the server model using conventional
machine learning techniques (that do not take fairness metrics into account).
Thus, we eliminate the need to handle fairness-specific aggregation weights
while preserving client privacy. Our approach requires only a single
communication between the clients and the server, thus making it
computationally cost-effective, able to maintain privacy, and able to ensuring
fairness. We present empirical evidence to demonstrate the advantages of our
approach. The results illustrate that our method effectively uses synthetic
data as a means to mitigate unfairness and preserve client privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.09510v1">Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for
  Privacy-Preserving Personalization of Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-09-14T19:18:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alireza Salemi, Hamed Zamani</p>
    <p><b>Summary:</b> Privacy-preserving methods for personalizing large language models (LLMs) are
relatively under-explored. There are two schools of thought on this topic: (1)
generating personalized outputs by personalizing the input prompt through
retrieval augmentation from the user's personal information (RAG-based
methods), and (2) parameter-efficient fine-tuning of LLMs per user that
considers efficiency and space limitations (PEFT-based methods). This paper
presents the first systematic comparison between two approaches on a wide range
of personalization tasks using seven diverse datasets. Our results indicate
that RAG-based and PEFT-based personalization methods on average yield 14.92%
and 1.07% improvements over the non-personalized LLM, respectively. We find
that combining RAG with PEFT elevates these improvements to 15.98%.
Additionally, we identify a positive correlation between the amount of user
data and PEFT's effectiveness, indicating that RAG is a better choice for
cold-start users (i.e., user's with limited personal data).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.09495v1">Protecting Vehicle Location Privacy with Contextually-Driven Synthetic
  Location Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-14T17:47:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sourabh Yadav, Chenyang Yu, Xinpeng Xie, Yan Huang, Chenxi Qiu</p>
    <p><b>Summary:</b> Geo-obfuscation is a Location Privacy Protection Mechanism used in
location-based services that allows users to report obfuscated locations
instead of exact ones. A formal privacy criterion, geoindistinguishability
(Geo-Ind), requires real locations to be hard to distinguish from nearby
locations (by attackers) based on their obfuscated representations. However,
Geo-Ind often fails to consider context, such as road networks and vehicle
traffic conditions, making it less effective in protecting the location privacy
of vehicles, of which the mobility are heavily influenced by these factors.
  In this paper, we introduce VehiTrack, a new threat model to demonstrate the
vulnerability of Geo-Ind in protecting vehicle location privacy from
context-aware inference attacks. Our experiments demonstrate that VehiTrack can
accurately determine exact vehicle locations from obfuscated data, reducing
average inference errors by 61.20% with Laplacian noise and 47.35% with linear
programming (LP) compared to traditional Bayesian attacks. By using contextual
data like road networks and traffic flow, VehiTrack effectively eliminates a
significant number of seemingly "impossible" locations during its search for
the actual location of the vehicles. Based on these insights, we propose
TransProtect, a new geo-obfuscation approach that limits obfuscation to
realistic vehicle movement patterns, complicating attackers' ability to
differentiate obfuscated from actual locations. Our results show that
TransProtect increases VehiTrack's inference error by 57.75% with Laplacian
noise and 27.21% with LP, significantly enhancing protection against these
attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.09363v2">Security and Privacy Perspectives of People Living in Shared Home
  Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-09-14T08:34:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nandita Pattnaik, Shujun Li, Jason R. C. Nurse</p>
    <p><b>Summary:</b> Security and privacy perspectives of people in a multi-user home are a
growing area of research, with many researchers reflecting on the complicated
power imbalance and challenging access control issues of the devices involved.
However, these studies primarily focused on the multi-user scenarios in
traditional family home settings, leaving other types of multi-user home
environments, such as homes shared by co-habitants without a familial
relationship, under-studied. This paper closes this research gap via
quantitative and qualitative analysis of results from an online survey and
content analysis of sampled online posts on Reddit. It explores the complex
roles of shared home users, which depend on various factors unique to the
shared home environment, e.g., who owns what home devices, how home devices are
used by multiple users, and more complicated relationships between the landlord
and people in the shared home and among co-habitants. Half (50.7%) of our
survey participants thought that devices in a shared home are less secure than
in a traditional family home. This perception was found statistically
significantly associated with factors such as the fear of devices being
tampered with in their absence and (lack of) trust in other co-habitants and
their visitors. Our study revealed new user types and relationships in a
multi-user environment such as ExternalPrimary-InternalPrimary while analysing
the landlord and shared home resident relationship with regard to shared home
device use. We propose a threat actor model for shared home environments, which
has a focus on possible malicious behaviours of current and past co-habitants
of a shared home, as a special type of insider threat in a home environment. We
also recommend further research to understand the complex roles co-habitants
can play in navigating and adapting to a shared home environment's security and
privacy landscape.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.09272v1">SafeEar: Content Privacy-Preserving Audio Deepfake Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> 
  <p><b>Published on:</b> 2024-09-14T02:45:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinfeng Li, Kai Li, Yifan Zheng, Chen Yan, Xiaoyu Ji, Wenyuan Xu</p>
    <p><b>Summary:</b> Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited
remarkable performance in generating realistic and natural audio. However,
their dark side, audio deepfake poses a significant threat to both society and
individuals. Existing countermeasures largely focus on determining the
genuineness of speech based on complete original audio recordings, which
however often contain private content. This oversight may refrain deepfake
detection from many applications, particularly in scenarios involving sensitive
information like business secrets. In this paper, we propose SafeEar, a novel
framework that aims to detect deepfake audios without relying on accessing the
speech content within. Our key idea is to devise a neural audio codec into a
novel decoupling model that well separates the semantic and acoustic
information from audio samples, and only use the acoustic information (e.g.,
prosody and timbre) for deepfake detection. In this way, no semantic content
will be exposed to the detector. To overcome the challenge of identifying
diverse deepfake audio without semantic clues, we enhance our deepfake detector
with real-world codec augmentation. Extensive experiments conducted on four
benchmark datasets demonstrate SafeEar's effectiveness in detecting various
deepfake techniques with an equal error rate (EER) down to 2.02%.
Simultaneously, it shields five-language speech content from being deciphered
by both machine and human auditory analysis, demonstrated by word error rates
(WERs) all above 93.93% and our user study. Furthermore, our benchmark
constructed for anti-deepfake and anti-content recovery evaluation helps
provide a basis for future research in the realms of audio privacy preservation
and deepfake detection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.11430v2">Federated Learning with Quantum Computing and Fully Homomorphic
  Encryption: A Novel Computing Paradigm Shift in Privacy-Preserving ML</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Neural and Evolutionary Computing-5BC0EB">
  <p><b>Published on:</b> 2024-09-14T01:23:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Siddhant Dutta, Pavana P Karanth, Pedro Maciel Xavier, Iago Leal de Freitas, Nouhaila Innan, Sadok Ben Yahia, Muhammad Shafique, David E. Bernal Neira</p>
    <p><b>Summary:</b> The widespread deployment of products powered by machine learning models is
raising concerns around data privacy and information security worldwide. To
address this issue, Federated Learning was first proposed as a
privacy-preserving alternative to conventional methods that allow multiple
learning clients to share model knowledge without disclosing private data. A
complementary approach known as Fully Homomorphic Encryption (FHE) is a
quantum-safe cryptographic system that enables operations to be performed on
encrypted weights. However, implementing mechanisms such as these in practice
often comes with significant computational overhead and can expose potential
security threats. Novel computing paradigms, such as analog, quantum, and
specialized digital hardware, present opportunities for implementing
privacy-preserving machine learning systems while enhancing security and
mitigating performance loss. This work instantiates these ideas by applying the
FHE scheme to a Federated Learning Neural Network architecture that integrates
both classical and quantum layers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.09222v1">Dark Patterns in the Opt-Out Process and Compliance with the California
  Consumer Privacy Act (CCPA)</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-09-13T22:20:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Van Hong Tran, Aarushi Mehrotra, Ranya Sharma, Marshini Chetty, Nick Feamster, Jens Frankenreiter, Lior Strahilevitz</p>
    <p><b>Summary:</b> To protect consumer privacy, the California Consumer Privacy Act (CCPA)
mandates that businesses provide consumers with a straightforward way to opt
out of the sale and sharing of their personal information. However, the control
that businesses enjoy over the opt-out process allows them to impose hurdles on
consumers aiming to opt out, including by employing dark patterns. Motivated by
the enactment of the California Privacy Rights Act (CPRA), which strengthens
the CCPA and explicitly forbids certain dark patterns in the opt-out process,
we investigate how dark patterns are used in opt-out processes and assess their
compliance with CCPA regulations. Our research reveals that websites employ a
variety of dark patterns. Some of these patterns are explicitly prohibited
under the CCPA; others evidently take advantage of legal loopholes. Despite the
initial efforts to restrict dark patterns by policymakers, there is more work
to be done.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.08913v2">HLTCOE JHU Submission to the Voice Privacy Challenge 2024</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-13T15:29:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Henry Li Xinyuan, Zexin Cai, Ashi Garg, Kevin Duh, Leibny Paola García-Perera, Sanjeev Khudanpur, Nicholas Andrews, Matthew Wiesner</p>
    <p><b>Summary:</b> We present a number of systems for the Voice Privacy Challenge, including
voice conversion based systems such as the kNN-VC method and the WavLM voice
Conversion method, and text-to-speech (TTS) based systems including
Whisper-VITS. We found that while voice conversion systems better preserve
emotional content, they struggle to conceal speaker identity in semi-white-box
attack scenarios; conversely, TTS methods perform better at anonymization and
worse at emotion preservation. Finally, we propose a random admixture system
which seeks to balance out the strengths and weaknesses of the two category of
systems, achieving a strong EER of over 40% while maintaining UAR at a
respectable 47%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.08636v1">Utilizing Data Fingerprints for Privacy-Preserving Algorithm Selection
  in Time Series Classification: Performance and Uncertainty Estimation on
  Unseen Datasets</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-09-13T08:43:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lars Böcking, Leopold Müller, Niklas Kühl</p>
    <p><b>Summary:</b> The selection of algorithms is a crucial step in designing AI services for
real-world time series classification use cases. Traditional methods such as
neural architecture search, automated machine learning, combined algorithm
selection, and hyperparameter optimizations are effective but require
considerable computational resources and necessitate access to all data points
to run their optimizations. In this work, we introduce a novel data fingerprint
that describes any time series classification dataset in a privacy-preserving
manner and provides insight into the algorithm selection problem without
requiring training on the (unseen) dataset. By decomposing the multi-target
regression problem, only our data fingerprints are used to estimate algorithm
performance and uncertainty in a scalable and adaptable manner. Our approach is
evaluated on the 112 University of California riverside benchmark datasets,
demonstrating its effectiveness in predicting the performance of 35
state-of-the-art algorithms and providing valuable insights for effective
algorithm selection in time series classification service systems, improving a
naive baseline by 7.32% on average in estimating the mean performance and
15.81% in estimating the uncertainty.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.08538v1">An Efficient Privacy-aware Split Learning Framework for Satellite
  Communications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-13T04:59:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jianfei Sun, Cong Wu, Shahid Mumtaz, Junyi Tao, Mingsheng Cao, Mei Wang, Valerio Frascolla</p>
    <p><b>Summary:</b> In the rapidly evolving domain of satellite communications, integrating
advanced machine learning techniques, particularly split learning, is crucial
for enhancing data processing and model training efficiency across satellites,
space stations, and ground stations. Traditional ML approaches often face
significant challenges within satellite networks due to constraints such as
limited bandwidth and computational resources. To address this gap, we propose
a novel framework for more efficient SL in satellite communications. Our
approach, Dynamic Topology Informed Pruning, namely DTIP, combines differential
privacy with graph and model pruning to optimize graph neural networks for
distributed learning. DTIP strategically applies differential privacy to raw
graph data and prunes GNNs, thereby optimizing both model size and
communication load across network tiers. Extensive experiments across diverse
datasets demonstrate DTIP's efficacy in enhancing privacy, accuracy, and
computational efficiency. Specifically, on Amazon2M dataset, DTIP maintains an
accuracy of 0.82 while achieving a 50% reduction in floating-point operations
per second. Similarly, on ArXiv dataset, DTIP achieves an accuracy of 0.85
under comparable conditions. Our framework not only significantly improves the
operational efficiency of satellite communications but also establishes a new
benchmark in privacy-aware distributed learning, potentially revolutionizing
data handling in space-based networks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.08503v1">Enhancing Privacy in ControlNet and Stable Diffusion via Split Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-13T02:55:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dixi Yao</p>
    <p><b>Summary:</b> With the emerging trend of large generative models, ControlNet is introduced
to enable users to fine-tune pre-trained models with their own data for various
use cases. A natural question arises: how can we train ControlNet models while
ensuring users' data privacy across distributed devices? Exploring different
distributed training schemes, we find conventional federated learning and split
learning unsuitable. Instead, we propose a new distributed learning structure
that eliminates the need for the server to send gradients back. Through a
comprehensive evaluation of existing threats, we discover that in the context
of training ControlNet with split learning, most existing attacks are
ineffective, except for two mentioned in previous literature. To counter these
threats, we leverage the properties of diffusion models and design a new
timestep sampling policy during forward processes. We further propose a
privacy-preserving activation function and a method to prevent private text
prompts from leaving clients, tailored for image generation with diffusion
models. Our experimental results demonstrate that our algorithms and systems
greatly enhance the efficiency of distributed training for ControlNet while
ensuring users' data privacy without compromising image generation quality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07997v1">Privacy-preserving federated prediction of pain intensity change based
  on multi-center survey data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-12T12:41:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Supratim Das, Mahdie Rafie, Paula Kammer, Søren T. Skou, Dorte T. Grønne, Ewa M. Roos, André Hajek, Hans-Helmut König, Md Shihab Ullaha, Niklas Probul, Jan Baumbacha, Linda Baumbach</p>
    <p><b>Summary:</b> Background: Patient-reported survey data are used to train prognostic models
aimed at improving healthcare. However, such data are typically available
multi-centric and, for privacy reasons, cannot easily be centralized in one
data repository. Models trained locally are less accurate, robust, and
generalizable. We present and apply privacy-preserving federated machine
learning techniques for prognostic model building, where local survey data
never leaves the legally safe harbors of the medical centers. Methods: We used
centralized, local, and federated learning techniques on two healthcare
datasets (GLA:D data from the five health regions of Denmark and international
SHARE data of 27 countries) to predict two different health outcomes. We
compared linear regression, random forest regression, and random forest
classification models trained on local data with those trained on the entire
data in a centralized and in a federated fashion. Results: In GLA:D data,
federated linear regression (R2 0.34, RMSE 18.2) and federated random forest
regression (R2 0.34, RMSE 18.3) models outperform their local counterparts
(i.e., R2 0.32, RMSE 18.6, R2 0.30, RMSE 18.8) with statistical significance.
We also found that centralized models (R2 0.34, RMSE 18.2, R2 0.32, RMSE 18.5,
respectively) did not perform significantly better than the federated models.
In SHARE, the federated model (AC 0.78, AUROC: 0.71) and centralized model (AC
0.84, AUROC: 0.66) perform significantly better than the local models (AC:
0.74, AUROC: 0.69). Conclusion: Federated learning enables the training of
prognostic models from multi-center surveys without compromising privacy and
with only minimal or no compromise regarding model performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.11423v1">Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large
  Language Models on Generated Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-12T10:14:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Atilla Akkus, Mingjie Li, Junjie Chu, Michael Backes, Yang Zhang, Sinem Sav</p>
    <p><b>Summary:</b> Large language models (LLMs) have shown considerable success in a range of
domain-specific tasks, especially after fine-tuning. However, fine-tuning with
real-world data usually leads to privacy risks, particularly when the
fine-tuning samples exist in the pre-training data. To avoid the shortcomings
of real data, developers often employ methods to automatically generate
synthetic data for fine-tuning, as data generated by traditional models are
often far away from the real-world pertaining data. However, given the advanced
capabilities of LLMs, the distinction between real data and LLM-generated data
has become negligible, which may also lead to privacy risks like real data. In
this paper, we present an empirical analysis of this underexplored issue by
investigating a key question: "Does fine-tuning with LLM-generated data enhance
privacy, or does it pose additional privacy risks?" Based on the structure of
LLM's generated data, our research focuses on two primary approaches to
fine-tuning with generated data: supervised fine-tuning with unstructured
generated data and self-instruct tuning. The number of successful Personal
Information Identifier (PII) extractions for Pythia after fine-tuning our
generated data raised over $20\%$. Furthermore, the ROC-AUC score of membership
inference attacks for Pythia-6.9b after self-instruct methods also achieves
more than $40\%$ improvements on ROC-AUC score than base models. The results
indicate the potential privacy risks in LLMs when fine-tuning with the
generated data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07809v1">Controllable Synthetic Clinical Note Generation with Privacy Guarantees</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-12T07:38:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tal Baumel, Andre Manoel, Daniel Jones, Shize Su, Huseyin Inan,  Aaron,  Bornstein, Robert Sim</p>
    <p><b>Summary:</b> In the field of machine learning, domain-specific annotated data is an
invaluable resource for training effective models. However, in the medical
domain, this data often includes Personal Health Information (PHI), raising
significant privacy concerns. The stringent regulations surrounding PHI limit
the availability and sharing of medical datasets, which poses a substantial
challenge for researchers and practitioners aiming to develop advanced machine
learning models. In this paper, we introduce a novel method to "clone" datasets
containing PHI. Our approach ensures that the cloned datasets retain the
essential characteristics and utility of the original data without compromising
patient privacy. By leveraging differential-privacy techniques and a novel
fine-tuning task, our method produces datasets that are free from identifiable
information while preserving the statistical properties necessary for model
training. We conduct utility testing to evaluate the performance of machine
learning models trained on the cloned datasets. The results demonstrate that
our cloned datasets not only uphold privacy standards but also enhance model
performance compared to those trained on traditional anonymized datasets. This
work offers a viable solution for the ethical and effective utilization of
sensitive medical data in machine learning, facilitating progress in medical
research and the development of robust predictive models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07773v1">PDC-FRS: Privacy-preserving Data Contribution for Federated Recommender
  System</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-09-12T06:13:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chaoqun Yang, Wei Yuan, Liang Qu, Thanh Tam Nguyen</p>
    <p><b>Summary:</b> Federated recommender systems (FedRecs) have emerged as a popular research
direction for protecting users' privacy in on-device recommendations. In
FedRecs, users keep their data locally and only contribute their local
collaborative information by uploading model parameters to a central server.
While this rigid framework protects users' raw data during training, it
severely compromises the recommendation model's performance due to the
following reasons: (1) Due to the power law distribution nature of user
behavior data, individual users have few data points to train a recommendation
model, resulting in uploaded model updates that may be far from optimal; (2) As
each user's uploaded parameters are learned from local data, which lacks global
collaborative information, relying solely on parameter aggregation methods such
as FedAvg to fuse global collaborative information may be suboptimal. To bridge
this performance gap, we propose a novel federated recommendation framework,
PDC-FRS. Specifically, we design a privacy-preserving data contribution
mechanism that allows users to share their data with a differential privacy
guarantee. Based on the shared but perturbed data, an auxiliary model is
trained in parallel with the original federated recommendation process. This
auxiliary model enhances FedRec by augmenting each user's local dataset and
integrating global collaborative information. To demonstrate the effectiveness
of PDC-FRS, we conduct extensive experiments on two widely used recommendation
datasets. The empirical results showcase the superiority of PDC-FRS compared to
baseline methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07751v1">Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-12T04:51:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhizheng Lai, Yufei Zhou, Peijia Zheng, Lin Chen</p>
    <p><b>Summary:</b> The recently proposed Kolmogorov-Arnold Networks (KANs) offer enhanced
interpretability and greater model expressiveness. However, KANs also present
challenges related to privacy leakage during inference. Homomorphic encryption
(HE) facilitates privacy-preserving inference for deep learning models,
enabling resource-limited users to benefit from deep learning services while
ensuring data security. Yet, the complex structure of KANs, incorporating
nonlinear elements like the SiLU activation function and B-spline functions,
renders existing privacy-preserving inference techniques inadequate. To address
this issue, we propose an accurate and efficient privacy-preserving inference
scheme tailored for KANs. Our approach introduces a task-specific polynomial
approximation for the SiLU activation function, dynamically adjusting the
approximation range to ensure high accuracy on real-world datasets.
Additionally, we develop an efficient method for computing B-spline functions
within the HE domain, leveraging techniques such as repeat packing, lazy
combination, and comparison functions. We evaluate the effectiveness of our
privacy-preserving KAN inference scheme on both symbolic formula evaluation and
image classification. The experimental results show that our model achieves
accuracy comparable to plaintext KANs across various datasets and outperforms
plaintext MLPs. Additionally, on the CIFAR-10 dataset, our inference latency
achieves over 7 times speedup compared to the naive method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07444v2">Echoes of Privacy: Uncovering the Profiling Practices of Voice
  Assistants</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-09-11T17:44:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tina Khezresmaeilzadeh, Elaine Zhu, Kiersten Grieco, Daniel J. Dubois, Konstantinos Psounis, David Choffnes</p>
    <p><b>Summary:</b> Many companies, including Google, Amazon, and Apple, offer voice assistants
as a convenient solution for answering general voice queries and accessing
their services. These voice assistants have gained popularity and can be easily
accessed through various smart devices such as smartphones, smart speakers,
smartwatches, and an increasing array of other devices. However, this
convenience comes with potential privacy risks. For instance, while companies
vaguely mention in their privacy policies that they may use voice interactions
for user profiling, it remains unclear to what extent this profiling occurs and
whether voice interactions pose greater privacy risks compared to other
interaction modalities.
  In this paper, we conduct 1171 experiments involving a total of 24530 queries
with different personas and interaction modalities over the course of 20 months
to characterize how the three most popular voice assistants profile their
users. We analyze factors such as the labels assigned to users, their accuracy,
the time taken to assign these labels, differences between voice and web
interactions, and the effectiveness of profiling remediation tools offered by
each voice assistant. Our findings reveal that profiling can happen without
interaction, can be incorrect and inconsistent at times, may take several days
to weeks for changes to occur, and can be influenced by the interaction
modality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07415v1">SoK: Security and Privacy Risks of Medical AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-11T16:59:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuanhaur Chang, Han Liu, Evin Jaff, Chenyang Lu, Ning Zhang</p>
    <p><b>Summary:</b> The integration of technology and healthcare has ushered in a new era where
software systems, powered by artificial intelligence and machine learning, have
become essential components of medical products and services. While these
advancements hold great promise for enhancing patient care and healthcare
delivery efficiency, they also expose sensitive medical data and system
integrity to potential cyberattacks. This paper explores the security and
privacy threats posed by AI/ML applications in healthcare. Through a thorough
examination of existing research across a range of medical domains, we have
identified significant gaps in understanding the adversarial attacks targeting
medical AI systems. By outlining specific adversarial threat models for medical
settings and identifying vulnerable application domains, we lay the groundwork
for future research that investigates the security and resilience of AI-driven
medical systems. Through our analysis of different threat models and
feasibility studies on adversarial attacks in different medical domains, we
provide compelling insights into the pressing need for cybersecurity research
in the rapidly evolving field of AI healthcare technology.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07224v1">Analytic Class Incremental Learning for Sound Source Localization with
  Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> 
  <p><b>Published on:</b> 2024-09-11T12:31:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinyuan Qian, Xianghu Yue, Jiadong Wang, Huiping Zhuang, Haizhou Li</p>
    <p><b>Summary:</b> Sound Source Localization (SSL) enabling technology for applications such as
surveillance and robotics. While traditional Signal Processing (SP)-based SSL
methods provide analytic solutions under specific signal and noise assumptions,
recent Deep Learning (DL)-based methods have significantly outperformed them.
However, their success depends on extensive training data and substantial
computational resources. Moreover, they often rely on large-scale annotated
spatial data and may struggle when adapting to evolving sound classes. To
mitigate these challenges, we propose a novel Class Incremental Learning (CIL)
approach, termed SSL-CIL, which avoids serious accuracy degradation due to
catastrophic forgetting by incrementally updating the DL-based SSL model
through a closed-form analytic solution. In particular, data privacy is ensured
since the learning process does not revisit any historical data
(exemplar-free), which is more suitable for smart home scenarios. Empirical
results in the public SSLR dataset demonstrate the superior performance of our
proposal, achieving a localization accuracy of 90.9%, surpassing other
competitive methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07187v1">A Simple Linear Space Data Structure for ANN with Application in
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2024-09-11T11:14:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Martin Aumüller, Fabrizio Boninsegna, Francesco Silvestri</p>
    <p><b>Summary:</b> Locality Sensitive Filters are known for offering a quasi-linear space data
structure with rigorous guarantees for the Approximate Near Neighbor search
problem. Building on Locality Sensitive Filters, we derive a simple data
structure for the Approximate Near Neighbor Counting problem under differential
privacy. Moreover, we provide a simple analysis leveraging a connection with
concomitant statistics and extreme value theory. Our approach achieves the same
performance as the recent findings of Andoni et al. (NeurIPS 2023) but with a
more straightforward method. As a side result, the paper provides a more
compact description and analysis of Locality Sensitive Filters for Approximate
Near Neighbor Search under inner product similarity, improving a previous
result in Aum\"{u}ller et al. (TODS 2022).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06955v2">Privacy-Preserving Federated Learning with Consistency via Knowledge
  Distillation Using Conditional Generator</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-09-11T02:36:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kangyang Luo, Shuai Wang, Xiang Li, Yunshi Lan, Ming Gao, Jinlong Shu</p>
    <p><b>Summary:</b> Federated Learning (FL) is gaining popularity as a distributed learning
framework that only shares model parameters or gradient updates and keeps
private data locally. However, FL is at risk of privacy leakage caused by
privacy inference attacks. And most existing privacy-preserving mechanisms in
FL conflict with achieving high performance and efficiency. Therefore, we
propose FedMD-CG, a novel FL method with highly competitive performance and
high-level privacy preservation, which decouples each client's local model into
a feature extractor and a classifier, and utilizes a conditional generator
instead of the feature extractor to perform server-side model aggregation. To
ensure the consistency of local generators and classifiers, FedMD-CG leverages
knowledge distillation to train local models and generators at both the latent
feature level and the logit level. Also, we construct additional classification
losses and design new diversity losses to enhance client-side training.
FedMD-CG is robust to data heterogeneity and does not require training extra
discriminators (like cGAN). We conduct extensive experiments on various image
classification tasks to validate the superiority of FedMD-CG.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06564v1">Advancing Android Privacy Assessments with Automation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-10T14:56:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mugdha Khedkar, Michael Schlichtig, Eric Bodden</p>
    <p><b>Summary:</b> Android apps collecting data from users must comply with legal frameworks to
ensure data protection. This requirement has become even more important since
the implementation of the General Data Protection Regulation (GDPR) by the
European Union in 2018. Moreover, with the proposed Cyber Resilience Act on the
horizon, stakeholders will soon need to assess software against even more
stringent security and privacy standards. Effective privacy assessments require
collaboration among groups with diverse expertise to function effectively as a
cohesive unit.
  This paper motivates the need for an automated approach that enhances
understanding of data protection in Android apps and improves communication
between the various parties involved in privacy assessments. We propose the
Assessor View, a tool designed to bridge the knowledge gap between these
parties, facilitating more effective privacy assessments of Android
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06455v1">Continual Domain Incremental Learning for Privacy-aware Digital
  Pathology</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-09-10T12:21:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pratibha Kumari, Daniel Reisenbüchler, Lucas Luttner, Nadine S. Schaadt, Friedrich Feuerhake, Dorit Merhof</p>
    <p><b>Summary:</b> In recent years, there has been remarkable progress in the field of digital
pathology, driven by the ability to model complex tissue patterns using
advanced deep-learning algorithms. However, the robustness of these models is
often severely compromised in the presence of data shifts (e.g., different
stains, organs, centers, etc.). Alternatively, continual learning (CL)
techniques aim to reduce the forgetting of past data when learning new data
with distributional shift conditions. Specifically, rehearsal-based CL
techniques, which store some past data in a buffer and then replay it with new
data, have proven effective in medical image analysis tasks. However, privacy
concerns arise as these approaches store past data, prompting the development
of our novel Generative Latent Replay-based CL (GLRCL) approach. GLRCL captures
the previous distribution through Gaussian Mixture Models instead of storing
past samples, which are then utilized to generate features and perform latent
replay with new data. We systematically evaluate our proposed framework under
different shift conditions in histopathology data, including stain and organ
shift. Our approach significantly outperforms popular buffer-free CL approaches
and performs similarly to rehearsal-based CL approaches that require large
buffers causing serious privacy violations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06422v1">A Pervasive, Efficient and Private Future: Realizing Privacy-Preserving
  Machine Learning Through Hybrid Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-10T11:04:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Khoa Nguyen, Mindaugas Budzys, Eugene Frimpong, Tanveer Khan, Antonis Michalas</p>
    <p><b>Summary:</b> Machine Learning (ML) has become one of the most impactful fields of data
science in recent years. However, a significant concern with ML is its privacy
risks due to rising attacks against ML models. Privacy-Preserving Machine
Learning (PPML) methods have been proposed to mitigate the privacy and security
risks of ML models. A popular approach to achieving PPML uses Homomorphic
Encryption (HE). However, the highly publicized inefficiencies of HE make it
unsuitable for highly scalable scenarios with resource-constrained devices.
Hence, Hybrid Homomorphic Encryption (HHE) -- a modern encryption scheme that
combines symmetric cryptography with HE -- has recently been introduced to
overcome these challenges. HHE potentially provides a foundation to build new
efficient and privacy-preserving services that transfer expensive HE operations
to the cloud. This work introduces HHE to the ML field by proposing
resource-friendly PPML protocols for edge devices. More precisely, we utilize
HHE as the primary building block of our PPML protocols. We assess the
performance of our protocols by first extensively evaluating each party's
communication and computational cost on a dummy dataset and show the efficiency
of our protocols by comparing them with similar protocols implemented using
plain BFV. Subsequently, we demonstrate the real-world applicability of our
construction by building an actual PPML application that uses HHE as its
foundation to classify heart disease based on sensitive ECG data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06360v1">SoK: Evaluating 5G Protocols Against Legacy and Emerging Privacy and
  Security Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-09-10T09:30:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Stavros Eleftherakis, Domenico Giustiniano, Nicolas Kourtellis</p>
    <p><b>Summary:</b> Ensuring user privacy remains a critical concern within mobile cellular
networks, particularly given the proliferation of interconnected devices and
services. In fact, a lot of user privacy issues have been raised in 2G, 3G,
4G/LTE networks. Recognizing this general concern, 3GPP has prioritized
addressing these issues in the development of 5G, implementing numerous
modifications to enhance user privacy since 5G Release 15. In this
systematization of knowledge paper, we first provide a framework for studying
privacy and security related attacks in cellular networks, setting as privacy
objective the User Identity Confidentiality defined in 3GPP standards. Using
this framework, we discuss existing privacy and security attacks in pre-5G
networks, analyzing the weaknesses that lead to these attacks. Furthermore, we
thoroughly study the security characteristics of 5G up to the new Release 19,
and examine mitigation mechanisms of 5G to the identified pre-5G attacks.
Afterwards, we analyze how recent 5G attacks try to overcome these mitigation
mechanisms. Finally, we identify current limitations and open problems in
security of 5G, and propose directions for future work.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06233v1">VBIT: Towards Enhancing Privacy Control Over IoT Devices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-10T06:00:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jad Al Aaraj, Olivia Figueira, Tu Le, Isabela Figueira, Rahmadi Trimananda, Athina Markopoulou</p>
    <p><b>Summary:</b> Internet-of-Things (IoT) devices are increasingly deployed at home, at work,
and in other shared and public spaces. IoT devices collect and share data with
service providers and third parties, which poses privacy concerns. Although
privacy enhancing tools are quite advanced in other applications domains (\eg~
advertising and tracker blockers for browsers), users have currently no
convenient way to know or manage what and how data is collected and shared by
IoT devices. In this paper, we present VBIT, an interactive system combining
Mixed Reality (MR) and web-based applications that allows users to: (1) uncover
and visualize tracking services by IoT devices in an instrumented space and (2)
take action to stop or limit that tracking. We design and implement VBIT to
operate at the network traffic level, and we show that it has negligible
performance overhead, and offers flexibility and good usability. We perform a
mixed-method user study consisting of an online survey and an in-person
interview study. We show that VBIT users appreciate VBIT's transparency,
control, and customization features, and they become significantly more willing
to install an IoT advertising and tracking blocker, after using VBIT. In the
process, we obtain design insights that can be used to further iterate and
improve the design of VBIT and other systems for IoT transparency and control.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06069v1">Privacy-Preserving Data Linkage Across Private and Public Datasets for
  Collaborative Agriculture Research</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-09-09T21:07:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Osama Zafar, Rosemarie Santa Gonzalez, Gabriel Wilkins, Alfonso Morales, Erman Ayday</p>
    <p><b>Summary:</b> Digital agriculture leverages technology to enhance crop yield, disease
resilience, and soil health, playing a critical role in agricultural research.
However, it raises privacy concerns such as adverse pricing, price
discrimination, higher insurance costs, and manipulation of resources,
deterring farm operators from sharing data due to potential misuse. This study
introduces a privacy-preserving framework that addresses these risks while
allowing secure data sharing for digital agriculture. Our framework enables
comprehensive data analysis while protecting privacy. It allows stakeholders to
harness research-driven policies that link public and private datasets. The
proposed algorithm achieves this by: (1) identifying similar farmers based on
private datasets, (2) providing aggregate information like time and location,
(3) determining trends in price and product availability, and (4) correlating
trends with public policy data, such as food insecurity statistics. We validate
the framework with real-world Farmer's Market datasets, demonstrating its
efficacy through machine learning models trained on linked privacy-preserved
data. The results support policymakers and researchers in addressing food
insecurity and pricing issues. This work significantly contributes to digital
agriculture by providing a secure method for integrating and analyzing data,
driving advancements in agricultural technology and development.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.05623v1">A Framework for Differential Privacy Against Timing Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-09T13:56:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zachary Ratliff, Salil Vadhan</p>
    <p><b>Summary:</b> The standard definition of differential privacy (DP) ensures that a
mechanism's output distribution on adjacent datasets is indistinguishable.
However, real-world implementations of DP can, and often do, reveal information
through their runtime distributions, making them susceptible to timing attacks.
In this work, we establish a general framework for ensuring differential
privacy in the presence of timing side channels. We define a new notion of
timing privacy, which captures programs that remain differentially private to
an adversary that observes the program's runtime in addition to the output. Our
framework enables chaining together component programs that are timing-stable
followed by a random delay to obtain DP programs that achieve timing privacy.
Importantly, our definitions allow for measuring timing privacy and output
privacy using different privacy measures. We illustrate how to instantiate our
framework by giving programs for standard DP computations in the RAM and Word
RAM models of computation. Furthermore, we show how our framework can be
realized in code through a natural extension of the OpenDP Programming
Framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.05249v1">NetDPSyn: Synthesizing Network Traces under Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-09-08T23:54:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Danyu Sun, Joann Qiongna Chen, Chen Gong, Tianhao Wang, Zhou Li</p>
    <p><b>Summary:</b> As the utilization of network traces for the network measurement research
becomes increasingly prevalent, concerns regarding privacy leakage from network
traces have garnered the public's attention. To safeguard network traces,
researchers have proposed the trace synthesis that retains the essential
properties of the raw data. However, previous works also show that synthesis
traces with generative models are vulnerable under linkage attacks.
  This paper introduces NetDPSyn, the first system to synthesize high-fidelity
network traces under privacy guarantees. NetDPSyn is built with the
Differential Privacy (DP) framework as its core, which is significantly
different from prior works that apply DP when training the generative model.
The experiments conducted on three flow and two packet datasets indicate that
NetDPSyn achieves much better data utility in downstream tasks like anomaly
detection. NetDPSyn is also 2.5 times faster than the other methods on average
in data synthesis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04877v1">Strong Privacy-Preserving Universally Composable AKA Protocol with
  Seamless Handover Support for Mobile Virtual Network Operator</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-07T18:04:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rabiah Alnashwan, Yang Yang, Yilu Dong, Prosanta Gope, Behzad Abdolmaleki, Syed Rafiul Hussain</p>
    <p><b>Summary:</b> Consumers seeking a new mobile plan have many choices in the present mobile
landscape. The Mobile Virtual Network Operator (MVNO) has recently gained
considerable attention among these options. MVNOs offer various benefits,
making them an appealing choice for a majority of consumers. These advantages
encompass flexibility, access to cutting-edge technologies, enhanced coverage,
superior customer service, and substantial cost savings. Even though MVNO
offers several advantages, it also creates some security and privacy concerns
for the customer simultaneously. For instance, in the existing solution, MVNO
needs to hand over all the sensitive details, including the users' identities
and master secret keys of their customers, to a mobile operator (MNO) to
validate the customers while offering any services. This allows MNOs to have
unrestricted access to the MVNO subscribers' location and mobile data,
including voice calls, SMS, and Internet, which the MNOs frequently sell to
third parties (e.g., advertisement companies and surveillance agencies) for
more profit. Although critical for mass users, such privacy loss has been
historically ignored due to the lack of practical and privacy-preserving
solutions for registration and handover procedures in cellular networks. In
this paper, we propose a universally composable authentication and handover
scheme with strong user privacy support, where each MVNO user can validate a
mobile operator (MNO) and vice-versa without compromising user anonymity and
unlinkability support. Here, we anticipate that our proposed solution will most
likely be deployed by the MVNO(s) to ensure enhanced privacy support to their
customer(s).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04716v1">Privacy enhanced collaborative inference in the Cox proportional hazards
  model for distributed data</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2024-09-07T05:32:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mengtong Hu, Xu Shi, Peter X. -K. Song</p>
    <p><b>Summary:</b> Data sharing barriers are paramount challenges arising from multicenter
clinical studies where multiple data sources are stored in a distributed
fashion at different local study sites. Particularly in the case of
time-to-event analysis when global risk sets are needed for the Cox
proportional hazards model, access to a centralized database is typically
necessary. Merging such data sources into a common data storage for a
centralized statistical analysis requires a data use agreement, which is often
time-consuming. Furthermore, the construction and distribution of risk sets to
participating clinical centers for subsequent calculations may pose a risk of
revealing individual-level information. We propose a new collaborative Cox
model that eliminates the need for accessing the centralized database and
constructing global risk sets but needs only the sharing of summary statistics
with significantly smaller dimensions than risk sets. Thus, the proposed
collaborative inference enjoys maximal protection of data privacy. We show
theoretically and numerically that the new distributed proportional hazards
model approach has little loss of statistical power when compared to the
centralized method that requires merging the entire data. We present a
renewable sieve method to establish large-sample properties for the proposed
method. We illustrate its performance through simulation experiments and a
real-world data example from patients with kidney transplantation in the Organ
Procurement and Transplantation Network (OPTN) to understand the factors
associated with the 5-year death-censored graft failure (DCGF) for patients who
underwent kidney transplants in the US.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04652v2">Privacy-Preserving Race/Ethnicity Estimation for Algorithmic Bias
  Measurement in the U.S</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T23:29:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Saikrishna Badrinarayanan, Osonde Osoba, Miao Cheng, Ryan Rogers, Sakshi Jain, Rahul Tandra, Natesh S. Pillai</p>
    <p><b>Summary:</b> AI fairness measurements, including tests for equal treatment, often take the
form of disaggregated evaluations of AI systems. Such measurements are an
important part of Responsible AI operations. These measurements compare system
performance across demographic groups or sub-populations and typically require
member-level demographic signals such as gender, race, ethnicity, and location.
However, sensitive member-level demographic attributes like race and ethnicity
can be challenging to obtain and use due to platform choices, legal
constraints, and cultural norms. In this paper, we focus on the task of
enabling AI fairness measurements on race/ethnicity for \emph{U.S. LinkedIn
members} in a privacy-preserving manner. We present the Privacy-Preserving
Probabilistic Race/Ethnicity Estimation (PPRE) method for performing this task.
PPRE combines the Bayesian Improved Surname Geocoding (BISG) model, a sparse
LinkedIn survey sample of self-reported demographics, and privacy-enhancing
technologies like secure two-party computation and differential privacy to
enable meaningful fairness measurements while preserving member privacy. We
provide details of the PPRE method and its privacy guarantees. We then
illustrate sample measurement operations. We conclude with a review of open
research and engineering challenges for expanding our privacy-preserving
fairness measurement capabilities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04366v1">Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T15:57:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lioba Heimbach, Yann Vonlanthen, Juan Villacis, Lucianna Kiffer, Roger Wattenhofer</p>
    <p><b>Summary:</b> Many blockchain networks aim to preserve the anonymity of validators in the
peer-to-peer (P2P) network, ensuring that no adversary can link a validator's
identifier to the IP address of a peer due to associated privacy and security
concerns. This work demonstrates that the Ethereum P2P network does not offer
this anonymity. We present a methodology that enables any node in the network
to identify validators hosted on connected peers and empirically verify the
feasibility of our proposed method. Using data collected from four nodes over
three days, we locate more than 15% of Ethereum validators in the P2P network.
The insights gained from our deanonymization technique provide valuable
information on the distribution of validators across peers, their geographic
locations, and hosting organizations. We further discuss the implications and
risks associated with the lack of anonymity in the P2P network and propose
methods to help validators protect their privacy. The Ethereum Foundation has
awarded us a bug bounty, acknowledging the impact of our results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04257v1">Privacy risk from synthetic data: practical proposals</a></h3>
  
  <p><b>Published on:</b> 2024-09-06T13:10:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gillian M Raab</p>
    <p><b>Summary:</b> This paper proposes and compares measures of identity and attribute
disclosure risk for synthetic data. Data custodians can use the methods
proposed here to inform the decision as to whether to release synthetic
versions of confidential data. Different measures are evaluated on two data
sets. Insight into the measures is obtained by examining the details of the
records identified as posing a disclosure risk. This leads to methods to
identify, and possibly exclude, apparently risky records where the
identification or attribution would be expected by someone with background
knowledge of the data. The methods described are available as part of the
\textbf{synthpop} package for \textbf{R}.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04194v1">Towards Privacy-Preserving Relational Data Synthesis via Probabilistic
  Relational Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-06T11:24:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Malte Luttermann, Ralf Möller, Mattis Hartwig</p>
    <p><b>Summary:</b> Probabilistic relational models provide a well-established formalism to
combine first-order logic and probabilistic models, thereby allowing to
represent relationships between objects in a relational domain. At the same
time, the field of artificial intelligence requires increasingly large amounts
of relational training data for various machine learning tasks. Collecting
real-world data, however, is often challenging due to privacy concerns, data
protection regulations, high costs, and so on. To mitigate these challenges,
the generation of synthetic data is a promising approach. In this paper, we
solve the problem of generating synthetic relational data via probabilistic
relational models. In particular, we propose a fully-fledged pipeline to go
from relational database to probabilistic relational model, which can then be
used to sample new synthetic relational data points from its underlying
probability distribution. As part of our proposed pipeline, we introduce a
learning algorithm to construct a probabilistic relational model from a given
relational database.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04173v1">NPU-NTU System for Voice Privacy 2024 Challenge</a></h3>
  
  <p><b>Published on:</b> 2024-09-06T10:32:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jixun Yao, Nikita Kuzmin, Qing Wang, Pengcheng Guo, Ziqian Ning, Dake Guo, Kong Aik Lee, Eng-Siong Chng, Lei Xie</p>
    <p><b>Summary:</b> Speaker anonymization is an effective privacy protection solution that
conceals the speaker's identity while preserving the linguistic content and
paralinguistic information of the original speech. To establish a fair
benchmark and facilitate comparison of speaker anonymization systems, the
VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition
planned for 2024. In this paper, we describe our proposed speaker anonymization
system for VPC 2024. Our system employs a disentangled neural codec
architecture and a serial disentanglement strategy to gradually disentangle the
global speaker identity and time-variant linguistic content and paralinguistic
information. We introduce multiple distillation methods to disentangle
linguistic content, speaker identity, and emotion. These methods include
semantic distillation, supervised speaker distillation, and frame-level emotion
distillation. Based on these distillations, we anonymize the original speaker
identity using a weighted sum of a set of candidate speaker identities and a
randomly generated speaker identity. Our system achieves the best trade-off of
privacy protection and emotion preservation in VPC 2024.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04167v1">Do Android App Developers Accurately Report Collection of
  Privacy-Related Data?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T10:05:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mugdha Khedkar, Ambuj Kumar Mondal, Eric Bodden</p>
    <p><b>Summary:</b> Many Android applications collect data from users. The European Union's
General Data Protection Regulation (GDPR) requires vendors to faithfully
disclose which data their apps collect. This task is complicated because many
apps use third-party code for which the same information is not readily
available. Hence we ask: how accurately do current Android apps fulfill these
requirements?
  In this work, we first expose a multi-layered definition of privacy-related
data to correctly report data collection in Android apps. We further create a
dataset of privacy-sensitive data classes that may be used as input by an
Android app. This dataset takes into account data collected both through the
user interface and system APIs.
  We manually examine the data safety sections of 70 Android apps to observe
how data collection is reported, identifying instances of over- and
under-reporting. Additionally, we develop a prototype to statically extract and
label privacy-related data collected via app source code, user interfaces, and
permissions. Comparing the prototype's results with the data safety sections of
20 apps reveals reporting discrepancies. Using the results from two Messaging
and Social Media apps (Signal and Instagram), we discuss how app developers
under-report and over-report data collection, respectively, and identify
inaccurately reported data categories.
  Our results show that app developers struggle to accurately report data
collection, either due to Google's abstract definition of collected data or
insufficient existing tool support.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04048v2">Exploring User Privacy Awareness on GitHub: An Empirical Study</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-09-06T06:41:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Costanza Alfieri, Juri Di Rocco, Paola Inverardi, Phuong T. Nguyen</p>
    <p><b>Summary:</b> GitHub provides developers with a practical way to distribute source code and
collaboratively work on common projects. To enhance account security and
privacy, GitHub allows its users to manage access permissions, review audit
logs, and enable two-factor authentication. However, despite the endless
effort, the platform still faces various issues related to the privacy of its
users. This paper presents an empirical study delving into the GitHub
ecosystem. Our focus is on investigating the utilization of privacy settings on
the platform and identifying various types of sensitive information disclosed
by users. Leveraging a dataset comprising 6,132 developers, we report and
analyze their activities by means of comments on pull requests. Our findings
indicate an active engagement by users with the available privacy settings on
GitHub. Notably, we observe the disclosure of different forms of private
information within pull request comments. This observation has prompted our
exploration into sensitivity detection using a large language model and BERT,
to pave the way for a personalized privacy assistant. Our work provides
insights into the utilization of existing privacy protection tools, such as
privacy settings, along with their inherent limitations. Essentially, we aim to
advance research in this field by providing both the motivation for creating
such privacy protection tools and a proposed methodology for personalizing
them.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04026v1">Efficient Fault-Tolerant Quantum Protocol for Differential Privacy in
  the Shuffle Model</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T04:53:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hassan Jameel Asghar, Arghya Mukherjee, Gavin K. Brennen</p>
    <p><b>Summary:</b> We present a quantum protocol which securely and implicitly implements a
random shuffle to realize differential privacy in the shuffle model. The
shuffle model of differential privacy amplifies privacy achievable via local
differential privacy by randomly permuting the tuple of outcomes from data
contributors. In practice, one needs to address how this shuffle is
implemented. Examples include implementing the shuffle via mix-networks, or
shuffling via a trusted third-party. These implementation specific issues raise
non-trivial computational and trust requirements in a classical system. We
propose a quantum version of the protocol using entanglement of quantum states
and show that the shuffle can be implemented without these extra requirements.
Our protocol implements k-ary randomized response, for any value of k > 2, and
furthermore, can be efficiently implemented using fault-tolerant computation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03707v1">A Different Level Text Protection Mechanism With Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-09-05T17:13:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qingwen Fu</p>
    <p><b>Summary:</b> The article introduces a method for extracting words of different degrees of
importance based on the BERT pre-training model and proves the effectiveness of
this method. The article also discusses the impact of maintaining the same
perturbation results for words of different importance on the overall text
utility. This method can be applied to long text protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03655v1">Privacy versus Emotion Preservation Trade-offs in Emotion-Preserving
  Speaker Anonymization</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-05T16:10:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zexin Cai, Henry Li Xinyuan, Ashi Garg, Leibny Paola García-Perera, Kevin Duh, Sanjeev Khudanpur, Nicholas Andrews, Matthew Wiesner</p>
    <p><b>Summary:</b> Advances in speech technology now allow unprecedented access to personally
identifiable information through speech. To protect such information, the
differential privacy field has explored ways to anonymize speech while
preserving its utility, including linguistic and paralinguistic aspects.
However, anonymizing speech while maintaining emotional state remains
challenging. We explore this problem in the context of the VoicePrivacy 2024
challenge. Specifically, we developed various speaker anonymization pipelines
and find that approaches either excel at anonymization or preserving emotion
state, but not both simultaneously. Achieving both would require an in-domain
emotion recognizer. Additionally, we found that it is feasible to train a
semi-effective speaker verification system using only emotion representations,
demonstrating the challenge of separating these two modalities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03568v1">Enabling Practical and Privacy-Preserving Image Processing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-09-05T14:22:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chao Wang, Shubing Yang, Xiaoyan Sun, Jun Dai, Dongfang Zhao</p>
    <p><b>Summary:</b> Fully Homomorphic Encryption (FHE) enables computations on encrypted data,
preserving confidentiality without the need for decryption. However, FHE is
often hindered by significant performance overhead, particularly for
high-precision and complex data like images. Due to serious efficiency issues,
traditional FHE methods often encrypt images by monolithic data blocks (such as
pixel rows), instead of pixels. However, this strategy compromises the
advantages of homomorphic operations and disables pixel-level image processing.
In this study, we address these challenges by proposing and implementing a
pixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS
scheme. To enhance computational efficiency, we introduce three novel caching
mechanisms to pre-encrypt radix values or frequently occurring pixel values,
substantially reducing redundant encryption operations. Extensive experiments
demonstrate that our approach achieves up to a 19-fold improvement in
encryption speed compared to the original CKKS, while maintaining high image
quality. Additionally, real-world image applications such as mean filtering,
brightness enhancement, image matching and watermarking are tested based on
FHE, showcasing up to a 91.53% speed improvement. We also proved that our
method is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,
providing strong encryption security. These results underscore the practicality
and efficiency of iCHEETAH, marking a significant advancement in
privacy-preserving image processing at scale.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03344v1">Rethinking Improved Privacy-Utility Trade-off with Pre-existing
  Knowledge for DP Training</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-05T08:40:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yu Zheng, Wenchao Zhang, Yonggang Zhang, Wei Song, Kai Zhou, Bo Han</p>
    <p><b>Summary:</b> Differential privacy (DP) provides a provable framework for protecting
individuals by customizing a random mechanism over a privacy-sensitive dataset.
Deep learning models have demonstrated privacy risks in model exposure as an
established learning model unintentionally records membership-level privacy
leakage. Differentially private stochastic gradient descent (DP- SGD) has been
proposed to safeguard training individuals by adding random Gaussian noise to
gradient updates in the backpropagation. Researchers identify that DP-SGD
typically causes utility loss since the injected homogeneous noise alters the
gradient updates calculated at each iteration. Namely, all elements in the
gradient are contaminated regardless of their importance in updating model
parameters. In this work, we argue that the utility loss mainly results from
the homogeneity of injected noise. Consequently, we propose a generic
differential privacy framework with heterogeneous noise (DP-Hero) by defining a
heterogeneous random mechanism to abstract its property. The insight of DP-Hero
is to leverage the knowledge encoded in the previously trained model to guide
the subsequent allocation of noise heterogeneity, thereby leveraging the
statistical perturbation and achieving enhanced utility. Atop DP-Hero, we
instantiate a heterogeneous version of DP-SGD, where the noise injected into
gradients is heterogeneous and guided by prior-established model parameters. We
conduct comprehensive experiments to verify and explain the effectiveness of
the proposed DP-Hero, showing improved training accuracy compared with
state-of-the-art works. Broadly, we shed light on improving the privacy-utility
space by learning the noise guidance from the pre-existing leaked knowledge
encoded in the previously trained model, showing a different perspective of
understanding the utility-improved DP training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03326v1">Enhancing User-Centric Privacy Protection: An Interactive Framework
  through Diffusion Models and Machine Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-09-05T07:55:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Huaxi Huang, Xin Yuan, Qiyu Liao, Dadong Wang, Tongliang Liu</p>
    <p><b>Summary:</b> In the realm of multimedia data analysis, the extensive use of image datasets
has escalated concerns over privacy protection within such data. Current
research predominantly focuses on privacy protection either in data sharing or
upon the release of trained machine learning models. Our study pioneers a
comprehensive privacy protection framework that safeguards image data privacy
concurrently during data sharing and model publication. We propose an
interactive image privacy protection framework that utilizes generative machine
learning models to modify image information at the attribute level and employs
machine unlearning algorithms for the privacy preservation of model parameters.
This user-interactive framework allows for adjustments in privacy protection
intensity based on user feedback on generated images, striking a balance
between maximal privacy safeguarding and maintaining model performance. Within
this framework, we instantiate two modules: a differential privacy diffusion
model for protecting attribute information in images and a feature unlearning
algorithm for efficient updates of the trained model on the revised image
dataset. Our approach demonstrated superiority over existing methods on facial
datasets across various attribute classifications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03294v1">Federated Prototype-based Contrastive Learning for Privacy-Preserving
  Cross-domain Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-09-05T06:59:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Li Wang, Quangui Zhang, Lei Sang, Qiang Wu, Min Xu</p>
    <p><b>Summary:</b> Cross-domain recommendation (CDR) aims to improve recommendation accuracy in
sparse domains by transferring knowledge from data-rich domains. However,
existing CDR methods often assume the availability of user-item interaction
data across domains, overlooking user privacy concerns. Furthermore, these
methods suffer from performance degradation in scenarios with sparse
overlapping users, as they typically depend on a large number of fully shared
users for effective knowledge transfer. To address these challenges, we propose
a Federated Prototype-based Contrastive Learning (CL) method for
Privacy-Preserving CDR, named FedPCL-CDR. This approach utilizes
non-overlapping user information and prototypes to improve multi-domain
performance while protecting user privacy. FedPCL-CDR comprises two modules:
local domain (client) learning and global server aggregation. In the local
domain, FedPCL-CDR clusters all user data to learn representative prototypes,
effectively utilizing non-overlapping user information and addressing the
sparse overlapping user issue. It then facilitates knowledge transfer by
employing both local and global prototypes returned from the server in a CL
manner. Simultaneously, the global server aggregates representative prototypes
from local domains to learn both local and global prototypes. The combination
of prototypes and federated learning (FL) ensures that sensitive user data
remains decentralized, with only prototypes being shared across domains,
thereby protecting user privacy. Extensive experiments on four CDR tasks using
two real-world datasets demonstrate that FedPCL-CDR outperforms the
state-of-the-art baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03796v1">Protecting Activity Sensing Data Privacy Using Hierarchical Information
  Dissociation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-04T15:38:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guangjing Wang, Hanqing Guo, Yuanda Wang, Bocheng Chen, Ce Zhou, Qiben Yan</p>
    <p><b>Summary:</b> Smartphones and wearable devices have been integrated into our daily lives,
offering personalized services. However, many apps become overprivileged as
their collected sensing data contains unnecessary sensitive information. For
example, mobile sensing data could reveal private attributes (e.g., gender and
age) and unintended sensitive features (e.g., hand gestures when entering
passwords). To prevent sensitive information leakage, existing methods must
obtain private labels and users need to specify privacy policies. However, they
only achieve limited control over information disclosure. In this work, we
present Hippo to dissociate hierarchical information including private metadata
and multi-grained activity information from the sensing data. Hippo achieves
fine-grained control over the disclosure of sensitive information without
requiring private labels. Specifically, we design a latent guidance-based
diffusion model, which generates multi-grained versions of raw sensor data
conditioned on hierarchical latent activity features. Hippo enables users to
control the disclosure of sensitive information in sensing data, ensuring their
privacy while preserving the necessary features to meet the utility
requirements of applications. Hippo is the first unified model that achieves
two goals: perturbing the sensitive attributes and controlling the disclosure
of sensitive information in mobile sensing data. Extensive experiments show
that Hippo can anonymize personal attributes and transform activity information
at various resolutions across different types of sensing data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02614v1">Evaluating the Effects of Digital Privacy Regulations on User Trust</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-09-04T11:11:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mehmet Berk Cetin</p>
    <p><b>Summary:</b> In today's digital society, issues related to digital privacy have become
increasingly important. Issues such as data breaches result in misuse of data,
financial loss, and cyberbullying, which leads to less user trust in digital
services. This research investigates the impact of digital privacy laws on user
trust by comparing the regulations in the Netherlands, Ghana, and Malaysia. The
study employs a comparative case study method, involving interviews with
digital privacy law experts, IT educators, and consumers from each country. The
main findings reveal that while the General Data Protection Regulation (GDPR)
in the Netherlands is strict, its practical impact is limited by enforcement
challenges. In Ghana, the Data Protection Act is underutilized due to low
public awareness and insufficient enforcement, leading to reliance on personal
protective measures. In Malaysia, trust in digital services is largely
dependent on the security practices of individual platforms rather than the
Personal Data Protection Act. The study highlights the importance of public
awareness, effective enforcement, and cultural considerations in shaping the
effectiveness of digital privacy laws. Based on these insights, a
recommendation framework is proposed to enhance digital privacy practices, also
aiming to provide valuable guidance for policymakers, businesses, and citizens
in navigating the challenges of digitalization.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02404v1">Learning Privacy-Preserving Student Networks via
  Discriminative-Generative Distillation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-04T03:06:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shiming Ge, Bochao Liu, Pengju Wang, Yong Li, Dan Zeng</p>
    <p><b>Summary:</b> While deep models have proved successful in learning rich knowledge from
massive well-annotated data, they may pose a privacy leakage risk in practical
deployment. It is necessary to find an effective trade-off between high utility
and strong privacy. In this work, we propose a discriminative-generative
distillation approach to learn privacy-preserving deep models. Our key idea is
taking models as bridge to distill knowledge from private data and then
transfer it to learn a student network via two streams. First, discriminative
stream trains a baseline classifier on private data and an ensemble of teachers
on multiple disjoint private subsets, respectively. Then, generative stream
takes the classifier as a fixed discriminator and trains a generator in a
data-free manner. After that, the generator is used to generate massive
synthetic data which are further applied to train a variational autoencoder
(VAE). Among these synthetic data, a few of them are fed into the teacher
ensemble to query labels via differentially private aggregation, while most of
them are embedded to the trained VAE for reconstructing synthetic data.
Finally, a semi-supervised student learning is performed to simultaneously
handle two tasks: knowledge transfer from the teachers with distillation on few
privately labeled synthetic data, and knowledge enhancement with tangent-normal
adversarial regularization on many triples of reconstructed synthetic data. In
this way, our approach can control query cost over private data and mitigate
accuracy degradation in a unified manner, leading to a privacy-preserving
student model. Extensive experiments and analysis clearly show the
effectiveness of the proposed approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02375v3">How Privacy-Savvy Are Large Language Models? A Case Study on Compliance
  and Privacy Technical Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-09-04T01:51:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xichou Zhu, Yang Liu, Zhou Shen, Yi Liu, Min Li, Yujun Chen, Benzi John, Zhenzhen Ma, Tao Hu, Zhi Li, Bolong Yang, Manman Wang, Zongxing Xie, Peng Liu, Dan Cai, Junhui Wang</p>
    <p><b>Summary:</b> The recent advances in large language models (LLMs) have significantly
expanded their applications across various fields such as language generation,
summarization, and complex question answering. However, their application to
privacy compliance and technical privacy reviews remains under-explored,
raising critical concerns about their ability to adhere to global privacy
standards and protect sensitive user data. This paper seeks to address this gap
by providing a comprehensive case study evaluating LLMs' performance in
privacy-related tasks such as privacy information extraction (PIE), legal and
regulatory key point detection (KPD), and question answering (QA) with respect
to privacy policies and data protection regulations. We introduce a Privacy
Technical Review (PTR) framework, highlighting its role in mitigating privacy
risks during the software development life-cycle. Through an empirical
assessment, we investigate the capacity of several prominent LLMs, including
BERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks
and technical privacy reviews. Our experiments benchmark the models across
multiple dimensions, focusing on their precision, recall, and F1-scores in
extracting privacy-sensitive information and detecting key regulatory
compliance points. While LLMs show promise in automating privacy reviews and
identifying regulatory discrepancies, significant gaps persist in their ability
to fully comply with evolving legal standards. We provide actionable
recommendations for enhancing LLMs' capabilities in privacy compliance,
emphasizing the need for robust model improvements and better integration with
legal and regulatory requirements. This study underscores the growing
importance of developing privacy-aware LLMs that can both support businesses in
compliance efforts and safeguard user privacy rights.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02364v1">Examining Caregiving Roles to Differentiate the Effects of Using a
  Mobile App for Community Oversight for Privacy and Security</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-09-04T01:21:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mamtaj Akter, Jess Kropczynski, Heather Lipford, Pamela Wisniewski</p>
    <p><b>Summary:</b> We conducted a 4-week field study with 101 smartphone users who
self-organized into 22 small groups of family, friends, and neighbors to use
``CO-oPS,'' a mobile app for co-managing mobile privacy and security. We
differentiated between those who provided oversight (i.e., caregivers) and
those who did not (i.e., caregivees) to examine differential effects on their
experiences and behaviors while using CO-oPS. Caregivers reported higher power
use, community trust, belonging, collective efficacy, and self-efficacy than
caregivees. Both groups' self-efficacy and collective efficacy for mobile
privacy and security increased after using CO-oPS. However, this increase was
significantly stronger for caregivees. Our research demonstrates how
community-based approaches can benefit people who need additional help managing
their digital privacy and security. We provide recommendations to support
community-based oversight for managing privacy and security within communities
of different roles and skills.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02044v1">FedMinds: Privacy-Preserving Personalized Brain Visual Decoding</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> 
  <p><b>Published on:</b> 2024-09-03T16:46:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guangyin Bao, Duoqian Miao</p>
    <p><b>Summary:</b> Exploring the mysteries of the human brain is a long-term research topic in
neuroscience. With the help of deep learning, decoding visual information from
human brain activity fMRI has achieved promising performance. However, these
decoding models require centralized storage of fMRI data to conduct training,
leading to potential privacy security issues. In this paper, we focus on
privacy preservation in multi-individual brain visual decoding. To this end, we
introduce a novel framework called FedMinds, which utilizes federated learning
to protect individuals' privacy during model training. In addition, we deploy
individual adapters for each subject, thus allowing personalized visual
decoding. We conduct experiments on the authoritative NSD datasets to evaluate
the performance of the proposed framework. The results demonstrate that our
framework achieves high-precision visual decoding along with privacy
protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01924v1">Privacy-Preserving and Post-Quantum Counter Denial of Service Framework
  for Wireless Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-03T14:14:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Saleh Darzi, Attila Altay Yavuz</p>
    <p><b>Summary:</b> As network services progress and mobile and IoT environments expand, numerous
security concerns have surfaced for spectrum access systems. The omnipresent
risk of Denial-of-Service (DoS) attacks and raising concerns about user privacy
(e.g., location privacy, anonymity) are among such cyber threats. These
security and privacy risks increase due to the threat of quantum computers that
can compromise long-term security by circumventing conventional cryptosystems
and increasing the cost of countermeasures. While some defense mechanisms exist
against these threats in isolation, there is a significant gap in the state of
the art on a holistic solution against DoS attacks with privacy and anonymity
for spectrum management systems, especially when post-quantum (PQ) security is
in mind. In this paper, we propose a new cybersecurity framework PACDoSQ, which
is (to the best of our knowledge) the first to offer location privacy and
anonymity for spectrum management with counter DoS and PQ security
simultaneously. Our solution introduces the private spectrum bastion (database)
concept to exploit existing architectural features of spectrum management
systems and then synergizes them with multi-server private information
retrieval and PQ-secure Tor to guarantee a location-private and anonymous
acquisition of spectrum information together with hash-based client-server
puzzles for counter DoS. We prove that PACDoSQ achieves its security
objectives, and show its feasibility via a comprehensive performance
evaluation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01710v1">Privacy-Preserving Multimedia Mobile Cloud Computing Using Protective
  Perturbation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2024-09-03T08:47:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhongze Tang, Mengmei Ye, Yao Liu, Sheng Wei</p>
    <p><b>Summary:</b> Mobile cloud computing has been adopted in many multimedia applications,
where the resource-constrained mobile device sends multimedia data (e.g.,
images) to remote cloud servers to request computation-intensive multimedia
services (e.g., image recognition). While significantly improving the
performance of the mobile applications, the cloud-based mechanism often causes
privacy concerns as the multimedia data and services are offloaded from the
trusted user device to untrusted cloud servers. Several recent studies have
proposed perturbation-based privacy preserving mechanisms, which obfuscate the
offloaded multimedia data to eliminate privacy exposures without affecting the
functionality of the remote multimedia services. However, the existing privacy
protection approaches require the deployment of computation-intensive
perturbation generation on the resource-constrained mobile devices. Also, the
obfuscated images are typically not compliant with the standard image
compression algorithms and suffer from significant bandwidth consumption. In
this paper, we develop a novel privacy-preserving multimedia mobile cloud
computing framework, namely $PMC^2$, to address the resource and bandwidth
challenges. $PMC^2$ employs secure confidential computing in the cloud to
deploy the perturbation generator, which addresses the resource challenge while
maintaining the privacy. Furthermore, we develop a neural compressor
specifically trained to compress the perturbed images in order to address the
bandwidth challenge. We implement $PMC^2$ in an end-to-end mobile cloud
computing system, based on which our evaluations demonstrate superior latency,
power efficiency, and bandwidth consumption achieved by $PMC^2$ while
maintaining high accuracy in the target multimedia service.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01661v1">$S^2$NeRF: Privacy-preserving Training Framework for NeRF</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-03T07:08:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bokang Zhang, Yanglin Zhang, Zhikun Zhang, Jinglan Yang, Lingying Huang, Junfeng Wu</p>
    <p><b>Summary:</b> Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and
graphics, facilitating novel view synthesis and influencing sectors like
extended reality and e-commerce. However, NeRF's dependence on extensive data
collection, including sensitive scene image data, introduces significant
privacy risks when users upload this data for model training. To address this
concern, we first propose SplitNeRF, a training framework that incorporates
split learning (SL) techniques to enable privacy-preserving collaborative model
training between clients and servers without sharing local data. Despite its
benefits, we identify vulnerabilities in SplitNeRF by developing two attack
methods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which
exploit the shared gradient data and a few leaked scene images to reconstruct
private scene information. To counter these threats, we introduce $S^2$NeRF,
secure SplitNeRF that integrates effective defense mechanisms. By introducing
decaying noise related to the gradient norm into the shared gradient
information, $S^2$NeRF preserves privacy while maintaining a high utility of
the NeRF model. Our extensive evaluations across multiple datasets demonstrate
the effectiveness of $S^2$NeRF against privacy breaches, confirming its
viability for secure NeRF training in sensitive applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01329v1">Assessing the Impact of Image Dataset Features on Privacy-Preserving
  Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-09-02T15:30:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lucas Lange, Maurice-Maximilian Heykeroth, Erhard Rahm</p>
    <p><b>Summary:</b> Machine Learning (ML) is crucial in many sectors, including computer vision.
However, ML models trained on sensitive data face security challenges, as they
can be attacked and leak information. Privacy-Preserving Machine Learning
(PPML) addresses this by using Differential Privacy (DP) to balance utility and
privacy. This study identifies image dataset characteristics that affect the
utility and vulnerability of private and non-private Convolutional Neural
Network (CNN) models. Through analyzing multiple datasets and privacy budgets,
we find that imbalanced datasets increase vulnerability in minority classes,
but DP mitigates this issue. Datasets with fewer classes improve both model
utility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR)
datasets deteriorate the utility-privacy trade-off. These insights offer
valuable guidance for practitioners and researchers in estimating and
optimizing the utility-privacy trade-off in image datasets, helping to inform
data and privacy modifications for better outcomes based on dataset
characteristics.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01088v1">Towards Split Learning-based Privacy-Preserving Record Linkage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-02T09:17:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michail Zervas, Alexandros Karakasidis</p>
    <p><b>Summary:</b> Split Learning has been recently introduced to facilitate applications where
user data privacy is a requirement. However, it has not been thoroughly studied
in the context of Privacy-Preserving Record Linkage, a problem in which the
same real-world entity should be identified among databases from different
dataholders, but without disclosing any additional information. In this paper,
we investigate the potentials of Split Learning for Privacy-Preserving Record
Matching, by introducing a novel training method through the utilization of
Reference Sets, which are publicly available data corpora, showcasing minimal
matching impact against a traditional centralized SVM-based technique.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00974v1">Enhancing Privacy in Federated Learning: Secure Aggregation for
  Real-World Healthcare Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-09-02T06:43:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Riccardo Taiello, Sergen Cansiz, Marc Vesin, Francesco Cremonesi, Lucia Innocenti, Melek Önen, Marco Lorenzi</p>
    <p><b>Summary:</b> Deploying federated learning (FL) in real-world scenarios, particularly in
healthcare, poses challenges in communication and security. In particular, with
respect to the federated aggregation procedure, researchers have been focusing
on the study of secure aggregation (SA) schemes to provide privacy guarantees
over the model's parameters transmitted by the clients. Nevertheless, the
practical availability of SA in currently available FL frameworks is currently
limited, due to computational and communication bottlenecks. To fill this gap,
this study explores the implementation of SA within the open-source Fed-BioMed
framework. We implement and compare two SA protocols, Joye-Libert (JL) and Low
Overhead Masking (LOM), by providing extensive benchmarks in a panel of
healthcare data analysis problems. Our theoretical and experimental evaluations
on four datasets demonstrate that SA protocols effectively protect privacy
while maintaining task accuracy. Computational overhead during training is less
than 1% on a CPU and less than 50% on a GPU for large models, with protection
phases taking less than 10 seconds. Incorporating SA into Fed-BioMed impacts
task accuracy by no more than 2% compared to non-SA scenarios. Overall this
study demonstrates the feasibility of SA in real-world healthcare applications
and contributes in reducing the gap towards the adoption of privacy-preserving
technologies in sensitive applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00740v1">VPVet: Vetting Privacy Policies of Virtual Reality Apps</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-01T15:07:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuxia Zhan, Yan Meng, Lu Zhou, Yichang Xiong, Xiaokuan Zhang, Lichuan Ma, Guoxing Chen, Qingqi Pei, Haojin Zhu</p>
    <p><b>Summary:</b> Virtual reality (VR) apps can harvest a wider range of user data than
web/mobile apps running on personal computers or smartphones. Existing law and
privacy regulations emphasize that VR developers should inform users of what
data are collected/used/shared (CUS) through privacy policies. However, privacy
policies in the VR ecosystem are still in their early stages, and many
developers fail to write appropriate privacy policies that comply with
regulations and meet user expectations. In this paper, we propose VPVet to
automatically vet privacy policy compliance issues for VR apps. VPVet first
analyzes the availability and completeness of a VR privacy policy and then
refines its analysis based on three key criteria: granularity, minimization,
and consistency of CUS statements. Our study establishes the first and
currently largest VR privacy policy dataset named VRPP, consisting of privacy
policies of 11,923 different VR apps from 10 mainstream platforms. Our vetting
results reveal severe privacy issues within the VR ecosystem, including the
limited availability and poor quality of privacy policies, along with their
coarse granularity, lack of adaptation to VR traits and the inconsistency
between CUS statements in privacy policies and their actual behaviors. We
open-source VPVet system along with our findings at repository
https://github.com/kalamoo/PPAudit, aiming to raise awareness within the VR
community and pave the way for further research in this field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00739v1">Designing and Evaluating Scalable Privacy Awareness and Control User
  Interfaces for Mixed Reality</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-09-01T15:06:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Marvin Strauss, Viktorija Paneva, Florian Alt, Stefan Schneegass</p>
    <p><b>Summary:</b> As Mixed Reality (MR) devices become increasingly popular across industries,
they raise significant privacy and ethical concerns due to their capacity to
collect extensive data on users and their environments. This paper highlights
the urgent need for privacy-aware user interfaces that educate and empower both
users and bystanders, enabling them to understand, control, and manage data
collection and sharing. Key research questions include improving user awareness
of privacy implications, developing usable privacy controls, and evaluating the
effectiveness of these measures in real-world settings. The proposed research
roadmap aims to embed privacy considerations into the design and development of
MR technologies, promoting responsible innovation that safeguards user privacy
while preserving the functionality and appeal of these emerging technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02715v1">Recoverable Anonymization for Pose Estimation: A Privacy-Enhancing
  Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-01T05:58:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenjun Huang, Yang Ni, Arghavan Rezvani, SungHeon Jeong, Hanning Chen, Yezi Liu, Fei Wen, Mohsen Imani</p>
    <p><b>Summary:</b> Human pose estimation (HPE) is crucial for various applications. However,
deploying HPE algorithms in surveillance contexts raises significant privacy
concerns due to the potential leakage of sensitive personal information (SPI)
such as facial features, and ethnicity. Existing privacy-enhancing methods
often compromise either privacy or performance, or they require costly
additional modalities. We propose a novel privacy-enhancing system that
generates privacy-enhanced portraits while maintaining high HPE performance.
Our key innovations include the reversible recovery of SPI for authorized
personnel and the preservation of contextual information. By jointly optimizing
a privacy-enhancing module, a privacy recovery module, and a pose estimator,
our system ensures robust privacy protection, efficient SPI recovery, and
high-performance HPE. Experimental results demonstrate the system's robust
performance in privacy enhancement, SPI recovery, and HPE.</p>
  </details>
</div>

