
<h2>2025-01</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.00192v1">PRECISE: PRivacy-loss-Efficient and Consistent Inference based on
  poSterior quantilEs</a></h3>
  
  <p><b>Published on:</b> 2025-01-31T22:18:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ruyu Zhou, Fang Liu</p>
    <p><b>Summary:</b> Differential privacy (DP) is a mathematical framework for releasing
information with formal privacy guarantees. Despite the existence of various DP
procedures for performing a wide range of statistical analysis and machine
learning tasks, methods of good utility are still lacking in valid statistical
inference with DP guarantees. We address this gap by introducing the notion of
valid Privacy-Preserving Interval Estimation (PPIE) and proposing
PRivacy-loss-Efficient and Consistent Inference based on poSterior quantilEs
(PRECISE). PRECISE is a general-purpose Bayesian approach for constructing
privacy-preserving posterior intervals. We establish the Mean-Squared-Error
(MSE) consistency for our proposed private posterior quantiles converging to
the population posterior quantile as sample size or privacy loss increases. We
conduct extensive experiments to compare the utilities of PRECISE with common
existing privacy-preserving inferential approaches in various inferential
tasks, data types and sizes,and privacy loss levels. The results demonstrated a
significant advantage of PRECISE with its nominal coverage and substantially
narrower intervals than the existing methods, which are prone to either
under-coverage or impractically wide intervals.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.19223v1">Through the Looking Glass: LLM-Based Analysis of AR/VR Android
  Applications Privacy Policies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-31T15:30:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abdulaziz Alghamdi, David Mohaisen</p>
    <p><b>Summary:</b> \begin{abstract} This paper comprehensively analyzes privacy policies in
AR/VR applications, leveraging BERT, a state-of-the-art text classification
model, to evaluate the clarity and thoroughness of these policies. By comparing
the privacy policies of AR/VR applications with those of free and premium
websites, this study provides a broad perspective on the current state of
privacy practices within the AR/VR industry. Our findings indicate that AR/VR
applications generally offer a higher percentage of positive segments than free
content but lower than premium websites. The analysis of highlighted segments
and words revealed that AR/VR applications strategically emphasize critical
privacy practices and key terms. This enhances privacy policies' clarity and
effectiveness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.00068v1">Privacy Preserving Charge Location Prediction for Electric Vehicles</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> 
  <p><b>Published on:</b> 2025-01-31T03:14:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Robert Marlin, Raja Jurdak, Alsharif Abuadbba, Dimity Miller</p>
    <p><b>Summary:</b> By 2050, electric vehicles (EVs) are projected to account for 70% of global
vehicle sales. While EVs provide environmental benefits, they also pose
challenges for energy generation, grid infrastructure, and data privacy.
Current research on EV routing and charge management often overlooks privacy
when predicting energy demands, leaving sensitive mobility data vulnerable. To
address this, we developed a Federated Learning Transformer Network (FLTN) to
predict EVs' next charge location with enhanced privacy measures. Each EV
operates as a client, training an onboard FLTN model that shares only model
weights, not raw data with a community-based Distributed Energy Resource
Management System (DERMS), which aggregates them into a community global model.
To further enhance privacy, non-transitory EVs use peer-to-peer weight sharing
and augmentation within their community, obfuscating individual contributions
and improving model accuracy. Community DERMS global model weights are then
redistributed to EVs for continuous training. Our FLTN approach achieved up to
92% accuracy while preserving data privacy, compared to our baseline
centralised model, which achieved 98% accuracy with no data privacy.
Simulations conducted across diverse charge levels confirm the FLTN's ability
to forecast energy demands over extended periods. We present a privacy-focused
solution for forecasting EV charge location prediction, effectively mitigating
data leakage risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.18862v2">Scalable Distributed Reproduction Numbers of Network Epidemics with
  Differential Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-01-31T03:08:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bo Chen, Baike She, Calvin Hawkins, Philip E. Par√©, Matthew T. Hale</p>
    <p><b>Summary:</b> Reproduction numbers are widely used for the estimation and prediction of
epidemic spreading processes over networks. However, conventional reproduction
numbers of an overall network do not indicate where an epidemic is spreading.
Therefore, we propose a novel notion of local distributed reproduction numbers
to capture the spreading behaviors of each node in a network. We first show how
to compute them and then use them to derive new conditions under which an
outbreak can occur. These conditions are then used to derive new conditions for
the existence, uniqueness, and stability of equilibrium states of the
underlying epidemic model. Building upon these local distributed reproduction
numbers, we define cluster distributed reproduction numbers to model the spread
between clusters composed of nodes. Furthermore, we demonstrate that the local
distributed reproduction numbers can be aggregated into cluster distributed
reproduction numbers at different scales. However, both local and cluster
distributed reproduction numbers can reveal the frequency of interactions
between nodes in a network, which raises privacy concerns. Thus, we next
develop a privacy framework that implements a differential privacy mechanism to
provably protect the frequency of interactions between nodes when computing
distributed reproduction numbers. Numerical experiments show that, even under
differential privacy, the distributed reproduction numbers provide accurate
estimates of the epidemic spread while also providing more insights than
conventional reproduction numbers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.00067v1">Decoding User Concerns in AI Health Chatbots: An Exploration of Security
  and Privacy in App Reviews</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-01-31T00:38:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Muhammad Hassan, Abdullah Ghani, Muhammad Fareed Zaffar, Masooda Bashir</p>
    <p><b>Summary:</b> AI powered health chatbot applications are increasingly utilized for
personalized healthcare services, yet they pose significant challenges related
to user data security and privacy. This study evaluates the effectiveness of
automated methods, specifically BART and Gemini GenAI, in identifying security
privacy related (SPR) concerns within these applications' user reviews,
benchmarking their performance against manual qualitative analysis. Our results
indicate that while Gemini's performance in SPR classification is comparable to
manual labeling, both automated methods have limitations, including the
misclassification of unrelated issues. Qualitative analysis revealed critical
user concerns, such as data collection practices, data misuse, and insufficient
transparency and consent mechanisms. This research enhances the understanding
of the relationship between user trust, privacy, and emerging mobile AI health
chatbot technologies, offering actionable insights for improving security and
privacy practices in AI driven health chatbots. Although exploratory, our
findings highlight the necessity for rigorous audits and transparent
communication strategies, providing valuable guidance for app developers and
vendors in addressing user security and privacy concerns.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.18727v2">Exploring Audio Editing Features as User-Centric Privacy Defenses
  Against Large Language Model(LLM) Based Emotion Inference Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> 
  <p><b>Published on:</b> 2025-01-30T20:07:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohd. Farhan Israk Soumik, W. K. M. Mithsara, Abdur R. Shahid, Ahmed Imteaj</p>
    <p><b>Summary:</b> The rapid proliferation of speech-enabled technologies, including virtual
assistants, video conferencing platforms, and wearable devices, has raised
significant privacy concerns, particularly regarding the inference of sensitive
emotional information from audio data. Existing privacy-preserving methods
often compromise usability and security, limiting their adoption in practical
scenarios. This paper introduces a novel, user-centric approach that leverages
familiar audio editing techniques, specifically pitch and tempo manipulation,
to protect emotional privacy without sacrificing usability. By analyzing
popular audio editing applications on Android and iOS platforms, we identified
these features as both widely available and usable. We rigorously evaluated
their effectiveness against a threat model, considering adversarial attacks
from diverse sources, including Deep Neural Networks (DNNs), Large Language
Models (LLMs), and and reversibility testing. Our experiments, conducted on
three distinct datasets, demonstrate that pitch and tempo manipulation
effectively obfuscates emotional data. Additionally, we explore the design
principles for lightweight, on-device implementation to ensure broad
applicability across various devices and platforms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.18174v1">Advancing Personalized Federated Learning: Integrative Approaches with
  AI for Enhanced Privacy and Customization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-01-30T07:03:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kevin Cooper, Michael Geller</p>
    <p><b>Summary:</b> In the age of data-driven decision making, preserving privacy while providing
personalized experiences has become paramount. Personalized Federated Learning
(PFL) offers a promising framework by decentralizing the learning process, thus
ensuring data privacy and reducing reliance on centralized data repositories.
However, the integration of advanced Artificial Intelligence (AI) techniques
within PFL remains underexplored. This paper proposes a novel approach that
enhances PFL with cutting-edge AI methodologies including adaptive
optimization, transfer learning, and differential privacy. We present a model
that not only boosts the performance of individual client models but also
ensures robust privacy-preserving mechanisms and efficient resource utilization
across heterogeneous networks. Empirical results demonstrate significant
improvements in model accuracy and personalization, along with stringent
privacy adherence, as compared to conventional federated learning models. This
work paves the way for a new era of truly personalized and privacy-conscious AI
systems, offering significant implications for industries requiring compliance
with stringent data protection regulations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.01649v1">Privacy-Preserving Edge Speech Understanding with Tiny Foundation Models</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">
  <p><b>Published on:</b> 2025-01-29T18:55:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Afsara Benazir, Felix Xiaozhu Lin</p>
    <p><b>Summary:</b> Robust speech recognition systems rely on cloud service providers for
inference. It needs to ensure that an untrustworthy provider cannot deduce the
sensitive content in speech. Sanitization can be done on speech content keeping
in mind that it has to avoid compromising transcription accuracy. Realizing the
under utilized capabilities of tiny speech foundation models (FMs), for the
first time, we propose a novel use: enhancing speech privacy on
resource-constrained devices. We introduce XYZ, an edge/cloud privacy
preserving speech inference engine that can filter sensitive entities without
compromising transcript accuracy. We utilize a timestamp based on-device
masking approach that utilizes a token to entity prediction model to filter
sensitive entities. Our choice of mask strategically conceals parts of the
input and hides sensitive data. The masked input is sent to a trusted cloud
service or to a local hub to generate the masked output. The effectiveness of
XYZ hinges on how well the entity time segments are masked. Our recovery is a
confidence score based approach that chooses the best prediction between cloud
and on-device model. We implement XYZ on a 64 bit Raspberry Pi 4B. Experiments
show that our solution leads to robust speech recognition without forsaking
privacy. XYZ with < 100 MB memory, achieves state-of-the-art (SOTA) speech
transcription performance while filtering about 83% of private entities
directly on-device. XYZ is 16x smaller in memory and 17x more compute efficient
than prior privacy preserving speech frameworks and has a relative reduction in
word error rate (WER) by 38.8-77.5% when compared to existing offline
transcription services.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.17762v2">Improving Privacy Benefits of Redaction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-29T16:53:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vaibhav Gusain, Douglas Leith</p>
    <p><b>Summary:</b> We propose a novel redaction methodology that can be used to sanitize natural
text data. Our new technique provides better privacy benefits than other state
of the art techniques while maintaining lower redaction levels.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.17750v1">Privacy Audit as Bits Transmission: (Im)possibilities for Audit by One
  Run</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-29T16:38:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zihang Xiang, Tianhao Wang, Di Wang</p>
    <p><b>Summary:</b> Auditing algorithms' privacy typically involves simulating a game-based
protocol that guesses which of two adjacent datasets was the original input.
Traditional approaches require thousands of such simulations, leading to
significant computational overhead. Recent methods propose single-run auditing
of the target algorithm to address this, substantially reducing computational
cost. However, these methods' general applicability and tightness in producing
empirical privacy guarantees remain uncertain.
  This work studies such problems in detail. Our contributions are twofold:
First, we introduce a unifying framework for privacy audits based on
information-theoretic principles, modeling the audit as a bit transmission
problem in a noisy channel. This formulation allows us to derive fundamental
limits and develop an audit approach that yields tight privacy lower bounds for
various DP protocols. Second, leveraging this framework, we demystify the
method of privacy audit by one run, identifying the conditions under which
single-run audits are feasible or infeasible. Our analysis provides general
guidelines for conducting privacy audits and offers deeper insights into the
privacy audit.
  Finally, through experiments, we demonstrate that our approach produces
tighter privacy lower bounds on common differentially private mechanisms while
requiring significantly fewer observations. We also provide a case study
illustrating that our method successfully detects privacy violations in flawed
implementations of private algorithms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.17634v1">Federated Learning With Individualized Privacy Through Client Sampling</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-29T13:11:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lucas Lange, Ole Borchardt, Erhard Rahm</p>
    <p><b>Summary:</b> With growing concerns about user data collection, individualized privacy has
emerged as a promising solution to balance protection and utility by accounting
for diverse user privacy preferences. Instead of enforcing a uniform level of
anonymization for all users, this approach allows individuals to choose privacy
settings that align with their comfort levels. Building on this idea, we
propose an adapted method for enabling Individualized Differential Privacy
(IDP) in Federated Learning (FL) by handling clients according to their
personal privacy preferences. By extending the SAMPLE algorithm from
centralized settings to FL, we calculate client-specific sampling rates based
on their heterogeneous privacy budgets and integrate them into a modified
IDP-FedAvg algorithm. We test this method under realistic privacy distributions
and multiple datasets. The experimental results demonstrate that our approach
achieves clear improvements over uniform DP baselines, reducing the trade-off
between privacy and utility. Compared to the alternative SCALE method in
related work, which assigns differing noise scales to clients, our method
performs notably better. However, challenges remain for complex tasks with
non-i.i.d. data, primarily stemming from the constraints of the decentralized
setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.17089v1">CRSet: Non-Interactive Verifiable Credential Revocation with Metadata
  Privacy for Issuers and Everyone Else</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-28T17:23:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Felix Hoops, Jonas Gebele, Florian Matthes</p>
    <p><b>Summary:</b> Like any digital certificate, Verifiable Credentials (VCs) require a way to
revoke them in case of an error or key compromise. Existing solutions for VC
revocation, most prominently Bitstring Status List, are not viable for many use
cases since they leak the issuer's behavior, which in turn leaks internal
business metrics. For instance, exact staff fluctuation through issuance and
revocation of employee IDs. We introduce CRSet, a revocation mechanism that
allows an issuer to encode revocation information for years worth of VCs as a
Bloom filter cascade. Padding is used to provide deniability for issuer
metrics. Issuers periodically publish this filter cascade on a decentralized
storage system. Relying Parties (RPs) can download it to perform any number of
revocation checks locally. Compared to existing solutions, CRSet protects the
metadata of subject, RPs, and issuer equally. At the same time, it is
non-interactive, making it work with wallet devices having limited hardware
power and drop-in compatible with existing VC exchange protocols and wallet
applications. We present a prototype using the Ethereum blockchain as
decentralized storage. The recently introduced blob-carrying transactions,
enabling cheaper data writes, allow us to write each CRSet directly to the
chain. We built software for issuers and RPs that we successfully tested
end-to-end with an existing publicly available wallet agents and the OpenID for
Verifiable Credentials protocols. Storage and bandwidth costs paid by issuers
and RP are higher than for Bitstring Status List, but still manageable at
around 1 MB for an issuer issuing hundreds of thousands of VCs annually and
covering decades.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.16885v1">"My Whereabouts, my Location, it's Directly Linked to my Physical
  Security": An Exploratory Qualitative Study of Location-Dependent Security
  and Privacy Perceptions among Activist Tech Users</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-01-28T12:13:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Christian Eichenm√ºller, Lisa Kuhn, Zinaida Benenson</p>
    <p><b>Summary:</b> Digital-safety research with at-risk users is particularly urgent. At-risk
users are more likely to be digitally attacked or targeted by surveillance and
could be disproportionately harmed by attacks that facilitate physical
assaults. One group of such at-risk users are activists and politically active
individuals. For them, as for other at-risk users, the rise of smart
environments harbors new risks. Since digitization and datafication are no
longer limited to a series of personal devices that can be switched on and off,
but increasingly and continuously surround users, granular geolocation poses
new safety challenges. Drawing on eight exploratory qualitative interviews of
an ongoing research project, this contribution highlights what activists with
powerful adversaries think about evermore data traces, including location data,
and how they intend to deal with emerging risks. Responses of activists include
attempts to control one's immediate technological surroundings and to more
carefully manage device-related location data. For some activists, threat
modeling has also shaped provider choices based on geopolitical considerations.
Since many activists have not enough digital-safety knowledge for effective
protection, feelings of insecurity and paranoia are widespread. Channeling the
concerns and fears of our interlocutors, we call for more research on how
activists can protect themselves against evermore fine-grained location data
tracking.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.16307v1">Privacy-aware Nash Equilibrium Synthesis with Partially Ordered LTL$_f$
  Objectives</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Science and Game Theory-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Logic in Computer Science-662E9B">
  <p><b>Published on:</b> 2025-01-27T18:46:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Caleb Probine, Abhishek Kulkarni, Ufuk Topcu</p>
    <p><b>Summary:</b> Nash equilibrium is a fundamental solution concept for modeling the behavior
of self-interested agents. We develop an algorithm to synthesize pure Nash
equilibria in two-player deterministic games on graphs where players have
partial preferences over objectives expressed with linear temporal logic over
finite traces. Previous approaches for Nash equilibrium synthesis assume that
players' preferences are common knowledge. Instead, we allow players'
preferences to remain private but enable communication between players. The
algorithm we design synthesizes Nash equilibria for a complete-information
game, but synthesizes these equilibria in an incomplete-information setting
where players' preferences are private. The algorithm is privacy-aware, as
instead of requiring that players share private preferences, the algorithm
reduces the information sharing to a query interface. Through this interface,
players exchange information about states in the game from which they can
enforce a more desirable outcome. We prove the algorithm's completeness by
showing that it either returns an equilibrium or certifies that one does not
exist. We then demonstrate, via numerical examples, the existence of games
where the queries the players exchange are insufficient to reconstruct players'
preferences, highlighting the privacy-aware nature of the algorithm we propose.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.16033v1">PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy
  Assessment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> 
  <p><b>Published on:</b> 2025-01-27T13:27:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vincent Freiberger, Arthur Fleig, Erik Buchmann</p>
    <p><b>Summary:</b> Protecting online privacy requires users to engage with and comprehend
website privacy policies, but many policies are difficult and tedious to read.
We present PRISMe (Privacy Risk Information Scanner for Me), a novel Large
Language Model (LLM)-driven privacy policy assessment tool, which helps users
to understand the essence of a lengthy, complex privacy policy while browsing.
The tool, a browser extension, integrates a dashboard and an LLM chat. One
major contribution is the first rigorous evaluation of such a tool. In a
mixed-methods user study (N=22), we evaluate PRISMe's efficiency, usability,
understandability of the provided information, and impacts on awareness. While
our tool improves privacy awareness by providing a comprehensible quick
overview and a quality chat for in-depth discussion, users note issues with
consistency and building trust in the tool. From our insights, we derive
important design implications to guide future policy analysis tools.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.18625v1">DUEF-GA: Data Utility and Privacy Evaluation Framework for Graph
  Anonymization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-27T12:22:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jordi Casas-Roma</p>
    <p><b>Summary:</b> Anonymization of graph-based data is a problem which has been widely studied
over the last years and several anonymization methods have been developed.
Information loss measures have been used to evaluate data utility and
information loss in the anonymized graphs. However, there is no consensus about
how to evaluate data utility and information loss in privacy-preserving and
anonymization scenarios, where the anonymous datasets were perturbed to hinder
re-identification processes. Authors use diverse metrics to evaluate data
utility and, consequently, it is complex to compare different methods or
algorithms in literature. In this paper we propose a framework to evaluate and
compare anonymous datasets in a common way, providing an objective score to
clearly compare methods and algorithms. Our framework includes metrics based on
generic information loss measures, such as average distance or betweenness
centrality, and also task-specific information loss measures, such as community
detection or information flow. Additionally, we provide some metrics to examine
re-identification and risk assessment. We demonstrate that our framework could
help researchers and practitioners to select the best parametrization and/or
algorithm to reduce information loss and maximize data utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15751v1">A Privacy Model for Classical & Learned Bloom Filters</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-27T03:35:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hayder Tirmazi</p>
    <p><b>Summary:</b> The Classical Bloom Filter (CBF) is a class of Probabilistic Data Structures
(PDS) for handling Approximate Query Membership (AMQ). The Learned Bloom Filter
(LBF) is a recently proposed class of PDS that combines the Classical Bloom
Filter with a Learning Model while preserving the Bloom Filter's one-sided
error guarantees. Bloom Filters have been used in settings where inputs are
sensitive and need to be private in the presence of an adversary with access to
the Bloom Filter through an API or in the presence of an adversary who has
access to the internal state of the Bloom Filter. Prior work has investigated
the privacy of the Classical Bloom Filter providing attacks and defenses under
various privacy definitions. In this work, we formulate a stronger differential
privacy-based model for the Bloom Filter. We propose constructions of the
Classical and Learned Bloom Filter that satisfy $(\epsilon, 0)$-differential
privacy. This is also the first work that analyses and addresses the privacy of
the Learned Bloom Filter under any rigorous model, which is an open problem.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15744v1">Noise disturbance and lack of privacy: Modeling acoustic dissatisfaction
  in open-plan offices</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">
  <p><b>Published on:</b> 2025-01-27T03:10:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Manuj Yadav, Jungsoo Kim, Valtteri Hongisto, Densil Cabrera, Richard de Dear</p>
    <p><b>Summary:</b> Open-plan offices are well-known to be adversely affected by acoustic issues.
This study aims to model acoustic dissatisfaction using measurements of room
acoustics, sound environment during occupancy, and occupant surveys (n = 349)
in 28 offices representing a diverse range of workplace parameters. As latent
factors, the contribution of $\textit{lack of privacy}$ (LackPriv) was 25%
higher than $\textit{noise disturbance}$ (NseDstrb) in predicting
$\textit{acoustic dissatisfaction}$ (AcDsat). Room acoustic metrics based on
sound pressure level (SPL) decay of speech ($L_{\text{p,A,s,4m}}$ and
$r_{\text{C}}$) were better in predicting these factors than distraction
distance ($r_{\text{D}}$) based on speech transmission index. This contradicts
previous findings, and the trends for SPL-based metrics in predicting AcDsat
and LackPriv go against expectations based on ISO 3382-3. For sound during
occupation, $L_{\text{A,90}}$ and psychoacoustic loudness ($N_{\text{90}}$)
predicted AcDsat, and a SPL fluctuation metric ($M_{\text{A,eq}}$) predicted
LackPriv. However, these metrics were weaker predictors than ISO 3382-3
metrics. Medium-sized offices exhibited higher dissatisfaction than larger
($\geq$50 occupants) offices. Dissatisfaction varied substantially across
parameters including ceiling heights, number of workstations, and years of
work, but not between offices with fixed seating compared to more flexible and
activity-based working configurations. Overall, these findings highlight the
complexities in characterizing occupants' perceptions using instrumental
acoustic measurements.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15653v1">A Privacy Enhancing Technique to Evade Detection by Street Video Cameras
  Without Using Adversarial Accessories</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-26T19:29:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jacob Shams, Ben Nassi, Satoru Koda, Asaf Shabtai, Yuval Elovici</p>
    <p><b>Summary:</b> In this paper, we propose a privacy-enhancing technique leveraging an
inherent property of automatic pedestrian detection algorithms, namely, that
the training of deep neural network (DNN) based methods is generally performed
using curated datasets and laboratory settings, while the operational areas of
these methods are dynamic real-world environments. In particular, we leverage a
novel side effect of this gap between the laboratory and the real world:
location-based weakness in pedestrian detection. We demonstrate that the
position (distance, angle, height) of a person, and ambient light level,
directly impact the confidence of a pedestrian detector when detecting the
person. We then demonstrate that this phenomenon is present in pedestrian
detectors observing a stationary scene of pedestrian traffic, with blind spot
areas of weak detection of pedestrians with low confidence. We show how
privacy-concerned pedestrians can leverage these blind spots to evade detection
by constructing a minimum confidence path between two points in a scene,
reducing the maximum confidence and average confidence of the path by up to
0.09 and 0.13, respectively, over direct and random paths through the scene. To
counter this phenomenon, and force the use of more costly and sophisticated
methods to leverage this vulnerability, we propose a novel countermeasure to
improve the confidence of pedestrian detectors in blind spots, raising the
max/average confidence of paths generated by our technique by 0.09 and 0.05,
respectively. In addition, we demonstrate that our countermeasure improves a
Faster R-CNN-based pedestrian detector's TPR and average true positive
confidence by 0.03 and 0.15, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15395v1">Hiding in Plain Sight: An IoT Traffic Camouflage Framework for Enhanced
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2025-01-26T04:33:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Adu Worae, Spyridon Mastorakis</p>
    <p><b>Summary:</b> The rapid growth of Internet of Things (IoT) devices has introduced
significant challenges to privacy, particularly as network traffic analysis
techniques evolve. While encryption protects data content, traffic attributes
such as packet size and timing can reveal sensitive information about users and
devices. Existing single-technique obfuscation methods, such as packet padding,
often fall short in dynamic environments like smart homes due to their
predictability, making them vulnerable to machine learning-based attacks. This
paper introduces a multi-technique obfuscation framework designed to enhance
privacy by disrupting traffic analysis. The framework leverages six
techniques-Padding, Padding with XORing, Padding with Shifting, Constant Size
Padding, Fragmentation, and Delay Randomization-to obscure traffic patterns
effectively. Evaluations on three public datasets demonstrate significant
reductions in classifier performance metrics, including accuracy, precision,
recall, and F1 score. We assess the framework's robustness against adversarial
tactics by retraining and fine-tuning neural network classifiers on obfuscated
traffic. The results reveal a notable degradation in classifier performance,
underscoring the framework's resilience against adaptive attacks. Furthermore,
we evaluate communication and system performance, showing that higher
obfuscation levels enhance privacy but may increase latency and communication
overhead.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15363v1">AI-Driven Secure Data Sharing: A Trustworthy and Privacy-Preserving
  Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-26T02:03:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Al Amin, Kamrul Hasan, Sharif Ullah, Liang Hong</p>
    <p><b>Summary:</b> In the era of data-driven decision-making, ensuring the privacy and security
of shared data is paramount across various domains. Applying existing deep
neural networks (DNNs) to encrypted data is critical and often compromises
performance, security, and computational overhead. To address these
limitations, this research introduces a secure framework consisting of a
learnable encryption method based on the block-pixel operation to encrypt the
data and subsequently integrate it with the Vision Transformer (ViT). The
proposed framework ensures data privacy and security by creating unique
scrambling patterns per key, providing robust performance against adversarial
attacks without compromising computational efficiency and data integrity. The
framework was tested on sensitive medical datasets to validate its efficacy,
proving its ability to handle highly confidential information securely. The
suggested framework was validated with a 94\% success rate after extensive
testing on real-world datasets, such as MRI brain tumors and histological scans
of lung and colon cancers. Additionally, the framework was tested under diverse
adversarial attempts against secure data sharing with optimum performance and
demonstrated its effectiveness in various threat scenarios. These comprehensive
analyses underscore its robustness, making it a trustworthy solution for secure
data sharing in critical applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15032v1">Stealthy Voice Eavesdropping with Acoustic Metamaterials: Unraveling a
  New Privacy Threat</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-01-25T02:30:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhiyuan Ning, Zhanyong Tang, Juan He, Weizhi Meng, Yuntian Chen</p>
    <p><b>Summary:</b> We present SuperEar, a novel privacy threat based on acoustic metamaterials.
Unlike previous research, SuperEar can surreptitiously track and eavesdrop on
the phone calls of a moving outdoor target from a safe distance. To design this
attack, SuperEar overcomes the challenges faced by traditional acoustic
metamaterials, including low low-frequency gain and audio distortion during
reconstruction. It successfully magnifies the speech signal by approximately 20
times, allowing the sound to be captured from the earpiece of the target phone.
In addition, SuperEar optimizes the trade-off between the number and size of
acoustic metamaterials, improving the portability and concealability of the
interceptor while ensuring effective interception performance. This makes it
highly suitable for outdoor tracking and eavesdropping scenarios. Through
extensive experimentation, we have evaluated SuperEar and our results show that
it can achieve an eavesdropping accuracy of over 80% within a range of 4.5
meters in the aforementioned scenario, thus validating its great potential in
real-world applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14974v2">Private Minimum Hellinger Distance Estimation via Hellinger Distance
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Probability-5BC0EB">    
  <p><b>Published on:</b> 2025-01-24T23:15:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fengnan Deng, Anand N. Vidyashankar</p>
    <p><b>Summary:</b> Objective functions based on Hellinger distance yield robust and efficient
estimators of model parameters. Motivated by privacy and regulatory
requirements encountered in contemporary applications, we derive in this paper
\emph{private minimum Hellinger distance estimators}. The estimators satisfy a
new privacy constraint, namely, Hellinger differential privacy, while retaining
the robustness and efficiency properties. We demonstrate that Hellinger
differential privacy shares several features of standard differential privacy
while allowing for sharper inference. Additionally, for computational purposes,
we also develop Hellinger differentially private gradient descent and
Newton-Raphson algorithms. We illustrate the behavior of our estimators in
finite samples using numerical experiments and verify that they retain
robustness properties under gross-error contamination.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14928v1">Decision Making in Changing Environments: Robustness, Query-Based
  Learning, and Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36">  
  <p><b>Published on:</b> 2025-01-24T21:31:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fan Chen, Alexander Rakhlin</p>
    <p><b>Summary:</b> We study the problem of interactive decision making in which the underlying
environment changes over time subject to given constraints. We propose a
framework, which we call \textit{hybrid Decision Making with Structured
Observations} (hybrid DMSO), that provides an interpolation between the
stochastic and adversarial settings of decision making. Within this framework,
we can analyze local differentially private (LDP) decision making, query-based
learning (in particular, SQ learning), and robust and smooth decision making
under the same umbrella, deriving upper and lower bounds based on variants of
the Decision-Estimation Coefficient (DEC). We further establish strong
connections between the DEC's behavior, the SQ dimension, local minimax
complexity, learnability, and joint differential privacy. To showcase the
framework's power, we provide new results for contextual bandits under the LDP
constraint.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14453v1">Optimal Strategies for Federated Learning Maintaining Client Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-24T12:34:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Uday Bhaskar, Varul Srivastava, Avyukta Manjunatha Vummintala, Naresh Manwani, Sujit Gujar</p>
    <p><b>Summary:</b> Federated Learning (FL) emerged as a learning method to enable the server to
train models over data distributed among various clients. These clients are
protective about their data being leaked to the server, any other client, or an
external adversary, and hence, locally train the model and share it with the
server rather than sharing the data. The introduction of sophisticated
inferencing attacks enabled the leakage of information about data through
access to model parameters. To tackle this challenge, privacy-preserving
federated learning aims to achieve differential privacy through learning
algorithms like DP-SGD. However, such methods involve adding noise to the
model, data, or gradients, reducing the model's performance.
  This work provides a theoretical analysis of the tradeoff between model
performance and communication complexity of the FL system. We formally prove
that training for one local epoch per global round of training gives optimal
performance while preserving the same privacy budget. We also investigate the
change of utility (tied to privacy) of FL models with a change in the number of
clients and observe that when clients are training using DP-SGD and argue that
for the same privacy budget, the utility improved with increased clients. We
validate our findings through experiments on real-world datasets. The results
from this paper aim to improve the performance of privacy-preserving federated
learning systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14313v1">Between Close Enough to Reveal and Far Enough to Protect: a New Privacy
  Region for Correlated Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-24T08:14:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Luis Ma√üny, Rawad Bitar, Fangwei Ye, Salim El Rouayheb</p>
    <p><b>Summary:</b> When users make personal privacy choices, correlation between their data can
cause inadvertent leakage about users who do not want to share their data by
other users sharing their data. As a solution, we consider local redaction
mechanisms. As prior works proposed data-independent privatization mechanisms,
we study the family of data-independent local redaction mechanisms and
upper-bound their utility when data correlation is modeled by a stationary
Markov process. In contrast, we derive a novel data-dependent mechanism, which
improves the utility by leveraging a data-dependent leakage measure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14309v1">BrainGuard: Privacy-Preserving Multisubject Image Reconstructions from
  Brain Activities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-24T08:10:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhibo Tian, Ruijie Quan, Fan Ma, Kun Zhan, Yi Yang</p>
    <p><b>Summary:</b> Reconstructing perceived images from human brain activity forms a crucial
link between human and machine learning through Brain-Computer Interfaces.
Early methods primarily focused on training separate models for each individual
to account for individual variability in brain activity, overlooking valuable
cross-subject commonalities. Recent advancements have explored multisubject
methods, but these approaches face significant challenges, particularly in data
privacy and effectively managing individual variability. To overcome these
challenges, we introduce BrainGuard, a privacy-preserving collaborative
training framework designed to enhance image reconstruction from multisubject
fMRI data while safeguarding individual privacy. BrainGuard employs a
collaborative global-local architecture where individual models are trained on
each subject's local data and operate in conjunction with a shared global model
that captures and leverages cross-subject patterns. This architecture
eliminates the need to aggregate fMRI data across subjects, thereby ensuring
privacy preservation. To tackle the complexity of fMRI data, BrainGuard
integrates a hybrid synchronization strategy, enabling individual models to
dynamically incorporate parameters from the global model. By establishing a
secure and collaborative training environment, BrainGuard not only protects
sensitive brain data but also improves the image reconstructions accuracy.
Extensive experiments demonstrate that BrainGuard sets a new benchmark in both
high-level and low-level metrics, advancing the state-of-the-art in brain
decoding through its innovative design.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14184v2">Tight Sample Complexity Bounds for Parameter Estimation Under Quantum
  Differential Privacy for Qubits</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-24T02:23:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Farhad Farokhi</p>
    <p><b>Summary:</b> This short note provides tight upper and lower bounds for minimal number of
samples (copies of quantum states) required to attain a prescribed accuracy
(measured by error variance) for scalar parameters using unbiased estimators
under quantum local differential privacy for qubits. In the small privacy
budget $\epsilon$ regime, i.e., $\epsilon\ll 1$, the sample complexity scales
as $\Theta(\epsilon^{-2})$. This bound matches that of classical parameter
estimation under differential privacy. The lower bound loosens (converges to
zero) in the large privacy budget regime, i.e., $\epsilon\gg 1$, but that case
is not particularly interesting as tight bounds for parameter estimation in the
noiseless case are widely known. That being said, extensions to systems with
higher dimensions and tightening the bounds for the large privacy budget regime
are interesting avenues for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14098v1">Exploring User Perspectives on Data Collection, Data Sharing
  Preferences, and Privacy Concerns with Remote Healthcare Technology</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-01-23T21:09:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniela Napoli, Heather Molyneaux, Helene Fournier, Sonia Chiasson</p>
    <p><b>Summary:</b> Remote healthcare technology can help tackle societal issues by improving
access to quality healthcare services and enhancing diagnoses through in-place
monitoring. These services can be implemented through a combination of mobile
devices, applications, wearable sensors, and other smart technology. It is
paramount to handle sensitive data that is collected in ways that meet users'
privacy expectations. We surveyed 384 people in Canada aged 20 to 93 years old
to explore participants' comfort with data collection, sharing preferences, and
potential privacy concerns related to remote healthcare technology. We explore
these topics within the context of various healthcare scenarios including
health emergencies and managing chronic health conditions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.13916v2">PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-23T18:53:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Linh Tran, Timothy Castiglia, Stacy Patterson, Ana Milanova</p>
    <p><b>Summary:</b> We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),
a communication-efficient Vertical Federated Learning algorithm with
Differential Privacy guarantees. PBM-VFL combines Secure Multi-Party
Computation with the recently introduced Poisson Binomial Mechanism to protect
parties' private datasets during model training. We define the novel concept of
feature privacy and analyze end-to-end feature and sample privacy of our
algorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We
also provide the first theoretical characterization of the relationship between
privacy budget, convergence error, and communication cost in
differentially-private VFL. Finally, we empirically show that our model
performs well with high levels of privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.13904v3">Privacy-Preserving Personalized Federated Prompt Learning for Multimodal
  Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-23T18:34:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Linh Tran, Wei Sun, Stacy Patterson, Ana Milanova</p>
    <p><b>Summary:</b> Multimodal Large Language Models (LLMs) are pivotal in revolutionizing
customer support and operations by integrating multiple modalities such as
text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed
approach that combines pre-trained multimodal LLMs such as vision-language
models with federated learning to create personalized, privacy-preserving AI
systems. However, balancing the competing goals of personalization,
generalization, and privacy remains a significant challenge.
Over-personalization can lead to overfitting, reducing generalizability, while
stringent privacy measures, such as differential privacy, can hinder both
personalization and generalization. In this paper, we propose a Differentially
Private Federated Prompt Learning (DP-FPL) approach to tackle this challenge by
leveraging a low-rank factorization scheme to capture generalization while
maintaining a residual term that preserves expressiveness for personalization.
To ensure privacy, we introduce a novel method where we apply local
differential privacy to the two low-rank components of the local prompt, and
global differential privacy to the global prompt. Our approach mitigates the
impact of privacy noise on the model performance while balancing the tradeoff
between personalization and generalization. Extensive experiments demonstrate
the effectiveness of our approach over other benchmarks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.13608v1">AirTOWN: A Privacy-Preserving Mobile App for Real-time Pollution-Aware
  POI Suggestion</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-01-23T12:28:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Giuseppe Fasano, Yashar Deldjoo, Tommaso Di Noia</p>
    <p><b>Summary:</b> This demo paper presents \airtown, a privacy-preserving mobile application
that provides real-time, pollution-aware recommendations for points of interest
(POIs) in urban environments. By combining real-time Air Quality Index (AQI)
data with user preferences, the proposed system aims to help users make
health-conscious decisions about the locations they visit. The application
utilizes collaborative filtering for personalized suggestions, and federated
learning for privacy protection, and integrates AQI data from sensor networks
in cities such as Bari, Italy, and Cork, UK. In areas with sparse sensor
coverage, interpolation techniques approximate AQI values, ensuring broad
applicability. This system offers a poromsing, health-oriented POI
recommendation solution that adapts dynamically to current urban air quality
conditions while safeguarding user privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.13321v1">Investigation of the Privacy Concerns in AI Systems for Young Digital
  Citizens: A Comparative Stakeholder Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-01-23T02:07:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Molly Campbell, Ankur Barthwal, Sandhya Joshi, Austin Shouli, Ajay Kumar Shrestha</p>
    <p><b>Summary:</b> The integration of Artificial Intelligence (AI) systems into technologies
used by young digital citizens raises significant privacy concerns. This study
investigates these concerns through a comparative analysis of stakeholder
perspectives. A total of 252 participants were surveyed, with the analysis
focusing on 110 valid responses from parents/educators and 100 from AI
professionals after data cleaning. Quantitative methods, including descriptive
statistics and Partial Least Squares Structural Equation Modeling, examined
five validated constructs: Data Ownership and Control, Parental Data Sharing,
Perceived Risks and Benefits, Transparency and Trust, and Education and
Awareness. Results showed Education and Awareness significantly influenced data
ownership and risk assessment, while Data Ownership and Control strongly
impacted Transparency and Trust. Transparency and Trust, along with Perceived
Risks and Benefits, showed minimal influence on Parental Data Sharing,
suggesting other factors may play a larger role. The study underscores the need
for user-centric privacy controls, tailored transparency strategies, and
targeted educational initiatives. Incorporating diverse stakeholder
perspectives offers actionable insights into ethical AI design and governance,
balancing innovation with robust privacy protections to foster trust in a
digital age.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.13278v1">On Subset Retrieval and Group Testing Problems with Differential Privacy
  Constraints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-23T00:05:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mira Gonen, Michael Langberg, Alex Sprintson</p>
    <p><b>Summary:</b> This paper focuses on the design and analysis of privacy-preserving
techniques for group testing and infection status retrieval. Our work is
motivated by the need to provide accurate information on the status of disease
spread among a group of individuals while protecting the privacy of the
infection status of any single individual involved. The paper is motivated by
practical scenarios, such as controlling the spread of infectious diseases,
where individuals might be reluctant to participate in testing if their
outcomes are not kept confidential.
  The paper makes the following contributions. First, we present a differential
privacy framework for the subset retrieval problem, which focuses on sharing
the infection status of individuals with administrators and decision-makers. We
characterize the trade-off between the accuracy of subset retrieval and the
degree of privacy guaranteed to the individuals. In particular, we establish
tight lower and upper bounds on the achievable level of accuracy subject to the
differential privacy constraints. We then formulate the differential privacy
framework for the noisy group testing problem in which noise is added either
before or after the pooling process. We establish a reduction between the
private subset retrieval and noisy group testing problems and show that the
converse and achievability schemes for subset retrieval carry over to
differentially private group testing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12911v1">A Selective Homomorphic Encryption Approach for Faster
  Privacy-Preserving Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-01-22T14:37:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abdulkadir Korkmaz, Praveen Rao</p>
    <p><b>Summary:</b> Federated learning is a machine learning method that supports training models
on decentralized devices or servers, where each holds its local data, removing
the need for data exchange. This approach is especially useful in healthcare,
as it enables training on sensitive data without needing to share them. The
nature of federated learning necessitates robust security precautions due to
data leakage concerns during communication. To address this issue, we propose a
new approach that employs selective encryption, homomorphic encryption,
differential privacy, and bit-wise scrambling to minimize data leakage while
achieving good execution performance. Our technique , FAS (fast and secure
federated learning) is used to train deep learning models on medical imaging
data. We implemented our technique using the Flower framework and compared with
a state-of-the-art federated learning approach that also uses selective
homomorphic encryption. Our experiments were run in a cluster of eleven
physical machines to create a real-world federated learning scenario on
different datasets. We observed that our approach is up to 90\% faster than
applying fully homomorphic encryption on the model weights. In addition, we can
avoid the pretraining step that is required by our competitor and can save up
to 20\% in terms of total execution time. While our approach was faster, it
obtained similar security results as the competitor.</p>
  </details>
</div>



<h2>2025-02</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.15680v1">Privacy Ripple Effects from Adding or Removing Personal Information in
  Language Model Training</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-21T18:59:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jaydeep Borkar, Matthew Jagielski, Katherine Lee, Niloofar Mireshghallah, David A. Smith, Christopher A. Choquette-Choo</p>
    <p><b>Summary:</b> Due to the sensitive nature of personally identifiable information (PII), its
owners may have the authority to control its inclusion or request its removal
from large-language model (LLM) training. Beyond this, PII may be added or
removed from training datasets due to evolving dataset curation techniques,
because they were newly scraped for retraining, or because they were included
in a new downstream fine-tuning stage. We find that the amount and ease of PII
memorization is a dynamic property of a model that evolves throughout training
pipelines and depends on commonly altered design choices. We characterize three
such novel phenomena: (1) similar-appearing PII seen later in training can
elicit memorization of earlier-seen sequences in what we call assisted
memorization, and this is a significant factor (in our settings, up to 1/3);
(2) adding PII can increase memorization of other PII significantly (in our
settings, as much as $\approx\!7.5\times$); and (3) removing PII can lead to
other PII being memorized. Model creators should consider these first- and
second-order privacy risks when training models to avoid the risk of new PII
regurgitation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.15567v1">Model Privacy: A Unified Framework to Understand Model Stealing Attacks
  and Defenses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-02-21T16:29:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ganghua Wang, Yuhong Yang, Jie Ding</p>
    <p><b>Summary:</b> The use of machine learning (ML) has become increasingly prevalent in various
domains, highlighting the importance of understanding and ensuring its safety.
One pressing concern is the vulnerability of ML applications to model stealing
attacks. These attacks involve adversaries attempting to recover a learned
model through limited query-response interactions, such as those found in
cloud-based services or on-chip artificial intelligence interfaces. While
existing literature proposes various attack and defense strategies, these often
lack a theoretical foundation and standardized evaluation criteria. In
response, this work presents a framework called ``Model Privacy'', providing a
foundation for comprehensively analyzing model stealing attacks and defenses.
We establish a rigorous formulation for the threat model and objectives,
propose methods to quantify the goodness of attack and defense strategies, and
analyze the fundamental tradeoffs between utility and privacy in ML models. Our
developed theory offers valuable insights into enhancing the security of ML
models, especially highlighting the importance of the attack-specific structure
of perturbations for effective defenses. We demonstrate the application of
model privacy from the defender's perspective through various learning
scenarios. Extensive experiments corroborate the insights and the effectiveness
of defense mechanisms developed under the proposed framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.15233v1">A General Pseudonymization Framework for Cloud-Based LLMs: Replacing
  Privacy Information in Controlled Text Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-21T06:15:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shilong Hou, Ruilin Shang, Zi Long, Xianghua Fu, Yin Chen</p>
    <p><b>Summary:</b> An increasing number of companies have begun providing services that leverage
cloud-based large language models (LLMs), such as ChatGPT. However, this
development raises substantial privacy concerns, as users' prompts are
transmitted to and processed by the model providers. Among the various privacy
protection methods for LLMs, those implemented during the pre-training and
fine-tuning phrases fail to mitigate the privacy risks associated with the
remote use of cloud-based LLMs by users. On the other hand, methods applied
during the inference phrase are primarily effective in scenarios where the
LLM's inference does not rely on privacy-sensitive information. In this paper,
we outline the process of remote user interaction with LLMs and, for the first
time, propose a detailed definition of a general pseudonymization framework
applicable to cloud-based LLMs. The experimental results demonstrate that the
proposed framework strikes an optimal balance between privacy protection and
utility. The code for our method is available to the public at
https://github.com/Mebymeby/Pseudonymization-Framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14780v1">ReVision: A Dataset and Baseline VLM for Privacy-Preserving
  Task-Oriented Visual Instruction Rewriting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-02-20T18:01:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abhijit Mishra, Richard Noh, Hsiang Fu, Mingda Li, Minji Kim</p>
    <p><b>Summary:</b> Efficient and privacy-preserving multimodal interaction is essential as AR,
VR, and modern smartphones with powerful cameras become primary interfaces for
human-computer communication. Existing powerful large vision-language models
(VLMs) enabling multimodal interaction often rely on cloud-based processing,
raising significant concerns about (1) visual privacy by transmitting sensitive
vision data to servers, and (2) their limited real-time, on-device usability.
This paper explores Visual Instruction Rewriting, a novel approach that
transforms multimodal instructions into text-only commands, allowing seamless
integration of lightweight on-device instruction rewriter VLMs (250M
parameters) with existing conversational AI systems, enhancing vision data
privacy. To achieve this, we present a dataset of over 39,000 examples across
14 domains and develop a compact VLM, pretrained on image captioning datasets
and fine-tuned for instruction rewriting. Experimental results, evaluated
through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic
parsing analysis, demonstrate that even a quantized version of the model
(<500MB storage footprint) can achieve effective instruction rewriting, thus
enabling privacy-focused, multimodal AI applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14761v1">User Awareness and Perspectives Survey on Privacy, Security and
  Usability of Auditory Prostheses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-02-20T17:36:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sohini Saha, Leslie M. Collins, Sherri L. Smith, Boyla O. Mainsah</p>
    <p><b>Summary:</b> According to the World Health Organization, over 466 million people worldwide
suffer from disabling hearing loss, with approximately 34 million of these
being children. Hearing aids (HA) and cochlear implants (CI) have become
indispensable tools for restoring hearing and enhancing the quality of life for
individuals with hearing impairments. Clinical research and consumer studies
indicate that users of HAs and CIs report significant improvements in their
daily lives, including enhanced communication abilities and social engagement
and reduced psychological stress. Modern auditory prosthetic devices are more
advanced and interconnected with digital networks to add functionality, such as
streaming audio directly from smartphones and other devices, remote adjustments
by audiologists, integration with smart home systems, and access to artificial
intelligence-driven sound enhancement features. With this interconnectivity,
issues surrounding data privacy and security have become increasingly
pertinent. There is limited research on the usability perceptions of current HA
and CI models from the perspective of end-users. In addition, no studies have
investigated consumer mental models during the purchasing process, particularly
which factors they prioritize when selecting a device. In this study, we
assessed participants' satisfaction levels with various features of their
auditory prostheses. This work contributes to the field by addressing gaps in
user perceptions of HA and CI usability, identifying key factors in consumer
purchasing decisions, and highlighting the need for improved privacy and
security awareness and education among users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14309v1">On Theoretical Limits of Learning with Label Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-02-20T06:51:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Puning Zhao, Chuan Ma, Li Shen, Shaowei Wang, Rongfei Fan</p>
    <p><b>Summary:</b> Label differential privacy (DP) is designed for learning problems involving
private labels and public features. While various methods have been proposed
for learning under label DP, the theoretical limits remain largely unexplored.
In this paper, we investigate the fundamental limits of learning with label DP
in both local and central models for both classification and regression tasks,
characterized by minimax convergence rates. We establish lower bounds by
converting each task into a multiple hypothesis testing problem and bounding
the test error. Additionally, we develop algorithms that yield matching upper
bounds. Our results demonstrate that under label local DP (LDP), the risk has a
significantly faster convergence rate than that under full LDP, i.e. protecting
both features and labels, indicating the advantages of relaxing the DP
definition to focus solely on labels. In contrast, under the label central DP
(CDP), the risk is only reduced by a constant factor compared to full DP,
indicating that the relaxation of CDP only has limited benefits on the
performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14291v1">A Note on Efficient Privacy-Preserving Similarity Search for Encrypted
  Vectors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-20T06:07:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dongfang Zhao</p>
    <p><b>Summary:</b> Traditional approaches to vector similarity search over encrypted data rely
on fully homomorphic encryption (FHE) to enable computation without decryption.
However, the substantial computational overhead of FHE makes it impractical for
large-scale real-time applications. This work explores a more efficient
alternative: using additively homomorphic encryption (AHE) for
privacy-preserving similarity search. We consider scenarios where either the
query vector or the database vectors remain encrypted, a setting that
frequently arises in applications such as confidential recommender systems and
secure federated learning. While AHE only supports addition and scalar
multiplication, we show that it is sufficient to compute inner product
similarity--one of the most widely used similarity measures in vector
retrieval. Compared to FHE-based solutions, our approach significantly reduces
computational overhead by avoiding ciphertext-ciphertext multiplications and
bootstrapping, while still preserving correctness and privacy. We present an
efficient algorithm for encrypted similarity search under AHE and analyze its
error growth and security implications. Our method provides a scalable and
practical solution for privacy-preserving vector search in real-world machine
learning applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14087v1">Learning from End User Data with Shuffled Differential Privacy over
  Kernel Densities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-02-19T20:27:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tal Wagner</p>
    <p><b>Summary:</b> We study a setting of collecting and learning from private data distributed
across end users. In the shuffled model of differential privacy, the end users
partially protect their data locally before sharing it, and their data is also
anonymized during its collection to enhance privacy. This model has recently
become a prominent alternative to central DP, which requires full trust in a
central data curator, and local DP, where fully local data protection takes a
steep toll on downstream accuracy.
  Our main technical result is a shuffled DP protocol for privately estimating
the kernel density function of a distributed dataset, with accuracy essentially
matching central DP. We use it to privately learn a classifier from the end
user data, by learning a private density function per class. Moreover, we show
that the density function itself can recover the semantic content of its class,
despite having been learned in the absence of any unprotected data. Our
experiments show the favorable downstream performance of our approach, and
highlight key downstream considerations and trade-offs in a practical ML
deployment of shuffled DP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13833v1">Contrastive Learning-Based privacy metrics in Tabular Synthetic Datasets</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-19T15:52:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Milton Nicol√°s Plasencia Palacios, Sebastiano Saccani, Gabriele Sgroi, Alexander Boudewijn, Luca Bortolussi</p>
    <p><b>Summary:</b> Synthetic data has garnered attention as a Privacy Enhancing Technology (PET)
in sectors such as healthcare and finance. When using synthetic data in
practical applications, it is important to provide protection guarantees. In
the literature, two family of approaches are proposed for tabular data: on the
one hand, Similarity-based methods aim at finding the level of similarity
between training and synthetic data. Indeed, a privacy breach can occur if the
generated data is consistently too similar or even identical to the train data.
On the other hand, Attack-based methods conduce deliberate attacks on synthetic
datasets. The success rates of these attacks reveal how secure the synthetic
datasets are.
  In this paper, we introduce a contrastive method that improves privacy
assessment of synthetic datasets by embedding the data in a more representative
space. This overcomes obstacles surrounding the multitude of data types and
attributes. It also makes the use of intuitive distance metrics possible for
similarity measurements and as an attack vector. In a series of experiments
with publicly available datasets, we compare the performances of
similarity-based and attack-based methods, both with and without use of the
contrastive learning-based embeddings. Our results show that relatively
efficient, easy to implement privacy metrics can perform equally well as more
advanced metrics explicitly modeling conditions for privacy referred to by the
GDPR.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14921v1">The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic
  Text</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-19T15:30:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Matthieu Meeus, Lukas Wutschitz, Santiago Zanella-B√©guelin, Shruti Tople, Reza Shokri</p>
    <p><b>Summary:</b> How much information about training samples can be gleaned from synthetic
data generated by Large Language Models (LLMs)? Overlooking the subtleties of
information flow in synthetic data generation pipelines can lead to a false
sense of privacy. In this paper, we design membership inference attacks (MIAs)
that target data used to fine-tune pre-trained LLMs that are then used to
synthesize data, particularly when the adversary does not have access to the
fine-tuned model but only to the synthetic data. We show that such data-based
MIAs do significantly better than a random guess, meaning that synthetic data
leaks information about the training data. Further, we find that canaries
crafted to maximize vulnerability to model-based MIAs are sub-optimal for
privacy auditing when only synthetic data is released. Such out-of-distribution
canaries have limited influence on the model's output when prompted to generate
useful, in-distribution synthetic data, which drastically reduces their
vulnerability. To tackle this problem, we leverage the mechanics of
auto-regressive models to design canaries with an in-distribution prefix and a
high-perplexity suffix that leave detectable traces in synthetic data. This
enhances the power of data-based MIAs and provides a better assessment of the
privacy risks of releasing synthetic data generated by LLMs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13564v1">PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language
  Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-19T09:17:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guangwei Li, Yuansen Zhang, Yinggui Wang, Shoumeng Yan, Lei Wang, Tao Wei</p>
    <p><b>Summary:</b> The rapid development of large language models (LLMs) is redefining the
landscape of human-computer interaction, and their integration into various
user-service applications is becoming increasingly prevalent. However,
transmitting user data to cloud-based LLMs presents significant risks of data
breaches and unauthorized access to personal identification information. In
this paper, we propose a privacy preservation pipeline for protecting privacy
and sensitive information during interactions between users and LLMs in
practical LLM usage scenarios. We construct SensitiveQA, the first privacy
open-ended question-answering dataset. It comprises 57k interactions in Chinese
and English, encompassing a diverse range of user-sensitive information within
the conversations. Our proposed solution employs a multi-stage strategy aimed
at preemptively securing user information while simultaneously preserving the
response quality of cloud-based LLMs. Experimental validation underscores our
method's efficacy in balancing privacy protection with maintaining robust
interaction quality. The code and dataset are available at
https://github.com/ligw1998/PRIV-QA.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13415v1">Indifferential Privacy: A New Paradigm and Its Applications to Optimal
  Matching in Dark Pool Auctions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-19T04:19:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antigoni Polychroniadou, T. -H. Hubert Chan, Adya Agrawal</p>
    <p><b>Summary:</b> Public exchanges like the New York Stock Exchange and NASDAQ act as
auctioneers in a public double auction system, where buyers submit their
highest bids and sellers offer their lowest asking prices, along with the
number of shares (volume) they wish to trade. The auctioneer matches compatible
orders and executes the trades when a match is found. However, auctioneers
involved in high-volume exchanges, such as dark pools, may not always be
reliable. They could exploit their position by engaging in practices like
front-running or face significant conflicts of interest, i.e., ethical breaches
that have frequently resulted in hefty fines and regulatory scrutiny within the
financial industry.
  Previous solutions, based on the use of fully homomorphic encryption (Asharov
et al., AAMAS 2020), encrypt orders ensuring that information is revealed only
when a match occurs. However, this approach introduces significant
computational overhead, making it impractical for high-frequency trading
environments such as dark pools.
  In this work, we propose a new system based on differential privacy combined
with lightweight encryption, offering an efficient and practical solution that
mitigates the risks of an untrustworthy auctioneer. Specifically, we introduce
a new concept called Indifferential Privacy, which can be of independent
interest, where a user is indifferent to whether certain information is
revealed after some special event, unlike standard differential privacy. For
example, in an auction, it's reasonable to disclose the true volume of a trade
once all of it has been matched. Moreover, our new concept of Indifferential
Privacy allows for maximum matching, which is impossible with conventional
differential privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13313v1">Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning
  Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-18T22:16:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Soumi Das, Camila Kolling, Mohammad Aflah Khan, Mahsa Amani, Bishwamittra Ghosh, Qinyuan Wu, Till Speicher, Krishna P. Gummadi</p>
    <p><b>Summary:</b> We study the inherent trade-offs in minimizing privacy risks and maximizing
utility, while maintaining high computational efficiency, when fine-tuning
large language models (LLMs). A number of recent works in privacy research have
attempted to mitigate privacy risks posed by memorizing fine-tuning data by
using differentially private training methods (e.g., DP), albeit at a
significantly higher computational cost (inefficiency). In parallel, several
works in systems research have focussed on developing (parameter) efficient
fine-tuning methods (e.g., LoRA), but few works, if any, investigated whether
such efficient methods enhance or diminish privacy risks. In this paper, we
investigate this gap and arrive at a surprising conclusion: efficient
fine-tuning methods like LoRA mitigate privacy risks similar to private
fine-tuning methods like DP. Our empirical finding directly contradicts
prevailing wisdom that privacy and efficiency objectives are at odds during
fine-tuning. Our finding is established by (a) carefully defining measures of
privacy and utility that distinguish between memorizing sensitive and
non-sensitive tokens in training and test datasets used in fine-tuning and (b)
extensive evaluations using multiple open-source language models from Pythia,
Gemma, and Llama families and different domain-specific datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.12976v1">Does Training with Synthetic Data Truly Protect Privacy?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-18T15:56:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunpeng Zhao, Jie Zhang</p>
    <p><b>Summary:</b> As synthetic data becomes increasingly popular in machine learning tasks,
numerous methods--without formal differential privacy guarantees--use synthetic
data for training. These methods often claim, either explicitly or implicitly,
to protect the privacy of the original training data. In this work, we explore
four different training paradigms: coreset selection, dataset distillation,
data-free knowledge distillation, and synthetic data generated from diffusion
models. While all these methods utilize synthetic data for training, they lead
to vastly different conclusions regarding privacy preservation. We caution that
empirical approaches to preserving data privacy require careful and rigorous
evaluation; otherwise, they risk providing a false sense of privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13191v1">On the Privacy Risks of Spiking Neural Networks: A Membership Inference
  Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-18T15:19:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Junyi Guan, Abhijith Sharma, Chong Tian, Salem Lahlou</p>
    <p><b>Summary:</b> Spiking Neural Networks (SNNs) are increasingly explored for their energy
efficiency and robustness in real-world applications, yet their privacy risks
remain largely unexamined. In this work, we investigate the susceptibility of
SNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an
adversary attempts to determine whether a given sample was part of the training
dataset. While prior work suggests that SNNs may offer inherent robustness due
to their discrete, event-driven nature, we find that its resilience diminishes
as latency (T) increases. Furthermore, we introduce an input dropout strategy
under black box setting, that significantly enhances membership inference in
SNNs. Our findings challenge the assumption that SNNs are inherently more
secure, and even though they are expected to be better, our results reveal that
SNNs exhibit privacy vulnerabilities that are equally comparable to Artificial
Neural Networks (ANNs). Our code is available at
https://anonymous.4open.science/r/MIA_SNN-3610.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.12658v1">R.R.: Unveiling LLM Training Privacy through Recollection and Ranking</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-18T09:05:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenlong Meng, Zhenyuan Guo, Lenan Wu, Chen Gong, Wenyan Liu, Weixian Li, Chengkun Wei, Wenzhi Chen</p>
    <p><b>Summary:</b> Large Language Models (LLMs) pose significant privacy risks, potentially
leaking training data due to implicit memorization. Existing privacy attacks
primarily focus on membership inference attacks (MIAs) or data extraction
attacks, but reconstructing specific personally identifiable information (PII)
in LLM's training data remains challenging. In this paper, we propose R.R.
(Recollect and Rank), a novel two-step privacy stealing attack that enables
attackers to reconstruct PII entities from scrubbed training data where the PII
entities have been masked. In the first stage, we introduce a prompt paradigm
named recollection, which instructs the LLM to repeat a masked text but fill in
masks. Then we can use PII identifiers to extract recollected PII candidates.
In the second stage, we design a new criterion to score each PII candidate and
rank them. Motivated by membership inference, we leverage the reference model
as a calibration to our criterion. Experiments across three popular PII
datasets demonstrate that the R.R. achieves better PII identical performance
compared to baselines. These results highlight the vulnerability of LLMs to PII
leakage even when training data has been scrubbed. We release the replicate
package of R.R. at a link.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13172v1">Unveiling Privacy Risks in LLM Agent Memory</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-17T19:55:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bo Wang, Weiyi He, Pengfei He, Shenglai Zeng, Zhen Xiang, Yue Xing, Jiliang Tang</p>
    <p><b>Summary:</b> Large Language Model (LLM) agents have become increasingly prevalent across
various real-world applications. They enhance decision-making by storing
private user-agent interactions in the memory module for demonstrations,
introducing new privacy risks for LLM agents. In this work, we systematically
investigate the vulnerability of LLM agents to our proposed Memory EXTRaction
Attack (MEXTRA) under a black-box setting. To extract private information from
memory, we propose an effective attacking prompt design and an automated prompt
generation method based on different levels of knowledge about the LLM agent.
Experiments on two representative agents demonstrate the effectiveness of
MEXTRA. Moreover, we explore key factors influencing memory leakage from both
the agent's and the attacker's perspectives. Our findings highlight the urgent
need for effective memory safeguards in LLM agent design and deployment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.11682v1">Double Momentum and Error Feedback for Clipping with Fast Rates and
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> 
  <p><b>Published on:</b> 2025-02-17T11:16:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rustem Islamov, Samuel Horvath, Aurelien Lucchi, Peter Richtarik, Eduard Gorbunov</p>
    <p><b>Summary:</b> Strong Differential Privacy (DP) and Optimization guarantees are two
desirable properties for a method in Federated Learning (FL). However, existing
algorithms do not achieve both properties at once: they either have optimal DP
guarantees but rely on restrictive assumptions such as bounded
gradients/bounded data heterogeneity, or they ensure strong optimization
performance but lack DP guarantees. To address this gap in the literature, we
propose and analyze a new method called Clip21-SGD2M based on a novel
combination of clipping, heavy-ball momentum, and Error Feedback. In
particular, for non-convex smooth distributed problems with clients having
arbitrarily heterogeneous data, we prove that Clip21-SGD2M has optimal
convergence rate and also near optimal (local-)DP neighborhood. Our numerical
experiments on non-convex logistic regression and training of neural networks
highlight the superiority of Clip21-SGD2M over baselines in terms of the
optimization performance for a given DP-budget.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.11658v2">"I'm not for sale" -- Perceptions and limited awareness of privacy risks
  by digital natives about location data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-17T10:49:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antoine Boutet, Victor Morel</p>
    <p><b>Summary:</b> Although mobile devices benefit users in their daily lives in numerous ways,
they also raise several privacy concerns. For instance, they can reveal
sensitive information that can be inferred from location data. This location
data is shared through service providers as well as mobile applications.
Understanding how and with whom users share their location data -- as well as
users' perception of the underlying privacy risks --, are important notions to
grasp in order to design usable privacy-enhancing technologies. In this work,
we perform a quantitative and qualitative analysis of smartphone users'
awareness, perception and self-reported behavior towards location data-sharing
through a survey of n=99 young adult participants (i.e., digital natives). We
compare stated practices with actual behaviors to better understand their
mental models, and survey participants' understanding of privacy risks before
and after the inspection of location traces and the information that can be
inferred therefrom.
  Our empirical results show that participants have risky privacy practices:
about 54% of participants underestimate the number of mobile applications to
which they have granted access to their data, and 33% forget or do not think of
revoking access to their data. Also, by using a demonstrator to perform
inferences from location data, we observe that slightly more than half of
participants (57%) are surprised by the extent of potentially inferred
information, and that 47% intend to reduce access to their data via permissions
as a result of using the demonstrator. Last, a majority of participants have
little knowledge of the tools to better protect themselves, but are nonetheless
willing to follow suggestions to improve privacy (51%). Educating people,
including digital natives, about privacy risks through transparency tools seems
a promising approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.11533v1">Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of
  Stealing Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-17T08:04:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhenyuan Guo, Yi Shi, Wenlong Meng, Chen Gong, Chengkun Wei, Wenzhi Chen</p>
    <p><b>Summary:</b> Model merging is a widespread technology in large language models (LLMs) that
integrates multiple task-specific LLMs into a unified one, enabling the merged
model to inherit the specialized capabilities of these LLMs. Most task-specific
LLMs are sourced from open-source communities and have not undergone rigorous
auditing, potentially imposing risks in model merging. This paper highlights an
overlooked privacy risk: \textit{an unsafe model could compromise the privacy
of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a
privacy attack approach that trains a phishing model capable of stealing
privacy using a crafted privacy phishing instruction dataset. Furthermore, we
introduce a novel model cloaking method that mimics a specialized capability to
conceal attack intent, luring users into merging the phishing model. Once
victims merge the phishing model, the attacker can extract personally
identifiable information (PII) or infer membership information (MI) by querying
the merged model with the phishing instruction. Experimental results show that
merging a phishing model increases the risk of privacy breaches. Compared to
the results before merging, PII leakage increased by 3.9\% and MI leakage
increased by 17.4\% on average. We release the code of PhiMM through a link.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.11163v1">VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and
  Privacy Risks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-16T15:28:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jingyuan Huang, Jen-tse Huang, Ziyi Liu, Xiaoyuan Liu, Wenxuan Wang, Jieyu Zhao</p>
    <p><b>Summary:</b> Visual-Language Models (VLMs) have shown remarkable performance across
various tasks, particularly in recognizing geographic information from images.
However, significant challenges remain, including biases and privacy concerns.
To systematically address these issues in the context of geographic information
recognition, we introduce a benchmark dataset consisting of 1,200 images paired
with detailed geographic metadata. Evaluating four VLMs, we find that while
these models demonstrate the ability to recognize geographic information from
images, achieving up to $53.8\%$ accuracy in city prediction, they exhibit
significant regional biases. Specifically, performance is substantially higher
for economically developed and densely populated regions compared to less
developed ($-12.5\%$) and sparsely populated ($-17.0\%$) areas. Moreover, the
models exhibit regional biases, frequently overpredicting certain locations;
for instance, they consistently predict Sydney for images taken in Australia.
The strong performance of VLMs also raises privacy concerns, particularly for
users who share images online without the intent of being identified. Our code
and dataset are publicly available at
https://github.com/uscnlp-lime/FairLocator.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.11009v1">Computing Inconsistency Measures Under Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-02-16T06:23:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shubhankar Mohapatra, Amir Gilad, Xi He, Benny Kimelfeld</p>
    <p><b>Summary:</b> Assessing data quality is crucial to knowing whether and how to use the data
for different purposes. Specifically, given a collection of integrity
constraints, various ways have been proposed to quantify the inconsistency of a
database. Inconsistency measures are particularly important when we wish to
assess the quality of private data without revealing sensitive information. We
study the estimation of inconsistency measures for a database protected under
Differential Privacy (DP). Such estimation is nontrivial since some measures
intrinsically query sensitive information, and the computation of others
involves functions on underlying sensitive data. Among five inconsistency
measures that have been proposed in recent work, we identify that two are
intractable in the DP setting. The major challenge for the other three is high
sensitivity: adding or removing one tuple from the dataset may significantly
affect the outcome. To mitigate that, we model the dataset using a conflict
graph and investigate private graph statistics to estimate these measures. The
proposed machinery includes adapting graph-projection techniques with parameter
selection optimizations on the conflict graph and a DP variant of approximate
vertex cover size. We experimentally show that we can effectively compute DP
estimates of the three measures on five real-world datasets with denial
constraints, where the density of the conflict graphs highly varies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10997v1">New Rates in Stochastic Decision-Theoretic Online Learning under
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-02-16T05:13:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ruihan Wu, Yu-Xiang Wang</p>
    <p><b>Summary:</b> Hu and Mehta (2024) posed an open problem: what is the optimal
instance-dependent rate for the stochastic decision-theoretic online learning
(with $K$ actions and $T$ rounds) under $\varepsilon$-differential privacy?
Before, the best known upper bound and lower bound are $O\left(\frac{\log
K}{\Delta_{\min}} + \frac{\log K\log T}{\varepsilon}\right)$ and
$\Omega\left(\frac{\log K}{\Delta_{\min}} + \frac{\log K}{\varepsilon}\right)$
(where $\Delta_{\min}$ is the gap between the optimal and the second actions).
In this paper, we partially address this open problem by having two new
results. First, we provide an improved upper bound for this problem
$O\left(\frac{\log K}{\Delta_{\min}} + \frac{\log^2K}{\varepsilon}\right)$,
where the $T$-dependency has been removed. Second, we introduce the
deterministic setting, a weaker setting of this open problem, where the
received loss vector is deterministic and we can focus on the analysis for
$\varepsilon$ regardless of the sampling error. At the deterministic setting,
we prove upper and lower bounds that match at $\Theta\left(\frac{\log
K}{\varepsilon}\right)$, while a direct application of the analysis and
algorithms from the original setting still leads to an extra log factor.
Technically, we introduce the Bernoulli resampling trick, which enforces a
monotonic property for the output from report-noisy-max mechanism that enables
a tighter analysis. Moreover, by replacing the Laplace noise with Gumbel noise,
we derived explicit integral form that gives a tight characterization of the
regret in the deterministic case.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10801v1">FaceSwapGuard: Safeguarding Facial Privacy from DeepFake Threats through
  Identity Obfuscation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-02-15T13:45:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Li Wang, Zheng Li, Xuhong Zhang, Shouling Ji, Shanqing Guo</p>
    <p><b>Summary:</b> DeepFakes pose a significant threat to our society. One representative
DeepFake application is face-swapping, which replaces the identity in a facial
image with that of a victim. Although existing methods partially mitigate these
risks by degrading the quality of swapped images, they often fail to disrupt
the identity transformation effectively. To fill this gap, we propose
FaceSwapGuard (FSG), a novel black-box defense mechanism against deepfake
face-swapping threats. Specifically, FSG introduces imperceptible perturbations
to a user's facial image, disrupting the features extracted by identity
encoders. When shared online, these perturbed images mislead face-swapping
techniques, causing them to generate facial images with identities
significantly different from the original user. Extensive experiments
demonstrate the effectiveness of FSG against multiple face-swapping techniques,
reducing the face match rate from 90\% (without defense) to below 10\%. Both
qualitative and quantitative studies further confirm its ability to confuse
human perception, highlighting its practical utility. Additionally, we
investigate key factors that may influence FSG and evaluate its robustness
against various adaptive adversaries.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10788v1">Analyzing Privacy Dynamics within Groups using Gamified Auctions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-02-15T12:48:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> H√ºseyin Aydƒ±n, Onuralp Ulusoy, Ilaria Liccardi, Pƒ±nar Yolum</p>
    <p><b>Summary:</b> Online shared content, such as group pictures, often contains information
about multiple users. Developing technical solutions to manage the privacy of
such "co-owned" content is challenging because each co-owner may have different
preferences. Recent technical approaches advocate group-decision mechanisms,
including auctions, to decide as how best to resolve these differences.
However, it is not clear if users would participate in such mechanisms and if
they do, whether they would act altruistically. Understanding the privacy
dynamics is crucial to develop effective mechanisms for privacy-respecting
collaborative systems. Accordingly, this work develops RESOLVE, a privacy
auction game to understand the sharing behavior of users in groups. Our results
of users' playing the game show that i) the users' understanding of individual
vs. group privacy differs significantly; ii) often users fight for their
preferences even at the cost of others' privacy; and iii) at times users
collaborate to fight for the privacy of others.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10701v1">Unpacking the Layers: Exploring Self-Disclosure Norms, Engagement
  Dynamics, and Privacy Implications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-02-15T07:15:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ehsan-Ul Haq, Shalini Jangra, Suparna De, Nishanth Sastry, Gareth Tyson</p>
    <p><b>Summary:</b> This paper characterizes the self-disclosure behavior of Reddit users across
11 different types of self-disclosure. We find that at least half of the users
share some type of disclosure in at least 10% of their posts, with half of
these posts having more than one type of disclosure. We show that different
types of self-disclosure are likely to receive varying levels of engagement.
For instance, a Sexual Orientation disclosure garners more comments than other
self-disclosures. We also explore confounding factors that affect future
self-disclosure. We show that users who receive interactions from
(self-disclosure) specific subreddit members are more likely to disclose in the
future. We also show that privacy risks due to self-disclosure extend beyond
Reddit users themselves to include their close contacts, such as family and
friends, as their information is also revealed. We develop a browser plugin for
end-users to flag self-disclosure in their content.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10635v2">Privacy Preservation through Practical Machine Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-15T02:25:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Robert Dilworth</p>
    <p><b>Summary:</b> Machine Learning models thrive on vast datasets, continuously adapting to
provide accurate predictions and recommendations. However, in an era dominated
by privacy concerns, Machine Unlearning emerges as a transformative approach,
enabling the selective removal of data from trained models. This paper examines
methods such as Naive Retraining and Exact Unlearning via the SISA framework,
evaluating their Computational Costs, Consistency, and feasibility using the
$\texttt{HSpam14}$ dataset. We explore the potential of integrating unlearning
principles into Positive Unlabeled (PU) Learning to address challenges posed by
partially labeled datasets. Our findings highlight the promise of unlearning
frameworks like $\textit{DaRE}$ for ensuring privacy compliance while
maintaining model performance, albeit with significant computational
trade-offs. This study underscores the importance of Machine Unlearning in
achieving ethical AI and fostering trust in data-driven systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10599v1">Federated Learning-Driven Cybersecurity Framework for IoT Networks with
  Privacy-Preserving and Real-Time Threat Detection Capabilities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2025-02-14T23:11:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Milad Rahmati</p>
    <p><b>Summary:</b> The rapid expansion of the Internet of Things (IoT) ecosystem has transformed
various sectors but has also introduced significant cybersecurity challenges.
Traditional centralized security methods often struggle to balance privacy
preservation and real-time threat detection in IoT networks. To address these
issues, this study proposes a Federated Learning-Driven Cybersecurity Framework
designed specifically for IoT environments. The framework enables decentralized
data processing by training models locally on edge devices, ensuring data
privacy. Secure aggregation of these locally trained models is achieved using
homomorphic encryption, allowing collaborative learning without exposing
sensitive information.
  The proposed framework utilizes recurrent neural networks (RNNs) for anomaly
detection, optimized for resource-constrained IoT networks. Experimental
results demonstrate that the system effectively detects complex cyber threats,
including distributed denial-of-service (DDoS) attacks, with over 98% accuracy.
Additionally, it improves energy efficiency by reducing resource consumption by
20% compared to centralized approaches.
  This research addresses critical gaps in IoT cybersecurity by integrating
federated learning with advanced threat detection techniques. The framework
offers a scalable and privacy-preserving solution adaptable to various IoT
applications. Future work will explore the integration of blockchain for
transparent model aggregation and quantum-resistant cryptographic methods to
further enhance security in evolving technological landscapes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.09744v1">Fine-Tuning Foundation Models with Federated Learning for Privacy
  Preserving Medical Time Series Forecasting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-13T20:01:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mahad Ali, Curtis Lisle, Patrick W. Moore, Tammer Barkouki, Brian J. Kirkwood, Laura J. Brattain</p>
    <p><b>Summary:</b> Federated Learning (FL) provides a decentralized machine learning approach,
where multiple devices or servers collaboratively train a model without sharing
their raw data, thus enabling data privacy. This approach has gained
significant interest in academia and industry due to its privacy-preserving
properties, which are particularly valuable in the medical domain where data
availability is often protected under strict regulations. A relatively
unexplored area is the use of FL to fine-tune Foundation Models (FMs) for time
series forecasting, potentially enhancing model efficacy by overcoming data
limitation while maintaining privacy. In this paper, we fine-tuned time series
FMs with Electrocardiogram (ECG) and Impedance Cardiography (ICG) data using
different FL techniques. We then examined various scenarios and discussed the
challenges FL faces under different data heterogeneity configurations. Our
empirical results demonstrated that while FL can be effective for fine-tuning
FMs on time series forecasting tasks, its benefits depend on the data
distribution across clients. We highlighted the trade-offs in applying FL to FM
fine-tuning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.09716v1">Genetic Data Governance in Crisis: Policy Recommendations for
  Safeguarding Privacy and Preventing Discrimination</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> 
  <p><b>Published on:</b> 2025-02-13T19:05:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vivek Ramanan, Ria Vinod, Cole Williams, Sohini Ramachandran, Suresh Venkatasubramanian</p>
    <p><b>Summary:</b> Genetic data collection has become ubiquitous today. The ability to
meaningfully interpret genetic data has motivated its widespread use, providing
crucial insights into human health and ancestry while driving important public
health initiatives. Easy access to genetic testing has fueled a rapid expansion
of recreational direct-to-consumer offerings. However, the growth of genetic
datasets and their applications has created significant privacy and
discrimination risks, as our understanding of the scientific basis for genetic
traits continues to evolve. In this paper, we organize the uses of genetic data
along four distinct "pillars": clinical practice, research, forensic and
government use, and recreational use. Using our scientific understanding of
genetics, genetic inference methods and their associated risks, and current
public protections, we build a risk assessment framework that identifies key
values that any governance system must preserve. We analyze case studies using
this framework to assess how well existing regulatory frameworks preserve
desired values. Our investigation reveals critical gaps in these frameworks and
identifies specific threats to privacy and personal liberties, particularly
through genetic discrimination. We propose comprehensive policy reforms to: (1)
update the legal definition of genetic data to protect against modern
technological capabilities, (2) expand the Genetic Information
Nondiscrimination Act (GINA) to cover currently unprotected domains, and (3)
establish a unified regulatory framework under a single governing body to
oversee all applications of genetic data. We conclude with three open questions
about genetic data: the challenges posed by its relational nature, including
consent for relatives and minors; the complexities of international data
transfer; and its potential integration into large language models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.09001v1">Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection:
  Balancing Security and Data Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-13T06:33:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shaobo Liu, Zihao Zhao, Weijie He, Jiren Wang, Jing Peng, Haoyuan Ma</p>
    <p><b>Summary:</b> Privacy-preserving network anomaly detection has become an essential area of
research due to growing concerns over the protection of sensitive data.
Traditional anomaly detection models often prioritize accuracy while neglecting
the critical aspect of privacy. In this work, we propose a hybrid ensemble
model that incorporates privacy-preserving techniques to address both detection
accuracy and data protection. Our model combines the strengths of several
machine learning algorithms, including K-Nearest Neighbors (KNN), Support
Vector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), to create
a robust system capable of identifying network anomalies while ensuring
privacy. The proposed approach integrates advanced preprocessing techniques
that enhance data quality and address the challenges of small sample sizes and
imbalanced datasets. By embedding privacy measures into the model design, our
solution offers a significant advancement over existing methods, ensuring both
enhanced detection performance and strong privacy safeguards.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08989v1">RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency
  Detection in Privacy-Preserving Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">  
  <p><b>Published on:</b> 2025-02-13T06:01:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nazatul H. Sultan, Yan Bo, Yansong Gao, Seyit Camtepe, Arash Mahboubi, Hang Thanh Bui, Aufeef Chauhan, Hamed Aboutorab, Michael Bewong, Praveen Gauravaram, Rafiqul Islam, Sharif Abuadbba</p>
    <p><b>Summary:</b> Federated Learning (FL) allows users to collaboratively train a global
machine learning model by sharing local model only, without exposing their
private data to a central server. This distributed learning is particularly
appealing in scenarios where data privacy is crucial, and it has garnered
substantial attention from both industry and academia. However, studies have
revealed privacy vulnerabilities in FL, where adversaries can potentially infer
sensitive information from the shared model parameters. In this paper, we
present an efficient masking-based secure aggregation scheme utilizing
lightweight cryptographic primitives to mitigate privacy risks. Our scheme
offers several advantages over existing methods. First, it requires only a
single setup phase for the entire FL training session, significantly reducing
communication overhead. Second, it minimizes user-side overhead by eliminating
the need for user-to-user interactions, utilizing an intermediate server layer
and a lightweight key negotiation method. Third, the scheme is highly resilient
to user dropouts, and the users can join at any FL round. Fourth, it can detect
and defend against malicious server activities, including recently discovered
model inconsistency attacks. Finally, our scheme ensures security in both
semi-honest and malicious settings. We provide security analysis to formally
prove the robustness of our approach. Furthermore, we implemented an end-to-end
prototype of our scheme. We conducted comprehensive experiments and
comparisons, which show that it outperforms existing solutions in terms of
communication and computation overhead, functionality, and security.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08970v1">A Decade of Metric Differential Privacy: Advancements and Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-13T05:18:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinpeng Xie, Chenyang Yu, Yan Huang, Yang Cao, Chenxi Qiu</p>
    <p><b>Summary:</b> Metric Differential Privacy (mDP) builds upon the core principles of
Differential Privacy (DP) by incorporating various distance metrics, which
offer adaptable and context-sensitive privacy guarantees for a wide range of
applications, such as location-based services, text analysis, and image
processing. Since its inception in 2013, mDP has garnered substantial research
attention, advancing theoretical foundations, algorithm design, and practical
implementations. Despite this progress, existing surveys mainly focus on
traditional DP and local DP, and they provide limited coverage of mDP. This
paper provides a comprehensive survey of mDP research from 2013 to 2024,
tracing its development from the foundations of DP. We categorize essential
mechanisms, including Laplace, Exponential, and optimization-based approaches,
and assess their strengths, limitations, and application domains. Additionally,
we highlight key challenges and outline future research directions to encourage
innovation and real-world adoption of mDP. This survey is designed to be a
valuable resource for researchers and practitioners aiming to deepen their
understanding and drive progress in mDP within the broader privacy ecosystem.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08966v2">RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-13T05:06:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peter Yong Zhong, Siyuan Chen, Ruiqi Wang, McKenna McCall, Ben L. Titzer, Heather Miller, Phillip B. Gibbons</p>
    <p><b>Summary:</b> Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external
tools for tasks beyond their standalone capabilities, such as searching
websites, booking flights, or making financial transactions. However, these
tools greatly increase the risks of prompt injection attacks, where malicious
content hijacks the LM agent to leak confidential data or trigger harmful
actions. Existing defenses (OpenAI GPTs) require user confirmation before every
tool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS),
which automatically detects and executes tool calls that preserve integrity and
confidentiality, requiring user confirmation only when these safeguards cannot
be ensured. RTBAS adapts Information Flow Control to the unique challenges
presented by TBAS. We present two novel dependency screeners, using
LM-as-a-judge and attention-based saliency, to overcome these challenges.
Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS
prevents all targeted attacks with only a 2% loss of task utility when under
attack, and further tests confirm its ability to obtain near-oracle performance
on detecting both subtle and direct privacy leaks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08202v1">Privacy amplification by random allocation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-12T08:32:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vitaly Feldman, Moshe Shenfeld</p>
    <p><b>Summary:</b> We consider the privacy guarantees of an algorithm in which a user's data is
used in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$
differentially private steps. We demonstrate that the privacy guarantees of
this sampling scheme can be upper bound by the privacy guarantees of the
well-studied independent (or Poisson) subsampling in which each step uses the
user's data with probability $(1+ o(1))k/t $. Further, we provide two
additional analysis techniques that lead to numerical improvements in some
parameter regimes. The case of $k=1$ has been previously studied in the context
of DP-SGD in Balle et al. (2020) and very recently in Chua et al. (2024).
Privacy analysis of Balle et al. (2020) relies on privacy amplification by
shuffling which leads to overly conservative bounds. Privacy analysis of Chua
et al. (2024a) relies on Monte Carlo simulations that are computationally
prohibitive in many practical scenarios and have additional inherent
limitations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08151v1">Local Differential Privacy is Not Enough: A Sample Reconstruction Attack
  against Federated Learning with Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-12T06:37:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhichao You, Xuewen Dong, Shujun Li, Ximeng Liu, Siqi Ma, Yulong Shen</p>
    <p><b>Summary:</b> Reconstruction attacks against federated learning (FL) aim to reconstruct
users' samples through users' uploaded gradients. Local differential privacy
(LDP) is regarded as an effective defense against various attacks, including
sample reconstruction in FL, where gradients are clipped and perturbed.
Existing attacks are ineffective in FL with LDP since clipped and perturbed
gradients obliterate most sample information for reconstruction. Besides,
existing attacks embed additional sample information into gradients to improve
the attack effect and cause gradient expansion, leading to a more severe
gradient clipping in FL with LDP. In this paper, we propose a sample
reconstruction attack against LDP-based FL with any target models to
reconstruct victims' sensitive samples to illustrate that FL with LDP is not
flawless. Considering gradient expansion in reconstruction attacks and noise in
LDP, the core of the proposed attack is gradient compression and reconstructed
sample denoising. For gradient compression, an inference structure based on
sample characteristics is presented to reduce redundant gradients against LDP.
For reconstructed sample denoising, we artificially introduce zero gradients to
observe noise distribution and scale confidence interval to filter the noise.
Theoretical proof guarantees the effectiveness of the proposed attack.
Evaluations show that the proposed attack is the only attack that reconstructs
victims' training samples in LDP-based FL and has little impact on the target
model's accuracy. We conclude that LDP-based FL needs further improvements to
defend against sample reconstruction attacks effectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08008v2">An Interactive Framework for Implementing Privacy-Preserving Federated
  Learning: Experiments on Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-11T23:07:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kasra Ahmadi, Rouzbeh Behnia, Reza Ebrahimi, Mehran Mozaffari Kermani, Jeremiah Birrell, Jason Pacheco, Attila A Yavuz</p>
    <p><b>Summary:</b> Federated learning (FL) enhances privacy by keeping user data on local
devices. However, emerging attacks have demonstrated that the updates shared by
users during training can reveal significant information about their data. This
has greatly thwart the adoption of FL methods for training robust AI models in
sensitive applications. Differential Privacy (DP) is considered the gold
standard for safeguarding user data. However, DP guarantees are highly
conservative, providing worst-case privacy guarantees. This can result in
overestimating privacy needs, which may compromise the model's accuracy.
Additionally, interpretations of these privacy guarantees have proven to be
challenging in different contexts. This is further exacerbated when other
factors, such as the number of training iterations, data distribution, and
specific application requirements, can add further complexity to this problem.
In this work, we proposed a framework that integrates a human entity as a
privacy practitioner to determine an optimal trade-off between the model's
privacy and utility. Our framework is the first to address the variable memory
requirement of existing DP methods in FL settings, where resource-limited
devices (e.g., cell phones) can participate. To support such settings, we adopt
a recent DP method with fixed memory usage to ensure scalable private FL. We
evaluated our proposed framework by fine-tuning a BERT-based LLM model using
the GLUE dataset (a common approach in literature), leveraging the new
accountant, and employing diverse data partitioning strategies to mimic
real-world conditions. As a result, we achieved stable memory usage, with an
average accuracy reduction of 1.33% for $\epsilon = 10$ and 1.9% for $\epsilon
= 6$, when compared to the state-of-the-art DP accountant which does not
support fixed memory usage.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08001v1">Unveiling Client Privacy Leakage from Public Dataset Usage in Federated
  Distillation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-11T22:48:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haonan Shi, Tu Ouyang, An Wang</p>
    <p><b>Summary:</b> Federated Distillation (FD) has emerged as a popular federated training
framework, enabling clients to collaboratively train models without sharing
private data. Public Dataset-Assisted Federated Distillation (PDA-FD), which
leverages public datasets for knowledge sharing, has become widely adopted.
Although PDA-FD enhances privacy compared to traditional Federated Learning, we
demonstrate that the use of public datasets still poses significant privacy
risks to clients' private training data. This paper presents the first
comprehensive privacy analysis of PDA-FD in presence of an honest-but-curious
server. We show that the server can exploit clients' inference results on
public datasets to extract two critical types of private information: label
distributions and membership information of the private training dataset. To
quantify these vulnerabilities, we introduce two novel attacks specifically
designed for the PDA-FD setting: a label distribution inference attack and
innovative membership inference methods based on Likelihood Ratio Attack
(LiRA). Through extensive evaluation of three representative PDA-FD frameworks
(FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance,
with label distribution attacks reaching minimal KL-divergence and membership
inference attacks maintaining high True Positive Rates under low False Positive
Rate constraints. Our findings reveal significant privacy risks in current
PDA-FD frameworks and emphasize the need for more robust privacy protection
mechanisms in collaborative learning systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10450v1">Trustworthy AI on Safety, Bias, and Privacy: A Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-11T20:08:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xingli Fang, Jianwei Li, Varun Mulchandani, Jung-Eun Kim</p>
    <p><b>Summary:</b> The capabilities of artificial intelligence systems have been advancing to a
great extent, but these systems still struggle with failure modes,
vulnerabilities, and biases. In this paper, we study the current state of the
field, and present promising insights and perspectives regarding concerns that
challenge the trustworthiness of AI models. In particular, this paper
investigates the issues regarding three thrusts: safety, privacy, and bias,
which hurt models' trustworthiness. For safety, we discuss safety alignment in
the context of large language models, preventing them from generating toxic or
harmful content. For bias, we focus on spurious biases that can mislead a
network. Lastly, for privacy, we cover membership inference attacks in deep
neural networks. The discussions addressed in this paper reflect our own
experiments and observations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.07693v2">SoK: A Classification for AI-driven Personalized Privacy Assistants</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-11T16:46:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Victor Morel, Leonardo Iwaya, Simone Fischer-H√ºbner</p>
    <p><b>Summary:</b> To help users make privacy-related decisions, personalized privacy assistants
based on AI technology have been developed in recent years. These AI-driven
Personalized Privacy Assistants (AI-driven PPAs) can reap significant benefits
for users, who may otherwise struggle to make decisions regarding their
personal data in environments saturated with privacy-related decision requests.
However, no study systematically inquired about the features of these AI-driven
PPAs, their underlying technologies, or the accuracy of their decisions. To
fill this gap, we present a Systematization of Knowledge (SoK) to map the
existing solutions found in the scientific literature. We screened 1697 unique
research papers over the last decade (2013-2023), constructing a classification
from 39 included papers. As a result, this SoK reviews several aspects of
existing research on AI-driven PPAs in terms of types of publications,
contributions, methodological quality, and other quantitative insights.
Furthermore, we provide a comprehensive classification for AI-driven PPAs,
delving into their architectural choices, system contexts, types of AI used,
data sources, types of decisions, and control over decisions, among other
facets. Based on our SoK, we further underline the research gaps and challenges
and formulate recommendations for the design and development of AI-driven PPAs
as well as avenues for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.06652v1">Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-10T16:42:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anna Leschanowsky, Zahra Kolagar, Erion √áano, Ivan Habernal, Dara Hallinan, Emanu√´l A. P. Habets, Birgit Popp</p>
    <p><b>Summary:</b> The transparency principle of the General Data Protection Regulation (GDPR)
requires data processing information to be clear, precise, and accessible.
While language models show promise in this context, their probabilistic nature
complicates truthfulness and comprehensibility.
  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)
systems enhanced with alignment techniques to fulfill GDPR obligations. We
evaluate RAG systems incorporating an alignment module like Rewindable
Auto-regressive Inference (RAIN) and our proposed multidimensional extension,
MultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness
and comprehensibility and are assessed through 21 metrics, including
deterministic and large language model-based evaluations.
  Our results show that RAG systems with an alignment module outperform
baseline RAG systems on most metrics, though none fully match human answers.
Principal component analysis of the results reveals complex interactions
between metrics, highlighting the need to refine metrics. This study provides a
foundation for integrating advanced natural language processing systems into
legal compliance frameworks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.06597v1">Continual Release Moment Estimation with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-02-10T15:58:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nikita P. Kalinin, Jalaj Upadhyay, Christoph H. Lampert</p>
    <p><b>Summary:</b> We propose Joint Moment Estimation (JME), a method for continually and
privately estimating both the first and second moments of data with reduced
noise compared to naive approaches. JME uses the matrix mechanism and a joint
sensitivity analysis to allow the second moment estimation with no additional
privacy cost, thereby improving accuracy while maintaining privacy. We
demonstrate JME's effectiveness in two applications: estimating the running
mean and covariance matrix for Gaussian density estimation, and model training
with DP-Adam on CIFAR-10.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.06425v1">Generating Privacy-Preserving Personalized Advice with Zero-Knowledge
  Proofs and LLMs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-10T13:02:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hiroki Watanabe, Motonobu Uchikoshi</p>
    <p><b>Summary:</b> Large language models (LLMs) are increasingly utilized in domains such as
finance, healthcare, and interpersonal relationships to provide advice tailored
to user traits and contexts. However, this personalization often relies on
sensitive data, raising critical privacy concerns and necessitating data
minimization. To address these challenges, we propose a framework that
integrates zero-knowledge proof (ZKP) technology, specifically zkVM, with
LLM-based chatbots. This integration enables privacy-preserving data sharing by
verifying user traits without disclosing sensitive information. Our research
introduces both an architecture and a prompting strategy for this approach.
Through empirical evaluation, we clarify the current constraints and
performance limitations of both zkVM and the proposed prompting strategy,
thereby demonstrating their practical feasibility in real-world scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.05765v1">Privacy-Preserving Dataset Combination</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-02-09T03:54:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Keren Fuentes, Mimee Xu, Irene Chen</p>
    <p><b>Summary:</b> Access to diverse, high-quality datasets is crucial for machine learning
model performance, yet data sharing remains limited by privacy concerns and
competitive interests, particularly in regulated domains like healthcare. This
dynamic especially disadvantages smaller organizations that lack resources to
purchase data or negotiate favorable sharing agreements. We present SecureKL, a
privacy-preserving framework that enables organizations to identify beneficial
data partnerships without exposing sensitive information. Building on recent
advances in dataset combination methods, we develop a secure multiparty
computation protocol that maintains strong privacy guarantees while achieving
>90\% correlation with plaintext evaluations. In experiments with real-world
hospital data, SecureKL successfully identifies beneficial data partnerships
that improve model performance for intensive care unit mortality prediction
while preserving data privacy. Our framework provides a practical solution for
organizations seeking to leverage collective data resources while maintaining
privacy and competitive advantages. These results demonstrate the potential for
privacy-preserving data collaboration to advance machine learning applications
in high-stakes domains while promoting more equitable access to data resources.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.05547v1">Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in
  Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-08T12:28:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Runhua Xu, Shiqi Gao, Chao Li, James Joshi, Jianxin Li</p>
    <p><b>Summary:</b> Federated learning (FL) is inherently susceptible to privacy breaches and
poisoning attacks. To tackle these challenges, researchers have separately
devised secure aggregation mechanisms to protect data privacy and robust
aggregation methods that withstand poisoning attacks. However, simultaneously
addressing both concerns is challenging; secure aggregation facilitates
poisoning attacks as most anomaly detection techniques require access to
unencrypted local model updates, which are obscured by secure aggregation. Few
recent efforts to simultaneously tackle both challenges offen depend on
impractical assumption of non-colluding two-server setups that disrupt FL's
topology, or three-party computation which introduces scalability issues,
complicating deployment and application. To overcome this dilemma, this paper
introduce a Dual Defense Federated learning (DDFed) framework. DDFed
simultaneously boosts privacy protection and mitigates poisoning attacks,
without introducing new participant roles or disrupting the existing FL
topology. DDFed initially leverages cutting-edge fully homomorphic encryption
(FHE) to securely aggregate model updates, without the impractical requirement
for non-colluding two-server setups and ensures strong privacy protection.
Additionally, we proposes a unique two-phase anomaly detection mechanism for
encrypted model updates, featuring secure similarity computation and
feedback-driven collaborative selection, with additional measures to prevent
potential privacy breaches from Byzantine clients incorporated into the
detection process. We conducted extensive experiments on various model
poisoning attacks and FL scenarios, including both cross-device and cross-silo
FL. Experiments on publicly available datasets demonstrate that DDFed
successfully protects model privacy and effectively defends against model
poisoning threats.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.05516v1">Evaluating Differential Privacy on Correlated Datasets Using Pointwise
  Maximal Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-02-08T10:30:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sara Saeidian, Tobias J. Oechtering, Mikael Skoglund</p>
    <p><b>Summary:</b> Data-driven advancements significantly contribute to societal progress, yet
they also pose substantial risks to privacy. In this landscape, differential
privacy (DP) has become a cornerstone in privacy preservation efforts. However,
the adequacy of DP in scenarios involving correlated datasets has sometimes
been questioned and multiple studies have hinted at potential vulnerabilities.
In this work, we delve into the nuances of applying DP to correlated datasets
by leveraging the concept of pointwise maximal leakage (PML) for a quantitative
assessment of information leakage. Our investigation reveals that DP's
guarantees can be arbitrarily weak for correlated databases when assessed
through the lens of PML. More precisely, we prove the existence of a pure DP
mechanism with PML levels arbitrarily close to that of a mechanism which
releases individual entries from a database without any perturbation. By
shedding light on the limitations of DP on correlated datasets, our work aims
to foster a deeper understanding of subtle privacy risks and highlight the need
for the development of more effective privacy-preserving mechanisms tailored to
diverse scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.05509v1">Do Spikes Protect Privacy? Investigating Black-Box Model Inversion
  Attacks in Spiking Neural Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Neural and Evolutionary Computing-5BC0EB">
  <p><b>Published on:</b> 2025-02-08T10:02:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hamed Poursiami, Ayana Moshruba, Maryam Parsa</p>
    <p><b>Summary:</b> As machine learning models become integral to security-sensitive
applications, concerns over data leakage from adversarial attacks continue to
rise. Model Inversion (MI) attacks pose a significant privacy threat by
enabling adversaries to reconstruct training data from model outputs. While MI
attacks on Artificial Neural Networks (ANNs) have been widely studied, Spiking
Neural Networks (SNNs) remain largely unexplored in this context. Due to their
event-driven and discrete computations, SNNs introduce fundamental differences
in information processing that may offer inherent resistance to such attacks. A
critical yet underexplored aspect of this threat lies in black-box settings,
where attackers operate through queries without direct access to model
parameters or gradients-representing a more realistic adversarial scenario in
deployed systems. This work presents the first study of black-box MI attacks on
SNNs. We adapt a generative adversarial MI framework to the spiking domain by
incorporating rate-based encoding for input transformation and decoding
mechanisms for output interpretation. Our results show that SNNs exhibit
significantly greater resistance to MI attacks than ANNs, as demonstrated by
degraded reconstructions, increased instability in attack convergence, and
overall reduced attack effectiveness across multiple evaluation metrics.
Further analysis suggests that the discrete and temporally distributed nature
of SNN decision boundaries disrupts surrogate modeling, limiting the attacker's
ability to approximate the target model.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.04758v1">Differential Privacy of Quantum and Quantum-Inspired-Classical
  Recommendation Algorithms</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-07T08:45:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenjian Li, Mingsheng Ying</p>
    <p><b>Summary:</b> We analyze the DP (differential privacy) properties of the quantum
recommendation algorithm and the quantum-inspired-classical recommendation
algorithm. We discover that the quantum recommendation algorithm is a privacy
curating mechanism on its own, requiring no external noise, which is different
from traditional differential privacy mechanisms. In our analysis, a novel
perturbation method tailored for SVD (singular value decomposition) and
low-rank matrix approximation problems is introduced. Using the perturbation
method and random matrix theory, we are able to derive that both the quantum
and quantum-inspired-classical algorithms are
$\big(\tilde{\mathcal{O}}\big(\frac 1n\big),\,\,
\tilde{\mathcal{O}}\big(\frac{1}{\min\{m,n\}}\big)\big)$-DP under some
reasonable restrictions, where $m$ and $n$ are numbers of users and products in
the input preference database respectively. Nevertheless, a comparison shows
that the quantum algorithm has better privacy preserving potential than the
classical one.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.04045v1">Comparing privacy notions for protection against reconstruction attacks
  in machine learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-02-06T13:04:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sayan Biswas, Mark Dras, Pedro Faustini, Natasha Fernandes, Annabelle McIver, Catuscia Palamidessi, Parastoo Sadeghi</p>
    <p><b>Summary:</b> Within the machine learning community, reconstruction attacks are a principal
concern and have been identified even in federated learning (FL), which was
designed with privacy preservation in mind. In response to these threats, the
privacy community recommends the use of differential privacy (DP) in the
stochastic gradient descent algorithm, termed DP-SGD. However, the
proliferation of variants of DP in recent years\textemdash such as metric
privacy\textemdash has made it challenging to conduct a fair comparison between
different mechanisms due to the different meanings of the privacy parameters
$\epsilon$ and $\delta$ across different variants. Thus, interpreting the
practical implications of $\epsilon$ and $\delta$ in the FL context and amongst
variants of DP remains ambiguous. In this paper, we lay a foundational
framework for comparing mechanisms with differing notions of privacy
guarantees, namely $(\epsilon,\delta)$-DP and metric privacy. We provide two
foundational means of comparison: firstly, via the well-established
$(\epsilon,\delta)$-DP guarantees, made possible through the R\'enyi
differential privacy framework; and secondly, via Bayes' capacity, which we
identify as an appropriate measure for reconstruction threats.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.03811v1">Privacy Risks in Health Big Data: A Systematic Literature Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-06T06:44:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhang Si Yuan, Manmeet Mahinderjit Singh</p>
    <p><b>Summary:</b> The digitization of health records has greatly improved the efficiency of the
healthcare system and promoted the formulation of related research and
policies. However, the widespread application of advanced technologies such as
electronic health records, genomic data, and wearable devices in the field of
health big data has also intensified the collection of personal sensitive data,
bringing serious privacy and security issues. Based on a systematic literature
review (SLR), this paper comprehensively outlines the key research in the field
of health big data security. By analyzing existing research, this paper
explores how cutting-edge technologies such as homomorphic encryption,
blockchain, federated learning, and artificial immune systems can enhance data
security while protecting personal privacy. This paper also points out the
current challenges and proposes a future research framework in this key area.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.03668v1">Privacy-Preserving Generative Models: A Comprehensive Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-05T23:24:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Debalina Padariya, Isabel Wagner, Aboozar Taherkhani, Eerke Boiten</p>
    <p><b>Summary:</b> Despite the generative model's groundbreaking success, the need to study its
implications for privacy and utility becomes more urgent. Although many studies
have demonstrated the privacy threats brought by GANs, no existing survey has
systematically categorized the privacy and utility perspectives of GANs and
VAEs. In this article, we comprehensively study privacy-preserving generative
models, articulating the novel taxonomies for both privacy and utility metrics
by analyzing 100 research publications. Finally, we discuss the current
challenges and future research directions that help new researchers gain
insight into the underlying concepts.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.05219v1">Enabling External Scrutiny of AI Systems with Privacy-Enhancing
  Technologies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-05T15:31:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kendrea Beers, Helen Toner</p>
    <p><b>Summary:</b> This article describes how technical infrastructure developed by the
nonprofit OpenMined enables external scrutiny of AI systems without
compromising sensitive information.
  Independent external scrutiny of AI systems provides crucial transparency
into AI development, so it should be an integral component of any approach to
AI governance. In practice, external researchers have struggled to gain access
to AI systems because of AI companies' legitimate concerns about security,
privacy, and intellectual property.
  But now, privacy-enhancing technologies (PETs) have reached a new level of
maturity: end-to-end technical infrastructure developed by OpenMined combines
several PETs into various setups that enable privacy-preserving audits of AI
systems. We showcase two case studies where this infrastructure has been
deployed in real-world governance scenarios: "Understanding Social Media
Recommendation Algorithms with the Christchurch Call" and "Evaluating Frontier
Models with the UK AI Safety Institute." We describe types of scrutiny of AI
systems that could be facilitated by current setups and OpenMined's proposed
future setups.
  We conclude that these innovative approaches deserve further exploration and
support from the AI governance community. Interested policymakers can focus on
empowering researchers on a legal level.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.04365v1">AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case
  Study on Detecting Time of Birth</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-05T07:01:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jorge Garc√≠a-Torres, √òyvind Meinich-Bache, Siren Rettedal, Kjersti Engan</p>
    <p><b>Summary:</b> Approximately 10% of newborns need some assistance to start breathing and 5\%
proper ventilation. It is crucial that interventions are initiated as soon as
possible after birth. Accurate documentation of Time of Birth (ToB) is thereby
essential for documenting and improving newborn resuscitation performance.
However, current clinical practices rely on manual recording of ToB, typically
with minute precision. In this study, we present an AI-driven, video-based
system for automated ToB detection using thermal imaging, designed to preserve
the privacy of healthcare providers and mothers by avoiding the use of
identifiable visual data. Our approach achieves 91.4% precision and 97.4%
recall in detecting ToB within thermal video clips during performance
evaluation. Additionally, our system successfully identifies ToB in 96% of test
cases with an absolute median deviation of 1 second compared to manual
annotations. This method offers a reliable solution for improving ToB
documentation and enhancing newborn resuscitation outcomes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.02913v4">Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient
  Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-05T06:20:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiayang Meng, Tao Huang, Hong Chen, Xin Shi, Qingyu Huang, Chen Hou</p>
    <p><b>Summary:</b> The widespread deployment of deep learning models in privacy-sensitive
domains has amplified concerns regarding privacy risks, particularly those
stemming from gradient leakage during training. Current privacy assessments
primarily rely on post-training attack simulations. However, these methods are
inherently reactive, unable to encompass all potential attack scenarios, and
often based on idealized adversarial assumptions. These limitations underscore
the need for proactive approaches to privacy risk assessment during the
training process. To address this gap, we propose the concept of privacy
tokens, which are derived directly from private gradients during training.
Privacy tokens encapsulate gradient features and, when combined with data
features, offer valuable insights into the extent of private information
leakage from training data, enabling real-time measurement of privacy risks
without relying on adversarial attack simulations. Additionally, we employ
Mutual Information (MI) as a robust metric to quantify the relationship between
training data and gradients, providing precise and continuous assessments of
privacy leakage throughout the training process. Extensive experiments validate
our framework, demonstrating the effectiveness of privacy tokens and MI in
identifying and quantifying privacy risks. This proactive approach marks a
significant advancement in privacy monitoring, promoting the safer deployment
of deep learning models in sensitive applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.02749v1">Unveiling Privacy and Security Gaps in Female Health Apps</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-04T22:34:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Muhammad Hassan, Mahnoor Jameel, Tian Wang, Masooda Bashir</p>
    <p><b>Summary:</b> Female Health Applications (FHA), a growing segment of FemTech, aim to
provide affordable and accessible healthcare solutions for women globally.
These applications gather and monitor health and reproductive data from
millions of users. With ongoing debates on women's reproductive rights and
privacy, it's crucial to assess how these apps protect users' privacy. In this
paper, we undertake a security and data protection assessment of 45 popular
FHAs. Our investigation uncovers harmful permissions, extensive collection of
sensitive personal and medical data, and the presence of numerous third-party
tracking libraries. Furthermore, our examination of their privacy policies
reveals deviations from fundamental data privacy principles. These findings
highlight a significant lack of privacy and security measures for FemTech apps,
especially as women's reproductive rights face growing political challenges.
The results and recommendations provide valuable insights for users, app
developers, and policymakers, paving the way for better privacy and security in
Female Health Applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.02520v1">Privacy by Design for Self-Sovereign Identity Systems: An in-depth
  Component Analysis completed by a Design Assistance Dashboard</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-02-04T17:42:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Montassar Naghmouchi, Maryline Laurent</p>
    <p><b>Summary:</b> The use of Self-Sovereign Identity (SSI) systems for digital identity
management is gaining traction and interest. Countries such as Bhutan have
already implemented an SSI infrastructure to manage the identity of their
citizens. The EU, thanks to the revised eIDAS regulation, is opening the door
for SSI vendors to develop SSI systems for the planned EU digital identity
wallet. These developments, which fall within the sovereign domain, raise
questions about individual privacy.
  The purpose of this article is to help SSI solution designers make informed
choices to ensure that the designed solution is privacy-friendly. The
observation is that the range of possible solutions is very broad, from DID and
DID resolution methods to verifiable credential types, publicly available
information (e.g. in a blockchain), type of infrastructure, etc. As a result,
the article proposes (1) to group the elementary building blocks of a SSI
system into 5 structuring layers, (2) to analyze for each layer the privacy
implications of using the chosen building block, and (3) to provide a design
assistance dashboard that gives the complete picture of the SSI, and shows the
interdependencies between architectural choices and technical building blocks,
allowing designers to make informed choices and graphically achieve a SSI
solution that meets their need for privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.02514v1">Privacy Attacks on Image AutoRegressive Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-04T17:33:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antoni Kowalczuk, Jan Dubi≈Ñski, Franziska Boenisch, Adam Dziedzic</p>
    <p><b>Summary:</b> Image autoregressive (IAR) models have surpassed diffusion models (DMs) in
both image quality (FID: 1.48 vs. 1.58) and generation speed. However, their
privacy risks remain largely unexplored. To address this, we conduct a
comprehensive privacy analysis comparing IARs to DMs. We develop a novel
membership inference attack (MIA) that achieves a significantly higher success
rate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% for
DMs). Using this MIA, we perform dataset inference (DI) and find that IARs
require as few as six samples to detect dataset membership, compared to 200 for
DMs, indicating higher information leakage. Additionally, we extract hundreds
of training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight
a fundamental privacy-utility trade-off: while IARs excel in generation quality
and speed, they are significantly more vulnerable to privacy attacks. This
suggests that incorporating techniques from DMs, such as per-token probability
modeling using diffusion, could help mitigate IARs' privacy risks. Our code is
available at https://github.com/sprintml/privacy_attacks_against_iars.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.02410v1">Privacy Amplification by Structured Subsampling for Deep Differentially
  Private Time Series Forecasting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-02-04T15:29:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jan Schuchardt, Mina Dalirrooyfard, Jed Guzelkabaagac, Anderson Schneider, Yuriy Nevmyvaka, Stephan G√ºnnemann</p>
    <p><b>Summary:</b> Many forms of sensitive data, such as web traffic, mobility data, or hospital
occupancy, are inherently sequential. The standard method for training machine
learning models while ensuring privacy for units of sensitive information, such
as individual hospital visits, is differentially private stochastic gradient
descent (DP-SGD). However, we observe in this work that the formal guarantees
of DP-SGD are incompatible with timeseries-specific tasks like forecasting,
since they rely on the privacy amplification attained by training on small,
unstructured batches sampled from an unstructured dataset. In contrast, batches
for forecasting are generated by (1) sampling sequentially structured time
series from a dataset, (2) sampling contiguous subsequences from these series,
and (3) partitioning them into context and ground-truth forecast windows. We
theoretically analyze the privacy amplification attained by this structured
subsampling to enable the training of forecasting models with sound and tight
event- and user-level privacy guarantees. Towards more private models, we
additionally prove how data augmentation amplifies privacy in self-supervised
training of sequence models. Our empirical evaluation demonstrates that
amplification by structured subsampling enables the training of forecasting
models with strong formal privacy guarantees.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.01904v1">Common Neighborhood Estimation over Bipartite Graphs under Local
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-02-04T00:33:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yizhang He, Kai Wang, Wenjie Zhang, Xuemin Lin, Ying Zhang</p>
    <p><b>Summary:</b> Bipartite graphs, formed by two vertex layers, arise as a natural fit for
modeling the relationships between two groups of entities. In bipartite graphs,
common neighborhood computation between two vertices on the same vertex layer
is a basic operator, which is easily solvable in general settings. However, it
inevitably involves releasing the neighborhood information of vertices, posing
a significant privacy risk for users in real-world applications. To protect
edge privacy in bipartite graphs, in this paper, we study the problem of
estimating the number of common neighbors of two vertices on the same layer
under edge local differential privacy (edge LDP). The problem is challenging in
the context of edge LDP since each vertex on the opposite layer of the query
vertices can potentially be a common neighbor. To obtain efficient and accurate
estimates, we propose a multiple-round framework that significantly reduces the
candidate pool of common neighbors and enables the query vertices to construct
unbiased estimators locally. Furthermore, we improve data utility by
incorporating the estimators built from the neighbors of both query vertices
and devise privacy budget allocation optimizations. These improve the
estimator's robustness and consistency, particularly against query vertices
with imbalanced degrees. Extensive experiments on 15 datasets validate the
effectiveness and efficiency of our proposed techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.01885v1">A Privacy-Preserving Domain Adversarial Federated learning for
  multi-site brain functional connectivity analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> 
  <p><b>Published on:</b> 2025-02-03T23:26:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yipu Zhang, Likai Wang, Kuan-Jui Su, Aiying Zhang, Hao Zhu, Xiaowen Liu, Hui Shen, Vince D. Calhoun, Yuping Wang, Hongwen Deng</p>
    <p><b>Summary:</b> Resting-state functional magnetic resonance imaging (rs-fMRI) and its derived
functional connectivity networks (FCNs) have become critical for understanding
neurological disorders. However, collaborative analyses and the
generalizability of models still face significant challenges due to privacy
regulations and the non-IID (non-independent and identically distributed)
property of multiple data sources. To mitigate these difficulties, we propose
Domain Adversarial Federated Learning (DAFed), a novel federated deep learning
framework specifically designed for non-IID fMRI data analysis in multi-site
settings. DAFed addresses these challenges through feature disentanglement,
decomposing the latent feature space into domain-invariant and domain-specific
components, to ensure robust global learning while preserving local data
specificity. Furthermore, adversarial training facilitates effective knowledge
transfer between labeled and unlabeled datasets, while a contrastive learning
module enhances the global representation of domain-invariant features. We
evaluated DAFed on the diagnosis of ASD and further validated its
generalizability in the classification of AD, demonstrating its superior
classification accuracy compared to state-of-the-art methods. Additionally, an
enhanced Score-CAM module identifies key brain regions and functional
connectivity significantly associated with ASD and MCI, respectively,
uncovering shared neurobiological patterns across sites. These findings
highlight the potential of DAFed to advance multi-site collaborative research
in neuroimaging while protecting data confidentiality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.01352v1">Metric Privacy in Federated Learning for Medical Imaging: Improving
  Convergence and Preventing Client Inference Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-03T13:41:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Judith S√°inz-Pardo D√≠az, Andreas Athanasiou, Kangsoo Jung, Catuscia Palamidessi, √Ålvaro L√≥pez Garc√≠a</p>
    <p><b>Summary:</b> Federated learning is a distributed learning technique that allows training a
global model with the participation of different data owners without the need
to share raw data. This architecture is orchestrated by a central server that
aggregates the local models from the clients. This server may be trusted, but
not all nodes in the network. Then, differential privacy (DP) can be used to
privatize the global model by adding noise. However, this may affect
convergence across the rounds of the federated architecture, depending also on
the aggregation strategy employed. In this work, we aim to introduce the notion
of metric-privacy to mitigate the impact of classical server side global-DP on
the convergence of the aggregated model. Metric-privacy is a relaxation of DP,
suitable for domains provided with a notion of distance. We apply it from the
server side by computing a distance for the difference between the local
models. We compare our approach with standard DP by analyzing the impact on six
classical aggregation strategies. The proposed methodology is applied to an
example of medical imaging and different scenarios are simulated across
homogeneous and non-i.i.d clients. Finally, we introduce a novel client
inference attack, where a semi-honest client tries to find whether another
client participated in the training and study how it can be mitigated using DP
and metric-privacy. Our evaluation shows that metric-privacy can increase the
performance of the model compared to standard DP, while offering similar
protection against client inference attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.01306v1">Expert-Generated Privacy Q&A Dataset for Conversational AI and User
  Study Insights</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-02-03T12:30:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anna Leschanowsky, Farnaz Salamatjoo, Zahra Kolagar, Birgit Popp</p>
    <p><b>Summary:</b> Conversational assistants process personal data and must comply with data
protection regulations that require providers to be transparent with users
about how their data is handled. Transparency, in a legal sense, demands
preciseness, comprehensibility and accessibility, yet existing solutions fail
to meet these requirements. To address this, we introduce a new
human-expert-generated dataset for Privacy Question-Answering (Q&A), developed
through an iterative process involving legal professionals and conversational
designers. We evaluate this dataset through linguistic analysis and a user
study, comparing it to privacy policy excerpts and state-of-the-art responses
from Amazon Alexa. Our findings show that the proposed answers improve
usability and clarity compared to existing solutions while achieving legal
preciseness, thereby enhancing the accessibility of data processing information
for Conversational AI and Natural Language Processing applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.00760v1">Privacy Preserving Properties of Vision Classifiers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-02-02T11:50:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pirzada Suhail, Amit Sethi</p>
    <p><b>Summary:</b> Vision classifiers are often trained on proprietary datasets containing
sensitive information, yet the models themselves are frequently shared openly
under the privacy-preserving assumption. Although these models are assumed to
protect sensitive information in their training data, the extent to which this
assumption holds for different architectures remains unexplored. This
assumption is challenged by inversion attacks which attempt to reconstruct
training data from model weights, exposing significant privacy vulnerabilities.
In this study, we systematically evaluate the privacy-preserving properties of
vision classifiers across diverse architectures, including Multi-Layer
Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Vision
Transformers (ViTs). Using network inversion-based reconstruction techniques,
we assess the extent to which these architectures memorize and reveal training
data, quantifying the relative ease of reconstruction across models. Our
analysis highlights how architectural differences, such as input
representation, feature extraction mechanisms, and weight structures, influence
privacy risks. By comparing these architectures, we identify which are more
resilient to inversion attacks and examine the trade-offs between model
performance and privacy preservation, contributing to the development of secure
and privacy-respecting machine learning models for sensitive applications. Our
findings provide actionable insights into the design of secure and
privacy-aware machine learning systems, emphasizing the importance of
evaluating architectural decisions in sensitive applications involving
proprietary or personal data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.00693v1">DPBloomfilter: Securing Bloom Filters with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-02T06:47:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yekun Ke, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song</p>
    <p><b>Summary:</b> The Bloom filter is a simple yet space-efficient probabilistic data structure
that supports membership queries for dramatically large datasets. It is widely
utilized and implemented across various industrial scenarios, often handling
massive datasets that include sensitive user information necessitating privacy
preservation. To address the challenge of maintaining privacy within the Bloom
filter, we have developed the DPBloomfilter. This innovation integrates the
classical differential privacy mechanism, specifically the Random Response
technique, into the Bloom filter, offering robust privacy guarantees under the
same running complexity as the standard Bloom filter. Through rigorous
simulation experiments, we have demonstrated that our DPBloomfilter algorithm
maintains high utility while ensuring privacy protections. To the best of our
knowledge, this is the first work to provide differential privacy guarantees
for the Bloom filter for membership query problems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.00451v1">Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and
  Opportunities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-01T15:10:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Aishik Mandal, Tanmoy Chakraborty, Iryna Gurevych</p>
    <p><b>Summary:</b> Mental illness is a widespread and debilitating condition with substantial
societal and personal costs. Traditional diagnostic and treatment approaches,
such as self-reported questionnaires and psychotherapy sessions, often impose
significant burdens on both patients and clinicians, limiting accessibility and
efficiency. Recent advances in Artificial Intelligence (AI), particularly in
Natural Language Processing and multimodal techniques, hold great potential for
recognizing and addressing conditions such as depression, anxiety, bipolar
disorder, schizophrenia, and post-traumatic stress disorder. However, privacy
concerns, including the risk of sensitive data leakage from datasets and
trained models, remain a critical barrier to deploying these AI systems in
real-world clinical settings. These challenges are amplified in multimodal
methods, where personal identifiers such as voice and facial data can be
misused. This paper presents a critical and comprehensive study of the privacy
challenges associated with developing and deploying AI models for mental
health. We further prescribe potential solutions, including data anonymization,
synthetic data generation, and privacy-preserving model training, to strengthen
privacy safeguards in practical applications. Additionally, we discuss
evaluation frameworks to assess the privacy-utility trade-offs in these
approaches. By addressing these challenges, our work aims to advance the
development of reliable, privacy-aware AI tools to support clinical
decision-making and improve mental health outcomes.</p>
  </details>
</div>

