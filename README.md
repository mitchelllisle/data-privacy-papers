
<h2>2024-02</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.00157v1">Privacy-Preserving Distributed Optimization and Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Science and Game Theory-5BC0EB">
  <p><b>Published on:</b> 2024-02-29T22:18:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ziqin Chen, Yongqiang Wang</p>
    <p><b>Summary:</b> Distributed optimization and learning has recently garnered great attention
due to its wide applications in sensor networks, smart grids, machine learning,
and so forth. Despite rapid development, existing distributed optimization and
learning algorithms require each agent to exchange messages with its neighbors,
which may expose sensitive information and raise significant privacy concerns.
In this survey paper, we overview privacy-preserving distributed optimization
and learning methods. We first discuss cryptography, differential privacy, and
other techniques that can be used for privacy preservation and indicate their
pros and cons for privacy protection in distributed optimization and learning.
We believe that among these approaches, differential privacy is most promising
due to its low computational and communication complexities, which are
extremely appealing for modern learning based applications with high dimensions
of optimization variables. We then introduce several differential-privacy
algorithms that can simultaneously ensure privacy and optimization accuracy.
Moreover, we provide example applications in several machine learning problems
to confirm the real-world effectiveness of these algorithms. Finally, we
highlight some challenges in this research domain and discuss future
directions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.19105v1">CollaFuse: Navigating Limited Resources and Privacy in Collaborative
  Generative AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-02-29T12:36:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Domenique Zipperling, Simeon Allmendinger, Lukas Struppek, Niklas Kühl</p>
    <p><b>Summary:</b> In the landscape of generative artificial intelligence, diffusion-based
models present challenges for socio-technical systems in data requirements and
privacy. Traditional approaches like federated learning distribute the learning
process but strain individual clients, especially with constrained resources
(e.g., edge devices). In response to these challenges, we introduce CollaFuse,
a novel framework inspired by split learning. Tailored for efficient and
collaborative use of denoising diffusion probabilistic models, CollaFuse
enables shared server training and inference, alleviating client computational
burdens. This is achieved by retaining data and computationally inexpensive GPU
processes locally at each client while outsourcing the computationally
expensive processes to the shared server. Demonstrated in a healthcare context,
CollaFuse enhances privacy by highly reducing the need for sensitive
information sharing. These capabilities hold the potential to impact various
application areas, such as the design of edge computing solutions, healthcare
research, or autonomous driving. In essence, our work advances distributed
machine learning, shaping the future of collaborative GenAI networks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.18973v1">Privacy Management and Interface Design for a Smart House</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-02-29T09:26:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ana-Maria Comeaga, Iuliana Marin</p>
    <p><b>Summary:</b> In today's life, more and more people tend to opt for a smart house. In this
way, the idea of including technology has become popular worldwide. Despite
this concept's many benefits, managing security remains an essential problem
due to the shared activities. The Internet of Things system behind a smart
house is based on several sensors to measure temperature, humidity, air
quality, and movement. Because of being supervised every day through sensors
and controlling their house only with a simple click, many people can be afraid
of this new approach in terms of their privacy, and this fact can constrain
them from following their habits. The security aspects should be constantly
analyzed to keep the data's confidentiality and make people feel safe in their
own houses. In this context, the current paper puts light on an alternative
design of a platform in which the safety of homeowners is the primary purpose,
and they maintain complete control over the data generated by smart devices.
The current research highlights the role of security and interface design in
controlling a smart house. The study underscores the importance of providing an
interface that can be used easily by any person to manage data and live
activities in a modern residence in an era dominated by continuously developing
technology.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.18864v1">Privacy-Preserving Autoencoder for Collaborative Object Detection</a></h3>
  
  <p><b>Published on:</b> 2024-02-29T05:27:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bardia Azizian, Ivan Bajic</p>
    <p><b>Summary:</b> Privacy is a crucial concern in collaborative machine vision where a part of
a Deep Neural Network (DNN) model runs on the edge, and the rest is executed on
the cloud. In such applications, the machine vision model does not need the
exact visual content to perform its task. Taking advantage of this potential,
private information could be removed from the data insofar as it does not
significantly impair the accuracy of the machine vision system. In this paper,
we present an autoencoder-style network integrated within an object detection
pipeline, which generates a latent representation of the input image that
preserves task-relevant information while removing private information. Our
approach employs an adversarial training strategy that not only removes private
information from the bottleneck of the autoencoder but also promotes improved
compression efficiency for feature channels coded by conventional codecs like
VVC-Intra. We assess the proposed system using a realistic evaluation framework
for privacy, directly measuring face and license plate recognition accuracy.
Experimental results show that our proposed method is able to reduce the
bitrate significantly at the same object detection accuracy compared to coding
the input images directly, while keeping the face and license plate recognition
accuracy on the images recovered from the bottleneck features low, implying
strong privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.18786v1">OpticalDR: A Deep Optical Imaging Model for Privacy-Protective
  Depression Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-02-29T01:20:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuchen Pan, Junjun Jiang, Kui Jiang, Zhihao Wu, Keyuan Yu, Xianming Liu</p>
    <p><b>Summary:</b> Depression Recognition (DR) poses a considerable challenge, especially in the
context of the growing concerns surrounding privacy. Traditional automatic
diagnosis of DR technology necessitates the use of facial images, undoubtedly
expose the patient identity features and poses privacy risks. In order to
mitigate the potential risks associated with the inappropriate disclosure of
patient facial images, we design a new imaging system to erase the identity
information of captured facial images while retain disease-relevant features.
It is irreversible for identity information recovery while preserving essential
disease-related characteristics necessary for accurate DR. More specifically,
we try to record a de-identified facial image (erasing the identifiable
features as much as possible) by a learnable lens, which is optimized in
conjunction with the following DR task as well as a range of face analysis
related auxiliary tasks in an end-to-end manner. These aforementioned
strategies form our final Optical deep Depression Recognition network
(OpticalDR). Experiments on CelebA, AVEC 2013, and AVEC 2014 datasets
demonstrate that our OpticalDR has achieved state-of-the-art privacy protection
performance with an average AUC of 0.51 on popular facial recognition models,
and competitive results for DR with MAE/RMSE of 7.53/8.48 on AVEC 2013 and
7.89/8.82 on AVEC 2014, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.18726v1">Unveiling Privacy, Memorization, and Input Curvature Links</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-28T22:02:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Deepak Ravikumar, Efstathia Soufleri, Abolfazl Hashemi, Kaushik Roy</p>
    <p><b>Summary:</b> Deep Neural Nets (DNNs) have become a pervasive tool for solving many
emerging problems. However, they tend to overfit to and memorize the training
set. Memorization is of keen interest since it is closely related to several
concepts such as generalization, noisy learning, and privacy. To study
memorization, Feldman (2019) proposed a formal score, however its computational
requirements limit its practical use. Recent research has shown empirical
evidence linking input loss curvature (measured by the trace of the loss
Hessian w.r.t inputs) and memorization. It was shown to be ~3 orders of
magnitude more efficient than calculating the memorization score. However,
there is a lack of theoretical understanding linking memorization with input
loss curvature. In this paper, we not only investigate this connection but also
extend our analysis to establish theoretical links between differential
privacy, memorization, and input loss curvature. First, we derive an upper
bound on memorization characterized by both differential privacy and input loss
curvature. Second, we present a novel insight showing that input loss curvature
is upper-bounded by the differential privacy parameter. Our theoretical
findings are further empirically validated using deep models on CIFAR and
ImageNet datasets, showing a strong correlation between our theoretical
predictions and results observed in practice.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.00030v2">GraphPub: Generation of Differential Privacy Graph with High
  Availability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-28T20:02:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wanghan Xu, Bin Shi, Ao Liu, Jiqiang Zhang, Bo Dong</p>
    <p><b>Summary:</b> In recent years, with the rapid development of graph neural networks (GNN),
more and more graph datasets have been published for GNN tasks. However, when
an upstream data owner publishes graph data, there are often many privacy
concerns, because many real-world graph data contain sensitive information like
person's friend list. Differential privacy (DP) is a common method to protect
privacy, but due to the complex topological structure of graph data, applying
DP on graphs often affects the message passing and aggregation of GNN models,
leading to a decrease in model accuracy. In this paper, we propose a novel
graph edge protection framework, graph publisher (GraphPub), which can protect
graph topology while ensuring that the availability of data is basically
unchanged. Through reverse learning and the encoder-decoder mechanism, we
search for some false edges that do not have a large negative impact on the
aggregation of node features, and use them to replace some real edges. The
modified graph will be published, which is difficult to distinguish between
real and false data. Sufficient experiments prove that our framework achieves
model accuracy close to the original graph with an extremely low privacy
budget.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.00028v1">Lower Bounds for Differential Privacy Under Continual Observation and
  Online Threshold Queries</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-28T16:45:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, Uri Stemmer</p>
    <p><b>Summary:</b> One of the most basic problems for studying the "price of privacy over time"
is the so called private counter problem, introduced by Dwork et al. (2010) and
Chan et al. (2010). In this problem, we aim to track the number of events that
occur over time, while hiding the existence of every single event. More
specifically, in every time step $t\in[T]$ we learn (in an online fashion) that
$\Delta_t\geq 0$ new events have occurred, and must respond with an estimate
$n_t\approx\sum_{j=1}^t \Delta_j$. The privacy requirement is that all of the
outputs together, across all time steps, satisfy event level differential
privacy. The main question here is how our error needs to depend on the total
number of time steps $T$ and the total number of events $n$. Dwork et al.
(2015) showed an upper bound of $O\left(\log(T)+\log^2(n)\right)$, and
Henzinger et al. (2023) showed a lower bound of $\Omega\left(\min\{\log n, \log
T\}\right)$. We show a new lower bound of $\Omega\left(\min\{n,\log
T\}\right)$, which is tight w.r.t. the dependence on $T$, and is tight in the
sparse case where $\log^2 n=O(\log T)$. Our lower bound has the following
implications:
  $\bullet$ We show that our lower bound extends to the "online thresholds
problem", where the goal is to privately answer many "quantile queries" when
these queries are presented one-by-one. This resolves an open question of Bun
et al. (2017).
  $\bullet$ Our lower bound implies, for the first time, a separation between
the number of mistakes obtainable by a private online learner and a non-private
online learner. This partially resolves a COLT'22 open question published by
Sanyal and Ramponi.
  $\bullet$ Our lower bound also yields the first separation between the
standard model of private online learning and a recently proposed relaxed
variant of it, called private online prediction.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.00023v1">Auditable Homomorphic-based Decentralized Collaborative AI with
  Attribute-based Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-02-28T14:51:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lo-Yao Yeh, Sheng-Po Tseng, Chia-Hsun Lu, Chih-Ya Shen</p>
    <p><b>Summary:</b> In recent years, the notion of federated learning (FL) has led to the new
paradigm of distributed artificial intelligence (AI) with privacy preservation.
However, most current FL systems suffer from data privacy issues due to the
requirement of a trusted third party. Although some previous works introduce
differential privacy to protect the data, however, it may also significantly
deteriorate the model performance. To address these issues, we propose a novel
decentralized collaborative AI framework, named Auditable Homomorphic-based
Decentralised Collaborative AI (AerisAI), to improve security with homomorphic
encryption and fine-grained differential privacy. Our proposed AerisAI directly
aggregates the encrypted parameters with a blockchain-based smart contract to
get rid of the need of a trusted third party. We also propose a brand-new
concept for eliminating the negative impacts of differential privacy for model
performance. Moreover, the proposed AerisAI also provides the broadcast-aware
group key management based on ciphertext-policy attribute-based encryption
(CPABE) to achieve fine-grained access control based on different service-level
agreements. We provide a formal theoretical analysis of the proposed AerisAI as
well as the functionality comparison with the other baselines. We also conduct
extensive experiments on real datasets to evaluate the proposed approach. The
experimental results indicate that our proposed AerisAI significantly
outperforms the other state-of-the-art baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.18321v2">Privacy Policies and Consent Management Platforms: Growth and Users'
  Interactions over Time</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-02-28T13:36:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nikhil Jha, Martino Trevisan, Marco Mellia, Daniel Fernandez, Rodrigo Irarrazaval</p>
    <p><b>Summary:</b> In response to growing concerns about user privacy, legislators have
introduced new regulations and laws such as the General Data Protection
Regulation (GDPR) and the California Consumer Privacy Act (CCPA) that force
websites to obtain user consent before activating personal data collection,
fundamental to providing targeted advertising. The cornerstone of this
consent-seeking process involves the use of Privacy Banners, the technical
mechanism to collect users' approval for data collection practices. Consent
management platforms (CMPs) have emerged as practical solutions to make it
easier for website administrators to properly manage consent, allowing them to
outsource the complexities of managing user consent and activating advertising
features.
  This paper presents a detailed and longitudinal analysis of the evolution of
CMPs spanning nine years. We take a twofold perspective: Firstly, thanks to the
HTTP Archive dataset, we provide insights into the growth, market share, and
geographical spread of CMPs. Noteworthy observations include the substantial
impact of GDPR on the proliferation of CMPs in Europe. Secondly, we analyse
millions of user interactions with a medium-sized CMP present in thousands of
websites worldwide. We observe how even small changes in the design of Privacy
Banners have a critical impact on the user's giving or denying their consent to
data collection. For instance, over 60% of users do not consent when offered a
simple "one-click reject-all" option. Conversely, when opting out requires more
than one click, about 90% of users prefer to simply give their consent. The
main objective is in fact to eliminate the annoying privacy banner rather the
make an informed decision. Curiously, we observe iOS users exhibit a higher
tendency to accept cookies compared to Android users, possibly indicating
greater confidence in the privacy offered by Apple devices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.18607v2">Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An
  Adversarial Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-28T12:21:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinjian Luo, Yangfan Jiang, Fei Wei, Yuncheng Wu, Xiaokui Xiao, Beng Chin Ooi</p>
    <p><b>Summary:</b> Diffusion models have recently gained significant attention in both academia
and industry due to their impressive generative performance in terms of both
sampling quality and distribution coverage. Accordingly, proposals are made for
sharing pre-trained diffusion models across different organizations, as a way
of improving data utilization while enhancing privacy protection by avoiding
sharing private data directly. However, the potential risks associated with
such an approach have not been comprehensively examined.
  In this paper, we take an adversarial perspective to investigate the
potential privacy and fairness risks associated with the sharing of diffusion
models. Specifically, we investigate the circumstances in which one party (the
sharer) trains a diffusion model using private data and provides another party
(the receiver) black-box access to the pre-trained model for downstream tasks.
We demonstrate that the sharer can execute fairness poisoning attacks to
undermine the receiver's downstream models by manipulating the training data
distribution of the diffusion model. Meanwhile, the receiver can perform
property inference attacks to reveal the distribution of sensitive features in
the sharer's dataset. Our experiments conducted on real-world datasets
demonstrate remarkable attack performance on different types of diffusion
models, which highlights the critical importance of robust data auditing and
privacy protection protocols in pertinent applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.17838v1">Personalizing Smart Home Privacy Protection With Individuals' Regulatory
  Focus: Would You Preserve or Enhance Your Information Privacy?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-02-27T19:03:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Reza Ghaiumy Anaraky, Yao Li, Hichang Cho, Danny Yuxing Huang, Kaileigh A. Byrne, Bart Knijnenburg, Oded Nov</p>
    <p><b>Summary:</b> In this study, we explore the effectiveness of persuasive messages endorsing
the adoption of a privacy protection technology (IoT Inspector) tailored to
individuals' regulatory focus (promotion or prevention). We explore if and how
regulatory fit (i.e., tuning the goal-pursuit mechanism to individuals'
internal regulatory focus) can increase persuasion and adoption. We conducted a
between-subject experiment (N = 236) presenting participants with the IoT
Inspector in gain ("Privacy Enhancing Technology" -- PET) or loss ("Privacy
Preserving Technology" -- PPT) framing. Results show that the effect of
regulatory fit on adoption is mediated by trust and privacy calculus processes:
prevention-focused users who read the PPT message trust the tool more.
Furthermore, privacy calculus favors using the tool when promotion-focused
individuals read the PET message. We discuss the contribution of understanding
the cognitive mechanisms behind regulatory fit in privacy decision-making to
support privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.17191v1">AI-Driven Anonymization: Protecting Personal Data Privacy While
  Leveraging Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-27T04:12:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Le Yang, Miao Tian, Duan Xin, Qishuo Cheng, Jiajian Zheng</p>
    <p><b>Summary:</b> The development of artificial intelligence has significantly transformed
people's lives. However, it has also posed a significant threat to privacy and
security, with numerous instances of personal information being exposed online
and reports of criminal attacks and theft. Consequently, the need to achieve
intelligent protection of personal information through machine learning
algorithms has become a paramount concern. Artificial intelligence leverages
advanced algorithms and technologies to effectively encrypt and anonymize
personal data, enabling valuable data analysis and utilization while
safeguarding privacy. This paper focuses on personal data privacy protection
and the promotion of anonymity as its core research objectives. It achieves
personal data privacy protection and detection through the use of machine
learning's differential privacy protection algorithm. The paper also addresses
existing challenges in machine learning related to privacy and personal data
protection, offers improvement suggestions, and analyzes factors impacting
datasets to enable timely personal data privacy detection and protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.17130v1">Privacy-Preserving Map-Free Exploration for Confirming the Absence of a
  Radioactive Source</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E">
  <p><b>Published on:</b> 2024-02-27T01:49:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Eric Lepowsky, David Snyder, Alexander Glaser, Anirudha Majumdar</p>
    <p><b>Summary:</b> Performing an inspection task while maintaining the privacy of the inspected
site is a challenging balancing act. In this work, we are motivated by the
future of nuclear arms control verification, which requires both a high level
of privacy and guaranteed correctness. For scenarios with limitations on
sensors and stored information due to the potentially secret nature of
observable features, we propose a robotic verification procedure that provides
map-free exploration to perform a source verification task without requiring,
nor revealing, any task-irrelevant, site-specific information. We provide
theoretical guarantees on the privacy and correctness of our approach,
validated by extensive simulated and hardware experiments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.16982v2">Synthesizing Tight Privacy and Accuracy Bounds via Weighted Model
  Counting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Programming Languages-D91E36">
  <p><b>Published on:</b> 2024-02-26T19:29:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lisa Oakley, Steven Holtzen, Alina Oprea</p>
    <p><b>Summary:</b> Programmatically generating tight differential privacy (DP) bounds is a hard
problem. Two core challenges are (1) finding expressive, compact, and efficient
encodings of the distributions of DP algorithms, and (2) state space explosion
stemming from the multiple quantifiers and relational properties of the DP
definition.
  We address the first challenge by developing a method for tight privacy and
accuracy bound synthesis using weighted model counting on binary decision
diagrams, a state of the art technique from the artificial intelligence and
automated reasoning communities for exactly computing probability
distributions. We address the second challenge by developing a framework for
leveraging inherent symmetries in DP algorithms. Our solution benefits from
ongoing research in probabilistic programming languages, allowing us to
succinctly and expressively represent different DP algorithms with approachable
language syntax that can be used by non-experts.
  We provide a detailed case study of our solution on the binary randomized
response algorithm. We also evaluate an implementation of our solution using
the Dice probabilistic programming language for the randomized response and
truncated geometric above threshold algorithms. We compare to prior work on
exact DP verification using Markov chain probabilistic model checking. Very few
existing works consider mechanized analysis of accuracy guarantees for DP
algorithms. We additionally provide a detailed analysis using our technique for
finding tight accuracy bounds for DP algorithms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.16515v1">LLM-based Privacy Data Augmentation Guided by Knowledge Distillation
  with a Distribution Tutor for Medical Text Classification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-26T11:52:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yiping Song, Juhua Zhang, Zhiliang Tian, Yuxin Yang, Minlie Huang, Dongsheng Li</p>
    <p><b>Summary:</b> As sufficient data are not always publically accessible for model training,
researchers exploit limited data with advanced learning algorithms or expand
the dataset via data augmentation (DA). Conducting DA in private domain
requires private protection approaches (i.e. anonymization and perturbation),
but those methods cannot provide protection guarantees. Differential privacy
(DP) learning methods theoretically bound the protection but are not skilled at
generating pseudo text samples with large models. In this paper, we transfer
DP-based pseudo sample generation task to DP-based generated samples
discrimination task, where we propose a DP-based DA method with a LLM and a
DP-based discriminator for text classification on private domains. We construct
a knowledge distillation model as the DP-based discriminator: teacher models,
accessing private data, teaches students how to select private samples with
calibrated noise to achieve DP. To constrain the distribution of DA's
generation, we propose a DP-based tutor that models the noised private
distribution and controls samples' generation with a low privacy cost. We
theoretically analyze our model's privacy protection and empirically verify our
model.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.16028v1">FedFDP: Federated Learning with Fairness and Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-25T08:35:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinpeng Ling, Jie Fu, Zhili Chen, Kuncan Wang, Huifa Li, Tong Cheng, Guanying Xu, Qin Li</p>
    <p><b>Summary:</b> Federated learning (FL) is a new machine learning paradigm to overcome the
challenge of data silos and has garnered significant attention. However,
through our observations, a globally effective trained model may performance
disparities in different clients. This implies that the jointly trained models
by clients may lead to unfair outcomes. On the other hand, relevant studies
indicate that the transmission of gradients or models in federated learning can
also give rise to privacy leakage issues, such as membership inference attacks.
  To address the first issue mentioned above, we propose a federated algorithm
with fairness, termed FedFair. Building upon FedFair, we introduce privacy
protection to form the FedFDP algorithm to address the second issue mentioned
above. In FedFDP, we devise a fairness-aware clipping strategy to achieve
differential privacy while adjusting fairness. Additionally, for the extra
uploaded loss values, we present an adaptive clipping approach to maximize
utility. Furthermore, we theoretically prove that our algorithm converges and
ensures differential privacy. Lastly, Extensive experimental results
demonstrate that FedFair and FedFDP significantly outperforms state-of-the-art
solutions in terms of model performance and fairness. The code is accessible at
https://anonymous.4open.science/r/FedFDP-E754.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.15797v1">Gait-Based Privacy Protection for Smart Wearable Devices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-24T12:05:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yu Su, Yongjiao Li, Zhu Cao</p>
    <p><b>Summary:</b> Smart wearable devices (SWDs) collect and store sensitive daily information
of many people. Its primary method of identification is still the password
unlocking method. However, several studies have shown serious security flaws in
that method, which makes the privacy and security concerns of SWDs particularly
urgent. Gait identification is well suited for SWDs because its built-in
sensors can provide data support for identification. However, existing gait
identification methods have low accuracy and neglect to protect the privacy of
gait features. In addition, the SWD can be used as an internet of things device
for users to share data. But few studies have used gait feature-based
encryption schemes to protect the privacy of message interactions between SWDs
and other devices. In this paper, we propose a gait identification network, a
bi-directional long short-term memory network with an attention mechanism
(ABLSTM), to improve the identification accuracy and a stochastic orthogonal
transformation (SOT) scheme to protect the extracted gait features from
leakage. In the experiments, ABLSTM achieves an accuracy of 95.28%, reducing
previous error rate by 19.3%. The SOT scheme is proved to be resistant to the
chosen plaintext attack (CPA) and is 30% faster than previous methods. A
biometric-based encryption scheme is proposed to enable secure message
interactions using gait features as keys after the gait identification stage is
passed, and offers better protection of the gait features compared to previous
schemes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.15780v1">Holding Secrets Accountable: Auditing Privacy-Preserving Machine
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-24T10:04:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hidde Lycklama, Alexander Viand, Nicolas Küchler, Christian Knabenhans, Anwar Hithnawi</p>
    <p><b>Summary:</b> Recent advancements in privacy-preserving machine learning are paving the way
to extend the benefits of ML to highly sensitive data that, until now, have
been hard to utilize due to privacy concerns and regulatory constraints.
Simultaneously, there is a growing emphasis on enhancing the transparency and
accountability of machine learning, including the ability to audit ML
deployments. While ML auditing and PPML have both been the subjects of
intensive research, they have predominately been examined in isolation.
However, their combination is becoming increasingly important. In this work, we
introduce Arc, an MPC framework for auditing privacy-preserving machine
learning. At the core of our framework is a new protocol for efficiently
verifying MPC inputs against succinct commitments at scale. We evaluate the
performance of our framework when instantiated with our consistency protocol
and compare it to hashing-based and homomorphic-commitment-based approaches,
demonstrating that it is up to 10^4x faster and up to 10^6x more concise.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.15738v1">Privacy-Preserving State Estimation in the Presence of Eavesdroppers: A
  Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-02-24T06:32:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinhao Yan, Guanzhong Zhou, Daniel E. Quevedo, Carlos Murguia, Bo Chen, Hailong Huang</p>
    <p><b>Summary:</b> Networked systems are increasingly the target of cyberattacks that exploit
vulnerabilities within digital communications, embedded hardware, and software.
Arguably, the simplest class of attacks -- and often the first type before
launching destructive integrity attacks -- are eavesdropping attacks, which aim
to infer information by collecting system data and exploiting it for malicious
purposes. A key technology of networked systems is state estimation, which
leverages sensing and actuation data and first-principles models to enable
trajectory planning, real-time monitoring, and control. However, state
estimation can also be exploited by eavesdroppers to identify models and
reconstruct states with the aim of, e.g., launching integrity (stealthy)
attacks and inferring sensitive information. It is therefore crucial to protect
disclosed system data to avoid an accurate state estimation by eavesdroppers.
This survey presents a comprehensive review of existing literature on
privacy-preserving state estimation methods, while also identifying potential
limitations and research gaps. Our primary focus revolves around three types of
methods: cryptography, data perturbation, and transmission scheduling, with
particular emphasis on Kalman-like filters. Within these categories, we delve
into the concepts of homomorphic encryption and differential privacy, which
have been extensively investigated in recent years in the context of
privacy-preserving state estimation. Finally, we shed light on several
technical and fundamental challenges surrounding current methods and propose
potential directions for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.16893v1">The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented
  Generation (RAG)</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-02-23T18:35:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, Jiliang Tang</p>
    <p><b>Summary:</b> Retrieval-augmented generation (RAG) is a powerful technique to facilitate
language model with proprietary and private data, where data privacy is a
pivotal concern. Whereas extensive research has demonstrated the privacy risks
of large language models (LLMs), the RAG technique could potentially reshape
the inherent behaviors of LLM generation, posing new privacy issues that are
currently under-explored. In this work, we conduct extensive empirical studies
with novel attack methods, which demonstrate the vulnerability of RAG systems
on leaking the private retrieval database. Despite the new risk brought by RAG
on the retrieval data, we further reveal that RAG can mitigate the leakage of
the LLMs' training data. Overall, we provide new insights in this paper for
privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG
systems builders. Our code is available at
https://github.com/phycholosogy/RAG-privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.15006v1">opp/ai: Optimistic Privacy-Preserving AI on Blockchain</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-22T22:54:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Cathie So, KD Conway, Xiaohang Yu, Suning Yao, Kartin Wong</p>
    <p><b>Summary:</b> The convergence of Artificial Intelligence (AI) and blockchain technology is
reshaping the digital world, offering decentralized, secure, and efficient AI
services on blockchain platforms. Despite the promise, the high computational
demands of AI on blockchain raise significant privacy and efficiency concerns.
The Optimistic Privacy-Preserving AI (opp/ai) framework is introduced as a
pioneering solution to these issues, striking a balance between privacy
protection and computational efficiency. The framework integrates
Zero-Knowledge Machine Learning (zkML) for privacy with Optimistic Machine
Learning (opML) for efficiency, creating a hybrid model tailored for blockchain
AI services. This study presents the opp/ai framework, delves into the privacy
features of zkML, and assesses the framework's performance and adaptability
across different scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.14983v1">Privacy-Enhancing Collaborative Information Sharing through Federated
  Learning -- A Case of the Insurance Industry</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-02-22T21:46:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Panyi Dong, Zhiyu Quan, Brandon Edwards, Shih-han Wang, Runhuan Feng, Tianyang Wang, Patrick Foley, Prashant Shah</p>
    <p><b>Summary:</b> The report demonstrates the benefits (in terms of improved claims loss
modeling) of harnessing the value of Federated Learning (FL) to learn a single
model across multiple insurance industry datasets without requiring the
datasets themselves to be shared from one company to another. The application
of FL addresses two of the most pressing concerns: limited data volume and data
variety, which are caused by privacy concerns, the rarity of claim events, the
lack of informative rating factors, etc.. During each round of FL,
collaborators compute improvements on the model using their local private data,
and these insights are combined to update a global model. Such aggregation of
insights allows for an increase to the effectiveness in forecasting claims
losses compared to models individually trained at each collaborator.
Critically, this approach enables machine learning collaboration without the
need for raw data to leave the compute infrastructure of each respective data
owner. Additionally, the open-source framework, OpenFL, that is used in our
experiments is designed so that it can be run using confidential computing as
well as with additional algorithmic protections against leakage of information
via the shared model updates. In such a way, FL is implemented as a
privacy-enhancing collaborative learning technique that addresses the
challenges posed by the sensitivity and privacy of data in traditional machine
learning solutions. This paper's application of FL can also be expanded to
other areas including fraud detection, catastrophe modeling, etc., that have a
similar need to incorporate data privacy into machine learning collaborations.
Our framework and empirical results provide a foundation for future
collaborations among insurers, regulators, academic researchers, and InsurTech
experts.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.14544v1">{A New Hope}: Contextual Privacy Policies for Mobile Applications and An
  Approach Toward Automated Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-02-22T13:32:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shidong Pan, Zhen Tao, Thong Hoang, Dawen Zhang, Tianshi Li, Zhenchang Xing, Sherry Xu, Mark Staples, Thierry Rakotoarivelo, David Lo</p>
    <p><b>Summary:</b> Privacy policies have emerged as the predominant approach to conveying
privacy notices to mobile application users. In an effort to enhance both
readability and user engagement, the concept of contextual privacy policies
(CPPs) has been proposed by researchers. The aim of CPPs is to fragment privacy
policies into concise snippets, displaying them only within the corresponding
contexts within the application's graphical user interfaces (GUIs). In this
paper, we first formulate CPP in mobile application scenario, and then present
a novel multimodal framework, named SeePrivacy, specifically designed to
automatically generate CPPs for mobile applications. This method uniquely
integrates vision-based GUI understanding with privacy policy analysis,
achieving 0.88 precision and 0.90 recall to detect contexts, as well as 0.98
precision and 0.96 recall in extracting corresponding policy segments. A human
evaluation shows that 77% of the extracted privacy policy segments were
perceived as well-aligned with the detected contexts. These findings suggest
that SeePrivacy could serve as a significant tool for bolstering user
interaction with, and understanding of, privacy policies. Furthermore, our
solution has the potential to make privacy notices more accessible and
inclusive, thus appealing to a broader demographic. A demonstration of our work
can be accessed at https://cpp4app.github.io/SeePrivacy/</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.13815v1">An Empirical Study on Oculus Virtual Reality Applications: Security and
  Privacy Perspectives</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-21T13:53:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hanyang Guo, Hong-Ning Dai, Xiapu Luo, Zibin Zheng, Gengyang Xu, Fengliang He</p>
    <p><b>Summary:</b> Although Virtual Reality (VR) has accelerated its prevalent adoption in
emerging metaverse applications, it is not a fundamentally new technology. On
one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS.
As a result, VR apps also inherit privacy and security deficiencies from
conventional mobile apps. On the other hand, in contrast to conventional mobile
apps, VR apps can achieve immersive experience via diverse VR devices, such as
head-mounted displays, body sensors, and controllers though achieving this
requires the extensive collection of privacy-sensitive human biometrics.
Moreover, VR apps have been typically implemented by 3D gaming engines (e.g.,
Unity), which also contain intrinsic security vulnerabilities. Inappropriate
use of these technologies may incur privacy leaks and security vulnerabilities
although these issues have not received significant attention compared to the
proliferation of diverse VR apps. In this paper, we develop a security and
privacy assessment tool, namely the VR-SP detector for VR apps. The VR-SP
detector has integrated program static analysis tools and privacy-policy
analysis methods. Using the VR-SP detector, we conduct a comprehensive
empirical study on 500 popular VR apps. We obtain the original apps from the
popular Oculus and SideQuest app stores and extract APK files via the Meta
Oculus Quest 2 device. We evaluate security vulnerabilities and privacy data
leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy
analysis. We find that a number of security vulnerabilities and privacy leaks
widely exist in VR apps. Moreover, our results also reveal conflicting
representations in the privacy policies of these apps and inconsistencies of
the actual data collection with the privacy-policy statements of the apps.
Based on these findings, we make suggestions for the future development of VR
apps.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.13659v1">Privacy-Preserving Instructions for Aligning Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-02-21T09:45:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu</p>
    <p><b>Summary:</b> Service providers of large language model (LLM) applications collect user
instructions in the wild and use them in further aligning LLMs with users'
intentions. These instructions, which potentially contain sensitive
information, are annotated by human workers in the process. This poses a new
privacy risk not addressed by the typical private optimization. To this end, we
propose using synthetic instructions to replace real instructions in data
annotation and model fine-tuning. Formal differential privacy is guaranteed by
generating those synthetic instructions using privately fine-tuned generators.
Crucial in achieving the desired utility is our novel filtering algorithm that
matches the distribution of the synthetic instructions to that of the real
ones. In both supervised fine-tuning and reinforcement learning from human
feedback, our extensive experiments demonstrate the high utility of the final
set of synthetic instructions by showing comparable results to real
instructions. In supervised fine-tuning, models trained with private synthetic
instructions outperform leading open-source models such as Vicuna.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.13087v1">How Does Selection Leak Privacy: Revisiting Private Selection and
  Improved Results for Hyper-parameter Tuning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-20T15:29:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zihang Xiang, Chenglong Wang, Di Wang</p>
    <p><b>Summary:</b> We study the problem of guaranteeing Differential Privacy (DP) in
hyper-parameter tuning, a crucial process in machine learning involving the
selection of the best run from several. Unlike many private algorithms,
including the prevalent DP-SGD, the privacy implications of tuning remain
insufficiently understood. Recent works propose a generic private solution for
the tuning process, yet a fundamental question still persists: is the current
privacy bound for this solution tight?
  This paper contributes both positive and negative answers to this question.
Initially, we provide studies affirming the current privacy analysis is indeed
tight in a general sense. However, when we specifically study the
hyper-parameter tuning problem, such tightness no longer holds. This is first
demonstrated by applying privacy audit on the tuning process. Our findings
underscore a substantial gap between the current theoretical privacy bound and
the empirical bound derived even under the strongest audit setup.
  The gap found is not a fluke. Our subsequent study provides an improved
privacy result for private hyper-parameter tuning due to its distinct
properties. Our privacy results are also more generalizable compared to prior
analyses that are only easily applicable in specific setups.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.12967v1">Quantifying Privacy via Information Density</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">   
  <p><b>Published on:</b> 2024-02-20T12:35:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leonhard Grosse, Sara Saeidian, Parastoo Sadeghi, Tobias J. Oechtering, Mikael Skoglund</p>
    <p><b>Summary:</b> We examine the relationship between privacy metrics that utilize information
density to measure information leakage between a private and a disclosed random
variable. Firstly, we prove that bounding the information density from above or
below in turn implies a lower or upper bound on the information density,
respectively. Using this result, we establish new relationships between local
information privacy, asymmetric local information privacy, pointwise maximal
leakage and local differential privacy. We further provide applications of
these relations to privacy mechanism design. Furthermore, we provide statements
showing the equivalence between a lower bound on information density and
risk-averse adversaries. More specifically, we prove an equivalence between a
guessing framework and a cost-function framework that result in the desired
lower bound on the information density.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.12500v1">Integrating kNN with Foundation Models for Adaptable and Privacy-Aware
  Image Classification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-02-19T20:08:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sebastian Doerrich, Tobias Archut, Francesco Di Salvo, Christian Ledig</p>
    <p><b>Summary:</b> Traditional deep learning models implicity encode knowledge limiting their
transparency and ability to adapt to data changes. Yet, this adaptability is
vital for addressing user data privacy concerns. We address this limitation by
storing embeddings of the underlying training data independently of the model
weights, enabling dynamic data modifications without retraining. Specifically,
our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a
vision-based foundation model, pre-trained self-supervised on natural images,
enhancing interpretability and adaptability. We share open-source
implementations of a previously unpublished baseline method as well as our
performance-improving contributions. Quantitative experiments confirm improved
classification across established benchmark datasets and the method's
applicability to distinct medical image classification tasks. Additionally, we
assess the method's robustness in continual learning and data removal
scenarios. The approach exhibits great promise for bridging the gap between
foundation models' performance and challenges tied to data privacy. The source
code is available at
https://github.com/TobArc/privacy-aware-image-classification-with-kNN.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.11989v1">Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-02-19T09:32:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zihao Luo, Xilie Xu, Feng Liu, Yun Sing Koh, Di Wang, Jingfeng Zhang</p>
    <p><b>Summary:</b> Low-rank adaptation (LoRA) is an efficient strategy for adapting latent
diffusion models (LDMs) on a training dataset to generate specific objects by
minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable
to membership inference (MI) attacks that can judge whether a particular data
point belongs to private training datasets, thus facing severe risks of privacy
leakage. To defend against MI attacks, we make the first effort to propose a
straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is
formulated as a min-max optimization problem where a proxy attack model is
trained by maximizing its MI gain while the LDM is adapted by minimizing the
sum of the adaptation loss and the proxy attack model's MI gain. However, we
empirically disclose that PrivateLoRA has the issue of unstable optimization
due to the large fluctuation of the gradient scale which impedes adaptation. To
mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by
minimizing the ratio of the adaptation loss to the MI gain, which implicitly
rescales the gradient and thus stabilizes the optimization. Our comprehensive
empirical results corroborate that adapted LDMs via Stable PrivateLoRA can
effectively defend against MI attacks while generating high-quality images. Our
code is available at https://github.com/WilliamLUO0/StablePrivateLoRA.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.11582v2">Publicly auditable privacy-preserving electoral rolls</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-02-18T13:11:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Prashant Agrawal, Mahabir Prasad Jhanwar, Subodh Vishnu Sharma, Subhashis Banerjee</p>
    <p><b>Summary:</b> While existing literature on electronic voting has extensively addressed
verifiability of voting protocols, the vulnerability of electoral rolls in
large public elections remains a critical concern. To ensure integrity of
electoral rolls, the current practice is to either make electoral rolls public
or share them with the political parties. However, this enables construction of
detailed voter profiles and selective targeting and manipulation of voters,
thereby undermining the fundamental principle of free and fair elections. In
this paper, we study the problem of designing publicly auditable yet
privacy-preserving electoral rolls. We first formulate a threat model and
provide formal security definitions. We then present a protocol for creation
and maintenance of electoral rolls that mitigates the threats. Eligible voters
can verify their inclusion, whereas political parties and auditors can
statistically audit the electoral roll. The entire electoral roll is never
revealed, which prevents any large-scale systematic voter targeting and
manipulation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.11526v1">Measuring Privacy Loss in Distributed Spatio-Temporal Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-18T09:53:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tatsuki Koga, Casey Meehan, Kamalika Chaudhuri</p>
    <p><b>Summary:</b> Statistics about traffic flow and people's movement gathered from multiple
geographical locations in a distributed manner are the driving force powering
many applications, such as traffic prediction, demand prediction, and
restaurant occupancy reports. However, these statistics are often based on
sensitive location data of people, and hence privacy has to be preserved while
releasing them. The standard way to do this is via differential privacy, which
guarantees a form of rigorous, worst-case, person-level privacy. In this work,
motivated by several counter-intuitive features of differential privacy in
distributed location applications, we propose an alternative privacy loss
against location reconstruction attacks by an informed adversary. Our
experiments on real and synthetic data demonstrate that our privacy loss better
reflects our intuitions on individual privacy violation in the distributed
spatio-temporal setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.11193v1">Privacy Impact Assessments in the Wild: A Scoping Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-17T05:07:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leonardo Horn Iwaya, Ala Sarah Alaqra, Marit Hansen, Simone Fischer-Hübner</p>
    <p><b>Summary:</b> Privacy Impact Assessments (PIAs) offer a systematic process for assessing
the privacy impacts of a project or system. As a privacy engineering strategy,
PIAs are heralded as one of the main approaches to privacy by design,
supporting the early identification of threats and controls. However, there is
still a shortage of empirical evidence on their uptake and proven effectiveness
in practice. To better understand the current state of literature and research,
this paper provides a comprehensive Scoping Review (ScR) on the topic of PIAs
"in the wild", following the well-established Preferred Reporting Items for
Systematic reviews and Meta-Analyses (PRISMA) guidelines. As a result, this ScR
includes 45 studies, providing an extensive synthesis of the existing body of
knowledge, classifying types of research and publications, appraising the
methodological quality of primary research, and summarising the positive and
negative aspects of PIAs in practice, as reported by studies. This ScR also
identifies significant research gaps (e.g., evidence gaps from contradictory
results and methodological gaps from research design deficiencies), future
research pathways, and implications for researchers, practitioners, and
policymakers developing and evaluating PIA frameworks. As we conclude, there is
still a significant need for more primary research on the topic, both
qualitative and quantitative. A critical appraisal of qualitative studies
(n=28) revealed deficiencies in the methodological quality, and only four
quantitative studies were identified, suggesting that current primary research
remains incipient. Nonetheless, PIAs can be regarded as a prominent sub-area in
the broader field of Empirical Privacy Engineering, warranting further research
toward more evidence-based practices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.10473v1">Privacy for Fairness: Information Obfuscation for Fair Representation
  Learning with Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-02-16T06:35:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Songjie Xie, Youlong Wu, Jiaxuan Li, Ming Ding, Khaled B. Letaief</p>
    <p><b>Summary:</b> As machine learning (ML) becomes more prevalent in human-centric
applications, there is a growing emphasis on algorithmic fairness and privacy
protection. While previous research has explored these areas as separate
objectives, there is a growing recognition of the complex relationship between
privacy and fairness. However, previous works have primarily focused on
examining the interplay between privacy and fairness through empirical
investigations, with limited attention given to theoretical exploration. This
study aims to bridge this gap by introducing a theoretical framework that
enables a comprehensive examination of their interrelation. We shall develop
and analyze an information bottleneck (IB) based information obfuscation method
with local differential privacy (LDP) for fair representation learning. In
contrast to many empirical studies on fairness in ML, we show that the
incorporation of LDP randomizers during the encoding process can enhance the
fairness of the learned representation. Our analysis will demonstrate that the
disclosure of sensitive information is constrained by the privacy budget of the
LDP randomizer, thereby enabling the optimization process within the IB
framework to effectively suppress sensitive information while preserving the
desired utility through obfuscation. Based on the proposed method, we further
develop a variational representation encoding approach that simultaneously
achieves fairness and LDP. Our variational encoding approach offers practical
advantages. It is trained using a non-adversarial method and does not require
the introduction of any variational prior. Extensive experiments will be
presented to validate our theoretical results and demonstrate the ability of
our proposed approach to achieve both LDP and fairness while preserving
adequate utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.10423v1">Connect the dots: Dataset Condensation, Differential Privacy, and
  Adversarial Uncertainty</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-02-16T03:12:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kenneth Odoh</p>
    <p><b>Summary:</b> Our work focuses on understanding the underpinning mechanism of dataset
condensation by drawing connections with ($\epsilon$, $\delta$)-differential
privacy where the optimal noise, $\epsilon$, is chosen by adversarial
uncertainty \cite{Grining2017}. We can answer the question about the inner
workings of the dataset condensation procedure. Previous work \cite{dong2022}
proved the link between dataset condensation (DC) and ($\epsilon$,
$\delta$)-differential privacy. However, it is unclear from existing works on
ablating DC to obtain a lower-bound estimate of $\epsilon$ that will suffice
for creating high-fidelity synthetic data. We suggest that adversarial
uncertainty is the most appropriate method to achieve an optimal noise level,
$\epsilon$. As part of the internal dynamics of dataset condensation, we adopt
a satisfactory scheme for noise estimation that guarantees high-fidelity data
while providing privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.10145v1">A chaotic maps-based privacy-preserving distributed deep learning for
  incomplete and Non-IID datasets</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-02-15T17:49:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Irina Arévalo, Jose L. Salmeron</p>
    <p><b>Summary:</b> Federated Learning is a machine learning approach that enables the training
of a deep learning model among several participants with sensitive data that
wish to share their own knowledge without compromising the privacy of their
data. In this research, the authors employ a secured Federated Learning method
with an additional layer of privacy and proposes a method for addressing the
non-IID challenge. Moreover, differential privacy is compared with
chaotic-based encryption as layer of privacy. The experimental approach
assesses the performance of the federated deep learning model with differential
privacy using both IID and non-IID data. In each experiment, the Federated
Learning process improves the average performance metrics of the deep neural
network, even in the case of non-IID data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.10102v1">A privacy-preserving, distributed and cooperative FCM-based learning
  approach for Cancer Research</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-02-15T16:56:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jose L. Salmeron, Irina Arévalo</p>
    <p><b>Summary:</b> Distributed Artificial Intelligence is attracting interest day by day. In
this paper, the authors introduce an innovative methodology for distributed
learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a
privacy-preserving way. The authors design a training scheme for collaborative
FCM learning that offers data privacy compliant with the current regulation.
This method is applied to a cancer detection problem, proving that the
performance of the model is improved by the Federated Learning process, and
obtaining similar results to the ones that can be found in the literature.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.10065v1">How Much Does Each Datapoint Leak Your Privacy? Quantifying the
  Per-datum Membership Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36">  
  <p><b>Published on:</b> 2024-02-15T16:30:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Achraf Azize, Debabrota Basu</p>
    <p><b>Summary:</b> We study the per-datum Membership Inference Attacks (MIAs), where an attacker
aims to infer whether a fixed target datum has been included in the input
dataset of an algorithm and thus, violates privacy. First, we define the
membership leakage of a datum as the advantage of the optimal adversary
targeting to identify it. Then, we quantify the per-datum membership leakage
for the empirical mean, and show that it depends on the Mahalanobis distance
between the target datum and the data-generating distribution. We further
assess the effect of two privacy defences, i.e. adding Gaussian noise and
sub-sampling. We quantify exactly how both of them decrease the per-datum
membership leakage. Our analysis builds on a novel proof technique that
combines an Edgeworth expansion of the likelihood ratio test and a
Lindeberg-Feller central limit theorem. Our analysis connects the existing
likelihood ratio and scalar product attacks, and also justifies different
canary selection strategies used in the privacy auditing literature. Finally,
our experiments demonstrate the impacts of the leakage score, the sub-sampling
ratio and the noise scale on the per-datum membership leakage as indicated by
the theory.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.10001v1">Privacy Attacks in Decentralized Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-15T15:06:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abdellah El Mrini, Edwige Cyffers, Aurélien Bellet</p>
    <p><b>Summary:</b> Decentralized Gradient Descent (D-GD) allows a set of users to perform
collaborative learning without sharing their data by iteratively averaging
local model updates with their neighbors in a network graph. The absence of
direct communication between non-neighbor nodes might lead to the belief that
users cannot infer precise information about the data of others. In this work,
we demonstrate the opposite, by proposing the first attack against D-GD that
enables a user (or set of users) to reconstruct the private data of other users
outside their immediate neighborhood. Our approach is based on a reconstruction
attack against the gossip averaging protocol, which we then extend to handle
the additional challenges raised by D-GD. We validate the effectiveness of our
attack on real graphs and datasets, showing that the number of users
compromised by a single or a handful of attackers is often surprisingly large.
We empirically investigate some of the factors that affect the performance of
the attack, namely the graph topology, the number of attackers, and their
position in the graph.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.09716v1">User Privacy Harms and Risks in Conversational AI: A Proposed Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-02-15T05:21:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ece Gumusel, Kyrie Zhixuan Zhou, Madelyn Rose Sanfilippo</p>
    <p><b>Summary:</b> This study presents a unique framework that applies and extends Solove
(2006)'s taxonomy to address privacy concerns in interactions with text-based
AI chatbots. As chatbot prevalence grows, concerns about user privacy have
heightened. While existing literature highlights design elements compromising
privacy, a comprehensive framework is lacking. Through semi-structured
interviews with 13 participants interacting with two AI chatbots, this study
identifies 9 privacy harms and 9 privacy risks in text-based interactions.
Using a grounded theory approach for interview and chatlog analysis, the
framework examines privacy implications at various interaction stages. The aim
is to offer developers, policymakers, and researchers a tool for responsible
and secure implementation of conversational AI, filling the existing gap in
addressing privacy issues associated with text-based AI chatbots.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.09715v1">DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated
  Learning as a Service</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-15T05:19:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yu Liu, Zibo Wang, Yifei Zhu, Chen Chen</p>
    <p><b>Summary:</b> Federated learning (FL) has emerged as a prevalent distributed machine
learning scheme that enables collaborative model training without aggregating
raw data. Cloud service providers further embrace Federated Learning as a
Service (FLaaS), allowing data analysts to execute their FL training pipelines
over differentially-protected data. Due to the intrinsic properties of
differential privacy, the enforced privacy level on data blocks can be viewed
as a privacy budget that requires careful scheduling to cater to diverse
training pipelines. Existing privacy budget scheduling studies prioritize
either efficiency or fairness individually. In this paper, we propose
DPBalance, a novel privacy budget scheduling mechanism that jointly optimizes
both efficiency and fairness. We first develop a comprehensive utility function
incorporating data analyst-level dominant shares and FL-specific performance
metrics. A sequential allocation mechanism is then designed using the Lagrange
multiplier method and effective greedy heuristics. We theoretically prove that
DPBalance satisfies Pareto Efficiency, Sharing Incentive, Envy-Freeness, and
Weak Strategy Proofness. We also theoretically prove the existence of a
fairness-efficiency tradeoff in privacy budgeting. Extensive experiments
demonstrate that DPBalance outperforms state-of-the-art solutions, achieving an
average efficiency improvement of $1.44\times \sim 3.49 \times$, and an average
fairness improvement of $1.37\times \sim 24.32 \times$.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.09710v1">Preserving Data Privacy for ML-driven Applications in Open Radio Access
  Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-02-15T05:06:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pranshav Gajjar, Azuka Chiejina, Vijay K. Shah</p>
    <p><b>Summary:</b> Deep learning offers a promising solution to improve spectrum access
techniques by utilizing data-driven approaches to manage and share limited
spectrum resources for emerging applications. For several of these
applications, the sensitive wireless data (such as spectrograms) are stored in
a shared database or multistakeholder cloud environment and are therefore prone
to privacy leaks. This paper aims to address such privacy concerns by examining
the representative case study of shared database scenarios in 5G Open Radio
Access Network (O-RAN) networks where we have a shared database within the
near-real-time (near-RT) RAN intelligent controller. We focus on securing the
data that can be used by machine learning (ML) models for spectrum sharing and
interference mitigation applications without compromising the model and network
performances. The underlying idea is to leverage a (i) Shuffling-based
learnable encryption technique to encrypt the data, following which, (ii)
employ a custom Vision transformer (ViT) as the trained ML model that is
capable of performing accurate inferences on such encrypted data. The paper
offers a thorough analysis and comparisons with analogous convolutional neural
networks (CNN) as well as deeper architectures (such as ResNet-50) as
baselines. Our experiments showcase that the proposed approach significantly
outperforms the baseline CNN with an improvement of 24.5% and 23.9% for the
percent accuracy and F1-Score respectively when operated on encrypted data.
Though deeper ResNet-50 architecture is obtained as a slightly more accurate
model, with an increase of 4.4%, the proposed approach boasts a reduction of
parameters by 99.32%, and thus, offers a much-improved prediction time by
nearly 60%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.09621v1">Schnorr Approval-Based Secure and Privacy-Preserving IoV Data
  Aggregation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-02-14T23:40:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rui Liu, Jianping Pan</p>
    <p><b>Summary:</b> Secure and privacy-preserving data aggregation in the Internet of Vehicles
(IoV) continues to be a focal point of interest in both the industry and
academia. Aiming at tackling the challenges and solving the remaining
limitations of existing works, this paper introduces a novel Schnorr
approval-based IoV data aggregation framework based on a two-layered
architecture. In this framework, a server can aggregate the IoV data from
clusters without inferring the raw data, real identity and trajectories of
vehicles. Notably, we avoid incorporating the widely-accepted techniques such
as homomorphic encryption and digital pseudonym to avoid introducing high
computation cost to vehicles. We propose a novel concept, data approval, based
on the Schnorr signature scheme. With the approval, the fake data injection
attack carried out by a cluster head can be defended against. The separation of
liability is achieved as well. The evaluation shows that the framework is
secure and lightweight for vehicles in terms of the computation and
communication costs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.09611v1">Towards Privacy-Aware Sign Language Translation at Scale</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-14T22:57:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Phillip Rust, Bowen Shi, Skyler Wang, Necati Cihan Camgöz, Jean Maillard</p>
    <p><b>Summary:</b> A major impediment to the advancement of sign language translation (SLT) is
data scarcity. Much of the sign language data currently available on the web
cannot be used for training supervised models due to the lack of aligned
captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears
privacy risks due to the presence of biometric information, which the
responsible development of SLT technologies should account for. In this work,
we propose a two-stage framework for privacy-aware SLT at scale that addresses
both of these issues. We introduce SSVP-SLT, which leverages self-supervised
video pretraining on anonymized and unannotated videos, followed by supervised
SLT finetuning on a curated parallel dataset. SSVP-SLT achieves
state-of-the-art finetuned and zero-shot gloss-free SLT performance on the
How2Sign dataset, outperforming the strongest respective baselines by over 3
BLEU-4. Based on controlled experiments, we further discuss the advantages and
limitations of self-supervised pretraining and anonymization via facial
obfuscation for SLT.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.09540v1">Why Does Differential Privacy with Large Epsilon Defend Against
  Practical Membership Inference Attacks?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-02-14T19:31:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Andrew Lowy, Zhuohang Li, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang</p>
    <p><b>Summary:</b> For small privacy parameter $\epsilon$, $\epsilon$-differential privacy (DP)
provides a strong worst-case guarantee that no membership inference attack
(MIA) can succeed at determining whether a person's data was used to train a
machine learning model. The guarantee of DP is worst-case because: a) it holds
even if the attacker already knows the records of all but one person in the
data set; and b) it holds uniformly over all data sets. In practical
applications, such a worst-case guarantee may be overkill: practical attackers
may lack exact knowledge of (nearly all of) the private data, and our data set
might be easier to defend, in some sense, than the worst-case data set. Such
considerations have motivated the industrial deployment of DP models with large
privacy parameter (e.g. $\epsilon \geq 7$), and it has been observed
empirically that DP with large $\epsilon$ can successfully defend against
state-of-the-art MIAs. Existing DP theory cannot explain these empirical
findings: e.g., the theoretical privacy guarantees of $\epsilon \geq 7$ are
essentially vacuous. In this paper, we aim to close this gap between theory and
practice and understand why a large DP parameter can prevent practical MIAs. To
tackle this problem, we propose a new privacy notion called practical
membership privacy (PMP). PMP models a practical attacker's uncertainty about
the contents of the private data. The PMP parameter has a natural
interpretation in terms of the success rate of a practical MIA on a given data
set. We quantitatively analyze the PMP parameter of two fundamental DP
mechanisms: the exponential mechanism and Gaussian mechanism. Our analysis
reveals that a large DP parameter often translates into a much smaller PMP
parameter, which guarantees strong privacy against practical MIAs. Using our
findings, we offer principled guidance for practitioners in choosing the DP
parameter.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.09316v1">Only My Model On My Data: A Privacy Preserving Approach Protecting one
  Model and Deceiving Unauthorized Black-Box Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-14T17:11:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weiheng Chai, Brian Testa, Huantao Ren, Asif Salekin, Senem Velipasalar</p>
    <p><b>Summary:</b> Deep neural networks are extensively applied to real-world tasks, such as
face recognition and medical image classification, where privacy and data
protection are critical. Image data, if not protected, can be exploited to
infer personal or contextual information. Existing privacy preservation
methods, like encryption, generate perturbed images that are unrecognizable to
even humans. Adversarial attack approaches prohibit automated inference even
for authorized stakeholders, limiting practical incentives for commercial and
widespread adaptation. This pioneering study tackles an unexplored practical
privacy preservation use case by generating human-perceivable images that
maintain accurate inference by an authorized model while evading other
unauthorized black-box models of similar or dissimilar objectives, and
addresses the previous research gaps. The datasets employed are ImageNet, for
image classification, Celeba-HQ dataset, for identity classification, and
AffectNet, for emotion classification. Our results show that the generated
images can successfully maintain the accuracy of a protected model and degrade
the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and
55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.08956v1">Seagull: Privacy preserving network verification system</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-02-14T05:56:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jaber Daneshamooz, Melody Yu, Sucheer Maddury</p>
    <p><b>Summary:</b> The current routing protocol used in the internet backbone is based on manual
configuration, making it susceptible to errors. To mitigate these
configuration-related issues, it becomes imperative to validate the accuracy
and convergence of the algorithm, ensuring a seamless operation devoid of
problems. However, the process of network verification faces challenges related
to privacy and scalability. This paper addresses these challenges by
introducing a novel approach: leveraging privacy-preserving computation,
specifically multiparty computation (MPC), to verify the correctness of
configurations in the internet backbone, governed by the BGP protocol. Not only
does our proposed solution effectively address scalability concerns, but it
also establishes a robust privacy framework. Through rigorous analysis, we
demonstrate that our approach maintains privacy by not disclosing any
information beyond the query result, thus providing a comprehensive and secure
solution to the intricacies associated with routing protocol verification in
large-scale networks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.08826v1">Equilibria of Data Marketplaces with Privacy-Aware Sellers under
  Endogenous Privacy Costs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Science and Game Theory-5BC0EB">
  <p><b>Published on:</b> 2024-02-13T22:10:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Diptangshu Sen, Jingyan Wang, Juba Ziani</p>
    <p><b>Summary:</b> We study a two-sided online data ecosystem comprised of an online platform,
users on the platform, and downstream learners or data buyers. The learners can
buy user data on the platform (to run a statistic or machine learning task).
Potential users decide whether to join by looking at the trade-off between i)
their benefit from joining the platform and interacting with other users and
ii) the privacy costs they incur from sharing their data.
  First, we introduce a novel modeling element for two-sided data platforms:
the privacy costs of the users are endogenous and depend on how much of their
data is purchased by the downstream learners. Then, we characterize marketplace
equilibria in certain simple settings. In particular, we provide a full
characterization in two variants of our model that correspond to different
utility functions for the users: i) when each user gets a constant benefit for
participating in the platform and ii) when each user's benefit is linearly
increasing in the number of other users that participate. In both variants,
equilibria in our setting are significantly different from equilibria when
privacy costs are exogenous and fixed, highlighting the importance of taking
endogeneity in the privacy costs into account. Finally, we provide simulations
and semi-synthetic experiments to extend our results to more general
assumptions. We experiment with different distributions of users' privacy costs
and different functional forms of the users' utilities for joining the
platform.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.08655v1">Assessing the Privacy Risk of Cross-Platform Identity Linkage using Eye
  Movement Biometrics</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-02-13T18:37:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Samantha Aziz, Oleg Komogortsev</p>
    <p><b>Summary:</b> The recent emergence of ubiquitous, multi-platform eye tracking has raised
user privacy concerns over re-identification across platforms, where a person
is re-identified across multiple eye tracking-enabled platforms using
personally identifying information that is implicitly expressed through their
eye movement. We present an empirical investigation quantifying a modern eye
movement biometric model's ability to link subject identities across three
different eye tracking devices using eye movement signals from each device. We
show that a state-of-the art eye movement biometrics model demonstrates
above-chance levels of biometric performance (34.99% equal error rate, 15%
rank-1 identification rate) when linking user identities across one pair of
devices, but not for the other. Considering these findings, we also discuss the
impact that eye tracking signal quality has on the model's ability to
meaningfully associate a subject's identity between two substantially different
eye tracking devices. Our investigation advances a fundamental understanding of
the privacy risks for identity linkage across platforms by employing both
quantitative and qualitative measures of biometric performance, including a
visualization of the model's ability to distinguish genuine and imposter
authentication attempts across platforms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.08227v1">Privacy-Preserving Language Model Inference with Instance Obfuscation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-02-13T05:36:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yixiang Yao, Fei Wang, Srivatsan Ravi, Muhao Chen</p>
    <p><b>Summary:</b> Language Models as a Service (LMaaS) offers convenient access for developers
and researchers to perform inference using pre-trained language models.
Nonetheless, the input data and the inference results containing private
information are exposed as plaintext during the service call, leading to
privacy issues. Recent studies have started tackling the privacy issue by
transforming input data into privacy-preserving representation from the
user-end with the techniques such as noise addition and content perturbation,
while the exploration of inference result protection, namely decision privacy,
is still a blank page. In order to maintain the black-box manner of LMaaS,
conducting data privacy protection, especially for the decision, is a
challenging task because the process has to be seamless to the models and
accompanied by limited communication and computation overhead. We thus propose
Instance-Obfuscated Inference (IOI) method, which focuses on addressing the
decision privacy issue of natural language understanding tasks in their
complete life-cycle. Besides, we conduct comprehensive experiments to evaluate
the performance as well as the privacy-protection strength of the proposed
method on various benchmarking tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.08223v3">The Limits of Price Discrimination Under Privacy Constraints</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Science and Game Theory-5BC0EB">
  <p><b>Published on:</b> 2024-02-13T05:28:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alireza Fallah, Michael I. Jordan, Ali Makhdoumi, Azarakhsh Malekian</p>
    <p><b>Summary:</b> We consider a producer's problem of selling a product to a continuum of
privacy-conscious consumers, where the producer can implement third-degree
price discrimination, offering different prices to different market segments.
In the absence of privacy constraints, Bergemann, Brooks, and Morris [2015]
characterize the set of all possible consumer-producer utilities, showing that
it is a triangle. We consider a privacy mechanism that provides a degree of
protection by probabilistically masking each market segment, and we establish
that the resultant set of all consumer-producer utilities forms a convex
polygon, characterized explicitly as a linear mapping of a certain
high-dimensional convex polytope into $\mathbb{R}^2$. This characterization
enables us to investigate the impact of the privacy mechanism on both producer
and consumer utilities. In particular, we establish that the privacy constraint
always hurts the producer by reducing both the maximum and minimum utility
achievable. From the consumer's perspective, although the privacy mechanism
ensures an increase in the minimum utility compared to the non-private
scenario, interestingly, it may reduce the maximum utility. Finally, we
demonstrate that increasing the privacy level does not necessarily intensify
these effects. For instance, the maximum utility for the producer or the
minimum utility for the consumer may exhibit nonmonotonic behavior in response
to an increase of the privacy level.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.08156v2">Group Decision-Making among Privacy-Aware Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B"> 
  <p><b>Published on:</b> 2024-02-13T01:38:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Marios Papachristou, M. Amin Rahimian</p>
    <p><b>Summary:</b> How can individuals exchange information to learn from each other despite
their privacy needs and security concerns? For example, consider individuals
deliberating a contentious topic and being concerned about divulging their
private experiences. Preserving individual privacy and enabling efficient
social learning are both important desiderata but seem fundamentally at odds
with each other and very hard to reconcile. We do so by controlling information
leakage using rigorous statistical guarantees that are based on differential
privacy (DP). Our agents use log-linear rules to update their beliefs after
communicating with their neighbors. Adding DP randomization noise to beliefs
provides communicating agents with plausible deniability with regard to their
private information and their network neighborhoods. We consider two learning
environments one for distributed maximum-likelihood estimation given a finite
number of private signals and another for online learning from an infinite,
intermittent signal stream. Noisy information aggregation in the finite case
leads to interesting tradeoffs between rejecting low-quality states and making
sure all high-quality states are accepted in the algorithm output. Our results
flesh out the nature of the trade-offs in both cases between the quality of the
group decision outcomes, learning accuracy, communication cost, and the level
of privacy protections that the agents are afforded.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.09477v1">PANORAMIA: Privacy Auditing of Machine Learning Models without
  Retraining</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-12T22:56:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mishaal Kazmi, Hadrien Lautraite, Alireza Akbari, Mauricio Soroco, Qiaoyue Tang, Tao Wang, Sébastien Gambs, Mathias Lécuyer</p>
    <p><b>Summary:</b> We introduce a privacy auditing scheme for ML models that relies on
membership inference attacks using generated data as "non-members". This
scheme, which we call PANORAMIA, quantifies the privacy leakage for large-scale
ML models without control of the training process or model re-training and only
requires access to a subset of the training data. To demonstrate its
applicability, we evaluate our auditing scheme across multiple ML domains,
ranging from image and tabular data classification to large-scale language
models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.07687v2">Privacy-Preserving Gaze Data Streaming in Immersive Interactive Virtual
  Reality: Robustness and User Experience</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-12T14:53:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ethan Wilson, Azim Ibragimov, Michael J. Proulx, Sai Deep Tetali, Kevin Butler, Eakta Jain</p>
    <p><b>Summary:</b> Eye tracking is routinely being incorporated into virtual reality (VR)
systems. Prior research has shown that eye tracking data, if exposed, can be
used for re-identification attacks. The state of our knowledge about currently
existing privacy mechanisms is limited to privacy-utility trade-off curves
based on data-centric metrics of utility, such as prediction error, and
black-box threat models. We propose that for interactive VR applications, it is
essential to consider user-centric notions of utility and a variety of threat
models. We develop a methodology to evaluate real-time privacy mechanisms for
interactive VR applications that incorporate subjective user experience and
task performance metrics. We evaluate selected privacy mechanisms using this
methodology and find that re-identification accuracy can be decreased to as low
as 14% while maintaining a high usability score and reasonable task
performance. Finally, we elucidate three threat scenarios (black-box, black-box
with exemplars, and white-box) and assess how well the different privacy
mechanisms hold up to these adversarial scenarios.
  This work advances the state of the art in VR privacy by providing a
methodology for end-to-end assessment of the risk of re-identification attacks
and potential mitigating solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.07584v1">Privacy-Optimized Randomized Response for Sharing Multi-Attribute Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-12T11:34:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Akito Yamamoto, Tetsuo Shibuya</p>
    <p><b>Summary:</b> With the increasing amount of data in society, privacy concerns in data
sharing have become widely recognized. Particularly, protecting personal
attribute information is essential for a wide range of aims from crowdsourcing
to realizing personalized medicine. Although various differentially private
methods based on randomized response have been proposed for single attribute
information or specific analysis purposes such as frequency estimation, there
is a lack of studies on the mechanism for sharing individuals' multiple
categorical information itself. The existing randomized response for sharing
multi-attribute data uses the Kronecker product to perturb each attribute
information in turn according to the respective privacy level but achieves only
a weak privacy level for the entire dataset. Therefore, in this study, we
propose a privacy-optimized randomized response that guarantees the strongest
privacy in sharing multi-attribute data. Furthermore, we present an efficient
heuristic algorithm for constructing a near-optimal mechanism. The time
complexity of our algorithm is O(k^2), where k is the number of attributes, and
it can be performed in about 1 second even for large datasets with k = 1,000.
The experimental results demonstrate that both of our methods provide
significantly stronger privacy guarantees for the entire dataset than the
existing method. In addition, we show an analysis example using genome
statistics to confirm that our methods can achieve less than half the output
error compared with that of the existing method. Overall, this study is an
important step toward trustworthy sharing and analysis of multi-attribute data.
The Python implementation of our experiments and supplemental results are
available at https://github.com/ay0408/Optimized-RR.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.07367v1">Utilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-12T01:55:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Liming Jiang</p>
    <p><b>Summary:</b> Mini-applications, commonly referred to as mini-apps, are compact software
programs embedded within larger applications or platforms, offering targeted
functionality without the need for separate installations. Typically web-based
or cloud-hosted, these mini-apps streamline user experiences by providing
focused services accessible through web browsers or mobile apps. Their
simplicity, speed, and integration capabilities make them valuable additions to
messaging platforms, social media networks, e-commerce sites, and various
digital environments. WeChat Mini Programs, a prominent feature of China's
leading messaging app, exemplify this trend, offering users a seamless array of
services without additional downloads. Leveraging WeChat's extensive user base
and payment infrastructure, Mini Programs facilitate efficient transactions and
bridge online and offline experiences, shaping China's digital landscape
significantly. This paper investigates the potential of employing Large
Language Models (LLMs) to detect privacy breaches within WeChat Mini Programs.
Given the widespread use of Mini Programs and growing concerns about data
privacy, this research seeks to determine if LLMs can effectively identify
instances of privacy leakage within this ecosystem. Through meticulous analysis
and experimentation, we aim to highlight the efficacy of LLMs in safeguarding
user privacy and security within the WeChat Mini Program environment, thereby
contributing to a more secure digital landscape.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.07180v2">MAGNETO: Edge AI for Human Activity Recognition -- Privacy and
  Personalization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-11T12:29:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jingwei Zuo, George Arvanitakis, Mthandazo Ndhlovu, Hakim Hacid</p>
    <p><b>Summary:</b> Human activity recognition (HAR) is a well-established field, significantly
advanced by modern machine learning (ML) techniques. While companies have
successfully integrated HAR into consumer products, they typically rely on a
predefined activity set, which limits personalizations at the user level (edge
devices). Despite advancements in Incremental Learning for updating models with
new data, this often occurs on the Cloud, necessitating regular data transfers
between cloud and edge devices, thus leading to data privacy issues. In this
paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the
Cloud to the Edge. MAGNETO allows incremental human activity learning directly
on the Edge devices, without any data exchange with the Cloud. This enables
strong privacy guarantees, low processing latency, and a high degree of
personalization for users. In particular, we demonstrate MAGNETO in an Android
device, validating the whole pipeline from data collection to result
visualization.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.07002v1">Clients Collaborate: Flexible Differentially Private Federated Learning
  with Guaranteed Improvement of Utility-Privacy Trade-off</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-10T17:39:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuecheng Li, Tong Wang, Chuan Chen, Jian Lou, Bin Chen, Lei Yang, Zibin Zheng</p>
    <p><b>Summary:</b> To defend against privacy leakage of user data, differential privacy is
widely used in federated learning, but it is not free. The addition of noise
randomly disrupts the semantic integrity of the model and this disturbance
accumulates with increased communication rounds. In this paper, we introduce a
novel federated learning framework with rigorous privacy guarantees, named
FedCEO, designed to strike a trade-off between model utility and user privacy
by letting clients ''Collaborate with Each Other''. Specifically, we perform
efficient tensor low-rank proximal optimization on stacked local model
parameters at the server, demonstrating its capability to flexibly truncate
high-frequency components in spectral space. This implies that our FedCEO can
effectively recover the disrupted semantic information by smoothing the global
semantic space for different privacy settings and continuous training
processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an
order of $\sqrt{d}$, where $d$ is the input dimension. We illustrate our
theoretical results with experiments on representative image datasets. It
observes significant performance improvements and strict privacy guarantees
under different privacy settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.06701v1">Privacy Profiles for Private Selection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-09T08:31:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antti Koskela, Rachel Redberg, Yu-Xiang Wang</p>
    <p><b>Summary:</b> Private selection mechanisms (e.g., Report Noisy Max, Sparse Vector) are
fundamental primitives of differentially private (DP) data analysis with wide
applications to private query release, voting, and hyperparameter tuning.
Recent work (Liu and Talwar, 2019; Papernot and Steinke, 2022) has made
significant progress in both generalizing private selection mechanisms and
tightening their privacy analysis using modern numerical privacy accounting
tools, e.g., R\'enyi DP. But R\'enyi DP is known to be lossy when
$(\epsilon,\delta)$-DP is ultimately needed, and there is a trend to close the
gap by directly handling privacy profiles, i.e., $\delta$ as a function of
$\epsilon$ or its equivalent dual form known as $f$-DPs. In this paper, we work
out an easy-to-use recipe that bounds the privacy profiles of ReportNoisyMax
and PrivateTuning using the privacy profiles of the base algorithms they
corral. Numerically, our approach improves over the RDP-based accounting in all
regimes of interest and leads to substantial benefits in end-to-end private
learning experiments. Our analysis also suggests new distributions, e.g.,
binomial distribution for randomizing the number of rounds that leads to more
substantial improvements in certain regimes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.06137v1">On the Privacy of Selection Mechanisms with Gaussian Noise</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-09T02:11:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jonathan Lebensold, Doina Precup, Borja Balle</p>
    <p><b>Summary:</b> Report Noisy Max and Above Threshold are two classical differentially private
(DP) selection mechanisms. Their output is obtained by adding noise to a
sequence of low-sensitivity queries and reporting the identity of the query
whose (noisy) answer satisfies a certain condition. Pure DP guarantees for
these mechanisms are easy to obtain when Laplace noise is added to the queries.
On the other hand, when instantiated using Gaussian noise, standard analyses
only yield approximate DP guarantees despite the fact that the outputs of these
mechanisms lie in a discrete space. In this work, we revisit the analysis of
Report Noisy Max and Above Threshold with Gaussian noise and show that, under
the additional assumption that the underlying queries are bounded, it is
possible to provide pure ex-ante DP bounds for Report Noisy Max and pure
ex-post DP bounds for Above Threshold. The resulting bounds are tight and
depend on closed-form expressions that can be numerically evaluated using
standard methods. Empirically we find these lead to tighter privacy accounting
in the high privacy, low data regime. Further, we propose a simple privacy
filter for composing pure ex-post DP guarantees, and use it to derive a fully
adaptive Gaussian Sparse Vector Technique mechanism. Finally, we provide
experiments on mobility and energy consumption datasets demonstrating that our
Sparse Vector Technique is practically competitive with previous approaches and
requires less hyper-parameter tuning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.05860v1">Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic
  Surgery</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-02-08T17:44:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mengya Xu, Mobarakol Islam, Long Bai, Hongliang Ren</p>
    <p><b>Summary:</b> Deep Neural Networks (DNNs) based semantic segmentation of the robotic
instruments and tissues can enhance the precision of surgical activities in
robot-assisted surgery. However, in biological learning, DNNs cannot learn
incremental tasks over time and exhibit catastrophic forgetting, which refers
to the sharp decline in performance on previously learned tasks after learning
a new one. Specifically, when data scarcity is the issue, the model shows a
rapid drop in performance on previously learned instruments after learning new
data with new instruments. The problem becomes worse when it limits releasing
the dataset of the old instruments for the old model due to privacy concerns
and the unavailability of the data for the new or updated version of the
instruments for the continual learning model. For this purpose, we develop a
privacy-preserving synthetic continual semantic segmentation framework by
blending and harmonizing (i) open-source old instruments foreground to the
synthesized background without revealing real patient data in public and (ii)
new instruments foreground to extensively augmented real background. To boost
the balanced logit distillation from the old model to the continual learning
model, we design overlapping class-aware temperature normalization (CAT) by
controlling model learning utility. We also introduce multi-scale
shifted-feature distillation (SD) to maintain long and short-range spatial
relationships among the semantic objects where conventional short-range spatial
features with limited information reduce the power of feature distillation. We
demonstrate the effectiveness of our framework on the EndoVis 2017 and 2018
instrument segmentation dataset with a generalized continual learning setting.
Code is available at~\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.05690v1">Overcoming Noise Limitations in QKD with Quantum Privacy Amplification</a></h3>
  
  <p><b>Published on:</b> 2024-02-08T14:07:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Philipp Sohr, Sebastian Ecker, Lukas Bulla, Martin Bohmann, Rupert Ursin</p>
    <p><b>Summary:</b> High-quality, distributed quantum entanglement is the distinctive resource
for quantum communication and forms the foundation for the unequalled level of
security that can be assured in quantum key distribution. While the
entanglement provider does not need to be trusted, the secure key rate drops to
zero if the entanglement used is too noisy. In this paper, we show
experimentally that QPA is able to increase the secure key rate achievable with
QKD by improving the quality of distributed entanglement, thus increasing the
quantum advantage in QKD. Beyond that, we show that QPA enables key generation
at noise levels that previously prevented key generation. These remarkable
results were only made possible by the efficient implementation exploiting
hyperentanglement in the polarisation and energy-time degrees of freedom. We
provide a detailed characterisation of the gain in secure key rate achieved in
our proof-of-principle experiment at different noise levels. The results are
paramount for the implementation of a global quantum network linking quantum
processors and ensuring future-proof data security.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.05453v1">Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-08T07:14:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhenlong Liu, Lei Feng, Huiping Zhuang, Xiaofeng Cao, Hongxin Wei</p>
    <p><b>Summary:</b> Machine learning models are susceptible to membership inference attacks
(MIAs), which aim to infer whether a sample is in the training set. Existing
work utilizes gradient ascent to enlarge the loss variance of training data,
alleviating the privacy risk. However, optimizing toward a reverse direction
may cause the model parameters to oscillate near local minima, leading to
instability and suboptimal performance. In this work, we propose a novel method
-- Convex-Concave Loss, which enables a high variance of training loss
distribution by gradient descent. Our method is motivated by the theoretical
analysis that convex losses tend to decrease the loss variance during training.
Thus, our key idea behind CCL is to reduce the convexity of loss functions with
a concave term. Trained with CCL, neural networks produce losses with high
variance for training data, reinforcing the defense against MIAs. Extensive
experiments demonstrate the superiority of CCL, achieving state-of-the-art
balance in the privacy-utility trade-off.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.06674v1">Understanding Practical Membership Privacy of Deep Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-07T14:23:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Marlon Tobaben, Gauri Pradhan, Yuan He, Joonas Jälkö, Antti Honkela</p>
    <p><b>Summary:</b> We apply a state-of-the-art membership inference attack (MIA) to
systematically test the practical privacy vulnerability of fine-tuning large
image classification models.We focus on understanding the properties of data
sets and samples that make them vulnerable to membership inference. In terms of
data set properties, we find a strong power law dependence between the number
of examples per class in the data and the MIA vulnerability, as measured by
true positive rate of the attack at a low false positive rate. For an
individual sample, large gradients at the end of training are strongly
correlated with MIA vulnerability.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.04840v1">Efficient Estimation of a Gaussian Mean with Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2024-02-07T13:41:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nikita Kalinin, Lukas Steinberger</p>
    <p><b>Summary:</b> In this paper we study the problem of estimating the unknown mean $\theta$ of
a unit variance Gaussian distribution in a locally differentially private (LDP)
way. In the high-privacy regime ($\epsilon\le 0.67$), we identify the exact
optimal privacy mechanism that minimizes the variance of the estimator
asymptotically. It turns out to be the extraordinarily simple sign mechanism
that applies randomized response to the sign of $X_i-\theta$. However, since
this optimal mechanism depends on the unknown mean $\theta$, we employ a
two-stage LDP parameter estimation procedure which requires splitting agents
into two groups. The first $n_1$ observations are used to consistently but not
necessarily efficiently estimate the parameter $\theta$ by
$\tilde{\theta}_{n_1}$. Then this estimate is updated by applying the sign
mechanism with $\tilde{\theta}_{n_1}$ instead of $\theta$ to the remaining
$n-n_1$ observations, to obtain an LDP and efficient estimator of the unknown
mean.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.04489v1">De-amplifying Bias from Differential Privacy in Language Model
  Fine-tuning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> 
  <p><b>Published on:</b> 2024-02-07T00:30:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sanjari Srivastava, Piotr Mardziel, Zhikhun Zhang, Archana Ahlawat, Anupam Datta, John C Mitchell</p>
    <p><b>Summary:</b> Fairness and privacy are two important values machine learning (ML)
practitioners often seek to operationalize in models. Fairness aims to reduce
model bias for social/demographic sub-groups. Privacy via differential privacy
(DP) mechanisms, on the other hand, limits the impact of any individual's
training data on the resulting model. The trade-offs between privacy and
fairness goals of trustworthy ML pose a challenge to those wishing to address
both. We show that DP amplifies gender, racial, and religious bias when
fine-tuning large language models (LLMs), producing models more biased than
ones fine-tuned without DP. We find the cause of the amplification to be a
disparity in convergence of gradients across sub-groups. Through the case of
binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA),
a known method for addressing bias, also mitigates bias amplification by DP. As
a consequence, DP and CDA together can be used to fine-tune models while
maintaining both fairness and privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.04033v1">On provable privacy vulnerabilities of graph representations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-06T14:26:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ruofan Wu, Guanhua Fang, Qiying Pan, Mingyang Zhang, Tengfei Liu, Weiqiang Wang, Wenbiao Zhao</p>
    <p><b>Summary:</b> Graph representation learning (GRL) is critical for extracting insights from
complex network structures, but it also raises security concerns due to
potential privacy vulnerabilities in these representations. This paper
investigates the structural vulnerabilities in graph neural models where
sensitive topological information can be inferred through edge reconstruction
attacks. Our research primarily addresses the theoretical underpinnings of
cosine-similarity-based edge reconstruction attacks (COSERA), providing
theoretical and empirical evidence that such attacks can perfectly reconstruct
sparse Erdos Renyi graphs with independent random features as graph size
increases. Conversely, we establish that sparsity is a critical factor for
COSERA's effectiveness, as demonstrated through analysis and experiments on
stochastic block models. Finally, we explore the resilience of (provably)
private graph representations produced via noisy aggregation (NAG) mechanism
against COSERA. We empirically delineate instances wherein COSERA demonstrates
both efficacy and deficiency in its capacity to function as an instrument for
elucidating the trade-off between privacy and utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.04013v1">Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and
  Defenses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-02-06T14:06:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Fang, Yixiang Qiu, Hongyao Yu, Wenbo Yu, Jiawei Kong, Baoli Chong, Bin Chen, Xuan Wang, Shu-Tao Xia</p>
    <p><b>Summary:</b> Model Inversion (MI) attacks aim to disclose private information about the
training data by abusing access to the pre-trained models. These attacks enable
adversaries to reconstruct high-fidelity data that closely aligns with the
private training data, which has raised significant privacy concerns. Despite
the rapid advances in the field, we lack a comprehensive overview of existing
MI attacks and defenses. To fill this gap, this paper thoroughly investigates
this field and presents a holistic survey. Firstly, our work briefly reviews
the traditional MI on machine learning scenarios. We then elaborately analyze
and compare numerous recent attacks and defenses on \textbf{D}eep
\textbf{N}eural \textbf{N}etworks (DNNs) across multiple modalities and
learning tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03907v1">Embedding Large Language Models into Extended Reality: Opportunities and
  Challenges for Inclusion, Engagement, and Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-02-06T11:19:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Efe Bozkir, Süleyman Özdel, Ka Hei Carrie Lau, Mengdi Wang, Hong Gao, Enkelejda Kasneci</p>
    <p><b>Summary:</b> Recent developments in computer graphics, hardware, artificial intelligence
(AI), and human-computer interaction likely lead to extended reality (XR)
devices and setups being more pervasive. While these devices and setups provide
users with interactive, engaging, and immersive experiences with different
sensing modalities, such as eye and hand trackers, many non-player characters
are utilized in a pre-scripted way or by conventional AI techniques. In this
paper, we argue for using large language models (LLMs) in XR by embedding them
in virtual avatars or as narratives to facilitate more inclusive experiences
through prompt engineering according to user profiles and fine-tuning the LLMs
for particular purposes. We argue that such inclusion will facilitate diversity
for XR use. In addition, we believe that with the versatile conversational
capabilities of LLMs, users will engage more with XR environments, which might
help XR be more used in everyday life. Lastly, we speculate that combining the
information provided to LLM-powered environments by the users and the biometric
data obtained through the sensors might lead to novel privacy invasions. While
studying such possible privacy invasions, user privacy concerns and preferences
should also be investigated. In summary, despite some challenges, embedding
LLMs into XR is a promising and novel research area with several opportunities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03702v1">On Learning Spatial Provenance in Privacy-Constrained Wireless Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> 
  <p><b>Published on:</b> 2024-02-06T04:44:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Manish Bansal, Pramsu Srivastava, J. Harshan</p>
    <p><b>Summary:</b> In Vehicle-to-Everything networks that involve multi-hop communication, the
Road Side Units (RSUs) typically aim to collect location information from the
participating vehicles to provide security and network diagnostics features.
While the vehicles commonly use the Global Positioning System (GPS) for
navigation, they may refrain from sharing their precise GPS coordinates with
the RSUs due to privacy concerns. Therefore, to jointly address the high
localization requirements by the RSUs as well as the vehicles' privacy, we
present a novel spatial-provenance framework wherein each vehicle uses Bloom
filters to embed their partial location information when forwarding the
packets. In this framework, the RSUs and the vehicles agree upon fragmenting
the coverage area into several smaller regions so that the vehicles can embed
the identity of their regions through Bloom filters. Given the probabilistic
nature of Bloom filters, we derive an analytical expression on the error-rates
in provenance recovery and then pose an optimization problem to choose the
underlying parameters. With the help of extensive simulation results, we show
that our method offers near-optimal Bloom filter parameters in learning spatial
provenance. Some interesting trade-offs between the communication-overhead,
spatial privacy of the vehicles and the error rates in provenance recovery are
also discussed.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03688v1">A Survey of Privacy Threats and Defense in Vertical Federated Learning:
  From Model Life Cycle Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-06T04:22:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lei Yu, Meng Han, Yiming Li, Changting Lin, Yao Zhang, Mingyang Zhang, Yan Liu, Haiqin Weng, Yuseok Jeon, Ka-Ho Chow, Stacy Patterson</p>
    <p><b>Summary:</b> Vertical Federated Learning (VFL) is a federated learning paradigm where
multiple participants, who share the same set of samples but hold different
features, jointly train machine learning models. Although VFL enables
collaborative machine learning without sharing raw data, it is still
susceptible to various privacy threats. In this paper, we conduct the first
comprehensive survey of the state-of-the-art in privacy attacks and defenses in
VFL. We provide taxonomies for both attacks and defenses, based on their
characterizations, and discuss open challenges and future research directions.
Specifically, our discussion is structured around the model's life cycle, by
delving into the privacy threats encountered during different stages of machine
learning and their corresponding countermeasures. This survey not only serves
as a resource for the research community but also offers clear guidance and
actionable insights for practitioners to safeguard data privacy throughout the
model's life cycle.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03612v1">Privacy risk in GeoData: A survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-06T00:55:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mahrokh Abdollahi Lorestani, Thilina Ranbaduge, Thierry Rakotoarivelo</p>
    <p><b>Summary:</b> With the ubiquitous use of location-based services, large-scale
individual-level location data has been widely collected through
location-awareness devices. The exposure of location data constitutes a
significant privacy risk to users as it can lead to de-anonymisation, the
inference of sensitive information, and even physical threats. Geoprivacy
concerns arise on the issues of user identity de-anonymisation and location
exposure. In this survey, we analyse different geomasking techniques that have
been proposed to protect the privacy of individuals in geodata. We present a
taxonomy to characterise these techniques along different dimensions, and
conduct a survey of geomasking techniques. We then highlight shortcomings of
current techniques and discuss avenues for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03582v1">Matcha: An IDE Plugin for Creating Accurate Privacy Nutrition Labels</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-05T23:17:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianshi Li, Lorrie Faith Cranor, Yuvraj Agarwal, Jason I. Hong</p>
    <p><b>Summary:</b> Apple and Google introduced their versions of privacy nutrition labels to the
mobile app stores to better inform users of the apps' data practices. However,
these labels are self-reported by developers and have been found to contain
many inaccuracies due to misunderstandings of the label taxonomy. In this work,
we present Matcha, an IDE plugin that uses automated code analysis to help
developers create accurate Google Play data safety labels. Developers can
benefit from Matcha's ability to detect user data accesses and transmissions
while staying in control of the generated label by adding custom Java
annotations and modifying an auto-generated XML specification. Our evaluation
with 12 developers showed that Matcha helped our participants improved the
accuracy of a label they created with Google's official tool for a real-world
app they developed. We found that participants preferred Matcha for its
accuracy benefits. Drawing on Matcha, we discuss general design recommendations
for developer tools used to create accurate standardized privacy notices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03531v1">Fairness and Privacy Guarantees in Federated Contextual Bandits</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-05T21:38:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sambhav Solanki, Shweta Jain, Sujit Gujar</p>
    <p><b>Summary:</b> This paper considers the contextual multi-armed bandit (CMAB) problem with
fairness and privacy guarantees in a federated environment. We consider
merit-based exposure as the desired fair outcome, which provides exposure to
each action in proportion to the reward associated. We model the algorithm's
effectiveness using fairness regret, which captures the difference between fair
optimal policy and the policy output by the algorithm. Applying fair CMAB
algorithm to each agent individually leads to fairness regret linear in the
number of agents. We propose that collaborative -- federated learning can be
more effective and provide the algorithm Fed-FairX-LinUCB that also ensures
differential privacy. The primary challenge in extending the existing privacy
framework is designing the communication protocol for communicating required
information across agents. A naive protocol can either lead to weaker privacy
guarantees or higher regret. We design a novel communication protocol that
allows for (i) Sub-linear theoretical bounds on fairness regret for
Fed-FairX-LinUCB and comparable bounds for the private counterpart,
Priv-FairX-LinUCB (relative to single-agent learning), (ii) Effective use of
privacy budget in Priv-FairX-LinUCB. We demonstrate the efficacy of our
proposed algorithm with extensive simulations-based experiments. We show that
both Fed-FairX-LinUCB and Priv-FairX-LinUCB achieve near-optimal fairness
regret.</p>
  </details>
</div>



<h2>2024-03</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.04485v1">Privacy in Cloud Computing through Immersion-based Coding</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-03-07T13:38:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haleh Hayati, Nathan van de Wouw, Carlos Murguia</p>
    <p><b>Summary:</b> Cloud computing enables users to process and store data remotely on
high-performance computers and servers by sharing data over the Internet.
However, transferring data to clouds causes unavoidable privacy concerns. Here,
we present a synthesis framework to design coding mechanisms that allow sharing
and processing data in a privacy-preserving manner without sacrificing data
utility and algorithmic performance. We consider the setup where the user aims
to run an algorithm in the cloud using private data. The cloud then returns
some data utility back to the user (utility refers to the service that the
algorithm provides, e.g., classification, prediction, AI models, etc.). To
avoid privacy concerns, the proposed scheme provides tools to co-design: 1)
coding mechanisms to distort the original data and guarantee a prescribed
differential privacy level; 2) an equivalent-but-different algorithm (referred
here to as the target algorithm) that runs on distorted data and produces
distorted utility; and 3) a decoding function that extracts the true utility
from the distorted one with a negligible error. Then, instead of sharing the
original data and algorithm with the cloud, only the distorted data and target
algorithm are disclosed, thereby avoiding privacy concerns. The proposed scheme
is built on the synergy of differential privacy and system immersion tools from
control theory. The key underlying idea is to design a higher-dimensional
target algorithm that embeds all trajectories of the original algorithm and
works on randomly encoded data to produce randomly encoded utility. We show
that the proposed scheme can be designed to offer any level of differential
privacy without degrading the algorithm's utility. We present two use cases to
illustrate the performance of the developed tools: privacy in
optimization/learning algorithms and a nonlinear networked control system.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.04468v1">A Survey of Graph Neural Networks in Real world: Imbalance, Noise,
  Privacy and OOD Challenges</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2024-03-07T13:10:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wei Ju, Siyu Yi, Yifan Wang, Zhiping Xiao, Zhengyang Mao, Hourun Li, Yiyang Gu, Yifang Qin, Nan Yin, Senzhang Wang, Xinwang Liu, Xiao Luo, Philip S. Yu, Ming Zhang</p>
    <p><b>Summary:</b> Graph-structured data exhibits universality and widespread applicability
across diverse domains, such as social network analysis, biochemistry,
financial fraud detection, and network security. Significant strides have been
made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success
in these areas. However, in real-world scenarios, the training environment for
models is often far from ideal, leading to substantial performance degradation
of GNN models due to various unfavorable factors, including imbalance in data
distribution, the presence of noise in erroneous data, privacy protection of
sensitive information, and generalization capability for out-of-distribution
(OOD) scenarios. To tackle these issues, substantial efforts have been devoted
to improving the performance of GNN models in practical real-world scenarios,
as well as enhancing their reliability and robustness. In this paper, we
present a comprehensive survey that systematically reviews existing GNN models,
focusing on solutions to the four mentioned real-world challenges including
imbalance, noise, privacy, and OOD in practical scenarios that many existing
reviews have not considered. Specifically, we first highlight the four key
challenges faced by existing GNNs, paving the way for our exploration of
real-world GNN models. Subsequently, we provide detailed discussions on these
four aspects, dissecting how these solutions contribute to enhancing the
reliability and robustness of GNN models. Last but not least, we outline
promising directions and offer future perspectives in the field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.04451v1">Membership Inference Attacks and Privacy in Topic Modeling</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-03-07T12:43:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nico Manzonelli, Wanrong Zhang, Salil Vadhan</p>
    <p><b>Summary:</b> Recent research shows that large language models are susceptible to privacy
attacks that infer aspects of the training data. However, it is unclear if
simpler generative models, like topic models, share similar vulnerabilities. In
this work, we propose an attack against topic models that can confidently
identify members of the training data in Latent Dirichlet Allocation. Our
results suggest that the privacy risks associated with generative modeling are
not restricted to large neural models. Additionally, to mitigate these
vulnerabilities, we explore differentially private (DP) topic modeling. We
propose a framework for private topic modeling that incorporates DP vocabulary
selection as a pre-processing step, and show that it improves privacy while
having limited effects on practical utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.04124v1">Privacy-preserving Fine-tuning of Large Language Models through Flatness</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> 
  <p><b>Published on:</b> 2024-03-07T00:44:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tiejin Chen, Longchao Da, Huixue Zhou, Pingzhi Li, Kaixiong Zhou, Tianlong Chen, Hua Wei</p>
    <p><b>Summary:</b> The privacy concerns associated with the use of Large Language Models (LLMs)
have grown recently with the development of LLMs such as ChatGPT. Differential
Privacy (DP) techniques are explored in existing work to mitigate their privacy
risks at the cost of generalization degradation. Our paper reveals that the
flatness of DP-trained models' loss landscape plays an essential role in the
trade-off between their privacy and generalization. We further propose a
holistic framework to enforce appropriate weight flatness, which substantially
improves model generalization with competitive privacy preservation. It
innovates from three coarse-to-grained levels, including perturbation-aware
min-max optimization on model weights within a layer, flatness-guided sparse
prefix-tuning on weights across layers, and weight knowledge distillation
between DP \& non-DP weights copies. Comprehensive experiments of both
black-box and white-box scenarios are conducted to demonstrate the
effectiveness of our proposal in enhancing generalization and maintaining DP
characteristics. For instance, on text classification dataset QNLI, DP-Flat
achieves similar performance with non-private full fine-tuning but with DP
guarantee under privacy budget $\epsilon=3$, and even better performance given
higher privacy budgets. Codes are provided in the supplement.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.04024v1">Enhancing chest X-ray datasets with privacy-preserving large language
  models and multi-type annotations: a data-driven approach for improved
  classification</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-03-06T20:10:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ricardo Bigolin Lanfredi, Pritam Mukherjee, Ronald Summers</p>
    <p><b>Summary:</b> In chest X-ray (CXR) image analysis, rule-based systems are usually employed
to extract labels from reports, but concerns exist about label quality. These
datasets typically offer only presence labels, sometimes with binary
uncertainty indicators, which limits their usefulness. In this work, we present
MAPLEZ (Medical report Annotations with Privacy-preserving Large language model
using Expeditious Zero shot answers), a novel approach leveraging a locally
executable Large Language Model (LLM) to extract and enhance findings labels on
CXR reports. MAPLEZ extracts not only binary labels indicating the presence or
absence of a finding but also the location, severity, and radiologists'
uncertainty about the finding. Over eight abnormalities from five test sets, we
show that our method can extract these annotations with an increase of 5
percentage points (pp) in F1 score for categorical presence annotations and
more than 30 pp increase in F1 score for the location annotations over
competing labelers. Additionally, using these improved annotations in
classification supervision, we demonstrate substantial advancements in model
quality, with an increase of 1.7 pp in AUROC over models trained with
annotations from the state-of-the-art approach. We share code and annotations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.03612v1">Using the Dual-Privacy Framework to Understand Consumers' Perceived
  Privacy Violations Under Different Firm Practices in Online Advertising</a></h3>
   
  <p><b>Published on:</b> 2024-03-06T11:06:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kinshuk Jerath, Klaus M. Miller</p>
    <p><b>Summary:</b> In response to privacy concerns about collecting and using personal data, the
online advertising industry has been developing privacy-enhancing technologies
(PETs), e.g., under Google's Privacy Sandbox initiative. In this research, we
use the dual-privacy framework, which postulates that consumers have intrinsic
and instrumental preferences for privacy, to understand consumers' perceived
privacy violations (PPVs) for current and proposed online advertising
practices. The key idea is that different practices differ in whether
individual data leaves the consumer's machine or not and in how they track and
target consumers; these affect, respectively, the intrinsic and instrumental
components of privacy preferences differently, leading to different PPVs for
different practices. We conducted online studies focused on consumers in the
United States to elicit PPVs for various advertising practices. Our findings
confirm the intuition that tracking and targeting consumers under the industry
status quo of behavioral targeting leads to high PPV. New technologies or
proposals that ensure that data are kept on the consumer's machine lower PPV
relative to behavioral targeting but, importantly, this decrease is small.
Furthermore, group-level targeting does not differ significantly from
individual-level targeting in reducing PPV. Under contextual targeting, where
there is no tracking, PPV is significantly reduced. Interestingly, with respect
to PPV, consumers are indifferent between seeing untargeted ads and no ads when
they are not being tracked. We find that consumer perceptions of privacy
violations under different tracking and targeting practices may differ from
what technical definitions suggest. Therefore, rather than relying solely on
technical perspectives, a consumer-centric approach to privacy is needed, based
on, for instance, the dual-privacy framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.03610v1">Paying for Privacy: Pay-or-Tracking Walls</a></h3>
   
  <p><b>Published on:</b> 2024-03-06T10:59:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Timo Mueller-Tribbensee, Klaus M. Miller, Bernd Skiera</p>
    <p><b>Summary:</b> Prestigious news publishers, and more recently, Meta, have begun to request
that users pay for privacy. Specifically, users receive a notification banner,
referred to as a pay-or-tracking wall, that requires them to (i) pay money to
avoid being tracked or (ii) consent to being tracked. These walls have invited
concerns that privacy might become a luxury. However, little is known about
pay-or-tracking walls, which prevents a meaningful discussion about their
appropriateness. This paper conducts several empirical studies and finds that
top EU publishers use pay-or-tracking walls. Their implementations involve
various approaches, including bundling the pay option with advertising-free
access or additional content. The price for not being tracked exceeds the
advertising revenue that publishers generate from a user who consents to being
tracked. Notably, publishers' traffic does not decline when implementing a
pay-or-tracking wall and most users consent to being tracked; only a few users
pay. In short, pay-or-tracking walls seem to provide the means for expanding
the practice of tracking. Publishers profit from pay-or-tracking walls and may
observe a revenue increase of 16.4% due to tracking more users than under a
cookie consent banner.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.03600v1">A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain
  Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-03-06T10:40:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Li Wang, Lei Sang, Quangui Zhang, Qiang Wu, Min Xu</p>
    <p><b>Summary:</b> Cross-domain recommendation (CDR) aims to enhance recommendation accuracy in
a target domain with sparse data by leveraging rich information in a source
domain, thereby addressing the data-sparsity problem. Some existing CDR methods
highlight the advantages of extracting domain-common and domain-specific
features to learn comprehensive user and item representations. However, these
methods can't effectively disentangle these components as they often rely on
simple user-item historical interaction information (such as ratings, clicks,
and browsing), neglecting the rich multi-modal features. Additionally, they
don't protect user-sensitive data from potential leakage during knowledge
transfer between domains. To address these challenges, we propose a
Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain
Recommendation, called P2M2-CDR. Specifically, we first design a multi-modal
disentangled encoder that utilizes multi-modal information to disentangle more
informative domain-common and domain-specific embeddings. Furthermore, we
introduce a privacy-preserving decoder to mitigate user privacy leakage during
knowledge transfer. Local differential privacy (LDP) is utilized to obfuscate
the disentangled embeddings before inter-domain exchange, thereby enhancing
privacy protection. To ensure both consistency and differentiation among these
obfuscated disentangled embeddings, we incorporate contrastive learning-based
domain-inter and domain-intra losses. Extensive Experiments conducted on four
real-world datasets demonstrate that P2M2-CDR outperforms other
state-of-the-art single-domain and cross-domain baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.03592v1">Wildest Dreams: Reproducible Research in Privacy-preserving Neural
  Network Training</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-03-06T10:25:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tanveer Khan, Mindaugas Budzys, Khoa Nguyen, Antonis Michalas</p>
    <p><b>Summary:</b> Machine Learning (ML), addresses a multitude of complex issues in multiple
disciplines, including social sciences, finance, and medical research. ML
models require substantial computing power and are only as powerful as the data
utilized. Due to high computational cost of ML methods, data scientists
frequently use Machine Learning-as-a-Service (MLaaS) to outsource computation
to external servers. However, when working with private information, like
financial data or health records, outsourcing the computation might result in
privacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have
enabled ML training and inference over protected data through the use of
Privacy-Preserving Machine Learning (PPML). However, these techniques are still
at a preliminary stage and their application in real-world situations is
demanding. In order to comprehend discrepancy between theoretical research
suggestions and actual applications, this work examines the past and present of
PPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party
Computation (SMPC) applied to ML. This work primarily focuses on the ML model's
training phase, where maintaining user data privacy is of utmost importance. We
provide a solid theoretical background that eases the understanding of current
approaches and their limitations. In addition, we present a SoK of the most
recent PPML frameworks for model training and provide a comprehensive
comparison in terms of the unique properties and performances on standard
benchmarks. Also, we reproduce the results for some of the papers and examine
at what level existing works in the field provide support for open science. We
believe our work serves as a valuable contribution by raising awareness about
the current gap between theoretical advancements and real-world applications in
PPML, specifically regarding open-source availability, reproducibility, and
usability.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.03337v1">Fine-Grained Privacy Guarantees for Coverage Problems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-03-05T21:40:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Laxman Dhulipala, George Z. Li</p>
    <p><b>Summary:</b> We introduce a new notion of neighboring databases for coverage problems such
as Max Cover and Set Cover under differential privacy. In contrast to the
standard privacy notion for these problems, which is analogous to node-privacy
in graphs, our new definition gives a more fine-grained privacy guarantee,
which is analogous to edge-privacy. We illustrate several scenarios of Set
Cover and Max Cover where our privacy notion is desired one for the
application.
  Our main result is an $\epsilon$-edge differentially private algorithm for
Max Cover which obtains an $(1-1/e-\eta,\tilde{O}(k/\epsilon))$-approximation
with high probability. Furthermore, we show that this result is nearly tight:
we give a lower bound show that an additive error of $\Omega(k/\epsilon)$ is
necessary under edge-differential privacy. Via group privacy properties, this
implies a new algorithm for $\epsilon$-node differentially private Max Cover
which obtains an $(1-1/e-\eta,\tilde{O}(fk/\epsilon))$-approximation, where $f$
is the maximum degree of an element in the set system. When $f\ll k$, this
improves over the best known algorithm for Max Cover under pure (node)
differential privacy, which obtains an
$(1-1/e,\tilde{O}(k^2/\epsilon))$-approximation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.03126v1">A Federated Deep Learning Approach for Privacy-Preserving Real-Time
  Transient Stability Predictions in Power Systems</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-03-05T17:12:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Maeshal Hijazi, Payman Dehghanian</p>
    <p><b>Summary:</b> Maintaining the privacy of power system data is essential for protecting
sensitive information and ensuring the operation security of critical
infrastructure. Therefore, the adoption of centralized deep learning (DL)
transient stability assessment (TSA) frameworks can introduce risks to electric
utilities. This is because these frameworks make utility data susceptible to
cyber threats and communication issues when transmitting data to a central
server for training a single TSA model. Additionally, the centralized approach
demands significant computational resources, which may not always be readily
available. In light of these challenges, this paper introduces a federated
DL-based TSA framework designed to identify the operating states of the power
system. Instead of local utilities transmitting their data to a central server
for centralized model training, they independently train their own TSA models
using their respective datasets. Subsequently, the parameters of each local TSA
model are sent to a central server for model aggregation, and the resulting
model is shared back with the local clients. This approach not only preserves
the integrity of local utility data, making it resilient against cyber threats
but also reduces the computational demands for local TSA model training. The
proposed approach is tested on four local clients each having the IEEE 39-bus
test system.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.03048v1">Design of Stochastic Quantizers for Privacy Preservation</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-03-05T15:31:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Le Liu, Yu Kawano, Ming Cao</p>
    <p><b>Summary:</b> In this paper, we examine the role of stochastic quantizers for privacy
preservation. We first employ a static stochastic quantizer and investigate its
corresponding privacy-preserving properties. Specifically, we demonstrate that
a sufficiently large quantization step guarantees $(0, \delta)$ differential
privacy. Additionally, the degradation of control performance caused by
quantization is evaluated as the tracking error of output regulation. These two
analyses characterize the trade-off between privacy and control performance,
determined by the quantization step. This insight enables us to use
quantization intentionally as a means to achieve the seemingly conflicting two
goals of maintaining control performance and preserving privacy at the same
time; towards this end, we further investigate a dynamic stochastic quantizer.
Under a stability assumption, the dynamic stochastic quantizer can enhance
privacy, more than the static one, while achieving the same control
performance. We further handle the unstable case by additionally applying input
Gaussian noise.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.02694v1">Privacy-Aware Semantic Cache for Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> 
  <p><b>Published on:</b> 2024-03-05T06:23:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Waris Gill, Mohamed Elidrisi, Pallavi Kalapatapu, Ali Anwar, Muhammad Ali Gulzar</p>
    <p><b>Summary:</b> Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2
have revolutionized natural language processing and search engine dynamics.
However, these models incur exceptionally high computational costs. For
instance, GPT-3 consists of 175 billion parameters and inference on these
models also demands billions of floating-point operations. Caching is a natural
solution to reduce LLM inference costs on repeated queries. However, existing
caching methods are incapable of finding semantic similarities among LLM
queries, leading to unacceptable false hit-and-miss rates.
  This paper introduces MeanCache, a semantic cache for LLMs that identifies
semantically similar queries to determine cache hit or miss. Using MeanCache,
the response to a user's semantically similar query can be retrieved from a
local cache rather than re-querying the LLM, thus reducing costs, service
provider load, and environmental impact. MeanCache leverages Federated Learning
(FL) to collaboratively train a query similarity model in a distributed manner
across numerous users without violating privacy. By placing a local cache in
each user's device and using FL, MeanCache reduces the latency and costs and
enhances model performance, resulting in lower cache false hit rates. Our
experiments, benchmarked against the GPTCache, reveal that MeanCache attains an
approximately 17% higher F-score and a 20% increase in precision during
semantic cache hit-and-miss decisions. Furthermore, MeanCache reduces the
storage requirement by 83% and accelerates semantic cache hit-and-miss
decisions by 11%, while still surpassing GPTCache.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.02631v1">Privacy in Multi-agent Systems</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E">
  <p><b>Published on:</b> 2024-03-05T03:40:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yongqiang Wang</p>
    <p><b>Summary:</b> With the increasing awareness of privacy and the deployment of legislations
in various multi-agent system application domains such as power systems and
intelligent transportation, the privacy protection problem for multi-agent
systems is gaining increased traction in recent years. This article discusses
some of the representative advancements in the filed.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.02409v1">Privacy-Respecting Type Error Telemetry at Scale</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Programming Languages-D91E36">
  <p><b>Published on:</b> 2024-03-04T19:07:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ben Greenman, Alan Jeffrey, Shriram Krishnamurthi, Mitesh Shah</p>
    <p><b>Summary:</b> Context: Roblox Studio lets millions of creators build interactive
experiences by programming in a variant of Lua called Luau. The creators form a
broad group, ranging from novices writing their first script to professional
developers; thus, Luau must support a wide audience. As part of its efforts to
support all kinds of programmers, Luau includes an optional, gradual type
system and goes to great lengths to minimize false positive errors.
  Inquiry: Since Luau is currently being used by many creators, we want to
collect data to improve the language and, in particular, the type system. The
standard way to collect data is to deploy client-side telemetry; however, we
cannot scrape personal data or proprietary information, which means we cannot
collect source code fragments, error messages, or even filepaths. The research
questions are thus about how to conduct telemetry that is not invasive and
obtain insights from it about type errors.
  Approach: We designed and implemented a pseudonymized, randomly-sampling
telemetry system for Luau. Telemetry records include a timestamp, a session id,
a reason for sending, and a numeric summary of the most recent type analyses.
This information lets us study type errors over time without revealing private
data. We deployed the system in Roblox Studio during Spring 2023 and collected
over 1.5 million telemetry records from over 340,000 sessions.
  Knowledge: We present several findings about Luau, all of which suggest that
telemetry is an effective way to study type error pragmatics. One of the
less-surprising findings is that opt-in gradual types are unpopular: there is
an 100x gap between the number of untyped Luau sessions and the number of typed
ones. One surprise is that the strict mode for type analysis is overly
conservative about interactions with data assets. A reassuring finding is that
type analysis rarely hits its internal limits on problem size.
  Grounding: Our findings are supported by a dataset of over 1.5 million
telemetry records. The data and scripts for analyzing it are available in an
artifact.
  Importance: Beyond the immediate benefits to Luau, our findings about types
and type errors have implications for adoption and ergonomics in other gradual
languages such as TypeScript, Elixir, and Typed Racket. Our telemetry design is
of broad interest, as it reports on type errors without revealing sensitive
information.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.02324v1">Preserving Smart Grid Integrity: A Differential Privacy Framework for
  Secure Detection of False Data Injection Attacks in the Smart Grid</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-03-04T18:55:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nikhil Ravi, Anna Scaglione, Sean Peisert, Parth Pradhan</p>
    <p><b>Summary:</b> In this paper, we present a framework based on differential privacy (DP) for
querying electric power measurements to detect system anomalies or bad data
caused by false data injections (FDIs). Our DP approach conceals consumption
and system matrix data, while simultaneously enabling an untrusted third party
to test hypotheses of anomalies, such as an FDI attack, by releasing a
randomized sufficient statistic for hypothesis-testing. We consider a
measurement model corrupted by Gaussian noise and a sparse noise vector
representing the attack, and we observe that the optimal test statistic is a
chi-square random variable. To detect possible attacks, we propose a novel DP
chi-square noise mechanism that ensures the test does not reveal private
information about power injections or the system matrix. The proposed framework
provides a robust solution for detecting FDIs while preserving the privacy of
sensitive power system data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.02292v1">A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-03-04T18:21:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Omer Akgul, Sai Teja Peddinti, Nina Taft, Michelle L. Mazurek, Hamza Harkous, Animesh Srivastava, Benoit Seguin</p>
    <p><b>Summary:</b> We present an analysis of 12 million instances of privacy-relevant reviews
publicly visible on the Google Play Store that span a 10 year period. By
leveraging state of the art NLP techniques, we can examine what users have been
writing about privacy along multiple dimensions: time, countries, app types,
diverse privacy topics, and even across a spectrum of emotions. We find
consistent growth of privacy-relevant reviews, and explore topics that are
trending (such as Data Deletion and Data Theft), as well as those on the
decline (such as privacy-relevant reviews on sensitive permissions). We find
that although privacy reviews come from more than 200 countries, 33 countries
provide 90% of privacy reviews. We conduct a comparison across countries by
examining the distribution of privacy topics a country's users write about, and
find that geographic proximity is not a reliable indicator that nearby
countries have similar privacy perspectives. We uncover some countries with
unique patterns and explore those herein. Surprisingly, we uncover that it is
not uncommon for reviews that discuss privacy to be positive (32%); many users
express pleasure about privacy features within apps or privacy-focused apps. We
also uncover some unexpected behaviors, such as the use of reviews to deliver
privacy disclaimers to developers. Finally, we demonstrate the value of
analyzing app reviews with our approach as a complement to existing methods for
understanding users' perspectives about privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.02116v1">Inf2Guard: An Information-Theoretic Framework for Learning
  Privacy-Preserving Representations against Inference Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-03-04T15:20:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sayedeh Leila Noorbakhsh, Binghui Zhang, Yuan Hong, Binghui Wang</p>
    <p><b>Summary:</b> Machine learning (ML) is vulnerable to inference (e.g., membership inference,
property inference, and data reconstruction) attacks that aim to infer the
private information of training data or dataset. Existing defenses are only
designed for one specific type of attack and sacrifice significant utility or
are soon broken by adaptive attacks. We address these limitations by proposing
an information-theoretic defense framework, called Inf2Guard, against the three
major types of inference attacks. Our framework, inspired by the success of
representation learning, posits that learning shared representations not only
saves time/costs but also benefits numerous downstream tasks. Generally,
Inf2Guard involves two mutual information objectives, for privacy protection
and utility preservation, respectively. Inf2Guard exhibits many merits: it
facilitates the design of customized objectives against the specific inference
attack; it provides a general defense framework which can treat certain
existing defenses as special cases; and importantly, it aids in deriving
theoretical results, e.g., inherent utility-privacy tradeoff and guaranteed
privacy leakage. Extensive evaluations validate the effectiveness of Inf2Guard
for learning privacy-preserving representations against inference attacks and
demonstrate the superiority over the baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.02051v1">Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2024-03-04T13:53:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Umut Şimşekli, Mert Gürbüzbalaban, Sinan Yıldırım, Lingjiong Zhu</p>
    <p><b>Summary:</b> Injecting heavy-tailed noise to the iterates of stochastic gradient descent
(SGD) has received increasing attention over the past few years. While various
theoretical properties of the resulting algorithm have been analyzed mainly
from learning theory and optimization perspectives, their privacy preservation
properties have not yet been established. Aiming to bridge this gap, we provide
differential privacy (DP) guarantees for noisy SGD, when the injected noise
follows an $\alpha$-stable distribution, which includes a spectrum of
heavy-tailed distributions (with infinite variance) as well as the Gaussian
distribution. Considering the $(\epsilon, \delta)$-DP framework, we show that
SGD with heavy-tailed perturbations achieves $(0, \tilde{\mathcal{O}}(1/n))$-DP
for a broad class of loss functions which can be non-convex, where $n$ is the
number of data points. As a remarkable byproduct, contrary to prior work that
necessitates bounded sensitivity for the gradients or clipping the iterates,
our theory reveals that under mild assumptions, such a projection step is not
actually necessary. We illustrate that the heavy-tailed noising mechanism
achieves similar DP guarantees compared to the Gaussian case, which suggests
that it can be a viable alternative to its light-tailed counterparts.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.01788v1">K-stars LDP: A Novel Framework for (p, q)-clique Enumeration under Local
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2024-03-04T07:30:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Henan Sun, Zhengyu Wu, Rong-Hua Li, Guoren Wang, Zening Li</p>
    <p><b>Summary:</b> (p,q)-clique enumeration on a bipartite graph is critical for calculating
clustering coefficient and detecting densest subgraph. It is necessary to carry
out subgraph enumeration while protecting users' privacy from any potential
attacker as the count of subgraph may contain sensitive information. Most
recent studies focus on the privacy protection algorithms based on edge LDP
(Local Differential Privacy). However, these algorithms suffer a large
estimation error due to the great amount of required noise. In this paper, we
propose a novel idea of k-stars LDP and a novel k-stars LDP algorithm for (p,
q)-clique enumeration with a small estimation error, where a k-stars is a
star-shaped graph with k nodes connecting to one node. The effectiveness of
edge LDP relies on its capacity to obfuscate the existence of an edge between
the user and his one-hop neighbors. This is based on the premise that a user
should be aware of the existence of his one-hop neighbors. Similarly, we can
apply this premise to k-stars as well, where an edge is a specific genre of
1-stars. Based on this fact, we first propose the k-stars neighboring list to
enable our algorithm to obfuscate the existence of k-stars with Warner' s RR.
Then, we propose the absolute value correction technique and the k-stars
sampling technique to further reduce the estimation error. Finally, with the
two-round user-collector interaction mechanism, we propose our k-stars LDP
algorithm to count the number of (p, q)-clique while successfully protecting
users' privacy. Both the theoretical analysis and experiments have showed the
superiority of our algorithm over the algorithms based on edge LDP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.01438v1">Privacy-Preserving Collaborative Split Learning Framework for Smart Grid
  Load Forecasting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-03-03T08:24:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Asif Iqbal, Prosanta Gope, Biplab Sikdar</p>
    <p><b>Summary:</b> Accurate load forecasting is crucial for energy management, infrastructure
planning, and demand-supply balancing. Smart meter data availability has led to
the demand for sensor-based load forecasting. Conventional ML allows training a
single global model using data from multiple smart meters requiring data
transfer to a central server, raising concerns for network requirements,
privacy, and security. We propose a split learning-based framework for load
forecasting to alleviate this issue. We split a deep neural network model into
two parts, one for each Grid Station (GS) responsible for an entire
neighbourhood's smart meters and the other for the Service Provider (SP).
Instead of sharing their data, client smart meters use their respective GSs'
model split for forward pass and only share their activations with the GS.
Under this framework, each GS is responsible for training a personalized model
split for their respective neighbourhoods, whereas the SP can train a single
global or personalized model for each GS. Experiments show that the proposed
models match or exceed a centrally trained model's performance and generalize
well. Privacy is analyzed by assessing information leakage between data and
shared activations of the GS model split. Additionally, differential privacy
enhances local data privacy while examining its impact on performance. A
transformer model is used as our base learner.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.01435v1">Distributed Least-Squares Optimization Solvers with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-03-03T08:14:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weijia Liu, Lei Wang, Fanghong Guo, Zhengguang Wu, Hongye Su</p>
    <p><b>Summary:</b> This paper studies the distributed least-squares optimization problem with
differential privacy requirement of local cost functions, for which two
differentially private distributed solvers are proposed. The first is
established on the distributed gradient tracking algorithm, by appropriately
perturbing the initial values and parameters that contain the privacy-sensitive
data with Gaussian and truncated Laplacian noises, respectively. Rigorous
proofs are established to show the achievable trade-off between the
({\epsilon}, {\delta})-differential privacy and the computation accuracy. The
second solver is established on the combination of the distributed shuffling
mechanism and the average consensus algorithm, which enables each agent to
obtain a noisy version of parameters characterizing the global gradient. As a
result, the least-squares optimization problem can be eventually solved by each
agent locally in such a way that any given ({\epsilon}, {\delta})-differential
privacy requirement can be preserved while the solution may be computed with
the accuracy independent of the network size, which makes the latter more
suitable for large-scale distributed least-squares problems. Numerical
simulations are presented to show the effectiveness of both solvers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.01356v1">Security and Privacy Enhancing in Blockchain-based IoT Environments via
  Anonym Auditing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-03-03T01:09:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peyman Khordadpour, Saeed Ahmadi</p>
    <p><b>Summary:</b> The integration of blockchain technology in Internet of Things (IoT)
environments is a revolutionary step towards ensuring robust security and
enhanced privacy. This paper delves into the unique challenges and solutions
associated with securing blockchain-based IoT systems, with a specific focus on
anonymous auditing to reinforce privacy and security. We propose a novel
framework that combines the decentralized nature of blockchain with advanced
security protocols tailored for IoT contexts. Central to our approach is the
implementation of anonymization techniques in auditing processes, ensuring user
privacy while maintaining the integrity and transparency of blockchain
transactions. We outline the architecture of blockchain in IoT environments,
emphasizing the workflow and specific security mechanisms employed.
Additionally, we introduce a security protocol that integrates
privacy-enhancing tools and anonymous auditing methods, including the use of
advanced cryptographic techniques for anonymity. This study also includes a
comparative analysis of our proposed framework against existing models in the
domain. Our work aims to provide a comprehensive blueprint for enhancing
security and privacy in blockchain-based IoT environments, paving the way for
more secure and private digital ecosystems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.01229v1">REWIND Dataset: Privacy-preserving Speaking Status Segmentation from
  Multimodal Body Movement Signals in the Wild</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-03-02T15:14:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jose Vargas Quiros, Chirag Raman, Stephanie Tan, Ekin Gedik, Laura Cabrera-Quiros, Hayley Hung</p>
    <p><b>Summary:</b> Recognizing speaking in humans is a central task towards understanding social
interactions. Ideally, speaking would be detected from individual voice
recordings, as done previously for meeting scenarios. However, individual voice
recordings are hard to obtain in the wild, especially in crowded mingling
scenarios due to cost, logistics, and privacy concerns. As an alternative,
machine learning models trained on video and wearable sensor data make it
possible to recognize speech by detecting its related gestures in an
unobtrusive, privacy-preserving way. These models themselves should ideally be
trained using labels obtained from the speech signal. However, existing
mingling datasets do not contain high quality audio recordings. Instead,
speaking status annotations have often been inferred by human annotators from
video, without validation of this approach against audio-based ground truth. In
this paper we revisit no-audio speaking status estimation by presenting the
first publicly available multimodal dataset with high-quality individual speech
recordings of 33 subjects in a professional networking event. We present three
baselines for no-audio speaking status segmentation: a) from video, b) from
body acceleration (chest-worn accelerometer), c) from body pose tracks. In all
cases we predict a 20Hz binary speaking status signal extracted from the audio,
a time resolution not available in previous datasets. In addition to providing
the signals and ground truth necessary to evaluate a wide range of speaking
status detection methods, the availability of audio in REWIND makes it suitable
for cross-modality studies not feasible with previous mingling datasets.
Finally, our flexible data consent setup creates new challenges for multimodal
systems under missing modalities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.01218v1">Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense
  of Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-03-02T14:22:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, Nicolas Papernot</p>
    <p><b>Summary:</b> The high cost of model training makes it increasingly desirable to develop
techniques for unlearning. These techniques seek to remove the influence of a
training example without having to retrain the model from scratch. Intuitively,
once a model has unlearned, an adversary that interacts with the model should
no longer be able to tell whether the unlearned example was included in the
model's training set or not. In the privacy literature, this is known as
membership inference. In this work, we discuss adaptations of Membership
Inference Attacks (MIAs) to the setting of unlearning (leading to their
``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into
``population U-MIAs'', where the same attacker is instantiated for all
examples, and ``per-example U-MIAs'', where a dedicated attacker is
instantiated for each example. We show that the latter category, wherein the
attacker tailors its membership prediction to each example under attack, is
significantly stronger. Indeed, our results show that the commonly used U-MIAs
in the unlearning literature overestimate the privacy protection afforded by
existing unlearning techniques on both vision and language models. Our
investigation reveals a large variance in the vulnerability of different
examples to per-example U-MIAs. In fact, several unlearning algorithms lead to
a reduced vulnerability for some, but not all, examples that we wish to
unlearn, at the expense of increasing it for other examples. Notably, we find
that the privacy protection for the remaining training examples may worsen as a
consequence of unlearning. We also discuss the fundamental difficulty of
equally protecting all examples using existing unlearning schemes, due to the
different rates at which examples are unlearned. We demonstrate that naive
attempts at tailoring unlearning stopping criteria to different examples fail
to alleviate these issues.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2403.00278v1">Shifted Interpolation for Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36">  
  <p><b>Published on:</b> 2024-03-01T04:50:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinho Bok, Weijie Su, Jason M. Altschuler</p>
    <p><b>Summary:</b> Noisy gradient descent and its variants are the predominant algorithms for
differentially private machine learning. It is a fundamental question to
quantify their privacy leakage, yet tight characterizations remain open even in
the foundational setting of convex losses. This paper improves over previous
analyses by establishing (and refining) the "privacy amplification by
iteration" phenomenon in the unifying framework of $f$-differential
privacy--which tightly captures all aspects of the privacy loss and immediately
implies tighter privacy accounting in other notions of differential privacy,
e.g., $(\varepsilon,\delta)$-DP and Renyi DP. Our key technical insight is the
construction of shifted interpolated processes that unravel the popular
shifted-divergences argument, enabling generalizations beyond divergence-based
relaxations of DP. Notably, this leads to the first exact privacy analysis in
the foundational setting of strongly convex optimization. Our techniques extend
to many settings: convex/strongly convex, constrained/unconstrained,
full/cyclic/stochastic batches, and all combinations thereof. As an immediate
corollary, we recover the $f$-DP characterization of the exponential mechanism
for strongly convex optimization in Gopi et al. (2022), and moreover extend
this result to more general settings.</p>
  </details>
</div>

