
<h2>2024-09</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07444v1">Echoes of Privacy: Uncovering the Profiling Practices of Voice
  Assistants</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-09-11T17:44:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tina Khezresmaeilzadeh, Elaine Zhu, Kiersten Grieco, Daniel J. Dubois, Konstantinos Psounis, David Choffnes</p>
    <p><b>Summary:</b> Many companies, including Google, Amazon, and Apple, offer voice assistants
as a convenient solution for answering general voice queries and accessing
their services. These voice assistants have gained popularity and can be easily
accessed through various smart devices such as smartphones, smart speakers,
smartwatches, and an increasing array of other devices. However, this
convenience comes with potential privacy risks. For instance, while companies
vaguely mention in their privacy policies that they may use voice interactions
for user profiling, it remains unclear to what extent this profiling occurs and
whether voice interactions pose greater privacy risks compared to other
interaction modalities.
  In this paper, we conduct 1171 experiments involving a total of 24530 queries
with different personas and interaction modalities over the course of 20 months
to characterize how the three most popular voice assistants profile their
users. We analyze factors such as the labels assigned to users, their accuracy,
the time taken to assign these labels, differences between voice and web
interactions, and the effectiveness of profiling remediation tools offered by
each voice assistant. Our findings reveal that profiling can happen without
interaction, can be incorrect and inconsistent at times, may take several days
to weeks for changes to occur, and can be influenced by the interaction
modality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07415v1">SoK: Security and Privacy Risks of Medical AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-11T16:59:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuanhaur Chang, Han Liu, Evin Jaff, Chenyang Lu, Ning Zhang</p>
    <p><b>Summary:</b> The integration of technology and healthcare has ushered in a new era where
software systems, powered by artificial intelligence and machine learning, have
become essential components of medical products and services. While these
advancements hold great promise for enhancing patient care and healthcare
delivery efficiency, they also expose sensitive medical data and system
integrity to potential cyberattacks. This paper explores the security and
privacy threats posed by AI/ML applications in healthcare. Through a thorough
examination of existing research across a range of medical domains, we have
identified significant gaps in understanding the adversarial attacks targeting
medical AI systems. By outlining specific adversarial threat models for medical
settings and identifying vulnerable application domains, we lay the groundwork
for future research that investigates the security and resilience of AI-driven
medical systems. Through our analysis of different threat models and
feasibility studies on adversarial attacks in different medical domains, we
provide compelling insights into the pressing need for cybersecurity research
in the rapidly evolving field of AI healthcare technology.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07224v1">Analytic Class Incremental Learning for Sound Source Localization with
  Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> 
  <p><b>Published on:</b> 2024-09-11T12:31:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinyuan Qian, Xianghu Yue, Jiadong Wang, Huiping Zhuang, Haizhou Li</p>
    <p><b>Summary:</b> Sound Source Localization (SSL) enabling technology for applications such as
surveillance and robotics. While traditional Signal Processing (SP)-based SSL
methods provide analytic solutions under specific signal and noise assumptions,
recent Deep Learning (DL)-based methods have significantly outperformed them.
However, their success depends on extensive training data and substantial
computational resources. Moreover, they often rely on large-scale annotated
spatial data and may struggle when adapting to evolving sound classes. To
mitigate these challenges, we propose a novel Class Incremental Learning (CIL)
approach, termed SSL-CIL, which avoids serious accuracy degradation due to
catastrophic forgetting by incrementally updating the DL-based SSL model
through a closed-form analytic solution. In particular, data privacy is ensured
since the learning process does not revisit any historical data
(exemplar-free), which is more suitable for smart home scenarios. Empirical
results in the public SSLR dataset demonstrate the superior performance of our
proposal, achieving a localization accuracy of 90.9%, surpassing other
competitive methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07187v1">A Simple Linear Space Data Structure for ANN with Application in
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2024-09-11T11:14:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Martin Aum√ºller, Fabrizio Boninsegna, Francesco Silvestri</p>
    <p><b>Summary:</b> Locality Sensitive Filters are known for offering a quasi-linear space data
structure with rigorous guarantees for the Approximate Near Neighbor search
problem. Building on Locality Sensitive Filters, we derive a simple data
structure for the Approximate Near Neighbor Counting problem under differential
privacy. Moreover, we provide a simple analysis leveraging a connection with
concomitant statistics and extreme value theory. Our approach achieves the same
performance as the recent findings of Andoni et al. (NeurIPS 2023) but with a
more straightforward method. As a side result, the paper provides a more
compact description and analysis of Locality Sensitive Filters for Approximate
Near Neighbor Search under inner product similarity, improving a previous
result in Aum\"{u}ller et al. (TODS 2022).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06955v1">Privacy-Preserving Federated Learning with Consistency via Knowledge
  Distillation Using Conditional Generator</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-09-11T02:36:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kangyang Luo, Shuai Wang, Xiang Li, Yunshi Lan, Ming Gao, Jinlong Shu</p>
    <p><b>Summary:</b> Federated Learning (FL) is gaining popularity as a distributed learning
framework that only shares model parameters or gradient updates and keeps
private data locally. However, FL is at risk of privacy leakage caused by
privacy inference attacks. And most existing privacy-preserving mechanisms in
FL conflict with achieving high performance and efficiency. Therefore, we
propose FedMD-CG, a novel FL method with highly competitive performance and
high-level privacy preservation, which decouples each client's local model into
a feature extractor and a classifier, and utilizes a conditional generator
instead of the feature extractor to perform server-side model aggregation. To
ensure the consistency of local generators and classifiers, FedMD-CG leverages
knowledge distillation to train local models and generators at both the latent
feature level and the logit level. Also, we construct additional classification
losses and design new diversity losses to enhance client-side training.
FedMD-CG is robust to data heterogeneity and does not require training extra
discriminators (like cGAN). We conduct extensive experiments on various image
classification tasks to validate the superiority of FedMD-CG.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06564v1">Advancing Android Privacy Assessments with Automation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-10T14:56:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mugdha Khedkar, Michael Schlichtig, Eric Bodden</p>
    <p><b>Summary:</b> Android apps collecting data from users must comply with legal frameworks to
ensure data protection. This requirement has become even more important since
the implementation of the General Data Protection Regulation (GDPR) by the
European Union in 2018. Moreover, with the proposed Cyber Resilience Act on the
horizon, stakeholders will soon need to assess software against even more
stringent security and privacy standards. Effective privacy assessments require
collaboration among groups with diverse expertise to function effectively as a
cohesive unit.
  This paper motivates the need for an automated approach that enhances
understanding of data protection in Android apps and improves communication
between the various parties involved in privacy assessments. We propose the
Assessor View, a tool designed to bridge the knowledge gap between these
parties, facilitating more effective privacy assessments of Android
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06455v1">Continual Domain Incremental Learning for Privacy-aware Digital
  Pathology</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-09-10T12:21:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pratibha Kumari, Daniel Reisenb√ºchler, Lucas Luttner, Nadine S. Schaadt, Friedrich Feuerhake, Dorit Merhof</p>
    <p><b>Summary:</b> In recent years, there has been remarkable progress in the field of digital
pathology, driven by the ability to model complex tissue patterns using
advanced deep-learning algorithms. However, the robustness of these models is
often severely compromised in the presence of data shifts (e.g., different
stains, organs, centers, etc.). Alternatively, continual learning (CL)
techniques aim to reduce the forgetting of past data when learning new data
with distributional shift conditions. Specifically, rehearsal-based CL
techniques, which store some past data in a buffer and then replay it with new
data, have proven effective in medical image analysis tasks. However, privacy
concerns arise as these approaches store past data, prompting the development
of our novel Generative Latent Replay-based CL (GLRCL) approach. GLRCL captures
the previous distribution through Gaussian Mixture Models instead of storing
past samples, which are then utilized to generate features and perform latent
replay with new data. We systematically evaluate our proposed framework under
different shift conditions in histopathology data, including stain and organ
shift. Our approach significantly outperforms popular buffer-free CL approaches
and performs similarly to rehearsal-based CL approaches that require large
buffers causing serious privacy violations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06422v1">A Pervasive, Efficient and Private Future: Realizing Privacy-Preserving
  Machine Learning Through Hybrid Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-10T11:04:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Khoa Nguyen, Mindaugas Budzys, Eugene Frimpong, Tanveer Khan, Antonis Michalas</p>
    <p><b>Summary:</b> Machine Learning (ML) has become one of the most impactful fields of data
science in recent years. However, a significant concern with ML is its privacy
risks due to rising attacks against ML models. Privacy-Preserving Machine
Learning (PPML) methods have been proposed to mitigate the privacy and security
risks of ML models. A popular approach to achieving PPML uses Homomorphic
Encryption (HE). However, the highly publicized inefficiencies of HE make it
unsuitable for highly scalable scenarios with resource-constrained devices.
Hence, Hybrid Homomorphic Encryption (HHE) -- a modern encryption scheme that
combines symmetric cryptography with HE -- has recently been introduced to
overcome these challenges. HHE potentially provides a foundation to build new
efficient and privacy-preserving services that transfer expensive HE operations
to the cloud. This work introduces HHE to the ML field by proposing
resource-friendly PPML protocols for edge devices. More precisely, we utilize
HHE as the primary building block of our PPML protocols. We assess the
performance of our protocols by first extensively evaluating each party's
communication and computational cost on a dummy dataset and show the efficiency
of our protocols by comparing them with similar protocols implemented using
plain BFV. Subsequently, we demonstrate the real-world applicability of our
construction by building an actual PPML application that uses HHE as its
foundation to classify heart disease based on sensitive ECG data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06360v1">SoK: Evaluating 5G Protocols Against Legacy and Emerging Privacy and
  Security Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-09-10T09:30:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Stavros Eleftherakis, Domenico Giustiniano, Nicolas Kourtellis</p>
    <p><b>Summary:</b> Ensuring user privacy remains a critical concern within mobile cellular
networks, particularly given the proliferation of interconnected devices and
services. In fact, a lot of user privacy issues have been raised in 2G, 3G,
4G/LTE networks. Recognizing this general concern, 3GPP has prioritized
addressing these issues in the development of 5G, implementing numerous
modifications to enhance user privacy since 5G Release 15. In this
systematization of knowledge paper, we first provide a framework for studying
privacy and security related attacks in cellular networks, setting as privacy
objective the User Identity Confidentiality defined in 3GPP standards. Using
this framework, we discuss existing privacy and security attacks in pre-5G
networks, analyzing the weaknesses that lead to these attacks. Furthermore, we
thoroughly study the security characteristics of 5G up to the new Release 19,
and examine mitigation mechanisms of 5G to the identified pre-5G attacks.
Afterwards, we analyze how recent 5G attacks try to overcome these mitigation
mechanisms. Finally, we identify current limitations and open problems in
security of 5G, and propose directions for future work.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06233v1">VBIT: Towards Enhancing Privacy Control Over IoT Devices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-10T06:00:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jad Al Aaraj, Olivia Figueira, Tu Le, Isabela Figueira, Rahmadi Trimananda, Athina Markopoulou</p>
    <p><b>Summary:</b> Internet-of-Things (IoT) devices are increasingly deployed at home, at work,
and in other shared and public spaces. IoT devices collect and share data with
service providers and third parties, which poses privacy concerns. Although
privacy enhancing tools are quite advanced in other applications domains (\eg~
advertising and tracker blockers for browsers), users have currently no
convenient way to know or manage what and how data is collected and shared by
IoT devices. In this paper, we present VBIT, an interactive system combining
Mixed Reality (MR) and web-based applications that allows users to: (1) uncover
and visualize tracking services by IoT devices in an instrumented space and (2)
take action to stop or limit that tracking. We design and implement VBIT to
operate at the network traffic level, and we show that it has negligible
performance overhead, and offers flexibility and good usability. We perform a
mixed-method user study consisting of an online survey and an in-person
interview study. We show that VBIT users appreciate VBIT's transparency,
control, and customization features, and they become significantly more willing
to install an IoT advertising and tracking blocker, after using VBIT. In the
process, we obtain design insights that can be used to further iterate and
improve the design of VBIT and other systems for IoT transparency and control.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06069v1">Privacy-Preserving Data Linkage Across Private and Public Datasets for
  Collaborative Agriculture Research</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-09-09T21:07:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Osama Zafar, Rosemarie Santa Gonzalez, Gabriel Wilkins, Alfonso Morales, Erman Ayday</p>
    <p><b>Summary:</b> Digital agriculture leverages technology to enhance crop yield, disease
resilience, and soil health, playing a critical role in agricultural research.
However, it raises privacy concerns such as adverse pricing, price
discrimination, higher insurance costs, and manipulation of resources,
deterring farm operators from sharing data due to potential misuse. This study
introduces a privacy-preserving framework that addresses these risks while
allowing secure data sharing for digital agriculture. Our framework enables
comprehensive data analysis while protecting privacy. It allows stakeholders to
harness research-driven policies that link public and private datasets. The
proposed algorithm achieves this by: (1) identifying similar farmers based on
private datasets, (2) providing aggregate information like time and location,
(3) determining trends in price and product availability, and (4) correlating
trends with public policy data, such as food insecurity statistics. We validate
the framework with real-world Farmer's Market datasets, demonstrating its
efficacy through machine learning models trained on linked privacy-preserved
data. The results support policymakers and researchers in addressing food
insecurity and pricing issues. This work significantly contributes to digital
agriculture by providing a secure method for integrating and analyzing data,
driving advancements in agricultural technology and development.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.05623v1">A Framework for Differential Privacy Against Timing Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-09T13:56:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zachary Ratliff, Salil Vadhan</p>
    <p><b>Summary:</b> The standard definition of differential privacy (DP) ensures that a
mechanism's output distribution on adjacent datasets is indistinguishable.
However, real-world implementations of DP can, and often do, reveal information
through their runtime distributions, making them susceptible to timing attacks.
In this work, we establish a general framework for ensuring differential
privacy in the presence of timing side channels. We define a new notion of
timing privacy, which captures programs that remain differentially private to
an adversary that observes the program's runtime in addition to the output. Our
framework enables chaining together component programs that are timing-stable
followed by a random delay to obtain DP programs that achieve timing privacy.
Importantly, our definitions allow for measuring timing privacy and output
privacy using different privacy measures. We illustrate how to instantiate our
framework by giving programs for standard DP computations in the RAM and Word
RAM models of computation. Furthermore, we show how our framework can be
realized in code through a natural extension of the OpenDP Programming
Framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.05249v1">NetDPSyn: Synthesizing Network Traces under Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-09-08T23:54:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Danyu Sun, Joann Qiongna Chen, Chen Gong, Tianhao Wang, Zhou Li</p>
    <p><b>Summary:</b> As the utilization of network traces for the network measurement research
becomes increasingly prevalent, concerns regarding privacy leakage from network
traces have garnered the public's attention. To safeguard network traces,
researchers have proposed the trace synthesis that retains the essential
properties of the raw data. However, previous works also show that synthesis
traces with generative models are vulnerable under linkage attacks.
  This paper introduces NetDPSyn, the first system to synthesize high-fidelity
network traces under privacy guarantees. NetDPSyn is built with the
Differential Privacy (DP) framework as its core, which is significantly
different from prior works that apply DP when training the generative model.
The experiments conducted on three flow and two packet datasets indicate that
NetDPSyn achieves much better data utility in downstream tasks like anomaly
detection. NetDPSyn is also 2.5 times faster than the other methods on average
in data synthesis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04877v1">Strong Privacy-Preserving Universally Composable AKA Protocol with
  Seamless Handover Support for Mobile Virtual Network Operator</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-07T18:04:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rabiah Alnashwan, Yang Yang, Yilu Dong, Prosanta Gope, Behzad Abdolmaleki, Syed Rafiul Hussain</p>
    <p><b>Summary:</b> Consumers seeking a new mobile plan have many choices in the present mobile
landscape. The Mobile Virtual Network Operator (MVNO) has recently gained
considerable attention among these options. MVNOs offer various benefits,
making them an appealing choice for a majority of consumers. These advantages
encompass flexibility, access to cutting-edge technologies, enhanced coverage,
superior customer service, and substantial cost savings. Even though MVNO
offers several advantages, it also creates some security and privacy concerns
for the customer simultaneously. For instance, in the existing solution, MVNO
needs to hand over all the sensitive details, including the users' identities
and master secret keys of their customers, to a mobile operator (MNO) to
validate the customers while offering any services. This allows MNOs to have
unrestricted access to the MVNO subscribers' location and mobile data,
including voice calls, SMS, and Internet, which the MNOs frequently sell to
third parties (e.g., advertisement companies and surveillance agencies) for
more profit. Although critical for mass users, such privacy loss has been
historically ignored due to the lack of practical and privacy-preserving
solutions for registration and handover procedures in cellular networks. In
this paper, we propose a universally composable authentication and handover
scheme with strong user privacy support, where each MVNO user can validate a
mobile operator (MNO) and vice-versa without compromising user anonymity and
unlinkability support. Here, we anticipate that our proposed solution will most
likely be deployed by the MVNO(s) to ensure enhanced privacy support to their
customer(s).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04716v1">Privacy enhanced collaborative inference in the Cox proportional hazards
  model for distributed data</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2024-09-07T05:32:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mengtong Hu, Xu Shi, Peter X. -K. Song</p>
    <p><b>Summary:</b> Data sharing barriers are paramount challenges arising from multicenter
clinical studies where multiple data sources are stored in a distributed
fashion at different local study sites. Particularly in the case of
time-to-event analysis when global risk sets are needed for the Cox
proportional hazards model, access to a centralized database is typically
necessary. Merging such data sources into a common data storage for a
centralized statistical analysis requires a data use agreement, which is often
time-consuming. Furthermore, the construction and distribution of risk sets to
participating clinical centers for subsequent calculations may pose a risk of
revealing individual-level information. We propose a new collaborative Cox
model that eliminates the need for accessing the centralized database and
constructing global risk sets but needs only the sharing of summary statistics
with significantly smaller dimensions than risk sets. Thus, the proposed
collaborative inference enjoys maximal protection of data privacy. We show
theoretically and numerically that the new distributed proportional hazards
model approach has little loss of statistical power when compared to the
centralized method that requires merging the entire data. We present a
renewable sieve method to establish large-sample properties for the proposed
method. We illustrate its performance through simulation experiments and a
real-world data example from patients with kidney transplantation in the Organ
Procurement and Transplantation Network (OPTN) to understand the factors
associated with the 5-year death-censored graft failure (DCGF) for patients who
underwent kidney transplants in the US.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04652v1">Privacy-Preserving Race/Ethnicity Estimation for Algorithmic Bias
  Measurement in the U.S</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T23:29:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Saikrishna Badrinarayanan, Osonde Osoba, Miao Cheng, Ryan Rogers, Sakshi Jain, Rahul Tandra, Natesh S. Pillai</p>
    <p><b>Summary:</b> AI fairness measurements, including tests for equal treatment, often take the
form of disaggregated evaluations of AI systems. Such measurements are an
important part of Responsible AI operations. These measurements compare system
performance across demographic groups or sub-populations and typically require
member-level demographic signals such as gender, race, ethnicity, and location.
However, sensitive member-level demographic attributes like race and ethnicity
can be challenging to obtain and use due to platform choices, legal
constraints, and cultural norms. In this paper, we focus on the task of
enabling AI fairness measurements on race/ethnicity for \emph{U.S. LinkedIn
members} in a privacy-preserving manner. We present the Privacy-Preserving
Probabilistic Race/Ethnicity Estimation (PPRE) method for performing this task.
PPRE combines the Bayesian Improved Surname Geocoding (BISG) model, a sparse
LinkedIn survey sample of self-reported demographics, and privacy-enhancing
technologies like secure two-party computation and differential privacy to
enable meaningful fairness measurements while preserving member privacy. We
provide details of the PPRE method and its privacy guarantees. We then
illustrate sample measurement operations. We conclude with a review of open
research and engineering challenges for expanding our privacy-preserving
fairness measurement capabilities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04366v1">Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T15:57:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lioba Heimbach, Yann Vonlanthen, Juan Villacis, Lucianna Kiffer, Roger Wattenhofer</p>
    <p><b>Summary:</b> Many blockchain networks aim to preserve the anonymity of validators in the
peer-to-peer (P2P) network, ensuring that no adversary can link a validator's
identifier to the IP address of a peer due to associated privacy and security
concerns. This work demonstrates that the Ethereum P2P network does not offer
this anonymity. We present a methodology that enables any node in the network
to identify validators hosted on connected peers and empirically verify the
feasibility of our proposed method. Using data collected from four nodes over
three days, we locate more than 15% of Ethereum validators in the P2P network.
The insights gained from our deanonymization technique provide valuable
information on the distribution of validators across peers, their geographic
locations, and hosting organizations. We further discuss the implications and
risks associated with the lack of anonymity in the P2P network and propose
methods to help validators protect their privacy. The Ethereum Foundation has
awarded us a bug bounty, acknowledging the impact of our results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04257v1">Privacy risk from synthetic data: practical proposals</a></h3>
  
  <p><b>Published on:</b> 2024-09-06T13:10:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gillian M Raab</p>
    <p><b>Summary:</b> This paper proposes and compares measures of identity and attribute
disclosure risk for synthetic data. Data custodians can use the methods
proposed here to inform the decision as to whether to release synthetic
versions of confidential data. Different measures are evaluated on two data
sets. Insight into the measures is obtained by examining the details of the
records identified as posing a disclosure risk. This leads to methods to
identify, and possibly exclude, apparently risky records where the
identification or attribution would be expected by someone with background
knowledge of the data. The methods described are available as part of the
\textbf{synthpop} package for \textbf{R}.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04194v1">Towards Privacy-Preserving Relational Data Synthesis via Probabilistic
  Relational Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-06T11:24:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Malte Luttermann, Ralf M√∂ller, Mattis Hartwig</p>
    <p><b>Summary:</b> Probabilistic relational models provide a well-established formalism to
combine first-order logic and probabilistic models, thereby allowing to
represent relationships between objects in a relational domain. At the same
time, the field of artificial intelligence requires increasingly large amounts
of relational training data for various machine learning tasks. Collecting
real-world data, however, is often challenging due to privacy concerns, data
protection regulations, high costs, and so on. To mitigate these challenges,
the generation of synthetic data is a promising approach. In this paper, we
solve the problem of generating synthetic relational data via probabilistic
relational models. In particular, we propose a fully-fledged pipeline to go
from relational database to probabilistic relational model, which can then be
used to sample new synthetic relational data points from its underlying
probability distribution. As part of our proposed pipeline, we introduce a
learning algorithm to construct a probabilistic relational model from a given
relational database.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04173v1">NPU-NTU System for Voice Privacy 2024 Challenge</a></h3>
  
  <p><b>Published on:</b> 2024-09-06T10:32:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jixun Yao, Nikita Kuzmin, Qing Wang, Pengcheng Guo, Ziqian Ning, Dake Guo, Kong Aik Lee, Eng-Siong Chng, Lei Xie</p>
    <p><b>Summary:</b> Speaker anonymization is an effective privacy protection solution that
conceals the speaker's identity while preserving the linguistic content and
paralinguistic information of the original speech. To establish a fair
benchmark and facilitate comparison of speaker anonymization systems, the
VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition
planned for 2024. In this paper, we describe our proposed speaker anonymization
system for VPC 2024. Our system employs a disentangled neural codec
architecture and a serial disentanglement strategy to gradually disentangle the
global speaker identity and time-variant linguistic content and paralinguistic
information. We introduce multiple distillation methods to disentangle
linguistic content, speaker identity, and emotion. These methods include
semantic distillation, supervised speaker distillation, and frame-level emotion
distillation. Based on these distillations, we anonymize the original speaker
identity using a weighted sum of a set of candidate speaker identities and a
randomly generated speaker identity. Our system achieves the best trade-off of
privacy protection and emotion preservation in VPC 2024.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04167v1">Do Android App Developers Accurately Report Collection of
  Privacy-Related Data?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T10:05:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mugdha Khedkar, Ambuj Kumar Mondal, Eric Bodden</p>
    <p><b>Summary:</b> Many Android applications collect data from users. The European Union's
General Data Protection Regulation (GDPR) requires vendors to faithfully
disclose which data their apps collect. This task is complicated because many
apps use third-party code for which the same information is not readily
available. Hence we ask: how accurately do current Android apps fulfill these
requirements?
  In this work, we first expose a multi-layered definition of privacy-related
data to correctly report data collection in Android apps. We further create a
dataset of privacy-sensitive data classes that may be used as input by an
Android app. This dataset takes into account data collected both through the
user interface and system APIs.
  We manually examine the data safety sections of 70 Android apps to observe
how data collection is reported, identifying instances of over- and
under-reporting. Additionally, we develop a prototype to statically extract and
label privacy-related data collected via app source code, user interfaces, and
permissions. Comparing the prototype's results with the data safety sections of
20 apps reveals reporting discrepancies. Using the results from two Messaging
and Social Media apps (Signal and Instagram), we discuss how app developers
under-report and over-report data collection, respectively, and identify
inaccurately reported data categories.
  Our results show that app developers struggle to accurately report data
collection, either due to Google's abstract definition of collected data or
insufficient existing tool support.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04048v2">Exploring User Privacy Awareness on GitHub: An Empirical Study</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-09-06T06:41:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Costanza Alfieri, Juri Di Rocco, Paola Inverardi, Phuong T. Nguyen</p>
    <p><b>Summary:</b> GitHub provides developers with a practical way to distribute source code and
collaboratively work on common projects. To enhance account security and
privacy, GitHub allows its users to manage access permissions, review audit
logs, and enable two-factor authentication. However, despite the endless
effort, the platform still faces various issues related to the privacy of its
users. This paper presents an empirical study delving into the GitHub
ecosystem. Our focus is on investigating the utilization of privacy settings on
the platform and identifying various types of sensitive information disclosed
by users. Leveraging a dataset comprising 6,132 developers, we report and
analyze their activities by means of comments on pull requests. Our findings
indicate an active engagement by users with the available privacy settings on
GitHub. Notably, we observe the disclosure of different forms of private
information within pull request comments. This observation has prompted our
exploration into sensitivity detection using a large language model and BERT,
to pave the way for a personalized privacy assistant. Our work provides
insights into the utilization of existing privacy protection tools, such as
privacy settings, along with their inherent limitations. Essentially, we aim to
advance research in this field by providing both the motivation for creating
such privacy protection tools and a proposed methodology for personalizing
them.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04026v1">Efficient Fault-Tolerant Quantum Protocol for Differential Privacy in
  the Shuffle Model</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T04:53:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hassan Jameel Asghar, Arghya Mukherjee, Gavin K. Brennen</p>
    <p><b>Summary:</b> We present a quantum protocol which securely and implicitly implements a
random shuffle to realize differential privacy in the shuffle model. The
shuffle model of differential privacy amplifies privacy achievable via local
differential privacy by randomly permuting the tuple of outcomes from data
contributors. In practice, one needs to address how this shuffle is
implemented. Examples include implementing the shuffle via mix-networks, or
shuffling via a trusted third-party. These implementation specific issues raise
non-trivial computational and trust requirements in a classical system. We
propose a quantum version of the protocol using entanglement of quantum states
and show that the shuffle can be implemented without these extra requirements.
Our protocol implements k-ary randomized response, for any value of k > 2, and
furthermore, can be efficiently implemented using fault-tolerant computation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03707v1">A Different Level Text Protection Mechanism With Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-09-05T17:13:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qingwen Fu</p>
    <p><b>Summary:</b> The article introduces a method for extracting words of different degrees of
importance based on the BERT pre-training model and proves the effectiveness of
this method. The article also discusses the impact of maintaining the same
perturbation results for words of different importance on the overall text
utility. This method can be applied to long text protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03655v1">Privacy versus Emotion Preservation Trade-offs in Emotion-Preserving
  Speaker Anonymization</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-05T16:10:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zexin Cai, Henry Li Xinyuan, Ashi Garg, Leibny Paola Garc√≠a-Perera, Kevin Duh, Sanjeev Khudanpur, Nicholas Andrews, Matthew Wiesner</p>
    <p><b>Summary:</b> Advances in speech technology now allow unprecedented access to personally
identifiable information through speech. To protect such information, the
differential privacy field has explored ways to anonymize speech while
preserving its utility, including linguistic and paralinguistic aspects.
However, anonymizing speech while maintaining emotional state remains
challenging. We explore this problem in the context of the VoicePrivacy 2024
challenge. Specifically, we developed various speaker anonymization pipelines
and find that approaches either excel at anonymization or preserving emotion
state, but not both simultaneously. Achieving both would require an in-domain
emotion recognizer. Additionally, we found that it is feasible to train a
semi-effective speaker verification system using only emotion representations,
demonstrating the challenge of separating these two modalities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03568v1">Enabling Practical and Privacy-Preserving Image Processing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-09-05T14:22:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chao Wang, Shubing Yang, Xiaoyan Sun, Jun Dai, Dongfang Zhao</p>
    <p><b>Summary:</b> Fully Homomorphic Encryption (FHE) enables computations on encrypted data,
preserving confidentiality without the need for decryption. However, FHE is
often hindered by significant performance overhead, particularly for
high-precision and complex data like images. Due to serious efficiency issues,
traditional FHE methods often encrypt images by monolithic data blocks (such as
pixel rows), instead of pixels. However, this strategy compromises the
advantages of homomorphic operations and disables pixel-level image processing.
In this study, we address these challenges by proposing and implementing a
pixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS
scheme. To enhance computational efficiency, we introduce three novel caching
mechanisms to pre-encrypt radix values or frequently occurring pixel values,
substantially reducing redundant encryption operations. Extensive experiments
demonstrate that our approach achieves up to a 19-fold improvement in
encryption speed compared to the original CKKS, while maintaining high image
quality. Additionally, real-world image applications such as mean filtering,
brightness enhancement, image matching and watermarking are tested based on
FHE, showcasing up to a 91.53% speed improvement. We also proved that our
method is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,
providing strong encryption security. These results underscore the practicality
and efficiency of iCHEETAH, marking a significant advancement in
privacy-preserving image processing at scale.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03344v1">Rethinking Improved Privacy-Utility Trade-off with Pre-existing
  Knowledge for DP Training</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-05T08:40:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yu Zheng, Wenchao Zhang, Yonggang Zhang, Wei Song, Kai Zhou, Bo Han</p>
    <p><b>Summary:</b> Differential privacy (DP) provides a provable framework for protecting
individuals by customizing a random mechanism over a privacy-sensitive dataset.
Deep learning models have demonstrated privacy risks in model exposure as an
established learning model unintentionally records membership-level privacy
leakage. Differentially private stochastic gradient descent (DP- SGD) has been
proposed to safeguard training individuals by adding random Gaussian noise to
gradient updates in the backpropagation. Researchers identify that DP-SGD
typically causes utility loss since the injected homogeneous noise alters the
gradient updates calculated at each iteration. Namely, all elements in the
gradient are contaminated regardless of their importance in updating model
parameters. In this work, we argue that the utility loss mainly results from
the homogeneity of injected noise. Consequently, we propose a generic
differential privacy framework with heterogeneous noise (DP-Hero) by defining a
heterogeneous random mechanism to abstract its property. The insight of DP-Hero
is to leverage the knowledge encoded in the previously trained model to guide
the subsequent allocation of noise heterogeneity, thereby leveraging the
statistical perturbation and achieving enhanced utility. Atop DP-Hero, we
instantiate a heterogeneous version of DP-SGD, where the noise injected into
gradients is heterogeneous and guided by prior-established model parameters. We
conduct comprehensive experiments to verify and explain the effectiveness of
the proposed DP-Hero, showing improved training accuracy compared with
state-of-the-art works. Broadly, we shed light on improving the privacy-utility
space by learning the noise guidance from the pre-existing leaked knowledge
encoded in the previously trained model, showing a different perspective of
understanding the utility-improved DP training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03326v1">Enhancing User-Centric Privacy Protection: An Interactive Framework
  through Diffusion Models and Machine Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-09-05T07:55:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Huaxi Huang, Xin Yuan, Qiyu Liao, Dadong Wang, Tongliang Liu</p>
    <p><b>Summary:</b> In the realm of multimedia data analysis, the extensive use of image datasets
has escalated concerns over privacy protection within such data. Current
research predominantly focuses on privacy protection either in data sharing or
upon the release of trained machine learning models. Our study pioneers a
comprehensive privacy protection framework that safeguards image data privacy
concurrently during data sharing and model publication. We propose an
interactive image privacy protection framework that utilizes generative machine
learning models to modify image information at the attribute level and employs
machine unlearning algorithms for the privacy preservation of model parameters.
This user-interactive framework allows for adjustments in privacy protection
intensity based on user feedback on generated images, striking a balance
between maximal privacy safeguarding and maintaining model performance. Within
this framework, we instantiate two modules: a differential privacy diffusion
model for protecting attribute information in images and a feature unlearning
algorithm for efficient updates of the trained model on the revised image
dataset. Our approach demonstrated superiority over existing methods on facial
datasets across various attribute classifications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03294v1">Federated Prototype-based Contrastive Learning for Privacy-Preserving
  Cross-domain Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-09-05T06:59:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Li Wang, Quangui Zhang, Lei Sang, Qiang Wu, Min Xu</p>
    <p><b>Summary:</b> Cross-domain recommendation (CDR) aims to improve recommendation accuracy in
sparse domains by transferring knowledge from data-rich domains. However,
existing CDR methods often assume the availability of user-item interaction
data across domains, overlooking user privacy concerns. Furthermore, these
methods suffer from performance degradation in scenarios with sparse
overlapping users, as they typically depend on a large number of fully shared
users for effective knowledge transfer. To address these challenges, we propose
a Federated Prototype-based Contrastive Learning (CL) method for
Privacy-Preserving CDR, named FedPCL-CDR. This approach utilizes
non-overlapping user information and prototypes to improve multi-domain
performance while protecting user privacy. FedPCL-CDR comprises two modules:
local domain (client) learning and global server aggregation. In the local
domain, FedPCL-CDR clusters all user data to learn representative prototypes,
effectively utilizing non-overlapping user information and addressing the
sparse overlapping user issue. It then facilitates knowledge transfer by
employing both local and global prototypes returned from the server in a CL
manner. Simultaneously, the global server aggregates representative prototypes
from local domains to learn both local and global prototypes. The combination
of prototypes and federated learning (FL) ensures that sensitive user data
remains decentralized, with only prototypes being shared across domains,
thereby protecting user privacy. Extensive experiments on four CDR tasks using
two real-world datasets demonstrate that FedPCL-CDR outperforms the
state-of-the-art baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03796v1">Protecting Activity Sensing Data Privacy Using Hierarchical Information
  Dissociation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-04T15:38:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guangjing Wang, Hanqing Guo, Yuanda Wang, Bocheng Chen, Ce Zhou, Qiben Yan</p>
    <p><b>Summary:</b> Smartphones and wearable devices have been integrated into our daily lives,
offering personalized services. However, many apps become overprivileged as
their collected sensing data contains unnecessary sensitive information. For
example, mobile sensing data could reveal private attributes (e.g., gender and
age) and unintended sensitive features (e.g., hand gestures when entering
passwords). To prevent sensitive information leakage, existing methods must
obtain private labels and users need to specify privacy policies. However, they
only achieve limited control over information disclosure. In this work, we
present Hippo to dissociate hierarchical information including private metadata
and multi-grained activity information from the sensing data. Hippo achieves
fine-grained control over the disclosure of sensitive information without
requiring private labels. Specifically, we design a latent guidance-based
diffusion model, which generates multi-grained versions of raw sensor data
conditioned on hierarchical latent activity features. Hippo enables users to
control the disclosure of sensitive information in sensing data, ensuring their
privacy while preserving the necessary features to meet the utility
requirements of applications. Hippo is the first unified model that achieves
two goals: perturbing the sensitive attributes and controlling the disclosure
of sensitive information in mobile sensing data. Extensive experiments show
that Hippo can anonymize personal attributes and transform activity information
at various resolutions across different types of sensing data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02614v1">Evaluating the Effects of Digital Privacy Regulations on User Trust</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-09-04T11:11:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mehmet Berk Cetin</p>
    <p><b>Summary:</b> In today's digital society, issues related to digital privacy have become
increasingly important. Issues such as data breaches result in misuse of data,
financial loss, and cyberbullying, which leads to less user trust in digital
services. This research investigates the impact of digital privacy laws on user
trust by comparing the regulations in the Netherlands, Ghana, and Malaysia. The
study employs a comparative case study method, involving interviews with
digital privacy law experts, IT educators, and consumers from each country. The
main findings reveal that while the General Data Protection Regulation (GDPR)
in the Netherlands is strict, its practical impact is limited by enforcement
challenges. In Ghana, the Data Protection Act is underutilized due to low
public awareness and insufficient enforcement, leading to reliance on personal
protective measures. In Malaysia, trust in digital services is largely
dependent on the security practices of individual platforms rather than the
Personal Data Protection Act. The study highlights the importance of public
awareness, effective enforcement, and cultural considerations in shaping the
effectiveness of digital privacy laws. Based on these insights, a
recommendation framework is proposed to enhance digital privacy practices, also
aiming to provide valuable guidance for policymakers, businesses, and citizens
in navigating the challenges of digitalization.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02404v1">Learning Privacy-Preserving Student Networks via
  Discriminative-Generative Distillation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-04T03:06:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shiming Ge, Bochao Liu, Pengju Wang, Yong Li, Dan Zeng</p>
    <p><b>Summary:</b> While deep models have proved successful in learning rich knowledge from
massive well-annotated data, they may pose a privacy leakage risk in practical
deployment. It is necessary to find an effective trade-off between high utility
and strong privacy. In this work, we propose a discriminative-generative
distillation approach to learn privacy-preserving deep models. Our key idea is
taking models as bridge to distill knowledge from private data and then
transfer it to learn a student network via two streams. First, discriminative
stream trains a baseline classifier on private data and an ensemble of teachers
on multiple disjoint private subsets, respectively. Then, generative stream
takes the classifier as a fixed discriminator and trains a generator in a
data-free manner. After that, the generator is used to generate massive
synthetic data which are further applied to train a variational autoencoder
(VAE). Among these synthetic data, a few of them are fed into the teacher
ensemble to query labels via differentially private aggregation, while most of
them are embedded to the trained VAE for reconstructing synthetic data.
Finally, a semi-supervised student learning is performed to simultaneously
handle two tasks: knowledge transfer from the teachers with distillation on few
privately labeled synthetic data, and knowledge enhancement with tangent-normal
adversarial regularization on many triples of reconstructed synthetic data. In
this way, our approach can control query cost over private data and mitigate
accuracy degradation in a unified manner, leading to a privacy-preserving
student model. Extensive experiments and analysis clearly show the
effectiveness of the proposed approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02375v1">How Privacy-Savvy Are Large Language Models? A Case Study on Compliance
  and Privacy Technical Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-09-04T01:51:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xichou Zhu, Yang Liu, Zhou Shen, Yi Liu, Min Li, Yujun Chen, Benzi John, Zhenzhen Ma, Tao Hu, Bolong Yang, Manman Wang, Zongxing Xie, Peng Liu, Dan Cai, Junhui Wang</p>
    <p><b>Summary:</b> The recent advances in large language models (LLMs) have significantly
expanded their applications across various fields such as language generation,
summarization, and complex question answering. However, their application to
privacy compliance and technical privacy reviews remains under-explored,
raising critical concerns about their ability to adhere to global privacy
standards and protect sensitive user data. This paper seeks to address this gap
by providing a comprehensive case study evaluating LLMs' performance in
privacy-related tasks such as privacy information extraction (PIE), legal and
regulatory key point detection (KPD), and question answering (QA) with respect
to privacy policies and data protection regulations. We introduce a Privacy
Technical Review (PTR) framework, highlighting its role in mitigating privacy
risks during the software development life-cycle. Through an empirical
assessment, we investigate the capacity of several prominent LLMs, including
BERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks
and technical privacy reviews. Our experiments benchmark the models across
multiple dimensions, focusing on their precision, recall, and F1-scores in
extracting privacy-sensitive information and detecting key regulatory
compliance points. While LLMs show promise in automating privacy reviews and
identifying regulatory discrepancies, significant gaps persist in their ability
to fully comply with evolving legal standards. We provide actionable
recommendations for enhancing LLMs' capabilities in privacy compliance,
emphasizing the need for robust model improvements and better integration with
legal and regulatory requirements. This study underscores the growing
importance of developing privacy-aware LLMs that can both support businesses in
compliance efforts and safeguard user privacy rights.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02364v1">Examining Caregiving Roles to Differentiate the Effects of Using a
  Mobile App for Community Oversight for Privacy and Security</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-09-04T01:21:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mamtaj Akter, Jess Kropczynski, Heather Lipford, Pamela Wisniewski</p>
    <p><b>Summary:</b> We conducted a 4-week field study with 101 smartphone users who
self-organized into 22 small groups of family, friends, and neighbors to use
``CO-oPS,'' a mobile app for co-managing mobile privacy and security. We
differentiated between those who provided oversight (i.e., caregivers) and
those who did not (i.e., caregivees) to examine differential effects on their
experiences and behaviors while using CO-oPS. Caregivers reported higher power
use, community trust, belonging, collective efficacy, and self-efficacy than
caregivees. Both groups' self-efficacy and collective efficacy for mobile
privacy and security increased after using CO-oPS. However, this increase was
significantly stronger for caregivees. Our research demonstrates how
community-based approaches can benefit people who need additional help managing
their digital privacy and security. We provide recommendations to support
community-based oversight for managing privacy and security within communities
of different roles and skills.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02044v1">FedMinds: Privacy-Preserving Personalized Brain Visual Decoding</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> 
  <p><b>Published on:</b> 2024-09-03T16:46:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guangyin Bao, Duoqian Miao</p>
    <p><b>Summary:</b> Exploring the mysteries of the human brain is a long-term research topic in
neuroscience. With the help of deep learning, decoding visual information from
human brain activity fMRI has achieved promising performance. However, these
decoding models require centralized storage of fMRI data to conduct training,
leading to potential privacy security issues. In this paper, we focus on
privacy preservation in multi-individual brain visual decoding. To this end, we
introduce a novel framework called FedMinds, which utilizes federated learning
to protect individuals' privacy during model training. In addition, we deploy
individual adapters for each subject, thus allowing personalized visual
decoding. We conduct experiments on the authoritative NSD datasets to evaluate
the performance of the proposed framework. The results demonstrate that our
framework achieves high-precision visual decoding along with privacy
protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01924v1">Privacy-Preserving and Post-Quantum Counter Denial of Service Framework
  for Wireless Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-03T14:14:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Saleh Darzi, Attila Altay Yavuz</p>
    <p><b>Summary:</b> As network services progress and mobile and IoT environments expand, numerous
security concerns have surfaced for spectrum access systems. The omnipresent
risk of Denial-of-Service (DoS) attacks and raising concerns about user privacy
(e.g., location privacy, anonymity) are among such cyber threats. These
security and privacy risks increase due to the threat of quantum computers that
can compromise long-term security by circumventing conventional cryptosystems
and increasing the cost of countermeasures. While some defense mechanisms exist
against these threats in isolation, there is a significant gap in the state of
the art on a holistic solution against DoS attacks with privacy and anonymity
for spectrum management systems, especially when post-quantum (PQ) security is
in mind. In this paper, we propose a new cybersecurity framework PACDoSQ, which
is (to the best of our knowledge) the first to offer location privacy and
anonymity for spectrum management with counter DoS and PQ security
simultaneously. Our solution introduces the private spectrum bastion (database)
concept to exploit existing architectural features of spectrum management
systems and then synergizes them with multi-server private information
retrieval and PQ-secure Tor to guarantee a location-private and anonymous
acquisition of spectrum information together with hash-based client-server
puzzles for counter DoS. We prove that PACDoSQ achieves its security
objectives, and show its feasibility via a comprehensive performance
evaluation.</p>
  </details>
</div>

