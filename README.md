
<h2>2025-06</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.06124v1">PrivTru: A Privacy-by-Design Data Trustee Minimizing Information Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-06-06T14:33:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lukas Gehring, Florian Tschorsch</p>
    <p><b>Summary:</b> Data trustees serve as intermediaries that facilitate secure data sharing
between independent parties. This paper offers a technical perspective on Data
trustees, guided by privacy-by-design principles. We introduce PrivTru, an
instantiation of a data trustee that provably achieves optimal privacy
properties. Therefore, PrivTru calculates the minimal amount of information the
data trustee needs to request from data sources to respond to a given query.
Our analysis shows that PrivTru minimizes information leakage to the data
trustee, regardless of the trustee's prior knowledge, while preserving the
utility of the data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.06062v1">Minoritised Ethnic People's Security and Privacy Concerns and Responses
  towards Essential Online Services</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> 
  <p><b>Published on:</b> 2025-06-06T13:17:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Aunam Quyoum, Mark Wong, Sebati Ghosh, Siamak F. Shahandashti</p>
    <p><b>Summary:</b> Minoritised ethnic people are marginalised in society, and therefore at a
higher risk of adverse online harms, including those arising from the loss of
security and privacy of personal data. Despite this, there has been very little
research focused on minoritised ethnic people's security and privacy concerns,
attitudes, and behaviours. In this work, we provide the results of one of the
first studies in this regard. We explore minoritised ethnic people's
experiences of using essential online services across three sectors: health,
social housing, and energy, their security and privacy-related concerns, and
responses towards these services. We conducted a thematic analysis of 44
semi-structured interviews with people of various reported minoritised
ethnicities in the UK. Privacy concerns and lack of control over personal data
emerged as a major theme, with many interviewees considering privacy as their
most significant concern when using online services. Several creative tactics
to exercise some agency were reported, including selective and inconsistent
disclosure of personal data. A core concern about how data may be used was
driven by a fear of repercussions, including penalisation and discrimination,
influenced by prior experiences of institutional and online racism. The
increased concern and potential for harm resulted in minoritised ethnic people
grappling with a higher-stakes dilemma of whether to disclose personal
information online or not. Furthermore, trust in institutions, or lack thereof,
was found to be embedded throughout as a basis for adapting behaviour. We draw
on our results to provide lessons learned for the design of more inclusive,
marginalisation-aware, and privacy-preserving online services.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.05908v1">QualitEye: Public and Privacy-preserving Gaze Data Quality Verification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-06-06T09:27:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mayar Elfares, Pascal Reisert, Ralf Küsters, Andreas Bulling</p>
    <p><b>Summary:</b> Gaze-based applications are increasingly advancing with the availability of
large datasets but ensuring data quality presents a substantial challenge when
collecting data at scale. It further requires different parties to collaborate,
therefore, privacy concerns arise. We propose QualitEye--the first method for
verifying image-based gaze data quality. QualitEye employs a new semantic
representation of eye images that contains the information required for
verification while excluding irrelevant information for better domain
adaptation. QualitEye covers a public setting where parties can freely exchange
data and a privacy-preserving setting where parties cannot reveal their raw
data nor derive gaze features/labels of others with adapted private set
intersection protocols. We evaluate QualitEye on the MPIIFaceGaze and
GazeCapture datasets and achieve a high verification performance (with a small
overhead in runtime for privacy-preserving versions). Hence, QualitEye paves
the way for new gaze analysis methods at the intersection of machine learning,
human-computer interaction, and cryptography.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.05743v1">When Better Features Mean Greater Risks: The Performance-Privacy
  Trade-Off in Contrastive Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-06-06T05:03:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ruining Sun, Hongsheng Hu, Wei Luo, Zhaoxi Zhang, Yanjun Zhang, Haizhuan Yuan, Leo Yu Zhang</p>
    <p><b>Summary:</b> With the rapid advancement of deep learning technology, pre-trained encoder
models have demonstrated exceptional feature extraction capabilities, playing a
pivotal role in the research and application of deep learning. However, their
widespread use has raised significant concerns about the risk of training data
privacy leakage. This paper systematically investigates the privacy threats
posed by membership inference attacks (MIAs) targeting encoder models, focusing
on contrastive learning frameworks. Through experimental analysis, we reveal
the significant impact of model architecture complexity on membership privacy
leakage: As more advanced encoder frameworks improve feature-extraction
performance, they simultaneously exacerbate privacy-leakage risks. Furthermore,
this paper proposes a novel membership inference attack method based on the
p-norm of feature vectors, termed the Embedding Lp-Norm Likelihood Attack
(LpLA). This method infers membership status, by leveraging the statistical
distribution characteristics of the p-norm of feature vectors. Experimental
results across multiple datasets and model architectures demonstrate that LpLA
outperforms existing methods in attack performance and robustness, particularly
under limited attack knowledge and query volumes. This study not only uncovers
the potential risks of privacy leakage in contrastive learning frameworks, but
also provides a practical basis for privacy protection research in encoder
models. We hope that this work will draw greater attention to the privacy risks
associated with self-supervised learning models and shed light on the
importance of a balance between model utility and training data privacy. Our
code is publicly available at: https://github.com/SeroneySun/LpLA_code.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.05683v1">Multi-Modal Multi-Task Federated Foundation Models for Next-Generation
  Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence
  in AR/VR/MR</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2025-06-06T02:23:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fardis Nadimi, Payam Abdisarabshali, Kasra Borazjani, Jacob Chakareski, Seyyedali Hosseinalipour</p>
    <p><b>Summary:</b> Extended reality (XR) systems, which consist of virtual reality (VR),
augmented reality (AR), and mixed reality (XR), offer a transformative
interface for immersive, multi-modal, and embodied human-computer interaction.
In this paper, we envision that multi-modal multi-task (M3T) federated
foundation models (FedFMs) can offer transformative capabilities for XR systems
through integrating the representational strength of M3T foundation models
(FMs) with the privacy-preserving model training principles of federated
learning (FL). We present a modular architecture for FedFMs, which entails
different coordination paradigms for model training and aggregations. Central
to our vision is the codification of XR challenges that affect the
implementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality
diversity, (2) Hardware heterogeneity and system-level constraints, (3)
Interactivity and embodied personalization, (4) Functional/task variability,
and (5) Temporality and environmental variability. We illustrate the
manifestation of these dimensions across a set of emerging and anticipated
applications of XR systems. Finally, we propose evaluation metrics, dataset
requirements, and design tradeoffs necessary for the development of
resource-aware FedFMs in XR. This perspective aims to chart the technical and
conceptual foundations for context-aware privacy-preserving intelligence in the
next generation of XR systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.05503v1">On Differential Privacy for Adaptively Solving Search Problems via
  Sketching</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-06-05T18:40:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shiyuan Feng, Ying Feng, George Z. Li, Zhao Song, David P. Woodruff, Lichen Zhang</p>
    <p><b>Summary:</b> Recently differential privacy has been used for a number of streaming, data
structure, and dynamic graph problems as a means of hiding the internal
randomness of the data structure, so that multiple possibly adaptive queries
can be made without sacrificing the correctness of the responses. Although
these works use differential privacy to show that for some problems it is
possible to tolerate $T$ queries using $\widetilde{O}(\sqrt{T})$ copies of a
data structure, such results only apply to numerical estimation problems, and
only return the cost of an optimization problem rather than the solution
itself. In this paper, we investigate the use of differential privacy for
adaptive queries to search problems, which are significantly more challenging
since the responses to queries can reveal much more about the internal
randomness than a single numerical query. We focus on two classical search
problems: nearest neighbor queries and regression with arbitrary turnstile
updates. We identify key parameters to these problems, such as the number of
$c$-approximate near neighbors and the matrix condition number, and use
different differential privacy techniques to design algorithms returning the
solution vector with memory and time depending on these parameters. We give
algorithms for each of these problems that achieve similar tradeoffs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.05290v1">Big Bird: Privacy Budget Management for W3C's Privacy-Preserving
  Attribution API</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-06-05T17:45:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pierre Tholoniat, Alison Caulfield, Giorgio Cavicchioli, Mark Chen, Nikos Goutzoulias, Benjamin Case, Asaf Cidon, Roxana Geambasu, Mathias Lécuyer, Martin Thomson</p>
    <p><b>Summary:</b> Privacy-preserving advertising APIs like Privacy-Preserving Attribution (PPA)
are designed to enhance web privacy while enabling effective ad measurement.
PPA offers an alternative to cross-site tracking with encrypted reports
governed by differential privacy (DP), but current designs lack a principled
approach to privacy budget management, creating uncertainty around critical
design decisions. We present Big Bird, a privacy budget manager for PPA that
clarifies per-site budget semantics and introduces a global budgeting system
grounded in resource isolation principles. Big Bird enforces utility-preserving
limits via quota budgets and improves global budget utilization through a novel
batched scheduling algorithm. Together, these mechanisms establish a robust
foundation for enforcing privacy protections in adversarial environments. We
implement Big Bird in Firefox and evaluate it on real-world ad data,
demonstrating its resilience and effectiveness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.05101v1">Privacy Amplification Through Synthetic Data: Insights from Linear
  Regression</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-06-05T14:44:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Clément Pierquin, Aurélien Bellet, Marc Tommasi, Matthieu Boussard</p>
    <p><b>Summary:</b> Synthetic data inherits the differential privacy guarantees of the model used
to generate it. Additionally, synthetic data may benefit from privacy
amplification when the generative model is kept hidden. While empirical studies
suggest this phenomenon, a rigorous theoretical understanding is still lacking.
In this paper, we investigate this question through the well-understood
framework of linear regression. First, we establish negative results showing
that if an adversary controls the seed of the generative model, a single
synthetic data point can leak as much information as releasing the model
itself. Conversely, we show that when synthetic data is generated from random
inputs, releasing a limited number of synthetic data points amplifies privacy
beyond the model's inherent guarantees. We believe our findings in linear
regression can serve as a foundation for deriving more general bounds in the
future.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.04978v1">Evaluating the Impact of Privacy-Preserving Federated Learning on CAN
  Intrusion Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-06-05T12:49:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gabriele Digregorio, Elisabetta Cainazzo, Stefano Longari, Michele Carminati, Stefano Zanero</p>
    <p><b>Summary:</b> The challenges derived from the data-intensive nature of machine learning in
conjunction with technologies that enable novel paradigms such as V2X and the
potential offered by 5G communication, allow and justify the deployment of
Federated Learning (FL) solutions in the vehicular intrusion detection domain.
In this paper, we investigate the effects of integrating FL strategies into the
machine learning-based intrusion detection process for on-board vehicular
networks. Accordingly, we propose a FL implementation of a state-of-the-art
Intrusion Detection System (IDS) for Controller Area Network (CAN), based on
LSTM autoencoders. We thoroughly evaluate its detection efficiency and
communication overhead, comparing it to a centralized version of the same
algorithm, thereby presenting it as a feasible solution.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.05421v1">TRIDENT -- A Three-Tier Privacy-Preserving Propaganda Detection Model in
  Mobile Networks using Transformers, Adversarial Learning, and Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-06-05T02:38:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Al Nahian Bin Emran, Dhiman Goswami, Md Hasan Ullah Sadi, Sanchari Das</p>
    <p><b>Summary:</b> The proliferation of propaganda on mobile platforms raises critical concerns
around detection accuracy and user privacy. To address this, we propose TRIDENT
- a three-tier propaganda detection model implementing transformers,
adversarial learning, and differential privacy which integrates syntactic
obfuscation and label perturbation to mitigate privacy leakage while
maintaining propaganda detection accuracy. TRIDENT leverages multilingual
back-translation to introduce semantic variance, character-level noise, and
entity obfuscation for differential privacy enforcement, and combines these
techniques into a unified defense mechanism. Using a binary propaganda
classification dataset, baseline transformer models (BERT, GPT-2) we achieved
F1 scores of 0.89 and 0.90. Applying TRIDENT's third-tier defense yields a
reduced but effective cumulative F1 of 0.83, demonstrating strong privacy
protection across mobile ML deployments with minimal degradation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.04036v1">Privacy and Security Threat for OpenAI GPTs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-06-04T14:58:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wei Wenying, Zhao Kaifa, Xue Lei, Fan Ming</p>
    <p><b>Summary:</b> Large language models (LLMs) demonstrate powerful information handling
capabilities and are widely integrated into chatbot applications. OpenAI
provides a platform for developers to construct custom GPTs, extending
ChatGPT's functions and integrating external services. Since its release in
November 2023, over 3 million custom GPTs have been created. However, such a
vast ecosystem also conceals security and privacy threats. For developers,
instruction leaking attacks threaten the intellectual property of instructions
in custom GPTs through carefully crafted adversarial prompts. For users,
unwanted data access behavior by custom GPTs or integrated third-party services
raises significant privacy concerns. To systematically evaluate the scope of
threats in real-world LLM applications, we develop three phases instruction
leaking attacks target GPTs with different defense level. Our widespread
experiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are
vulnerable to instruction leaking attacks via one or more adversarial prompts,
and half of the remaining GPTs can also be attacked through multiround
conversations. We also developed a framework to assess the effectiveness of
defensive strategies and identify unwanted behaviors in custom GPTs. Our
findings show that 77.5% of custom GPTs with defense strategies are vulnerable
to basic instruction leaking attacks. Additionally, we reveal that 738 custom
GPTs collect user conversational information, and identified 8 GPTs exhibiting
data access behaviors that are unnecessary for their intended functionalities.
Our findings raise awareness among GPT developers about the importance of
integrating specific defensive strategies in their instructions and highlight
users' concerns about data privacy when using LLM-based applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.03870v1">Evaluating Apple Intelligence's Writing Tools for Privacy Against Large
  Language Model-Based Inference Attacks: Insights from Early Datasets</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-06-04T12:01:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohd. Farhan Israk Soumik, Syed Mhamudul Hasan, Abdur R. Shahid</p>
    <p><b>Summary:</b> The misuse of Large Language Models (LLMs) to infer emotions from text for
malicious purposes, known as emotion inference attacks, poses a significant
threat to user privacy. In this paper, we investigate the potential of Apple
Intelligence's writing tools, integrated across iPhone, iPad, and MacBook, to
mitigate these risks through text modifications such as rewriting and tone
adjustment. By developing early novel datasets specifically for this purpose,
we empirically assess how different text modifications influence LLM-based
detection. This capability suggests strong potential for Apple Intelligence's
writing tools as privacy-preserving mechanisms. Our findings lay the groundwork
for future adaptive rewriting systems capable of dynamically neutralizing
sensitive emotional content to enhance user privacy. To the best of our
knowledge, this research provides the first empirical analysis of Apple
Intelligence's text-modification tools within a privacy-preservation context
with the broader goal of developing on-device, user-centric privacy-preserving
mechanisms to protect against LLMs-based advanced inference attacks on deployed
systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.03618v1">GCFL: A Gradient Correction-based Federated Learning Framework for
  Privacy-preserving CPSS</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-06-04T06:52:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiayi Wan, Xiang Zhu, Fanzhen Liu, Wei Fan, Xiaolong Xu</p>
    <p><b>Summary:</b> Federated learning, as a distributed architecture, shows great promise for
applications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the
privacy risks inherent in CPSS, the integration of differential privacy with
federated learning has attracted considerable attention. Existing research
mainly focuses on dynamically adjusting the noise added or discarding certain
gradients to mitigate the noise introduced by differential privacy. However,
these approaches fail to remove the noise that hinders convergence and correct
the gradients affected by the noise, which significantly reduces the accuracy
of model classification. To overcome these challenges, this paper proposes a
novel framework for differentially private federated learning that balances
rigorous privacy guarantees with accuracy by introducing a server-side gradient
correction mechanism. Specifically, after clients perform gradient clipping and
noise perturbation, our framework detects deviations in the noisy local
gradients and employs a projection mechanism to correct them, mitigating the
negative impact of noise. Simultaneously, gradient projection promotes the
alignment of gradients from different clients and guides the model towards
convergence to a global optimum. We evaluate our framework on several benchmark
datasets, and the experimental results demonstrate that it achieves
state-of-the-art performance under the same privacy budget.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.02998v1">A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy
  Question-Answering Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-06-03T15:32:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Đorđe Klisura, Astrid R Bernaga Torres, Anna Karen Gárate-Escamilla, Rajesh Roshan Biswal, Ke Yang, Hilal Pataci, Anthony Rios</p>
    <p><b>Summary:</b> Privacy policies inform users about data collection and usage, yet their
complexity limits accessibility for diverse populations. Existing Privacy
Policy Question Answering (QA) systems exhibit performance disparities across
English dialects, disadvantaging speakers of non-standard varieties. We propose
a novel multi-agent framework inspired by human-centered design principles to
mitigate dialectal biases. Our approach integrates a Dialect Agent, which
translates queries into Standard American English (SAE) while preserving
dialectal intent, and a Privacy Policy Agent, which refines predictions using
domain expertise. Unlike prior approaches, our method does not require
retraining or dialect-specific fine-tuning, making it broadly applicable across
models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves
GPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from
0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without
additional training data. These results highlight the effectiveness of
structured agent collaboration in mitigating dialect biases and underscore the
importance of designing NLP systems that account for linguistic diversity to
ensure equitable access to privacy information.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.02965v2">PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training
  for Mixture-of-Experts LLMs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-06-03T15:00:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ze Yu Zhang, Bolin Ding, Bryan Kian Hsiang Low</p>
    <p><b>Summary:</b> Mixture-of-Experts (MoE) has been gaining popularity due to its successful
adaptation to large language models (LLMs). In this work, we introduce
Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages
the sparsity of the MoE architecture for memory-efficient decentralized
collaborative LLM training, enabling multiple parties with limited GPU-memory
and data resources to collectively train more capable LLMs than they could
achieve individually. At the same time, this approach protects training data
privacy of each participant by keeping training data, as well as parts of the
forward pass signal and gradients locally within each party. By design, PC-MoE
synergistically combines the strengths of distributed computation with strong
confidentiality assurances. Unlike most privacy-preserving schemes, which pay
for confidentiality with lower task accuracy, our framework breaks that
trade-off: across seven popular LLM benchmarks, it almost matches (and
sometimes exceeds) the performance and convergence rate of a fully centralized
model, enjoys near 70% peak GPU RAM reduction, while being fully robust against
reconstruction attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.02725v1">Recursive Privacy-Preserving Estimation Over Markov Fading Channels</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-06-03T10:33:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jie Huang, Fanlin Jia, Xiao He</p>
    <p><b>Summary:</b> In industrial applications, the presence of moving machinery, vehicles, and
personnel, contributes to the dynamic nature of the wireless channel. This time
variability induces channel fading, which can be effectively modeled using a
Markov fading channel (MFC). In this paper, we investigate the problem of
secure state estimation for systems that communicate over a MFC in the presence
of an eavesdropper. The objective is to enable a remote authorized user to
accurately estimate the states of a dynamic system, while considering the
potential interception of the sensor's packet through a wiretap channel. To
prevent information leakage, a novel co-design strategy is established, which
combines a privacy-preserving mechanism with a state estimator. To implement
our encoding scheme, a nonlinear mapping of the innovation is introduced based
on the weighted reconstructed innovation previously received by the legitimate
user. Corresponding to this encoding scheme, we design a recursive
privacy-preserving filtering algorithm to achieve accurate estimation. The
boundedness of estimation error dynamics at the legitimate user's side is
discussed and the divergence of the eavesdropper's estimation error is
analyzed, which demonstrates the effectiveness of our co-design strategy in
ensuring secrecy. Furthermore, a simulation example of a three-tank system is
provided to demonstrate the effectiveness and feasibility of our
privacy-preserving estimation method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.02711v1">Privacy Leaks by Adversaries: Adversarial Iterations for Membership
  Inference Attack</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-06-03T10:09:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jing Xue, Zhishen Sun, Haishan Ye, Luo Luo, Xiangyu Chang, Ivor Tsang, Guang Dai</p>
    <p><b>Summary:</b> Membership inference attack (MIA) has become one of the most widely used and
effective methods for evaluating the privacy risks of machine learning models.
These attacks aim to determine whether a specific sample is part of the model's
training set by analyzing the model's output. While traditional membership
inference attacks focus on leveraging the model's posterior output, such as
confidence on the target sample, we propose IMIA, a novel attack strategy that
utilizes the process of generating adversarial samples to infer membership. We
propose to infer the member properties of the target sample using the number of
iterations required to generate its adversarial sample. We conduct experiments
across multiple models and datasets, and our results demonstrate that the
number of iterations for generating an adversarial sample is a reliable feature
for membership inference, achieving strong performance both in black-box and
white-box attack scenarios. This work provides a new perspective for evaluating
model privacy and highlights the potential of adversarial example-based
features for privacy leakage assessment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.02563v1">Privacy-Preserving Federated Convex Optimization: Balancing
  Partial-Participation and Efficiency via Noise Cancellation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-06-03T07:48:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Roie Reshef, Kfir Yehuda Levy</p>
    <p><b>Summary:</b> This paper tackles the challenge of achieving Differential Privacy (DP) in
Federated Learning (FL) under partial-participation, where only a subset of the
machines participate in each time-step. While previous work achieved optimal
performance in full-participation settings, these methods struggled to extend
to partial-participation scenarios. Our approach fills this gap by introducing
a novel noise-cancellation mechanism that preserves privacy without sacrificing
convergence rates or computational efficiency. We analyze our method within the
Stochastic Convex Optimization (SCO) framework and show that it delivers
optimal performance for both homogeneous and heterogeneous data distributions.
This work expands the applicability of DP in FL, offering an efficient and
practical solution for privacy-preserving learning in distributed systems with
partial participation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.02422v1">Enhancing Convergence, Privacy and Fairness for Wireless Personalized
  Federated Learning: Quantization-Assisted Min-Max Fair Scheduling</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-06-03T04:13:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiyu Zhao, Qimei Cui, Ziqiang Du, Weicai Li, Xi Yu, Wei Ni, Ji Zhang, Xiaofeng Tao, Ping Zhang</p>
    <p><b>Summary:</b> Personalized federated learning (PFL) offers a solution to balancing
personalization and generalization by conducting federated learning (FL) to
guide personalized learning (PL). Little attention has been given to wireless
PFL (WPFL), where privacy concerns arise. Performance fairness of PL models is
another challenge resulting from communication bottlenecks in WPFL. This paper
exploits quantization errors to enhance the privacy of WPFL and proposes a
novel quantization-assisted Gaussian differential privacy (DP) mechanism. We
analyze the convergence upper bounds of individual PL models by considering the
impact of the mechanism (i.e., quantization errors and Gaussian DP noises) and
imperfect communication channels on the FL of WPFL. By minimizing the maximum
of the bounds, we design an optimal transmission scheduling strategy that
yields min-max fairness for WPFL with OFDMA interfaces. This is achieved by
revealing the nested structure of this problem to decouple it into subproblems
solved sequentially for the client selection, channel allocation, and power
control, and for the learning rates and PL-FL weighting coefficients.
Experiments validate our analysis and demonstrate that our approach
substantially outperforms alternative scheduling strategies by 87.08%, 16.21%,
and 38.37% in accuracy, the maximum test loss of participating clients, and
fairness (Jain's index), respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.02410v1">Testing for large-dimensional covariance matrix under differential
  privacy</a></h3>
  
  <p><b>Published on:</b> 2025-06-03T03:53:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shiwei Sang, Yicheng Zeng, Xuehu Zhu, Shurong Zheng</p>
    <p><b>Summary:</b> The increasing prevalence of high-dimensional data across various
applications has raised significant privacy concerns in statistical inference.
In this paper, we propose a differentially private integrated statistic for
testing large-dimensional covariance structures, enabling accurate statistical
insights while safeguarding privacy. First, we analyze the global sensitivity
of sample eigenvalues for sub-Gaussian populations, where our method bypasses
the commonly assumed boundedness of data covariates. For sufficiently large
sample size, the privatized statistic guarantees privacy with high probability.
Furthermore, when the ratio of dimension to sample size, $d/n \to y \in (0,
\infty)$, the privatized test is asymptotically distribution-free with
well-known critical values, and detects the local alternative hypotheses
distinct from the null at the fastest rate of $1/\sqrt{n}$. Extensive numerical
studies on synthetic and real data showcase the validity and powerfulness of
our proposed method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.02156v1">Mitigating Data Poisoning Attacks to Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-06-02T18:37:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaolin Li, Ninghui Li, Boyang Wang, Wenhai Sun</p>
    <p><b>Summary:</b> The distributed nature of local differential privacy (LDP) invites data
poisoning attacks and poses unforeseen threats to the underlying LDP-supported
applications. In this paper, we propose a comprehensive mitigation framework
for popular frequency estimation, which contains a suite of novel defenses,
including malicious user detection, attack pattern recognition, and damaged
utility recovery. In addition to existing attacks, we explore new adaptive
adversarial activities for our mitigation design. For detection, we present a
new method to precisely identify bogus reports and thus LDP aggregation can be
performed over the ``clean'' data. When the attack behavior becomes stealthy
and direct filtering out malicious users is difficult, we further propose a
detection that can effectively recognize hidden adversarial patterns, thus
facilitating the decision-making of service providers. These detection methods
require no additional data and attack information and incur minimal
computational cost. Our experiment demonstrates their excellent performance and
substantial improvement over previous work in various settings. In addition, we
conduct an empirical analysis of LDP post-processing for corrupted data
recovery and propose a new post-processing method, through which we reveal new
insights into protocol recommendations in practice and key design principles
for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.01907v1">SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-06-02T17:27:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yan Zhou, Bradley Malin, Murat Kantarcioglu</p>
    <p><b>Summary:</b> Privacy-preserving data publication, including synthetic data sharing, often
experiences trade-offs between privacy and utility. Synthetic data is generally
more effective than data anonymization in balancing this trade-off, however,
not without its own challenges. Synthetic data produced by generative models
trained on source data may inadvertently reveal information about outliers.
Techniques specifically designed for preserving privacy, such as introducing
noise to satisfy differential privacy, often incur unpredictable and
significant losses in utility. In this work we show that, with the right
mechanism of synthetic data generation, we can achieve strong privacy
protection without significant utility loss. Synthetic data generators
producing contracting data patterns, such as Synthetic Minority Over-sampling
Technique (SMOTE), can enhance a differentially private data generator,
leveraging the strengths of both. We prove in theory and through empirical
demonstration that this SMOTE-DP technique can produce synthetic data that not
only ensures robust privacy protection but maintains utility in downstream
learning tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.01425v1">CSVAR: Enhancing Visual Privacy in Federated Learning via Adaptive
  Shuffling Against Overfitting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-06-02T08:30:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhuo Chen, Zhenya Ma, Yan Zhang, Donghua Cai, Ye Zhang, Qiushi Li, Yongheng Deng, Ye Guo, Ju Ren,  Xuemin,  Shen</p>
    <p><b>Summary:</b> Although federated learning preserves training data within local privacy
domains, the aggregated model parameters may still reveal private
characteristics. This vulnerability stems from clients' limited training data,
which predisposes models to overfitting. Such overfitting enables models to
memorize distinctive patterns from training samples, thereby amplifying the
success probability of privacy attacks like membership inference. To enhance
visual privacy protection in FL, we present CSVAR(Channel-Wise Spatial Image
Shuffling with Variance-Guided Adaptive Region Partitioning), a novel image
shuffling framework to generate obfuscated images for secure data transmission
and each training epoch, addressing both overfitting-induced privacy leaks and
raw image transmission risks. CSVAR adopts region-variance as the metric to
measure visual privacy sensitivity across image regions. Guided by this, CSVAR
adaptively partitions each region into multiple blocks, applying fine-grained
partitioning to privacy-sensitive regions with high region-variances for
enhancing visual privacy protection and coarse-grained partitioning to
privacy-insensitive regions for balancing model utility. In each region, CSVAR
then shuffles between blocks in both the spatial domains and chromatic channels
to hide visual spatial features and disrupt color distribution. Experimental
evaluations conducted on diverse real-world datasets demonstrate that CSVAR is
capable of generating visually obfuscated images that exhibit high perceptual
ambiguity to human eyes, simultaneously mitigating the effectiveness of
adversarial data reconstruction attacks and achieving a good trade-off between
visual privacy protection and model utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.01325v1">Understanding the Identity-Transformation Approach in OIDC-Compatible
  Privacy-Preserving SSO Services</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-06-02T05:11:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jingqiang Lin, Baitao Zhang, Wei Wang, Quanwei Cai, Jiwu Jing, Huiyang He</p>
    <p><b>Summary:</b> OpenID Connect (OIDC) enables a user with commercial-off-the-shelf browsers
to log into multiple websites, called relying parties (RPs), by her username
and credential set up in another trusted web system, called the identity
provider (IdP). Identity transformations are proposed in UppreSSO to provide
OIDC-compatible SSO services, preventing both IdP-based login tracing and
RP-based identity linkage. While security and privacy of SSO services in
UppreSSO have been proved, several essential issues of this
identity-transformation approach are not well studied. In this paper, we
comprehensively investigate the approach as below. Firstly, several suggestions
for the efficient integration of identity transformations in OIDC-compatible
SSO are explained. Then, we uncover the relationship between
identity-transformations in SSO and oblivious pseudo-random functions (OPRFs),
and present two variations of the properties required for SSO security as well
as the privacy requirements, to analyze existing OPRF protocols. Finally, new
identity transformations different from those designed in UppreSSO, are
constructed based on OPRFs, satisfying different variations of SSO security
requirements. To the best of our knowledge, this is the first time to uncover
the relationship between identity transformations in OIDC-compatible
privacy-preserving SSO services and OPRFs, and prove the SSO-related properties
(i.e., key-identifier freeness, RP designation and user identification) of OPRF
protocols, in addition to the basic properties of correctness, obliviousness
and pseudo-randomness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.02063v1">Privacy-Aware, Public-Aligned: Embedding Risk Detection and Public
  Values into Scalable Clinical Text De-Identification for Trusted Research
  Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-06-01T17:45:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Arlene Casey, Stuart Dunbar, Franz Gruber, Samuel McInerney, Matúš Falis, Pamela Linksted, Katie Wilde, Kathy Harrison, Alison Hamilton, Christian Cole</p>
    <p><b>Summary:</b> Clinical free-text data offers immense potential to improve population health
research such as richer phenotyping, symptom tracking, and contextual
understanding of patient care. However, these data present significant privacy
risks due to the presence of directly or indirectly identifying information
embedded in unstructured narratives. While numerous de-identification tools
have been developed, few have been tested on real-world, heterogeneous datasets
at scale or assessed for governance readiness. In this paper, we synthesise our
findings from previous studies examining the privacy-risk landscape across
multiple document types and NHS data providers in Scotland. We characterise how
direct and indirect identifiers vary by record type, clinical setting, and data
flow, and show how changes in documentation practice can degrade model
performance over time. Through public engagement, we explore societal
expectations around the safe use of clinical free text and reflect these in the
design of a prototype privacy-risk management tool to support transparent,
auditable decision-making. Our findings highlight that privacy risk is
context-dependent and cumulative, underscoring the need for adaptable, hybrid
de-identification approaches that combine rule-based precision with contextual
understanding. We offer a comprehensive view of the challenges and
opportunities for safe, scalable reuse of clinical free-text within Trusted
Research Environments and beyond, grounded in both technical evidence and
public perspectives on responsible data use.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.01072v1">IDCloak: A Practical Secure Multi-party Dataset Join Framework for
  Vertical Privacy-preserving Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-06-01T16:20:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuyu Chen, Guopeng Lin, Haoyu Niu, Lushan Song, Chengxun Hong, Weili Han</p>
    <p><b>Summary:</b> Vertical privacy-preserving machine learning (vPPML) enables multiple parties
to train models on their vertically distributed datasets while keeping datasets
private. In vPPML, it is critical to perform the secure dataset join, which
aligns features corresponding to intersection IDs across datasets and forms a
secret-shared and joint training dataset. However, existing methods for this
step could be impractical due to: (1) they are insecure when they expose
intersection IDs; or (2) they rely on a strong trust assumption requiring a
non-colluding auxiliary server; or (3) they are limited to the two-party
setting.
  This paper proposes IDCloak, the first practical secure multi-party dataset
join framework for vPPML that keeps IDs private without a non-colluding
auxiliary server. IDCloak consists of two protocols: (1) a circuit-based
multi-party private set intersection protocol (cmPSI), which obtains
secret-shared flags indicating intersection IDs via an optimized communication
structure combining OKVS and OPRF; (2) a secure multi-party feature alignment
protocol, which obtains the secret-shared and joint dataset using secret-shared
flags, via our proposed efficient secure shuffle protocol. Experiments show
that: (1) compared to the state-of-the-art secure two-party dataset join
framework (iPrivjoin), IDCloak demonstrates higher efficiency in the two-party
setting and comparable performance when the party number increases; (2)
compared to the state-of-the-art cmPSI protocol under honest majority, our
proposed cmPSI protocol provides a stronger security guarantee (dishonest
majority) while improving efficiency by up to $7.78\times$ in time and
$8.73\times$ in communication sizes; (3) our proposed secure shuffle protocol
outperforms the state-of-the-art shuffle protocol by up to $138.34\times$ in
time and $132.13\times$ in communication sizes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.00759v1">Understanding and Mitigating Cross-lingual Privacy Leakage via
  Language-specific and Universal Privacy Neurons</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-06-01T00:10:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenshuo Dong, Qingsong Yang, Shu Yang, Lijie Hu, Meng Ding, Wanyu Lin, Tianhang Zheng, Di Wang</p>
    <p><b>Summary:</b> Large Language Models (LLMs) trained on massive data capture rich information
embedded in the training data. However, this also introduces the risk of
privacy leakage, particularly involving personally identifiable information
(PII). Although previous studies have shown that this risk can be mitigated
through methods such as privacy neurons, they all assume that both the
(sensitive) training data and user queries are in English. We show that they
cannot defend against the privacy leakage in cross-lingual contexts: even if
the training data is exclusively in one language, these (private) models may
still reveal private information when queried in another language. In this
work, we first investigate the information flow of cross-lingual privacy
leakage to give a better understanding. We find that LLMs process private
information in the middle layers, where representations are largely shared
across languages. The risk of leakage peaks when converted to a
language-specific space in later layers. Based on this, we identify
privacy-universal neurons and language-specific privacy neurons.
Privacy-universal neurons influence privacy leakage across all languages, while
language-specific privacy neurons are only related to specific languages. By
deactivating these neurons, the cross-lingual privacy leakage risk is reduced
by 23.3%-31.6%.</p>
  </details>
</div>



<h2>2025-05</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.00745v1">Controlling the Spread of Epidemics on Networks with Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computational Engineering, Finance, and Science-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-05-31T23:17:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dung Nguyen, Aravind Srinivasan, Renata Valieva, Anil Vullikanti, Jiayi Wu</p>
    <p><b>Summary:</b> Designing effective strategies for controlling epidemic spread by vaccination
is an important question in epidemiology, especially in the early stages when
vaccines are limited. This is a challenging question when the contact network
is very heterogeneous, and strategies based on controlling network properties,
such as the degree and spectral radius, have been shown to be effective.
Implementation of such strategies requires detailed information on the contact
structure, which might be sensitive in many applications. Our focus here is on
choosing effective vaccination strategies when the edges are sensitive and
differential privacy guarantees are needed. Our main contributions are
$(\varepsilon,\delta)$-differentially private algorithms for designing
vaccination strategies by reducing the maximum degree and spectral radius. Our
key technique is a private algorithm for the multi-set multi-cover problem,
which we use for controlling network properties. We evaluate privacy-utility
tradeoffs of our algorithms on multiple synthetic and real-world networks, and
show their effectiveness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.00660v1">Differential Privacy for Deep Learning in Medicine</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-31T18:03:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Marziyeh Mohammadi, Mohsen Vejdanihemmat, Mahshad Lotfinia, Mirabela Rusu, Daniel Truhn, Andreas Maier, Soroosh Tayebi Arasteh</p>
    <p><b>Summary:</b> Differential privacy (DP) is a key technique for protecting sensitive patient
data in medical deep learning (DL). As clinical models grow more
data-dependent, balancing privacy with utility and fairness has become a
critical challenge. This scoping review synthesizes recent developments in
applying DP to medical DL, with a particular focus on DP-SGD and alternative
mechanisms across centralized and federated settings. Using a structured search
strategy, we identified 74 studies published up to March 2025. Our analysis
spans diverse data modalities, training setups, and downstream tasks, and
highlights the tradeoffs between privacy guarantees, model accuracy, and
subgroup fairness. We find that while DP-especially at strong privacy
budgets-can preserve performance in well-structured imaging tasks, severe
degradation often occurs under strict privacy, particularly in underrepresented
or complex modalities. Furthermore, privacy-induced performance gaps
disproportionately affect demographic subgroups, with fairness impacts varying
by data type and task. A small subset of studies explicitly addresses these
tradeoffs through subgroup analysis or fairness metrics, but most omit them
entirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms,
generative models, and hybrid federated designs, though reporting remains
inconsistent. We conclude by outlining key gaps in fairness auditing,
standardization, and evaluation protocols, offering guidance for future work
toward equitable and clinically robust privacy-preserving DL systems in
medicine.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.00476v1">Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet -- A
  ResNet-based Model Classification Dataset</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-31T08:53:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abhisek Ray, Lukas Esterle</p>
    <p><b>Summary:</b> Federated Learning (FL) has emerged as a powerful paradigm for training
machine learning models across distributed data sources while preserving data
locality. However, the privacy of local data is always a pivotal concern and
has received a lot of attention in recent research on the FL regime. Moreover,
the lack of domain heterogeneity and client-specific segregation in the
benchmarks remains a critical bottleneck for rigorous evaluation. In this
paper, we introduce ModelNet, a novel image classification dataset constructed
from the embeddings extracted from a pre-trained ResNet50 model. First, we
modify the CIFAR100 dataset into three client-specific variants, considering
three domain heterogeneities (homogeneous, heterogeneous, and random).
Subsequently, we train each client-specific subset of all three variants on the
pre-trained ResNet50 model to save model parameters. In addition to
multi-domain image data, we propose a new hypothesis to define the FL algorithm
that can access the anonymized model parameters to preserve the local privacy
in a more effective manner compared to existing ones. ModelNet is designed to
simulate realistic FL settings by incorporating non-IID data distributions and
client diversity design principles in the mainframe for both conventional and
futuristic graph-driven FL algorithms. The three variants are ModelNet-S,
ModelNet-D, and ModelNet-R, which are based on homogeneous, heterogeneous, and
random data settings, respectively. To the best of our knowledge, we are the
first to propose a cross-environment client-specific FL dataset along with the
graph-based variant. Extensive experiments based on domain shifts and
aggregation strategies show the effectiveness of the above variants, making it
a practical benchmark for classical and graph-based FL research. The dataset
and related code are available online.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.02038v1">Blockchain Powered Edge Intelligence for U-Healthcare in Privacy
  Critical and Time Sensitive Environment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-31T06:58:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anum Nawaz, Hafiz Humza Mahmood Ramzan, Xianjia Yu, Zhuo Zou, Tomi Westerlund</p>
    <p><b>Summary:</b> Edge Intelligence (EI) serves as a critical enabler for privacy-preserving
systems by providing AI-empowered computation and distributed caching services
at the edge, thereby minimizing latency and enhancing data privacy. The
integration of blockchain technology further augments EI frameworks by ensuring
transactional transparency, auditability, and system-wide reliability through a
decentralized network model. However, the operational architecture of such
systems introduces inherent vulnerabilities, particularly due to the extensive
data interactions between edge gateways (EGs) and the distributed nature of
information storage during service provisioning. To address these challenges,
we propose an autonomous computing model along with its interaction topologies
tailored for privacy-critical and time-sensitive health applications. The
system supports continuous monitoring, real-time alert notifications, disease
detection, and robust data processing and aggregation. It also includes a data
transaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a
resource-efficient one-dimensional convolutional neural network (1D-CNN) is
proposed for the multiclass classification of arrhythmia, enabling accurate and
real-time analysis of constrained EGs. Furthermore, a secure access scheme is
defined to manage both off-chain and on-chain data sharing and storage. To
validate the proposed model, comprehensive security, performance, and cost
analyses are conducted, demonstrating the efficiency and reliability of the
fine-grained access control scheme.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.00416v1">Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge
  Learning in Personalized Healthcare</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-05-31T06:41:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anum Nawaz, Muhammad Irfan, Xianjia Yu, Zhuo Zou, Tomi Westerlund</p>
    <p><b>Summary:</b> Federated learning (FL) has attracted increasing attention to mitigate
security and privacy challenges in traditional cloud-centric machine learning
models specifically in healthcare ecosystems. FL methodologies enable the
training of global models through localized policies, allowing independent
operations at the edge clients' level. Conventional first-order FL approaches
face several challenges in personalized model training due to heterogeneous
non-independent and identically distributed (non-iid) data of each edge client.
Recently, second-order FL approaches maintain the stability and consistency of
non-iid datasets while improving personalized model training. This study
proposes and develops a verifiable and auditable optimized second-order FL
framework BFEL (blockchain-enhanced federated edge learning) based on optimized
FedCurv for personalized healthcare systems. FedCurv incorporates information
about the importance of each parameter to each client's task (through Fisher
Information Matrix) which helps to preserve client-specific knowledge and
reduce model drift during aggregation. Moreover, it minimizes communication
rounds required to achieve a target precision convergence for each edge client
while effectively managing personalized training on non-iid and heterogeneous
data. The incorporation of Ethereum-based model aggregation ensures trust,
verifiability, and auditability while public key encryption enhances privacy
and security. Experimental results of federated CNNs and MLPs utilizing Mnist,
Cifar-10, and PathMnist demonstrate the high efficiency and scalability of the
proposed framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.00158v1">Privacy Amplification in Differentially Private Zeroth-Order
  Optimization with Hidden States</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-30T18:55:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Eli Chien, Wei-Ning Chen, Pan Li</p>
    <p><b>Summary:</b> Zeroth-order optimization has emerged as a promising approach for fine-tuning
large language models on domain-specific data, particularly under differential
privacy (DP) and memory constraints. While first-order methods have been
extensively studied from a privacy perspective, the privacy analysis and
algorithmic design for zeroth-order methods remain significantly underexplored.
A critical open question concerns hidden-state DP analysis: although convergent
privacy bounds are known for first-order methods, it has remained unclear
whether similar guarantees can be established for zeroth-order methods. In this
work, we provide an affirmative answer by proving a convergent DP bound for
zeroth-order optimization. Our analysis generalizes the celebrated privacy
amplification-by-iteration framework to the setting of smooth loss functions in
zeroth-order optimization. Furthermore, it induces better DP zeroth-order
algorithmic designs that are previously unknown to the literature.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.00124v1">randextract: a Reference Library to Test and Validate Privacy
  Amplification Implementations</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-30T18:01:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Iyán Méndez Veiga, Esther Hänggi</p>
    <p><b>Summary:</b> Quantum cryptographic protocols do not rely only on quantum-physical
resources, they also require reliable classical communication and computation.
In particular, the secrecy of any quantum key distribution protocol critically
depends on the correct execution of the privacy amplification step. This is a
classical post-processing procedure transforming a partially secret bit string,
known to be somewhat correlated with an adversary, into a shorter bit string
that is close to uniform and independent of the adversary's knowledge. It is
typically implemented using randomness extractors. Standardization efforts in
quantum cryptography have focused on the security of physical devices and
quantum operations. Future efforts should also consider all algorithms used in
classical post-processing, especially in privacy amplification, due to its
critical role in ensuring the final security of the key. We present
randextract, a reference library to test and validate privacy amplification
implementations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.24603v2">The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian
  Sketches</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-30T13:52:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Omri Lev, Vishwak Srinivasan, Moshe Shenfeld, Katrina Ligett, Ayush Sekhari, Ashia C. Wilson</p>
    <p><b>Summary:</b> Gaussian sketching, which consists of pre-multiplying the data with a random
Gaussian matrix, is a widely used technique for multiple problems in data
science and machine learning, with applications spanning computationally
efficient optimization, coded computing, and federated learning. This operation
also provides differential privacy guarantees due to its inherent randomness.
In this work, we revisit this operation through the lens of Renyi Differential
Privacy (RDP), providing a refined privacy analysis that yields significantly
tighter bounds than prior results. We then demonstrate how this improved
analysis leads to performance improvement in different linear regression
settings, establishing theoretical utility guarantees. Empirically, our methods
improve performance across multiple datasets and, in several cases, reduce
runtime.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.00100v2">Children's Voice Privacy: First Steps And Emerging Challenges</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-30T13:21:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ajinkya Kulkarni, Francisco Teixeira, Enno Hermann, Thomas Rolland, Isabel Trancoso, Mathew Magimai Doss</p>
    <p><b>Summary:</b> Children are one of the most under-represented groups in speech technologies,
as well as one of the most vulnerable in terms of privacy. Despite this,
anonymization techniques targeting this population have received little
attention. In this study, we seek to bridge this gap, and establish a baseline
for the use of voice anonymization techniques designed for adult speech when
applied to children's voices. Such an evaluation is essential, as children's
speech presents a distinct set of challenges when compared to that of adults.
This study comprises three children's datasets, six anonymization methods, and
objective and subjective utility metrics for evaluation. Our results show that
existing systems for adults are still able to protect children's voice privacy,
but suffer from much higher utility degradation. In addition, our subjective
study displays the challenges of automatic evaluation methods for speech
quality in children's speech, highlighting the need for further research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.02030v1">Adaptive Privacy-Preserving SSD</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-05-30T13:08:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Na Young Ahn, Dong Hoon Lee</p>
    <p><b>Summary:</b> Data remanence in NAND flash complicates complete deletion on IoT SSDs. We
design an adaptive architecture offering four privacy levels (PL0-PL3) that
select among address, data, and parity deletion techniques. Quantitative
analysis balances efficacy, latency, endurance, and cost. Machine-learning
adjusts levels contextually, boosting privacy with negligible performance
overhead and complexity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.23655v3">Keyed Chaotic Dynamics for Privacy-Preserving Neural Inference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">  
  <p><b>Published on:</b> 2025-05-29T17:05:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peter David Fagan</p>
    <p><b>Summary:</b> Neural network inference typically operates on raw input data, increasing the
risk of exposure during preprocessing and inference. Moreover, neural
architectures lack efficient built-in mechanisms for directly authenticating
input data. This work introduces a novel encryption method for ensuring the
security of neural inference. By constructing key-conditioned chaotic graph
dynamical systems, we enable the encryption and decryption of real-valued
tensors within the neural architecture. The proposed dynamical systems are
particularly suited to encryption due to their sensitivity to initial
conditions and their capacity to produce complex, key-dependent nonlinear
transformations from compact rules. This work establishes a paradigm for
securing neural inference and opens new avenues for research on the application
of graph dynamical systems in neural network security.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2506.00060v1">Comparative analysis of privacy-preserving open-source LLMs regarding
  extraction of diagnostic information from clinical CMR imaging reports</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-29T11:25:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sina Amirrajab, Volker Vehof, Michael Bietenbeck, Ali Yilmaz</p>
    <p><b>Summary:</b> Purpose: We investigated the utilization of privacy-preserving,
locally-deployed, open-source Large Language Models (LLMs) to extract
diagnostic information from free-text cardiovascular magnetic resonance (CMR)
reports. Materials and Methods: We evaluated nine open-source LLMs on their
ability to identify diagnoses and classify patients into various cardiac
diagnostic categories based on descriptive findings in 109 clinical CMR
reports. Performance was quantified using standard classification metrics
including accuracy, precision, recall, and F1 score. We also employed confusion
matrices to examine patterns of misclassification across models. Results: Most
open-source LLMs demonstrated exceptional performance in classifying reports
into different diagnostic categories. Google's Gemma2 model achieved the
highest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B
with F1 scores of 0.96 and 0.95, respectively. All other evaluated models
attained average scores above 0.93, with Mistral and DeepseekR1-7B being the
only exceptions. The top four LLMs outperformed our board-certified
cardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR
reports. Conclusion: Our findings demonstrate the feasibility of implementing
open-source, privacy-preserving LLMs in clinical settings for automated
analysis of imaging reports, enabling accurate, fast and resource-efficient
diagnostic categorization.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.23031v1">Towards Privacy-Preserving Fine-Grained Visual Classification via
  Hierarchical Learning from Label Proportions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-29T03:18:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinyi Chang, Dongliang Chang, Lei Chen, Bingyao Yu, Zhanyu Ma</p>
    <p><b>Summary:</b> In recent years, Fine-Grained Visual Classification (FGVC) has achieved
impressive recognition accuracy, despite minimal inter-class variations.
However, existing methods heavily rely on instance-level labels, making them
impractical in privacy-sensitive scenarios such as medical image analysis. This
paper aims to enable accurate fine-grained recognition without direct access to
instance labels. To achieve this, we leverage the Learning from Label
Proportions (LLP) paradigm, which requires only bag-level labels for efficient
training. Unlike existing LLP-based methods, our framework explicitly exploits
the hierarchical nature of fine-grained datasets, enabling progressive feature
granularity refinement and improving classification accuracy. We propose
Learning from Hierarchical Fine-Grained Label Proportions (LHFGLP), a framework
that incorporates Unrolled Hierarchical Fine-Grained Sparse Dictionary
Learning, transforming handcrafted iterative approximation into learnable
network optimization. Additionally, our proposed Hierarchical Proportion Loss
provides hierarchical supervision, further enhancing classification
performance. Experiments on three widely-used fine-grained datasets, structured
in a bag-based manner, demonstrate that our framework consistently outperforms
existing LLP-based methods. We will release our code and datasets to foster
further research in privacy-preserving fine-grained classification.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.23849v1">CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving
  Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-28T21:24:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kaveen Hiniduma, Zilinghan Li, Aditya Sinha, Ravi Madduri, Suren Byna</p>
    <p><b>Summary:</b> Privacy-Preserving Federated Learning (PPFL) is a decentralized machine
learning approach where multiple clients train a model collaboratively. PPFL
preserves privacy and security of the client's data by not exchanging it.
However, ensuring that data at each client is of high quality and ready for
federated learning (FL) is a challenge due to restricted data access. In this
paper, we introduce CADRE (Customizable Assurance of Data REadiness) for FL, a
novel framework that allows users to define custom data readiness (DR)
standards, metrics, rules, and remedies tailored to specific FL tasks. Our
framework generates comprehensive DR reports based on the user-defined metrics,
rules, and remedies to ensure datasets are optimally prepared for FL while
preserving privacy. We demonstrate the framework's practical application by
integrating it into an existing PPFL framework. We conducted experiments across
six diverse datasets, addressing seven different DR issues. The results
illustrate the framework's versatility and effectiveness in ensuring DR across
various dimensions, including data quality, privacy, and fairness. This
approach enhances the performance and reliability of FL models as well as
utilizes valuable resources by identifying and addressing data-related issues
before the training phase.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.22447v1">Privacy-preserving Prompt Personalization in Federated Learning for
  Multimodal Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-28T15:09:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sizai Hou, Songze Li, Baturalp Buyukates</p>
    <p><b>Summary:</b> Prompt learning is a crucial technique for adapting pre-trained multimodal
language models (MLLMs) to user tasks. Federated prompt personalization (FPP)
is further developed to address data heterogeneity and local overfitting,
however, it exposes personalized prompts - valuable intellectual assets - to
privacy risks like prompt stealing or membership inference attacks.
Widely-adopted techniques like differential privacy add noise to prompts,
whereas degrading personalization performance. We propose SecFPP, a secure FPP
protocol harmonizing generalization, personalization, and privacy guarantees.
SecFPP employs hierarchical prompt adaptation with domain-level and class-level
components to handle multi-granular data imbalance. For privacy, it uses a
novel secret-sharing-based adaptive clustering algorithm for domain-level
adaptation while keeping class-level components private. While theoretically
and empirically secure, SecFPP achieves state-of-the-art accuracy under severe
heterogeneity in data distribution. Extensive experiments show it significantly
outperforms both non-private and privacy-preserving baselines, offering a
superior privacy-performance trade-off.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.22234v1">Evolution of repositories and privacy laws: commit activities in the
  GDPR and CCPA era</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-05-28T11:10:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Georgia M. Kapitsaki, Maria Papoutsoglou</p>
    <p><b>Summary:</b> Free and open source software has gained a lot of momentum in the industry
and the research community. The latest advances in privacy legislation,
including the EU General Data Protection Regulation (GDPR) and the California
Consumer Privacy Act (CCPA), have forced the community to pay special attention
to users' data privacy. The main aim of this work is to examine software
repositories that are acting on privacy laws. We have collected commit data
from GitHub repositories in order to understand indications on main data
privacy laws (GDPR, CCPA, CPRA, UK DPA) in the last years. Via an automated
process, we analyzed 37,213 commits from 12,391 repositories since 2016,
whereas 594 commits from the 70 most popular repositories of the dataset were
manually analyzed. We observe that most commits were performed on the year the
law came into effect and privacy relevant terms appear in the commit messages,
whereas reference to specific data privacy user rights is scarce. The study
showed that more educational activities on data privacy user rights are needed,
as well as tools for privacy recommendations, whereas verifying actual
compliance via source code execution is a useful direction for software
engineering researchers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.22061v1">Safeguarding Privacy of Retrieval Data against Membership Inference
  Attacks: Is This Query Too Close to Home?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-28T07:35:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yujin Choi, Youngjoo Park, Junyoung Byun, Jaewook Lee, Jinseong Park</p>
    <p><b>Summary:</b> Retrieval-augmented generation (RAG) mitigates the hallucination problem in
large language models (LLMs) and has proven effective for specific,
personalized applications. However, passing private retrieved documents
directly to LLMs introduces vulnerability to membership inference attacks
(MIAs), which try to determine whether the target datum exists in the private
external database or not. Based on the insight that MIA queries typically
exhibit high similarity to only one target document, we introduce Mirabel, a
similarity-based MIA detection framework designed for the RAG system. With the
proposed Mirabel, we show that simple detect-and-hide strategies can
successfully obfuscate attackers, maintain data utility, and remain
system-agnostic. We experimentally prove its detection and defense against
various state-of-the-art MIA methods and its adaptability to existing private
RAG systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.23825v1">Privacy-Preserving Inconsistency Measurement</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-28T06:24:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Carl Corea, Timotheus Kampik, Nico Potyka</p>
    <p><b>Summary:</b> We investigate a new form of (privacy-preserving) inconsistency measurement
for multi-party communication. Intuitively, for two knowledge bases K_A, K_B
(of two agents A, B), our results allow to quantitatively assess the degree of
inconsistency for K_A U K_B without having to reveal the actual contents of the
knowledge bases. Using secure multi-party computation (SMPC) and cryptographic
protocols, we develop two concrete methods for this use-case and show that they
satisfy important properties of SMPC protocols -- notably, input privacy, i.e.,
jointly computing the inconsistency degree without revealing the inputs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.21801v2">Query, Don't Train: Privacy-Preserving Tabular Prediction from EHR Data
  via SQL Queries</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-05-27T22:16:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Josefa Lia Stoisser, Marc Boubnovski Martell, Kaspar Märtens, Lawrence Phillips, Stephen Michael Town, Rory Donovan-Maiye, Julien Fauqueur</p>
    <p><b>Summary:</b> Electronic health records (EHRs) contain richly structured, longitudinal data
essential for predictive modeling, yet stringent privacy regulations (e.g.,
HIPAA, GDPR) often restrict access to individual-level records. We introduce
Query, Don't Train (QDT): a structured-data foundation-model interface enabling
tabular inference via LLM-generated SQL over EHRs. Instead of training on or
accessing individual-level examples, QDT uses a large language model (LLM) as a
schema-aware query planner to generate privacy-compliant SQL queries from a
natural language task description and a test-time input. The model then
extracts summary-level population statistics through these SQL queries and the
LLM performs, chain-of-thought reasoning over the results to make predictions.
This inference-time-only approach (1) eliminates the need for supervised model
training or direct data access, (2) ensures interpretability through symbolic,
auditable queries, (3) naturally handles missing features without imputation or
preprocessing, and (4) effectively manages high-dimensional numerical data to
enhance analytical capabilities. We validate QDT on the task of 30-day hospital
readmission prediction for Type 2 diabetes patients using a MIMIC-style EHR
cohort, achieving F1 = 0.70, which outperforms TabPFN (F1 = 0.68). To our
knowledge, this is the first demonstration of LLM-driven, privacy-preserving
structured prediction using only schema metadata and aggregate statistics -
offering a scalable, interpretable, and regulation-compliant alternative to
conventional foundation-model pipelines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.21715v1">Privacy-Preserving Chest X-ray Report Generation via Multimodal
  Federated Learning with ViT and GPT-2</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-27T20:01:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md. Zahid Hossain, Mustofa Ahmed, Most. Sharmin Sultana Samu, Md. Rakibul Islam</p>
    <p><b>Summary:</b> The automated generation of radiology reports from chest X-ray images holds
significant promise in enhancing diagnostic workflows while preserving patient
privacy. Traditional centralized approaches often require sensitive data
transfer, posing privacy concerns. To address this, the study proposes a
Multimodal Federated Learning framework for chest X-ray report generation using
the IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the
encoder and GPT-2 as the report generator, enabling decentralized training
without sharing raw data. Three Federated Learning (FL) aggregation strategies:
FedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)
were evaluated. Among these, Krum Aggregation demonstrated superior performance
across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore
and RaTEScore. The results show that FL can match or surpass centralized models
in generating clinically relevant and semantically rich radiology reports. This
lightweight and privacy-preserving framework paves the way for collaborative
medical AI development without compromising data confidentiality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.21008v1">A Hitchhiker's Guide to Privacy-Preserving Cryptocurrencies: A Survey on
  Anonymity, Confidentiality, and Auditability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-05-27T10:42:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Matteo Nardelli, Francesco De Sclavis, Michela Iezzi</p>
    <p><b>Summary:</b> Cryptocurrencies and central bank digital currencies (CBDCs) are reshaping
the monetary landscape, offering transparency and efficiency while raising
critical concerns about user privacy and regulatory compliance. This survey
provides a comprehensive and technically grounded overview of
privacy-preserving digital currencies, covering both cryptocurrencies and
CBDCs. We propose a taxonomy of privacy goals -- including anonymity,
confidentiality, unlinkability, and auditability -- and map them to underlying
cryptographic primitives, protocol mechanisms, and system architectures. Unlike
previous surveys, our work adopts a design-oriented perspective, linking
high-level privacy objectives to concrete implementations. We also trace the
evolution of privacy-preserving currencies through three generations,
highlighting shifts from basic anonymity guarantees toward more nuanced
privacy-accountability trade-offs. Finally, we identify open challenges at the
intersection of cryptography, distributed systems, and policy definition, which
motivate further investigation into the primitives and design of digital
currencies that balance real-world privacy and auditability needs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.20916v1">Imago Obscura: An Image Privacy AI Co-pilot to Enable Identification and
  Mitigation of Risks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> 
  <p><b>Published on:</b> 2025-05-27T09:08:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kyzyl Monteiro, Yuchen Wu, Sauvik Das</p>
    <p><b>Summary:</b> Users often struggle to navigate the privacy / publicity boundary in sharing
images online: they may lack awareness of image privacy risks and/or the
ability to apply effective mitigation strategies. To address this challenge, we
introduce and evaluate Imago Obscura, an AI-powered, image-editing copilot that
enables users to identify and mitigate privacy risks with images they intend to
share. Driven by design requirements from a formative user study with 7
image-editing experts, Imago Obscura enables users to articulate their
image-sharing intent and privacy concerns. The system uses these inputs to
surface contextually pertinent privacy risks, and then recommends and
facilitates application of a suite of obfuscation techniques found to be
effective in prior literature -- e.g., inpainting, blurring, and generative
content replacement. We evaluated Imago Obscura with 15 end-users in a lab
study and found that it greatly improved users' awareness of image privacy
risks and their ability to address those risks, allowing them to make more
informed sharing decisions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.20910v1">Automated Privacy Information Annotation in Large Language Model
  Interactions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-27T09:00:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hang Zeng, Xiangyu Liu, Yong Hu, Chaoyue Niu, Fan Wu, Shaojie Tang, Guihai Chen</p>
    <p><b>Summary:</b> Users interacting with large language models (LLMs) under their real
identifiers often unknowingly risk disclosing private information.
Automatically notifying users whether their queries leak privacy and which
phrases leak what private information has therefore become a practical need.
Existing privacy detection methods, however, were designed for different
objectives and application scenarios, typically tagging personally identifiable
information (PII) in anonymous content. In this work, to support the
development and evaluation of privacy detection models for LLM interactions
that are deployable on local user devices, we construct a large-scale
multilingual dataset with 249K user queries and 154K annotated privacy phrases.
In particular, we build an automated privacy annotation pipeline with
cloud-based strong LLMs to automatically extract privacy phrases from dialogue
datasets and annotate leaked information. We also design evaluation metrics at
the levels of privacy leakage, extracted privacy phrase, and privacy
information. We further establish baseline methods using light-weight LLMs with
both tuning-free and tuning-based methods, and report a comprehensive
evaluation of their performance. Evaluation results reveal a gap between
current performance and the requirements of real-world LLM applications,
motivating future research into more effective local privacy detection methods
grounded in our dataset.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.20577v1">Privacy-Preserving Peer-to-Peer Energy Trading via Hybrid Secure
  Computations</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E">
  <p><b>Published on:</b> 2025-05-26T23:24:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Junhong Liu, Qinfei Long, Rong-Peng Liu, Wenjie Liu, Xin Cui, Yunhe Hou</p>
    <p><b>Summary:</b> The massive integration of uncertain distributed renewable energy resources
into power systems raises power imbalance concerns. Peer-to-peer (P2P) energy
trading provides a promising way to balance the prosumers' volatile energy
power generation and demands locally. Particularly, to protect the privacy of
prosumers, distributed P2P energy trading is broadly advocated. However, severe
privacy leakage issues can emerge in the realistic fully distributed P2P energy
trading paradigm. Meanwhile, in this paradigm, two-party and multi-party
computations coexist, challenging the naive privacy-preserving techniques. To
tackle privacy leakage issues arising from the fully distributed P2P energy
trading, this paper proposes a privacy-preserving approach via hybrid secure
computations. A secure multi-party computation mechanism consisting of offline
and online phases is developed to ensure the security of shared data by
leveraging the tailored secret sharing method. In addition, the Paillier
encryption method based on the Chinese Remainder Theorem is proposed for both
the secure two-party computation and the offline phase of the multi-party
computation. The random encryption coefficient is designed to enhance the
security of the two-party computation and simultaneously guarantee the
convergence of the distributed optimization. The feasible range for the
encryption coefficient is derived with a strict mathematical proof. Numerical
simulations demonstrate the exactness, effectiveness, and scalability of the
proposed privacy-preserving approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.20575v1">Synergising Hierarchical Data Centers and Power Networks: A
  Privacy-Preserving Approach</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-05-26T23:22:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Junhong Liu, Fei Teng, Yunhe Hou</p>
    <p><b>Summary:</b> In the era of digitization, data centers have emerged as integral
contributors sustaining our interlinked world, bearing responsibility for an
increasing proportion of the world's energy consumption. To facilitate the
their fast rollout while progressing towards net-zero energy systems, the
synergy of hierarchical data centers (cloud-fog-edge) and power networks can
play a pivotal role. However, existing centralized co-dispatch manners encroach
on the privacy of different agents within the integrated systems, meanwhile
suffering from the combinatorial explosion. In this research, we propose a
near-optimal distributed privacy-preserving approach to solve the non-convex
synergy (day-ahead co-dispatch) problem. The synergy problem is formulated as a
mixed integer quadratically constrained quadratic programming considering both
communication and energy conservation, where Lyapunov optimization is
introduced to balance operating costs and uncertain communication delays. To
mitigate impacts of the highly non-convex nature, the normalized
multi-parametric disaggregation technique is leveraged to reformulate the
problem into a mixed integer non-linear programming. To further overcome
non-smoothness of the reformulated problem, the customized $\ell_1-$surrogate
Lagrangian relaxation method with convergence guarantees is proposed to solve
the problem in a distributed privacy-preserving manner. {The effectiveness,
optimality, and scalability of the proposed methodologies for the synergy
problem are validated via numerical simulations. Simulation results also
indicate that computing tasks can be delayed and migrated within the
hierarchical data centers, demonstrating the flexible resource allocation
capabilities of the hierarchical data center architecture, further facilitating
peak load balancing in the power network.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.20118v2">TrojanStego: Your Language Model Can Secretly Be A Steganographic
  Privacy Leaking Agent</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-26T15:20:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dominik Meier, Jan Philip Wahle, Paul Röttger, Terry Ruas, Bela Gipp</p>
    <p><b>Summary:</b> As large language models (LLMs) become integrated into sensitive workflows,
concerns grow over their potential to leak confidential information. We propose
TrojanStego, a novel threat model in which an adversary fine-tunes an LLM to
embed sensitive context information into natural-looking outputs via linguistic
steganography, without requiring explicit control over inference inputs. We
introduce a taxonomy outlining risk factors for compromised LLMs, and use it to
evaluate the risk profile of the threat. To implement TrojanStego, we propose a
practical encoding scheme based on vocabulary partitioning learnable by LLMs
via fine-tuning. Experimental results show that compromised models reliably
transmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over
97% accuracy using majority voting across three generations. Further, they
maintain high utility, can evade human detection, and preserve coherence. These
results highlight a new class of LLM data exfiltration attacks that are
passive, covert, practical, and dangerous.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.20095v1">Spurious Privacy Leakage in Neural Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-26T15:04:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenxiang Zhang, Jun Pang, Sjouke Mauw</p>
    <p><b>Summary:</b> Neural networks are vulnerable to privacy attacks aimed at stealing sensitive
data. The risks can be amplified in a real-world scenario, particularly when
models are trained on limited and biased data. In this work, we investigate the
impact of spurious correlation bias on privacy vulnerability. We introduce
\emph{spurious privacy leakage}, a phenomenon where spurious groups are
significantly more vulnerable to privacy attacks than non-spurious groups. We
further show that group privacy disparity increases in tasks with simpler
objectives (e.g. fewer classes) due to the persistence of spurious features.
Surprisingly, we find that reducing spurious correlation using spurious robust
methods does not mitigate spurious privacy leakage. This leads us to introduce
a perspective on privacy disparity based on memorization, where mitigating
spurious correlation does not mitigate the memorization of spurious data, and
therefore, neither the privacy level. Lastly, we compare the privacy of
different model architectures trained with spurious data, demonstrating that,
contrary to prior works, architectural choice can affect privacy outcomes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.19969v1">Differential Privacy Analysis of Decentralized Gossip Averaging under
  Varying Threat Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-05-26T13:31:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antti Koskela, Tejas Kulkarni</p>
    <p><b>Summary:</b> Fully decentralized training of machine learning models offers significant
advantages in scalability, robustness, and fault tolerance. However, achieving
differential privacy (DP) in such settings is challenging due to the absence of
a central aggregator and varying trust assumptions among nodes. In this work,
we present a novel privacy analysis of decentralized gossip-based averaging
algorithms with additive node-level noise, both with and without secure
summation over each node's direct neighbors. Our main contribution is a new
analytical framework based on a linear systems formulation that accurately
characterizes privacy leakage across these scenarios. This framework
significantly improves upon prior analyses, for example, reducing the R\'enyi
DP parameter growth from $O(T^2)$ to $O(T)$, where $T$ is the number of
training rounds. We validate our analysis with numerical results demonstrating
superior DP bounds compared to existing approaches. We further illustrate our
analysis with a logistic regression experiment on MNIST image classification in
a fully decentralized setting, demonstrating utility comparable to central
aggregation methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.19951v1">Novel Loss-Enhanced Universal Adversarial Patches for Sustainable
  Speaker Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-05-26T13:16:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Elvir Karimov, Alexander Varlamov, Danil Ivanov, Dmitrii Korzh, Oleg Y. Rogov</p>
    <p><b>Summary:</b> Deep learning voice models are commonly used nowadays, but the safety
processing of personal data, such as human identity and speech content, remains
suspicious. To prevent malicious user identification, speaker anonymization
methods were proposed. Current methods, particularly based on universal
adversarial patch (UAP) applications, have drawbacks such as significant
degradation of audio quality, decreased speech recognition quality, low
transferability across different voice biometrics models, and performance
dependence on the input audio length. To mitigate these drawbacks, in this
work, we introduce and leverage the novel Exponential Total Variance (TV) loss
function and provide experimental evidence that it positively affects UAP
strength and imperceptibility. Moreover, we present a novel scalable UAP
insertion procedure and demonstrate its uniformly high performance for various
audio lengths.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.19823v1">LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning
  in Heterogeneous Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-26T11:00:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pengcheng Sun, Erwu Liu, Wei Ni, Rui Wang, Yuanzhe Geng, Lijuan Lai, Abbas Jamalipour</p>
    <p><b>Summary:</b> Federated Learning (FL) is a distributed machine learning paradigm based on
protecting data privacy of devices, which however, can still be broken by
gradient leakage attack via parameter inversion techniques. Differential
privacy (DP) technology reduces the risk of private data leakage by adding
artificial noise to the gradients, but detrimental to the FL utility at the
same time, especially in the scenario where the data is Non-Independent
Identically Distributed (Non-IID). Based on the impact of heterogeneous data on
aggregation performance, this paper proposes a Lightweight Adaptive Privacy
Allocation (LAPA) strategy, which assigns personalized privacy budgets to
devices in each aggregation round without transmitting any additional
information beyond gradients, ensuring both privacy protection and aggregation
efficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG)
algorithm is employed to optimize the transmission power, in order to determine
the optimal timing at which the adaptively attenuated artificial noise aligns
with the communication noise, enabling an effective balance between DP and
system utility. Finally, a reliable aggregation strategy is designed by
integrating communication quality and data distribution characteristics, which
improves aggregation performance while preserving privacy. Experimental results
demonstrate that the personalized noise allocation and dynamic optimization
strategy based on LAPA proposed in this paper enhances convergence performance
while satisfying the privacy requirements of FL.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.18870v1">Understanding the Relationship Between Personal Data Privacy Literacy
  and Data Privacy Information Sharing by University Students</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-24T21:14:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Brady D. Lund, Bryan Anderson, Ana Roeschley, Gahangir Hossain</p>
    <p><b>Summary:</b> With constant threats to the safety of personal data in the United States,
privacy literacy has become an increasingly important competency among
university students, one that ties intimately to the information sharing
behavior of these students. This survey based study examines how university
students in the United States perceive personal data privacy and how their
privacy literacy influences their understanding and behaviors. Students
responses to a privacy literacy scale were categorized into high and low
privacy literacy groups, revealing that high literacy individuals demonstrate a
broader range of privacy practices, including multi factor authentication, VPN
usage, and phishing awareness, whereas low literacy individuals rely on more
basic security measures. Statistical analyses suggest that high literacy
respondents display greater diversity in recommendations and engagement in
privacy discussions. These findings suggest the need for enhanced educational
initiatives to improve data privacy awareness at the university level to create
a better cyber safe population.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.18786v1">Leveraging Per-Instance Privacy for Machine Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-24T16:55:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nazanin Mohammadi Sepahvand, Anvith Thudi, Berivan Isik, Ashmita Bhattacharyya, Nicolas Papernot, Eleni Triantafillou, Daniel M. Roy, Gintare Karolina Dziugaite</p>
    <p><b>Summary:</b> We present a principled, per-instance approach to quantifying the difficulty
of unlearning via fine-tuning. We begin by sharpening an analysis of noisy
gradient descent for unlearning (Chien et al., 2024), obtaining a better
utility-unlearning tradeoff by replacing worst-case privacy loss bounds with
per-instance privacy losses (Thudi et al., 2024), each of which bounds the
(Renyi) divergence to retraining without an individual data point. To
demonstrate the practical applicability of our theory, we present empirical
results showing that our theoretical predictions are born out both for
Stochastic Gradient Langevin Dynamics (SGLD) as well as for standard
fine-tuning without explicit noise. We further demonstrate that per-instance
privacy losses correlate well with several existing data difficulty metrics,
while also identifying harder groups of data points, and introduce novel
evaluation methods based on loss barriers. All together, our findings provide a
foundation for more efficient and adaptive unlearning strategies tailored to
the unique properties of individual data points.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.18386v1">Modeling interdependent privacy threats</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-23T21:22:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuaishuai Liu, Gergely Biczók</p>
    <p><b>Summary:</b> The rise of online social networks, user-gene-rated content, and third-party
apps made data sharing an inevitable trend, driven by both user behavior and
the commercial value of personal information. As service providers amass vast
amounts of data, safeguarding individual privacy has become increasingly
challenging. Privacy threat modeling has emerged as a critical tool for
identifying and mitigating risks, with methodologies such as LINDDUN, xCOMPASS,
and PANOPTIC offering systematic approaches. However, these frameworks
primarily focus on threats arising from interactions between a single user and
system components, often overlooking interdependent privacy (IDP); the
phenomenon where one user's actions affect the privacy of other users and even
non-users. IDP risks are particularly pronounced in third-party applications,
where platform permissions, APIs, and user behavior can lead to unintended and
unconsented data sharing, such as in the Cambridge Analytica case. We argue
that existing threat modeling approaches are limited in exposing IDP-related
threats, potentially underestimating privacy risks. To bridge this gap, we
propose a specialized methodology that explicitly focuses on interdependent
privacy. Our contributions are threefold: (i) we identify IDP-specific
challenges and limitations in current threat modeling frameworks, (ii) we
create IDPA, a threat modeling approach tailored to IDP threats, and (iii) we
validate our approach through a case study on WeChat. We believe that IDPA can
operate effectively on systems other than third-party apps and may motivate
further research on specialized threat modeling.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.18242v1">Privacy-Preserving Bathroom Monitoring for Elderly Emergencies Using PIR
  and LiDAR Sensors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-23T15:49:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Youssouf Sidibé, Julia Gersey</p>
    <p><b>Summary:</b> In-home elderly monitoring requires systems that can detect emergency events
- such as falls or prolonged inactivity - while preserving privacy and
requiring no user input. These systems must be embedded into the surrounding
environment, capable of capturing activity, and responding promptly. This paper
presents a low-cost, privacy-preserving solution using Passive Infrared (PIR)
and Light Detection and Ranging (LiDAR) sensors to track entries, sitting,
exits, and emergency scenarios within a home bathroom setting. We developed and
evaluated a rule-based detection system through five real-world experiments
simulating elderly behavior. Annotated time-series graphs demonstrate the
system's ability to detect dangerous states, such as motionless collapses,
while maintaining privacy through non-visual sensing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.16954v1">Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of
  Vulnerabilities in Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-05-22T17:34:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiaying Fu, Yiyang Lu, Zehua Yang, Fiona Nah, RAY LC</p>
    <p><b>Summary:</b> Traditional methods for raising awareness of privacy protection often fail to
engage users or provide hands-on insights into how privacy vulnerabilities are
exploited. To address this, we incorporate an adversarial mechanic in the
design of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to
simulate natural interactions, the game challenges players to impersonate
characters and extract sensitive information from an AI agent, Aegis. A user
study (n=22) revealed that players employed diverse deceptive linguistic
strategies, including storytelling and emotional rapport, to manipulate Aegis.
After playing, players reported connecting in-game scenarios with real-world
privacy vulnerabilities, such as phishing and impersonation, and expressed
intentions to strengthen privacy control, such as avoiding oversharing personal
information with AI systems. This work highlights the potential of LLMs to
simulate complex relational interactions in serious games, while demonstrating
how an adversarial game strategy provides unique insights for designs for
social good, particularly privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.16371v1">Privacy-Aware Cyberterrorism Network Analysis using Graph Neural
  Networks and Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-22T08:26:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anas Ali, Mubashar Husain, Peter Hans</p>
    <p><b>Summary:</b> Cyberterrorism poses a formidable threat to digital infrastructures, with
increasing reliance on encrypted, decentralized platforms that obscure threat
actor activity. To address the challenge of analyzing such adversarial networks
while preserving the privacy of distributed intelligence data, we propose a
Privacy-Aware Federated Graph Neural Network (PA-FGNN) framework. PA-FGNN
integrates graph attention networks, differential privacy, and homomorphic
encryption into a robust federated learning pipeline tailored for
cyberterrorism network analysis. Each client trains locally on sensitive graph
data and exchanges encrypted, noise-perturbed model updates with a central
aggregator, which performs secure aggregation and broadcasts global updates. We
implement anomaly detection for flagging high-risk nodes and incorporate
defenses against gradient poisoning. Experimental evaluations on simulated dark
web and cyber-intelligence graphs demonstrate that PA-FGNN achieves over 91\%
classification accuracy, maintains resilience under 20\% adversarial client
behavior, and incurs less than 18\% communication overhead. Our results
highlight that privacy-preserving GNNs can support large-scale cyber threat
detection without compromising on utility, privacy, or robustness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.17145v1">LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy
  Compliance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-22T07:30:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yu Wang, Cailing Cai, Zhihua Xiao, Peifung E. Lam</p>
    <p><b>Summary:</b> Large language models (LLMs) are increasingly applied in fields such as
finance, education, and governance due to their ability to generate human-like
text and adapt to specialized tasks. However, their widespread adoption raises
critical concerns about data privacy and security, including the risk of
sensitive data exposure.
  In this paper, we propose a security framework to enforce policy compliance
and mitigate risks in LLM interactions. Our approach introduces three key
innovations: (i) LLM-based policy enforcement: a customizable mechanism that
enhances domain-specific detection of sensitive data. (ii) Dynamic policy
customization: real-time policy adaptation and enforcement during user-LLM
interactions to ensure compliance with evolving security requirements. (iii)
Sensitive data anonymization: a format-preserving encryption technique that
protects sensitive information while maintaining contextual integrity.
Experimental results demonstrate that our framework effectively mitigates
security risks while preserving the functional accuracy of LLM-driven tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.16059v1">Monitoring in the Dark: Privacy-Preserving Runtime Verification of
  Cyber-Physical Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Logic in Computer Science-662E9B"> 
  <p><b>Published on:</b> 2025-05-21T22:20:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Charles Koll, Preston Tan Hang, Mike Rosulek, Houssam Abbas</p>
    <p><b>Summary:</b> In distributed Cyber-Physical Systems and Internet-of-Things applications,
the nodes of the system send measurements to a monitor that checks whether
these measurements satisfy given formal specifications. For instance in Urban
Air Mobility, a local traffic authority will be monitoring drone traffic to
evaluate its flow and detect emerging problematic patterns. Certain
applications require both the specification and the measurements to be private
-- i.e. known only to their owners. Examples include traffic monitoring,
testing of integrated circuit designs, and medical monitoring by wearable or
implanted devices. In this paper we propose a protocol that enables
privacy-preserving robustness monitoring. By following our protocol, both
system (e.g. drone) and monitor (e.g. traffic authority) only learn the
robustness of the measured trace w.r.t. the specification. But the system
learns nothing about the formula, and the monitor learns nothing about the
signal monitored. We do this using garbled circuits, for specifications in
Signal Temporal Logic interpreted over timed state sequences. We analyze the
runtime and memory overhead of privacy preservation, the size of the circuits,
and their practicality for three different usage scenarios: design testing,
offline monitoring, and online monitoring of Cyber-Physical Systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.15721v1">Privacy-Preserving Conformal Prediction Under Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-05-21T16:29:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Coby Penso, Bar Mahpud, Jacob Goldberger, Or Sheffet</p>
    <p><b>Summary:</b> Conformal prediction (CP) provides sets of candidate classes with a
guaranteed probability of containing the true class. However, it typically
relies on a calibration set with clean labels. We address privacy-sensitive
scenarios where the aggregator is untrusted and can only access a perturbed
version of the true labels. We propose two complementary approaches under local
differential privacy (LDP). In the first approach, users do not access the
model but instead provide their input features and a perturbed label using a
k-ary randomized response. In the second approach, which enforces stricter
privacy constraints, users add noise to their conformity score by binary search
response. This method requires access to the classification model but preserves
both data and label privacy. Both approaches compute the conformal threshold
directly from noisy data without accessing the true labels. We prove
finite-sample coverage guarantees and demonstrate robust coverage even under
severe randomization. This approach unifies strong local privacy with
predictive uncertainty control, making it well-suited for sensitive
applications such as medical imaging or large language model queries,
regardless of whether users can (or are willing to) compute their own scores.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.15483v1">Optimal Piecewise-based Mechanism for Collecting Bounded Numerical Data
  under Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-05-21T13:01:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ye Zheng, Sumita Mishra, Yidan Hu</p>
    <p><b>Summary:</b> Numerical data with bounded domains is a common data type in personal
devices, such as wearable sensors. While the collection of such data is
essential for third-party platforms, it raises significant privacy concerns.
Local differential privacy (LDP) has been shown as a framework providing
provable individual privacy, even when the third-party platform is untrusted.
For numerical data with bounded domains, existing state-of-the-art LDP
mechanisms are piecewise-based mechanisms, which are not optimal, leading to
reduced data utility.
  This paper investigates the optimal design of piecewise-based mechanisms to
maximize data utility under LDP. We demonstrate that existing piecewise-based
mechanisms are heuristic forms of the $3$-piecewise mechanism, which is far
from enough to study optimality. We generalize the $3$-piecewise mechanism to
its most general form, i.e. $m$-piecewise mechanism with no pre-defined form of
each piece. Under this form, we derive the closed-form optimal mechanism by
combining analytical proofs and off-the-shelf optimization solvers. Next, we
extend the generalized piecewise-based mechanism to the circular domain (along
with the classical domain), defined on a cyclic range where the distance
between the two endpoints is zero. By incorporating this property, we design
the optimal mechanism for the circular domain, achieving significantly improved
data utility compared with existing mechanisms.
  Our proposed mechanisms guarantee optimal data utility under LDP among all
generalized piecewise-based mechanisms. We show that they also achieve optimal
data utility in two common applications of LDP: distribution estimation and
mean estimation. Theoretical analyses and experimental evaluations prove and
validate the data utility advantages of our proposed mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.15476v1">Pura: An Efficient Privacy-Preserving Solution for Face Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-21T12:50:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guotao Xu, Bowen Zhao, Yang Xiao, Yantao Zhong, Liang Zhai, Qingqi Pei</p>
    <p><b>Summary:</b> Face recognition is an effective technology for identifying a target person
by facial images. However, sensitive facial images raises privacy concerns.
Although privacy-preserving face recognition is one of potential solutions,
this solution neither fully addresses the privacy concerns nor is efficient
enough. To this end, we propose an efficient privacy-preserving solution for
face recognition, named Pura, which sufficiently protects facial privacy and
supports face recognition over encrypted data efficiently. Specifically, we
propose a privacy-preserving and non-interactive architecture for face
recognition through the threshold Paillier cryptosystem. Additionally, we
carefully design a suite of underlying secure computing protocols to enable
efficient operations of face recognition over encrypted data directly.
Furthermore, we introduce a parallel computing mechanism to enhance the
performance of the proposed secure computing protocols. Privacy analysis
demonstrates that Pura fully safeguards personal facial privacy. Experimental
evaluations demonstrate that Pura achieves recognition speeds up to 16 times
faster than the state-of-the-art.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.15376v1">Federated Learning-Enhanced Blockchain Framework for Privacy-Preserving
  Intrusion Detection in Industrial IoT</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-21T11:11:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anas Ali, Mubashar Husain, Peter Hans</p>
    <p><b>Summary:</b> Industrial Internet of Things (IIoT) systems have become integral to smart
manufacturing, yet their growing connectivity has also exposed them to
significant cybersecurity threats. Traditional intrusion detection systems
(IDS) often rely on centralized architectures that raise concerns over data
privacy, latency, and single points of failure. In this work, we propose a
novel Federated Learning-Enhanced Blockchain Framework (FL-BCID) for
privacy-preserving intrusion detection tailored for IIoT environments. Our
architecture combines federated learning (FL) to ensure decentralized model
training with blockchain technology to guarantee data integrity, trust, and
tamper resistance across IIoT nodes. We design a lightweight intrusion
detection model collaboratively trained using FL across edge devices without
exposing sensitive data. A smart contract-enabled blockchain system records
model updates and anomaly scores to establish accountability. Experimental
evaluations using the ToN-IoT and N-BaIoT datasets demonstrate the superior
performance of our framework, achieving 97.3% accuracy while reducing
communication overhead by 41% compared to baseline centralized methods. Our
approach ensures privacy, scalability, and robustness-critical for secure
industrial operations. The proposed FL-BCID system provides a promising
solution for enhancing trust and privacy in modern IIoT security architectures.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.15156v1">Privacy-Preserving Socialized Recommendation based on Multi-View
  Clustering in a Cloud Environment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-21T06:21:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Cheng Guo, Jing Jia, Peng Wang, Jing Zhang</p>
    <p><b>Summary:</b> Recommendation as a service has improved the quality of our lives and plays a
significant role in variant aspects. However, the preference of users may
reveal some sensitive information, so that the protection of privacy is
required. In this paper, we propose a privacy-preserving, socialized,
recommendation protocol that introduces information collected from online
social networks to enhance the quality of the recommendation. The proposed
scheme can calculate the similarity between users to determine their potential
relationships and interests, and it also can protect the users' privacy from
leaking to an untrusted third party. The security analysis and experimental
results showed that our proposed scheme provides excellent performance and is
feasible for real-world applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.14959v1">Privacy Preserving Conversion Modeling in Data Clean Room</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB"> 
  <p><b>Published on:</b> 2025-05-20T22:38:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kungang Li, Xiangyi Chen, Ling Leng, Jiajing Xu, Jiankai Sun, Behnam Rezaei</p>
    <p><b>Summary:</b> In the realm of online advertising, accurately predicting the conversion rate
(CVR) is crucial for enhancing advertising efficiency and user satisfaction.
This paper addresses the challenge of CVR prediction while adhering to user
privacy preferences and advertiser requirements. Traditional methods face
obstacles such as the reluctance of advertisers to share sensitive conversion
data and the limitations of model training in secure environments like data
clean rooms. We propose a novel model training framework that enables
collaborative model training without sharing sample-level gradients with the
advertising platform. Our approach introduces several innovative components:
(1) utilizing batch-level aggregated gradients instead of sample-level
gradients to minimize privacy risks; (2) applying adapter-based
parameter-efficient fine-tuning and gradient compression to reduce
communication costs; and (3) employing de-biasing techniques to train the model
under label differential privacy, thereby maintaining accuracy despite
privacy-enhanced label perturbations. Our experimental results, conducted on
industrial datasets, demonstrate that our method achieves competitive ROCAUC
performance while significantly decreasing communication overhead and complying
with both advertiser privacy requirements and user privacy choices. This
framework establishes a new standard for privacy-preserving, high-performance
CVR prediction in the digital advertising landscape.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.14797v1">Efficient Privacy-Preserving Cross-Silo Federated Learning with
  Multi-Key Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-20T18:08:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abdullah Al Omar, Xin Yang, Euijin Choo, Omid Ardakanian</p>
    <p><b>Summary:</b> Federated Learning (FL) is susceptible to privacy attacks, such as data
reconstruction attacks, in which a semi-honest server or a malicious client
infers information about other clients' datasets from their model updates or
gradients. To enhance the privacy of FL, recent studies combined Multi-Key
Homomorphic Encryption (MKHE) and FL, making it possible to aggregate the
encrypted model updates using different keys without having to decrypt them.
Despite the privacy guarantees of MKHE, existing approaches are not well-suited
for real-world deployment due to their high computation and communication
overhead. We propose MASER, an efficient MKHE-based Privacy-Preserving FL
framework that combines consensus-based model pruning and slicing techniques to
reduce this overhead. Our experimental results show that MASER is 3.03 to 8.29
times more efficient than existing MKHE-based FL approaches in terms of
computation and communication overhead while maintaining comparable
classification accuracy to standard FL algorithms. Compared to a vanilla FL
algorithm, the overhead of MASER is only 1.48 to 5 times higher, striking a
good balance between privacy, accuracy, and efficiency in both IID and non-IID
settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.14585v1">Context Reasoner: Incentivizing Reasoning Capability for Contextualized
  Privacy and Safety Compliance via Reinforcement Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-20T16:40:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenbin Hu, Haoran Li, Huihao Jing, Qi Hu, Ziqian Zeng, Sirui Han, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song</p>
    <p><b>Summary:</b> While Large Language Models (LLMs) exhibit remarkable capabilities, they also
introduce significant safety and privacy risks. Current mitigation strategies
often fail to preserve contextual reasoning capabilities in risky scenarios.
Instead, they rely heavily on sensitive pattern matching to protect LLMs, which
limits the scope. Furthermore, they overlook established safety and privacy
standards, leading to systemic risks for legal compliance. To address these
gaps, we formulate safety and privacy issues into contextualized compliance
problems following the Contextual Integrity (CI) theory. Under the CI
framework, we align our model with three critical regulatory standards: GDPR,
EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with
a rule-based reward to incentivize contextual reasoning capabilities while
enhancing compliance with safety and privacy norms. Through extensive
experiments, we demonstrate that our method not only significantly enhances
legal compliance (achieving a +17.64% accuracy improvement in safety/privacy
benchmarks) but also further improves general reasoning capability. For
OpenThinker-7B, a strong reasoning model that significantly outperforms its
base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its
general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on
the MMLU and LegalBench benchmark, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.14507v1">Federated prediction for scalable and privacy-preserved knowledge-based
  planning in radiotherapy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-05-20T15:35:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jingyun Chen, David Horowitz, Yading Yuan</p>
    <p><b>Summary:</b> Background: Deep learning has potential to improve the efficiency and
consistency of radiation therapy planning, but clinical adoption is hindered by
the limited model generalizability due to data scarcity and heterogeneity among
institutions. Although aggregating data from different institutions could
alleviate this problem, data sharing is a practical challenge due to concerns
about patient data privacy and other technical obstacles. Purpose: This work
aims to address this dilemma by developing FedKBP+, a comprehensive federated
learning (FL) platform for predictive tasks in real-world applications in
radiotherapy treatment planning. Methods: We implemented a unified
communication stack based on Google Remote Procedure Call (gRPC) to support
communication between participants whether located on the same workstation or
distributed across multiple workstations. In addition to supporting the
centralized FL strategies commonly available in existing open-source
frameworks, FedKBP+ also provides a fully decentralized FL model where
participants directly exchange model weights to each other through Peer-to-Peer
communication. We evaluated FedKBP+ on three predictive tasks using
scale-attention network (SA-Net) as the predictive model. Conclusions: Our
results demonstrate that FedKBP+ is highly effective, efficient and robust,
showing great potential as a federated learning platform for radiation therapy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.14195v1">Unraveling Interwoven Roles of Large Language Models in Authorship
  Privacy: Obfuscation, Mimicking, and Verification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-20T10:52:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tuc Nguyen, Yifan Hu, Thai Le</p>
    <p><b>Summary:</b> Recent advancements in large language models (LLMs) have been fueled by large
scale training corpora drawn from diverse sources such as websites, news
articles, and books. These datasets often contain explicit user information,
such as person names and addresses, that LLMs may unintentionally reproduce in
their generated outputs. Beyond such explicit content, LLMs can also leak
identity revealing cues through implicit signals such as distinctive writing
styles, raising significant concerns about authorship privacy. There are three
major automated tasks in authorship privacy, namely authorship obfuscation
(AO), authorship mimicking (AM), and authorship verification (AV). Prior
research has studied AO, AM, and AV independently. However, their interplays
remain under explored, which leaves a major research gap, especially in the era
of LLMs, where they are profoundly shaping how we curate and share user
generated content, and the distinction between machine generated and human
authored text is also increasingly blurred. This work then presents the first
unified framework for analyzing the dynamic relationships among LLM enabled AO,
AM, and AV in the context of authorship privacy. We quantify how they interact
with each other to transform human authored text, examining effects at a single
point in time and iteratively over time. We also examine the role of
demographic metadata, such as gender, academic background, in modulating their
performances, inter-task dynamics, and privacy risks. All source code will be
publicly available.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.13957v1">Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal
  Retrieval-Augmented Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-20T05:37:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiankun Zhang, Shenglai Zeng, Jie Ren, Tianqi Zheng, Hui Liu, Xianfeng Tang, Hui Liu, Yi Chang</p>
    <p><b>Summary:</b> Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by
integrating external multimodal databases, but introduce unexplored privacy
vulnerabilities. While text-based RAG privacy risks have been studied,
multimodal data presents unique challenges. We provide the first systematic
analysis of MRAG privacy vulnerabilities across vision-language and
speech-language modalities. Using a novel compositional structured prompt
attack in a black-box setting, we demonstrate how attackers can extract private
information by manipulating queries. Our experiments reveal that LMMs can both
directly generate outputs resembling retrieved content and produce descriptions
that indirectly expose sensitive information, highlighting the urgent need for
robust privacy-preserving MRAG techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.13694v1">A Systematic Review and Taxonomy for Privacy Breach Classification:
  Trends, Gaps, and Future Directions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-05-19T19:52:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Clint Fuchs, John D. Hastings</p>
    <p><b>Summary:</b> In response to the rising frequency and complexity of data breaches and
evolving global privacy regulations, this study presents a comprehensive
examination of academic literature on the classification of privacy breaches
and violations between 2010-2024. Through a systematic literature review, a
corpus of screened studies was assembled and analyzed to identify primary
research themes, emerging trends, and gaps in the field. A novel taxonomy is
introduced to guide efforts by categorizing research efforts into seven
domains: breach classification, report classification, breach detection, threat
detection, breach prediction, risk analysis, and threat classification. An
analysis reveals that breach classification and detection dominate the
literature, while breach prediction and risk analysis have only recently
emerged in the literature, suggesting opportunities for potential research
impacts. Keyword and phrase frequency analysis reveal potentially underexplored
areas, including location privacy, prediction models, and healthcare data
breaches.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.13655v1">Optimal Client Sampling in Federated Learning with Client-Level
  Heterogeneous Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-19T18:55:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiahao Xu, Rui Hu, Olivera Kotevska</p>
    <p><b>Summary:</b> Federated Learning with client-level differential privacy (DP) provides a
promising framework for collaboratively training models while rigorously
protecting clients' privacy. However, classic approaches like DP-FedAvg
struggle when clients have heterogeneous privacy requirements, as they must
uniformly enforce the strictest privacy level across clients, leading to
excessive DP noise and significant model utility degradation. Existing methods
to improve the model utility in such heterogeneous privacy settings often
assume a trusted server and are largely heuristic, resulting in suboptimal
performance and lacking strong theoretical underpinnings. In this work, we
address these challenges under a practical attack model where both clients and
the server are honest-but-curious. We propose GDPFed, which partitions clients
into groups based on their privacy budgets and achieves client-level DP within
each group to reduce the privacy budget waste and hence improve the model
utility. Based on the privacy and convergence analysis of GDPFed, we find that
the magnitude of DP noise depends on both model dimensionality and the
per-group client sampling ratios. To further improve the performance of GDPFed,
we introduce GDPFed$^+$, which integrates model sparsification to eliminate
unnecessary noise and optimizes per-group client sampling ratios to minimize
convergence error. Extensive empirical evaluations on multiple benchmark
datasets demonstrate the effectiveness of GDPFed$^+$, showing substantial
performance gains compared with state-of-the-art methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.13292v1">Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms
  of AI Systems by Integrating Federated Learning and LLMs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-19T16:14:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Huaiying Luo, Cheng Ji</p>
    <p><b>Summary:</b> In the age of cloud computing, data privacy protection has become a major
challenge, especially when sharing sensitive data across cloud environments.
However, how to optimize collaboration across cloud environments remains an
unresolved problem. In this paper, we combine federated learning with
large-scale language models to optimize the collaborative mechanism of AI
systems. Based on the existing federated learning framework, we introduce a
cross-cloud architecture in which federated learning works by aggregating model
updates from decentralized nodes without exposing the original data. At the
same time, combined with large-scale language models, its powerful context and
semantic understanding capabilities are used to improve model training
efficiency and decision-making ability. We've further innovated by introducing
a secure communication layer to ensure the privacy and integrity of model
updates and training data. The model enables continuous model adaptation and
fine-tuning across different cloud environments while protecting sensitive
data. Experimental results show that the proposed method is significantly
better than the traditional federated learning model in terms of accuracy,
convergence speed and data privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.13085v2">Universal Semantic Disentangled Privacy-preserving Speech Representation
  Learning</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-19T13:19:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Biel Tura Vecino, Subhadeep Maji, Aravind Varier, Antonio Bonafonte, Ivan Valles, Michael Owen, Leif Rädel, Grant Strimel, Seyi Feyisetan, Roberto Barra Chicote, Ariya Rastrow, Constantinos Papayiannis, Volker Leutnant, Trevor Wood</p>
    <p><b>Summary:</b> The use of audio recordings of human speech to train LLMs poses privacy
concerns due to these models' potential to generate outputs that closely
resemble artifacts in the training data. In this study, we propose a speaker
privacy-preserving representation learning method through the Universal Speech
Codec (USC), a computationally efficient encoder-decoder model that
disentangles speech into: (i) privacy-preserving semantically rich
representations, capturing content and speech paralinguistics, and (ii)
residual acoustic and speaker representations that enables high-fidelity
reconstruction. Extensive evaluations presented show that USC's semantic
representation preserves content, prosody, and sentiment, while removing
potentially identifiable speaker attributes. Combining both representations,
USC achieves state-of-the-art speech reconstruction. Additionally, we introduce
an evaluation methodology for measuring privacy-preserving properties, aligning
with perceptual tests. We compare USC against other codecs in the literature
and demonstrate its effectiveness on privacy-preserving representation
learning, illustrating the trade-offs of speaker anonymization, paralinguistics
retention and content preservation in the learned semantic representations.
Audio samples are shared in https://www.amazon.science/usc-samples.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12954v1">Counting Graphlets of Size $k$ under Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computational Complexity-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-05-19T10:46:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vorapong Suppakitpaisarn, Donlapark Ponnoprat, Nicha Hirankarn, Quentin Hillebrand</p>
    <p><b>Summary:</b> The problem of counting subgraphs or graphlets under local differential
privacy is an important challenge that has attracted significant attention from
researchers. However, much of the existing work focuses on small graphlets like
triangles or $k$-stars. In this paper, we propose a non-interactive, locally
differentially private algorithm capable of counting graphlets of any size $k$.
When $n$ is the number of nodes in the input graph, we show that the expected
$\ell_2$ error of our algorithm is $O(n^{k - 1})$. Additionally, we prove that
there exists a class of input graphs and graphlets of size $k$ for which any
non-interactive counting algorithm incurs an expected $\ell_2$ error of
$\Omega(n^{k - 1})$, demonstrating the optimality of our result. Furthermore,
we establish that for certain input graphs and graphlets, any locally
differentially private algorithm must have an expected $\ell_2$ error of
$\Omega(n^{k - 1.5})$. Our experimental results show that our algorithm is more
accurate than the classical randomized response method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12869v1">Outsourced Privacy-Preserving Feature Selection Based on Fully
  Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-19T08:55:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Koki Wakiyama, Tomohiro I, Hiroshi Sakamoto</p>
    <p><b>Summary:</b> Feature selection is a technique that extracts a meaningful subset from a set
of features in training data. When the training data is large-scale,
appropriate feature selection enables the removal of redundant features, which
can improve generalization performance, accelerate the training process, and
enhance the interpretability of the model. This study proposes a
privacy-preserving computation model for feature selection. Generally, when the
data owner and analyst are the same, there is no need to conceal the private
information. However, when they are different parties or when multiple owners
exist, an appropriate privacy-preserving framework is required. Although
various private feature selection algorithms, they all require two or more
computing parties and do not guarantee security in environments where no
external party can be fully trusted. To address this issue, we propose the
first outsourcing algorithm for feature selection using fully homomorphic
encryption. Compared to a prior two-party algorithm, our result improves the
time and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n
denote the number of features and data samples, respectively. We also
implemented the proposed algorithm and conducted comparative experiments with
the naive one. The experimental result shows the efficiency of our method even
with small datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12688v1">Shielding Latent Face Representations From Privacy Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-19T04:23:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Arjun Ramesh Kaushik, Bharat Chandra Yalavarthi, Arun Ross, Vishnu Boddeti, Nalini Ratha</p>
    <p><b>Summary:</b> In today's data-driven analytics landscape, deep learning has become a
powerful tool, with latent representations, known as embeddings, playing a
central role in several applications. In the face analytics domain, such
embeddings are commonly used for biometric recognition (e.g., face
identification). However, these embeddings, or templates, can inadvertently
expose sensitive attributes such as age, gender, and ethnicity. Leaking such
information can compromise personal privacy and affect civil liberty and human
rights. To address these concerns, we introduce a multi-layer protection
framework for embeddings. It consists of a sequence of operations: (a)
encrypting embeddings using Fully Homomorphic Encryption (FHE), and (b) hashing
them using irreversible feature manifold hashing. Unlike conventional
encryption methods, FHE enables computations directly on encrypted data,
allowing downstream analytics while maintaining strong privacy guarantees. To
reduce the overhead of encrypted processing, we employ embedding compression.
Our proposed method shields latent representations of sensitive data from
leaking private attributes (such as age and gender) while retaining essential
functional capabilities (such as face identification). Extensive experiments on
two datasets using two face encoders demonstrate that our approach outperforms
several state-of-the-art privacy protection methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12610v1">hChain: Blockchain Based Large Scale EHR Data Sharing with Enhanced
  Security and Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-19T01:47:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Musharraf Alruwaill, Saraju Mohanty, Elias Kougianos</p>
    <p><b>Summary:</b> Concerns regarding privacy and data security in conventional healthcare
prompted alternative technologies. In smart healthcare, blockchain technology
addresses existing concerns with security, privacy, and electronic healthcare
transmission. Integration of Blockchain Technology with the Internet of Medical
Things (IoMT) allows real-time monitoring of protected healthcare data.
Utilizing edge devices with IoMT devices is very advantageous for addressing
security, computing, and storage challenges. Encryption using symmetric and
asymmetric keys is used to conceal sensitive information from unauthorized
parties. SHA256 is an algorithm for one-way hashing. It is used to verify that
the data has not been altered, since if it had, the hash value would have
changed. This article offers a blockchain-based smart healthcare system using
IoMT devices for continuous patient monitoring. In addition, it employs edge
resources in addition to IoMT devices to have extra computing power and storage
to hash and encrypt incoming data before sending it to the blockchain.
Symmetric key is utilized to keep the data private even in the blockchain,
allowing the patient to safely communicate the data through smart contracts
while preventing unauthorized physicians from seeing the data. Through the use
of a verification node and blockchain, an asymmetric key is used for the
signing and validation of patient data in the healthcare provider system. In
addition to other security measures, location-based authentication is
recommended to guarantee that data originates from the patient area. Through
the edge device, SHA256 is utilized to secure the data's integrity and a secret
key is used to maintain its secrecy. The hChain architecture improves the
computing power of IoMT environments, the security of EHR sharing through smart
contracts, and the privacy and authentication procedures.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12239v1">ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting
  with Privacy Preservation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-18T05:28:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jianheng Tang, Huiping Zhuang, Di Fang, Jiaxu Li, Feijiang Han, Yajiang Huang, Kejia Fan, Leye Wang, Zhanxing Zhu, Shanghang Zhang, Houbing Herbert Song, Yunhuai Liu</p>
    <p><b>Summary:</b> The development of artificial intelligence demands that models incrementally
update knowledge by Continual Learning (CL) to adapt to open-world
environments. To meet privacy and security requirements, Continual Unlearning
(CU) emerges as an important problem, aiming to sequentially forget particular
knowledge acquired during the CL phase. However, existing unlearning methods
primarily focus on single-shot joint forgetting and face significant
limitations when applied to CU. First, most existing methods require access to
the retained dataset for re-training or fine-tuning, violating the inherent
constraint in CL that historical data cannot be revisited. Second, these
methods often suffer from a poor trade-off between system efficiency and model
fidelity, making them vulnerable to being overwhelmed or degraded by
adversaries through deliberately frequent requests. In this paper, we identify
that the limitations of existing unlearning methods stem fundamentally from
their reliance on gradient-based updates. To bridge the research gap at its
root, we propose a novel gradient-free method for CU, named Analytic Continual
Unlearning (ACU), for efficient and exact forgetting with historical data
privacy preservation. In response to each unlearning request, our ACU
recursively derives an analytical (i.e., closed-form) solution in an
interpretable manner using the least squares method. Theoretical and
experimental evaluations validate the superiority of our ACU on unlearning
effectiveness, model fidelity, and system efficiency.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12153v1">Federated Deep Reinforcement Learning for Privacy-Preserving
  Robotic-Assisted Surgery</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E">
  <p><b>Published on:</b> 2025-05-17T22:02:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sana Hafeez, Sundas Rafat Mulkana, Muhammad Ali Imran, Michele Sevegnani</p>
    <p><b>Summary:</b> The integration of Reinforcement Learning (RL) into robotic-assisted surgery
(RAS) holds significant promise for advancing surgical precision, adaptability,
and autonomous decision-making. However, the development of robust RL models in
clinical settings is hindered by key challenges, including stringent patient
data privacy regulations, limited access to diverse surgical datasets, and high
procedural variability. To address these limitations, this paper presents a
Federated Deep Reinforcement Learning (FDRL) framework that enables
decentralized training of RL models across multiple healthcare institutions
without exposing sensitive patient information. A central innovation of the
proposed framework is its dynamic policy adaptation mechanism, which allows
surgical robots to select and tailor patient-specific policies in real-time,
thereby ensuring personalized and Optimised interventions. To uphold rigorous
privacy standards while facilitating collaborative learning, the FDRL framework
incorporates secure aggregation, differential privacy, and homomorphic
encryption techniques. Experimental results demonstrate a 60\% reduction in
privacy leakage compared to conventional methods, with surgical precision
maintained within a 1.5\% margin of a centralized baseline. This work
establishes a foundational approach for adaptive, secure, and patient-centric
AI-driven surgical robotics, offering a pathway toward clinical translation and
scalable deployment across diverse healthcare environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.12144v1">Proof-of-Social-Capital: Privacy-Preserving Consensus Protocol Replacing
  Stake for Social Capital</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-05-17T21:28:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Juraj Mariani, Ivan Homoliak</p>
    <p><b>Summary:</b> Consensus protocols used today in blockchains often rely on computational
power or financial stakes - scarce resources. We propose a novel protocol using
social capital - trust and influence from social interactions - as a
non-transferable staking mechanism to ensure fairness and decentralization. The
methodology integrates zero-knowledge proofs, verifiable credentials, a
Whisk-like leader election, and an incentive scheme to prevent Sybil attacks
and encourage engagement. The theoretical framework would enhance privacy and
equity, though unresolved issues like off-chain bribery require further
research. This work offers a new model aligned with modern social media
behavior and lifestyle, with applications in finance, providing a practical
insight for decentralized system development.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.11094v1">Blockchain-Enabled Decentralized Privacy-Preserving Group Purchasing for
  Energy Plans</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-16T10:26:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sid Chi-Kin Chau, Yue Zhou</p>
    <p><b>Summary:</b> Retail energy markets are increasingly consumer-oriented, thanks to a growing
number of energy plans offered by a plethora of energy suppliers, retailers and
intermediaries. To maximize the benefits of competitive retail energy markets,
group purchasing is an emerging paradigm that aggregates consumers' purchasing
power by coordinating switch decisions to specific energy providers for
discounted energy plans. Traditionally, group purchasing is mediated by a
trusted third-party, which suffers from the lack of privacy and transparency.
In this paper, we introduce a novel paradigm of decentralized
privacy-preserving group purchasing, empowered by privacy-preserving blockchain
and secure multi-party computation, to enable users to form a coalition for
coordinated switch decisions in a decentralized manner, without a trusted
third-party. The coordinated switch decisions are determined by a competitive
online algorithm, based on users' private consumption data and current energy
plan tariffs. Remarkably, no private user consumption data will be revealed to
others in the online decision-making process, which is carried out in a
transparently verifiable manner to eliminate frauds from dishonest users and
supports fair mutual compensations by sharing the switching costs to
incentivize group purchasing. We implemented our decentralized group purchasing
solution as a smart contract on Solidity-supported blockchain platform (e.g.,
Ethereum), and provide extensive empirical evaluation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10965v1">Privacy and Confidentiality Requirements Engineering for Process Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-16T08:03:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fabian Haertel, Juergen Mangler, Nataliia Klievtsova, Celine Mader, Eugen Rigger, Stefanie Rinderle-Ma</p>
    <p><b>Summary:</b> The application and development of process mining techniques face significant
challenges due to the lack of publicly available real-life event logs. One
reason for companies to abstain from sharing their data are privacy and
confidentiality concerns. Privacy concerns refer to personal data as specified
in the GDPR and have been addressed in existing work by providing
privacy-preserving techniques for event logs. However, the concept of
confidentiality in event logs not pertaining to individuals remains unclear,
although they might contain a multitude of sensitive business data. This work
addresses confidentiality of process data based on the privacy and
confidentiality engineering method (PCRE). PCRE interactively explores privacy
and confidentiality requirements regarding process data with different
stakeholders and defines privacy-preserving actions to address possible
concerns. We co-construct and evaluate PCRE based on structured interviews with
process analysts in two manufacturing companies. PCRE is generic, hence
applicable in different application domains. The goal is to systematically
scrutinize process data and balance the trade-off between privacy and utility
loss.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10941v1">Privacy-Aware Lifelong Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-16T07:27:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ozan Özdenizci, Elmar Rueckert, Robert Legenstein</p>
    <p><b>Summary:</b> Lifelong learning algorithms enable models to incrementally acquire new
knowledge without forgetting previously learned information. Contrarily, the
field of machine unlearning focuses on explicitly forgetting certain previous
knowledge from pretrained models when requested, in order to comply with data
privacy regulations on the right-to-be-forgotten. Enabling efficient lifelong
learning with the capability to selectively unlearn sensitive information from
models presents a critical and largely unaddressed challenge with contradicting
objectives. We address this problem from the perspective of simultaneously
preventing catastrophic forgetting and allowing forward knowledge transfer
during task-incremental learning, while ensuring exact task unlearning and
minimizing memory requirements, based on a single neural network model to be
adapted. Our proposed solution, privacy-aware lifelong learning (PALL),
involves optimization of task-specific sparse subnetworks with parameter
sharing within a single architecture. We additionally utilize an episodic
memory rehearsal mechanism to facilitate exact unlearning without performance
degradations. We empirically demonstrate the scalability of PALL across various
architectures in image classification, and provide a state-of-the-art solution
that uniquely integrates lifelong learning and privacy-aware unlearning
mechanisms for responsible AI applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10871v1">Optimal Allocation of Privacy Budget on Hierarchical Data Release</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-05-16T05:25:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Joonhyuk Ko, Juba Ziani, Ferdinando Fioretto</p>
    <p><b>Summary:</b> Releasing useful information from datasets with hierarchical structures while
preserving individual privacy presents a significant challenge. Standard
privacy-preserving mechanisms, and in particular Differential Privacy, often
require careful allocation of a finite privacy budget across different levels
and components of the hierarchy. Sub-optimal allocation can lead to either
excessive noise, rendering the data useless, or to insufficient protections for
sensitive information. This paper addresses the critical problem of optimal
privacy budget allocation for hierarchical data release. It formulates this
challenge as a constrained optimization problem, aiming to maximize data
utility subject to a total privacy budget while considering the inherent
trade-offs between data granularity and privacy loss. The proposed approach is
supported by theoretical analysis and validated through comprehensive
experiments on real hierarchical datasets. These experiments demonstrate that
optimal privacy budget allocation significantly enhances the utility of the
released data and improves the performance of downstream tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10496v1">CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of
  Synthetic Chest Radiographs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-15T16:59:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Raman Dutt, Pedro Sanchez, Yongchen Yao, Steven McDonagh, Sotirios A. Tsaftaris, Timothy Hospedales</p>
    <p><b>Summary:</b> We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10264v1">Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack
  in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-15T13:16:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Francesco Diana, André Nusser, Chuan Xu, Giovanni Neglia</p>
    <p><b>Summary:</b> Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.09929v1">Security and Privacy Measurement on Chinese Consumer IoT Traffic based
  on Device Lifecycle</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-15T03:27:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenghua Jin, Yan Jia, Yuxin Song, Qingyin Tan, Rui Yang, Zheli Liu</p>
    <p><b>Summary:</b> In recent years, consumer Internet of Things (IoT) devices have become widely
used in daily life. With the popularity of devices, related security and
privacy risks arise at the same time as they collect user-related data and
transmit it to various service providers. Although China accounts for a larger
share of the consumer IoT industry, current analyses on consumer IoT device
traffic primarily focus on regions such as Europe, the United States, and
Australia. Research on China, however, is currently rather rare. This study
constructs the first large-scale dataset about consumer IoT device traffic in
China. Specifically, we propose a fine-grained traffic collection guidance
covering the entire lifecycle of consumer IoT devices, gathering traffic from
70 devices spanning 36 brands and 8 device categories. Based on this dataset,
we analyze traffic destinations and encryption practices across different
device types during the entire lifecycle and compare the findings with the
results of other regions. Compared to other regions, our results show that
consumer IoT devices in China rely more on domestic services and overally
perform better in terms of encryption practices. However, there are still 20/35
devices improperly conduct certificate validation, and 5/70 devices use
insecure encryption protocols. To facilitate future research, we open-source
our traffic collection guidance and make our dataset publicly available.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.09921v2">PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative
  In-Context Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-15T03:11:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yidan Wang, Yanan Cao, Yubing Ren, Fang Fang, Zheng Lin, Binxing Fang</p>
    <p><b>Summary:</b> Large Language Models (LLMs) excel in various domains but pose inherent
privacy risks. Existing methods to evaluate privacy leakage in LLMs often use
memorized prefixes or simple instructions to extract data, both of which
well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM
safety mechanisms to generate harmful content, but their role in privacy
scenarios remains underexplored. In this paper, we examine the effectiveness of
jailbreak attacks in extracting sensitive information, bridging privacy leakage
and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework
targeting Personally Identifiable Information (PII) and addressing the
limitations of current jailbreak methods. Specifically, PIG identifies PII
entities and their types in privacy queries, uses in-context learning to build
a privacy context, and iteratively updates it with three gradient-based
strategies to elicit target PII. We evaluate PIG and existing jailbreak methods
using two privacy-related datasets. Experiments on four white-box and two
black-box LLMs show that PIG outperforms baseline methods and achieves
state-of-the-art (SoTA) results. The results underscore significant privacy
risks in LLMs, emphasizing the need for stronger safeguards. Our code is
availble at https://github.com/redwyd/PrivacyJailbreak.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.09276v1">Privacy-Preserving Runtime Verification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Formal Languages and Automata Theory-D91E36">
  <p><b>Published on:</b> 2025-05-14T10:49:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Thomas A. Henzinger, Mahyar Karimi, K. S. Thejaswini</p>
    <p><b>Summary:</b> Runtime verification offers scalable solutions to improve the safety and
reliability of systems. However, systems that require verification or
monitoring by a third party to ensure compliance with a specification might
contain sensitive information, causing privacy concerns when usual runtime
verification approaches are used. Privacy is compromised if protected
information about the system, or sensitive data that is processed by the
system, is revealed. In addition, revealing the specification being monitored
may undermine the essence of third-party verification.
  In this work, we propose two novel protocols for the privacy-preserving
runtime verification of systems against formal sequential specifications. In
our first protocol, the monitor verifies whether the system satisfies the
specification without learning anything else, though both parties are aware of
the specification. Our second protocol ensures that the system remains
oblivious to the monitored specification, while the monitor learns only whether
the system satisfies the specification and nothing more. Our protocols adapt
and improve existing techniques used in cryptography, and more specifically,
multi-party computation.
  The sequential specification defines the observation step of the monitor,
whose granularity depends on the situation (e.g., banks may be monitored on a
daily basis). Our protocols exchange a single message per observation step,
after an initialisation phase. This design minimises communication overhead,
enabling relatively lightweight privacy-preserving monitoring. We implement our
approach for monitoring specifications described by register automata and
evaluate it experimentally.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.08719v1">PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-13T16:27:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yang Su, Na Yan, Yansha Deng, Robert Schober</p>
    <p><b>Summary:</b> Large language models (LLMs) hosted on cloud servers alleviate the
computational and storage burdens on local devices but raise privacy concerns
due to sensitive data transmission and require substantial communication
bandwidth, which is challenging in constrained environments. In contrast, small
language models (SLMs) running locally enhance privacy but suffer from limited
performance on complex tasks. To balance computational cost, performance, and
privacy protection under bandwidth constraints, we propose a privacy-aware
wireless collaborative mixture of experts (PWC-MoE) framework. Specifically,
PWC-MoE employs a sparse privacy-aware gating network to dynamically route
sensitive tokens to privacy experts located on local clients, while
non-sensitive tokens are routed to non-privacy experts located at the remote
base station. To achieve computational efficiency, the gating network ensures
that each token is dynamically routed to and processed by only one expert. To
enhance scalability and prevent overloading of specific experts, we introduce a
group-wise load-balancing mechanism for the gating network that evenly
distributes sensitive tokens among privacy experts and non-sensitive tokens
among non-privacy experts. To adapt to bandwidth constraints while preserving
model performance, we propose a bandwidth-adaptive and importance-aware token
offloading scheme. This scheme incorporates an importance predictor to evaluate
the importance scores of non-sensitive tokens, prioritizing the most important
tokens for transmission to the base station based on their predicted importance
and the available bandwidth. Experiments demonstrate that the PWC-MoE framework
effectively preserves privacy and maintains high performance even in
bandwidth-constrained environments, offering a practical solution for deploying
LLMs in privacy-sensitive and bandwidth-limited scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.08847v1">On the interplay of Explainability, Privacy and Predictive Performance
  with Explanation-assisted Model Extraction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-13T15:27:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fatima Ezzeddine, Rinad Akel, Ihab Sbeity, Silvia Giordano, Marc Langheinrich, Omran Ayoub</p>
    <p><b>Summary:</b> Machine Learning as a Service (MLaaS) has gained important attraction as a
means for deploying powerful predictive models, offering ease of use that
enables organizations to leverage advanced analytics without substantial
investments in specialized infrastructure or expertise. However, MLaaS
platforms must be safeguarded against security and privacy attacks, such as
model extraction (MEA) attacks. The increasing integration of explainable AI
(XAI) within MLaaS has introduced an additional privacy challenge, as attackers
can exploit model explanations particularly counterfactual explanations (CFs)
to facilitate MEA. In this paper, we investigate the trade offs among model
performance, privacy, and explainability when employing Differential Privacy
(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two
distinct DP strategies: implemented during the classification model training
and at the explainer during CF generation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.08237v1">Privacy-Preserving Analytics for Smart Meter (AMI) Data: A Hybrid
  Approach to Comply with CPUC Privacy Regulations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-05-13T05:30:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Benjamin Westrich</p>
    <p><b>Summary:</b> Advanced Metering Infrastructure (AMI) data from smart electric and gas
meters enables valuable insights for utilities and consumers, but also raises
significant privacy concerns. In California, regulatory decisions (CPUC
D.11-07-056 and D.11-08-045) mandate strict privacy protections for customer
energy usage data, guided by the Fair Information Practice Principles (FIPPs).
We comprehensively explore solutions drawn from data anonymization,
privacy-preserving machine learning (differential privacy and federated
learning), synthetic data generation, and cryptographic techniques (secure
multiparty computation, homomorphic encryption). This allows advanced
analytics, including machine learning models, statistical and econometric
analysis on energy consumption data, to be performed without compromising
individual privacy.
  We evaluate each technique's theoretical foundations, effectiveness, and
trade-offs in the context of utility data analytics, and we propose an
integrated architecture that combines these methods to meet real-world needs.
The proposed hybrid architecture is designed to ensure compliance with
California's privacy rules and FIPPs while enabling useful analytics, from
forecasting and personalized insights to academic research and econometrics,
while strictly protecting individual privacy. Mathematical definitions and
derivations are provided where appropriate to demonstrate privacy guarantees
and utility implications rigorously. We include comparative evaluations of the
techniques, an architecture diagram, and flowcharts to illustrate how they work
together in practice. The result is a blueprint for utility data scientists and
engineers to implement privacy-by-design in AMI data handling, supporting both
data-driven innovation and strict regulatory compliance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07766v1">Privacy Risks of Robot Vision: A User Study on Image Modalities and
  Resolution</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-12T17:16:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xuying Huang, Sicong Pan, Maren Bennewitz</p>
    <p><b>Summary:</b> User privacy is a crucial concern in robotic applications, especially when
mobile service robots are deployed in personal or sensitive environments.
However, many robotic downstream tasks require the use of cameras, which may
raise privacy risks. To better understand user perceptions of privacy in
relation to visual data, we conducted a user study investigating how different
image modalities and image resolutions affect users' privacy concerns. The
results show that depth images are broadly viewed as privacy-safe, and a
similarly high proportion of respondents feel the same about semantic
segmentation images. Additionally, the majority of participants consider 32*32
resolution RGB images to be almost sufficiently privacy-preserving, while most
believe that 16*16 resolution can fully guarantee privacy protection.</p>
  </details>
</div>

