
<h2>2025-09</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20324v1">RAG Security and Privacy: Formalizing the Threat Model and Attack
  Surface</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-24T17:11:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Atousa Arzanipour, Rouzbeh Behnia, Reza Ebrahimi, Kaushik Dutta</p>
    <p><b>Summary:</b> Retrieval-Augmented Generation (RAG) is an emerging approach in natural
language processing that combines large language models (LLMs) with external
document retrieval to produce more accurate and grounded responses. While RAG
has shown strong potential in reducing hallucinations and improving factual
consistency, it also introduces new privacy and security challenges that differ
from those faced by traditional LLMs. Existing research has demonstrated that
LLMs can leak sensitive information through training data memorization or
adversarial prompts, and RAG systems inherit many of these vulnerabilities. At
the same time, reliance of RAG on an external knowledge base opens new attack
surfaces, including the potential for leaking information about the presence or
content of retrieved documents, or for injecting malicious content to
manipulate model behavior. Despite these risks, there is currently no formal
framework that defines the threat landscape for RAG systems. In this paper, we
address a critical gap in the literature by proposing, to the best of our
knowledge, the first formal threat model for retrieval-RAG systems. We
introduce a structured taxonomy of adversary types based on their access to
model components and data, and we formally define key threat vectors such as
document-level membership inference and data poisoning, which pose serious
privacy and integrity risks in real-world deployments. By establishing formal
definitions and attack models, our work lays the foundation for a more rigorous
and principled understanding of privacy and security in RAG systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20283v1">Monitoring Violations of Differential Privacy over Time</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36">  
  <p><b>Published on:</b> 2025-09-24T16:15:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Önder Askin, Tim Kutta, Holger Dette</p>
    <p><b>Summary:</b> Auditing differential privacy has emerged as an important area of research
that supports the design of privacy-preserving mechanisms. Privacy audits help
to obtain empirical estimates of the privacy parameter, to expose flawed
implementations of algorithms and to compare practical with theoretical privacy
guarantees. In this work, we investigate an unexplored facet of privacy
auditing: the sustained auditing of a mechanism that can go through changes
during its development or deployment. Monitoring the privacy of algorithms over
time comes with specific challenges. Running state-of-the-art (static) auditors
repeatedly requires excessive sampling efforts, while the reliability of such
methods deteriorates over time without proper adjustments. To overcome these
obstacles, we present a new monitoring procedure that extracts information from
the entire deployment history of the algorithm. This allows us to reduce
sampling efforts, while sustaining reliable outcomes of our auditor. We derive
formal guarantees with regard to the soundness of our methods and evaluate
their performance for important mechanisms from the literature. Our theoretical
findings and experiments demonstrate the efficacy of our approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20153v1">Affective Computing and Emotional Data: Challenges and Implications in
  Privacy Regulations, The AI Act, and Ethics in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-24T14:18:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicola Fabiano</p>
    <p><b>Summary:</b> This paper examines the integration of emotional intelligence into artificial
intelligence systems, with a focus on affective computing and the growing
capabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to
recognize and respond to human emotions. Drawing on interdisciplinary research
that combines computer science, psychology, and neuroscience, the study
analyzes foundational neural architectures - CNNs for processing facial
expressions and RNNs for sequential data, such as speech and text - that enable
emotion recognition. It examines the transformation of human emotional
experiences into structured emotional data, addressing the distinction between
explicit emotional data collected with informed consent in research settings
and implicit data gathered passively through everyday digital interactions.
That raises critical concerns about lawful processing, AI transparency, and
individual autonomy over emotional expressions in digital environments. The
paper explores implications across various domains, including healthcare,
education, and customer service, while addressing challenges of cultural
variations in emotional expression and potential biases in emotion recognition
systems across different demographic groups. From a regulatory perspective, the
paper examines emotional data in the context of the GDPR and the EU AI Act
frameworks, highlighting how emotional data may be considered sensitive
personal data that requires robust safeguards, including purpose limitation,
data minimization, and meaningful consent mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20024v1">Generative Adversarial Networks Applied for Privacy Preservation in
  Biometric-Based Authentication and Identification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-24T11:39:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lubos Mjachky, Ivan Homoliak</p>
    <p><b>Summary:</b> Biometric-based authentication systems are getting broadly adopted in many
areas. However, these systems do not allow participating users to influence the
way their data is used. Furthermore, the data may leak and can be misused
without the users' knowledge. In this paper, we propose a new authentication
method that preserves the privacy of individuals and is based on a generative
adversarial network (GAN). Concretely, we suggest using the GAN for translating
images of faces to a visually private domain (e.g., flowers or shoes).
Classifiers, which are used for authentication purposes, are then trained on
the images from the visually private domain. Based on our experiments, the
method is robust against attacks and still provides meaningful utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19925v1">CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-24T09:29:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ajeet Kumar Singh, Rajsabi Surya, Anurag Tripathi, Santanu Choudhury, Sudhir Bisane</p>
    <p><b>Summary:</b> As enterprises increasingly integrate cloud-based large language models
(LLMs) such as ChatGPT and Gemini into their legal document workflows,
protecting sensitive contractual information - including Personally
Identifiable Information (PII) and commercially sensitive clauses - has emerged
as a critical challenge. In this work, we propose CON-QA, a hybrid
privacy-preserving framework designed specifically for secure question
answering over enterprise contracts, effectively combining local and
cloud-hosted LLMs. The CON-QA framework operates through three stages: (i)
semantic query decomposition and query-aware document chunk retrieval using a
locally deployed LLM analysis, (ii) anonymization of detected sensitive
entities via a structured one-to-many mapping scheme, ensuring semantic
coherence while preventing cross-session entity inference attacks, and (iii)
anonymized response generation by a cloud-based LLM, with accurate
reconstruction of the original answer locally using a session-consistent
many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce
CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world
CUAD contract documents, encompassing simple, complex, and summarization-style
queries. Empirical evaluations, complemented by detailed human assessments,
confirm that CON-QA effectively maintains both privacy and utility, preserves
answer quality, maintains fidelity to legal clause semantics, and significantly
mitigates privacy risks, demonstrating its practical suitability for secure,
enterprise-level contract documents.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19906v1">Voice Privacy Preservation with Multiple Random Orthogonal Secret Keys:
  Attack Resistance Analysis</a></h3>
  
  <p><b>Published on:</b> 2025-09-24T09:05:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kohei Tanaka, Hitoshi Kiya, Sayaka Shiota</p>
    <p><b>Summary:</b> Recently, opportunities to transmit speech data to deep learning models
executed in the cloud have increased. This has led to growing concerns about
speech privacy, including both speaker-specific information and the linguistic
content of utterances. As an approach to preserving speech privacy, a speech
privacy-preserving method based on encryption using a secret key with a random
orthogonal matrix has been proposed. This method enables cloud-based model
inference while concealing both the speech content and the speaker identity.
However, the method has limited attack resistance and is constrained in terms
of the deep learning models to which the encryption can be applied. In this
work, we propose a method that enhances the attack resistance of the
conventional speech privacy-preserving technique by employing multiple random
orthogonal matrices as secret keys. We also introduce approaches to relax the
model constraints, enabling the application of our method to a broader range of
deep learning models. Furthermore, we investigate the robustness of the
proposed method against attacks using extended attack scenarios based on the
scenarios employed in the Voice Privacy Challenge. Our experimental results
confirmed that the proposed method maintains privacy protection performance for
speaker concealment, even under more powerful attack scenarios not considered
in prior work.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19661v1">Consistent Estimation of Numerical Distributions under Local
  Differential Privacy by Wavelet Expansion</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-24T00:37:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Puning Zhao, Zhikun Zhang, Bo Sun, Li Shen, Liang Zhang, Shaowei Wang, Zhe Liu</p>
    <p><b>Summary:</b> Distribution estimation under local differential privacy (LDP) is a
fundamental and challenging task. Significant progresses have been made on
categorical data. However, due to different evaluation metrics, these methods
do not work well when transferred to numerical data. In particular, we need to
prevent the probability mass from being misplaced far away. In this paper, we
propose a new approach that express the sample distribution using wavelet
expansions. The coefficients of wavelet series are estimated under LDP. Our
method prioritizes the estimation of low-order coefficients, in order to ensure
accurate estimation at macroscopic level. Therefore, the probability mass is
prevented from being misplaced too far away from its ground truth. We establish
theoretical guarantees for our methods. Experiments show that our wavelet
expansion method significantly outperforms existing solutions under Wasserstein
and KS distances.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19599v1">Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method
  for Multi-Agent Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-23T21:46:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Danilo Trombino, Vincenzo Pecorella, Alessandro de Giulii, Davide Tresoldi</p>
    <p><b>Summary:</b> Multi-agent systems (MAS) are increasingly tasked with solving complex,
knowledge-intensive problems where effective agent orchestration is critical.
Conventional orchestration methods rely on static agent descriptions, which
often become outdated or incomplete. This limitation leads to inefficient task
routing, particularly in dynamic environments where agent capabilities
continuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a
novel approach that augments static descriptions with dynamic,
privacy-preserving relevance signals derived from each agent's internal
knowledge base (KB). In the proposed framework, when static descriptions are
insufficient for a clear routing decision, the orchestrator prompts the
subagents in parallel. Each agent then assesses the task's relevance against
its private KB, returning a lightweight ACK signal without exposing the
underlying data. These collected signals populate a shared semantic cache,
providing dynamic indicators of agent suitability for future queries. By
combining this novel mechanism with static descriptions, our method achieves
more accurate and adaptive task routing preserving agent autonomy and data
confidentiality. Benchmarks show that our KBA Orchestration significantly
outperforms static description-driven methods in routing precision and overall
system efficiency, making it suitable for large-scale systems that require
higher accuracy than standard description-driven routing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19041v1">Position: Human-Robot Interaction in Embodied Intelligence Demands a
  Shift From Static Privacy Controls to Dynamic Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-23T14:10:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuning Zhang, Hong Jia, Simin Li, Ting Dang, Yongquan `Owen' Hu, Xin Yi, Hewu Li</p>
    <p><b>Summary:</b> The reasoning capabilities of embodied agents introduce a critical,
under-explored inferential privacy challenge, where the risk of an agent
generate sensitive conclusions from ambient data. This capability creates a
fundamental tension between an agent's utility and user privacy, rendering
traditional static controls ineffective. To address this, this position paper
proposes a framework that reframes privacy as a dynamic learning problem
grounded in theory of Contextual Integrity (CI). Our approach enables agents to
proactively learn and adapt to individual privacy norms through interaction,
outlining a research agenda to develop embodied agents that are both capable
and function as trustworthy safeguards of user privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18949v1">Towards Privacy-Aware Bayesian Networks: A Credal Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-23T12:58:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Niccolò Rocchi, Fabio Stella, Cassio de Campos</p>
    <p><b>Summary:</b> Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18871v1">R-CONV++: Uncovering Privacy Vulnerabilities through Analytical Gradient
  Inversion Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-23T10:10:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tamer Ahmed Eltaras, Qutaibah Malluhi, Alessandro Savino, Stefano Di Carlo, Adnan Qayyum</p>
    <p><b>Summary:</b> Federated learning has emerged as a prominent privacy-preserving technique
for leveraging large-scale distributed datasets by sharing gradients instead of
raw data. However, recent studies indicate that private training data can still
be exposed through gradient inversion attacks. While earlier analytical methods
have demonstrated success in reconstructing input data from fully connected
layers, their effectiveness significantly diminishes when applied to
convolutional layers, high-dimensional inputs, and scenarios involving multiple
training examples. This paper extends our previous work \cite{eltaras2024r} and
proposes three advanced algorithms to broaden the applicability of gradient
inversion attacks. The first algorithm presents a novel data leakage method
that efficiently exploits convolutional layer gradients, demonstrating that
even with non-fully invertible activation functions, such as ReLU, training
samples can be analytically reconstructed directly from gradients without the
need to reconstruct intermediate layer outputs. Building on this foundation,
the second algorithm extends this analytical approach to support
high-dimensional input data, substantially enhancing its utility across complex
real-world datasets. The third algorithm introduces an innovative analytical
method for reconstructing mini-batches, addressing a critical gap in current
research that predominantly focuses on reconstructing only a single training
example. Unlike previous studies that focused mainly on the weight constraints
of convolutional layers, our approach emphasizes the pivotal role of gradient
constraints, revealing that successful attacks can be executed with fewer than
5\% of the constraints previously deemed necessary in certain layers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18696v1">FlowCrypt: Flow-Based Lightweight Encryption with Near-Lossless Recovery
  for Cloud Photo Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-23T06:25:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaohui Yang, Ping Ping, Feng Xu</p>
    <p><b>Summary:</b> The widespread adoption of smartphone photography has led users to
increasingly rely on cloud storage for personal photo archiving and sharing,
raising critical privacy concerns. Existing deep learning-based image
encryption schemes, typically built upon CNNs or GANs, often depend on
traditional cryptographic algorithms and lack inherent architectural
reversibility, resulting in limited recovery quality and poor robustness.
Invertible neural networks (INNs) have emerged to address this issue by
enabling reversible transformations, yet the first INN-based encryption scheme
still relies on an auxiliary reference image and discards by-product
information before decryption, leading to degraded recovery and limited
practicality. To address these limitations, this paper proposes FlowCrypt, a
novel flow-based image encryption framework that simultaneously achieves
near-lossless recovery, high security, and lightweight model design. FlowCrypt
begins by applying a key-conditioned random split to the input image, enhancing
forward-process randomness and encryption strength. The resulting components
are processed through a Flow-based Encryption/Decryption (FED) module composed
of invertible blocks, which share parameters across encryption and decryption.
Thanks to its reversible architecture and reference-free design, FlowCrypt
ensures high-fidelity image recovery. Extensive experiments show that FlowCrypt
achieves recovery quality with 100dB on three datasets, produces uniformly
distributed cipher images, and maintains a compact architecture with only 1M
parameters, making it suitable for mobile and edge-device applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18413v1">VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership
  Inference Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-22T20:57:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Efthymios Tsaprazlis, Thanathai Lertpetchpun, Tiantian Feng, Sai Praneeth Karimireddy, Shrikanth Narayanan</p>
    <p><b>Summary:</b> Voice anonymization aims to conceal speaker identity and attributes while
preserving intelligibility, but current evaluations rely almost exclusively on
Equal Error Rate (EER) that obscures whether adversaries can mount
high-precision attacks. We argue that privacy should instead be evaluated in
the low false-positive rate (FPR) regime, where even a small number of
successful identifications constitutes a meaningful breach. To this end, we
introduce VoxGuard, a framework grounded in differential privacy and membership
inference that formalizes two complementary notions: User Privacy, preventing
speaker re-identification, and Attribute Privacy, protecting sensitive traits
such as gender and accent. Across synthetic and real datasets, we find that
informed adversaries, especially those using fine-tuned models and
max-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR
despite similar EER. For attributes, we show that simple transparent attacks
recover gender and accent with near-perfect accuracy even after anonymization.
Our results demonstrate that EER substantially underestimates leakage,
highlighting the need for low-FPR evaluation, and recommend VoxGuard as a
benchmark for evaluating privacy leakage.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18311v1">Fine-Tuning Robot Policies While Maintaining User Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E">
  <p><b>Published on:</b> 2025-09-22T18:36:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Benjamin A. Christie, Sagar Parekh, Dylan P. Losey</p>
    <p><b>Summary:</b> Recent works introduce general-purpose robot policies. These policies provide
a strong prior over how robots should behave -- e.g., how a robot arm should
manipulate food items. But in order for robots to match an individual person's
needs, users typically fine-tune these generalized policies -- e.g., showing
the robot arm how to make their own preferred dinners. Importantly, during the
process of personalizing robots, end-users leak data about their preferences,
habits, and styles (e.g., the foods they prefer to eat). Other agents can
simply roll-out the fine-tuned policy and see these personally-trained
behaviors. This leads to a fundamental challenge: how can we develop robots
that personalize actions while keeping learning private from external agents?
We here explore this emerging topic in human-robot interaction and develop
PRoP, a model-agnostic framework for personalized and private robot policies.
Our core idea is to equip each user with a unique key; this key is then used to
mathematically transform the weights of the robot's network. With the correct
key, the robot's policy switches to match that user's preferences -- but with
incorrect keys, the robot reverts to its baseline behaviors. We show the
general applicability of our method across multiple model types in imitation
learning, reinforcement learning, and classification tasks. PRoP is practically
advantageous because it retains the architecture and behaviors of the original
policy, and experimentally outperforms existing encoder-based approaches. See
videos and code here: https://prop-icra26.github.io.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18014v1">Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data
  Synthesis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-09-22T16:53:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Joshua Ward, Xiaofeng Lin, Chi-Hua Wang, Guang Cheng</p>
    <p><b>Summary:</b> Tabular Generative Models are often argued to preserve privacy by creating
synthetic datasets that resemble training data. However, auditing their
empirical privacy remains challenging, as commonly used similarity metrics fail
to effectively characterize privacy risk. Membership Inference Attacks (MIAs)
have recently emerged as a method for evaluating privacy leakage in synthetic
data, but their practical effectiveness is limited. Numerous attacks exist
across different threat models, each with distinct implementations targeting
various sources of privacy leakage, making them difficult to apply
consistently. Moreover, no single attack consistently outperforms the others,
leading to a routine underestimation of privacy risk.
  To address these issues, we propose a unified, model-agnostic threat
framework that deploys a collection of attacks to estimate the maximum
empirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an
open-source Python library that streamlines this auditing process through a
novel testbed that integrates seamlessly into existing synthetic data
evaluation pipelines through a Scikit-Learn-like API. Our software implements
13 attack methods through a Scikit-Learn-like API, designed to enable fast
systematic estimation of privacy leakage for practitioners as well as
facilitate the development of new attacks and experiments for researchers.
  We demonstrate our framework's utility in the largest tabular synthesis
privacy benchmark to date, revealing that higher synthetic data quality
corresponds to greater privacy leakage, that similarity-based privacy metrics
show weak correlation with MIA results, and that the differentially private
generator PATEGAN can fail to preserve privacy under such attacks. This
underscores the necessity of MIA-based auditing when designing and deploying
Tabular Generative Models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.17871v1">B-Privacy: Defining and Enforcing Privacy in Weighted Voting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-22T15:11:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Samuel Breckenridge, Dani Vilardell, Andrés Fábrega, Amy Zhao, Patrick McCorry, Rafael Solari, Ari Juels</p>
    <p><b>Summary:</b> In traditional, one-vote-per-person voting systems, privacy equates with
ballot secrecy: voting tallies are published, but individual voters' choices
are concealed.
  Voting systems that weight votes in proportion to token holdings, though, are
now prevalent in cryptocurrency and web3 systems. We show that these
weighted-voting systems overturn existing notions of voter privacy. Our
experiments demonstrate that even with secret ballots, publishing raw tallies
often reveals voters' choices.
  Weighted voting thus requires a new framework for privacy. We introduce a
notion called B-privacy whose basis is bribery, a key problem in voting systems
today. B-privacy captures the economic cost to an adversary of bribing voters
based on revealed voting tallies.
  We propose a mechanism to boost B-privacy by noising voting tallies. We prove
bounds on its tradeoff between B-privacy and transparency, meaning
reported-tally accuracy. Analyzing 3,582 proposals across 30 Decentralized
Autonomous Organizations (DAOs), we find that the prevalence of large voters
("whales") limits the effectiveness of any B-Privacy-enhancing technique.
However, our mechanism proves to be effective in cases without extreme voting
weight concentration: among proposals requiring coalitions of $\geq5$ voters to
flip outcomes, our mechanism raises B-privacy by a geometric mean factor of
$4.1\times$.
  Our work offers the first principled guidance on transparency-privacy
tradeoffs in weighted-voting systems, complementing existing approaches that
focus on ballot secrecy and revealing fundamental constraints that voting
weight concentration imposes on privacy mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.17488v1">Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation
  for LLM-Powered Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-22T08:19:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shouju Wang, Fenglin Yu, Xirui Liu, Xiaoting Qin, Jue Zhang, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan</p>
    <p><b>Summary:</b> The increasing autonomy of LLM agents in handling sensitive communications,
accelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A)
frameworks, creates urgent privacy challenges. While recent work reveals
significant gaps between LLMs' privacy Q&A performance and their agent
behavior, existing benchmarks remain limited to static, simplified scenarios.
We present PrivacyChecker, a model-agnostic, contextual integrity based
mitigation approach that effectively reduces privacy leakage from 36.08% to
7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving
task helpfulness. We also introduce PrivacyLens-Live, transforming static
benchmarks into dynamic MCP and A2A environments that reveal substantially
higher privacy risks in practical. Our modular mitigation approach integrates
seamlessly into agent protocols through three deployment strategies, providing
practical privacy protection for the emerging agentic ecosystem. Our data and
code will be made available at https://aka.ms/privacy_in_action.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.17266v1">Privacy-Preserving State Estimation with Crowd Sensors: An
  Information-Theoretic Respective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">  
  <p><b>Published on:</b> 2025-09-21T22:44:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Farhad Farokhi</p>
    <p><b>Summary:</b> Privacy-preserving state estimation for linear time-invariant dynamical
systems with crowd sensors is considered. At any time step, the estimator has
access to measurements from a randomly selected sensor from a pool of sensors
with pre-specified models and noise profiles. A Luenberger-like observer is
used to fuse the measurements with the underlying model of the system to
recursively generate the state estimates. An additive privacy-preserving noise
is used to constrain information leakage. Information leakage is measured via
mutual information between the identity of the sensors and the state estimate
conditioned on the actual state of the system. This captures an omnipotent
adversary that not only can access state estimates but can also gather direct
high-quality state measurements. Any prescribed level of information leakage is
shown to be achievable by appropriately selecting the variance of the
privacy-preserving noise. Therefore, privacy-utility trade-off can be
fine-tuned.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.16962v1">Temporal Drift in Privacy Recall: Users Misremember From Verbatim Loss
  to Gist-Based Overexposure Over Time</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-21T07:50:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoze Guo, Ziqi Wei</p>
    <p><b>Summary:</b> With social media content traversing the different platforms, occasionally
resurfacing after periods of time, users are increasingly prone to unintended
disclosure resulting from a misremembered acceptance of privacy. Context
collapse and interface cues are two factors considered by prior researchers,
yet we know less about how time-lapse basically alters recall of past audiences
destined for exposure. Likewise, the design space for mitigating this temporal
exposure risk remains underexplored. Our work theorizes temporal drift in
privacy recall as verbatim memory of prior settings blowing apart and
eventually settling with gist-based heuristics, which more often than not
select an audience larger than the original one. Grounded in memory research,
contextual integrity, and usable privacy, we examine why such a drift occurs,
why it tends to bias toward broader sharing, and how it compounds upon repeat
exposure. Following that, we suggest provenance-forward interface schemes and a
risk-based evaluation framework that mutates recall into recognition. The merit
of our work lies in establishing a temporal awareness of privacy design as an
essential safety rail against inadvertent overexposure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.16915v1">Differential Privacy for Euclidean Jordan Algebra with Applications to
  Private Symmetric Cone Programming</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-21T04:34:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhao Song, Jianfei Xue, Lichen Zhang</p>
    <p><b>Summary:</b> In this paper, we study differentially private mechanisms for functions whose
outputs lie in a Euclidean Jordan algebra. Euclidean Jordan algebras capture
many important mathematical structures and form the foundation of linear
programming, second-order cone programming, and semidefinite programming. Our
main contribution is a generic Gaussian mechanism for such functions, with
sensitivity measured in $\ell_2$, $\ell_1$, and $\ell_\infty$ norms. Notably,
this framework includes the important case where the function outputs are
symmetric matrices, and sensitivity is measured in the Frobenius, nuclear, or
spectral norm. We further derive private algorithms for solving symmetric cone
programs under various settings, using a combination of the multiplicative
weights update method and our generic Gaussian mechanism. As an application, we
present differentially private algorithms for semidefinite programming,
resolving a major open question posed by [Hsu, Roth, Roughgarden, and Ullman,
ICALP 2014].</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.15755v1">Utility-based Privacy Preserving Data Mining</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-09-19T08:30:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qingfeng Zhou, Wensheng Gan, Zhenlian Qi, Philip S. Yu</p>
    <p><b>Summary:</b> With the advent of big data, periodic pattern mining has demonstrated
significant value in real-world applications, including smart home systems,
healthcare systems, and the medical field. However, advances in network
technology have enabled malicious actors to extract sensitive information from
publicly available datasets, posing significant threats to data providers and,
in severe cases, hindering societal development. To mitigate such risks,
privacy-preserving utility mining (PPUM) has been proposed. However, PPUM is
unsuitable for addressing privacy concerns in periodic information mining. To
address this issue, we innovatively extend the existing PPUM framework and
propose two algorithms, Maximum sensitive Utility-MAximum maxPer item (MU-MAP)
and Maximum sensitive Utility-MInimum maxPer item (MU-MIP). These algorithms
aim to hide sensitive periodic high-utility itemsets while generating sanitized
datasets. To enhance the efficiency of the algorithms, we designed two novel
data structures: the Sensitive Itemset List (SISL) and the Sensitive Item List
(SIL), which store essential information about sensitive itemsets and their
constituent items. Moreover, several performance metrics were employed to
evaluate the performance of our algorithms compared to the state-of-the-art
PPUM algorithms. The experimental results show that our proposed algorithms
achieve an Artificial Cost (AC) value of 0 on all datasets when hiding
sensitive itemsets. In contrast, the traditional PPUM algorithm yields non-zero
AC. This indicates that our algorithms can successfully hide sensitive periodic
itemsets without introducing misleading patterns, whereas the PPUM algorithm
generates additional itemsets that may interfere with user decision-making.
Moreover, the results also reveal that our algorithms maintain Database Utility
Similarity (DUS) of over 90\% after the sensitive itemsets are hidden.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18187v1">V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor
  Fusion Framework for Road Safety & Driver Behaviour Modelling</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-18T21:55:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Muhammad Naveed, Nazia Perwaiz, Sidra Sultana, Mohaira Ahmad, Muhammad Moazam Fraz</p>
    <p><b>Summary:</b> Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.15047v1">Distributed Batch Matrix Multiplication: Trade-Offs in Download Rate,
  Randomness, and Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-09-18T15:10:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amirhosein Morteza, Remi A. Chou</p>
    <p><b>Summary:</b> We study the trade-off between communication rate and privacy for distributed
batch matrix multiplication of two independent sequences of matrices
$\mathbf{A}$ and $\mathbf{B}$ with uniformly distributed entries. In our
setting, $\mathbf{B}$ is publicly accessible by all the servers while
$\mathbf{A}$ must remain private. A user is interested in evaluating the
product $\mathbf{AB}$ with the responses from the $k$ fastest servers. For a
given parameter $\alpha \in [0, 1]$, our privacy constraint must ensure that
any set of $\ell$ colluding servers cannot learn more than a fraction $\alpha$
of $\mathbf{A}$. Additionally, we study the trade-off between the amount of
local randomness needed at the encoder and privacy. Finally, we establish the
optimal trade-offs when the matrices are square and identify a linear
relationship between information leakage and communication rate.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.15278v1">Assessing metadata privacy in neuroimaging</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> 
  <p><b>Published on:</b> 2025-09-18T12:56:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Emilie Kibsgaard, Anita Sue Jwa, Christopher J Markiewicz, David Rodriguez Gonzalez, Judith Sainz Pardo, Russell A. Poldrack, Cyril R. Pernet</p>
    <p><b>Summary:</b> The ethical and legal imperative to share research data without causing harm
requires careful attention to privacy risks. While mounting evidence
demonstrates that data sharing benefits science, legitimate concerns persist
regarding the potential leakage of personal information that could lead to
reidentification and subsequent harm. We reviewed metadata accompanying
neuroimaging datasets from six heterogeneous studies openly available on
OpenNeuro, involving participants across the lifespan, from children to older
adults, with and without clinical diagnoses, and including associated clinical
score data. Using metaprivBIDS (https://github.com/CPernet/metaprivBIDS), a
novel tool for the systematic assessment of privacy in tabular data, we found
that privacy is generally well maintained, with serious vulnerabilities being
rare. Nonetheless, minor issues were identified in nearly all datasets and
warrant mitigation. Notably, clinical score data (e.g., neuropsychological
results) posed minimal reidentification risk, whereas demographic variables
(age, sex, race, income, and geolocation) represented the principal privacy
vulnerabilities. We outline practical measures to address these risks, enabling
safer data sharing practices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14603v1">Towards Privacy-Preserving and Heterogeneity-aware Split Federated
  Learning via Probabilistic Masking</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-18T04:28:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xingchen Wang, Feijie Wu, Chenglin Miao, Tianchun Li, Haoyu Hu, Qiming Cao, Jing Gao, Lu Su</p>
    <p><b>Summary:</b> Split Federated Learning (SFL) has emerged as an efficient alternative to
traditional Federated Learning (FL) by reducing client-side computation through
model partitioning. However, exchanging of intermediate activations and model
updates introduces significant privacy risks, especially from data
reconstruction attacks that recover original inputs from intermediate
representations. Existing defenses using noise injection often degrade model
performance. To overcome these challenges, we present PM-SFL, a scalable and
privacy-preserving SFL framework that incorporates Probabilistic Mask training
to add structured randomness without relying on explicit noise. This mitigates
data reconstruction risks while maintaining model utility. To address data
heterogeneity, PM-SFL employs personalized mask learning that tailors submodel
structures to each client's local data. For system heterogeneity, we introduce
a layer-wise knowledge compensation mechanism, enabling clients with varying
resources to participate effectively under adaptive model splitting.
Theoretical analysis confirms its privacy protection, and experiments on image
and wireless sensing tasks demonstrate that PM-SFL consistently improves
accuracy, communication efficiency, and robustness to privacy attacks, with
particularly strong performance under data and system heterogeneity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14581v1">Can I Trust This Chatbot? Assessing User Privacy in AI-Healthcare
  Chatbot Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-09-18T03:29:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ramazan Yener, Guan-Hung Chen, Ece Gumusel, Masooda Bashir</p>
    <p><b>Summary:</b> As Conversational Artificial Intelligence (AI) becomes more integrated into
everyday life, AI-powered chatbot mobile applications are increasingly adopted
across industries, particularly in the healthcare domain. These chatbots offer
accessible and 24/7 support, yet their collection and processing of sensitive
health data present critical privacy concerns. While prior research has
examined chatbot security, privacy issues specific to AI healthcare chatbots
have received limited attention. Our study evaluates the privacy practices of
12 widely downloaded AI healthcare chatbot apps available on the App Store and
Google Play in the United States. We conducted a three-step assessment
analyzing: (1) privacy settings during sign-up, (2) in-app privacy controls,
and (3) the content of privacy policies. The analysis identified significant
gaps in user data protection. Our findings reveal that half of the examined
apps did not present a privacy policy during sign up, and only two provided an
option to disable data sharing at that stage. The majority of apps' privacy
policies failed to address data protection measures. Moreover, users had
minimal control over their personal data. The study provides key insights for
information science researchers, developers, and policymakers to improve
privacy protections in AI healthcare chatbot apps.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14050v1">AI For Privacy in Smart Homes: Exploring How Leveraging AI-Powered Smart
  Devices Enhances Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-09-17T14:53:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wael Albayaydh, Ivan Flechais, Rui Zhao, Jood Albayaydh</p>
    <p><b>Summary:</b> Privacy concerns and fears of unauthorized access in smart home devices often
stem from misunderstandings about how data is collected, used, and protected.
This study explores how AI-powered tools can offer innovative privacy
protections through clear, personalized, and contextual support to users.
Through 23 in-depth interviews with users, AI developers, designers, and
regulators, and using Grounded Theory analysis, we identified two key themes:
Aspirations for AI-Enhanced Privacy - how users perceive AI's potential to
empower them, address power imbalances, and improve ease of use- and AI
Ethical, Security, and Regulatory Considerations-challenges in strengthening
data security, ensuring regulatory compliance, and promoting ethical AI
practices. Our findings contribute to the field by uncovering user aspirations
for AI-driven privacy solutions, identifying key security and ethical
challenges, and providing actionable recommendations for all stakeholders,
particularly targeting smart device designers and AI developers, to guide the
co-design of AI tools that enhance privacy protection in smart home devices. By
bridging the gap between user expectations, AI capabilities, and regulatory
frameworks, this work offers practical insights for shaping the future of
privacy-conscious AI integration in smart homes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13987v1">Differential Privacy in Federated Learning: Mitigating Inference Attacks
  with Randomized Response</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-17T13:59:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ozer Ozturk, Busra Buyuktanir, Gozde Karatas Baydogmus, Kazim Yildiz</p>
    <p><b>Summary:</b> Machine learning models used for distributed architectures consisting of
servers and clients require large amounts of data to achieve high accuracy.
Data obtained from clients are collected on a central server for model
training. However, storing data on a central server raises concerns about
security and privacy. To address this issue, a federated learning architecture
has been proposed. In federated learning, each client trains a local model
using its own data. The trained models are periodically transmitted to the
central server. The server then combines the received models using federated
aggregation algorithms to obtain a global model. This global model is
distributed back to the clients, and the process continues in a cyclical
manner. Although preventing data from leaving the clients enhances security,
certain concerns still remain. Attackers can perform inference attacks on the
obtained models to approximate the training dataset, potentially causing data
leakage. In this study, differential privacy was applied to address the
aforementioned security vulnerability, and a performance analysis was
conducted. The Data-Unaware Classification Based on Association (duCBA)
algorithm was used as the federated aggregation method. Differential privacy
was implemented on the data using the Randomized Response technique, and the
trade-off between security and performance was examined under different epsilon
values. As the epsilon value decreased, the model accuracy declined, and class
prediction imbalances were observed. This indicates that higher levels of
privacy do not always lead to practical outcomes and that the balance between
security and performance must be carefully considered.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13739v1">ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-09-17T06:45:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zihou Wu, Yuecheng Li, Tianchi Liao, Jian Lou, Chuan Chen</p>
    <p><b>Summary:</b> Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13627v1">Secure, Scalable and Privacy Aware Data Strategy in Cloud</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-09-17T01:56:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vijay Kumar Butte, Sujata Butte</p>
    <p><b>Summary:</b> The enterprises today are faced with the tough challenge of processing,
storing large amounts of data in a secure, scalable manner and enabling
decision makers to make quick, informed data driven decisions. This paper
addresses this challenge and develops an effective enterprise data strategy in
the cloud. Various components of an effective data strategy are discussed and
architectures addressing security, scalability and privacy aspects are
provided.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13625v3">Privacy-Aware In-Context Learning for Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-17T01:50:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bishnu Bhusal, Manoj Acharya, Ramneet Kaur, Colin Samplawski, Anirban Roy, Adam D. Cobb, Rohit Chadha, Susmit Jha</p>
    <p><b>Summary:</b> Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models. The proposed method performs
inference on private records and aggregates the resulting per-token output
distributions. This enables the generation of longer and coherent synthetic
text while maintaining privacy guarantees. Additionally, we propose a simple
blending operation that combines private and public inference to further
enhance utility. Empirical evaluations demonstrate that our approach
outperforms previous state-of-the-art methods on in-context-learning (ICL)
tasks, making it a promising direction for privacy-preserving text generation
while maintaining high utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13509v1">Practitioners' Perspectives on a Differential Privacy Deployment
  Registry</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-16T20:15:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Priyanka Nanayakkara, Elena Ghazi, Salil Vadhan</p>
    <p><b>Summary:</b> Differential privacy (DP) -- a principled approach to producing statistical
data products with strong, mathematically provable privacy guarantees for the
individuals in the underlying dataset -- has seen substantial adoption in
practice over the past decade. Applying DP requires making several
implementation decisions, each with significant impacts on data privacy and/or
utility. Hence, to promote shared learning and accountability around DP
deployments, Dwork, Kohli, and Mulligan (2019) proposed a public-facing
repository ("registry") of DP deployments. The DP community has recently
started to work toward realizing this vision. We contribute to this effort by
(1) developing a holistic, hierarchical schema to describe any given DP
deployment and (2) designing and implementing an interactive interface to act
as a registry where practitioners can access information about past DP
deployments. We (3) populate our interface with 21 real-world DP deployments
and (4) conduct an exploratory user study with DP practitioners ($n=16$) to
understand how they would use the registry, as well as what challenges and
opportunities they foresee around its adoption. We find that participants were
enthusiastic about the registry as a valuable resource for evaluating prior
deployments and making future deployments. They also identified several
opportunities for the registry, including that it can become a "hub" for the
community and support broader communication around DP (e.g., to legal teams).
At the same time, they identified challenges around the registry gaining
adoption, including the effort and risk involved with making implementation
choices public and moderating the quality of entries. Based on our findings, we
offer recommendations for encouraging adoption and increasing the registry's
value not only to DP practitioners, but also to policymakers, data users, and
data subjects.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14284v1">The Sum Leaks More Than Its Parts: Compositional Privacy Risks and
  Mitigations in Multi-Agent Collaboration</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-09-16T16:57:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vaidehi Patil, Elias Stengel-Eskin, Mohit Bansal</p>
    <p><b>Summary:</b> As large language models (LLMs) become integral to multi-agent systems, new
privacy risks emerge that extend beyond memorization, direct inference, or
single-turn evaluations. In particular, seemingly innocuous responses, when
composed across interactions, can cumulatively enable adversaries to recover
sensitive information, a phenomenon we term compositional privacy leakage. We
present the first systematic study of such compositional privacy leaks and
possible mitigation methods in multi-agent LLM systems. First, we develop a
framework that models how auxiliary knowledge and agent interactions jointly
amplify privacy risks, even when each response is benign in isolation. Next, to
mitigate this, we propose and evaluate two defense strategies: (1)
Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent
by anticipating how their outputs may be exploited by adversaries, and (2)
Collaborative Consensus Defense (CoDef), where responder agents collaborate
with peers who vote based on a shared aggregated state to restrict sensitive
information spread. Crucially, we balance our evaluation across compositions
that expose sensitive information and compositions that yield benign
inferences. Our experiments quantify how these defense strategies differ in
balancing the privacy-utility trade-off. We find that while chain-of-thought
alone offers limited protection to leakage (~39% sensitive blocking rate), our
ToM defense substantially improves sensitive query blocking (up to 97%) but can
reduce benign task success. CoDef achieves the best balance, yielding the
highest Balanced Outcome (79.8%), highlighting the benefit of combining
explicit reasoning with defender collaboration. Together, our results expose a
new class of risks in collaborative LLM deployments and provide actionable
insights for designing safeguards against compositional, context-driven privacy
leakage.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13051v1">More than Meets the Eye: Understanding the Effect of Individual Objects
  on Perceived Visual Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-16T13:10:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mete Harun Akcay, Siddharth Prakash Rao, Alexandros Bakas, Buse Gul Atli</p>
    <p><b>Summary:</b> User-generated content, such as photos, comprises the majority of online
media content and drives engagement due to the human ability to process visual
information quickly. Consequently, many online platforms are designed for
sharing visual content, with billions of photos posted daily. However, photos
often reveal more than they intended through visible and contextual cues,
leading to privacy risks. Previous studies typically treat privacy as a
property of the entire image, overlooking individual objects that may carry
varying privacy risks and influence how users perceive it. We address this gap
with a mixed-methods study (n = 92) to understand how users evaluate the
privacy of images containing multiple sensitive objects. Our results reveal
mental models and nuanced patterns that uncover how granular details, such as
photo-capturing context and co-presence of other objects, affect privacy
perceptions. These novel insights could enable personalized, context-aware
privacy protection designs on social media and future technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12958v1">Forget What's Sensitive, Remember What Matters: Token-Level Differential
  Privacy in Memory Sculpting for Continual Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-16T11:01:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bihao Zhan, Jie Zhou, Junsong Li, Yutao Yang, Shilian Chen, Qianjun Pan, Xin Li, Wen Wu, Xingjiao Wu, Qin Chen, Hang Yan, Liang He</p>
    <p><b>Summary:</b> Continual Learning (CL) models, while adept at sequential knowledge
acquisition, face significant and often overlooked privacy challenges due to
accumulating diverse information. Traditional privacy methods, like a uniform
Differential Privacy (DP) budget, indiscriminately protect all data, leading to
substantial model utility degradation and hindering CL deployment in
privacy-sensitive areas. To overcome this, we propose a privacy-enhanced
continual learning (PeCL) framework that forgets what's sensitive and remembers
what matters. Our approach first introduces a token-level dynamic Differential
Privacy strategy that adaptively allocates privacy budgets based on the
semantic sensitivity of individual tokens. This ensures robust protection for
private entities while minimizing noise injection for non-sensitive, general
knowledge. Second, we integrate a privacy-guided memory sculpting module. This
module leverages the sensitivity analysis from our dynamic DP mechanism to
intelligently forget sensitive information from the model's memory and
parameters, while explicitly preserving the task-invariant historical knowledge
crucial for mitigating catastrophic forgetting. Extensive experiments show that
PeCL achieves a superior balance between privacy preserving and model utility,
outperforming baseline models by maintaining high accuracy on previous tasks
while ensuring robust privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12899v1">EByFTVeS: Efficient Byzantine Fault Tolerant-based Verifiable
  Secret-sharing in Distributed Privacy-preserving Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-16T09:54:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhen Li, Zijian Zhang, Wenjin Yang, Pengbo Wang, Zhaoqi Wang, Meng Li, Yan Wu, Xuyang Liu, Jing Sun, Liehuang Zhu</p>
    <p><b>Summary:</b> Verifiable Secret Sharing (VSS) has been widespread in Distributed
Privacy-preserving Machine Learning (DPML), because invalid shares from
malicious dealers or participants can be recognized by verifying the commitment
of the received shares for honest participants. However, the consistency and
the computation and communitation burden of the VSS-based DPML schemes are
still two serious challenges. Although Byzantine Fault Tolerance (BFT) system
has been brought to guarantee the consistency and improve the efficiency of the
existing VSS-based DPML schemes recently, we explore an Adaptive Share Delay
Provision (ASDP) strategy, and launch an ASDP-based Customized Model Poisoning
Attack (ACuMPA) for certain participants in this paper. We theoretically
analyzed why the ASDP strategy and the ACuMPA algorithm works to the existing
schemes. Next, we propose an [E]fficient [By]zantine [F]ault [T]olerant-based
[Ve]rifiable [S]ecret-sharing (EByFTVeS) scheme. Finally, the validity,
liveness, consistency and privacy of the EByFTVeS scheme are theoretically
analyzed, while the efficiency of the EByFTVeS scheme outperforms that of
the-state-of-art VSS scheme according to comparative experiment results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14278v1">Beyond Data Privacy: New Privacy Risks for Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-16T09:46:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuntao Du, Zitao Li, Ninghui Li, Bolin Ding</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have achieved remarkable progress in natural
language understanding, reasoning, and autonomous decision-making. However,
these advancements have also come with significant privacy concerns. While
significant research has focused on mitigating the data privacy risks of LLMs
during various stages of model training, less attention has been paid to new
threats emerging from their deployment. The integration of LLMs into widely
used applications and the weaponization of their autonomous abilities have
created new privacy vulnerabilities. These vulnerabilities provide
opportunities for both inadvertent data leakage and malicious exfiltration from
LLM-powered systems. Additionally, adversaries can exploit these systems to
launch sophisticated, large-scale privacy attacks, threatening not only
individual privacy but also financial security and societal trust. In this
paper, we systematically examine these emerging privacy risks of LLMs. We also
discuss potential mitigation strategies and call for the research community to
broaden its focus beyond data privacy risks, developing new defenses to address
the evolving threats posed by increasingly powerful LLMs and LLM-powered
systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14275v1">FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated
  LLMs in Mental Health</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-16T07:08:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nobin Sarwar, Shubhashis Roy Dipta</p>
    <p><b>Summary:</b> Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive
domains (e.g., mental health) requires balancing strict confidentiality with
model utility and safety. We propose FedMentor, a federated fine-tuning
framework that integrates Low-Rank Adaptation (LoRA) and domain-aware
Differential Privacy (DP) to meet per-domain privacy budgets while maintaining
performance. Each client (domain) applies a custom DP noise scale proportional
to its data sensitivity, and the server adaptively reduces noise when utility
falls below a threshold. In experiments on three mental health datasets, we
show that FedMentor improves safety over standard Federated Learning without
privacy, raising safe output rates by up to three points and lowering toxicity,
while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the
non-private baseline and close to the centralized upper bound. The framework
scales to backbones with up to 1.7B parameters on single-GPU clients, requiring
< 173 MB of communication per round. FedMentor demonstrates a practical
approach to privately fine-tune LLMs for safer deployments in healthcare and
other sensitive fields.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12590v1">DPCheatSheet: Using Worked and Erroneous LLM-usage Examples to Scaffold
  Differential Privacy Implementation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-16T02:33:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shao-Yu Chu, Yuhe Tian, Yu-Xiang Wang, Haojian Jin</p>
    <p><b>Summary:</b> This paper explores how programmers without specialized expertise in
differential privacy (DP) (i.e., novices) can leverage LLMs to implement DP
programs with minimal training. We first conducted a need-finding study with 6
novices and 3 experts to understand how they utilize LLMs in DP implementation.
While DP experts can implement correct DP analyses through a few prompts,
novices struggle to articulate their requirements in prompts and lack the
skills to verify the correctness of the generated code. We then developed
DPCheatSheet, an instructional tool that helps novices implement DP using LLMs.
DPCheatSheet combines two learning concepts: it annotates an expert's workflow
with LLMs as a worked example to bridge the expert mindset to novices, and it
presents five common mistakes in LLM-based DP code generation as erroneous
examples to support error-driven learning. We demonstrated the effectiveness of
DPCheatSheet with an error identification study and an open-ended DP
implementation study.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12578v1">Conflect: Designing Reflective Thinking-Based Contextual Privacy Policy
  for Mobile Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-16T02:11:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuning Zhang, Sixing Tao, Eve He, Yuting Yang, Ying Ma, Ailei Wang, Xin Yi, Hewu Li</p>
    <p><b>Summary:</b> Privacy policies are lengthy and complex, leading to user neglect. While
contextual privacy policies (CPPs) present information at the point of risk,
they may lack engagement and disrupt tasks. We propose Conflect, an interactive
CPP for mobile apps, guided by a reflective thinking framework. Through three
workshops with experienced designers and researchers, we constructed the design
space of reflective thinking-based CPP design, and identified the disconnect
between context and action as the most critical problem. Based on participants'
feedback, we designed Conflect to use sidebar alerts, allowing users to reflect
on contextualized risks and fostering their control. Our system contextually
detects privacy risks, extracts policy segments, and automatically generates
risk descriptions with 94.0% policy extraction accuracy on CPP4APP dataset and
a 4.35s latency. A user study (N=28) demonstrated that Conflect improves user
understanding, trust, and satisfaction while lowering cognitive load compared
to CPPs, privacy policies and privacy labels.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12465v1">Efficient Privacy-Preserving Training of Quantum Neural Networks by
  Using Mixed States to Represent Input Data Ensembles</a></h3>
  
  <p><b>Published on:</b> 2025-09-15T21:20:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gaoyuan Wang, Jonathan Warrell, Mark Gerstein</p>
    <p><b>Summary:</b> Quantum neural networks (QNNs) are gaining increasing interest due to their
potential to detect complex patterns in data by leveraging uniquely quantum
phenomena. This makes them particularly promising for biomedical applications.
In these applications and in other contexts, increasing statistical power often
requires aggregating data from multiple participants. However, sharing data,
especially sensitive information like personal genomic sequences, raises
significant privacy concerns. Quantum federated learning offers a way to
collaboratively train QNN models without exposing private data. However, it
faces major limitations, including high communication overhead and the need to
retrain models when the task is modified. To overcome these challenges, we
propose a privacy-preserving QNN training scheme that utilizes mixed quantum
states to encode ensembles of data. This approach allows for the secure sharing
of statistical information while safeguarding individual data points. QNNs can
be trained directly on these mixed states, eliminating the need to access raw
data. Building on this foundation, we introduce protocols supporting
multi-party collaborative QNN training applicable across diverse domains. Our
approach enables secure QNN training with only a single round of communication
per participant, provides high training speed and offers task generality, i.e.,
new analyses can be conducted without reacquiring information from
participants. We present the theoretical foundation of our scheme's utility and
privacy protections, which prevent the recovery of individual data points and
resist membership inference attacks as measured by differential privacy. We
then validate its effectiveness on three different datasets with a focus on
genomic studies with an indication of how it can used in other domains without
adaptation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12403v1">Privacy-Driven Network Data for Smart Cities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-09-15T19:48:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tânia Carvalho, José Barata, Henish Balu, Filipa Moreira, João Bastos, Luís Antunes</p>
    <p><b>Summary:</b> A smart city is essential for sustainable urban development. In addition to
citizen engagement, a smart city enables connected infrastructure, data-driven
decision making and smart mobility. For most of these features, network data
plays a critical role, particularly from public Wi-Fi infrastructures, where
cities can benefit from optimized services such as public transport management
and the safety and efficiency of large events. One of the biggest concerns in
developing a smart city is using secure and private data. This is particularly
relevant in the case of Wi-Fi network data, where sensitive information can be
collected. This paper specifically addresses the problem of sharing secure data
to enhance the quality of the Wi-Fi network in a city. Despite the high
importance of this type of data, related work focuses on improving the safety
of mobility patterns, targeting only the protection of MAC addresses. On the
opposite side, we provide a practical methodology for safeguarding all
attributes in real Wi-Fi network data. This study was developed in
collaboration with a multidisciplinary team of legal experts, data custodians
and technical privacy specialists, resulting in high-quality data. On top of
that, we show how to integrate the legal considerations for secure data
sharing. Our approach promotes data-driven innovation and privacy awareness in
the context of smart city initiatives, which have been tested in a real
scenario.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12338v1">Privacy in continuous-variable distributed quantum sensing</a></h3>
  
  <p><b>Published on:</b> 2025-09-15T18:06:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> A. de Oliveira Junior, Anton L. Andersen, Benjamin Lundgren Larsen, Sean William Moore, Damian Markham, Masahiro Takeoka, Jonatan Bohr Brask, Ulrik L. Andersen</p>
    <p><b>Summary:</b> Can a distributed network of quantum sensors estimate a global parameter
while protecting every locally encoded value? We answer this question
affirmatively by introducing and analysing a protocol for distributed quantum
sensing in the continuous-variable regime. We consider a multipartite network
in which each node encodes a local phase into a shared entangled Gaussian
state. We show that the average phase can be estimated with high precision,
exhibiting Heisenberg scaling in the total photon number, while individual
phases are inaccessible. Although complete privacy - where all other
combinations of phases remain entirely hidden - is unattainable for finite
squeezing in multi-party settings, it emerges in the large-squeezing limit. We
further investigate the impact of displacements and optical losses, revealing
trade-offs between estimation accuracy and privacy. Finally, we benchmark the
protocol against other continuous-variable resource states.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11939v1">PrivWeb: Unobtrusive and Content-aware Privacy Protection For Web Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-15T13:58:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuning Zhang, Yutong Jiang, Rongjun Ma, Yuting Yang, Mingyao Xu, Zhixin Huang, Xin Yi, Hewu Li</p>
    <p><b>Summary:</b> While web agents gained popularity by automating web interactions, their
requirement for interface access introduces significant privacy risks that are
understudied, particularly from users' perspective. Through a formative study
(N=15), we found users frequently misunderstand agents' data practices, and
desired unobtrusive, transparent data management. To achieve this, we designed
and implemented PrivWeb, a trusted add-on on web agents that utilizes a
localized LLM to anonymize private information on interfaces according to user
preferences. It features privacy categorization schema and adaptive
notifications that selectively pauses tasks for user control over information
collection for highly sensitive information, while offering non-disruptive
options for less sensitive information, minimizing human oversight. The user
study (N=14) across travel, information retrieval, shopping, and entertainment
tasks compared PrivWeb with baselines without notification and without control
for private information access, where PrivWeb reduced perceived privacy risks
with no associated increase in cognitive effort, and resulted in higher overall
satisfaction.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11917v1">Distributed Finite-Horizon Optimal Control for Consensus with
  Differential Privacy Guarantees</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-09-15T13:34:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuwen Ma, Yongqiang Wang, Sarah K. Spurgeon, Boli Chen</p>
    <p><b>Summary:</b> This paper addresses the problem of privacy-preserving consensus control for
multi-agent systems (MAS) using differential privacy. We propose a novel
distributed finite-horizon linear quadratic regulator (LQR) framework, in which
agents share individual state information while preserving the confidentiality
of their local pairwise weight matrices, which are considered sensitive data in
MAS. Protecting these matrices effectively safeguards each agent's private cost
function and control preferences. Our solution injects consensus
error-dependent Laplace noise into the communicated state information and
employs a carefully designed time-dependent scaling factor in the local cost
functions. {This approach guarantees bounded consensus and achieves rigorous
$\epsilon$-differential privacy for the weight matrices without relying on
specific noise distribution assumptions.} Additionally, we analytically
characterize the trade-off between consensus accuracy and privacy level,
offering clear guidelines on how to enhance consensus performance through
appropriate scaling of the LQR weight matrices and the privacy budget.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11870v1">Efficient Byzantine-Robust Privacy-Preserving Federated Learning via
  Dimension Compression</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-15T12:43:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xian Qin, Xue Yang, Xiaohu Tang</p>
    <p><b>Summary:</b> Federated Learning (FL) allows collaborative model training across
distributed clients without sharing raw data, thus preserving privacy. However,
the system remains vulnerable to privacy leakage from gradient updates and
Byzantine attacks from malicious clients. Existing solutions face a critical
trade-off among privacy preservation, Byzantine robustness, and computational
efficiency. We propose a novel scheme that effectively balances these competing
objectives by integrating homomorphic encryption with dimension compression
based on the Johnson-Lindenstrauss transformation. Our approach employs a
dual-server architecture that enables secure Byzantine defense in the
ciphertext domain while dramatically reducing computational overhead through
gradient compression. The dimension compression technique preserves the
geometric relationships necessary for Byzantine defence while reducing
computation complexity from $O(dn)$ to $O(kn)$ cryptographic operations, where
$k \ll d$. Extensive experiments across diverse datasets demonstrate that our
approach maintains model accuracy comparable to non-private FL while
effectively defending against Byzantine clients comprising up to $40\%$ of the
network.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11761v1">On Spatial-Provenance Recovery in Wireless Networks with Relaxed-Privacy
  Constraints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-09-15T10:28:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Manish Bansal, Pramsu Shrivastava, J. Harshan</p>
    <p><b>Summary:</b> In Vehicle-to-Everything (V2X) networks with multi-hop communication, Road
Side Units (RSUs) intend to gather location data from the vehicles to offer
various location-based services. Although vehicles use the Global Positioning
System (GPS) for navigation, they may refrain from sharing their exact GPS
coordinates to the RSUs due to privacy considerations. Thus, to address the
localization expectations of the RSUs and the privacy concerns of the vehicles,
we introduce a relaxed-privacy model wherein the vehicles share their partial
location information in order to avail the location-based services. To
implement this notion of relaxed-privacy, we propose a low-latency protocol for
spatial-provenance recovery, wherein vehicles use correlated linear Bloom
filters to embed their position information. Our proposed spatial-provenance
recovery process takes into account the resolution of localization, the
underlying ad hoc protocol, and the coverage range of the wireless technology
used by the vehicles. Through a rigorous theoretical analysis, we present
extensive analysis on the underlying trade-off between relaxed-privacy and the
communication-overhead of the protocol. Finally, using a wireless testbed, we
show that our proposed method requires a few bits in the packet header to
provide security features such as localizing a low-power jammer executing a
denial-of-service attack.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11625v1">Inducing Uncertainty for Test-Time Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-15T06:38:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Muhammad H. Ashiq, Peter Triantafillou, Hung Yun Tseng, Grigoris G. Chrysos</p>
    <p><b>Summary:</b> Unlearning is the predominant method for removing the influence of data in
machine learning models. However, even after unlearning, models often continue
to produce the same predictions on the unlearned data with high confidence.
This persistent behavior can be exploited by adversaries using confident model
predictions on incorrect or obsolete data to harm users. We call this threat
model, which unlearning fails to protect against, *test-time privacy*. In
particular, an adversary with full model access can bypass any naive defenses
which ensure test-time privacy. To address this threat, we introduce an
algorithm which perturbs model weights to induce maximal uncertainty on
protected instances while preserving accuracy on the rest of the instances. Our
core algorithm is based on finetuning with a Pareto optimal objective that
explicitly balances test-time privacy against utility. We also provide a
certifiable approximation algorithm which achieves $(\varepsilon, \delta)$
guarantees without convexity assumptions. We then prove a tight, non-vacuous
bound that characterizes the privacy-utility tradeoff that our algorithms
incur. Empirically, our method obtains $>3\times$ stronger uncertainty than
pretraining with $<0.2\%$ drops in accuracy on various image recognition
benchmarks. Altogether, this framework provides a tool to guarantee additional
protection to end users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11249v1">Make Identity Unextractable yet Perceptible: Synthesis-Based Privacy
  Protection for Subject Faces in Photos</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-14T12:47:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tao Wang, Yushu Zhang, Xiangli Xiao, Kun Xu, Lin Yuan, Wenying Wen, Yuming Fang</p>
    <p><b>Summary:</b> Deep learning-based face recognition (FR) technology exacerbates privacy
concerns in photo sharing. In response, the research community developed a
suite of anti-FR methods to block identity extraction by unauthorized FR
systems. Benefiting from quasi-imperceptible alteration, perturbation-based
methods are well-suited for privacy protection of subject faces in photos, as
they allow familiar persons to recognize subjects via naked eyes. However, we
reveal that perturbation-based methods provide a false sense of privacy through
theoretical analysis and experimental validation.
  Therefore, new alternative solutions should be found to protect subject
faces. In this paper, we explore synthesis-based methods as a promising
solution, whose challenge is to enable familiar persons to recognize subjects.
To solve the challenge, we present a key insight: In most photo sharing
scenarios, familiar persons recognize subjects through identity perception
rather than meticulous face analysis. Based on the insight, we propose the
first synthesis-based method dedicated to subject faces, i.e., PerceptFace,
which can make identity unextractable yet perceptible. To enhance identity
perception, a new perceptual similarity loss is designed for faces, reducing
the alteration in regions of high sensitivity to human vision.
  As a synthesis-based method, PerceptFace can inherently provide reliable
identity protection. Meanwhile, out of the confine of meticulous face analysis,
PerceptFace focuses on identity perception from a more practical scenario,
which is also enhanced by the designed perceptual similarity loss. Sufficient
experiments show that PerceptFace achieves a superior trade-off between
identity protection and identity perception compared to existing methods. We
provide a public API of PerceptFace and believe that it has great potential to
become a practical anti-FR tool.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18134v1">A Weighted Gradient Tracking Privacy-Preserving Method for Distributed
  Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E">
  <p><b>Published on:</b> 2025-09-14T07:29:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Furan Xie, Bing Liu, Li Chai</p>
    <p><b>Summary:</b> This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.11022v2">Privacy-Preserving Uncertainty Disclosure for Facilitating Enhanced
  Energy Storage Dispatch</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E">
  <p><b>Published on:</b> 2025-09-14T01:22:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ning Qi, Xiaolong Jin, Kai Hou, Zeyu Liu, Hongjie Jia, Wei Wei</p>
    <p><b>Summary:</b> This paper proposes a novel privacy-preserving uncertainty disclosure
framework, enabling system operators to release marginal value function bounds
to reduce the conservativeness of interval forecast and mitigate excessive
withholding, thereby enhancing storage dispatch and social welfare. We develop
a risk-averse storage arbitrage model based on stochastic dynamic programming,
explicitly accounting for uncertainty intervals in value function training.
Real-time marginal value function bounds are derived using a rolling-horizon
chance-constrained economic dispatch formulation. We rigorously prove that the
bounds reliably cap the true opportunity cost and dynamically converge to the
hindsight value. We verify that both the marginal value function and its bounds
monotonically decrease with the state of charge (SoC) and increase with
uncertainty, providing a theoretical basis for risk-averse strategic behaviors
and SoC-dependent designs. An adjusted storage dispatch algorithm is further
designed using these bounds. We validate the effectiveness of the proposed
framework via an agent-based simulation on the ISO-NE test system. Under 50%
renewable capacity and 35% storage capacity, the proposed bounds enhance
storage response by 38.91% and reduce the optimality gap to 3.91% through
improved interval predictions. Additionally, by mitigating excessive
withholding, the bounds yield an average system cost reduction of 0.23% and an
average storage profit increase of 13.22%. These benefits further scale with
higher prediction conservativeness, storage capacity, and system uncertainty.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.10691v1">Privacy-Preserving Decentralized Federated Learning via Explainable
  Adaptive Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-12T20:52:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fardin Jalil Piran, Zhiling Chen, Yang Zhang, Qianyu Zhou, Jiong Tang, Farhad Imani</p>
    <p><b>Summary:</b> Decentralized federated learning faces privacy risks because model updates
can leak data through inference attacks and membership inference, a concern
that grows over many client exchanges. Differential privacy offers principled
protection by injecting calibrated noise so confidential information remains
secure on resource-limited IoT devices. Yet without transparency, black-box
training cannot track noise already injected by previous clients and rounds,
which forces worst-case additions and harms accuracy. We propose PrivateDFL, an
explainable framework that joins hyperdimensional computing with differential
privacy and keeps an auditable account of cumulative noise so each client adds
only the difference between the required noise and what has already been
accumulated. We evaluate on MNIST, ISOLET, and UCI-HAR to span image, signal,
and tabular modalities, and we benchmark against transformer-based and deep
learning-based baselines trained centrally with Differentially Private
Stochastic Gradient Descent (DP-SGD) and Renyi Differential Privacy (RDP).
PrivateDFL delivers higher accuracy, lower latency, and lower energy across IID
and non-IID partitions while preserving formal (epsilon, delta) guarantees and
operating without a central server. For example, under non-IID partitions,
PrivateDFL achieves 24.42% higher accuracy than the Vision Transformer on MNIST
while using about 10x less training time, 76x lower inference latency, and 11x
less energy, and on ISOLET it exceeds Transformer accuracy by more than 80%
with roughly 10x less training time, 40x lower inference latency, and 36x less
training energy. Future work will extend the explainable accounting to
adversarial clients and adaptive topologies with heterogeneous privacy budgets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.10163v1">Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and
  Energy-Aware Resource Management in 6G Edge Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-09-12T11:41:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Francisco Javier Esono Nkulu Andong, Qi Min</p>
    <p><b>Summary:</b> As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.10018v1">GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation
  Enhanced by Domain Rules and Disproof Method</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-12T07:22:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hailong Yang, Renhuo Zhao, Guanjin Wang, Zhaohong Deng</p>
    <p><b>Summary:</b> With the rapid advancement of Large Language Model (LLM), LLM-based agents
exhibit exceptional abilities in understanding and generating natural language,
facilitating human-like collaboration and information transmission in LLM-based
Multi-Agent System (MAS). High-performance LLMs are often hosted on remote
servers in public spaces. When tasks involve privacy data, MAS cannot securely
utilize these LLMs without implementing privacy-preserving mechanisms. To
address this challenge, we propose a General Anonymizing Multi-Agent system
(GAMA), which divides the agents' workspace into private and public spaces and
protects privacy through the anonymizing mechanism. In the private space,
agents handle sensitive data, while in the public space, only anonymized data
is utilized. GAMA incorporates two key modules to mitigate semantic loss caused
by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and
Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public
question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The
results demonstrate that GAMA has superior performance compared to the
state-of-the-art models. To further assess its privacy-preserving capabilities,
we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy
Preservation. The final results highlight GAMA's exceptional effectiveness in
both task processing and privacy preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.09916v1">Immersive Invaders: Privacy Threats from Deceptive Design in Virtual
  Reality Games and Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-12T01:31:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hilda Hadan, Michaela Valiquette, Lennart E. Nacke, Leah Zhang-Kennedy</p>
    <p><b>Summary:</b> Virtual Reality (VR) technologies offer immersive experiences but collect
substantial user data. While deceptive design is well-studied in 2D platforms,
little is known about its manifestation in VR environments and its impact on
user privacy. This research investigates deceptive designs in privacy
communication and interaction mechanisms of 12 top-rated VR games and
applications through autoethnographic evaluation of the applications and
thematic analysis of privacy policies. We found that while many deceptive
designs rely on 2D interfaces, some VR-unique features, while not directly
enabling deception, amplified data disclosure behaviors, and obscured actual
data practices. Convoluted privacy policies and manipulative consent practices
further hinder comprehension and increase privacy risks. We also observed
privacy-preserving design strategies and protective considerations in VR
privacy policies. We offer recommendations for ethical VR design that balance
immersive experiences with strong privacy protections, guiding researchers,
designers, and policymakers to improve privacy in VR environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.09844v1">Privacy-Preserving Automated Rosacea Detection Based on Medically
  Inspired Region of Interest Selection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-09-11T20:54:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chengyu Yang, Rishik Reddy Yesgari, Chengjun Liu</p>
    <p><b>Summary:</b> Rosacea is a common but underdiagnosed inflammatory skin condition that
primarily affects the central face and presents with subtle redness, pustules,
and visible blood vessels. Automated detection remains challenging due to the
diffuse nature of symptoms, the scarcity of labeled datasets, and privacy
concerns associated with using identifiable facial images. A novel
privacy-preserving automated rosacea detection method inspired by clinical
priors and trained entirely on synthetic data is presented in this paper.
Specifically, the proposed method, which leverages the observation that rosacea
manifests predominantly through central facial erythema, first constructs a
fixed redness-informed mask by selecting regions with consistently high red
channel intensity across facial images. The mask thus is able to focus on
diagnostically relevant areas such as the cheeks, nose, and forehead and
exclude identity-revealing features. Second, the ResNet-18 deep learning
method, which is trained on the masked synthetic images, achieves superior
performance over the full-face baselines with notable gains in terms of
accuracy, recall and F1 score when evaluated using the real-world test data.
The experimental results demonstrate that the synthetic data and clinical
priors can jointly enable accurate and ethical dermatological AI systems,
especially for privacy sensitive applications in telemedicine and large-scale
screening.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.09787v1">ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full
  Version)</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-11T18:44:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nojan Sheybani, Alessandro Pegoraro, Jonathan Knauer, Phillip Rieger, Elissa Mollakuqe, Farinaz Koushanfar, Ahmad-Reza Sadeghi</p>
    <p><b>Summary:</b> Split Learning (SL) is a distributed learning approach that enables
resource-constrained clients to collaboratively train deep neural networks
(DNNs) by offloading most layers to a central server while keeping in- and
output layers on the client-side. This setup enables SL to leverage server
computation capacities without sharing data, making it highly effective in
resource-constrained environments dealing with sensitive data. However, the
distributed nature enables malicious clients to manipulate the training
process. By sending poisoned intermediate gradients, they can inject backdoors
into the shared DNN. Existing defenses are limited by often focusing on
server-side protection and introducing additional overhead for the server. A
significant challenge for client-side defenses is enforcing malicious clients
to correctly execute the defense algorithm.
  We present ZORRO, a private, verifiable, and robust SL defense scheme.
Through our novel design and application of interactive zero-knowledge proofs
(ZKPs), clients prove their correct execution of a client-located defense
algorithm, resulting in proofs of computational integrity attesting to the
benign nature of locally trained DNN portions. Leveraging the frequency
representation of model partitions enables ZORRO to conduct an in-depth
inspection of the locally trained models in an untrusted environment, ensuring
that each client forwards a benign checkpoint to its succeeding client. In our
extensive evaluation, covering different model architectures as well as various
attack strategies and data scenarios, we show ZORRO's effectiveness, as it
reduces the attack success rate to less than 6\% while causing even for models
storing \numprint{1000000} parameters on the client-side an overhead of less
than 10 seconds.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.09485v2">Balancing Utility and Privacy: Dynamically Private SGD with Random
  Projection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-11T14:17:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhanhong Jiang, Md Zahid Hasan, Nastaran Saadati, Aditya Balu, Chao Liu, Soumik Sarkar</p>
    <p><b>Summary:</b> Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.09285v1">The Impact of Device Type, Data Practices, and Use Case Scenarios on
  Privacy Concerns about Eye-tracked Augmented Reality in the United States and
  Germany</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-11T09:21:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Efe Bozkir, Babette Bühler, Xiaoyuan Wu, Enkelejda Kasneci, Lujo Bauer, Lorrie Faith Cranor</p>
    <p><b>Summary:</b> Augmented reality technology will likely be prevalent with more affordable
head-mounted displays. Integrating novel interaction modalities such as eye
trackers into head-mounted displays could lead to collecting vast amounts of
biometric data, which may allow inference of sensitive user attributes like
health status or sexual preference, posing privacy issues. While previous works
broadly examined privacy concerns about augmented reality, ours is the first to
extensively explore privacy concerns on behavioral data, particularly eye
tracking in augmented reality. We crowdsourced four survey studies in the
United States (n1 = 48, n2 = 525) and Germany (n3 = 48, n4 = 525) to understand
the impact of user attributes, augmented reality devices, use cases, data
practices, and country on privacy concerns. Our findings indicate that
participants are generally concerned about privacy when they know what
inferences can be made based on the collected data. Despite the more prominent
use of smartphones in daily life than augmented reality glasses, we found no
indications of differing privacy concerns depending on the device type. In
addition, our participants are more comfortable when a particular use case
benefits them and less comfortable when other humans can consume their data.
Furthermore, participants in the United States are less concerned about their
privacy than those in Germany. Based on our findings, we provide several
recommendations to practitioners and policymakers for privacy-aware augmented
reality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.09103v1">AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-11T02:29:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chanti Raju Mylay, Bobin Deng, Zhipeng Cai, Honghui Xu</p>
    <p><b>Summary:</b> Crop diseases pose significant threats to global food security, agricultural
productivity, and sustainable farming practices, directly affecting farmers'
livelihoods and economic stability. To address the growing need for effective
crop disease management, AI-based disease alerting systems have emerged as
promising tools by providing early detection and actionable insights for timely
intervention. However, existing systems often overlook critical aspects such as
data privacy, market pricing power, and farmer-friendly usability, leaving
farmers vulnerable to privacy breaches and economic exploitation. To bridge
these gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM
Crop Disease Alerting System. AgriSentinel incorporates a differential privacy
mechanism to protect sensitive crop image data while maintaining classification
accuracy. Its lightweight deep learning-based crop disease classification model
is optimized for mobile devices, ensuring accessibility and usability for
farmers. Additionally, the system includes a fine-tuned, on-device large
language model (LLM) that leverages a curated knowledge pool to provide farmers
with specific, actionable suggestions for managing crop diseases, going beyond
simple alerting. Comprehensive experiments validate the effectiveness of
AgriSentinel, demonstrating its ability to safeguard data privacy, maintain
high classification performance, and deliver practical, actionable disease
management strategies. AgriSentinel offers a robust, farmer-friendly solution
for automating crop disease alerting and management, ultimately contributing to
improved agricultural decision-making and enhanced crop productivity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.09097v1">DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large
  Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-11T02:16:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Honghui Xu, Shiva Shrestha, Wei Chen, Zhiyuan Li, Zhipeng Cai</p>
    <p><b>Summary:</b> As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.09091v1">Towards Confidential and Efficient LLM Inference with Dual Privacy
  Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-11T01:54:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Honglan Yu, Yibin Wang, Feifei Dai, Dong Liu, Haihui Fan, Xiaoyan Gu</p>
    <p><b>Summary:</b> CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.08995v1">When FinTech Meets Privacy: Securing Financial LLMs with Differential
  Private Fine-Tuning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-10T20:43:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sichen Zhu, Hoyeung Leung, Xiaoyi Wang, Jia Wei, Honghui Xu</p>
    <p><b>Summary:</b> The integration of Large Language Models (LLMs) into financial technology
(FinTech) has revolutionized the analysis and processing of complex financial
data, driving advancements in real-time decision-making and analytics. With the
growing trend of deploying AI models on edge devices for financial
applications, ensuring the privacy of sensitive financial data has become a
significant challenge. To address this, we propose DPFinLLM, a
privacy-enhanced, lightweight LLM specifically designed for on-device financial
applications. DPFinLLM combines a robust differential privacy mechanism with a
streamlined architecture inspired by state-of-the-art models, enabling secure
and efficient processing of financial data. This proposed DPFinLLM can not only
safeguard user data from privacy breaches but also ensure high performance
across diverse financial tasks. Extensive experiments on multiple financial
sentiment datasets validate the effectiveness of DPFinLLM, demonstrating its
ability to achieve performance comparable to fully fine-tuned models, even
under strict privacy constraints.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.08804v1">Approximate Algorithms for Verifying Differential Privacy with Gaussian
  Distributions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Programming Languages-D91E36"> 
  <p><b>Published on:</b> 2025-09-10T17:37:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bishnu Bhusal, Rohit Chadha, A. Prasad Sistla, Mahesh Viswanathan</p>
    <p><b>Summary:</b> The verification of differential privacy algorithms that employ Gaussian
distributions is little understood. This paper tackles the challenge of
verifying such programs by introducing a novel approach to approximating
probability distributions of loop-free programs that sample from both discrete
and continuous distributions with computable probability density functions,
including Gaussian and Laplace. We establish that verifying
$(\epsilon,\delta)$-differential privacy for these programs is \emph{almost
decidable}, meaning the problem is decidable for all values of $\delta$ except
those in a finite set. Our verification algorithm is based on computing
probabilities to any desired precision by combining integral approximations,
and tail probability bounds. The proposed methods are implemented in the tool,
DipApprox, using the FLINT library for high-precision integral computations,
and incorporate optimizations to enhance scalability. We validate {\ourtool} on
fundamental privacy-preserving algorithms, such as Gaussian variants of the
Sparse Vector Technique and Noisy Max, demonstrating its effectiveness in both
confirming privacy guarantees and detecting violations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.08782v1">Extended Version: Security and Privacy Perceptions of Pakistani Facebook
  Matrimony Group Users</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-09-10T17:12:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mah Jan Dorazahi, Deepthi Mungara, Yasemin Acar, Harshini Sri Ramulu</p>
    <p><b>Summary:</b> In Pakistan, where dating apps are subject to censorship, Facebook matrimony
groups -- also referred to as marriage groups -- serve as alternative virtual
spaces for members to search for potential life partners. To participate in
these groups, members often share sensitive personal information such as
photos, addresses, and phone numbers, which exposes them to risks such as
fraud, blackmail, and identity theft. To better protect users of Facebook
matrimony groups, we need to understand aspects related to user safety, such as
how users perceive risks, what influences their trust in sharing personal
information, and how they navigate security and privacy concerns when seeking
potential partners online. In this study, through 23 semi-structured
interviews, we explore how Pakistani users of Facebook matrimony groups
perceive and navigate risks of sharing personal information, and how cultural
norms and expectations influence their behavior in these groups.
  We find elevated privacy concerns among participants, leading them to share
limited personal information and creating mistrust among potential partners.
Many also expressed concerns about the authenticity of profiles and major
security risks, such as identity theft, harassment, and social judgment. Our
work highlights the challenges of safely navigating Facebook matrimony groups
in Pakistan and offers recommendations for such as implementing stronger
identity verification by group admins, enforcing stricter cybersecurity laws,
clear platform guidelines to ensure accountability, and technical feature
enhancements -- including restricting screenshots, picture downloads, and
implementing anonymous chats -- to protect user data and build trust.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.08722v1">SilentLedger: Privacy-Preserving Auditing for Blockchains with Complete
  Non-Interactivity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-10T16:14:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zihan Liu, Xiaohu Wang, Chao Lin, Minghui Xu, Debiao He, Xinyi Huang</p>
    <p><b>Summary:</b> Privacy-preserving blockchain systems are essential for protecting
transaction data, yet they must also provide auditability that enables auditors
to recover participant identities and transaction amounts when warranted.
Existing designs often compromise the independence of auditing and
transactions, introducing extra interactions that undermine usability and
scalability. Moreover, many auditable solutions depend on auditors serving as
validators or recording nodes, which introduces risks to both data security and
system reliability.
  To overcome these challenges, we propose SilentLedger, a privacy-preserving
transaction system with auditing and complete non-interactivity. To support
public verification of authorization, we introduce a renewable anonymous
certificate scheme with formal semantics and a rigorous security model.
SilentLedger further employs traceable transaction mechanisms constructed from
established cryptographic primitives, enabling users to transact without
interaction while allowing auditors to audit solely from on-chain data. We
formally prove security properties including authenticity, anonymity,
confidentiality, and soundness, provide a concrete instantiation, and evaluate
performance under a standard 2-2 transaction model. Our implementation and
benchmarks demonstrate that SilentLedger achieves superior performance compared
with state-of-the-art solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.08720v1">PAnDA: Rethinking Metric Differential Privacy Optimization at Scale with
  Anchor-Based Approximation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-10T16:14:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ruiyao Liu, Chenxi Qiu</p>
    <p><b>Summary:</b> Metric Differential Privacy (mDP) extends the local differential privacy
(LDP) framework to metric spaces, enabling more nuanced privacy protection for
data such as geo-locations. However, existing mDP optimization methods,
particularly those based on linear programming (LP), face scalability
challenges due to the quadratic growth in decision variables. In this paper, we
propose Perturbation via Anchor-based Distributed Approximation (PAnDA), a
scalable two-phase framework for optimizing metric differential privacy (mDP).
To reduce computational overhead, PAnDA allows each user to select a small set
of anchor records, enabling the server to solve a compact linear program over a
reduced domain. We introduce three anchor selection strategies, exponential
decay (PAnDA-e), power-law decay (PAnDA-p), and logistic decay (PAnDA-l), and
establish theoretical guarantees under a relaxed privacy notion called
probabilistic mDP (PmDP). Experiments on real-world geo-location datasets
demonstrate that PAnDA scales to secret domains with up to 5,000 records, two
times larger than prior LP-based methods, while providing theoretical
guarantees for both privacy and utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.08704v1">Tight Privacy Audit in One Run</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-10T15:55:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zihang Xiang, Tianhao Wang, Hanshen Xiao, Yuan Tian, Di Wang</p>
    <p><b>Summary:</b> In this paper, we study the problem of privacy audit in one run and show that
our method achieves tight audit results for various differentially private
protocols. This includes obtaining tight results for auditing
$(\varepsilon,\delta)$-DP algorithms where all previous work fails to achieve
in any parameter setups. We first formulate a framework for privacy audit
\textit{in one run} with refinement compared with previous work. Then, based on
modeling privacy by the $f$-DP formulation, we study the implications of our
framework to obtain a theoretically justified lower bound for privacy audit. In
the experiment, we compare with previous work and show that our audit method
outperforms the rest in auditing various differentially private algorithms. We
also provide experiments that give contrasting conclusions to previous work on
the parameter settings for privacy audits in one run.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.08554v1">Acceptability of AI Assistants for Privacy: Perceptions of Experts and
  Users on Personalized Privacy Assistants</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-10T12:59:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Meihe Xu, Aurelia Tamò-Larrieux, Arianna Rossi</p>
    <p><b>Summary:</b> Individuals increasingly face an overwhelming number of tasks and decisions.
To cope with the new reality, there is growing research interest in developing
intelligent agents that can effectively assist people across various aspects of
daily life in a tailored manner, with privacy emerging as a particular area of
application. Artificial intelligence (AI) assistants for privacy, such as
personalized privacy assistants (PPAs), have the potential to automatically
execute privacy decisions based on users' pre-defined privacy preferences,
sparing them the mental effort and time usually spent on each privacy decision.
This helps ensure that, even when users feel overwhelmed or resigned about
privacy, the decisions made by PPAs still align with their true preferences and
best interests. While research has explored possible designs of such agents,
user and expert perspectives on the acceptability of such AI-driven solutions
remain largely unexplored. In this study, we conducted five focus groups with
domain experts (n = 11) and potential users (n = 26) to uncover key themes
shaping the acceptance of PPAs. Factors influencing the acceptability of AI
assistants for privacy include design elements (such as information sources
used by the agent), external conditions (such as regulation and literacy
education), and systemic conditions (e.g., public or market providers and the
need to avoid monopoly) to PPAs. These findings provide theoretical extensions
to technology acceptance models measuring PPAs, insights on design, and policy
implications for PPAs, as well as broader implications for the design of AI
assistants.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.08387v1">Infinite Stream Estimation under Personalized $w$-Event Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-09-10T08:27:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leilei Du, Peng Cheng, Lei Chen, Heng Tao Shen, Xuemin Lin, Wei Xi</p>
    <p><b>Summary:</b> Streaming data collection is indispensable for stream data analysis, such as
event monitoring. However, publishing these data directly leads to privacy
leaks. $w$-event privacy is a valuable tool to protect individual privacy
within a given time window while maintaining high accuracy in data collection.
Most existing $w$-event privacy studies on infinite data stream only focus on
homogeneous privacy requirements for all users. In this paper, we propose
personalized $w$-event privacy protection that allows different users to have
different privacy requirements in private data stream estimation. Specifically,
we design a mechanism that allows users to maintain constant privacy
requirements at each time slot, namely Personalized Window Size Mechanism
(PWSM). Then, we propose two solutions to accurately estimate stream data
statistics while achieving $w$-event level $\epsilon$ personalized differential
privacy ( ($w$, $\epsilon$)-EPDP), namely Personalized Budget Distribution
(PBD) and Peronalized Budget Absorption (PBA). PBD always provides at least the
same privacy budget for the next time step as the amount consumed in the
previous release. PBA fully absorbs the privacy budget from the previous $k$
time slots, while also borrowing from the privacy budget of the next $k$ time
slots, to increase the privacy budget for the current time slot. We prove that
both PBD and PBA outperform the state-of-the-art private stream estimation
methods while satisfying the privacy requirements of all users. We demonstrate
the efficiency and effectiveness of our PBD and PBA on both real and synthetic
data sets, compared with the recent uniformity $w$-event approaches, Budget
Distribution (BD) and Budget Absorption (BA). Our PBD achieves 68% less error
than BD on average on real data sets. Besides, our PBA achieves 24.9% less
error than BA on average on synthetic data sets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.10561v1">AVEC: Bootstrapping Privacy for Local LLMs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">  
  <p><b>Published on:</b> 2025-09-10T07:59:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Madhava Gaikwad</p>
    <p><b>Summary:</b> This position paper presents AVEC (Adaptive Verifiable Edge Control), a
framework for bootstrapping privacy for local language models by enforcing
privacy at the edge with explicit verifiability for delegated queries. AVEC
introduces an adaptive budgeting algorithm that allocates per-query
differential privacy parameters based on sensitivity, local confidence, and
historical usage, and uses verifiable transformation with on-device integrity
checks. We formalize guarantees using R\'enyi differential privacy with
odometer-based accounting, and establish utility ceilings, delegation-leakage
bounds, and impossibility results for deterministic gating and hash-only
certification. Our evaluation is simulation-based by design to study mechanism
behavior and accounting; we do not claim deployment readiness or task-level
utility with live LLMs. The contribution is a conceptual architecture and
theoretical foundation that chart a pathway for empirical follow-up on
privately bootstrapping local LLMs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.08142v1">Privacy Preserving Semantic Communications Using Vision Language Models:
  A Segmentation and Generation Approach</a></h3>
  
  <p><b>Published on:</b> 2025-09-09T20:49:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoran Chang, Mingzhe Chen, Huaxia Wang, Qianqian Zhang</p>
    <p><b>Summary:</b> Semantic communication has emerged as a promising paradigm for
next-generation wireless systems, improving the communication efficiency by
transmitting high-level semantic features. However, reliance on unimodal
representations can degrade reconstruction under poor channel conditions, and
privacy concerns of the semantic information attack also gain increasing
attention. In this work, a privacy-preserving semantic communication framework
is proposed to protect sensitive content of the image data. Leveraging a
vision-language model (VLM), the proposed framework identifies and removes
private content regions from input images prior to transmission. A shared
privacy database enables semantic alignment between the transmitter and
receiver to ensure consistent identification of sensitive entities. At the
receiver, a generative module reconstructs the masked regions using learned
semantic priors and conditioned on the received text embedding. Simulation
results show that generalizes well to unseen image processing tasks, improves
reconstruction quality at the authorized receiver by over 10% using text
embedding, and reduces identity leakage to the eavesdropper by more than 50%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.08018v1">Enhancing Privacy Preservation and Reducing Analysis Time with Federated
  Transfer Learning in Digital Twins-based Computed Tomography Scan Analysis</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-09T04:54:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Avais Jan, Qasim Zia, Murray Patterson</p>
    <p><b>Summary:</b> The application of Digital Twin (DT) technology and Federated Learning (FL)
has great potential to change the field of biomedical image analysis,
particularly for Computed Tomography (CT) scans. This paper presents Federated
Transfer Learning (FTL) as a new Digital Twin-based CT scan analysis paradigm.
FTL uses pre-trained models and knowledge transfer between peer nodes to solve
problems such as data privacy, limited computing resources, and data
heterogeneity. The proposed framework allows real-time collaboration between
cloud servers and Digital Twin-enabled CT scanners while protecting patient
identity. We apply the FTL method to a heterogeneous CT scan dataset and assess
model performance using convergence time, model accuracy, precision, recall, F1
score, and confusion matrix. It has been shown to perform better than
conventional FL and Clustered Federated Learning (CFL) methods with better
precision, accuracy, recall, and F1-score. The technique is beneficial in
settings where the data is not independently and identically distributed
(non-IID), and it offers reliable, efficient, and secure solutions for medical
diagnosis. These findings highlight the possibility of using FTL to improve
decision-making in digital twin-based CT scan analysis, secure and efficient
medical image analysis, promote privacy, and open new possibilities for
applying precision medicine and smart healthcare systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.07131v1">SoK: Security and Privacy of AI Agents for Blockchain</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-08T18:32:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicolò Romandini, Carlo Mazzocca, Kai Otsuki, Rebecca Montanari</p>
    <p><b>Summary:</b> Blockchain and smart contracts have garnered significant interest in recent
years as the foundation of a decentralized, trustless digital ecosystem,
thereby eliminating the need for traditional centralized authorities. Despite
their central role in powering Web3, their complexity still presents
significant barriers for non-expert users. To bridge this gap, Artificial
Intelligence (AI)-based agents have emerged as valuable tools for interacting
with blockchain environments, supporting a range of tasks, from analyzing
on-chain data and optimizing transaction strategies to detecting
vulnerabilities within smart contracts. While interest in applying AI to
blockchain is growing, the literature still lacks a comprehensive survey that
focuses specifically on the intersection with AI agents. Most of the related
work only provides general considerations, without focusing on any specific
domain. This paper addresses this gap by presenting the first Systematization
of Knowledge dedicated to AI-driven systems for blockchain, with a special
focus on their security and privacy dimensions, shedding light on their
applications, limitations, and future research directions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.07055v1">Sequentially Auditing Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-09-08T17:57:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tomás González, Mateo Dulce-Rubio, Aaditya Ramdas, Mónica Ribero</p>
    <p><b>Summary:</b> We propose a practical sequential test for auditing differential privacy
guarantees of black-box mechanisms. The test processes streams of mechanisms'
outputs providing anytime-valid inference while controlling Type I error,
overcoming the fixed sample size limitation of previous batch auditing methods.
Experiments show this test detects violations with sample sizes that are orders
of magnitude smaller than existing methods, reducing this number from 50K to a
few hundred examples, across diverse realistic mechanisms. Notably, it
identifies DP-SGD privacy violations in \textit{under} one training run, unlike
prior methods needing full model training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.06444v1">HyFedRAG: A Federated Retrieval-Augmented Generation Framework for
  Heterogeneous and Privacy-Sensitive Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-08T08:44:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Cheng Qian, Hainan Zhang, Yongxin Tong, Hong-Wei Zheng, Zhiming Zheng</p>
    <p><b>Summary:</b> Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive
data, especially in distributed healthcare settings where patient data spans
SQL, knowledge graphs, and clinical notes. Clinicians face difficulties
retrieving rare disease cases due to privacy constraints and the limitations of
traditional cloud-based RAG systems in handling diverse formats and edge
devices. To address this, we introduce HyFedRAG, a unified and efficient
Federated RAG framework tailored for Hybrid data modalities. By leveraging an
edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across
diverse data sources while preserving data privacy. Our key contributions are:
(1) We design an edge-cloud collaborative RAG framework built on Flower, which
supports querying structured SQL data, semi-structured knowledge graphs, and
unstructured documents. The edge-side LLMs convert diverse data into
standardized privacy-preserving representations, and the server-side LLMs
integrates them for global reasoning and generation. (2) We integrate
lightweight local retrievers with privacy-aware LLMs and provide three
anonymization tools that enable each client to produce semantically rich,
de-identified summaries for global inference across devices. (3) To optimize
response latency and reduce redundant computation, we design a three-tier
caching strategy consisting of local cache, intermediate representation cache,
and cloud inference cache. Experimental results on PMC-Patients demonstrate
that HyFedRAG outperforms existing baselines in terms of retrieval quality,
generation consistency, and system efficiency. Our framework offers a scalable
and privacy-compliant solution for RAG over structural-heterogeneous data,
unlocking the potential of LLMs in sensitive and diverse data environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.06368v1">From Perception to Protection: A Developer-Centered Study of Security
  and Privacy Threats in Extended Reality (XR)</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-08T06:48:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kunlin Cai, Jinghuai Zhang, Ying Li, Zhiyuan Wang, Xun Chen, Tianshi Li, Yuan Tian</p>
    <p><b>Summary:</b> The immersive nature of XR introduces a fundamentally different set of
security and privacy (S&P) challenges due to the unprecedented user
interactions and data collection that traditional paradigms struggle to
mitigate. As the primary architects of XR applications, developers play a
critical role in addressing novel threats. However, to effectively support
developers, we must first understand how they perceive and respond to different
threats. Despite the growing importance of this issue, there is a lack of
in-depth, threat-aware studies that examine XR S&P from the developers'
perspective. To fill this gap, we interviewed 23 professional XR developers
with a focus on emerging threats in XR. Our study addresses two research
questions aiming to uncover existing problems in XR development and identify
actionable paths forward.
  By examining developers' perceptions of S&P threats, we found that: (1) XR
development decisions (e.g., rich sensor data collection, user-generated
content interfaces) are closely tied to and can amplify S&P threats, yet
developers are often unaware of these risks, resulting in cognitive biases in
threat perception; and (2) limitations in existing mitigation methods, combined
with insufficient strategic, technical, and communication support, undermine
developers' motivation, awareness, and ability to effectively address these
threats. Based on these findings, we propose actionable and stakeholder-aware
recommendations to improve XR S&P throughout the XR development process. This
work represents the first effort to undertake a threat-aware,
developer-centered study in the XR domain -- an area where the immersive,
data-rich nature of the XR technology introduces distinctive challenges.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.06361v2">Speaker Privacy and Security in the Big Data Era: Protection and Defense
  against Deepfake</a></h3>
  
  <p><b>Published on:</b> 2025-09-08T06:22:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Liping Chen, Kong Aik Lee, Zhen-Hua Ling, Xin Wang, Rohan Kumar Das, Tomoki Toda, Haizhou Li</p>
    <p><b>Summary:</b> In the era of big data, remarkable advancements have been achieved in
personalized speech generation techniques that utilize speaker attributes,
including voice and speaking style, to generate deepfake speech. This has also
amplified global security risks from deepfake speech misuse, resulting in
considerable societal costs worldwide. To address the security threats posed by
deepfake speech, techniques have been developed focusing on both the protection
of voice attributes and the defense against deepfake speech. Among them, the
voice anonymization technique has been developed to protect voice attributes
from extraction for deepfake generation, while deepfake detection and
watermarking have been utilized to defend against the misuse of deepfake
speech. This paper provides a short and concise overview of the three
techniques, describing the methodologies, advancements, and challenges. A
comprehensive version, offering additional discussions, will be published in
the near future.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.06264v1">PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss
  Random Variable Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-08T01:06:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qin Yang, Nicholas Stout, Meisam Mohammady, Han Wang, Ayesha Samreen, Christopher J Quinn, Yan Yan, Ashish Kundu, Yuan Hong</p>
    <p><b>Summary:</b> Differentially Private Stochastic Gradient Descent (DP-SGD) is a standard
method for enforcing privacy in deep learning, typically using the Gaussian
mechanism to perturb gradient updates. However, conventional mechanisms such as
Gaussian and Laplacian noise are parameterized only by variance or scale. This
single degree of freedom ties the magnitude of noise directly to both privacy
loss and utility degradation, preventing independent control of these two
factors. The problem becomes more pronounced when the number of composition
rounds T and batch size B vary across tasks, as these variations induce
task-dependent shifts in the privacy-utility trade-off, where small changes in
noise parameters can disproportionately affect model accuracy. To address this
limitation, we introduce PLRV-O, a framework that defines a broad search space
of parameterized DP-SGD noise distributions, where privacy loss moments are
tightly characterized yet can be optimized more independently with respect to
utility loss. This formulation enables systematic adaptation of noise to
task-specific requirements, including (i) model size, (ii) training duration,
(iii) batch sampling strategies, and (iv) clipping thresholds under both
training and fine-tuning settings. Empirical results demonstrate that PLRV-O
substantially improves utility under strict privacy constraints. On CIFAR-10, a
fine-tuned ViT achieves 94.03% accuracy at epsilon approximately 0.5, compared
to 83.93% with Gaussian noise. On SST-2, RoBERTa-large reaches 92.20% accuracy
at epsilon approximately 0.2, versus 50.25% with Gaussian.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.06142v2">RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric
  Privacy Preserving</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-09-07T17:16:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhengquan Luo, Chi Liu, Dongfu Xiao, Zhen Yu, Yueye Wang, Tianqing Zhu</p>
    <p><b>Summary:</b> The integration of AI with medical images enables the extraction of implicit
image-derived biomarkers for a precise health assessment. Recently, retinal
age, a biomarker predicted from fundus images, is a proven predictor of
systemic disease risks, behavioral patterns, aging trajectory and even
mortality. However, the capability to infer such sensitive biometric data
raises significant privacy risks, where unauthorized use of fundus images could
lead to bioinformation leakage, breaching individual privacy. In response, we
formulate a new research problem of biometric privacy associated with medical
images and propose RetinaGuard, a novel privacy-enhancing framework that
employs a feature-level generative adversarial masking mechanism to obscure
retinal age while preserving image visual quality and disease diagnostic
utility. The framework further utilizes a novel multiple-to-one knowledge
distillation strategy incorporating a retinal foundation model and diverse
surrogate age encoders to enable a universal defense against black-box age
prediction models. Comprehensive evaluations confirm that RetinaGuard
successfully obfuscates retinal age prediction with minimal impact on image
quality and pathological feature representation. RetinaGuard is also flexible
for extension to other medical image derived biomarkers. RetinaGuard is also
flexible for extension to other medical image biomarkers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.06133v1">VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored
  Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">  
  <p><b>Published on:</b> 2025-09-07T16:40:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pradyumna Kaushal</p>
    <p><b>Summary:</b> Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,
and service centers that are difficult to verify and prone to fraud. We propose
VehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with
zero-knowledge proofs (ZKPs) for privacy-preserving verification.
VehiclePassport immutably commits to manufacturing, telemetry, and service
events while enabling selective disclosure via short-lived JWTs and Groth16
proofs. Our open-source reference stack anchors hashes on Polygon zkEVM at
<$0.02 per event, validates proofs in <10 ms, and scales to millions of
vehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant
traceability, and establishes a trustless foundation for insurance, resale, and
regulatory applications in global mobility data markets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.05608v1">Cross-Service Threat Intelligence in LLM Services using
  Privacy-Preserving Fingerprints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-06T05:57:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Waris Gill, Natalie Isak, Matthew Dressman</p>
    <p><b>Summary:</b> The widespread deployment of LLMs across enterprise services has created a
critical security blind spot. Organizations operate multiple LLM services
handling billions of queries daily, yet regulatory compliance boundaries
prevent these services from sharing threat intelligence about prompt injection
attacks, the top security risk for LLMs. When an attack is detected in one
service, the same threat may persist undetected in others for months, as
privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence
system that enables secure sharing of attack fingerprints across compliance
boundaries. BinaryShield transforms suspicious prompts through a unique
pipeline combining PII redaction, semantic embedding, binary quantization, and
randomized response mechanism to potentially generate non-invertible
fingerprints that preserve attack patterns while providing privacy. Our
evaluations demonstrate that BinaryShield achieves an F1-score of 0.94,
significantly outperforming SimHash (0.77), the privacy-preserving baseline,
while achieving 64x storage reduction and 38x faster similarity search compared
to dense embeddings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.05265v1">On Evaluating the Poisoning Robustness of Federated Learning under Local
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-05T17:23:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zijian Wang, Wei Tong, Tingxuan Han, Haoyu Chen, Tianling Zhang, Yunlong Mao, Sheng Zhong</p>
    <p><b>Summary:</b> Federated learning (FL) combined with local differential privacy (LDP)
enables privacy-preserving model training across decentralized data sources.
However, the decentralized data-management paradigm leaves LDPFL vulnerable to
participants with malicious intent. The robustness of LDPFL protocols,
particularly against model poisoning attacks (MPA), where adversaries inject
malicious updates to disrupt global model convergence, remains insufficiently
studied. In this paper, we propose a novel and extensible model poisoning
attack framework tailored for LDPFL settings. Our approach is driven by the
objective of maximizing the global training loss while adhering to local
privacy constraints. To counter robust aggregation mechanisms such as
Multi-Krum and trimmed mean, we develop adaptive attacks that embed carefully
crafted constraints into a reverse training process, enabling evasion of these
defenses. We evaluate our framework across three representative LDPFL
protocols, three benchmark datasets, and two types of deep neural networks.
Additionally, we investigate the influence of data heterogeneity and privacy
budgets on attack effectiveness. Experimental results demonstrate that our
adaptive attacks can significantly degrade the performance of the global model,
revealing critical vulnerabilities and highlighting the need for more robust
LDPFL defense strategies against MPA. Our code is available at
https://github.com/ZiJW/LDPFL-Attack</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.05162v1">Verifiability and Privacy in Federated Learning through Context-Hiding
  Multi-Key Homomorphic Authenticators</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-05T14:57:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Simone Bottoni, Giulio Zizzo, Stefano Braghin, Alberto Trombetta</p>
    <p><b>Summary:</b> Federated Learning has rapidly expanded from its original inception to now
have a large body of research, several frameworks, and sold in a variety of
commercial offerings. Thus, its security and robustness is of significant
importance. There are many algorithms that provide robustness in the case of
malicious clients. However, the aggregator itself may behave maliciously, for
example, by biasing the model or tampering with the weights to weaken the
models privacy. In this work, we introduce a verifiable federated learning
protocol that enables clients to verify the correctness of the aggregators
computation without compromising the confidentiality of their updates. Our
protocol uses a standard secure aggregation technique to protect individual
model updates with a linearly homomorphic authenticator scheme that enables
efficient, privacy-preserving verification of the aggregated result. Our
construction ensures that clients can detect manipulation by the aggregator
while maintaining low computational overhead. We demonstrate that our approach
scales to large models, enabling verification over large neural networks with
millions of parameters.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.04919v1">Optimal Variance and Covariance Estimation under Differential Privacy in
  the Add-Remove Model and Beyond</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-05T08:37:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shokichi Takakura, Seng Pei Liew, Satoshi Hasegawa</p>
    <p><b>Summary:</b> In this paper, we study the problem of estimating the variance and covariance
of datasets under differential privacy in the add-remove model. While
estimation in the swap model has been extensively studied in the literature,
the add-remove model remains less explored and more challenging, as the dataset
size must also be kept private. To address this issue, we develop efficient
mechanisms for variance and covariance estimation based on the \emph{B\'{e}zier
mechanism}, a novel moment-release framework that leverages Bernstein bases. We
prove that our proposed mechanisms are minimax optimal in the high-privacy
regime by establishing new minimax lower bounds. Moreover, beyond worst-case
scenarios, we analyze instance-wise utility and show that the B\'{e}zier-based
estimator consistently achieves better utility compared to alternative
mechanisms. Finally, we demonstrate the effectiveness of the B\'{e}zier
mechanism beyond variance and covariance estimation, showcasing its
applicability to other statistical tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.12222v1">Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO
  Satellite Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-09-05T03:33:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Binquan Guo, Junteng Cao, Marie Siew, Binbin Chen, Tony Q. S. Quek, Zhu Han</p>
    <p><b>Summary:</b> Large-scale low-Earth-orbit (LEO) satellite systems are increasingly valued
for their ability to enable rapid and wide-area data exchange, thereby
facilitating the collaborative training of artificial intelligence (AI) models
across geographically distributed regions. Due to privacy concerns and
regulatory constraints, raw data collected at remote clients cannot be
centrally aggregated, posing a major obstacle to traditional AI training
methods. Federated learning offers a privacy-preserving alternative by training
local models on distributed devices and exchanging only model parameters.
However, the dynamic topology and limited bandwidth of satellite systems will
hinder timely parameter aggregation and distribution, resulting in prolonged
training times. To address this challenge, we investigate the problem of
scheduling federated learning over satellite networks and identify key
bottlenecks that impact the overall duration of each training round. We propose
a discrete temporal graph-based on-demand scheduling framework that dynamically
allocates communication resources to accelerate federated learning. Simulation
results demonstrate that the proposed approach achieves significant performance
gains over traditional statistical multiplexing-based model exchange
strategies, reducing overall round times by 14.20% to 41.48%. Moreover, the
acceleration effect becomes more pronounced for larger models and higher
numbers of clients, highlighting the scalability of the proposed approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.05382v1">User Privacy and Large Language Models: An Analysis of Frontier
  Developers' Privacy Policies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-05T01:01:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jennifer King, Kevin Klyman, Emily Capstick, Tiffany Saade, Victoria Hsieh</p>
    <p><b>Summary:</b> Hundreds of millions of people now regularly interact with large language
models via chatbots. Model developers are eager to acquire new sources of
high-quality training data as they race to improve model capabilities and win
market share. This paper analyzes the privacy policies of six U.S. frontier AI
developers to understand how they use their users' chats to train models.
Drawing primarily on the California Consumer Privacy Act, we develop a novel
qualitative coding schema that we apply to each developer's relevant privacy
policies to compare data collection and use practices across the six companies.
We find that all six developers appear to employ their users' chat data to
train and improve their models by default, and that some retain this data
indefinitely. Developers may collect and train on personal information
disclosed in chats, including sensitive information such as biometric and
health data, as well as files uploaded by users. Four of the six companies we
examined appear to include children's chat data for model training, as well as
customer data from other products. On the whole, developers' privacy policies
often lack essential information about their practices, highlighting the need
for greater transparency and accountability. We address the implications of
users' lack of consent for the use of their chat data for model training, data
security issues arising from indefinite chat data retention, and training on
children's chat data. We conclude by providing recommendations to policymakers
and developers to address the data privacy challenges posed by LLM-powered
chatbots.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.04710v1">Network-Aware Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-04T23:53:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhou Li, Yu Zheng, Tianhao Wang, Sang-Woo Jun</p>
    <p><b>Summary:</b> Differential privacy (DP) is a privacy-enhancement technology (PET) that
receives prominent attention from the academia, industry, and government. One
main development over the past decade has been the decentralization of DP,
including local DP and shuffle DP. Despite that decentralized DP heavily relies
on network communications for data collection,we found that: 1) no systematic
study has surveyed the research opportunities at the intersection of networking
and DP; 2) nor have there been significant efforts to develop DP mechanisms
that are explicitly tailored for network environments. In this paper, we seek
to address this gap by initiating a new direction of network-aware DP. We
identified two focus areas where the network research can offer substantive
contributions to the design and deployment of DP, related to network security
and topology. Through this work, we hope to encourage more research that
adapt/optimize DP's deployment in various network environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.04358v1">Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the
  Roles of Information Transparency, User Control, and Proactivity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E">
  <p><b>Published on:</b> 2025-09-04T16:19:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Atikkhan Faridkhan Nilgar, Manuel Dietrich, Kristof Van Laerhoven</p>
    <p><b>Summary:</b> Social robots are increasingly recognized as valuable supporters in the field
of well-being coaching. They can function as independent coaches or provide
support alongside human coaches, and healthcare professionals. In coaching
interactions, these robots often handle sensitive information shared by users,
making privacy a relevant issue. Despite this, little is known about the
factors that shape users' privacy perceptions. This research aims to examine
three key factors systematically: (1) the transparency about information usage,
(2) the level of specific user control over how the robot uses their
information, and (3) the robot's behavioral approach - whether it acts
proactively or only responds on demand. Our results from an online study (N =
200) show that even when users grant the robot general access to personal data,
they additionally expect the ability to explicitly control how that information
is interpreted and shared during sessions. Experimental conditions that
provided such control received significantly higher ratings for perceived
privacy appropriateness and trust. Compared to user control, the effects of
transparency and proactivity on privacy appropriateness perception were low,
and we found no significant impact. The results suggest that merely informing
users or proactive sharing is insufficient without accompanying user control.
These insights underscore the need for further research on mechanisms that
allow users to manage robots' information processing and sharing, especially
when social robots take on more proactive roles alongside humans.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.05377v1">Enhancing Gradient Variance and Differential Privacy in Quantum
  Federated Learning</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-04T15:29:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Duc-Thien Phan, Minh-Duong Nguyen, Quoc-Viet Pham, Huilong Pi</p>
    <p><b>Summary:</b> Upon integrating Quantum Neural Network (QNN) as the local model, Quantum
Federated Learning (QFL) has recently confronted notable challenges. Firstly,
exploration is hindered over sharp minima, decreasing learning performance.
Secondly, the steady gradient descent results in more stable and predictable
model transmissions over wireless channels, making the model more susceptible
to attacks from adversarial entities. Additionally, the local QFL model is
vulnerable to noise produced by the quantum device's intermediate noise states,
since it requires the use of quantum gates and circuits for training. This
local noise becomes intertwined with learning parameters during training,
impairing model precision and convergence rate. To address these issues, we
propose a new QFL technique that incorporates differential privacy and
introduces a dedicated noise estimation strategy to quantify and mitigate the
impact of intermediate quantum noise. Furthermore, we design an adaptive noise
generation scheme to alleviate privacy threats associated with the vanishing
gradient variance phenomenon of QNN and enhance robustness against device
noise. Experimental results demonstrate that our algorithm effectively balances
convergence, reduces communication costs, and mitigates the adverse effects of
intermediate quantum noise while maintaining strong privacy protection. Using
real-world datasets, we achieved test accuracy of up to 98.47\% for the MNIST
dataset and 83.85\% for the CIFAR-10 dataset while maintaining fast execution
times.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.04290v1">An Interactive Framework for Finding the Optimal Trade-off in
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-04T15:02:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yaohong Yang, Aki Rehn, Sammie Katt, Antti Honkela, Samuel Kaski</p>
    <p><b>Summary:</b> Differential privacy (DP) is the standard for privacy-preserving analysis,
and introduces a fundamental trade-off between privacy guarantees and model
performance. Selecting the optimal balance is a critical challenge that can be
framed as a multi-objective optimization (MOO) problem where one first
discovers the set of optimal trade-offs (the Pareto front) and then learns a
decision-maker's preference over them. While a rich body of work on interactive
MOO exists, the standard approach -- modeling the objective functions with
generic surrogates and learning preferences from simple pairwise feedback -- is
inefficient for DP because it fails to leverage the problem's unique structure:
a point on the Pareto front can be generated directly by maximizing accuracy
for a fixed privacy level. Motivated by this property, we first derive the
shape of the trade-off theoretically, which allows us to model the Pareto front
directly and efficiently. To address inefficiency in preference learning, we
replace pairwise comparisons with a more informative interaction. In
particular, we present the user with hypothetical trade-off curves and ask them
to pick their preferred trade-off. Our experiments on differentially private
logistic regression and deep transfer learning across six real-world datasets
show that our method converges to the optimal privacy-accuracy trade-off with
significantly less computational cost and user interaction than baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.04232v1">Rethinking Layer-wise Gaussian Noise Injection: Bridging Implicit
  Objectives and Privacy Budget Allocation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-04T14:09:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qifeng Tan, Shusen Yang, Xuebin Ren, Yikai Zhang</p>
    <p><b>Summary:</b> Layer-wise Gaussian mechanisms (LGM) enhance flexibility in differentially
private deep learning by injecting noise into partitioned gradient vectors.
However, existing methods often rely on heuristic noise allocation strategies,
lacking a rigorous understanding of their theoretical grounding in connecting
noise allocation to formal privacy-utility tradeoffs. In this paper, we present
a unified analytical framework that systematically connects layer-wise noise
injection strategies with their implicit optimization objectives and associated
privacy budget allocations. Our analysis reveals that several existing
approaches optimize ill-posed objectives -- either ignoring inter-layer
signal-to-noise ratio (SNR) consistency or leading to inefficient use of the
privacy budget. In response, we propose a SNR-Consistent noise allocation
strategy that unifies both aspects, yielding a noise allocation scheme that
achieves better signal preservation and more efficient privacy budget
utilization. Extensive experiments in both centralized and federated learning
settings demonstrate that our method consistently outperforms existing
allocation strategies, achieving better privacy-utility tradeoffs. Our
framework not only offers diagnostic insights into prior methods but also
provides theoretical guidance for designing adaptive and effective noise
injection schemes in deep models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.05376v1">Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye
  Tracking for Interactive Learning Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-04T13:08:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abdul Rehman, Are Dæhlen, Ilona Heldal, Jerry Chun-wei Lin</p>
    <p><b>Summary:</b> Eye-tracking technology can aid in understanding neurodevelopmental disorders
and tracing a person's identity. However, this technology poses a significant
risk to privacy, as it captures sensitive information about individuals and
increases the likelihood that data can be traced back to them. This paper
proposes a human-centered framework designed to prevent identity backtracking
while preserving the pedagogical benefits of AI-powered eye tracking in
interactive learning environments. We explore how real-time data anonymization,
ethical design principles, and regulatory compliance (such as GDPR) can be
integrated to build trust and transparency. We first demonstrate the potential
for backtracking student IDs and diagnoses in various scenarios using serious
game-based eye-tracking data. We then provide a two-stage privacy-preserving
framework that prevents participants from being tracked while still enabling
diagnostic classification. The first phase covers four scenarios: I) Predicting
disorder diagnoses based on different game levels. II) Predicting student IDs
based on different game levels. III) Predicting student IDs based on randomized
data. IV) Utilizing K-Means for out-of-sample data. In the second phase, we
present a two-stage framework that preserves privacy. We also employ Federated
Learning (FL) across multiple clients, incorporating a secure identity
management system with dummy IDs and administrator-only access controls. In the
first phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%
accuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully
identifying and assigning a new student ID in scenario 4. In phase 2, we
effectively prevented backtracking and established a secure identity management
system with dummy IDs and administrator-only access controls, achieving an
overall accuracy of 99.40%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.04169v1">Privacy Risks in Time Series Forecasting: User- and Record-Level
  Membership Inference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-04T12:43:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicolas Johansson, Tobias Olsson, Daniel Nilsson, Johan Östman, Fazeleh Hoseini</p>
    <p><b>Summary:</b> Membership inference attacks (MIAs) aim to determine whether specific data
were used to train a model. While extensively studied on classification models,
their impact on time series forecasting remains largely unexplored. We address
this gap by introducing two new attacks: (i) an adaptation of multivariate
LiRA, a state-of-the-art MIA originally developed for classification models, to
the time-series forecasting setting, and (ii) a novel end-to-end learning
approach called Deep Time Series (DTS) attack. We benchmark these methods
against adapted versions of other leading attacks from the classification
setting.
  We evaluate all attacks in realistic settings on the TUH-EEG and ELD
datasets, targeting two strong forecasting architectures, LSTM and the
state-of-the-art N-HiTS, under both record- and user-level threat models. Our
results show that forecasting models are vulnerable, with user-level attacks
often achieving perfect detection. The proposed methods achieve the strongest
performance in several settings, establishing new baselines for privacy risk
assessment in time series forecasting. Furthermore, vulnerability increases
with longer prediction horizons and smaller training populations, echoing
trends observed in large language models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.05362v2">AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and
  Conversational Scambaiting by Leveraging LLMs and Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-09-04T00:19:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ismail Hossain, Sai Puppala, Sajedul Talukder, Md Jahangir Alam</p>
    <p><b>Summary:</b> Scams exploiting real-time social engineering -- such as phishing,
impersonation, and phone fraud -- remain a persistent and evolving threat
across digital platforms. Existing defenses are largely reactive, offering
limited protection during active interactions. We propose a privacy-preserving,
AI-in-the-loop framework that proactively detects and disrupts scam
conversations in real time. The system combines instruction-tuned artificial
intelligence with a safety-aware utility function that balances engagement with
harm minimization, and employs federated learning to enable continual model
updates without raw data sharing. Experimental evaluations show that the system
produces fluent and engaging responses (perplexity as low as 22.3, engagement
$\approx$0.80), while human studies confirm significant gains in realism,
safety, and effectiveness over strong baselines. In federated settings, models
trained with FedAvg sustain up to 30 rounds while preserving high engagement
($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage
($\leq$0.0085). Even with differential privacy, novelty and safety remain
stable, indicating that robust privacy can be achieved without sacrificing
performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,
MD-Judge) shows a straightforward pattern: stricter moderation settings reduce
the chance of exposing personal information, but they also limit how much the
model engages in conversation. In contrast, more relaxed settings allow longer
and richer interactions, which improve scam detection, but at the cost of
higher privacy risk. To our knowledge, this is the first framework to unify
real-time scam-baiting, federated privacy preservation, and calibrated safety
moderation into a proactive defense paradigm.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.03350v1">Exposing Privacy Risks in Anonymizing Clinical Data: Combinatorial
  Refinement Attacks on k-Anonymity Without Auxiliary Information</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-03T14:36:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Somiya Chhillar, Mary K. Righi, Rebecca E. Sutter, Evgenios M. Kornaropoulos</p>
    <p><b>Summary:</b> Despite longstanding criticism from the privacy community, k-anonymity
remains a widely used standard for data anonymization, mainly due to its
simplicity, regulatory alignment, and preservation of data utility. However,
non-experts often defend k-anonymity on the grounds that, in the absence of
auxiliary information, no known attacks can compromise its protections. In this
work, we refute this claim by introducing Combinatorial Refinement Attacks
(CRA), a new class of privacy attacks targeting k-anonymized datasets produced
using local recoding. This is the first method that does not rely on external
auxiliary information or assumptions about the underlying data distribution.
CRA leverages the utility-optimizing behavior of local recoding anonymization
of ARX, which is a widely used open-source software for anonymizing data in
clinical settings, to formulate a linear program that significantly reduces the
space of plausible sensitive values. To validate our findings, we partnered
with a network of free community health clinics, an environment where (1)
auxiliary information is indeed hard to find due to the population they serve
and (2) open-source k-anonymity solutions are attractive due to regulatory
obligations and limited resources. Our results on real-world clinical microdata
reveal that even in the absence of external information, established
anonymization frameworks do not deliver the promised level of privacy, raising
critical privacy concerns.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.03294v2">A Comprehensive Guide to Differential Privacy: From Theory to User
  Expectations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-09-03T13:23:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Napsu Karmitsa, Antti Airola, Tapio Pahikkala, Tinja Pitkämäki</p>
    <p><b>Summary:</b> The increasing availability of personal data has enabled significant advances
in fields such as machine learning, healthcare, and cybersecurity. However,
this data abundance also raises serious privacy concerns, especially in light
of powerful re-identification attacks and growing legal and ethical demands for
responsible data use. Differential privacy (DP) has emerged as a principled,
mathematically grounded framework for mitigating these risks. This review
provides a comprehensive survey of DP, covering its theoretical foundations,
practical mechanisms, and real-world applications. It explores key algorithmic
tools and domain-specific challenges - particularly in privacy-preserving
machine learning and synthetic data generation. The report also highlights
usability issues and the need for improved communication and transparency in DP
systems. Overall, the goal is to support informed adoption of DP by researchers
and practitioners navigating the evolving landscape of data privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.10516v1">Privacy-Preserving Personalization in Education: A Federated Recommender
  System for Student Performance Prediction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-09-03T11:28:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rodrigo Tertulino</p>
    <p><b>Summary:</b> The increasing digitalization of education presents unprecedented
opportunities for data-driven personalization, yet it introduces significant
student data privacy challenges. Conventional recommender systems rely on
centralized data, a paradigm often incompatible with modern data protection
regulations. A novel privacy-preserving recommender system is proposed and
evaluated to address this critical issue using Federated Learning (FL). The
approach utilizes a Deep Neural Network (DNN) with rich, engineered features
from the large-scale ASSISTments educational dataset. A rigorous comparative
analysis of federated aggregation strategies was conducted, identifying FedProx
as a significantly more stable and effective method for handling heterogeneous
student data than the standard FedAvg baseline. The optimized federated model
achieves a high-performance F1-Score of 76.28\%, corresponding to 82.85\% of
the performance of a powerful, centralized XGBoost model. These findings
validate that a federated approach can provide highly effective content
recommendations without centralizing sensitive student data. Consequently, our
work presents a viable and robust solution to the personalization-privacy
dilemma in modern educational platforms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.03024v1">Efficient Privacy-Preserving Recommendation on Sparse Data using Fully
  Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-03T05:15:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Moontaha Nishat Chowdhury, André Bauer, Minxuan Zhou</p>
    <p><b>Summary:</b> In today's data-driven world, recommendation systems personalize user
experiences across industries but rely on sensitive data, raising privacy
concerns. Fully homomorphic encryption (FHE) can secure these systems, but a
significant challenge in applying FHE to recommendation systems is efficiently
handling the inherently large and sparse user-item rating matrices. FHE
operations are computationally intensive, and naively processing various sparse
matrices in recommendation systems would be prohibitively expensive.
Additionally, the communication overhead between parties remains a critical
concern in encrypted domains. We propose a novel approach combining Compressed
Sparse Row (CSR) representation with FHE-based matrix factorization that
efficiently handles matrix sparsity in the encrypted domain while minimizing
communication costs. Our experimental results demonstrate high recommendation
accuracy with encrypted data while achieving the lowest communication costs,
effectively preserving user privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.02856v1">Managing Correlations in Data and Privacy Demand</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-02T22:03:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Syomantak Chaudhuri, Thomas A. Courtade</p>
    <p><b>Summary:</b> Previous works in the differential privacy literature that allow users to
choose their privacy levels typically operate under the heterogeneous
differential privacy (HDP) framework with the simplifying assumption that user
data and privacy levels are not correlated. Firstly, we demonstrate that the
standard HDP framework falls short when user data and privacy demands are
allowed to be correlated. Secondly, to address this shortcoming, we propose an
alternate framework, Add-remove Heterogeneous Differential Privacy (AHDP), that
jointly accounts for user data and privacy preference. We show that AHDP is
robust to possible correlations between data and privacy. Thirdly, we formalize
the guarantees of the proposed AHDP framework through an operational hypothesis
testing perspective. The hypothesis testing setup may be of independent
interest in analyzing other privacy frameworks as well. Fourthly, we show that
there exists non-trivial AHDP mechanisms that notably do not require prior
knowledge of the data-privacy correlations. We propose some such mechanisms and
apply them to core statistical tasks such as mean estimation, frequency
estimation, and linear regression. The proposed mechanisms are simple to
implement with minimal assumptions and modeling requirements, making them
attractive for real-world use. Finally, we empirically evaluate proposed AHDP
mechanisms, highlighting their trade-offs using LLM-generated synthetic
datasets, which we release for future research.</p>
  </details>
</div>

